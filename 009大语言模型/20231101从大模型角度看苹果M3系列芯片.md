## 极智 AI | 从大模型角度看苹果 M3 系列芯片

[极智AI | 从大模型角度看苹果M3系列芯片](https://mp.weixin.qq.com/s/SLUS3ssvgn75CdtIZVMsdw)

原创极智视界极智视界 2023-10-31

大家好，我是极智视界，本文来介绍一下从大模型角度看苹果 M3 系列芯片

### 前沿

邀您加入我的知识星球「极智视界」，星球内有超多好玩的项目实战源码和资源下载，链接：https://t.zsxq.com/0aiNxERDq

北京时间今天早上，Apple 发布了 Apple Event - October 30，一个极简的发布会，推出了性能强悍的 M3 系列芯片。在这个只有三十分钟的视频里，尽显苹果的最高科技和炫酷的性能效果，完整的 Apple Event 可以在苹果的官网上看到，有兴趣的同学可以去看看。

先简单介绍一下苹果 M3 系列芯片，要说 M3 芯片，首先还是要提一下 M1 芯片。在 2020 年 11 月 11 日，也就是咱们那年的双十一，Apple M1 正式发布。M1 芯片是 Apple 首款专为 Mac 打造的芯片，拥有格外出色的性能、众多强大的功能以及令人惊叹的能效表现。M1 芯片是 Apple 首款采用先进的 5nm 制程打造的个人笔记本电脑芯片，封装了数量惊人的 160 亿个晶体管，当时真的引起不小的轰动。也就是从那个时候起，MacBook 有了 Intel 芯片 x86 版本的和 M 芯片版本的。

这次推出的 M3 系列芯片，很明显就是 M 系列芯片的第三代，上一代是 M2 系列，可以看到这一代代发展地贼快。在这次的发布会中有说，搭载 M3 芯片的 Mac 相比 Intel 版的 Mac，性能提升会是颠覆性的，差不多会有 11x 的性能提升。所以，后面应该会是 M 系列的 Mac 为主了。(顺便提一句，这次还推出了深空黑色版的 Mac，实属有点帅了，如下)

接着咱们来说说从大模型部署的角度看苹果这次发布的 M3 系列芯片。

需要明确的是，每个系列的 M 芯片都会有三种型号，比如这次的 M3、M3 Pro、M3 Max。当然，你可能还听说过 M2 Ultra，这中 Ultra 系列的芯片其实就是将两块 M2 拼接在一起了 (这跟英伟达 H100 NVL 版本 (双 GPU 版))，主要是用于苹果的 Mac Studio 产品中。众所周知，苹果是不单卖芯片的，都是会以产品的形式集成出售。比如这里的 M3 就会集成在 24 英寸的 iMac 中，而 M3 Pro 和 M3 Max 会被集成在 14/16 英寸的 Macbook Pro 中。

对于大模型的部署，算力设备的下面几个指标至关重要：

显存；

带宽；

算力；

功耗；

框架；

价格；

咱们分别来看，首先是显存。对于大模型的部署来说，显存大真的太重要了，不然直接就是塞不下。苹果 M 系列芯片都采用了统一内存架构，当然 M3 也不会例外，M3 芯片支持的内存容量最高达 128 GB。这有多夸张呢，举个例子，拿今年 (2023 年 6 月 6 日) 发布的 M2 UItra 来说，它的最大内存甚至可以飙到足足 192 GB (这里因为双芯片拼接，AMD 也有这技术)，在苹果芯片的统一内存架构下，这 192 GB 内存就是 192 GB 显存，这意味着八张 4090、两张 H100 才能装得下的 AI 大模型，在 M2 Ultra 这一张芯片上就能够跑起来。那要是后面的 M3 Ultra 呢 (按现在 M3 最大 128 GB 来估算，M3 Ultra 甚至能够轻松突破 200+ GB)，想想是非常可怕。而这一切都是得益于苹果芯片的「统一内存架构」，简单点说就是内存能当显存用。拿 LLaMA-65B 来说，它有 650 亿参数，显存需求是 130 GB，这还真就能直接装得下。在大模型越来越流行的今天，苹果的这套架构就非常具有想象空间了。

再说带宽，在苹果的统一内存架构中，CPU、GPU 和统一内存是直接通过硅介质层连在一起的，所以数据传输带宽非常之高，拿 M2 Ultra 来说，数据读写带宽达到了 800 GB/s。这就已经能够跟正儿八经的独立显卡的显存速度掰掰手腕了，拿 4090 来说，它的显存带宽速度是 1008 GB/s，这是在一个数量级上的。但也需要明确的是，比如 A100 的显存带宽就达到了 1935 GB/s、H100 的显存带宽甚至达到了 2TB/s，这种专业计算卡的带宽都是要比 M2 Ultra 快不少的。但要是拿 NvLink 的带宽 600 GB/s 来说，M2 Ultra 还是胜出的。对于很多 AI 同学还是拿 PCIE 进行数据传输的来说，真的是神仙打架...

再说算力，算力这一块在苹果芯片上是比较迷的指标，而英伟达的显卡通常都会标的十分清晰明了，比如 H100 的 FP32 算力是 51 TFLOPS、Int8 算力是 1513 TOPS，比如 A100 的 FP32 算力是 19.5 TFLOPS、Int8 算力是 624 TOPS。这可能跟苹果和英伟达的产品矩阵不同有关，英伟达的主要产品就是显卡，而算力是显卡的必要参数；而苹果的产品不在于算力卡，所以强调的性能大多是结合产品表现的，比如在 Mac 上就是跑分。

再说功耗，这个指标是苹果 M 芯片比较有优势的地方，由于本身的定位就是个人笔记本电脑产品，所以功耗设计不可能很高。最重要的，M3 芯片采用了目前最为先进的 3nm 工艺生产的，这意味着在同样的芯片面积上能够堆叠更加多的晶体管，能效比的提升会十分明显。

再说框架/软件生态的支持，现在应该可以直接说最流行的深度学习框架就是 PyTorch 了吧，咱们进 PyTorch 的官网，其实 PyTorch 很早就有对 Mac M 芯片的支持。所以对于采用 PyTorch 进行训练、微调大模型的场景，迁移成本会很小。

最后说价格，先不说英伟达的高端显卡买不买得到的问题，就单纯说说价格。一张 H100 在日本的出售价格高达 4745950 日元，折合人民币 24.2 万元；一张 A100 40G 版本的价格为 75599 元、80G 版本的为 91999 元。反观苹果，一台全新的 M3 Max 128 GB 的 16 寸 Mac Pro 售价 39499 元；一台 192 GB M2 Ultra 的 Mac Studio 售价 44999 元。「苹果」、「性价比」？这两个词在这里像是事实但好像又不太搭 ...