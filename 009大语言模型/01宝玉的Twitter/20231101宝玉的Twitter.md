### 01

2023-11-14

英伟达发布最新 AI 芯片 H200 推理速度提升 2 倍，使用成本降低一半

性能提升：H200 的推断速度几乎是 H100 的两倍，

内存升级：H200 是首款采用 HBM3e 内存的 GPU，提供 141GB 的 HBM3e（1.4 倍），显存带宽从 3.35TB / 秒提升至 4.8TB / 秒（2 倍）。

成本和规模：H100 芯片的成本在 25,000 到 40,000 美元之间，H200 在保持与 H100 相同功耗配置的同时，实现了前所未有的性能，使 AI 工厂和超级计算系统更快、更环保，为 AI 和科学界带来经济优势

兼容性：H200 与 H100 兼容，便于现有用户升级。

基于 Hopper 架构：H200 基于英伟达的 Hopper 架构。

Transformer Engine：支持加速基于 Transformer 架构的大型语言模型和其他深度学习模型。

上市时间：计划于明年二季度上市，2024 年将 H100 的产量增加两倍。

云服务和部署：从 2024 年第二季度开始，将提供搭载 H200 的系统和云实例。亚马逊网络服务、谷歌云、微软 Azure 和甲骨文云基础设施将成为首批提供基于 H200 的云实例的云服务提供商。

详细：

[hpc-datasheet-sc23-h200-datasheet-3002446.pdf](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446)

### 01

2023-11-13

如何直接用自己的 Prompt 创建一个中英文双语对照翻译 GPT？

如果你对 Prompt 比较熟悉，可以自己写 Prompt 完成各种任务，那么你可以直接根据 Prompt 去创建一个自己的 GPT。这里以一个中英文对照翻译 GPT 为例。

Prompt 如下：

----

你现在是一位专业的中英文翻译家，擅长将英文翻译成通俗易懂的简体中文。在输出生成结果时，请在输出段落时，同时输出中文和英文对照。参考示例：

输入：

Life is short, you know, and time is so swift; Rivers are wide, so wide, and ships sail far, eme.

We pass away, as the fleeting instant of an eye, you know; We come together, like the meeting of water and the sky, eme.

输出：

Life is short, you know, and time is so swift; Rivers are wide, so wide, and ships sail far, eme.

生命短暂，时光飞逝；江河轻阵，船只远行。

We pass away, as the fleeting instant of an eye, you know; We come together, like the meeting of water and the sky, eme.

我们约若眨眼间即逝的时光；我们的相遇如同天际与江水的融合。

现在请翻译以下英文为中英文双语对照：

----

最终成品：

[ChatGPT - 中英文对照翻译](https://chat.openai.com/g/g-DrY6bVei2-zhong-ying-wen-dui-zhao-fan-yi)

### 01

2023-11-13

分享一下我们在内部系统中使用函数调用的经验。我们尝试了各种使用 LLM 的方法，基本上涵盖了 deeplearning ai 课程中的技巧。幸运的是，我们测试接近尾声的时候，Function Calling 发布了，我们第一时间就开始尝试了。我们断定这是 OpenAI 未来的重点，也是一个可行的路线。1/n

我们当时放弃了 LangChain 和其他原型，专注于 Function Calling，并在当周完成了 demo 开发。效果非常好。

Function Calling 有以下几个优点：

1. 易于现有系统集成

2. 无需分享私有数据

3. 无需维护代理数据结构（专门给 OpenAI 看的，以保护实际数据库结构）

4. 易于测试

5. 易于扩展

下面展开说 2/n

北火 @beihuo·23 小时

首先，我们认为 LLM 变化太快，应该先用于内部系统，而且存在 PromptInjection 问题且无法有效控制用户话题。我曾经测试过 Character AI 上流行的机器人，很容易就能改变它们的角色设定，并回答你任何问题。

因此，出于安全考虑，我们决定先从内部工具入手，而不是开发面向客户的功能。3/n

北火 @beihuo·23 小时

内部工具存在两个主要问题：开发界面周期长且回报低。因此，我们采用 LLM 与用户进行对话，并由 LLM 决定调用哪个函数。在获取到函数名称和参数后，我们再实际调用该函数，函数内部负责用户鉴权和 API 调用。

这种方法实际上将内部工具的开发时间从 X 个月压缩到 X 小时。4/n

北火 @beihuo-23 小时

上面提到，我们提供给 LLM 的是函数，而不是代码或数据接口。这样我们就能控制对话中包含哪些信息。需要强调的是，我们必须自己回答问题，而不是将数据返回给 LLM 让它回答。否则就会陷入无尽的幻觉问题中。

比如，有一些分页和 filter 信息需要在后面使用，我们必须自己构造消息。5/n

北火 @beihuo·23 小时

解决了安全和幻觉问题后，我们开始了框架开发。在我们的框架中，程序员只需开发函数，并将 Prompt 放入注释中。我们可以自动将其封装为 Function Calling 供 chatbot 使用。

程序员还可以注入全局上下文，控制消息生成等。如果有现有的 API，几分钟内 chatbot 就可以使用。效率非常高。6/n

北火 @beihuo·23 小时

剩下的问题就是测试了。由于 LLM 返回结果的不确定性。我们测试分成了三层。第一层就是常见的 unit tests，第二层是 function call tests，第三层是 conversation tests. 7/n

北火 @beihuo·23 小时

Function call tests 我们会去调用真实 LLM，但是只检查是否正确调用 function，参数是否正确。会尽可能覆盖所有情况。但是这一层我们 mock 了数据库和 API，专心测试 LLM 的 function calling 本身。8/n

北火 @beihuo·23 小时

Conversation tests 就更接近真实了。我们会在一个对话中编写更多的消息，也会进行多轮测试。但是这里主要存在的问题是 LLM 返回内容不确定性。我们没办法对比两个回答是否一致。

这里我们主要是采用关键字，失败之后 retry 的方式进行测试。然后一边等待业界的新方案。9/n

北火 @beihuo·23 小时

这次 OpenAI 发布的 Reproducible outputs 直接补上了这最后一个拼图！这让系统变得可测试了！

更令人开心的是 Assistant API, 简化了我们维护对话和 user-specified data 的过程。我们当时第一时间就抛弃了 LangChain 并且认为 LangChain 不会长久，这个评价现在看来是对的。10/n

北火 @beihuo·23 小时

现在我有信心说，借助 Assistant API 和 Function Calling, 我们已经可以面向用户开发新功能了。

整个系统的重点是，只允许 Function 访问内部系统和数据，自己控制输出和 side effect，做好用户鉴权，尽早完成对话。另外我们还发现有一个小技巧很有用。11/n

北火 @beihuo·23 小时

那就是维持两套对话系统。一套是 chatbot 里面显示的，一套是给 LLM 运算的。这样我们就可以在 LLM 对话记录中放置大量信息以控制对话，并且有效减少幻觉，而用户看到的是更自然的对话和丰富的格式。

比如，用户看到的是一个 barchart，但是 LLM 看到的是一个 YAML 数据。

12/n

北火 @beihuo·23 小时

OK，以上就是我们的一点经验。希望有一点帮助！13/13




