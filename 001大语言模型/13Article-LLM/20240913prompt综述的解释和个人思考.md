## 20240913prompt综述的解释和个人思考

20240913prompt综述的解释和个人思考

AINLP 2024 年 08 月 31 日 22:45 江苏

上周手上不太方便，即使后续好了也没有搞定（不过说实话，这篇文章的量似乎没读完也不好搞定）。

最近是有 3 篇 prompt 的综述非常出名：

The Prompt Report：A Systematic Survey of Prompting Techniques

A Systematic Survey of Prompt Engineering in Large Language Models：Techniques and Applications

A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks

每篇综述的内容都比较丰富，尤其是第一篇，已经到了 76 页，然而从综述角度，需要把尽可能完善地把研究领域内的关键文章都给说明白，然而从应用角度或者学习角度，从中能找到适合自己的部分，快速过滤和筛选则更为重要，今天这篇文章，是综合上述 3 篇综述的内容以及个人在实际应用中的经验，从中抽取可靠的方案和思路总结。

此处描述的 prompt 限定在文本的、不经过模型参数更新的 prompt 系列方案，涉及多模态方面的，暂时不展开。本文的核心行文主要依照上述第一篇的思路来进行，中间会补充另外两篇的内容。

### 01. prompt 的概念和组成成分

首先还是明确一下这里 prompt 的定义，用的是 [1] 的解释：

A prompt is an input to a Generative AI model，that is used to guide its output.

同样是在 [1] 中，也给出了 prompt 常见的组成成分，这里非常建议大家在写 prompt 的时候按照这 4 个要素来设计，内容表示的越清楚，最终效果就会越好。

1、指令，即核心目标，如「请判断这句话是积极还是消极」。

2、示例，给出一些案例供参考和比对，如「请参考下面几个案例」。

3、输出格式，对最终输出的结果要求，例如分点、json 格式之类的。

4、角色。已经有大量实验证明，给大模型赋予角色，能很大程度提升大模型的输出结果。

5、附加信息。其他需要配合任务进行输出的结果，如要做翻译，那就得提供原文，或者是 RAG 需要提供 reference 等。

### 02. 类目和场景的划分

prompt 现阶段的方案已经有很多了，但在了解 prompt 下的具体方案之前，还是想先和大家讲一下，这些方案如何归类，具体的倾向性和使用场景是啥样的。

在 [2] 中，论文按照主要功能对 prompt 中的常用方案进行了划分，基本结构是这样的：

从这里可以看到，作者是按照核心功能来给各种 promp 方案进行方案的划分，可以看到在 reasoning and logic 上有积累大量的工作，类似大家比较熟悉的 CoT 就被划分在这里面，除此之外，降低幻觉、APE、代码生成、一致性、优化效率等问题也是大家所关注的焦点。

而在 [1] 这篇论文里，则把各种方案总结为了 6 个核心类别：

1、上下文学习（in-context learning）。通俗而言，举例子，换个角度，few shot 之类的也属于这个范畴。

2、零样本学习。类似 CoT、Role-Play（角色）、风格、情感的提示，都属于这个范畴。

3、思想生成。常见的就是 CoT 及其变体。

4、分解。把问题进行拆解然后逐步完成的思想，论文似乎都不太出名，不过日常工作挺经常用到的，核心难点在于如何拆解，论文里类似「Least-to-Most Prompting」之类的都有提及。

5、集成。这个严格来说不算分解的反义，这里指的是通过多次或者多种方式对统一方案进行验证来实现最终效果的可靠性，比较典型常用的就是 self-consistancy，通过重复让大模型生成自己的答案来加强对最终结果的验证，配合 CoT 和非 0 的 temperature 即可实现多次的结果生成，而 Prompt Paraphrasing 则是通过改写 prompt 来确认最终效果，也是类似的思想。

6、自我批评。集成强调的是生成结果的多次，而自我批评则是强调对生成结果的验证，让大模型自己判断内容是否正确。Self-Calibration 就是非常典型的，在原有生成的基础上，把问题 + 回复重新输入大模型让大模型来判断是否正确。

这 6 个类型虽不互斥，但基本就是我们常见的比较直接调优 prompt 的核心思路，在面对大模型的 bad case 想要通过 prompt 来优化时，非常建议通过这个思路来考虑调优。

这里补充说一下 [3]，这篇论文是 NLP 任务项，在第二章内讲解了 39 种 Prompt 的方法，倒是没有进行归类式的讨论，基本都是摘要级的描述，大家有兴趣还是可以展开找到对应论文来阅读，这个挺耗费时间不过还是会有些价值。

### 03. Prompt 方案

论文里提供的方法很多，也比较杂，这里提供几个比较常用或者好用的方案，大家可以在日常尝试。

#### in-context learning

来看看 [1] 中给的定义：

ICL refers to the ability of GenAIs to learn skills and tasks by providing them with exemplars and or relevant instructions within the prompt，without the need for weight updates/retraining.

这里强调的是 exemplars 和 relevant instructions。前者，简单地说，就是在 prompt 提供例子，供模型参考，这点其实和人也比较类似，很多事情说不明白的时候，给例子能大幅度降低描述成本，告诉模型「你就照着做就好了」，而在实践过程中，通常要注意这几个方面：

示例的个数。由于长文本等原因，个数并非多多益善，合适的个数对模型的接受程度有提升，多了反而不合适，可以筛选（例如用相似度来晒，之前的文章有提到：心法利器 [114] | 通用大模型文本分类实践（含代码）），也可以直接就开始训练吧。

[心法利器[114] | 通用大模型文本分类实践（含代码）](https://mp.weixin.qq.com/s?__biz=MzIzMzYwNzY2NQ==&mid=2247490153&idx=1&sn=01d744d622abed04550dc7be30a03f65&chksm=e88242f7dff5cbe1b1e892ff95e26853b21f8ca24d82fd34fe3fa4d5e1674f2aaff66f7dd90d&scene=21#wechat_redirect)

示例的排序。实验表明，示例的排序对最终结果是有影响的，合理的排序有利于最终效果的提升。

标签的分布。即示例内每个标签的个数，某个类目的标签过多也会导致模型的误判。

标签的质量。即标注准确性，这个无论在哪都挺重要的，但大模型下似乎有一定容忍度，不想一般的模型训练那么敏感。

示例的格式。因为一般会给多条，示例需要一定的标准格式，这个格式会约束模型最终输出的正确性和规范性。

示例的相似度。一般而言，例子给得越贴切，模型预测会越准。

除了例子外，relevant instructions 也是有用的，在例子可能并不好举，或者用文字描述可能更清楚的时候，这时候直接用描述会更方便，当然这个的收益很大程度以来模型自己本身的能力，尤其是指令的遵循能力，此时可以考虑的角度，参考如下：

角色。制定对应任务的角色，会让模型带入，能提升效果，甚至加一些夸奖的褒义词，如「优秀的」，也会有收益。

情感提示。加一些类似「这对我的职业生涯很重要」的情感提示，会有效果收益。前段时间的类似「给你 10 美元」的梗，也是实践中多少有些效的。

类 System 2 Attention 方法。先让大模型重写自己写好的 prompt，删除、补充特定的内容，然后再请求大模型处理得到新的内容。这里的处理可以是删除多余、误导、错误内容（System 2 Attention），可以是调整得到的事实内容（SimToM），也可以是对 prompt 进行重新调整（Rephrase and Respond）之类的，包括 Re-reading 这种灵活的重写 prompt。

#### Chain-of-Thought（CoT）

CoT 相信大家都比较熟悉了，思维链是一种引导大模型自己思考然后生成的方法，通过引导大模型逐步思考，能让模型解决更复杂的问题的能力，同时对于简单问题的回复也会更稳定。我理解其核心机理是，大模型提供的思考过程能让最终解码时要考虑的全局最优信息更加稳定可靠。

最简单的方案就是直接加一句指令「让我们一步一步来思考这个问题」，当然在此技术上，可以结合问题的内容，提供诸如「从 XX 角度逐步思考」、「请按照 XX 步骤思考」来进行强化，类似的方案在基础能力比较强（逻辑推理能力）的模型里，都会有比较大的收益。

这种方案的优点就是修改成本会比较低，但缺点是大模型最终生成答案内容会比较长，伴随的问题就是解析难度变高、耗时变长等。

#### 解析分解

我自己的理解，解析分解是一种 CoT 的升级思路，把一个任务分解为多个任务，逐步完成，一方面能更好控制整个过程，另一方面准确率会有提升。这是主要的思想吧，而在实际操作中，拆解的方式可能会有所不同，这里从论文中抽几个作为例子，供大家实际情况选择和判断。

Least-to-Most Prompting：考虑分步，依次解决每个问题，然后最终推导出最终结果。

Decomposed Prompting：用 few-shot 配合大模型决策调用什么函数来解决特定问题，函数对应的是解决特定问题的方案，这个看起来在 agent 领域已经被广泛使用，当然了这个也跟搜索对话里的意图识别 + 特定意图的处理有异曲同工之妙。

Plan-and-Solve Prompting：让模型自己设计计划，然后按照这个计划自己执行。

这里可以看到，在任务拆解后，请求大模型的次数会变多，次数的增加对耗时的压力无疑是巨大的，在实际应用中，这个需要被纳入考虑，另外还需要注意两个点：

拆解后的每一步，最好都能监控到其效果，避免出现显著明显短板从而影响最终效果。

拆解后的部分步骤都可以仔细评估，反思是否每个步骤都刚需大模型，类似 Decomposed Prompting 中的决策，在数据比较多的场景，可能小模型或者规则也能做好这种决策，此时可以有效提升效率而降低成本。

#### 集成

集成的思想在原来深度学习、机器学习的早期就已经有考虑到，其核心思想就是构造多个类似的结果然后合并，类似 Self-Consistency 通过 CoT 产生多个结果然后综合评估，Demonstration Ensembling 通过多种 few-shot 结果来判断，Mixture of Reasoning Experts 是用 MoE 多专家系统提供不同的推理思路。

#### Self-Criticism

自我批评旨在利用大模型回溯，验证自己生成的结果是否正确，最简单的方式就是直接把问题 + 大模型答案通过 prompt 拼接让模型进行结果验证（Self-Calibration），更进一步则有 Self-Refine 进一步提供修改建议或者完成修改。

prompt 并非局限在自己的编辑，研究上还会有很多细分的场景和思路。

### 04. 其他 prompt 的研究方向和思路

#### Prompt 工程

Prompt 工程根据 [1] 中的定义，是指在 Prompt 工程是「automatically optimize prompts」，即自动优化 prompt。

Meta Prompting 是通过大模型直接生成一个比较基础的 baseline，同时能通过数据反复调优这个 prompt，内部人工的干预会大幅减少。

AutoPrompt 利用已有样本，通过推理反馈自动化逐步调优原有的 prompt。

Automatic Prompt Engineer 利用一组样本生成 zero-shot 的 prompt，从而选择最优的结果。

Prompt Optimization with Textual Gradients 看起来是前面的升级版，会通过 Self-Criticism 的手段进一步调优最终的效果。

prompt 工程强调的是自动化调优，实践上能很大程度给调出一个不错的结果，但是从实践经验上看，上限可能并不会很高，在自动化调优过程中，如果数据集弄得不好，会出现 prompt 可能会被往错误的方向带偏的错误，因此在使用过程中注意监控 prompt 后续修改后的结果。

#### 答案工程

答案工程旨在解决我们在实践的过程中经常会遇到的一个问题 —— 答案内容不符合我们预期的格式，一般地会有两种方案，通过 prompt 进行约束，另一种是进行答案的后处理，无论是何种方式，基本不会逃离下面几个关键要素：

答案形式，文字图片还是视频，文字的字数和格式限制，或者是用 json、xml 格式来处理。

答案空间，类似分类问题，是在给定空间内进行有限的选择。

答案提取器，例如 json 结构，我们需要借助正则之类的方式来识别大括号来划定 json 的范围进行转化。

这方面在 [1] 内，除了在答案工程里提到，另外在大模型的 output format 里面也提到了类似的东西，此处我把他们放一起说了。

#### Agent

Agent 相信很多人都有了解甚至在做这个方向，prompt 无疑在这个方向也有很大的发挥空间。

图片

这里作者把 tool use（function call）、基于代码的生成、Observation-Based（有点像 RL）、RAG 之类的方法放里面，这块的总结说实话相比专门做对应领域的，例如我所比较熟悉的 RAG，前段时间在这块我已经写了很多文章了（心法利器 [111] | 近期 RAG 技术总结和串讲（4w 字 RAG 文章纪念）），prompt 内讲的还是比较粗的，此处就不展开了，大家可以结合自己的需要去阅读自己的综述，这里就是抛砖引玉了。

#### prompt 的评估

综述文总避不开对研究对象的评估方法，文章中讲了很多，但全文看下来基本把常见的 NLP 指标都覆盖了进去，从综述的角度确实是有必要的，但从实践角度，我个人认为还是应该从任务本身的目标出发，看具体要做的是什么任务，分类、评分、实体抽取、回复生成还是什么方面，然后结合实际情况设计合适的指标才是正解，做研究要求相对规范，但是实践是肯定要因地制宜的。

#### 安全和对齐问题

这里的安全是指需要应对设计恶意 prompt 来诱导生成式 AI 产生错误或者有害信息的安全问题。常见的是通过诱导来注入或者诱导输出有害信息，造成信息的泄露或者破坏系统的可靠性和完整性，常见的防御手段是对 prompt 内容进行质检校验，使用对抗性策略增强模型的鲁棒性，同时定期进行安全性评估和监控，及时发现和应对提示攻击。

至于对齐，此处考虑的是人和大模型之间理解的对齐，我们很容易会想到 RLHF 之类的手段来优化，而在 [1] 中，作者把对齐问题拆解成了 prompt 敏感性、过度自信的矫正、偏见刻板和文化差异、歧义这几个问题，论文中给出了一些解决方案，当然这里也提到了一些上面说到的方法，例如 few-shot 等。

图片

### 05. 一些选型的建议

prompt 上的技巧还是很多的，这里列举几个可能比较有用而且性价比还不错的角度，供大家选择和使用。

首先是从 prompt 的组成成分而言，尽量把各个组分给准备清楚，即前面章节提到的指令、示例、格式、角色、附加信息，在尽可能完整的情况下，一般会有还可以的效果。尤其要关注附加信息，对特定任务，合理、完整、严谨描述是最终效果的必要保证。

in-context learning，简单的说就是例子，在例子的辅助下，大模型的生成能更加符合我们的预期，无论是格式上还是结果的准确率上。

CoT，我是指最简单的 CoT，复杂的 CoT 变体对任务的特异型还是比较高的，但是基础的 CoT 其实已经一定程度能为大模型提供支持，但因为大模型自己还要做思维链推理，所以耗时的提升还是会很明显的。

Self-Criticism 对最终效果的提升还是有的，而且还挺明显，但问题是需要再用一次大模型，本身自我批评的 prompt 还得写，稍微有些麻烦，但在最终效果上收益还挺大的。

分解稍微有些推荐吧，对复杂的任务，无论是横向的多个相同然后集成，还是纵向的把任务分步做，都会有一定收益，但是着实增加大模型请求次数了。

对于一些性能压力比较大的情况，CoT、Self-Criticism、分解、集成之类的思路其实也能做，可以用来做一个比较高的 baseline，然后对大模型进行进一步微调，调成更简单的模式，也是一个思路，相当于预标注 + 微调的蒸馏模式了。

### 06. 小结

说实话，prompt 工程这块的诸多论文和结论，本质都挺实验科学地，通过不断的实验来得出效果的好坏结论，本期是文章是结合论文的阅读和自己的经验总结的文章，有需要的同学还是很推荐大家看完的。

### 07. 参考文献

首先是本文重点关注的 3 篇论文。

[1] The Prompt Report：A Systematic Survey of Prompting Techniques

[2] A Systematic Survey of Prompt Engineering in Large Language Models：Techniques and Applications

[3] A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks

关于 AINLP

AINLP 是一个有趣有 AI 的自然语言处理社区，专注于 AI、NLP、机器学习、深度学习、推荐算法等相关技术的分享，主题包括 LLM、预训练模型、自动生成、文本摘要、智能问答、聊天机器人、机器翻译、知识图谱、推荐系统、计算广告、招聘信息、求职经验分享等，欢迎关注！加技术交流群请添加 AINLP 小助手微信（id：ainlp2），备注工作 / 研究方向 + 加群目的。
