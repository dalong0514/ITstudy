## 20250116Tumbling-Training-Costs-Desktop-AI-Supercomputer-Tighter-AI-Export-Restrictions-Improved-Contrastive-Loss

[Tumbling Training Costs, Desktop AI Supercomputer, Tighter AI Export Restrictions, Improved Contrastive Loss](https://info.deeplearning.ai/tumbling-training-costs-desktop-ai-supercomputer-tighter-ai-export-restrictions-improved-contrastive-loss-2?ecid=ACsprvt8VThKD6YuE1oMkpDB7E750GwQmvzXdTaI1OXOTVj6MYW5vjqARhroDs5D4t_26QAwDT6B&utm_campaign=The%20Batch&utm_medium=email&_hsenc=p2ANqtz-_J2Q53tNazOzBpJ3oL7MKzmjR62FoI6_0fiAFsowJfMZmUg44867Gi34nKMpahvyIRiWxcynoenzfI_TzkTGHjFf8qGA&_hsmi=342717512&utm_content=342717257&utm_source=hs_email)

The Batch top banner - January 15, 2025

Dear friends,

Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!

开发软件，特别是制作原型，成本正在降低。这将导致对那些能够决定开发什么产品的人才的需求增加。AI 产品管理（AI Product Management）前景一片光明！

Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.

软件开发通常由团队协作完成，团队中包括产品经理（PM），他们负责决定开发什么（例如为哪些用户实现哪些功能）；以及软件开发人员，他们负责编写代码来实现产品。经济学原理表明，当两种商品互为补充时 —— 比如汽车（内燃机）和汽油 —— 其中一种商品的价格下降会导致对另一种商品的需求上升。举例来说，当汽车价格下降时，会有更多人购买汽车，从而导致对汽油的需求增加。软件领域也会发生类似的情况。在明确了需要构建的具体内容后，生成式 AI（Generative AI）正在让软件的构建过程变得更加快速和低廉。这将显著增加对那些能够为有价值的软件产品提出明确需求规范的人才的需求。

This is why I'm excited about the future of Product Management, the discipline of developing and managing software products. I'm especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.

这就是为什么我对产品管理的未来感到兴奋，它是一门关于开发和管理软件产品的学科。我尤其对人工智能（AI）产品管理的未来充满期待，这是一门关于开发和管理人工智能软件产品的学科。

Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.

许多公司的工程师和产品经理的比例，例如是 6:1。（这个比例因公司和行业而异，通常在 4:1 到 10:1 之间。）随着编码效率的提高，我认为团队将需要更多的产品管理工作（以及设计工作），这些工作在员工总数中所占的比例也会增加。也许工程师会参与承担一些这项工作，但是如果这项工作仍然由专门的产品经理负责，那么这些职位的需求将会增加。

This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.

软件开发团队的构成转变目前还未完全加速。尤其是在 AI 产品管理方面，一个阻碍这种转变的主要因素是，软件工程师由于其技术背景，对 AI 的理解和接受速度远超产品经理。即使在今天，大多数公司都很难找到既具备产品开发能力又了解 AI 的人才，而且我预计这种人才短缺的情况还会更加严重。

Fig: Two colleagues look at a computer screen. One mentions their chatbot's Hollywood success, and the other suggests hiring an AI Product Manager.

两位同事看着电脑屏幕。一位说他们的聊天机器人在好莱坞取得了成功，另一位建议聘请一位 AI 产品经理。

Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:

此外，AI 产品管理所需的技能与传统软件产品管理不同。它需要：

1 Technical proficiency in AI. PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models.

AI 技术能力。产品经理需要了解在技术层面，哪些产品是可行的。他们还需要了解 AI 项目的生命周期，包括数据收集、模型构建，以及后续的监控和维护。

2 Iterative development. Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process.

迭代开发。由于人工智能（AI）开发比传统软件更加强调迭代，并且在开发过程中需要更多地调整方向，因此 PM 需要了解如何管理这样的过程。

3 Data proficiency. AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.

数据运用能力。人工智能产品通过数据学习，并且它们还能被设计成生成比传统软件更丰富的数据。

4 Skill in managing ambiguity. Because AI's performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.

应对模糊性的能力。由于人工智能的性能难以提前预测，产品经理需要适应这种情况，并制定相应的应对策略。

5 Ongoing learning. AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users' lives.

持续学习。人工智能技术正在快速发展。产品经理（PM）和其他任何想要充分利用这项技术的人一样，需要跟上最新的技术进步、产品理念以及它们如何融入用户的生活。

Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at gathering feedback fast to keep projects moving. Increasingly, I also expect strong product managers to be able to build prototypes for themselves .

最后，AI 产品经理需要了解如何负责任地部署 AI，例如，当我们需要设置防护措施以避免不良后果时，并且还应擅长快速收集反馈，以推动项目进展。此外，我越来越期望优秀的产品经理能够自己构建产品原型。

The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.

市场对优秀的 AI 产品经理的需求将非常大。随着 AI 产品管理学科的发展，一些工程师也可能会承担更多的产品管理工作。

The variety of valuable things we can build is nearly unlimited. What a great time to build!

我们能创造的价值几乎是无限的，真是大展身手的好时代！

Keep learning,

不断学习，

Andrew

A MESSAGE FROM DEEPLEARNING.AI

来自 DeepLearning.AI 的消息亲爱的 Andrew，

我们很高兴地宣布，由 DeepLearning.AI 与 AWS 合作开发的全新课程「构建和应用生成式 AI（Generative AI）」正式上线。

生成式 AI（Generative AI）正在迅速改变各行各业。这项变革性技术带来了前所未有的机遇，同时也伴随着一定的复杂性。为了帮助大家更好地了解和应用生成式 AI（Generative AI），我们推出了这门全面的课程，旨在提供在实际工作中构建和应用生成式 AI（Generative AI）模型所需的实用技能。

在本课程中，您将：

*  探索生成式 AI（Generative AI）的核心概念，包括扩散模型（diffusion models）、变分自编码器（variational autoencoders）和生成对抗网络（generative adversarial networks）等。简单来说，这些都是生成式 AI（Generative AI）用来创造新数据的不同方法。
*  学习如何使用 Hugging Face Transformers 等工具库，来微调和部署生成式 AI（Generative AI）模型。就像给 AI 模型做精细调整，让它们更好地为我们工作。
*  通过实践，获得关于模型评估、监控和负责任的 AI 等方面的最佳实践经验。
*  深入了解生成式 AI（Generative AI）在实际中的应用，包括文本生成（例如写作助手）、图像生成（例如图片编辑）和音乐生成（例如创作新音乐）等。

无论您是经验丰富的从业者，希望进一步提升技能，还是渴望探索生成式 AI（Generative AI）领域的新手，本课程都将帮助您掌握在工作中应用生成式 AI（Generative AI）的各项技能。

本课程将使用亚马逊云科技（Amazon Web Services）的工具和平台。您将有机会体验 Amazon SageMaker 以及其他相关服务。

我们相信，这门课程将为您提供在生成式 AI（Generative AI）领域取得成功所需的知识和专业技能。诚挚邀请您今天就加入我们的学习之旅。

真诚的，

DeepLearning.AI 团队

Promo banner for "Reasoning with o1"

Get up-close and personal with OpenAI's groundbreaking o1 model! In our short course "Reasoning with o1," you'll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting. Enroll today

「探索 o1 的推理能力」促销横幅深入了解 OpenAI 的突破性 o1 模型！在我们的短期课程「探索 o1 的推理能力」中，你将学习如何在编程、规划和 STEM（科学、技术、工程和数学）任务中获得最佳性能；执行复杂的多步骤任务；并通过元提示（meta prompting）优化提示。立即加入，开启你的 AI 之旅！

[Reasoning with o1 - DeepLearning.AI](https://www.deeplearning.ai/short-courses/reasoning-with-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=342717512&_hsenc=p2ANqtz-_IJhJ_9TU5XH5GfAenDl_X-mivelGhXaEpFY1gp4f_em2dOr7oufI5kxBG0I98V_GQ3Bftg9NJ2APLr8vk8TTard_4zg)

### News

新闻

DeepSeek-V3 accuracy across benchmarks compared to other AI models.

DeepSeek-V3 在各项基准测试中的准确率，与其他 AI 模型对比。

DeepSeek Ups the Open Weights Ante

DeepSeek 提高了开放权重门槛

A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.

来自杭州初创公司 DeepSeek 的一款新模型展现了卓越的性能，并有可能改变训练成本的计算方式。

What's new: DeepSeek-V3 is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are open except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them here.

最新消息：DeepSeek-V3 是一个开源的大型语言模型（Large Language Model），在关键基准测试中优于 Llama 3.1 405B 和 GPT-4o，并且在编程和数学方面取得了卓越的成绩。该模型的参数是公开的，但以下应用除外：涉及军事用途、危害未成年人、生成虚假信息以及其他类似限制。您可以在这里下载模型参数。

How it works: DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 the time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.

工作原理：DeepSeek-V3 是一种混合专家（MoE）架构的 Transformer 模型。Transformer 是一种深度学习模型，这里 DeepSeek-V3 拥有 6710 亿个参数，但并非所有参数都会同时工作，在任何时刻只有 370 亿个参数处于激活状态。DeepSeek 团队仅用 279 万 GPU 小时就完成了模型的训练，这还不到训练 Llama 3.1 405B 所需时间的十分之一。值得一提的是，DeepSeek-V3 的性能显著超越了 Llama 3.1 405B，而训练成本仅为 560 万美元，可以说成本非常低。

The developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by DeepSeek-R1 and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as group relative policy optimization.

该模型使用大约 15 万亿个 Token（Token）进行了训练，其中编码和数学数据的比例相较于 DeepSeek-V2 更高。研究人员利用 DeepSeek-R1 和 DeepSeek-V2.5 生成的输出，在各种任务上对该模型进行了微调。此外，他们还采用了名为群体相对策略优化的强化学习算法，进一步提升了该模型在多个领域的性能。

Earlier work showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren't used at inference.

之前的研究表明，训练模型预测接下来的两个 Token ，比只预测一个 Token 能获得更好的性能。作者们采用了这种方法。模型首先像往常一样学习预测第一个 Token ，然后利用一组额外的网络层来学习预测第二个 Token。这些额外的网络层在推理阶段不会被使用。

Following DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.

继 DeepSeek-V2 之后，DeepSeek-V3 使用了多头潜在注意力（multi-head latent attention）技术，这种技术相较于其他注意力机制的变体，在模型运行时能够节省更多的内存。

Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.

与 DeepSeek-V2 类似，新模型也采用了结合专用（routed）专家和共享专家的架构。对于特定的输入，模型会从 256 个专家中选择 8 个进行处理，同时还会使用一个处理所有输入的共享专家。

Results: In DeepSeek's tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.

结果：DeepSeek 的测试表明，DeepSeek-V3 在所有方面都超越了 Llama 3.1 405B 和 Qwen 2.5 72B，并且其性能可与 GPT-4o 媲美。

DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on Polyglot, which tests a model's ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).

DeepSeek-V3 在编码和数学任务中表现出卓越的性能。在编码方面，DeepSeek-V3 在七项测试基准中的五项上都表现出色。然而，根据公开排行榜，DeepSeek-V3 在这五项中的一项上输给了 o1。具体来说，在 Polyglot 基准测试中，该测试旨在评估模型在多种编程语言中针对复杂请求生成代码的能力，DeepSeek-V3 的准确率为 48.5%，虽然击败了 Claude Sonnet 3.5（准确率 45.3%），但仍逊色于 o1（准确率 61.7%）。

In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.

在语言处理任务中，它的表现与 Claude 3.5 Sonnet 难分伯仲，在某些任务中得分更高，在另一些任务中则稍逊一筹。

Behind the news: OpenAI's o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.

新闻解读：OpenAI 的 o1 模型之所以表现出色，得益于其采用的智能体工作方式，在这种方式下，模型会检查自身的输出结果，使用各种工具等等。而 DeepSeek 则另辟蹊径，在不依赖这种智能体工作方式的情况下，也取得了卓越的成果。

Why it matters: Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3's shockingly low training cost. The team doesn't explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits "meticulous engineering optimizations.") But it's likely that DeepSeek's steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn't employ MoE. In 2022, Microsoft found that MoE cost five times less in training for equal performance compared to a dense model, and Google and Meta reported that MoE achieved better performance than dense models trained on the same numbers of tokens.

重要性在于：开放模型不断对闭源模型发起挑战，为开发者提供可自由修改和部署的高质量选项。更重要的是，DeepSeek-V3 的训练成本低得惊人。该团队并未明确解释该模型如何在如此低的计算资源消耗下实现卓越性能。(该论文归功于「细致的工程优化」。）但 DeepSeek 对混合专家模型（MoE）的持续改进很可能是一个关键因素。DeepSeek-V2 也是一个混合专家模型（MoE），与未采用混合专家模型（MoE）的早期版本 DeepSeek 67B 相比，训练成本节省了 40% 以上。2022 年，微软的研究表明，与稠密模型相比，混合专家模型（MoE）在达到相同性能水平时，训练成本降低了五倍，而谷歌和 Meta 的研究也表明，在相同的 Token 数量下训练，混合专家模型（MoE）的性能优于稠密模型。

We're thinking: If they can be replicated, DeepSeek's results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.

我们认为：如果 DeepSeek 的研究结果可以被复现，那么它将对基础模型的训练成本产生重大影响。如果构建一个 GPT-4o 级别的模型确实只需要大约 500 万美元，那么将会有更多的团队能够训练这类模型，并且与 AI 巨头竞争的成本可能会大幅降低。

World map of AI export restrictions: Tier 1 (green), Tier 2 (gray), Tier 3 (red).

人工智能出口限制的世界地图：第一级（绿色），第二级（灰色），第三级（红色）。

U.S. Moves to Expand AI Export Restrictions

美国着手扩大人工智能出口限制

The United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.

美国提议对人工智能（AI）技术出口进行限制，这将在之前限制的基础上大幅扩大，从而建立一套获取先进芯片和模型的新国际等级体系。

What's new: The Biden administration, which will transition to leadership under incoming President Trump next week, issued new rules that restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.' first-ever restrictions on exporting closed weights for large AI models.

最新消息：拜登政府在下周即将交接给候任总统特朗普之际，发布了新的规则，限制向少数亲密盟友以外的大部分国家出口人工智能（AI）芯片和模型。这些规则尚未最终确定，将建立一个三层体系，限制对一些亲密盟友的出口，并禁止中国、伊朗、朝鲜、俄罗斯等国家访问。此外，这些规则还将首次限制美国出口大型人工智能模型（Large AI Model）的封闭权重。

How it works: The restrictions were announced shortly after a leak reached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.

运作方式：这些限制措施在泄露给媒体后不久便对外公布。为期 120 天的公众评议期将使新一届美国政府能够听取来自商界和外交界的意见，并在规则生效前对其进行修改。这些规则预计将在一年后生效。

A new hierarchy divides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.

一项新的等级制度将世界各国分为三个等级，它们在获取美国设计的、以及在国外使用美国技术制造的 AI 芯片，以及专有 AI 模型（proprietary AI models）方面，拥有不同的权限。

Tier 1: Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.

第一梯队：澳大利亚、日本、台湾、英国和欧洲大部分地区将保持几乎完全开放的访问权限。但是，这些国家必须将 75% 的人工智能计算能力保留在盟友国家内部。为了确保先进人工智能的发展主要集中在美国的紧密盟友之间，任何单一国家都不得将超过 10% 的计算能力转移到该集团之外。

Tier 2: Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) — roughly the computational capacity of 32,000 Nvidia H100 chips — through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.

第二类：像以色列、沙特阿拉伯和新加坡这样的美国传统盟友和贸易伙伴，在 2025 年第一季度面临总处理能力（TPP，Total Processing Power）的初始上限为 5.07 亿 TPP，这大约相当于 32,000 个 Nvidia H100 芯片的计算能力。到 2027 年，这一上限将增加到 10.2 亿 TPP。在这些国家运营的美国企业可以申请更高的限额：2025 年第一季度为 6.33 亿 TPP，到 2027 年第一季度将提升至 50.64 亿 TPP。

Tier 3: China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems.

在第三级限制下，中国、俄罗斯以及约二十余个其他国家无法获得先进的 AI 芯片、模型训练中的权重参数，以及有关这些系统的专业技术知识。

The U.S. Commerce Department's export control agency must approve the export of models or transfer of weights of closed models that were trained using more than 1026 computational operations. These rules target future systems, as no known models today used this amount of computation during training.

美国商务部的出口管制机构必须批准，对那些使用超过 1026 次计算操作训练的闭源模型（closed models）进行模型出口，或进行模型权重（weights）的转移。这些规则主要针对未来的系统，因为目前没有已知的模型在训练期间使用过如此巨大的计算量。

Companies based in the U.S. must maintain at least 50 percent of their total AI computing power within U.S. borders. They also must track distribution of their models, implement security measures, and submit to regular audits.

美国公司必须将其总 AI 计算能力的至少 50% 部署在美国境内。他们还必须追踪其模型的部署情况，实施安全措施，并接受定期审计。

Behind the news: The proposed rules build on 2022's CHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022 barred semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S. tightened restrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China.

新闻解读：新规则是在 2022 年的《芯片和科学法案》基础上制定的，该法案旨在加强美国国内的半导体生产，并限制可能威胁美国安全的技术出口。在 2022 年末，第一轮限制禁止 AMD 和 Nvidia 等半导体供应商向中国公司出售先进芯片。2024 年 11 月，美国进一步收紧了限制，命令为这些芯片代工的台积电停止为中国生产先进芯片。

Plus green AI infrastructure: In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.

绿色 AI 基础设施的补充：此外，拜登总统发布了一项行政命令，旨在鼓励加速建设用于 AI 的计算基础设施。联邦政府将通过竞争的方式，让私营公司租赁其拥有的土地，用于自费建设数据中心。在选择场地时，会考虑清洁能源的可用性，包括对核能的支持。政府将加快这些场地的审批流程，并支持在周围建设能源传输线路。此外，还将鼓励国际盟友投资以清洁能源为动力的 AI 基础设施。

Why it matters: Protecting the United States' advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have warned that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to reconsider their plans.

重要性：近十年来，保护美国在高科技领域的领先地位一直是白宫日益重视的要务。此前实施的出口限制迫使许多中国人工智能开发人员转而使用性能较弱的硬件。而新的限制措施可能会产生更深远的影响，它可能会迫使第二、三梯队国家的开发人员构建对资源要求不高的模型，并促使他们之间加强合作，从而降低美国技术在全球范围内的价值。这些限制还可能损害美国芯片供应商的利益，他们已经警告说，这些规则可能会削弱美国在全球经济中的竞争力。此外，那些正在建设大型数据中心以进行人工智能计算的公司，可能也不得不重新评估他们的计划。

We're thinking: The Biden administration's embargo on AI chips has been leaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potential suppliers that aren't connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world's dependence on U.S. chips and models would result in a very different global AI ecosystem.

我们认为：拜登政府对 AI 芯片的禁运措施存在漏洞。到目前为止，该禁令仅在小程度上减缓了竞争对手的发展速度，同时也刺激了对可能与美国没有关联的供应商的大量投资。虽然公开征求意见阶段旨在收集游说集团和行业反馈，但最终地缘政治的优先考量可能将占据主导。无论最终结果如何，降低全球对美国芯片和模型的依赖程度都将导致一个截然不同的全球 AI 生态系统。

GB10 Superchip architecture with Blackwell GPU and Grace CPU.

GB10 超级芯片体系结构，搭载 Blackwell GPU 和 Grace CPU。

AI Supercomputer on Your Desk

桌面上的 AI 超级计算机

Nvidia's new desktop computer is built specifically to run large AI models.

英伟达推出了一款全新的台式电脑，专为运行大型 AI 模型而设计。

What's new: Project Digits is a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.

全新亮点：Project Digits 是一款个人超级计算机，旨在帮助开发人员在本地对大型模型进行微调和运行。Project Digits 尺寸小巧，单手即可握持，预计将于 5 月上市，起价 3000 美元。

How it works: Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they're quantized to 4 bits of precision. Two units can be connected to run models such as Meta's Llama 3.1 405B. Complete specifications are not yet available.

工作原理：Digits 项目旨在运行参数高达 2000 亿的模型 —— 大约是普通消费级硬件运行容量的五倍 —— 前提是这些模型被量化为 4 位精度。两个单元可以连接起来运行例如 Meta 的 Llama 3.1 405B 这样的模型。详细规格尚未公布。

Project Digits runs Nvidia's DGX operating system, a flavor of Ubuntu Linux.

「数字」项目采用英伟达 DGX 操作系统，这是一种基于 Ubuntu Linux 的操作系统。

The system is based on a GB10 system-on-a-chip that combines the Nvidia Blackwell GPU architecture (which serves as the basis for its latest B100 GPUs) and Grace CPU architecture (designed to manage AI workloads in data centers), connected via high-bandwidth NVLink interconnect.

该系统基于 GB10 系统级芯片（system-on-a-chip），它集成了英伟达 Blackwell GPU 架构（作为其最新 B100 GPU 的基础）和 Grace CPU 架构（专为管理数据中心的人工智能工作负载而设计），并通过高带宽 NVLink 互联技术连接。

It comes with 128 GB of unified memory and 4 terabytes of solid-state storage.

它拥有 128 GB 的统一内存和 4 TB 的固态硬盘。

The system connects to Nvidia's DGX Cloud service to enable developers to deploy models from a local machine to cloud infrastructure.

该系统与 Nvidia 的 DGX 云服务相连，让开发人员能够将模型从本地计算机部署到云端。

Behind the news: In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot's state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers.

新闻解读：在消费电子展（CES）上密集发布的一系列公告里，英伟达还推出一个名为 Cosmos 的平台，用于开发机器人、自动驾驶汽车和其他实体 AI 系统。Cosmos 平台包含预训练的语言和视觉模型，这些模型的参数规模从 40 亿到 140 亿不等，可以用来为机器人生成合成训练数据，或者构建策略模型，将机器人的状态转化为下一步的行动指令。英伟达还发布了 Cosmos Nemotron，这是一个拥有 340 亿参数的视觉 - 语言模型，专为 AI 智能体设计。此外，还提供了一个视频 Tokenizer（视频分词器）以及其他用于机器人开发人员的工具。

Why it matters: It's common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines.

重要性：通常，模型训练是在 Nvidia A100 或 H100 GPU 上进行的，这些 GPU 的价格分别至少为 8,000 美元或 20,000 美元，并且配备 40 GB 到 80 GB 的内存。这些高昂的配置要求迫使许多开发者从云服务提供商那里购买云端计算资源。Project Digits 的售价为 3,000 美元，并拥有 128 GB 的内存，旨在帮助机器学习工程师在自己的设备上训练和运行更大的模型。

We're thinking: We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.

我们期待看到在 Project Digits、A100 和 H100 上运行模型的成本和效率对比。

X-CLR loss: training models to link text captions and image similarity.

X-CLR 损失：这是一种训练模型的方法，目的是建立文本描述与图像相似性之间的关联。

Calibrating Contrast

校准对比度

Contrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.

对比损失函数（Contrastive loss functions）使得在没有标签数据的情况下生成良好的嵌入（embeddings）成为可能。对这种方法的一种改进使得嵌入更加有用。

What's new: Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introduced X-Sample contrastive loss (X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.

新研究：Meta、纽约大学、布朗大学、基因泰克和加拿大高级研究所的 Vlad Sobal 及其同事提出了一种名为 X-Sample 对比损失（X-CLR）的自监督损失函数。该函数能让视觉模型学习到更精细的嵌入表示，从而更准确地捕捉不同样本之间的细微相似之处和差异。

Key insight: Contrastive loss functions like SimCLR equally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores.

核心思想：像 SimCLR 这样的对比损失函数（contrastive loss functions），会以相同的方式鼓励模型为猫、狗和卡车的图像生成差异较大的特征表示。但是，很明显，猫和狗之间的相似程度要高于它们与卡车的相似程度。X-CLR 并没有简单地将样本标记为相似或不相似，而是为它们分配相似性分数。这样，模型就可以学习生成能够反映这些相似性分数的特征表示。

How it works: The authors used X-CLR to train an embedding model on [Conceptual Captions]https://aclanthology.org/P18-1238/ datasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar to CLIP, except the text encoder was a sentence transformer pretrained on sentence pairs, and the vision encoder was a ResNet-50 pretrained on ImageNet.

工作原理：作者使用 X-CLR 在从网络抓取的图文对数据集 [概念字幕] https://aclanthology.org/P18-1238/ 上训练了一个嵌入模型，数据集包括 CC-3M（300 万个图文对）和 CC-12M（1200 万个图文对）。该模型与 CLIP 类似，区别在于其文本编码器是预训练于句子对的句子 Transformer，而图像编码器是预训练于 ImageNet 的 ResNet-50。

The sentence transformer embedded text captions for all examples. The system computed similarity scores according to cosine similarity between the text embeddings.

句子 Transformer 模型（Sentence Transformer）将所有示例的文本标题进行了嵌入（embedding）处理。系统根据文本嵌入之间的余弦相似性计算了相似度得分。

Similarly, a ResNet-50 computed image embeddings, and the system computed similarity scores between them.

类似地，一个 ResNet-50 模型提取了图像的嵌入向量，并且系统计算了这些图像嵌入向量之间的相似度得分。

The authors froze the sentence transformer and used the text similarity scores as labels in the loss function. The loss function minimized the difference between the similarity scores of the text embeddings and the corresponding similarity scores of the image embeddings.

作者在训练过程中冻结了句子 Transformer 模型的参数，并将文本相似度得分作为损失函数的训练标签。损失函数旨在最小化文本嵌入向量的相似度得分与相应的图像嵌入向量的相似度得分之间的差异。

Results: Systems trained using X-CLR outperformed competitors in ImageNet classification, especially when less training data was available. (The authors followed CLIP's method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image's classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)

结果：使用 X-CLR 训练的模型在 ImageNet 图像分类任务中表现卓越，尤其是在训练数据较少的情况下，其性能明显优于其他模型。(研究人员采用了 CLIP 的分类方法：他们首先将图像和所有类别的文本都转换为向量形式的「嵌入」（embedding），然后计算图像嵌入与每个类别文本嵌入之间的相似度。最终，模型将图像归类为与其图像嵌入最相似的文本嵌入所对应的类别。)

The authors compared a system trained using X-CLR, one trained using SimCLR, and CLIP. After training on the CC-3M dataset, the X-CLR system achieved 58.2 percent accuracy on ImageNet, while the SimCLR model achieved 57.0 percent and CLIP achieved 41.0 percent.

作者比较了分别使用 X-CLR、SimCLR 和 CLIP 训练的三个系统。在 CC-3M 数据集上完成训练后，X-CLR 系统在 ImageNet 上的准确率达到了 58.2%，SimCLR 模型达到了 57.0%，而 CLIP 则达到了 41.0%。

Training on CC-12M resulted in smaller differences: X-CLR achieved 59.4 percent accuracy, SimCLR achieved 58.9 percent, and CLIP achieved 58.8 percent.

在 CC-12M 数据集上进行训练，各模型之间的性能差异较小：X-CLR 的准确率达到 59.4%，SimCLR 为 58.9%，而 CLIP 为 58.8%。

Why it matters: Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that's continuous rather than discrete.

重要性：对比损失函数非常有效，但简单的「相似」或「不相似」的划分方式忽略了很多重要的细节。与 CLIP 类似，X-CLR 也利用图像及其对应的标题进行自监督学习。不同的是，CLIP 的学习方式是将图像 - 文本对识别为相似或不相似，而 X-CLR 则使用标题作为一种连续的相似性信号，来匹配图像 - 图像对，这种信号并非简单的二元分类。

We're thinking: Reality is not black and white. Allowing for shades of gray makes for better modeling.

我们正在思考：现实并非只有黑白两色。允许存在灰色地带，能让建模更加出色。

## 原文

View in browser
The Batch top banner - January 15, 2025

Dear friends,

Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!


Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.

 

This is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products. 


Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.


This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow. 

Two colleagues look at a computer screen. One mentions their chatbot’s Hollywood success, and the other suggests hiring an AI Product Manager.
Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:

Technical proficiency in AI. PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models. 
Iterative development. Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process. 
Data proficiency. AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.
Skill in managing ambiguity. Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.
Ongoing learning. AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives.
Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at gathering feedback fast to keep projects moving. Increasingly, I also expect strong product managers to be able to build prototypes for themselves . 


The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work. 


The variety of valuable things we can build is nearly unlimited. What a great time to build!

 

Keep learning,

Andrew 

 

 

A MESSAGE FROM DEEPLEARNING.AI
Promo banner for "Reasoning with o1"
Get up-close and personal with OpenAI’s groundbreaking o1 model! In our short course “Reasoning with o1,” you’ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting. Enroll today

 

News
DeepSeek-V3 accuracy across benchmarks compared to other AI models.
DeepSeek Ups the Open Weights Ante
A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.
What’s new: DeepSeek-V3 is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are open except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them here. 
How it works: DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 the time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.

The developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by DeepSeek-R1 and DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as group relative policy optimization.  
Earlier work showed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren’t used at inference.
Following DeepSeek-V2, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.
Also like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs. 
Results: In DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.

DeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on Polyglot, which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy). 
In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.
Behind the news:  OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.

Why it matters: Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost. The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022, Microsoft found that MoE cost five times less in training for equal performance compared to a dense model, and Google and Meta reported that MoE achieved better performance than dense models trained on the same numbers of tokens. 

We’re thinking: If they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.


World map of AI export restrictions: Tier 1 (green), Tier 2 (gray), Tier 3 (red).
U.S. Moves to Expand AI Export Restrictions
The United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models.

What’s new: The Biden administration, which will transition to leadership under incoming President Trump next week, issued new rules that restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.’ first-ever restrictions on exporting closed weights for large AI models.

How it works: The restrictions were announced shortly after a leak reached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year.  

A new hierarchy divides nations into three groups that would have different degrees of access to AI chips both designed in the U.S. and manufactured abroad using U.S. technology, as well as proprietary AI models.
Tier 1: Australia, Japan, Taiwan, the United Kingdom, and most of Europe would retain nearly unrestricted access. However, these nations must keep 75 percent of their AI computing power within allied countries. No more than 10 percent can be transferred to any single country outside this group to ensure that advanced AI development remains concentrated among close U.S. allies.
Tier 2: Traditional U.S. allies and trade partners like Israel, Saudi Arabia, and Singapore face an initial cap of 507 million units of total processing power (TPP) — roughly the computational capacity of 32,000 Nvidia H100 chips — through the first quarter of 2025. The cap would increase to 1.02 billion TPP by 2027. U.S. companies that operate in these countries can apply for higher limits: 633 million TPP in Q1 2025, rising to 5.064 billion TPP by Q1 2027.
Tier 3: China, Russia, and around two dozen other countries are blocked from receiving advanced AI chips, model weights, and specialized knowledge related to these systems.
The U.S. Commerce Department’s export control agency must approve the export of models or transfer of weights of closed models that were trained using more than 1026 computational operations. These rules target future systems, as no known models today used this amount of computation during training.
Companies based in the U.S. must maintain at least 50 percent of their total AI computing power within U.S. borders. They also must track distribution of their models, implement security measures, and submit to regular audits.
Behind the news: The proposed rules build on 2022’s CHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022 barred semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S. tightened restrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China. 

Plus green AI infrastructure: In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy.

Why it matters: Protecting the United States’ advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have warned that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to reconsider their plans.

We’re thinking: The Biden administration’s embargo on AI chips has been leaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potential suppliers that aren’t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world’s dependence on U.S. chips and models would result in a very different global AI ecosystem.

 

GB10 Superchip architecture with Blackwell GPU and Grace CPU.
AI Supercomputer on Your Desk
Nvidia’s new desktop computer is built specifically to run large AI models. 
What’s new: Project Digits is a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000.
How it works: Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available. 

Project Digits runs Nvidia’s DGX operating system, a flavor of Ubuntu Linux.
The system is based on a GB10 system-on-a-chip that combines the Nvidia Blackwell GPU architecture (which serves as the basis for its latest B100 GPUs) and Grace CPU architecture (designed to manage AI workloads in data centers), connected via high-bandwidth NVLink interconnect.
It comes with 128 GB of unified memory and 4 terabytes of solid-state storage.
The system connects to Nvidia’s DGX Cloud service to enable developers to deploy models from a local machine to cloud infrastructure.
Behind the news: In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers. 

Why it matters: It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines. 
We’re thinking: We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.


X-CLR loss: training models to link text captions and image similarity.
Calibrating Contrast
Contrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings.
What’s new: Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introduced X-Sample contrastive loss (X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety.
Key insight: Contrastive loss functions like SimCLR equally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores. 
How it works: The authors used X-CLR to train an embedding model on [Conceptual Captions]https://aclanthology.org/P18-1238/ datasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar to CLIP, except the text encoder was a sentence transformer pretrained on sentence pairs, and the vision encoder was a ResNet-50 pretrained on ImageNet.

The sentence transformer embedded text captions for all examples. The system computed similarity scores according to cosine similarity between the text embeddings.
Similarly, a ResNet-50 computed image embeddings, and the system computed similarity scores between them.
The authors froze the sentence transformer and used the text similarity scores as labels in the loss function. The loss function minimized the difference between the similarity scores of the text embeddings and the corresponding similarity scores of the image embeddings. 
Results: Systems trained using X-CLR outperformed competitors in ImageNet classification, especially when less training data was available. (The authors followed CLIP’s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image’s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.)

The authors compared a system trained using X-CLR, one trained using SimCLR, and CLIP. After training on the CC-3M dataset, the X-CLR system achieved 58.2 percent accuracy on ImageNet, while the SimCLR model achieved 57.0 percent and CLIP achieved 41.0 percent.
Training on CC-12M resulted in smaller differences: X-CLR achieved 59.4 percent accuracy, SimCLR achieved 58.9 percent, and CLIP achieved 58.8 percent. 
Why it matters: Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that’s continuous rather than discrete. 
We’re thinking: Reality is not black and white. Allowing for shades of gray makes for better modeling.