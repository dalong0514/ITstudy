## 20231213大语言模型LLM推理及训练显存计算方法

### 01. 推理：显存计算

推理的显存大头就是：参数量，参数类型版本一般有以下四种：

float 32 位浮点数 4 字节

half / BF16 16 位浮点数 2 字节

int8 8 位整数 1 字节

int4 4 位整数 0.5 字节

以 7B-BF16 版本为例，需要显存 = 数量 * 类型大小 = 70 亿 * 2 字节 = 140 亿字节

所以 140 亿字节 = 14 * 1000 * 1000 * 1000 / 1024 / 1024 / 1024 = 14 * (1000 / 1024) ^3 = 14 * 0.93 = 13 GB

注 1: (1000/1024)^3 = 0.93...

注 2: 估算的话，干脆直接记 1 就好了，也就是 7B-BF 就是 7 * 2 = 14 GB 的显存，因为除了参数，推理还需要另外的显存，溢出一点没坏处。

按估算，各个 llama2-13B 版本推理大概用到多少显存（假设存在对应类型）

7B 参数的 4 种常见类型所需推理显存计算如下：

float 7 * 4 = 28 GB

half / BF16 7 * 2 = 14 GB

int8 7 * 1 = 7 GB

int4 7 * 0.5 = 3.5 GB

13B 参数的 4 种常见类型所需推理显存计算如下：

float 13 * 4 = 52 GB

half / BF16 13 * 2 = 26 GB

int8 13 * 1 = 13 GB

int4 13 * 0.5 = 6.5 GB

### 02. 训练：显存

为了让模型收敛，训练的参数类型不能是 int8 /int4；一般是 float，效果差点的就用 BF16；

因为反向传播 / Adam - 优化 / Transformer 架构等因素，一般来说，训练需要的显存，是同样规模 LLM 推理的 3-4 倍；

保守估计，按 4 倍计算。

例如：7B 训练需要的显存，估算结果：

参数类型所需显存

float 7 * 4 * 4 = 112 GB

half / BF16 7 * 2 * 4 = 56 GB

梯度：参数量的 1 倍

优化器的状态：参数量的 2 倍

用 AdamW 优化器，需要 2 倍参数量

SGD 优化器，需要 1 倍参数量

关于 LoRA / QLoRA 显存，以 LoRA 为例：

LoRA 只需要给原始模型做推理，训练一个更小的模型来实现和训练原始参数差不多的效果。

例：原本需要微调 1024×512 的参数，用了 LoRA 之后，如果选择的 Rank=8，则只需要微调这么多参数：1024×8+512×8

在这个基础上跑一次原始参数量的推理（不需要梯度和优化器状态，但仍然需要一些显存来存储运算过程的数据），合起来就是 LoRA 的所需要的显存。