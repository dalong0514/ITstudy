## 20250108Agent-Architectures

[Agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)

Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context.

è®¸å¤šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨åœ¨è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‰åï¼Œä¼šæ‰§è¡Œç‰¹å®šçš„æ­¥éª¤æµç¨‹ã€‚ä¾‹å¦‚ï¼ŒRAG ä¼šå…ˆæ£€ç´¢ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ï¼Œç„¶åå°†è¿™äº›æ–‡æ¡£ä¼ é€’ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè®©æ¨¡å‹çš„å›ç­”åŸºäºæ‰€æä¾›çš„æ–‡æ¡£å†…å®¹ã€‚

Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:

ä¸é‡‡ç”¨ç¡¬ç¼–ç çš„å›ºå®šæ§åˆ¶æµä¸åŒï¼Œæˆ‘ä»¬æœ‰æ—¶éœ€è¦å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©æ§åˆ¶æµç¨‹ï¼Œä»¥ä¾¿è§£å†³æ›´ä¸ºå¤æ‚çš„é—®é¢˜ã€‚è¿™ä¾¿æ˜¯æ™ºèƒ½ä½“çš„ä¸€ç§å®šä¹‰ï¼šæ™ºèƒ½ä½“æ˜¯ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æ¥å†³å®šåº”ç”¨ç¨‹åºæ§åˆ¶æµç¨‹çš„ç³»ç»Ÿã€‚å¤§è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼æ¥æ§åˆ¶åº”ç”¨ç¨‹åºï¼š

1 An LLM can route between two potential paths

ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ä¸¤ä¸ªæ½œåœ¨è·¯å¾„ä¹‹é—´è¿›è¡Œé€‰æ‹©

2 An LLM can decide which of many tools to call

ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å¯ä»¥å†³å®šé€‰æ‹©è°ƒç”¨å“ªä¸ªå·¥å…·

3 An LLM can decide whether the generated answer is sufficient or more work is needed

ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å¯ä»¥åˆ¤æ–­å…¶ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å……åˆ†ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥å®Œå–„ã€‚

As a result, there are many different types of agent architectures, which give an LLM varying levels of control.

å› æ­¤ï¼Œå‡ºç°äº†å¤šç§å¤šæ ·çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œè¿™äº›æ¶æ„èµ‹äºˆäº†å¤§è¯­è¨€æ¨¡å‹ä¸åŒç¨‹åº¦çš„è‡ªä¸»æ§åˆ¶èƒ½åŠ›ã€‚

### Router

è·¯ç”±å™¨

A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.

è·¯ç”±å™¨å…è®¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»é¢„å…ˆè®¾å®šå¥½çš„ä¸€ç³»åˆ—é€‰é¡¹ä¸­æŒ‘é€‰ä¸€ä¸ªæ‰§è¡Œæ­¥éª¤ã€‚è¿™ç§æ™ºèƒ½ä½“æ¶æ„çš„æ§åˆ¶èƒ½åŠ›ç›¸å¯¹æœ‰é™ï¼Œå› ä¸ºå¤§è¯­è¨€æ¨¡å‹é€šå¸¸åªä¸“æ³¨äºåšå•ä¸€å†³ç­–ï¼Œå¹¶ä»æœ‰é™çš„ã€é¢„å…ˆå®šä¹‰å¥½çš„é€‰é¡¹ä¸­ç”Ÿæˆç‰¹å®šçš„ç»“æœã€‚è·¯ç”±å™¨é€šå¸¸ä¼šåˆ©ç”¨ä¸€äº›ä¸åŒçš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚

Structured Output

ç»“æ„åŒ–è¾“å‡º

Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„ç»“æ„åŒ–è¾“å‡ºï¼Œæ˜¯æŒ‡è®©å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå›å¤æ—¶éµå¾ªé¢„å®šçš„æ ¼å¼æˆ–æ¨¡å¼ã€‚è¿™å’Œå·¥å…·è°ƒç”¨ç±»ä¼¼ï¼Œä½†åº”ç”¨èŒƒå›´æ›´å¹¿ã€‚å·¥å…·è°ƒç”¨é€šå¸¸æ˜¯é€‰æ‹©å¹¶ä½¿ç”¨é¢„å…ˆå®šä¹‰å¥½çš„åŠŸèƒ½ï¼Œè€Œç»“æ„åŒ–è¾“å‡ºåˆ™å¯ä»¥ç”¨äºä»»ä½•ç±»å‹çš„æ ¼å¼åŒ–å›å¤ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å®ç°ç»“æ„åŒ–è¾“å‡ºçš„å¸¸ç”¨æ–¹æ³•ï¼š

1 Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.

æç¤ºè¯æŠ€å·§ï¼šé€šè¿‡ç³»ç»Ÿçº§æŒ‡ä»¤å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ä»¥ç‰¹å®šæ ¼å¼è¾“å‡ºã€‚

2 Output parsers: Using post-processing to extract structured data from LLM responses.

è¾“å‡ºè§£æå™¨ï¼šä½¿ç”¨åç»­å¤„ç†æ­¥éª¤ä»å¤§æ¨¡å‹å›å¤ä¸­æå–ç»“æ„åŒ–æ•°æ®ã€‚

3 Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.

å·¥å…·è°ƒç”¨ï¼šåˆ©ç”¨æŸäº›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰å†…ç½®çš„å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œå¯ä»¥ç”Ÿæˆç»“æ„åŒ–çš„è¾“å‡ºç»“æœã€‚

Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about structured outputs in this how-to guide.

ç»“æ„åŒ–è¾“å‡ºå¯¹äºä¿¡æ¯ä¼ é€’è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½ç¡®ä¿ç³»ç»Ÿå¯ä»¥å‡†ç¡®ç†è§£å¹¶æ‰§è¡Œå¤§è¯­è¨€æ¨¡å‹çš„å†³ç­–ã€‚æƒ³æ·±å…¥äº†è§£ç»“æ„åŒ–è¾“å‡ºï¼Ÿè¯·å‚è€ƒè¿™ç¯‡æ“ä½œæŒ‡å—ã€‚

[How to return structured data from a model | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/how_to/structured_output/)

### Tool calling agent

å·¥å…·è°ƒç”¨çš„ AI æ™ºèƒ½ä½“

While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:

è™½ç„¶åƒè·¯ç”±å™¨è¿™æ ·çš„å·¥å…·å¯ä»¥è®©å¤§è¯­è¨€æ¨¡å‹åªåšä¸€ä¸ªç®€å•çš„å†³å®šï¼Œä½†æ›´å¤æ‚çš„ AI æ™ºèƒ½ä½“æ¶æ„å¯ä»¥é€šè¿‡ä¸¤ç§å…³é”®æ–¹å¼ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹æ‹¥æœ‰æ›´å¼ºçš„æ§åˆ¶èƒ½åŠ›ï¼š

1 Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.

å¤šæ­¥å†³ç­–ï¼šå¤§è¯­è¨€æ¨¡å‹å¯ä»¥è¿›è¡Œä¸€ç³»åˆ—è¿ç»­çš„å†³ç­–ï¼Œè€Œä¸æ˜¯ä»…ä»…åšå‡ºä¸€ä¸ªå†³ç­–ã€‚

2 Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.

å·¥å…·ä½¿ç”¨ï¼šå¤§è¯­è¨€æ¨¡å‹å¯ä»¥é€‰æ‹©å¹¶ä½¿ç”¨å„ç§å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚

ReAct is a popular general purpose agent architecture that combines these expansions, integrating three core concepts.

ReAct æ˜¯ä¸€ç§æµè¡Œçš„é€šç”¨ AI æ™ºèƒ½ä½“æ¶æ„ï¼Œå®ƒèåˆäº†å¤šç§æ‰©å±•åŠŸèƒ½ï¼Œå¹¶æ•´åˆäº†ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚

1 Tool calling: Allowing the LLM to select and use various tools as needed.

å·¥å…·è°ƒç”¨ï¼šå®ƒå…è®¸å¤§è¯­è¨€æ¨¡å‹æ ¹æ®éœ€è¦é€‰æ‹©å¹¶ä½¿ç”¨å„ç§å·¥å…·ã€‚

2 Memory: Enabling the agent to retain and use information from previous steps.

è®°å¿†ï¼šè®© AI æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½å¹¶åˆ©ç”¨ä¹‹å‰æ­¥éª¤çš„ä¿¡æ¯ã€‚

3 Planning: Empowering the LLM to create and follow multi-step plans to achieve goals.

è§„åˆ’ï¼šèµ‹äºˆå¤§è¯­è¨€æ¨¡å‹åˆ¶å®šå¹¶æ‰§è¡Œå¤šæ­¥éª¤è®¡åˆ’ä»¥è¾¾æˆç›®æ ‡çš„èƒ½åŠ›ã€‚

This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. You can use it with create_react_agent.

è¿™ç§æ¶æ„ä½¿å¾— AI æ™ºèƒ½ä½“èƒ½å¤Ÿå±•ç°æ›´å¤æ‚å’Œçµæ´»çš„è¡Œä¸ºï¼Œå®ƒä¸å†å±€é™äºç®€å•çš„ä»»åŠ¡åˆ†é…ï¼ˆè·¯ç”±ï¼‰ï¼Œè€Œæ˜¯èƒ½å¤Ÿè¿›è¡Œå¤šæ­¥éª¤çš„åŠ¨æ€é—®é¢˜è§£å†³ã€‚ä½ å¯ä»¥ä½¿ç”¨ `create_react_agent` æ¥æ„å»ºè¿™ç§æ™ºèƒ½ä½“ã€‚

Tool calling

å·¥å…·è°ƒç”¨

Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema.

å½“éœ€è¦ AI æ™ºèƒ½ä½“ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’æ—¶ï¼Œå·¥å…·å°±æ˜¾å¾—éå¸¸æœ‰ç”¨ã€‚å¤–éƒ¨ç³»ç»Ÿï¼Œä¾‹å¦‚åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ï¼Œé€šå¸¸éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼æˆ–æ•°æ®è´Ÿè½½ï¼Œè€Œä¸æ˜¯è‡ªç„¶è¯­è¨€ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†ä¸€ä¸ª API ä½œä¸ºå·¥å…·é›†æˆæ—¶ï¼Œæˆ‘ä»¬ä¼šè®©æ¨¡å‹äº†è§£æ‰€éœ€çš„è¾“å…¥æ ¼å¼ã€‚æ¨¡å‹ä¼šæ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œé€‰æ‹©è°ƒç”¨åˆé€‚çš„å·¥å…·ï¼Œå¹¶è¿”å›ç¬¦åˆè¯¥å·¥å…·è¦æ±‚çš„æ ¼å¼åŒ–è¾“å‡ºã€‚

Many LLM providers support tool calling and tool calling interface in LangChain is simple: you can simply pass any Python function into ChatModel.bind_tools(function).

è®¸å¤šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœåŠ¡éƒ½æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œåœ¨ LangChain ä¸­ä½¿ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½ä¹Ÿå¾ˆç®€å•ï¼šä½ åªéœ€è¦å°†ä»»ä½• Python å‡½æ•°ä¼ é€’ç»™ ChatModel.bind_toolsï¼ˆfunctionï¼‰å³å¯ã€‚

Memory

è®°å¿†

Memory is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:

è®°å¿†å¯¹äº AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰è‡³å…³é‡è¦ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨è§£å†³é—®é¢˜çš„è¿‡ç¨‹ä¸­ä¿ç•™å’Œåˆ©ç”¨ä¿¡æ¯ã€‚å®ƒæœ‰ä¸åŒçš„è¿ä½œæ–¹å¼ï¼š

1 Short-term memory: Allows the agent to access information acquired during earlier steps in a sequence.

çŸ­æœŸè®°å¿†ï¼šå…è®¸ AI æ™ºèƒ½ä½“è®¿é—®åœ¨æµç¨‹ä¸­ä¹‹å‰çš„æ­¥éª¤ä¸­è·å–çš„ä¿¡æ¯ã€‚

2 Long-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.

é•¿æœŸè®°å¿†ï¼šä½¿ AI æ™ºèƒ½ä½“èƒ½å¤Ÿå›å¿†èµ·ä¹‹å‰äº¤äº’çš„ä¿¡æ¯ï¼Œä¾‹å¦‚å¯¹è¯ä¸­è¿‡å»çš„æ¶ˆæ¯ã€‚

LangGraph provides full control over memory implementation:

LangGraph æä¾›äº†å¯¹è®°å¿†åŠŸèƒ½å®ç°çš„å®Œå…¨æ§åˆ¶ï¼š

1 State: User-defined schema specifying the exact structure of memory to retain.

çŠ¶æ€ï¼šç”¨æˆ·è‡ªå®šä¹‰çš„ç»“æ„æ¨¡å¼ï¼Œç”¨äºæ˜ç¡®æŒ‡å®šéœ€è¦ä¿ç•™çš„å†…å­˜ä¿¡æ¯ã€‚

2 Checkpointers: Mechanism to store state at every step across different interactions.

æ£€æŸ¥ç‚¹ï¼šä¸€ç§æœºåˆ¶ï¼Œç”¨äºåœ¨æ¯æ¬¡äº¤äº’çš„ä¸åŒæ­¥éª¤ä¸­å­˜å‚¨çŠ¶æ€ã€‚

This flexible approach allows you to tailor the memory system to your specific agent architecture needs. For a practical guide on adding memory to your graph, see this tutorial.

è¿™ç§çµæ´»çš„æ–¹å¼ï¼Œè®©ä½ å¯ä»¥æ ¹æ®è‡ªå·±ç‰¹å®šçš„ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰æ¶æ„éœ€æ±‚ï¼Œæ¥å®šåˆ¶å†…å­˜ç³»ç»Ÿã€‚å…³äºå¦‚ä½•å‘ä½ çš„å›¾ç»“æ„ä¸­æ·»åŠ å†…å­˜çš„å®ç”¨æŒ‡å—ï¼Œè¯·å‚è€ƒæœ¬æ•™ç¨‹ã€‚

[How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)

Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time.

æœ‰æ•ˆçš„è®°å¿†ç®¡ç†æå‡äº† AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰ä¿æŒä¸Šä¸‹æ–‡ã€ä»è¿‡å¾€ç»éªŒä¸­å­¦ä¹ ä»¥åŠéšç€æ—¶é—´è¿›å±•åšå‡ºæ›´æ˜æ™ºå†³ç­–çš„èƒ½åŠ›ã€‚

Planning

è§„åˆ’

In the ReAct architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.

åœ¨ ReAct æ¶æ„ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šåœ¨ä¸€ä¸ª while å¾ªç¯ä¸­è¢«åå¤è°ƒç”¨ã€‚åœ¨å¾ªç¯çš„æ¯ä¸€æ­¥ï¼ŒAI æ™ºèƒ½ä½“éƒ½ä¼šå†³å®šè¦è°ƒç”¨å“ªäº›å·¥å…·ï¼Œä»¥åŠè¿™äº›å·¥å…·éœ€è¦å“ªäº›è¾“å…¥ã€‚ç„¶åï¼Œè¿™äº›å·¥å…·ä¼šè¢«æ‰§è¡Œï¼Œå®ƒä»¬çš„è¾“å‡ºç»“æœä¼šä½œä¸ºè§‚å¯Ÿä¿¡æ¯åé¦ˆç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å½“ AI æ™ºèƒ½ä½“åˆ¤æ–­å®ƒå·²ç»æŒæ¡è¶³å¤Ÿçš„ä¿¡æ¯æ¥è§£å†³ç”¨æˆ·è¯·æ±‚ï¼Œå¹¶ä¸”ç»§ç»­è°ƒç”¨å·¥å…·çš„æ”¶ç›Šä¸å¤§æ—¶ï¼Œè¿™ä¸ª while å¾ªç¯å°±ä¼šç»“æŸã€‚

ReAct implementation

ReAct çš„å®ç°

There are several differences between this paper and the pre-built create_react_agent implementation:

è¿™ç¯‡è®ºæ–‡æ‰€æè¿°çš„ ReAct å®ç°ï¼Œä¸é¢„å…ˆæ„å»ºå¥½çš„ `create_react_agent` å®ç°ä¹‹é—´å­˜åœ¨ä¸€äº›å·®å¼‚ï¼š

2023012ReAct: Synergizing Reasoning and Acting in Language Models

[[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.

é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å·¥å…·è°ƒç”¨è®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è°ƒç”¨å·¥å…·ï¼Œè€Œè®ºæ–‡ä¸­é‡‡ç”¨çš„æ˜¯æç¤ºæ–¹æ³•åŠ ä¸Šå¯¹åŸå§‹è¾“å‡ºçš„è§£æã€‚ä¹‹æ‰€ä»¥è¿™æ ·åšï¼Œæ˜¯å› ä¸ºåœ¨è®ºæ–‡æ’°å†™æ—¶å·¥å…·è°ƒç”¨æŠ€æœ¯å°šæœªå‡ºç°ï¼Œä½†é€šå¸¸æ¥è¯´ï¼Œå·¥å…·è°ƒç”¨æ–¹æ³•æ›´å¥½ã€æ›´å¯é ã€‚

Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.

å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡æ¶ˆæ¯æ¥å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ï¼Œè€Œè®ºæ–‡ä¸­åˆ™é‡‡ç”¨çš„æ˜¯å­—ç¬¦ä¸²æ ¼å¼åŒ–ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºåœ¨æ’°å†™æœ¬æ–‡æ¡£æ—¶ï¼Œå½“æ—¶çš„å¤§è¯­è¨€æ¨¡å‹ç”šè‡³è¿˜æ²¡æœ‰æä¾›åŸºäºæ¶ˆæ¯çš„æ¥å£ï¼Œè€Œç°åœ¨ï¼Œæ¶ˆæ¯æ¥å£å·²æˆä¸ºå®ƒä»¬å”¯ä¸€å¯¹å¤–æä¾›çš„æ¥å£ã€‚

Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.

ç¬¬ä¸‰ï¼Œé‚£ç¯‡è®ºæ–‡è¦æ±‚æ‰€æœ‰å·¥å…·çš„è¾“å…¥éƒ½å¿…é¡»æ˜¯å•ç‹¬çš„ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå½“æ—¶çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›è¿˜æ¯”è¾ƒå¼±ï¼Œä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªè¾“å…¥ã€‚è€Œæˆ‘ä»¬çš„å®ç°å…è®¸ä½¿ç”¨é‚£äº›éœ€è¦å¤šä¸ªè¾“å…¥çš„å·¥å…·ã€‚

Fourth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.

ç¬¬å››ï¼Œè¿™ç¯‡è®ºæ–‡åªç ”ç©¶äº†æ¯æ¬¡è°ƒç”¨ä¸€ä¸ªå·¥å…·çš„æƒ…å†µï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå½“æ—¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„æ€§èƒ½è¿˜ä¸å¤Ÿå¼ºã€‚è€Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ä¸€æ¬¡è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚

Finally, the paper asked the LLM to explicitly generate a "Thought" step before deciding which tools to call. This is the "Reasoning" part of "ReAct". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.

æœ€åï¼Œè¯¥è®ºæ–‡è¦æ±‚å¤§è¯­è¨€æ¨¡å‹åœ¨å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ä¹‹å‰ï¼Œå…ˆæ˜ç¡®ç”Ÿæˆä¸€ä¸ªã€Œæ€è€ƒã€æ­¥éª¤ã€‚è¿™æ˜¯ã€ŒReAct æ–¹æ³•ã€ä¸­çš„ã€Œæ¨ç†ã€éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„å®ç°é»˜è®¤ä¸è¿›è¡Œæ­¤æ“ä½œï¼Œä¸»è¦æ˜¯å› ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›å·²å¤§å¹…æå‡ï¼Œè¿™ä¸ªæ­¥éª¤å·²ä¸å†æ˜¯å¿…éœ€çš„ã€‚å½“ç„¶ï¼Œå¦‚æœä½ å¸Œæœ›é€šè¿‡æç¤ºæ¥è®©å®ƒæ‰§è¡Œæ­¤æ“ä½œï¼Œä¹Ÿæ˜¯å¯ä»¥çš„ã€‚

### Custom agent architectures

è‡ªå®šä¹‰æ™ºèƒ½ä½“æ¶æ„

While routers and tool-calling agents (like ReAct) are common, customizing agent architectures often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:

è™½ç„¶è·¯ç”±å™¨å’Œå…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼ˆå¦‚ ReActï¼‰å¾ˆå¸¸è§ï¼Œä½†è‡ªå®šä¹‰æ™ºèƒ½ä½“æ¶æ„é€šå¸¸ä¼šä¸ºç‰¹å®šä»»åŠ¡å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚LangGraph æä¾›äº†å‡ ä¸ªå¼ºå¤§çš„åŠŸèƒ½æ¥æ„å»ºé‡èº«å®šåˆ¶çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼š

Human-in-the-loop

äººæœºåä½œ

Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:

äººçš„å‚ä¸å¯ä»¥æ˜¾è‘—æé«˜ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰çš„å¯é æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ‰§è¡Œæ•æ„Ÿä»»åŠ¡æ—¶ã€‚è¿™å¯èƒ½åŒ…æ‹¬ï¼š

1 Approving specific actions

æ‰¹å‡†æ‰§è¡Œç‰¹å®šæ“ä½œ

2 Providing feedback to update the agent's state

æä¾›åé¦ˆä»¥æ›´æ–° AI æ™ºèƒ½ä½“çš„çŠ¶æ€

3 Offering guidance in complex decision-making processes

åœ¨å¤æ‚å†³ç­–è¿‡ç¨‹ä¸­æä¾›æŒ‡å¯¼

Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our human-in-the-loop guide.

å½“å®Œå…¨è‡ªåŠ¨åŒ–ä¸å¯è¡Œæˆ–ä¸ç†æƒ³æ—¶ï¼ŒäººæœºååŒæ¨¡å¼è‡³å…³é‡è¦ã€‚åœ¨æˆ‘ä»¬çš„äººæœºååŒæŒ‡å—ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/)

Parallelization

Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its Send API, enabling:

å¹¶è¡ŒåŒ–å¹¶è¡Œå¤„ç†å¯¹äºé«˜æ•ˆçš„å¤šä¸ª AI æ™ºèƒ½ä½“ååŒå·¥ä½œçš„ç³»ç»Ÿå’Œå¤æ‚ä»»åŠ¡è‡³å…³é‡è¦ã€‚LangGraph é€šè¿‡å…¶ Send API æ”¯æŒå¹¶è¡ŒåŒ–ï¼Œå¯ä»¥å®ç°ï¼š

1 Concurrent processing of multiple states

åŒæ—¶å¤„ç†å¤šç§çŠ¶æ€

2 Implementation of map-reduce-like operations

å®ç°ç±»ä¼¼äº map-reduce çš„æ“ä½œ

3 Efficient handling of independent subtasks

é«˜æ•ˆå¤„ç†ç‹¬ç«‹å­ä»»åŠ¡

For practical implementation, see our map-reduce tutorial.

å…³äºå®é™…åº”ç”¨ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„ map-reduce æ•™ç¨‹ã€‚

[How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)

Subgraphs

å­ç»“æ„

Subgraphs are essential for managing complex agent architectures, particularly in multi-agent systems. They allow:

å­ç»“æ„å¯¹äºç®¡ç†å¤æ‚çš„ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰æ¶æ„è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ã€‚å®ƒä»¬å¯ä»¥å®ç°ï¼š

1 Isolated state management for individual agents

ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬ç®¡ç†çŠ¶æ€

2 Hierarchical organization of agent teams

æ™ºèƒ½ä½“å›¢é˜Ÿçš„åˆ†å±‚ç»“æ„

3 Controlled communication between agents and the main system

æ™ºèƒ½ä½“ä¸ä¸»ç³»ç»Ÿä¹‹é—´çš„å—æ§é€šä¿¡

Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our subgraph how-to guide.

å­å›¾é€šè¿‡åœ¨çŠ¶æ€æ¨¡å¼ï¼ˆstate schemaï¼‰ä¸­ä½¿ç”¨å…±äº«çš„é”®ï¼ˆoverlapping keysï¼‰ä¸çˆ¶å›¾è¿›è¡Œé€šä¿¡ã€‚è¿™ç§æœºåˆ¶å®ç°äº†çµæ´»ä¸”æ¨¡å—åŒ–çš„æ™ºèƒ½ä½“è®¾è®¡ã€‚å…³äºå…·ä½“å®ç°ç»†èŠ‚ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„å­å›¾ä½¿ç”¨æŒ‡å—ã€‚

[How to add and use subgraphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/)

Reflection

Reflection mechanisms can significantly improve agent reliability by:

åæ€æœºåˆ¶å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾è‘—æé«˜ AI æ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ï¼š

1 Evaluating task completion and correctness

è¡¡é‡ä»»åŠ¡çš„å®Œæˆæƒ…å†µå’Œå‡†ç¡®åº¦

2 Providing feedback for iterative improvement

æä¾›åé¦ˆä»¥ä¿ƒè¿›æŒç»­æ”¹è¿›

Enabling self-correction and learning

While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.

å¯ç”¨è‡ªæˆ‘çº æ­£å’Œå­¦ä¹ åæ€è¿‡ç¨‹è™½ç„¶é€šå¸¸åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†ä¹Ÿå¯ä»¥é‡‡ç”¨ç¡®å®šæ€§æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­ï¼Œç¼–è¯‘é”™è¯¯å°±å¯ä»¥ä½œä¸ºåé¦ˆã€‚è¿™ä¸ªæ–¹æ³•åœ¨ä¸€æ®µä½¿ç”¨ LangGraph å®ç°è‡ªæˆ‘çº æ­£ä»£ç ç”Ÿæˆçš„è§†é¢‘ä¸­å¾—åˆ°äº†å±•ç¤ºã€‚

By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.

å€ŸåŠ©è¿™äº›åŠŸèƒ½ï¼ŒLangGraph å¯ä»¥æ„å»ºç²¾å·§çš„ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ AI æ™ºèƒ½ä½“æ¶æ„ã€‚è¿™äº›æ¶æ„ä¸ä»…èƒ½åº”å¯¹å¤æ‚çš„å·¥ä½œæµç¨‹ï¼Œè¿˜èƒ½é«˜æ•ˆååŒå·¥ä½œï¼Œå¹¶æŒç»­æå‡è‡ªèº«çš„è¡¨ç°ã€‚

## åŸæ–‡

Agent-architectures
Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context.

Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:

An LLM can route between two potential paths
An LLM can decide which of many tools to call
An LLM can decide whether the generated answer is sufficient or more work is needed
As a result, there are many different types of agent architectures, which give an LLM varying levels of control.

Agent Types

Router
A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.

Structured Output
Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:

Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.
Output parsers: Using post-processing to extract structured data from LLM responses.
Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.
Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about structured outputs in this how-to guide.

Tool calling agent
While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:

Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.
Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.
ReAct is a popular general purpose agent architecture that combines these expansions, integrating three core concepts.

Tool calling: Allowing the LLM to select and use various tools as needed.
Memory: Enabling the agent to retain and use information from previous steps.
Planning: Empowering the LLM to create and follow multi-step plans to achieve goals.
This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. You can use it with create_react_agent.

Tool calling
Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema.

Many LLM providers support tool calling and tool calling interface in LangChain is simple: you can simply pass any Python function into ChatModel.bind_tools(function).

Tools

Memory
Memory is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:

Short-term memory: Allows the agent to access information acquired during earlier steps in a sequence.
Long-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.
LangGraph provides full control over memory implementation:

State: User-defined schema specifying the exact structure of memory to retain.
Checkpointers: Mechanism to store state at every step across different interactions.
This flexible approach allows you to tailor the memory system to your specific agent architecture needs. For a practical guide on adding memory to your graph, see this tutorial.

Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time.

Planning
In the ReAct architecture, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.

ReAct implementation
There are several differences between this paper and the pre-built create_react_agent implementation:

First, we use tool-calling to have LLMs call tools, whereas the paper used prompting + parsing of raw output. This is because tool calling did not exist when the paper was written, but is generally better and more reliable.
Second, we use messages to prompt the LLM, whereas the paper used string formatting. This is because at the time of writing, LLMs didn't even expose a message-based interface, whereas now that's the only interface they expose.
Third, the paper required all inputs to the tools to be a single string. This was largely due to LLMs not being super capable at the time, and only really being able to generate a single input. Our implementation allows for using tools that require multiple inputs.
Fourth, the paper only looks at calling a single tool at the time, largely due to limitations in LLMs performance at the time. Our implementation allows for calling multiple tools at a time.
Finally, the paper asked the LLM to explicitly generate a "Thought" step before deciding which tools to call. This is the "Reasoning" part of "ReAct". Our implementation does not do this by default, largely because LLMs have gotten much better and that is not as necessary. Of course, if you wish to prompt it do so, you certainly can.
Custom agent architectures
While routers and tool-calling agents (like ReAct) are common, customizing agent architectures often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:

Human-in-the-loop
Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:

Approving specific actions
Providing feedback to update the agent's state
Offering guidance in complex decision-making processes
Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our human-in-the-loop guide.

Parallelization
Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its Send API, enabling:

Concurrent processing of multiple states
Implementation of map-reduce-like operations
Efficient handling of independent subtasks
For practical implementation, see our map-reduce tutorial.

Subgraphs
Subgraphs are essential for managing complex agent architectures, particularly in multi-agent systems. They allow:

Isolated state management for individual agents
Hierarchical organization of agent teams
Controlled communication between agents and the main system
Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our subgraph how-to guide.

Reflection
Reflection mechanisms can significantly improve agent reliability by:

Evaluating task completion and correctness
Providing feedback for iterative improvement
Enabling self-correction and learning
While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.

By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.