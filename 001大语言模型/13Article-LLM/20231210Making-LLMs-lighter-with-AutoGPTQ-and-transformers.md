## 20231210Making-LLMs-lighter-with-AutoGPTQ-and-transformers

[Making LLMs lighter with AutoGPTQ and transformers](https://huggingface.co/blog/gptq-integration)

[ä½¿ç”¨ AutoGPTQ å’Œ transformers è®©å¤§è¯­è¨€æ¨¡å‹æ›´è½»é‡åŒ–](https://huggingface.co/blog/zh/gptq-integration)

[blog/gptq-integration.md at main Â· huggingface/blog](https://github.com/huggingface/blog/blob/main/gptq-integration.md)

Making LLMs lighter with AutoGPTQ and transformers

Published August 23, 2023

This article is also available in Chinese ç®€ä½“ä¸­æ–‡.

Large language models have demonstrated remarkable capabilities in understanding and generating human-like text, revolutionizing applications across various domains. However, the demands they place on consumer hardware for training and deployment have become increasingly challenging to meet.

ğŸ¤— Hugging Face's core mission is to democratize good machine learning, and this includes making large models as accessible as possible for everyone. In the same spirit as our bitsandbytes collaboration, we have just integrated the AutoGPTQ library in Transformers, making it possible for users to quantize and run models in 8, 4, 3, or even 2-bit precision using the GPTQ algorithm (Frantar et al. 2023). There is negligible accuracy degradation with 4-bit quantization, with inference speed comparable to the fp16 baseline for small batch sizes. Note that GPTQ method slightly differs from post-training quantization methods proposed by bitsandbytes as it requires to pass a calibration dataset.

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.

å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆäººç±»æ°´å¹³çš„æ–‡å­—æ–¹é¢æ‰€å±•ç°å‡ºçš„éå‡¡èƒ½åŠ›ï¼Œæ­£åœ¨è®¸å¤šé¢†åŸŸå¸¦æ¥åº”ç”¨ä¸Šçš„é©æ–°ã€‚ç„¶è€Œï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè®­ç»ƒå’Œéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹çš„éœ€æ±‚ä¹Ÿå˜å¾—è¶Šæ¥è¶Šéš¾ä»¥æ»¡è¶³ã€‚

ğŸ¤— Hugging Face çš„æ ¸å¿ƒä½¿å‘½æ˜¯è®©ä¼˜ç§€çš„æœºå™¨å­¦ä¹ æ™®æƒ åŒ–ï¼Œè€Œè¿™æ­£åŒ…æ‹¬äº†å°½å¯èƒ½åœ°è®©æ‰€æœ‰äººéƒ½èƒ½å¤Ÿä½¿ç”¨ä¸Šå¤§æ¨¡å‹ã€‚æœ¬ç€ä¸ bitsandbytes åˆä½œä¸€æ ·çš„ç²¾ç¥ï¼Œæˆ‘ä»¬å°† AutoGPTQ ä»£ç åº“é›†æˆåˆ°äº† Transformers ä¸­ï¼Œè®©ç”¨æˆ·ä½¿ç”¨ GPTQ ç®—æ³• (Frantar et al. 2023) åœ¨ 8 ä½ã€4 ä½ã€3 ä½ï¼Œç”šè‡³æ˜¯ 2 ä½ç²¾åº¦ä¸‹é‡åŒ–å’Œè¿è¡Œæ¨¡å‹æˆä¸ºå¯èƒ½ã€‚å½“ä½¿ç”¨ int4 é‡åŒ–æ—¶ï¼Œç²¾åº¦çš„ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ï¼ŒåŒæ—¶åœ¨å°æ‰¹é‡æ¨ç†ä¸Šä¿æŒç€ä¸ fp16 åŸºçº¿ç›¸å½“çš„é€Ÿåº¦ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒGPTQ æ–¹æ³•ä¸ bitsandbytes æå‡ºçš„è®­ç»ƒåé‡åŒ–æ–¹æ³•æœ‰æ‰€ä¸åŒï¼šå®ƒéœ€è¦åœ¨é‡åŒ–é˜¶æ®µæä¾›ä¸€ä¸ªæ ¡å‡†æ•°æ®é›†ã€‚

æœ¬æ¬¡é›†æˆæ”¯æŒè‹±ä¼Ÿè¾¾ GPU å’ŒåŸºäº RoCm çš„ AMD GPUã€‚

### 01. Table of contents

Resources

A gentle summary of the GPTQ paper

AutoGPTQ library â€“ the one-stop library for efficiently leveraging GPTQ for LLMs

Native support of GPTQ models in ğŸ¤— Transformers

Quantizing models with the Optimum library

Running GPTQ models through Text-Generation-Inference

Fine-tune quantized models with PEFT

Room for improvement

Supported models

Conclusion and final words

Acknowledgements

ç›¸å…³èµ„æº

GPTQ è®ºæ–‡æ€»ç»“

AutoGPTQ ä»£ç åº“ â€”â€” ä¸€ç«™å¼åœ°å°† GPTQ æ–¹æ³•åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹

ğŸ¤— Transformers å¯¹ GPTQ æ¨¡å‹çš„æœ¬åœ°åŒ–æ”¯æŒ

ä½¿ç”¨ Optimum ä»£ç åº“é‡åŒ–æ¨¡å‹

é€šè¿‡ Text-Generation-Inference ä½¿ç”¨ GPTQ æ¨¡å‹

ä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹

æ”¹è¿›ç©ºé—´

å·²æ”¯æŒçš„æ¨¡å‹

ç»“è®ºå’Œç»“è¯­

è‡´è°¢

### 02. Resources

This blogpost and release come with several resources to get started with GPTQ quantization:

01 Original Paper

[[2210.17323] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)

02 Basic usage Google Colab notebook - This notebook shows how to quantize your transformers model with GPTQ method, how to do inference, and how to do fine-tuning with the quantized model.

[AutoGPTQ-transformers.ipynb - Colaboratory](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing)

03 Transformers integration documentation

[Quantization](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)

04 Optimum integration documentation

[Quantization](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)

05 The Bloke repositories with compatible GPTQ models.

[TheBloke (Tom Jobbins)](https://huggingface.co/TheBloke?sort_models=likes#models)

### 03. A gentle summary of the GPTQ paper

Quantization methods usually belong to one of two categories:

Post-Training Quantization (PTQ): We quantize a pre-trained model using moderate resources, such as a calibration dataset and a few hours of computation.

Quantization-Aware Training (QAT): Quantization is performed before training or further fine-tuning.

GPTQ falls into the PTQ category and this is particularly interesting for massive models, for which full model training or even fine-tuning can be very expensive.

Specifically, GPTQ adopts a mixed int4/fp16 quantization scheme where weights are quantized as int4 while activations remain in float16. During inference, weights are dequantized on the fly and the actual compute is performed in float16.

The benefits of this scheme are twofold:

Memory savings close to x4 for int4 quantization, as the dequantization happens close to the compute unit in a fused kernel, and not in the GPU global memory.

Potential speedups thanks to the time saved on data communication due to the lower bitwidth used for weights.

The GPTQ paper tackles the layer-wise compression problem:

Once this is solved per layer, a solution to the global problem can be obtained by combining the layer-wise solutions.

In order to solve this layer-wise compression problem, the author uses the Optimal Brain Quantization framework (Frantar et al 2022). The OBQ method starts from the observation that the above equation can be written as the sum of the squared errors, over each row of Wl.

This means that we can quantize each row independently. This is called per-channel quantization. For each row XX, OBQ quantizes one weight at a time while always updating all not-yet-quantized weights, in order to compensate for the error incurred by quantizing a single weight. The update on selected weights has a closed-form formula, utilizing Hessian matrices.

The GPTQ paper improves this framework by introducing a set of optimizations that reduces the complexity of the quantization algorithm while retaining the accuracy of the model.

Compared to OBQ, the quantization step itself is also faster with GPTQ: it takes 2 GPU-hours to quantize a BERT model (336M) with OBQ, whereas with GPTQ, a Bloom model (176B) can be quantized in less than 4 GPU-hours.

To learn more about the exact algorithm and the different benchmarks on perplexity and speedups, check out the original paper.

GPTQ è®ºæ–‡æ€»ç»“

é€šå¸¸ï¼Œé‡åŒ–æ–¹æ³•å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸¤ç±»ï¼š

1ã€è®­ç»ƒåé‡åŒ– (Post Training Quantization, PTQ)ï¼šé€‚åº¦åœ°ä½¿ç”¨ä¸€äº›èµ„æºæ¥é‡åŒ–é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¦‚ä¸€ä¸ªæ ¡å‡†æ•°æ®é›†å’Œå‡ å°æ—¶çš„ç®—åŠ›ã€‚

2ã€é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization Aware Training, QAT)ï¼šåœ¨è®­ç»ƒæˆ–è¿›ä¸€æ­¥å¾®è°ƒä¹‹å‰æ‰§è¡Œé‡åŒ–ã€‚

GPTQ å±äºè®­ç»ƒåé‡åŒ–ï¼Œè¿™å¯¹äºå¤§æ¨¡å‹è€Œè¨€æ ¼å¤–æœ‰è¶£ä¸”æœ‰æ„ä¹‰ï¼Œå› ä¸ºå¯¹å…¶è¿›è¡Œå…¨å‚æ•°è®­ç»ƒä»¥åŠç”šè‡³ä»…ä»…æ˜¯å¾®è°ƒéƒ½ååˆ†æ˜‚è´µã€‚

å…·ä½“è€Œè¨€ï¼ŒGPTQ é‡‡ç”¨ int4/fp16 (W4A16) çš„æ··åˆé‡åŒ–æ–¹æ¡ˆï¼Œå…¶ä¸­æ¨¡å‹æƒé‡è¢«é‡åŒ–ä¸º int4 æ•°å€¼ç±»å‹ï¼Œè€Œæ¿€æ´»å€¼åˆ™ä¿ç•™åœ¨ float16ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æƒé‡è¢«åŠ¨æ€åœ°åé‡åŒ–å› float16 å¹¶åœ¨è¯¥æ•°å€¼ç±»å‹ä¸‹è¿›è¡Œå®é™…çš„è¿ç®—ã€‚

è¯¥æ–¹æ¡ˆæœ‰ä»¥ä¸‹ä¸¤æ–¹é¢çš„ä¼˜ç‚¹ï¼š

1ã€int4 é‡åŒ–èƒ½å¤ŸèŠ‚çœæ¥è¿‘ 4 å€çš„å†…å­˜ï¼Œè¿™æ˜¯å› ä¸ºåé‡åŒ–æ“ä½œå‘ç”Ÿåœ¨ç®—å­çš„è®¡ç®—å•å…ƒé™„è¿‘ï¼Œè€Œä¸æ˜¯åœ¨ GPU çš„å…¨å±€å†…å­˜ä¸­ã€‚

2ã€ç”±äºç”¨äºæƒé‡çš„ä½å®½è¾ƒä½ï¼Œå› æ­¤å¯ä»¥èŠ‚çœæ•°æ®é€šä¿¡çš„æ—¶é—´ï¼Œä»è€Œæ½œåœ¨åœ°æå‡äº†æ¨ç†é€Ÿåº¦ã€‚

GPTQ è®ºæ–‡è§£å†³äº†åˆ†å±‚å‹ç¼©çš„é—®é¢˜ï¼š

ç»™å®šä¸€ä¸ªæ‹¥æœ‰æƒé‡çŸ©é˜µ XX å’Œè¾“å…¥ XX çš„ç½‘ç»œå±‚ï¼Œæˆ‘ä»¬æœŸæœ›è·å¾—ä¸€ä¸ªé‡åŒ–ç‰ˆæœ¬çš„æƒé‡çŸ©é˜µï¼š

ä»¥æœ€å°åŒ–å‡æ–¹è¯¯å·® (MSE)ï¼š

ä¸€æ—¦æ¯å±‚éƒ½å®ç°äº†ä¸Šè¿°ç›®æ ‡ï¼Œå°±å¯ä»¥é€šè¿‡ç»„åˆå„ç½‘ç»œå±‚é‡åŒ–ç»“æœçš„æ–¹å¼æ¥è·å¾—ä¸€ä¸ªå®Œæ•´çš„é‡åŒ–æ¨¡å‹ã€‚

ä¸ºè§£å†³è¿™ä¸€åˆ†å±‚å‹ç¼©é—®é¢˜ï¼Œè®ºæ–‡ä½œè€…é‡‡ç”¨äº†æœ€ä¼˜è„‘é‡åŒ– (Optimal Brain Quantization, OBQ) æ¡†æ¶ (Frantar et al 2022) ã€‚OBQ æ–¹æ³•çš„å‡ºå‘ç‚¹åœ¨äºå…¶è§‚å¯Ÿåˆ°ï¼šä»¥ä¸Šç­‰å¼å¯ä»¥æ”¹å†™æˆæƒé‡çŸ©é˜µ XX æ¯ä¸€è¡Œçš„å¹³æ–¹è¯¯å·®ä¹‹å’Œï¼š

è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç‹¬ç«‹åœ°å¯¹æ¯ä¸€è¡Œæ‰§è¡Œé‡åŒ–ã€‚å³æ‰€è°“çš„ per-channel quantizationã€‚å¯¹æ¯ä¸€è¡Œ XXï¼ŒOBQ åœ¨æ¯ä¸€æ—¶åˆ»åªé‡åŒ–ä¸€ä¸ªæƒé‡ï¼ŒåŒæ—¶æ›´æ–°æ‰€æœ‰æœªè¢«é‡åŒ–çš„æƒé‡ï¼Œä»¥è¡¥å¿é‡åŒ–å•ä¸ªæƒé‡æ‰€å¸¦æ¥çš„è¯¯å·®ã€‚æ‰€é€‰æƒé‡çš„æ›´æ–°é‡‡ç”¨ä¸€ä¸ªé—­ç¯å…¬å¼ï¼Œå¹¶åˆ©ç”¨äº†æµ·æ£®çŸ©é˜µ (Hessian Matrices)ã€‚

GPTQ è®ºæ–‡é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—ä¼˜åŒ–æªæ–½æ¥æ”¹è¿›ä¸Šè¿°é‡åŒ–æ¡†æ¶ï¼Œåœ¨é™ä½é‡åŒ–ç®—æ³•å¤æ‚åº¦çš„åŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„ç²¾åº¦ã€‚

ç›¸è¾ƒäº OBQï¼ŒGPTQ çš„é‡åŒ–æ­¥éª¤æœ¬èº«ä¹Ÿæ›´å¿«ï¼šOBQ éœ€è¦èŠ±è´¹ 2 ä¸ª GPU æ—¶æ¥å®Œæˆ BERT æ¨¡å‹ (336M) çš„é‡åŒ–ï¼Œè€Œä½¿ç”¨ GPTQï¼Œé‡åŒ–ä¸€ä¸ª Bloom æ¨¡å‹ (176B) åˆ™åªéœ€ä¸åˆ° 4 ä¸ª GPU æ—¶ã€‚

ä¸ºäº†è§£ç®—æ³•çš„æ›´å¤šç»†èŠ‚ä»¥åŠåœ¨å›°æƒ‘åº¦ (perplexity, PPL) æŒ‡æ ‡å’Œæ¨ç†é€Ÿåº¦ä¸Šçš„ä¸åŒæµ‹è¯„æ•°æ®ï¼Œå¯æŸ¥é˜…åŸå§‹è®ºæ–‡ã€‚

### 04. AutoGPTQ library â€“ the one-stop library for efficiently leveraging GPTQ for LLMs

The AutoGPTQ library enables users to quantize ğŸ¤— Transformers models using the GPTQ method. While parallel community efforts such as GPTQ-for-LLaMa, Exllama and llama.cpp implement quantization methods strictly for the Llama architecture, AutoGPTQ gained popularity through its smooth coverage of a wide range of transformer architectures.

Since the AutoGPTQ library has a larger coverage of transformers models, we decided to provide an integrated ğŸ¤— Transformers API to make LLM quantization more accessible to everyone. At this time we have integrated the most common optimization options, such as CUDA kernels. For more advanced options like Triton kernels or fused-attention compatibility, check out the AutoGPTQ library.

AutoGPTQ ä»£ç åº“ â€”â€” ä¸€ç«™å¼åœ°å°† GPTQ æ–¹æ³•åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹

AutoGPTQ ä»£ç åº“è®©ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ GPTQ æ–¹æ³•é‡åŒ– ğŸ¤— Transformers ä¸­æ”¯æŒçš„å¤§é‡æ¨¡å‹ï¼Œè€Œç¤¾åŒºä¸­çš„å…¶ä»–å¹³è¡Œå·¥ä½œå¦‚ GPTQ-for-LLaMa ã€Exllama å’Œ llama.cpp åˆ™ä¸»è¦é’ˆå¯¹ Llama æ¨¡å‹æ¶æ„å®ç°é‡åŒ–ç­–ç•¥ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒAutoGPTQ å› å…¶å¯¹ä¸°å¯Œçš„ transformers æ¶æ„çš„å¹³æ»‘è¦†ç›–è€Œå¹¿å—æ¬¢è¿ã€‚

æ­£å› ä¸º AutoGPTQ ä»£ç åº“è¦†ç›–äº†å¤§é‡çš„ transformers æ¨¡å‹ï¼Œæˆ‘ä»¬å†³å®šæä¾›ä¸€ä¸ª ğŸ¤— Transformers çš„ API é›†æˆï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½å¤Ÿæ›´å®¹æ˜“åœ°ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹é‡åŒ–æŠ€æœ¯ã€‚æˆªæ­¢ç›®å‰ï¼Œæˆ‘ä»¬å·²ç»é›†æˆäº†åŒ…æ‹¬ CUDA ç®—å­åœ¨å†…çš„æœ€å¸¸ç”¨çš„ä¼˜åŒ–é€‰é¡¹ã€‚å¯¹äºæ›´å¤šé«˜çº§é€‰é¡¹å¦‚ä½¿ç”¨ Triton ç®—å­å’Œï¼ˆæˆ–ï¼‰å…¼å®¹æ³¨æ„åŠ›çš„ç®—å­èåˆï¼Œè¯·æŸ¥çœ‹ AutoGPTQ ä»£ç åº“ã€‚

### 05. Native support of GPTQ models in ğŸ¤— Transformers

After installing the AutoGPTQ library and optimum (pip install optimum), running GPTQ models in Transformers is now as simple as:

from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GPTQ", torch_dtype=torch.float16, device_map="auto")

Check out the Transformers documentation to learn more about all the features.

Our AutoGPTQ integration has many advantages:

Quantized models are serializable and can be shared on the Hub.

GPTQ drastically reduces the memory requirements to run LLMs, while the inference latency is on par with FP16 inference.

AutoGPTQ supports Exllama kernels for a wide range of architectures.

The integration comes with native RoCm support for AMD GPUs.

Finetuning with PEFT is available.

You can check on the Hub if your favorite model has already been quantized. TheBloke, one of Hugging Face top contributors, has quantized a lot of models with AutoGPTQ and shared them on the Hugging Face Hub. We worked together to make sure that these repositories will work out of the box with our integration.

This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A100-SXM4-80GB GPU. We used a prompt length of 512, and generated exactly 512 new tokens. The first row is the unquantized fp16 baseline, while the other rows show memory consumption and performance using different AutoGPTQ kernels.

gptq	act_order	bits	group_size	kernel	Load time (s)	Per-token latency (ms)	Throughput (tokens/s)	Peak memory (MB)

False	None	None	None	None	26.0	36.958	27.058	29152.98

True	False	4	128	exllama	36.2	33.711	29.663	10484.34

True	False	4	128	autogptq-cuda-old	36.2	46.44	21.53	10344.62

A more comprehensive reproducible benchmark is available here.

Quantizing models with the Optimum library

To seamlessly integrate AutoGPTQ into Transformers, we used a minimalist version of the AutoGPTQ API that is available in Optimum, Hugging Face's toolkit for training and inference optimization. By following this approach, we achieved easy integration with Transformers, while allowing people to use the Optimum API if they want to quantize their own models! Check out the Optimum documentation if you want to quantize your own LLMs.

Quantizing ğŸ¤— Transformers models with the GPTQ method can be done in a few lines:

from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"

tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=quantization_config)

Quantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are required if one uses a large dataset (e.g. `"c4"``). As mentioned above, many GPTQ models are already available on the Hugging Face Hub, which bypasses the need to quantize a model yourself in most use cases. Nevertheless, you can also quantize a model using your own dataset appropriate for the particular domain you are working on.

Running GPTQ models through Text-Generation-Inference

In parallel to the integration of GPTQ in Transformers, GPTQ support was added to the Text-Generation-Inference library (TGI), aimed at serving large language models in production. GPTQ can now be used alongside features such as dynamic batching, paged attention and flash attention for a wide range of architectures.

As an example, this integration allows to serve a 70B model on a single A100-80GB GPU! This is not possible using a fp16 checkpoint as it exceeds the available GPU memory.

You can find out more about the usage of GPTQ in TGI in the documentation.

Note that the kernel integrated in TGI does not scale very well with larger batch sizes. Although this approach saves memory, slowdowns are expected at larger batch sizes.

Fine-tune quantized models with PEFT

You can not further train a quantized model using the regular methods. However, by leveraging the PEFT library, you can train adapters on top! To do that, we freeze all the layers of the quantized model and add the trainable adapters. Here are some examples on how to use PEFT with a GPTQ model: colab notebook and finetuning script.

Room for improvement

Our AutoGPTQ integration already brings impressive benefits at a small cost in the quality of prediction. There is still room for improvement, both in the quantization techniques and the kernel implementations.

First, while AutoGPTQ integrates (to the best of our knowledge) with the most performant W4A16 kernel (weights as int4, activations as fp16) from the exllama implementation, there is a good chance that the kernel can still be improved. There have been other promising implementations from Kim et al. and from MIT Han Lab that appear to be promising. Moreover, from internal benchmarks, there appears to still be no open-source performant W4A16 kernel written in Triton, which could be a direction to explore.

On the quantization side, let's emphasize again that this method only quantizes the weights. There have been other approaches proposed for LLM quantization that can quantize both weights and activations at a small cost in prediction quality, such as LLM-QAT where a mixed int4/int8 scheme can be used, as well as quantization of the key-value cache. One of the strong advantages of this technique is the ability to use actual integer arithmetic for the compute, with e.g. Nvidia Tensor Cores supporting int8 compute. However, to the best of our knowledge, there are no open-source W4A8 quantization kernels available, but this may well be an interesting direction to explore.

On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

Conclusion and final words

In this blogpost we have presented the integration of the AutoGPTQ library in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs.

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.

The collaboration with the AutoGPTQ team has been very fruitful, and we are very grateful for their support and their work on this library.

We hope that this integration will make it easier for everyone to use LLMs in their applications, and we are looking forward to seeing what you will build with it!

Do not miss the useful resources shared above for better understanding the integration and how to quickly get started with GPTQ quantization.

Original Paper

Basic usage Google Colab notebook - This notebook shows how to quantize your transformers model with GPTQ method, how to do inference, and how to do fine-tuning with the quantized model.

Transformers integration documentation

Optimum integration documentation

The Bloke repositories with compatible GPTQ models.

Acknowledgements

We would like to thank William for his support and his work on the amazing AutoGPTQ library and for his help in the integration. We would also like to thank TheBloke for his work on quantizing many models with AutoGPTQ and sharing them on the Hub and for his help with the integration. We would also like to aknowledge qwopqwop200 for his continuous contributions on AutoGPTQ library and his work on extending the library for CPU that is going to be released in the next versions of AutoGPTQ.

Finally, we would like to thank Pedro Cuenca for his help with the writing of this blogpost.

### ä¸­æ–‡


ğŸ¤— Transformers å¯¹ GPTQ æ¨¡å‹çš„æœ¬åœ°åŒ–æ”¯æŒ

åœ¨å®‰è£… AutoGPTQ ä»£ç åº“å’Œ optimum (pip install optimum) ä¹‹åï¼Œåœ¨ Transformers ä¸­è¿è¡Œ GPTQ æ¨¡å‹å°†éå¸¸ç®€å•ï¼š

from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("TheBloke/Llama-2-7b-Chat-GPTQ", torch_dtype=torch.float16, device_map="auto")

è¯·æŸ¥é˜… Transformers çš„è¯´æ˜æ–‡æ¡£ä»¥äº†è§£æœ‰å…³æ‰€æœ‰ç‰¹æ€§çš„æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬çš„ AutoGPTQ é›†æˆæœ‰ä»¥ä¸‹è¯¸å¤šä¼˜ç‚¹ï¼š

é‡åŒ–æ¨¡å‹å¯è¢«åºåˆ—åŒ–å¹¶åœ¨ Hugging Face Hub ä¸Šåˆ†äº«ã€‚

GPTQ æ–¹æ³•å¤§å¤§é™ä½è¿è¡Œå¤§è¯­è¨€æ¨¡å‹æ‰€éœ€çš„å†…å­˜ï¼ŒåŒæ—¶ä¿æŒç€ä¸ FP16 ç›¸å½“çš„æ¨ç†é€Ÿåº¦ã€‚

AutoGPTQ åœ¨æ›´å¹¿æ³›çš„ transformers æ¶æ„ä¸Šæ”¯æŒ Exllama ç®—å­ã€‚

è¯¥é›†æˆå¸¦æœ‰åŸºäº RoCm çš„ AMD GPU çš„æœ¬åœ°åŒ–æ”¯æŒã€‚

èƒ½å¤Ÿä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹ ã€‚

ä½ å¯ä»¥åœ¨ Hugging Face Hub ä¸ŠæŸ¥çœ‹ä½ æ‰€å–œçˆ±çš„æ¨¡å‹æ˜¯å¦å·²ç»æ‹¥æœ‰ GPTQ é‡åŒ–ç‰ˆæœ¬ã€‚TheBlokeï¼ŒHugging Face çš„é¡¶çº§è´¡çŒ®è€…ä¹‹ä¸€ï¼Œå·²ç»ä½¿ç”¨ AutoGPTQ é‡åŒ–äº†å¤§é‡çš„æ¨¡å‹å¹¶åˆ†äº«åœ¨ Hugging Face Hub ä¸Šã€‚åœ¨æˆ‘ä»¬çš„å…±åŒåŠªåŠ›ä¸‹ï¼Œè¿™äº›æ¨¡å‹ä»“åº“éƒ½å°†å¯ä»¥ä¸æˆ‘ä»¬çš„é›†æˆä¸€èµ·å¼€ç®±å³ç”¨ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ batch size = 1 çš„æµ‹è¯„ç»“æœç¤ºä¾‹ã€‚è¯¥æµ‹è¯„ç»“æœé€šè¿‡åœ¨è‹±ä¼Ÿè¾¾ A100-SXM4-80GB GPU ä¸Šè¿è¡Œå¾—åˆ°ã€‚æˆ‘ä»¬ä½¿ç”¨é•¿åº¦ä¸º 512 ä¸ªè¯å…ƒçš„æç¤ºæ–‡æœ¬ï¼Œå¹¶ç²¾ç¡®åœ°ç”Ÿæˆ 512 ä¸ªæ–°è¯å…ƒã€‚è¡¨æ ¼çš„ç¬¬ä¸€è¡Œå±•ç¤ºçš„æ˜¯æœªé‡åŒ–çš„ fp16 åŸºçº¿ï¼Œå¦å¤–ä¸¤è¡Œåˆ™å±•ç¤ºä½¿ç”¨ AutoGPTQ ä¸åŒç®—å­çš„å†…å­˜å¼€é”€å’Œæ¨ç†æ€§èƒ½ã€‚

gptq	act_order	bits	group_size	kernel	Load time (s)	Per-token latency (ms)	Throughput (tokens/s)	Peak memory (MB)

False	None	None	None	None	26.0	36.958	27.058	29152.98

True	False	4	128	exllama	36.2	33.711	29.663	10484.34

True	False	4	128	autogptq-cuda-old	36.2	46.44	21.53	10344.62

ä¸€ä¸ªæ›´å…¨é¢çš„ã€å¯å¤ç°çš„æµ‹è¯„ç»“æœå¯ä»¥åœ¨è¿™é‡Œå–å¾—ã€‚

ä½¿ç”¨ Optimum ä»£ç åº“é‡åŒ–æ¨¡å‹

ä¸ºäº†å°† AutoGPTQ æ— ç¼é›†æˆåˆ° Transformers ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† AutoGPTQ API çš„ä¸€ä¸ªæç®€ç‰ˆæœ¬ï¼Œå…¶å¯åœ¨ Optimum ä¸­è·å¾— â€”â€” è¿™æ˜¯ Hugging Face é’ˆå¯¹è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–è€Œå¼€å‘çš„ä¸€ä¸ªå·¥å…·åŒ…ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬è½»æ¾åœ°å®ç°äº†ä¸ Transformers çš„é›†æˆï¼ŒåŒæ—¶ï¼Œå¦‚æœäººä»¬æƒ³è¦é‡åŒ–ä»–ä»¬è‡ªå·±çš„æ¨¡å‹ï¼Œä»–ä»¬ä¹Ÿå®Œå…¨å¯ä»¥å•ç‹¬ä½¿ç”¨ Optimum çš„ APIï¼å¦‚æœæƒ³è¦é‡åŒ–ä½ è‡ªå·±çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œè¯·æŸ¥é˜… Optimum çš„è¯´æ˜æ–‡æ¡£ ã€‚

åªéœ€æ•°è¡Œä»£ç ï¼Œå³å¯ä½¿ç”¨ GPTQ æ–¹æ³•é‡åŒ– ğŸ¤— Transformers çš„æ¨¡å‹ï¼š

from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = "facebook/opt-125m"

tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = GPTQConfig(bits=4, dataset = "c4", tokenizer=tokenizer)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=quantization_config)

é‡åŒ–ä¸€ä¸ªæ¨¡å‹å¯èƒ½èŠ±è´¹è¾ƒé•¿çš„æ—¶é—´ã€‚å¯¹äºä¸€ä¸ª 175B å‚æ•°é‡çš„æ¨¡å‹ï¼Œå¦‚æœä½¿ç”¨ä¸€ä¸ªå¤§å‹æ ¡å‡†æ•°æ®é›†ï¼ˆå¦‚ã€Œc4ã€ï¼‰ï¼Œè‡³å°‘éœ€è¦ 4 ä¸ª GPU æ—¶ã€‚æ­£å¦‚ä¸Šé¢æåˆ°çš„é‚£æ ·ï¼Œè®¸å¤š GPTQ æ¨¡å‹å·²ç»å¯ä»¥åœ¨ Hugging Face Hub ä¸Šè¢«å–å¾—ï¼Œè¿™è®©ä½ åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ— éœ€è‡ªè¡Œé‡åŒ–æ¨¡å‹ã€‚å½“ç„¶ï¼Œä½ ä»å¯ä»¥ä½¿ç”¨ä½ æ‰€ä¸“æ³¨çš„ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†æ¥é‡åŒ–æ¨¡å‹ã€‚

é€šè¿‡ Text-Generation-Inference ä½¿ç”¨ GPTQ æ¨¡å‹

åœ¨å°† GPTQ é›†æˆåˆ° Transformers ä¸­çš„åŒæ—¶ï¼ŒText-Generation-Inference ä»£ç åº“ (TGI) å·²ç»æ·»åŠ äº† GPTQ çš„æ”¯æŒï¼Œæ—¨åœ¨ä¸ºç”Ÿäº§ä¸­çš„å¤§è¯­è¨€æ¨¡å‹æä¾›æœåŠ¡ã€‚ç°åœ¨ï¼ŒGPTQ å·²ç»å¯ä»¥ä¸åŠ¨æ€æ‰¹å¤„ç†ã€paged attentionã€flash attention ç­‰ç‰¹æ€§ä¸€èµ·è¢«åº”ç”¨äºå¹¿æ³›çš„ transformers æ¨¡å‹æ¶æ„ ã€‚

ä¾‹å¦‚ï¼Œè¿™ä¸€é›†æˆå…è®¸åœ¨å•ä¸ª A100-80GB GPU ä¸ŠæœåŠ¡ 70B æ¨¡å‹ï¼è€Œè¿™åœ¨ä½¿ç”¨ fp16 çš„æ¨¡å‹æƒé‡æ—¶æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºå®ƒè¶…å‡ºäº†æœ€å¤§å¯ç”¨çš„ GPU å†…å­˜ã€‚

ä½ å¯ä»¥åœ¨ TGI çš„è¯´æ˜æ–‡æ¡£ä¸­æ‰¾åˆ°æ›´å¤šæœ‰å…³ GPTQ çš„ç”¨æ³•ã€‚

éœ€è¦æ³¨æ„çš„æ—¶ï¼ŒTGI ä¸­é›†æˆçš„ç®—å­ä¸èƒ½å¾ˆå¥½åœ°æ‰©å±•åˆ°è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°ã€‚å› æ­¤ï¼Œè¿™ä¸€æ–¹å¼è™½ç„¶èŠ‚çœäº†å†…å­˜ï¼Œä½†åœ¨è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°ä¸Šå‘ç”Ÿé€Ÿåº¦çš„ä¸‹é™æ˜¯ç¬¦åˆé¢„æœŸçš„ã€‚

ä½¿ç”¨ PEFT å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹

åœ¨å¸¸è§„çš„æ–¹æ³•ä¸‹ï¼Œä½ æ— æ³•è¿›ä¸€æ­¥å¾®è°ƒé‡åŒ–åçš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œé€šè¿‡ä½¿ç”¨ PEFT ä»£ç åº“ï¼Œä½ å¯ä»¥åœ¨é‡åŒ–åçš„æ¨¡å‹ä¹‹ä¸Šè®­ç»ƒé€‚åº”æ€§ç½‘ç»œï¼ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å†»ç»“äº†é‡åŒ–è¿‡çš„åŸºåº§æ¨¡å‹çš„æ‰€æœ‰ç½‘ç»œå±‚ï¼Œå¹¶é¢å¤–æ·»åŠ å¯è®­ç»ƒçš„é€‚åº”æ€§ç½‘ç»œã€‚è¿™é‡Œæ˜¯ä¸€äº›å…³äºå¦‚ä½•ä½¿ç”¨ PEFT è®­ç»ƒ GPTQ æ¨¡å‹çš„ä¾‹å­ï¼šColab ç¬”è®°æœ¬å’Œå¾®è°ƒè„šæœ¬ ã€‚

æ”¹è¿›ç©ºé—´

è™½ç„¶æˆ‘ä»¬çš„ AutoGPTQ é›†æˆåœ¨æå°çš„é¢„æµ‹è´¨é‡æŸå¤±ä»£ä»·ä¸‹ï¼Œå¸¦æ¥äº†å¼•äººç©ç›®çš„ä¼˜åŠ¿ã€‚ä½†åœ¨é‡åŒ–æŠ€æœ¯åº”ç”¨å’Œç®—å­å®ç°æ–¹é¢ä»æœ‰æå‡çš„ç©ºé—´ã€‚

é¦–å…ˆï¼Œå°½ç®¡ AutoGPTQ ï¼ˆåœ¨æˆ‘ä»¬çš„è®¤çŸ¥èŒƒå›´å†…ï¼‰å·²ç»é›†æˆäº† exllama ä¸­æ‰€å®ç°çš„æœ€ä½³æ€§èƒ½çš„ W4A16 ç®—å­ï¼ˆæƒé‡ä¸º int4 æ•°å€¼ç±»å‹ï¼Œæ¿€æ´»å€¼ä¸º fp16 æ•°å€¼ç±»å‹ï¼‰ï¼Œå…¶ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ¥è‡ª Kim ç­‰äººçš„å®ç°å’Œ MIT Han Lab çš„æ–¹æ³•ä¼¼ä¹ååˆ†å¯é ã€‚æ­¤å¤–ï¼Œæ ¹æ®æˆ‘ä»¬çš„å†…éƒ¨æµ‹è¯„ï¼Œä¼¼ä¹æš‚æœªæœ‰å¼€æºçš„é«˜æ€§èƒ½çš„ Triton ç‰ˆæœ¬çš„ W4A16 ç®—å­å®ç°ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªå€¼å¾—æ¢ç´¢çš„æ–¹å‘ã€‚

åœ¨é‡åŒ–å±‚é¢ï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡å¼ºè°ƒ GPTQ æ–¹æ³•åªå¯¹æ¨¡å‹æƒé‡è¿›è¡Œé‡åŒ–ã€‚è€Œé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–ï¼Œå­˜åœ¨å…¶ä»–çš„æ–¹æ³•ï¼Œæä¾›äº†ä»¥è¾ƒå°çš„é¢„æµ‹è´¨é‡æŸå¤±ä¸ºä»£ä»·ï¼ŒåŒæ—¶é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼çš„æ–¹æ¡ˆã€‚å¦‚ LLM-QAT é‡‡ç”¨ int4/int8 çš„æ··åˆç²¾åº¦æ–¹æ¡ˆï¼ŒåŒæ—¶è¿˜å¯¹ KV Cache æ–½è¡Œé‡åŒ–ã€‚è¿™ä¸€æŠ€æœ¯çš„å¼ºå¤§ä¼˜ç‚¹æ˜¯èƒ½å®é™…ä½¿ç”¨æ•´æ•°è¿ç®—ç®—æ³•æ¥è¿›è¡Œè®¡ç®—ï¼Œä¸€ä¸ªä¾‹å­æ˜¯è‹±ä¼Ÿè¾¾çš„å¼ é‡æ ¸å¿ƒæ”¯æŒ int8 è®¡ç®— ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰æš‚æ— å¼€æºçš„ W4A8 é‡åŒ–ç®—å­ï¼Œä½†è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå€¼å¾—æ¢ç´¢çš„æ–¹å‘ ã€‚

åœ¨ç®—å­å±‚é¢ï¼Œä¸ºæ›´å¤§çš„æ‰¹å¤„ç†å¤§å°è®¾è®¡é«˜æ€§èƒ½çš„ W4A16 ç®—å­ä»ç„¶æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚

å·²æ”¯æŒçš„æ¨¡å‹

åœ¨åˆå§‹å®ç°ä¸­ï¼Œæš‚æ—¶åªæ”¯æŒçº¯ç¼–ç å™¨æˆ–çº¯è§£ç å™¨æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ã€‚è¿™å¬èµ·æ¥ä¼¼ä¹æœ‰è¾ƒå¤§çš„å±€é™æ€§ï¼Œä½†å…¶å®å·²ç»æ¶µç›–äº†å½“å‰ç»å¤§å¤šæ•°æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ Llamaã€OPTã€GPT-Neoã€GPT-NeoX ç­‰ã€‚

å¤§å‹çš„è§†è§‰ã€è¯­éŸ³å’Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ç°é˜¶æ®µæš‚ä¸è¢«æ”¯æŒã€‚

ç»“è®ºå’Œç»“è¯­

æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† Transformers å¯¹ AutoGPTQ ä»£ç åº“çš„é›†æˆï¼Œä½¿å¾—ç¤¾åŒºä¸­çš„ä»»ä½•äººéƒ½å¯ä»¥æ›´æ–¹ä¾¿åœ°åˆ©ç”¨ GPTQ æ–¹æ³•é‡åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›ä»¤äººæ¿€åŠ¨çš„å¤§è¯­è¨€æ¨¡å‹å·¥å…·å’Œåº”ç”¨çš„æ„å»ºã€‚

è¿™ä¸€é›†æˆæ”¯æŒè‹±ä¼Ÿè¾¾ GPU å’ŒåŸºäº RoCm çš„ AMD GPUï¼Œè¿™æ˜¯å‘æ”¯æŒæ›´å¹¿æ³› GPU æ¶æ„çš„é‡åŒ–æ¨¡å‹çš„æ™®æƒ åŒ–è¿ˆå‡ºçš„ä¸€å¤§æ­¥ã€‚

ä¸ AutoGPTQ å›¢é˜Ÿçš„åˆä½œéå¸¸å¯Œæœ‰æˆæ•ˆï¼Œæˆ‘ä»¬éå¸¸æ„Ÿè°¢ä»–ä»¬çš„æ”¯æŒå’Œä»–ä»¬åœ¨è¯¥ä»£ç åº“ä¸Šçš„å·¥ä½œã€‚

æˆ‘ä»¬å¸Œæœ›æœ¬æ¬¡é›†æˆå°†ä½¿æ¯ä¸ªäººéƒ½æ›´å®¹æ˜“åœ°åœ¨ä»–ä»¬çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬è¿«ä¸åŠå¾…åœ°æƒ³è¦çœ‹åˆ°å¤§å®¶å³å°†ä½¿ç”¨å®ƒæ‰€åˆ›é€ å‡ºçš„ä¸€åˆ‡ï¼

å†æ¬¡æé†’ä¸è¦é”™è¿‡æ–‡ç« å¼€å¤´åˆ†äº«çš„æœ‰ç”¨èµ„æºï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æœ¬æ¬¡é›†æˆçš„ç‰¹æ€§ä»¥åŠå¦‚ä½•å¿«é€Ÿå¼€å§‹ä½¿ç”¨ GPTQ é‡åŒ–ã€‚

åŸå§‹è®ºæ–‡

è¿è¡Œäº Google Colab ç¬”è®°æœ¬ä¸Šçš„åŸºç¡€ç”¨ä¾‹ â€”â€” è¯¥ç¬”è®°æœ¬ä¸Šçš„ç”¨ä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ GPTQ æ–¹æ³•é‡åŒ–ä½ çš„ transformers æ¨¡å‹ã€å¦‚ä½•è¿›è¡Œé‡åŒ–æ¨¡å‹çš„æ¨ç†ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

Transformers ä¸­é›†æˆ GPTQ çš„è¯´æ˜æ–‡æ¡£

Optimum ä¸­é›†æˆ GPTQ çš„è¯´æ˜æ–‡æ¡£

TheBloke æ¨¡å‹ä»“åº“ä¸­çš„ GPTQ æ¨¡å‹ã€‚

è‡´è°¢

æ„Ÿè°¢æ½˜å…¶å¨å¯¹æ°å‡ºçš„ AutoGPTQ ä»£ç åº“çš„æ”¯æŒå’Œæ‰€ä½œçš„å·¥ä½œï¼Œä»¥åŠä»–å¯¹æœ¬æ¬¡é›†æˆçš„å¸®åŠ©ã€‚æ„Ÿè°¢ TheBloke ä½¿ç”¨ AutoGPTQ é‡åŒ–å¤§é‡çš„æ¨¡å‹å¹¶åˆ†äº«åœ¨ Hugging Face Hub ä¸Šï¼Œä»¥åŠä»–åœ¨æœ¬æ¬¡é›†æˆä¸­æ‰€æä¾›çš„å¸®åŠ©ã€‚æ„Ÿè°¢ qwopqwop200 å¯¹ AutoGPTQ ä»£ç åº“çš„æŒç»­è´¡çŒ®ï¼Œç›®å‰ï¼Œä»–æ­£è‡´åŠ›äºå°†è¯¥ä»£ç åº“çš„ä½¿ç”¨åœºæ™¯æ‹“å±•è‡³ CPU ï¼Œè¿™ä¸€ç‰¹æ€§å°†åœ¨ AutoGPTQ çš„ä¸‹ä¸€ç‰ˆæœ¬ä¸­å‘å¸ƒã€‚

æœ€åï¼Œæˆ‘ä»¬è¿˜è¦æ„Ÿè°¢ Pedro Cuenca å¯¹æœ¬æ–‡çš„æ’°å†™æ‰€æä¾›çš„å¸®åŠ©ã€‚


