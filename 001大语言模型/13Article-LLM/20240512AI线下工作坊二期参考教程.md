## 20240512AI线下工作坊二期参考教程

模块一练习：如何使用 FastGPT 创建在线知识库与应用？

第一步，打开 https://fastgpt.in/，注册账号并登录。

第二步，在「我的知识库」页面，新建一个知识库。

选择通用知识库，填入知识库名称，选择模型。此处的两个模型均使用默认值。

第三步，导入外部数据。

目前支持三种导入和新建数据集的方式：手动数据集、文本数据集、表格数据集。

以文本数据集为例，有三种导入文本的方式：

支持 txt、docx、csv、pdf、md、html 类型的文件

配置数据处理参数。可根据实际需求，调整训练模式、自定义文本分割的规则。

第四步，进入应用页面，创建一个【知识库 + 对话引导】的应用。

当前支持四种应用类型：

创建一个【知识库 + 对话引导】应用

第五步，进入应用配置环节，设置各类参数，最后进行调试预览。

需要配置的参数如下：

【AI 模型】，选择默认的 FastAI-turbo

【关联知识库】，选择刚刚上一步创建的知识库

填写【对话开场白】

点击「发布」，测试效果：

第六步，发布应用。第一种方式：以分享链接的形式。

点击左侧【发布应用】

选择发布为【免登录窗口】，填入名称

确认即可生成一条新的免登录窗口

点击【开始使用】，选择第一种使用方式：将链接复制到浏览器中打开

在任何有网络的终端，打开上面这个链接，即可与刚刚创建好的应用对话

第七步，发布应用。第二种方式：以 API 访问的形式。

创建一个新的 API key

打开 42chat 设置页面（网址：42chat.io），贴入接口地址（为 fastgpt 中的 API 根地址）、API key，即可将刚刚创建的知识库应用，接入 42chat 对话界面。

对话测试

模块二练习：如何使用 Apify 爬取网页数据？

第一步，打开 https://apify.com/ ，点击 Sign up 注册账号。使用邮箱注册即可。

第二步，注册成功后登录控制台，https://console.apify.com/。点击左边的 store，进入 apify store 界面。

第三步，选择 Website Content Crawler 进入详细界面，https://console.apify.com/actors/aYG0l9s7dbB7j3gbS/console

第四步，将自己需要爬取的网页 URLs 填入 Start URLs

第五步，点击 Start，即可开始自动爬取，查看爬取进程

第六步，等待爬取完成后。点击 Export results，导出数据。选取自己需要的数据格式。

模块三练习：如何使用 Ollama 本地运行大模型？

第一步，打开 https://ollama.com/ ，点击下载并完成安装。

第二步，进入 https://ollama.com/library 模型仓库，查看 ollama 支持哪些模型。

第三步，点击你想要在本地运行的大模型，查看详细信息。此处以阿里通义千问 qwen 为例，进入模型详情页。

点击右边的 Tags，可以查看历史所有版本：

下拉框，选择你想要运行的版本，然后 copy 右边的命令：

第四步，下载大模型文件到本地。打开电脑终端，粘贴刚刚复制的命令：ollama run qwen:0.5b-chat，即可开始下载大模型。（打开电脑终端方式 MAC 电脑：按下 Command（⌘）+ Space 键，输入「Terminal」，选择并打开「Terminal」。Windows 电脑点击屏幕左下角的「开始」按钮，输入「cmd」，选择并打开「命令提示符」。）

下图为正在下载大模型文件，需等待安装完成。【下载时间受诸多因素影响，课堂上可以优先下载小参数的模型】

下载完成完成，按照最后一栏的提示，输入 /? 可以查看帮助。

第五步，与大模型对话。

在命令行中输入信息，即可与大模型对话。

在与大模型对话中，想寻求帮助，可以输入 /？。

第六步，输入命令 /bye 结束此次对话。

第七步，如果想再次开启与大模型对话窗口，只需 1. 启动 Ollama APP，2. 打开终端，3. 在终端输入 【ollama run + 大模型名称】（如输入 ollama run qwen:0.5b-chat，即可运行 qwen:0.5b-chat）。

ollama 常见运行命令：其中最常用的是 ollama run、ollama list、ollama pull、ollama rm。

寻求帮助：输入 `ollama help`，可查看相关的运行命令，如果忘记了某个命令，也可以使用 ollama help 查看。

查看本地已安装大模型：使用 ollama list

删除某个大模型：ollama rm + 模型名称

课后福利包

如何安装 Open WebUI ? （需先安装 docker）

第一步，安装 docker。打开 https://www.docker.com/products/docker-desktop/，选择操作系统，点击下载，然后一步步安装。

第二步，打开并运行 Ollama。

第三步，打开 https://github.com/open-webui/open-webui ，找到使用 Docker 快速安装的方法：

在本地终端输入命令：docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

第四步，打开 Docker，查看正在运行的 open-webui

显示正在运行：

点击本地端口链接，即可跳转到 webui 页面。

第五步，首次进入，需要注册 webui 的账号。并登录。

登录后的界面如下：

第六步，选择具体模型，即可开始与大模型对话。

如何使用 Ollama+Open WebUI 实现本地 RAG?（需先安装 docker）

第一步，启动 Ollama 和 Open WebUI

第二步，打开 webui，点击左边的 documents，查看文档。

此处可以查看你使用过的文档。以及上传本地文档。

第三步，在对话界面，点击 + 号，选择本地文档，然后输入 prompt。webui 会自动处理上传的文档。

如何使用 Ollama+AnythingLLM 实现本地 RAG?

第一步，安装 AnythingLLM。打开 https://useanything.com/download，选择操作系统，下载并安装。

第二步，打开 AnythingLLM，开始配置。选择大模型：使用 Ollama

选择 Ollama

填入 Ollama Base URL：http://127.0.0.1:11434

选择模型，设定 token context window

第三步，选择嵌入模型：

此处选择默认自带的 AnythingLLMEmbedder：

第四步，选择向量数据库：

此处选择默认的，纯本地的 LanceDB

第五步，进入对话界面，点击 upload a document

上传需要对话的文档

然后点击 move to workspace

最后点击 save and embed

第六步，开始对话。回答可以显示具体的引用来源。