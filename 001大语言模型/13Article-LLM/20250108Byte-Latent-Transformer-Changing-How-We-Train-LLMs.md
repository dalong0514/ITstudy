## 20250108Byte-Latent-Transformer-Changing-How-We-Train-LLMs

Byte Latent Transformer: Changing How We Train LLMs

2025-01-06

Vishal Rajput

Published in AIGuys

We all know that computers can't read text, what they read are numbers. All the text is converted into numbers using different strategies and fed to the computers. But what about AI, can't LLMs read and write a text? No, they read and write tokens. Tokens are a way to represent text, which are the fundamental units of text used to process and generate language. Tokens can represent characters, subwords, words, or even punctuation, depending on how the language model's tokenizer works.

æˆ‘ä»¬éƒ½çŸ¥é“ï¼Œè®¡ç®—æœºæœ¬èº«æ— æ³•ç›´æ¥ç†è§£æ–‡æœ¬ï¼Œå®ƒä»¬å¤„ç†çš„æ˜¯æ•°å­—ã€‚æ‰€æœ‰çš„æ–‡æœ¬éƒ½éœ€è¦é€šè¿‡ä¸åŒçš„æ–¹æ³•è½¬æ¢æˆæ•°å­—ï¼Œæ‰èƒ½è¢«è®¡ç®—æœºè¯»å–ã€‚é‚£ä¹ˆï¼ŒAIï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œéš¾é“å¯ä»¥ç›´æ¥è¯»å–å’Œä¹¦å†™æ–‡æœ¬å—ï¼Ÿ ç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œå®ƒä»¬å®é™…ä¸Šè¯»å–å’Œä¹¦å†™çš„æ˜¯ Tokenï¼ˆTokenï¼‰ã€‚Token æ˜¯ä¸€ç§æ–‡æœ¬çš„è¡¨ç¤ºæ–¹å¼ï¼Œæ˜¯ç”¨äºå¤„ç†å’Œç”Ÿæˆè¯­è¨€çš„åŸºæœ¬å•å…ƒã€‚æ ¹æ®è¯­è¨€æ¨¡å‹æ‰€ä½¿ç”¨çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰çš„ä¸åŒï¼ŒToken å¯ä»¥ä»£è¡¨å­—ç¬¦ã€å­è¯ã€å•è¯ï¼Œç”šè‡³æ˜¯æ ‡ç‚¹ç¬¦å·ã€‚

However, new work from Meta's FAIR lab is challenging the well-established paradigm of tokens in LLM space, and it has developed a thing called Patches and Byte Latent Transformer. So, without further ado, let's go deep into this new paper.

ä¸è¿‡ï¼ŒMeta å…¬å¸ FAIR å®éªŒå®¤çš„æœ€æ–°ç ”ç©¶æ­£åœ¨æŒ‘æˆ˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸä¸­å·²è¢«å¹¿æ³›æ¥å—çš„ Token æœºåˆ¶ï¼Œä»–ä»¬å¼€å‘äº†ä¸€ç§åä¸º Patches å’Œ Byte Latent Transformer çš„æ–°æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨è¿™ç¯‡æ–°è®ºæ–‡ã€‚

### Tokens and Tokenization

A token is a segment of text that the model processes as a single unit.

ä¸€ä¸ª Token æ˜¯æ¨¡å‹å°†å…¶è§†ä¸ºä¸€ä¸ªå•ç‹¬å•å…ƒè¿›è¡Œå¤„ç†çš„æ–‡æœ¬ç‰‡æ®µã€‚

For example:

Word-based tokenization: "Artificial Intelligence" â†’ ["Artificial", "Intelligence"]

åŸºäºè¯çš„ Token åŒ–ï¼šã€ŒArtificial Intelligenceã€è½¬æ¢ä¸º [ã€Œäººå·¥æ™ºèƒ½ã€]

Subword tokenization: "Artificial Intelligence" â†’ ["Art", "ificial", "Int", "elligence"]

Character-based tokenization: "Artificial Intelligence" â†’ ["A", "r", "t", "i", ...]

å­è¯æ ‡è®°åŒ–ï¼ˆSubword tokenization)ï¼š"Artificial Intelligenceã€å˜ä¸º ["Artã€ï¼Œ"ificialã€ï¼Œ"Intã€ï¼Œ"elligence"]

åŸºäºå­—ç¬¦çš„æ ‡è®°åŒ–ï¼ˆCharacter-based tokenization)ï¼š"Artificial Intelligenceã€å˜ä¸º ["Aã€ï¼Œ"rã€ï¼Œ"tã€ï¼Œ"iã€ï¼Œ...]

Most modern LLMs like GPT, Claude, or any other Transformer model use subword tokenization (e.g., Byte Pair Encoding or SentencePiece) to handle out-of-vocabulary words efficiently.

å¤§å¤šæ•°ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¾‹å¦‚ GPTã€Claude æˆ–è€…å…¶ä»–ä»»ä½• Transformer æ¨¡å‹ï¼Œéƒ½é‡‡ç”¨å­è¯åˆ†è¯ï¼ˆsubword tokenizationï¼‰æŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œå­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼‰æˆ– SentencePieceï¼‰ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°å¤„ç†è¯æ±‡è¡¨ä¸­æœªå‡ºç°çš„è¯ã€‚

Example (using GPT-like tokenization):

Input: "Hello, world!"

ä½ å¥½ï¼Œä¸–ç•Œï¼

Tokenized Output: [15496, 11, 995]

"Hello" â†’ 15496

Token åºåˆ—ï¼š[15496ï¼Œ11ï¼Œ995]

"Helloã€å¯¹åº” 15496

"," â†’ 11

"world" â†’ 995

æœªæ‰¾åˆ°æ„è¯‘å†…å®¹

Input Representation: To do any processing, the model converts raw text into tokens. These tokens are later mapped to embeddings (numerical representations) for computation.

è¾“å…¥æ•°æ®çš„è¡¨ç¤ºæ–¹å¼ï¼šä¸ºäº†è¿›è¡Œå¤„ç†ï¼Œæ¨¡å‹ä¼šå°†åŸå§‹æ–‡æœ¬åˆ†è§£æˆ Tokenï¼ˆTokenï¼‰ã€‚è¿™äº› Token ä¼šè¢«è¿›ä¸€æ­¥è½¬æ¢æˆåµŒå…¥ï¼ˆç”¨æ•°å€¼è¡¨ç¤ºï¼‰ï¼Œä»¥ä¾¿è¿›è¡Œè®¡ç®—ã€‚

Output Generation: When generating text, LLMs produce tokens in an autoregressive fashion (one at a time), which are then decoded back into human-readable text.

è¾“å‡ºç”Ÿæˆï¼šåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ï¼ŒLLM ä»¥è‡ªå›å½’çš„æ–¹å¼ï¼ˆæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªï¼‰ç”Ÿæˆ Tokenï¼Œç„¶åå°†è¿™äº› Token è§£ç ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬ã€‚

### Tokenization Algorithms

Byte Pair Encoding (BPE)

### æ–‡æœ¬æ ‡è®°åŒ–ç®—æ³•å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼ŒBPE)

Breaks words into subwords based on frequency. Suppose the data to be encoded is

aaabdaaabac

æ ¹æ®è¯é¢‘å°†å•è¯æ‹†åˆ†æˆå­è¯ã€‚ä¾‹å¦‚ï¼Œå¾…ç¼–ç çš„æ•°æ®å¦‚ä¸‹ï¼š

aaabdaaabac

The byte pair "aa" occurs most often, so it will be replaced by a byte that is not used in the data, such as "Z". Now there is the following data and replacement table:

å­—èŠ‚å¯¹ã€Œaaã€å‡ºç°çš„é¢‘ç‡æœ€é«˜ï¼Œæ‰€ä»¥ä¼šè¢«æ›¿æ¢æˆä¸€ä¸ªæ•°æ®ä¸­æœªä½¿ç”¨çš„å­—èŠ‚ï¼Œæ¯”å¦‚ã€ŒZã€ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹çš„æ•°æ®å’Œæ›¿æ¢è¡¨ï¼š

ZabdZabac

Z=aa

ZabdZabac

Z=aa

Then the process is repeated with byte pair "ab", replacing it with "Y":

ZYdZYac

æ¥ä¸‹æ¥ï¼Œé’ˆå¯¹å­—ç¬¦ä¸²ã€ŒZYdZYacã€ï¼Œé‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œå°†å­—èŠ‚å¯¹ã€Œabã€æ›¿æ¢ä¸ºã€ŒY"ï¼š

ZYdZYac

Y=ab

Z=aa

Y=ab

Z=aa

The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing "ZY" with "X":

å‰©ä¸‹çš„å”¯ä¸€ä¸€ä¸ªåŸå§‹å­—èŠ‚å¯¹ä»…å‡ºç°ä¸€æ¬¡ï¼Œç¼–ç åˆ°æ­¤å¯èƒ½å°±ç»“æŸäº†ã€‚æˆ–è€…ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥ç»§ç»­è¿›è¡Œé€’å½’çš„å­—èŠ‚å¯¹ç¼–ç ï¼Œä¹Ÿå°±æ˜¯å°†ã€ŒZYã€æ›¿æ¢ä¸ºã€ŒXã€ï¼š

XdXac

X=ZY

XdXac

X=ZY

Y=ab

Z=aa

Y=ab

Z=aa

This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.

To decompress the data, simply perform the replacements in the reverse order.

å½“å‰æ•°æ®æ— æ³•é€šè¿‡å­—èŠ‚å¯¹ç¼–ç ï¼ˆbyte pair encodingï¼‰è¿›ä¸€æ­¥å‹ç¼©ï¼Œå› ä¸ºå…¶ä¸­ä¸å­˜åœ¨é‡å¤å‡ºç°çš„å­—èŠ‚å¯¹ã€‚

è¦è§£å‹ç¼©æ•°æ®ï¼Œåªéœ€æŒ‰ç…§æ›¿æ¢çš„é€†åºæ‰§è¡Œå³å¯ã€‚

So, what's the problem with BPE? It can have instances where there is more than one way to encode a particular word. It then gets difficult for the algorithm to choose subword tokens as there is no way to prioritize which one to use first. Hence, the same input can be represented by different encodings impacting the accuracy of the learned representations.

é‚£ä¹ˆï¼Œå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æœ‰ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿå®ƒå¯èƒ½ä¼šå‡ºç°ä¸€ä¸ªå•è¯æœ‰å¤šç§ç¼–ç æ–¹å¼çš„æƒ…å†µã€‚è¿™æ—¶ï¼Œç®—æ³•å°±éš¾ä»¥é€‰æ‹©åˆé€‚çš„å­è¯ Tokenï¼Œå› ä¸ºæ²¡æœ‰æ˜ç¡®çš„è§„åˆ™æ¥å†³å®šä¼˜å…ˆä½¿ç”¨å“ªä¸€ä¸ªã€‚å› æ­¤ï¼ŒåŒæ ·çš„è¾“å…¥å¯èƒ½ä¼šè¢«ç¼–ç æˆä¸åŒçš„å½¢å¼ï¼Œè¿™ä¼šå½±å“æ¨¡å‹å­¦ä¹ åˆ°çš„ç‰¹å¾è¡¨ç¤ºçš„å‡†ç¡®æ€§ã€‚

WordPiece

Similar to BPE but optimized for maximum likelihood estimation.

WordPiece

ä¸ BPE ç±»ä¼¼ï¼Œä½†å®ƒé’ˆå¯¹æœ€å¤§ä¼¼ç„¶ä¼°è®¡è¿›è¡Œäº†ä¼˜åŒ–ã€‚

The only difference between WordPiece and BPE is the way in which symbol pairs are added to the vocabulary. At each iterative step, WordPiece chooses a symbol pair which will result in the largest increase in likelihood upon merging. Maximizing the likelihood of the training data is equivalent to finding the symbol pair whose probability divided by the probability of the first followed by the probability of the second symbol in the pair is greater than any other symbol pair.

WordPiece å’Œ BPE ä¹‹é—´å”¯ä¸€çš„åŒºåˆ«åœ¨äºå¦‚ä½•å°†ç¬¦å·å¯¹æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒWordPiece ä¼šé€‰æ‹©åˆå¹¶åèƒ½ä½¿ä¼¼ç„¶æ€§å¢åŠ æœ€å¤§çš„ç¬¦å·å¯¹ã€‚æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶æ€§ï¼Œç­‰ä»·äºæ‰¾åˆ°è¿™æ ·ä¸€ä¸ªç¬¦å·å¯¹ï¼šå®ƒçš„æ¦‚ç‡é™¤ä»¥è¯¥å¯¹ä¸­ç¬¬ä¸€ä¸ªç¬¦å·çš„æ¦‚ç‡å’Œç¬¬äºŒä¸ªç¬¦å·çš„æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œå¾—åˆ°çš„ç»“æœè¦å¤§äºä»»ä½•å…¶ä»–ç¬¦å·å¯¹ã€‚

WordPiece starts with a fixed initial vocabulary. This usually includes:

Single characters: ["u", "n", "h", "a", "p", "i", "e", "s"]

WordPiece ä»ä¸€ä¸ªå›ºå®šçš„åˆå§‹è¯æ±‡è¡¨å¼€å§‹ã€‚ä¾‹å¦‚ï¼Œå®ƒé€šå¸¸åŒ…æ‹¬ï¼š

å•ä¸ªå­—ç¬¦ï¼š["uã€ï¼Œ"nã€ï¼Œ"hã€ï¼Œ"aã€ï¼Œ"pã€ï¼Œ"iã€ï¼Œ"eã€ï¼Œ"s"]

A special symbol to indicate subword continuation: ## (e.g., ##ness)

Common words or frequent subwords: ["happy", "un", "##ness"]

ç”¨ç‰¹æ®Šç¬¦å· ## æ¥è¡¨ç¤ºå­è¯çš„å»¶ç»­ï¼Œä¾‹å¦‚ ##nessã€‚

ä¸€äº›å¸¸ç”¨è¯æˆ–è€…é«˜é¢‘å‡ºç°çš„å­è¯åŒ…æ‹¬ï¼š["happyã€ï¼Œ"unã€ï¼Œ"##ness"]ï¼Œå…¶ä¸­ã€Œ##nessã€è¡¨ç¤ºã€Œnessã€æ˜¯ä¸€ä¸ªå­è¯ï¼Œå®ƒé€šå¸¸ä¼šå’Œå‰é¢çš„è¯ç»„åˆåœ¨ä¸€èµ·ä½¿ç”¨ã€‚

Tokenization Process

The tokenization breaks the word into subwords using the vocabulary.

Token åŒ–è¿‡ç¨‹

Token åŒ–è¿‡ç¨‹ä¼šä½¿ç”¨è¯æ±‡è¡¨å°†å•è¯åˆ†è§£ä¸ºå­è¯ã€‚

For "unhappiness":

Start with the whole word: WordPiece tries to match the longest possible token in the vocabulary.

å¯¹äºã€Œunhappinessã€ï¼š

ä»æ•´ä¸ªå•è¯å¼€å§‹ï¼šWordPiece å°è¯•åœ¨è¯æ±‡è¡¨ä¸­åŒ¹é…æœ€é•¿çš„ Token ã€‚

un is in the vocabulary, so it's selected.

Remaining part: happiness.

'un' è¿™ä¸ªè¯åœ¨è¯æ±‡è¡¨ä¸­ï¼Œå› æ­¤è¢«é€‰ä¸­ã€‚

å‰©ä½™éƒ¨åˆ†æ˜¯ï¼š'happiness'ã€‚

Continue with the next part:

The next longest match is happy (from happiness).

æ¥ä¸‹æ¥ï¼Œæœ€é•¿çš„åŒ¹é…æ˜¯ happyï¼ˆå®ƒæ¥è‡ª happinessï¼‰ã€‚

Remaining part: ness.

Handle the rest:

å‰©ä½™éƒ¨åˆ†ï¼šåŠŸèƒ½æ€§ã€‚

ness is in the vocabulary, but because it's a suffix, it's tokenized as ##ness.

Final Tokens

ã€Œnessã€è¿™ä¸ªè¯ç¼€å­˜åœ¨äºè¯æ±‡ä¸­ï¼Œä½†å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªåç¼€ï¼Œæ‰€ä»¥è¢« Token åŒ–ä¸º ##nessã€‚

æœ€ç»ˆçš„ Token ç»“æœ

["un", "happy", "##ness"]

SentencePiece

["unã€ï¼Œ"happyã€ï¼Œ"##ness"]

SentencePiece

Works directly with raw text, handling languages with complex scripts (e.g., Japanese, Chinese).

Character-Based Tokenization: SentencePiece works directly on raw text, bypassing language-specific preprocessing (e.g., no need to split the text into words).

ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ—¥è¯­ã€ä¸­æ–‡ç­‰å¤æ‚æ–‡å­—çš„è¯­è¨€ã€‚

åŸºäºå­—ç¬¦çš„ Tokenizationï¼ˆCharacter-Based Tokenization)ï¼šSentencePiece ç›´æ¥å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œæ— éœ€è¿›è¡Œç‰¹å®šçš„è¯­è¨€é¢„å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œä¸éœ€è¦å°†æ–‡æœ¬æ‹†åˆ†æˆå•è¯ï¼‰ã€‚

Subword Units: It breaks text into subword units (like WordPiece) but doesn't rely on spaces as word boundaries.

Model-Based Tokenization: Uses algorithms like Byte Pair Encoding (BPE) or Unigram Language Model for tokenization.

å­è¯ï¼ˆsubwordï¼‰å•å…ƒï¼šå®ƒå°†æ–‡æœ¬åˆ†è§£ä¸ºå­è¯å•å…ƒï¼ˆä¾‹å¦‚ WordPieceï¼‰ï¼Œä½†ä¸ä¾èµ–ç©ºæ ¼ä½œä¸ºè¯çš„è¾¹ç•Œã€‚

ä½¿ç”¨æ¨¡å‹è¿›è¡Œ Token åŒ–ï¼šä½¿ç”¨ä¾‹å¦‚å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encodingï¼ŒBPEï¼‰æˆ– Unigram è¯­è¨€æ¨¡å‹ç­‰ç®—æ³•è¿›è¡Œ Token åŒ–ï¼ˆtokenizationï¼‰ã€‚

Input: "I like to learn SentencePiece."

Handling spaces "_I_like_to_learn_SentencePiece."

æˆ‘å–œæ¬¢å­¦ä¹  SentencePiece ã€‚

SentencePiece é€šè¿‡åœ¨è¯çš„å¼€å¤´æ·»åŠ ä¸‹åˆ’çº¿ `_` æ¥è¡¨ç¤ºç©ºæ ¼ï¼Œä¾‹å¦‚ã€Œ_I_like_to_learn_SentencePiece.ã€ã€‚

SentencePiece builds a vocabulary using subword units based on frequency in the corpus. It might include:

Characters: ["I", "l", "k", "t", "o", "e", "n", "c", "P", "S"]

SentencePiece é€šè¿‡åˆ†æè¯­æ–™åº“ä¸­è¯æ±‡çš„é¢‘ç‡ï¼Œä½¿ç”¨å­è¯å•å…ƒæ¥æ„å»ºè¯æ±‡è¡¨ã€‚å…¶ä¸­å¯èƒ½åŒ…å«ä»¥ä¸‹å­—ç¬¦ï¼š

å­—ç¬¦ï¼š["Iã€ï¼Œ"lã€ï¼Œ"kã€ï¼Œ"tã€ï¼Œ"oã€ï¼Œ"eã€ï¼Œ"nã€ï¼Œ"cã€ï¼Œ"Pã€ï¼Œ"S"]

Common subwords: ["I", "like", "to", "_learn", "Sentence", "Piece"]

Special tokens: ["_", "##"]

å¸¸ç”¨å­è¯ï¼š[ã€Œæˆ‘ã€ï¼Œã€Œå–œæ¬¢ã€ï¼Œã€Œå»ã€ï¼Œã€Œ_å­¦ä¹ ï¼ˆlearn)ã€ï¼Œã€Œå¥å­ã€ï¼Œã€Œç‰‡æ®µã€]

ç‰¹æ®Š tokensï¼š[ã€Œ_ï¼ˆè¡¨ç¤ºè¯çš„å¼€å¤´)ã€ï¼Œã€Œ##ï¼ˆè¡¨ç¤ºå­è¯)ã€]

Final result: ["_I", "_like", "_to", "_learn", "_Sentence", "Piece"]

Byte-Level Tokenization

è¾“å‡ºç»“æœä¸ºï¼š["_Iã€ï¼Œ"_likeã€ï¼Œ"_toã€ï¼Œ"_learnã€ï¼Œ"_Sentenceã€ï¼Œ"Piece"]

å­—èŠ‚çº§ Token åŒ–ï¼ˆByte-Level Tokenization)

Operates at the byte level for flexibility with various scripts and encodings.

Text to Bytes:

ä»¥å­—èŠ‚ä¸ºå•ä½è¿›è¡Œæ“ä½œï¼Œä»è€Œçµæ´»åœ°æ”¯æŒå„ç§æ–‡å­—å’Œç¼–ç æ ¼å¼ã€‚

æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚ï¼š

"unhappiness" â†’ [117, 110, 104, 97, 112, 112, 105, 110, 101, 115, 115]

Map Bytes to Tokens:

ã€Œunhappinessã€â†’ [117ï¼Œ110ï¼Œ104ï¼Œ97ï¼Œ112ï¼Œ112ï¼Œ105ï¼Œ110ï¼Œ101ï¼Œ115ï¼Œ115]

** å­—èŠ‚åˆ° Token çš„æ˜ å°„ï¼š**

Byte Mapping: [117 â†’ 32, 110 â†’ 45, 104 â†’ 67, ...]

Result: [32, 45, 67, 90, 103, 103, 104, 45, 23, 8

å­—èŠ‚å¯¹åº”å…³ç³»ï¼š[117 â†’ 32ï¼Œ110 â†’ 45ï¼Œ104 â†’ 67ï¼Œç­‰ç­‰]

è½¬æ¢åçš„ç»“æœï¼š[32ï¼Œ45ï¼Œ67ï¼Œ90ï¼Œ103ï¼Œ103ï¼Œ104ï¼Œ45ï¼Œ23ï¼Œ8]

Tokens to Bytes to Text:

[32, 45, 67, 90, 103, 103, 104, 45, 23, 88, 88] â†’ [117, 110, 104, 97, 112, 112, 105, 110, 101, 115, 115] â†’ `"unhappiness"`

Token åˆ°å­—èŠ‚åˆ°æ–‡æœ¬çš„è½¬æ¢ï¼š

[32ï¼Œ45ï¼Œ67ï¼Œ90ï¼Œ103ï¼Œ103ï¼Œ104ï¼Œ45ï¼Œ23ï¼Œ88ï¼Œ88] è½¬æ¢ä¸º [117ï¼Œ110ï¼Œ104ï¼Œ97ï¼Œ112ï¼Œ112ï¼Œ105ï¼Œ110ï¼Œ101ï¼Œ115ï¼Œ115] è½¬æ¢ä¸º `"unhappiness"` ï¼ˆä¸å¿«ä¹ï¼‰ã€‚

Multilingual and Cross-Language Models (handling scripts from different languages in one model).

Handling Rare, Special, or Out-of-Vocabulary Tokens (URLs, emojis, code snippets).

å¤šè¯­è¨€å’Œè·¨è¯­è¨€æ¨¡å‹ï¼ˆMultilingual and Cross-Language Models)ï¼š åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸­å¤„ç†æ¥è‡ªä¸åŒè¯­è¨€çš„æ–‡æœ¬ã€‚

å¤„ç†ç¨€æœ‰ã€ç‰¹æ®Šæˆ–è¯æ±‡è¡¨å¤–çš„ Tokenï¼ˆTokenï¼‰ï¼š åº”å¯¹æˆ–è¯†åˆ« URLã€è¡¨æƒ…ç¬¦å·ã€ä»£ç ç‰‡æ®µç­‰åœ¨è¯æ±‡è¡¨ä¸­ä¸å¸¸è§çš„ç‰¹æ®Š Tokenã€‚

Text Compression and Flexibility (fewer vocabulary requirements, but still retaining the ability to reconstruct the original input).

### The Problem

æ–‡æœ¬å‹ç¼©å’Œçµæ´»æ€§ï¼ˆé™ä½è¯æ±‡é‡çš„è¦æ±‚ï¼Œä½†ä»èƒ½å®Œæ•´è¿˜åŸåŸå§‹è¾“å…¥ï¼‰ã€‚

### é—®é¢˜

Standard LLM training pipeline

We can easily see how different tokenization algorithm treats the same piece of text.

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„æ ‡å‡†è®­ç»ƒæµç¨‹æˆ‘ä»¬å¯ä»¥å¾ˆè½»æ˜“åœ°è§‚å¯Ÿåˆ°ï¼Œä¸åŒçš„åˆ†è¯ï¼ˆTokenizationï¼‰ç®—æ³•æ˜¯å¦‚ä½•å¤„ç†åŒä¸€æ–‡æœ¬çš„ã€‚

Patches end when H(xi) exceeds the global threshold eg, shown as a red horizontal line. The start of new patches is shown with vertical gray lines. For example, the entropies of "G" and "e" in "George R.R. Martin" exceed Og, so "G" is the start of a single byte patch and "e" of a larger patch extending to the end of the named entity as the entropy H(xi) stays low, resulting in no additional patches.

å½“ Hï¼ˆxiï¼‰çš„å€¼è¶…è¿‡å…¨å±€é˜ˆå€¼æ—¶ï¼Œæ•°æ®å—ï¼ˆpatchï¼‰å°±ç»“æŸäº†ï¼Œè¿™åœ¨å›¾ä¸­ç”¨çº¢è‰²æ°´å¹³çº¿è¡¨ç¤ºã€‚æ–°çš„æ•°æ®å—èµ·å§‹ä½ç½®ç”¨ç°è‰²å‚ç›´çº¿æ ‡å‡ºã€‚ä¾‹å¦‚ï¼Œåœ¨ã€ŒGeorge R.R. Martinã€è¿™ä¸ªå­—ç¬¦ä¸²ä¸­ï¼Œ"Gã€å’Œã€Œeã€çš„ç†µå€¼éƒ½è¶…è¿‡äº† Og ï¼ˆå…¨å±€ç†µé˜ˆå€¼ï¼‰ï¼Œæ‰€ä»¥ã€ŒGã€æ ‡å¿—ç€ä¸€ä¸ªå•å­—èŠ‚æ•°æ®å—çš„å¼€å§‹ï¼Œè€Œã€Œeã€åˆ™æ ‡å¿—ç€ä¸€ä¸ªæ›´å¤§çš„æ•°æ®å—çš„å¼€å§‹ï¼Œè¿™ä¸ªæ•°æ®å—ä¸€ç›´å»¶ä¼¸åˆ°è¯¥å‘½åå®ä½“çš„æœ«å°¾ã€‚å› ä¸ºç†µå€¼ Hï¼ˆxiï¼‰ä¸€ç›´ä¿æŒåœ¨è¾ƒä½æ°´å¹³ï¼Œæ‰€ä»¥æ²¡æœ‰äº§ç”Ÿé¢å¤–çš„æ•°æ®å—ã€‚

This patching process is important because, in large language models, the way text is divided into chunks can significantly impact both processing efficiency and the model's understanding of the text. Different patching schemes offer different tradeoffs between computational efficiency and maintaining meaningful linguistic units.

è¿™ç§åˆ†å—å¤„ç†è¿‡ç¨‹éå¸¸é‡è¦ï¼Œå› ä¸ºåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ–‡æœ¬åˆ†å—æ–¹å¼ä¼šæ˜¾è‘—å½±å“å¤„ç†æ•ˆç‡å’Œæ¨¡å‹å¯¹æ–‡æœ¬çš„ç†è§£ã€‚ä¸åŒçš„åˆ†å—æ–¹æ¡ˆåœ¨è®¡ç®—æ•ˆç‡å’Œä¿ç•™è¯­ä¹‰å®Œæ•´æ€§ä¹‹é—´è¿›è¡Œäº†ä¸åŒçš„æƒè¡¡ã€‚

Directly training LLMs on Bytes is prohibitively costly at scale due to long sequence length.

### Dynamic Tokenization

ç›´æ¥åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸Šç”¨å­—èŠ‚ï¼ˆBytesï¼‰è¿›è¡Œè®­ç»ƒï¼Œç”±äºåºåˆ—é•¿åº¦è¿‡é•¿ï¼Œä¼šå¯¼è‡´è®­ç»ƒè§„æ¨¡æ‰©å¤§ï¼Œæˆæœ¬é«˜å¾—ä»¤äººéš¾ä»¥æ‰¿å—ã€‚

### åŠ¨æ€ Tokenï¼ˆTokenï¼‰åŒ–

Dynamic tokenization: a way to dynamically decide on token boundaries based on the input text via a subword-merging algorithm inspired by byte-pair encoding. We merge frequent subword sequences in batch, then apply a pre-trained embedding-prediction hypernetwork to compute the token embeddings on the fly.

åŠ¨æ€ Token åˆ’åˆ†ï¼šè¿™æ˜¯ä¸€ç§æ ¹æ®è¾“å…¥æ–‡æœ¬ï¼Œåˆ©ç”¨å—å­—èŠ‚å¯¹ç¼–ç ï¼ˆbyte-pair encodingï¼‰å¯å‘çš„å­è¯åˆå¹¶æ–¹æ³•ï¼ŒåŠ¨æ€ç¡®å®š Token è¾¹ç•Œçš„æŠ€æœ¯ã€‚æˆ‘ä»¬æ‰¹é‡åˆå¹¶é¢‘ç¹å‡ºç°çš„å­è¯åºåˆ—ï¼Œç„¶ååº”ç”¨é¢„è®­ç»ƒçš„åµŒå…¥é¢„æµ‹è¶…ç½‘ç»œï¼ˆembedding-prediction hypernetworkï¼‰æ¥å®æ—¶è®¡ç®— Token åµŒå…¥ã€‚

Just look at how dynamic token reduced how many tokens are used. (https://arxiv.org/pdf/2411.18553)

"Tokens" to refer to byte-groups drawn from a finite vocabulary, determined prior to training.

ä¸å¦¨çœ‹çœ‹åŠ¨æ€ Tokenï¼ˆTokenï¼‰å¦‚ä½•å‡å°‘äº†æ‰€ä½¿ç”¨çš„ Token æ•°é‡ã€‚(https://arxiv.org/pdf/2411.18553)

è¿™é‡Œçš„ã€ŒTokenã€æŒ‡çš„æ˜¯ä»ä¸€ä¸ªé¢„å…ˆç¡®å®šçš„æœ‰é™è¯æ±‡è¡¨ä¸­æå–çš„å­—èŠ‚ç»„åˆï¼Œè¿™ä¸ªè¯æ±‡è¡¨æ˜¯åœ¨æ¨¡å‹è®­ç»ƒä¹‹å‰å°±å®šä¹‰å¥½çš„ã€‚

"Patches" which refer to dynamically grouped sequences without a fixed vocabulary.

For both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer.

ã€Œè¡¥ä¸ã€(Patchesï¼‰æŒ‡çš„æ˜¯åŠ¨æ€ç»„åˆçš„åºåˆ—ï¼Œå®ƒä»¬ä¸ä¾èµ–äºå›ºå®šçš„è¯æ±‡è¡¨ã€‚

æ— è®ºæ˜¯åŸºäº Token çš„æ¨¡å‹è¿˜æ˜¯åŸºäºè¡¥ä¸çš„æ¨¡å‹ï¼Œå¤„ç†æ•°æ®æ‰€éœ€çš„è®¡ç®—é‡ä¸»è¦å–å†³äºä¸» Transformer æ‰§è¡Œçš„æ­¥éª¤æ•°é‡ã€‚

In Byte Latent Transformer (BLT), this is the number of patches needed to encode the data with a given patching function. Consequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function.

åœ¨å­—èŠ‚æ½œåœ¨ Transformerï¼ˆByte Latent Transformerï¼ŒBLTï¼‰ä¸­ï¼Œåˆ†å—çš„æ•°é‡å–å†³äºå¦‚ä½•ä½¿ç”¨ç»™å®šçš„åˆ†å—å‡½æ•°å¯¹æ•°æ®è¿›è¡Œç¼–ç ã€‚å› æ­¤ï¼Œåˆ†å—çš„å¹³å‡å¤§å°ï¼Œä¹Ÿå°±æ˜¯é€šå¸¸æ‰€è¯´çš„å—å¤§å°ï¼Œæ˜¯å†³å®šä½¿ç”¨ç»™å®šçš„åˆ†å—å‡½æ•°åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å¤„ç†æ•°æ®æˆæœ¬çš„ä¸»è¦å› ç´ ã€‚

BLT processes raw text as a continuous stream of bytes, dynamically grouping them into variable-sized "patches" based on the complexity of the data. This dynamic patching allows BLT to allocate computational resources more efficiently, focusing more on complex parts of the text and less on simpler ones. The architecture is divided into three stages:

BLT å°†åŸå§‹æ–‡æœ¬è§†ä¸ºè¿ç»­çš„å­—èŠ‚æµè¿›è¡Œå¤„ç†ï¼Œå¹¶æ ¹æ®æ•°æ®å¤æ‚ç¨‹åº¦ï¼ŒåŠ¨æ€åœ°å°†è¿™äº›å­—èŠ‚æµåˆ’åˆ†ä¸ºå¤§å°ä¸ä¸€çš„ã€ŒåŒºå—ã€(patchesï¼‰ã€‚è¿™ç§åŠ¨æ€åŒºå—åˆ’åˆ†æ–¹æ³•ä½¿å¾— BLT èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—èµ„æºï¼Œå°†æ›´å¤šçš„ç®—åŠ›é›†ä¸­åœ¨æ–‡æœ¬ä¸­è¾ƒä¸ºå¤æ‚çš„éƒ¨åˆ†ï¼Œè€Œå¯¹ç®€å•çš„éƒ¨åˆ†åˆ™æŠ•å…¥è¾ƒå°‘çš„ç®—åŠ›ã€‚æ•´ä¸ªæ¶æ„å¯ä»¥åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

1. Local Encoder

Function: The Local Encoder processes raw byte sequences, converting them into initial patch representations that capture local contextual information.

1. å±€éƒ¨ä¿¡æ¯ç¼–ç å™¨åŠŸèƒ½ï¼šå±€éƒ¨ä¿¡æ¯ç¼–ç å™¨å¤„ç†åŸå§‹å­—èŠ‚æ•°æ®ï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºåˆæ­¥çš„å±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºèƒ½å¤Ÿæå–å±€éƒ¨ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

Operation:

Byte Embeddings: Each byte in the input sequence is transformed into a continuous vector representation.

æ“ä½œï¼š

å­—èŠ‚åµŒå…¥ï¼ˆByte Embeddings)ï¼šè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå­—èŠ‚éƒ½è¢«è½¬æ¢æˆä¸€ä¸ªè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼ˆvector representationï¼‰ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯å°†æ¯ä¸ªå­—èŠ‚éƒ½ç”¨ä¸€ä¸ªæ•°å€¼å‘é‡æ¥è¡¨ç¤ºï¼Œä»¥ä¾¿è®¡ç®—æœºæ›´å¥½åœ°ç†è§£å’Œå¤„ç†ã€‚

Initial Patch Sequence: An initial sequence of patch representations is created.

Cross-Attention Mechanism: The byte embeddings and initial patch sequence interact through cross-attention, enabling the model to dynamically group bytes into variable-sized patches based on the complexity of the data.

åˆå§‹è¡¥ä¸åºåˆ—ï¼šé¦–å…ˆä¼šç”Ÿæˆä¸€ä¸ªåˆå§‹çš„è¡¥ä¸è¡¨ç¤ºåºåˆ—ã€‚

äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šå­—èŠ‚åµŒå…¥å’Œåˆå§‹è¡¥ä¸åºåˆ—é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œäº¤äº’ï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ•°æ®çš„å¤æ‚ç¨‹åº¦ï¼ŒåŠ¨æ€åœ°å°†å­—èŠ‚åˆ†ç»„ä¸ºå¤§å°å¯å˜çš„è¡¥ä¸ã€‚

Purpose: This module ensures that local patterns and dependencies within the byte sequence are effectively captured, facilitating efficient processing in subsequent stages.

ç›®çš„ï¼šè¿™ä¸ªæ¨¡å—æ—¨åœ¨æœ‰æ•ˆåœ°æ•æ‰å­—èŠ‚åºåˆ—ä¸­çš„å±€éƒ¨æ¨¡å¼å’Œä¾èµ–å…³ç³»ï¼Œä»¥ä¾¿åœ¨åç»­é˜¶æ®µä¸­è¿›è¡Œé«˜æ•ˆå¤„ç†ã€‚

2. Latent Transformer

Function: Serving as the core component, the Latent Transformer processes the patch representations to model global context and dependencies across the entire sequence.

2. éšå¼ Transformerï¼ˆLatent Transformer)

åŠŸèƒ½ï¼šä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œéšå¼ Transformer å¤„ç†å›¾åƒå—ï¼ˆpatchï¼‰çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œå¯¹æ•´ä¸ªåºåˆ—çš„å…¨å±€ä¸Šä¸‹æ–‡å’Œä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚

Operation:

Input: Receives the patch representations from the Local Encoder.

æ“ä½œï¼š

è¾“å…¥ï¼šæ¥æ”¶æ¥è‡ªå±€éƒ¨ç‰¹å¾ç¼–ç å™¨çš„å›¾åƒå—ç‰¹å¾ã€‚

Transformer Layers: Employs multiple layers of self-attention and feed-forward networks to capture intricate relationships and dependencies between patches.

Output: Generates refined patch representations that encapsulate both local and global contextual information.

Transformer å±‚ï¼šåˆ©ç”¨å¤šå±‚è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œï¼Œæ¥æ•æ‰å›¾åƒå—ä¹‹é—´å¤æ‚çš„å…³ç³»å’Œä¾èµ–ã€‚

è¾“å‡ºï¼šç”Ÿæˆæ›´å®Œå–„çš„å›¾åƒå—ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åŒ…å«äº†å±€éƒ¨å’Œæ•´ä½“çš„èƒŒæ™¯ä¿¡æ¯ã€‚

Purpose: This module enables the model to understand and generate coherent outputs by effectively integrating information across different parts of the input sequence.

ç›®çš„ï¼šè¿™ä¸ªæ¨¡å—è®©æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ•´åˆè¾“å…¥åºåˆ—ä¸­ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯ï¼Œä»è€Œç†è§£å¹¶ç”Ÿæˆæµç•…çš„è¾“å‡ºã€‚

3. Local Decoder

Function: The Local Decoder translates the processed patch representations back into byte sequences, facilitating accurate text generation.

3. å±€éƒ¨è§£ç å™¨åŠŸèƒ½ï¼šå±€éƒ¨è§£ç å™¨å°†å¤„ç†åçš„å›¾åƒå—çš„ç‰¹å¾è¡¨ç¤ºè½¬æ¢å›å­—èŠ‚åºåˆ—ï¼Œä»è€Œå®ç°å‡†ç¡®çš„æ–‡æœ¬ç”Ÿæˆã€‚

Operation:

Input: Takes the refined patch representations from the Latent Transformer.

æ“ä½œï¼š

è¾“å…¥ï¼šä»éšç©ºé—´ Transformer ä¸­è·å–ä¼˜åŒ–åçš„å›¾åƒå—ç‰¹å¾ã€‚

Residual Connection: Incorporates a residual connection with the output of the Local Encoder, providing direct access to the original byte-level information.

Cross-Attention Mechanism: Utilizes cross-attention between the refined patch representations and the original byte embeddings to reconstruct the byte sequence.

æ®‹å·®è¿æ¥ï¼šé€šè¿‡ä¸å±€éƒ¨ç¼–ç å™¨çš„è¾“å‡ºå»ºç«‹æ®‹å·®è¿æ¥ï¼Œå¯ä»¥ç›´æ¥è·å–åŸå§‹çš„å­—èŠ‚çº§åˆ«ä¿¡æ¯ã€‚

äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼šåˆ©ç”¨ç²¾ç‚¼åçš„å›¾åƒå—ï¼ˆpatchï¼‰è¡¨ç¤ºå’ŒåŸå§‹å­—èŠ‚åµŒå…¥ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›ï¼Œæ¥é‡å»ºå­—èŠ‚åºåˆ—ã€‚

Byte-Level Transformer: A lightweight byte-level transformer is employed to predict the next byte in the sequence, ensuring accurate and coherent text generation.

å­—èŠ‚çº§ Transformerï¼ˆByte-Level Transformer)ï¼šæˆ‘ä»¬ä½¿ç”¨ä¸€ç§è½»é‡çº§çš„å­—èŠ‚çº§ Transformer æ¥é¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—èŠ‚ï¼Œä»è€Œä¿è¯ç”Ÿæˆæ–‡æœ¬çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚

Purpose: This module ensures that the final output maintains fidelity to the original input, allowing for precise and contextually appropriate text generation.

By integrating these three modules, the BLT architecture effectively processes raw byte sequences into meaningful outputs without relying on traditional tokenization methods, enhancing efficiency and scalability in language modeling.

ç›®çš„ï¼šè¿™ä¸ªæ¨¡å—ç¡®ä¿æœ€ç»ˆçš„è¾“å‡ºå¿ å®äºåŸå§‹è¾“å…¥ï¼Œä»è€Œå®ç°ç²¾ç¡®ä¸”ç¬¦åˆä¸Šä¸‹æ–‡çš„æ–‡æœ¬ç”Ÿæˆã€‚

é€šè¿‡æ•´åˆè¿™ä¸‰ä¸ªæ¨¡å—ï¼ŒBLT æ¶æ„èƒ½å¤Ÿé«˜æ•ˆåœ°å°†åŸå§‹å­—èŠ‚åºåˆ—è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„è¾“å‡ºï¼Œå¹¶ä¸”ä¸ä¾èµ–ä¼ ç»Ÿçš„ Token åŒ–ï¼ˆTokenizationï¼‰æ–¹æ³•ï¼Œè¿™æé«˜äº†è¯­è¨€å»ºæ¨¡çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚

Writing articles like this takes considerable effort and time. Please subscribe and follow me if my content adds any value to you.

åˆ›ä½œè¿™æ ·çš„æ–‡ç« éœ€è¦æŠ•å…¥å¤§é‡çš„ç²¾åŠ›å’Œæ—¶é—´ã€‚å¦‚æœæˆ‘çš„å†…å®¹å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è®¢é˜…å¹¶å…³æ³¨æˆ‘ã€‚

Newsletter: https://medium.com/aiguys/newsletter

ğ•: https://x.com/RealAIGuys

## åŸæ–‡

Byte Latent Transformer: Changing How We Train LLMs

2025-01-06

Vishal Rajput

Published in AIGuys

We all know that computers canâ€™t read text, what they read are numbers. All the text is converted into numbers using different strategies and fed to the computers. But what about AI, canâ€™t LLMs read and write a text? No, they read and write tokens. Tokens are a way to represent text, which are the fundamental units of text used to process and generate language. Tokens can represent characters, subwords, words, or even punctuation, depending on how the language modelâ€™s tokenizer works.

However, new work from Metaâ€™s FAIR lab is challenging the well-established paradigm of tokens in LLM space, and it has developed a thing called Patches and Byte Latent Transformer. So, without further ado, letâ€™s go deep into this new paper.

Table of Contents:
Tokens and Tokenization
Tokenization Algorithms
The Problem
Dynamic Tokenization
Tokens and Tokenization
A token is a segment of text that the model processes as a single unit.

For example:

Word-based tokenization: â€œArtificial Intelligenceâ€ â†’ ["Artificial", "Intelligence"]
Subword tokenization: â€œArtificial Intelligenceâ€ â†’ ["Art", "ificial", "Int", "elligence"]
Character-based tokenization: â€œArtificial Intelligenceâ€ â†’ ["A", "r", "t", "i", ...]
Most modern LLMs like GPT, Claude, or any other Transformer model use subword tokenization (e.g., Byte Pair Encoding or SentencePiece) to handle out-of-vocabulary words efficiently.

Example (using GPT-like tokenization):

Input: â€œHello, world!â€

Tokenized Output: [15496, 11, 995]

â€œHelloâ€ â†’ 15496
â€œ,â€ â†’ 11
â€œworldâ€ â†’ 995
Input Representation: To do any processing, the model converts raw text into tokens. These tokens are later mapped to embeddings (numerical representations) for computation.

Output Generation: When generating text, LLMs produce tokens in an autoregressive fashion (one at a time), which are then decoded back into human-readable text.

Tokenization Algorithms
Byte Pair Encoding (BPE)
Breaks words into subwords based on frequency. Suppose the data to be encoded is

aaabdaaabac
The byte pair â€œaaâ€ occurs most often, so it will be replaced by a byte that is not used in the data, such as â€œZâ€. Now there is the following data and replacement table:

ZabdZabac
Z=aa
Then the process is repeated with byte pair â€œabâ€, replacing it with â€œYâ€:

ZYdZYac
Y=ab
Z=aa
The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing â€œZYâ€ with â€œXâ€:

XdXac
X=ZY
Y=ab
Z=aa
This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.

To decompress the data, simply perform the replacements in the reverse order.

So, whatâ€™s the problem with BPE? ğŸ¤” It can have instances where there is more than one way to encode a particular word. It then gets difficult for the algorithm to choose subword tokens as there is no way to prioritize which one to use first. Hence, the same input can be represented by different encodings impacting the accuracy of the learned representations. ğŸ¤¦â€â™€ï¸

WordPiece
Similar to BPE but optimized for maximum likelihood estimation.

The only difference between WordPiece and BPE is the way in which symbol pairs are added to the vocabulary. At each iterative step, WordPiece chooses a symbol pair which will result in the largest increase in likelihood upon merging. Maximizing the likelihood of the training data is equivalent to finding the symbol pair whose probability divided by the probability of the first followed by the probability of the second symbol in the pair is greater than any other symbol pair.

WordPiece starts with a fixed initial vocabulary. This usually includes:

Single characters: ["u", "n", "h", "a", "p", "i", "e", "s"]
A special symbol to indicate subword continuation: ## (e.g., ##ness)
Common words or frequent subwords: [â€œhappyâ€, â€œunâ€, â€œ##nessâ€]
Tokenization Process
The tokenization breaks the word into subwords using the vocabulary.

For "unhappiness":

Start with the whole word: WordPiece tries to match the longest possible token in the vocabulary.

un is in the vocabulary, so itâ€™s selected.
Remaining part: happiness.
Continue with the next part:

The next longest match is happy (from happiness).
Remaining part: ness.
Handle the rest:

ness is in the vocabulary, but because it's a suffix, itâ€™s tokenized as ##ness.
Final Tokens

["un", "happy", "##ness"]

SentencePiece
Works directly with raw text, handling languages with complex scripts (e.g., Japanese, Chinese).

Character-Based Tokenization: SentencePiece works directly on raw text, bypassing language-specific preprocessing (e.g., no need to split the text into words).
Subword Units: It breaks text into subword units (like WordPiece) but doesnâ€™t rely on spaces as word boundaries.
Model-Based Tokenization: Uses algorithms like Byte Pair Encoding (BPE) or Unigram Language Model for tokenization.
Input: â€œI like to learn SentencePiece.â€

Handling spaces â€œ_I_like_to_learn_SentencePiece.â€

SentencePiece builds a vocabulary using subword units based on frequency in the corpus. It might include:

Characters: ["I", "l", "k", "t", "o", "e", "n", "c", "P", "S"]
Common subwords: ["I", "like", "to", "_learn", "Sentence", "Piece"]
Special tokens: ["_", "##"]
Final result: [â€œ_Iâ€, â€œ_likeâ€, â€œ_toâ€, â€œ_learnâ€, â€œ_Sentenceâ€, â€œPieceâ€]

Byte-Level Tokenization
Operates at the byte level for flexibility with various scripts and encodings.

Text to Bytes:

"unhappiness" â†’ [117, 110, 104, 97, 112, 112, 105, 110, 101, 115, 115]

Map Bytes to Tokens:

Byte Mapping: [117 â†’ 32, 110 â†’ 45, 104 â†’ 67, ...] 
Result: [32, 45, 67, 90, 103, 103, 104, 45, 23, 8
Tokens to Bytes to Text:

[32, 45, 67, 90, 103, 103, 104, 45, 23, 88, 88] â†’ [117, 110, 104, 97, 112, 112, 105, 110, 101, 115, 115] â†’ `"unhappiness"`

Multilingual and Cross-Language Models (handling scripts from different languages in one model).
Handling Rare, Special, or Out-of-Vocabulary Tokens (URLs, emojis, code snippets).
Text Compression and Flexibility (fewer vocabulary requirements, but still retaining the ability to reconstruct the original input).
The Problem

Standard LLM training pipeline

Src
We can easily see how different tokenization algorithm treats the same piece of text.


Src
Patches end when H(xi) exceeds the global threshold eg, shown as a red horizontal line. The start of new patches is shown with vertical gray lines. For example, the entropies of â€œGâ€ and â€œeâ€ in â€œGeorge R.R. Martinâ€ exceed Og, so â€œGâ€ is the start of a single byte patch and â€œeâ€ of a larger patch extending to the end of the named entity as the entropy H(xi) stays low, resulting in no additional patches.

This patching process is important because, in large language models, the way text is divided into chunks can significantly impact both processing efficiency and the modelâ€™s understanding of the text. Different patching schemes offer different tradeoffs between computational efficiency and maintaining meaningful linguistic units.

Directly training LLMs on Bytes is prohibitively costly at scale due to long sequence length.


Dynamic Tokenization
Dynamic tokenization: a way to dynamically decide on token boundaries based on the input text via a subword-merging algorithm inspired by byte-pair encoding. We merge frequent subword sequences in batch, then apply a pre-trained embedding-prediction hypernetwork to compute the token embeddings on the fly.


Just look at how dynamic token reduced how many tokens are used. (https://arxiv.org/pdf/2411.18553)
â€œTokensâ€ to refer to byte-groups drawn from a finite vocabulary, determined prior to training.

â€œPatchesâ€ which refer to dynamically grouped sequences without a fixed vocabulary.

For both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer.

In Byte Latent Transformer (BLT), this is the number of patches needed to encode the data with a given patching function. Consequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function.


BLT processes raw text as a continuous stream of bytes, dynamically grouping them into variable-sized â€œpatchesâ€ based on the complexity of the data. This dynamic patching allows BLT to allocate computational resources more efficiently, focusing more on complex parts of the text and less on simpler ones. The architecture is divided into three stages:

1. Local Encoder
Function: The Local Encoder processes raw byte sequences, converting them into initial patch representations that capture local contextual information.

Operation:

Byte Embeddings: Each byte in the input sequence is transformed into a continuous vector representation.
Initial Patch Sequence: An initial sequence of patch representations is created.
Cross-Attention Mechanism: The byte embeddings and initial patch sequence interact through cross-attention, enabling the model to dynamically group bytes into variable-sized patches based on the complexity of the data.
Purpose: This module ensures that local patterns and dependencies within the byte sequence are effectively captured, facilitating efficient processing in subsequent stages.

2. Latent Transformer
Function: Serving as the core component, the Latent Transformer processes the patch representations to model global context and dependencies across the entire sequence.

Operation:

Input: Receives the patch representations from the Local Encoder.
Transformer Layers: Employs multiple layers of self-attention and feed-forward networks to capture intricate relationships and dependencies between patches.
Output: Generates refined patch representations that encapsulate both local and global contextual information.
Purpose: This module enables the model to understand and generate coherent outputs by effectively integrating information across different parts of the input sequence.

3. Local Decoder
Function: The Local Decoder translates the processed patch representations back into byte sequences, facilitating accurate text generation.

Operation:

Input: Takes the refined patch representations from the Latent Transformer.
Residual Connection: Incorporates a residual connection with the output of the Local Encoder, providing direct access to the original byte-level information.
Cross-Attention Mechanism: Utilizes cross-attention between the refined patch representations and the original byte embeddings to reconstruct the byte sequence.
Byte-Level Transformer: A lightweight byte-level transformer is employed to predict the next byte in the sequence, ensuring accurate and coherent text generation.
Purpose: This module ensures that the final output maintains fidelity to the original input, allowing for precise and contextually appropriate text generation.

By integrating these three modules, the BLT architecture effectively processes raw byte sequences into meaningful outputs without relying on traditional tokenization methods, enhancing efficiency and scalability in language modeling.

Writing articles like this takes considerable effort and time. Please subscribe and follow me if my content adds any value to you.

Newsletter: https://medium.com/aiguys/newsletter

ğ•: https://x.com/RealAIGuys