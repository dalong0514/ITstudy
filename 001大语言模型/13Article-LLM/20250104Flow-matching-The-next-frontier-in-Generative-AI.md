## 20250104Flow-matching-The-next-frontier-in-Generative-AI

[Flow matching : The next frontier in Generative AI | by Siddhant Rai | Medium](https://medium.com/@rsiddhant73/flow-matching-the-next-frontier-in-generative-ai-7cf02ebbe859)

Flow matching: The next frontier in Generative AI

Siddhant Rai

Aug 13, 2024

Recently Flux series of models by Flux.ai gained lots of attention from the tech and non-tech community for all the right reasons, model is fast, easy to use thanks to diffusers as well as easy to tune (again thanks to diffusers). But, the reason behind Flux brilliant image-text alignment and high quality generations is a new methodology beyond standard diffusion process, called as "Flow matching". In this post I will cover about methods and advancements that lead to flow matching.

[A Better Way to Build PCBs | Flux](https://www.flux.ai/p)

[Flux â€“ The Future of Electronics Design - YouTube](https://www.youtube.com/watch?v=xPW6KDfCIeo&t=180s)

æœ€è¿‘ï¼ŒFlux.ai çš„ Flux ç³»åˆ—æ¨¡å‹å› å…¶è¯¸å¤šä¼˜ç‚¹ï¼Œåœ¨ç§‘æŠ€ç•Œå’Œéç§‘æŠ€ç•Œéƒ½å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è¯¥æ¨¡å‹é€Ÿåº¦å¿«ã€æ˜“äºä½¿ç”¨ï¼Œå¹¶ä¸”æ˜“äºè°ƒæ•´ï¼Œè¿™éƒ½å¾—ç›Šäº diffusers åº“ã€‚ç„¶è€Œï¼ŒFlux æ¨¡å‹ä¹‹æ‰€ä»¥èƒ½å¤Ÿå®ç°å‡ºè‰²çš„å›¾åƒ â€”â€” æ–‡æœ¬å¯¹é½å’Œé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆï¼Œå…¶èƒŒåçš„å…³é”®æ˜¯ä¸€ç§åä¸ºã€ŒæµåŒ¹é…ã€ï¼ˆFlow matchingï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒè¶…è¶Šäº†æ ‡å‡†çš„æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ·±å…¥æ¢è®¨ä¿ƒæˆæµåŒ¹é…æ–¹æ³•å‡ºç°çš„æŠ€æœ¯åŸç†å’Œå‘å±•å†ç¨‹ã€‚

Every generative model is ideally a density estimaor; hence models a probability density, which eventually is a JPD, with two expected characteristics, sampling and compression, compression is basically pushing data to information space, which is seemingly lower dimensional, whereas sampling is ability to generate P(x|z) starting from any characteristic distribution (z), could be normal distribution (as in case of VAE), hence, on a very high level, we are trying to find a map/function that maps z to x as well as x to z (sampling and compression).

ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç”Ÿæˆæ¨¡å‹éƒ½æ˜¯ä¸€ä¸ªå¯†åº¦ä¼°è®¡å™¨ï¼Œå®ƒæ¨¡æ‹Ÿä¸€ä¸ªæ¦‚ç‡å¯†åº¦ï¼Œæœ€ç»ˆå½¢æˆä¸€ä¸ªè”åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆJPDï¼‰ã€‚ç”Ÿæˆæ¨¡å‹æœ‰ä¸¤ä¸ªä¸»è¦ç‰¹æ€§ï¼šé‡‡æ ·å’Œå‹ç¼©ã€‚å‹ç¼©æœ¬è´¨ä¸Šæ˜¯å°†æ•°æ®æŠ•å°„åˆ°ä¿¡æ¯ç©ºé—´ï¼Œè¿™ä¸ªç©ºé—´é€šå¸¸æ˜¯ä½ç»´çš„ï¼›è€Œé‡‡æ ·åˆ™æ˜¯æŒ‡ä»ä»»ä½•ç‰¹å¾åˆ†å¸ƒï¼ˆzï¼‰å‡ºå‘ï¼Œç”Ÿæˆ Pï¼ˆx|zï¼‰çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ï¼Œ(zï¼‰å¯ä»¥æ˜¯æ­£æ€åˆ†å¸ƒï¼ˆå¦‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸­æ‰€ç¤ºï¼‰ã€‚å› æ­¤ï¼Œä»æœ¬è´¨ä¸Šè®²ï¼Œæˆ‘ä»¬è¯•å›¾æ‰¾åˆ°ä¸€ä¸ªæ˜ å°„æˆ–å‡½æ•°ï¼Œå®ƒæ—¢èƒ½å°† z æ˜ å°„åˆ° xï¼Œä¹Ÿèƒ½å°† x æ˜ å°„åˆ° zï¼ˆåˆ†åˆ«å¯¹åº”é‡‡æ ·å’Œå‹ç¼©ï¼‰ã€‚

Let's assume two PDFs one represented as z (latent or tractable distribution) and other as X (data distribution), as we want to find a function that can map z to x, we are presented with a relationship between density of X and Z, which necessarily points out that given X and Z are conjugate distribution (distribution of same familyz before and after transformation), the change in X and Z should be relativistic, hence, change in X is some function of Z and vice versa. But, scaled by some quantity. This quantity is represented by the change across each dimension between z and x, given by Jacobian matrix, on a very simple scale it's basically change of variable between Z and X. But, it's not that simple, as X and Z are not actually conjugate, hence, we are left with iterative sampling and approximation methods, like optimal transport or Gibbs sampling (used in RBMs). Given these restrictions, most of the methods take a detour to model distributions and approximate an non-exact mapping, whereas methods like Normalizing flow, makes simplifying assumption to make the calculation and formulation tractable of form p(x)dx = p(z)dz, which can be reformulated to get two terms, first a MLE term and second a determinant of Jacobian.

å‡è®¾æœ‰ä¸¤ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼Œä¸€ä¸ªè¡¨ç¤ºä¸º zï¼ˆæ½œåœ¨çš„æˆ–æ˜“äºå¤„ç†çš„åˆ†å¸ƒï¼‰ï¼Œå¦ä¸€ä¸ªè¡¨ç¤ºä¸º Xï¼ˆæ•°æ®åˆ†å¸ƒï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥å°† z æ˜ å°„åˆ° xã€‚è¿™é‡Œå­˜åœ¨ X å’Œ Z çš„æ¦‚ç‡å¯†åº¦ä¹‹é—´çš„å…³ç³»ï¼Œè¿™è¡¨æ˜ï¼Œå¦‚æœ X å’Œ Z æ˜¯å…±è½­åˆ†å¸ƒï¼ˆå˜æ¢å‰åå±äºåŒä¸€åˆ†å¸ƒæ—ï¼‰ï¼Œé‚£ä¹ˆ X å’Œ Z çš„å˜åŒ–åº”è¯¥æ˜¯ç›¸å¯¹çš„ã€‚å› æ­¤ï¼ŒX çš„å˜åŒ–æ˜¯ Z çš„æŸä¸ªå‡½æ•°ï¼Œåä¹‹äº¦ç„¶ï¼Œä½†éœ€è¦ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾é‡ã€‚è¿™ä¸ªç¼©æ”¾é‡ç”± z å’Œ x ä¹‹é—´æ¯ä¸ªç»´åº¦çš„å˜åŒ–å†³å®šï¼Œç”±é›…å¯æ¯”çŸ©é˜µç»™å‡ºã€‚ç®€å•æ¥è¯´ï¼Œå®ƒå°±æ˜¯ Z å’Œ X ä¹‹é—´çš„å˜é‡å˜æ¢ã€‚ç„¶è€Œï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ç®€å•ï¼Œå› ä¸º X å’Œ Z å®é™…ä¸Šå¹¶éå…±è½­åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸é‡‡ç”¨è¿­ä»£é‡‡æ ·å’Œè¿‘ä¼¼æ–¹æ³•ï¼Œä¾‹å¦‚æœ€ä¼˜ä¼ è¾“æˆ–å‰å¸ƒæ–¯é‡‡æ ·ï¼ˆç”¨äº RBMï¼‰ã€‚è€ƒè™‘åˆ°è¿™äº›é™åˆ¶ï¼Œå¤§å¤šæ•°æ–¹æ³•ä¼šè½¬è€Œå¯¹åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è¿‘ä¼¼ä¸€ä¸ªéç²¾ç¡®çš„æ˜ å°„ã€‚è€Œè¯¸å¦‚ Normalizing flowï¼ˆå½’ä¸€åŒ–æµï¼‰çš„æ–¹æ³•ï¼Œåˆ™ä¼šè¿›è¡Œç®€åŒ–å‡è®¾ï¼Œä½¿å¾— pï¼ˆxï¼‰dx = pï¼ˆzï¼‰dz å½¢å¼çš„è®¡ç®—å’Œå…¬å¼å˜å¾—æ˜“äºå¤„ç†ã€‚è¿™ä¸ªç­‰å¼å¯ä»¥è¢«é‡æ–°æ•´ç†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMaximum Likelihood Estimationï¼‰é¡¹ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯é›…å¯æ¯”è¡Œåˆ—å¼ï¼ˆJacobian determinantï¼‰ã€‚

(Normalizing because the change of variables always gives a normalized density function, flow as it models the trajectory/flow from source to target iteratively)

ï¼ˆä¹‹æ‰€ä»¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ˜¯å› ä¸ºå˜é‡çš„å˜æ¢æ€»ä¼šäº§ç”Ÿä¸€ä¸ªå½’ä¸€åŒ–çš„å¯†åº¦å‡½æ•°ã€‚è€Œã€Œæµã€çš„æ¦‚å¿µï¼ŒæŒ‡çš„æ˜¯å®ƒé€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œæ¨¡æ‹Ÿä»æºå¤´åˆ°ç›®æ ‡ç‚¹çš„è½¨è¿¹æˆ–æµåŠ¨è¿‡ç¨‹ï¼‰

Problem start from here, for such a function to exist there are two conditions,

è¦ä½¿è¿™æ ·çš„å‡½æ•°å­˜åœ¨ï¼Œéœ€è¦æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼š

1 MLE formulation for p(x,z) has to be bijective.

å…³äº pï¼ˆx,zï¼‰çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å…¬å¼å¿…é¡»æ˜¯åŒå°„çš„ï¼ˆbijectiveï¼‰ã€‚

2 The determinant of jacobian is efficiently computable.

é›…å¯æ¯”è¡Œåˆ—å¼æ˜¯å¯é«˜æ•ˆè®¡ç®—çš„ã€‚

Two solve this, we need Assumption of dependency of states across z and X, such that it's bijective and determinant is efficiently computable, there are three major approaches (we won't go very deep in this)

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å‡è®¾ z å’Œ X ä¹‹é—´çš„çŠ¶æ€å…·æœ‰ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”è¿™ç§å…³ç³»æ˜¯åŒå°„çš„ï¼ŒåŒæ—¶å…¶è¡Œåˆ—å¼æ˜¯å¯é«˜æ•ˆè®¡ç®—çš„ã€‚è¿™é‡Œæœ‰ä¸‰ç§ä¸»è¦çš„æ–¹æ³• ï¼ˆæˆ‘ä»¬åœ¨æ­¤ä¸åšæ·±å…¥è®¨è®ºï¼‰ã€‚

1 Coupling blocks : basically you break z into two chunks, only last k value predict X last k value (through mean/variance based sampling), other parts of X is basically direct copy of z, how does it help? Because of this method, the Jacobian becomes a diagonal matrix, the top left (<k) part is identity, the bottom right (>=k) becomes element wise product, the top right becomes 0, as there is no dependency between z(<k) and x(>=k), hence, the determinant of Jacobian is efficient to compute.

è€¦åˆå±‚ï¼š åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å°† z åˆ†å‰²æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œåªæœ‰æœ€å k ä¸ªå€¼ç”¨äºé¢„æµ‹ X çš„æœ€å k ä¸ªå€¼ï¼ˆé€šè¿‡åŸºäºå‡å€¼ / æ–¹å·®çš„é‡‡æ ·ï¼‰ï¼ŒX çš„å…¶ä½™éƒ¨åˆ†åˆ™ç›´æ¥å¤åˆ¶è‡ª zã€‚è¿™æ ·åšæœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿç”±äºè¿™ç§æ–¹æ³•ï¼Œé›…å¯æ¯”çŸ©é˜µå˜æˆäº†ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œè¿™æ„å‘³ç€çŸ©é˜µä¸­åªæœ‰å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éé›¶ï¼Œå…¶ä½™å…ƒç´ å‡ä¸ºé›¶ã€‚å…¶ä¸­ï¼Œå·¦ä¸Šè§’ï¼ˆå°äº k çš„éƒ¨åˆ†ï¼‰æ˜¯ä¸€ä¸ªå•ä½çŸ©é˜µï¼Œå³ä¸‹è§’ï¼ˆå¤§äºç­‰äº k çš„éƒ¨åˆ†ï¼‰çš„å…ƒç´ ä¸ºä¹˜ç§¯å½¢å¼ï¼Œè€Œå³ä¸Šè§’çš„å…ƒç´ å‡ä¸º 0ã€‚è¿™æ˜¯å› ä¸º z çš„å‰ k ä¸ªå€¼ä¸ x çš„å k ä¸ªå€¼ä¹‹é—´æ²¡æœ‰ä¾èµ–å…³ç³»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°è®¡ç®—é›…å¯æ¯”çŸ©é˜µçš„è¡Œåˆ—å¼ã€‚

2 AR flows or auto-regressive flows, is the next logical extension, instead of making large k chunks, why not treat each state/feature part of Markov chain, hence killing extra dependencies, this results in a lower triangular matrix for Jacobian which is also straightforward to calculate. But, this method retains more feature and is not susceptible to permutation operation which we do in coupling layers for feature retention.

è‡ªå›å½’æµï¼ˆauto-regressive flowsï¼‰æ˜¯ä¸‹ä¸€ä¸ªé€»è¾‘å»¶ä¼¸ã€‚ä¸å…¶ç”Ÿæˆå¤§çš„ k å—ï¼Œä¸å¦‚å°†é©¬å°”å¯å¤«é“¾ï¼ˆMarkov chainï¼‰çš„æ¯ä¸ªçŠ¶æ€æˆ–ç‰¹å¾éƒ½è§†ä¸ºç‹¬ç«‹çš„ä¸ªä½“ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é¢å¤–çš„ä¾èµ–ã€‚è¿™ä½¿å¾—é›…å¯æ¯”çŸ©é˜µï¼ˆJacobianï¼‰æˆä¸ºä¸‹ä¸‰è§’çŸ©é˜µï¼Œä»è€Œæ˜“äºè®¡ç®—ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä¿ç•™æ›´å¤šçš„ç‰¹å¾ã€‚è€Œä¸”ï¼Œå®ƒä¸åƒè€¦åˆå±‚é‚£æ ·å®¹æ˜“å—åˆ°ä¸ºä¿ç•™ç‰¹å¾è€Œè¿›è¡Œçš„ç½®æ¢æ“ä½œçš„å½±å“ã€‚

3 Finally, Residual flows, where we preserve the entire feature space, without sacrificing compute. Idea is simple yet backed by beautifully intricate maths. The formulation is a residual form x = z + f(z), but, this is is not bijective, because f is a neural network. Interestingly, thanks to banach and his contractive map, in ideal case there exists a unique z* which always maps to same x (stable state z), hence, this becomes bijective as well of form x = z* + f(z*) where f is a contractive map(function bounded by lipschitz less than 1, hence, change in z is bounded by change in X), The form also provides us with a way to represent z(k+1) given previous z(k), which helps in iteratively approximating the X instead of a single shot framework. We can go from z(0) to z(t) by the same formulation and can revert back as well, sounds familiar, this is roughly diffusion. What about determinant, the iterative transformation leads to a sum of infinite terms of trace of Jacobians, this is horrifying given a full rank Jacobian, but could be simply calculated through similar matrix formulation using Hutchinsons method for trace estimation.

æœ€åæ˜¯æ®‹å·®æµï¼ˆResidual flowsï¼‰ï¼Œå®ƒå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…çš„å‰æä¸‹ï¼Œä¿ç•™å®Œæ•´çš„ç‰¹å¾ç©ºé—´ã€‚è¿™ä¸ªæ¦‚å¿µè™½ç„¶ç®€å•ï¼Œä½†å…¶èƒŒåæœ‰ç€ç²¾å·§çš„æ•°å­¦ç†è®ºæ”¯æ’‘ã€‚å…¶åŸºæœ¬å½¢å¼æ˜¯æ®‹å·®å½¢å¼ x = z + fï¼ˆzï¼‰ï¼Œä½†ç”±äº f æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè¿™ä¸ªæ˜ å°„å¹¶éåŒå°„ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæ ¹æ®å·´æ‹¿èµ«çš„å‹ç¼©æ˜ å°„åŸç†ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå­˜åœ¨å”¯ä¸€çš„ z*ï¼Œå®ƒæ€»æ˜¯æ˜ å°„åˆ°ç›¸åŒçš„ xï¼ˆå³ç¨³å®šçŠ¶æ€ zï¼‰ï¼Œå› æ­¤ï¼Œæ˜ å°„å…³ç³»å¯ä»¥è½¬åŒ–ä¸ºåŒå°„å½¢å¼ x = z* + fï¼ˆz*ï¼‰ï¼Œå…¶ä¸­ f æ˜¯ä¸€ä¸ªå‹ç¼©æ˜ å°„ï¼ˆå…¶åˆ©æ™®å¸ŒèŒ¨å¸¸æ•°å°äº 1ï¼Œè¿™æ„å‘³ç€ z çš„å˜åŒ–å¹…åº¦ä¼šå—åˆ° X çš„å˜åŒ–å¹…åº¦é™åˆ¶ï¼‰ã€‚è¿™ç§å½¢å¼è¿˜æä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥æ ¹æ®ä¹‹å‰çš„ zï¼ˆkï¼‰æ¥è¡¨ç¤º zï¼ˆk+1ï¼‰ï¼Œä»è€Œå¸®åŠ©æˆ‘ä»¬è¿­ä»£åœ°é€¼è¿‘ Xï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§å®Œæˆã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ç›¸åŒçš„å…¬å¼ä» zï¼ˆ0ï¼‰æ¨å¯¼åˆ° zï¼ˆtï¼‰ï¼Œä¹Ÿå¯ä»¥åå‘æ¨å¯¼å›å»ã€‚è¿™å¬èµ·æ¥æ˜¯ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Ÿè¿™ä¸æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelï¼‰çš„åŸç†éå¸¸ç›¸ä¼¼ã€‚é‚£ä¹ˆè¡Œåˆ—å¼åˆå¦‚ä½•è®¡ç®—å‘¢ï¼Ÿè¿­ä»£å˜æ¢ä¼šå¯¼è‡´é›…å¯æ¯”çŸ©é˜µï¼ˆJacobianï¼‰çš„è¿¹å‡ºç°æ— é™é¡¹ä¹‹å’Œã€‚è€ƒè™‘åˆ°æ»¡ç§©çš„é›…å¯æ¯”çŸ©é˜µï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªå·¨å¤§çš„éš¾é¢˜ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å“ˆé’¦æ£®æ–¹æ³•ï¼ˆHutchinsons methodï¼‰ï¼Œé€šè¿‡ç±»ä¼¼çš„çŸ©é˜µå…¬å¼æ¥ç®€å•åœ°ä¼°è®¡è¿¹ã€‚

Everything we talked until now, makes a discrete assumption over sample and trajectory state (z=>x), why not make it continuous or basically a continuous residual flow?

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€è®¨è®ºçš„å†…å®¹ï¼Œéƒ½éšå«åœ°å¯¹æ ·æœ¬å’Œè½¨è¿¹çŠ¶æ€ï¼ˆz=>xï¼‰è¿›è¡Œäº†ç¦»æ•£åŒ–çš„å‡è®¾ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆä¸å°†å…¶è§†ä¸ºè¿ç»­çš„ï¼Œæˆ–è€…æ›´å…·ä½“åœ°è¯´ï¼Œä½¿ç”¨è¿ç»­çš„æ®‹å·®æµæ¥å¤„ç†å‘¢ï¼Ÿè¿™é‡Œï¼Œã€Œz=>xã€è¡¨ç¤ºä»æ½œåœ¨ç©ºé—´ z åˆ°æ ·æœ¬ç©ºé—´ x çš„æ˜ å°„ã€‚

The residual form, x(k+1) = x(k) + a.f(x(k)), could be written as a differential where k tends to infinity, does this seems familiar? Yes, this is now a neural ODE, but, we are trying to model a state at t of probability density, which remains constant. The state change over a density is modelled through continuity or transport equation. Specifically, the amount of mass moved from one part of density function to other could be seen as divergence between orginal mass and the current/after-movement mass. This divergence being a continuous function over the entire trajectory has to be integrated and hence solved by a numerical ODE which makes it non scalable.

å½“ k è¶‹äºæ— ç©·å¤§æ—¶ï¼Œæ®‹å·®å½¢å¼ xï¼ˆk+1ï¼‰= xï¼ˆkï¼‰+ a.fï¼ˆxï¼ˆk)ï¼‰å¯ä»¥å†™æˆå¾®åˆ†å½¢å¼ï¼Œè¿™æ˜¯å¦è®©ä½ æ„Ÿåˆ°ç†Ÿæ‚‰ï¼Ÿæ˜¯çš„ï¼Œè¿™å®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆneural ODEï¼‰ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è¯•å›¾å¯¹æ¦‚ç‡å¯†åº¦åœ¨ t æ—¶åˆ»çš„çŠ¶æ€è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”è¯¥çŠ¶æ€ä¿æŒä¸å˜ã€‚å¯†åº¦ä¸Šçš„çŠ¶æ€å˜åŒ–æ˜¯é€šè¿‡è¿ç»­æ€§æ–¹ç¨‹æˆ–è¾“è¿æ–¹ç¨‹æ¥å»ºæ¨¡çš„ã€‚å…·ä½“æ¥è¯´ï¼Œå¯ä»¥æŠŠä»å¯†åº¦å‡½æ•°ä¸€éƒ¨åˆ†ç§»åŠ¨åˆ°å¦ä¸€éƒ¨åˆ†çš„è´¨é‡çœ‹ä½œæ˜¯åŸå§‹è´¨é‡ä¸å½“å‰ï¼ˆæˆ–ç§»åŠ¨åï¼‰è´¨é‡ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ä¸ªå·®å¼‚æ˜¯æ•´ä¸ªè½¨è¿¹ä¸Šçš„ä¸€ä¸ªè¿ç»­å‡½æ•°ï¼Œå¿…é¡»å¯¹å…¶è¿›è¡Œç§¯åˆ†ï¼Œå› æ­¤éœ€è¦é€šè¿‡æ•°å€¼å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¥æ±‚è§£ï¼Œè¿™å¯¼è‡´äº†å…¶ä¸å¯æ‰©å±•æ€§ã€‚

This is like comparison of states, why not compare the path?

è¿™å°±åƒæ˜¯æ¯”è¾ƒä¸åŒçš„çŠ¶æ€ï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥æ¯”è¾ƒå˜åŒ–çš„è·¯å¾„å‘¢ï¼Ÿ

This leads to method behind Flux known as Flow matching. Ideas goes something like thisâ€¦we start from a very simple distribution and move it towards the expected one, but, as we don't know the expected distribution, we condition it through iterative perturbation and estimating the known added noise which eventually models the underlying distribution, the process is simply known as conditional flow matching, theoretically it's proven that in ideal condition the conditional and unconstitutional objectives are exactly same, hence, by optimizing the CFM we tend to optimize the main objective behind flow matching.

è¿™å°±å¼•å‡ºäº† Flux èƒŒåçš„æµåŒ¹é…ï¼ˆFlow matchingï¼‰æ–¹æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šä»ä¸€ä¸ªéå¸¸ç®€å•çš„åˆ†å¸ƒå‡ºå‘ï¼Œé€æ­¥å°†å…¶è½¬åŒ–ä¸ºæˆ‘ä»¬æœŸæœ›çš„åˆ†å¸ƒã€‚ç”±äºæˆ‘ä»¬äº‹å…ˆå¹¶ä¸çŸ¥é“æœŸæœ›çš„åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥éœ€è¦é€šè¿‡è¿­ä»£åœ°æ–½åŠ æ‰°åŠ¨ï¼Œå¹¶ä¼°è®¡å·²çŸ¥çš„å™ªå£°æ¥å¼•å¯¼è¿™ä¸ªè½¬åŒ–è¿‡ç¨‹ï¼Œæœ€ç»ˆå»ºç«‹èµ·å¯¹æ½œåœ¨åˆ†å¸ƒçš„å»ºæ¨¡ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºæ¡ä»¶æµåŒ¹é…ï¼ˆconditional flow matchingï¼‰ã€‚ç†è®ºè¯æ˜ï¼Œåœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¡ä»¶å’Œæ— æ¡ä»¶çš„ç›®æ ‡æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚å› æ­¤ï¼Œé€šè¿‡ä¼˜åŒ–æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨ä¼˜åŒ–æµåŒ¹é…æ–¹æ³•èƒŒåçš„æ ¹æœ¬ç›®æ ‡ã€‚

This is exactly the diffusion process, but, the major difference lies in definition of boundary conditions or initial and final state, the diffusion process assumes pure noise as z(t) and data as z(0), but, this is under assumption when t tends to infinity. But, that is empirically infeasible, instead we perform this for sufficient timestamps, hence, we remain in a much shallower manifold and is slower as well (handled by LCMs later on), it's like finding way/direction in low resolution map, whereas in flow matching, the pure noise and data space is modelled as lerp (linear interpolation) of form x(t) = t*x(t-1) + (1-t)*x(0), hence, at t=0 we are on pure data sample and at t=t we are at pure noise, which gives much refined state/manifold to model, hence, a more representative, high resolution map, also called as conditional flow matching (conditioning by noise distribution).

è¿™å…¶å®å°±æ˜¯æ‰©æ•£è¿‡ç¨‹ï¼Œä½†ä¸»è¦åŒºåˆ«åœ¨äºè¾¹ç•Œæ¡ä»¶ï¼Œæˆ–è€…è¯´åˆå§‹å’Œæœ€ç»ˆçŠ¶æ€çš„å®šä¹‰ã€‚æ‰©æ•£è¿‡ç¨‹å‡è®¾ zï¼ˆtï¼‰ä»£è¡¨çº¯å™ªå£°ï¼Œè€Œ zï¼ˆ0ï¼‰ä»£è¡¨æ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§å‡è®¾æˆç«‹çš„å‰ææ˜¯æ—¶é—´ t è¶‹äºæ— ç©·å¤§ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å¹¶ä¸å¯è¡Œã€‚ç›¸åï¼Œæˆ‘ä»¬ä¼šåœ¨æœ‰é™çš„æ—¶é—´æ­¥é•¿å†…è¿›è¡Œè®¡ç®—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨ä¸€ä¸ªè¾ƒæµ…çš„æµå½¢ä¸­è¿›è¡Œæ“ä½œï¼Œé€Ÿåº¦ä¹Ÿè¾ƒæ…¢ï¼ˆåç»­ä¼šç”± LCM å¤„ç†ï¼‰ã€‚è¿™å°±åƒæ˜¯åœ¨ä½åˆ†è¾¨ç‡åœ°å›¾ä¸­å¯»æ‰¾æ–¹å‘ã€‚è€Œåœ¨æµåŒ¹é…ä¸­ï¼Œçº¯å™ªå£°å’Œæ•°æ®ç©ºé—´è¢«å»ºæ¨¡ä¸ºä¸€ç§çº¿æ€§æ’å€¼ï¼ˆlerpï¼‰çš„å½¢å¼ï¼Œå³ xï¼ˆtï¼‰= t*xï¼ˆt-1ï¼‰+ï¼ˆ1-t)*xï¼ˆ0ï¼‰ã€‚è¿™æ ·ï¼Œå½“ t=0 æ—¶ï¼Œæˆ‘ä»¬å¤„äºçº¯æ•°æ®æ ·æœ¬çš„çŠ¶æ€ï¼›å½“ t ç­‰äº t æ—¶ï¼Œæˆ‘ä»¬å¤„äºçº¯å™ªå£°çš„çŠ¶æ€ã€‚è¿™ç§æ–¹å¼èƒ½æä¾›æ›´ç²¾ç»†çš„çŠ¶æ€æˆ–æµå½¢è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå¾—åˆ°æ›´å…·ä»£è¡¨æ€§çš„é«˜åˆ†è¾¨ç‡ã€Œåœ°å›¾ã€ï¼Œä¹Ÿè¢«ç§°ä¸ºæ¡ä»¶æµåŒ¹é…ï¼ˆé€šè¿‡å™ªå£°åˆ†å¸ƒè¿›è¡Œè°ƒèŠ‚ï¼‰ã€‚

Flow matching, for sure is a unique yet intuitive idea which is going to gain a lot of traction from researchers and the products out of it would be enlightening to the consumer market. My explanation is in no way near the actual depth and breadth of the topic, and hence, a more comprensive and technical deep dive is required, but, I hope the terms/keywords provide a building block and intermediary connections between topics.

æµå¼åŒ¹é…ï¼Œæ— ç–‘æ˜¯ä¸€ä¸ªç‹¬ç‰¹ä¸”ç›´è§‚çš„ç†å¿µï¼Œå®ƒå°†å—åˆ°ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ï¼Œå¹¶ä¸”ç”±æ­¤è¯ç”Ÿçš„äº§å“ä¹Ÿå°†ä¸ºæ¶ˆè´¹å¸‚åœºå¸¦æ¥æ–°çš„å¯ç¤ºã€‚æˆ‘çš„è§£é‡Šè¿œæœªè§¦åŠè¯¥ä¸»é¢˜çš„å®é™…æ·±åº¦å’Œå¹¿åº¦ï¼Œå› æ­¤ï¼Œè¿˜éœ€è¦æ›´å…¨é¢å’Œæ·±å…¥çš„æŠ€æœ¯æ¢è®¨ã€‚ä¸è¿‡ï¼Œæˆ‘å¸Œæœ›è¿™äº›æœ¯è¯­å’Œå…³é”®è¯èƒ½å¤Ÿä¸ºç†è§£ç›¸å…³ä¸»é¢˜æä¾›åŸºç¡€ï¼Œå¹¶å»ºç«‹èµ·å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚

## åŸæ–‡

Recently Flux series of models by Flux.ai gained lots of attention from the tech and non-tech community for all the right reasons, model is fast, easy to use thanks to diffusers as well as easy to tune (again thanks to diffusers). But, the reason behind Flux brilliant image-text alignment and high quality generations is a new methodology beyond standard diffusion process, called as "Flow matching". In this post I will cover about methods and advancements that lead to flow matching.

Every generative model is ideally a density estimaor; hence models a probability density, which eventually is a JPD, with two expected characteristics, sampling and compression, compression is basically pushing data to information space, which is seemingly lower dimensional, whereas sampling is ability to generate P(x|z) starting from any characteristic distribution (z), could be normal distribution (as in case of VAE), hence, on a very high level, we are trying to find a map/function that maps z to x as well as x to z (sampling and compression).

Let's assume two PDFs one represented as z (latent or tractable distribution) and other as X (data distribution), as we want to find a function that can map z to x, we are presented with a relationship between density of X and Z, which necessarily points out that given X and Z are conjugate distribution (distribution of same familyz before and after transformation), the change in X and Z should be relativistic, hence, change in X is some function of Z and vice versa. But, scaled by some quantity. This quantity is represented by the change across each dimension between z and x, given by Jacobian matrix, on a very simple scale it's basically change of variable between Z and X. But, it's not that simple, as X and Z are not actually conjugate, hence, we are left with iterative sampling and approximation methods, like optimal transport or Gibbs sampling (used in RBMs). Given these restrictions, most of the methods take a detour to model distributions and approximate an non-exact mapping, whereas methods like Normalizing flow, makes simplifying assumption to make the calculation and formulation tractable of form p(x)dx = p(z)dz, which can be reformulated to get two terms, first a MLE term and second a determinant of Jacobian.

(Normalizing because the change of variables always gives a normalized density function, flow as it models the trajectory/flow from source to target iteratively)

Problem start from here, for such a function to exist there are two conditions,

1. MLE formulation for p(x,z) has to be bijective.

2. The determinant of jacobian is efficiently computable.

Two solve this, we need Assumption of dependency of states across z and X, such that it's bijective and determinant is efficiently computable, there are three major approaches (we won't go very deep in this)

1. Coupling blocks : basically you break z into two chunks, only last k value predict X last k value (through mean/variance based sampling), other parts of X is basically direct copy of z, how does it help? Because of this method, the Jacobian becomes a diagonal matrix, the top left (<k) part is identity, the bottom right (>=k) becomes element wise product, the top right becomes 0, as there is no dependency between z(<k) and x(>=k), hence, the determinant of Jacobian is efficient to compute.

2. AR flows or auto-regressive flows, is the next logical extension, instead of making large k chunks, why not treat each state/feature part of Markov chain, hence killing extra dependencies, this results in a lower triangular matrix for Jacobian which is also straightforward to calculate. But, this method retains more feature and is not susceptible to permutation operation which we do in coupling layers for feature retention.

3. Finally, Residual flows, where we preserve the entire feature space, without sacrificing compute. Idea is simple yet backed by beautifully intricate maths. The formulation is a residual form x = z + f(z), but, this is is not bijective, because f is a neural network. Interestingly, thanks to banach and his contractive map, in ideal case there exists a unique z* which always maps to same x (stable state z), hence, this becomes bijective as well of form x = z* + f(z*) where f is a contractive map(function bounded by lipschitz less than 1, hence, change in z is bounded by change in X), The form also provides us with a way to represent z(k+1) given previous z(k), which helps in iteratively approximating the X instead of a single shot framework. We can go from z(0) to z(t) by the same formulation and can revert back as well, sounds familiar, this is roughly diffusion. What about determinant, the iterative transformation leads to a sum of infinite terms of trace of Jacobians, this is horrifying given a full rank Jacobian, but could be simply calculated through similar matrix formulation using Hutchinsons method for trace estimation.

Everything we talked until nowğŸ˜ƒ, makes a discrete assumption over sample and trajectory state (z=>x), why not make it continuous or basically a continuous residual flow?

The residual form, x(k+1) = x(k) + a.f(x(k)), could be written as a differential where k tends to infinity, does this seems familiar? Yes, this is now a neural ODE, but, we are trying to model a state at t of probability density, which remains constant. The state change over a density is modelled through continuity or transport equation. Specifically, the amount of mass moved from one part of density function to other could be seen as divergence between orginal mass and the current/after-movement mass. This divergence being a continuous function over the entire trajectory has to be integrated and hence solved by a numerical ODE which makes it non scalable.

This is like comparison of states, why not compare the path?

This leads to method behind Flux known as Flow matching. Ideas goes something like thisâ€¦we start from a very simple distribution and move it towards the expected one, but, as we don't know the expected distribution, we condition it through iterative perturbation and estimating the known added noise which eventually models the underlying distribution, the process is simply known as conditional flow matching, theoretically it's proven that in ideal condition the conditional and unconstitutional objectives are exactly same, hence, by optimizing the CFM we tend to optimize the main objective behind flow matching.

This is exactly the diffusion process, but, the major difference lies in definition of boundary conditions or initial and final state, the diffusion process assumes pure noise as z(t) and data as z(0), but, this is under assumption when t tends to infinity. But, that is empirically infeasible, instead we perform this for sufficient timestamps, hence, we remain in a much shallower manifold and is slower as well (handled by LCMs later on), it's like finding way/direction in low resolution map, whereas in flow matching, the pure noise and data space is modelled as lerp (linear interpolation) of form x(t) = t*x(t-1) + (1-t)*x(0), hence, at t=0 we are on pure data sample and at t=t we are at pure noise, which gives much refined state/manifold to model, hence, a more representative, high resolution map, also called as conditional flow matching (conditioning by noise distribution).

Flow matching, for sure is a unique yet intuitive idea which is going to gain a lot of traction from researchers and the products out of it would be enlightening to the consumer market. My explanation is in no way near the actual depth and breadth of the topic, and hence, a more comprensive and technical deep dive is required, but, I hope the terms/keywords provide a building block and intermediary connections between topics.

You can find the paper here : https://arxiv.org/abs/2210.02747

Helpful paper (residual flow) : https://arxiv.org/abs/1906.02735

Special blogpost : https://lilianweng.github.io/posts/2018-10-13-flow-models/

Helpful blogpost : https://jmtomczak.github.io/blog/18/18_fm.html