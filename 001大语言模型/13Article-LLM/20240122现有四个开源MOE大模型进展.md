## 20240122现有四个开源MOE大模型进展

[现有四个开源 MOE 大模型进展：从 Mixtral-8x7B 到 LLaMA MOE 再到 DeepSeek-MoE](https://mp.weixin.qq.com/s/-UF-zxUqEsuNhJyPkSpMgA?v_p=90&WBAPIAnalysisOriUICodes=10000001_10000002&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10DC293010)

原创刘焕勇老刘说 NLP 2024-01-19 15:15 发表于北京

我们来看看 MOE 的一些事情。

通过增加参数和计算预算来扩大语言模型规模可以得到更强大的模型。然而，与之相关的问题是极高的计算成本。为了解决这个问题，不少工作使用了 Mixture-of-Experts（MoE）架构，该架构可以在保持计算成本适度的情况下实现参数扩展。

而从 Mixtral 8x7B 推出以来，目前已经陆续出现了许多「跟风」之作，本文主要介绍了这不到一个月以来出现的四个 moe 现有模型，供大家参考。

### 01. Mixtral-8x7B

Mixtral 8x7B 是一个稀疏专家混合模型 (SMoE)，开放权重，在模型结构上是一个仅解码器的模型，其中前馈块从一组 8 个不同的参数组中进行选择。在每一层，对于每个标记，一个路由器网络选择这些组中的两个 (专家) 来处理该标记并相加它们的输出。

可处理 32k 个标记的上下文，可处理英语、法语、意大利语、德语和西班牙语；在代码生成方面表现不错；

地址：https://mistral.ai/news/mixtral-of-experts/

### 02. Chinese-Mixtral-8x7B

中文扩词表增量预训练混合专家模型 Chinese-Mixtral-8x7B，其提出动机在于：Mixtral-8x7B 词表不支持中文，因此对中文的编解码效率较低，限制了中文场景下的实用性。因此，基于 Mixtral-8x7B 进行了中文扩词表增量预训练，项目地址在：

https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B

1、在词表扩充上，使用 sentencepiece 在 12G 知乎数据和 2G 悟道数据上训练中文 BPE 词表；

2、在训练策略上，使用 QLoRA 对模型进行训练；

3、在训练数据上：

中文使用 Skywork/SkyPile-150B (https://huggingface.co/datasets/Skywork/SkyPile-150B)，总量 30B，仅使用 2022 + 2023 年的数据；

英文使用 DKYoon/SlimPajama-6B (https://huggingface.co/datasets/DKYoon/SlimPajama-6B)，总量 12B，数据集重复 2 个 Epoch。

地址：

https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B

### 03. LLaMA-MoE

LLaMA-MoE《LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training》是基于 LLaMA 系列和 SlimPajama 的 MoE 模型，实现很简单：将 LLaMA 的 FFNs 划分为稀疏专家，并为每层专家插入 top-K 个 gate，使用来自 Sheared LLaMA 的优化数据采样权重和来自 SlimPajama 的过滤数据集持续预训练初始化的 MoE 模型。

其具体实现架构如下：

在开源模型上：

一个有趣的问题：

Q: 问 moe 模型的构建是通过多个 llama 模型还是 1 个 llama 模型呢？

A: LLaMA-MoE 基于一个完整的 llama 模型进行切分。

Q: 这个 repo 的用途是将 1 个 llama 模型的 FFN 层通过不同的切分方法，切分为多个 FFN 来扮演多个专家。然后将 llama 模型的其余模型层和权重与切分后的 FFN 和 gate 进行拼接变成 moe 模型嘛？

A: 是的。我们只切分了 llama 的 FFN 层，之后再加一个 gate 进行 token 路由。

Q: 是否支持将多个 llama 结构模型的 FFN 层合并，基于一个 base 的 llama 模型结构构建 Moe 呢？

A: 目前不支持。不过这个想法基于同构模型，我想是比较容易实现的

地址：

https://github.com/pjlab-sys4nlp/llama-moe

### 04. DeepSeek-MoE

但现有的 MoE 架构可能存在知识混杂（Knowledge Hybridity）和知识冗余（Knowledge Redundancy）的问题，限制了专家的专业化。

在实现思想上，DeepSeek-MoE 采用两个主要策略：

一个是 Fine-Grained Expert Segmentation —— 细粒度的专家分割，通过细化 FFN 中间隐藏维度，维持参数数量不变的同时激活更多细粒度的专家，使得激活的专家更加灵活和适应性更强；

另一个是 Shared Expert Isolation —— 共享专家隔离，将某些专家隔离为共享专家，始终激活，旨在捕捉和巩固不同上下文中的共同知识。

在验证实验上，从 2B 参数的规模开始验证了 DeepSeekMoE 架构的可行性，将模型参数扩展到 16B，仅使用大约 40% 的计算资源就能达到与 DeepSeek 7B 和 LLaMA2 7B 相媲美的性能。将 DeepSeekMoE 扩展到 145B，其与 DeepSeek 67B 相媲美的性能，仅使用 28.5%（可能甚至是 18.2%）的计算资源。

在训练数据上，该模型是在 2T 中英文 token 上从头开始训练。

报告地址：https://arxiv.org/pdf/2401.06066.pdf

地址：

[deepseek-ai/DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE)

### 05. 总结

本文主要介绍了 moe 的四个现有模型，后后面有进展的，我们进一步跟踪，供大家一起参考。

参考文献：

1、https://www.zhihu.com/question/639062017/answer/3360042351

2、https://github.com/deepseek-ai/DeepSeek-MoE

3、https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B

4、https://mistral.ai/news/mixtral-of-experts/

关于我们：

老刘，刘焕勇，NLP 开源爱好者与践行者，主页：https://liuhuanyong.github.io。

老刘说 NLP，将定期发布语言资源、工程实践、技术总结等内容，欢迎关注。

对于想加入更优质的知识图谱、事件图谱、大模型 AIGC 实践、相关分享的，可关注公众号，在后台菜单栏中点击会员社区 -> 会员入群加入。