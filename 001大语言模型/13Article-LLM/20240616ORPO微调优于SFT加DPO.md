## 20240616ORPOå¾®è°ƒä¼˜äºSFTåŠ DPO

[ORPO Outperforms SFT+DPO | Train Phi-2 with ORPO | by Zain ul Abideen | Medium](https://medium.com/@zaiinn440/orpo-outperforms-sft-dpo-train-phi-2-with-orpo-3ee6bf18dbf2)

ORPO Outperforms SFT+DPO | Train Phi-2 with ORPO

Train Phi-2 with ORPO with LazyOrpo

Zain ul Abideen

Mar 22, 2024

ORPO ä¼˜äº SFT+DPO | ä½¿ç”¨ ORPO è®­ç»ƒ Phi-2

ä½¿ç”¨ LazyOrpo è®­ç»ƒ Phi-2

### Introduction

Before jumping into ORPO, I am going to assume that you are well-acquainted with the process of fine-tuning LLMs for optimal performance. One of the most common technique used for fine-tuning is the Supervised Fine-Tuning (SFT). The most common way for doing SFT is to load the model in 4-bit and apply the config to the model for Lora training. Then we use TRL's SFTTrainer to fine-tune models. That's one way of reaching an optimal LLM. Another technique that has been here for some time now is the DPO (Direct Preference Optimization). For DPO, the dataset should be in a specific format i.e. it should contain a chosen response and a rejected response along with the instruction. DPO has shown great results in aligning the model while requiring less compute for the training process. To further improve the model's performance, recently people have adopted to SFT followed by DPO on the same model. This combination of SFT+DPO has proved to be quite effective but at the same time requires more compute resources.

What if I tell you, there is another better fine-tuning technique that can replace both SFT+DPO and have shown promising results. I am referring to ORPO (Odds Ratio Preference Optimization). The main highlight is its loss function. It incorporates an odds ratio-based penalty to the conventional negative log-likelihood (NLL) loss for differentiating the generation styles between favored and disfavored responses.

å¼•è¨€

åœ¨ä»‹ç» ORPO ä¹‹å‰ï¼Œå‡è®¾ä½ å·²ç»ç†Ÿæ‚‰äº†å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥è·å–æœ€ä½³æ€§èƒ½çš„æµç¨‹ã€‚æœ€å¸¸è§çš„å¾®è°ƒæŠ€æœ¯ä¹‹ä¸€æ˜¯ç›‘ç£å¾®è°ƒï¼ˆSFT)ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šå°†æ¨¡å‹åŠ è½½åˆ° 4 ä½ï¼Œå¹¶åº”ç”¨ Lora è®­ç»ƒçš„é…ç½®ï¼Œç„¶åä½¿ç”¨ TRL çš„ SFTTrainer æ¥å¾®è°ƒæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ç§è¾¾åˆ°æœ€ä½³ LLM çš„æ–¹æ³•ã€‚å¦ä¸€ç§å·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´çš„æŠ€æœ¯æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPO)ã€‚å¯¹äº DPOï¼Œæ•°æ®é›†å¿…é¡»åŒ…å«ä¸€ä¸ªé€‰æ‹©çš„å“åº”å’Œä¸€ä¸ªè¢«æ‹’ç»çš„å“åº”ä»¥åŠæŒ‡ä»¤ã€‚DPO å·²ç»åœ¨å¯¹é½æ¨¡å‹æ–¹é¢å±•ç¤ºäº†å‡ºè‰²çš„æ•ˆæœï¼ŒåŒæ—¶éœ€è¦è¾ƒå°‘çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œæœ€è¿‘äººä»¬å¼€å§‹åœ¨åŒä¸€ä¸ªæ¨¡å‹ä¸Šå…ˆè¿›è¡Œ SFTï¼Œç„¶åå†è¿›è¡Œ DPOã€‚è¿™ç§ SFT+DPO çš„ç»„åˆè¢«è¯æ˜éå¸¸æœ‰æ•ˆï¼Œä½†ä¹Ÿéœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚

å¦‚æœæˆ‘å‘Šè¯‰ä½ ï¼Œæœ‰ä¸€ç§æ–°æ–¹æ³•å¯ä»¥å–ä»£ SFT+DPOï¼Œå¹¶ä¸”å·²ç»æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ•ˆæœå‘¢ï¼Ÿè¿™ç§æ–¹æ³•æ˜¯èµ”ç‡æ¯”åå¥½ä¼˜åŒ–ï¼ˆORPO)ã€‚å®ƒçš„ä¸»è¦äº®ç‚¹åœ¨äºå…¶æŸå¤±å‡½æ•°ï¼Œå®ƒç»“åˆäº†åŸºäºèµ”ç‡æ¯”çš„æƒ©ç½šä¸ä¼ ç»Ÿçš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰æŸå¤±ï¼Œä»¥åŒºåˆ†åå¥½å’Œéåå¥½å“åº”çš„ç”Ÿæˆé£æ ¼ã€‚

### Can ORPO redefine how we train and align LLMs for RLHF?

State-of-the-art LLMs followed the process of Base Model â†’ Supervised Fine-tuning â†’ RLHF (PPO/DPO). This is very resource-intensive and complex. Odds Ratio Preference Optimization (ORPO) proposes a new method to train LLMs by combining SFT and Alignment into a new objective (loss function), achieving state of the art results. DPO not only reduces the cost of the training but also outperforms the results from first fine-tuning the model and then doing RLHF (DPO) on the fine-tuned version. ORPO does not require a reference model, unlike RLHF and DPO. In that sense, ORPO is computationally more efficient than RLHF and DPO in two perspectives:

Memory allocation

Fewer FLOPs per batch.

So, in my opinion the answer to the above question is most probably a "Yes". It can certainly influence the way how we train our models in the future or may have an impact on future research work regarding fine-tuning LLMs.

ORPO èƒ½å¦é‡æ–°å®šä¹‰æˆ‘ä»¬å¦‚ä½•ä¸º RLHF è®­ç»ƒå’Œå¯¹é½ LLMsï¼Ÿ

ç›®å‰æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ–¹æ³•é€šå¸¸éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼šåŸºç¡€æ¨¡å‹ â†’ ç›‘ç£å¾®è°ƒ â†’ RLHFï¼ˆPPO/DPO)ã€‚è¿™ç§æ–¹æ³•éå¸¸è€—è´¹èµ„æºä¸”å¤æ‚ã€‚Odds Ratio Preference Optimizationï¼ˆORPOï¼‰æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¯¹é½ï¼ˆAlignmentï¼‰ç»“åˆåˆ°ä¸€ä¸ªæ–°çš„ç›®æ ‡ï¼ˆæŸå¤±å‡½æ•°ï¼‰ä¸­ï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDPO ä¸ä»…é™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œè¿˜ä¼˜äºå…ˆå¾®è°ƒæ¨¡å‹ç„¶ååœ¨å¾®è°ƒç‰ˆæœ¬ä¸Šè¿›è¡Œ RLHFï¼ˆDPOï¼‰çš„ç»“æœã€‚è€Œ ORPO ä¸éœ€è¦å‚è€ƒæ¨¡å‹ï¼Œè¿™ä½¿å¾—å®ƒåœ¨ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢æ¯” RLHF å’Œ DPO æ›´é«˜æ•ˆï¼š

- å†…å­˜åˆ†é…
- æ¯æ‰¹æ¬¡çš„æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼ˆFLOPsï¼‰æ›´å°‘å› æ­¤ï¼Œæˆ‘è®¤ä¸ºä¸Šè¿°é—®é¢˜çš„ç­”æ¡ˆå¾ˆå¯èƒ½æ˜¯ã€Œæ˜¯ã€ã€‚ORPO ç¡®å®æœ‰å¯èƒ½å½±å“æˆ‘ä»¬æœªæ¥çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæˆ–å¯¹æœªæ¥å…³äºå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒçš„ç ”ç©¶äº§ç”Ÿå½±å“ã€‚

### ORPO details

ğŸ† ORPO Outperforms SFT, SFT+DPO on PHI-2, Llama 2, and Mistral

ğŸ“Š Mistral ORPO achieves 12.20% on AlpacaEval2.0, 66.19% on IFEval, and 7.32 on MT-Bench Zephyr Beta

Results from the ORPO paper are impressive and to test verify the results of this paper, I decided to try it out on Phi-2 with Argilla's dpo-mix-7k dataset. Some results from the paper are shown below.

AlpacaEvals from ORPO paper

The reason for choosing Phi-2 is that because it shows an insane amount of improvement on this technique as compare to SFT+DPO.

ORPO è¯¦æƒ…

ğŸ† ORPO åœ¨ PHI-2ã€Llama 2 å’Œ Mistral ä¸Šçš„è¡¨ç°ä¼˜äº SFT å’Œ SFT+DPO

ğŸ“Š æ ¹æ® ORPO è®ºæ–‡ï¼ŒMistral ORPO åœ¨ AlpacaEval2.0 ä¸Šè¾¾åˆ°äº† 12.20%ï¼Œåœ¨ IFEval ä¸Šè¾¾åˆ°äº† 66.19%ï¼Œåœ¨ MT-Bench Zephyr Beta ä¸Šè¾¾åˆ°äº† 7.32

ORPO è®ºæ–‡çš„ç»“æœéå¸¸ä»¤äººå°è±¡æ·±åˆ»ã€‚ä¸ºäº†éªŒè¯è¿™äº›ç»“æœï¼Œæˆ‘å†³å®šåœ¨ Phi-2 ä¸Šä½¿ç”¨ Argilla çš„ dpo-mix-7k æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚ä»¥ä¸‹æ˜¯è®ºæ–‡ä¸­çš„ä¸€äº›ç»“æœã€‚

å›¾ï¼šæ¥è‡ª ORPO è®ºæ–‡çš„ AlpacaEvals

é€‰æ‹© Phi-2 çš„åŸå› æ˜¯å®ƒåœ¨è¿™é¡¹æŠ€æœ¯ä¸Šæ¯” SFT+DPO æ˜¾ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚

### Training process

1ï¸âƒ£ For implementing ORPO, we will require a dataset that is in DPO format i.e. it should have a chosen and rejected responses. Fot this experiment, we will opt for Argilla's dpo-mix-7k preference dataset.

2ï¸âƒ£ Make sure the dataset doesn't contain instances where the chosen and rejected responses are the same, or one is empty.

3ï¸âƒ£ Select a pre-trained LLM (e.g., Llama-2, Mistral). In this case, I have selected Phi-2 as the base model.

4ï¸âƒ£ Train the Base model with ORPO objective on preference dataset

There is no extra SFT step that is directly applied to base model. The model was trained for 1 epoch on 1x A40 for almost 6 hours. The training parameters used are below:

Learning Rate: 5e-6

Warmup Steps: 100

Model Name: microsoft/phi-2

Data Name: argilla/dpo-mix-7k

Number of Training Epochs: 1

Maximum Length of Prompt: 128

Maximum Length of Response: 2048

Per Device Training Batch Size: 4

Per Device Evaluation Batch Size: 4

Gradient Accumulation Steps: 1

Number of Processes: 1

The training process has also been logged to Weights and Biases. Some of the graphs are shown below:

è®­ç»ƒè¿‡ç¨‹

1ã€è¦å®ç° ORPOï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª DPO æ ¼å¼çš„æ•°æ®é›†ï¼Œä¹Ÿå°±æ˜¯åŒ…å«é€‰æ‹©å’Œæ‹’ç»å“åº”çš„æ•°æ®é›†ã€‚å¯¹äºè¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬é€‰æ‹© Argilla çš„ dpo-mix-7k åå¥½æ•°æ®é›†ã€‚

[argilla/dpo-mix-7k Â· Datasets at Hugging Face](https://huggingface.co/datasets/argilla/dpo-mix-7k?row=0)

2ã€ç¡®ä¿æ•°æ®é›†ä¸­æ²¡æœ‰é€‰å®šå’Œæ‹’ç»å“åº”ç›¸åŒæˆ–ä¸ºç©ºçš„å®ä¾‹ã€‚

3ã€é€‰æ‹©ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ Llama-2, Mistral)ã€‚åœ¨è¿™ä¸ªå®éªŒä¸­ï¼Œæˆ‘é€‰æ‹©äº† Phi-2 ä½œä¸ºåŸºç¡€æ¨¡å‹ã€‚

4ã€ç”¨åå¥½æ•°æ®é›†ä¸Šçš„ ORPO ç›®æ ‡è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚

åŸºç¡€æ¨¡å‹æ²¡æœ‰ç›´æ¥è¿›è¡Œé¢å¤–çš„ SFT æ­¥éª¤ã€‚è¯¥æ¨¡å‹åœ¨ä¸€å— A40 GPU ä¸Šè®­ç»ƒäº† 1 ä¸ª epochï¼Œè€—æ—¶å°†è¿‘ 6 ä¸ªå°æ—¶ã€‚è®­ç»ƒå‚æ•°å¦‚ä¸‹ï¼š

- å­¦ä¹ ç‡ï¼š5e-6
- é¢„çƒ­æ­¥éª¤ï¼š100
- æ¨¡å‹åç§°ï¼šmicrosoft/phi-2
- æ•°æ®åç§°ï¼šargilla/dpo-mix-7k
- è®­ç»ƒå‘¨æœŸæ•°ï¼š1
- æç¤ºçš„æœ€å¤§é•¿åº¦ï¼š128
- å“åº”çš„æœ€å¤§é•¿åº¦ï¼š2048
- æ¯è®¾å¤‡è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼š4
- æ¯è®¾å¤‡è¯„ä¼°æ‰¹æ¬¡å¤§å°ï¼š4
- æ¢¯åº¦ç´¯ç§¯æ­¥éª¤ï¼š1
- è¿›ç¨‹æ•°ï¼š1

è®­ç»ƒè¿‡ç¨‹è®°å½•åœ¨ Weights and Biases ä¸Šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å›¾è¡¨ï¼š

### LazyORPO

LazyORPO (Automated tool to train your model with ORPO). ORPO is a new technique that replaces SFT+DPO. I gave ORPO a shot with Phi-2 and Argilla dpo-mix-7k yielding Phi2-pro.

Since Odds Ratio Preference Optimization (ORPO) proposes a new method to train LLMs by combining SFT and Alignment into a new objective (loss function), achieving state-of-the-art results, Orpo is not yet included in HF's TRL, so in order to make the training phase much easier, I have made a notebook that automates the entire training process with ORPO. Just mention the base model, dataset, epochs, and learning rate to shoot the training. One thing to notice is that ORPO required more memory VRAM as I was not able to fit an 8B Gemma model on A40 48GB VRAM. So, do your calculations accordingly.

A colab notebook is available for you to try it out. You can access the GPUs using RunPod.

LazyORPO

LazyORPO æ˜¯ä¸€ä¸ªç”¨ ORPO è‡ªåŠ¨è®­ç»ƒæ¨¡å‹çš„å·¥å…·ã€‚ORPO æ˜¯ä¸€ç§æ–°æŠ€æœ¯ï¼Œæ›¿ä»£äº† SFT+DPOã€‚æˆ‘å°è¯•ç”¨ ORPO ç»“åˆ Phi-2 å’Œ Argilla dpo-mix-7kï¼Œå¾—åˆ°äº† Phi2-proã€‚

ç”±äº Odds Ratio Preference Optimizationï¼ˆORPOï¼‰æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡å°† SFT å’Œ Alignment ç»“åˆæˆä¸€ä¸ªæ–°çš„ç›®æ ‡ï¼ˆæŸå¤±å‡½æ•°ï¼‰æ¥è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM)ï¼Œå¹¶å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚ç›®å‰ ORPO è¿˜æ²¡æœ‰åŒ…å«åœ¨ Hugging Face çš„ TRL ä¸­ã€‚ä¸ºäº†ç®€åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘åˆ¶ä½œäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è®­ç»ƒçš„ç¬”è®°æœ¬ï¼Œåªéœ€æŒ‡å®šåŸºç¡€æ¨¡å‹ã€æ•°æ®é›†ã€è®­ç»ƒå‘¨æœŸå’Œå­¦ä¹ ç‡å³å¯å¼€å§‹è®­ç»ƒã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒORPO éœ€è¦æ›´å¤šçš„æ˜¾å­˜ VRAMï¼Œå› ä¸ºæˆ‘æ— æ³•åœ¨ A40 48GB VRAM ä¸Šé€‚é… 8B Gemma æ¨¡å‹ã€‚å› æ­¤ï¼Œè¯·æ ¹æ®éœ€è¦è¿›è¡Œè®¡ç®—ã€‚

ä½ å¯ä»¥åœ¨ colab ç¬”è®°æœ¬ä¸Šå°è¯•è¿™ä¸ªæ–¹æ³•ï¼Œå¹¶é€šè¿‡ RunPod è®¿é—® GPUã€‚

1ã€

ç›®å‰å®˜æ–¹è®­ç»ƒå·¥å…· AutoTrain å·²ç»æ”¯æŒ ORPO è®­ç»ƒäº†ã€‚ï¼ˆ2024-06-16ï¼‰

[AutoTrain Configs](https://huggingface.co/docs/autotrain/config)

[How to Finetune phi-3 on MacBook Pro](https://huggingface.co/blog/abhishek/phi3-finetune-macbook)

ã€

LazyORPO

ğŸ’¥ LazyORPO: https://colab.research.google.com/drive/19ci5XIcJDxDVPY2xC1ftZ5z1kc2ah_rx?usp=sharing

ğŸ¤— Model: https://huggingface.co/abideen/phi2-pro

ğŸ“ Paper: https://arxiv.org/abs/2403.07691

[[2403.07691] ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)
