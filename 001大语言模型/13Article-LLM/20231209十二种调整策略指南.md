## 20231209十二种调整策略指南

[A Guide on 12 Tuning Strategies for Production-Ready RAG Applications | by Leonie Monigatti | Dec, 2023 | Towards Data Science](https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439)

[12 种调整策略指南：为生产环境打造高效的 RAG 应用 [译] | 宝玉的分享](https://baoyu.io/translations/rag/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications)

A Guide on 12 Tuning Strategies for Production-Ready RAG Applications

How to improve the performance of your Retrieval-Augmented Generation (RAG) pipeline with these "hyperparameters" and tuning strategies

Leonie Monigatti

Fig: Tuning Strategies for Retrieval-Augmented Generation Applications

Data Science is an experimental science. It starts with the「No Free Lunch Theorem,」which states that there is no one-size-fits-all algorithm that works best for every problem. And it results in data scientists using experiment tracking systems to help them tune the hyperparameters of their Machine Learning (ML) projects to achieve the best performance.

This article looks at a Retrieval-Augmented Generation (RAG) pipeline through the eyes of a data scientist. It discusses potential "hyperparameters" you can experiment with to improve your RAG pipeline's performance. Similar to experimentation in Deep Learning, where, e.g., data augmentation techniques are not a hyperparameter but a knob you can tune and experiment with, this article will also cover different strategies you can apply, which are not per se hyperparameters.

[Intro to MLOps: Experiment Tracking for Machine Learning | by Leonie Monigatti | Medium](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)

[Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation | by Leonie Monigatti | Nov, 2023 | Towards Data Science](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)

12 种调整策略指南：为生产环境打造高效的 RAG 应用 [译]

如何通过这些「超参数」和调整策略优化你的检索增强生成（RAG）流程

Leonie Monigatti

Published on December 7, 2023

针对检索增强生成应用的调整策略

数据科学本质上是一门实验性科学。从「没有免费午餐定理」出发，这个定理阐述了没有万能的算法能够解决所有问题。因此，数据科学家们经常利用实验跟踪系统来调整他们机器学习 (ML) 项目的超参数，以追求最优的性能表现。

本文以数据科学家的视角深入探讨了检索增强生成 (RAG) 流程。文中将讨论一系列可能的「超参数」，通过这些参数的调整，可以有效提升 RAG 流程的表现。类似于深度学习中的实验手段，例如数据增强技术，并不直接是超参数，而是一个可调整和实验的关键因素，本文也会介绍可以应用的不同策略，这些策略本身并不是直接的超参数。

This article covers the following "hyperparameters" sorted by their relevant stage. In the ingestion stage of a RAG pipeline, you can achieve performance improvements by:

Data cleaning

Chunking

Embedding models

Metadata

Multi-indexing

Indexing algorithms

And in the inferencing stage (retrieval and generation), you can tune:

Query transformations

Retrieval parameters

Advanced retrieval strategies

Re-ranking models

LLMs

Prompt engineering

Note that this article covers text-use cases of RAG. For multimodal RAG applications, different considerations may apply.

文章围绕以下按其应用阶段分类的「超参数」展开。在 RAG 流程的数据录入阶段，你可以通过以下方法提升性能：

数据清洗

文档分块

嵌入模型

元数据

多重索引

索引算法

在推理阶段（包括检索和生成），则可以调整：

查询转换

检索参数

高级检索策略

重排序模型

大语言模型 (LLMs)

提示工程

请注意，本文主要介绍 RAG 在文本应用方面的内容。至于多模态 RAG 应用，可能涉及不同的考量。

### 01. Ingestion Stage

The ingestion stage is a preparation step for building a RAG pipeline, similar to the data cleaning and preprocessing steps in an ML pipeline. Usually, the ingestion stage consists of the following steps:

1. Collect data

2. Chunk data

3. Generate vector embeddings of chunks

4. Store vector embeddings and chunks in a vector database

Documents are first chunked, then the chunks are embedded, and the embeddings are stored in the vector database

Ingestion stage of a RAG pipeline

This section discusses impactful techniques and hyperparameters that you can apply and tune to improve the relevance of the retrieved contexts in the inferencing stage.

数据录入阶段

在构建 RAG 系统时，数据录入阶段是一个关键的准备工序，其过程类似于机器学习中的数据清理和预处理环节。一般来说，数据录入阶段包括以下几个步骤：

1、收集数据。

2、数据分割。

3、为数据分割生成向量嵌入。

4、将向量嵌入及数据分割存入向量数据库。

首先对文档进行分割，然后对分割后的部分进行向量嵌入，并将这些嵌入保存在向量数据库中。

图：RAG 系统的数据录入阶段

这一部分主要讨论了一些有着显著影响的技术和超参数，通过对它们的应用和调整，可以在后续的推理阶段提高检索到的内容的相关性。

Data cleaning

Like any Data Science pipeline, the quality of your data heavily impacts the outcome in your RAG pipeline [8, 9]. Before moving on to any of the following steps, ensure that your data meets the following criteria:

1. Clean: Apply at least some basic data cleaning techniques commonly used in Natural Language Processing, such as making sure all special characters are encoded correctly.

2. Correct: Make sure your information is consistent and factually accurate to avoid conflicting information confusing your LLM.

数据清理

和所有数据科学项目一样，数据的质量对 RAG 系统的成果有着直接影响 [8, 9]。在进行后续步骤之前，请确保您的数据满足以下标准：

1、清洁：至少进行基本的数据清理，这在自然语言处理中非常常见，比如确保所有特殊字符都被正确编码。

2、准确：确保您的信息是连贯且事实正确的，以免出现相互矛盾的信息，从而混淆大型语言模型。

Chunking

Chunking your documents is an essential preparation step for your external knowledge source in a RAG pipeline that can impact the performance [1, 8, 9]. It is a technique to generate logically coherent snippets of information, usually by breaking up long documents into smaller sections (but it can also combine smaller snippets into coherent paragraphs).

One consideration you need to make is the choice of the chunking technique. For example, in LangChain, different text splitters split up documents by different logics, such as by characters, tokens, etc. This depends on the type of data you have. For example, you will need to use different chunking techniques if your input data is code vs. if it is a Markdown file.

The ideal length of your chunk (chunk_size) depends on your use case: If your use case is question answering, you may need shorter specific chunks, but if your use case is summarization, you may need longer chunks. Additionally, if a chunk is too short, it might not contain enough context. On the other hand, if a chunk is too long, it might contain too much irrelevant information.

Additionally, you will need to think about a "rolling window" between chunks (overlap) to introduce some additional context.

[Document transformers | 🦜️🔗 Langchain](https://python.langchain.com/docs/modules/data_connection/document_transformers/)

文档分块技术

在构建 RAG（检索式生成网络）管道时，将文档进行恰当的分块是关键准备步骤之一，这直接影响系统的性能 [1, 8, 9]。文档分块是一种将长文档拆分成小段或将小段信息组合成连贯段落的技术，旨在生成逻辑上连贯、信息丰富的文档片段。

重要的一点是要选择适当的分块技术。例如，在 LangChain 中，不同的文本分割器使用不同的逻辑来切分文档，如基于字符、Token 等。这种选择取决于您手头的数据类型。比如，处理代码类数据和处理 Markdown 文件时，您需要采用不同的分块策略。

理想的分块长度（chunk_size） 也因应用场景而异：如果是问答系统，可能需要较短的精确信息块；如果是内容摘要，可能需要较长的信息块。同时，块的大小也需恰到好处：太短可能缺乏足够上下文，太长则可能含有过多无关信息。

此外，还需要在不同块之间考虑设置「滚动窗口」（overlap），以便增加额外的上下文信息。

Embedding models

Embedding models are at the core of your retrieval. The quality of your embeddings heavily impacts your retrieval results [1, 4]. Usually, the higher the dimensionality of the generated embeddings, the higher the precision of your embeddings.

For an idea of what alternative embedding models are available, you can look at the Massive Text Embedding Benchmark (MTEB) Leaderboard, which covers 164 text embedding models (at the time of this writing).

MTEB Leaderboard - a Hugging Face Space by mteb

Discover amazing ML apps made by the community

huggingface.co

While you can use general-purpose embedding models out-of-the-box, it may make sense to fine-tune your embedding model to your specific use case in some cases to avoid out-of-domain issues later on [9]. According to experiments conducted by LlamaIndex, fine-tuning your embedding model can lead to a 5–10% performance increase in retrieval evaluation metrics [2].

Note that you cannot fine-tune all embedding models (e.g., OpenAI's text-ebmedding-ada-002 can't be fine-tuned at the moment).

[MTEB Leaderboard - a Hugging Face Space by mteb](https://huggingface.co/spaces/mteb/leaderboard)

[finetune-embedding/evaluate.ipynb at main · run-llama/finetune-embedding](https://github.com/run-llama/finetune-embedding/blob/main/evaluate.ipynb)

[Fine-tuning - OpenAI API](https://platform.openai.com/docs/guides/fine-tuning)

嵌入模型简介

在你的信息检索系统中，嵌入模型发挥着关键作用。嵌入的品质极大地影响了检索的效果 [1, 4]。一般来说，嵌入向量的维度越高，其精准度也越高。

想了解更多可替换的嵌入模型？可以参考 Massive Text Embedding Benchmark (MTEB) 排行榜，这里汇集了目前（截至本文写作时）164 种文本嵌入模型的比较。

MTEB 排行榜 - 由 mteb 创建的 Hugging Face Space

尽管通用嵌入模型可以直接使用，但在一些特定场景下，对嵌入模型进行微调更有利于适应特定的使用环境，避免未来出现领域外问题 [9]。LlamaIndex 的实验显示，微调嵌入模型可以在检索效果上提升大约 5–10% [2]。

需要注意的是，并不是所有嵌入模型都支持微调。例如，OpenAI 提供的 text-ebmedding-ada-002 目前还不支持微调。

Metadata

When you store vector embeddings in a vector database, some vector databases let you store them together with metadata (or data that is not vectorized). Annotating vector embeddings with metadata can be helpful for additional post-processing of the search results, such as metadata filtering [1, 3, 8, 9]. For example, you could add metadata, such as the date, chapter, or subchapter reference.

[Explaining Vector Databases in 3 Levels of Difficulty | by Leonie Monigatti | Towards Data Science](https://towardsdatascience.com/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78)

元数据

当你在向量数据库中存储向量嵌入时，有些数据库支持你将向量与元数据（或非向量化的数据）一同存储。给向量嵌入添加元数据标注可以在后续的搜索结果处理中发挥重要作用，如进行元数据筛选 [1, 3, 8, 9]。比如，你可以加入诸如日期、章节或小节的引用等额外信息。

Multi-indexing

If the metadata is not sufficient enough to provide additional information to separate different types of context logically, you may want to experiment with multiple indexes [1, 9]. For example, you can use different indexes for different types of documents. Note that you will have to incorporate some index routing at retrieval time [1, 9]. If you are interested in a deeper dive into metadata and separate collections, you might want to learn more about the concept of native multi-tenancy.

[Solving Multi-Tenancy In Vector Search Requires A Paradigm Shift, Etienne Dilocker, CTO, Weviate - YouTube](https://www.youtube.com/watch?v=KT2RFMTJKGs)

多重索引技术

在元数据无法充分区分不同上下文类型的情况下，您可以考虑尝试多重索引技术 [1, 9]。比如，针对不同文档类型采用不同的索引策略。但请注意，这样做需要在数据检索时加入索引路由机制 [1, 9]。如果您对如何利用元数据和分离集合有更深的兴趣，建议您深入了解原生多租户这一概念。

Indexing algorithms

To enable lightning-fast similarity search at scale, vector databases and vector indexing libraries use an Approximate Nearest Neighbor (ANN) search instead of a k-nearest neighbor (kNN) search. As the name suggests, ANN algorithms approximate the nearest neighbors and thus can be less precise than a kNN algorithm.

There are different ANN algorithms you could experiment with, such as Facebook Faiss (clustering), Spotify Annoy (trees), Google ScaNN (vector compression), and HNSWLIB (proximity graphs). Also, many of these ANN algorithms have some parameters you could tune, such as ef, efConstruction, and maxConnections for HNSW [1].

Additionally, you can enable vector compression for these indexing algorithms. Analogous to ANN algorithms, you will lose some precision with vector compression. However, depending on the choice of the vector compression algorithm and its tuning, you can optimize this as well.

However, in practice, these parameters are already tuned by research teams of vector databases and vector indexing libraries during benchmarking experiments and not by developers of RAG systems. However, if you want to experiment with these parameters to squeeze out the last bits of performance, I recommend this article as a starting point:

[An Overview on RAG Evaluation | Weaviate - Vector Database](https://weaviate.io/blog/rag-evaluation?source=post_page-----7ca646833439--------------------------------#indexing-knobs)

Learn about new trends in RAG evaluation and the current state of the art.

[facebookresearch/faiss: A library for efficient similarity search and clustering of dense vectors.](https://github.com/facebookresearch/faiss)

[spotify/annoy: Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk](https://github.com/spotify/annoy)

[google-research/scann at master · google-research/google-research](https://github.com/google-research/google-research/tree/master/scann)

[nmslib/hnswlib: Header-only C++/python library for fast approximate nearest neighbors](https://github.com/nmslib/hnswlib)

索引算法

为了实现在大数据量下快速且高效的相似性搜索，向量数据库和索引库通常采用近似最近邻 (ANN) 搜索方法，而不是传统的 k-最近邻 (kNN) 搜索。ANN 算法通过近似计算来定位最近邻，因此可能在精确度上稍逊于 kNN 算法。

您可以考虑尝试多种 ANN 算法，例如 Facebook Faiss 的聚类算法、Spotify Annoy 的树状结构算法、Google ScaNN 的向量压缩技术，以及 HNSWLIB 的邻近图算法。这些算法中许多都提供了可调整的参数，如 HNSW 的 ef、efConstruction 和 maxConnections [1]。

另外，您还可以为这些索引算法启用向量压缩技术。虽然向量压缩可能会牺牲一定的精度，但通过合理选择压缩算法并调整参数，您可以在保证效率的同时最大限度地减小精度损失。

不过，在实际应用中，这些参数通常由专门的研究团队在进行基准测试时调整，而非由 RAG 系统的开发人员设置。如果您想通过调整这些参数来提高系统性能，我推荐您阅读这篇文章作为入门：

关于 RAG 评估的概述 | Weaviate - 向量数据库

### 02. Inferencing Stage (Retrieval & Generation)

The main components of the RAG pipeline are the retrieval and the generative components. This section mainly discusses strategies to improve the retrieval (Query transformations, retrieval parameters, advanced retrieval strategies, and re-ranking models) as this is the more impactful component of the two. But it also briefly touches on some strategies to improve the generation (LLM and prompt engineering).

Fig: Inference stage of a RAG pipeline - Standard RAG schema

推理阶段（检索与生成）

RAG 管道的核心是检索和生成两大部分。这一节主要介绍提升检索效果的策略（如查询转换、检索参数、高级检索策略以及重排序模型），因为相比之下，检索部分对整体影响更大。同时，也会简要介绍一些提升生成部分效果的方法（比如使用大语言模型 (LLM) 和提示工程 (prompt engineering)）。

图：RAG 管道的推理阶段图解 —— 标准 RAG 架构

Query transformations

Since the search query to retrieve additional context in a RAG pipeline is also embedded into the vector space, its phrasing can also impact the search results. Thus, if your search query doesn't result in satisfactory search results, you can experiment with various query transformation techniques [5, 8, 9], such as:

1. Rephrasing: Use an LLM to rephrase the query and try again.

2. Hypothetical Document Embeddings (HyDE): Use an LLM to generate a hypothetical response to the search query and use both for retrieval.

3. Sub-queries: Break down longer queries into multiple shorter queries.

查询转换

在 RAG 管道中，用于检索附加信息的搜索查询也会被嵌入到向量空间里，因此查询的措辞会直接影响搜索结果。所以，如果你发现搜索结果不尽人意，可以尝试以下几种查询转换方法 [5, 8, 9]，以提升检索效率：

1、重新措辞：用大语言模型 (LLM) 改写你的查询语句，然后再试一次。

2、假设性文档嵌入（HyDE）：使用大语言模型 (LLM) 生成一个针对查询的假设性回答，并结合使用以进行检索。

3、子查询：将复杂的长查询分解成几个简短的小查询。

Retrieval parameters

The retrieval is an essential component of the RAG pipeline. The first consideration is whether semantic search will be sufficient for your use case or if you want to experiment with hybrid search.

In the latter case, you need to experiment with weighting the aggregation of sparse and dense retrieval methods in hybrid search [1, 4, 9]. Thus, tuning the parameter alpha, which controls the weighting between semantic (alpha = 1) and keyword-based search (alpha = 0), will become necessary.

[Improving Retrieval Performance in RAG Pipelines with Hybrid Search | by Leonie Monigatti | Nov, 2023 | Towards Data Science](https://towardsdatascience.com/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5)

How to find more relevant search results by combining traditional keyword-based search with modern vector search

Also, the number of search results to retrieve will play an essential role. The number of retrieved contexts will impact the length of the used context window (see Prompt Engineering). Also, if you are using a re-ranking model, you need to consider how many contexts to input to the model (see Re-ranking models).

Note, while the used similarity measure for semantic search is a parameter you can change, you should not experiment with it but instead set it according to the used embedding model (e.g., text-embedding-ada-002 supports cosine similarity or multi-qa-MiniLM-l6-cos-v1 supports cosine similarity, dot product, and Euclidean distance).

检索参数

检索过程是 RAG（检索增强生成）流程的核心环节。首先，你需要决定仅使用语义搜索是否足够，或者你是否想尝试更复杂的混合搜索。

在选择混合搜索时，你需要研究如何在稀疏和密集检索方法之间进行有效的权重分配 [1, 4, 9]。这就涉及到调整 alpha 参数，该参数负责平衡基于语义的搜索（alpha = 1）和基于关键词的搜索（alpha = 0）的重要性。

另一个关键因素是检索结果的数量。检索到的上下文数量会影响所用上下文窗口的长度（参见提示工程）。此外，如果你使用的是重排模型，你还需要考虑向模型输入多少个上下文（参见重排序模型）。

请注意，虽然你可以更改用于语义搜索的相似度度量参数，但最好不要随意变动，而是根据所使用的嵌入模型来设定。例如，text-embedding-ada-002 支持余弦相似度，而 multi-qa-MiniLM-l6-cos-v1 支持余弦相似度、点积和欧几里得距离。

Advanced retrieval strategies

This section could technically be its own article. For this overview, we will keep this as concise as possible. For an in-depth explanation of the following techniques, I recommend this DeepLearning.AI course:

[Building and Evaluating Advanced RAG Applications - DeepLearning.AI](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----7ca646833439--------------------------------)

Learn methods like sentence-window retrieval and auto-merging retrieval, improving your RAG pipeline's performance…

The underlying idea of this section is that the chunks for retrieval shouldn't necessarily be the same chunks used for the generation. Ideally, you would embed smaller chunks for retrieval (see Chunking) but retrieve bigger contexts. [7]

Sentence-window retrieval: Do not just retrieve the relevant sentence, but the window of appropriate sentences before and after the retrieved one.

Auto-merging retrieval: The documents are organized in a tree-like structure. At query time, separate but related, smaller chunks can be consolidated into a larger context.

高级检索策略

本节内容足够编写成一篇独立文章。为了保持简洁，我们仅提供一个概览。想要深入了解下面提到的技术，我建议你参考 DeepLearning.AI 提供的这个课程：

构建和评估高级 RAG 应用

这一节的核心理念是：用于检索的数据块不必是用于生成内容的同一数据块。理想情况下，应该嵌入更小的数据块来进行检索（参见文档分块），但同时检索更广泛的上下文。[7]

句子窗口检索：在检索时，不只是找到相关的单个句子，而是要获取该句子前后的相关句子。

自动合并检索：文档按树状结构组织。在查询时，可以把若干个小的、相关的数据块合并成一个更大的上下文。

Re-ranking models

While semantic search retrieves context based on its semantic similarity to the search query, "most similar" doesn't necessarily mean "most relevant". Re-ranking models, such as Cohere's Rerank model, can help eliminate irrelevant search results by computing a score for the relevance of the query for each retrieved context [1, 9].

"most similar" doesn't necessarily mean "most relevant"

If you are using a re-ranker model, you may need to re-tune the number of search results for the input of the re-ranker and how many of the reranked results you want to feed into the LLM.

As with the embedding models, you may want to experiment with fine-tuning the re-ranker to your specific use case.

[Rerank - Optimize Your Search With One Line of Code | Cohere](https://cohere.com/rerank?ref=txt.cohere.com&__hstc=14363112.8fc20f6b1a1ad8c0f80dcfed3741d271.1697800567394.1701091033915.1701173515537.7&__hssc=14363112.1.1701173515537&__hsfp=3638092843)

重排序模型

虽然语义搜索是根据搜索查询与上下文的语义相似度来进行的，但「最相似」并不总等同于「最相关」。像 Cohere 的 Rerank 这样的重排序模型能够通过为每个检索结果计算与查询相关性的得分，帮助排除不相关的搜索结果 [1, 9]。

「最相似」不总意味着「最相关」

使用重排序模型时，你可能需要重新调整搜索结果的数量，以及你想要输入到大语言模型 (Large Language Model) 的经过重排序的结果数量。

和嵌入模型一样，你可能需要尝试针对你的特定应用场景对重排序模型进行微调。

LLMs

The LLM is the core component for generating the response. Similarly to the embedding models, there is a wide range of LLMs you can choose from depending on your requirements, such as open vs. proprietary models, inferencing costs, context length, etc. [1]

As with the embedding models or re-ranking models, you may want to experiment with fine-tuning the LLM to your specific use case to incorporate specific wording or tone of voice.

大语言模型 (LLM)

在生成回应的过程中，大语言模型 (LLM) 扮演着核心角色。正如嵌入模型那样，LLM 的选择取决于您的具体需求，比如选择开放源代码还是专有模型、考虑推理成本、上下文长度等因素。您可以从众多 LLM 中挑选最适合您的 [1]。

与处理嵌入模型或重新排序模型时相似，您可能需要针对特定场景对 LLM 进行微调，以便更好地融入特定的词汇或语调。

Prompt engineering

How you phrase or engineer your prompt will significantly impact the LLM's completion [1, 8, 9].

Please base your answer only on the search results and nothing else!

Very important! Your answer MUST be grounded in the search results provided.

Please explain why your answer is grounded in the search results!

Additionally, using few-shot examples in your prompt can improve the quality of the completions.

As mentioned in Retrieval parameters, the number of contexts fed into the prompt is a parameter you should experiment with [1]. While the performance of your RAG pipeline can improve with increasing relevant context, you can also run into a "Lost in the Middle" [6] effect where relevant context is not recognized as such by the LLM if it is placed in the middle of many contexts.

提示工程 (Prompt engineering)

如何巧妙地设计您的提示词或问题，将直接影响到 LLM 的回答效果 [1, 8, 9]。一个好的提示词设计能让 LLM 提供更加准确和高质量的回答。

请确保您的回答仅基于搜索结果，不要添加任何其他信息！

这一点非常关键！您的回答必须严格基于提供的搜索结果。

并请说明您的回答为何与搜索结果密切相关！

在提示词中加入少样本 (few-shot) 示例，可以有效提升 LLM 生成内容的质量。

正如在检索参数一节中提到的，探索在提示词中加入的上下文数量也是值得尝试的。虽然增加相关上下文有助于提高您的 RAG (检索增强生成) 系统的性能，但同时也可能产生「迷失在中间」[6] 的现象，即 LLM 无法有效识别被放置在众多上下文中间的相关信息。

### 03. Summary

As more and more developers gain experience with prototyping RAG pipelines, it becomes more important to discuss strategies to bring RAG pipelines to production-ready performances. This article discussed different "hyperparameters" and other knobs you can tune in a RAG pipeline according to the relevant stages:

This article covered the following strategies in the ingestion stage:

Data cleaning: Ensure data is clean and correct.

Chunking: Choice of chunking technique, chunk size (chunk_size) and chunk overlap (overlap).

Embedding models: Choice of the embedding model, incl. dimensionality, and whether to fine-tune it.

Metadata: Whether to use metadata and choice of metadata.

Multi-indexing: Decide whether to use multiple indexes for different data collections.

Indexing algorithms: Choice and tuning of ANN and vector compression algorithms can be tuned but are usually not tuned by practitioners.

And the following strategies in the inferencing stage (retrieval and generation):

Query transformations: Experiment with rephrasing, HyDE, or sub-queries.

Retrieval parameters: Choice of search technique (alpha if you have hybrid search enabled) and the number of retrieved search results.

Advanced retrieval strategies: Whether to use advanced retrieval strategies, such as sentence-window or auto-merging retrieval.

Re-ranking models: Whether to use a re-ranking model, choice of re-ranking model, number of search results to input into the re-ranking model, and whether to fine-tune the re-ranking model.

LLMs: Choice of LLM and whether to fine-tune it.

Prompt engineering: Experiment with different phrasing and few-shot examples.

Enjoyed This Story?

Subscribe for free to get notified when I publish a new story.

Get an email whenever Leonie Monigatti publishes.

Get an email whenever Leonie Monigatti publishes. By signing up, you will create a Medium account if you don't already…

medium.com

Find me on LinkedIn, Twitter, and Kaggle!

随着越来越多的开发人员积极探索 RAG (检索增强生成) 管道的原型设计，如何将 RAG 管道提升至适用于生产环境的性能水平变得愈发重要。本文探讨了在 RAG 管道不同阶段可以调整的各种「超参数」和其他可调节项：

文章在数据录入阶段中介绍了以下策略：

1、数据清洗：确保数据的清洁和准确性。

2、分块：选择合适的分块技术、块大小（chunk_size）和块之间的重叠（overlap）。

3、嵌入模型：选择嵌入模型，包括模型的维度以及是否需要进行微调。

4、元数据：决定是否使用元数据以及选择哪些元数据。

5、多重索引：决定是否为不同的数据集合使用多个索引。

6、索引算法：选择和调整近似最近邻 (ANN) 和向量压缩算法，虽然这通常不是实践者所调整的。

以及在推理阶段（检索和生成）中介绍的策略：

1、查询转换：尝试使用改写、HyDE 或子查询等方式。

2、检索参数：选择搜索技术（如果启用混合搜索，则为 alpha）和检索的搜索结果数量。

4、高级检索策略：决定是否使用高级检索策略，如句子窗口或自动合并检索。

5、重新排序模型：是否使用重新排序模型，选择哪种重新排序模型，确定输入到该模型中的搜索结果数量，以及是否对模型进行微调。

6、大语言模型 (LLM)：选择适合的大语言模型并决定是否进行微调。

7、提示工程：尝试使用不同的措辞和少样本示例。

在 LinkedIn、Twitter 和 Kaggle 上找到我！

### References

Literature

[1] Connor Shorten and Erika Cardenas (2023). Weaviate Blog. An Overview on RAG Evaluation (accessed Nov. 27, 2023)

[An Overview on RAG Evaluation | Weaviate - Vector Database](https://weaviate.io/blog/rag-evaluation)

[2] Jerry Liu (2023). LlamaIndex Blog. Fine-Tuning Embeddings for RAG with Synthetic Data (accessed Nov. 28, 2023)

[Fine-Tuning Embeddings for RAG with Synthetic Data | by Jerry Liu | LlamaIndex Blog](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)

[3] LlamaIndex Documentation (2023). Building Performant RAG Applications for Production (accessed Nov. 28, 2023)

[Building Performant RAG Applications for Production - LlamaIndex 🦙 0.9.13](https://docs.llamaindex.ai/en/stable/optimizing/production_rag.html)

[4] Voyage AI (2023). Embeddings Drive the Quality of RAG: A Case Study of Chat.LangChain (accessed Dec. 5, 2023)

[Embeddings Drive the Quality of RAG: A Case Study of Chat.LangChain  – Voyage AI](https://blog.voyageai.com/2023/10/29/a-case-study-of-chat-langchain/)

[5] LlamaIndex Documentation (2023). Query Transformations (accessed Nov. 28, 2023)

[6] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.

[7] DeepLearning.AI (2023). Building and Evaluating Advanced RAG Applications (accessed Dec 4, 2023)

[Building and Evaluating Advanced RAG Applications - DeepLearning.AI](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)

[8] Ahmed Besbes (2023). Towards Data Science. Why Your RAG Is Not Reliable in a Production Environment (accessed Nov. 27, 2023)

[Why Your RAG Is Not Reliable in a Production Environment | by Ahmed Besbes | Oct, 2023 | Towards Data Science](https://towardsdatascience.com/why-your-rag-is-not-reliable-in-a-production-environment-9e6a73b3eddb)

[9] Matt Ambrogi (2023). Towards Data Science. 10 Ways to Improve the Performance of Retrieval Augmented Generation Systems (accessed Nov. 27, 2023)

[10 Ways to Improve the Performance of Retrieval Augmented Generation Systems | by Matt Ambrogi | Towards Data Science](https://towardsdatascience.com/10-ways-to-improve-the-performance-of-retrieval-augmented-generation-systems-5fa2cee7cd5c)