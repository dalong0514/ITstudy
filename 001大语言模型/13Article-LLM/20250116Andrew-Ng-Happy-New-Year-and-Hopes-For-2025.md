## 20250116Andrew-Ng-Happy-New-Year-and-Hopes-For-2025

[Happy New Year! Hopes For 2025 With Mustafa Suleyman, Audrey Tang, Albert Gu, Hanno Basse, and more...](https://www.deeplearning.ai/the-batch/issue-282/)

### Andrew Ng

Dear friends,

亲爱的朋友们，

Happy sum(i**3 for i in range(10)) !

祝大家 1³ + 2³ + ... + 9³ 这么开心！

Despite having worked on AI since I was a teenager, I'm now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!

虽然我从青少年时期就开始接触人工智能（AI），但现在我比以往任何时候都更兴奋于 AI 的应用前景，尤其是在构建 AI 应用程序方面。这个领域正在蓬勃发展，我相信 2025 年将是 AI 应用大放异彩的一年！

One aspect of AI that I'm particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.

AI 中我特别兴奋的一个方面是它能如此容易地构建软件原型。AI 正在降低软件开发的成本，并拓展其应用场景。虽然 AI 可以帮助扩展或维护大型软件系统，但它在快速构建原型和其他简单应用方面尤其突出。

If you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1's help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI's finance team), or analyzes user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding.

如果你想开发一个应用，比如为孩子打印学习卡片（我最近在 o1 的帮助下只用了几个小时就完成了），或者开发一个监控外汇汇率以管理国际银行账户的应用（DeepLearning.AI 金融团队的真实案例），又或者自动分析用户评论以快速发现产品问题（DeepLearning.AI 内容团队正在使用），现在借助 AI 辅助编程，这些应用的开发都可以快速完成。

I find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don't have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it's challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).

我发现，AI 辅助编码在原型设计阶段尤其高效，原因在于：（i）独立的原型所需的上下文信息和软件集成相对较少；(ii）处于 alpha 测试的原型通常对可靠性的要求不高。尽管生成式 AI（Generative AI）也能辅助构建大型、对任务至关重要的软件系统，但在这些领域，生产力的提升并不如原型设计阶段那样显著。这是因为，要让 AI 系统充分理解大型代码库的上下文，并确保生成的代码可靠（例如，考虑到各种特殊情况），仍然是一项具有挑战性的任务。

Andrew Ng celebrating and wishing a Happy New Year 2025 with sparklers.

Andrew Ng 放着烟火庆祝，并祝大家 2025 新年快乐。

Until now, a huge friction point for getting a prototype into users' hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed — sometimes using an AI-enabled IDE like Cursor — and finally assemble the modules.)

目前，将原型交付到用户手中最大的障碍之一就是部署。像 Bolt、Replit Agent、Vercel V0 这样的平台，利用生成式 AI（Generative AI）和 AI 智能体（AI Agent）的工作流程来提高代码质量，更重要的是，它们还能帮助直接部署生成的应用程序。（我个人觉得这些系统很有用，但我的工作流程通常是先用大语言模型（LLM）设计系统架构，然后生成代码，如果有多个大型模块，就一次生成一个。接着，我会测试每个模块，如果需要还会进一步编辑代码 —— 有时会用像 Cursor 这样具有 AI 功能的 IDE —— 最后把这些模块组装起来。）

Building prototypes quickly is an efficient way to test ideas and get tasks done. It's also a great way to learn. Perhaps most importantly, it's really fun! (At least I think it is.)

快速制作原型是测试想法和完成任务的有效方法。这也是一种很好的学习方式。也许最重要的是，这真的很有趣！（至少我是这么认为的。)

How can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:

在新的一年里，你如何把握这些机遇？在制定新年计划时，我希望你：

Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!

制定一个学习计划！为了高效地进行构建，我们都需要紧跟不断涌现的新变化。你计划在 2025 年每个月参加多少个短期课程？如果你和朋友分享学习计划，可以互相帮助。例如，我们推出了学习总结页面，可以查看大家已完成的短期课程。DeepLearning.AI 团队的成员们还发起了一场友好的竞赛，比比看谁在 2025 年能完成更多课程！

Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don't yet code, it would be well worth your while to learn! Even small wins — like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night — make life better. Perhaps you'll invent something that really takes off. And even if you don't, you'll have fun and learn a lot along the way.

开始动手吧！如果你已经会编程，我鼓励你随时抓住灵感，利用空闲时间构建原型。如果你还不会编程，那么非常值得去学习一下！即使是小的进步 —— 比如我打印的抽认卡，就激励我女儿昨晚多练习了 20 分钟乘法表 —— 也能让生活更美好。也许你会发明出真正有突破性的东西。即使没有，你也会在这个过程中收获乐趣并学到很多。

Happy New Year!

新年快乐！

Andrew

安德鲁

P.S. I develop mostly in Python. But if you prefer JavaScript: Happy Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) !

补充说明。我主要用 Python 进行开发。但是如果你更喜欢 JavaScript：Happy Array.from（{length：10}，(_，i）=> i ** 3).reduce（(a，b）=> a + b，0）!

2025 Beckons

2025 年快到了

We stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues, their answers offer inspiring views of what we may build and the good we can bring.

我们正迈入一个新时代：在这个时代，人工智能（AI）系统展现出卓越的能力，它们能够理解世界，把握我们的意图，并采取行动来实现这些意图。我们将如何运用这些强大的力量？我们邀请了该领域的领军人物，分享他们对来年的展望。与我们以往的新年特刊一样，他们的回答为我们展现了未来可能构建的成就，以及由此带来的福祉，这些都令人深受鼓舞。

### HANNO BASSE

Hanno Basse: Generative AI for Artists

Hanno Basse：为艺术家而生的生成式 AI（Generative AI)

Stability AI's aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.

Stability AI 的目标是让各行各业的艺术家们从重复性的机械工作中解放出来，从而将更多时间投入到创作中。因此，我们对来年最大的期望是，生成式 AI 能帮助人们变得更富创造力，并提高生产效率。

In addition, I hope the AI community will focus on:

此外，我希望 AI 社区将关注：

Safety and integrity: Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling.

安全和诚信：通过在开发的早期阶段就融入诚信理念来构建安全的产品，确保这项技术被负责任地使用，并为故事讲述的艺术做出有意义的贡献。

Accessibility: Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience.

易用性：生成式 AI（Generative AI）产品和工具必须面向尽可能广泛的受众，做到易用和便捷。目前，许多生成式 AI 主要还是面向具有高级技术背景的专业人士，例如工程师。为了解决这个问题，我们需要在基础模型之上开发更完善的工具，从而使它们能为不同的用户群体创造价值。

Customization: Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world.

定制化趋势：展望未来，我们认为生成式 AI（Generative AI）将朝着更加专业化的方向发展。除了大型基础模型之外，我们预计会出现大量更小、经过微调的模型，这些模型是为特定且通常非常狭窄的用例和应用量身定制的，甚至可以细化到针对单一任务的模型。这才能真正体现生成式 AI 的潜力。而且，这也是在实际应用中部署生成式 AI 最安全、最负责任的方式。

Hanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp.

Hanno Basse 是 Stability AI 的首席技术官。在此之前，他曾任 Digital Domain、Microsoft Azure Media and Entertainment 以及 20th Century Fox Film Corp. 的首席技术官。

### DAVID DING

David Ding: Generated Video With Music, Sound Effects, and Dialogue

David Ding：生成带有音乐、音效和对话的视频

Last year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity.

去年，我们见证了能够高质量生成视频或音频的模型大量涌现。在未来一年，我期待能够生成包含语音、音乐和音效等完整配乐的视频片段的模型出现。我希望这些模型将开启影视创作的新篇章。

The technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that's left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we've already seen something like this: Meta's Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.)

用于制作电影级视频生成器的技术已经成熟。多家公司都提供了极具竞争力的视频模型，像 Udio 这样的公司也开发了音乐模型。现在要做的就是同时对视频和音频进行建模，包括对话和旁白。事实上，我们已经见过类似的应用：Meta 公司的 Movie Gen。用户只需描述一个场景，Movie Gen 就能生成一段带有配乐和音效的视频短片。

Of course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models.

当然，训练这样的模型，也就是视频生成模型，将需要大量的数据集。但我认为，目前用来训练视频生成器的视频，它们的配乐中已经包含了这些声音元素，所以数据可能不会成为开发这类模型的障碍。

Initially, these models won't produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they'll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs.

最初，这些模型生成的作品还无法与专业视频编辑的顶尖作品相媲美。但是，它们将快速发展。很快，它们就能生成在原始质量上接近好莱坞大片水准的视频和音轨，就像现在的图像模型可以生成媲美高端照片的图像一样。

At the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn't control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I'm sure video modeling teams are doing similar research on controllability.

与此同时，用户对视频和音频输出的控制能力会不断提升。例如，当我们首次发布 Udio 时，用户无法控制它生成的和声。几个月后，我们发布了一项更新，使用户能够指定调性。这样，用户就可以将现有歌曲以其他调性重新混音。我们正在继续研究为用户提供更多控制选项，例如声音、旋律和节拍。我相信视频建模团队也在进行类似的控制能力方面的研究。

Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I've found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you'll learn that they don't always enjoy writing the boilerplate code for a website. Having a tool that builds a site's scaffold lets them spend their time on development tasks they find more stimulating and fun.

有些人可能会觉得生成式模型（Generative Model）可以制作完整的电影视频，这让人感到不安。我理解这种感受。我喜欢摄影和演奏音乐，并且发现图像和音频生成器是我进行创意工作的良好开端。例如，如果我需要，AI 可以提供一个基础图像，让我在 Photoshop 中继续完善；或者提供一段音乐，我可以从中采样或进一步创作。再比如，AI 编码助手可以生成整个网站的文件。你可能不再需要依赖 Web 开发人员，但如果你和他们交流，你会发现他们其实并不总是喜欢编写网站的重复性代码。拥有一个能够搭建网站框架的工具，可以让开发者们把时间投入到他们认为更有趣、更有挑战的开发任务中。

In a similar way, you'll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.

同样地，你将能够编写剧本，并快速生成电影样貌的粗略草稿。你可以生成 1000 个版本，从中挑选你最喜欢的一个，并以此为灵感，指导摄影师和演员。

Art is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you're an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels — and they'll be able to make those choices more fluidly, quickly, and interactively.

艺术的精髓在于其中蕴含的创造性选择。你和我都可以用 Midjourney 生成风景画，但如果你是艺术家，并且对你想要呈现的风景有清晰的想法，你用 Midjourney 生成的作品会比我的更具吸引力。同样，任何人都能用 Udio 制作出高品质的音乐，但如果你有好的音乐品味，你的音乐作品也会更加出色。视频依然会是一种艺术形式，因为每个人都可以选择他们的影片内容、视觉呈现和情感表达 —— 并且他们能够更流畅、快速和互动地完成这些选择。

David Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.

David Ding 是一位资深音乐人，也是 Udio 的联合创始人。Udio 是一家开发音乐创作 Web 应用的公司，该应用让用户可以轻松创作原创音乐。在此之前，他曾是 Google DeepMind 的高级研究工程师。

### JOSEPH GONZALEZ

Joseph Gonzalez: General Intelligence

Joseph Gonzalez：关于通用人工智能

In 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack. I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.

在 2025 年，我预计基础模型（foundation model）训练的进展将会放缓，因为我们逐渐接近扩展的极限，并且使用 AI 进行推理的成本持续上升。相反，我希望看到在 AI 之上的创新蓬勃发展，例如快速发展的 AI 智能体（agents stack）技术栈。我希望我们能够看到如何将 AI 与各种工具和现有系统结合起来的创新，从而提供令人兴奋的新功能，并创造出全新的产品类别。也许最重要的是，我非常期待看到人们如何在这个新世界中做出改变。

We have achieved AGI. Now what? Let's start with — and hopefully end — the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general. I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.

我们已经实现了通用人工智能（AGI）。接下来呢？让我们首先讨论，并希望结束，长期以来关于通用人工智能（AGI）的争论。我知道这个观点存在争议，但我认为我们已经实现了通用人工智能（AGI），至少从定义上来说是这样：我们的人工智能现在具有通用性。关于感知能力和超人工智能的更深入的探讨，我将留给哲学家们，而我将专注于这项关键的创新：通用性。

The artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them, are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.

过去几十年的人工智能或机器学习虽然很智能，但高度专业化。它通常能在特定任务（例如图像识别或内容推荐）上超越人类的能力。今天的模型，以及更重要的，围绕这些模型的系统，能够完成非常广泛的任务，其表现通常和人类一样好，甚至在某些情况下更好。正是这种通用性，使得工程师、科学家和艺术家能够以模型开发者从未想象过的方式利用这些模型进行创新。正是这种通用性，以及市场力量的驱动，将使 2025 年充满期待。

Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM, where I'm a co-founder, we're building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We've also found — to our surprise — that users share much more with an AI than they would share with another person.

拥抱 AI 时代：这些模型的通用性和自然语言界面意味着每个人都可以使用和探索 AI。我们正是如此！我们正在学习向机器描述我们的问题，提供情境信息和引导，并期望获得定制化的答案和解决办法。我作为 RunLLM 的联合创始人，我们正在构建高质量的技术支持智能体（technical support agents）。我们发现用户越来越多地使用我们的智能体，不仅是为了解决问题，更是为了根据他们的特定任务定制解决方案。令我们惊讶的是，用户更愿意与 AI 分享信息，而不是与他人分享。

Meanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.

同时，在加州大学伯克利分校，我看到一些学生使用 AI 来重新解释我的讲座内容，或者利用 AI 生成的练习题进行学习，这让我印象深刻。他们已经找到方法，利用 AI 来帮助他们个性化学习，并提升学习体验。也许到了 2025 年，当我们需要帮助或学习新知识时，我们可能会更倾向于选择 AI 而不是人类。

Across all these use cases, we're clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.

在所有这些应用场景中，我们显然越来越擅长克服大语言模型（Large Language Model）的局限性，并以我 12 个月前无法想象的方式使用人工智能（AI）。

Return on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems — reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.

人工智能的回报：2025 年，重点将转向展现过去投资的实际价值。投资者和企业将期望初创公司和企业人工智能团队从技术探索阶段过渡到解决实际问题，例如降低成本、增加收入、提升客户体验等。对于需要筹集研究经费的学者而言，这可能不是好消息（如果您在 2024 财年有剩余资金，请私信我），但对于其他人来说，这无疑是好消息，他们将享受到由人工智能驱动的新功能所带来的便利。

There will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features — the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users' needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow's agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.

将会有激烈的竞争，各方竞相探索将人工智能融入产品和业务各个环节的创新途径。在初期，我们可能会看到许多匆忙上线的聊天机器人和自动摘要功能，这仅仅是人工智能应用的初步尝试。我希望这些功能能够迅速被更智能的 AI 智能体所取代，这些 AI 智能体能够适应用户的需求，并从与用户的互动中学习。疫情加速了远程（数字）助手的发展，并展示了一个几乎完全可以通过数字化方式访问的工作场所，其中也包括了未来 AI 智能体所需的工具。这些 AI 智能体很可能会专注于接替曾经由人类承担的工作，或者填补由其他 AI 智能体创造的新职位。也许，当每个人都能管理自己的定制 AI 智能体团队时，我们就将见证人工智能真正兑现其承诺。

Chat is only the beginning: My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.

聊天只是开端：我希望在 2025 年，我们能超越简单的聊天，探索如何利用 AI 成就伟大的事业！我期待看到 AI 智能体在后台默默工作，无形地协助我们处理日常事务。它们会在我们做决策时提供恰当的背景信息，并在世界变化时帮助我们学习。通过这些背景信息和工具，它们会提醒我们注意遗漏之处，并弥补我们的疏忽。我们将减少聊天，而由 AI 驱动的智能体将代表我们完成更多工作。我期待着有一天，我可以自信地放下键盘，专注于那些重要的人际交往。

Joseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.

Joseph Gonzalez 是加州大学伯克利分校的教授，RunLLM 的联合创始人，同时也是 Genmo 和 Letta 的顾问。

### ALBERT GU

Albert Gu: More Learning, Less Data

Albert Gu：提升学习效率，减少数据依赖

Building a foundation model takes tremendous amounts of data. In the coming year, I hope we'll enable models to learn more from less data.

构建一个基础模型（foundation model）需要海量数据。我希望在接下来的一年里，我们能让模型在更少的数据中学习到更多的知识。

The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we're running out of new sources of data for training large models.

AI 社区通过扩大 Transformer 模型和数据集的规模取得了显著的成功。但是，这种方法可能正在达到收益递减的阶段 —— 这种观点在预训练社区尝试训练下一代模型时越来越普遍。然而，目前的方法也带来了一些实际问题。在规模巨大的数据集上训练大型模型会消耗大量的时间和能量，并且，用于训练大型模型的新数据来源也正在枯竭。

The fact is, current models consume much more data than humans require for learning. We've known this for a while, but we've ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there's a difference in sample efficiency between our best models and humans. Human learning shows that there's a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models.

事实上，目前的模型在学习时消耗的数据远超人类所需。我们对此早已知晓，但由于模型扩展带来的惊人效果，我们一直忽略了这个问题。训练一个模型需要数万亿个 Token（Token），而人类成长为具有基本智能的个体，所需的数据量则要少几个数量级。这表明，我们目前最好的模型和人类在样本效率（sample efficiency）上存在显著差异。人类的学习过程表明，一定存在某种学习算法、目标函数、架构，或它们的组合，能够比当前的模型更高效地进行学习。

One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI:

解决这个问题的关键之一是让模型能够生成更高层次的抽象概念，并滤除噪声。我认为这个概念，以及由此引出的数据效率问题，与当前人工智能领域的其他几个问题密切相关：

Data curation: We know that the specific data we use to train our models is extremely important. It's an open secret that most of the work that goes into training foundation models these days is about the data, not the architecture. Why is this? I think it's related to the fact that our models don't learn efficiently. We have to do the work ahead of time to prepare the data for a model, which may hinder the core potential of AI as an automatic process for learning from data.

数据整理：我们知道，用于训练模型的特定数据至关重要。如今，训练基础模型的大部分工作都集中在数据而非架构上，这已是业内公开的秘密。为什么会这样？我认为这与模型学习效率较低有关。我们必须提前整理好数据，为模型做好准备，这可能会阻碍 AI 作为从数据中自动学习的核心潜力。

Feature engineering: In deep learning, we always move toward more generalized approaches. From the beginning of the deep learning revolution, we've progressively removed handcrafted features such as edge detectors in computer vision and n-grams in natural language processing. But that engineering has simply moved to other parts of the pipeline. Tokenization, for instance, involves engineering implicit features. This suggests that there's still a lot of room to make model architectures that are more data-efficient and more generally able to handle more raw modalities and data streams.

特征工程：在深度学习领域，我们始终致力于探索更加通用的方法。自从深度学习革命开始以来，我们逐步摒弃了手工设计的特征，例如计算机视觉中的边缘检测器和自然语言处理中的 n-gram。然而，这种工程设计只是转移到了流程的其他环节。例如，Token 化就包含了对隐含特征的设计。这表明，在提升模型架构的数据利用效率，并使其更广泛地处理原始数据模态和数据流方面，仍然有很大的进步空间。

Multimodality: The key to training a model to understand a variety of data types together is figuring out the core abstractions in common and relating them to each other. This should enable models to learn from less data by leveraging all the modalities jointly, which is a core goal of multimodal learning.

多模态：要训练模型理解多种类型的数据，关键在于找到这些数据类型共通的核心抽象概念，并将它们彼此关联起来。这样做可以让模型通过联合利用所有模态的数据进行学习，从而减少对数据量的需求。而这正是多模态学习的核心目标。

Interpretability and robustness: To determine why a model produced the output it did, it needs to be able to produce higher-level abstractions, and we need to track the way it captures those abstractions. The better a model is at doing this, the more interpretable it should be, the more robust it should be to noise, and likely the less data it should need for learning.

可解释性和鲁棒性：为了探究模型为何会产生特定的输出，模型需要具备生成更高层次抽象概念的能力，并且我们需要追踪模型如何捕获这些抽象概念。模型在这方面表现得越好，其可解释性就越高，对噪声的鲁棒性也越强，并且学习所需的数据量也可能越少。

Reasoning: Extracting higher-level patterns and abstractions should allow models to reason better over them. Similarly, better reasoning should mean less training data.

推理：提取更高层次的模式和抽象概念，应该能让模型更好地利用这些信息进行推理。同样的，更强的推理能力也意味着模型对训练数据的需求量会减少。

Democratization: State-of-the-art models are expensive to build, and that includes the cost of collecting and preparing enormous amounts of data. Few players can afford to do it. This makes developments in the field less applicable to domains that lack sufficient data or wealth. Thus more data-efficient models would be more accessible and useful.

普及化：构建顶尖模型成本高昂，这包括收集和准备海量数据的成本。只有少数机构能够承担得起。这使得该领域的技术发展难以应用到缺乏足够数据或资金的领域。因此，数据高效的模型将更容易获取和使用。

Considering data efficiency in light of these other problems, I believe they're all related. It's not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models.

综合考虑数据效率和其他相关问题，我认为它们之间存在关联。究竟哪个是因，哪个是果，目前还不太明确。如果我们能解决模型的可解释性问题，那么我们所设计的机制或许能帮助模型提取更有效的特征，从而实现更高的数据效率。反之，我们也可能发现，更高的数据效率反而会带来更易于理解的模型。

Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year.

总之，数据利用率至关重要，这方面的进步将是衡量人工智能（AI）领域更广泛进展的一个重要指标。我希望在未来一年看到显著突破。

Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time's list of the most influential people in AI in 2024.

Albert Gu 是卡内基梅隆大学的机器学习助理教授，同时也是 Cartesia AI 的首席科学家。他被《时代》杂志评为 2024 年人工智能领域最具影响力的人物之一。

### MUSTAFA SULEYMAN

Mustafa Suleyman: Agents of Action

穆斯塔法·苏莱曼：行动的 AI 智能体

In 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf.

在 2025 年，人工智能将具备视觉能力，它会更聪明、更准确，并且它将开始代表你做事。

Today AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don't have a full understanding of what we're doing or aiming for beyond that. To really grasp our intentions, they need to see what we see.

今天，人工智能系统难以理解我们的全部语境。它们的感知仅限于聊天窗口和相当狭窄的一组交互。它们并不完全理解我们正在做什么，以及我们更深层次的目标。为了真正理解我们的意图，它们需要像我们一样感知世界。

This capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you're looking at or working on and make it a true two-way interaction.

这种能力已经成为现实。AI 可以融入我们日常使用的软件中，与我们协同工作，一起浏览网页。如果说文本是最初与 AI 互动的方式，那么语音则是 2024 年的突破性进展，我认为视觉将在 2025 年迎来类似的爆发。在 Microsoft AI，我的首要任务之一是创建一个能在浏览器中与你并肩工作的 AI，你可以通过对话来交流你正在看的内容或处理的任务，实现真正的双向互动。

Vision is a step change, palpably different from the ways we've been able to use computers in the past. I can't wait to see where it goes in the coming months.

视觉技术是一次飞跃式的进步，它与我们过去使用计算机的方式截然不同。我非常期待它在未来几个月内的发展。

Alongside vision, we'll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they'll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving.

除了视觉技术，我们还会看到在减少生成式 AI（Generative AI）的「幻觉」(hallucinations）现象方面取得显著进展。这仍然是 AI 被广泛应用的主要障碍。如果人们对 AI 给出的信息有所怀疑，就会大大限制他们使用 AI 的场景。信任是 AI 的根本基石。好消息是，模型本身的质量，以及它们的信息检索和「接地」（grounding）能力都在快速提升。

While I don't think we'll eliminate hallucinations entirely, by this time next year, we won't be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn't about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference.

虽然我不认为我们能完全消除生成式 AI（Generative AI）的「幻觉」现象，但到明年这个时候，我们可能不会再像现在这样为此烦恼了。「幻觉」指的是 AI 在生成内容时出现的事实性错误或者虚构信息。在大多数主题上，与 AI 智能体（AI Agent）交谈至少会像使用搜索引擎一样可靠，甚至可能更可靠。这并非依赖于单一的技术突破，而是整个领域持续迭代进步的体现。这将带来巨大的改变。

Lastly, we're entering the agentic era. We've been dreaming of this moment for decades. In my book, The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma, I proposed that we start thinking about ACI, or artificially capable intelligence: the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn't just talking to us, it's doing things. This is a critical change, and it's right around the corner.

最后，我们正在进入 AI 智能体时代。我们梦想着这一刻已经几十年了。在我的书《即将到来的浪潮：技术、力量和 21 世纪最大的困境》中，我提出我们开始思考 ACI，或者说具备行动能力的人工智能：即 AI 开始代表用户采取具体行动的时刻。当 AI 被赋予行动能力时，它就不再只是与我们对话，而是开始实际执行任务。这是一个关键的改变，而且这一刻即将到来。

If we get it right, we'll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems.

如果我们能把握正确方向，就能立刻让生活变得更轻松、更平静，同时大幅提升企业和个人生产力。但是，智能体（agentic）的功能需要满足最高的安全性、保障性和责任标准。与此同时，创造真正有用的智能体仍然面临诸多挑战，尤其是在与众多系统集成方面。

The momentum is there. Actions are on their way. 2025 is going to be a big year.

发展势头已经形成，各项行动正在展开，2025 年将是至关重要的一年。

Mustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies.

Mustafa Suleyman 现任 Microsoft AI 首席执行官，他曾联合创立 Inflection AI，并创建了 DeepMind Technologies。

### AUDREY TANG

Audrey Tang: AI That Unites Us

Audrey Tang：团结你我的 AI

As we approach 2025, my greatest hope for AI is that it will enable prosocial platforms that promote empathy, understanding, and collaboration rather than division.

展望 2025 年，我对人工智能最大的期望是，它能催生更多促进同理心、理解和协作的亲社会平台，而非导致社会分裂。

For too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can — and should — help us transcend these entrenched divides.

长期以来，驱动社交媒体的算法就像一台台「注意力收割机」，在攫取用户注意力的同时，也侵蚀着信任和社会凝聚力。最终留下的是一个个「贫瘠」的线上空间，同情心难以在这里生根发芽，集体解决问题也找不到发展的土壤。人工智能可以 —— 而且应该 —— 帮助我们跨越这些根深蒂固的隔阂。

To achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward "bridging content" that reveals common ground. They should clearly identify the communities a piece of content relates to — whether physical, religious, political, social, cultural, or professional — and illuminate the specific lines of division it seeks to mend.

为了达成这个目标，我们必须设计以亲社会价值观为核心的 AI 系统。推荐算法不应该加剧社会分裂，而是应该引导我们发现能够揭示共同之处的「桥梁内容」。这些算法应该清晰地标明一段内容所关联的社群，包括地理位置、宗教、政治、社会、文化或专业等方面的社群，并阐明内容试图弥合的具体分歧。

Realizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight "surprising validators," or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorously assess their impact on democratic life.

要实现这个目标，我们需要在优化方向上进行根本性的转变。我们不应该仅仅关注用户互动数据，而应该采用以价值观为导向的指标，优先考虑建设性的讨论和相互理解。例如，我们可以关注那些「提出意想不到的见解的人」，他们能够富有成效地挑战既有假设，从而帮助我们更好地理解那些看似无法调和的观点。研究人员和开发人员应该共同设计新的排序和内容推荐方法，并将其应用到常用平台中，同时严格评估这些方法对社会民主的影响。

At the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research on pluralistic alignment stresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools like Polis, which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few.

与此同时，人工智能（AI）社区必须拥抱参与式、包容性的发展和治理方法。关于多方协同对齐的研究强调，人工智能系统产生于复杂的社会背景中并在其中运作，听取多方意见有助于防范机构的认知盲区。像 Polis 这样的工具，可以将观点可视化并揭示隐藏的共识领域，已经说明了如何将复杂性转化为清晰性。这种参与式方法确保人工智能能够反映其所服务社会的优先事项和价值观，而不是放大少数人的偏见。

By embracing these inclusive, democratic principles, AI can help us co-create digital public squares that foster social cohesion rather than erode it. Embedding collective input at every stage — from how we build datasets to how we set governance policies — ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding.

通过采纳这些包容和民主的原则，人工智能（AI）可以帮助我们构建数字公共空间，这些空间能够促进社会凝聚力，而不是削弱它。在每个环节都融入集体的意见 —— 从如何构建数据集，到如何制定管理政策 —— 确保 AI 系统真正符合人类多元的价值观，并成为促进共识的催化剂。

Audrey Tang is Taiwan's Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy.

唐凤是台湾的网路大使、前数位发展部部长，同时也是《多元：协作科技与民主的未来》一书的合著者。

## 原文

Andrew Ng Happy New Year

Dear friends,

Happy sum(i**3 for i in range(10)) !

Despite having worked on AI since I was a teenager, I’m now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!

One aspect of AI that I’m particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.

If you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1’s help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI’s finance team), or analyzes  user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding.

I find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don’t have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it’s challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).

Andrew Ng celebrating and wishing a Happy New Year 2025 with sparklers.
Until now, a huge friction point for getting a prototype into users’ hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed — sometimes using an AI-enabled IDE like Cursor — and finally assemble the modules.)

Building prototypes quickly is an efficient way to test ideas and get tasks done. It’s also a great way to learn. Perhaps most importantly, it’s really fun! (At least I think it is.)

How can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:

Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!  
Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don’t yet code, it would be well worth your while to learn! Even small wins — like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night — make life better. Perhaps you’ll invent something that really takes off. And even if you don’t, you’ll have fun and learn a lot along the way.
Happy New Year! 
Andrew

P.S. I develop mostly in Python. But if you prefer JavaScript: Happy Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) !

2025 Beckons
We stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues, their answers offer inspiring views of what we may build and the good we can bring.

HANNO BASSE
Hanno Basse: Generative AI for Artists
Stability AI’s aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.

     In addition, I hope the AI community will focus on:

Safety and integrity: Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling.
Accessibility: Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains  accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience.
Customization: Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world.
Hanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp.

DAVID DING
David Ding: Generated Video With Music, Sound Effects, and Dialogue
Last year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity.

     The technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that’s left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we’ve already seen something like this: Meta’s Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.)

     Of course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models.

     Initially, these models won’t produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they’ll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs.

     At the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn’t control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I’m sure video modeling teams are doing similar research on controllability.

     Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I’ve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you’ll learn that they don’t always enjoy writing the boilerplate code for a website. Having a tool that builds a site’s scaffold lets them spend their time on development tasks they find more stimulating and fun.

     In a similar way, you’ll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors.

     Art is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you’re an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels — and they’ll be able to make those choices more fluidly, quickly, and interactively.

David Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.

JOSEPH GONZALEZ
Joseph Gonzalez: General Intelligence
In 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack. I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.

     We have achieved AGI. Now what? Let’s start with — and hopefully end — the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general. I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.

     The artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them, are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.

     Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM, where I’m a co-founder, we’re building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We’ve also found — to our surprise — that users share much more with an AI than they would share with another person.

     Meanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.

     Across all these use cases, we’re clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.

     Return on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems — reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.

     There will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features — the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users’ needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow’s agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.

     Chat is only the beginning: My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.

Joseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.

ALBERT GU
Albert Gu: More Learning, Less Data
Building a foundation model takes tremendous amounts of data. In the coming year, I hope we’ll enable models to learn more from less data.

     The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we’re running out of new sources of data for training large models.

     The fact is, current models consume much more data than humans require for learning. We’ve known this for a while, but we’ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there’s a difference in sample efficiency between our best models and humans. Human learning shows that there’s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models.

     One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI: 

Data curation: We know that the specific data we use to train our models is extremely important. It’s an open secret that most of the work that goes into training foundation models these days is about the data, not the architecture. Why is this? I think it’s related to the fact that our models don’t learn efficiently. We have to do the work ahead of time to prepare the data for a model, which may hinder the core potential of AI as an automatic process for learning from data.
Feature engineering: In deep learning, we always move toward more generalized approaches. From the beginning of the deep learning revolution, we’ve progressively removed handcrafted features such as edge detectors in computer vision and n-grams in natural language processing. But that engineering has simply moved to other parts of the pipeline. Tokenization, for instance, involves engineering implicit features. This suggests that there’s still a lot of room to make model architectures that are more data-efficient and more generally able to handle more raw modalities and data streams.
Multimodality: The key to training a model to understand a variety of data types together is figuring out the core abstractions in common and relating them to each other. This should enable models to learn from less data by leveraging all the modalities jointly, which is a core goal of multimodal learning. 
Interpretability and robustness: To determine why a model produced the output it did, it needs to be able to produce higher-level abstractions, and we need to track the way it captures those abstractions. The better a model is at doing this, the more interpretable it should be, the more robust it should be to noise, and likely the less data it should need for learning.
Reasoning: Extracting higher-level patterns and abstractions should allow models to reason better over them. Similarly, better reasoning should mean less training data.
Democratization: State-of-the-art models are expensive to build, and that includes the cost of collecting and preparing enormous amounts of data. Few players can afford to do it. This makes developments in the field less applicable to domains that lack sufficient data or wealth. Thus more data-efficient models would be more accessible and useful. 
     Considering data efficiency in light of these other problems, I believe they’re all related. It’s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models.

     Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year.

Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time’s list of the most influential people in AI in 2024.

MUSTAFA SULEYMAN
Mustafa Suleyman: Agents of Action
In 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf.

     Today AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don’t have a full understanding of what we’re doing or aiming for beyond that. To really grasp our intentions, they need to see what we see.

     This capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you’re looking at or working on and make it a true two-way interaction.

     Vision is a step change, palpably different from the ways we’ve been able to use computers in the past. I can’t wait to see where it goes in the coming months.

     Alongside vision, we’ll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they’ll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving.

     While I don’t think we’ll eliminate hallucinations entirely, by this time next year, we won’t be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn’t about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference.

     Lastly, we’re entering the agentic era. We’ve been dreaming of this moment for decades. In my book, The Coming Wave: Technology, Power, and the 21st Century’s Greatest Dilemma, I proposed that we start thinking about ACI, or artificially capable intelligence: the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn’t just talking to us, it’s doing things. This is a critical change, and it’s right around the corner.

     If we get it right, we’ll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems.

     The momentum is there. Actions are on their way. 2025 is going to be a big year.

Mustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies.

AUDREY TANG
Audrey Tang: AI That Unites Us
As we approach 2025, my greatest hope for AI is that it will enable prosocial platforms that promote empathy, understanding, and collaboration rather than division.

     For too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can — and should — help us transcend these entrenched divides.

     To achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward “bridging content” that reveals common ground. They should clearly identify the communities a piece of content relates to — whether physical, religious, political, social, cultural, or professional — and illuminate the specific lines of division it seeks to mend.

     Realizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight “surprising validators,” or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorously assess their impact on democratic life.

     At the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research on pluralistic alignment stresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools like Polis, which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few.

     By embracing these inclusive, democratic principles, AI can help us co-create digital public squares that foster social cohesion rather than erode it. Embedding collective input at every stage — from how we build datasets to how we set governance policies — ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding.

Audrey Tang is Taiwan’s Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy.