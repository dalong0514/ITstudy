## 20240216视频生成模型-构建虚拟世界的模拟器

[视频生成模型：构建虚拟世界的模拟器 [译] | 宝玉的分享](https://baoyu.io/translations/openai/video-generation-models-as-world-simulators)

原文：

[Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators)

We explore large-scale training of generative models on video data. Specifically, we train text-conditional diffusion models jointly on videos and images of variable durations, resolutions and aspect ratios. We leverage a transformer architecture that operates on spacetime patches of video and image latent codes. Our largest model, Sora, is capable of generating a minute of high fidelity video. Our results suggest that scaling video generation models is a promising path towards building general purpose simulators of the physical world.

我们致力于在视频数据上开展生成模型的大规模训练。具体来说，我们针对不同时长、分辨率和宽高比的视频及图像，联合训练了基于文本条件的扩散模型。我们采用了一种 Transformer 架构，这种架构能够处理视频和图像潜在编码的时空片段。我们的最大型号模型，Sora，能生成高质量的一分钟视频。我们的研究显示，扩展视频生成模型的规模是向着创建能够模拟物理世界的通用工具迈出的有前途的一步。

This technical report focuses on (1) our method for turning visual data of all types into a unified representation that enables large-scale training of generative models, and (2) qualitative evaluation of Sora’s capabilities and limitations. Model and implementation details are not included in this report.

Much prior work has studied generative modeling of video data using a variety of methods, including recurrent networks,1,2,3 generative adversarial networks,4,5,6,7 autoregressive transformers,8,9 and diffusion models.10,11,12 These works often focus on a narrow category of visual data, on shorter videos, or on videos of a fixed size. Sora is a generalist model of visual data—it can generate videos and images spanning diverse durations, aspect ratios and resolutions, up to a full minute of high definition video.

本技术报告主要介绍了两方面内容：

1、我们如何将各种类型的视觉数据转化为统一的表示形式，从而实现生成模型的大规模训练；

2、对 Sora 模型能力和局限性的定性评价。报告中没有包含模型和实施的详细信息。

之前的很多研究都探讨过利用各种方法对视频数据进行生成模型的建模，包括循环网络 1,2,3，生成对抗网络 4,5,6,7，自回归 Transformer 8,9 以及扩散模型 10,11,12。这些研究通常关注于特定类别的视觉数据，较短的视频，或是固定尺寸的视频。Sora 是一种对视觉数据进行广义建模的模型，它能够生成各种时长、宽高比和分辨率的视频和图像，最长可达一分钟的高清视频。

### 01. 视觉数据的创新转化：补片技术

Turning visual data into patches

We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data.13,14 The success of the LLM paradigm is enabled in part by the use of tokens that elegantly unify diverse modalities of text—code, math and various natural languages. In this work, we consider how generative models of visual data can inherit such benefits. Whereas LLMs have text tokens, Sora has visual patches. Patches have previously been shown to be an effective representation for models of visual data.15,16,17,18 We find that patches are a highly-scalable and effective representation for training generative models on diverse types of videos and images.

Figure Patches

At a high level, we turn videos into patches by first compressing videos into a lower-dimensional latent space,19 and subsequently decomposing the representation into spacetime patches.

视觉数据的创新转化：补片技术

受到大语言模型（LLM）在处理互联网规模数据、培养全能技能方面成功经验的启发，13,14 我们探索了如何将类似的优势应用于视觉数据的生成模型。大语言模型通过使用 tokens —— 一种统一处理代码、数学及多种自然语言的高效方式 —— 实现了模态间的无缝转换。在本研究中，我们引入了视觉领域的对应物：视觉补片（patches）。研究表明，补片是一种高效的视觉数据表现形式，15,16,17,18 它们能极大地提升生成模型处理多样化视频和图像数据的能力。

图 1: 补片示意图

具体来说，我们通过先将视频数据压缩到低维度潜在空间，19 再将其分解成时空补片，从而实现视频到补片的转化。

### 02. 视频压缩网络

Video compression network

We train a network that reduces the dimensionality of visual data.20 This network takes raw video as input and outputs a latent representation that is compressed both temporally and spatially. Sora is trained on and subsequently generates videos within this compressed latent space. We also train a corresponding decoder model that maps generated latents back to pixel space.

视频压缩网络

我们开发了一种降维技术，20 该技术能够处理原始视频数据，并生成在时间和空间上都进行了压缩的潜在表征。Sora 在这种压缩的潜在空间中接受训练，并能够生成新的视频内容。此外，我们还开发了一个解码器，能够将这些潜在表征还原为像素级的视频图像。

### 03. 时空补片技术

Spacetime latent patches

Given a compressed input video, we extract a sequence of spacetime patches which act as transformer tokens. This scheme works for images too since images are just videos with a single frame. Our patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. At inference time, we can control the size of generated videos by arranging randomly-initialized patches in an appropriately-sized grid.

时空补片技术

通过对压缩后的视频输入进行处理，我们能够提取出一系列的时空补片，这些补片在模型中扮演着类似于 Transformer Tokens 的角色。值得一提的是，这套方案同样适用于图像处理，因为从本质上来说，图像可以被视为单帧的视频。采用基于补片的表现形式，Sora 能够适应不同分辨率、持续时间及宽高比的视频和图像。在生成新视频内容时，我们可以通过将这些随机初始化的补片按照需要的大小排列成网格，来控制最终视频的大小和形式。

### 04. 视频生成的 Transformer 扩展技术

Scaling transformers for video generation

Sora is a diffusion model21,22,23,24,25; given input noisy patches (and conditioning information like text prompts), it’s trained to predict the original “clean” patches. Importantly, Sora is a diffusion transformer.26 Transformers have demonstrated remarkable scaling properties across a variety of domains, including language modeling,13,14 computer vision,15,16,17,18 and image generation.27,28,29

Figure Diffusion

In this work, we find that diffusion transformers scale effectively as video models as well. Below, we show a comparison of video samples with fixed seeds and inputs as training progresses. Sample quality improves markedly as training compute increases.

Base compute

4x compute

32x compute

视频生成的 Transformer 扩展技术

Sora 是一种扩散模型 21,22,23,24,25；它能够接受带有噪声的图像块（及条件信息如文本提示）作为输入，并被训练以预测出原始的「清晰」图像块。值得注意的是，Sora 属于扩散型 Transformer26。Transformer 技术在多个领域，包括语言建模 13,14、计算机视觉 15,16,17,18 以及图像生成 27,28,29 中都展现出了卓越的扩展能力。

图 Diffusion

本研究发现，扩散型 Transformer 同样能在视频模型领域高效扩展。下文中，我们通过对比训练过程中固定种子和输入条件下的视频样本，展示了训练资源增加带来的样本质量显著提升。

基础计算

4 倍计算

16 倍计算

### 05. 视频的多样化持续时间、分辨率和宽高比

Variable durations, resolutions, aspect ratios

Past approaches to image and video generation typically resize, crop or trim videos to a standard size—e.g., 4 second videos at 256x256 resolution. We find that instead training on data at its native size provides several benefits.

Sampling flexibility

Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. This lets Sora create content for different devices directly at their native aspect ratios. It also lets us quickly prototype content at lower sizes before generating at full resolution—all with the same model.

Improved framing and composition

We empirically find that training on videos at their native aspect ratios improves composition and framing. We compare Sora against a version of our model that crops all training videos to be square, which is common practice when training generative models. The model trained on square crops (left) sometimes generates videos where the subject is only partially in view. In comparison, videos from Sora (right) have improved framing.

Language understanding

Training text-to-video generation systems requires a large amount of videos with corresponding text captions. We apply the re-captioning technique introduced in DALL·E 330 to videos. We first train a highly descriptive captioner model and then use it to produce text captions for all videos in our training set. We find that training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.

Similar to DALL·E 3, we also leverage GPT to turn short user prompts into longer detailed captions that are sent to the video model. This enables Sora to generate high quality videos that accurately follow user prompts.

视频的多样化持续时间、分辨率和宽高比

传统的图像和视频生成方法通常会将视频调整至标准尺寸，例如 4 秒长的视频以 256x256 的分辨率进行处理。我们发现，直接在视频的原始尺寸上进行训练能带来多重好处。

灵活的采样能力

Sora 能够生成各种尺寸的视频，包括宽屏的 1920x1080p、竖屏的 1080x1920 以及介于两者之间的任何格式。这使得 Sora 能够直接为不同设备制作符合其原生宽高比的内容。此外，它还允许我们在生成全分辨率内容之前，快速地以较低尺寸原型化内容，所有这些都能通过同一模型实现。

构图与布局的优化

我们的实验表明，在视频的原生宽高比上进行训练，能够显著提升视频的构图与布局质量。我们将 Sora 与另一个训练模型进行了对比，后者将所有训练视频裁剪为正方形，这是训练生成模型时的常规做法。与被裁剪成正方形的模型（左侧）相比，Sora 生成的视频（右侧）展现了更佳的构图效果，有时候裁剪成正方形的模型生成的视频中主题只能部分展示。而 Sora 则能够更好地捕捉完整的场景。

语言理解

开发能够从文字生成视频的系统，我们需要大量的视频及其对应的文字说明。我们采用了 DALL·E 330 中引入的一种重新标注技术，并将其应用于视频。首先，我们训练了一个能够生成详细描述的模型，然后利用这个模型为训练集里的所有视频创建文字说明。我们发现，使用描述性强的视频说明进行训练，不仅能提高文字的准确度，还能显著提升视频的整体质量。

就像 DALL·E 3 一样，我们还使用 GPT 把用户的简短提示转化成详尽的说明，再将这些说明送给视频生成模型。这一过程使得 Sora 能够根据用户的指令，制作出高品质的视频。

语言理解能力示例（点击展开）

### 06. 图片和视频的提示功能

Prompting with images and videos

All of the results above and in our landing page show text-to-video samples. But Sora can also be prompted with other inputs, such as pre-existing images or video. This capability enables Sora to perform a wide range of image and video editing tasks—creating perfectly looping video, animating static images, extending videos forwards or backwards in time, etc.

Animating DALL·E images

Sora is capable of generating videos provided an image and prompt as input. Below we show example videos generated based on DALL·E 231 and DALL·E 330 images.

A Shiba Inu dog wearing a beret and black turtleneck.

Monster Illustration in flat design style of a diverse family of monsters. The group includes a furry brown monster, a sleek black monster with antennas, a spotted green monster, and a tiny polka-dotted monster, all interacting in a playful environment.

An image of a realistic cloud that spells「SORA」.

In an ornate, historical hall, a massive tidal wave peaks and begins to crash. Two surfers, seizing the moment, skillfully navigate the face of the wave.

Extending generated videos

Sora is also capable of extending videos, either forward or backward in time. Below are four videos that were all extended backward in time starting from a segment of a generated video. As a result, each of the four videos starts different from the others, yet all four videos lead to the same ending.

00:00

00:20

We can use this method to extend a video both forward and backward to produce a seamless infinite loop.

Video-to-video editing

Diffusion models have enabled a plethora of methods for editing images and videos from text prompts. Below we apply one of these methods, SDEdit,32 to Sora. This technique enables Sora to transform the styles and environments of input videos zero-shot.

Input video

change the setting to be in a lush jungle

Connecting videos

We can also use Sora to gradually interpolate between two input videos, creating seamless transitions between videos with entirely different subjects and scene compositions. In the examples below, the videos in the center interpolate between the corresponding videos on the left and right.

图片和视频的提示功能

我们网站上的所有示例和展示的视频，都是从文字转化而来。不过，Sora 还能接受图片或已有视频作为输入。这项功能让 Sora 能够完成各种图片和视频编辑任务，比如制作无缝循环视频、给静态图片添加动画效果、延长视频的播放时间等。

让 DALL·E 图片动起来

只需一张图片和一个提示，Sora 就能创造出视频。下面展示了一些基于 DALL·E 231 和 DALL·E 330 图片生成的视频示例。

一只戴着贝雷帽和黑色高领衫的柴犬。

一个包含各种怪兽的家庭的平面设计风格插画。其中有一个毛茸茸的棕色怪兽，一个光滑的黑色怪兽配有触角，一个斑点绿色怪兽，以及一个小巧的带有圆点的怪兽，它们在愉快的环境中互动。

形成「SORA」字样的逼真云朵图像。

在一个装饰华丽的历史大厅里，一道巨大的海浪正准备冲击而来。两位冲浪者抓住机会，巧妙地驾驭着海浪。

视频时间延伸

Sora 同样能够把视频往前或往后延伸。下面是四个视频，它们都是从一个生成的视频片段开始，向后延伸。因此，尽管这四个视频的开头各不相同，但它们最终都汇聚于同一个结尾。

利用这种技术，我们能够将视频向前或向后扩展，创造出完美的无限循环效果。

视频到视频的创新编辑

扩散模型为基于文本提示的图像和视频编辑开辟了新天地。接下来，我们利用这些创新方法之一，SDEdit,32 对 Sora 进行应用。这项技术赋予了 Sora 力量，让它能够不需要任何预先示例，就能改变视频中的风格和环境。

输入视频

将设置更改为郁郁葱葱的丛林。

将设置更改为 1920 年代，使用旧学校的 captureRejectionSymbol。确保保持红色。

使其在水下。

将视频设置更改为不同于山脉的场景？也许是约书亚树？

将视频放置在太空中，并有一条彩虹路。

保持视频相同，但使其成为冬天。

用粘土动画风格制作。

用炭笔画风格重新创作，确保是黑白的。

将设置更改为赛博朋克。

将视频更改为中世纪主题。

使其有恐龙。

用像素艺术风格重写视频。

视频之间的流畅过渡

我们还可以利用 Sora 把两个风格迥异的视频平滑连接起来，使它们之间能够自然过渡，仿佛融为一体。在下方的示例中，你会看到，中间的视频巧妙地融合了左右两侧视频的元素。

### 07. 图像的魔法般创造

Image generation capabilities

Sora is also capable of generating images. We do this by arranging patches of Gaussian noise in a spatial grid with a temporal extent of one frame. The model can generate images of variable sizes—up to 2048x2048 resolution.

Close-up portrait shot of a woman in autumn, extreme detail, shallow depth of field

Vibrant coral reef teeming with colorful fish and sea creatures

Digital art of a young tiger under an apple tree in a matte painting style with gorgeous details

A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f/1.2

图像的魔法般创造

Sora 的能力不仅限于视频，它还能创造出令人惊叹的图像。我们通过在一个时间仅为一帧的空间网格里排列高斯噪声块来完成这一魔法。这样，Sora 能够创造出各种尺寸的图像，最大分辨率达到了 2048x2048。

秋日中，一位女士的特写肖像，细节惊人，景深浅得令人称奇。

一片生机勃勃的珊瑚礁，色彩斑斓的鱼类和海洋生物穿梭其间。

在苹果树下的年轻老虎的数字艺术作品，展现了哑光画风中的细致美。

一座雪覆盖的山村，温馨的小屋和北极光的展现，细节精致，仿佛用 dslr 拍摄的 50mm f/1.2 镜头下的画面。

### 08. 涌现的模拟能力

Emerging simulation capabilities

We find that video models exhibit a number of interesting emergent capabilities when trained at scale. These capabilities enable Sora to simulate some aspects of people, animals and environments from the physical world. These properties emerge without any explicit inductive biases for 3D, objects, etc.—they are purely phenomena of scale.

3D consistency. Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.

Long-range coherence and object permanence. A significant challenge for video generation systems has been maintaining temporal consistency when sampling long videos. We find that Sora is often, though not always, able to effectively model both short- and long-range dependencies. For example, our model can persist people, animals and objects even when they are occluded or leave the frame. Likewise, it can generate multiple shots of the same character in a single sample, maintaining their appearance throughout the video.

Interacting with the world. Sora can sometimes simulate actions that affect the state of the world in simple ways. For example, a painter can leave new strokes along a canvas that persist over time, or a man can eat a burger and leave bite marks.

Simulating digital worlds. Sora is also able to simulate artificial processes–one example is video games. Sora can simultaneously control the player in Minecraft with a basic policy while also rendering the world and its dynamics in high fidelity. These capabilities can be elicited zero-shot by prompting Sora with captions mentioning「Minecraft.」

These capabilities suggest that continued scaling of video models is a promising path towards the development of highly-capable simulators of the physical and digital world, and the objects, animals and people that live within them.

涌现的模拟能力

我们发现，在大规模训练下，视频模型展示出了一系列引人注目的涌现能力。这些功能让 Sora 有能力在一定程度上模拟现实世界中的人、动物和环境。这种能力的涌现，并不需要对三维空间、物体等有任何特定的预设偏好 —— 它们纯粹是由数据规模驱动的结果。

三维空间的连贯性。Sora 能生成带有动态视角变化的视频。当摄像机位置和角度变动时，视频中的人物和场景元素能够在三维空间中保持连贯移动。

远距离连续性与物体持久性。在生成长视频时，保持时间上的连续性一直是个挑战。我们观察到，Sora 通常能够有效处理短距离和长距离的依赖关系。比如，即使人物、动物或物体被遮挡或移出画面，我们的模型也能保持它们的连续存在。同样，它能在同一视频样本中多次展示同一角色，确保其外观贯穿始终。

与世界的互动。Sora 有时能模拟出简单地影响世界状态的行为。例如，画家在画布上留下的笔触随时间持久存在，或者某人吃汉堡留下的咬痕。

数字世界的模拟。Sora 还能模拟数字化过程，如视频游戏。它能在控制 Minecraft 游戏角色进行基本操作的同时，高质量渲染游戏世界及其动态。仅需通过提及「Minecraft」等字样的提示，即可激发这些能力的展现。

这些功能展示了，不断扩大视频模型的规模，是发展出能高度模拟物理及数字世界 —— 包括其中的物体、动物和人 —— 的高级模拟器的一条有前景的路径。

### 讨论

Discussion

Sora currently exhibits numerous limitations as a simulator. For example, it does not accurately model the physics of many basic interactions, like glass shattering. Other interactions, like eating food, do not always yield correct changes in object state. We enumerate other common failure modes of the model—such as incoherencies that develop in long duration samples or spontaneous appearances of objects—in our landing page.

We believe the capabilities Sora has today demonstrate that continued scaling of video models is a promising path towards the development of capable simulators of the physical and digital world, and the objects, animals and people that live within them.

讨论

作为一个模拟器，Sora 当前还有许多局限。比如，它无法精确模拟像玻璃破碎这样的基本物理互动。有些互动，比如吃东西，并不总能正确反映物体状态的改变。我们在 OpenAI Sora 介绍页中详细列出了模型的其它常见失误，包括长时间视频样本中出现的不一致性或物体的突然出现等问题。

我们相信，Sora 现有的能力展现了，继续扩展视频模型的规模是朝向开发出能够精准模拟物理和数字世界以及其中的物体、动物和人类的高级模拟器的一条充满希望的途径。

### References

1 Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhudinov. "Unsupervised learning of video representations using lstms." International conference on machine learning. PMLR, 2015.

2 Chiappa, Silvia, et al. "Recurrent environment simulators." arXiv preprint arXiv:1704.02254 (2017).

3 Ha, David, and Jürgen Schmidhuber. "World models." arXiv preprint arXiv:1803.10122 (2018).

4 Vondrick, Carl, Hamed Pirsiavash, and Antonio Torralba. "Generating videos with scene dynamics." Advances in neural information processing systems 29 (2016).

5 Tulyakov, Sergey, et al. "Mocogan: Decomposing motion and content for video generation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.

6 Clark, Aidan, Jeff Donahue, and Karen Simonyan. "Adversarial video generation on complex datasets." arXiv preprint arXiv:1907.06571 (2019).

7 Brooks, Tim, et al. "Generating long videos of dynamic scenes." Advances in Neural Information Processing Systems 35 (2022): 31769-31781.

8 Yan, Wilson, et al. "Videogpt: Video generation using vq-vae and transformers." arXiv preprint arXiv:2104.10157 (2021).

9 Wu, Chenfei, et al. "Nüwa: Visual synthesis pre-training for neural visual world creation." European conference on computer vision. Cham: Springer Nature Switzerland, 2022.

10 Ho, Jonathan, et al. "Imagen video: High definition video generation with diffusion models." arXiv preprint arXiv:2210.02303 (2022).

11 Blattmann, Andreas, et al. "Align your latents: High-resolution video synthesis with latent diffusion models." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

12 Gupta, Agrim, et al. "Photorealistic video generation with diffusion models." arXiv preprint arXiv:2312.06662 (2023).

13 Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).↩︎

14 Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.↩︎

15 Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020).↩︎

16 Arnab, Anurag, et al. "Vivit: A video vision transformer." Proceedings of the IEEE/CVF international conference on computer vision. 2021.↩︎

17 He, Kaiming, et al. "Masked autoencoders are scalable vision learners." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.↩︎

18 Dehghani, Mostafa, et al. "Patch n'Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution." arXiv preprint arXiv:2307.06304 (2023).↩︎

19 Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

20 Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).

21 Sohl-Dickstein, Jascha, et al. "Deep unsupervised learning using nonequilibrium thermodynamics." International conference on machine learning. PMLR, 2015.

22 Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in neural information processing systems 33 (2020): 6840-6851.

23 Nichol, Alexander Quinn, and Prafulla Dhariwal. "Improved denoising diffusion probabilistic models." International Conference on Machine Learning. PMLR, 2021.

24 Dhariwal, Prafulla, and Alexander Quinn Nichol. "Diffusion Models Beat GANs on Image Synthesis." Advances in Neural Information Processing Systems. 2021.

25 Karras, Tero, et al. "Elucidating the design space of diffusion-based generative models." Advances in Neural Information Processing Systems 35 (2022): 26565-26577.

26 Peebles, William, and Saining Xie. "Scalable diffusion models with transformers." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

27 Chen, Mark, et al. "Generative pretraining from pixels." International conference on machine learning. PMLR, 2020.

28 Ramesh, Aditya, et al. "Zero-shot text-to-image generation." International Conference on Machine Learning. PMLR, 2021.

29 Yu, Jiahui, et al. "Scaling autoregressive models for content-rich text-to-image generation." arXiv preprint arXiv:2206.10789 2.3 (2022): 5.

30 Betker, James, et al. "Improving image generation with better captions." Computer Science. https://cdn.openai.com/papers/dall-e-3. pdf 2.3 (2023): 8↩︎

31Ramesh, Aditya, et al. "Hierarchical text-conditional image generation with clip latents." arXiv preprint arXiv:2204.06125 1.2 (2022): 3.

32 Meng, Chenlin, et al. "Sdedit: Guided image synthesis and editing with stochastic differential equations." arXiv preprint arXiv:2108.01073 (2021).

Authors

Tim Brooks

Bill Peebles

Connor Holmes

Will DePue

Yufei Guo

Li Jing

David Schnurr

Joe Taylor

Troy Luhman

Eric Luhman

Clarence Wing Yin Ng

Ricky Wang

Aditya Ramesh

Acknowledgments

Citation

Please cite as OpenAI et al., and use the following bibtex for citation: https://openai.com/bibtex/videoworldsimulators2024.bib