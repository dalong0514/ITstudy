## 20231201语言大模型的分布式训练与高效微调指南

语言大模型的分布式训练与高效微调指南

[语言大模型的分布式训练与高效微调指南](https://mp.weixin.qq.com/s/m_zMNx7xR58YwMyBgrO-CA)

OneFlow 2023-11-29 09:28 发表于北京

作者 | Sumanth R Hegde

OneFlow 编译

翻译｜杨婷、宛子琳

最近语言大模型（LLM）异常火爆，一个非常特别的开源社区正在探索在消费级硬件上微调、提供服务和进行推理的最佳方式。为满足上述需求，出现了许多出色的开源代码库，以 HuggingFace 生态系统为中心，这些代码库还包括 FastChat、Axolotl 和 LLama.cpp。

本文专注于分布式训练策略的具体细节，特别是 DeepSpeed 和 FSDP，并总结了以多 GPU 和多节点训练为主的不同高效微调方法。显然，当前的趋势是，我们会使用越来越多的计算资源，因此将需要更多 GPU 来运行更大的模型。

在这种情况下，理解这些主题尤为重要，尤其是当你想要将几个 3090 家庭服务器升级到具有 8 个 A100 80GB 的 GCP 容器时，另外，这对于试图微调自己的语言模型的初创公司或企业来说也很重要。大型科技公司进行的实际大规模训练涉及大量资料，这些内容大部分来自主导了 BLOOM-176B 训练的 Stas Bekman，GPU 匮乏的用户（即 GPU-poors）关注这些资料的意义并不大。

本文从多个优秀的资源中整合了各种观点，着重讨论了 HuggingFace 生态系统的相关内容，并考虑了一些来自在线资源以及作者本人在 2023 年暑期实习中所学到的实际情况。

综上，本文希望能够回答以下问题：

1、在分布式训练和性能优化方面，我们应该关注什么？DeepSpeed 和 FSDP 在背后是如何运作的？

2、不同的分布式训练策略需要的硬件设置和注意事项？

3、各种有效的微调优化器以及可能存在的权衡？

4、一些可以覆盖所有重要微调优化的实用指南，用以在多 GPU 和多节点设置中训练大模型。

5、现在可以使用的开源代码库以及各自的优缺点？

（本文作者为加州大学圣地亚哥分校计算机科学系的硕士研究生 Sumanth R Hegde。以下内容由 OneFlow 编译发布，转载请联系授权。原文：[Everything about Distributed Training and Efficient Finetuning | Sumanth's Personal Website](https://sumanthrh.com/post/distributed-and-efficient-finetuning/)）

### 01. 分布式训练

分布式训练涵盖的范围非常广泛，因此本文无法覆盖所有内容。在训练 / 微调 LLM 时，我们通常会面对超 10 亿参数的庞大模型和大规模数据集（超 1 万亿词元的预训练数据集，超 100 万词元的监督微调数据集）。我们最终的目标是尽快完成训练，以最大化吞吐量，即希望能够每秒处理尽可能多的样本。



LLM 在训练过程中需要大量的 GPU 显存，不仅仅因为模型参数数量庞大（例如，Falcon 40B 拥有 40 亿参数，在 BF16 格式下仅模型权重就需要约 74GB 显存），还因为优化器状态所需的显存 —— 例如，使用普通的 AdamW 优化器，每个参数需要 12 个字节来存储模型权重的副本、动量和方差参数。因此，我们需要智能的分布式训练策略，以确保每个 GPU worker 只需处理部分训练状态和数据。

主要的并行策略有：

1、数据并行（Data Parallelism, DP）：每个 GPU worker 获取整个小批量数据的一部分，并在该部分数据上计算梯度。然后在所有 worker 上对梯度进行平均，以更新模型权重。在其最基本的形式中，比如 PyTorch 中的 DDP，每个 GPU 存储模型权重的副本、优化器状态以及所处理数据部分的梯度。

2、模型并行 / 垂直模型并行（MP）：在模型并行中，模型被分割成多个部分，每个部分被放置在不同的 GPU 上，这被称为垂直模型并行。举例来说，如果有一个包含 12 层的单一模型，该模型的不同层会被分别放置在 3 个不同的 GPU 上。

--------------- --------------- -----------------
1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |
--------------- --------------- -----------------






在朴素模型并行（naive model parallelism）中，所有 GPU 在处理相同的数据批次时都需要等待前一个 GPU 完成计算，然后才能处理数据。这意味着在任何给定时刻，除了一个 GPU 外，其他 GPU 实际上都处于闲置状态（因此称为「朴素」）。为改善这种情况，可以使用流水线并行（PP），这种方式通过让不同微批次数据的计算重叠，给你带来并行的错觉。这类似于计算机架构中的经典流水线。参见以下关于 GPipe 的博文：

为了在多个加速器上实现高效训练，GPipe 将模型划分到不同加速器，并自动将一个小批次的训练样本分成更小的微批次。通过在这些微批次上进行流水线式的执行，各个加速器可以进行并行计算。

图片

3. 张量并行（TP)：在张量并行中，每个 GPU 通过在 GPU worker 之间对模型进行水平切片，仅处理张量的一部分。每个 worker 处理相同的数据批次，计算他们所拥有权重部分的激活值，并交换彼此需要的部分，每个 worker 计算他们所拥有权重部分的梯度。

我们可以将上述各种并行策略结合起来，以实现更好的吞吐量增益。接下来，我们将更详细地了解两种用于数据并行训练的改进方法：零冗余优化器（Zero Redundancy Optimizer）和密切相关的全切片数据并行策略（Fully Sharded Data-Parallel strategies）。

注释：我将使用术语「GPU worker」来指代在每个 GPU 上运行的各个进程。虽然这种表述并不十分准确，但在数据并行设置中，这样说更方便，更易于理解。

拓展阅读：

多个 GPU 上的高效训练：https://huggingface.co/docs/transformers/perf_train_gpu_many

如何在多个 GPU 上训练大模型？

https://lilianweng.github.io/posts/2021-09-25-train-large/

2

ZeRO 驱动的数据并行

这是当前最高效、最热门的分布式训练策略之一。DeepSpeed 的 ZeRO 是一种数据并行处理形式，它极大提高了内存效率，其主要思想是利用数据并行训练中的内存冗余和快速 GPU 间通信的最新改进来提高吞吐量，根据不同阶段，会增加一些通信量。实际上，ZeRO 有两个组成部分：ZeRO-DP（数据并行）和 ZeRO-R（残留内存）。DeepSpeed 团队还提出了一些后续优化措施，这进一步提升了 ZeRO 的吸引力，例如 ZeRO-Offload/Infinity（将计算卸载到 CPU/NVMe 磁盘）和 ZeRO++（实现了灵活的多节点训练和量化权重）。

ZeRO-DP 的可视化图表如下（来自 DeepSpeed 的博客文章）：

图片

在 64 个 GPU 上训练 7.5B 参数模型时，模型表现如下：

1. 基准：PyTorch DDP

2. ZeRO 第一阶段 / 图片：内存减少 4 倍（特定示例），与基准的通信量相同（没有额外的 GPU 间通信）。

3. ZeRO 第二阶段 / 图片：内存减少 8 倍（特定示例），与基准的通信量相同。

4. ZeRO 第三阶段 / 图片：内存减少 64 倍（特定示例），通信量为基准的 1.5 倍（这里的 1.5 倍针对的是不同硬件设置和模型大小）。

基准

PyTorch DDP 实现了简单的数据并行性。每个 GPU worker 都有模型权重、优化器状态和梯度的副本。后向传播后，各个 worker 间的梯度会被平均化（即全局归约（all-reduce）），并且模型权重会被更新。

关于通信量的注释：为理解 ZeRO 的好处，我认为，重要的是要明确通讯量的确切含义。典型 DP 中存在一个全局归约步骤，其中每个 worker 都会发送出其所拥有的梯度数组，然后接收其他 worker 的梯度数组，以获得平均值。下文摘自 ZeRO 论文：

当前最先进的全局归约实现采用了两步操作：第一步 reduce-scatter，它在不同进程上的数据的不同部分会被归约。第二步是 all-gather，其中每个进程都在这一步中收集所有进程上的归约数据。这两步的结果就是全局归约。同时，reduce-scatter 和 all-gather 都采用了流水线方式实现，这导致每个步骤需要 Ψ 个元素（对于具有 Ψ 个元素的数据）的总数据搬运量。因此，在标准的数据并行性中，每个训练步骤会产生 2Ψ 的数据搬运量。

在这种情况下，「数据」指的是我们的梯度，而进程指的是每个 GPU 上运行的单个 worker。总而言之，我想表达的是：如果你有 Ψ 个参数，那么普通数据并行性将产生 2Ψ 通信成本。

ZeRO 第一阶段 / Pos（优化器状态分区）

在这种情况下，只有优化器状态在 GPU worker 之间进行了分区 / 分片，而模型权重和梯度在所有 worker 之间进行了复制。反向传播后，需要进行一次常规全局归约，以便在所有 worker 间获取平均梯度值。然后，每个 worker 更新其分区中的优化器状态。Adam 方程如下。

图片

图片

(w、g、v 和 m 分别对应权重、梯度、速度和动量)。需要注意的是，这些都是逐元素操作，在计算梯度后，各个权重分片之间并没有依赖关系。

通信量：首先我们会执行一次全局归约操作，将更新后的梯度通信给所有 GPU，然后，在更新各自分片的优化器状态之后，每个 GPU 仍需要从其他 GPU 获取更新后的权重。ZeRO 论文并没有清楚表明这样做是否会增加通信量。在我看来，对 ZeRO 的第 1 阶段和第 2 阶段来说，这种实现实际上是一样的：

梯度的全局归约由两部分组成：reduce-scatter 和 all-gather。

在 DeepSpeed ZeRO1 和 2 中，首先进行 reduce-scatter 操作，在不同 GPU 上分别减少梯度的不同部分。接着，每个 GPU 计算其所管理的优化器分区对应的更新后权重，之后我们只需进行一次 all-gather 操作，将更新后的模型参数传播给所有其他 GPU。这样就能在通信量不变的情况下，减少内存消耗。

ZeRO Stage 2 / 图片（优化器状态 + 梯度分区）

在这种情况下，优化器状态和梯度都被分区 / 分片到不同的 worker 上。这意味着，两个 GPU worker 在训练期间不仅要关注不同的微批次数据，还要维护模型参数子集的梯度。关键在于，每个 worker 都在更新其优化器状态的分区，因此对于一个 worker 而言，它所需的梯度（或者说，经过归约 / 平均的梯度）只是对应于该状态分区的梯度。至于实现方式，正如上面提到的，DeepSpeed 有效地执行了 reduce-scatter 操作，其中每个 worker 对应的梯度在该 worker 处被平均（而不是对所有参数进行典型的 all-reduce 操作）。这意味着，在相同通信量下节省了更多内存，也就是说，与 DDP 相比，这里没有额外的数据搬运成本。

注意：在使用 ZeRO Stage 1 和 2 时，仍然需要整个模型适配单个 GPU。此外，在使用 RAM 时，需要注意如下事项：随着进程 / GPU 数量的增加以及模型大小的扩展（超过 40 亿参数），模型初始化会占用大量 RAM。ZeRO 3 对此有所改进。

ZeRO Stage 3 / 图片（优化器状态 + 梯度 + 参数分区）

对我来说，这是 ZeRO 最有趣的阶段。除优化器状态和梯度外，第 3 阶段还跨 worker 划分了模型参数。来自 DeepSpeed 的可视化图表如下：

图片

使用 4 个 GPU 实现不同训练状态的可视化

图片

使用 DeepSpeed ZeRO-3 对训练状态分片

借用 Stas Bekman 指南中的一个例子，假设有以下 3 层模型和 4 个 GPU：

La | Lb | Lc

---|----|---

a0 | b0 | c0

a1 | b1 | c1

a2 | b2 | c2

a3 | b3 | c3

使用 DeepSpeed ZeRO 3，GPU 的配置方式如下：

GPU 0:

La | Lb | Lc

---|----|---

a0 | b0 | c0

GPU 1:

La | Lb | Lc

---|----|---

a1 | b1 | c1

GPU 2:

La | Lb | Lc

---|----|---

a2 | b2 | c2

GPU 3:

La | Lb | Lc

---|----|---

a3 | b3 | c3

在 ZeRO-3 中，模型的每一层都被水平切片，每个 worker 存储权重张量的一部分。在前向和后向传播过程中（每个 GPU worker 仍然看到不同的微批次数据），不同的 GPU worker 交换它们所拥有的每一层的部分（按需进行参数通信），并计算激活 / 梯度。

其余部分类似 ZeRO Stage 2。很容易看出，ZeRO-3 的通信量是基准 DDP 的 1.5 倍：在每个训练步骤中，我们需要在前向传播中额外进行一次模型参数的 all-gather 操作。在这个操作中移动的数据量为 Ψ（每个 GPU），因此总通信量为 Ψ（参数 all-gather）+ Ψ（梯度 reduce-scatter）+ Ψ（all-gather，用于更新的参数）= 3Ψ = 1.5 倍 DDP。考虑到内存消耗被 GPU workerN 削减，这给人留下了深刻印象。ZeRO 论文的另一个关键洞察如下：

只要有足够的设备来共享模型状态，ZeRO 就可以使 DP 适应任意大小的模型。

也就是说，只要有足够多的 GPU，在进行数据并行（DP）训练时，我们就不会再受到每个 GPU 显存的限制（说起来容易做起来难）。

ZeRO-R

我并不想深入探讨这一话题，但 ZeRO-R 在 ZeRO-DP 的基础上，通过关注激活的内存消耗和管理内存碎片化做了改进提升。ZeRO-R 通过对激活进行分区，减少了激活的内存占用。此外，它还在管理临时缓冲区方面进行了一些改进，你可以将之视为在 worker 间进行梯度累积和归约期间分配用于存储中间结果的内存。

ZeRO-Offload

ZeRO-Offload 是一种优化技术，可以将优化器和计算从 GPU 卸载到主机 CPU 上。在 2021 年 1 月发布时，ZeRO-Offload 可在 1 个 NVIDIA V100 GPU 上实现 40 TFLOPS（V100 32 GB vRAM，最大吞吐量为 130 TFLOPS），适用于 10 亿参数模型。而采用 PyTorch DDP 时，最大值为 30 TFLOPS，适用于 14 亿参数模型，在不耗尽内存的情况下，这是可以运行的最大模型。将计算卸载到 CPU 的主要问题是，以吞吐量衡量，CPU 要比 GPU 慢上多个数量级。ZeRO-Offload 采用的这种策略是，只将较少的密集计算（<<O (MB)，其中 M 是模型大小，B 是批次大小）卸载到 CPU，以使总计算复杂度保持不变（O (MB)）。在实践中，这意味着诸如范数计算（norm calculation）、权重更新等操作可以在 CPU 上完成，而前向和后向传播的矩阵乘法需要在 GPU 上完成。ZeRO-Offload 适用于 ZeRO 的所有阶段（1、2 和 3）。

这里（https://docs.it4i.cz/dgx2/introduction/）给出了用于实验的 DGX-2 节点规格，该节点配备了 16 个 V100 GPU。需要注意的是，如果处于 ZeRO-2 设置中，ZeRO-Offload 仍然会受到每个 GPU 可用内存的限制，即在每个 GPU 上容纳整个模型可能成为瓶颈。

ZeRO-Infinity

ZeRO-Infinity 是 ZeRO-Offload 的改进版本，于 2021 年 4 月推出。它允许将计算卸载到磁盘（NVMe 内存），并对 CPU 卸载进行了改进。研究表明，ZeRO-Infinity 在一个 DGX-2 节点上训练时，可以适应 10-100 万亿参数的模型（万亿级别！）。ZeRO-Infinity 通过同时利用 CPU 和 NVMe 内存来实现这一点。以下是论文中的可视化图表：

图片

以上是 ZeRO Infinity 在 4 个数据并行排布（GPU）上的截图，描述了反向传播期间的状态。分区 / 分片的参数从慢速内存（CPU+ NVMe）移动到 GPU，然后被收集起来形成完整的层。在梯度计算之后，它们被聚合、重新分区，然后卸载到慢速内存。分区 / 分片的参数从慢速内存（CPU+ NVMe）移动到 GPU，然后被收集起来形成完整的层。在梯度计算之后，它们被聚合、重新分区，然后卸载到慢速内存。

与 ZeRO-Offload 不同，ZeRO-Infinity 是基于 ZeRO-3 专门构建的。作者在 32 个 DGX-2 节点、512 个 GPU 上对模型速度进行了评估。结果表明，ZeRO-Infinity 可训练高达 20 万亿参数的模型，每个 GPU 的吞吐量可达 49 TFlops，而 3D 并行等替代并行策略可训练的模型规模要小 40 倍。ZeRO-Infinity 要想成为有竞争力的选择，需要满足一些带宽要求，即 NVMe-CPU 和 CPU-GPU 通信。

关于 ZeRO-Offload 和 ZeRO-Infinity 之间的差异，以下是 DeepSpeed 团队的评论：

DeepSpeed 首先包含了 ZeRO-Offload 的卸载功能，ZeRO-Offload 是一个用于将优化器和梯度状态卸载到 ZeRO-2 内的 CPU 内存的系统。ZeRO-Infinity 是 ZeRO-3 可以使用的下一代卸载功能。相比 ZeRO-Offload，ZeRO-Infinity 能够处理更多数据，能更有效地利用带宽，并且能更好地实现计算和通信重叠。

默认情况下，使用 ZeRO-3 进行卸载时，ZeRO-Infinity 的优化机制会自动生效；使用 Stage 1/2 进行卸载时，ZeRO-Offload 会生效。

拓展阅读：

1. ZeRO-Offload/Infinity 教程: https://www.deepspeed.ai/tutorials/zero-offload/

2. ZeRO-Offload - 十亿级模型训练大众化 : https://arxiv.org/abs/2101.06840

3. ZeRO-Infinity - 打破 GPU 内存墙，实现超大规模深度学习: https://arxiv.org/abs/2104.07857

ZeRO++

ZeRO++ 是 DeepSpeed 团队对 ZeRO-3 的最新改进。主要改进如下：

1. 量化权重（qwZ）：通过将模型权重量化为 int8，将 all-gather 参数通信量减少一半。

2. 层次划分 (hpZ)：层次划分是一种混合分区方案，适用于 DeepSpeed ZeRO 3 的多节点设置。在这种情况下，模型参数会在一个节点内分片，然后在不同节点之间复制。这意味着，与典型的 ZeRO-3 全面运行相比，我们无法节省相同数量的内存，但可以避免昂贵的节点间参数通信开销，从而提高整体吞吐量。我更倾向于 FSDP 中使用的 "混合分片（hybrid sharding）" ，而非 "层次划分"，下文讨论 FSDP 时，会再次深入讨论这个问题。

3. 量化梯度（qgZ）：通过在梯度 reduce-scatter 操作中使用 int4 量化数据替换 fp16，可以节省更大通信量。（回顾：这是发生在 ZeRO 2/3 阶段，针对分片梯度进行的梯度聚集和平均）

总体而言，与 ZeRO-3 相比，ZeRO++ 通过这三项改进将通信量减少了 4 倍。

拓展阅读：

1. ZeRO：训练万亿参数模型的内存优化: https://arxiv.org/abs/1910.02054

2. ZeRO 教程: https://www.deepspeed.ai/tutorials/zero/

3. ZeRO++：针对巨型模型训练的超高效集体通信: https://arxiv.org/abs/2306.10209

4. ZeRO++ 教程: https://www.deepspeed.ai/tutorials/zeropp/

3

全分片数据并行

全分片数据并行（FSDP）是另一种旨在提高内存效率、减少通信计算开销以提高吞吐量的数据并行技术。FSDP 的分片策略基于 Xu 等人和 ZeRO 的想法。FSDP 有两种分片策略：全分片（Full Sharding）和混合分片（Hybrid Sharding）。

全分片

这基本上与 ZeRO-3 相同，其中参数、优化器状态和梯度被分片到各个 worker 或设备上。下图来自 FSDP 博客，展示了两个设备之间不同操作的低层次可视化。

图片

正如你所看到的，每个 worker / 设备仅持有权重的子集，并且可以按需进行参数通信来计算中间激活和梯度。以下是来自 PyTorch 的 FSDP 教程：

「在前向传播中

运行 all_gather 从所有 rank 中收集全部分片，以恢复该 FSDP 单元中的完整参数

运行前向计算

丢弃刚刚收集的参数分片

在后向传播中

运行 all_gather 从所有 rank 中收集全部分片，以恢复该 FSDP 单元中的完整参数

运行后向计算

运行 reduce_scatter 以同步梯度

丢弃参数」

此处，「rank」指的是一个 GPU worker。

下图是另一篇论文中有帮助的可视化图，特别是对比了全分片和混合分片：

图片

混合分片

图片

混合分片包括分片和复制。这意味着，对于给定数量的 worker/ GPUW，分片仅在大小为 F 的子集之间发生，并在不同子集之间复制。

具体来说，假设我们想要跨 2 个节点进行多节点训练，每个节点在 GCP 中都是一个 a2-ultragpu-8g 节点。每个节点有 8 个 A100 GPU，共有 16 个 worker。那么可以使用混合分片在每个节点内部对模型参数进行分片，然后在节点之间复制。这意味着，在每个前向 / 反向传播中，每个节点内部有类似的 all-gather 和 reduce-scatter 操作，即从其他 GPU 获取模型参数（节点内部），并计算中间激活和梯度。然后，我们需要进一步在节点之间进行另一个 all-gather 操作，以获得在该训练步骤中处理的总小批量数据的平均梯度值。当我们被迫处理分片参数（即不能选择 ZeRO 2/1 时），并处于多节点设置中时，这种方法尤其有吸引力。这类似于 ZeRO++ 中的「层次划分（hierarchical partitioning）」。

4

实现

如何使用 DeepSpeed 和 FSDP？

DeepSpeed ZeRO/FSDP 的主要优势之一在于，当实际上只处于数据并行设置时，我们可以在数据 + 张量并行的情况下节省内存，提升吞吐量。这意味着，我们无需进行任何临时的架构更改，也无需通过混乱的 `.to ()` 设备转换或任何定制操作来改变前向传播过程。因此，ZeRO/FSDP 可以在不同的架构中运行，实现良好的集成效果。ZeRO 是在微软的 DeepSpeed 库中实现的，并已整合到 Accelerate 库中。而 FSDP 则是 PyTorch 本身的一部分，同样也整合到了 Accelerate 库。因此，我们可以使用 Trainer API（其后端使用了 Accelerate）中的任一策略，或者直接使用 Accelerate。

流水线并行和张量并行？

目前，流水线并行（PP）和张量（TP）并行需要进行架构更改和 / 或调整模型的前向传播过程。如果想要在 DeepSpeed 中使用流水线并行，那么则需要更改模型的架构定义。

此外，由于 Transformers 库为每个模型实现了数十种特性，这使得对其进行集成变得非常复杂。当然，我们已经可以通过 device_map="auto" 实现朴素模型并行，但在使用多个 GPU 时，这是一种非常糟糕的策略，它只在模型无法适配单个 GPU 的情况下才有意义（否则最好还是只使用单个 GPU）。如果确实需要流水线并行和张量并行，那么目前最好的选择是使用 Megatron-LM，并专注于他们支持的模型（如 BERT、GPT-2、T5、Llama）。

此外，还可以在 Megatron-DeepSpeed 中使用 ZeRO 支持的 DP、DeepSpeed PP 和 Megatron TP，但这些方法仅适用于基于 BERT、GPT-2 和 T5 的模型训练。Stas Bekman 曾试图在 HuggingFace 中实现流水线并行和张量并行，但未能成功。以下摘自 Stas Bekman 的博文：

Transformer 现状：目前的情况是，没有一个模型支持完全的流水线并行（full-PP）。GPT2 和 T5 模型具有朴素模型并行（naive MP）支持。目前的主要障碍是无法将模型转换为 nn.Sequential，并将所有输入转化为张量。因为目前模型中包含许多特性，这些特性使转换变得非常复杂，因此我们需要删除特性才能实现转换。

我们已经尝试了开源，但这个问题太过棘手，尽管已向 Stas Bekman 求助，但仍未能解决。

DeepSpeed ZeRO 和 FSDP 会继续存在吗？

我认为，由于其易用性，DeepSpeed 的 ZeRO 和 PyTorch 的 FSDP 很可能会被保留下来，或者说只会被非常相似的策略取代。在语言大模型领域，变化是唯一的不变。

随着新模型、架构、注意力实现、位置嵌入改进等的出现，能够在几小时内用一个架构替换另一个架构，并在 10M 个样本上训练 40 亿 + 参数的模型变得非常重要。即使新的 DP + PP + TP 策略最终能够实现比 ZeRO 驱动的 DP 更高的吞吐量，可能也不会有太多的采纳量。出于相似原因，我认为，DeepSpeed ZeRO 和 FSDP 会获得更多关注度，它们的吞吐量将进一步优化，甚至可能出现针对纯数据并行类别的定制计算集群配置。因此，如果我们坚持仅采用 DP，结果不会太差。

（当然，这与 OpenAI、Anthropic 等所进行的超大型语言模型训练无关）

5

高效微调

这又是一个热门话题！接下来我将简要列举一些最受欢迎的优化方法：

混合精度

对于大模型训练而言，这是一个不言而喻的选择。简而言之，权重、激活和梯度以半精度格式存储，同时你有一个 FP32 / 单精度格式的「主副本」权重。两种常用的半精度格式是 BF16（由 Google Brain 开发的「Brain Float 16」）和 FP16。FP16 需要额外的损失扩展以防止梯度下溢，而 BF16 似乎不存在这些问题。

拓展阅读：

混合精度训练：https://arxiv.org/abs/1710.03740

性能和可扩展性：如何适配更大的模型并进行更快的训练：https://huggingface.co/docs/transformers/v4.18.0/en/performance

参数高效微调（PEFT）

PEFT 旨在通过冻结大部分模型权重，将其中的子集 / 少量额外参数设置为可训练，从而在微调过程中降低内存需求。LoRA 是最流行的 PEFT，其中对模型参数进行了低秩版本的权重更新微调。

另一种有效的 PEFT 是 IA (3)，它在基于 Transformer 体系结构中的键、值和前馈层中注入可训练向量。对于 LoRA 和 IA (3)，添加的权重 / 向量可以与基础权重合并，这意味着，在推理时无需进行额外的计算（加法 / 乘法）。不足之处在于，其性能可能不及完全微调时的性能。然而，这一情况已迅速改变，实际上，只要你向所有线性层添加可训练权重，基于 LoRA 的方法可以达到完全微调的性能。

就 IA (3) 而言，在我对 GPT-2（770M）这样的小型模型进行的实验中，发现 IA (3) 可以在可训练参数的数量不到 1/10 的情况下与 LoRA 的性能相匹配。然而，这仍需要更多社区实验，尤其是在 LLama-2-7B 或 Falcon-40B 的规模上。

拓展阅读：

语言大模型的低秩调整：https://arxiv.org/abs/2106.09685

PEFT 中的 LoRA 概念指南：https://huggingface.co/docs/peft/conceptual_guides/lora

IA3 概念指南：

PEFT 中 IA3 的概念指南：https://huggingface.co/docs/peft/conceptual_guides/ia3

使用 LoRA 高效微调 T5-XXL：https://www.philschmid.de/fine-tune-flan-t5-peft

Flash Attention

Flash Attention 是一种快速（速度得到提升！）、内存高效（节省内存！）、精确（无需近似！）、IO 感知（通过考虑 GPU 内存的不同级别进行读 / 写操作！）的注意力算法。使用 Flash Attention 2（自 2023 年 7 月份推出），你可以在 A100 80GB 上获得 220 TFLOPS 以上的性能（其最高性能为 315 TFLOPS）。换句话说，当 Flash Attention 1 在 2022 年中问世时，它拥有当时可达到的最佳吞吐量，最高可达 124 TFLOPS。此外，根据我们之前研究的部分 DeepSpeed ZeRO 论文，预计在 2021 年，Tesla V100 的可实现峰值吞吐量约为 70 TFLOPS！

目前，Flash Attention 支持 Ampere、Ada 或 Hopper NVIDIA GPU（如 A100、RTX 3090、H100 等），且仅支持半精度数据类型 bf16/fp16。要在 Transformers 中使用 Flash Attention，只需对 LLama 和 Falcon 进行一处更改（将 use_flash_attention=True 传递给 AutoModel）。对于其他模型，则需手动将 forward 中使用的注意力函数改为 Flash-attention 的高吞吐量版本，不过这一点正在快速取得进展。Flash Attention 很快将集成到 PyTorch 的 scaled_dot_product_attention 中，这样就不必再依赖猴补丁（monkey patches）。（这本应在 v2.1 版本中实现，但有望在不久的将来实现）

Flash Attention v1.0 集成在 Optimum 中也已经有一段时间了，但不能使用填充词元（padding tokens），这使得它的作用非常受限。

拓展阅读：

ELI5：Flash Attention：https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad

FlashAttention-2：更快的关注力与更好的并行性和工作分区 https://tridao.me/publications/flash2/flash2.pdf

梯度 / 激活检查点

通常情况下，每次在前向传播中，所有中间激活都会被保留在内存中，因为它们在计算反向传播时是必需的。梯度 / 激活检查点（checkpointing）是一种通过仅保留部分中间激活，并根据需要重新计算其余部分来减少内存消耗的技术。其中牵涉到的权衡是额外的重新计算时间。据 HuggingFace 提供的经验法则，梯度检查点会使训练减慢约 20%。当模型层数为 N 时，激活的内存需求从 O (N) 下降到图片。当然，因为总内存中存储的不仅仅是激活，所以内存消耗可能没有那么大。

拓展阅读：

《将大型网络适配到内存中》：https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9

《性能和可扩展性：如何适配更大的模型并加速训练》：https://huggingface.co/docs/transformers/v4.18.0/en/performance

量化

量化是黑客的最爱，主要有两种量化方法，下面我将简要介绍：

后训练量化（PTQ）：旨在实现高效推断。LLM.int8 ()，GPTQ 就属于这一类。

量化感知训练：从术语的原始意义出发，这意味着从一开始就使用量化的权重和激活来训练模型，以备后续推理使用。这正是我们想要的 —— 一种使用量化参数进行训练的策略。QLoRA 属于这一类（在某种程度上，因为它将量化整合到训练过程中）。QLoRA 主要将基础预训练模型权重量化为 8/4bits，然后以浮点半精度 / 全精度训练额外的 LoRA 参数。这是一种十分强大的策略，能在单个拥有 48GB vRAM 的 GPU 上微调超过 600 亿参数的模型。当然，需要注意的是，其吞吐量会大大降低。原因很简单：每当计算给定层的激活时，都会发生额外的去量化步骤（具体数字取决于硬件设置。例如，你可能能够在 8 个 A100 上运行 DeepSpeed ZeRO 3 的 Falcon-40B 训练时不进行量化，在这里单独使用量化毫无意义，即使你可以获得更好的批次大小。不过，在只有 2 个 A100 的情况下就不一样了）。

补充信息：完整的 QLoRA 论文值得一读。他们的方法除了能在消费级 GPU 上训练 65B 参数的模型（这是当时最大的开源语言模型）之外，该论文还表示，基于 LoRA 的训练可以与完全微调相媲美（他们增加了更多可训练的层，但通过对基础权重进行量化使其比原始 LoRA 配置更高效），并且数据集的质量在监督微调中至关重要（450K FLAN 样本比 9K 高质量人工标注样本更糟糕）。

拓展阅读：

QLoRA：https://arxiv.org/abs/2305.14314

对 Transformer 模型进行量化：https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/quantization

梯度累积

梯度累积是一种提高有效批次大小的方法，尽管吞吐量会有所下降（有时甚至会下降至零）。

比如，假设你的批次大小为 2，梯度累积步长为 4。使用梯度累积，你会在每个训练步骤中进行常规的前向和后向传播，但是，你会在每 4 个训练步骤中调用一次优化器步骤（在 PyTorch 中为 optimizer.step ()）。这意味着，在 4 个步骤中会进行梯度累积，然后使用这 8 个样本的平均梯度更新模型权重。这样一来，你的批次大小增加了，这使得权重更新变得更加平稳，但内存消耗保持不变。实际上，使用多 GPU / 多节点训练的梯度累积可以增加批次大小，同时加快训练速度。因为在正常训练中，梯度在本地（由 GPU 处理的批次）被局部平均，并且每个训练步骤都需执行一次全局归约。而使用梯度累积，你可以在更大的间隔（即 gradient_accumulation_steps）上执行这一全局归约操作。减少此类全局归约可降低 worker 间的通信负担（可能还包括节点间的通信），进而提高训练速度。

拓展阅读：

使用 DeepSpeed API 进行训练：https://www.deepspeed.ai/training/

是否应该一直尝试增加批次大小？

阅读完我们讨论的所有内容之后，这是一个吸引人的问题。答案是：否！要记得，我们的目标是尽可能快地使用现有硬件，训练出可能的最优神经网络。因此，即使你只使用了可用 GPU 内存的 75%（例如在 A100 中为 60GB/80GB），也可能已经达到了系统中的最大吞吐量。这意味着，进一步增加批次大小将导致相应时延的增加，结果是吞吐量不会提高，甚至可能降低。此外，即使你有一套强大的硬件配置来训练 40B + 参数的模型，仅通过上述内存优化来增加批次大小还不够，因为这可能会影响收敛性能。因此，很难针对大模型展开一项良好的研究。

在 Transformer 时代之前有一篇论文（https://openreview.net/forum?id=H1oyRlYgg）显示，大型批次大小可能会影响泛化性能。DeepSpeed 提到，你可以找到更好的超参数 / 优化器选择，使更大型批次大小发挥作用，正如 1-cycle 学习速率调度指南（https://www.deepspeed.ai/tutorials/one-cycle/）所证明的那样。DeepSpeed 的一位作者还提出，为使大规模训练运行达到目标收敛速率，全局批次大小通常是固定的。总之，更大型的批次大小并不一定更好！

到底多大的批次大小算大？

便于参考，BLOOM-176B 预训练采用了 366B 个词元，全局批次大小为 2048。尤其是在微调阶段，目前还不清楚批次大小大到何种程度才会影响模型的收敛。

6

实践指南

总结

综上所述，以下是尝试在 1000 万以上规模的数据集上对 10-100B + 模型参数进行实验和微调的实用指南（我有 DeepSpeed 的经验，尚未使用 FSDP，所以我将重点放在这上面）：

默认情况下使用 BF16/ FP16。BF16 基本不需要其他配置参数，通常不会出现任何溢出问题（相反，FP16 可能会因不同的损失扩展因子导致不同的结果，并且由于动态范围较小，可能出现更多的溢出问题），因此非常方便。

使用 LoRA，并将可训练参数添加到所有线性层。如果你想紧密遵循 QLoRA，可在这里（https://github.com/artid‍oro/qlora/blob/7f4e95a68dc076bea9b3a413d2b512eca6d004e5/qlora.py#L248）使用他们的实用函数。

如果你 GPU 支持 Flash Attention，那么可以使用它。目前，Flash Attention 2 可在 HuggingFace 的 Llama 2 和 Falcon 上使用，其他模型可能需要进行调整。

使用梯度 / 激活检查点。这将略微降低吞吐量。如果你使用了 Flash Attention，可能就不需要梯度检查点。参考 Flash Attention 论文（另见 Tri Dao 的建议，https://github.com/EleutherAI/gpt-neox/pull/725#issuecomment-1374134498）。

图片

在你的数据加载器中使用高效的采样器，如多重包采样器（multi-pack sampler）。

如果你有多个 GPU，首先尝试使用 BF16+LoRA + 梯度检查点 + DeepSpeed ZeRO 3。

GPU 内存有限时，可使用量化。QLoRA 风格的训练目前仅适用于 DeepSpeed ZeRO 1/2。因此，即使它在模型参数方面非常高效，但在 ZeRO 1/2 中仍有参数冗余，并且吞吐量也会减少。

随着 GPU 的增加（如 8 个 V100 或 A100），DS ZeRO-3 成为了最佳选择。DS ZeRO-2 也不错，但你可能会受到 CPU 内存限制的影响（在模型初始化期间），因为模型参数会在所有工作节点上被复制。例如，如果要在具有 8 个 GPU 的节点上使用 Falcon-40B 模型，那么 CPU 内存需要超过 1.5TB，而云容器实例（AWS、GCP 等）很少有这么大的内存，所以会受到限制。当然，在家用服务器上可能不适用，因为家用服务器没有用于 GPU 间通信的 NVLink。DS ZeRO-3 具有更多的 GP 间通信，因此 NVLink 很重要。

在小规模多节点设置中，启用了层次分区（或 FSDP 的混合分片）的 DeepSpeed ZeRO-3 似乎是最佳选择。如果你有 Infiniband 互联，大多可以使用普通的 DeepSpeed ZeRO-3，并用于更大模型。

如果在上述所有优化之后仍然批次大小不足，应该使用梯度累积。对于大模型和多 GPU / 多节点设置，梯度累积可加快训练时间。

如果你的 GPU 内存非常有限，可以激活 CPU / 磁盘卸载（通过硬盘，这必须是 ZeRO-Infinity 的 NVMe）。随着 Flash Attention 2 的出现，我们需要对纯 GPU 训练和 GPU + NVMe/CPU 卸载之间的吞吐量差距进行另一项研究。我怀疑现在这个差距比以前大得多，因此只有在真正受限的情况下才进行卸载（这就是为什么这是要尝试的最后一个优化）。ZeRO-Infinity 优于 ZeRO-Offload，对于这一点你必须使用 ZeRO Stage 3。

计算有效的批次大小，并相应调整超参数。一个通用的指导原则是，随着有效批次大小的增加，要增加学习率。正如 OpenAI 的微调文档中提到的，即使对于 100B + 的模型，这似乎仍然成立。

最后，在开始训练时，使用 htop 监控 RAM 使用情况（有时会出现 RAM OOM 问题），同时使用 nvidia-smi 确保 GPU 没有被数据预处理拖慢（你应该争取接近 100% 的 GPU 利用率，即使 GPU 内存使用较少）。

关于超参数的更多内容：以下为学习率缩放的相关评论。事实证明，在预训练过程中，随着模型的增大（比如 100B+），即使使用了更大的批次大小，通常来说，我们还是应该降低而非增加学习率，这一点可以在 OpenAI 的 GPT-3 论文和 BLOOM 论文中得到证实。要建立对这种模式的直觉十分困难，因此我们只能采取非常实证的方法，如果实验结果与之相悖，就迅速更新所有先验知识。

补充指南

在 Transformers 的文档中提供了许多关于性能和可扩展性的有用技巧。如果你关心在家搭建和管理服务器时可能遇到的问题，或者 NVLink 的具体作用，又或者想深入了解内存管理，这些内容都非常有价值。Stas Bekman 系列（https://github.com/stas00/ml-engineering）收录了许多与调试和性能相关的实用提示，可能会有所帮助。

7

关于 DeepSpeed 和 FSDP 的更多内容

使用 DeepSpeed ZeRO-3 进行多节点训练

对于普通 ZeRO-3，你需要确保跨节点的参数通信不会成为一个巨大的瓶颈，否则，即便使用两个节点，吞吐量的提升可能也非常有限。通过 Stas Bekman 的调查，我们对需要具备怎样的跨节点网络有了清晰的认识：

对于大小为 M 的模型，有 N 个节点，每个节点有 G 个 GPU，每个节点在每个训练步骤中发送约 48*M/N bits 的数据。即便是对于两个 8xA100 节点，配备一个 40B 参数模型，每个节点在每个训练步骤中通信的数据量就达到了 120GB！对于大规模训练（64GPU+），确实需要具备 1000Gbps 的 InfiniBand 互连。对于较小规模的多节点训练，可选择 100-400Gbps 的带宽。在亚马逊 EC2 P4d 实例（配备了 8 个 A100，用于机器学习训练的节点）上，通常使用弹性网络适配器（Elastic Fabric Adapter，EFA）作为跨节点的网络接口，根据规格，你可以获得高达 400Gbps 的网络带宽，这相当不错！实际上，你获得的确切带宽约为 340Mbps，因此应该规划使用规格中列出的最大带宽的 80-85%。用于 AWS P5 实例的 EFA v2 配备了 H100，速度快了整整 8 倍。

要测算你所获得的跨节点网络带宽，可使用 Stas Bekman 的实用函 (https://github.com/stas00/ml-engineering/blob/9a51114f8377350bfbf1764f23feac441e865401/multi-node/all_reduce_bench.py）。这个函数专门用于对所有归约操作进行基准测试，因此能够准确测试训练过程中所能看到的情况。

建议：在没有 Infiniband 的情况下，最好使用 ZeRO++ 和层次分区（hpZ）。要实现这一点，你需要将 zero_hpz_partition_size 配置参数设置为每个节点的 GPU 数量 /rank。例如，如果你使用两个节点进行训练，每个节点有 8 个 A100 GPU，那么 zero_hpz_partition_size 将设置为 8。

Accelerate/Transformer 支持：目前尚不清楚加速 DeepSpeed 集成是否支持层次分区（hpZ）。表面上看，应该支持，因为加速应该整合 DeepSpeed ZeRO 的所有功能，而 hpZ 只是 DeepSpeed 配置文件中的一个参数更改。我已经提了一个 issue（2023 年 10 月 1 日），如有需要，我会更新这篇帖子。

对于 FSDP，你将使用混合分片策略（HYBRID_SHARD）。对于多节点训练（至少是小规模的，没有 Infiniband 的情况下），这似乎是一个不错的选择，但有用户通过艰难的方式发现了这一点。Accelerate 已经支持带有混合分片的 FSDP。

DeepSpeed 内存需求

当你拥有新的基础架构设置，并希望尝试 DeepSpeed 时，绝对应该使用 DeepSpeed 的内存估算器。

DeepSpeed ZeRO 1/2

deepspeed.runtime.zero.stage_1_and_2.estimate_zero2_model_states_mem_needs_all_live(model,

num_gpus_per_node=1, num_nodes=1, additional_buffer_factor=1.5)

使用默认缓冲因子（这是一个简单地扩展所有 CPU 和 GPU 内存估算的估算因子），对于一个拥有 3B 参数的模型，在拥有 1 个节点和 8 个 GPU 的环境中，你将得到以下结果，来自于 DeepSpeed 文档：

python -c 'from transformers import AutoModel; \

from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live; \

model = AutoModel.from_pretrained("t5-3b"); \

estimate_zero2_model_states_mem_needs_all_live(model, num_gpus_per_node=8, num_nodes=1)'

Estimated memory needed for params, optim states and gradients for a:

HW: Setup with 1 node, 8 GPUs per node.

SW: Model with 2851M total params.

per CPU | per GPU |  Options

127.48GB |  5.31GB | offload_optimizer=cpu

127.48GB | 15.93GB | offload_optimizer=none

以 Falcon-40b 为例，使用 ZeRO 1/2 所需的 CPU RAM 大于 1.5TB。你无需实际运行训练 / 微调代码来测试这一点，只需使用上述模型和硬件设置运行上述命令，就会得到一个估算。对于 FSDP，我没有找到类似的内存估算器，因此可直接使用 DeepSpeed ZeRO-3 对 FSDP 的全分片估算。

注意：再补充一点关于 CPU RAM 不足的情况。这可能很难调试，因为你的进程将在日志中没有任何信息的情况下失败。添加 PyTorch 分布式调试标志（NCCL_DEBUG=INFO, TORCH_DISTRIBUTED_DEBUG=INFO ）也无济于事，因为这是一个 RAM 问题。我只是在初始训练阶段通过监视 htop（如前文所述）发现了这一困难。

DeepSpeed ZeRO 3

python deepspeed.runtime.zero.stage3.estimate_zero3_model_states_mem_needs_all_live(model, \

num_gpus_per_node=1, num_nodes=1, additional_buffer_factor=1.5)

在一个节点上，每个节点有 8 个 GPU 的硬件设置下，对于一个包含 3B 参数的模型，使用以下命令可以估算参数、优化状态和梯度所需的内存：

python -c 'from transformers import AutoModel; \

from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \

model = AutoModel.from_pretrained("t5-3b"); \

estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=8, num_nodes=1)'

Estimated memory needed for params, optim states and gradients for a:

HW: Setup with 1 node, 8 GPUs per node.

SW: Model with 2851M total params, 32M largest layer params.

per CPU | per GPU |  Options

71.71GB |  0.12GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1

127.48GB |  0.12GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0

63.74GB |  0.79GB | offload_param=none, offload_optimizer=cpu , zero_init=1

127.48GB |  0.79GB | offload_param=none, offload_optimizer=cpu , zero_init=0

1.47GB |  6.10GB | offload_param=none, offload_optimizer=none, zero_init=1

127.48GB |  6.10GB | offload_param=none, offload_optimizer=none, zero_init=0

这也让你对不同的卸载策略和初始化有了一个概念！使用 zero_init=1 时，模型权重将以一种内存可扩展的方式初始化，一旦分配后，权重将立即在你的 worker 之间分区。当 zero_init=0 时，CPU RAM 需求可能会飙升（对于 10B + 模型和多个 GPU 来说可能会达到 TB 级别），因此你绝对应该使用 zero_init=1。在多 GPU 设置中使用 CPU 卸载也会增加大量 RAM 的需求，并且可能导致 RAM OOM。使用估算器，之后如果确实需要，如 CPU RAM 不足的话，可以切换到 NVMe 卸载。

使用 Accelerate

Accelerate 旨在提供一个统一的界面，用于启动各种分布式训练运行，同时为你提供在纯 PyTorch 中编写代码的灵活性。目前看来，如果你打算在 FSDP 和 DeepSpeed 之间切换同一段代码，可能会有一些注意事项。例如，对于 FSDP，你必须在实例化优化器之前调用 accelerator.prepare (model)。我不确定相同的方法是否适用于 DeepSpeed（对于 DeepSpeed，你可以对一切进行.prepare () 调用）。还有一些其他注意事项，这里就不展开了，但可以查看以下 Accelerate 的文档。

拓展阅读：

1. 使用 Accelerate 进行 FSDP：https://huggingface.co/docs/accelerate/usage_guides/fsdp

2. 使用 PyTorch FSDP 对 Llama 2 70B 进行微调：https://huggingface.co/blog/ram-efficient-pytorch-fsdp

8

开源代码库

短短几个月内，开源代码库已取得了长足进步。在这里，我主要想回答「如果我想立即开始使用开源代码库进行微调，可以使用什么？应该牢记什么？」等问题。如往常一样，细节决定成败。接下来我将总结 FastChat 和 Axolotl 的平台功能，这是两个最实用、最受欢迎的平台。

FastChat

FastChat 是一个用于微调、服务和评估 LLM-based chatbots from LMSys 的平台。其功能包括：

Serving

你可以使用 FastChat 为 Llama、Falcon、WizardLM 等模型提供服务。（支持的模型列表）添加新模型进行推理 / 服务似乎非常简单，并支持因果模型（如 Llama）和序列到序列模型（如 T5）。他们还支持使用 CPU 卸载和不同的量化方案等服务。在幕后，FastChat 使用了出色的 vLLM 库进行高效推理，这也是另一个 LMSys 开源项目。

微调

支持微调的模型有 Llama、T5 和 Baichuan。如有错误，烦请指正。主要的微调脚本仅适用于 Llama 模型，并有针对 Llama 特定的魔法数字。此外，还有针对 T5 和 Baichuan 模型的额外微调脚本。Falcon 模型的训练支持仍有待解决。

专门针对（单轮 / 多轮）对话数据监督微调，以训练聊天机器人。在类似 FLAN 的数据集上，用（instruction, response）对进行指令微调是有可能的（单轮对话），但不可能混合因果语言建模数据集等其他数据集格式，或者在多个数据集上进行训练。

支持 LoRA 和基于 QLoRA 的训练。提供的 DeepSpeed 配置是参考配置，适用于低资源环境（例如，1 个 V100 32GB 用于微调 Llama-13B），因此会默认启动 CPU 卸载。请确保根据你的硬件设置进行修改。更多参考信息请参阅训练文档。

FastChat 使用 Trainer API，并且与几乎所有可用的开源训练包一样，在训练和评估中仅支持单一的同质数据集（在这种情况下，还必须是对话数据）。这意味着，你可以看到组合数据集的训练损失和评估损失，但在监控运行时不会看到其他信息。

评估

FastChat 的评估包以 MT-bench 为基础，是一个基于多轮对话的评估数据集。在评分方面，他们使用 LLM 作为评委，可以利用 GPT-3.5 或 GPT-4 等更强大的语言模型对模型输出评分。

Axolotl

Axolotl 是一个用于微调语言模型的大规模开源项目，具有以下显著特点：

支持 Llama、MPT、Falcon 等各种因果语言模型。目前尚不支持序列到序列模型。

具有多种格式，支持在多个数据集上训练。在我看来，对 Axolotl 来说，这是一个极其重要的功能。数据集格式的完整列表太过庞大，但你可以在类似 FLAN 的指令微调数据集、类似 ShareGPT 的基于对话的数据集以及简单的基于文本补全的数据集上进行训练（用于因果语言建模的纯文本 / 代码）。Axolotl 还支持预标记数据集。

支持 LoRA、QLoRA，以及多包采样器，该采样器将相似长度的序列打包在一起，以避免在填充词元上浪费计算。

Axolotl 还使用 Trainer API，并具有许多用于自定义评估和记录的功能。你可以在 MMLU 上进行评估，或者在本地基准数据集上记录训练期间的损失 / 准确性。

此外，Axolotl 还支持 FSDP 和 DeepSpeed，主要是因为它们只需让 Trainer 处理这一部分。Flash-Attention 也适用于 Llama、BTLM 和新的 Mistral 等其他模型。

因为在 Trainer 上定制起来比较困难，所以 Axolotl 不支持对不同任务（如 MMLU 和 MATH）进行评估，不能分开可视化损失曲线。

对此，我有一个小小的疑问，除 prediction_step 和 training_step 等方法之外，Axolotl 最终几乎定制了 Trainer 的每个部分。如果他们已经可以使用 torch.distributed.gather ，我想知道为什么不直接从一开始就用纯 PyTorch + Accelerate 编写所有代码，这样本就可以避免冗余。Trainer 还提供了十几种优化器以供选择，但监督微调只需要其中的一小部分。

实用的微调指南

HuggingFace 近期发布的几篇值得阅读的微调指南：

在 2 个 8xA100-ultra-80GB 节点上使用 PEFT 和 FSDP 对 Llama 2 70B 进行微调：https://huggingface.co/blog/ram-efficient-pytorch-fsdp

在 2 个 8X100-ultra-80GB 节点上使 PEFT 和 DeepSpeed 对 Falcon 180B 进行微调：https://medium.com/@sourabmangrulkar/falcon-180b-finetuning-using-peft-and-deepspeed-b92643091d99

在 1 个 8xA100-ultra-80GB 节点上使用 PEFT 和 DeepSpeed 对 Falcon 180B 进行微调：https://www.philschmid.de/deepspeed-lora-flash-attention

9

超大型模型训练

首先，在搭载 8 枚 A100 显卡的单一节点上，我们可以训练多大的模型。通过上述优化措施，目前我们可以在搭载 8 枚 A100-80GB 显卡的 DGX 节点上微调 Falcon 180B。根据 HuggingFace 的指南（https://www.philschmid.de/deepspeed-lora-flash-attention），你可以在 153 分钟内，使用 15000 个样本，在 dolly 数据集上对一个包含 180 亿参数的模型进行 3 轮微调，其中有效批次大小为 64，序列长度为 2048 个词元。由于文本块是作为预处理步骤进行的，因此不知道准确的吞吐量数值，当然我们也需要估算每个训练步骤所需的时间（因为总训练时间包括了评估和检查点所需的时间），但每秒吞吐量可能大于 5 个样本，对于 180B 参数的模型来说，这是相当不错的表现。

如果想了解更多关于管理大规模训练运行的信息，可以向 Stas Bekman 寻求帮助。前文已经提到过，但我还是再次重申：开源社区要感谢 Stas Bekman 的地方很多，其中包括他对大规模训练的出色论述。大规模训练的调试、监控、深入研究大规模多节点网络设置等内容都可以在他的合集中找到。不过其中一些关于分布式训练的资料和建议可能有点过时。

其他人都在看

GPU 架构与计算入门指南

为什么开源大模型终将胜出

LoRA 和 QLoRA 微调语言大模型

可复现的语言大模型推理性能指标

ChatGPT 规模化服务的经验与教训

微调语言大模型选 LoRA 还是全参数

开源语言大模型演进史：向 LLaMA2 看齐

试用 OneFlow: github.com/Oneflow-Inc/oneflow/