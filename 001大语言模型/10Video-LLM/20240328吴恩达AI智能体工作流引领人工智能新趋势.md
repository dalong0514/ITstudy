## 20240614吴恩达-AI智能体工作流引领人工智能新趋势

[吴恩达：AI 智能体工作流引领人工智能新趋势 [译] | 宝玉的分享](https://baoyu.io/translations/transcript/whats-next-for-ai-agentic-workflows)

[What's next for AI agentic workflows ft. Andrew Ng of AI Fund - YouTube](https://www.youtube.com/watch?v=sal78ACtGTc)

### 原文

All of you know Andrew Ng as a famous computer science professor at Stanford. He was really early on in the development of neural networks with GPUs. Of course, he is the creator of Coursera and popular courses like deeplearning.ai. Also, he was the founder, creator, and early lead of Google Brain. But one thing I've always wanted to ask you, Andrew, before I hand it over to you while you're on stage, is a question I think would be relevant to the whole audience. 10 years ago, on problem set number two of CS229, you gave me a B. And I was wondering, I looked it over, and I was wondering what you saw that I did incorrectly. So anyway, Andrew, over to you.

Thank you, Hanxin. I'm looking forward to sharing with all of you what I'm seeing with AI agents, which I think is an exciting trend that everyone building in AI should pay attention to. And I'm also excited about all the other What's Next presentations.

Today, the way most of us use large language models is like this, with a non-agentic workflow, where you type a prompt and generate an answer. That's a bit like if you ask a person to write an essay on a topic, and I say, "Please sit down at the keyboard and just type the essay from start to finish without ever using backspace." Despite how hard this is, LLMs do it remarkably well.

In contrast, with an agentic workflow, this is what it may look like: Have an AI, have an LLM, say "Write an essay outline. Do you need to do any web research? If so, let's do that. Then write the first draft and read your own first draft and think about what parts need revision. And then revise your draft and think about what parts need revision, and then revise your draft," and you go on and on. So this workflow is much more iterative where you may have the LLM do some thinking, revise this article, do some more thinking, and iterate this through a number of times. What not many people appreciate is that there are remarkably better results. I've actually really surprised myself working with these agent workflows, how well they work.

I was doing one case study and my team analyzed some data using a coding benchmark called the HumanEval benchmark released by OpenAI a few years ago. This has coding problems like "Given a non-empty list of integers, return the sum of all the odd elements that are in even positions." And it turns out the answer is a code snippet like that.

Today, a lot of us will use zero-shot prompting, meaning we tell the AI, "Write the code and have it run on the first pass." Who codes like that? No human codes like that. We just type out the code and run it. Maybe you do. I can't do that.

It turns out that if you use GPT-3.5 with zero-shot prompting, it gets it 48 percent right. GPT-4, way better, 67 percent right. But if you take an agentic workflow and wrap it around GPT-3.5, it actually does better than even GPT-4. And if you were to wrap this type of workflow around GPT-4, you know, it also does very well. You'll notice that GPT-3.5 with an agentic workflow actually outperforms GPT-4. I think this has significant consequences for how we all approach building applications.

"Agents" is a term that's been tossed around a lot. There are a lot of consultant reports about how agents are the future of AI, blah, blah, blah. I want to be a bit concrete and share with you the broad design patterns I'm seeing in agents. It's a very messy, chaotic space with tons of research and open source. There's a lot going on, but I'll try to categorize a bit more concretely what's going on in agents.

Reflection is a tool that I think many of us should just use. It just works. I think it's more widely appreciated, but it actually works pretty well. I think of these as pretty robust technologies. When I use them, I can almost always get them to work well.

Planning and multi-agent collaboration, I think, are more emerging. When I use them, sometimes my mind is blown by how well they work. But at least at this moment in time, I don't feel like I can always get them to work reliably.

So let me walk through these four design patterns in a few slides. If some of you go back and ask your engineers to use these, I think you'll get a productivity boost quite quickly.

Reflection, here's an example. Let's say I ask a system, "Please write code for me for a given task." Then we have a coder agent, just an LLM that you prompt to write code to say, "Yo, dev, do tasks, write a function like that." An example of self-reflection would be if you then prompt the LLM with something like this: "Here's code intended for a task," and just give it back the exact same code that it just generated. And then say, "Check the code carefully for correctness, sound efficiency, good construction algorithms." Just write a prompt like that. It turns out the same LLM that you prompted to write the code may be able to spot problems like "This bug in line 5, we fix it by blah, blah, blah." If you now take its own feedback and give it to it and re-prompt it, it may come up with a version 2 of the code that could well work better than the first version. Not guaranteed, but it works often enough for this to be worth trying for a lot of applications.

To foreshadow tool use, if you let it run unit tests, if it fails a unit test, then why did it fail the unit test? Have that conversation and maybe it'll figure out, "Failed the unit test, so you should try changing something," and come up with v3.

By the way, for those of you that want to learn more about these technologies, I'm very excited about them. For each of the four sections, I have a little recommended reading section at the bottom that hopefully gives more references.

And again, just to foreshadow multi-agent systems, I've described a single coder agent that you prompt to have this conversation with yourself. One natural evolution of this idea is instead of a single coder agent, you can have two agents where one is a coder agent and the second is a critic agent. These could be the same base LLM model, but you prompt them in different ways. We say to one, "You're an expert coder, write code," and to the other one, "You're an expert code reviewer, so review this code." This type of workflow is actually pretty easy to implement. I think it's such a very general purpose technology for a lot of workflows. This would give you a significant boost in the performance of LLMs.

The second design pattern is tool use. Many of you will have already seen LLM-based systems using tools. On the left is a screenshot from Copilot. On the right is something that I kind of extracted from GPT-4. But you know, LLMs today, if you ask them "What's the best coffee maker?" they will web search for some problems. LLMs will generate code and run code. It turns out that there are a lot of different tools that many different people are using for analysis, for gathering information, for taking actions, for personal productivity.

It turns out a lot of the early work in tool use originated from the computer vision community because before large language models, LLMs, they couldn't do anything with images. So the only option was to have the LLM generate a function call that could manipulate an image, like generate an image or do object detection or whatever. So if you actually look at literature, it's been interesting how much of the work in tool use seems like it originated from vision because LMs were blind to images before GPT-4, DALL-E, and so on.

That's tool use and it expands what an LM can do.

And then planning, you know, for those who have not yet played a lot with planning algorithms, I feel like a lot of people talk about the ChatGPT moment where you're like, "Wow, never seen anything like this." I think if you've not used planning algorithms, many people will have a kind of an AI agent "wow" moment where you think, "I couldn't imagine an AI agent doing this." I've run live demos where something failed and the AI agent rerouted around the failures. I've actually had quite a few of those moments where I'm like, "Wow, can't believe my AI system just did that autonomously."

But one example that I adapted from a HuggingFace GPT paper, you say, "Please generate an image where a girl is reading a book, and the pose is the same as the boy in the image, example.jpg, and please describe the new image with your voice." So given an example like this, today with AI agents, you can kind of decide, "First thing I need to do is determine the pose of the boy, then find the right model, maybe on Hugging Face, to extract the pose. Then next, need to find a pose-image model to synthesize a picture of a girl following the instructions, then use image-to-text, and then finally use text-to-speech." And today we actually have agents that, I don't want to say they work reliably, you know, they're kind of finicky, they don't always work, but when it works, it's actually pretty amazing. But with agentic loops, sometimes you can recover from earlier failures as well.

So I find myself already using research agents for some of my work where I'll want a piece of research, but I don't feel like Googling myself and spending a long time. I'll send it to a research agent, come back in a few minutes and see what it's come up with. And it sometimes works, sometimes doesn't, right? But that's already a part of my personal workflow.

The final design pattern, multi-agent collaboration. This is one of those funny things, but it works much better than you might think. On the left is a screenshot from a paper called CHAT-DEV, which is actually open source. Many of you saw the flashy social media announcements and demo of CHAT-DEV. CHAT-DEV is actually open source. It runs on my laptop. And what CHAT-DEV does is an example of a multi-agent system where you prompt one LLM to sometimes act like the CEO of a software engineering company, sometimes act like a designer, sometimes act like a product manager, sometimes act like a tester. And this flock of agents that you build by prompting an LLM, telling them, "You are now a CEO, you are now a software engineer," they collaborate, have an extended conversation, so that if you tell it, "Please develop a game, develop a GoMoku game," they will actually spend a few minutes writing code, testing it, iterating, and then generate surprisingly complex programs. It doesn't always work. I've used it, sometimes it doesn't work, sometimes it's amazing, but this technology is really getting better.

[[2307.07924] ChatDev: Communicative Agents for Software Development](https://arxiv.org/abs/2307.07924)

[[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)

And just one other design pattern, it turns out that multi-agent debate where you have different agents, for example, it could be having ChatGPT and LLaMA debate each other. That actually results in better performance as well. So having multiple simulated AI agents work together has been a powerful design pattern as well.

So just to summarize, I think these are the patterns I've seen. And I think that if we were to use these patterns in our work, a lot of us can get a productivity boost quite quickly. I think that agentic reasoning design patterns are going to be important.

This is my last slide. I expect that the set of tasks AI can do will expand dramatically this year because of agentic workflows. And one thing that is actually difficult for people to get used to is when we prompt an LLM, we want a response right away. In fact, a decade ago when I was having discussions at Google on what we call the "big box search," typing in a long prompt, one of the reasons I failed to push successfully for that was because when you do a web search, you want a response back in half a second. That's just human nature. We like that instant feedback. But for a lot of the agent workflows, I think we'll need to learn to delegate a task to an AI agent and patiently wait minutes, maybe even hours for a response.

But just like I've seen a lot of novice managers delegate something to someone and they check in five minutes later, right? And that's not productive. I think we need to, it is really difficult, we need to do that with some of our AI agents as well.

I saw, I saw a sun glass.

And then one of the important trends, fast token generation is important because with these agentic workflows, we're iterating over and over. So the LM is generating tokens for the LM to read. So being able to generate tokens way faster than any human to read is fantastic. And I think that generating more tokens really quickly from even a slightly lower quality LM might give good results compared to slower tokens from a better LLM, maybe. It's a little bit controversial because it may let you go around this loop a lot more times, kind of like the results I showed with GPT-3 and an agent architecture on the first slide.

And candidly, I'm really looking forward to PaLM 2, GPT-5, Gemini 2.0, and all these other wonderful models that are being built. And part of me feels like if you're looking forward to running your thing on GPT-5 zero-shot, you may be able to get closer to that level of performance on some applications than you might think with agentic reasoning, but on an earlier model. I think this is an important trend. And honestly, the path to AGI feels like a journey rather than a destination.

1 Reflection

[[2303.17651] Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)

[[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)

2 Tool use

[[2305.15334] Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)

[[2303.11381] MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381)

3 Planning

[[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

[[2303.17580] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/abs/2303.17580)

4 Multi-agent collaboration

[[2307.07924] ChatDev: Communicative Agents for Software Development](https://arxiv.org/abs/2307.07924)

[[2308.08155] AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)

### 宝玉的翻译

Translated on March 28, 2024

Published on March 26, 2024

我期待与大家分享我在 AI 智能体方面的发现，我认为这是一个令人兴奋的趋势，所有涉及 AI 开发的人都应该关注。同时，我也对所有即将介绍的「未来趋势」充满期待。

所以，让我们来谈谈 AI 智能体。现在，我们大多数人使用大语言模型的方式就像这样，通过一个无智能体的工作流程，我们输入一段提示词，然后生成一段答案。这有点像你让一个人编写一篇关于某个主题的文章，我说你只需要坐在键盘前，一气呵成地把文章打出来，就像不允许使用退格键一样。尽管这项任务非常困难，但大语言模型的表现却令人惊讶的好。

与此相对，一个有 AI 智能体的工作流可能是这样的。让 AI 或者大语言模型写一篇文章的提纲。需要在网上查找一些东西吗？如果需要，那就去查。然后写出初稿，并阅读你自己写的初稿，思考哪些部分需要修改。然后修改你的初稿，然后继续前进。所以这个工作流是迭代的，你可能会让大语言模型进行一些思考，然后修改文章，再进行一些思考，如此反复。很少有人意识到，这种方式的结果更好。这些 AI 智能体的工作流程的效果让我自己都感到惊讶。

我要做一个案例研究。我的团队分析了一些数据，用的是一个名为「人类评估基准」的编程基准，这是 OpenAI 几年前发布的。这个基准包含一些编程问题，比如给出一个非空的整数列表，求出所有奇数元素或者奇数位置上的元素之和。答案可能是这样一段代码片段。现在，我们很多人会使用零样本提示，意思是我们告诉 AI 写代码，然后让它一次就运行。谁会这样编程？没有人会这样。我们只是写下代码然后运行它。也许你会这样做。我做不到。

所以事实上，如果你使用 GPT 3.5 进行零样本提示，它的正确率是 48%。GPT-4 的表现要好得多，正确率是 67%。但是，如果你在 GPT 3.5 的基础上建立一个 AI 智能体的工作流，它甚至能比 GPT-4 做得更好。如果你将这种工作流应用于 GPT-4，效果也非常好。你会注意到，带有 AI 智能体工作流的 GPT 3.5 实际上优于 GPT-4。这意味着这将对我们构建应用程序的方式产生重大影响。

AI 智能体这个术语被广泛讨论，有很多咨询报告讨论关于 AI 智能体，AI 的未来等等。我想更实质性地与你分享我在 AI 智能体中看到的一些常见设计模式。这是一个复杂混乱的领域，有大量的研究，大量的开源项目。有很多东西正在进行。但我试图更贴切地概述 AI 智能体中的现状。

反思是我认为我们大多数人应该使用的一个工具。它确实很有效。我认为它应该得到更广泛的应用。这确实是一种非常稳健的技术。当我使用它们时，我总能让它们正常工作。至于规划和多智能体协作，我认为它是一个新兴的领域。当我使用它们时，有时我会对它们的效果感到惊讶。但至少在此刻，我不能确定我总是能让它们稳定运行。所以让我在接下来的几页幻灯片中详细介绍这四种设计模式。如果你们中有人回去并亲自尝试，或者让你们的工程师使用这些模式，我认为你会很快看到生产力的提升。

所以，关于反思，这是一个例子。比如说，我要求一个系统为我编写一项任务的代码。然后我们有一个编程智能体，只需给它一个编码任务的提示，比如说，定义一个执行任务的函数，编写一个这样的函数。一个自我反思的例子就是，你可以这样对大语言模型进行提示。这是一段为某个任务编写的代码。然后把它刚生成的完全一样的代码再呈现给它。然后让它仔细检查这段代码是否正确、高效且结构良好，像这样提出问题。结果显示，你之前提示编写代码的同一大语言模型可能能够发现像第五行的 bug 这样的问题，并修复它。等等。如果你现在把它自己的反馈再次呈现给它，它可能会创作出版本二的代码，这个版本可能比第一个版本表现得更好。虽然不能保证，但是在大多数情况下，这种方法在许多应用中值得尝试。提前透露一下，如果你让它运行单元测试，如果它没有通过单元测试，那么你可以询问它为什么没有通过单元测试？进行这样的对话，也许我们可以找出原因，没能通过单元测试，所以你应该尝试改变一些东西，然后生成 V3 版本的代码。顺便说一句，对于那些想要了解更多关于这些技术的人，我对这些技术感到非常兴奋。对于讲解的每个部分，我都在底部附有一些推荐阅读的资料，希望能提供更多的参考。

再次预告一下多智能体系统，我描述的是一个编程智能体，你可以提示它和自己进行这样的对话。这个想法的一个自然演变就是，不只有一个编程智能体，你可以设定两个智能体，一个是编程智能体，另一个是评审智能体。这些都可能基于同一款大语言模型，只是我们提供的提示方式不同。我们对一方说，你是编程专家，请写代码。对另一方我们会说，你是代码审查专家，请审查这段代码。实际上，这样的工作流程非常便于实施。我认为这是一种非常通用的技术，能够适应各种工作流程。这将显著提升大语言模型的性能。

第二种设计模式是使用工具。你们中的许多人可能已经看到过基于大语言模型的系统如何使用工具。左边是来自副驾驶的截图，右边是我从 GPT-4 中提取的部分内容。然而，如果你让今天的大语言模型去回答网页搜索中哪款复印机最好这样的问题，它会生成并运行代码。实际上，有很多不同的工具，被许多人用来进行分析，收集信息，采取行动，提高个人效率。

早期在工具使用方面的研究，大部分来自计算机视觉社区。因为在大语言模型出现之前，它们无法处理图像。所以，唯一的选择就是让大语言模型生成一个可以操作图像的函数，比如生成图像或者进行物体检测等。因此，如果你仔细研究相关文献，你会发现很多工具使用的研究看似起源于视觉领域，因为在 GPT-4 和 LLaVA 等出现之前，大语言模型对图像一无所知。这就是工具的使用，它扩大了大语言模型的应用范围。

接下来是规划。对于那些还未深入研究规划算法的人，我觉得很多人都会谈到 ChatGPT 的震撼时刻，那种前所未有的感觉。我觉得你们可能还没有使用过规划算法。有很多人会感叹，哇，我没想到 AI 智能体能做得这么好。我曾经进行过现场演示，当某件事情失败了，AI 智能体会重新规划路径来规避失败。事实上，已经有好几次我被自己的 AI 系统的自主能力所震惊了。

我曾经从一篇关于 GPT 模型的论文中改编过一个例子，你可以让它生成一张女孩正在读书的图片，与图片中的男孩姿势一致，例如，example.jpeg，然后它会描述新图片中的男孩。利用现有的 AI 智能体，你可以决定首先确定男孩的姿势，然后找到合适的模型，可能在 HuggingFace 这个平台上，来提取姿势。接下来，你需要找到一个后处理图像的模型，合成一张根据指令的女孩的图片，然后使用图片转化为文本，最后使用文本转化为语音的技术。

目前，我们有一些 AI 智能体，虽然它们并不总是可靠，有时候会有些繁琐，不一定能成功，但是一旦它们成功了，效果是相当惊人的。有了这种智能体循环的设计，有时候我们甚至可以从之前的失败中恢复过来。我发现我已经开始在一些工作中使用这样的研究型智能体，我需要一些研究，但是我并不想自己去搜索，花费大量的时间。我会将任务交给研究型智能体，过一会儿再回来看它找到了什么。有时候它能找到有效的结果，有时候则不行。但无论如何，这已经成为我个人工作流程的一部分了。

最后一个设计模式是多智能体协作。这个模式可能看起来有些奇怪，但实际效果比你想象的要好得多。左边是一篇名为 "Chat Dev" 的论文的截图，这个项目是完全开放的，实际上已经开源了。许多人可能见过那些炫耀的社交媒体发布的 "Devin" 的演示，在我的笔记本电脑上也可以运行 "Chat Dev"。"Chat Dev" 是一个多智能体系统的例子，你可以设置一个大语言模型（LLM）去扮演软件工程公司的 CEO、设计师、产品经理，或者测试员等角色。你只需要告诉 LLM，你现在是 CEO，你现在是软件工程师，然后它们就会开始协作，进行深入的对话。如果你告诉它们去开发一个游戏，比如 GoMoki 游戏，它们会花几分钟来编写代码，测试，迭代，然后生成出惊人的复杂程序。虽然并不总是成功，我也遇到过失败的情况，但有时它的表现让人惊叹，而且这个技术正在不断进步。另外，另一种设计模式是让不同的智能体辩论，你可以有多个不同的智能体，比如 ChatGPT 和 Gemini 进行辩论，也是一种有效提升性能的模式。所以，让多个模拟的 AI 智能体协同工作，已经被证明是一个非常强大的设计模式。

总的来说，这些就是我观察到的设计模式，我认为如果我们能在工作中应用这些模式，我们可以更快地提升 AI 效果。我相信智能体推理设计模式将会是一个重要的发展方向。

这是我的最后一张幻灯片。我预计，人工智能能做的任务将在今年大幅度扩展，这是由于智能体工作流的影响。有一点人们可能难以接受的是，当我们向 LLM 发送提示词时，我们希望马上得到回应。实际上，十年前我在谷歌进行的一项名为「大盒子搜索」的讨论中，我们输入很长的提示词。我当时未能成功推动这一点，因为当你进行网络搜索时，你希望在半秒钟内得到回应，这是人性。我们喜欢即时的反馈。但是对于很多智能体工作流程，我认为我们需要学会将任务委派给 AI 智能体，并且耐心等待几分钟，甚至可能需要等待几个小时来获取回应。就像我看到的许多新手经理，他们将任务委派给别人，然后五分钟后就去查看情况，这并不高效，我们也需要对一些 AI 智能体这样做，尽管这非常困难。我以为我听到了一些笑声。

另外，快速生成 token 是一个重要的趋势，因为我们在不断迭代这些智能体工作流程。LLM 为自己阅读生成 token，能够比任何人都快速的生成 token 是很棒的。我认为，甚至来自稍微质量低点的 LLM，也能快速生成更多的 token，可能会得到好的结果，相比之下，从质量更好的 LLM 中慢速生成 token，也许会不尽如人意。这个观点可能会引起一些争议，因为它可能让你在这个过程中多转几圈，就像我在第一张幻灯片上展示的 GPT-3 和智能体架构的结果一样。

坦率地说，我非常期待 Claude 4，GPT-5，Gemini 2.0，以及正在建设中的所有其他精彩模型。在我看来，如果你期待在 GPT-5 零样本学习上运行你的项目，你可能会发现，通过在早期模型上使用智能体和推理，你可能比预期更早地接近 GPT-5 性能水平。我认为这是一个重要的趋势。

诚实地说，通向通用人工智能的道路更像是一段旅程，而不是一个目的地，但我认为这种智能体工作流可能帮助我们在这个非常长的旅程上迈出一小步。

谢谢。