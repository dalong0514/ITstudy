## 20241221Building-Anthropic-A-Conversation-With-Our-Co-Founders

[Building Anthropic | A conversation with our co-founders - YouTube](https://www.youtube.com/watch?v=om2lIWXLLN4)

Why are we working on AI in the first place? I'm just going to arbitrarily pick Jared. Why are you doing AI at all? I mean, I was working on physics for a long time, and I got bored, and I wanted to hang out with more of my friends. I thought Dario pitched you on it. I don't think I explicitly pitched you any point. I just kind of showed you results of AI models and was trying to make the point that they're very general and they don't only apply to one thing. And then just at some point after I showed you enough of them, you were like, oh, yeah, that seems like it's right.

为什么我们一开始要研究 AI 呢？就拿 Jared 来说吧，你为什么选择从事 AI 工作？我自己在物理领域工作了很长时间，后来感觉有些厌倦，也想多和朋友们交流。我以为是 Dario 向你推荐的，但我自己并没有特别地向你推销过 AI。我只是展示了一些 AI 模型的成果，想说明这些模型非常通用，不仅限于单一应用。后来，在我展示了足够多的成果之后，你就认为这确实是个不错的方向。

How long have you been a professor when you started? I think like six years or so. I think I helped recruit Sam. I talked to you and you were like, I think I've created a good bubble here. And my goal is to get Tom to come back. And then it worked. And did you meet everyone through Google when you were doing the interpretability stuff, Chris? No. So I guess I actually met a bunch of you when I was 19. And I was visiting the Bay Area for the first time. So I guess I met Dario and Jared then. And I guess they were postdocs, which I thought was very cool at the time.

你当教授的时候已经有几年经验了？大概六年左右吧。我记得我参与了招募 Sam 的过程。我跟你聊过，你说：「我觉得我在这里营造了一个很好的氛围，我的目标是让 Tom 回来。」结果成功了。Chris，你是在 Google 做可解释性研究时认识所有人的吗？不是的。我其实在 19 岁第一次访问湾区时就认识了你们中的很多人。当时我遇到了 Dario 和 Jared，他们是博士后，我觉得这很酷。

And then I was working at Google Brain. And Dario joined. And we sat side by side, actually, for a while. We had desks beside each other and I worked with Tom there as well. And then of course I went to work with all of you at OpenAI when I went there. So I guess I've known a lot of you for like more than a decade, which is kind of wild. If I remember correctly, I met Dario in 2015 when I went to a conference you were at, and I tried to interview you, and Google PR said I would have read all of your research papers.

后来我在 Google Brain 工作，Dario 加入了我们。我们曾经并肩坐了一段时间，桌子就在一起，我也与 Tom 合作过。后来，我加入 OpenAI，与大家一起工作。我想，我认识你们之中的很多人已经超过十年了，真是不可思议。如果我没记错的话，我是在 2015 年与 Dario 相识的。当时我参加了一个会议，您也在场，我尝试采访您，但 Google 的公关建议我应该先阅读您所有的研究论文。

Yeah, I think I was writing concrete problems in AI safety when I was at Google. I think you wrote a story about that paper. I did. I remember right before I started working with you, I think you invited me to the office to come chat and just tell me everything about AI. And you explained. I remember afterwards being like, oh, I guess this stuff is much more serious than I realized. And you were probably explaining the big blob of compute and parameter counting and how many neurons are in the brain and everything. I feel like Dario often has that effect on people. This is much more serious than I realized. Yeah. I'm the bringer of happy tidings.

是的，我记得在 Google 工作时，我正专注于撰写有关 AI 安全的具体问题。我想你写过关于那篇论文的报道。确实，我写过。我还记得在开始和你合作之前，你邀请我去办公室聊天，向我详细介绍了 AI 的一切。你解释得很清楚，让我意识到这些问题比我想象的要严峻得多。你可能当时讲解了大量计算、参数计数以及大脑中神经元的数量等等。我觉得 Dario 经常给人这样的印象，就是这些问题比我们意识到的要严重得多。是的，我就像个「快乐消息」的传递者。

But I remember when we were at OpenAI, where there was the scaling laws stuff and just making things bigger, and it started to feel like it was working. And then it kind of kept on eerily working on a bunch of different projects, which I think is how we all ended up working closely together, because it was first GPT-2, and then scaling laws and GPT-2, and then scaling laws and GPT-3, and we ended up being a big group of people. Yeah, we were the plop of people that were making things work. Yeah. That's right.

我记得在 OpenAI 的时候，我们研究扩展法则，只是将规模扩大，结果开始觉得这方法确实奏效。然后，这个方法在多个项目中持续奏效，这可能就是我们为什么会紧密合作的原因。最初是 GPT-2，然后是扩展法则与 GPT-2，接着是扩展法则与 GPT-3，最后我们形成了一个庞大的团队。是的，我们就是那个让事情变得可行的团队。没错。

I think we were also excited about safety, because in that era, there was sort of this idea that AI would become very powerful, but potentially not understand human values or not even be able to communicate with us. And so I think we were all pretty excited about language models as a way to kind of guarantee that AI systems would have to understand kind of implicit knowledge. And RL for human feedback on top of language models, which was the whole reason for scaling these models up was that the models weren't smart enough to do RLHSF on top of. So that's the kind of intertwinement of safety and scaling of the models that we still believe in today.

我认为我们对安全性也很感兴趣，因为在那个时代，人们普遍认为 AI 会变得非常强大，但可能无法理解人类的价值观，甚至无法与我们交流。因此，我们对语言模型非常期待，因为它可以确保 AI 系统必须理解隐性知识。而在语言模型上进行人类反馈的强化学习（RL）也是类似，扩大这些模型的原因是因为模型本身还不够智能，不能直接进行 RLHSF。这就是安全性与模型规模之间密切的关系，也是我们至今仍然相信的。

I think there was also an element of the scaling work was done as part of the safety team that Dario started at OpenAI because we thought that forecasting AI trends was important to be able to have us take them seriously and take safety seriously as a problem. Correct. Yeah, I mean, we took, I remember being in some airport in England sampling from GPT-2 and using it to write fake news articles and slacking Dario and being like, oh, this stuff actually works and might have like huge policy implications. I think Dario said something like, yes. It's a typical way. But then we worked on that a bunch, as well as the release stuff, which was kind of wild.

我认为，Dario 在 OpenAI 成立的安全团队在扩展工作中发挥了重要作用。我们意识到，预测 AI 趋势对于认真对待安全问题至关重要。是的，我记得当时在英国的某个机场，我们用 GPT-2 撰写虚假新闻文章，并把结果发给 Dario，我意识到这项技术的确有效，可能会对政策产生重大影响。Dario 的回答是肯定的。这种情况很典型。之后，我们在这个领域做了很多工作，包括发布相关的事宜，过程相当疯狂。

Yeah, I remember the release stuff. I think that was when we first started working together. That was a fun time, the GPT-2 launch. Yeah, but I think it was good for us, because we did a kind of slightly strange, safety-oriented thing all together, and then we ended up doing Anthropic, which is a much larger, slightly strange strange safety-oriented thing altogether. And then we ended up doing Anthropic, which is a much larger, slightly strange safety-oriented thing altogether.

是的，我记得发布会的事情。那应该是我们第一次合作的时期。GPT-2 的发布真是段有趣的时光。我觉得这对我们来说是件好事，因为我们一同做了一些有点另类但注重安全的事情，后来我们成立了 Anthropic，这是一个规模更大且更加注重安全的项目。

So I guess just going back to the concrete problems, because I remember I joined OpenAI in 2016, one of the first 20 employees or whatever with Udario. And I remember at that time the concrete problems in AI safety seemed like it was the first mainstream AI safety paper. I don't really know if I ever asked you what the story was for how that came about. Chris knows the story because he was involved in it. I think we were both at Google. I forget what other project I was working on.

我想回到具体问题上，因为我记得在 2016 年加入 OpenAI 时，我是前 20 名员工之一，那时和 Udario 一起工作。我记得当时 AI 安全领域的具体问题似乎是我们撰写的第一篇主流 AI 安全论文。我不确定是否曾询问过你这个论文是如何产生的。Chris 知道其中的故事，因为他也参与了。我记得我们那时都在 Google，但我忘了自己当时还在做什么其他项目。

But like with many things, it was my attempt to kind of procrastinate from whatever other project I was working on that I've now completely forgotten what it was. But I think it was like Chris and I decided to write down what are some open problems in terms of AI safety? And also, AI safety is usually talked about in this very kind of abstruse, abstract way. Can we kind of ground it in the ML that was going on at the time? I mean, now there's been like, you know, six, seven years of work in that vein, but it was almost a strange idea at the time.

但和许多事情一样，我试图拖延当时正在进行的其他项目，而我现在已经完全忘记了那是什么项目。但我想起我和 Chris 决定写下 AI 安全方面的一些开放问题。AI 安全通常被谈论得非常晦涩抽象，我们能不能把它和当时的机器学习实践结合起来？现在已经有六七年的相关工作了，但在当时这几乎是一个奇怪的想法。

Yeah, I think there's a way in which it was almost like kind of political project, where at the time, a lot of people didn't take safety seriously. So I think that there was sort of this goal to collate a list of problems that sort of people agreed were reasonable, often already existed in literature, and then get a bunch of people across different institutions who are credible to be authors. And I remember I had this whole long period where I just talked to 20 different researchers at Brain to build support for publishing the paper.

是的，我认为这几乎是一种有策略性的计划。在当时，很多人并不认真对待安全问题。因此，目标之一是汇总出一份安全问题清单，这些问题被认为是合理的，并且通常已经存在于文献中，然后邀请来自不同机构的可信作者共同参与。我记得有一段很长的时间，我与 Brain 的 20 位不同研究人员交谈，以争取他们对发表论文的支持。

In some ways, if you look at it in terms of the problems and a lot of things that emphasized, I think it hasn't held up that well in that it's, you know, I think it's not really the right problems. But I think if you sort of see it instead as a consensus building exercise that there's something here that is real and that is worth taking seriously, then it was a pretty important moment.

从某种角度来看，如果你关注的是其中强调的问题，我认为这些问题并没有得到很好的解决，因为我觉得这些不是正确的问题。然而，如果你把它视为一种建立共识的练习，承认其中确实有一些值得认真对待的内容，那么这确实是一个相当重要的时刻。

I mean, you end up in this really weird sci-fi world where I remember at the start of Anthropic, we were talking about constitutional AI. And I think Jared said, oh, we're just going to write like a constitution for a language model and that'll change all of its behavior. And I remember that sounded like incredibly crazy at the time. But why did you guys think that was going to work? Because I remember that was one of the first early like big research ideas we had at the company.

我的意思是，你会发现自己进入了一个非常奇特的科幻世界。我记得在 Anthropic 刚成立的时候，我们在讨论一种叫做宪法 AI（Constitutional AI）的概念。我记得 Jared 当时说，我们只需为大语言模型写下类似宪法的规则，就能改变它的所有行为。当时这听起来简直疯狂。但你们为什么认为这会奏效呢？我记得这是我们公司早期的一个重要研究构想。

Yeah, I mean, I think Dario and I had talked about it for a while. I think simple things just work really, really well in AI. I think the first versions of that were quite complicated, but then we kind of whittled away into just use the fact that AI systems are good at solving multiple choice exams and give them a prompt that tells them what they're looking for. And that was kind of a lot of what we needed. And then we were able to just write down these principles.

是的，其实我和 Dario 已经讨论这个问题很久了。简单的方法在 AI 领域往往非常有效。最初的版本相当复杂，但后来我们简化了，充分利用 AI 系统在解决选择题上的优势，并给它们一个提示，告诉它们要找什么。这样，我们就获得了很多我们需要的东西，然后我们便能够总结出这些原则。

I mean, it goes back to the big blob of compute or the bitter lesson or the scaling hypothesis. If you can identify something that you can give the AI data for and that's kind of a clear target, you'll get it to do it. So here's this set of instructions. Here's this set of principles. AI language models can like read that set of principles and they can like compare it to the behavior that they themselves are engaging in. So like you've got your training target there. So once you know that, I think my view and Jared's view is there's a way to get it to work. You just have to fiddle with enough of the details.

我的意思是，这可以追溯到计算能力的提升、经验教训或者模型扩展的假设。如果你能找到一个明确的目标，并为 AI 提供相应的数据，你就可以引导它完成任务。这里有一套指令和原则。AI 语言模型可以分析这些原则，并将它们与自身的行为进行比较。这样，你就设定了训练目标。一旦明确了这些，我和 Jared 都认为，只要调整好足够多的细节，就能让它成功运作。

Yeah. I think it was always weird for me, especially in these early eras, because I was in physics and then coming from physics. And I think now we forget about this because everyone's excited about AI. But I remember talking to Dario about concrete problems and other things. And I just got the sense that AI researchers were very, very kind of psychologically damaged by the AI winter, where they just kind of felt like having really ambitious ideas or ambitious visions was very disallowed. And that's kind of how I imagine it was in terms of talking about safety.

我一直觉得这很奇怪，特别是在早期阶段，因为我来自物理学领域。我想现在我们都忘记了这一点，因为每个人都对 AI 感到兴奋。但我记得和 Dario 谈论具体问题和其他事情时，我感受到 AI 研究人员在心理上受到了「AI 寒冬」的严重影响。在那个时期，他们觉得拥有雄心勃勃的想法或愿景是不被允许的。这让我想象到在讨论安全性问题时的情形。

In order to care about safety, you have to believe that AI systems could actually be really powerful and really useful. And I think that there was kind of a prohibition and against being ambitious. And I think one of the benefits is that physicists are very arrogant. And so they're constantly doing really ambitious things and talking about things in terms of grand schemes. And so, yeah, I mean, I think that's I think that's definitely true. Like I remember in 2014, it was like there were just like, I don't know, there were just like some things you couldn't say, right.

如果要关注安全性，你必须相信 AI 系统可能非常强大且有用。在过去，有一种不鼓励追求雄心壮志的趋势。而物理学家通常非常自信，他们常常从宏观角度来讨论和进行雄心勃勃的研究。我记得在 2014 年，有些话题似乎是不能随意讨论的。

But but I actually think it was kind of an extension of problems that exist across academia other than maybe theoretical physics, where they've kind of evolved into very risk-averse institutions for a number of reasons. And even the industrial parts of AI had kind of transplanted or forklifted that mentality. And it took a long time. I think it took until like 2022 to get out of that mentality. There's a weird thing about like, what does it mean to be conservative and respectable, where you might think like one version you could have is that what it means to be conservative is to take the risks or the potential harms of what you're doing really seriously and worry about that.

我认为这实际上是学术界普遍存在问题的一种延续，除了可能在理论物理学领域。由于多种原因，学术机构已经变得非常规避风险。即使是 AI 行业的一些部分也采纳了这种心态。这种情况持续了很长时间，我认为直到 2022 年才有所改变。关于保守和受人尊敬的含义，有一种奇怪的观点，即保守可能意味着要非常认真地对待你所做事情的风险和潜在危害，并对此感到担忧。

But another kind of conservatism is to be like, ah, you know, taking an idea too seriously and believing that it might succeed is sort of like scientific arrogance. And so I think there's like kind of two different kinds of conservatism or caution. And I think we were sort of in a regime that was very controlled by that one. You see it historically, right? If you look at the early discussions in 1939 between people involved in nuclear physics about whether nuclear bombs were sort of a serious concern. You see exactly the same thing with Fermi resisting these ideas because it just seemed kind of like a crazy thing. And other people like Zillard or Teller taking the ideas seriously because they were worried about the risks.

但另一种保守主义就像是，把一个想法过于认真地对待并相信它可能会成功，这有点像科学上的傲慢。这让我想到，其实有两种不同的保守态度或者说谨慎的方式。我认为，我们所处的环境很大程度上受到了这种谨慎态度的影响。从历史上看，我们可以看到类似的例子。在 1939 年，核物理学界的人们就曾讨论过核弹是否真的是一个值得担忧的问题。当时，费米对于这些想法持怀疑态度，因为他觉得这听起来有点疯狂。而齐拉德和泰勒等人则认真对待这些想法，因为他们担心其中潜在的风险。

Perhaps the deepest lesson that I've learned in the last 10 years, and probably all of you have learned some form of it as well, is there can be this kind of seeming consensus, these things that kind of everyone knows that, I don't know, seem sort of wise, seem like they're common sense, but really they're just kind of hurting behavior masquerading as maturity and sophistication. And when you've seen, the consensus can change overnight. And when you've seen it happen a number of times, you suspected, but you didn't really bet on it. And you're like, oh, man, I kind of thought this, but what do I know? How can I be right and all these people are wrong? You see that a few times, then you just start saying, nope, this is the bet we're going to make.

在过去十年中，我学到的最深刻的教训之一，也许你们很多人也有类似的体会，就是：有时候社会上存在一种看似普遍的共识，这些被认为是常识的东西，似乎很有智慧，但实际上可能只是从众行为，伪装成成熟和老练。当你见证过几次这种共识在一夜之间发生改变时，你可能会开始怀疑，但并没有付诸行动。你会想：「哦，我曾经有这样的想法，但我能知道多少呢？我怎么可能是对的，而所有人都是错的呢？」当你多次看到这种情况后，你会坚定地说：「不，这就是我们要做的决定。」

I don't know for sure if we're right, but just ignore all this other stuff, see it happen. And I don't know, even if you're right 50% of the time, being right 50% of the time contributes so much, right? You're adding so much that is not being added by anyone else. And it feels like that's where we are today with some safety stuff, where there's like a consensus view that a lot of this safety stuff is unusual or doesn't naturally fall out of the technology.

我不确定我们是否正确，但只需忽略其他事情，看看它的发生。即使你只有一半的时间是正确的，这也会带来很大的贡献，对吧？你正在提供其他人没有提供的独特视角。感觉我们今天在处理一些安全问题时也是这样，大家普遍认为很多安全问题是异常的，或者不是技术自然发展出来的结果。

And then at Anthropic, we do all of this research where weird safety misalignment problems fall out as a natural dividend of the tech we're building. So it feels like we're in that counter consensus view right now. But I feel like that has been shifting over the past, even just like 18. We've been helping to shift. We've definitely been publishing. Publishing and doing research. Constant force. But I also think just world sentiment around AI has shifted really dramatically.

然后在 Anthropic，我们进行的研究中，一些奇怪的安全不匹配问题自然地成为了我们所构建技术的副产品。所以现在我们似乎持有一种与主流相反的观点。不过，我感觉这种情况在过去一年半里已经有所改变。我们一直在努力推动这种改变，持续地进行发表和研究。与此同时，我也认为全球对 AI 的看法确实发生了显著变化。

And it's more common in the user research that we do to hear just customers, regular people, say, I'm really worried about what the impact of AI on the world more broadly is going to be. And sometimes that means jobs or bias or toxicity. But it also sometimes means, is this just going to mess up the world? How is this going to contribute to fundamentally changing how humans work together, operate? I wouldn't have predicted that, actually.

在我们进行的用户研究中，常常听到普通用户表达他们对 AI 可能对世界产生更大影响的担忧。有时，他们担心的是工作机会、偏见或有害信息。但也有时候，他们会问，AI 是否会扰乱我们的世界？这将如何从根本上改变人类合作和运作的方式？说实话，我没有预料到这种担忧。

For whatever reason, it seems like people in the ML research sphere have always been more pessimistic about AI becoming very powerful than the general public. Maybe it's a weird form of humility or something. And when Daria and I went to the White House in 2023, in that meeting, Harris and Raimondo and stuff basically said, paraphrase, but basically said, we've got our eye on you guys. AI is going to be a really big deal, and we're now actually paying attention, which is. And they're right. They're right. They're absolutely right.

由于某种原因，机器学习（ML）研究领域的人们似乎总是比普通大众更悲观地看待 AI 变得非常强大的可能性。也许这是一种奇怪的谦逊。当 Daria 和我在 2023 年访问白宫时，在那次会议上，Harris 和 Raimondo 等人几乎明确表示，他们在密切关注我们的工作。AI 将会变得非常重要，他们现在确实开始重视这一点。他们是对的。

But I think in 2018, you wouldn't have been like, the president will call you to the White House to tell you they're paying close attention to the development of language models. Yeah. That was not a big event. Like in 2018. One thing that I think is interesting too, is I guess all of us kind of got into this when it didn't seem like there was like, like we thought, like we thought, we thought that it could happen, but yeah, it was, it was like, like fair me being like skeptical of the atomic bomb. It was like, he had, he was just a good scientist and like, there was some evidence that it could happen, but there also was a lot of evidence against it happening. And he, I guess decided that it would be worthwhile because if it was true, then it would be a big deal.

但我认为在 2018 年，你不会想到总统会打电话邀请你去白宫，并告诉你他们正在密切关注语言模型的发展。是的，那时这并不算什么大事。让我觉得有趣的是，当我们进入这个领域时，似乎一切都还不明朗。尽管我们相信这可能会发生，但也存在许多不确定性。就像科学家对原子弹持怀疑态度一样，他只是一位优秀的科学家，虽然有证据表明可能性，但也有很多证据表明这不太可能。然而，他认为值得尝试，因为如果这真的成真，那将是一件大事。

And I think for all of us, it was like 2015, 2016, 2017. There was some evidence and increasing evidence that this might be a big deal. But I remember in 2016, talking to all my advisors, and I was like, I've done startup stuff. I want to help out with AI safety, but I'm not great at math. I don't exactly know how I can do it. And I think at the time, people were like, either were like, well, you need to be super good at decision theory in order to help out. And I was like, eh, it's probably not going to work. Or they were like, it doesn't really seem like we're going to get some crazy AI thing. And so I had only a few people, basically, that were like, yeah, okay, that seems like a good thing to do.

对于我们来说，2015 到 2017 年是一个特别的时期。有越来越多的证据表明，这可能是一项重大事件。但我记得在 2016 年与我的导师们交流时，我曾表示，我有创业的经验，并希望能在 AI 安全方面做出贡献。但由于数学不够好，我不知道该如何下手。当时，有人认为你需要在决策理论上非常出色才能有所帮助，我觉得这对我来说可能不太现实。也有人认为不会有非常激进的 AI 发展。最终，只有少数几个人支持我，认为这确实是一件值得去做的事情。

I remember in 2014 making graphs of ImageNet results over time when I was a journalist and trying to get stories published about them, and people thought I was completely mad. And then I remember in 2015 trying to persuade Bloomberg to let me write a story about NVIDIA, because every AI research paper had started mentioning the use of GPUs, and they said that was completely mad. And then in 2016, when I left journalism to go into AI, I had these emails saying, you're making the worst mistake of your life, which I now occasionally look back on. But it all seemed crazy at the time from many perspectives to go and take this seriously, that scaling was going to work, and something was maybe different about the technology paradigm.

我记得在 2014 年，作为一名记者，我绘制了 ImageNet 结果的时间变化图，并尝试发表相关报道。然而，当时很多人认为我简直疯了。到了 2015 年，我试图说服 Bloomberg 允许我撰写关于 NVIDIA 的文章，因为几乎所有 AI 研究论文都开始提到使用 GPU，但他们仍然觉得这很荒唐。2016 年，我决定离开新闻业，投身 AI 领域。当时我收到很多邮件警告我说这是人生最大的错误，如今回想起来，这些警告让我忍俊不禁。当时，从多个角度来看，认真对待扩展性和技术范式的变化似乎是疯狂的。

You're like Michael Jordan and that coach that didn't believe in him in high school. How did you actually make the decision, though? Did you feel torn, or was it obvious to you? I did a crazy counter bet where I said, let me become your full-time AI reporter and double my salary, which I knew that they wouldn't say yes to. Then I went to sleep, and then I woke up and resigned. It was all fairly relaxed. You're just a decisive guy. In that instance, I was.

未找到意译内容

I think it's because I was going to work, reading archive papers, and then printing archive papers off and coming home and reading archive papers, including Dario's papers from the Baidu stuff, and being like, something completely crazy is happening here. And at some point, I thought you should bet with conviction, which I think everyone here has done in their careers is just betting with conviction that this is going to work.

我觉得是因为我每天上班时都在阅读档案论文，打印出来带回家接着看，包括 Dario 在百度的那些论文。我意识到，这里发生了一些非常不可思议的事情。在某个时刻，我想，我们应该坚信这一切会成功，就像在座的每一位在职业生涯中做出的坚定决定一样。

Yeah. I definitely was not as decisive as you. I spent like six months like flip-flopping, like, okay, like, should I do it? Like, should I try to do a startup? Should I try to do this thing? But I also feel like back then, there wasn't as much talk of engineers and the impact that an engineer can have in AI. That feels so natural to us now and we're at the same sort of talent raise for engineers of all different types. But at the time it was like you're a researcher and that's the only people that can work on AI. So I don't think it was crazy that you were spending time thinking about that. Yeah, yeah.

是的。我当时没有你那么果断。我花了大约六个月犹豫不决，不知道是否应该尝试创业。但我觉得当时大家对工程师在 AI 领域的影响力讨论并不多。如今我们对各种工程师的需求已变得非常自然，但那时似乎只有研究人员才能从事 AI 工作。所以，我认为你花时间思考这个问题并不奇怪。是的，是的。

And I think that was basically the thing that got me to join OpenAI was like, I like, I messaged the people there and they were like, yeah, we actually think that you can help out by doing engineering work. Yeah. And like that, that you can help out with AI safety in that way, which I think there hadn't really been an opportunity for that. So that was what. That's right. You were my, you were my manager at that opening. I was, I think I joined after you'd been there for a while. I was at brain for a bit. Yeah. I don't know if I ever asked you like what it was that they got you to join.

让我加入 OpenAI 的主要原因是，我联系了那里的团队，他们认为我可以通过工程工作来帮助推进 AI 安全。在这之前，我并没有这样的机会。没错，当时你是我的经理。我加入的时候，你已经在那里工作了一段时间。我之前在 Brain 工作过一段时间。我不确定是否问过你，是什么吸引你加入的。

Yeah. So I had been at Stripe for about five and a half years and I knew Greg, he had been my boss. He was my boss at Stripe for a while. And I actually introduced him and Dario because I said when he was starting OpenAI, I was like, the smartest person that I know is Dario. You would be really lucky to get him. So Dario was at OpenAI. I had a few friends from Stripe that had gone there too. And I think sort of like you, I'd been thinking about what I wanted to do after Stripe. I had gone there just because I wanted to get more skills after working in, you know, nonprofit and international development.

是的，我在 Stripe 工作了大约五年半，认识 Greg，他曾经是我的老板。在 Stripe 的一段时间里，Greg 是我的上司。实际上，是我把他介绍给 Dario 的。当时 Greg 正在启动 OpenAI，我告诉他，我所认识的人中最聪明的就是 Dario。要是你能得到他，那就太幸运了。所以 Dario 后来去了 OpenAI。我有几个在 Stripe 的朋友也加入了那边。我想我和你一样，一直在思考离开 Stripe 之后该做些什么。我之所以加入 Stripe，是因为想在从事非营利和国际发展工作之后，获得更多的技能。

And I actually thought I was going to go back to doing that. Like essentially, I had always been working, I was like, I really want to help people that have, you know, less than I do. But I didn't have the skills when I was doing it before Stripe. And so I looked at going back to public health, I thought about going back into politics very briefly. But I was also looking around at other tech companies and other sort of ways of having impact. And OpenAI at the time felt like it was a really nice intersection.

我原本以为我会回到之前的工作中去。实际上，我一直以来都在思考，我想帮助那些生活条件比我差的人。但在加入 Stripe 之前，我并不具备这样的技能。所以我考虑过重回公共卫生领域，甚至短暂地考虑过重新进入政治领域。同时，我也在关注其他科技公司和能够产生影响的不同方式。当时，OpenAI 给我一种很好的结合点的感觉。

It was a nonprofit. They were working on this really big lofty mission. I really believed in sort of the AI, you know, potential because, I mean, I know Dario a little bit. And so he was. They needed management help. They definitely needed management help. That is a fact. And so there was, I think it felt very me-shaped, right? I was like, oh, there's this nonprofit, and there's all these really great people with these really good intentions, but it seems like they're a little bit of a mess. And that felt really exciting to me to get to come in. And even, you know, just. I was such a utility player. I was running people, but I was also running some of the technical teams.

这是一家非营利组织，正在致力于一个非常宏大的使命。我非常相信 AI 的潜力，因为我对 Dario 有一点了解。他们确实需要管理方面的帮助，这一点毋庸置疑。我感觉这个职位非常适合我。这里有一个非营利组织，聚集了很多优秀且有良好意图的人，但他们的运作似乎有些混乱。能够加入并帮助他们让我感到很兴奋。我可以说自己是一个多面手，不仅负责管理人员，也在带领一些技术团队。

Yeah, the scaling org. I worked on the language team. I worked on some policy stuff. I worked with Chris. And I felt like there was just so much goodness in so many of the employees there. And I felt a very strong desire to come and try to help make the company a little more functional. I remember towards the end, after we'd done GPT-3, you were like, have you guys heard of something called trust and safety? Yes. I said, you know, I used to run some trust and safety teams at Stripe. There's a thing called trust and safety that you might want to consider for a technology like this.

是的，那个扩展组织。我在语言团队工作，也参与了一些政策相关的事务，并和 Chris 共事。我觉得那里许多员工都非常出色。我强烈希望能帮助公司变得更高效。我记得在我们完成 GPT-3 的时候，你问我们是否了解「信任与安全」这个概念。我说，我曾在 Stripe 领导过信任与安全团队，对于这样的技术，可能需要考虑这个方面。

And it's funny because it sort of is the intermediary step between AI safety research, which is how do you actually make the model safe to something just much more practical? I do think there was a value in saying, this is going to be a big thing. We also have to be doing this sort of practical work day to day to build the muscles for when things are going to be a lot higher stakes. That might be a good transition point to talk about things like the responsible scaling policy and how we came up with that or why we came up with it and how we're using it now, especially given how much trust and safety work we do on today's models.

有趣的是，这项工作相当于 AI 安全研究和实际应用之间的桥梁。AI 安全研究关注如何确保模型的安全，而我们的工作则更侧重于实际操作。我认为，明确指出这项工作的重要性是有意义的。同时，我们需要在日常工作中不断积累经验，以便在未来面临更大挑战时能够应对自如。这也为我们讨论「负责任的扩展政策」提供了一个很好的切入点。我们可以探讨这项政策的制定过程、背后的原因，以及我们在现有模型中如何应用它，尤其是在我们当前进行的大量信任与安全工作的背景下。

So whose idea was the RSP? You and Paul? Yeah, it was me and Paul first talked about it in late, Paul Cristiano, in late 2022. First, it was like, oh, should we cap scaling at a particular point until we've discovered how to solve certain safety problems? And then it was like, well, it's kind of strange to have this one place where you cap it and then you uncap it. So let's have a bunch of thresholds. And then at each threshold, you have to do certain tests to see if the model is capable and you have to take increasing safety and security measures. Originally, we had this idea.

RSP 的想法是怎么来的？是你和 Paul 提出的？是的，我和 Paul Cristiano 最早在 2022 年底讨论了这个问题。起初我们考虑的是，是否应该在某个点限制模型的扩展，直到找到解决安全问题的方法。后来我们意识到，这样限制和解除限制的方式有些奇怪，因此决定设立多个阈值。在每个阈值，我们都需要进行测试来确认模型的能力，并且逐步加强安全和保安措施。这就是我们最初的想法。

And then the thought was just look like, you know, this will go better if, you know, if it's done by some third party, like we shouldn't we shouldn't be the ones to do it, right? It shouldn't come from one company, because then other companies are less likely to adopt it. So Paul actually went off and designed it and many features of it changed. And we were kind of on our side working on how it should work. And once Paul had something together, then pretty much immediately after he announced the concept, we announced ours within a month or two. I mean, many of us were heavily involved in it. I remember writing at least one draft of it myself, but there were, like, several drafts of it.

起初，我们认为如果由第三方来完成这个项目，效果可能会更好。因为如果是由一家单独的公司来做，其他公司可能不会乐于接受。因此，Paul 开始设计这个项目，并对许多特性进行了修改。我们则在研究项目的具体实现方式。当 Paul 完成他的设计后，几乎是他宣布概念的同时，我们也在一两个月内发布了我们的版本。我记得自己至少参与撰写了一个草稿，其实一共有好几个草稿。

There were so many drafts. I think it's gone through the most drafts of any doc. Which makes sense, right? It's like, I feel like it is in the same way that, like, the U.S. treats, like, the Constitution as, like, the holy document. Yeah. Which, like, I think is just a big thing that, like, strengthens the U.S. treats, like, the Constitution as, like, the holy document. Yeah. Which, like, I think is just a big thing that, like, strengthens the U.S. Yes. And, like, we don't expect the U.S. to go off the rails in part because just, like, every single person in the U.S. is, like, the Constitution is a big deal and if you tread on that, like, I'm mad.

有很多草稿。我想这可能是任何文档中草稿最多的。这是可以理解的，对吧？就像美国把宪法视为神圣的文件一样。我认为这是强化美国的重要因素之一。正因为如此，我们不认为美国会轻易出问题，因为每个美国人都认为宪法非常重要，任何人如果触犯宪法，都会引起公愤。

Yeah. Like, I think that, like, the RSP is our, like, it holds that thing. It's, like, the holy document for Anthropic. So it's, like, worth doing a lot of iterations getting it right. Some of what I think has been so cool to watch about the RSP development at Anthropic too is it feels like it has gone through so many different phases and there's so many different skills that are needed to make it work. There's the big ideas, which I feel like Dario and Paul and Sam and Jared and so many others are like, what are the principles? What are we trying to say? How do we know if we're right?

是的。我认为 RSP 是我们最重要的文档，它就像是 Anthropic 的「圣典」。因此，为了确保其准确性，进行多次迭代是非常值得的。在 Anthropic，观察 RSP 的发展是非常有趣的，因为它经历了许多不同的阶段，并且需要多种技能才能使其成功。这里有一些大的构想，比如 Dario、Paul、Sam 和 Jared 以及其他许多人都在思考：我们的原则是什么？我们想表达什么？我们如何判断是否正确？

But there's also this very, like, operational approach to just iterating where we're like, well, we thought that we were going to see this at this, you know, safety level. And we didn't. So should we change it so that we're making sure that we're holding ourselves accountable? And then there's all kinds of organizational things, right? We just were like, let's change the structure of the RSP organization for clearer accountability. And I think my sense is that for a document that's as important as this, right? I love the constitution analogy. It's like, there's all of these bodies and systems that exist in the US to like, make sure that we follow the constitution, right?

然而，还有一种非常务实的操作方法来进行迭代。我们可能会认为在某个安全级别会看到特定的结果，但事实并非如此。因此，我们需要调整策略，以确保我们对自己的行为负责。此外，还有一些组织结构上的调整，例如，我们决定改变 RSP 组织的架构，以便责任更加明确。对于这样重要的文件，我很喜欢用宪法来做比喻。就像美国有一系列组织和系统来确保我们遵循宪法一样。

There's the courts, there's the Supreme Court, there's the presidency, there's the, you know, the both houses of, you know, Congress, and they do all kinds of other things, of course. But there's like all of this infrastructure that you need around this like one document. And I feel like we're also learning that lesson here. I think it sort of reflects a view a lot of us have about safety, which is that it's a solvable problem. It's just a very, very hard problem that's going to take tons and tons of work.

有法院、最高法院、总统和国会两院，这些机构当然也处理许多其他事务。然而，围绕着一个关键文件（如宪法），需要建立大量的基础设施。我感觉我们在这里也在学习类似的道理。这反映了我们许多人对于安全问题的看法：安全问题是可以解决的，只是它非常复杂，需要付出大量的努力。

All of these institutions that we need to build up, like there's all sorts of institutions built up around automotive safety, built up over many, many years. But we're like, do we have the time to do that? We've got to go as fast as we can to figure out what the institutions we need for AI safety are and build those and try to build them here first but make it exportable. It forces unity also because if any part of the org is not kind of in line with our safety values, it shows up through kind of the RSP. The RSP is going to block them from doing what they want to do.

我们需要快速建立起 AI 安全相关的机构，就像多年来围绕汽车安全建立的那些机构一样。然而，我们时间紧迫，必须尽快找出必要的 AI 安全机构，并在本地建立以便将来推广。这也有助于组织内部的团结，因为如果有部分不符合我们的安全价值观，它会通过 RSP 系统显现，而 RSP 会阻止他们的行动。

And so it's a way to remind everyone over and over again, basically to make safety a product requirement, part of the product planning process. And so it's not just a bunch of bromides that we repeat. It's something that you actually, if you show up here and you're not aligned, you actually run into it. And you either have to learn to get with the program or it doesn't work out.

因此，这是一种反复提醒每个人的方法，基本上就是将安全性作为产品的一个必要条件，纳入产品规划的过程。这样一来，它不再仅仅是我们反复提及的空话。实际上，如果你加入这个团队但没有遵循这一原则，你就会发现问题。你要么学会适应这个要求，要么就无法继续下去。

The RSP has become kind of funny over time because we spend thousands of hours of work on it. And then I go and talk to senators, and I explain the RSP, and I'm like, we have some stuff that means it's hard to steal what we make, and also that it's safe. And they're like, yes, that's a completely normal thing to do. Are you telling me not everyone does this? And you're like, oh, okay, yeah. It is the fact true that not everyone does this? It's amazing. We've spent so much effort on it here. And when you boil it down, they're like, yes, that sounds like a normal way. Yeah, that sounds good. That's been the goal. Like Daniela was saying, let's make this as boring and normal. Let's make this a finance thing. Yeah, management's like an audit. Yeah, yeah. Boring and normal is what we want, certainly in retrospect.

随着时间的推移，RSP 项目变得有些有趣，因为我们在这上面投入了大量的时间和精力。当我向参议员们解释 RSP 时，我告诉他们，我们的设计使得我们的成果不容易被盗取，同时也很安全。参议员们通常会回答说，这样的做法是完全合理和正常的。他们甚至感到惊讶，问道：「难道不是所有人都这样做吗？」我们花了这么多心血，当他们得知这并不是普遍做法时，他们感到很惊讶。实际上，我们的目标就是把它做得看似普通和平常，正如 Daniela 所说，要让它像处理金融事务一样严谨，就像进行审计一样。确实，追求这种平凡和常规正是我们所希望的，尤其在事后看来更是如此。

Well, also, Dario, I think in addition what we want, certainly in retrospect. Well, also, Dario, I think in addition to driving alignment, it also drives clarity because it's really, it's written down what we're trying to do. And it's legible to everyone in the company. And it's legible externally what we think we're supposed to be aiming towards from a safety perspective. It's not perfect. We're iterating on it. We're making it better. But I think there's some value in saying, like, this is what we're worried about. This thing over here, like, you can't just use this word to sort of derail something in either direction, right?

另外，Dario，我认为除了促进一致性，它还提高了透明度，因为我们正在尝试做的事情确实被记录下来了。公司里的每个人都能看懂，从外部来看，人们也能清晰理解我们从安全的角度努力的方向。虽然它并不完美，但我们正在不断改进。我认为有价值的是，我们可以明确指出我们所担心的问题，这样一来，就不能简单地用某个词语来偏离我们的目标，对吗？

To say, oh, because of safety, we can't do X. Or because of safety, we have to do X. We're really trying to make it clearer what we mean. Yeah, you can't. It prevents you from worrying about every last little thing under the sun. That's right. Right? Because it's actually the fire drills that damage the cause of safety in the long run. I've said if there's a building and the fire alarm goes off every week, that's a really unsafe building because when there's actually a fire, you're like, oh, it just goes off all the time. So it's very important to be calibrated. Yes.

很多时候我们会说，因为安全原因我们不能做 X，或者因为安全原因我们必须做 X。我们真正想做的是更明确地表达我们的意思。其实，过多的担心每一件小事是没有必要的。长期来看，频繁的消防演习反而会削弱人们对安全的重视。比如，如果一栋楼的火警每周都响一次，那么当真正发生火灾时，人们可能会以为又是误报。因此，保持合适的警戒水平非常重要。

A slightly different frame that I find kind of clarifying is that I think that RSP creates healthy incentives at a lot of levels. So I think internally it aligns the incentives of every team with safety because it means if we don't make progress on safety, we're going to block. I also think that externally it creates a lot of healthier incentives than other possibilities at least that I see because it means that if we at some point have to take some kind of dramatic action, like if at some point we have to say, you know, our model, we've reached some point and we can't yet make a model safe, it aligns that with sort of the point where there's evidence that supports that decision.

有一个稍微不同的视角，我觉得可以更清晰地理解 RSP 带来的健康激励机制。在内部，它让每个团队都关注安全，因为如果我们在安全性上没有取得进展，项目就会停滞不前。从外部来看，我认为这比其他可能的选择带来了更健康的激励机制，因为如果我们必须在某个时候采取重大行动，比如承认我们的模型到了某个阶段却仍然无法保证安全，这种机制确保了此类决定有充分的证据支持。

And there's sort of a pre-existing framework for thinking about it and it's legible. And so I think there's a lot of levels at which the RRSP, I think in ways that maybe I didn't initially understand when we were talking about the early versions of it, it creates a better framework than any of the other ones that I've thought about. I think this is all true, but I feel like it undersells how challenging it's been to figure out what the right policies and evaluations and what the lines should be. I think that we have and continue to sort of iterate a lot on that.

已经有一个现成的框架来理解这个问题，而且它是容易理解的。因此，我认为在很多方面，RRSP 提供了一个比我之前考虑的其他框架更好的解决方案，尽管在我们讨论其早期版本时，我可能没有完全意识到这一点。我相信这些都是事实，但我觉得这低估了确定正确政策、评估标准以及界限的挑战。我们一直在这方面进行许多次迭代，并将继续这样做。

And I think there is a question also that's difficult of sort of, you could be at a point where it's very clear something's dangerous or very clear that something's safe, but with some technology that's so new, there's actually like a big gray area. And so I think that has been like all the things that we're saying were things that made me really, really excited about the RRSP at the beginning and still do. But also I think enacting this in a clear way and making it work has been much harder and more complicated than I anticipated. Yeah, I think this is exactly the point. The gray areas are impossible to predict. There's so many of them. Until, until you actually try to implement everything, you, like, don't know what's going to go wrong.

我认为有一个问题很难处理，有时你可能很清楚某项技术是安全的或危险的，但对于一些新兴技术而言，存在一个大的灰色地带。这让我想起了我们所谈到的内容，这些都让我一开始对 RRSP 感到无比兴奋，现在依然如此。然而，要清楚地实施它并使其奏效，比我预期的要复杂得多。是的，这就是问题所在。灰色地带无法预测，数量众多。在实际尝试实施之前，你无法知道会出现什么问题。

So what we're trying to do is go and implement everything so we can see as early as possible what's going to go wrong. Yeah, you have to do three or four passes before you really get it right. Like, iteration is just very powerful, and, you know, you're not going to get it right on the first time. And so, you know, if the stakes are increasing, you want to get your iterations in early. You don't want to get it right on the first time. And so if the stakes are increasing, you want to get your iterations in early. You don't want to get them in late. You're also building the internal institutions and processes, so the specifics might change a lot, but building the muscle of just doing it is the really valuable thing.

我们希望尽早实施所有计划，以便及早识别出可能出现的问题。通常需要经过三到四次的尝试才能真正完善，因为迭代是一种非常有效的方法，第一次尝试很难做到完美。因此，如果风险在增加，应该尽早开始迭代，而不是拖到最后。同时，我们在构建内部的机制和流程，尽管具体细节可能会发生很大变化，但培养「立即动手实践」的能力才是最有价值的。

I'm responsible for compute at AnthropBakon. That's important. Thank you. compute at Anthropic. That's important. Thank you. I think so. So I think that for me, I guess we have to deal with external folks. And different external folks are on different spectrums of the how fast do they think stuff is going to get. I think that's also been a thing where I started out not thinking stuff would be that fast and have changed over time. And so I have sympathy for that.

我负责管理 AnthropBakon 和 Anthropic 的计算资源，这对于我的工作非常重要。感谢您的关注。我认为我们需要与外部人员合作，而这些人员对于事情发展的速度有着不同的预期。我最初也不认为事情会发展得如此之快，随着时间的推移，我的看法发生了变化，因此我对他们的观点表示理解。

And so I think the RSP has been extremely useful for me in communicating with people who think that things might take longer, because then we have a thing where it's like, we don't need to do extreme safety measures until stuff gets really intense. And then we can be like, they might be like, I don't think stuff will get intense for a long time. And then I'll be like, okay, yeah, we don't have to do extreme safety measures. And so that makes it a lot easier to communicate with other folks externally. Yeah, yeah. It makes it like a normal thing you can talk about rather than something really strange.

因此，我觉得 RSP 在与那些认为事情可能会拖延的人沟通时非常有用。因为我们可以说，直到事情变得非常紧张之前，我们不需要采取极端的安全措施。对方可能会认为事情在很长一段时间内不会变得紧张，我就会回应说，是的，我们暂时不需要采取极端措施。这让与外部人士的沟通变得更容易，也让这些话题显得更平常，而不是特别奇怪。

Yeah. How else has it shown up for people? Evals, evals, evals. Good. It's all about evals. Everyone's doing evals. The training team is doing evals all the time. We're trying to figure out, like, has this model gotten enough better that it has the potential to be dangerous? So how many teams do we have that are evals teams? You have Frontier Red Team. There must be. I mean, there's a lot of people. Every team produces evals, basically. And that means you're just measuring against the RSP. Like, measuring for certain signs of things that would concern you or not concern you. Exactly.

对，人们还会从哪些方面注意到它呢？都是关于评估的问题。所有人都在进行评估。培训团队总是在进行评估。我们想弄清楚，这个模型是否已经进步到可能带来危险。那么，我们有多少个团队在做评估呢？比如说，「Frontier Red Team」就是其中之一。其实，有很多人参与，基本上每个团队都会进行评估。这意味着你只是在用 RSP 来衡量，看看是否有一些会引起你关注的迹象。确实如此。

It's easy to lower bound the abilities of a model, but it's hard to upper bound. So we just put tons and tons of research effort into saying, can this model do this dangerous thing or not? Maybe there's some trick that we haven't thought of, like chain of thought or best event or some kind of tool use that's going to make it so it can help you do something very dangerous. It's been really useful in policy because it's been a really abstract concept what safety is. And when I'm like, we have an eval which changes whether we deploy the model or not.

我们很容易确定一个模型能力的下限，但要确定其上限则非常困难。因此，我们投入了大量的研究力量来探讨，这个模型是否能够执行某些危险的任务。也许有一些我们尚未想到的方法，比如思维链（chain of thought）、最佳事件或某种工具的使用，这些方法可能会让模型能够帮助你完成一些非常危险的事情。在政策制定中，这种探讨非常有用，因为安全性一直是一个非常抽象的概念。我会说，我们有一个评估机制来决定是否部署该模型。

And then you can go and calibrate with policymakers or experts in national security or some of these CBRN areas that we do to actually help us build evals that are well calibrated. And that counterfactually just wouldn't have happened otherwise. But once you've got the specific thing, people are a lot more motivated to help you make it accurate. So it's been useful for that. The RSP shows up for me, for sure. Everything? Often. I actually think, weirdly, the way that I think about the RSP the most is what it sounds like. Just like the tone.

然后你可以与政策制定者或国家安全领域的专家，或我们在 CBRN 领域的专家进行交流和校准，以帮助我们建立精确的评估。如果没有这样的合作，这些事情可能就不会发生。但是，一旦有了具体的目标，人们就更有动力帮助你提高准确性。因此，这种方法非常有用。RSP 对我来说确实很有帮助。所有的事情？经常如此。我觉得，奇怪的是，我对 RSP 的思考更多地是关于它听起来如何，就像关注语气一样。

I think we just did a big rewrite of the tone of the RSP because it felt overly technocratic and even a little bit adversarial. I spent a lot of time thinking about how do you build a system that people just want to be a part of. It's so much better if the RSP is something that everyone in the company can walk around and tell you, just like with OKRs like we do right now, what are the top goals of the RRSP? How do we know if we're meeting them? What AI safety level are we at right now? Are we at ASL2? Are we at ASL3? That people know what to look for because that is how you're going to have good common knowledge of if something's going wrong.

我认为我们刚刚对 RSP 的语气进行了大幅改写，因为它给人的感觉过于技术官僚，甚至有点对立。我花了很多时间思考如何构建一个让人们乐于参与的系统。就像现在大家能够随口说出 OKR 的目标一样，如果 RSP 也是这样，那就太好了。比如，RRSP 的主要目标是什么？我们怎么知道是否达成了这些目标？当前的 AI 安全级别是多少？是 ASL2 还是 ASL3？人们知道应该关注什么，因为这将有助于在问题出现时，共同了解情况。

If it's overly technocratic and it's something that only particular people in the company feel is accessible to them, it's just like not as productive, right? And I think it's been really cool to watch it sort of transition into this document where I actually think most, if not everybody at the company, regardless of their role, could read it and say, this feels really reasonable. I want to make sure that we're building AI in the following ways, and I see why I would be worried about these things. And I also kind of know what to look for if I bump into something, right?

如果一件事情过于技术官僚化，只有公司中特定的人能理解，那它的生产力就会受到影响，对吧？我觉得很棒的一点是，看到它逐渐转变成一份文件，我认为公司里的大多数人，甚至可能是所有人，无论他们的岗位是什么，都能读懂，并觉得这很合理。他们会想，我们应该以这样的方式来构建 AI，我也理解为什么这些问题值得关注。我还大致知道，在遇到问题时该注意些什么，对吧？

It's almost like, make it simple enough that if you're working at a manufacturing plant, and you're like, huh, it looks like the safety seatbelt on this should connect this way, but it doesn't connect, that you can spot it. And that there's just like healthy feedback flow between leadership and the board and the rest of the company and the people that are actually building it. Because I actually think the way this stuff goes wrong in most cases is just like the wires don't connect or they get crossed. And that would just be a really sad way for things to go wrong. It's just all about operationalizing it, making it easy for people to understand.

就像在制造厂工作时，如果你发现安全带看起来应该这样连接，但实际上却没有连接上，你能立刻发现问题。而且，领导层、董事会与公司其他部门以及实际负责生产的员工之间应该保持有效的反馈交流。我认为，很多问题的出现往往是因为沟通不畅，就像电线没接好或者交错了一样，那样的错误很遗憾。关键在于将这些流程简化，让大家更容易理解。

Yeah, the thing I would say is none of us wanted to found a company. We felt like it was our duty, right? I felt like we had to. Like, we have to do this thing. This is the way we're going to make things go better with AI. Like, that's also why we did the pledge, right? Because we're like, the reason we're doing this feels like our duty. I wanted to invent and discover things in some kind of beneficial way. That was how I came to it. And that led to working on AI. And AI required a lot of engineering. And eventually, AI required a lot of capital.

是的，我想说的是，我们起初并没有想要创办一家公司，而是觉得有责任这样做。我感到我们必须采取行动，因为这样才能推动 AI 的积极发展。这也是我们做出承诺的原因，因为我们认为这是一种责任感的驱使。我想通过某种有益的方式进行发明和发现，这种想法引导我走上了 AI 的研究之路。而从事 AI 工作需要大量的工程技术，最终也需要大量的资金投入。

But what I found was that if you don't do this in a way where you're setting the environment, where you set up the company, then a lot of it gets done. A lot of it repeats the same mistakes that I found so alienating about the tech community. It's the same people. It's the same attitude. It's the same pattern matching. And so at some point, it just seemed inevitable that we do it in a different way.

但是我发现，如果你不以一种系统化的方式来设置环境和建立公司，那么很多事情会自然地完成，但也会重复科技界那些让我感到疏远的错误。依然是那些人，依然是那种态度，依然是相同的模式匹配。因此，在某种程度上，我们必须以不同的方式来进行这项工作。

When we were hanging out in graduate school, I remember you had kind of this whole like program of trying to figure out how to do science in a way that would sort of advance the public good. And I think I think that's like pretty similar to how we we think about this. Maybe you have like this like Project Vannevar or something to do that. I was a professor. I think basically I just looked at the situation and I was convinced that AI was on a very, very, very steep trajectory in terms of impact. It didn't seem like because of the necessity for capital, like as a physics professor, I could continue doing that. And I kind of wanted to work with people that I trusted in building an institution to try to make kind of AI go well. But yeah, I would never recommend founding a company or really want to do it. I mean, yeah, I think it's just a means to an end.

在研究生院时，我记得你有一套计划，尝试通过科学研究促进公共利益。我觉得这与你对这件事的理解很相似。也许你有一个类似 Project Vannevar 的项目来实现这一目标。作为一名教授，我观察了一下情况，确信 AI 的影响力正在快速上升。由于资本需求，我感觉自己作为物理学教授无法继续从事这方面的研究。我希望能和信任的人一起建立一个机构，推动 AI 良性发展。然而，我并不推荐创办公司，也不是真心想这样做。我觉得这只是实现目标的一种手段。

I mean, I think that's like usually how things go well, though. If you're doing something just to sort of like enrich yourself or gain power or like you have to sort of actually care about accomplishing a real goal in the world and then you find whatever means you have to. Well, something I think about a lot is just a strategic advantage for us is, I mean, it sounds really funny to say, but just like how much trust there is at this table, right? Like, I think that's not, I mean, Tom, you were at other startups. I was never a founder before, but it's actually really hard to get a group of, like a big group of people to have like the same mission. Right. And I think is really clear. And it's very pure. And I think that is something that I don't see as often, to Dario's point, in the tech industry. It feels like there's just a wholesomeness to what we're trying to do.

我觉得，通常事情顺利发展的方式是这样的：如果你做某件事只是为了个人利益或权力，那可能不会成功。你需要真正关心在世界上实现一个具体目标，并找到实现目标的各种方法。我经常想到的一点是，我们有一个战略优势，那就是我们这群人之间的信任。听起来可能有点滑稽，但正是因为这种信任，汤姆，你在其他初创公司工作过。我从没做过创始人，但我知道让一大群人拥有共同的使命并不容易。我觉得我们团队的目标非常明确，而且这种目标是纯粹的。这种纯粹性在科技行业中并不常见，正如达里奥所说，我们正在做的事情有一种真实而纯粹的感觉。

I agree. None of us were like, let's just go found a company. I felt like we had to do it. It just felt like we couldn't keep doing what we were doing the place we were doing it. We had to do it by ourselves. couldn't keep doing what we were doing the place we were doing it. We had to do it by ourselves. It felt like with GPT-3, which all of us had touched or worked on, and scaling laws and everything else, we could see it in front of us in 2020. And it felt like, well, if we don't do something soon altogether, you're going to hit the point of no return. And you have to do something to have any ability to change the environment.

我同意。我们并不是一开始就想着要去创办一家公司，而是感觉不得不这么做。我们无法继续在原来的地方做那些事情，必须自己来实现。在 2020 年，我们都曾接触或参与过 GPT-3，以及扩展定律（Scaling Laws）和其他一切。当时感觉，如果我们再不一起采取行动，就会到达无法回头的地步。我们必须采取一些措施，才能有能力去改变现状。

I think building off Daniel, I do think that there's just a lot of trust in this group. I think each of us knows that we got into this because we want to help out with the world. We did the 80% pledge thing, and that was a thing that everybody was just like, yes, obviously we're going to do this. I do think that the trust thing is a special thing that's extremely rare. I credit Daniela with keeping the bar high. I credit you with the fact that we scaled. She's clown racer. You're the reason the culture scaled, I think. People say how nice people are here, which is actually a wildly important thing.

我认为基于 Daniel 的观点，这个团队中的信任感非常强。我们每个人都知道，我们参与进来是因为想要为世界做些贡献。我们进行了 80% 的承诺，每个人都觉得这显然是我们会去做的事情。我确实认为，这种信任感极其罕见。我将这种高标准的维持归功于 Daniela。我认为，我们能扩展到现在的规模，也要归功于你。人们常说这里的人都很友好，这实际上是一个极其重要的特点。

I think Anthropic is really low politics. And of course, we all have a different vantage point than average, and I try to remember that. It's because of low ego. But it's low ego, and I do think our interview process and just the type of people who work here, there's almost an allergic reaction to politics. And unity. Unity is so important. The idea that the product team, the research team, the trust and safety team, the go-to-market team, the policy team, the safety folks, they're all trying to contribute to kind of the same goal, the same mission of the company.

我认为在 Anthropic，政治因素的影响很小。当然，我们每个人的视角都与普通人有所不同，我努力去记住这一点。这是因为我们注重谦逊。我们的面试流程以及在这里工作的人，几乎都会对政治因素产生排斥。团结非常重要。产品团队、研究团队、信任与安全团队、市场拓展团队、政策团队和安全人员，都在为实现公司共同的目标和使命而努力。

I think it's dysfunctional when different parts of the company think they're trying to accomplish different things, think the company is about different things, or think that other parts of the company are trying to undermine what they're doing. And I think the most important thing we've managed to preserve is, and again, things like the RSP drive it, this idea that it's not, you know, there's some parts of the company causing damage and other parts of the company trying to repair it, but that there are different parts of the company doing different functions, and they all function under a single theory of change.

我认为，当公司内部的不同部门各自为政，追求不同的目标，或者误认为其他部门在破坏他们的工作，这种情况是不正常的。我们成功保持的最重要的一点是，通过类似 RSP 的机制，我们并不是将公司分成一些造成损害的部分和一些修复损害的部分，而是让公司各个部门在不同职能上协作，并在一个统一的变革理论下共同运作。

Extreme pragmatism, right? Yeah. You know, the reason I went to OpenAI in the first place, you know, it was a nonprofit. It was a place where I could go and focus on safety. And I think over time, that maybe wasn't as good a fit. And there were some difficult decisions. And I think in a lot of ways, I really trusted Dario and Danielle on that. But I didn't want to leave. That was something that I think I was actually pretty reluctant to go along with. Because I think for one thing, I didn't know that it was good for the world to have more AI labs.

可以说，我当时是非常务实的。最初选择加入 OpenAI，是因为它作为一个非营利组织，能够让我专注于 AI 安全性。然而，随着时间的推移，我发现这与我的期待并不完全吻合，过程中也做出了一些艰难的决定。我非常信任 Dario 和 Danielle 的判断，但我并不想离开。实际上，我对离开感到相当犹豫，因为我不确定更多 AI 实验室的出现是否对世界有益。

And I think it was something that I was pretty reluctant for. And I think as well, when we did leave, I think I was reluctant to start a company. I think I was arguing for a long time that we should do a nonprofit instead and just focus on safety research. time that we should do a nonprofit instead and just focus on safety research. I think it really took pragmatism and confronting the constraints and just being honest about what the constraints implied for accomplishing that mission that led to Anthropic.

我当时对这个决定是相当不情愿的。当我们离开时，我也犹豫是否要创办一家公司。我一直主张应成立一个非营利组织，专注于安全研究。最终，正是因为我们务实地面对限制，诚实地评估这些限制对实现目标的影响，这才促成了 Anthropic 的成立。

I think just a really important lesson that we were good about early on is make less promises and keep more of them. Like try to be calibrated, be realistic, confront the trade-offs because trust and credibility are more important than any particular policy. It is so unusual to have what we have. And watching Mike Krieger defend safety things of reasons why we shouldn't ship a product yet, but also then to watch Binet say, okay, we have to do the right thing for the business. How do we get this across the finish line?

我认为，我们早期学到的一个非常重要的经验就是：少承诺，多兑现。保持校准和现实，勇于面对权衡，因为信任和信誉比任何具体政策都重要。我们所拥有的这种情况是很少见的。看到 Mike Krieger 为安全因素辩护，解释为何产品尚未发布，同时也看到 Binet 说，我们必须为企业做正确的事情，如何才能顺利完成任务。

And to hear people deep in the technical safety org talking about how it's also important that we build things that are practical for people and hearing engineers on inference talk about safety, that's amazing. I think that is, again, one of the most special things about working here is everybody with that unity is prioritizing the pragmatism, the safety, the business. That's wild.

听到技术安全部门的人谈论建造对人们实用的产品的重要性，以及推理领域的工程师关注安全性，真是令人惊喜。我认为，这再次表明了在这里工作的独特之处之一，就是大家共同优先考虑实用性、安全性和商业利益。这样的氛围实在是太难得了。

I think about it as spreading the trade-offs from just the leadership of the company to everyone, right? I think the dysfunctional world is like, you have a bunch of people who only see a big, you know, safety is like, we always have to do this. And product is like, we always have to do this. And research is like, you know, this is the only thing we care about. And then you're stuck at the top, right? You're stuck at the top. You have to decide between, you don't have as much information as either of them. That's the dysfunctional world.

我认为这就像是将权衡从仅仅是公司领导层的责任扩展到每个员工。在一个运作不正常的公司环境中，各个部门各自为政：安全部门坚持一贯的做法，产品部门也有固定的工作方式，而研究部门只关注自己的目标。作为领导者，你被困在决策顶端，需要在这些部门之间做出选择，但你并没有他们那么多的信息。这就是运作不正常的公司世界。

The functional world is when you're able to communicate to everyone. There are these trade-offs we're all facing together. The world is a far from perfect place. There's trade-offs. Everything you do is going to be suboptimal. Everything you do is going to be some attempt to get the best of both worlds that doesn't work out as well as you thought it was. And everyone is on the same page about confronting those tradeoffs together. And they just feel like they're confronting them from a particular post, from a particular job, as part of the overall job of confronting all the tradeoffs.

当你能够与所有人顺畅沟通时，这就是一个功能完善的世界。我们都在共同面对一些权衡取舍，因为现实世界并不完美。无论做什么决定，总会有不尽如人意的地方。每个人都在尝试追求两全其美的结果，但最终效果往往不如预期。大家在各自的岗位上，作为整体的一部分，一起面对这些权衡取舍。

It's a bet on Race to the Top, right? It's a bet on Race to the Top. Like, it's not a pure upside bet. Things could go wrong. But, like, we're all aligned on, like, this is the bet that we're making. It's a race to the top, right? It's a race to the top. It's not a pure upside bet. Things could go wrong. We're all aligned on this is the bet we're making. And markets are pragmatic. So the more successful and proper it becomes as a company, the more incentive there is for people to copy the things that make us successful.

这是一场「争取领先」的赌注，对吧？虽然这不是一个单纯的收益赌注，事情可能会出错，但我们都同意这是我们要做出的选择。市场很实际，因此公司越是成功和运作得当，其他人就越有动力去模仿我们的成功之道。

And the more that success is tied to actual safety stuff we do, the more it just creates a gravitational force in the industry that will actually get the rest of industry to compete. And it's like, sure, we'll build seatbelts and everyone else can copy them. That's good. That's like good world. That's really good. Yeah, this is the race to the top, right? But if you're saying, well, we're not going to build the technology, you're not going to build it better than someone else, that in the end, that just doesn't work because you're not proving that it's possible to get from here to there.

当成功与我们在安全领域的实际工作紧密联系时，它会在行业中形成一种引力，促使其他公司参与竞争。就像我们开发了安全带，其他人也可以模仿。这是好事，这意味着我们生活在一个更好的世界。这就是所谓的良性竞争，对吧？但是，如果你决定不去开发这项技术，而别人却做得比你更好，那最终是行不通的，因为你没有证明从起点到终点是可以实现的。

Where the world needs to get, never mind the industry, never mind one company, is it needs to get us successfully through from this technology doesn't exist to the technology exists in a very powerful way. And society has actually managed it. And I think the only way that's going to happen is that if you have, at the level of a single company and eventually at the level of the industry, you're actually confronting those tradeoffs. You have to find a way to actually be competitive, to actually lead the industry in some cases, and yet manage to do things safely.

世界必须实现的是，不仅仅限于某个行业或公司，而是成功地从「技术不存在」过渡到「技术强大存在」，并且社会能够有效管理它。我认为，要实现这一目标，必须在公司层面乃至整个行业层面，勇于面对权衡取舍。企业需要找到一种既能保持竞争力、在某些情况下引领行业，又能安全运行的方法。

And if you can do that, the gravitational pull you exert is so great. There's so many factors from the regulatory environment to the kinds of people who want to work at different places to even sometimes the views of customers that kind of drive in the direction of if you can show that you can do well on safety without sacrificing competitiveness, right? If you can find these kind of win-wins, then others are incentivized to do the same thing. Yeah, I mean, I think that's why getting things like the RRSP right is so important. Because I think that we ourselves, seeing where the technology is headed, have often thought, oh, wow, we need to be really careful of this thing.

如果能够做到这一点，那么你所产生的影响将是巨大的。无论是监管环境、员工选择的工作地点，还是客户的看法，这些因素都会产生影响。如果能够在保持竞争力的同时确保安全性，这将会是一个双赢的局面，从而激励其他人效仿。正因如此，像 RRSP 这样的措施显得尤为重要。因为我们在观察技术发展方向时，常常会意识到需要对这些问题保持高度的谨慎。

But at the same time, we have to be even more careful not to be crying wolf, saying that, like, innovation needs to stop here. We need to sort of find a way to make AI useful, innovative, delightful for customers, but also figure out what the constraints really have to be that we can stand behind that make systems safe so that what the constraints really have to be that we can stand behind that make systems safe so that it's possible for others to think that they can do that too, and they can succeed, they can compete with us. We're not doomers, right? We want to build the positive thing. We want to build the good thing. And we've seen it happen in practice. A few months after we came out with our RSP, the three most prominent AI companies had one, right?

但同时，我们也必须谨慎，不要妨碍创新的发展。我们需要找到一种方法，使 AI 既能为客户带来实用性、创新性和愉悦感，又能明确我们需要坚守的安全约束，以便让其他人也能相信他们能够做到，并且能够与我们竞争。我们不是末日论者，我们希望创造积极和有益的事物。我们已经在实践中看到这一点。就在我们推出 RSP 几个月后，三家最著名的 AI 公司也相继推出了类似产品，对吧？

Interpretability research. That's another area we've done it. Just the focus on safety overall, like collaboration with the AI safety institutes, other areas. Yeah, the Frontier Red team got cloned almost immediately, which is good. You want all the labs to be testing for, like, very, very scary risks. Export the seatbelts. Yeah. Export the seatbelts. Yeah, export the seatbelts. Well, Jack also mentioned it earlier, but customers also really care about safety, right? Customers don't want models that are hallucinating. They don't want models that are easy to jailbreak. They want models that are helpful and harmless, right?

我们也在进行解释性研究，这是我们关注的另一个领域。总体上，我们非常重视安全性，比如与 AI 安全研究机构的合作等方面。前沿红队几乎立刻就被复制了，这是个好现象。我们希望所有实验室都能检测出非常可怕的风险。我们要像出口安全带一样，推广这些安全措施。Jack 之前提到过，客户也非常关心安全性。他们不希望模型产生幻觉，也不希望模型容易被破解。他们希望模型既有用又无害。

And so a lot of the time what we hear in customer calls is just we're going with Claude because we know it's safer. I think that is also a huge market impact, right? Because our ability to have models that are trustworthy and reliable, that matters for the market pressure that it puts on competitors, too. Maybe to unpack something that Dario said a little bit more. I think there's this narrative or this idea that maybe the virtuous thing is to almost like nobly fail, right? It's like you should go and put safety, you should go and put things, you should sort of demonstrate like in an impragmatic way so that you can sort of demonstrate your purity to the cause or something like this.

在很多客户电话中，我们听到他们说「我们选择 Claude，因为它更安全。」这实际上对市场有很大影响。因为拥有值得信赖和可靠的模型，不仅对客户有吸引力，也给竞争对手带来了市场压力。可能需要更深入地分析 Dario 提到的观点。有一种说法是，或许最理想的做法是宁愿失败，也要坚持高标准。例如，强调安全性或其他原则，即便这些做法在实际中可能显得不切实际，但它们能体现出对事业的纯粹追求。

And I think if you do that, it's actually very self-defeating. For one thing, it means that you're going to have the people who are making decisions be self-selected for being people who don't care and for people who aren't prioritizing safety and who don't care about it. And I think on the other hand, if you try really hard to find the way to align the incentives and make it so that if there are hard decisions, they happen at the points where there is the most force to go and support making the correct hard decisions and where there's the most evidence. Then you can sort of start to trigger this race to the top that Dario is describing, where instead of going and having the people who care get pushed out of influence, you instead pull other people to have to go and follow.

我认为如果这么做，其实是适得其反的。这样做的结果是，让那些不重视安全、不优先考虑安全的人来做决策。而另一方面，如果努力去找到一种方法来协调激励措施，确保在做出困难决策时，有足够的支持和证据来支持正确的决策，那么就可以引发 Dario 所说的积极竞争氛围。这样一来，不是让关心安全的人被排挤出决策圈，而是吸引其他人也加入到重视安全的行列中。

So what are you all excited about when it comes to the next things we'll be working on? I think there's a bunch of reasons you can be excited about interoperability. One is obviously safety. But there's another one that I think I find at an emotional level equally exciting or equally meaningful to me, which is just that I think neural networks are beautiful. And I think that there's a lot of beauty in them that we don't see. We treat them like these black boxes that we're not particularly interested in the internal stuff. But when you start to go and look inside them, they're just full of amazing, beautiful structure.

对于我们即将开展的新工作，大家有什么让人兴奋的地方呢？我认为有很多理由可以让人对互操作性感到期待。首先是安全性，这是显而易见的。但在我看来，还有一个同样令人振奋而且有意义的原因，那就是神经网络的美。我觉得我们往往忽略了其中的美丽。通常，我们把神经网络视为黑箱，对其内部机制不甚关心。然而，当你深入研究它们时，会发现其中包含着令人惊叹的美丽结构。

You know, it's sort of like if people looked at biology and they were like, you know, like evolution is really boring. It's a simple thing that goes and runs for a long time and then it makes animals. And instead, it's like actually each one of those animals that evolution produces, and I think that it's an optimization process, like training a neural network. They're full of incredible complexity and structure. And we have an entire sort of artificial biology inside of neural networks. If you're just willing to look inside them, there's all of this amazing stuff. And I think that we're just starting to slowly unpack it. And it's incredible. And there's so much there. But there's just so much to be discovered there. We're just starting to crack it open.

你知道，这就像如果人们看待生物学时说，进化是一件无聊的事情。它是一个运行很长时间的简单过程，最后才形成动物。其实，每一种进化产生的动物就像是在训练一个神经网络，它们都是一种优化过程，充满了复杂性和结构。在神经网络中，我们可以发现一种完整的「人工生物学」。如果你愿意去探索其中的奥秘，就会发现许多惊人的细节。我认为，我们才刚刚开始揭开这些奥秘，这真是令人惊叹。还有许多未知的领域等待我们去发现，我们才刚刚开始探索。

And I think it's going to be amazing and beautiful. And sometimes I imagine, you know, like a decade in the future, walking into a bookstore and buying, you know, the textbook on neural network interpolability or really like on the biology of neural networks and just the kind of wild things that are going to be inside of it. And I think that in the next decade, we're going to, in the next couple of years, even we're going to go and start to go and really discover all of those things. And it's going to be wild and incredible. It's also going to be great that you get to buy your own textbook.

我认为这将会是令人惊叹且美丽的。有时我会想象，比如十年后，走进书店，买一本关于神经网络互操作性或神经网络生物学的教科书，其中将包含各种令人惊讶的内容。我相信在未来十年，甚至在接下来的几年里，我们将开始真正发现这些神奇的事物。那将是令人惊喜且难以置信的。能够买到自己的教科书也是一件很棒的事情。

I'm excited that a few years ago, if you had said, like, governments will set up new bodies to like test and evaluate AI systems, and they will actually be competent and good, you would have not thought that was going to be the case. But it's happened. And it's kind of like governments have built these new embassies almost to deal with this new kind of class of technology or like thing that Chris studies. And I'm just very excited to see where that goes. I think it actually means that we have state capacity to deal with this kind of societal transition so it's not just companies. And I'm excited to help with that.

几年前，如果有人说政府会成立新机构来测试和评估 AI 系统，而且这些机构非常有能力，你可能不会相信。但这已经成为现实。政府就像设立了新的大使馆，专门应对这类新技术或 Chris 所研究的领域。我很高兴看到这种发展。这实际上表明我们具备国家的能力来应对这样的社会转型，而不仅仅依赖公司。我很高兴能够参与其中。

I'm already excited about this to a certain extent today, but I think just imagining the future world of what AI is going to be able to do for people, it's impossible to not feel excited about that. Dario talks about this a lot, but I think even just the sort of glimmers of Claude being able to help with vaccine development and cancer research and biological research is crazy, just to be able to watch what it can do now. But when I fast forward three years in the future or five years in the future, imagining that Claude could actually solve so many of the fundamental problems that we just face as humans, just even just from a health perspective alone, even if you sort of take everything else out, feels really exciting to me, just like thinking back to my international development times, it would be amazing if Claude was responsible for helping to do a lot of the work that I was trying to do a lot less effectively when I was like 25.

今天我已经对此感到非常兴奋，而想象一下未来 AI 能为人类带来的改变，更是让人无法抑制这种兴奋之情。Dario 经常谈到这个话题，我觉得，即便只是看到 Claude 在疫苗开发、癌症研究和生物研究中展现出的初步能力，就已经很惊人了。现在能看到它的表现，已经让人觉得不可思议。展望三年或五年后的未来，设想 Claude 可以解决我们人类所面临的许多根本问题，尤其是从健康角度来看，即使忽略其他方面，也让我感到异常兴奋。回想起我曾经从事国际发展的日子，如果 Claude 能够完成我在 25 岁时努力却不太成功的工作，那将是非常了不起的。

I mean, I guess similarly, I'm excited to build Claude for work. I'm excited to build Claude into the company and into companies all over the world. I guess I'm excited just for I guess like personally I like using Claude a lot. Definitely, there's been increasing amounts of home times with me just chatting with Claude about stuff. I think the biggest recent thing has been code, where six months ago, I didn't use Claude to do any coding work. Our teams didn't really use Claude that much for coding. And now it's just a phase difference. I gave a talk at YC a week before last. And at the beginning, I just asked, how many folks here use Claude for coding now? And literally 95% of hands. All the hands in the room, which is totally different than how it was four months ago.

我很兴奋能够将 Claude 集成到我的工作中，并推广到全球的企业。我个人非常喜欢使用 Claude，最近我常常在家里花时间和 Claude 聊天。过去最大的变化是在编程方面。六个月前，我和团队几乎没有用 Claude 来编程，而现在情况截然不同。前两周我在 YC 做了一个演讲，开场时我问有多少人在用 Claude 编程，结果几乎所有人都举手，这与四个月前的情况完全不同。

So when I think about what I'm excited about, I think about places where, you know, like I said before, where there's this kind of consensus that, again, seems like consensus, seems like what everyone wise thinks, and then it just kind of breaks. And so places where I think that's about to happen and it hasn't happened yet. One of them is interpretability. I think interpretability is both the key to steering and making safe AI systems, and we're about to understand. And interpretability contains insights about intelligent optimization problems and about how the human brain works.

所以，当我想到让我感到兴奋的事物时，我会想到那些地方。就像之前提到的，那里似乎达成了一种共识，看起来像是所有智慧的人所认同的观点，然后这种共识突然瓦解。我认为即将发生这种变化的领域之一是可解释性。我相信，可解释性不仅是引导和确保 AI 系统安全的关键，我们也即将对此有更深入的理解。可解释性还揭示了关于智能优化问题和人类大脑工作方式的见解。

I've said, and I'm really not joking, Chris Ola is gonna be a future Nobel medicine laureate. I'm serious. I'm serious because a lot of these, I used to be a neuroscientist, a lot of these mental illnesses, the ones we haven't figured out, right? Schizophrenia or the mood disorders. I suspect there's some higher level system thing going on and that it's hard to make sense of those with brains because brains are so mushy and hard to open up and interact with. Neural nets are not like this. They're not a perfect analogy, but as time goes on, they will be a better analogy.

我曾说过，我绝不是在开玩笑，Chris Ola 有望在未来获得诺贝尔医学奖。我是认真的。之所以这样说，是因为作为一名前神经科学家，我对许多精神疾病有过研究，尤其是那些我们尚未完全理解的疾病，比如精神分裂症和情绪障碍。我怀疑在这些疾病背后，可能存在某种更高层次的系统性问题。而通过研究大脑来理解这些问题非常困难，因为大脑结构复杂，不易直接分析和交互。神经网络则不同。虽然它们不是对大脑的完美类比，但随着技术的发展，它们将成为更有效的研究工具。

That's one area. Second is related to that, I think, just the use of AI for biology. Biology is an incredibly difficult problem. People continue to be skeptical for a number of reasons. I think that consensus is starting to break. We saw a Nobel Prize in Chemistry awarded for AlphaFold, remarkable accomplishment. We should be trying to build things that can help us create 100 AlphaFolds. And then finally, using AI to enhance democracy. We worry about if AI is built in the wrong way, it can be a tool for authoritarianism. How can AI be a tool for freedom and self-determination? I think that one is earlier than the other two, but it's going to be just as important.

这是一个领域。其次是与此相关的领域 —— 在生物学中应用 AI。我认为生物学是一个极其复杂的学科，长期以来由于多种原因，人们一直对其持怀疑态度。然而，我认为这种看法正在改变。诺贝尔化学奖授予了 AlphaFold，这是一项非凡的成就。我们应该努力开发能够创造出 100 个 AlphaFold 的技术。最后，AI 在增强民主方面的应用也值得关注。我们担心如果 AI 的构建方式不当，它可能被用作独裁的工具。那么，AI 如何能成为促进自由和自我决策的工具呢？我认为这个领域的发展可能还在早期，但同样重要。

Yeah, I mean, I guess two things that at least connect to what you were saying earlier. I mean, one is I feel like people frequently join Anthropic because they're sort of scientifically really curious about AI and then kind of get convinced by AI progress to sort of share the vision of the need, not just to advance the technology, but to understand it more deeply and to make sure that it's safe. And I feel like it's actually just sort of exciting to have people that you're working with, like kind of more and more united in their vision for both what AI development looks like and the sort of sense of responsibility associated with it.

有两件事至少与您之前提到的内容有关。首先，我发现很多人加入 Anthropic 是因为他们对 AI 科学有浓厚的兴趣，然后逐渐被 AI 的发展所吸引，不仅想推动技术进步，还希望更深入地理解并确保其安全性。其次，与一群对 AI 开发理念和责任感有共同认识的人一起工作，确实让人感到振奋。

And I feel like that's been happening a lot due to a lot of advances that have happened in the last year, like what Tom talked about. Another is that, I mean, going back really to concrete problems, I feel like we've done a lot of work on AI safety up until this point. A lot of it's really important. But I think we're now, with some recent developments, really getting a glimmer of what kinds of risks might literally come about from systems that are very, very advanced so that we can investigate and study them directly with interpretability, with other kinds of safety mechanisms, and really understand what the risks from very advanced AI might look like. And I think that that's something that is really going to allow us to sort of further the mission in a really deeply scientific empirical way. And so I'm excited about sort of the next six months of how we use our understanding of what can go wrong with advanced systems to characterize that and figure out how to avoid those pitfalls. Perfect. Finn. We did it! Good time. We've got to do this for often. This is the only time we ever get to catch up.

我觉得这在过去一年里发生得很频繁，正如汤姆所提到的，由于许多技术进步使然。另一个方面是，谈到具体问题时，我认为我们在 AI 安全领域已经取得了许多重要进展。然而，随着最近的一些发展，我们开始真正看到可能会从非常先进的系统中出现的风险。这样我们就能通过可解释性和其他安全机制直接研究这些风险，并深入了解这些极其先进的 AI 系统可能带来的威胁。我相信这将使我们能够以一种科学实证的方式来推进这项使命。因此，我对接下来六个月感到兴奋，因为我们将利用我们对这些先进系统潜在问题的理解来确定并避免可能的陷阱。太好了，Finn。我们做到了！这是一次愉快的时光。我们必须经常这样聚一聚，因为这可是我们唯一能好好交流的机会。