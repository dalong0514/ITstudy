## 20241221Building-Anthropic-A-Conversation-With-Our-Co-Founders

[Building Anthropic | A conversation with our co-founders - YouTube](https://www.youtube.com/watch?v=om2lIWXLLN4)

SPEAKER_01 位于左 2，Jack Clark

SPEAKER_02 位于右 1，Jared Kaplan

SPEAKER_03 位于右 2，Dario Amodei，Anthropic 的 CEO，就是之前发表对未来 AI 看法长篇的那个大牛，没想到这么年轻。

SPEAKER_04 位于右 3，黑色衣服的

SPEAKER_05 位于右 4，Sam McCandlish，中间的那位，墨绿色卫衣

SPEAKER_06 位于左 1，Chris Olah

SPEAKER_07 位于左 3，Daniela Amodei，唯一的那位女士

---

SPEAKER_01(Jack Clark): Why are we working on AI in the first place?

为什么我们一开始要研究 AI 呢？

I'm just going to arbitrarily pick Jared. Why are you doing AI at all? 

就拿 Jared 来说吧，你为什么选择从事 AI 工作？

SPEAKER_02(Jared Kaplan): I mean, I was working on physics for a long time, and I got bored, and I wanted to hang out with more of my friends. 

我自己在物理领域工作了很长时间，后来感觉有些厌倦，也想多和朋友们交流。

SPEAKER_04: I thought Dario pitched you on it.

我以为是 Dario 向你推销的。

SPEAKER_03(Dario Amodei): I don't think I explicitly pitched you any point. I just kind of showed you results of AI models and was trying to make the point that they're very general and they don't only apply to one thing. And then just at some point after I showed you enough of them, you were like, oh, yeah, that seems like it's right. 

我自己并没有特别地向你推销过。我只是展示了一些 AI 模型的成果，想说明这些模型非常通用，不仅限于单一应用。后来，在我展示了足够多的成果之后，你就认为这确实是个不错的方向。

SPEAKER_04: How long have you been a professor when you started?

你当教授的时候已经有几年经验了？

SPEAKER_02(Jared Kaplan): I think like six years or so. I think I helped recruit Sam. 

大概六年左右吧。我记得我参与了招募 Sam 的过程。

SPEAKER_05(Sam McCandlish): I talked to you and you were like, I think I've created a good bubble here. And my goal is to get Tom to come back. And then it worked.

我跟你聊过，你说：「我觉得我在这里营造了一个很好的氛围，我的目标是让 Tom 回来。」结果成功了。

SPEAKER_01(Jack Clark): And did you meet everyone through Google when you were doing the interpretability stuff, Chris?

Chris，你是在 Google 做可解释性研究时认识所有人的吗？

SPEAKER_06(Chris Olah): No. So I guess I actually met a bunch of you when I was 19. And I was visiting the Bay Area for the first time. So I guess I met Dario and Jared then. And I guess they were postdocs, which I thought was very cool at the time.

不是的。我其实在 19 岁第一次访问湾区时就认识了你们中的很多人。当时我遇到了 Dario 和 Jared，他们是博士后，我觉得这很酷。

And then I was working at Google Brain. And Dario joined. And we sat side by side, actually, for a while. We had desks beside each other and I worked with Tom there as well. And then of course I went to work with all of you at OpenAI when I went there. So I guess I've known a lot of you for like more than a decade, which is kind of wild. 

后来我在 Google Brain 工作，Dario 加入了我们。我们曾经并肩坐了一段时间，桌子就在一起，我也与 Tom 合作过。后来，我加入 OpenAI，与大家一起工作。我想，我认识你们之中的很多人已经超过十年了，真是不可思议。

SPEAKER_01(Jack Clark): If I remember correctly, I met Dario in 2015 when I went to a conference you were at, and I tried to interview you, and Google PR said I would have read all of your research papers.

如果我没记错的话，我是在 2015 年与 Dario 相识的。当时我参加了一个会议，您也在场，我尝试采访您，但 Google 的公关建议我应该先阅读您所有的研究论文。

SPEAKER_03(Dario Amodei): Yeah, I think I was writing concrete problems in AI safety when I was at Google. I think you wrote a story about that paper.

是的，我记得在 Google 工作时，我正专注于撰写有关 AI 安全的具体问题。我想你写过关于那篇论文的报道。

SPEAKER_01(Jack Clark): I did. 

确实，我写过。

SPEAKER_05(Sam McCandlish): I remember right before I started working with you, I think you invited me to the office to come chat and just tell me everything about AI. And you explained. I remember afterwards being like, oh, I guess this stuff is much more serious than I realized. And you were probably explaining the big blob of compute and parameter counting and how many neurons are in the brain and everything. 

我还记得在开始和你合作之前，你邀请我去办公室聊天，向我详细介绍了 AI 的一切。你解释得很清楚，让我意识到这些问题比我想象的要严峻得多。你可能当时讲解了大量计算、参数计数以及大脑中神经元的数量等等。

SPEAKER_06(Chris Olah): I feel like Dario often has that effect on people. This is much more serious than I realized.

我觉得 Dario 经常给人这样的印象，就是这些问题比我们意识到的要严重得多。

SPEAKER_03(Dario Amodei): Yeah. I'm the bringer of happy tidings.

是的，我就像个「快乐消息」的传递者。

SPEAKER_01(Jack Clark): But I remember when we were at OpenAI, where there was the scaling laws stuff and just making things bigger, and it started to feel like it was working. And then it kind of kept on eerily working on a bunch of different projects, which I think is how we all ended up working closely together, because it was first GPT-2, and then scaling laws and GPT-2, and then scaling laws and GPT-3, and we ended up being a big group of people.

我记得当我们在 OpenAI 工作时，我们讨论了关于扩展规律（scaling laws）的研究，只是简单地把模型做得更大，结果开始显得很有效。后来，这种方法在许多不同的项目中似乎持续有效，这可能也是我们最终紧密合作的原因之一。最初是 GPT-2，然后是扩展规律结合 GPT-2，接着是扩展规律结合 GPT-3，最终我们形成了一个大型的合作团队。

SPEAKER_03(Dario Amodei): Yeah, we were the plop of people that were making things work.

SPEAKER_03：是的，我们就是那些让事情顺利进行的人。

SPEAKER_02(Jared Kaplan): That's right. I think we were also excited about safety, because in that era, there was sort of this idea that AI would become very powerful, but potentially not understand human values or not even be able to communicate with us. And so I think we were all pretty excited about language models as a way to kind of guarantee that AI systems would have to understand kind of implicit knowledge.

SPEAKER_02：没错。我想我们那时也对安全性非常关注，因为那个年代有一种观点认为，AI 可能会变得非常强大，但可能无法理解人类的价值观，甚至无法与我们沟通。因此，我觉得我们对大语言模型特别感兴趣，因为它们能确保 AI 系统理解隐含的知识。

SPEAKER_03(Dario Amodei): And RL for human feedback on top of language models, which was the whole reason for scaling these models up was that the models weren't smart enough to do RLHSF on top of. So that's the kind of intertwinement of safety and scaling of the models that we still believe in today.

SPEAKER_03：基于人类反馈的强化学习（RL for Human Feedback）是建立在语言模型之上的，扩展这些模型规模的主要原因是因为模型不够智能，无法支持 RL for Human Feedback。这体现了我们今天仍然相信的模型安全性与规模扩展的密切关系。

SPEAKER_06(Chris Olah): I think there was also an element of the scaling work was done as part of the safety team that Dario started at OpenAI because we thought that forecasting AI trends was important to be able to have us take them seriously and take safety seriously as a problem.

我认为，规模化工作的一个方面是在 Dario 在 OpenAI 发起的安全团队中完成的。我们认为，预测 AI 趋势对于我们认真对待这些趋势和安全问题是至关重要的。

SPEAKER_01(Jack Clark): Correct. Yeah, I mean, we took, I remember being in some airport in England sampling from GPT-2 and using it to write fake news articles and slacking Dario and being like, oh, this stuff actually works and might have like huge policy implications. I think Dario said something like, yes.

SPEAKER_01：没错。我记得我们在英国的某个机场，用 GPT-2 来生成虚假新闻文章，并通过 Slack 联系了 Dario，告诉他这项技术确实有效，并可能带来重大的政策影响。我记得 Dario 的回应是肯定的。

It's a typical way. But then we worked on that a bunch, as well as the release stuff, which was kind of wild. Yeah, I remember the release stuff. I think that was when we first started working together. That was a fun time, the GPT-2 launch. 

这是一个常见的方法。后来我们在这方面做了很多工作，还有发布的事情，确实有些疯狂。是的，我记得发布的事情。我想那是我们第一次开始合作的时候。GPT-2 的发布真是段有趣的时光。

SPEAKER_01(Jack Clark): Yeah, but I think it was good for us, because we did a kind of slightly strange, safety-oriented thing all together, and then we ended up doing Anthropic, which is a much larger, slightly strange strange safety-oriented thing altogether. And then we ended up doing Anthropic, which is a much larger, slightly strange safety-oriented thing altogether.

是的，我认为这对我们有利，因为我们曾经一起做了一些稍显奇怪但注重安全的事情，最终我们发展出了 Anthropic，这是一项更大规模且更具奇特性的安全项目。

SPEAKER_04: So I guess just going back to the concrete problems, because I remember I joined OpenAI in 2016, one of the first 20 employees or whatever with Udario. And I remember at that time the concrete problems in AI safety seemed like it was the first mainstream AI safety paper. I don't really know if I ever asked you what the story was for how that came about. 

SPEAKER_04：所以我想回到具体的问题，因为我记得我是在 2016 年加入 OpenAI，当时是最早的 20 名员工之一，还有 Udario。我记得那时关于 AI 安全的具体问题似乎是第一篇被广泛接受的 AI 安全论文。我不太确定我是否曾问过你，这篇论文是如何酝酿出来的。

SPEAKER_03(Dario Amodei): Chris knows the story because he was involved in it. I think we were both at Google. I forget what other project I was working on. But like with many things, it was my attempt to kind of procrastinate from whatever other project I was working on that I've now completely forgotten what it was. But I think it was like Chris and I decided to write down what are some open problems in terms of AI safety? And also, AI safety is usually talked about in this very kind of abstruse, abstract way.

Chris 知道这个故事，因为他参与其中。我记得当时我们都在 Google，而我已经忘记了自己正在从事什么其他项目。像很多时候一样，我那时可能是想逃避手头的工作。于是，我和 Chris 决定列出一些关于 AI 安全的未解决问题。此外，AI 安全经常以一种深奥而抽象的方式进行讨论。

Can we kind of ground it in the ML that was going on at the time? I mean, now there's been like, you know, six, seven years of work in that vein, but it was almost a strange idea at the time.

我们能不能把这个想法和当时的机器学习（ML）研究联系起来呢？我的意思是，现在这个领域已经有六七年的发展了，但在当时，这个想法几乎显得很新奇。

SPEAKER_06(Chris Olah): Yeah, I think there's a way in which it was almost like kind of political project, where at the time, a lot of people didn't take safety seriously. So I think that there was sort of this goal to collate a list of problems that sort of people agreed were reasonable, often already existed in literature, and then get a bunch of people across different institutions who are credible to be authors. And I remember I had this whole long period where I just talked to 20 different researchers at Brain to build support for publishing the paper. In some ways, if you look at it in terms of the problems and a lot of things that emphasized, I think it hasn't held up that well in that it's, you know, I think it's not really the right problems. But I think if you sort of see it instead as a consensus building exercise that there's something here that is real and that is worth taking seriously, then it was a pretty important moment.

是的，我认为这几乎是一个政治项目，当时很多人并没有认真对待安全性问题。所以我认为当时的目标是整理出一份大家普遍认为合理的问题清单，这些问题通常已经存在于文献中，然后让来自不同机构的可信作者参与其中。我记得我曾经花了很长时间，与 Brain 的 20 位不同研究人员交谈，为了获得对发表论文的支持。从某种程度上看，如果你从问题和强调的许多方面来看，我认为它没有很好地经受住考验，因为我认为这些问题并不是真正合适的问题。但我认为，如果把它看作是一个建立共识的过程，说明这里有一些值得认真对待的真实问题，那么这确实是一个相当重要的时刻。

SPEAKER_01(Jack Clark): I mean, you end up in this really weird sci-fi world where I remember at the start of Anthropic, we were talking about constitutional AI. And I think Jared said, oh, we're just going to write like a constitution for a language model and that'll change all of its behavior. And I remember that sounded like incredibly crazy at the time. But why did you guys think that was going to work? Because I remember that was one of the first early like big research ideas we had at the company.

SPEAKER_01：我是说，进入这样一个科幻般的世界，我记得在 Anthropic 成立之初，我们讨论了宪法 AI（constitutional AI）。我记得 Jared 说，我们要为一个语言模型制定一部宪法，这将改变它的所有行为。当时我觉得这听起来非常不可思议。但你们为什么认为这会奏效呢？因为我记得那是我们公司早期的一个重大研究想法。

SPEAKER_02(Jared Kaplan): Yeah, I mean, I think Dario and I had talked about it for a while. I think simple things just work really, really well in AI. I think the first versions of that were quite complicated, but then we kind of whittled away into just use the fact that AI systems are good at solving multiple choice exams and give them a prompt that tells them what they're looking for. And that was kind of a lot of what we needed. And then we were able to just write down these principles.

SPEAKER_02：是的，其实我和 Dario 已经讨论这个话题有一段时间了。我发现简单的方法在 AI 中非常有效。虽然最初的版本比较复杂，但我们后来简化为利用 AI 系统擅长解决多项选择题的能力，并通过提示引导它们。这基本上满足了我们的需求，然后我们就可以将这些原则记录下来。

SPEAKER_03(Dario Amodei): I mean, it goes back to the big blob of compute or the bitter lesson or the scaling hypothesis. If you can identify something that you can give the AI data for and that's kind of a clear target, you'll get it to do it. So here's this set of instructions. Here's this set of principles. AI language models can like read that set of principles and they can like compare it to the behavior that they themselves are engaging in. So like, you've got your training target there. So once you know that, I think my view and Jared's view is there's a way to get it to work. You just have to fiddle with enough of the details.

SPEAKER_03：我的意思是，这可以追溯到大规模计算、痛苦的教训或者扩展假说。如果你能找到一个明确的目标，给 AI 提供数据，那么它就能完成任务。比如，这里有一套指令和原则，AI 语言模型可以读取这些原则，并将其与自身的行为进行比较。因此，你有了训练目标。在了解这一点后，我和 Jared 都认为，只要调整足够多的细节，就能让它奏效。

SPEAKER_02(Jared Kaplan): Yeah. I think it was always weird for me, especially in these early eras, because I was in physics and then coming from physics. And I think now we forget about this because everyone's excited about AI. But I remember talking to Dario about concrete problems and other things. And I just got the sense that AI researchers were very, very kind of psychologically damaged by the AI winter, where they just kind of felt like having really ambitious ideas or ambitious visions was very disallowed. And that's kind of how I imagine it was in terms of talking about safety. In order to care about safety, you have to believe that AI systems could actually be really powerful and really useful. And I think that there was kind of a prohibition and against being ambitious. And I think one of the benefits is that physicists are very arrogant. And so they're constantly doing really ambitious things and talking about things in terms of grand schemes. And so, yeah.

SPEAKER_02：是的，我一直觉得这有点奇怪，尤其是在早期阶段，因为我本来是在研究物理，然后转到了这个领域。现在由于大家都对 AI 很感兴趣，我们可能已经忘记了这一点。但我记得曾经和 Dario 讨论过一些具体问题，我感到 AI 研究人员在经历了所谓的「AI 冬天」后，心理上受到了很大影响。「AI 冬天」是指 AI 研究一度停滞不前的时期，这让他们觉得有雄心壮志的想法和愿景是不被认可的。这也影响了关于安全性的讨论。要真正关心 AI 的安全性，你得相信 AI 系统能变得非常强大和有用。但是那时，似乎存在一种不鼓励雄心壮志的氛围。相比之下，物理学家通常比较自信，总是敢于尝试具有雄心的计划，并以宏伟的方式讨论问题。所以，是的。

SPEAKER_03(Dario Amodei): I mean, I think that's I think that's definitely true. Like I remember in 2014, it was like there were just like, I don't know, there were just like some things you couldn't say, right. But but I actually think it was kind of an extension of problems that exist across academia other than maybe theoretical physics, where they've kind of evolved into very risk-averse institutions for a number of reasons. And even the industrial parts of AI had kind of transplanted or forklifted that mentality. And it took a long time. I think it took until like 2022 to get out of that mentality.

我认为这确实是事实。我记得在 2014 年的时候，有些话题是不能公开讨论的。但是我觉得，这实际上反映了一个普遍存在于学术界的问题，特别是除了理论物理学之外的其他领域。由于多种原因，学术机构变得非常规避风险。甚至 AI 的工业领域也受到了这种心态的影响。我认为这种心态直到 2022 年才开始有所改变。

SPEAKER_06(Chris Olah): There's a weird thing about like, what does it mean to be conservative and respectable, where you might think like one version you could have is that what it means to be conservative is to take the risks or the potential harms of what you're doing really seriously and worry about that. But another kind of conservatism is to be like, ah, you know, taking an idea too seriously and believing that it might succeed is sort of like scientific arrogance. And so I think there's like kind of two different kinds of conservatism or caution. And I think we were sort of in a regime that was very controlled by that one. You see it historically, right? If you look at the early discussions in 1939 between people involved in nuclear physics about whether nuclear bombs were sort of a serious concern. You see exactly the same thing with Fermi resisting these ideas because it just seemed kind of like a crazy thing. And other people like Zillard or Teller taking the ideas seriously because they were worried about the risks.

SPEAKER_06：关于保守和受人尊敬的含义，有一个有趣的现象。有人可能认为，保守意味着非常认真地对待自己所做事情的风险或潜在危害，并对此表示担忧。但还有一种保守主义的观点是，如果过于认真地对待一个想法并相信它可能会成功，这可能表现为科学上的自负。所以我认为，这里存在两种不同类型的保守主义或谨慎态度。在历史上，我们可以看到类似的情况。例如，在 1939 年，核物理学界的人们对于核弹是否是一个严重问题展开了早期讨论。费米反对这些想法，因为在他看来这似乎过于疯狂。而其他人如齐拉德或泰勒则认真对待这些想法，因为他们对潜在风险感到担忧。

SPEAKER_03(Dario Amodei): Perhaps the deepest lesson that I've learned in the last 10 years, and probably all of you have learned some form of it as well, is there can be this kind of seeming consensus, these things that kind of everyone knows that, I don't know, seem sort of wise, seem like they're common sense, but really they're just kind of hurting behavior masquerading as maturity and sophistication. And when you've seen, the consensus can change overnight.

SPEAKER_03：在过去十年中，我学到的最深刻的教训之一，可能大家也有类似的体会，就是我们常常会遇到一些看似普遍的共识。这些共识似乎充满智慧，被当作常识，但实际上它们只是披着成熟和复杂外衣的从众行为。你会发现，这种共识可能会在一夜之间发生改变。

And when you've seen it happen a number of times, you suspected, but you didn't really bet on it. And you're like, oh, man, I kind of thought this, but what do I know? How can I be right and all these people are wrong? You see that a few times, then you just start saying, no, this is the bet we're going to make. I don't know for sure if we're right, but just ignore all this other stuff, see it happen. And I don't know, even if you're right 50% of the time, being right 50% of the time contributes so much, right? You're adding so much that is not being added by anyone else.

当你多次注意到这种情况时，你可能会怀疑，但不一定会立刻做出决定。你可能会想，哦，我有这样的感觉，但我能知道多少呢？我怎么可能正确，而这么多人都是错的呢？经过几次这样的经历后，你可能会坚定地说，这就是我们要做的选择。我不确定我们是否正确，但可以忽略其他干扰，专注于看到的事实。即使你的判断只有 50% 是正确的，这种准确性仍然会带来巨大的贡献。因为你正在做出其他人没有做到的增益。

SPEAKER_01(Jack Clark): And it feels like that's where we are today with some safety stuff, where there's like a consensus view that a lot of this safety stuff is unusual or doesn't naturally fall out of the technology. And then at Anthropic, we do all of this research where weird safety misalignment problems fall out as a natural dividend of the tech we're building. So it feels like we're in that counter consensus view right now. 

SPEAKER_01：现在，我们处理安全问题时普遍认为这些问题有些反常，不是技术自然带来的。然而，在 Anthropic，我们的研究发现，这些奇怪的安全不匹配问题实际上是我们构建技术的自然结果。所以，我们现在似乎持有一种与普遍观点相反的看法。

SPEAKER_07(Daniela Amodei): But I feel like that has been shifting over the past, even just like 18. 

SPEAKER_07：但我感觉在过去的时间里，这种情况已经在发生变化，甚至可能是过去的 18 个月。

SPEAKER_01(Jack Clark): We've been helping to shift. We've definitely been publishing. Publishing and doing research. Constant force. 

SPEAKER_01：我们一直在帮助推动这种变化。我们确实一直在发表论文和进行研究，成为一股持续的推动力量。

SPEAKER_07(Daniela Amodei): But I also think just world sentiment around AI has shifted really dramatically. And it's more common in the user research that we do to hear just customers, regular people, say, I'm really worried about what the impact of AI on the world more broadly is going to be.

SPEAKER_07：但我也认为，全球对 AI 的看法已经发生了显著变化。在我们进行的用户研究中，越来越多的普通用户表示，他们非常担心 AI 对世界产生的更广泛影响。

And sometimes that means jobs or bias or toxicity. But it also sometimes means, is this just going to mess up the world? How is this going to contribute to fundamentally changing how humans work together, operate? I wouldn't have predicted that, actually.

有时这可能与工作、偏见或毒性有关。但也有时候，问题在于这是否会扰乱世界？它将如何从根本上改变人类的合作与运作方式？其实，我并没有预料到这一点。

SPEAKER_05(Sam McCandlish): For whatever reason, it seems like people in the ML research sphere have always been more pessimistic about AI becoming very powerful than the general public.

SPEAKER_05：无论出于何种原因，似乎机器学习研究领域的人总是比普通大众对 AI 发展得非常强大更加悲观。

SPEAKER_07(Daniela Amodei): Maybe it's a weird form of humility or something. 

SPEAKER_07：也许这是一种奇怪的谦逊表现。

SPEAKER_01(Jack Clark): And when Daria and I went to the White House in 2023, in that meeting, Harris and Raimondo and stuff basically said, paraphrase, but basically said, we've got our eye on you guys. AI is going to be a really big deal, and we're now actually paying attention, which is.

SPEAKER_01：当 Daria 和我在 2023 年拜访白宫时，那次会议上，Harris 和 Raimondo 等人基本上表示，他们在关注我们。AI 将会成为一个非常重要的领域，我们现在确实在密切关注，这确实是。

SPEAKER_03(Dario Amodei): And they're right. They're absolutely right.

SPEAKER_03：他们是对的，确实如此。

SPEAKER_01(Jack Clark): But I think in 2018, you wouldn't have been like, the president will call you to the White House to tell you they're paying close attention to the development of language models. Yeah. That was not a big event. Like in 2018.

SPEAKER_01：但是我觉得，在 2018 年，你不会想到总统会邀请你去白宫，并告诉你他们正在密切关注语言模型的发展。是的，那在 2018 年并不是一个大事件。

SPEAKER_04: One thing that I think is interesting too, is I guess all of us kind of got into this when it didn't seem like there was like, like we thought, like we thought, we thought that it could happen, but yeah, it was, it was like, like fair me being like skeptical of the atomic bomb. It was like, he had, he was just a good scientist and like, there was some evidence that it could happen, but there also was a lot of evidence against it happening.

SPEAKER_04：我觉得有趣的是，我们所有人似乎都是在一种不确定的情况下进入这个领域的。当时我们认为这可能会发生，但也有很多怀疑和反对意见。这让我想起了一个优秀的科学家对原子弹的怀疑：虽然有证据表明它可能发生，但也有许多证据表明它不可能。

And he, I guess decided that it would be worthwhile because if it was true, then it would be a big deal. And I think for all of us, it was like 2015, 2016, 2017. There was some evidence and increasing evidence that this might be a big deal. But I remember in 2016, talking to all my advisors, and I was like, I've done startup stuff. I want to help out with AI safety, but I'm not great at math. I don't exactly know how I can do it. And I think at the time, people were like, either were like, well, you need to be super good at decision theory in order to help out. And I was like, eh, it's probably not going to work.

我想他认为这件事值得去做，因为如果这是真的，那将会很重要。我记得在 2015 年到 2017 年之间，有一些证据逐渐增多，显示这可能会成为一件大事。在 2016 年，我和所有的导师讨论过，我说我做过创业，现在想要投入到 AI 安全领域，但我数学不是很强，不知道自己能做些什么。当时，有人告诉我，必须非常精通决策理论才能有所贡献，我心想，这恐怕不太现实。

Or they were like, it doesn't really seem like we're going to get some crazy AI thing. And so I had only a few people, basically, that were like, yeah, okay, that seems like a good thing to do.

或者他们觉得，我们似乎不会得到什么特别出众的 AI 技术。因此，只有极少数人认为，这似乎是一个不错的选择。

SPEAKER_01(Jack Clark): I remember in 2014 making graphs of ImageNet results over time when I was a journalist and trying to get stories published about them, and people thought I was completely mad. And then I remember in 2015 trying to persuade Bloomberg to let me write a story about NVIDIA, because every AI research paper had started mentioning the use of GPUs, and they said that was completely mad. And then in 2016, when I left journalism to go into AI, I had these emails saying, you're making the worst mistake of your life, which I now occasionally look back on.

SPEAKER_01：我记得在 2014 年，当我还是记者时，我做了一个关于 ImageNet 结果随时间变化的图表，并试图发表相关的报道。当时，人们觉得我简直是疯了。接着在 2015 年，我努力说服 Bloomberg 让我撰写一篇关于 NVIDIA 的文章，因为当时几乎所有的 AI 研究论文都开始提到 GPU 的使用，而他们则认为这想法简直疯狂。到了 2016 年，我决定从新闻业转入 AI 领域，收到一些邮件说这可能是我人生中最大的错误，如今我偶尔会回头看看这些邮件。

But it all seemed crazy at the time from many perspectives to go and take this seriously, that scaling was going to work, and something was maybe different about the technology paradigm.

但是，当时从许多角度来看，这一切都显得非常疯狂。要认真对待扩展技术的成功性，并认为技术的发展可能会与以往不同。

SPEAKER_07(Daniela Amodei): You're like Michael Jordan and that coach that didn't believe in him in high school.

SPEAKER_07：你就像 Michael Jordan，当初他的高中教练不相信他。

SPEAKER_02(Jared Kaplan): How did you actually make the decision, though? Did you feel torn, or was it obvious to you?

SPEAKER_02：你是怎么做出这个决定的？你当时是否感到纠结，还是很清楚自己的选择？

SPEAKER_01(Jack Clark): I did a crazy counter bet where I said, let me become your full-time AI reporter and double my salary, which I knew that they wouldn't say yes to. Then I went to sleep, and then I woke up and resigned. It was all fairly relaxed. 

SPEAKER_01：我做了一个大胆的提议，我说，让我成为你们的全职 AI 记者，并把我的工资翻倍。我知道他们不会同意。然后我就去睡觉，醒来后我就辞职了。这一切都进行得挺轻松的。

SPEAKER_02(Jared Kaplan): You're just a decisive guy. 

SPEAKER_02：你是个做决定很果断的人。

SPEAKER_01(Jack Clark): In that instance, I was. I think it's because I was going to work, reading archive papers, and then printing archive papers off and coming home and reading archive papers, including Dario's papers from the Baidu stuff. And being like, something completely crazy is happening here. And at some point, I thought you should bet with conviction, which I think everyone here has done in their careers is just betting with conviction that this is going to work.

SPEAKER_01：那时我确实是这样。我想是因为我在工作时不断阅读和打印存档论文，回家后继续研读，包括 Dario 关于百度的论文。我意识到这里有些非常疯狂的事情正在发生。我觉得在某个时刻，你应该坚定地下注。我想我们在座的每一位在职业生涯中都曾这样，坚定地相信某件事会成功。

SPEAKER_04: I definitely was not as decisive as you. I spent like six months like flip-flopping, like, okay, like, should I do it? Like, should I try to do a startup? Should I try to do this thing?

SPEAKER_04：我肯定没有你那么果断。我花了大约六个月时间犹豫不决，思考是否应该尝试创办一家初创公司，或者去做其他事情。

SPEAKER_07(Daniela Amodei): But I also feel like back then, there wasn't as much talk of engineers and the impact that an engineer can have in AI. That feels so natural to us now and we're at the same sort of talent raise for engineers of all different types. But at the time it was like you're a researcher and that's the only people that can work on AI. So I don't think it was crazy that you were spending time thinking about that.

SPEAKER_07：但我也觉得，那时候关于工程师及其在 AI 领域影响的讨论并不多。如今，工程师在这方面的重要性对我们来说已经是理所当然的了，各种类型的工程师都在提升他们的技能。然而在当时，似乎只有研究人员才能涉足 AI 工作。所以我认为你花时间思考这个问题并不奇怪。

SPEAKER_04: Yeah, yeah. And I think that was basically the thing that got me to join OpenAI was like, I like, I messaged the people there and they were like, yeah, we actually think that you can help out by doing engineering work. Yeah. And like that, that you can help out with AI safety in that way, which I think there hadn't really been an opportunity for that. So that was what. That's right. You were my, you were my manager at that opening. 

SPEAKER_04：对，对。我加入 OpenAI 的主要原因是，我联系了那里的团队，他们认为我能通过工程工作来贡献力量。是的，我可以在 AI 安全方面提供帮助。在那之前，我觉得并没有这样的机会。没错，你当时是我的经理。

SPEAKER_07(Daniela Amodei): I was.

SPEAKER_07：是的，我是。

SPEAKER_04: I think I joined after you'd been there for a while. I was at brain for a bit. Yeah. I don't know if I ever asked you like what it was that they got you to join. 

SPEAKER_04：我好像是在你在那里工作了一段时间后才加入的。我之前在 Brain 工作过一段时间。我不确定自己是否问过你，是什么让你决定加入的？

SPEAKER_07(Daniela Amodei): Yeah. So I had been at Stripe for about five and a half years and I knew Greg, he had been my boss. He was my boss at Stripe for a while. And I actually introduced him and Dario because I said when he was starting OpenAI, I was like, the smartest person that I know is Dario. You would be really lucky to get him. So Dario was at OpenAI. I had a few friends from Stripe that had gone there too. And I think sort of like you, I'd been thinking about what I wanted to do after Stripe. I had gone there just because I wanted to get more skills after working in, you know, nonprofit and international development. And I actually thought I was going to go back to doing that. Like essentially, I had always been working. I was like, I really want to help people that have, you know, less than I do. But I didn't have the skills when I was doing it before Stripe. And so I looked at going back to public health. I thought about going back into politics very briefly. But I was also looking around at other tech companies and other sort of ways of having impact. And OpenAI at the time felt like it was a really nice intersection. It was a nonprofit. They were working on this really big lofty mission. I really believed in sort of the AI, you know, potential because, I mean, I know Dario a little bit. And so he was.

SPEAKER_07：是的。我在 Stripe 工作了大约五年半，并且认识 Greg，他曾是我的老板。有一次，我把他介绍给 Dario，因为 Greg 想创办 OpenAI，我就对他说，我认识的最聪明的人就是 Dario，要是能请到他，你就太幸运了。于是，Dario 就加入了 OpenAI。我还有一些在 Stripe 的朋友也去了那里。我和你一样，一直在考虑离开 Stripe 后想做些什么。我当初选择去 Stripe，是因为我想获得更多的技能，之前我一直在非营利组织和国际发展领域工作，其实我本以为会回到那些领域。基本上，我一直想帮助那些生活条件不如我的人。但在 Stripe 之前，我缺乏所需的技能。所以，我考虑过重返公共卫生领域，也曾短暂地想过从政。但我也在寻找其他科技公司和其他能够产生影响的方法。那时候，OpenAI 给我一种很好的感觉。它是一个非营利组织，致力于一项非常宏伟的使命。我真的相信 AI 的潜力，因为我对 Dario 也有一些了解。

SPEAKER_04: They needed management help. They definitely needed management help.

SPEAKER_04：他们确实需要管理方面的帮助。

SPEAKER_07(Daniela Amodei): That is a fact. And so there was, I think that felt, it felt very me-shaped, right? I was like, oh, there's this nonprofit and they like, there's all these really great people with these like really good intentions, but it seems like they're a little bit of a mess.

SPEAKER_07：这是事实。所以，我觉得这正好适合我。我当时想，有个非营利组织，他们有很多优秀的人，怀着良好的意图，但似乎有些混乱。

And that was, that was, that felt really exciting to me to get to come in. And even, you know, just, I was such a utility player. I was running people, but I was also running some of the technical teams. 

那时，我感到非常兴奋能够加入这里。我不仅负责人员管理，还负责一些技术团队的运作，因为我本身是个多面手。

Yeah, the scaling org. I worked on the language team. I worked on some policy stuff. I worked with Chris. And I felt like there was just so much goodness in so many of the employees there. And I felt a very strong desire to come and try to help make the company a little more functional. 

是的，在公司规模扩大的组织中。我在语言团队工作，也参与了一些政策方面的工作，并且和 Chris 一起合作。我觉得那里的很多员工都非常优秀，因此我很希望能帮助公司变得更加高效。

SPEAKER_01(Jack Clark): I remember towards the end, after we'd done GPT-3, you were like, have you guys heard of something called trust and safety? 

SPEAKER_01：我记得在我们完成 GPT-3 之后，你提到过一个叫做信任与安全的概念。

SPEAKER_07(Daniela Amodei): Yes. I said, you know, I used to run some trust and safety teams at Stripe. There's a thing called trust and safety that you might want to consider for a technology like this. And it's funny because it sort of is the intermediary step between AI safety research, which is how do you actually make the model safe to something just much more practical? I do think there was a value in saying, this is going to be a big thing. We also have to be doing this sort of practical work day to day to build the muscles for when things are going to be a lot higher stakes. 

SPEAKER_07：是的。我当时说，我曾在 Stripe 负责过一些信任与安全团队。对于这种技术，你可能需要考虑信任与安全。这很有趣，因为它似乎是介于 AI 安全研究（即如何确保模型安全）和更实际应用之间的一个过渡步骤。我确实认为，指出这一点将成为一件重要的事情。同时，我们也需要每天进行这种实际操作，以便在未来面临更高风险时做好准备。

SPEAKER_01(Jack Clark): That might be a good transition point to talk about things like the responsible scaling policy and how we came up with that or why we came up with it and how we're using it now, especially given how much trust and safety work we do on today's models. So whose idea was the RSP? You and Paul? 

SPEAKER_01：这可能是个好时机来讨论一下负责任扩展政策（Responsible Scaling Policy，RSP），我们是如何以及为什么制定这个政策的，以及我们目前如何在模型的信任与安全工作中应用它。RSP 的想法是您和 Paul 提出来的吗？

SPEAKER_03(Dario Amodei): Yeah, it was me and Paul first talked about it in late, Paul Cristiano, in late 2022. First, it was like, oh, should we cap scaling at a particular point until we've discovered how to solve certain safety problems? And then it was like, well, it's kind of strange to have this one place where you cap it and then you uncap it. So let's have a bunch of thresholds. And then at each threshold, you have to do certain tests to see if the model is capable and you have to take increasing safety and security measures. Originally, we had this idea. And then the thought was just look like, you know, this will go better if, you know, if it's done by some third party, like we shouldn't we shouldn't be the ones to do it, right? It shouldn't come from one company, because then other companies are less likely to adopt it. So Paul actually went off and designed it and many features of it changed. And we were kind of on our side working on how it should work. And once Paul had something together, then pretty much immediately after he announced the concept, we announced ours within a month or two. I mean, many of us were heavily involved in it. I remember writing at least one draft of it myself, but there were, like, several drafts of it. There were so many drafts.

是的，我和 Paul Cristiano 第一次讨论这个问题是在 2022 年底。最初的想法是，我们是否应该在某个阶段限制扩展规模，直到解决某些安全问题？接着，我们意识到在一个点上限制然后再解除限制有些不妥，因此决定设定一系列阈值。在每个阈值处，需要进行特定测试以评估模型的能力，并采取更严格的安全和保障措施。最初我们提出了这个想法，后来觉得如果由第三方实施效果会更好。因为如果由我们公司来做，其他公司可能不愿意采纳这个方案。所以，Paul 实际上去设计了这个框架，许多特性在过程中发生了变化。我们则在探讨其具体运作方式。当 Paul 完成概念设计后，在他宣布后的一两个月内，我们也发布了自己的版本。我记得我至少撰写过一份草稿，但总共有好几个草稿，版本修改了很多次。

SPEAKER_02(Jared Kaplan): I think it's gone through the most drafts of any doc. Which makes sense, right? 

SPEAKER_02：我觉得这份文件经历的草稿次数比其他任何文件都多。这很合理，对吧？

SPEAKER_04: It's like, I feel like it is in the same way that, like, the U.S. treats, like, the Constitution as, like, the holy document. I think is just a big thing that, like, strengthens the U.S. treats like the Constitution as like the holy document which like I think is just a big thing that like strengthens the U.S. And like we don't expect the U.S. to go off the rails in part because just like every single person in the U.S. is like the Constitution is a big deal and if you tread on that like I'm mad. Yeah. Like I think that like the RSP is our like it holds that thing. It's like the holy document for Anthropic so it's like worth doing a lot of iterations getting it right. 

SPEAKER_04：我觉得这就像美国对待宪法，把它当作神圣的文件。这是一个重要的事情，加强了美国。而且，美国不会偏离轨道，部分原因是每个美国人都认为宪法很重要，如果有人侵犯它，他们会很生气。是的，我觉得 RSP 对于我们来说就像是 Anthropic 的神圣文件，所以值得经过多次迭代来完善它。

SPEAKER_07(Daniela Amodei): Some of what I think has been so cool to watch about the RSP development at Anthropic too is it feels like it has gone through so many different phases and there's so many different skills that are needed to make it work. There's the big ideas, which I feel like Dario and Paul and Sam and Jared and so many others are like, what are the principles? What are we trying to say? How do we know if we're right? But there's also this very, like, operational approach to just iterating where we're like, well, we thought that we were going to see this at this, you know, safety level. And we didn't. So should we change it so that we're making sure that we're holding ourselves accountable? And then there's all kinds of organizational things, right? We just were like, let's change the structure of the RSP organization for clearer accountability. And I think my sense is that for a document that's as important as this, right? I love the constitution analogy. It's like, there's all of these bodies and systems that exist in the US to like, make sure that we follow the constitution, right? There's the courts, there's the Supreme Court, there's the presidency, there's the, you know, the both houses of, you know, Congress, and they do all kinds of other things, of course. But there's like all of this infrastructure that you need around this like one document. And I feel like we're also learning that lesson here.

SPEAKER_07：我觉得在 Anthropic 观察 RSP 的发展过程真的很有趣。它经历了许多阶段，需要多种技能才能成功。像 Dario、Paul、Sam 和 Jared 这样的人一直在思考大方向：我们的原则是什么？我们想传达什么？我们怎么知道自己是对的？与此同时，也需要一种实际的操作方法来不断改进。比如，我们原本以为在某个安全级别会看到某些结果，但没有出现。这时我们就需要反思，是否应该做出改变以确保我们对自己的判断负责。此外，还有一些组织结构方面的问题。我们刚刚决定调整 RSP 组织的结构，以便更明确地分配责任。我觉得对于这样重要的文件来说，这种做法是必要的。我喜欢用宪法来作比喻。在美国，宪法周围有一整套系统来确保其实施，比如法院、最高法院、总统和国会两院。当然，他们还有其他责任。但为了维护这个重要的文件，需要有这些支持系统。我觉得我们在这里也学到了类似的经验。

SPEAKER_05(Sam McCandlish): I think it sort of reflects a view a lot of us have about safety, which is that it's a solvable problem. It's just a very, very hard problem that's going to take tons and tons of work.

SPEAKER_05：我认为这反映了我们很多人对安全性的看法，那就是安全性是一个可以解决的问题。虽然这确实是一个非常难解决的问题，但只要付出大量努力，就可以解决。

All of these institutions that we need to build up, like there's all sorts of institutions built up around automotive safety, built up over many, many years. But we're like, do we have the time to do that? We've got to go as fast as we can to figure out what the institutions we need for AI safety are and build those and try to build them here first but make it exportable.

我们需要建立一系列保障 AI 安全的机构，就像过去多年间我们围绕汽车安全建立的各类机构一样。然而，我们是否有足够的时间去建立这些机构呢？我们必须尽快确定所需的 AI 安全机构，并在本地优先建立，同时确保这些机构的模式可以推广到其他地方。

SPEAKER_03(Dario Amodei): It forces unity also because if any part of the org is not kind of in line with our safety values, it shows up through kind of the RSP. The RSP is going to block them from doing what they want to do. And so it's a way to remind everyone over and over again, basically to make safety a product requirement, part of the product planning process. And so it's not just a bunch of bromides that we repeat. It's something that you actually, if you show up here and you're not aligned, you actually run into it. And you either have to learn to get with the program or it doesn't work out.

SPEAKER_03：它还促进了团队的团结，因为如果组织的任何部分没有遵循我们的安全价值观，RSP 系统会立刻显示出这个问题。RSP 会阻止他们继续他们的计划。因此，这种机制不断提醒每一个人，把安全作为产品要求和规划过程中的一部分。这样一来，这不仅仅是一堆空洞的口号。如果你在这里工作却不遵循这些原则，就会遇到障碍。你要么学会适应这种要求，要么就无法顺利工作。

SPEAKER_01(Jack Clark): The RSP has become kind of funny over time because we spend thousands of hours of work on it. And then I go and talk to senators, and I explain the RSP, and I'm like, we have some stuff that means it's hard to steal what we make, and also that it's safe. And they're like, yes, that's a completely normal thing to do. Are you telling me not everyone does this? And you're like, oh, okay, yeah.

SPEAKER_01：随着时间的推移，RSP 逐渐显得有些滑稽，因为我们在上面投入了上千小时的工作。我去和参议员交流，向他们解释 RSP，我说，我们有一些措施使得我们的成果很难被窃取，并且确保其安全。他们会说，是的，这完全正常。你是在告诉我不是每个人都这样做吗？然后我就会说，哦，好吧，是的。

It is the exact truth that not everyone does this. It's amazing. We've spent so much effort on it here. And when you boil it down, they're like, yes, that sounds like a normal way. Yeah, that sounds good. That's been the goal.

实际上，并不是所有人都会这样做。这让人感到惊讶。我们在这里为此付出了很多努力。归根结底，他们会觉得，这听起来就是一种正常的方式。没错，这就是我们一直以来的目标。

SPEAKER_05(Sam McCandlish): Like Daniela was saying, let's make this as boring and normal. Let's make this a finance thing. Yeah, management's like an audit.

SPEAKER_05：就像 Daniela 说的那样，我们让这件事变得无聊和平常一些。把它当作金融上的事情来处理。是的，管理就像是在进行审计。

SPEAKER_03(Dario Amodei): Boring and normal is what we want, certainly in retrospect.

SPEAKER_03：无聊和平常就是我们想要的，尤其是从回头来看。

SPEAKER_07(Daniela Amodei): Well, also, Dario, I think in addition what we want, certainly in retrospect. Well, also, Dario, I think in addition to driving alignment, it also drives clarity because it's really, it's written down what we're trying to do. And it's legible to everyone in the company.

此外，Dario，我认为我们除了要推动一致性，还需要推动清晰性。因为我们明确地记录了我们的目标，公司里的每个人都能清楚地了解这些目标。

And it's legible externally what we think we're supposed to be aiming towards from a safety perspective. It's not perfect. We're iterating on it. We're making it better. But I think there's some value in saying, like, this is what we're worried about. This thing over here, like, you can't just use this word to sort of derail something in either direction, right? To say, oh, because of safety, we can't do X. Or because of safety, we have to do X. We're really trying to make it clearer what we mean. 

从安全的角度来看，我们追求的目标在外界看来是可以理解的。虽然并不完美，但我们正在不断改进，使其更好。我认为，明确我们所担心的问题是有价值的。我们希望传达出一个清晰的信息：不能仅仅因为安全的缘故就简单地决定不能做 X 或必须做 X。我们确实在努力让我们的意思更加明确。

SPEAKER_03(Dario Amodei): Yeah, you can't. It prevents you from worrying about every last little thing under the sun. Because it's actually the fire drills that damage the cause of safety in the long run. I've said if there's a building and the fire alarm goes off every week, that's a really unsafe building because when there's actually a fire, you're like, oh, it just goes off all the time. So it's very important to be calibrated.

SPEAKER_03：是的，你无法避免每件小事都去担忧。因为长远来看，频繁的火灾演习实际上会对安全造成损害。我曾说过，如果一栋楼每周都响火警，那就说明它非常不安全。因为当真正发生火灾时，人们可能会觉得火警总是误报。因此，确保警报系统的准确性是非常重要的。

SPEAKER_06(Chris Olah): A slightly different frame that I find kind of clarifying is that I think that RSP creates healthy incentives at a lot of levels. So I think internally it aligns the incentives of every team with safety because it means if we don't make progress on safety, we're going to block. I also think that externally it creates a lot of healthier incentives than other possibilities at least that I see because it means that if we at some point have to take some kind of dramatic action, like if at some point we have to say, you know, our model, we've reached some point and we can't yet make a model safe, it aligns that with sort of the point where there's evidence that supports that decision. And there's sort of a pre-existing framework for thinking about it and it's legible. And so I think there's a lot of levels at which the RRSP, I think in ways that maybe I didn't initially understand when we were talking about the early versions of it, it creates a better framework than any of the other ones that I've thought about.

SPEAKER_06：我发现用一种稍微不同的方式来看待问题可以让我更清楚地理解，我认为 RSP 在多个层面上建立了健康的激励机制。首先，从内部来看，RSP 使每个团队的激励机制与安全目标保持一致，因为如果我们在安全方面没有取得进展，项目就会受阻。其次，从外部来看，RSP 比其他我能想到的方案提供了更健康的激励机制。因为如果我们在某个时候必须采取重大行动，例如我们发现我们的模型达到某个阶段但尚未确保安全性，RSP 能够将这种情况与支持这一决定的证据点相匹配，并且有一个现成的框架来处理这个问题，使其变得清晰易懂。因此，在多个层面上，我认为 RSP 提供的框架比我曾经考虑过的其他任何框架都要好。

SPEAKER_02(Jared Kaplan): I think this is all true, but I feel like it undersells how challenging it's been to figure out what the right policies and evaluations and what the lines should be. I think that we have and continue to sort of iterate a lot on that. And I think there is a question also that's difficult of sort of, you could be at a point where it's very clear something's dangerous or very clear that something's safe, but with some technology that's so new, there's actually like a big gray area. And so I think that has been like all the things that we're saying were things that made me really, really excited about the RRSP at the beginning and still do. But also I think enacting this in a clear way and making it work has been much harder and more complicated than I anticipated. 

SPEAKER_02：我认为这些观点都很正确，但我觉得这低估了确定合适的政策和评估标准，以及划定界限的困难程度。我觉得我们一直在不断地进行调整。还存在一个难题，就是在某些情况下，你可能非常清楚某个事物的危险性或安全性，但对于一些全新的技术，实际上存在一个巨大的灰色地带。因此，我认为这些挑战正是让我对 RRSP 项目最初产生极大兴趣的原因，现在仍然如此。但同时，我发现要以一种清晰的方式实施这些政策，并使其有效，比我预期的要困难和复杂得多。

YPEAKER_05: eah, I think this is exactly the point. The gray areas are impossible to predict. There's so many of them. Until, until you actually try to implement everything, you, like, don't know what's going to go wrong. So what we're trying to do is go and implement everything so we can see as early as possible what's going to go wrong.

是的，我认为这正是关键所在。灰色地带难以预测，数量众多。在真正尝试实施之前，我们无法预知会出现哪些问题。因此，我们正努力去实施所有计划，以便尽早发现潜在的问题。

SPEAKER_03(Dario Amodei): Yeah, you have to do three or four passes before you really get it right. Like, iteration is just very powerful, and, you know, you're not going to get it right on the first time. And so, you know, if the stakes are increasing, you want to get your iterations in early. You don't want to get it right on the first time. And so if the stakes are increasing, you want to get your iterations in early. You don't want to get them in late.

SPEAKER_03：是的，你需要尝试三到四次才能真正做到完美。迭代是非常强大的工具，你不可能第一次就成功。所以，如果事情的重要性在提升，你应该尽早开始多次尝试，而不是等到最后才进行。

SPEAKER_01(Jack Clark): You're also building the internal institutions and processes, so the specifics might change a lot, but building the muscle of just doing it is the really valuable thing. 

SPEAKER_01：你也在构建内部机构和流程，因此具体细节可能会有很大变化，但关键在于培养执行的能力，这才是最有价值的。

SPEAKER_04: I'm responsible for compute at AnthropBakon. That's important. Thank you. compute at Anthropic. That's important. Thank you. I think so. So I think that for me, I guess we have to deal with external folks. And different external folks are on different spectrums of the how fast do they think stuff is going to get. I think that's also been a thing where I started out not thinking stuff would be that fast and have changed over time. And so I have sympathy for that. And so I think the RSP has been extremely useful for me in communicating with people who think that things might take longer, because then we have a thing where it's like, we don't need to do extreme safety measures until stuff gets really intense. And then we can be like, they might be like, I don't think stuff will get intense for a long time. And then I'll be like, okay, yeah, we don't have to do extreme safety measures. And so that makes it a lot easier to communicate with other folks externally.

我负责在 AnthropBakon 处理计算方面的工作，这一点非常重要。谢谢。在 Anthropic 负责计算，这一点也很重要。谢谢。我个人认为，在与外部人员打交道时，不同的人对事情进展速度的看法存在差异。我自己一开始也觉得进展不会那么快，但随着时间推移，我的看法改变了，所以我理解他们的观点。我觉得 RSP 在与那些认为事情需要更长时间的人沟通时非常有用，因为这样我们可以达成一致：在情况变得非常紧急之前，不需要采取极端的安全措施。有些人可能会认为这种紧急情况不会很快出现，这时我可以回应说，没问题，我们暂时不需要极端措施。这大大简化了与外部人员的沟通。

SPEAKER_01(Jack Clark): Yeah, yeah. It makes it like a normal thing you can talk about rather than something really strange. Yeah. How else is it showing up for people?

SPEAKER_01：是的，是的。这样一来，这就变成了一个可以正常讨论的话题，而不再是一些很奇怪的东西。那么，它还以其他什么方式表现出来呢？

SPEAKER_05(Sam McCandlish): Evals, evals, evals. Good. It's all about evals. Everyone's doing evals. The training team is doing evals all the time. We're trying to figure out, like, has this model gotten enough better that it has the potential to be dangerous? So how many teams do we have that are evals teams? You have Frontier Red Team. There must be. I mean, there's a lot of people.

SPEAKER_05：评估，评估，评估。没错，现在一切都在围绕评估展开。大家都在进行评估，培训团队也是一直在进行评估。我们在努力判断，这个模型是否已经改进到可能会带来危险的程度。那么，我们有多少评估团队呢？比如说，Frontier 红队。肯定有很多人参与其中。

SPEAKER_01(Jack Clark): Every team produces evals, basically.

SPEAKER_01：每个团队几乎都会做评估。

SPEAKER_07(Daniela Amodei): And that means you're just measuring against the RSP. Like, measuring for certain signs of things that would concern you or not concern you.

SPEAKER_07：这意味着你只是衡量是否符合 RSP 的标准，比如检查某些特征是否会引起你的关注。

SPEAKER_05(Sam McCandlish): Exactly. It's easy to lower bound the abilities of a model, but it's hard to upper bound. So we just put tons and tons of research effort into saying, can this model do this dangerous thing or not? Maybe there's some trick that we haven't thought of, like chain of thought or best event or some kind of tool use that's going to make it so it can help you do something very dangerous. 

SPEAKER_05：没错。我们可以轻松确定一个模型的最低能力，但要界定其最高能力却很难。因此，我们投入了大量研究精力来探讨，这个模型是否能做出一些危险的事情。也许存在一些我们尚未发现的技巧，比如「思维链」或「最佳事件」，或者某种工具使用方法，这些都可能让模型具备帮助你做出非常危险行为的能力。

SPEAKER_01(Jack Clark): It's been really useful in policy because it's been a really abstract concept what safety is. And when I'm like, we have an eval which changes whether we deploy the model or not. And then you can go and calibrate with policymakers or experts in national security or some of these CBRN areas that we do to actually help us build evals that are well calibrated. And that counterfactually just wouldn't have happened otherwise. But once you've got the specific thing, people are a lot more motivated to help you make it accurate. So it's been useful for that.

SPEAKER_01：在制定政策时，这非常有帮助，因为安全性一直是一个难以具体化的概念。当我提到我们有一个评估标准来决定是否部署模型时，你可以与政策制定者、国家安全专家或一些 CBRN 领域的专家进行校准，以帮助我们建立更准确的评估标准。如果没有这种评估，事情可能不会这样发展。但是，一旦有了具体的评估标准，人们会更加积极地帮助提高其准确性。所以在这方面很有用。

SPEAKER_07(Daniela Amodei): The RSP shows up for me, for sure. Everything? Often. I actually think, weirdly, the way that I think about the RSP the most is like what it sounds like, just like the tone. I think we just did a big rewrite of the tone of the RSP because it felt overly like technocratic and even a little bit adversarial. I spent a lot of time thinking about like, how do you build a system that people just want to be a part of, right? 

SPEAKER_07：对我而言，RSP 确实很显著。每个方面吗？经常是这样。我发现自己常常关注 RSP 的感觉，尤其是其语气。我们最近对 RSP 的语气进行了大幅度的修改，因为之前它显得过于技术化，甚至有些对立。我一直在思考，如何创建一个让人们愿意参与其中的系统。

It's so much better if the RSP is something that everyone in the company can walk around and tell you, just like with OKRs like we do right now, what are the top goals of the RRSP? How do we know if we're meeting them? What AI safety level are we at right now? Are we at ASL2? Are we at ASL3? That people know what to look for because that is how you're going to have good common knowledge of if something's going wrong. If it's overly technocratic and it's something that only particular people in the company feel is accessible to them, it's just like not as productive, right? 

如果公司的 RSP 能像大家熟知的 OKR 一样，成为每个人都能随时讨论的内容，那就太好了。RRSP 的主要目标是什么？我们如何判断是否达成了这些目标？当前我们的 AI 安全级别是多少？是 ASL2 还是 ASL3？这样，每个人都知道需要注意什么，以便及时发现问题。如果 RSP 过于技术化，只让公司中某些人能够理解，那就不够高效，对吧？

And I think it's been really cool to watch it sort of transition into this document where I actually think most, if not everybody at the company, regardless of their role, could read it and say, this feels really reasonable. I want to make sure that we're building AI in the following ways, and I see why I would be worried about these things. And I also kind of know what to look for if I bump into something, right? It's almost like, make it simple enough that if you're working at a manufacturing plant, and you're like, huh, it looks like the safety seatbelt on this should connect this way, but it doesn't connect, that you can spot it. And that there's just like healthy feedback flow between leadership and the board and the rest of the company and the people that are actually building it. Because I actually think the way this stuff goes wrong in most cases is just like the wires don't connect or they get crossed. And that would just be a really sad way for things to go wrong. It's just all about operationalizing it, making it easy for people to understand.

我觉得，看着它逐步演变成这样一个文件真的很有趣。我认为，公司里的大多数人，甚至可能所有人，不论他们的职位，都能读懂这个文件，并觉得它非常合理。我希望我们以这样的方式来构建 AI，并理解为什么这些问题值得关注。我也大概知道，如果遇到问题时应该注意什么。就像在制造工厂工作时，如果发现安全带应该这样连接却没有连接，你就能察觉到。这种文件促进了领导层、董事会与公司其他部门及实际开发人员之间的有效反馈交流。我认为，问题常常出在信息不通畅上，这确实是个令人惋惜的原因。因此，我们要把事情操作化，使大家更容易理解。

SPEAKER_05(Sam McCandlish): Yeah, the thing I would say is none of us wanted to found a company. We felt like it was our duty, right?

SPEAKER_05：是啊，我想说，我们都不想创办公司，但觉得这是我们的责任。

SPEAKER_07(Daniela Amodei): I felt like we had to.

SPEAKER_07：我感觉我们不得不这样做。

SPEAKER_05(Sam McCandlish): Like, we have to do this thing. This is the way we're going to make things go better with AI. Like, that's also why we did the pledge, right? Because we're like, the reason we're doing this feels like our duty.

SPEAKER_05：我们必须采取行动，这是改善 AI 的方法。这也是我们做出承诺的原因，因为我们觉得这是一种责任。

SPEAKER_03(Dario Amodei): I wanted to invent and discover things in some kind of beneficial way. That was how I came to it. And that led to working on AI. And AI required a lot of engineering. And eventually, AI required a lot of capital. But what I found was that if you don't do this in a way where you're setting the environment, where you set up the company, then a lot of it gets done. A lot of it repeats the same mistakes that I found so alienating about the tech community. It's the same people. It's the same attitude. It's the same pattern matching. And so at some point, it just seemed inevitable that we do it in a different way.

我希望能以一种对社会有益的方式进行创新和探索。这是我投身于 AI 研究的初衷。AI 的发展需要大量的工程技术和资金支持。但我发现，如果不从环境和公司架构上进行合理设计，很多事情虽然完成了，却会不断重蹈科技圈中让我感到疏远的错误。仍然是那些人、那些态度和相同的思维模式。因此，我们必然要以不同的方式来进行这项工作。

SPEAKER_02(Jared Kaplan): When we were hanging out in graduate school, I remember you had kind of this whole like program of trying to figure out how to do science in a way that would sort of advance the public good. And I think I think that's like pretty similar to how we we think about this. Maybe you have like this like Project Vannevar or something to do that. I was a professor. I think basically I just looked at the situation and I was convinced that AI was on a very, very, very steep trajectory in terms of impact. It didn't seem like because of the necessity for capital, like as a physics professor, I could continue doing that. And I kind of wanted to work with people that I trusted in building an institution to try to make kind of AI go well. But yeah, I would never recommend founding a company or really want to do it. I mean, yeah, I think it's just a means to an end. I mean, I think that's like usually how things go well, though. If you're doing something just to sort of like enrich yourself or gain power or like you have to sort of actually care about accomplishing a real goal in the world and then you find whatever means you have to.

当我们还在研究生院的时候，我记得你一直在探索如何以促进公众利益的方式进行科学研究。我觉得这和我们的思路很相似。也许你有一个类似「Project Vannevar」的计划来实现这个目标。作为一名教授，我观察到 AI 的发展轨迹非常迅猛，影响深远。由于需要大量资金，作为物理教授的我无法继续从事这方面的研究。我希望与我信任的人一起建立一个组织，推动 AI 的良性发展。但我并不推荐创办公司或真心想这样做。我认为这只是实现目标的手段之一。通常，事情顺利进行的关键在于专注于实现世界上真正有意义的目标，然后利用一切可能的手段去实现它。

SPEAKER_07(Daniela Amodei): Well, something I think about a lot is just a strategic advantage for us is, I mean, it sounds really funny to say, but just like how much trust there is at this table, right? Like, I think that's not, I mean, Tom, you were at other startups. I was never a founder before, but it's actually really hard to get a group of, like a big group of people to have like the same mission. Right. And I think the thing that I feel like the happiest about when I come into work and probably the most proud of at Anthropic is how well that has scaled to a lot of people. It feels to me like in this group and with the rest of leadership, everyone is here for the mission and our mission is really clear and it's it's very pure. Right. And I think that is something that I don't see as often, to Dario's point, in the tech industry. It feels like there's just a wholesomeness to what we're trying to do. I agree. None of us were like, let's just go found a company. I felt like we had to do it. It just felt like we couldn't keep doing what we were doing the place we were doing it. We had to do it by ourselves. couldn't keep doing what we were doing the place we were doing it. We had to do it by ourselves.

SPEAKER_07：我常常在想，我们的一个战略优势就是，这听起来可能有些有趣，但就是我们团队之间的高度信任。Tom，你在其他初创公司工作过，我之前没有创办过公司，但我了解到，要让一个大团队拥有共同的使命其实很难。而在 Anthropic，我感到最欣慰和自豪的就是我们成功地将这种信任扩展到了众多人之间。在这个团队和其他领导层中，每个人都为了一个明确而纯粹的使命而努力。正如 Dario 所指出的，这在科技行业并不常见。我们的目标充满了一种正面的能量。我同意，我们没有一个人是单纯为了创办公司而来，而是因为我们感觉必须这么做。我们无法在原来的地方继续实现我们的目标，所以选择自己去完成。

SPEAKER_01(Jack Clark): It felt like with GPT-3, which all of us had touched or worked on, and scaling laws and everything else, we could see it in front of us in 2020. And it felt like, well, if we don't do something soon altogether, you're going to hit the point of no return. And you have to do something to have any ability to change the environment.

SPEAKER_01：当时大家都在使用或研究 GPT-3，加上规模法则等因素，我们在 2020 年看到了发展的趋势。如果我们不迅速采取行动，就可能到达一个无法回头的地步。因此，必须采取措施才能有能力去改变环境。

SPEAKER_04: I think building off Daniel, I do think that there's just a lot of trust in this group. I think each of us knows that we got into this because we want to help out with the world. We did the 80% pledge thing, and that was a thing that everybody was just like, yes, obviously we're going to do this. I do think that the trust thing is a special thing that's extremely rare.

SPEAKER_04：接着 Daniel 的话，我确实觉得在这个小组中充满了信任。我认为我们每个人都明白，我们参与其中是因为想要帮助世界。我们做出了 80% 的承诺，这对大家来说是理所当然的。我确实认为信任是一件特别的事情，非常罕见。

SPEAKER_05(Sam McCandlish): I credit Daniela with keeping the bar high. I credit you with the fact that we scaled.  She's clown racers. You're the reason the culture scaled, I think.

SPEAKER_05：我认为 Daniela 一直将标准保持得很高。我还认为，我们能够实现扩展，多亏了你的贡献。她就像小丑赛车手一样富有趣味。而你，我觉得是让这种文化得以扩展的关键。

SPEAKER_01(Jack Clark): People say how nice people are here, which is actually a wildly important thing.

SPEAKER_01：人们常说这里的人都很友善，这确实是件非常重要的事情。

SPEAKER_07(Daniela Amodei): I think Anthropic is really low politics. And of course, we all have a different vantage point than average, and I try to remember that. It's because of low ego. But it's low ego, and I do think our interview process and just the type of people who work here, there's almost an allergic reaction to politics.

SPEAKER_07：我觉得在 Anthropic，政治斗争很少。当然，我们的视角与普通人不太一样，我努力记住这一点。这是因为大家的自我意识都比较低。我认为我们的面试流程和在这里工作的人，几乎对政治斗争有一种排斥反应。

SPEAKER_03(Dario Amodei): And unity. Unity is so important. The idea that the product team, the research team, the trust and safety team, the go-to-market team, the policy team, the safety folks, they're all trying to contribute to kind of the same goal, the same mission of the company. I think it's dysfunctional when different parts of the company think they're trying to accomplish different things, think the company is about different things, or think that other parts of the company are trying to undermine what they're doing.

SPEAKER_03：以及团结。团结非常重要。产品团队、研究团队、信任与安全团队、市场拓展团队、政策团队、安全人员，他们都在努力为公司共同的目标和使命做出贡献。我认为，当公司不同部门认为他们在尝试完成不同的事情，或者认为公司是关于不同事情的，或者认为其他部门在试图破坏他们的工作时，这会导致公司运作不协调。

And I think the most important thing we've managed to preserve is, and again, things like the RSP drive it, this idea that it's not, you know, there's some parts of the company causing damage and other parts of the company trying to repair it, but that there are different parts of the company doing different functions, and they all function under a single theory of change. Extreme pragmatism, right?

我认为我们成功保留的最重要理念是，由 RSP 等因素推动的，不是说公司的一些部门在造成损害，而其他部门在修复，而是公司各个部分在执行不同职责，并在统一的变革理论下运作。这就是极端实用主义，对吧？

SPEAKER_06(Chris Olah): You know, the reason I went to OpenAI in the first place, you know, it was a nonprofit. It was a place where I could go and focus on safety. And I think over time, that maybe wasn't as good a fit. And there were some difficult decisions. And I think in a lot of ways, I really trusted Dario and Danielle on that.

SPEAKER_06：你知道，我最初选择去 OpenAI，是因为它是一个非营利组织。在那里，我可以专注于安全问题。然而，随着时间的推移，我发现这份工作与我的期望不太匹配，面临了一些艰难的决定。在这些方面，我非常信任 Dario 和 Danielle 的判断。

But I didn't want to leave. That was something that I think I was actually pretty reluctant to go along with. Because I think for one thing, I didn't know that it was good for the world to have more AI labs. And I think it was something that I was pretty reluctant for. And I think as well, when we did leave, I think I was reluctant to start a company. I think I was arguing for a long time that we should do a nonprofit instead and just focus on safety research. time that we should do a nonprofit instead and just focus on safety research. I think it really took pragmatism and confronting the constraints and just being honest about what the constraints implied for accomplishing that mission that led to Anthropic.

但是我并不想离开。我对这个决定其实是相当犹豫的。因为我觉得，多设立一些 AI 实验室是否真的对世界有利，这是个未知数。我当时更倾向于成立一个非营利组织，专注于安全研究，而不是去创办一家公司。最终，我们选择成立 Anthropic，是因为我们务实地面对现实限制，并坦诚地认识到这些限制对实现我们的使命所意味着什么。

SPEAKER_03(Dario Amodei): I think just a really important lesson that we were good about early on is make less promises and keep more of them. Like try to be calibrated, be realistic, confront the trade-offs because trust and credibility are more important than any particular policy.

SPEAKER_03：我认为我们在早期学到了一个非常重要的教训，那就是少做承诺，多兑现。尽量保持适度，保持现实，面对各种取舍，因为信任和信誉比任何特定政策都更为重要。

SPEAKER_07(Daniela Amodei): It is so unusual to have what we have. And watching Mike Krieger defend safety things of reasons why we shouldn't ship a product yet, but also then to watch Binet say, okay, we have to do the right thing for the business. How do we get this across the finish line? And to hear people deep in the technical safety org talking about how it's also important that we build things that are practical for people and hearing engineers on inference talk about safety, that's amazing. I think that is, again, one of the most special things about working here is everybody with that unity is prioritizing the pragmatism, the safety, the business. That's wild.

SPEAKER_07：我们拥有的这种情况真是少见。看到 Mike Krieger 为了安全理由坚持不立即发布产品，同时又看到 Binet 强调必须为业务做出正确决策，真是令人印象深刻。技术安全团队的人讨论如何构建对人们有用的产品，以及推理技术的工程师谈论安全问题，都是非常了不起的。我认为，这就是在这里工作最特别的地方之一：每个人都在协同努力中优先考虑务实、安全和业务需求。这实在是太棒了。

SPEAKER_03(Dario Amodei): I think about it as spreading the trade-offs from just the leadership of the company to everyone, right? I think the dysfunctional world is like, you have a bunch of people who only see a big, you know, safety is like, we always have to do this. And product is like, we always have to do this. And research is like, you know, this is the only thing we care about. And then you're stuck at the top, right?

SPEAKER_03：我认为应该将公司的决策权衡从领导层扩展到每一个员工。现实中不理想的情况是，不同部门的人只关注自己的领域，比如安全部门总是强调必须这样做，产品部门也是坚持他们的做法，而研究部门则只关心他们的研究。结果就是，所有问题的解决都集中在最高层，导致决策的瓶颈。

You're stuck at the top. You have to decide between, you don't have as much information as either of them. That's the dysfunctional world. The functional world is when you're able to communicate to everyone. There are these trade-offs we're all facing together.

你站在高处，面临着必须做出选择的困境，却没有其他人那么多的信息。这就是一个信息不畅的世界。相反，当你能够与每个人都顺畅沟通时，世界就变得正常了。我们面临着共同的权衡与选择。

The world is a far from perfect place. There's trade-offs. Everything you do is going to be suboptimal. Everything you do is going to be some attempt to get the best of both worlds that doesn't work out as well as you thought it was. And everyone is on the same page about confronting those tradeoffs together. And they just feel like they're confronting them from a particular post, from a particular job, as part of the overall job of confronting all the tradeoffs. It's a bet on Race to the Top, right?

世界并不完美，总是存在权衡。我们所做的一切都是次优的，是在尝试两全其美，但结果往往不如预期。每个人都意识到需要共同面对这些权衡，他们只是从各自的岗位出发，作为整体努力的一部分来应对这些挑战。这就像是在下注一场「向上的竞赛」，对吗？

SPEAKER_05(Sam McCandlish): It's a bet on Race to the Top. Like, it's not a pure upside bet. Things could go wrong. But, like, we're all aligned on, like, this is the bet that we're making. It's a race to the top, right? It's a race to the top. It's not a pure upside bet. Things could go wrong. We're all aligned on this is the bet that we're making.

SPEAKER_05：这是一个争取领先的赌注。它并不是完全没有风险，事情可能会出错。但我们一致认为，这就是我们的选择。大家都在争先恐后，这是一场抢占顶峰的竞赛。我们都达成共识，这是我们下注的方向。

SPEAKER_01(Jack Clark): And markets are pragmatic. So the more successful and proper it becomes as a company, the more incentive there is for people to copy the things that make us successful. And the more that success is tied to actual safety stuff we do, the more it just creates a gravitational force in the industry that will actually get the rest of industry to compete. And it's like, sure, we'll build seatbelts and everyone else can copy them. That's good. That's like good world. That's really good.

SPEAKER_01：市场具有实用性。因此，随着公司变得更成功和完善，其他公司就越有动力模仿我们的成功经验。而我们的成功越与实际的安全措施挂钩，它就越会在行业中产生一种吸引力，促使其他公司参与竞争。就像我们制造了安全带，其他人可以复制它们。这对世界有好处，真的很好。

SPEAKER_03(Dario Amodei): Yeah, this is the race to the top, right? But if you're saying, well, we're not going to build the technology, you're not going to build it better than someone else, that in the end, that just doesn't work because you're not proving that it's possible to get from here to there. Where the world needs to get, never mind the industry, never mind one company, is it needs to get us successfully through from this technology doesn't exist to the technology exists in a very powerful way. And society has actually managed it. And I think the only way that's going to happen is that if you have, at the level of a single company and eventually at the level of the industry, you're actually confronting those tradeoffs. You have to find a way to actually be competitive, to actually lead the industry in some cases, and yet manage to do things safely. And if you can do that, the gravitational pull you exert is so great. There's so many factors from the regulatory environment to the kinds of people who want to work at different places to even sometimes the views of customers that kind of drive in the direction of if you can show that you can do well on safety without sacrificing competitiveness, right? If you can find these kind of win-wins, then others are incentivized to do the same thing.

SPEAKER_03：是的，这是一个追求卓越的过程，对吧？但如果你说我们不打算发展这项技术，你就无法比别人做得更好，这最终是行不通的，因为这没有证明从无到有的可能性。无论是从行业还是某个公司的角度，世界需要做到的是成功地从技术不存在的阶段，发展到技术以一种非常强大的方式存在，并且社会能够良好地管理这种变化。我认为，要实现这一点，唯一的方法是在公司层面，甚至在整个行业层面，去面对这些权衡。你必须找到一种方法，既能在竞争中占据优势，甚至引领行业，同时又能安全地推进发展。如果你能做到这一点，你会产生巨大的影响力。从监管环境，到想要在不同公司工作的人，再到客户的看法，各种因素都会推动这种趋势。如果你能展示在不牺牲竞争力的情况下也能确保安全，那么其他人也会受到激励，去寻求这种双赢的局面。

SPEAKER_02(Jared Kaplan): Yeah, I mean, I think that's why getting things like the RRSP right is so important. Because I think that we ourselves, seeing where the technology is headed, have often thought, oh, wow, we need to be really careful of this thing. But at the same time, we have to be even more careful not to be crying wolf, saying that, like, innovation needs to stop here. We need to sort of find a way to make AI useful, innovative, delightful for customers, but also figure out what the constraints really have to be that we can stand behind that make systems safe so that what the constraints really have to be that we can stand behind that make systems safe so that it's possible for others to think that they can do that too, and they can succeed, they can compete with us. 

SPEAKER_02：是的，我觉得这就是为什么像 RRSP 这样的项目非常重要。因为我们自己看到技术的发展方向，常常意识到需要非常谨慎。然而，我们也必须谨防过度反应，比如阻止创新。我们需要找到方法，使 AI 对客户有用、创新且令人愉悦，同时明确我们需要坚持的安全限制，以便系统安全。这样其他人也能相信他们可以做到，他们可以成功并与我们竞争。

SPEAKER_05(Sam McCandlish): We're not doomers, right? We want to build the positive thing. We want to build the good thing.

SPEAKER_05：我们不是消极的人，对吗？我们希望创造积极和有益的事物。

SPEAKER_03(Dario Amodei): And we've seen it happen in practice. A few months after we came out with our RSP, the three most prominent AI companies had one, right? Interpretability research. That's another area we've done it. Just the focus on safety overall, like collaboration with the AI safety institutes, other areas.

SPEAKER_03：我们已经在实际中看到这样的成果。就在我们推出 RSP 几个月后，三家最知名的 AI 公司也纷纷推出了类似的项目。再比如，在可解释性研究方面，我们也取得了进展。我们始终关注 AI 安全，与 AI 安全研究所等机构展开了合作，并在多个领域进行了探索。

SPEAKER_01(Jack Clark): Yeah, the Frontier Red team got cloned almost immediately, which is good. You want all the labs to be testing for, like, very, very scary risks. Export the seatbelts. Yeah. Export the seatbelts. Yeah, export the seatbelts. 

SPEAKER_01：是的，Frontier Red 团队几乎立刻被复制了，这其实是好事。我们希望所有实验室都能测试那些极其可怕的风险。把安全措施推广出去。是的，推广安全措施。是的，推广安全措施。

SPEAKER_07(Daniela Amodei): Well, Jack also mentioned it earlier, but customers also really care about safety, right? Customers don't want models that are hallucinating. They don't want models that are easy to jailbreak. They want models that are helpful and harmless, right? And so a lot of the time what we hear in customer calls is just we're going with Claude because we know it's safer. I think that is also a huge market impact, right? Because our ability to have models that are trustworthy and reliable, that matters for the market pressure that it puts on competitors, too.

SPEAKER_07：Jack 之前提到过，客户非常关注模型的安全性。客户不希望使用会产生错误信息的模型，也不希望模型容易被破解。他们需要的是既有用又无害的模型。因此，我们常在客户电话中听到他们选择 Claude 的原因是它更安全。我认为这对市场影响很大，因为拥有可信赖可靠的模型也会对竞争对手施加市场压力。

SPEAKER_06(Chris Olah): Maybe to unpack something that Dario said a little bit more. I think there's this narrative or this idea that maybe the virtuous thing is to almost like nobly fail, right? It's like you should go and put safety, you should go and put things, you should sort of demonstrate like in an impragmatic way so that you can sort of demonstrate your purity to the cause or something like this. And I think if you do that, it's actually very self-defeating. For one thing, it means that you're going to have the people who are making decisions be self-selected for being people who don't care and for people who aren't prioritizing safety and who don't care about it. And I think on the other hand, if you try really hard to find the way to align the incentives and make it so that if there are hard decisions, they happen at the points where there is the most force to go and support making the correct hard decisions and where there's the most evidence. Then you can sort of start to trigger this race to the top that Dario is describing, where instead of going and having the people who care get pushed out of influence, you instead pull other people to have to go and follow.

SPEAKER_06：让我们更深入地探讨一下 Dario 提到的观点。有一种观点认为，高尚的行为似乎是为了崇高而失败。也就是说，应该优先考虑安全，以一种不切实际的方式来展示对事业的忠诚。然而，这种做法实际上是自我挫败的。这样做的结果是，决策者往往是那些不关心安全的人。另一方面，如果我们努力寻找方法来对齐激励措施，使得关键决策发生在最有力支持正确选择和证据最充分的时刻，我们就能触发 Dario 所描述的「向上的竞争」。这样，就不会让那些关心安全的人失去影响力，而是促使其他人也必须追随这种做法。

SPEAKER_01(Jack Clark): So what are you all excited about when it comes to the next things we'll be working on?

SPEAKER_01：那么，关于我们接下来要做的事情，你们有什么让人兴奋的期待吗？

SPEAKER_06(Chris Olah): I think there's a bunch of reasons you can be excited about interoperability. One is obviously safety. But there's another one that I think I find at an emotional level equally exciting or equally meaningful to me, which is just that I think neural networks are beautiful. And I think that there's a lot of beauty in them that we don't see. We treat them like these black boxes that we're not particularly interested in the internal stuff. But when you start to go and look inside them, they're just full of amazing, beautiful structure. You know, it's sort of like if people looked at biology and they were like, you know, like evolution is really boring. It's a simple thing that goes and runs for a long time and then it makes animals. And instead, it's like actually each one of those animals that evolution produces, and I think that it's an optimization process, like training a neural network. They're full of incredible complexity and structure. And we have an entire sort of artificial biology inside of neural networks. If you're just willing to look inside them, there's all of this amazing stuff. And I think that we're just starting to slowly unpack it. And it's incredible. And there's so much there. But there's just so much to be discovered there. We're just starting to crack it open. And I think it's going to be amazing and beautiful. And sometimes I imagine, you know, like a decade in the future, walking into a bookstore and buying, you know, the textbook on neural network interpolability or really like on the biology of neural networks and just the kind of wild things that are going to be inside of it. And I think that in the next decade, we're going to, in the next couple of years, even we're going to go and start to go and really discover all of those things. And it's going to be wild and incredible.

SPEAKER_06：我觉得有很多理由对互操作性感到兴奋。首先是安全性，这是显而易见的。但对我来说，在情感上同样激动的是，我觉得神经网络本身非常美丽。我们往往把它们视为不透明的「黑箱」，对其内部结构并不特别感兴趣。然而，当你深入探索时，会发现神经网络内部充满了令人惊叹的美丽结构。这有点像有人看待生物进化时，可能觉得进化过程简单而乏味，只是漫长的时间塑造出动物。然而，实际上，每一个进化而来的生物都蕴含着极其复杂的结构，就像训练神经网络是一个优化过程一样。我们在神经网络中也有类似于人工生物学的复杂体系。如果你愿意深入研究，就会发现其中的奥妙。我认为我们才刚刚开始揭开这一切的面纱，真是不可思议。未来有太多的内容等待我们去发现。想象一下，十年后走进书店，买一本关于神经网络互操作性或「神经网络生物学」的教科书，其中将包含各种令人惊叹的内容。我相信，在未来的几年甚至十年内，我们将开始探索这些领域，必将带来激动人心的发现。

SPEAKER_01(Jack Clark): It's also going to be great that you get to buy your own textbook. I'm excited that a few years ago, if you had said, like, governments will set up new bodies to like test and evaluate AI systems, and they will actually be competent and good, you would have not thought that was going to be the case. But it's happened. And it's kind of like governments have built these new embassies almost to deal with this new kind of class of technology or like thing that Chris studies. And I'm just very excited to see where that goes. I think it actually means that we have state capacity to deal with this kind of societal transition so it's not just companies. And I'm excited to help with that.

SPEAKER_01：你可以购买自己的教材，这真让人期待。回想几年前，如果有人说政府会成立新的机构来测试和评估 AI 系统，而且这些机构会很有能力和高效，大家可能都不敢相信。但这确实发生了。就像政府为应对 Chris 研究的新技术类别，几乎建立了新的大使馆。我很期待看到这些发展。我认为这表明我们有国家能力来应对社会转型，而不仅仅依赖公司。我很高兴能参与其中。

SPEAKER_07(Daniela Amodei): I'm already excited about this to a certain extent today, but I think just imagining the future world of what AI is going to be able to do for people, it's impossible to not feel excited about that. Dario talks about this a lot, but I think even just the sort of glimmers of Claude being able to help with vaccine development and cancer research and biological research is crazy, just to be able to watch what it can do now. But when I fast forward three years in the future or five years in the future, imagining that Claude could actually solve so many of the fundamental problems that we just face as humans, just even just from a health perspective alone, even if you sort of take everything else out, feels really exciting to me, just like thinking back to my international development times, it would be amazing if Claude was responsible for helping to do a lot of the work that I was trying to do a lot less effectively when I was like 25.

SPEAKER_07：今天我对此已经感到有些兴奋，但想象一下未来 AI 能为人类所做的事情，真是让人无法不感到激动。Dario 经常谈到这个问题，而我认为即使只是 Claude 在疫苗开发、癌症研究和生物研究中展现出的一些初步能力，也已经让人惊叹不已。我们现在就能看到它的潜力。当我设想三到五年后的未来，Claude 可能会真正解决我们人类面临的许多基本问题，尤其是在健康领域，这让我感到非常振奋。回想起我在国际发展工作时期，如果 Claude 能够完成我在 25 岁时效率低下的那些工作，那会是非常了不起的。

SPEAKER_05(Sam McCandlish): I mean, I guess similarly, I'm excited to build Claude for work. I'm excited to build Claude into the company and into companies all over the world.

SPEAKER_05：我想，我同样感到兴奋的是，将 Claude 应用于工作中。我很期待把 Claude 引入到公司内部，并推广到世界各地的企业。

SPEAKER_04: I guess I'm excited just for I guess like personally I like using Claude a lot. Definitely, there's been increasing amounts of home times with me just chatting with Claude about stuff. I think the biggest recent thing has been code, where six months ago, I didn't use Claude to do any coding work. Our teams didn't really use Claude that much for coding. And now it's just a phase difference. I gave a talk at YC a week before last. And at the beginning, I just asked, how many folks here use Claude for coding now? And literally 95% of hands. All the hands in the room, which is totally different than how it was four months ago.

SPEAKER_04：我想我很兴奋，因为我个人非常喜欢用 Claude 聊天。最近，我在家和 Claude 聊天的时间明显增多。我觉得最大的变化是编程工作。六个月前，我没有用 Claude 来做任何编程工作，我们的团队在编程上也不怎么用 Claude。而现在，这种情况发生了显著的改变。我在上上周在 YC 做了一次演讲。刚开始时，我问在座的有多少人现在使用 Claude 进行编程？结果几乎所有人都举手，这和四个月前的情况大不相同。

SPEAKER_03(Dario Amodei): So when I think about what I'm excited about, I think about places where, you know, like I said before, where there's this kind of consensus that, again, seems like consensus, seems like what everyone wise thinks, and then it just kind of breaks. And so places where I think that's about to happen and it hasn't happened yet. One of them is interpretability. I think interpretability is both the key to steering and making safe AI systems, and we're about to understand. And interpretability contains insights about intelligent optimization problems and about how the human brain works. I've said, and I'm really not joking, Chris Ola is gonna be a future Nobel medicine laureate.

SPEAKER_03：当我想到让我感到兴奋的事时，我会想到那些似乎已经形成共识的领域。这些领域看似是所有明智之士都认同的，但却可能很快会被颠覆。我认为，可解释性就是这样一个即将发生变化的领域。可解释性不仅是引导和确保 AI 系统安全的关键，我们也即将更深入地理解它。它还揭示了有关智能优化问题和人类大脑运作方式的深刻见解。我曾说过，我并非在开玩笑，Chris Ola 可能会成为未来的诺贝尔医学奖得主。

I'm serious because a lot of these, I used to be a neuroscientist, a lot of these mental illnesses, the ones we haven't figured out, right? Schizophrenia or the mood disorders. I suspect there's some higher level system thing going on and that it's hard to make sense of those with brains because brains are so mushy and hard to open up and interact with. Neural nets are not like this. They're not a perfect analogy, but as time goes on, they will be a better analogy. That's one area. Second is related to that, I think, just the use of AI for biology. Biology is an incredibly difficult problem. People continue to be skeptical for a number of reasons. I think that consensus is starting to break. We saw a Nobel Prize in Chemistry awarded for AlphaFold, remarkable accomplishment. We should be trying to build things that can help us create 100 AlphaFolds. And then finally, using AI to enhance democracy. We worry about if AI is built in the wrong way, it can be a tool for authoritarianism. How can AI be a tool for freedom and self-determination? I think that one is earlier than the other two, but it's going to be just as important.

我非常认真地说，因为这些情况，我过去是一名神经科学家，很多精神疾病，比如我们尚未完全理解的精神分裂症或情绪障碍。我怀疑这些问题涉及一些更高层次的系统，而这很难通过大脑来理解，因为大脑是如此复杂且难以直接研究。相比之下，神经网络并不具有这些难题。虽然它们不是完美的类比，但随着时间的推移，它们将成为更好的类比。这是一个领域。第二个相关领域是 AI 在生物学上的应用。生物学是一个极其复杂的问题，出于多种原因，人们对此仍持怀疑态度。然而，我认为这种共识正在改变。我们见证了化学领域的诺贝尔奖授予了 AlphaFold，这是一项非凡的成就。我们应该努力开发能够帮助我们创建更多类似 AlphaFold 成就的工具。最后，利用 AI 来增强民主。我们担心如果 AI 被错误地构建，它可能成为专制主义的工具。那么，如何让 AI 成为自由与自我决定的工具呢？我认为这方面的发展比前两个领域要早期，但它将同样重要。

SPEAKER_02(Jared Kaplan): Yeah, I mean, I guess two things that at least connect to what you were saying earlier. I mean, one is I feel like people frequently join Anthropic because they're sort of scientifically really curious about AI and then kind of get convinced by AI progress to sort of share the vision of the need, not just to advance the technology, but to understand it more deeply and to make sure that it's safe. And I feel like it's actually just sort of exciting to have people that you're working with, like kind of more and more united in their vision for both what AI development looks like and the sort of sense of responsibility associated with it. And I feel like that's been happening a lot due to a lot of advances that have happened in the last year, like what Tom talked about. Another is that, I mean, going back really to concrete problems, I feel like we've done a lot of work on AI safety up until this point. A lot of it's really important. But I think we're now, with some recent developments, really getting a glimmer of what kinds of risks might literally come about from systems that are very, very advanced so that we can investigate and study them directly with interpretability, with other kinds of safety mechanisms, and really understand what the risks from very advanced AI might look like. And I think that that's something that is really going to allow us to sort of further the mission in a really deeply scientific empirical way. And so I'm excited about sort of the next six months of how we use our understanding of what can go wrong with advanced systems to characterize that and figure out how to avoid those pitfalls.

SPEAKER_02：是的，我想至少有两点与你之前提到的内容相关。首先，很多人加入 Anthropic 是因为他们对 AI 充满科学好奇心，随着 AI 的进步，他们不仅想推动技术发展，更希望深入理解并确保其安全。与同事们在 AI 开发的愿景和责任感上逐渐达成共识，这让人感到振奋。由于过去一年中，尤其是 Tom 提到的那些进展，这种情况变得更加普遍。其次，回到具体问题上，我们在 AI 安全领域已经做了很多重要工作。最近的进展让我们开始看到非常先进的系统可能带来的风险，这使我们能够通过可解释性和其他安全机制直接研究这些风险，并深入了解高级 AI 的潜在威胁。我认为，这将帮助我们以一种科学实证的方式推动我们的使命。因此，我对接下来六个月内如何利用我们对高级系统可能问题的理解来描述和规避这些风险感到兴奋。