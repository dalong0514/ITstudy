### è·‘æ¨¡å‹

MODEL=/Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin uvicorn chatglm_cpp.openai_api:app --host 0.0.0.0 --port 8000


### é‡åŒ–æ–¹æ¡ˆ

[li-plus/chatglm.cpp: C++ implementation of ChatGLM-6B & ChatGLM2-6B & ChatGLM3 & more LLMs](https://github.com/li-plus/chatglm.cpp)

2023-12-02

1ã€clone ä»“åº“ã€‚

git clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp

æ–°å»ºä¸€ä¸ªè‡ªå·±çš„åˆ†æ”¯ï¼Œçœ‹æ“ä½œåçš„å˜åŒ–ï¼š

origin/hotfix/dalong

2ã€å®‰è£…ä¸‰æ–¹åŒ…ã€‚

python -m pip install torch tabulate tqdm transformers accelerate sentencepiece

æ ¸æŸ¥äº† torchã€tqdmã€transformersã€accelerateã€sentencepieceï¼Œä¹‹å‰ç¯å¢ƒéƒ½æœ‰äº†ï¼Œé‚£ä¹ˆåªéœ€ï¼š

pip install tabulate

3ã€é‡åŒ–ã€‚

python chatglm_cpp/convert.py -i /Users/Daglas/dalong.chatglm/chatglm3-6b -t q4_0 -o chatglm-ggml.bin

The original model (-i <model_name_or_path>) can be a Hugging Face model name or a local path to your pre-downloaded model. Currently supported models are:

ChatGLM-6B: THUDM/chatglm-6b, THUDM/chatglm-6b-int8, THUDM/chatglm-6b-int4
ChatGLM2-6B: THUDM/chatglm2-6b, THUDM/chatglm2-6b-int4
ChatGLM3-6B: THUDM/chatglm3-6b
CodeGeeX2: THUDM/codegeex2-6b, THUDM/codegeex2-6b-int4
Baichuan & Baichuan2: baichuan-inc/Baichuan-13B-Chat, baichuan-inc/Baichuan2-7B-Chat, baichuan-inc/Baichuan2-13B-Chat
You are free to try any of the below quantization types by specifying -t <type>:

q4_0: 4-bit integer quantization with fp16 scales.
q4_1: 4-bit integer quantization with fp16 scales and minimum values.
q5_0: 5-bit integer quantization with fp16 scales.
q5_1: 5-bit integer quantization with fp16 scales and minimum values.
q8_0: 8-bit integer quantization with fp16 scales.
f16: half precision floating point weights without quantization.
f32: single precision floating point weights without quantization.
For LoRA model, add -l <lora_model_name_or_path> flag to merge your LoRA weights into the base model.

4ã€æ„å»ºã€‚

cmake -B build
cmake --build build -j --config Release

5ã€è·‘æ¨¡å‹ã€‚

./build/bin/main -m chatglm-ggml.bin -i

è¡¥å……ï¼šé‡åŒ–åè·‘èµ·æ¥ï¼Œæ˜æ˜¾é€Ÿåº¦å¿«äº†å¥½å¤šå€ã€‚ï¼ˆ2023-12-02ï¼‰

è·‘ä¸‹é¢çš„å¸®åŠ©å¯ä»¥çœ‹åˆ°æ›´å¤šçš„ç©æ³•ã€‚

./build/bin/main -h

é’ˆå¯¹ ChatGLM3-6B çš„æ›´å¤šç©æ³•ï¼ŒåŒ…æ‹¬ Chat modeã€Function call ç­‰ã€‚

### éƒ¨ç½²è®°å½•

#### chaglm.cpp éƒ¨ç½²

1ã€ä¸‹è½½ä»“åº“ã€‚

[li-plus/chatglm.cpp: C++ implementation of ChatGLM-6B & ChatGLM2-6B & ChatGLM3 & more LLMs --- li-plus/chatglm.cppï¼šChatGLM-6B & ChatGLM2-6B & ChatGLM3 åŠæ›´å¤šæ³•å­¦ç¡•å£«çš„ C++ å®ç°](https://github.com/li-plus/chatglm.cpp)

git clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp

å¦‚æœæ‚¨åœ¨å…‹éš†å­˜å‚¨åº“æ—¶å¿˜è®°äº† --recursive æ ‡å¿—ï¼Œè¯·åœ¨ chatglm.cpp æ–‡ä»¶å¤¹ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

git submodule update --init --recursive

2ã€é‡åŒ–ã€‚

python chatglm_cpp/convert.py -i /Users/Daglas/dalong.datasets/chatglm3-6b-32k -t q8_0 -o /Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin

python chatglm_cpp/convert.py -i /Users/Daglas/dalong.datasets/chatglm3-6b-32k -t q4_0 -o /Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q4_0.bin

3ã€æ„å»ºå’Œè¿è¡Œã€‚

ä½¿ç”¨ CMake ç¼–è¯‘é¡¹ç›®ï¼š

cmake -B build
cmake --build build -j --config Release

4ã€è¿è¡Œã€‚

./build/bin/main -m /Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin -p ä½ å¥½
\# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚

./build/bin/main -m /Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin -i

5ã€è·‘ api æ¥å£

è·‘ OpenAI æ¨¡å¼çš„ APIï¼š

MODEL=/Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin uvicorn chatglm_cpp.openai_api:app --host 0.0.0.0 --port 8000

éªŒè¯ï¼š

curl http://127.0.0.1:8000/v1/chat/completions -H 'Content-Type: application/json' \
    -d '{"messages": [{"role": "user", "content": "ä½ å¥½"}]}'

è·‘ langchain æ¨¡å¼çš„ APIï¼š

MODEL=/Users/Daglas/dalong.datasets/chatglm3-6b-32k-ggml_q8_0.bin uvicorn chatglm_cpp.langchain_api:app --host 0.0.0.0 --port 8000

éªŒè¯ï¼š

curl http://127.0.0.1:8000 -H 'Content-Type: application/json' -d '{"prompt": "ä½ å¥½"}'

æ³¨æ„äº‹é¡¹ï¼šä¸èƒ½ç›´æ¥åœ¨ä»“åº“æ ¹ç›®å½•æ–‡ä»¶é‡Œè·‘ä¸Šé¢å¯åŠ¨çš„æœåŠ¡ï¼Œè¦åœ¨ä¸Šå±‚æ–‡ä»¶æˆ–å…¶ä»–è·¯å¾„æ–‡ä»¶é‡Œè·‘ã€‚




#### å®˜æ–¹å¸¸è§„éƒ¨ç½²

1ã€ä¸º M3 çš„ Mac å•ç‹¬å®‰è£… PyTorchã€‚

Mac éƒ¨ç½²

å¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„ Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM3-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ å®˜æ–¹è¯´æ˜ å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.x.x.dev2023xxxxï¼Œè€Œä¸æ˜¯ 2.x.xï¼‰ã€‚

ç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š

model = AutoModel.from_pretrained("your local path", trust_remote_code=True).to('mps')
åŠ è½½åŠç²¾åº¦çš„ ChatGLM3-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook Proï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚

[Accelerated PyTorch training on Mac - Metal - Apple Developer](https://developer.apple.com/metal/pytorch/)

pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu

2ã€å®‰è£…å…¶ä»– Python åŒ…ã€‚

pip install -r requirements.txt

å¦å¤–ï¼Œå‘ç°æ¼äº†ä¸€ä¸ªåŒ…ï¼šmdtex2html

requirements.txt æ–‡ä»¶å†…å®¹ï¼š

protobuf
transformers>=4.30.2
cpm_kernels
`#` torch>=2.0 # need install PyTorch-Nightly
gradio~=3.39
sentencepiece
accelerate
sse-starlette
streamlit>=1.24.0
fastapi>=0.104.1
uvicorn~=0.24.0
loguru~=0.7.2
latex2mathml
mdtex2html

3ã€ä¸‹è½½æœ¬åœ°å¤§æ¨¡å‹ã€‚

[chatglm3-6b Â· æ¨¡å‹åº“](https://modelscope.cn/models/ZhipuAI/chatglm3-6b/summary)

åœ¨ HuggingFace ä¸Šä¸‹è½½ï¼š

GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm3-6b
cd chatglm3-6b
wget "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/pytorch_model-00001-of-00007.bin"
wget "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/pytorch_model-00002-of-00007.bin"
...

å‘ç°å¤ªæ…¢äº†ï¼Œæ”¹æˆäº†ä» modelscope ä¸Šä¸‹è½½ï¼š

[chatglm3-6b Â· æ¨¡å‹åº“](https://modelscope.cn/models/ZhipuAI/chatglm3-6b/summary)

git lfs install
git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git

4ã€å®ç°æœ¬åœ°è·‘æ¨¡å‹ã€‚

å‚è€ƒæ”¾åœ¨ basic_demo ä¸‹é¢çš„ test.pyï¼Œå®ç°äº†åŸºæœ¬çš„è·‘æœ¬åœ°æ¨¡å‹çš„åŠŸèƒ½ã€‚

5ã€ç»¼åˆ webã€‚

pip install -r requirements.txt

ipython kernel install --name chatglm3-demo --user

export MODEL_PATH=/Users/Daglas/dalong.chatglm/chatglm3-6b

è¡¥å……ï¼šä¸Šé¢çš„å‘½ä»¤åªæ˜¯ä¸´æ—¶æ€§çš„å°† MODEL_PATH ç¯å¢ƒå˜é‡æ›´æ”¹ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œshell å…³äº†åå°±é‡ç½®äº†ã€‚

æ°¸ä¹…æ€§çš„æ›´æ”¹ï¼š

client.py ä¸­å°†æ¨¡å‹çš„è·¯å¾„ï¼ˆTHUDM/chatglm3-6bï¼‰æ›´æ”¹ä¸ºï¼š/Users/Daglas/dalong.datasets/chatglm3-6b

ç„¶åè·‘ï¼š

streamlit run main.py





éƒ¨ç½²æ—¶å‚è€ƒçš„èµ„æ–™ï¼š

æˆ‘ç°åœ¨æƒ³è¦åœ¨è‹¹æœ M3 èŠ¯ç‰‡çš„ MacBook ä¸Šéƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹{HUDM/chatglm3-6b}ï¼Œå…¶åœ¨  Hugging Face ä¸Šçš„ url åœ°å€ä¸º{https://huggingface.co/THUDM/chatglm3-6b}ã€‚

è¯·æœç´¢å…·ä½“çš„éƒ¨ç½²æ“ä½œæ­¥éª¤ï¼Œè¶Šè¯¦ç»†è¶Šå¥½ï¼Œè¯·ä¸€æ­¥ä¸€æ­¥æ¥

[ChatGLM3-6B éƒ¨ç½²æŒ‡å— - Hacker and Geeker's Way](https://zhaozhiming.github.io/2023/10/31/chatglm3-deploy/#:~:text=,r%20requirements.txt)

[ä½¿ç”¨ FastChat éƒ¨ç½² LLM - Hacker and Geeker's Way](https://zhaozhiming.github.io/2023/08/22/use-fastchat-deploy-llm/)

åœ¨è‹¹æœ M3 èŠ¯ç‰‡çš„ MacBook ä¸Šéƒ¨ç½² ChatGLM3-6B æ¨¡å‹çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. **ç¯å¢ƒå®‰è£…**ï¼š
   - å…‹éš† ChatGLM3 çš„ä»£ç ä»“åº“å¹¶å®‰è£…ç›¸å…³ä¾èµ–ï¼š
     ```
     git clone https://github.com/THUDM/ChatGLM3
     cd ChatGLM3
     pip install -r requirements.txt
     ```
   ã€39â€ sourceã€‘ã€‚

2. **æ¨¡å‹ä¸‹è½½**ï¼š
   - ä½¿ç”¨ Hugging Face ä¸‹è½½ ChatGLM3-6B çš„æ¨¡å‹æ–‡ä»¶ï¼š
     ```
     GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm3-6b
     cd chatglm3-6b
     wget "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/pytorch_model-00001-of-00007.bin"
     wget "https://huggingface.co/THUDM/chatglm3-6b/resolve/main/pytorch_model-00002-of-00007.bin"
     ...
     ```
   ã€40â€ sourceã€‘ã€‚

3. **è®¾ç½® Gradio æœåŠ¡**ï¼š
   - ä¿®æ”¹ `web_demo.py` æ–‡ä»¶ä¸­çš„æ¨¡å‹åœ°å€ä¸ºæœ¬åœ°åœ°å€ï¼š
     ```
     -tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
     -model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).cuda()
     +tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/chatglm3-6b", trust_remote_code=True)
     +model = AutoModel.from_pretrained("/root/autodl-tmp/chatglm3-6b", trust_remote_code=True).cuda()
     ```
   - å¯åŠ¨ Gradio æœåŠ¡ï¼š
     ```
     python web_demo.py
     ```
   ã€41â€ sourceã€‘ã€42â€ sourceã€‘ã€‚

4. **è®¾ç½® Streamlit æœåŠ¡**ï¼š
   - ä¿®æ”¹ `web_demo2.py` æ–‡ä»¶ä¸­çš„æ¨¡å‹åœ°å€ä¸ºæœ¬åœ°åœ°å€ï¼š
     ```
     -model_path = "THUDM/chatglm3-6b"
     +model_path = "/root/autodl-tmp/chatglm3-6b"
     ```
   - å¯åŠ¨ Streamlit æœåŠ¡ï¼š
     ```
     streamlit run web_demo2.py
     ```
   ã€43â€ sourceã€‘ã€‚

5. **å¯åŠ¨ API æœåŠ¡**ï¼š
   - å¯åŠ¨ API æœåŠ¡ï¼ˆé»˜è®¤ç«¯å£ä¸º 7861ï¼‰å¹¶ä½¿ç”¨ OpenAI æ¥å£è°ƒç”¨æ–¹å¼ï¼š
     ```
     python openai_api.py
     ```
   - é€šè¿‡ API å‘é€è¯·æ±‚ï¼š
     ```
     curl -X 'POST' \
       'https://localhost:7861/v1/chat/completions' \
       -H 'accept: application/json' \
       -H 'Content-Type: application/json' \
       -d '{
         "model": "gpt-3.5-turbo",
         "messages": [
           {
             "role": "user",
             "content": "ä½ å¥½"
           }
         ]
       }'
     ```
   ã€44â€ sourceã€‘ã€‚

ä»¥ä¸Šæ­¥éª¤æä¾›äº†åœ¨è‹¹æœ M3 èŠ¯ç‰‡çš„ MacBook ä¸Šéƒ¨ç½² ChatGLM3-6B å¤§è¯­è¨€æ¨¡å‹çš„è¯¦ç»†æŒ‡å—ã€‚è¯·ç¡®ä¿åœ¨æ‰§è¡Œè¿™äº›æ­¥éª¤ä¹‹å‰ï¼Œæ‚¨çš„ç³»ç»Ÿå·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„ä¾èµ–é¡¹å’Œå·¥å…·ã€‚

### é—®é¢˜æ±‡æ€»

1ã€è·‘ chat é¡µé¢ web_demo.py æ—¶ï¼Œè¾“å‡ºæ¡†é‡Œçš„è¾“å‡ºå†…å®¹æ€»æ˜¯æ— æ³•æ˜¾ç¤ºå‡ºæ¥ã€‚

æ‰“æ–­ç‚¹æ˜ç¡®äº†ï¼Œè¾“å‡ºæ•°æ®æ˜¯æ²¡é—®é¢˜çš„ï¼Œå°±æ˜¯æ˜¾ç¤ºä¸å‡ºæ¥ã€‚

ç›®å‰è¿˜æ˜¯æ²¡è§£å†³ã€‚ï¼ˆ2023-12-01ï¼‰

chmod 755 /var/folders/yg/v6bk_km13z7c5pzfk9pkq7_c0000gn/T


sudo chmod -R 755 /var/folders/yg/v6bk_km13z7c5pzfk9pkq7_c0000gn/T

tempfile.tempdir = "~/.tmp"

tempfile.tempdir = "/var/folders/yg/v6bk_km13z7c5pzfk9pkq7_c0000gn/T"

GPT:

å…³äºChatGLM3-6Bé¡¹ç›®ä¸­web_demo.pyæ–‡ä»¶æ— æ³•æ˜¾ç¤ºæ–‡å­—å†…å®¹çš„é—®é¢˜ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„è§£å†³æ–¹æ³•ï¼š

ä»£ç è°ƒæ•´ï¼šç¡®ä¿ä½ çš„ä»£ç æ­£ç¡®è®¾ç½®äº†Gradioæ¨¡å—å’Œæ¨¡å‹ã€‚è¿™åŒ…æ‹¬æ­£ç¡®åœ°å¯¼å…¥AutoModelå’ŒAutoTokenizerï¼Œä»¥åŠä½¿ç”¨æ­£ç¡®çš„é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ã€‚ç¡®ä¿ä½¿ç”¨model.eval()è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå¹¶æ­£ç¡®å®ç°é¢„æµ‹å‡½æ•°â€‹â€‹ã€‚

CPUå’Œå†…å­˜æ£€æŸ¥ï¼šæ£€æŸ¥ä½ çš„ç³»ç»Ÿèµ„æºæ˜¯å¦è¶³å¤Ÿã€‚è¿è¡ŒChatGLMæ¨¡å‹å¯èƒ½éœ€è¦è¾ƒé«˜çš„CPUå’Œå†…å­˜èµ„æºã€‚æœ‰äº›ç”¨æˆ·æŠ¥å‘Šéœ€è¦è‡³å°‘32GBçš„å†…å­˜ï¼Œå¦‚æœCPUè®¡ç®—æ…¢ï¼Œä¹Ÿå¯èƒ½ä¼šå½±å“æ€§èƒ½â€‹â€‹â€‹â€‹ã€‚

ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼šå¦‚æœå†…å­˜ä¸è¶³ï¼Œè€ƒè™‘ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼Œå¦‚chatglm-6b-int4ã€‚é‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦æ›´å°‘çš„å†…å­˜èµ„æºâ€‹â€‹ã€‚

Gradioæ¨¡å—æƒé™è°ƒæ•´ï¼šæœ‰äº›ç”¨æˆ·å‘ç°ï¼ŒGradioæ¨¡å—åœ¨ä½¿ç”¨æœåŠ¡å™¨æš‚å­˜ä¸´æ—¶æ–‡ä»¶æ—¶å¯èƒ½ä¼šå‡ºç°æƒé™é—®é¢˜ã€‚æ›´æ”¹Gradioæ¨¡å—çš„æƒé™æˆ–ä¸´æ—¶æ–‡ä»¶å­˜å‚¨ä½ç½®å¯èƒ½æœ‰åŠ©äºè§£å†³é—®é¢˜â€‹â€‹ã€‚

æŸ¥çœ‹ç›¸å…³é—®é¢˜è§£å†³æ–¹æ¡ˆï¼šæŸ¥é˜…ç›¸å…³GitHubè®¨è®ºå’Œè§£å†³æ–¹æ¡ˆï¼Œè¿™å¯èƒ½ä¼šæä¾›ç‰¹å®šäºä½ é‡åˆ°çš„é—®é¢˜çš„è§£å†³æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹ä¸ç±»ä¼¼é—®é¢˜ç›¸å…³çš„GitHubé—®é¢˜é“¾æ¥ï¼Œå¦‚THUDM/VisualGLM-6B#29ï¼Œå¯èƒ½æœ‰åŠ©äºæ‰¾åˆ°è§£å†³æ–¹æ¡ˆâ€‹â€‹ã€‚

æ€»çš„æ¥è¯´ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜å¯èƒ½éœ€è¦å¯¹ä»£ç è¿›è¡Œè°ƒæ•´ï¼Œæ£€æŸ¥å’Œä¼˜åŒ–ä½ çš„ç³»ç»Ÿèµ„æºï¼Œä»¥åŠå‚è€ƒç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚

[è¿è¡Œweb\_demo\_hf.pyæ—¶é‡åˆ°tmpæ–‡ä»¶å†™å…¥è¢«æ‹’çš„é—®é¢˜ Â· Issue #29 Â· THUDM/VisualGLM-6B](https://github.com/THUDM/VisualGLM-6B/issues/29)

### ä¸ fastgpt ç»“åˆå®ç°çŸ¥è¯†åº“éƒ¨ç½²è®°å½•

1ã€ä¸‹è½½ m3e æ¨¡å‹æ–‡ä»¶ã€‚

[M3E Models Â· æ¨¡å‹åº“](https://www.modelscope.cn/models/xrunda/m3e-base/files)

[moka-ai/m3e-base at main](https://huggingface.co/moka-ai/m3e-base/tree/main)

git lfs install
git clone https://huggingface.co/moka-ai/m3e-base

huggingface ä¸Šå¤ªæ…¢äº†ï¼Œä¸‹è½½ä¸ä¸‹æ¥ã€‚è¿˜æ˜¯åœ¨ modelscope ä¸Šä¸‹è½½çš„ã€‚

git lfs install
git clone https://www.modelscope.cn/xrunda/m3e-base.git

2ã€ä¸‹è½½ FastGPT ä»“åº“ã€‚ï¼ˆå‚è€ƒä»£ç ç”¨ï¼‰

git clone https://github.com/labring/FastGPT.git

3ã€ä¿®æ”¹ openai_api_demoã€‚

å‡ ä¸ªæ³¨æ„ç‚¹ï¼š

1ï¼‰æ›´æ”¹æœ¬åœ°æ¨¡å‹æ•°æ® chatglm3-6b å’Œ m3e-base çš„è·¯å¾„ã€‚

2ï¼‰æœŸé—´æœ‰ 3 ä¸ªåŒ…è¦æ–°å®‰è£…ï¼š

import tiktoken
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import PolynomialFeatures

pip install tiktoken sentence_transformers scikit-learn

4ã€éƒ¨ç½² FastGPTã€‚

æŠŠæ–‡ä»¶

docker compose up -d

http://localhost:3000/

root

1234



curl --location --request POST 'http://127.0.0.1:8000/v1/embeddings' \
--header 'Authorization: Bearer sk-aaabbbcccdddeeefffggghhhiiijjjkkk' \
--header 'Content-Type: application/json' \
--data-raw '{
  "model": "m3e",
  "input": ["lafæ˜¯ä»€ä¹ˆ"]
}'

curl -X POST "http://127.0.0.1:8000/v1/chat/completions" \
-H "Content-Type: application/json" \
-d "{\"model\": \"chatglm3-6b\", \"messages\": [{\"role\": \"system\", \"content\": \"You are ChatGLM3, a large language model trained by Zhipu.AI. Follow the user's instructions carefully. Respond using markdown.\"}, {\"role\": \"user\", \"content\": \"ä½ å¥½ï¼Œç»™æˆ‘è®²ä¸€ä¸ªæ•…äº‹ï¼Œå¤§æ¦‚100å­—\"}], \"stream\": false, \"max_tokens\": 100, \"temperature\": 0.8, \"top_p\": 0.8}"



docker compose up -d




192.168.31.208

å…¬å¸ï¼š
  
192.168.28.58



1ã€IP åœ°å€çš„å¡ç‚¹ã€‚

ifconfig

å– en0 é‡Œçš„ inet 192.168.28.58 


### èµ„æºè®°å½•

2023-11-30


[å®‰è£… Git Large File Storage - GitHub æ–‡æ¡£](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)


2023-11-27

æ™ºè°±çš„æŠ€æœ¯æ–‡æ¡£ï¼š

[â¡â£â€â€‹â£â€â€‹â€‹â€‹â£â¤ChatGLM3 æŠ€æœ¯æ–‡æ¡£ - é£ä¹¦äº‘æ–‡æ¡£](https://lslfd0slxc.feishu.cn/wiki/WvQbwIJ9tiPAxGk8ywDck6yfnof)



2023-11-26

æœ€æºå¤´çš„ä¿¡æ¯ï¼š

[THUDM (Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University)](https://huggingface.co/THUDM)

[THUDM/chatglm3-6b Â· Hugging Face](https://huggingface.co/THUDM/chatglm3-6b)

ç„¶åè½¬åˆ° GitHubï¼š

[THUDM/ChatGLM3: ChatGLM3 series: Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹](https://github.com/THUDM/ChatGLM3)

GitHub æåŠçš„é…å¥—å·¥å…·ï¼š

[li-plus/chatglm.cpp: C++ implementation of ChatGLM-6B & ChatGLM2-6B & ChatGLM3 & more LLMs](https://github.com/li-plus/chatglm.cpp)

å“”ç«™ä¸Šçœ‹åˆ°æ™ºæ™®çš„ä¸“æ ï¼Œæœ‰ä¸€ç¯‡è®² ChatGLM-6B çš„ï¼š

[ä½é…ç½®éƒ¨ç½² ChatGLM3-6B | æ™ºè°± Ã— é­”æ­ç¤¾åŒº\_å“”å“©å“”å“©\_bilibili](https://www.bilibili.com/video/BV11N4y1D7HC/?spm_id_from=333.999.0.0&vd_source=280fc27368a92928cafc2cb72c54a549)

å‰é¢ä¸€ä¸ªè€å¸ˆè®²åœ¨é­”å¡”ä¸Šéƒ¨ç½²ï¼Œè·å¾—çš„ä¿¡æ¯æœ‰é™ã€‚21min çš„æ—¶å€™æ¢äº†å¦ä¸€ä¸ªè€å¸ˆè®²è®­ç»ƒç›¸å…³çš„ä¸œè¥¿ã€‚

