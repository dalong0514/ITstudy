### èµ„æº

[langchain-ai/langchain: ğŸ¦œğŸ”— Build context-aware reasoning applications](https://github.com/langchain-ai/langchain)

### éƒ¨ç½²è®°å½•

å®‰è£…ä¾èµ–åŒ…ï¼š

pip install langchain

å®‰è£… OpenAI sdkï¼š

pip install -U langchain-openai


### è·‘æ¨¡å‹è®°å½•

#### 2024-05-11

```py

```

gemini

```py
\# -*- coding:utf-8 -*-
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import time, os
import api_key as api
import common_tools as common_tools

api_key = api.gemini_api_key()
\# å°†API Keyä¿å­˜ä¸ºç¯å¢ƒå˜é‡
os.environ["GOOGLE_API_KEY"] = api_key
model_name='gemini-1.5-flash-latest'

model = ChatGoogleGenerativeAI(
    model=model_name
)

output_parser = StrOutputParser()

def translate_once(prompt, origin_content):
    chain = prompt | model | output_parser
    response = chain.invoke({"input": origin_content})
    out_content = common_tools.extract_translation(response)
    out_content = common_tools.modify_text(out_content)
    result = out_content
    # ä½¿ç”¨format()æ–¹æ³•æ‹¼æ¥å­—ç¬¦ä¸²å¹¶åŠ å…¥æ¢è¡Œç¬¦
    # result = "{}\n\n{}".format(origin_content, out_content)
    return result

\# æ­¥éª¤äºŒï¼šå¤„ç†å†…å®¹
def process_chunks(prompt, chunks):
    processed_chunks = []
    for chunk in chunks:
        modified_chunk = translate_once(prompt, chunk)
        processed_chunks.append(modified_chunk)
    return processed_chunks

def merge_and_save_chunks(chunks, filename):
    merged_text = '\n\n'.join(chunks)
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(merged_text)

def translate():
    prompt_content = common_tools.read_file('./translate_prompt.md')
    origin_content = common_tools.read_file('/Users/Daglas/Desktop/input.md')
    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_content),
        ("user", "{input}")
    ])
    chunks = common_tools.split_text_by_length(origin_content, 2000)
    processed_chunks = process_chunks(prompt, chunks)
    merge_and_save_chunks(processed_chunks, '/Users/Daglas/Desktop/output.md')

start_time = time.time()
print('waiting...\n')
translate()
end_time = time.time()
print('Time Used: ' + str((end_time - start_time)/60) + 'min')
```

#### 2024-05-14

openai

```py
\# -*- coding:utf-8 -*-
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import time, os
import api_key as api
import common_tools as common_tools

api_key = api.openai_api_key()
\# å°†API Keyä¿å­˜ä¸ºç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = api_key
model_name='gpt-4o'

model = ChatOpenAI(
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

def translate_once(prompt, origin_content):
    chain = prompt | model | output_parser
    response = chain.invoke({"input": origin_content})
    out_content = common_tools.extract_translation(response)
    out_content = common_tools.modify_text(out_content)
    result = out_content
    # ä½¿ç”¨format()æ–¹æ³•æ‹¼æ¥å­—ç¬¦ä¸²å¹¶åŠ å…¥æ¢è¡Œç¬¦
    # result = "{}\n\n{}".format(origin_content, out_content)
    return result

\# æ­¥éª¤äºŒï¼šå¤„ç†å†…å®¹
def process_chunks(prompt, chunks):
    processed_chunks = []
    for chunk in chunks:
        modified_chunk = translate_once(prompt, chunk)
        processed_chunks.append(modified_chunk)
    return processed_chunks

def merge_and_save_chunks(chunks, filename):
    merged_text = '\n\n'.join(chunks)
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(merged_text)

def translate():
    prompt_content = common_tools.read_file('./translate_prompt.md')
    origin_content = common_tools.read_file('/Users/Daglas/Desktop/input.md')
    prompt = ChatPromptTemplate.from_messages([
        ("system", prompt_content),
        ("user", "{input}")
    ])
    chunks = common_tools.split_text_by_length(origin_content, 2000)
    processed_chunks = process_chunks(prompt, chunks)
    merge_and_save_chunks(processed_chunks, '/Users/Daglas/Desktop/output.md')

start_time = time.time()
print('waiting...\n')
translate()
end_time = time.time()
print('Time Used: ' + str((end_time - start_time)/60) + 'min')
```


#### 2024-05-11

```py
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings

loader = TextLoader("/Users/Daglas/Downloads/20230705èˆ’ä¼Ÿæ°åŸ¹è®­HAZOP.md")
docs = loader.load()

text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=500,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

texts = text_splitter.split_documents(docs)

\# save to disk
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)
```


```py
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import os
import api_key as api

os.environ["OPENAI_API_KEY"] = api.openai_api_key()

\# load from disk
vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=OpenAIEmbeddings(),)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

api_key = api.deepseek_api_key()
base_url= 'https://api.deepseek.com/v1'
model_name='deepseek-chat'

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

model = ChatOpenAI(
    base_url=base_url,
    api_key=api_key,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

setup_and_retrieval = RunnableParallel(
    {"context": retriever, "question": RunnablePassthrough()}
)

context_and_question = setup_and_retrieval.invoke("ä»€ä¹ˆæ˜¯ HAZOP åˆ†æï¼Œç»“åˆå®é™…æ¡ˆä¾‹è¯¦ç»†è®²è§£ï¼Œä¸€æ­¥ä¸€æ­¥æ¥")
print(context_and_question)

chain = setup_and_retrieval | prompt | model | output_parser

response = chain.stream("ä»€ä¹ˆæ˜¯ HAZOP åˆ†æï¼Œç»“åˆå®é™…æ¡ˆä¾‹è¯¦ç»†è®²è§£ï¼Œä¸€æ­¥ä¸€æ­¥æ¥")
for value in response:
    print(value, end='', flush=True)
```



```py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import os
import api_key as api

os.environ["OPENAI_API_KEY"] = api.openai_api_key()

vectorstore = Chroma.from_texts(
    ["harrison worked at kensho", "bears like to eat honey"],
    embedding=OpenAIEmbeddings(),
)
retriever = vectorstore.as_retriever()

api_key = api.deepseek_api_key()
base_url= 'https://api.deepseek.com/v1'
model_name='deepseek-chat'

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

llm = ChatOpenAI(
    base_url=base_url,
    api_key=api_key,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

setup_and_retrieval = RunnableParallel(
    {"context": retriever, "question": RunnablePassthrough()}
)

chain = setup_and_retrieval | prompt | llm | output_parser

response = chain.stream("where did harrison work?")
for value in response:
    print(value, end='', flush=True)
```


```py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
import os
import api_key as api

os.environ["OPENAI_API_KEY"] = api.openai_api_key()

vectorstore = Chroma.from_texts(
    ["harrison worked at kensho", "bears like to eat honey"],
    embedding=OpenAIEmbeddings(),
)
retriever = vectorstore.as_retriever()

api_key = api.deepseek_api_key()
base_url= 'https://api.deepseek.com/v1'
model_name='deepseek-chat'

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

llm = ChatOpenAI(
    base_url=base_url,
    api_key=api_key,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

setup_and_retrieval = RunnableParallel(
    {"context": retriever, "question": RunnablePassthrough()}
)

chain = setup_and_retrieval | prompt | llm | output_parser

chain.invoke("where did harrison work?")
```


#### 2024-05-08


```py
import importlib
import api_key as api

\# é‡æ–°åŠ è½½api_keyæ¨¡å—
importlib.reload(api)

api_key = api.deepseek_api_key()
translate_prompt = api.read_file('./translate_prompt.md')
print(translate_prompt)
```


```py
import api_key as api
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

api_key = api.deepseek_api_key()
base_url= 'https://api.deepseek.com/v1'
model_name='deepseek-chat'

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    ("user", "{input}")
])

llm = ChatOpenAI(
    base_url=base_url,
    api_key=api_key,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

chain = prompt | llm | output_parser
response = chain.stream({"input": "ä¸–ç•Œè½´å¿ƒæ—¶ä»£çš„å››ä½åœ£äºº"})

for value in response:
    print(value, end='', flush=True)
```



```py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

base_url= 'http://192.168.10.109:11434/v1'
model_name='qwen:110b'

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    ("user", "{input}")
])

llm = ChatOpenAI(
    base_url=base_url,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

chain = prompt | llm | output_parser
response = chain.stream({"input": "ä¸–ç•Œè½´å¿ƒæ—¶ä»£çš„å››ä½åœ£äºº"})

for value in response:
    print(value, end='', flush=True)
```


```py
import api_key as api
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

api_key = api.deepseek_api_key()
base_url= 'https://api.deepseek.com/v1'
model_name='deepseek-chat'

llm = ChatOpenAI(
    base_url=base_url,
    api_key=api_key,
    model_name=model_name,
    streaming=True
)

output_parser = StrOutputParser()

chain = llm | output_parser
response = chain.stream("ä¸–ç•Œè½´å¿ƒæ—¶ä»£çš„å››ä½åœ£äºº")

for value in response:
    print(value, end='', flush=True)
```

---

```py
import os
import api_key as api
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from IPython.display import Markdown

os.environ["OPENAI_API_KEY"] = api.openai_api_key()

llm = ChatOpenAI(
    streaming=True
)
output_parser = StrOutputParser()

chain = llm | output_parser
response = chain.stream("ä¸–ç•Œè½´å¿ƒæ—¶ä»£çš„å››ä½åœ£äºº")

for value in response:
    print(value, end='', flush=True)
```

---

```py
import os
import api_key as api
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from IPython.display import Markdown

os.environ["OPENAI_API_KEY"] = api.openai_api_key()

llm = ChatOpenAI(
    streaming=True
)
output_parser = StrOutputParser()

chain = llm | output_parser
output = chain.invoke("ä¸–ç•Œè½´å¿ƒæ—¶ä»£çš„å››ä½åœ£äºº")
display(Markdown(output))
```

### Jupyter Notebook ä¸­å®éªŒ

1ã€å…ˆæ¿€æ´» llama è™šæ‹Ÿç¯å¢ƒã€‚

2ã€å®‰è£… Jupyter Notebook å’Œ ipykernelã€‚

pip install notebook ipykernel

3ã€æ·»åŠ è™šæ‹Ÿç¯å¢ƒåˆ° Jupyter Notebook å†…æ ¸åˆ—è¡¨ä¸­ã€‚

python3.10 -m ipykernel install --user --name=llama --display-name="Python (llama)"

4ã€å¯åŠ¨ã€‚

jupyter notebook

### é—®é¢˜æ±‡æ€»

1ã€Jupyter Notebook é‡Œæ²¡æ³•ç”¨ llama è™šæ‹Ÿç¯å¢ƒã€‚

GPT-4 å›å¤çš„è§£å†³æ–¹æ¡ˆï¼š

åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£… Jupyter å’Œ ipykernelï¼šç¡®ä¿è™šæ‹Ÿç¯å¢ƒæ¿€æ´»åï¼Œå®‰è£… Jupyter Notebook å’Œ ipykernelï¼š

pip install notebook ipykernel

æ·»åŠ è™šæ‹Ÿç¯å¢ƒåˆ° Jupyter Notebook å†…æ ¸åˆ—è¡¨ä¸­ï¼šå®‰è£…å®Œ ipykernel åï¼Œä½ éœ€è¦åˆ›å»ºä¸€ä¸ªå†…æ ¸æŒ‡å‘ä½ çš„è™šæ‹Ÿç¯å¢ƒï¼Œè¿™æ · Jupyter Notebook å°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªç¯å¢ƒäº†ã€‚

python3.10 -m ipykernel install --user --name=llama --display-name="Python (llama)"

è¿™é‡Œçš„ --name æ˜¯ä½ ä¸ºå†…æ ¸æŒ‡å®šçš„æ ‡è¯†ï¼Œ--display-name æ˜¯åœ¨ Jupyter ä¸­æ˜¾ç¤ºçš„åç§°ã€‚

æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ç”¨ python3.10 -m ipykernel å‘½ä»¤ï¼ŒGPT-4 å›å¤é‡Œçš„ python -m ipykernel å‘½ä»¤æ— æ•ˆçš„ï¼Œåœ¨ Notebook ç”¨ pip list çœ‹ä¸åˆ°å·²ç»å®‰è£…çš„ langchain åŒ…ï¼Œæ”¹æˆ python3.10 è§£å†³çš„ã€‚

å¯åŠ¨ Jupyter Notebookï¼šç°åœ¨å¯ä»¥å¯åŠ¨ Jupyter Notebook äº†ï¼š

jupyter notebook

åœ¨ Jupyter ä¸­åˆ‡æ¢åˆ°è™šæ‹Ÿç¯å¢ƒï¼šåœ¨ Jupyter Notebook ä¸­ï¼Œåˆ›å»ºæ–°çš„ç¬”è®°æœ¬æˆ–æ‰“å¼€ä¸€ä¸ªå·²æœ‰çš„ç¬”è®°æœ¬ï¼Œåœ¨ç¬”è®°æœ¬çš„å³ä¸Šè§’æ‰¾åˆ°å†…æ ¸é€‰æ‹©èœå•ï¼Œé€‰æ‹©ä¹‹å‰åˆ›å»ºçš„ "Python (myenv)" å†…æ ¸ã€‚