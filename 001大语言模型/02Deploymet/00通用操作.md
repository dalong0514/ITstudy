ps aux | grep 20704

python examples/chat_gradio.py --model_name_or_path /Users/Daglas/dalong.datasets/Llama2-Chinese-13b-Chat-ms

python chat_gradio.py --model_name_or_path /Users/Daglas/dalong.datasets/Llama2-Chinese-13b-Chat-ms


### 01. 下载仓库代码

1、首先 fork 仓库代码。

2、添加远程上游：

git remote add upstream https://github.com/THUDM/ChatGLM3.git

git remote add upstream https://github.com/FlagAlpha/Llama2-Chinese.git

以使用 `git remote -v` 命令来查看现在的远程仓库，确认 upstream 已经添加成功。

git remote -v

获取更新：然后，你需要从原项目获取更新。你可以使用以下命令来获取：

git fetch upstream

这个命令会将原项目的所有更新下载到你的本地电脑上。

合并更新：获取更新之后，你需要将这些更新合并到你的项目中。你可以使用以下命令来合并：

git merge upstream/main

注意：这里的 `main` 是你要合并的分支，如果原项目使用的是其他名称，如 `master`，你需要将 `main` 替换为相应的名称。

推送更新：最后，你需要将合并后的更新推送到 GitHub 上。你可以使用以下命令来推送：

git push origin main

同样，这里的 `main` 是你要推送的分支，如果你使用的是其他名称，你需要将 `main` 替换为相应的名称。

3、新建自己的分支。

origin/hotfix/dalong



### 02. 下载模型文件

2024-05-12

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download ggerganov/whisper.cpp ggml-medium.bin --local-dir . --local-dir-use-symlinks False

huggingface-cli download --resume-download --local-dir-use-symlinks False BAAI/bge-reranker-v2-m3 --local-dir bge-reranker-v2-m3

BAAI/bge-m3

BAAI/bge-reranker-v2-m3

2024-05-09

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False openai/whisper-medium.en --local-dir whisper-medium-en

mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx

openai/whisper-medium.en


2024-04-11


huggingface-cli download Qwen/Qwen1.5-32B-Chat-GGUF qwen1_5-32b-chat-q8_0.gguf --local-dir . --local-dir-use-symlinks False

huggingface-cli download Qwen/Qwen1.5-32B-Chat-GGUF qwen1_5-32b-chat-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False







export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False Qwen/Qwen1.5-32B-Chat-GGUF --local-dir Qwen1.5-32B-Chat-GGUF


huggingface-cli download --resume-download --local-dir-use-symlinks False mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx --local-dir Mixtral-8x7B-v0.1-hf-4bit-mlx




2024-03-07

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx --local-dir Mixtral-8x7B-v0.1-hf-4bit-mlx

mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx

mlx-community/quantized-gemma-7b-it


2024-02-06

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False Qwen/Qwen1.5-72B-Chat --local-dir Qwen1.5-72B-Chat


Qwen/Qwen1.5-72B-Chat


2024-01-25

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False mlx-community/Mixtral-8x7B-Instruct-v0.1-hf-4bit-mlx --local-dir Mixtral-8x7B-Instruct-v0.1-hf-4bit-mlx


mlx-community/Mistral-7B-Instruct-v0.2-8-bit-mlx

mlx-community/Mixtral-8x7B-Instruct-v0.1-hf-4bit-mlx



2024-01-23

export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download --resume-download --local-dir-use-symlinks False HIT-SCIR/Chinese-Mixtral-8x7B --local-dir chinese-mixtral-8x7b

huggingface-cli download --resume-download --local-dir-use-symlinks False HIT-SCIR/Chinese-Mixtral-8x7B-adapter --local-dir chinese-mixtral-8x7b-adapter


[hf-mirror.com - Huggingface 镜像站](https://hf-mirror.com/)

本站域名 hf-mirror.com，用于镜像 huggingface.co 域名。

更多用法（多线程加速等）详见这篇文章。简介：

方法一：使用huggingface 官方提供的 huggingface-cli 命令行工具。

(1) 安装依赖

pip install -U huggingface_hub

(2) 基本命令示例：

export HF_ENDPOINT=https://hf-mirror.com

测试下载一个模型：

huggingface-cli download --resume-download --local-dir-use-symlinks False bigscience/bloom-560m --local-dir bloom-560m

(3) 下载需要登录的模型（Gated Model）

请添加--token hf_***参数，其中hf_***是 access token，请在huggingface官网这里获取。示例：

huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf

huggingface-cli download --token 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 --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf

方法二：使用url直接下载时，将 huggingface.co 直接替换为本站域名hf-mirror.com。使用浏览器或者 wget -c、curl -L、aria2c 等命令行方式即可。

下载需登录的模型需命令行添加 --header hf_*** 参数，token 获取具体参见上文。

方法三：(非侵入式，能解决大部分情况)huggingface 提供的包会获取系统变量，所以可以使用通过设置变量来解决。

HF_ENDPOINT=https://hf-mirror.com python your_script.py

不过有些数据集有内置的下载脚本，那就需要手动改一下脚本内的地址来实现了



[YeungNLP/firefly-mixtral-8x7b at main](https://huggingface.co/YeungNLP/firefly-mixtral-8x7b/tree/main)

huggingface-cli download --resume-download --local-dir-use-symlinks False YeungNLP/firefly-mixtral-8x7b --local-dir firefly-mixtral-8x7b

注意事项：每次下载之前都需要设置下环境变量：

export HF_ENDPOINT=https://hf-mirror.com

应该也可以设置为全部变量，现在就按这种模式，先别设全部环境变量。


huggingface-cli download --resume-download --local-dir-use-symlinks False mlx-community/Mixtral-8x7B-Instruct-v0.1 --local-dir mlx-mixtral-8x7b-instruct-v0.1

huggingface-cli download --resume-download --local-dir-use-symlinks False mistralai/Mistral-7B-Instruct-v0.2 --local-dir Mistral-7B-Instruct-v0.2



huggingface-cli download --resume-download --local-dir-use-symlinks False stabilityai/stable-diffusion-xl-base-1.0 --local-dir stable-diffusion-xl-base-1.0





mistralai/Mistral-7B-Instruct-v0.2


核心是用 git lfs 下载大容量文件。

先用 git lfs install 启动。

注意事项：

1、下载模型的时候把代理退出来。

2、在哪个位置找下载 URL。

对于 modelscope，进入「模型文件」TAB 页面，右侧有个「下面模型」，点开，有 git 的下载地址。

对于 hugging face，进入「模型文件」TAB 页面，右侧有个「下面模型」，点开，有 git 的下载地址。

举例：

[通义千问-72B-Chat · 模型库](https://modelscope.cn/models/qwen/Qwen-72B-Chat/files)

git lfs install
git clone https://www.modelscope.cn/qwen/Qwen-72B-Chat.git

3、看下载文件的状态。

git lfs ls-files

4、下载终止后的处理措施。

Git Large File Storage (LFS) 是一个用于处理大文件的 Git 扩展。如果你在使用 Git LFS 下载大文件时遇到中断，通常可以通过再次执行 `git lfs pull` 或 `git pull` 命令来重新开始下载流程。

备注：要开代理干其他事情的时候，可以人为的中断下（ctrl+c）。



以下是一些步骤，可以帮助你继续下载被中断的文件：

1、重试拉取命令：

执行 `git lfs pull` 或 `git pull` 来尝试重新开始下载。Git LFS 会尝试从上次中断的地方继续下载。

git lfs pull

或者

git pull

2、增加 Git LFS 的超时时间：

如果你认为下载中断是因为 Git LFS 超时，你可以尝试增加超时设置：

git config lfs.dialtimeout 10

git config lfs.tlstimeout 10

git config lfs.activitytimeout 30

这里的时间是以秒为单位的。调整这些值可能会帮助你避免因网络延迟导致的超时问题。

3、清理 Git LFS 缓存：

有时候，清理 Git LFS 的缓存可以解决下载问题：

git lfs prune

4、查看 Git LFS 日志：

如果上述步骤都不起作用，检查 Git LFS 的日志可能会提供更多关于为什么下载失败的信息：

git lfs logs last

5、手动下载：

如果自动下载一直失败，你可以尝试手动下载问题文件。首先找到文件的 LFS 指针文件，它通常是一个文本文件，里面包含了文件的元数据。然后，你可以使用 `git lfs fetch` 命令和对应的文件指针来手动下载文件。

git lfs fetch --all

或者，单独为特定文件：

git lfs fetch origin <path-to-file>

如果你仍然遇到问题，可能需要查看你的 Git LFS 配置或者联系你的 Git 仓库托管服务的支持团队，以获得更具体的帮助。

当使用 Git LFS 管理的项目有更新时，你通常会按照正常的 Git 工作流程来更新你的本地仓库。这包括 `fetch`、`pull` 或 `checkout` 等命令。Git LFS 会自动处理大文件的下载。

以下是更新到最新数据的一般步骤：

1、获取最新的仓库信息：

首先，你需要获取远程仓库的最新信息。可以使用 `git fetch` 命令来完成这个操作。

git fetch

2、切换到相应的分支：

如果你不在要更新的分支上，使用 `git checkout` 命令切换到正确的分支。

git checkout main

或者切换到你想要更新的任何分支。

3、拉取最新的更新：

使用 `git pull` 命令拉取最新的更新。这个命令会合并远程分支的变更到你的本地分支。

git pull origin main

或者拉取你想要更新的任何分支。

如果项目使用了 Git LFS，并且有 LFS 对象的更新，`git pull` 会自动调用 `git lfs pull` 来下载最新的 LFS 对象。

4、确保 LFS 对象是最新的：

有时候，你可能需要手动触发 LFS 文件的下载，尤其是在有大量 LFS 对象时。你可以使用 `git lfs pull` 来确保所有 LFS 对象都是最新的。

git lfs pull

5、检查 Git LFS 状态：

要检查 Git LFS 管理的文件的状态，你可以使用以下命令：

git lfs ls-files

这个命令会列出所有由 Git LFS 跟踪的文件，以及它们的版本信息。

6、处理可能的合并冲突：

如果在拉取最新的更新时遇到合并冲突，你需要手动解决这些冲突。解决冲突后，提交这些更改以完成合并。

git add .

git commit -m "Resolve merge conflicts"

如果你遇到了下载 LFS 对象时的问题，可以按照之前提到的步骤来检查和解决这些问题。记住，保持 Git 和 Git LFS 的客户端更新到最新版本可以帮助避免兼容性和其他问题。

### 03. 跑 LLm 环境配置标准动作

2024-03-02

近期腾讯云的软件镜像源不够稳定，建议各位同学优先使用清华的。

pip 参考地址：

[pypi | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)

brew 参考地址：

[homebrew | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror](https://mirrors.tuna.tsinghua.edu.cn/help/homebrew/)

[homebrew-bottles | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror](https://mirrors.tuna.tsinghua.edu.cn/help/homebrew-bottles/)


2023-12-18

CONDA_SUBDIR=osx-arm64 conda create --name llama python=3.10

验证是否安装的是正确的：

python -c "import platform; print(platform.processor())"

pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu






一定要安装 osx-arm64 版的 Python。之前没有引起重视，直到今天看 llama-cpp-python 才知道两者差距这么大。

[abetlen/llama-cpp-python at localhost --- abetlen/llama-cpp-python 在本地主机](https://github.com/abetlen/llama-cpp-python?ref=localhost)

Note: If you are using Apple Silicon (M1) Mac, make sure you have installed a version of Python that supports arm64 architecture. For example:
注意：如果您使用的是Apple Silicon (M1) Mac，请确保您已安装支持arm64架构的Python版本。例如：

wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh
bash Miniforge3-MacOSX-arm64.sh
Otherwise, while installing it will build the llama.cpp x86 version which will be 10x slower on Apple Silicon (M1) Mac.
否则，安装时将构建 llama.cpp x86 版本，该版本在 Apple Silicon (M1) Mac 上速度会慢 10 倍。

Detailed MacOS Metal GPU install documentation is available at docs/install/macos.md
详细的 MacOS Metal GPU 安装文档位于 docs/install/macos.md

[MacOS Install with Metal GPU - llama-cpp-python](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)



2024-03-08

CONDA_SUBDIR=osx-arm64 conda create --name dalong python=3.10



2023-12-10

CONDA_SUBDIR=osx-arm64 conda create -n native numpy -c conda-forge





1、新建一个干净的虚拟环境。

conda create --name llm

conda create --name llm python=3.10

conda create --name modelscope python=3.10

CONDA_SUBDIR=osx-arm64 conda create --name mlx python=3.10

conda create --name langchain-chat python=3.10

2、为 M3 的 Mac 单独安装 PyTorch。

pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu



conda env remove --name mlx

kill -9 4029


### 03. 量化模型

[QwenLM/qwen.cpp: C++ implementation of Qwen-LM](https://github.com/QwenLM/qwen.cpp)

2023-12-06

1、先 buid 项目。

直接在仓库文件根目录下面运行：

make

2、转为 gguf 文件。

python convert-hf-to-gguf.py /Users/Daglas/dalong.datasets/Qwen-72B-Chat --outfile /Users/Daglas/dalong.datasets/qwen72b-chat-f16.gguf --outtype f16

3、量化。

IN4 的：

./quantize /Users/Daglas/dalong.datasets/qwen72b-chat-f16.gguf /Users/Daglas/dalong.datasets/qwen72b-chat-q4_0.gguf q4_0

IN8 的：

./quantize /Users/Daglas/dalong.datasets/qwen72b-chat-f16.gguf /Users/Daglas/dalong.datasets/qwen72b-chat-q8_0.gguf q8_0

4、运行。

./main -m /Users/Daglas/dalong.datasets/qwen72b-chat-q4_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt

信息源：

[ggerganov/llama.cpp: Port of Facebook's LLaMA model in C/C++](https://github.com/ggerganov/llama.cpp)

[Merge qwen to llama cpp by simonJJJ · Pull Request #4281 · ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp/pull/4281)

\# convert Qwen HF models to gguf fp16 format
python convert-hf-to-gguf.py --outfile /Users/Daglas/dalong.datasets/qwen72b-chat-f16.gguf --outtype f16 Qwen-7B-Chat

\# quantize the model to 4-bits (using q4_0 method)
./build/bin/quantize qwen7b-chat-f16.gguf qwen7b-chat-q4_0.gguf q4_0

\# chat with Qwen models
./build/bin/main -m qwen7b-chat-q4_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt



2023-12-04

1、clone 仓库。

git clone --recursive https://github.com/QwenLM/qwen.cpp && cd qwen.cpp

新建一个自己的分支，看操作后的变化：

origin/hotfix/dalong

2、安装三方包。

python -m pip install torch tabulate tqdm transformers accelerate sentencepiece

核查了 torch、tqdm、transformers、accelerate、sentencepiece，之前环境都有了，那么只需：

pip install tabulate

3、量化。

72B 的模型：

python qwen_cpp/convert.py -i /Users/Daglas/dalong.datasets/Qwen-72B-Chat -t q4_0 -o qwen72b-ggml.bin

14B 的模型：

python qwen_cpp/convert.py -i /Users/Daglas/dalong.datasets/Qwen-14B-Chat -t q4_0 -o qwen14b-ggml.bin

python qwen_cpp/convert.py -i /Users/Daglas/dalong.datasets/Qwen-14B-Chat -t q8_0 -o qwen14b-ggml.bin

成功后的提示：

GGML model saved to qwen14b-ggml.bin

4、构建。

cmake -B build
cmake --build build -j --config Release

5、跑模型。

./build/bin/main -m qwen14b-ggml.bin --tiktoken /Users/Daglas/dalong.datasets/Qwen-14B-Chat/qwen.tiktoken -i

补充：量化后跑起来，明显速度快了好多倍。（2023-12-02）

跑下面的帮助可以看到更多的玩法。

./build/bin/main -h

针对 ChatGLM3-6B 的更多玩法，包括 Chat mode、Function call 等。

### python 包汇总

2023-12-18

absl-py #
accelerate #                    0.25.0
aiofiles #                      23.2.1
aiohttp #                     3.9.1
aiosignal #                     1.3.1
altair #                        5.2.0
annotated-types #               0.6.0
anyio #                         3.7.1
argcomplete #                   3.1.6
asgiref #                       3.7.2
async-timeout #                 4.0.3
attrs #                         23.1.0
bitsandbytes #                  0.41.2.post2
blinker #                       1.7.0
cachetools #                    5.3.2
certifi #                       2023.11.17
chardet #                      5.2.0
charset-normalizer #            3.3.2
click #                         8.1.7
colorama #                      0.4.6
coloredlogs #                   15.0.1
contourpy #                     1.2.0
cycler #                        0.12.1
datasets #                      2.15.0
dill #                          0.3.7
einops #                        0.7.0
exceptiongroup #                1.2.0
fastapi #                       0.104.1
ffmpy #                        0.3.1
filelock #                      3.13.1
Flask #                         3.0.0
fonttools #                     4.46.0
frozenlist #                    1.4.0
fsspec #                        2023.10.0
gekko #                         1.0.6
google-auth #                   2.25.2
google-auth-oauthlib #          1.2.0
gradio #                        4.8.0
gradio_client #                 0.7.1
grpcio #                        1.60.0
h11 #                           0.14.0
httpcore #                      1.0.2
httpx #                         0.25.2
huggingface-hub #               0.19.4
humanfriendly #                 10.0
idna #                          3.6
importlib-resources #           6.1.1
itsdangerous #                  2.1.2
Jinja2 #                        3.1.2
joblib #                        1.3.2
jsonschema #                    4.20.0
jsonschema-specifications #     2023.11.2
kiwisolver #                    1.4.5
Markdown #                      3.5.1
markdown-it-py #                3.0.0
MarkupSafe #                    2.1.3
matplotlib #                    3.8.2
mdurl #                         0.1.2
mpmath #                        1.3.0
multidict #                     6.0.4
multiprocess #                  0.70.15
networkx #                      3.2.1
nltk #                          3.8.1
numpy #                        1.26.2
oauthlib #                      3.2.2
optimum #                       1.14.1
orjson #                        3.9.10
packaging #                     23.2
pandas #                        2.1.3
peft #                          0.6.2
pipx #                          1.3.3
platformdirs #                  4.1.0
protobuf #                      4.23.4
psutil #                        5.9.6
pyarrow #                       14.0.1
pyarrow-hotfix #                0.6
pyasn1 #                        0.5.1
pyasn1-modules #                0.3.0
pydantic #                      2.5.2
pydantic_core #                 2.14.5
pydub #                         0.25.1
Pygments #                      2.17.2
pyparsing #                     3.1.1
python-dateutil #               2.8.2
python-multipart #              0.0.6
pytz #                          2023.3.post1
PyYAML #                        6.0.1
referencing #                   0.31.1
regex #                         2023.10.3
requests #                      2.31.0
requests-oauthlib #             1.3.1
rich #                          13.7.0
rouge #                         1.0.1
rpds-py #                       0.13.2
rsa #                           4.9
safetensors #                   0.4.1
scikit-learn #                  1.3.2
scipy #                         1.11.4
semantic-version #              2.10.0
sentence-transformers #         2.2.2
sentencepiece #                 0.1.99
setuptools #                    68.0.0
shellingham #                   1.5.4
six #                           1.16.0
sniffio #                       1.3.0
sse-starlette #                 1.8.2
starlette #                     0.27.0
sympy #                         1.12
tabulate #                      0.9.0
tensorboard #                   2.15.1
tensorboard-data-server #       0.7.2
threadpoolctl #                 3.2.0
tiktoken #                      0.5.2
tokenizers #                    0.13.3
tomli #                         2.0.1
tomlkit #                       0.12.0
toolz #                         0.12.0
tqdm #                          4.66.1
transformers #                  4.32.0
transformers-stream-generator # 0.0.4
typer #                         0.9.0
typing_extensions #             4.9.0rc1
tzdata #                        2023.3
urllib3 #                       2.1.0
userpath #                      1.9.1
uvicorn #                       0.24.0.post1
websockets #                    11.0.3
Werkzeug #                      3.0.1
xxhash #                        3.4.1
yarl #                          1.9.3

