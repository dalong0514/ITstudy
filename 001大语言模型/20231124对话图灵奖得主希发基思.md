## 对话图灵奖得主希发基思：AI 带来的最大威胁是使人类变成「奴隶的奴隶」

原创腾讯科技腾讯科技 2023-07-18 16:53 发表于北京

[对话图灵奖得主希发基思：AI 带来的最大威胁是使人类变成“奴隶的奴隶”](https://mp.weixin.qq.com/s/9syO6MvmSG-dzGqxOEgz9w)

《AI 未来指北》栏目由腾讯新闻推出，邀约全球业内专家、创业者、投资人，探讨 AI 领域的技术发展、商业模式、应用场景、及治理挑战。

自 2023 年初，ChatGPT 让世人陷入 AI 狂热之中。随着 GPT4 亮相，它涌现出的种种强大能力让人们觉得，只要再过几年时间，AI 将成为无所不能的存在。

但基于大语言模型 Transformer 范式的 AI 上限到底在哪里？它是否能真的完全取代我们？这些问题曾经有过很多答案。有人认为大语言模型将带来一个新的时代，它距离能够完成所有人类工作的人工智能非常接近；但也有人认为它不过是随机鹦鹉，根本无法理解这个世界。目前，不论哪一方观点都缺乏足够的阐释和成型的体系。

为了让人们更全面地看清楚这个问题，中科院外籍院士约瑟夫·希发基思写下了《理解和改变世界》，从认知原理的角度阐述了他长达几十年对于人工智能通向 AGI 的潜在道路的思考。约瑟夫·希发基思早于 Hinton 等人十年就已获得图灵奖，这次他非常清晰地从认知原理的角度阐述了他对于「人工智能的能与不能」，「通向 AGI 的潜在道路和风险」长达几十年的思考。

1、人和人工智能是互补的，而非互相替代的关系。人类拥有的常识知识和形成模型的抽象能力，这是当前范式的人工智能无法做到的，因此它们也不可能形成原理性的创新。而人工智能是可以在不掌握原理的情况下，对于可能存在大量变量的复杂事物进行预测。这是人因为认知能力限制而无法达到的，这被希发基思称为「AI 神谕」，它可能带来一种「新的科学」。

2、AI 会带来的最大威胁是人类对它的依赖可能导致我们无限让渡自己的判断，丧失决策权，最终成为「奴隶的奴隶」。为避免这一点，人类必须能够掌握知识的发展、应用的所有过程，确保不会让这些机器自身为我们做出关键决策。

3、根据能力互补的情况，对于人类来说，最好的未来情景是机器和人类之间达成一种和谐合作，并通过这种合作达成新的繁荣。在这个过程中，社会必须以人类生活的改善为目标发展并应用技术。

### 01. 当前的 AI 离 AGI 还很远

腾讯科技：ChatGPT 的出现对人工智能意味着什么？它是一个新的范式，还是更多的是一个已有范式特定的应用？

约瑟夫·希发基思：我认为，ChatGPT 和其他语言模型的出现是人工智能发展的重要一步。事实上，我们经历了一个范式转变，使得几乎任何自然语言查询都可以得到回应，并且通常是和问题非常相关的回答。大语言模型解决了长期悬而未决的自然语言处理问题。这是一个研究者们几十年来一直未能成功的领域，传统的方法是象征学派的方法，这一学派通过分离语言的语法和语义来构建人工智能的规则。

而现在，大语言模型采用了不同的方法，他们认为一个词的含义是由其所有使用上下文定义的。他们使用机器学习来执行概率分布的计算。对于单词，这个概率分布被用来预测在一个句子中最可能出现的下一个单词。这是一个非常简单但有效的方法。它有点天真，但证明它非常适合概括文本。当然，它采用的解决方案的性质也决定了它的局限性。语言模型非常适合创建一些文本的摘要，甚至写诗。如果你让它对 20 世纪中国历史进行摘要，它可以做得非常好。但另一方面，如果你问一些非常精确的问题或解决一些非常简单的逻辑问题，它可能会出错。我们可以理解这一点，因为这类问题无关上下文的模型，因此，我们无法检查文本的连贯性和它提供的答案。

腾讯科技：现在出现了很多新技术，例如逻辑树（LOT），它们可以帮助机器自我指导了解逻辑过程。现在，大语言模型正在训练自己开发更具体或更复杂的逻辑过程。在神经网络中都有很多层，层级越高，理解越抽象。如果在这些层级较高的神经元可能存在一些像模型或对世界的结构性理解的东西，这是可能的吗？

约瑟夫·希发基思：在我的书中，我解释了人类和机器开发和应用不同类型的知识。根据知识的有效程度和普遍性，这些知识使人类和机器能够解决不同类型的问题。一个重要的区别是「科学和技术知识「与「通过学习获得的隐式经验知识「之间的区别。例如，当我说话、走路时，我的大脑实际上解决了非常困难的问题，但我并不理解它们的工作原理，而神经网络产生了同样的隐式经验知识，使我们能够解决问题，而不必理解它们的工作原理。

这就是我们所说的基于数据或数据驱动的知识。相反，科学和技术知识是基于使用提供物体、构件的物理现象的深入理解的数学模型，这是是非常重要的。例如，当你建造一座桥时，（根据其原理）你可以确信这座桥在未来几个世纪内不会倒塌。但是，对于神经网络来说，我们可以得出某种预测，但我们不理解它们的工作原理，构建解释神经网络行为的理论是不可能的。这一特性使得大语言模型在没有人类参与的关键应用中存在严重的限制。

问题是这些 GPT-LM 系统是否能达到人类水平的智能。这是问题所在。我认为关于智能是什么，以及如何实现智能存在很多混淆。因为如果我们对智能没有明确的概念，就无法发展关于智能如何工作的理论，也无法清楚地界定智能。

而当今存在着很多混乱。最近，我写了一篇论文讨论这个问题。事实上，如果你打开字典，比如牛津词典，你会看到智能被定义为学习、理解和思考世界的能力，以及实现目标和有目的行动的能力。

机器可以做出令人印象深刻的事情。它们在游戏中可以超越人类。它们能够执行各种任务。最近也取得了巨大的成就。它们可以执行感官能力相关的任务，比如视觉识别。然而，当涉及到情境感知、环境变化的适应和创造性思维时，机器无法超越人类。很简单，GPT 非常擅长于翻译自然语言，但它无法驾驶汽车。你不能用 GPT 来驾驶汽车。这之间仍然存在很大的差距。我认为我们还有很长的路要走。今天我们只有弱人工智能，我们只有通用智能的一些组件。我们需要更多的东西。

我认为朝着通用智能迈出的重要一步将是自主系统。这个概念现在已经很清楚了，自主系统是由进一步自动化现有组织的需求产生的，通过用自主代理取代人类，这也是物联网所设想的。事实上，我们谈论的是自动驾驶汽车、智能电网、智能工厂、智能农场、更智能的电信网络。这些系统与弱人工智能存在很大的区别，因为这些系统由代理组成，它们受到实时约束，必须处理许多不同的目标。这些目标涉及许多不同领域的行动和活动的变化，而 GPT 在这方面并不擅长，它擅长于处理自然语言和文档的转换。此外，我们还需要能够与人类代理和谐合作的系统。所有这些对于其他语言模型来说是不可能的。所以我们离人工通用智能还相当遥远。当然，一切还是归结到我们究竟认为什么是智能的问题上，因为如果智能被定义为仅仅是对话和游戏，那么我们已经达到了人工通用智能，但我不同意这个定义。

腾讯科技：智能过往的标准测试是图灵测试。很明显在对话方面 GPT 已经通过了图灵测试，但它并非是自主的智能。在这种情况下，我们如何判断 AI 的智能程度？

约瑟夫·希发基思：我最近写了一篇论文，论证图灵测试是不够的。我提出了另一个测试，我称之为替代测试。实际上，这个想法是，如果我可以用一台机器替代执行任务的另一个代理，那么我会说这个代理和执行任务的代理一样聪明。如果我可以将机器代替人类驾驶汽车、教人类或成为一名优秀的外科医生，那么我会说机器和人类一样聪明。

因此，如果采用这个定义，替代测试，你会认为人类的智能实际上是一种技能的组合。这样说你明白我们离通用智能有多远了吗？在这个替代测试中，有些行为可能必须由机器，比如机器人来完成。当你想做园艺工作时，你需要一台机器人来完成。GPT 只是一个语言模型，它并不包含这些机器人部分。

腾讯科技：根据你的定义，只有当计算和系统可以自动执行大量文本并适应不断变化的环境时，我们才能看到人工智能和人类智能之间的差距消失。而且现在比如 AutoGPT 或 Baby AGI 这种应用可以将任务分成不同的步骤，并试图通过不同的过程实现任务的目标。它在某种程度上相当自动化。你认为在这个过程中，它是否更接近了 AGI？

约瑟夫·希发基思：这里有很多问题，包括系统工程的问题。仅仅拥有一个超级智能代理是不够的，因为你还必须保证可以解释它的行为。这也是我在我的论文中广泛讨论的一个问题，就是大家都在谈论的可解释人工智能或安全人工智能的问题。

人们不理解的是，对于神经网络来说，我们是无法理解它们的行为的。显然你无法解释为什么它会做出如此输出，因为你不能有一个数学模型来描述它们的行为。当然，我们完全理解神经网络的每个节点的数学函数是怎么计算的。它就是输入的线性组合，加上一些非线性函数，所以我们可以理解每一个节点的行为。但是当我们试图理解整个神经网络的涌现属性时，我们会感到绝望。但这不是一个特定于人工智能的问题，这是科学中的一个普遍问题。

你不能仅仅从氧原子和氢原子的性质来推断水的性质。即使你完全理解这一点，还有一个尺度和复杂性的问题。这是令人绝望的点。我们无法用技术组合的逻辑或还原论的方法来通过神经网络中元素的行为去理解它的整体行为。所以我们唯一能应用于神经网络的方法是测试，因为我们无法验证它的行为，也没法通过推理理解它。但是如果只应用测试，这意味着你采用了一种纯粹的实验方法，而非理论理解。所以你实际上能够测试的内容类型会变的很不同：例如，你无法测试整体性的安全问题，因为你无法分析整体行为。但你可以防御性的进行安全测试。

我们一直将测试应用于硬件和软件。但是为了进行测试，你得有测试应该持续的时间的标准。对于硬件和软件，我们有模型和覆盖标准。但对于神经网络来说，我们没有这种标准。我不是说这是一个非常难解决的问题，对于神经网络来说，我们有一些备选的可能，比如说是对抗性样本。但这些操作会破坏它们行为中的某种鲁棒性。所以你看，如果我问你一个问题，你会给出一个答案。如果我稍微修改你的问题，如果你是个人类的话，你会给出一些类似的答案。但我们知道，当我们稍微改变神经元的输入时，响应可能会有很大的不同。所以这也是需要考虑的。

02

涌现永远也无法被理解

腾讯科技：涌现这个概念，即从基本的能力到更高级的能力的转化，您认为是无法解释的吗？

约瑟夫·希发基思：是的。你把一个学科比如物理学拿出来。物理学是一个非常成熟的学科。物理学家试图在粒子理论、量子理论或广义相对论之间建立逻辑联系，我认为他们在这方面永远不会成功，因为存在一个尺度问题。我认为在任何类型的系统中都存在类似的问题。

腾讯科技：那么在你看来，由于这种无法解释的出现现象，我们实际上无法预测大语言模型到底能做到什么？

约瑟夫·希发基思：很明显，我们无法建立模型去预测它能做什么。我们无法建立模型，我指的是数学模型。在这里，人工智能社区使用模型这个词来表示神经网络，这是一种混淆的来源。

我认为我们应该采取另一种整体的方法。因为我们形不成相关的模型，也许我们可以有一种形成一种基于测试和经验观察的理论。它应该是一种关于统计学性质的测试理论。但依照我的理解，我们有些需求在当今的神经网络中从技术上很难被满足。

腾讯科技：是的。所以为了理解它们所涌现出的这些能力，我们需要建立类似心理学这样的学科去理解它？

约瑟夫·希发基思：完全准确。这是个好问题。但如果用 GPT 本身去建立这样的理解会有点问题。因为实际上有些人现在说，GPT 成功通过了成为律师或医生的考试，那么为什么这样的 GPT 不能成为医生或律师呢？

我认为这是一个非常有趣的论点，但这涉及到我之前提到的鲁棒性问题。同样是通过考试，人类与神经网络之间的能力是非常不同的。

鲁棒性的问题就是如果说如果让一个神志正常的人去回答问题，如果你稍微改变一下问题，答案会是相似。而 GPT 并不能保证答案的统一性。另一个问题是，人类可以依靠逻辑控制他们的行为和应该说的话。但因为神经网络，比较典型的就像 ChatGPT 对于它所做的事情没有受到语义上的控制，所以它可能会做一些明显错误的事情。有理智的人都不会犯这种错误。所以整个论证的结论是，如果 GPT 可以有逻辑地控制它所说的事情的一致性，而且它还具有相对应的鲁棒性的话，那么允许 GPT 成为律师就很棒。但我们实际上离这种程度的人工智能还很远。

腾讯科技：ChatGPT 为什么会这么难以控制？是因为它作为计算机的一种分布式运算的特性吗？

约瑟夫·希发基思：GPT 是一种不同类型的计算机。它是一台自然计算机。它不是你编写程序时执行程序的计算机，你对系统可以做和不能做有绝对控制。当你训练神经网络时，你就失去了这种控制。这些系统可以具有某种意义上的创造性，因为它们具有一定的自由度。

现在，如果我们能够控制这些自由度并理解它们的行为，那就没问题了。问题是我们无法控制神经网络的这种巨大的自由度，理论上去控制它几乎是无望的。你可以对它们的行为做一个粗略的近似，但你不会有准确的结果。如果你有一个传统计算机程序，即使它是一个很长的程序，你仍然可以提取语义模型并理解里面发生了什么。这是一个非常重要的区别。

腾讯科技：能详细讲讲自然机器的概念吗？

约瑟夫·希发基思：自然机器就是利用了自然现象的智能。比如神经网络就是一种类似于量子计算机或其他计算机的自然机器。在过去，当我还是学生的时候，我们也有很多计算机。在构建这种自然机器的过程中，我们会利用物理现象中的一些原理，因为任何物理现象都包含了一些信息内容。比如当我扔一块石头时，石头就像一台计算机，它计算出一个抛物线，这就形成了一种运算法则。你可以观察任何现象，并且可以利用自然现象来制造计算机。但是这些计算机不是预先编程的。它们利用了物理学或数学中的某些规律。神经网络就是这样的情况。

腾讯科技：让我们谈谈你书中的其他一些内容，你曾经讨论过一些研究和创新的问题。我们都知道，虽然神经网络的许多理念来自欧洲或日本，但使用它并产出产品的公司，如 OpenAI 和 Deepmind 都是在美国。你认为这是什么原因呢？

约瑟夫·希发基思：关注度和创新能力之间是存在差异的。因为创新是将研究应用于开发新产品或服务，以实现技术突破的能力。

我认为这是美国的一个非常强大的优势，他们在创新方面做得很出色。这始于加利福尼亚，在那里有我所说的创新生态系统。创新生态系统汇集了非常优秀的学术机构、大型科技公司、初创企业以及风险投资和资本。这种一致性可以有效而高效地转化新的成果和应用。其他国家也采用了这种模式。创新生态系统的理念是一种普遍的思想，像以色列和瑞士这样的小国家也取得了很大的成功。所以总结一下，我认为要实现创新，你应该将卓越的大学和卓越的产业联系在一起。这不仅取决于物质资源，还取决于文化因素、教育以及制度应该认可个体的创造力和创业精神。

03

神经网络神谕：一种无法被理解的新科学

腾讯科技：你刚刚提到神经网络是模拟生物大脑和物理世界的过程。在我们对生物大脑的理解仍然非常有限的情况下，是如何进行这种模拟的？这种神经网络与我们的生物大脑之间的差距有多大？

约瑟夫·希发基思：这是个好问题。我刚才说神经网络是一种自然计算机，采用了与传统计算机不同的范式。具体来说，神经网络是受到我们大脑中的神经运作启发所产生的。它模仿了一些神经运作的自然过程。然而，神经网络只是模仿了大脑的计算原理，而大脑是更加复杂的，因为它在不同的区域有着不同的结构和功能。而这些不同的功能是建立在更复杂的架构之上的，我们仍在努力去理解它。而且大脑的神经网络是一种并行计算模式。神经网络在这方面也与它有很多不同之处。

还应该理解的是，如果只研究生物水平的大脑，我认为我们无法完全捕捉到所有的人类意图。举个例子，用你的笔记本电脑运行一个软件。然后我给你电子仪器来通过测量研究这个硬件怎么工作。如果你已经编译了程序，所有的知识都在硬件层面上以电信号的形式呈现了。但只通过分析这个电信号，是不可能找到有问题的软件源代码的，因为你有这个尺度问题。我认为这是理解人类智能的关键，我们必须研究大脑，但不仅仅是大脑。因此，大脑的计算现象是电信号、物理化学现象和心理现象的结合。

而今天的问题是如何将心理现象与大脑计算相连接。这是我认为的一个重大挑战。如果我们在这方面不成功，我认为我们将永远无法理解人类智能。

腾讯科技：你曾提到人工智能正在为发展人类知识开辟一条新的道路，突破人类大脑处理复杂性问题的限制。那您觉得 AI 可以在哪些点上完全的超越人类？

约瑟夫·希发基思：是的。在我的书中，我解释了机器可以帮助我们克服一些思维的限制。这一点已经得到了心理学家的确认。这里的限制包括人的思维受到认知复杂性的限制。我们人类无法理解超过五个独立参数之间的关系。这也是我们发展的理论都很简单的原因。我们没有一个包含 1000 个独立参数形成的理论。

图片

我认为现在可以发展出一个拥有数千个参数的理论。我认为今天我们可以借助超级计算机和人工智能构建我所称之为 "神经网络神谕" 的东西。神经网络神谕是一个经过训练的神经网络，用于理解和分析复杂的现象或复杂的系统。这些复杂现象可能依赖于成千上万个参数。以物理现象为例，现在有一些有趣的项目，比如训练神经网络来预测地震。这些项目的参与者不需要拥有太多的科学知识，只需要用数据库喂养模型。他们手中都有全球各地的地震数据。他们发表了一篇论文，解释说用非常简单的训练过程，他们可以做出比使用现有复杂理论更好的预测。

所以我认为这是未来非常重要的方向。我们将拥有更多帮助我们预测复杂现象或复杂系统发展的 "神谕"。例如，我们将拥有智能数字孪生系统，这些系统将帮助我们进行预测，但并不理解（预测的逻辑）。因此我们将拥有一种新型的科学。我认为能够使用这种科学是很有趣的，但我们也需要控制所产生的知识的质量。你应该思考这个问题，因为人类将不再独自享有产生知识的特权。现在人得与机器竞争了。

所以对我们的社会来说，重要的问题是我们是否能够与机器合作，掌握由机器辅助开发的知识的发展和演进。或者我们将发展出一种由人驱动的科学和机器驱动的科学并存的情况。如果我们拥有这些机器所推动的并行科学，那将是一个有趣的情景。

腾讯科技：你提到人类思维也是一种计算系统。与自动机器相比，这两种系统在其组成部分上非常相似。所以与强人工智能相比，人类具有哪些独特的能力？

约瑟夫·希发基思：这是一个非常好的问题。因为我一直在研究自主系统，所以我尝试了设计自动驾驶汽车。对于自动驾驶汽车，你会有一些功能，比如感知，将感官信息转化为概念。你会有一个反射功能，基于外部世界建立模型，然后做出决策。做决策意味着管理多个不同的目标。为了实现这些目标，你需要规划等等。自主系统和人类思维之间确实有很多相似之处。

然而人和自主系统也存在一些重要的差异。一个非常重要的差异是人类拥有我所称之为常识知识。常识知识是我们从出生以来发展起来的知识网络。我们拥有一种机制，我们不知道它是如何工作的。但每天通过经验，你会丰富这个网络，并获得理解世界的常识知识。对于人类来说，他进行思考时，会将感官信息与这个常识概念模型连接起来。然后吧分析结果从概念模型反馈到感官信息。这与神经网络有很大的区别。让我举个例子：我向你展示一块被雪覆盖了一部分的停车标志，你毫无疑问会立刻说这是停车标志。

现在，如果你想训练一个神经网络来识别部分被雪覆盖的停车标志，这意味着由于神经网络无法将感官信息与概念模型连接起来，你将不得不训练神经网络去了解所有的天气条件下的情况。这就是为什么儿童比神经网络更容易学习的原因。如果你向儿童展示一次汽车，他下次就会说这是一辆汽车。因为他们通过观察形成了一个抽象模型，知道什么是一辆汽车。他们可以将感官信息与这个概念模型联系起来。这是当今人工智能面临的最大挑战之一。这也是自动驾驶汽车所面临的一个重要问题。自动驾驶汽车应该能够收集感官信息，并将这些信息与地图等进行连接。如果它们仅仅根据感官信息进行决策可能会造成危险。我们之前就有过这样的例子。

我们并不清楚为什人类能够在缺乏大量分析和计算的情况下理解复杂的情境。我们之所以能够做到这一点，是因为我们可以将感官信息与某些概念性信息、抽象信息相连接。因此在我们完全不会出错的地方，神经网络可能会出现很多问题。我记得有一次我开的特斯拉突然停下来，因为它认为月亮和树木的组合是是一个黄色的交通灯。这种情况绝对不会发生在人身上，因为人类可以将信息置于背景中进行理解。我立即就能理解到那是月亮，因为交通灯不可能飘在天空中。

因此当有人说这些系统在某些方面可以与人类竞争时，也许它确实能。但是人类智能的特点是你能够理解世界，可以有目的地提问。人工智能离这个目标还很远。

腾讯科技：因为你研究了自动驾驶，这已经包含了对环境、认知和感知的理解。Lecun 认为因为我们是视觉动物，我们对世界的理解很大程度上基于视觉。如果大语言模型能够做做到多模态，也能从环境中学习，他们能够理解世界本身？

约瑟夫·希发基思：我认为如果 AI 不能将具体知识与符号知识相连接，仅仅依靠大语言模型的方法是做不到对世界的理解的。只有通过将具体的知识，也就是数据库里的知识与符号知识相结合，人工智能才能实现这一点。如果它做不到，那人类智能将优于机器。对此我非常确信。我知道很多人会不同意我的观点，因为计算智能可以通过数百万个参数对数据进行分析和提取。人类在这方面做得不好。但是人类擅长处理抽象问题。

人类智能依赖于使用类比和隐喻的能力。即使我们不理解人类创造力是如何工作的，我仍然可以说这非常重要。因为在人类的创造力中应该去区分发现和发明。机器可以通过使用数据分析从更复杂、更庞大的数据中发现一些东西。但是发明是另一回事。发明是说，我发明了某个理论。我认为我们距离理解人类智能的这部分能力还隔得很远。

但发现的能力也是有用的，因为它可以帮助人类猜测更普遍的规律。这是我们自己的思维无法发现。但我不认为机器将能够创造新的科学理论或者创造新机器。它们将提供它们拥有的知识的综合。就像蒸馏过程一样，它们拥有大量的知识，然后将其提炼并呈现给你。这是了不起的。但这还不够。要实现更多的可能还是需要人类的能力。

在我写的一篇论文中，我解释了实际上存在不同类型的智能。人类智能是非常特殊的，因为人类智能的发展的基础是我们努力生活在的这个特别的世界。如果我们在另一个世界出生，也许我们会发展出另一种智能。智能是产生知识、解决问题的能力。当然，现在我们看到机器可以解决一些我们无法解决的问题，它们实际上拥有另一种智能。这很棒，我们有着某种互补性。

04

科技的发展应该优先考虑改善人类的生活

腾讯科技：我们刚才进行了一些哲学讨论，现在会讨论一些关于 AI 对社会的道德影响的问题。第一个问题是，与乐观认为新技术将创造足够的新工作岗位不同，你提到人工智能将导致严重的失业问题。而且这些问题能否在不改变社会经济体系的情况下难以得到解决。你能解释一下为什么这样说吗？因为很多人对此感到担忧。

约瑟夫·希发基思：AI 的发展会增加生产力。在经济学中有一些非常简单的规律：如果生产力增加，那么你就需要越来越少的人来做同样的工作。这一点非常明确。

现在有些人认为 AI 会创造一些就业机会，尤其对高素质的人来说会创造某些新的就业机会。但是如果你权衡 AI 创造的工作和因为它而失去的工作，你会发现 AI 的影响一定是负面的。

现在每个人都同意 AI 会导致失业问题。这是显而易见的。但在人类历史的整个过程中技术都能够提高生产力，这最终改善了人们的生活质量。几个世纪以来，人们的工作时间变少了。我们应该考虑通过适当的经济社会改革来解决这个问题。包括教育改革，因为你必须教育人们来适应这个新的时代。

腾讯科技：在工业革命中，最初人们的生活并没有得到很大改善。他们在工厂里工作，一天可能要工作 14 个小时。你觉得技术革新初期，人的生活情况会不会更糟？

约瑟夫·希发基思：不，我认为工业革命总体上还是提高了人类的生活质量。这是问题的核心。我认为今天社会的问题在于没有认真对待这个目标，他们认为技术进步是该被优先考虑的。但我认为最优先考虑的是如何改善人类的生活，这应该是首要任务。至少我是一个人道主义者。

腾讯科技：我也是一个人道主义者，明白这个问题有多严重。您觉得除了失业，AI 还可能带来严重后果吗？

约瑟夫·希发基思：是有这种可能的。但问题是，有些人说人工智能会对人类产生威胁，乃至于我们可能会成为机器的奴隶。我不喜欢这种说法。我在我的书中说了技术是中立的。你有原子能，你可以用原子能来发电，也可以用它来制造炸弹杀人。这是你的决定。如果你真的有思考过这个问题，那所有这些说人工智能对人类构成威胁的人都是完全愚蠢的。因为技术的使用是人类的责任。

我认为这些人之所以这么说，只是因为他们也想减少人类对此的责任。因为他们希望人们接受人工智能，这太糟糕了。人们应该负起责任应对可能的问题。我不知道中国的情况怎么样，但不幸的是，在西方世界人们对此并不太敏感。他们认为技术（的负面影响）是命中注定的，这非常糟糕。我在我的书中也说过，风险不在于人类被机器所统治，而在于人类接受机器做出所有关键决策。如果我有一个奴隶，它可以按我的心愿做任何事情，就像那些阿拉伯神话里描绘的那样，那最终我将成为我的奴隶的奴隶。所以危险来自于人们。我在法国的学校中也看到了这一点，如果一个孩子能接触到聊天机器人，他会变得写不出作文，组织不了自己的思维，并最终依赖于机器。这对人类来说是一个并不乐观的情景。

腾讯科技：前几天很多知名 AI 领域的人物，包括 Sam Altman 都签署了一个 AI 灭绝威胁的声明，您在书中说目前媒体和业内人士都在夸大 AI 的能力和威胁，这算不算事其中一例？您觉得当下范式的 AI 有带来人类文明危机的可能吗？

约瑟夫·希发基思：AI 带来的危险是明确的，主要可能来自于对它的滥用。但不幸的是，今天我们没有针对这种危险的相关法规。因为政府不知道这些东西是如何开发的，缺乏透明度也就意味着无法应用法规。这对社会来说太糟糕了。AI 非常可能被滥用，因此我也签署了请愿书，支持对公司做调查。

技术是非常好的，我没有反对技术。我们拥有聊天机器人是一件很棒的事情，我们应该在这种方向上取得进展。人工智能、包括通用人工智能都是好事，我不反对这些。我所反对的是对这些技术的滥用。各个国家、国际机构应该强制执行法规，虽然因为大语言模型本身缺乏解释性，这存在着一定困难。但我们依然可以要求开发公司达到某种透明度，例如讲明数据集的构建方式，以及这些引擎的训练方式。

腾讯科技：最近美国国会就人工智能和标准人举行了听证会。包括 Sam Altman，Marcus 都参加了，欧洲也正在通过相关的法案。您认为这是个好的开始吗？

约瑟夫·希发基思：但问题在于，当人们谈论安全的人工智能时，很多时候说的并不是同一件事。作为一名工程师，对我来说安全有着非常明确的定义。而其他人可能认为安全的人工智能意味着能像相信人类一样相信人工智能。这个想法的底层逻辑是把人工智能视为人类，而不是机器。还有许多其他论文认为，人工智能所做的事情并不重要，重要的是人工智能的意图，因此你得能把意图和结果分开等等。所以有很多讨论。我希望所有这些讨论能够带来一些严肃的法规，而不只是愿望清单。

腾讯科技：所以让我们聊些更光明的可能性。如果人工智能不被滥用，它在什么方面可以改变我们的生活？

约瑟夫·希发基思：如果我们不滥用人工智能，那未来相当可期。这是一场巨大的革命。它有巨大的潜力来发展知识，以应对人类今天面临的一些重大挑战，如气候变化、资源管理、人口问题、大流行病等等。

我之前说过，人与机器之间存在着明显的互补性。对于人类来说，最好的情景是机器和人类之间达成一种和谐合作。而且在这个过程中，人类将能够掌握知识的发展、应用的所有过程，确保不会让这些机器自身为我们做出关键决策。

未来的挑战是我们找到正确的平衡，找到人类和机器之间角色分配的正确平衡。我希望我们能够成功做到这一点。