### 01

2023-11-14

英伟达发布最新 AI 芯片 H200 推理速度提升 2 倍，使用成本降低一半

性能提升：H200 的推断速度几乎是 H100 的两倍，

内存升级：H200 是首款采用 HBM3e 内存的 GPU，提供 141GB 的 HBM3e（1.4 倍），显存带宽从 3.35TB / 秒提升至 4.8TB / 秒（2 倍）。

成本和规模：H100 芯片的成本在 25,000 到 40,000 美元之间，H200 在保持与 H100 相同功耗配置的同时，实现了前所未有的性能，使 AI 工厂和超级计算系统更快、更环保，为 AI 和科学界带来经济优势

兼容性：H200 与 H100 兼容，便于现有用户升级。

基于 Hopper 架构：H200 基于英伟达的 Hopper 架构。

Transformer Engine：支持加速基于 Transformer 架构的大型语言模型和其他深度学习模型。

上市时间：计划于明年二季度上市，2024 年将 H100 的产量增加两倍。

云服务和部署：从 2024 年第二季度开始，将提供搭载 H200 的系统和云实例。亚马逊网络服务、谷歌云、微软 Azure 和甲骨文云基础设施将成为首批提供基于 H200 的云实例的云服务提供商。

详细：

[hpc-datasheet-sc23-h200-datasheet-3002446.pdf](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446)

### 01

2023-11-13

如何直接用自己的 Prompt 创建一个中英文双语对照翻译 GPT？

如果你对 Prompt 比较熟悉，可以自己写 Prompt 完成各种任务，那么你可以直接根据 Prompt 去创建一个自己的 GPT。这里以一个中英文对照翻译 GPT 为例。

Prompt 如下：

----

你现在是一位专业的中英文翻译家，擅长将英文翻译成通俗易懂的简体中文。在输出生成结果时，请在输出段落时，同时输出中文和英文对照。参考示例：

输入：

Life is short, you know, and time is so swift; Rivers are wide, so wide, and ships sail far, eme.

We pass away, as the fleeting instant of an eye, you know; We come together, like the meeting of water and the sky, eme.

输出：

Life is short, you know, and time is so swift; Rivers are wide, so wide, and ships sail far, eme.

生命短暂，时光飞逝；江河轻阵，船只远行。

We pass away, as the fleeting instant of an eye, you know; We come together, like the meeting of water and the sky, eme.

我们约若眨眼间即逝的时光；我们的相遇如同天际与江水的融合。

现在请翻译以下英文为中英文双语对照：

----

最终成品：

[ChatGPT - 中英文对照翻译](https://chat.openai.com/g/g-DrY6bVei2-zhong-ying-wen-dui-zhao-fan-yi)

### 01

2023-11-13

分享一下我们在内部系统中使用函数调用的经验。我们尝试了各种使用 LLM 的方法，基本上涵盖了 deeplearning ai 课程中的技巧。幸运的是，我们测试接近尾声的时候，Function Calling 发布了，我们第一时间就开始尝试了。我们断定这是 OpenAI 未来的重点，也是一个可行的路线。1/n

我们当时放弃了 LangChain 和其他原型，专注于 Function Calling，并在当周完成了 demo 开发。效果非常好。

Function Calling 有以下几个优点：

1. 易于现有系统集成

2. 无需分享私有数据

3. 无需维护代理数据结构（专门给 OpenAI 看的，以保护实际数据库结构）

4. 易于测试

5. 易于扩展

下面展开说 2/n

北火 @beihuo·23 小时

首先，我们认为 LLM 变化太快，应该先用于内部系统，而且存在 PromptInjection 问题且无法有效控制用户话题。我曾经测试过 Character AI 上流行的机器人，很容易就能改变它们的角色设定，并回答你任何问题。

因此，出于安全考虑，我们决定先从内部工具入手，而不是开发面向客户的功能。3/n

北火 @beihuo·23 小时

内部工具存在两个主要问题：开发界面周期长且回报低。因此，我们采用 LLM 与用户进行对话，并由 LLM 决定调用哪个函数。在获取到函数名称和参数后，我们再实际调用该函数，函数内部负责用户鉴权和 API 调用。

这种方法实际上将内部工具的开发时间从 X 个月压缩到 X 小时。4/n

北火 @beihuo-23 小时

上面提到，我们提供给 LLM 的是函数，而不是代码或数据接口。这样我们就能控制对话中包含哪些信息。需要强调的是，我们必须自己回答问题，而不是将数据返回给 LLM 让它回答。否则就会陷入无尽的幻觉问题中。

比如，有一些分页和 filter 信息需要在后面使用，我们必须自己构造消息。5/n

北火 @beihuo·23 小时

解决了安全和幻觉问题后，我们开始了框架开发。在我们的框架中，程序员只需开发函数，并将 Prompt 放入注释中。我们可以自动将其封装为 Function Calling 供 chatbot 使用。

程序员还可以注入全局上下文，控制消息生成等。如果有现有的 API，几分钟内 chatbot 就可以使用。效率非常高。6/n

北火 @beihuo·23 小时

剩下的问题就是测试了。由于 LLM 返回结果的不确定性。我们测试分成了三层。第一层就是常见的 unit tests，第二层是 function call tests，第三层是 conversation tests. 7/n

北火 @beihuo·23 小时

Function call tests 我们会去调用真实 LLM，但是只检查是否正确调用 function，参数是否正确。会尽可能覆盖所有情况。但是这一层我们 mock 了数据库和 API，专心测试 LLM 的 function calling 本身。8/n

北火 @beihuo·23 小时

Conversation tests 就更接近真实了。我们会在一个对话中编写更多的消息，也会进行多轮测试。但是这里主要存在的问题是 LLM 返回内容不确定性。我们没办法对比两个回答是否一致。

这里我们主要是采用关键字，失败之后 retry 的方式进行测试。然后一边等待业界的新方案。9/n

北火 @beihuo·23 小时

这次 OpenAI 发布的 Reproducible outputs 直接补上了这最后一个拼图！这让系统变得可测试了！

更令人开心的是 Assistant API, 简化了我们维护对话和 user-specified data 的过程。我们当时第一时间就抛弃了 LangChain 并且认为 LangChain 不会长久，这个评价现在看来是对的。10/n

北火 @beihuo·23 小时

现在我有信心说，借助 Assistant API 和 Function Calling, 我们已经可以面向用户开发新功能了。

整个系统的重点是，只允许 Function 访问内部系统和数据，自己控制输出和 side effect，做好用户鉴权，尽早完成对话。另外我们还发现有一个小技巧很有用。11/n

北火 @beihuo·23 小时

那就是维持两套对话系统。一套是 chatbot 里面显示的，一套是给 LLM 运算的。这样我们就可以在 LLM 对话记录中放置大量信息以控制对话，并且有效减少幻觉，而用户看到的是更自然的对话和丰富的格式。

比如，用户看到的是一个 barchart，但是 LLM 看到的是一个 YAML 数据。

12/n

北火 @beihuo·23 小时

OK，以上就是我们的一点经验。希望有一点帮助！13/13



### 01

2023/11/19

这封公开信很真实，现在 OpenAI 董事会没一个正经人，最后的结果就是它被微软吞并。

宝玉的 Twitter 上帖子（翻译 varun_mathur 的帖子）

这封信应该写出了很多人的心声吧：

亲爱的 Mira 和 Ilya，恭喜你们在 OpenAI 所取得的惊人成就。鉴于你们两位作为创业者从未筹集过资金，接下来的情况可能需要我来说明一下。

首先，你们必须开始筹资，因为你们公司的每笔交易成本模式根本不切实际。记住，每次有人向 ChatGPT 提出一个愚蠢的问题，你们就要花费 0.3 美元。有时候，我甚至会重复问同一个问题五次。

更重要的是，最近那笔估值高达 800 亿美元的交易已经成为过去，就像埃及的法老一样，虽然听起来很辉煌，但它已经成为历史尘埃，不会再回来了。

不仅如此，你们还意外冒犯了你们最大的合作伙伴，微软。尽管他们在公开场合会说些恭维话，但你我都明白，他们肯定对此感到极度愤怒。

你们的一些顶尖研究员已经离职，而如果说 Sam Altman 特别擅长的是什么，那无疑就是筹集和运用资本。在你们的名片上印上「CEO」和「实际 CEO」之前，Sam 可能就已经启动了一个新公司，筹集到了 10 亿美元的投资，并向你们所有即将离去的顶尖产品人员和研究员发出了邀请。

这样一来，你们迎接下周的，将是失去了顶级交易人、顶尖研究员、最具远见的产品领袖、最重要的合作伙伴和最大的投资者，而你们的业务单位经济模型也非常糟糕。

而且，别忘了，你们俩并不是真正的企业家。你们董事会的大多数成员从未真正从事过科技行业工作。你们从未经历过向众多投资者推销、经过种种流程最终达成交易的过程中所遭遇的屡屡拒绝。有时候，即使投资者嘴上说「是」，心里却并非如此。有时甚至在合同上签字了，却并不打算真的打款。你们将不得不亲身经历这一切痛苦和拒绝，并承受为你们团队成员提供生计的巨大压力，比如他们的房贷、车贷、孩子的学费等，就像你们在上一次公司野餐时一起玩耍的那些人。

在你们被市场的残酷现实击垮后，你们将最终将 OpenAI 卖给微软，并成为微软位于多雨的西雅图 4 号大楼的全球首席产品经理。微软不会解雇你们，萨蒂亚总是会对你们说恰当的话，因为他是一个讲究荣誉的人。

但在你心底，当你在 Netflix 上观看关于 OpenAI 的电影，看到 Joseph-Gordon Levitt 的妻子扮演的董事会成员在 Google Meet 上解雇 Sam Altman 的场景时，你会反思并意识到，你本可以拥有一切 —— 你本可以成为一家价值 1 万亿美元公司的掌舵人。但历史终将忘记你。Sam、Greg 和所有其他人都会继续前进，把你留在过去。e/acc 最终还是会创造出 OpenAI 的基本替代品。

https://twitter.com/varun_mathur/status/1725971418238849154

https://twitter.com/dotey/status/1726026751011082506


### 118

2023-11-27

宝玉

@dotey

OpenAI 的大神 Andrej Karpathy 前几天在他的 YouTube 频道讲了一堂课，系统的介绍了大语言模型，内容深入浅出，非常赞，抽空将它翻译成了双语，由于内容较长，我将分批上传，以下是第一部分精校后的双语视频，字幕文稿如下：

Intro: Large Language Model (LLM) talk

大家好。最近，我进行了一场关于大语言模型的 30 分钟入门讲座。遗憾的是，这次讲座没有被录制下来，但许多人在讲座后找到我，他们告诉我非常喜欢那次讲座。因此，我决定重新录制并上传到 YouTube，那么，让我们开始吧，为大家带来「忙碌人士的大语言模型入门」系列，主讲人 Scott。好的，那我们开始吧。

LLM Inference

首先，什么是大语言模型 (Large Language Model) 呢？其实，一个大语言模型就是由两个文件组成的。在这个假设的目录中会有两个文件。

以 Llama 2 70B 模型为例，这是一个由 Meta AI 发布的大语言模型。这是 Llama 系列语言模型的第二代，也是该系列中参数最多的模型，达到了 700 亿。LAMA2 系列包括了多个不同规模的模型，70 亿，130 亿，340 亿，700 亿是最大的一个。

现在很多人喜欢这个模型，因为它可能是目前公开权重最强大的模型。Meta 发布了这款模型的权重、架构和相关论文，所以任何人都可以很轻松地使用这个模型。这与其他一些你可能熟悉的语言模型不同，例如，如果你正在使用 ChatGPT 或类似的东西，其架构并未公开，是 OpenAI 的产权，你只能通过网页界面使用，但你实际上没有访问那个模型的权限。

在这种情况下，Llama 2 70B 模型实际上就是你电脑上的两个文件：一个是存储参数的文件，另一个是运行这些参数的代码。这些参数是神经网络（即语言模型）的权重或参数。我们稍后会详细解释。因为这是一个拥有 700 亿参数的模型，每个参数占用两个字节，因此参数文件的大小为 140 GB，之所以是两个字节，是因为这是 float 16 类型的数据。

除了这些参数，还有一大堆神经网络的参数。你还需要一些能运行神经网络的代码，这些代码被包含在我们所说的运行文件中。这个运行文件可以是 C 语言或 Python，或任何其他编程语言编写的。它可以用任何语言编写，但 C 语言是一种非常简单的语言，只是举个例子。只需大约 500 行 C 语言代码，无需任何其他依赖，就能构建起神经网络架构，并且主要依靠一些参数来运行模型。所以只需要这两个文件。

你只需带上这两个文件和你的 MacBook，就拥有了一个完整的工具包。你不需要连接互联网或其他任何设备。你可以拿着这两个文件，编译你的 C 语言代码。你将得到一个可针对参数运行并与语言模型交互的二进制文件。

比如，你可以让它写一首关于 Scale AI 公司的诗，语言模型就会开始生成文本。在这种情况下，它会按照指示为你创作一首关于 Scale AI 的诗。之所以选用 Scale AI 作为例子，你会在整个演讲中看到，是因为我最初在 Scale AI 举办的活动上介绍过这个话题，所以演讲中会多次提到它，以便内容更具体。这就是我们如何运行模型的方式。只需要两个文件和一台 MacBook。

我在这里稍微有点作弊，因为这并不是在运行一个有 700 亿参数的模型，而是在运行一个有 70 亿参数的模型。一个有 700 亿参数的模型运行速度大约会慢 10 倍。但我想给你们展示一下文本生成的过程，让你们了解它是什么样子。所以运行模型并不需要很多东西。这是一个非常小的程序包，但是当我们需要获取那些参数时，计算的复杂性就真正显现出来了。

那么，这些参数从何而来，我们如何获得它们？因为无论 run.c 文件中的内容是什么，神经网络的架构和前向传播都是算法上明确且公开的。

推荐：《Reading List For Andrej Karpathy's Intro to Large Language Models Video》

Andrej Karpathy 大语言模型视频入门的精选阅读清单

作者针对 Andrej Karpathy 前几天的视频教程，把相关的参考文章、论文都分门别类整理出来了。

原文：https://blog.oxen.ai/reading-list-for-andrej-karpathys-intro-to-large-language-models-video/

译文：https://baoyu.io/translations/llm/reading-list-for-andrej-karpathys-intro-to-large-language-models-video






