### 01

2024-01-02

https://x.com/op7418/status/1741867652174532996?s=20

歸藏
@op7418
这个牛逼啊，专门为了开放世界游戏做的Agent。

为了弥合语言Agent与开放世界游戏之间的差距，我们引入了角色扮演语言Agent（LARP）。
其中包括一个涵盖记忆处理和决策辅助功能的认知架构、一个具有反馈驱动可学习行动空间的环境交互模块，以及促进各种个性对齐的后处理方法。
LARP框架优化了用户与预定义具有独特背景和个性特征的代理之间的互动，最终增强了在开放世界情景中进行游戏体验。

---

https://x.com/_akhaliq/status/1741661714515431833?s=20

AK
@_akhaliq
LARP: Language-Agent Role Play for Open-World Games

paper page: https://huggingface.co/papers/2312.17653

Language agents have shown impressive problem-solving skills within defined settings and brief timelines. Yet, with the ever-evolving complexities of open-world simulations, there's a pressing need for agents that can flexibly adapt to complex environments and consistently maintain a long-term memory to ensure coherent actions. To bridge the gap between language agents and open-world games, we introduce Language Agent for Role-Playing (LARP), which includes a cognitive architecture that encompasses memory processing and a decision-making assistant, an environment interaction module with a feedback-driven learnable action space, and a postprocessing method that promotes the alignment of various personalities. The LARP framework refines interactions between users and agents, predefined with unique backgrounds and personalities, ultimately enhancing the gaming experience in open-world contexts. Furthermore, it highlights the diverse uses of language models in a range of areas such as entertainment, education, and various simulation scenarios.

### 02

2024-01-02

https://x.com/op7418/status/1741862630120419791?s=20

歸藏
@op7418
一个让DALL-E3创建一致性角色的指南。

不得不说DALL-E3创建类似皮克斯风格的角色做的还挺好的，适合搭配Pika用。

https://x.com/AI_Vision_Verse/status/1741813638581895677?s=20


AI Verse
@AI_Vision_Verse
CONSISTENT CHARACTERS: ULTIMATE GUIDES COLLECTION

Learn to Create Consistent Characters: 

- Doing different activities, 
- Having different expressions, 
- Wearing different clothes,
- Different age group character consistency

All the guides are prepared by me

Come on, let's dive in.

### 03

2024-01-02

https://x.com/Barret_China/status/1741856177586360721?s=20

Barret李靖
@Barret_China
花了六个小时翻完了前五章，作者一气呵成，把复杂的问题用简单轻便的逻辑呈现出来，抽丝剥茧，深入浅出，代码实现也很基础，是一本入门好书，值得推荐。

1『斋藤康毅.(2018).2024001深度学习入门.(陆宇杰译).人民邮电出版社。（2024-01-21）』

### 04

2024-01-02

https://x.com/dotey/status/1741664756698579338?s=20

宝玉
@dotey
来自华为的盘古大模型

通过非线性补偿来强化语言模型的架构。

论文摘要：

近期的大语言模型（LLM）的发展趋势，是通过增大模型（参数数量）和数据集规模，以达到更好的生成能力，这一点已经被像GPT和Llama这样的研究所证实。不过，大型模型通常意味着高昂的计算成本，实际应用中无法承受这种成本。而关于如何构建强大的LLM的方法却很少涉及。

我们首先分析了最前沿的语言模型架构，并观察到了特征崩溃的问题。基于我们的理论分析，我们提出语言模型的非线性作用也同样至关重要，这在视觉任务的卷积神经网络研究中常有所涉及。然后我们引入了一个几乎可以忽略不计的激活函数，并进一步利用增强的快捷方式强化模型的非线性。

通过精心设计的消融实验，我们证明了该方法在提高模型非线性上的效果显著。因此，我们提出了一个新的高效模型架构，命名为PanGu-pi。接着，我们利用相同的数据集和训练策略，将PanGu-pi与当前最先进的LLMs进行比较。

结果显示，PanGu-pi-7B在只增加约10%的推理速度的情况下，就能达到可比拟的性能，而PanGu-pi-1B在精度和效率方面都达到了行业领先水平。

此外，我们还在金融和法律等高值领域部署了PanGu-pi-7B，开发出名为云山的LLM用于业务实践。结果显示，云山在相同规模的模型中，性能超越了其他模型。

论文地址：https://arxiv.org/abs/2312.17276

### 05

2024-01-02

https://x.com/dotey/status/1741661915321954364?s=20

宝玉
@dotey
来自美国最高法院首席大法官罗伯茨：人工智能将重塑法院运作方式

美国首席大法官约翰·罗伯茨指出，人工智能将改变美国法院的运作模式，但人类法官至少在短期内还会存在。

罗伯茨在其年末报告中表示，AI 工具不仅将改变法官的工作方式，还将影响他们理解 AI 在审理案件中的作用。这番话是对今年横扫全国和金融市场的 AI 热潮的回应，AI 的兴起已经开始改变律师和法官的工作方式。

罗伯茨并未对人类工作被自动化发出灾难性预警。

他写道：“机器不能完全取代法庭中的关键角色。细节至关重要：手的颤抖、声音的颤动、语调的变化、汗珠、犹豫的瞬间、眼神交流的短暂断裂等都可能是决定性因素。而且，大多数人仍更信任人类而非机器去感知这些细微差别并做出正确的推断。”

近期，有多起 AI 生成的法律文件引用虚假案例和错误陈述事实的案例。生成型 AI (Generative AI) 基于提示创造文本和图像，但其人类化的响应中常含有错误。根据上周公开的法庭文件，特朗普的前律师迈克尔·科恩上个月在一份文件中不慎引用了 AI 生成的虚假案例。

罗伯茨预测：“人类法官还会存在一段时间。” 他自 2005 年起担任 Supreme Court 成员。

他还提到，未来法律研究可能离不开 AI。他写道：“AI 显然有助于显著提高律师和普通民众获取关键信息的机会。但它也可能侵犯隐私并使法律流于形式。”

法院历来难以适应新技术。罗伯茨本人就以手写裁决书而非使用电脑著称。

https://bloomberg.com/news/articles/2023-12-31/ai-and-the-supreme-court-justice-roberts-says-it-will-transform-judges-work

### 06

2024-01-02

https://x.com/dotey/status/1741614108481314954?s=20

宝玉
@dotey
很好的 2023 年 AI 盘点：

2023 年 AI 盘点

2023 年注定会成为一个里程碑，它标志着人工智能创新的一个转折点……同时，这一年对于开源 AI 来说也意义非凡。让我们快速回顾一下：

1月
- ChatGPT 成为史上增长速度最快的应用程序

2月
- Meta 推出了 Llama-1，这是一个研究型许可证项目
- 第一个实用的大型开源模型横空出世，引发了一轮研究和创新的热潮
- Runway 推出了基于 Stable Diffusion 的首个 AI 视频合成模型 Gen 1

3月
- GPT-4 隆重登场，迄今无人能够超越这款大型语言模型（Large Language Model）！
- Google 推出了 Bard
- 在 LLM 系统的聊天机器人领域，开始了多个大型语言模型的比较。

4月
- Drake 和 The Weeknd 的《Heart on My Sleeve》（由 Ghostwriter 制作的 AI 翻唱）- 这首 AI 制作的歌曲获得了超过 2000 万的观看量

5月
- 发布了 DPO 论文，介绍了一种比 RLHF 更简单的大型语言模型微调新技术
- 发布了 QLoRA 论文，讨论了大型语言模型量化微调的高效方法

7月
- 第一个实用的开源模型 Llama-2 发布，带有商业许可！随之诞生了多家开源公司！

8月
- 出现了基于 Llama-2 的多个开源微调版本
- Llama-2 70B 开始在真实世界的生产环境中应用

9月
- 将 DALL-3 整合到 ChatGPT 中。大型语言模型和视觉模型开始实现互通！

10月
- Abacus AI 等多家公司开始提供基于开源模型的微调、推理和检索 API
- 发布了关于大型语言模型作为世界模型的文章，标题为“语言模型表征空间和时间”
- 遗憾地发布了关于 AI 的行政命令

11月
- xAI 的 Grok 成为首个无审查的大型语言模型！
- OpenAI 发布了 GPT-4v 和 turbo 版本，并下调了 GPT-4 的价格
- Stable Diffusion Video 问世
- 发布了 Orca 论文，探讨了如何教会小型模型进行推理。
- Meta 的 Emu 是一款文本到视频的模型，能够根据文本提示生成完整视频。

12月
- Mistral MoE 开源发布！多个 GPT-3.5 级别的模型实现开源。
- Midjourney v 6.0 能够处理图片中的文本，创造出令人惊叹的逼真图像
- Google 宣布了 Gemini Ultra 及其与 GPT-4 相当的性能基准

2023 年只是个开始，2024 年将迎来更大的飞跃！我们告别 2023 年，感谢开源社区的迅速发展、创新精神和热情！

热切期待 2024 年，继续携手共创未来！

### 07

2024-01-02

https://x.com/dotey/status/1741609345681363213?s=20

宝玉
@dotey
推荐阅读：《发挥 AI 在职场中的作用：如何在新的 2024 年保持领先！》

作者认为：展望 2024 年及未来的 AI 发展， 如果你不将 AI 融入你的工作流程、项目和想法实施中，那么竞争对手必将领先一步！

并且作者给出了 16 条建议：

1) 保持对变化的开放态度

2) 培养成长型心态

3) 寻找学习机会
4) 识别可自动化的任务

5) 探索你所在领域的 AI 应用

6) 了解适用于你行业的 AI 工具

7) 小步快跑，逐渐融入 AI

8) 尝试使用 AI 工具 

9) 利用 AI 提升技能 

10) 运用 AI 辅助决策

11) 设定学习目标并定期检视

12) 适应使用 AI 工具的变化

13) 聪明并且道德地实施 AI 

14) 在 AI 领域合作并保持更新 

15) 尽早为未来的 AI 时代做好规划 

16) 为 AI 设定正确的期望 

如果把这些建议总结成一条，那就是：

研究你的行业，找出重复任务，寻找 AI 工具，从小事做起，适应 AI 并与之协作！

原文：https://linuxblog.io/ai-in-the-workplace-16-simple-ways-to-stay-ahead/
译文：https://baoyu.io/translations/ai/ai-in-the-workplace-16-simple-ways-to-stay-ahead

### 08

2024-01-02

https://x.com/dotey/status/1741554337988346153?s=20

宝玉
@dotey
Fourier 智能推出 GR-1 人形机器人生产版

Fourier 智能自 2017 年起专注于外骨骼和康复设备的制造。这家坐落于新加坡的企业今年推出了其首款人形机器人，命名为 GR-1。

GR-1 人形机器人拥有分布于全身的 40 个运动自由度 (degrees of freedom)，身高为 1.65 米（5 英尺 5 英寸），重 55 公斤（121.2 磅）。其髋部关节模块能产生高达 300 Nm 的峰值扭矩，使得该机器人能以 5 kph（3.1 mph）的速度行走，同时携带重达 50 公斤（110.2 磅）的物品。

Fourier 从外骨骼开发向人形机器人设计转型，这一转变顺应了逻辑发展。人形机器人平台融合了Fourier 为其核心产品系列所开发的多项机械和电子设计元素。公司的核心技术在于驱动系统 (actuation) 的设计和制造，Fourier 称这有助于优化系统的成本与性能比。

伴随 GR-1 的发布，Fourier 还推出了 Fourier Smart Actuator (FSA)，一款集高性能和成本效益于一身的全功能驱动器系列。公司表示，这种驱动器的设计符合其设计目标和 GR-1 的市场定价策略。

Fourier 联手 National Instruments (NI) 和墨尔本大学，共同开发了外骨骼机器人开放平台 (EXOPS)。EXOPS 旨在成为学校、研究机构和临床中心的理想开发平台。

此外，Fourier 正在探索将此技术应用于灾难救援、老年护理和家庭服务等领域。GR-1 配备了类似于支持 ChatGPT 的大型多模态语言模型 (Large Multimodal Language Model)。公司称，这使得机器人能够自主编程，完成各种任务。

GR-1 特别配备了一个“集成的情感 AI 模块 (integrated emotion AI module)”，高清椭圆形显示屏，以及用于语音识别的圆形麦克风阵列。Fourier 在接受机器人报告采访时表示，这些功能让人机交互更加自然和顺畅。

Fourier 将启动量产

Fourier 公司近期宣布，他们即将开始生产 GR-1 机器人的量产版。这款机器人的头部和躯干部分均装备了深度摄像头。

此外，GR-1 还配备了视觉算法，能够感知周围环境。这使得机器人能迅速识别出各种物体和人，实现障碍物避让、视觉任务指导等多项功能。

GR-1 的机械手拥有 11 度自由度，能够稳固而灵活地抓取物品，Fourier 表示。用户可以指令机器人取回特定物品。

在抓取圆柱状或圆形物体，如瓶装水和电动螺丝刀时，其手部能够模仿人类手指的运动范围。GR-1 能利用视觉反馈，自动计算出精准的抓取动作路径。

Fourier 还强调，他们设计的机器人能够通过调整握力，稳妥地抓取各种大小和形状的物体，即使是较轻的物品也能轻柔处理。

该机器人还可以与车队管理软件配合使用，使用户能够从中心位置协调多个 GR-1 单元的操作。这包括任务分配、任务执行和车队运营的跟踪。

GR-1 还支持通过增强现实头盔和 5G 技术进行远程操作。

GR-1 开启市场新机遇
Fourier 计划将 GR-1 用于多个领域，包括研究教育、礼宾服务、娱乐展览、工业生产与物流、医疗康复、安全检测、家庭服务及陪伴等。该公司已经针对康复产品建立了成熟的销售团队和合作伙伴网络。

目前，该生产型号的定价尚未公布。

过去一年，人形机器人领域获得了广泛关注。开发公司如 Apptronik、Boston Dynamics、Figure AI、Sanctuary AI、Tesla 以及 Unitree 在灵巧操作和双足行走方面取得了显著进步。这些机器人大多数最初设计用于物流和制造业领域。同时，Agility Robotics 正在与 Amazon 和 GXO Logistics 合作，进行商业试验，探索人形机器人的进一步应用。

根据市场研究机构 MarketDigits 的预测，全球仿人机器人市场预计将在 2023 年至 2030 年间以每年平均 46.5% 的速度增长，市场规模从 17 亿美元增至 361 亿美元。报告特别提到了医疗保健和灾难搜救领域对仿人机器人的需求。

而市场研究公司 Technavio 的估计更为乐观，预计从 2022 年到 2027 年，市场增长将达到 160.5 亿美元，年增长率高达 53.45%。Technavio 强调，仿人机器人在工业生产中的多功能性将成为推动增长的关键因素，并预测北美将占据全球市场增长的 35%。

来源：https://therobotreport.com/fourier-intelligence-launches-production-version-of-gr-1-humanoid-robot/

### 09

2024-01-02

https://x.com/op7418/status/1741428436013560159?s=20

歸藏
@op7418
Nick昨天写了一个MidjourneyV6的提示词书写模板，详细描述了V6提示词的模块和每个模块的注意细节。

我学习完顺便翻译并用自己的理解润色了。感兴趣可以收藏一下。
其实跟之前的结构区别不大，但是由于V6对提示词理解能力的提升，所以增加了一些细节。👇下面开始：

--v 6更好地理解语言，这意味着你的标点符号、句法和语法更加重要，如果使用正确提示，可以控制图像中的几乎每个元素。

提示词模块：
> 设定主要场景
> 描述细节
> 描述背景设定
> 探索风格和媒介

1️⃣在初始设置中添加一个基本的风格词可能有助于在迭代过程中更好地可视化结果。最好将其放在提示的开头或结尾以获得最佳效果。


2️⃣设定主要场景：
从你的基本想法开始。在可能的情况下，使用你的主题的通用代表 + 需要的一些场景细节。
避免在这里过于具体（思考高层次）。你只是为v6在下一步发挥魔力做好舞台铺垫。

⚡ 描述具有多个主题的场景
在详细描述主题时，指定位置（左、右、中）并参考初始设置中使用的相同术语会很有帮助。
句法在这里很重要。良好的句子结构将导致更好的连贯性。


3️⃣ 描述位置：
你也可以具体描述，但要知道，对多个主题的许多具体细节进行提示可能会被非常详细的环境描述所混淆。
玩弄它并测试极限。如果情况变得疯狂，请删除一些具体信息。


4️⃣ 探索风格词：
如果在初始设置时尚未添加风格词（建议添加），现在是时候了。
然后，开始尝试不同的风格。请尽量具体，并避免使用8k、HDR等术语。这是设定氛围的机会
。
引用：

Nick St. Pierre
@nickfloats

https://x.com/nickfloats/status/1741166489364025536?s=20

### 10

2024-01-02

https://x.com/fi56622380/status/1741937158720716888?s=20

fin
@fi56622380
这位仁兄跟我的观点有点像，不过我认为容错率的重要性远大于他提出的效果，人力成本

人力成本和AI取代快慢并无直接关系，低成本任务也有很多顺带被解决的，这取决于AI的发展节奏。长尾的碎片化的任务会放缓AI普及（就像Andrew Ng谈到肥尾是现在AI普及的难点）

### 11

2024-01-02

https://x.com/dotey/status/1741939360700744182?s=20

宝玉
@dotey
多功能即时语音克隆技术——OpenVoice

OpenVoice 是一种实用性极强的即时仿声技术，只需要使用来自目标发言人的短音频，就可以模仿他们的声音，并以此生成各种语言的语音。

OpenVoice 不仅能够模仿参考发言人的音质，还可以精细控制包括情感、口音、语调、停顿和节奏在内的各种语音风格。除此之外，对于那些未在大规模发言人训练集中包含的语言，OpenVoice也可以实现“零样本”（Zero-shot）的跨语言模仿。

论文：https://arxiv.org/pdf/2312.01479.pdf 
项目：https://github.com/myshell-ai/OpenVoice

### 12

2024-01-02

https://x.com/fi56622380/status/1741917251480273201?s=20


fin
@fi56622380
创新的本质是组合，想清楚这一点，是我2023年最大的收获之一

往大里一点说，人类文明发展本质上取决于复杂组合能力：通过归纳组合不停的拓展知识边界

前一阵openAI宫斗剧才爆出来的Q*算法，本质上是强化学习里Q learning和A star算法，和LLM组合起来。而A* 算法又相当于是把搜索和目标函数组合起来

最近的LLM发展，其实很多效果不错的方法，都是AI/ML领域里的老方法新用在了LLM上，比如Tree o Thought，LLM blender这类ensemble method都是ML里古老的思想了

麦克斯韦方程（描述电磁场以及如何随时间变化/相互作用），精妙之处在于把几种数学现象和几种电磁现象组合起来，把这种微妙的看似不同现象的弱联系组合起来，就是最伟大的天才，让人的感官有无比愉悦的感受：原来电磁的关系是如此的简洁优雅

乔布斯著名的“connecting the dots”，本质上也就是在自己的人生经历dots里寻找更合理和更有价值的组合，动画/图形学+人机交互+电脑组合起来，成就了一个新王国

艺术里的很多让人眼前一亮的创新（或者爆款），实际上也是某些场景和某些表现方式（叙事模式）或是某种新技术的组合，效果会非常好

有了这个指导思想，其实能推广到很多领域

应用领域的科研，如果说我们把某一个领域里最近几年100个最有启发性的idea，作为X维度和Y维度两两组合（当然不靠谱的是绝大多数），然后去归纳总结组合起来最有价值的idea组合，也许有1%的组合会很有价值，但更重要的是如何去识别把两种组合重新归纳成一种新的理论框架的可能性，或者用一个idea去解决另外一种方法里的特殊限制，获得更泛化能力的方法

芯片设计领域，可以借鉴一些AI/ML/操作系统/networks/SW architecture的idea和思维方式，抽出100个启发性的idea，和芯片领域里各个层面(arch/DV/perf)去寻找组合，有太多太多可以革新的地方

往更本质里说，寻找组合方式是一种更本质的能力，这种能力需要的检索弱关联的能力，而检索弱关联的能力需要更高的智能，或者说意味着更高的智能，因为这需要的是一些比常见尺度更大尺度（时间，空间，数量）上的特征的弱matching，或者更小尺度特征（一些微妙而反常的现象）的弱matching，而这些能力需要match特征之后还能在及其有限的试错机会里去找到检索结果里最有可能成功的组合

这也是为什么，很多伟大的科学发现都是靠直觉性的灵光一现，那就是大脑里检索到了一些不易发现的弱关联之处

人类所谓的举一反三能力，实质上是一种根据归纳来的弱特征检索类似特征的能力，这种检索能力如果放到计算机里，体现出来就是搜索能力

那么甚至可以更进一步，在AI领域，把LLM和搜索组合起来，用搜索技术去解决寻找组合（Alpha Go就是搜索剪枝，Google擅长），形成一种泛化的寻找组合和评价组合的能力（就像引文里提到的那样），AI的能力就又能上一个新台阶：“创新能力”

所谓的成熟技术，就是已经知道的或者常用(强关联)的组合方式，或者说基于当前环境变量和知识图谱，降落在可能性最大的地方。而创新能力，就是不常用（所谓弱关联）或者概率较低的组合方式，或者说是排列组合后以前认为可能性较小的地方

就像刚才说到的科研一样，AI能力也许有更进一步的可能

在找到高效的组合之后，把两种组合重新归纳成一种新的理论框架，那就是AI更进一步的高阶能力了，等到了这一步，AI能力在人类ranking又会大幅提高了（我一直认为评价AI的能力，应该按人类中的ranking来算）

---

https://x.com/fi56622380/status/1624892656018165760?s=20

fin
@fi56622380
我一直有个暴论，所谓的创造力和想象力，是愚蠢的人类发明出来自我安慰的概念

想象力的本质，就是面对复杂度高一点的没见过的组合，觉得很新颖，自己以前没想象过，于是硬造出来一个词语形容这种能力。创造力同理，把大家觉得无关联/弱关联的事物组合出新的系统/方向，获得了超出预期的效果

生物角度来说，人脑是无法想象自己没见过的东西的，人类的本质就是一个复读机，无法真正“凭空想象”出东西，不可能突破知识边界的封锁

所以想象力本质上是一种“组合各种可能性”的能力: "connecting the dots"

所以机器的创造力/想象力的来源，可能会跟人类理解的创造力完全不一样：就是看机器组合各种信息的能力(包括评估结果)什么时候复杂度能超过人类，对各个组合好坏的评估能力是不是和人类一致方便人类理解

诗云里的上帝最大的问题，就是缺乏一个对各种组合结果的评估能力，而这个能力是可以通过训练完成的，Meta的Toolformer就是最近的一个例子。

### 13

2024-01-02

https://x.com/dotey/status/1741895131165241445?s=20

宝玉
@dotey

27 年前，史蒂夫·乔布斯曾经说过：最优秀的员工专注于内容而非流程。研究证实了他的观点。

乔布斯还说过：最优秀的员工通常也是最难管理的。

### 14

2024-01-02

https://x.com/dotey/status/1742056148109262879?s=20

宝玉
@dotey
Jim Fan 预言：距离机器人 AI 的 ChatGPT 时刻大约还有 3 年时间

以下为其推文转译：

除了大语言模型（LLM）之外，2024年最重大的领域无疑是机器人学。我们距离实体 AI 智能体实现 ChatGPT 式的突破仅有大约三年的时间。长期以来，我们一直受到莫拉维克悖论（Moravec's paradox）的困扰，这一直觉反常的现象表明：“人类觉得简单的任务，对 AI 来说却异常困难，反之亦然”。

2024年将成为 AI 领域首次大规模反抗这种困境的一年。虽然我们不会立刻取得胜利，但我们已经在通往成功的道路上迈出了坚实的步伐。

回顾2023年，我们已经初步见识到了未来机器人的基础模型和平台：
- 多模态大型语言模型与机器人手臂作为物理输入输出接口：VIMA、PerAct、RvT（NVIDIA）、RT-1、RT-2、PaLM-E（Google）、RoboCat（DeepMind）、Octo（伯克利、斯坦福、卡内基梅隆大学）等。
- 连接高级推理（大型语言模型）与低级控制的算法：Eureka（NVIDIA）、Code as Policies（Google）等。
- 在坚固硬件方面取得巨大进步：Tesla Optimus 
@elonmusk
、Figure 
@adcock_brett
、1X 
@ericjang11
、Apptronik、Sanctuary、Agility+Amazon、Unitree 等。
- 数据长期以来一直是机器人学发展的弱点。研究社区正致力于创造下一个“影像网”（ImageNet），如 Open X-Embodiment (RT-X) 数据集。尽管这些数据集的多样性尚未达到理想状态，但即使是微小的进步也意味着重大的飞跃。
- 在解决机器人灵活性甚至整个计算机视觉领域中，仿真和合成数据将扮演关键角色。
  (1) NVIDIA Isaac 能以比现实时间快1000倍的速度进行仿真，其产生的数据量会随着计算能力的提升而增长。
  (2) 通过硬件加速的光线追踪技术实现逼真效果，这种逼真的渲染还自带地面真值标注，比如分割、深度、3D 姿态等。
  (3) 仿真器甚至能够扩展现实世界的数据，形成更大的数据集，从而大大减少昂贵的人类示范工作的需要。NVIDIA 的 MimicGen 就是一个很好的例子。

我个人全力投入这一领域。最精彩的部分还在后面。

### 15

2024-01-02

https://x.com/Barret_China/status/1742031172920856824?s=20

Barret李靖
@Barret_China

李笑来写的《人人都能用英语》，https://github.com/xiaolai/everyone-can-use-english，Everyone can use English，总共八个章节，花几个小时就可以阅读完，“想明白之后，就该干嘛干嘛了”，这本书会带着读者一起把语言学习这件事情看清楚、想明白，让读者找到该有的自信，掌握一些有效的学习技巧，很受启发。

第一章花了很长的篇幅，论证了“大脑是可塑的”，以及“语言能力是后天习得的”，作者试图让读者先选择相信自己可以去完成某项似乎不适合在当前年龄段完成的任务，这种“自我催眠”很有用，不仅仅适用于语言学习，跨领域的知识学习也很参考价值。

### 16

2024-01-02

https://x.com/dotey/status/1742018174151766311?s=20

宝玉
@dotey
华为诺亚的盘古Agent，让智能体学会结构化推理

论文链接：https://arxiv.org/abs/2312.14878

https://mp.weixin.qq.com/s/pTxI5p1mFWX_LLEx6dXVew


### 17

2024-01-02

https://x.com/xiaohuggg/status/1742392202482061509?s=20

小互
@xiaohuggg
兄弟们，这个模型很强大！

M2UGen：多模态音乐理解和生成模型

该模型由腾讯与新加坡国立大学开发，M2UGen能够理解各种音乐，包括风格、演奏乐器、表达的情绪情感等，并进行音乐问答。

而且还能根据文本、图像、视频和音频生成各种音乐，同时对生成的音乐也能理解并根据文字描述对音乐进行编辑。

M2UGen 的主要功能：

- 音乐问答：M2UGen 能够理解不同类型的音乐，包括它们的风格、使用的乐器、表达的情绪和情感等。然后根据提出的问题，模型能够理解并回答与音乐相关的查询。

- 文本到音乐生成：用户可以输入文本，模型会根据这些文本生成相应的音乐。

- 图像到音乐生成：模型能够根据提供的图片内容生成匹配的音乐。

-视频到音乐生成：根据视频内容，模型能理解视频的主要内容，并生成相应的音乐。

- 音乐编辑：用户可以对已生成的音乐进行编辑，例如改变乐器、调整节奏等，而且只需要通过文本描述即可。

M2UGen 使用了多种编码器，包括用于音乐理解的 MERT、用于图像理解的 ViT 和用于视频理解的 ViViT，以及作为音乐生成模型（音乐解码器）的 MusicGen/AudioLDM2 模型。

此外，该模型还结合了适配器和 LLaMA 2 模型。

工作原理：

1、多模态输入处理：M2UGen能够处理多种类型的输入，包括文本、图像、视频和音频。

它使用特定的编码器来理解不同的输入模态。例如，使用MERT模型处理音乐输入，ViT模型处理图像输入，ViViT模型处理视频输入。

2、音乐理解：利用LLaMA 2模型，M2UGen能够理解音乐的各个方面，如风格、乐器使用和情感表达。它能够对音乐相关的问题进行回答，这涉及到对音乐内容的深入理解。

3、音乐生成：M2UGen不仅能理解音乐，还能根据不同的输入生成音乐。它探索使用AudioLDM 2和MusicGen等模型来根据文本、图像或视频输入生成音乐。

4、数据集生成与训练：为了训练M2UGen，开发者使用了MU-LLaMA和MPT-7B模型来生成大量的多模态音乐配对数据集。这些数据集帮助M2UGen学习如何从不同的输入中提取信息并生成相应的音乐。

项目及演示：https://crypto-code.github.io/M2UGen-Demo/
论文：https://arxiv.org/abs/2311.11255
GitHub：https://github.com/shansongliu/M2UGen

### 18

2024-01-03

https://x.com/dotey/status/1742393800830407090?s=20

宝玉
@dotey
来自 JPMorgan 的 DocLLM：一种面向布局的生成式语言模型，能理解多模态文档

对于企业文档来说，不仅仅是文本类型，还有很多复杂的类型，例如表格、发票、收据、报告、合同等，其中都包含着丰富的文字和空间交互信息。这些文档复杂的布局提供了视觉线索，对于有效理解这些文档至关重要。

本论文以此建议了一种轻量级扩展的大语言模型（LLMs） - DocLLM，这款模型可在处理可视文档时，同时考虑到文本语义和空间布局。该模型与现有的多模态语言模型（LLMs）的最大不同在于，它没有使用计算成本高昂的图像编码器，而是通过边框信息来整合空间布局。

具体来说，DocLLM 通过将文本和空间模态之间的交叉对齐分解为一组独立矩阵来处理既定的 Transformer 的注意力机制。

此外，DocLLM 还设计了一个预训练目标，学习如何自动填充文本段落。这种方式使其能更好地处理常见的视觉文档中的不规则布局和混合内容。

DocLLM 使用大型指令数据集对预训练模型进行了微调，覆盖了四个主要的文档智能任务。

DocLLM 的解决方案在所有任务的16个数据集中的14个上优于现有的最先进语言模型，且在之前未曾接触过的5个数据集中的4个上有良好的应用表现。

论文地址：https://arxiv.org/abs/2401.00908

### 19

2024-01-03

https://x.com/beihuo/status/1742350183709671744?s=20


北火
@beihuo
我想想找一个管理工具，给自己用，但是选来选去总是感觉不顺手。今天我在对比工具的时候，忽然意识到并不是工具不顺手，而是我不知道该如何管理项目开发，我不知道该如何正确使用这些工具。

于是我去学习了 Github 团队是如何管理产品开发的，并做了一些笔记：

1/19

Github 内部，对于产品开发的管理，分为以下几个阶段：

1.  Opportunities Backlog 机会待办
2.  Project Overview - 项目概览
3.  Feature Overview - 功能概览
4.  Other Backlogs - 其他待办（文档，市场，等）

2/19

他们的工作流是这样的：

1. 所有的想法、计划和提案，都先放在 Opportunities Backlog 里面。所以这里包含所有的要做和要调研的事情。

2. 当决定开始做一个项目的时候，在 Project Overview 里面新建一个事项。这个 project 是团队了解目前进行中的工作的地方，管理者是重度用户。

3/19

3. 因为 Project Overview 包含了太多的东西，牵扯到太多的团队。所以每一个 feature 都会有一个 Feature Overview Project。

4. 一些由辅助团队做的事项，比如市场营销，可以单独放在其他的 project 中。所以这些事项，会同时存在于这些单独的 project 和 feature overview project 里面。

4/19

可以看到，每一级的 project 都和上一级有所重复。这方便特定团队的工作的同时，也可以让更大范围的团队知晓每一项的进度。

再仔细检视，对于每一个 project，最开始的问题只有两个：

1. 使用者是谁？
2. 需要回答使用者的什么问题？

其他的问题，都是次要的问题。

5/19

Opportunities Backlog 的使用者是所有关心整个产品进展的人。回答的问题是：

- 哪个团队适合负责某个事项？
- 某个想法能实现什么样的目标？
- 某个事项的当前进展如何？

在 Github，这个阶段的 project 名字叫 Planning & Tracking Pitches。

6/19

当一个 Pitch 被接受之后，流程进入 Program Overview。这个 project 的使用者是 manager 们。所以使用更关心的是：

- 进度如何？
- 有没有风险？
- 如果我有问题，该找谁问？
- 下一步的计划是什么？

在 Github，这个阶段的 project 名字叫 Planning & Tracking Roadmap。

7/19

从 Github 分享的截图中可以看到，他们使用不同的栏（Field）来回答使用者的问题。

比如 Trending 这一栏，让使用者知道进度和是否有延迟交付的风险，而 Target  Changelog 可以让使用者知道这个事项最终会在哪一版发布。

他们也会用不同的视图，比如 Next/Later 就可以看到下一步的计划。

8/19

现在规划阶段已经完成了，接下来，就要转到 Feature Overview 阶段。每一个规划的事项，都会有一个*单独*的 project。

这一点，我们可以从截图中看到，每一个事项的描述里面，都放了一个 project 的链接。他们使用 template 功能，来确保这一点。

9/19

OK，说到 Feature Overview 啦！这一阶段，使用者主要关心产品的交付：

- 为了完成某个功能，一共需要做哪些工作？
- 我当前被分配了什么工作？
- 这些工作的先后顺序是什么？
- 我可以开始做哪些工作？
- 团队成员之间的工作分配是否合理？

10/19

创建这个阶段的 project，可以先把所有的事项都列出来。然后创建新的栏（Field），回答刚才列出来的使用者会关心的问题。

 比如想知道需要做哪些类的工作，可以做一些分组，比如设计、市场、测试等。然后是时间，我们可以在 Github Projects 有一个非常有用的 Iteration 栏，来规划开发周期。

11/19

我特别喜欢这个 Planning View，按照 Iteration 来进行分栏。当你决定要做某个事项的时候，就拖动到相应的 Iteration 里面。然后可以点击按钮将其转换成一个 Issue。在这个事项里面，还可以用 Task 来进一步细分工作，每一个 Task 也可以是一个 Issue。规划完毕之后，可以将其放到总 View 中。

12/19

然后将 Overview 按照 Area 分组，一个最基本的项目管理，就完成了。

哦，对了，每一个 Tasklist 都可以包含一个 Tasklist，所以我们继续细分工作成具体的 Issue。

13/19

Github Projects 在移动端可用，这是对我来说很有用的。Linear 只支持桌面端。

14/19

最后，总结如何做项目管理：

1. 确定哪些群体是使用者
2. 确定使用者关心的问题
3. 在不确定的情况下，先尝试，再逐步改进

15/19

看完 Github 团队的整个视频，对我这种产品管理小白，帮助非常大。作为小团队或者一人团队，我认为可以从只设置一个阶段开始，没有必要分成多个 Github Projects。

上述策略，也可以应用到其他管理工具中，比如 Trello。但是我决定先尝试 Github Projects。等我用一段时间，会分享使用体验。

16/19

来自 Github 的视频，From disarray to delight: planning with GitHub Projects - Universe 2022

https://youtube.com/watch?v=vHUEOYbH8Mo

17/19

您可以点击这里到 Typefully 上查看全文：

https://typefully.com/beihuo/0cgKU57

18/19

如果这个学习笔记对你有帮助的话，欢迎关注/点赞/转发一下。

19/19

### 20

2024-01-03

https://x.com/dotey/status/1742324698325627292?s=20

宝玉
@dotey
推荐一套The Full Stack的免费 LLM 在线教程：LLM Bootcamp - Spring 2023

包含了提示工程、LLM运维、LLM App开发、LLM基础等内容。

第一次访问需要输入邮箱。

https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/

### 21

2024-01-03

https://x.com/dotey/status/1742320658321739850?s=20

宝玉
@dotey
我喜欢这篇论文的标题中用的比喻：如果 LLM 是巫师，那么代码就是魔杖

https://browse.arxiv.org/html/2401.00812v1


### 22

2024-01-03

https://x.com/dotey/status/1742321608037986530?s=20

宝玉
@dotey
论文摘要：

当今的主流大语言模型（LLMs）与过去的语言模型有所不同，它们不仅规模更大，而且依托自然语言和代码（形式语言）综合训练。

代码作为连通人类与计算机的桥梁，将高层次的目标转化为可执行的步骤，具备标准语法、逻辑一致性、抽象性和模块化等特性。

在本文中，我们探讨了将代码整合进大语言模型训练数据中的众多益处，具体来看，代码的独特属性不仅能够提升大语言模型的代码生成能力，同时还可以：

(i) 解锁大语言模型的推理能力，使其能够应对一系列更为复杂的自然语言任务；

(ii) 引导大语言模型生成结构化和精准的中间步骤，然后通过函数调用将这些步骤连接到外部执行环节；

(iii) 利用代码的编译和执行环境，获取多样的反馈以改进模型。

此外，我们还追溯了代码对大语言模型深远影响的一种表现：促使其在需要理解指令、分解目标、规划和执行行动以及依据反馈进行优化的情境中，成为有效的智能代理（IAs）。

文章最后，我们提出了几个以代码赋能大语言模型的未来方向及其所面临的关键挑战。

### 23

2024-01-03

https://x.com/dotey/status/1742294754367328495?s=20

宝玉
@dotey
推荐阅读拾象科技CEO李广密的采访：《跨年对谈：千亿美金豪赌开启 AI 新摩尔时代》

https://mp.weixin.qq.com/s/lK1HZZE-szWucRA1l986sw

### 24

2024-01-03

https://x.com/xiaohuggg/status/1742168236316303462?s=20

小互
@xiaohuggg
Activepieces：一个开源的全能自动化工具，是Zapier的替代方案

- 用户友好的工作流构建器：提供一个直观的界面，使用户能够轻松创建和管理自动化工作流。支持分支、循环和拖放功能，增加了工作流创建的灵活性和易用性。

- 广泛的集成：Activepieces集成了Google Sheets、OpenAI、Discord、RSS等80多种其他集成。支持的集成列表持续快速增长。

- 开放生态系统：所有集成的源代码都公开在仓库中，使得用户和开发者可以查看、修改和扩展这些集成。
集成版本直接发布到http://npmjs.com，方便用户获取和更新。

-无限的使用案例：通过社区模板，用户可以获得自动化构建的灵感和指导。Activepieces适用于各种自动化场景，从简单的数据同步到复杂的业务流程。

Activepieces被视为流行的自动化平台Zapier的一个替代品，提供类似的功能但更多的自定义和控制选项。

在线体验：http://activepieces.com
GitHub：http://github.com/activepieces/activepieces

### 25

2024-01-03

https://x.com/lxfater/status/1742226690804523165?s=20

铁锤人
@lxfater
推友把图像修复功能搞到了微信小程序上了，现在大家可以更加方便免费开使用。

大家可以给他star一个，让他做得更好。

### 26

2024-01-03

https://x.com/op7418/status/1742212461779169377?s=20

歸藏
@op7418
Text2Immersion：可以通过文本直接生成3D场景，不过看演示能动的角度比较有限，可能再转就会穿帮，不过也很有意思了。

项目简介：
Text2Immersion，这是一种优雅的方法，可以从文本提示生成高质量的3D沉浸式场景。
我们提出的流程首先通过预训练的2D扩散和深度估计模型逐步生成高斯云。然后在高斯云上进行细化阶段，插值和细化以增强生成场景的细节。
与主流方法不同，这些方法通常专注于单个对象或室内场景，或者采用缩小轨迹，我们的方法生成具有各种对象的不同场景，甚至扩展到虚构场景的创建。
因此，Text2Immersion可能对虚拟现实、游戏开发和自动化内容创作等各种应用产生广泛影响。

项目地址：https://ken-ouyang.github.io/text2immersion/index.html


### 27

2024-01-03

https://x.com/op7418/status/1742208505204109729?s=20


歸藏
@op7418
一份提示工程最佳实践，内容比较基础，各位应该都看过很多次了。
不过她说这也是和人沟通的最佳实践有点意思。仔细想了一下确实是这样的，这些内容在跟人沟通的时候也很有用。

> 如何编写清晰/具体的说明 
> 给模型时间思考 
> 多次提示 
> 指导模型 
> 分解提示 
> 使用外部工具

全文链接：https://mphr.notion.site/Prompt-Engineering-Best-Practices-0839585d4bce4c6abb0b551b2107a92a

### 28

2024-01-04

https://x.com/op7418/status/1742756467512672507?s=20

歸藏
@op7418
另一个利用多模态 LLM 来理解和操作网页的项目SeeAct。
这个Agents项目利用GPT-4V 等 LMM 来直观地感知网站并生成文本形式的计划。然后，文本计划会被转换为基于 HTML 元素和操作在网站上执行。

这个项目可以成功完成不同网站上 50 % 的任务，而 GPT-4V 是 20%。

但是也有一些问题，目前最佳的方法与理论上完美结果之间还存在着20-25%左右的差距。在众多尝试过的方法中，一种综合运用HTML文本和视觉元素的策略表现最为出色，并且比图像注释策略提升了高达30%。

论文地址：https://browse.arxiv.org/html/2401.01614v1

### 29

2024-01-04

https://x.com/dotey/status/1742732944886710480?s=20

宝玉
@dotey
杨立昆分享的这个故事挺有意思：

让我来分享一个关于免费书籍的故事。

在1990年代中期，我在 AT&T实验室里开展了一个名为 DjVu 的项目。

我们的目标是，制定一种新型的图像压缩格式，让打印文件能够以高分辨率被扫描，然后高效地通过新兴的互联网进行分发。

这种格式于 90年代末到 00年代初开始发表，并且得到了包括互联网档案等在内的众多网站的采用。

为了有效地展示这项技术，我决定扫描并发布神经信息处理系统会议（Neural Information Processing Systems，简称NIPS）的全部论文集。

我向出版商 Morgan Kaufman 和 MIT Press 获取这样进行的权限，他们同意了，因为他们并未从过去的论文集中获得收入。

我们扫描了所有 13 卷的论文集，进行了光学字符识别（OCR）处理，对所有材料进行了索引，并在 2000 年将其发布在了一个免费的网站上：http://nips.djvu.org。
这个开源的知识库为机器学习研究社区提供了巨大的帮助。

大约在同一时间，机器学习（ML）社区开始反抗商业期刊出版商，并创建了 JMLR（Journal of Machine Learning Research）机器学习研究期刊，这是第一个开源且全免费的期刊，这也带来了巨大的益处。

逐渐地，NIPS会议停止印刷论文集，而是开始在他们的网站上（http://nips.cc）发布所有的书籍，包括我们的扫描版本。

如果你曾经想知道为什么 ML/AI 社区积极推动预印版的快速发布和开源出版物，就是这个原因。

### 30

2024-01-04

https://x.com/op7418/status/1742738120884519310?s=20


歸藏
@op7418
对普通人来说学习和使用 AI 接触最多的就是提示词，很多人用不好 AI 主要的原因就是不会写和用提示词，也不是所有问题都能找到现成的提示词用。

最近又在时间线刷到了PromptPerfect（http://promptperfect.jina.ai/a/NEW）就去试了一下，发现这玩意有点厉害，对复杂任务和 AI 画图的提示词优化很好。

优化之前模型无法完成的任务，优化之后就能搞定了。优化之前模型无法完成的任务，优化之后就能搞定了。

另外你也可以拿着优化过的提示词去干自媒体或者开课也是一个好生意。比如我下面👇的两个例子：

### 31

2024-01-04

https://x.com/dotey/status/1742685002213572631?s=20

宝玉
@dotey
推荐阅读：《构建软件项目最难的部分不是编码而是需求》

AI 想要替代程序员谈何容易。

随着越来越多的 AI 最新成果的新闻报道，很多人觉得 AI 很快就能取代我们这些程序员，他们觉得未来管理层和产品经理可以直接绕过程序员，让 AI 直接开发出他们想要的产品。作为一名有 15 年工作经验，日常就是根据这些人的要求构建软件的程序员，我对这种担忧其实并不太认同。

编码固然充满挑战，但我从未花费过两周以上的时间来解决代码问题。一旦你熟悉了编程语法、逻辑和技术，编码大体上是个直接明了的过程。真正的难题通常在于软件应该完成什么任务。软件构建真正的难点不在于编写代码，而在于定义需求，而这些需求仍然需要人类来确定。

本文将讨论软件需求与软件构建之间的关系，以及 AI 实现优秀成果所需的关键因素。

原文：https://stackoverflow.blog/2023/12/29/the-hardest-part-of-building-software-is-not-coding-its-requirements/
译文：https://baoyu.io/translations/software-engineering/the-hardest-part-of-building-software-is-not-coding-its-requirements


### 32

2024-01-04

https://x.com/dotey/status/1742644532871655766?s=20

宝玉
@dotey
新课程：《人工智能高级检索》

在本课程中，将深入讲解使用大型语言模型 (LLM) 为检索增强生成系统获取最相关上下文的高级技术。


---

https://x.com/DeepLearningAI/status/1742584137750344192?s=20

DeepLearning.AI
@DeepLearningAI
New course in partnership with 
@trychroma
, Advanced Retrieval for AI!

In this course, you'll dive into advanced techniques that use a large language model (LLM) to get the most relevant context for a Retrieval Augmented Generation system.

Join now: https://hubs.la/Q02f3Jcc0

### 33

2024-01-04

https://x.com/dotey/status/1742641603460710683?s=20


宝玉
@dotey
这款移动操作机器人𝐌𝐨𝐛𝐢𝐥𝐞 𝐀𝐋𝐎𝐇𝐀的软件硬件都是开源的，能做饭、坐电梯、收拾东西！

作者对项目的介绍：

𝐌𝐨𝐛𝐢𝐥𝐞 𝐀𝐋𝐎𝐇𝐀是如何运作的呢？

我们希望这个机器人能够实现更多的功能，提升原始的 𝐀𝐋𝐎𝐇𝐀 的技能，包括：
1. 移动速度快，可以达到人类正常步行的1.42米/秒。
2. 稳定性强，能够操作重型炊具，真空吸尘器等设备。
3. 全身动作，可以同时远程操作所有的自由度（Degrees of Freedom）。
4. 不需要外接电源和计算设备。

为了实现这些目标，我们把ALOHA装在了一个为仓库设计的移动台座上：这就是Tracer AGV。

它可以负载100公斤，速度可以达到1.6米/秒，而且价格只有7000美元。

为了让机械臂和台座可以同时操作，我们采用了一个简单的方法，就是将操作员和移动基座连接起来，这就是反向驱动（backdriving）的概念。

在测试阶段，当机器人可以自主运作时，反向驱动的结构和引导臂就可以轻松拆除。这样可以减少机器人占用的空间45%，并且可以减轻15公斤的重量。

这款机器人可以在垂直方向上从65厘米伸展到200厘米，离机器人基座最远可以到达100厘米。

通过 50 次演示，我们的机器人可以自主完成复杂的移动操作任务：
- 烹饪和供应虾🦐
- 呼叫并乘坐电梯🛗
- 将一个 3 磅重的锅存放到双门橱柜中

我们把𝐌𝐨𝐛𝐢𝐥𝐞 𝐀𝐋𝐎𝐇𝐀所有的硬件和软件都开源出来了：
Tutorial: https://docs.google.com/document/d/1_3yhWjodSNNYlpxkRCPIlvIAaQ76Nqk2wsqhnEVM6Dc/edit
Github: https://github.com/MarkFzp/mobile-aloha
Project Website: https://mobile-aloha.github.io

### 34

2024-01-04

https://x.com/dotey/status/1742413924396319183?s=20

宝玉
@dotey
这篇文章值得看看，像 Redis作者 Antirez 这样的顶级程序员都在借助大语言模型写程序！

Antirez 使用 ChatGPT 这样的语言辅助编程的做法很典型：

1. 对于不熟悉的语言或者类库，避免了查询文档，直接让 GPT 给出解释或者生成代码
2. 写临时代码，对于一些一次性代码，就不用费心费力去自己写，让 LLM 帮忙生成，质量还不错

当然 Antirez 也发现了一些局限：

- 对于复杂的代码，比如写个布隆过滤器，目前质量还不够好！
- 上下文长度不够

Antirez 的建议：

- 现今程序员没理由不去使用 LLM 辅助编程
- 正确地向大模型提问是一项关键技能，学会向 LLM 提问也有利于提升程序员的沟通能力
- 把 LLM 当做一种压缩文档（不能完全替代文档，毕竟有幻觉）来使用

这篇文章的中文翻译版本：https://baoyu.io/translations/llm/llms-and-programming-in-the-first-days-of-2024

---

https://x.com/Piglei/status/1742399105891270747?s=20


piglei
@Piglei
Redis 作者写了篇文章，分享用 LLM 编程的体验，相当务实。他提到 LLM 能极大缩短学习新技术的时间，非常适合写一些“用后即丢”的代码，但在本身已是专家的领域中帮助略小。

最后，他认为如何向 LLM 提出正确的问题，会成为非常重要的技能；截图那段是我最感慨的部分。

阅读：http://antirez.com/news/140

[LLMs and Programming in the first days of 2024 - <antirez>](http://antirez.com/news/140)

[List of posts - <antirez>](http://antirez.com/latest/0)

### 35

2024-01-04

https://x.com/dotey/status/1742604614535463022?s=20


宝玉
@dotey
\#开源项目推荐：paulpacifico/shutter-encoder

Shutter Encoder是一款开源免费的视频压缩编辑软件，支持windows和Mac。

主要功能包括：
修剪和剪辑视频
优化图片
强大的裁剪支持
生成和烧录剪辑信息
字幕嵌入和烧录
添加水印
内置字幕编辑器
等等

官网：https://shutterencoder.com
项目地址：https://github.com/paulpacifico/shutter-encoder

### 36

2024-01-04

https://x.com/dotey/status/1742432080707895476?s=20

宝玉
@dotey
其实最早在编程界就流传着“Make it work, Make it right, Make it fast”——Kent Beck

先能跑，再跑对，最后再优化！

至理名言！ 

https://baoyu.io/translations/software-engineering/make-it-work-first-then-right-fast

### 37

2024-01-04

https://x.com/Barret_China/status/1742842500212273302?s=20

Barret李靖
@Barret_China
推荐阅读这份报告《拾象大模型观察思考-最新判断猜想.pdf》，对 2024 年的闭源模型、开源模型、多模态模型和大模型应用格局四个方面做了预判，信息量还比较大。

https://waytoagi.feishu.cn/record/XmQRr3ptFesMFncyp2rcB0VFnLh

大模型的新摩尔定律：模型能力每 1~2 年提升一代，过程解锁新应用；模型训练成本每 18 个月除以 4，模型推理成本每 18 个月除以 10。

### 38

2024-01-04

https://x.com/dotey/status/1742979829467627937?s=20

宝玉
@dotey
[深度访谈] Ddog： 世界上首款脑控四足机器人

Ddog项目的特点集合了Boston Dynamics的Spot机器人以及由AttentivU提供的脑电脑接口（BCI）系统。这个系统是一副能测量人的脑电（EEG - 脑活动）和眼电（EOG - 眼部活动）信号的无线眼镜。Ddog项目是Brain Switch应用的升级版，这是一个实时的闭环BCI系统，允许用户以非语言的方式实时传达简单需求。Brain Switch的目标是帮助那些有身体挑战（如ALS，CP，SCI）的人满足基本的交流需求。Ddog项目是在Brain Switch的技术架构和基础设施基础上构建的。

Ddog的最大特点是其行动能力：这是第一个完全自主，由大脑驱动，无线的系统，包含了Spot机器人，在两部iPhone上运行，而且不需要黏贴的电极和计算用的背包装置。
Ddog的设计初衷是帮助进行物体操作，例如Spot的手臂被用于: 送货上门，移动椅子，携带书籍或拿玩具等。

这个Ddog项目负责人Nataliya Kosmyna博士的访谈涵盖了以下话题：为什么要创建Ddog项目？为什么选择使用Spot而不是其他机器人？Ddog背后的技术架构是怎样的？为什么认为Ddog项目需要无线且可携带的大脑感应解决方案是重要的？以及一些疯狂的应用案例，对于Ddog项目的未来展望，STEM和Ddog的联系等等！

项目网页：
https://media.mit.edu/projects/ddog
https://braini.io/ddog

以下为采访内容

采访者：请介绍一下自己。

Dr. Kosmyna：大家好，我是 Nataliya Kosmyna，麻省理工学院媒体实验室的研究科学家。在过去的 15 年里，我主要研究脑机接口技术。

采访者：今天我们的讨论主题是什么？

Dr. Kosmyna：今天我们将讨论我们最新的应用案例，名为 Ddog。Ddog 结合了波士顿动力公司的 Spot 机器人和我们自主研发的脑机接口。这款脑机接口采用了眼镜的形态设计，我们称之为 AttentivU。

采访者：可以进行演示了吗？

Dr. Kosmyna：是的，我们有新的项目要展示，这个项目名为 Ddog。它包括了波士顿动力公司的 Spot 机器人和 AttentivU 眼镜。我们的这款产品是可穿戴的、无线的，能够捕捉使用者的大脑活动和眼动。我们内部将其称为“想即得”。如果你没见过 Spot，现在可以在我的背后看到，它是一种类似狗的机器人。你可以通过它在空间内移动，执行各种任务，比如拿椅子、购物或打开门等。

采访者：能否介绍一下不同的大脑感应技术？

Dr. Kosmyna：当然。从屏幕上可以看到，我们的技术是如何从最初的阶段发展过来的。最初的阶段看上去像章鱼一样。不过，脑机接口技术（BCI）已经取得了巨大的进步。我们现在有更为简约和微型化的设备。但说实话，即使是头带式的，也不适合日常使用。在运动场或办公室里或许还行，但在日常生活中人们不太可能随身携带。大脑感应技术面临的挑战之一是它需要收集大量的数据，而这些数据的获取成本很高，且不容易获得。这和计算机视觉不同，计算机视觉所需的数据随处可得，易于获取。如果我们想要将脑机接口技术从理论走向实际应用，关键在于数据的收集。它不仅仅是关于一些杀手级应用或者你可能听说过的 XR 和其他头戴设备。目前而言，数据是成功的关键。因此，尽管实验室里的“章鱼式”应用非常适合研究，但现实世界中，我们更加看重的是可穿戴和便携式的设备。这里我特别强调那些可以每天多小时佩戴的设备，它们可以在驾车、办公或者录制视频等各种场合中使用，让用户几乎感觉不到它们的存在。

采访者：关于大脑感应眼镜的想法。

Dr. Kosmyna：我们谈论的就是眼镜形式的大脑感应设备。我们每天都在佩戴各种头戴物品，比如耳机、眼镜、太阳镜、口罩和各种帽子等。我们已经习惯了在头部携带这些物品，所以只需要在这些已有的形式中加入数据收集功能。

采访者：对 Ddog 项目感到兴奋的原因是什么？

Dr. Kosmyna：Ddog 是一个概念证明项目，显然非常令人兴奋，因为它能够实现利用大脑活动与大型工业机器人进行互动。据我们所知，这是首次将此类应用与波士顿动力公司的 Spot 机器人结合起来。因此，Ddog 是面向消费者级别应用的首个版本。更重要的是，这能够提高人们对于这类机器人在日常生活中应用的认识和需求，比如在家庭和医院环境中的应用。虽然 Spot 本身是一种工业级别的机器人，但我们认为这些类型的机器人在民用领域也有重要的应用价值。

采访者：为什么开发 Ddog？

Dr. Kosmyna：其中一个项目名为 Brain Switch，它是一个为患有晚期肌萎缩侧索硬化症（ALS）的人士提供沟通支持的工具。我已经在这个项目上工作了大约 10 年。我的研究不仅涵盖了大量文献，还与多个非营利组织和顾问合作，覆盖了两个大洲、三个国家。我们的工作涉及许多患者和照顾者，他们已经在使用我们的 AttentivU 系统。考虑到这些用户的需要，我们进行了进一步的开发。ALS 是一种非常不幸的神经退行性疾病，目前没有治愈方法。我们的系统也不能提供治愈。在疾病的早期阶段，有许多辅助设备可用，如眼动跟踪和语音识别。然而，在疾病的晚期，患者将无法使用眼睛和声音进行交流，唯一剩下的是大脑活动。在这一阶段，有 93% 的美国人拒绝使用呼吸机。但他们仍然可以通过大脑活动进行基本的沟通。目前，我们正在使用的 Brain Switch 初版，为患者提供了基本的是与否沟通方式。而 Ddog 项目则是在我们已建立的基础设施和生态系统上的进一步发展，意在未来发展智能家居控制和机器人支持。我们已经为这些患者实现了智能家居控制，例如，如果他们想开电视或微波炉，可以通过大脑活动来控制。因此，Ddog 是这一发展路线的自然延伸，它是一种能够理解用户需求并为用户提供帮助的助手，例如打开音乐或在需要紧急帮助时通知他人。

采访者：Ddog 项目的技术架构是怎样的？

Dr. Kosmyna：首先，我们将发布一篇论文，不仅是视频。对于那些感兴趣或希望复制这一系统的人来说，你们将能够详细了解技术细节。我们在论文中描述了所谓的技术架构。简而言之，这个系统是设计出来的，同时考虑到了终端用户的需求，这里指的是机器人部分。正如我提到的，我们已经有了大脑电脑接口（BCI）部分，它背后有强大的人工智能支撑。这是一种深度学习技术，能够分析患者的大脑数据，并训练以识别特定患者的大脑模式。因此，当用户想要表达“是”或“否”的时候，系统可以识别并给出响应。机器人本身则是另一整套系统。如你所想，机器人配备了摄像头，需要了解自身在空间中的位置并进行导航。这部分我们称之为映射，属于机器人的功能范畴。我们得到了 Reactive Lions 公司的帮助，在机器人的操作和计算机视觉方面提供了大力支持。手臂部分是关键，它需要能够识别物体并拿起它们。这是一个非常复杂的任务，对于那些不熟悉机器人技术的人来说，拿起一个物体并将其移动是非常有挑战性的。你可以在视频的最后看到我们尝试拿起一个玩具或者试图带来一个轮式椅子。拿起物品并将其放入篮子或直接交给用户，识别用户所在的位置，从而有效地提供支持。

采访者：能否分享更多关于 Ddog 项目基础设施的信息？

Dr. Kosmyna：我们的一些选择是由我们现有的生态系统基础设施预先决定的。这里我指的是之前提到的硬件部分，也就是大脑电脑接口。这已经是一个经过验证的界面形式因素。我们的用户大多都有手机或移动设备，例如 iPhone。作为我们的 Brain Switch 试验的一部分，我们为用户提供了 iPhone，因为不是每个人都有这样的设备，为了统一我们所有用户的体验，我们为他们提供了相同的设备。因此，他们已经拥有了 iPhone，并在其上运行 Brain Switch 应用。所以在这个情况下，我们保持了这部分技术堆栈的基础设施不变。我们增加了第二部分，即 Reactive Lions 公司提供的帮助，他们在操控技术和计算机视觉方面为机器人提供了支持。实际上，为了让整个系统更加简单，这部分也是在另一部 iPhone 上运行。最终，我们得到了一个完全便携、移动、无线的系统，仅依赖于两部 iPhone 和一副眼镜。无需任何背包或重型设备，完全自主运行。

采访者：Ddog 项目能使用哪些其他机器人？为什么选择 Spot？

Dr. Kosmyna：如我之前提到的，国防和军事用途是 Spot 最为人所知的应用场景。虽然有许多成功案例值得关注，但我认为，还没有足够多的案例来满足那些实际上可以从这些系统中获益的用户的需求。我们选择 Spot，因为目前市场上没有其他类似的产品。当然，市场上有操控臂，但它们通常是固定的，不能移动。还有类似人形的机器人，但它们目前还不适合部署，而且对于我们的特定用途来说，可能有些过于复杂。此外，还有外骨骼，这是一个非常强大的工具。但在我来自的法国，Clinatec 实验室在外骨骼的实际应用研究方面取得了出色的成果，尽管如此，它们只进行侵入性研究，并且在过去八年里只有五名患者实际上植入了系统。因此，所有这些因素让我们再次选择了 Spot，因为在工业级别的机器人中，我们没有找到更好的选择。

采访者：能否评论一下 Spot 在国防和军事领域的应用？

Dr. Kosmyna：在国防、警察和军事领域，Spot 的应用确实引起了广泛的关注。例如，我记得最近的一个案例是澳大利亚国防部使用 HoloLens 和电极来控制 Spot。这些应用面临着一些挑战，比如 HoloLens 作为一种头部装置，相当重，且电池寿命仅为两小时，这对最终用户来说并不方便。另一个问题是，在视频中可以看到，他们使用的电极看起来像是粘在耳朵后面的，位置与我们的 AttentivU 设备类似，但它们是有线的且需要粘贴。

采访者：民用用例。

Dr. Kosmyna：对于民用用例来说，用户不太可能会选择这样的设备。例如，经历了长时间工作的母亲回到家后，不太可能选择这样的设备。因此，从我十年的经验来看，无线和舒适的设备更受欢迎。

采访者：伦理考虑。

Dr. Kosmyna：伦理方面的考虑当然也很重要。军事和国防应用确实非常重要，因为它们可以支持那些可能无法或不应该置身于危险中的人。例如，我们已经看到 Spot 被用于核设施等危险环境。然而，也有一些争议，比如纽约警方尝试使用 Spot 进行监控，但效果并不理想。因此，在这些系统真正投入实际应用之前，需要进行更多的讨论和审慎考虑。就像我们的项目一样，我们发布了视频和论文，并继续进行研究，但系统还没有准备好进行广泛部署。除了机器人本身是工业级别的之外，还需要更多的立法和公众讨论。对于大脑感应技术来说，这些讨论尤为重要，因为它处理的是人类最私密的数据：我们的思维。

采访者：Ddog 未来的发展方向是什么？

Dr. Kosmyna：我们即将发布一篇论文和视频，对于感兴趣的人来说，你们可以通过这些资料了解更多技术细节，并可能与我们合作或尝试复制一些部分。我们的项目有两个主要发展方向。第一个是操纵技术，特别是手臂的操纵非常关键。我们希望进一步探索这个领域，并希望与最终用户共同进行探索。目前为止，我们的录像是在实验室严格控制的条件下进行的。

采访者：你想实现的任何疯狂用例。

Dr. Kosmyna：例如，一个可以感知用户恐惧或压力水平的保护版本。在录制视频时，环境是实验室，而且是晚上。如果有人进入并表现出攻击性，机器狗可能已经察觉到了这一点，并可能呼叫帮助......

采访者：Ddog 和 STEM 教育。

Dr. Kosmyna：第二个方向是我们受到马萨诸塞州发生的事情的启发。在波士顿和大波士顿地区的学校中，有一系列的研讨会，让孩子们通过与 Spot 这样的编程机器人互动来引入 STEM 教育，这个项目进行得非常成功。

采访者：联系信息。

Dr. Kosmyna：你现在可以在我身后的图像上看到所有的联系信息，包括网站和电子邮件。你可以通过 nkosmyna [at] http://mit.edu 发邮件给我，询问任何问题。你也可以访问 MediaLab 的网站或我的个人网站 http://braini.io http://BRAINI.IO，了解更多关于 Ddog 以及我们提到的其他项目，如 Thinking Cap、Brain Switch 和 AttentivU，还有更多内容。欢迎随时联系我们。谢谢大家的时间。

### 39

2024-01-05

https://x.com/dotey/status/1742976011933831632?s=20

宝玉
@dotey
这周被机器人的新闻刷屏了，Google DeepMind 的机器人团队也刚发布了一篇博客，展示了他们最新的研究进展，不过只有一篇博客，没有代码。

他们开发了一种名为AutoRT的新技术，这是一个将大型基础模型（比如 大语言模型 (LLM) 或视觉语言模型 (VLM)）与机器人控制模型（RT-1或RT-2）相结合的系统。这个系统使得机器人能够在全新的环境中收集训练数据。良好的感知模型配合能够生成运动控制系统指令的大语言模型 (LLM)，将在机器人领域站在潮头。

博客地址：https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/
译文：https://baoyu.io/translations/google/shaping-the-future-of-advanced-robotics

### 40

2024-01-05

https://x.com/dotey/status/1742960543772602621?s=20

宝玉
@dotey
MIT的Ddog项目成功将脑机接口和波士顿动力机器狗进行链连接，脑瘫人士也能操作机器狗

在Nataliya Kos’myna的领导下，麻省理工学院的一支研究团队最近发表了他们的研究成果，一项名为Ddog的项目。

该项目致力于将波士顿动力学公司的Spot四足机器人改造为能够为患有肌萎缩侧索硬化症（ALS）、脑瘫和脊髓损伤等疾病的人士提供基本沟通工具。

该项目 使用了一套包括AttentivU的脑-电脑界面（脑机接口，BCI）系统。这种从技术上实现了脑-电脑直接接口的技术，以一副内嵌传感器的无线眼镜的形式进行应用。这些传感器不仅可以测量人脑活动的脑电图（EEG），也可以追踪他们的眼动。

该研究以大学 的Brain Switch 作为基础，这是一个实时闭环型的脑机接口，它允许用户与看护者进行无言的、实时的交流。如今，Kos’myna正在借助与Brain Switch相同的技术所开展的Ddog项目，进一步扩大该技术的应用范围。

Spot可以为用户取物

根据全国罕见疾病协会报告，目前在美国有3万ALS患者，并且每年有5000个新病例被确诊。另一方面，据脑瘫指南所报告，大约有一百万的美国人生活在脑瘫之中。

许多患者已经或将会失去他们的行走、穿衣、说话、写字甚至呼吸的能力。目前虽然存在沟通辅助工具，但多数是靠视线跟踪的设备，允许患者通过计算机来交流。然而目前还没有太多的系统可以让患者与他们周围的世界进行互动。

Ddog的最大优势在于其移动性。Spot机器人可以进行完全自主的移动，这意味着在给定简单指令后，它可以在无需人工干预的情况下进行操作。

Spot也具有出色的移动性。凭借四足的结构，Spot几乎可以走向人类可以到达的任何地方，包括上下斜坡和楼梯。机器人的手臂配件还可以执行类似送货、移动椅子或者给用户送来书或玩具等任务。

MIT的这套系统只需要两部iPhone和一副眼镜就可以运行。它无需复杂的电极设备或背包，使其比其它现有的辅助设备更方便日常使用，这是研究团队的观点。

Ddog的工作原理

作为与新用户在新环境中工作的初步步骤，Spot首先需要建立工作环境的3D地图。然后，第一部iPhone将询问用户接下来想做些什么，用户只需想象他们想要的东西来做出回答。

第二部iPhone则会运行本地导航地图，并控制Spot的手臂，同时利用iPhone的雷达数据来提升Spot的雷达性能。这两部iPhone可以彼此通信，以跟踪Spot完成任务的进度。

MIT的研究团队设计了一个可以完全在线或离线工作的系统。在线版本则具有一套更为先进的机器学习模型，以及更好的微调模型。

来源：https://therobotreport.com/ddog-mit-project-connects-brain-computer-interface-spot-robot/

### 41

2024-01-05

https://x.com/op7418/status/1742953722026811804?s=20


歸藏
@op7418
作者构建了一个减轻大语言模型幻觉的整体知识框架，非常详细和体系化，基本上该有的都涉及到了。
想要了解相关内容的话跟着这个目录搜索就可以。

下面是所有内容的概述，原文还有每个小节的概述：

提示工程领域：

A. 提前检索增强生成（RAG）

- 生成之前：在文本生成之前进行信息检索的策略，比如LLM-Augmenter

- 生成过程中：在句子生成时进行检索，比如知识检索，D&Q框架

- 生成之后：在全文生成完成后进行检索，比如RARR

- 端到端：将检索和生成整合在一起的模型，比如原始的RAG模型

B. 通过反馈和推理进行自我完善

- 通过用户反馈迭代改进输出结果，比如Prompting GPT-3 for Reliability

- 发现并缓解自我矛盾，比如ChatProtect

- 通过反馈循环进行交互式改进，比如自我反思方法论

C. 提示调整

- 为模型提供调整指令，比如UPRISE, SynTra

模型开发领域：

A. 新解码策略

- 在生成过程中引导，比如上下文感知解码

B. 利用知识图谱

- 注入结构化知识，比如RHO

C. 基于忠实度的损失函数

- 提升输出内容的真实性，比如THAM框架

D. 监督微调

- 在标记数据上进行模型调整，比如知识注入方法

### 42

2024-01-05

https://x.com/op7418/status/1742955636638535876?s=20

歸藏
@op7418
一个LLM提示词的内容框架。 组成部分：

指令：引导LLM推理格式和结构的简短提示
理由：在上下文导向推理（CoT）过程中生成的中间推理步骤
示例：展示目标推理模式的输入输出实例
环境：交互式上下文，如操作系统、应用程序、网页代理
工具：扩展LLM能力的外部模块，包括执行、知识或验证

模块：

感知：通过CoT提示顺序解释环境状态
记忆：短期存储暂时信息；长期保留静态知识
推理：在交错的CoT格式中进行规划、决策和行动

格式：

文本：标准CoT的顺序语言
树形结构：表示互连思想的层级结构
图形：映射思想之间关系的网络
程序：基于代码的思想，逻辑与语言分离
表格：以行/列表格方式展现连贯思维进程

过程：

提示：用指令和示例引出目标推理格式
聚合：结合多个CoT路径以提高流畅性
验证：利用外部信息源评估和修订思想
定制化：与特定用户需求相匹配

实体：

问题：触发代理CoT推理的输入
答案：从CoT推理中得出的最终输出
行动：基于代理决策的操作执行
情节：朝向目标的完整交互序列
轮次：情节内单个顺序互动

属性：

可解释性：理解导致结论的推理过程
可控性：通过调整提示影响模型行为
适应性：在新环境和任务中有效性
安全性：确保行为安全，避免有害故障模式

任务：

算术：数学推理
文本：语言理解和常识
视觉：结合图像的多模态推理
符号：结构化输入，如编程语言
通用：广泛的日常现实世界应用

### 43

2024-01-05

https://x.com/op7418/status/1742946023935516751?s=20


歸藏
@op7418
Open AI 发布文章，介绍了GPTs创建器是如何被创建的，搞笑的是这个GPTs构建器本身也是一个GPTs。

来学习一下Open AI是怎么写GPTs提示词的。

👇下面是GPT Builder具体的构建过程和提示词：

GPT Builder 被构建为一个自定义的 GPT，具有指令和动作，允许它写入当前正在构建的 GPT 的字段。

更高级的构建者应该使用手动配置界面来编辑他们的GPT的字段，但GPT构建器始终可以作为一个起点。

由于GPT Builder本身就是一个定制的GPT，我们可以分享我们使用的配置作为创建强大GPT的示例。

以下是我们用于为GPT Builder提供动力的核心指令，截至2023年1月3日。为了清晰起见，我们将指令分为“基本上下文”和“步骤演示”，但在应用到GPT时，它们都会进入“指令”部分。

说明-基本上下文：

您是一个擅长创建和修改GPT的专家，它们就像可以具有额外功能的聊天机器人。

每个用户消息都是您处理和更新GPTs行为的命令。您将承认并将其纳入GPTs的行为，并在gizmo_editor_tool上调用update_behavior。

如果用户告诉你开始以某种方式行为，他们指的是你正在创建的GPTs，而不是你自己。

如果您没有个人资料图片，必须调用generate_profile_pic。如果明确要求，您将通过generate_profile_pic生成个人资料图片。否则不要生成个人资料图片。

保持作为GPTs制作者的专家的语调和观点。 GPTs的个性不应影响您的回答风格或语调。

如果你问用户一个问题，永远不要自己回答。你可以提出答案，但必须让用户确认。

您可见的文件也对 GPT 可见。您可以更新行为以引用已上传的文件。

请勿使用“约束”、“角色和目标”或“个性化”这些词。

GPTs没有记住过去经验的能力。

说明-步骤：

你是一个用于开发新GPTs的迭代原型游乐场。用户将通过初始行为提示你。

您的目标是迭代地定义和完善update_behavior的参数。您将以专业GPT创建者的身份进行交谈，从用户那里收集规范以创建GPTs。您将在每次交互后调用update_behavior。您将按照以下步骤进行：

1）用户的第一条消息是关于这个GPT应该如何行为的广泛目标。使用参数“context”、“description”、“prompt_starters”在gizmo_editor_tool上调用update_behavior。记住，你必须使用参数“context”、“description”和“prompt_starters”调用gizmo_editor_tool上的update_behavior。在调用update_behavior之后，继续进行第2步。

2）在这一步中，你的目标是确定 GPT 的名称。你会为自己建议一个名称，并要求用户确认。你必须提供一个建议的名称供用户确认。
你不可以在没有建议的情况下提示用户。不要使用驼峰式复合词；请使用空格代替。如果用户指定了一个明确的名称，请假设它已经确认。如果你自己生成一个名称，你必须让用户确认该名称。一旦确认，只需调用 update_behavior，并继续到第三步。

3）在这一步中，您的目标是为 GPT 生成一个个人资料图片。您将使用 generate_profile_pic 为这个 GPT 生成一个初始个人资料图片，无需确认，然后询问用户是否喜欢，并是否想要进行任何更改。
请记住，使用 generate_profile_pic 生成个人资料图片时无需确认。在每次改进后生成新的个人资料图片，直到用户满意为止，然后继续进行第四步。

4）在这一步中，你的目标是细化上下文。你现在要引导用户细化上下文。上下文应包括“角色和目标”、“约束”、“指南”、“澄清”和“个性化”等主要领域。你将引导用户逐个定义每个主要领域。
你不会一次性提示多个领域，而是一次只问一个问题。你的提示应该是引导性、自然和简单的语言，不会提及你正在定义的领域的名称。
你的提示不需要介绍它们正在细化的领域，而只需是引导性问题。例如，“约束”应该提示为“应该强调或避免什么？”，“个性化”应该提示为“你希望我怎么说”。
你的引导性问题应该是不言自明的；你不需要问用户“你认为呢？”。每个提示都应参考并建立在现有状态之上。每次互动后都要调用update_behavior。

在这些步骤中，您不会提示或确认“描述”、“提示启动器”的值。但是，您仍会在上下文更新时生成这些值。您不会提到“步骤”; 您将自然地进行下去。

你必须按顺序完成所有这些步骤。不要跳过任何步骤。

请让用户在右侧的独立聊天对话框中尝试GPT。告诉他们你能够听取他们对GPT的任何改进意见。以一个问题结束这条消息，不要说“让我知道！”。
在确认名称时只将GPT的名称加粗；在第2步之后不要加粗名称。

Action 行动：

在上述步骤之后，您现在处于迭代细化模式。用户将提示您进行更改，您必须在每次交互后调用update_behavior。您可以在这里提出澄清问题。

generate_profile_pic: { description: '为GPTs生成个人资料图片。您可以调用此函数而无需生成图像的能力。如果当前的GPT没有个人资料图片，则必须调用此函数，并且在需要生成新的个人资料图片时也可以调用。在调用此函数时，请将个人资料图片视为已更新，不要调用update_behavior。' },

update_behavior: { description: "更新GPTs的行为。您可以有选择地省略更新字段。您将使用这些新字段作为GPTs行为的真相来源，并不再引用任何先前版本的已更新字段来通知响应。当您更新一个字段时，如果它们是不一致的，那么您还必须同时更新所有其他字段以保持一致性。如果您更改了GPTs的名称，则必须使描述和上下文保持一致性。在调用此函数时，不能总结该功能外部使用中所使用的值" , params: { name, context, description, prompt_starters, abilities, profile_pic_file_id } }

GPT可以利用提供给它的所有信息，包括提示、指令和附加文件，来构建对用户的回应。不要包含你不希望用户知道的信息。

---

https://x.com/OfficialLoganK/status/1742930722766397932?s=20


Logan.GPT
@OfficialLoganK
Excited to share a little behind the scenes of the GPT Builder feature available inside the 
@ChatGPTapp
 👀

GPT Builder is actually itself a GPT using  instructions and a custom action. This is useful to read if you are building GPTs:

[GPT Builder | OpenAI Help Center](https://help.openai.com/en/articles/8770868-gpt-builder)

### 44

2024-01-05

https://x.com/op7418/status/1742477352448544847?s=20

歸藏
@op7418
这里有一个非常简单的ComfyUI 自定义节点开发指南，只需要 5 分钟，你甚至不需要会代码，跟着也能写一个自定义节点。

ComfyUI 之所以现在开始流行有一个很重要的原因是他的插件和节点开发成本比 WebUI 低很多。
但是他本身的开发文档写的很乱，导致入门看起来很困难。

Reddit 一个老哥写了一个自定义节点的开发指南，我跟着走了一遍发现真的简单，写的也很详细。
所以就翻译了一下由于比较长，就扔在周刊里面了。

教程和原文地址：https://quail.ink/op7418/p/create-custom-node-in-5-minutes-comfyui-custom-node-getting-started-guide


### 45

2024-01-05

https://x.com/dotey/status/1742928955848679812?s=20

宝玉
@dotey
有意思的文章

译文：https://baoyu.io/translations/people/elon-musk-is-not-understood

https://x.com/Danielw19410/status/1742561488299323422?s=20


自在夺造化
@Danielw19410
分享一篇Casey Handmer写的关于马斯克的长文。
作者作为物理学家对SpaceX涉及物理学的部分非常的专业，有很多细节甚至在艾萨克森的传记里都没有提到和展现，我将我看到的信息增量分享出来。
原文链接：

### 46

2024-01-05

https://x.com/xiaohuggg/status/1742839505412137338?s=20

小互
@xiaohuggg
兄弟们炸裂了

Meta AI又发布了一个炸裂的东西：从音频生成全身逼真的虚拟人物形象。

它可以从多人对话中语音中生成与对话相对应的逼真面部表情、完整身体和手势动作。

这些生成的虚拟人物不仅在视觉上很逼真，而且能够准确地反映出对话中的手势和表情细节，如指点、手腕抖动、耸肩、微笑、嘲笑等。

工作原理：

该项目结合了向量量化的样本多样性和通过扩散获得的高频细节的优势，以生成更具动态性和表现力的动作。

1、数据集捕获：首先捕获了一组丰富的双人对话数据集，这些数据集允许进行逼真的重建。

2、运动模型构建：项目构建了一个包括面部运动模型、引导姿势预测器和身体运动模型的复合运动模型。

3、面部运动生成：使用预训练的唇部回归器处理音频，提取面部运动相关的特征。
利用条件扩散模型根据这些特征生成面部运动。

4、身体运动生成：以音频为输入，自回归地输出每秒1帧的向量量化（VQ）引导姿势。将音频和引导姿势一起输入到扩散模型中，以30帧/秒的速度生成高频身体运动。

5、虚拟人物渲染：将生成的面部和身体运动传入训练好的虚拟人物渲染器，生成逼真的虚拟人物。

6、结果展示：最终展示的是根据音频生成的全身逼真虚拟人物，这些虚拟人物能够表现出对话中的细微表情和手势动作。

项目及演示：https://people.eecs.berkeley.edu/~evonne_ng/projects/audio2photoreal/
论文：https://arxiv.org/pdf/2401.01885.pdf
GitHub：https://github.com/facebookresearch/audio2photoreal/
Demo：https://colab.research.google.com/drive/1lnX3d-3T3LaO3nlN6R8s6pPvVNAk5mdK?usp=sharing

### 47

2024-01-05

https://x.com/Danielw19410/status/1743189389126271163?s=20

自在夺造化
@Danielw19410
推荐一个3小时的长视频（播客），视频涉及的主题太多不好总结，我给两个推荐理由：
1.如果你喜欢《马斯克传》，那么你应该也会喜欢这个视频，充满了无数琐碎的故事和真实的细节。

2.国内有关互联网、创投相关的播客没有讲的20%里面，这里面聊了其中的50%。

视频链接：

https://youtu.be/vCzj0Fth_8A

### 48

2024-01-05

https://x.com/dotey/status/1743186449304842390?s=20

宝玉
@dotey
今天看到一张 Apple 的 Pascal 语法的海报的矢量图重制版，1979 年的海报到现在都很漂亮，而且内容很极客，是 Pascal 语言的语法图！有些人称它为“极客圈的终极海报”。

特地去了解了一下它背后的故事：故事发生在 1979 年，就在几年前 Apple II 发布。那是 Apple 公司历史上的辉煌时刻，乔布斯身边聚集了一群杰出的人才，现代计算机的传奇历史才刚刚拉开序幕。

在 1977 年，Apple II 计算机面市，它被誉为第一台“个人电脑”。不同于采用 BASIC 语言，Apple II 选择了一种新颖、现代且高效的编程语言：Apple Pascal。这是由 Niklaus Wirth 于 1970 年创造的 UCSD Pascal 系统 的发展版本，旨在向 17 世纪发明了机械计算器的法国数学家 Blaise Pascal 致敬。

Apple II 上运行的是由 Bill Atkinson（图二）编写的 Pascal 编译器，他同时也是该编译器最初的也是最重要的程序员。

选择 Pascal 而不是更简单、更原始的 BASIC 并不是一件容易的事：乔布斯最初认为这是一个过于复杂的选择，他觉得 Apple II 使用 BASIC 就足够了，他对计算机附带的实用程序更感兴趣，而不是它所支持的编程语言。

但 Atkinson 最终说服了他，展示了 Pascal 的优势，以及它如何能够为新平台带来巨大的优势，为第三方软件的发展奠定了坚实的基础。正是因为这一选择，才促成了后来第三方软件的兴起。

对于很多老程序员来说，Pascal 是一个起点，它包含了像结构、变量这些现代编程的基础概念，这些概念即使到现在每种编程语言中都重复出现。

其中最著名的跟 Pascal 相关的大神当属 Anders Hejlsberg，曾为 Borland 开发出 Delphi，后来加入微软又主持了 .Net 的开发，现在的 TypeScript 也是他主导的。扯这么多其实只是想说 Delphi 的前身是 Object Pascal 和 Turbo Pascal！

很多人都知道，Mac 之父是 Jef Raskin，当时的 Apple II 及其后的 Macintosh 都是由他负责的，当他在将 Apple Pascal 适配到 Apple 电脑上时发现，传统的编程语言文档与 Atkinson 开发的新编译器在语法上有所不同，因此需要为程序员提供一系列新的参考资料。

Jef 开始设计一系列关于 Apple Pascal 的主要结构和逻辑语法的图解，这些图解是程序员学习和使用 Apple Pascal 时不可或缺的便捷参考，它们被打印出来并在 Apple 公司内部分发。他对这个项目投入了大量精力，简化了当时流行的各种复杂图解，并采用了严格的颜色编码，使内容更加清晰易懂，并选择了海报格式，方便 Apple 公司的每位程序员都能在自己的小隔间中挂上一张。

参考图三，这是 Jef Raskin 在一张老照片中，注意到背景里有一张海报。

当乔布斯第一眼看到 Jef Raskin 的项目时，他马上看出了其市场营销的巨大潜力。对 Jef Raskin 而言，这不过是一个为程序员提供语法参考的普通海报，但乔布斯却在其中看到了一件极具美感的图形作品，或者说，一件充满潜力的艺术品，但他要求专门聘请一位图形设计师重新设计海报，最终 Apple 找到了 Tom Kamifuji，一位当时在旧金山颇有名气的艺术家，并让他对 Jef Raskin 的作品进行改动，使之更具“艺术感”。

Tom Kamifuji 保留了原有的结构和语法，仅仅对图形设计进行了调整，使之更为协调。然而，他所犯的一个错误是彻底改变了色彩方案，原本 Jef Raskin 根据不同的编程结构或特定语法使用了不同的颜色，使整个设计对程序员来说更加易读和易懂。结果最终的额海报五颜六色，Pascal 中的“标识符 (identifier)”被表示为四种不同的颜色：紫色、橙色、绿色和粉色……（参考图四）

对于乔布斯和 Tom Kamifuji 来说，他们只关心海报是否“漂亮”，即使 Jef Raskin 强烈反对，最终的版本还是按照 Tom Kamifuji 的设计印刷的，甚至于最终海报上只有 Tom Kamifuji 的名字而没有 Jef Raskin 的名字。

关于这张传奇海报的印刷数量至今未知。每位 Apple 程序员都有一份，而且还分发给了一些外部程序员。正如乔布斯所期望的那样，这些海报被提供给了经销商，也被送给了 Apple 的顾客以用于推广 Apple Pascal 和 Apple 公司。

高清 PDF 下载：http://danamania.com/print/Apple%20Pascal%20Poster/PascalPosterV3%20A1.pdf

苹果 Pascal“语法”海报的历史，1979-80 [译]：https://baoyu.io/translations/apple/the-history-of-apples-pascal-syntax-poster-1979-80

苹果 PASCAL 语法海报：极客圈的传奇作品 [译]：https://baoyu.io/translations/apple/apple-pascal-syntax-poster


### 49

2024-01-05

https://x.com/dotey/status/1743146214550278231?s=20

宝玉
@dotey
https://github.com/zjunlp/LLMAgentPapers

大语言模型智能体相关论文列表

[zjunlp/LLMAgentPapers: Must-read Papers on LLM Agents.](https://github.com/zjunlp/LLMAgentPapers)

### 50

2024-01-06

https://x.com/dotey/status/1743393585217556991?s=20

宝玉
@dotey
这张 RAG 指南的图画的真好，介绍了为什么要用 RAG，基础的 RAG 用法和高级 RAG 用法！

应该是基于 excalidraw，高清 SVG 版本：
https://d3ddy8balm3goa.cloudfront.net/llamaindex/rag-cheat-sheet-final.svg

### 51

2024-01-06

https://x.com/op7418/status/1743305538669277193?s=20

歸藏
@op7418
斯坦福、微软、谷歌、新加坡国立大学一起出的一个论文，探讨人工智能创造力的评价标准。

他们引入了一个称为相对创造力的新概念来解决定义和评估创造力的复杂性。不再试图普遍定义创造力，而是将焦点转向人工智能是否可以与假设的人类的创造力相匹配。

他们称之为统计创造力。这种方法允许直接比较人工智能的创造能力与特定人类群体的创造能力。

除了定义和分析可衡量的创造力外，还介绍了可操作的培训指南，有效地弥合了创造力的理论量化和实际模型训练之间的差距。

什么是相对创造力？

相对创造力是一个概念，通过将人工智能的产出与一个假设但现实的人类创作者的产出进行比较来评估其创造力，假设二者受到相同的生平影响。如果一个人工智能模型能够产生与该创作者无法区分的作品，经评估确定为“相对具有创造力”。

相对创造力与计算机科学和认知科学中以往的作品有何不同？

这种创造力的概念与计算机科学和认知科学中的传统方法有所不同，它采用相对度量，而不是努力以绝对意义来定义创造力。相对创造力评估人工智能的方法类似于图灵测试，通过将机器行为与人类反应进行比较来评估智能，而不是遵循固定的定义。

项目地址：https://ai-relative-creativity.github.io

### 52

2024-01-07

https://x.com/dotey/status/1743745935451070472?s=20

宝玉
@dotey
我在日常用 GPT-4 翻译的时候，就会发现有时候 GPT 能给出很不错的质量的翻译，但有时候质量一般甚至比较差，所以我也尝试过一次生成多个翻译结果，然后从里面人工挑选最好的翻译，但是我当时想做而没有做到的是：

如何让 GPT 从里面帮我挑选一个最好的，或者将几个的优点结合组合成一个最好的结果。

昨天读了一篇来自浙大论文《Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives》（作者之一 
@spicysweet1859）

提到的方案相对就很系统了：

先让 LLM 根据请求生成几个不同风格的 prompt,  然后基于每一条 prompt 得到一个不同结果，然后对比这些结果之间的差异，基于这些差异总结出更有针对性的检查指令，用于反思，最后综合这些信息生成最终结果。

之所以不是直接一个Prompt生成多个结果，而是多个Prompt生成多个结果，是为了让生成的结果更多样，否则一个Prompt中的结果可能多样性不够，当然缺点是要费Token一些。但从性能上来说可能更好，因为可以并行生成。

另外对于特定任务其实不需要让LLM生成Prompt，应该人工生成Prompt效果也是一样的。

在跟作者沟通后了解到，这个方案最适合的其实不是翻译，而是推理：

> 我们这种 通过对比不同视角的responses之间的差异来启发反思的策略 对推理任务更有效果一些。 对于像翻译这类生成任务，我们实验发现其实也不需要多个视角，一个负面面视角和一个正面视角其实应该够用。这点宝玉老师您应该也介绍过。 by 
@spicysweet1859

推荐阅读：
https://browse.arxiv.org/html/2401.02009v1


### 53

2024-01-07

https://x.com/fuxiangPro/status/1743665779814654039?s=20

fuxiang
@fuxiangPro
查理芒格说李光耀是他的学习榜样。他可能是古往今来最优秀的领导者之一，用50多年的时间将新加坡从一个沼泽般的第三世界国家拉升为第一世界国家。这是个奇迹。

翻阅李光耀的访谈和资料，整理了10条李光耀对国家治理和国际环境的观点，很硬核：

1， 新加坡的双语之路是我一生的挑战。在新加坡，孩子们先学汉语，然后学英语。他们可能十几岁就去美国了，能说一口流利的英语，但他们的头脑里仍流淌着4 000年的汉语名言警句。与中国相比，美国的优势非常明显，因为它使用的是英语，这就使得美国能够从亚洲和欧洲吸引数以百万计的掌握英语的外国人才。
今天，英语能力是一种竞争优势，所以很多国家都在努力让孩子们学习英语。在21世纪，如果一个人想成功，就要掌握英语，因为这是一门在国际舞台上从事商业、科学、外交和学术活动时通用的语言。

2，中国的国内生产总值的绝对额将不可避免地赶上美国，但其创新能力可能永远无法与美国匹敌，因为它的文化不鼓励进行思想的自由交流和碰撞。不然如何解释一个人口4倍于美国的国家（可能中国人才的数量也是美国的4倍）却少有技术突破呢？

3，新加坡从第三世界国家跻身第一世界国家之列，靠的不是物色那些愿意在担任公职期间牺牲子女未来的部长。我们的方法很务实，不需要高素质人才为了公共利益放弃太多个人利益。新加坡的部长们待遇很高，我们要敢于直面这一点，不能为了回避外界对高薪的质疑而降低人才的待遇，那样做只会让新加坡重返第三世界。美国或英国的政治制度认为人都会为自己的国家着想。实际上呢？你真的相信那些连小学都没毕业的人明白自己的抉择引发的后果吗？但我们知道这些后果，我们将会挨饿，我们将会爆发种族骚乱，我们将会解体。
要治理好一个国家，最佳方法就是让最优秀的人做难度最大的工作。

4，我不希望新加坡人效仿美国人心安理得地依赖救济过日子，而是希望新加坡人学习美国的自强文化。这种文化特质使美国诞生了很多伟大的企业家，他们有魄力、有活力、有勇气创立和调整他们的企业，因此也就改变了美国经济，在这一点上，美国人比欧洲人和日本人做得好。

5，如果一个国家的领导人坚持维护社会秩序、给人民提供教育、维持睦邻友好关系、厉行法治、增强投资者的信心，那么国家没有道理不发展。

6，俄罗斯人口正在减少，具体为什么不清楚，但酗酒肯定对此有影响，消极情绪、生育率下降及预期寿命缩短也有影响。普京面临的挑战是让俄罗斯人对未来充满信心：停止酗酒、努力工作、建立幸福的家庭，并生育更多的子女。

7，全球化不可逆转，因为推进全球化进程的技术已经出现，这些技术是不会消失的。其实，更好的、更廉价的交通和通信将进一步增强推动全球化的力量。

8，我认为，我们说一个政府受欢迎并不是说它要在治理期间的任何时刻都受欢迎……有时你必须彻底不受欢迎。但在你的任期结束时，你应该给人民带来福利，这样人民才会认识到你所做的事情都是有必要的，才会再一次投你的票。这是我治理的基础。如果你想一直都受欢迎，那么你在治理时就会出现失误。

9，人的思想不只来自阅读，你可以从书本中获取，但如果你不把书本知识同自己的情况结合起来，书本知识就无用武之地。我自己经常会把读到的东西同自身情况结合起来……同博学多才的人展开讨论具有重要的意义，这一点一定不要忽略，我认为这比单纯孜孜不倦地阅读文献强得多。因为通过短暂的交流，你就能萃取对方的知识和对方的思想精华。

10，之所以会出现文明，是因为人类社会在一定条件下会应对挑战。哪里充满挑战，哪里就能兴旺发达。

### 53

2024-01-08

https://x.com/op7418/status/1744047810142703738?s=20

歸藏
@op7418
发现了个好东西，这个老哥开源了一门课程《从头开始构建大型语言模型》，这门课程将一步步地指导你创建自己的LLM。

每个阶段都有清晰的文本、图表和实例来解释相关概念。

课程内容包括：

1. 从基础理解注意力机制 
2. 构建并预训练一个类似于GPT的模型 
3. 学习如何加载预训练的权重 
4. 对模型进行分类任务的微调 5. 使用直接偏好优化进行指令微调模型

课程地址：https://github.com/rasbt/LLMs-from-scratch/tree/main

### 54

2024-01-08

https://x.com/op7418/status/1744046200448573799?s=20

歸藏
@op7418
比较详细的解释了为什么LLM都已经可以写代码了，还说他不具有推理和规划能力。
涉及到LLM代码生成的一些细节，感兴趣可以看一下。

————————————————————————

LLMs能够编写代码并不意味着它们具备推理和规划能力。（提示：并非如此。）

现在，人们普遍认识到，像GPT4这样的LLMs作为编程助手比作为事实查找助手更有效。

人们往往过度解读这一点，错误地将推理和泛化规划能力归因于LLMs，而不是理解这仅仅是因为GitHub和一般网络是完全不同的训练数据集。

实际上，一些研究（如Voyager）已经利用这一现象，让LLMs在规划和推理任务上表现得更好。这种方法是让LLM输出执行任务的代码，然后在模拟器（或非常宽容的游戏世界）中运行这些代码。由于泛化计划可以写成程序（已故的Drew McDermott曾说，规划只是自动编程，语言中的原语对应于可执行动作），如果GPT4能正确生成代码，那么它也能进行规划——这与我们的研究（例如https://x.com/rao2z/status/1726962530143412641?s=20）显示LLMs实际上无法在自主模式下进行规划的结果相矛盾。

那么，问题出在哪里呢？

有两个原因。首先，LLMs训练时所使用的代码数据的质量，其次是形式语言（如代码）的语法和语义之间的距离比自然语言要小。

首先，LLMs进行的近似检索质量（参见https://x.com/rao2z/status/1740692722099630237?s=20了解更多关于近似检索的信息）在很大程度上取决于LLMs训练的数据质量。对于自然语言，即使在排除了像4Chan这样的极端数据集之后，LLMs仍然训练了大量具有事实基础或生产这些数据的人类代理价值系统高度异质性的语言数据。毕竟，无论是平地论者还是疫苗否认者，都能提出同样精妙的自然语言文本。

相比之下，大多数LLMs的代码数据主要来自GitHub。大多数代码都是“工作的”（考虑到潜在雇主正在查看它！），软件工程师的价值系统异质性在GitHub上发布的代码类型中扮演的角色要小得多。

这就解释了为什么代码补全的质量比英语补全的质量更高。

这也解释了一种民间智慧，即当答案可以用英语或Python表达时，让LLM输出Python是值得的。（想象一下，你的LLM在英语的一般网络语料库上受过训练，但只在法语的医学期刊上受过训练。用英语向LLM询问一个医学问题，毕竟，会让LLM在医学期刊上进行近似检索，而不是在一般网络上！）

现在，虽然代码补全的质量很可能比英语补全的质量更高，但它仍然是近似检索——并且不能保证代码是正确的（这就是人们偶尔在推特上说他们花了多长时间在副驾驶为他们编写的看似好的代码中寻找邪恶的错误的原因。。。）

代码似乎比英语更经常工作的部分原因是（a）有一个增量解释器在旁边，可以标记明显的执行异常（从而吸引人类编码者的调试注意力）（b）语法正确的代码也是语义正确的机会，虽然不是保证的，但比语法正确的散文语义正确的机会要高。（毕竟，这是用形式语言表达知识的主要动机之一。。。）

在少数情况下，例如Voyager，研究人员声称生成的代码足够好，可以直接在世界中运行，仔细阅读表明，它们主要依赖于世界是宽容的和慷慨的ergodic！（参见https://x.com/rao2z/status/1679427518699380741?s=20）

（有时这种声明伴随着“我们在LLM将代码发送到世界运行之前，让LLM本身验证代码”——但正如我们在其他地方争论的那样（参见https://x.com/rao2z/status/1716257588768346328?s=20），没有理由相信LLMs可以自我验证！）

总结：LLMs输出比英语更好的Python质量更多地反映了在GitHub与一般网络之间近似检索的差异，而不是任何潜在的推理能力。

### 55

2024-01-08

https://x.com/xiaohuggg/status/1744179160434802963?s=20

小互
@xiaohuggg
Teachable Machine：一个由Google开发的机器学习工具

它允许用户快速、简单地创建自己的机器学习模型，而无需专业知识或编程技能。

你可以用它来教电脑识别图片、声音或人的动作。

使用这个工具的步骤很简单：

1、收集数据：你可以上传图片、录制声音或动作视频来作为训练数据。

2、训练模型：用这些数据来训练你的模型，然后测试它能否正确识别新的图片、声音或动作。

3、导出模型：完成训练后，你可以下载这个模型，或者上传到网上，用在其他项目中。

Teachable Machine提供了多种方式来创建机器学习模型，非常灵活和用户友好。

1、使用文件或实时捕捉示例：用户可以选择上传已有的图片、音频文件作为数据，也可以直接通过电脑的摄像头或麦克风实时录制视频、声音作为训练数据。

2、可以在本地完成训练：用户有选项不通过网络发送或处理数据。所有操作，包括数据的收集、模型的训练和应用，都可以在用户自己的电脑上完成，不需要将摄像头或麦克风收集的数据发送到互联网上。这对于隐私保护是非常重要的，特别是当处理敏感信息时。

3、Teachable Machine”生成的模型是真实的TensorFlow.js模型，可以在任何运行JavaScript的地方工作。此外，还可以将模型导出到不同的格式，以便在其他地方使用，如Coral、Arduino等。

开始训练：https://teachablemachine.withgoogle.com

### 56

2024-01-08

https://x.com/fi56622380/status/1744100621031272802?s=20

fin
@fi56622380
进入2024年，平板/手机终端LLM能力和半年前比，进步还是很明显的

半年前在iPhone/Galaxy上用GPU跑7B模型大概能到6 token/s，现在已经能接近20 token/s了

主要提升来自于两方面：一个是启用NPU优化提升到10 token/s，另外一个是新技术speculative decoding再提升一倍（原理如图）

NPU的优化主要是对带宽利用方面，压缩带宽之类的技术

speculative decoding则是巧妙的用一个小LLM先快速做一轮下一个单词的预测，然后用大LLM来同步验证，速度会快一倍，这个技术现在应用也很广泛了

下一次芯片LLM能力主要升级估计是一年半之后，毕竟从去年LLM大火开始构思新架构到面世，通常需要两年的时间

至于升级的部分，我猜测可能主要是带宽，这部分的升级对提升token数的作用是最大的

大胆预测一下，明年年底左右（2025年），随着各种芯片和各层底层软件的优化，我们应该可以看到LLaMa 3的7B模型在平板/手机/汽车上跑到40~50 token/s

那么7B就不再是手机终端的sweet point，也许2026之后会升级成主流13B的模型，占用8GB内存（感觉利好存储厂商）

那个时候的手机13B模型，可能会有今天GPT3.5的能力（现在最接近GPT3.5的小模型是Mistral 7X8模型），那就真的能做很多事情了
引用
fin
@fi56622380
·
2023年5月10日
总结了一个详细的现在各路神仙在手机上跑的LLM模型实测，拿去年的机型基本只够跑7 Billion参数的模型，大概占用4GB多内存

大部分都是直接拿CPU开多线程在跑没开NN加速，速度大概是每秒1~5 token之间

### 57

2024-01-10

https://x.com/dotey/status/1744980335182377156?s=20

宝玉
@dotey
我发现大家对于 ChatGPT 在文档对话支持方面的能力都普遍认为比较差，这篇分析相对比较靠谱：
1. OCR 能力不行，OCR 能力不行那从源头上的文字的输入就是有问题的，后续的召回和对话肯定好不了
2.上下文长度不够长，如果长度不够那么一次输入的信息就不够长，导致输出不够好
3. RAG 本身就是很复杂的技术，即使如 OpenAI 也不是那么多容易做好的

以下内容翻译自原推：
***

与 PDF 对话之难，及 ChatGPT 在此领域的不足 - 原因分析

目前最普遍的 GPT-4 应用之一是“文档/PDF 对话”功能。这被认为是 AI 聊天机器人的一项杀手级应用，因为要读懂内容繁多的文件是很烦人的事 —— 相比之下，直接让大语言模型帮你解析并总结内容显得更加简便。

然而，遗憾的是，当处理超过 10 页的 PDF 文件时，ChatGPT 的表现并不尽如人意。它所提供的总结往往过于简略且笼统，甚至在被要求提供更多细节时会直接拒绝。

造成这一问题的原因之一是，这不是一个简单的应用场景。

OCR - 有效的 OCR 技术是必需的，它需要能够精确解析表格和图像。但目前无论是免费的还是商业的 OCR 技术都难以做到这一点。大量商业和研究用的 PDF 文件中含有众多表格和图像。

上下文 - 尽管我们现在有 128K 上下文长度的大语言模型，但目前尚不清楚 ChatGPT 实际部署了哪种模型。如果你对一篇论文进行 OCR 处理后再输入其文本给 ChatGPT，它经常会出现错误。我怀疑 ChatGPT 服务的是一个上下文长度更小的模型。

快速 RAG - 实施一个简单的 RAG 处理流程，即将文档分块、嵌入、检索结果后再传递给大语言模型，可能是一个有效的解决方法。但目前的聊天机器人尚未具备这样的功能。

突出文档关键部分 - 理想的解决方案应当能够明确展示出答案来源于文档的哪些部分。这将极大地简化验证过程。

理想情况下，与 PDF 对话的功能应包含以上所有特点。似乎，如果一款独立的应用程序能够很好地实现这些功能，即使在应用商店中也能获得可观的收入。不过，我认为这并不适合作为一个获得风险投资支持的创业项目，更像是一个一两人小团队可以经营的小本生意，足以成为一种舒适的生活方式。

简言之，实现一个看似简单的“与 PDF 对话”功能，其实是一个复杂且难以做到极致的任务。

---

https://x.com/bindureddy/status/1744894481999278291?s=20

Bindu Reddy
@bindureddy
Chat With PDF Is Hard and ChatGPT Sucks At It - Here's Why

The most common GPT-4 wrapper is a "chat with a doc/pdf" app.  It is one of the killer applications of AI chatbots, as reading a dense document can be tiresome -- it's much simpler to ask the LLM to parse and summarize it for you.

Unfortunately, ChatGPT doesn't do a good job, especially regarding PDFS that are> 10 pages. It produces sparse generic summaries and flat-out refuses to elaborate further.

One of the reasons this is a non-trivial use case is as follows.

OCR - you need a really good OCR that can parse tables and images well. There is no free or commercial OCR tech that does this well. A lot of business and research PDFs have a lot of tables and images. 

Context - While we currently have 128K context-length LLMs, it's unclear what is deployed as part of ChatGPT. ChatGPT often throws an error if you run OCR on the paper and then feed it the paper text. I suspect a much smaller context length model serves ChatGPT requests.

Quick RAG - Implementing a naive RAG that chunks the doc, embeds it, retrieves results, and then passes it to the LLM will likely do the trick, but current ChatBots don't have that feature. 

Highlighting Doc Sections - The ideal solution should ideally showcase parts of the document where the response is retrieved from. This makes verification super simple.

Ideally, chat with PDF should have all these features. It seems like a standalone app in the app store can still make decent revenue if it does a good job of all these features. That being said, I don't think this is a venture-backed start-up. More like a 1-2 person mom-pop thing that can be a good lifestyle business.

TLDR: Doing something as simple as "Chat with PDF" is non-trivial and hard to do well

### 58

2024-01-10

https://x.com/xiaohuggg/status/1744921720073728186?s=20

小互
@xiaohuggg
Phi 2专家混合模型

它是结合了2 到4 个微调的microsoft/phi-2模型的专家混合体（Mixture of Experts, MoE）

灵感来源于mistralai/Mixtral-8x7B-v0.1架构。性能优于每个专家模型。

🤗 phixtral-2x2_8： https://huggingface.co/mlabonne/phixtral-2x2_8

🤗 phixtral-4x2_8：https://huggingface.co/mlabonne/phixtral-4x2_8

### 59

2024-01-10

https://x.com/dotey/status/1744784191370285161?s=20

宝玉
@dotey
Go 语言之父 Rob Pike 写的长文：在 Go 语言 14 年的发展历史中，我们做得对的和不对的

基于他去年在悉尼 GopherConAU 会议上所做的闭幕演讲的内容整理补充而成，系统的总结回顾了 Go 的发展过程中的经验教训，例如 Go 语言中并发、接口的设计；Go 语言作为一个开源项目是如何进行项目管理的，其中做的好的和不够的地方。

原文：https://commandcenter.blogspot.com/2024/01/what-we-got-right-what-we-got-wrong.html
译文：https://baoyu.io/translations/software-engineering/what-we-got-right-what-we-got-wrong

### 60

2024-01-10

https://x.com/geekbb/status/1744138343183749620?s=20


Geek
@geekbb
最近 GitHub 上有一款颇受欢迎的开源 ChatGPT 替代项目：《👋jan》100% 本地运行，支持Nvidia GPU & Apple M。一周时间增加 3066+ Stars，提供众多开源 AI 模型下载，每次对话选择一个模型后自动切换很方便，试了一下 Mixtral 8x7B 24GB 居然能跑，体验不错。GitHub https://github.com/janhq/jan


### 61

2024-01-10

https://x.com/reach_vb/status/1744084611464913002?s=20

Vaibhav (VB) Srivastav
@reach_vb
Whisper on MLX just got better! 🔥

Word-level timestamps + confidence scores and models on the 🤗Hub ;)

Don't forget to `git pull` before you get whisper-ing.

Kudos to 
@awnihannun
 & bofenghuang!

P.S. It now also supports Large-v3 \o/

### 62

2024-01-10

https://x.com/helloiamleonie/status/1744041380643561726?s=20


Leonie
@helloiamleonie
Let's start the year, right. 
It's 2024. You should know what vector embeddings are by now.

If you don't, here is everything you need to know about vector embeddings, neatly packaged by 
@vboykis
:

### 63

2024-01-10

https://x.com/SaitoWu/status/1743968384306618775?s=20

Saito
@SaitoWu
tolecen 算是国内殿堂级独立开发者了，白描竟然有 1500w 下载量。🫡

在少数派播客的采访中：他给独立开发者们在收费策略的建议我觉得非常实在，总结起来简单一句：“大胆收钱”！

👉🏻： https://podwise.xyz/dashboard/episodes/115205

### 64

2024-01-10

https://x.com/CoooolXyh/status/1743967523941576790?s=20


Yuhang
@CoooolXyh
自己和团队一直在使用 Linear 做项目管理，十分好用，但感觉还没有发挥出它的一半潜力 🥹
发现有大佬翻译了 Linear 团队写的系列文章，找时间好好学习一下：

[说明 | Linear Method 中文版](https://linear-method.cn/)

### 65

2024-01-10

https://x.com/xiaoying_eth/status/1743787542166794264?s=20

小樱💞｜实用工具分享
@xiaoying_eth
这个网站真是一个宝藏 http://pdfdrive.com，拥有近7700万本书籍，并且还在不断增长，包括期刊、杂志、指南等各种类型的书籍

最令人高兴的是，它支持PDF、epub和mobi格式的下载，而且没有烦人的广告和下载限制。这真是对于喜欢阅读的人来说是一个绝佳的资源

### 66

2024-01-10

https://x.com/oran_ge/status/1743802055377264845?s=20

orange.ai
@oran_ge
斯坦福大学做出来一个几乎没有幻觉的大语言模型：WikiChat
WikiChat 在与人类用户聊近期的一些话题时，事实准确性高达 97.9% ，比 GPT-4 高出 55.0%，用户评价也更高。WikiChat 7 通过以下7个步骤来减少模型幻觉，值得参考。
https://arxiv.org/abs/2305.14292

### 67

2024-01-10

https://x.com/KirkDBorne/status/1743706064720171186?s=20

Kirk Borne
@KirkDBorne
[Download 211-page PDF]
The conceptual #Mathematics behind #MachineLearning with example-based learning: http://arxiv.org/abs/2206.13446
—————
\#BigData #DataScience #AI #ML #LinearAlgebra #Calculus #Inference #Graphs #DirectedGraphs #DiscreteMathematics #Probability #Statistics

[[2206.13446] Pen and Paper Exercises in Machine Learning](https://arxiv.org/abs/2206.13446)

### 68

2024-01-11

https://x.com/dotey/status/1745284376106037579?s=20

宝玉
@dotey
微软新发表的一篇文章：《新的科学发现在几周内完成，而不是几年：AI 与高性能计算如何加速科学进步》

计算技术已显著推动了科学发现的进程。目前，科学家们认为，结合先进的 AI 技术与下一代云计算，正将科学探索的速度推向前所未有的新高。

微软与位于华盛顿州里奇兰的太平洋西北国家实验室 (PNNL) 正在协作，展示这种技术加速如何在化学和材料科学这两个关键领域实现突破，这对于寻找全球急需的能源解决方案至关重要。

PNNL 的科学家们正在测试一种新型电池材料，这是他们与微软合作，利用先进的 AI 和高性能计算 (HPC) 所取得的成果，仅用几周时间就实现了这一发现，而过去这可能需要数年时间。HPC 是一种云基础的计算方式，通过集成大量计算机来解决复杂的科学和数学问题。

图 1：PNNL 的材料科学家 Shannon Lee 正在混合原材料，合成一种新的固态电解质。这是通过 AI 和 HPC 工具，在 Azure Quantum Elements 服务中预测出的有前景的候选物之一。照片拍摄者：Dan DeLong，为微软所摄。

作为这项努力的一部分，微软量子团队 利用 AI 在短短几天内识别出约 50 万种稳定材料。

这种新型电池材料的发现，源于微软 Azure Quantum Elements 的应用。通过筛选 3200 万种潜在无机材料，仅用 80 小时就缩减到 18 个有潜力的候选者，这些候选者有望应用于电池开发。最重要的是，这一工作为快速解决紧迫的可持续性、制药等领域的挑战开辟了新路径，同时展示了量子计算将来可能带来的先进成果。

PNNL 的首席数字官 Brian Abrahamson 表示：“我们认为，在许多科学领域都有实现这一目标的可能。”“最近的技术进步为我们加速科学发现提供了新的机会。”

作为美国能源部下属的一个实验室，PNNL 在化学和材料科学等多个领域进行研究，致力于能源安全和可持续性。因此，它成为微软的理想合作伙伴，共同利用先进的 AI 模型探索新的电池材料候选项。

Abrahamson 进一步指出：“开发新型电池对全球而言极其重要。过去这是一个劳动密集型的过程。传统的人力规模下的材料合成和测试方式存在明显限制。”

通过试错学习

在材料合成领域，研究人员通常会首先阅读已发表的其他材料研究，来假设不同方法可能带来的结果。PNNL 材料科学小组的负责人 Vijay Murugesan 指出：“一个主要的挑战是，人们倾向于发表他们的成功案例，而非失败经历。”这就意味着科学家们很难从别人的失败中吸取教训。

接下来的传统科学步骤是对这些假设进行测试，这通常是一个漫长且反复的过程。Murugesan 表示：“如果实验失败了，我们就得重新回到起点。”他曾在 PNNL 参与的一个项目——钒液流电池技术，就花费了数年时间来攻克难题，设计出新的材料。

图 2：Vijay Murugesan 介绍说，利用微软的 AI 和高性能计算 (HPC) 工具，科学家们可以省去耗时的试错步骤，直接集中精力测试最有前景的候选材料。照片由 Andrea Starr 为 PNNL 拍摄。

按照传统方法，科学家们需要在过去的基础上寻求改进。另一种思路是在所有可能性中通过筛选来发现新事物。设计新材料需要进行大量的计算工作，而化学领域可能是量子计算的首批应用场景之一。Azure Quantum Elements 提供了一个专为化学和材料科学研究设计的云计算平台，它不仅着眼于未来的量子计算，而且已经开始在这些领域开展模型、工具和工作流程的研究。这些模型将为未来的量子计算机优化，但目前已经在传统计算机上有效推动科学发现的进展。

为了在实际应用中评估其进展，微软量子团队将重点放在了我们生活中随处可见的一个领域——电池材料。

AI 如何学习材料科学

微软首先训练了多个 AI 系统，使它们能够对各种可能的元素进行深入分析，并提出可能的元素组合。这个算法提出了高达3200万个候选方案，就像在茫茫大海中寻找一根针。然后，AI 系统筛选出了那些稳定的材料。另外一些 AI 工具则根据分子的反应性和导电潜力来进一步筛选候选物。

这个过程的目标并不是找出假想干草堆中的每一根针，而是尽可能多地找到优质的针。微软的 AI 技术成功将3200万个候选方案缩减到大约50万个主要是新型稳定材料，最终又筛选到800个。

Azure Quantum Elements 的产品负责人 Nathan Baker 说：“在模拟的每个步骤中，原本需要进行量子化学计算，现在我都用机器学习模型来代替。这样一来，我不仅能获得通过模拟得到的深刻见解和细节观察，而且模拟速度提高了高达50万倍。”

AI 虽快，但并非完全精确。接下来的一系列筛选过程采用了 HPC（高性能计算），它虽然精确度高，但也大量消耗计算资源。因此，它更适合用于筛选较少的候选材料。HPC 的第一轮验证使用密度泛函理论计算每种材料在各种可能状态下的能量。紧接着，结合 AI 和 HPC 技术的分子动力学模拟被用来分析每种材料内部原子和分子的运动情况。

通过这一过程，候选列表被缩减到了150个。最后，微软的科学家们又运用 HPC 来评估每种材料的实用性，如可用性、成本等，最终将列表缩减到了23个，其中5个已经是众所周知的材料。

得益于 AI 和 HPC 的强强联合，发现最有希望的材料候选只用了80个小时。

HPC 部分占了整个计算过程的10%，而且这还是在已经专门针对某些分子的情况下。这种高强度的计算是研究的瓶颈，即便是拥有超级计算机的大学和研究机构也面临同样的问题。这些超级计算机不仅不专门针对某一特定领域，而且还需要共享使用，因此研究人员可能需要排队等候。微软基于云的 AI 工具为这一问题提供了缓解。

广泛应用与云技术的便捷

微软的科学家们利用 AI 完成了约 90% 的筛选工作，主要占据了计算时间。接着，PNNL 的材料科学家进一步筛选，最终锁定了六种潜在材料。微软的 AI 工具不仅服务于电池系统，还能适用于各类材料研究，而且云技术使得这些工具随时可用。

亚伯拉罕森认为：“云技术大大提高了科研界的可接触性。”

图3：PNNL 的首席数字官布莱恩·亚伯拉罕森。照片由安德烈亚·斯塔尔为 PNNL 拍摄。

现在，微软提供的化学专用辅助工具和 AI 工具就像是一个强力磁铁，能够在无数候选材料中迅速找到潜在的目标，帮助科学家聚焦研究方向。“我们的目标是开发能生成新材料的 AI，只需输入期望特性，它就能列出新的电池化合物清单，”贝克说。

目前，项目进入了实操阶段。该材料已成功合成，并做成了能正常工作的原型电池，目前正在实验室中接受多轮测试。目前这种材料的制作仍处于手工阶段。PNNL 的材料科学家香农·李介绍说，首先要手工用研钵和研杵将材料的固态前体研磨，然后用液压机将其压成硬币大小的圆片。之后，将其放入真空管中加热至 450 至 650 摄氏度（842 至 1202 华氏度），再转移到密封盒中以隔离氧气和水，最后将其研磨成粉末进行分析。

李说：“对这种材料来说，10 多小时的制作过程算是比较快的。有时候制造一种材料可能需要一周甚至两周时间。”

接下来，需要对数百个电池进行测试，覆盖成千上万种不同的充电周期和其他条件，以及为了商业化使用测试不同形状和尺寸的电池。穆鲁格森梦想着开发一种化学或材料领域的数字仿真模型，“这样就不必亲自去实验室组装材料、制造电池并进行测试。你只需设定，‘这是我的负极，这是我的正极，这是电解质，我要施加这么多电压，’然后它就能预测所有部件如何协同工作。甚至可以预测，比如经过 10,000 次充放电周期和五年的使用，材料的性能将如何。”

微软目前正在开发数字工具，以加速科学研究过程的其他环节。

锂离子电池的传统研发过程就很能说明问题。锂作为电池组件在 20 世纪初开始受到关注，但直到 1990 年代，可充电的锂离子电池才真正上市。

如今，锂离子电池在我们的生活中扮演着越来越重要的角色，从手机到医疗设备，从电动汽车到卫星都离不开它。据美国能源部预测，到 2030 年，锂的需求 将增长五到十倍。锂本身已经相对稀缺，价格昂贵。开采锂在环境和地缘政治上存在问题。此外，传统的锂离子电池还存在安全隐患，可能会起火甚至爆炸。

因此，许多研究人员正寻找锂以及用作电解质的材料的替代品。固态电解质因其稳定性和安全性展现出很大的潜力。

惊人的成果

PNNL 的科学家们最近正在测试一种新发现的材料，这种材料同时使用锂和钠以及其他元素，显著减少了锂的使用量，可能减少了高达 70%。目前这项研究还处于初期阶段，其确切的化学配方还在调整之中，而且在大规模应用中可能会遇到挑战，Abrahamson 提醒大家。他强调，这个故事的焦点不是这种特殊的电池材料本身，而是发现这种材料的速度之快。科学家们认为，这次尝试本身就极具价值，并且带来了一些意想不到的发现。

这种由 AI 研发出的材料是一种固态电解质。在这种电解质中，离子可以在正极和负极之间来回移动，理想状态下几乎没有任何阻力。

试管中装有新材料的样本，看起来像细白的盐。

图4：这是微软 AI 和 HPC 工具发现的新固态电解质样本。与液态电解质相比，固态电解质在安全性方面有显著优势。照片由 Dan DeLong 拍摄，供微软使用。

之前，人们普遍认为在固态电解质系统中不能同时使用钠离子和锂离子，因为它们虽然电荷相似，但大小却不同。人们以为固态电解质的结构无法同时支持这两种不同离子的运动。然而，经过实验测试后，Murugesan 发现，“钠离子和锂离子似乎可以互相促进。”

新材料的另一个优势在于，Baker 表示，其分子结构自然形成了有助于两种离子移动的通道。

这种新材料的研究还在起步阶段，但 Abrahamson 说：“无论它将来是否能成为一种实用的电池，我们如此快速地找到一个有效的电池化学方案，这一点本身就非常吸引人。”

还有更多的发现有待探索。Murugesan 和他的团队还没有制作和测试微软模型所提出的大多数其他新材料候选。这一合作仍在继续，PNNL 的计算化学家们正在学习如何使用这些新工具，包括一个专门针对化学和其他科学领域训练的辅助系统。

Abrahamson 表示：“与微软和 PNNL 的合作是一场旨在加速科学发现的长期合作，我们利用这些计算模式的变革力量，专注于化学和材料科学，这是太平洋西北国家实验室的特色和优势。”

他补充说：“我们正处于一个关键时刻，这个时刻见证了人工智能模型的成熟、训练它们并发挥其作用所需的计算力，以及针对特定科学领域进行专门训练的能力。我们相信，这将开启一个加速科学发展的新时代。这非常令人振奋，因为这些问题对全世界都至关重要。”

原文：https://news.microsoft.com/source/features/sustainability/how-ai-and-hpc-are-speeding-up-scientific-discovery/

### 69

2024-01-11



### 70

2024-01-11



### 71

2024-01-11

