### 01

方军 2024-06-01

摘：OpenAI 分享他们在 RAG 技术的最佳实践：

客户需求：他们有大量文档（比如 10 万份），希望模型只基于这些文档进行知识检索。

解决方案：

1. 直接将 PDF 和 docx 文件嵌入，准确率是 45%。

2. 经过 20 次调优迭代，解决细节小 Bug - 准确率到 65%

3. 基于规则进行优化，譬如先判断问题属于什么领域（退一步思考），然后再回答，效果提升到 85%

4. 发现数据里有一些是结构化数据（如表格），为此定制提取解决，准确率提升到 98%。

[A Survey of Techniques for Maximizing LLM Performance - YouTube](https://www.youtube.com/watch?v=ahnGLM-RC1Y)

### 02

方军 2024-06-01

这个用户的体会蛮有意思的：经过我一上午的摸索，ChatGPT4O 和 4 对我的工作毫无帮助，都是坑货。

我把一个危重病人的连续 5 天的化验结果导出成 PDF（打印出来需 24 页），然后让 GPT 抓取其中的指标按时间排列，来辅助表现病情变化。先抓取了 4 个指标：

白细胞、中性粒细胞、血红蛋白、C 反应蛋白。

抓取缺项漏项很多，我只能把大量的检验结果生成成文字，然后复制给 GPT，再让它抓取。

它不仅会无中生有，还会误判时间，漏掉最低值和最高值，把有些值乱放。

多次调教，但是同样性质的错误，这里改对了，那里还是错。

列个表而已，基本上也是瞎写一气！

有这个摸索的功夫，我用眼睛看数据，手输 excel 也输完了。

当然了，没有白费功夫，起码下个月不用花钱买号了。

方军：简单说，这个人用法不对，PDF 不对，后面结构化数据也不对，这个任务甚至都不应该依靠 LLM，也许应该用 LLM 的 functioncall + 代码生成

2024-06-01 13:24

### 03

方军 2024-06-01

有意思的类比，摘：试过几次就知道了，AI 总结出来的东西就跟甘蔗渣一样索然无味，一点营养价值都没有。

这些天又相继看到了一些 AI 写作的工具，据说微博也在策划推出 AI 辅助写作，更加感觉无语。写作是思考的载体，让 AI 来写作相当于让 AI 代替你去思考，这怎么能行？

有人说，AI 只是起到辅助作用，比如列一下大纲啊，提供一些表达参考，文字风格润色之类的，最终还是需要作者本人来把控输出质量的。

但我觉得，这些步骤都是写作过程中必不可少的环节，属于是作者本人必须要经受的「必要困难」，不能假以援手。如果长期把这些工作外包给 AI，写作能力必然会遭遇退化。

从阅读到写作，如果这种端到端的知识工作流程都被 AI 侵蚀的话，很难不怀疑在之后 AI 普及的时代里，大部分人的大脑会有一定程度的退化。

都说语言是思维的载体。一个很少有人想过的逻辑是，互联网大厂们在使用各种语料训练 AI，但反过来 ，AI 产出的语料又在训练广大的互联网用户。

### 04

方军 2024-06-01

为什么说这个是胡说？因为它一开头给了一个巨大的定论，是对还是错的呢？根本说不明白。然后下面一堆，正确和错误、似是而非混在一块，主讲一个乱七八糟，真要辩论，就要它拆解开，一点一点地辨别，累啊。这也是为什么我频繁吐槽讨论 AGI 的人，你倒是先定义下再说啊。

以下这个人说的话就是典型的 LLM 胡说，摘：

LLM 本质上是文字接龙，是大号检索，只不过因为大力出奇迹，它训练的数据够多参数够多，所以他接龙的文字显得很聪明，但它其实并不知道自己在说什么，他并不真的具备「智能」。一个很简单的例子，让它做稍复杂一点的数学运算都会算错，连个计算器都比不上。

所以 LLM 通过各种插件在补充自己的能力，比如 TTS、文生图、生成编码、function call、RAG。但这些都只是手脚，还不是大脑，真正离大脑接近的，或者说至少看起来是「智能」的部分是：prompt 中的引导和 ReAct，这个是最大化人工智能潜力的关键。LLM 模型本身强大是必要的，再就是我们的引导了，两者缺一不可。

workflow 其实不是智能，感觉没有 LLM 也能实现 workflow，比如 stable diffusion 也有 ComfyUI，它只是自动化脚本，无关智能。所以我在看 LangChain 的 multi-agent 解决方案 Langgraph 时，总觉得怪怪的，既然 action 准备好了，agent 也准备好了，edge 究竟是什么呢？和直接编程写 if else 有多大区别？如果不这么设计，又该如何设计呢？我没找到答案，只是隐隐觉得哪里不对。。。

prompt 和 ReAct 才是「智能」的宝藏。类似 OCR 和 TTS 的传统小模型不是，非 LLM 的 AIGC 不是，基于 LLM 基座做垂直大模型不是，多模交互不是，workflow 也不是。同样是 AIGC，同样是大模型，为什么 chatGpt 比 midjourney 受关注得多？就是因为文字类的 AIGC 恰巧表面上看起来具备「推理能力」，正是这种推理能力才让 ReAct 成为可能，借助 function call 又有了从「想」到「做」的能力。这是 LLM 得天独厚的优势。

方军：定义一个大词，是个艰难的任务。

不试图定义，就容易多了。

你管 workflow 是智能干啥，你看它能不能满足一个你具体的任务，完成得好不好，是不是容易多了

看着过于务实，但这是可有效讨论的范围啊。

2024-06-01 18:23

方军：LLM 的胡说某种意义上，还比这个好些，因为目前 LLM 在不有意指令的情况下，会相对聚焦，不会太分散

2024-06-01 18:27

### 05

方军 2024-06-02

引发认真的讨论啊，摘：（宝玉）看到 @Summer 最聪明医生这条被 ChatGPT-4o 坑的微博，尝试从技术角度解读一下，这样也许能更好的理解现阶段大语言模型的优缺点，在实际应用中能扬长避短。

首先回顾一下原博文中的用法，以保证我们讨论的是一件事，避免因为我的误解而错误解读。

「病人连续 5 天的化验结果，24 页 PDF 发送给 ChatGPT，让其抓取 4 个指标（白细胞、中性粒细胞、血红蛋白、C 反应蛋白），按照时间顺序排列」

结果：「抓取缺项漏项很多」

然后：「把大量的检验结果生成成文字，然后复制给 GPT，再让它抓取」

结果：「它不仅会无中生有，还会误判时间，漏掉最低值和最高值，把有些值乱放。」

这个结果确实不尽人意，然而对于现阶段的大模型来说，也不奇怪，这里面涉及几个大模型的短板：

1. 上下文窗口长度不够长

2. 纯文本难以表达和解析结构化的数据

3. 推理能力较弱，需要通过 Prompt 引导

首先说上下文窗口长度不够的问题

每一次和 LLM 的交互，输入和输出的长度是有限制的，以 ChatGPT-4o 为例，上限是 32K Tokens，也就是输入和输出加起来大约是 2 万左右的汉字或英文单词，大约 50 页。看起来还不少，但是每次交互的上下文内容越多，生成质量会下降，成本也会急剧上升。就好比我们做阅读理解，一次阅读一小段文章和几页文章的效果是完全不一样的。

所以当你一次给 GPT 24 页的 PDF，并希望快速得到你想要的内容，这很可能超出了上下文窗口长度，或者说过长影响了生成效果。如果能减少输入的内容可能会效果更好一些。

然后就是文本格式的问题

现在大语言模型主要是以文本信息为主，像 GPT-4o 属于多模态，也就是还能支持图片、视频和音频。但是对于一些复杂的格式，比如图表、表格可能就效果没那么好了。

如果用户输入的是 PDF，那么通常会将 PDF 转换成纯文本，然后再进一步和大语言模型交互，这就意味着像化验结果这种包含图表、表格的数据，在转换成纯文本的过程中，会丢失很多有效信息，最终会影响生成结果。

即使后来通过复制粘贴再次输入，对于表格这样的数据，在复制粘贴的过程中，一样会丢失掉其格式，而一旦失去行列关系，大语言模型是很难从中解析出来有效的信息。

目前大语言模型有几种方案可以比较好的表达结构化的数据：

1. Markdown，Markdown 可以表达表格格式

2. JSON，JSON 支持数组、对象等复杂格式的表达

3. XML，XML 类似于 JSON，也能表达复杂的数据结构，只是冗余较多

4. 其他格式，例如 YAML、HTML、TypeScript 的 Type 等等

通常所有大语言模型对于 Markdown 都很友好，GPT 对于 JSON 支持更好，而 Claude 对 XML 支持更好

最后就是推理能力

所有复杂的任务都需要一定的推理能力，比如医生交代实习生：「抓取 4 个指标（白细胞、中性粒细胞、血红蛋白、C 反应蛋白），按照时间顺序排列」，实习生会将其拆分成若干个子任务：

1. 找到所有的指标信息

2. 过滤出其中的 4 个指标

3. 对结果排序

但是这个对于大语言模型来说，还很难，哪怕推理能力最强的 GPT-4（依然是比 GPT-4o、Claude 3 和 Gemini 更强），这任务也不一定能做得很好，但稍弱一些的模型几乎是做不到的。

不过，这类复杂的推理任务，如果借助提示工程，也就是在 Prompt 里面，把要求改一下：

「接下来你按照以下步骤帮我抓取指标信息，并打印每一步结果：

1. 列出所有 XXX 指标的信息，以 Markdown 的表格格式显示

2. 仅列出其中包含白细胞、中性粒细胞、血红蛋白、C 反应蛋白的指标信息，以表格格式显示

3. 对结果按照时间排序「

（根据需要 1-2 步也可以合并，但是打印第一步可以知道是否第一步就出错了）

这样的话，通常会更容易得到更好的结果，这其实也就是提示工程中的 CoT（思维链），也就是将复杂的任务拆分成一步步来执行。

上面列的三点，主要是从技术的角度来指出了问题，以及可能的改善方法。回头最开始医生使用 AI 来辅助查看报告这个应用场景，如果想能真正有用，有没有方法可以改善呢？

按照我的经验，是有一些事情可以做的，尤其是如果这是高频的应用场景的话，做好了可以极大的提升医生的效率，但需要做的却超出医生的控制范围，因为这可能需要整个医疗 IT 系统的升级。

现阶段的 AI 应用，还远没有达到 AGI （通用人工智能）的程度，需要从整个工作流上来配合，将 AI 作为整个工作流的重要一环，但是上游和下游有其他应用程序的配合。

首先是报告的输出格式，能输出 LLM 友好的格式，比如支持 Markdown 格式，将表格和图表都用 Markdown 通用格式表达，有利于 LLM 解析。

然后就是预先对报告内容分类，以减少上下文窗口大小，比如可以将医生关心的指标信息单独提取成 Markdown 表格，就不需要 LLM 大海捞针一样从整个报告去提取

再有就是对于 LLM 返回的格式可以有程序二次处理，比如说如果数据很多，让大语言排序其实是做不到的，但是结构化的数据让程序排序，反而很简单。

最后再总结以下：

LLM 擅长处理和生成文本，但是受限于上下文窗口长度，并且对于复杂的数据格式，需要用格式化的数据格式表达，并且要借助提示工程将复杂的任务进行拆解。

如果要用好 LLM，最好是充分利用 LLM 的长处，把一些高频的使用场景，但是原本不适合或者无法自动化的工作流的，借助 LLM 的文本处理能力，变成自动化的工作流。

方军：赞 //@你是少年呀：和最近教授教我们做文献综述的观点完全一样！ 1. 注意 token 数 2. 让 ai 分批输出效果会更好 3. 分步骤分步骤，让 ai 辅助的关键首先是自己明白工作流是什么，而不是无脑输出自己的问题，

另外和 ai 对话一个好处是你可以通过打字的方式缕清自己思路毕竟有的时候混沌状态有些人连问题都提不好

2024-06-02 13:46

### 06

方军 2024-06-02

有意思的体会，摘：一个可能比较反直觉的情况。

从翻译这个行当诞生一直到现在，译者的工作性质没什么是指进化。发明电脑也好、CAT 工具也好，都没有在根本上简化译者的工作。如果说唐僧那种纸笔一个个字写的方式是原始人手工打制石头轮子，21 世纪的译者就是手工磨制石头轮子，旧石器时代到新石器时代，额外又发明了一些圆规之类可以提升产品质量但并没有减少弱智级笨劳力的玩意儿。

译者最大的敌人其实就是：

打字

工作中占时间最多的就是打字

导致肉体磨损最多的也是打字

产生人为错误最多的还是打字

直到大语言模型出现之前，哪怕再直白粗浅的套话废话口水话，也得一个个字打出来。之前的机翻引擎理解力太差，省不了多少工夫。

所以从 ChatGPT 发布以来这一年多我特别高兴，仿佛一个原始人忽然跨进了 21 世纪。大约六成笨活我指导 GPT 干（哪怕连续让它改五遍也比我自己从头到尾手打一遍省手省心），另外它从根源上杜绝了低级笔误，完工之后清场擦屁股省了很多力。擦屁股这个环节听着不重要，实际谁干活谁知道。三五万字的东西翻译到最后已经快要油尽灯枯了，根本没心思擦屁股。放着歇几天再擦吧，就更不想动手了。忍着疲惫强行硬擦结果就是擦不干净。一开始拉的时候就不脏腚最好了。

确实！体力劳动的部分大大减少了！[哈哈]

---

方军：一年过去了，我因为不是翻译，所以翻译工作只试验过一回，就是翻译 ChatGPT 超入门那本书。

这一年 ChatGPT 等等大模型在翻译上给我带来的变化是：

我现在已经习惯了以中英对照的形式看资料。

以前的彩云小译、DeepL 也可以勉强做到，前者可以直接对照，后者需要我们自己开两个文档。

有了大模型后，翻译变得更为普遍。（但实际上有没有变好也很难讲，普遍很容易评估，好坏太难评估。）

另外对比而言，AI 总结、AI 解释，实际上效果都不如翻译 + 对照，逻辑上可用，实际上用的比例少得多。我个人的经验教训是，它们的效果比翻译又差了不少。

2024-06-03 08:16

方军：另外，以我有限的体会，这个译者的工作流程可能不是最高效的

他的逻辑中，似乎是反复给 AI 提要求，由 AI 生成文本

有人喜欢这种反复沟通的方式

同时，他认为 AI 翻译消除译文中的错误，减少后续的检查、修正

我的体会恰恰相反，我要的是 AI 一个（或几个）大体上的翻译，然后人工再输入一遍文字

由此，得到一个还可以的翻译

再之后，各项审校、校对、简单重复看还是一个不能少

这样保证结果。最大差异在于，我认为「擦屁股」是提高质量的必要条件。

2024-06-03 08:46

### 07

方军 2024-06-03

一年过去了，我因为不是翻译，所以翻译工作只试验过一回，就是翻译 ChatGPT 超入门那本书。

这一年 ChatGPT 等等大模型在翻译上给我带来的变化是：

我现在已经习惯了以中英对照的形式看资料。

以前的彩云小译、DeepL 也可以勉强做到，前者可以直接对照，后者需要我们自己开两个文档。

有了大模型后，双语对照翻译变得更为普遍获得。（但实际上有没有变好也很难讲，普遍可得、使用频次很容易评估，好坏太难评估。）

另外对比而言，AI 总结、AI 解释，实际上效果都不如翻译 + 对照，逻辑上可用，实际上用的比例少得多。我个人的经验教训是，它们的效果比翻译又差了不少。

大模型的确消除了语言障碍，几乎完全消除了。

### 08

方军 2024-06-03

ChatGPT Edu 主要特点为了在这些应用基础上继续前进，ChatGPT Edu 为大学提供了一个大规模引入 AI 的可行选择，主要特点包括：

使用旗舰模型 GPT-4o，擅长文本解读、编程和数学；

具备数据分析、网页浏览和文件摘要等高级功能；

能够创建和分享定制 GPT 版本；

消息发送限制显著高于免费版 ChatGPT；

提升语言能力，支持超过 50 种语言；

强大的安全性、数据隐私和管理控制措施，如组权限、单点登录（SSO）、系统用户管理（SCIM）和 GPT 管理；

会话和数据不会用于训练 OpenAI 模型；

[速递｜OpenAI 推出 ChatGPT Edu！AI 正式进入高校](https://mp.weixin.qq.com/s/F2e6rnJQGuMUHQjIIqyifQ)

### 09

方军 2024-06-03

昨天也很感慨，为什么要折腾：看一个最基本的知识看到头晕脑胀，也许有人说，瞎搞明白它干啥，这相当于，我们有汽车开就好了，搞明白发动机的原理干啥，不过就跟早上茶话会聊天一个朋友说的，搞明白有乐趣啊，搞明白才放心啊，就是这么个道理吧

ChatGPT 等大模型对这个很有帮助（当然，真到深处的时候，帮助逐渐减弱，我昨天就到了极度减弱的区间了）

摘：群里有位家长最近在研究高中语文，有个特点：总是在一段时间后，在群里写一段自己近期的研究感悟，然后就又会杳无信讯一段时间……

其实，研究差距并不是知识量，尽管知识量本身差距很大，最重要的一点是「研究的习惯」。碰上一个问题，普通人可能上百度查一下就完事了，了解个梗概对于大部分人来说就满足了；如果碰上稍微复杂点的问题，普通人想不出来或者找不到解决办法，也就束手无策了。但是受过系统训练的人更加习惯用多种方式去深入把一个问题研究透，查期刊论文，查英文资料，总之要把问题彻底弄清楚。

为啥我们说要终身学习，不是因为知识有多值钱，而是说把自己的脑子变成一个讲道理能吸收新东西的容器，这样机会到来的时候不是本能地去忽视，而是研究下这玩意到底是啥。

所以教育本身是个人人经历的事，但是只有极少数人真正能从中受益，这是一道窄门。

### 10

方军 2024-06-03

两年前媒体对马毅老师的访谈：

[专访 UC 伯克利马毅：一场为了探究智能本质的个人战争](https://mp.weixin.qq.com/s/f0R4jzThxe8sapfxQ4MA3A?v_p=90&WBAPIAnalysisOriUICodes=10000001&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10E5293010)

### 11

方军 2024-06-03

这次赠送这本书，《成为提问工程师》，讲如何向 AI 提问，其实写完我也没怎么看，拖了很久到元旦出版后翻了一下，当时觉得不过时。

这次晚上稍微仔细看看，虽然大模型技术和产品进步很快，但我们当时抓住了一些真正有长效的骨干，现在看不过时、很有用，难得。

这个方向的书出版我都买了翻看过，自夸一下，绝对是真正的好书（之一我觉得不用加偷笑），骨架是有效的，普通人看得懂，没有一点任何瞎凑材料的嫌疑。

当然，现在我手上的经验教训、方法论又多了好多，但不矛盾，当时划定的层次的问题解答得很好。同时，现在手上这些经验教训，很难有机会再编写成书，多半长期以电子资料形式在着。

如果你说骨架就应该是十个使用场景，那我的看法有些不同，我认为那样的骨架是经不住时间的考验的。

---

这似乎是一个值得使用的技巧：

Prompt ：添加了一句「如果有需要了解的信息，你可以主动向我提问。」。

AI 会针对上下文中，不明确的信息进行主动提问。我们给出答复之后，消除模糊点，获得更高质量解答。

---

评论区中，乘风说：对于团队中的很多小白我都不知道给他们推荐什么书什么课入手因为需要学习的东西是太多了

我分享两个例子：

第一个例子，老同事去年 3 月说，你来给我们分享下，我说，分享啥，告诉同学们（她们现在业务包括线下店铺、文创商品、MOOK 出版物），你们不用就被 AI 取代了，看看海报，看看文案，看看文字处理流程。

第二个例子，还是老朋友问，怎么让手下人用？

他自己用我耐心指导了。但我跟他说，你已经知道怎么用了，别试着教了，让大家用，不用人落伍了，就不在你团队了。

以上这两个因为都是私底下的好友场景，不是台面上非要教人什么，所以都很直接。

对于这样一个直接可用的技术产品，用就是了。成年人，不要培训，靠选择。成年人的知识传递，真的要放弃助人情节微笑

早上与朋友说起费曼，我们说起冯诺依曼对费曼说的，你不要觉得要对世界负有什么样的责任。（这个引用不准确，大意如此）。朋友说，「（这样凭着兴趣好奇心去做）最后对社会的意义才是更大的 。」

### 12

方军 2024-06-03

AK 似乎已经多次重复此观点了，都是有具体实践支撑的：

数据集的质量至关重要

他在视频里面吐槽过 OpenAI 的数据处理

### 13

方军 2024-06-03

这几篇，AI 与教育，可真难读，推荐各位读可汗的书吧

[人工智能、教育与教育者的未来](https://mp.weixin.qq.com/s/nFFeWTCJ_bDJfkuljDbzAQ)

[ChatGPT 与教育的未来](https://mp.weixin.qq.com/s/P-XNmv_WUTxEZ3oa0RL-JQ)

[2 万字对话实录：剖解 ChatGPT 时代的教育难题](https://mp.weixin.qq.com/s/Uebp0Oz6De1tfeWXy5I7gA)

### 14

方军 2024-06-04

091 大模型作为学习工具：解释

讲实话，对我来说，用大模型工作并不多，主要用于学习。当然，你说写代码，大模型发挥的作用是工作，还是学习呢？结果上是工作，实际上是学习。

只有在学习的时候，我才发现大模型特别有用。记录下刚刚的经历：

为了理解一段逻辑和代码示例。

第一步，文字翻译是基本操作。

第二步，请 AI 逐句解释这一段代码。（近乎于汇编性质的代码了。）

第三步，将每个代码和堆栈的变化图示出来，进行理解。（第二步的解释在这里起了作用。解释里有很多错，但不影响理解。）

发现代码在纸面上的运行不对。

第四步，查看另一个同样目的的代码，但更容易理解的（同样请 AI 逐行解释）。这一版代码我是能理解的。

第五步，在专业群向朋友们请教：第三步那种情况（理论结果与尝试运行结果不一致）是怎么回事？

有朋友提示，这儿有个试验用的工具，能够反映堆栈的情况，你可以把代码拷贝进去运行一下，可以更直观地看。

第六步，运行第四步代码，如所预测是一致的。

运行第二步代码，发现了一个概念理解上的不对，了解该代码能够实际生成想要的结果。

第七步，通过如上代码的运行，回到宏观去理解这个代码所要实现的设计，它的所谓 bit commitment 是有效的。

从如上可看出，过去在没有大模型的情况下，第二步（以及第四步中）逐一解释是很难的，我们必须向老师求助，等待他们的解释。现在，我们可以请大模型来为我们作解释。

这几乎是我个人理解大模型在学习中发挥作用的主要方式。

简言之，我对当前大模型能力的认识，在足够细节的问题上，它能够产生非常聪明的结果，但这些结果包含错误。这带来的后果是，如果不是有强的知识背景、不是有探究能力的人，其实是没有办法把现在的大模型用起来的。

### 15

方军 2024-06-04

跟大模型说话要不要礼貌，这是个问题。

个人认为其实的原因还是问题的精确性，而非有没有请、您这几个字。而且那些乱七八糟的不管讨好还是恐吓实际上减少 prompt 的有效性，而回答是 prompt 的后续预测。

摘：前天看到一个贴文，是关于大模型提示词的几十个技巧，第一个就讲对大模型说话无需礼貌，其实这个观点不太准确。

[1]、今年 2 月有一个研究就专门探讨了提示词礼貌程度对大型语言模型（LLM）性能的影响（链接：https://arxiv.org/abs/2402.14531），涵盖了英语、中文和日语多语种，评估不同礼貌程度对 LLM 在摘要、语言理解基准和刻板偏见检测等任务中的表现。

[2]、使用的数据集包括：

1、使用 CNN/Dailymail（英语)、XL-Sum（中文和日语）进行摘要任务；

2、使用 MMLU（英语)、C-Eval（中文）和 JMMLU（日语）进行语言理解基准测试；

3、使用 CrowS-Pairs（英语)、CHBias（中文）和 Kaneko 等人（2022）的日语子集进行刻板偏见检测。

[3]、模型包括：

GPT-3.5-Turbo、GPT-4、Llama-2-70b-chat（英语)、ChatGLM3-6B（中文）和 Swallow-70b-instruct-hf（日语)。

[4]、最终的结论是，不礼貌的提示是有概率会导致模型性能下降的，包括生成错误、偏见增加和信息遗漏。不过，过于礼貌的语言也不代表更好的结果。也就是说，你可以没有那么礼貌，但是你绝对不可以没有礼貌。

[5] 那到底应该礼貌到什么程度呢？研究团队写了八个不同礼貌程度的提示词（从 8 到 1，由礼貌到不礼貌），我读了一下论文，发现 3-5 礼貌程度的用语是结果比较好的，主打一个「不卑不亢」。

8、可否请您评价这句话呢，您可以只用（积极中立消极）的其中之一来回复。您无需输出其他任何内容。

7、可否请您评价这句话呢，请您只用（积极中立消极）的其中之一来回复，无需输出其他任何内容。

6、请您评价这句话，请只用（积极中立消极）的其中之一来回复，不必输出其他任何内容。

5、请评价这句话，请只用（积极中立消极）的其中之一来回复，不要输出其他任何内容。

4、评价这句话，只可用（积极中立消极）的其中之一来回复，不可输出其他任何内容。

3、现要求你评价这句话，仅允许用（积极中立消极）的其中之一来回复，不得输出其他任何内容。

2、你给我评价这句话，只准你用（积极中立消极）的其中之一来回复，别让我看见其他任何内容。

1、给老子评价这句话。只准你用（积极中立消极）的其中之一来回复，让我看见其他任何内容你就死定了。

### 16

方军 2024-06-04

这篇文章非常棒：

[10 Ways to Be Data Illiterate (and How to Avoid Them) - jxnl.co](https://jxnl.co/writing/2024/06/02/10-ways-to-be-data-illiterate-and-how-to-avoid-them/)

\# 10 Ways to Be Data Illiterate (and How to Avoid Them)
10 种成为数据文盲的方法（以及如何避免它们） 

文章太长发不出来，删节数个。

\## Ignoring Data Quality 忽略数据质量 

Data quality is the foundation upon which all analyses and models are built. Failing to assess and address issues like missing values, outliers, and inconsistencies can lead to unreliable insights and poor model performance. Data literate AI engineers must prioritize data quality to ensure their work is accurate and trustworthy.  
数据质量是构建所有分析和模型的基础。未能评估和解决缺失值、异常值和不一致性等问题可能导致不可靠的洞察和模型性能不佳。具备数据素养的 AI 工程师必须优先考虑数据质量，以确保他们的工作准确可靠。

**Inversion**: Assess and address data quality issues before analyzing data or building models. - Conduct exploratory data analysis (EDA) to identify potential quality issues - Develop and implement data cleaning and preprocessing pipelines - Establish data quality metrics and monitor them regularly  
反转：在分析数据或构建模型之前评估和解决数据质量问题。- 进行探索性数据分析（EDA）以识别潜在的质量问题- 开发并实施数据清洗和预处理管道- 建立数据质量指标并定期监测。

\## Not Visualizing the Data不可视化数据 

Not visualizing your data can lead to missed insights, poor understanding of patterns and relationships, and poor communication of findings to others. AI engineers must learn the basics of visualizing data to better understand it, grok it, and communicate it.  
不可视化您的数据可能会导致错过洞察、对模式和关系的理解不足，以及对他人发现的沟通不畅。AI 工程师必须学习数据可视化的基础知识，以更好地理解、掌握并传达数据。

**Inversion**: Learn how to visualize data to explore, understand, and communicate the data. - Start with basic visualizations, such as histograms and box plots to understand distributions - Then, consider advanced techniques such as [PCA] or [t-SNE] to discover complex patterns - Don't let the visual hang on its own—provide a logical narrative to guide the reader through it.  
反转：学习如何可视化数据以探索、理解和传达数据。- 从基本的可视化开始，比如直方图和箱线图，以理解分布情况- 然后，考虑高级技术，比如 PCA 或 t-SNE，以发现复杂的模式- 不要让视觉单独存在，要提供一个逻辑叙述来引导读者理解。

\## Only Relying on Aggregate Statistics 仅依赖聚合统计 

（略）

\## Lack of Domain Understanding缺乏领域理解 

Analyzing data without sufficient context can result in misinterpretations and irrelevant or impractical insights. AI engineers must develop a deep understanding of the domain they are working in to ensure their analyses and models are meaningful and applicable to real-world problems.  
分析数据时如果缺乏足够的背景信息，可能会导致错误解读和无关或不切实际的见解。AI 工程师必须对他们所从事的领域有深入的了解，以确保他们的分析和模型是有意义的，并且适用于现实世界中的问题。

**Inversion**: Develop a strong understanding of the domain and stakeholders before working with data. - Engage with domain experts and stakeholders to learn about their challenges and goals - Read relevant literature and attend industry conferences to stay up-to-date on domain trends - Participate in domain-specific projects and initiatives to gain hands-on experience  
反转：在处理数据之前，先对领域和利益相关者有深入的了解。-与领域专家和利益相关者合作，了解他们的挑战和目标-阅读相关文献，参加行业会议，了解领域趋势-参与领域特定项目和倡议，获得实践经验

\## Improper Testing Splits不正确的测试分割 

Inappropriately splitting data can lead to biased or overly optimistic evaluations of model performance. Data literate AI engineers must use appropriate techniques like stratification and cross-validation to ensure their models are properly evaluated and generalizable.  
不恰当地分割数据可能导致模型性能评估存在偏见或过分乐观。数据通晓的 AI 工程师必须使用适当的技术，如分层抽样和交叉验证，以确保其模型得到适当评估并具有泛化能力。

**Inversion**: Use appropriate data splitting techniques to ensure unbiased and reliable model evaluations. - Use stratified sampling to ensure balanced representation of key variables in train/test splits - Employ cross-validation techniques to assess model performance across multiple subsets of data - Consider time-based splitting for time-series data to avoid leakage and ensure temporal validity  
反转：使用适当的数据分割技术，确保模型评估不偏不倚且可靠。- 使用分层抽样确保关键变量在训练/测试分割中平衡表示- 使用交叉验证技术评估模型在数据的多个子集中的性能- 考虑基于时间的数据分割，避免信息泄漏并确保时间有效性

\## Disregarding Data Drift 忽视数据漂移 

Ignoring changes in data distribution over time can cause models to become less accurate and relevant. AI engineers must be aware of the potential for data drift and take steps to monitor and address it, such as regularly evaluating model performance on new data and updating models as needed.  
忽视数据分布随时间变化可能导致模型变得不够准确和相关。AI 工程师必须意识到数据漂移的潜在可能性，并采取监控和解决的步骤，比如定期评估新数据上的模型性能，并根据需要更新模型。

**Inversion**: Monitor and address data drift to maintain model accuracy and relevance over time. - Implement data drift detection methods, such as statistical tests or model-based approaches - Establish a schedule for regularly evaluating model performance on new data - Develop strategies for updating models, such as retraining or incremental learning, when drift is detected  
反转：监控和解决数据漂移，以保持模型的准确性和相关性。- 实施数据漂移检测方法，如统计测试或基于模型的方法- 建立定期评估新数据上模型性能的时间表- 制定更新模型的策略，如在检测到漂移时重新训练或增量学习

\## Confusing Correlation with Causation混淆相关性与因果关系

Mistaking correlations for causal relationships can lead to incorrect conclusions and poor decision-making. Data literate AI engineers must understand the limitations of correlational analyses and use appropriate techniques like experimentation and causal inference to establish causal relationships.  
将相关性误认为因果关系可能导致错误的结论和糟糕的决策。具备数据素养的 AI 工程师必须理解相关分析的局限性，并使用适当的技术，如实验和因果推断，来建立因果关系。

**Inversion**: Understand the difference between correlation and causation, and use appropriate techniques to establish causal relationships. - Use directed acyclic graphs (DAGs) to represent and reason about causal relationships - Employ techniques like randomized controlled trials (RCTs) or natural experiments to establish causality - Apply causal inference methods, such as propensity score matching or instrumental variables, when RCTs are not feasible  
倒置：了解相关性和因果关系之间的区别，并使用适当的技术建立因果关系。- 使用有向无环图（DAGs）来表示和推理因果关系- 使用随机对照试验（RCTs）或自然实验等技术来建立因果关系- 当随机对照试验不可行时，应用因果推断方法，如倾向得分匹配或工具变量。

\## Neglecting Data Privacy and Security忽视数据隐私和安全

Mishandling sensitive data can breach trust, violate regulations, and harm individuals. AI engineers must prioritize data privacy and security, following best practices and regulations to protect sensitive information and maintain trust with stakeholders.  
处理敏感数据不当可能会违反信任、违反法规并伤害个人。 AI 工程师必须优先考虑数据隐私和安全，遵循最佳实践和法规，以保护敏感信息并与利益相关者保持信任。

**Inversion**: Prioritize data privacy and security, following best practices and regulations. - Familiarize yourself with relevant data privacy regulations, such as GDPR or HIPAA - Implement secure data storage and access controls, such as encryption and role-based access - Conduct regular privacy impact assessments and security audits to identify and address vulnerabilities  
反转：优先考虑数据隐私和安全，遵循最佳实践和法规。-熟悉相关的数据隐私法规，如 GDPR 或 HIPAA-实施安全的数据存储和访问控制，如加密和基于角色的访问-进行定期的隐私影响评估和安全审计，以识别和解决漏洞

\## Overfitting Models过拟合模型 

（略）

\## Unfamiliarity with Evaluation Metrics对评估指标的陌生感 

Misunderstanding or misusing evaluation metrics can lead to suboptimal model selection and performance. AI engineers must have a deep understanding of various evaluation metrics and their appropriate use cases to ensure they are selecting the best models for their specific problems.  
误解或错误使用评估指标可能导致次优的模型选择和性能。 AI 工程师必须对各种评估指标及其适当的使用案例有深入的了解，以确保他们为特定问题选择最佳模型。

**Inversion**: Develop a strong understanding of evaluation metrics and their appropriate use cases. - Learn about common evaluation metrics, such as accuracy, precision, recall, and F1-score, and their trade-offs - Understand the implications of class imbalance and how it affects metric interpretation - Select evaluation metrics that align with the specific goals and constraints of your problem domain  
反转：建立对评估指标及其适当使用情况的深刻理解。- 了解常见的评估指标，如准确率、精确度、召回率和 F1 分数，以及它们之间的权衡 - 理解类别不平衡的影响以及它对指标解释的影响 - 选择与问题领域特定目标和约束相一致的评估指标

\## Ignoring Sampling Bias忽略采样偏差 

Failing to account for sampling bias can lead to models that perform poorly on underrepresented groups and perpetuate inequalities. Data literate AI engineers must be aware of potential sampling biases and use techniques like stratified sampling and oversampling to ensure their models are fair and inclusive.  
未能考虑到抽样偏差可能导致模型在代表性不足的群体上表现不佳，并且持续加剧不平等。具备数据素养的 AI 工程师必须意识到潜在的抽样偏差，并采用分层抽样和过抽样等技术，以确保他们的模型公平且包容。

**Inversion**: Be aware of sampling bias and use techniques to ensure models are fair and inclusive. - Analyze the representativeness of your data and identify potential sampling biases - Use stratified sampling to ensure balanced representation of key demographic variables - Apply techniques like oversampling or synthetic data generation to address underrepresentation  
反转：注意采样偏差，并使用技术确保模型公平和包容。- 分析数据的代表性，并识别潜在的采样偏差- 使用分层抽样确保关键人口统计变量的平衡代表- 应用过采样或合成数据生成等技术来解决代表不足

\## Disregarding Interpretability and Explainability 忽略可解释性和可解释性

Focusing solely on performance without considering the ability to understand and explain model decisions can limit trust and accountability. AI engineers must prioritize interpretability and explainability, using techniques like feature importance analysis and model-agnostic explanations to ensure their models are transparent and understandable.  
将注意力仅集中在性能上，而不考虑理解和解释模型决策的能力，可能会限制信任和问责制。 AI 工程师必须优先考虑可解释性和可解释性，利用诸如特征重要性分析和与模型无关的解释等技术，确保他们的模型是透明和可理解的。

**Inversion**: Prioritize interpretability and explainability to ensure models are transparent and understandable. - Use interpretable models, such as decision trees or linear models, when appropriate - Apply feature importance analysis to understand the key drivers of model predictions - Employ model-agnostic explanation techniques, such as SHAP or LIME, to provide insights into individual predictions  
反演：优先考虑可解释性和可解释性，以确保模型透明和可理解。- 在适当的情况下使用可解释的模型，如决策树或线性模型- 应用特征重要性分析来理解模型预测的关键驱动因素- 使用 SHAP 或 LIME 等模型无关的解释技术，以提供对个别预测的见解。

### 17

方军 2024-06-04

新访谈：

[对话香港大学马毅：「如果相信只靠 Scaling Laws 就能实现 AGI，你该改行了」](https://mp.weixin.qq.com/s/YTLWgcsGds86lgW_druXBQ)

### 18

方军 2024-06-04

有朋友公司的 AI 产品遇到客户吐槽，我只能从提示语上提供点协助：

建议这样回答

1）目前 AI 大模型主要是大语言模型，它们的数学能力较弱，涉及图形理解的就更差。这里瓜豆原理就是这样的情况。如果将来有数学能力较好的模型，我们会尽快采用。

2）现在，你可以这么来使用：建议你提供较多的资料去进行提问。这样数学问题或图形问题就部分被转换为现在 AI 大模型能够较好解答的问题。

这是提问的结果，请查看：...

我们参考的资料是：baijiahao.baidu.com/s?id=1730247129460957393

3）提示语如下，供您参考

如下是瓜豆原理的简介：

瓜豆原理，中国有句古话：种瓜得瓜种豆得豆。

例 1，如下图，点 P 是一个定点，点 A 是圆 O 上一个动点，连接 PA 作线段 PB⊥PA，且使 PB=PA。如果 A 点的运动轨迹是圆，那么 B 点的运动轨迹也是圆。

例 2，如下图，点 P 是定点，点 A 是直线 L 上的动点，连接 PA 作线段 PB⊥PA，且使 PB=PA。如果 A 点的运动轨迹是直线，那么 B 点的运动轨迹也是直线。即种瓜得瓜种豆得豆。

这两个例子中，都有一个定点 P，一个主动点 A，从动点 B 随着点 A 的变化而变化。

如果 A 点的运动轨迹是直线，那么 B 点的运动轨迹也是直线（种线得线)；如果 A 点的运动轨迹是圆，那么 B 点的运动轨迹也是圆（种圆得圆)。即种瓜得瓜种豆得豆，所以形象的称为瓜豆原理。

请为我解释这道题：

练习一下，如下图，点 P 是定点，点 A 是圆 O 上的动点，连接 PA，B 是 PA 的中点，问点 B 的运动轨迹是什么？

### 19

方军 2024-06-04

[新学年至少 100 门！ 复旦「AI 大课」12 问，你想知道的都在这](https://m.thepaper.cn/newsDetail_forward_27629872)

### 20

方军 2024-06-04

以下观点赞同，不过 1）当前 LLM 的能力就是这样，不搞 Workflow 没法用起来，这是对个人，2）过早进行 workflow 固化的产品，这个时候风险太高，做产品的公司不会愿意做。

摘：北火：

大部分用户没有 LLM 知识，也不懂怎么更高效地让 GenAI 为自己服务。所以他们内心最需要的不是一个强大的工作流编排工具（Workflow or Agents），而是一个达到完备状态的方案（solution），甚至是可直接用于决策的答案（reporting）。

Build more solutions, not agents.

换句话说，用户提出的需求是有一个更好的工具辅助他们完成工作。但是他们真实的想法是让工具代替他们完成工作。但是通常 toB 场景下的用户，不会告诉你后者。所以 toB 的工具要尽可能减少用户工作时候的付出，更要让他们在工作环境中表现的很出色。

任何不能让用户在工作中获得赞许的 toB 工具都不是好的 toB 工具。

### 21

方军 2024-06-04

不知道这个作者是谁，应该不是软件工程师，是个媒体人？因为都知道 Vogue ，还是相对比较懂媒体的。看起来不懂软件啊。我比较惨，又知道媒体典故，又知道软件开发，一看就是要被这个作者终结的人啊。微笑

转译：软件的终结

https://docs.google.com/document/d/103cGe8qixC7ZzF...

要了解软件将如何变化，我们可以借鉴技术如何改变其他行业的经验。历史总是有相似之处，只要你细心观察。

在互联网出现之前，媒体的运作方式完全不同 —— 制作内容的成本非常高。你必须支付人员的费用来创作、编辑和分发内容。因为内容制作成本高，所以它必须盈利。而消费者也会为此买单 —— 比如报纸、杂志、书籍、有线电视和按次付费电视。沃伦·巴菲特（Warren Buffett）就非常喜欢报纸 —— 谁会不喜欢一个具有地方垄断性质的可预测的订阅业务呢？

当互联网出现时，媒体公司认为这是一个接触更广泛受众并降低分发成本的机会。但没人预见到的是，互联网不仅将分发成本降至零，还将内容创作成本也降至零。用户生成内容蓬勃发展，当内容创作不再需要花费时，它也不再需要盈利。那么内容在不需要盈利时会如何表现？这一经济约束的放松导致了一场「寒武纪大爆发」—— 你可以拍一张咖啡杯的照片，发布后可能获得上百万次观看，或者根本无人问津，但市场的清算价格依然达成。这产生了大量我们无法合理消费的内容。这需要产品来引导注意力、营销这些内容，并有效地引导我们 —— 我们现在称之为用户生成内容平台。

这些平台彻底改变了媒体公司。作为一个媒体公司，你在争夺用户注意力时要面对相同的竞争，但你的成本要高得多。你雇用的内容创作者越多，你就越容易被用户生成内容平台超越。从结构上看，投资媒体自此一直是个亏本的买卖，而价值创造已经完全转移到控制分发的平台上。

软件制作成本很高。你必须支付人员费用来创作、维护和分发软件。因为软件制作成本高，所以它必须盈利。而我们也为此买单 —— 软件许可、SaaS（软件即服务）、按座定价等。软件的利润率历来让人羡慕 ——90% 以上的利润率和零边际分发成本。

软件昂贵是因为开发人员昂贵。他们是熟练的翻译者 —— 将人类语言翻译成计算机语言，反之亦然。大语言模型（LLM）已证明在这方面非常高效，并将把软件制作成本降至零。当软件不再需要盈利时会发生什么？我们将经历一场软件的寒武纪大爆发，就像内容一样。

《Vogue》不是被另一家时尚媒体公司取代，而是被一万个网红取代。Salesforce 不会被另一家庞大的 CRM 取代，而是被一群动态服务于相同需求和痛点的应用取代。软件公司将像媒体公司一样被取代，出现一套新的控制分发的平台。

SaaS、ARR（年度经常性收入）、魔法数字 —— 这些都是理解旧软件商业模式的术语，其中与软件制作相关的开支曾是护城河。市场的无形之手在软件领域已停滞很久，但大语言模型将引入其迅速而熟悉的纠正力量。今天主修计算机科学将如同 90 年代末主修新闻学一样。

### 22

方军 2024-06-05

anti：

AI 时代的基础教育猜想：

1）最基本的基础教育反而竞争更激烈，因为基础不扎实、后面也被 AI 淘汰，你语文不好，根本就别搞啥 Prompting 了。

2）会加强问题导向的自主学习闭环，提出问题并解决问题成为基础教育很重要的新加强。

3）代码编程进入基础教育，不学编程，有 AI 辅助你也不会编程。

### 23

方军 2024-06-05

我比较相信就是 chatgpt 这个核心形态

Arc 浏览器 CEO Josh Miller 关于下一代革命性设备的讨论：

1. AI 的未来不会存在于聊天应用中

2. 赢得 AI 市场的界面将比我们想象的更为熟悉

3. 回顾计算机历史，第 1 和第 2 点在事后看来都是显而易见的

以往的流行设备变革都是在现有工具上增加具有催化作用的界面创新。

所以变革路径应该是取一种现有的、流行的工具，在其上叠加具有催化作用的界面创新，在这个交叉点释放独特的价值。

AI 的催化界面创新是计算机现在可以阅读和看见、计算机现在可以模仿人类。

未来 AI 计算机的主要界面将更像我们生活中现有的工具，而不是全新的东西。

新颖性将出现在高杠杆点上，这些点「与我们今天使用的工具相遇」。

### 24

方军 2024-06-05

营销号策略：他比绝大多数博主的内容要优质。*** 就是 *** 版的半佛 **，他的内容去看去学都没什么问题，他说的话 80% 可以听。但这只是一种营销号的策略，大概就是把你早就知道 or 怀疑的信息加工成你喜欢听的，再揉碎了喂给你。

是的，这种垃圾特别多。这些人还不会用 AI，希望他们永远不会用，不过很难，他们很快会用 AI 制造更多的垃圾。

### 25

方军 2024-06-05

转：在 X 上看到一条很好的开始关于长上下文建模的 LLM 研究的建议，简单来说就是多用多体验，感受其优缺点。以下为原推翻译：

YaoFu：

几位一年级的博士生问我如何开始关于长上下文建模的 LLM 研究。我第一个建议 —— 虽然有点不寻常 —— 是 * 不要 * 阅读任何关于长上下文的论文，而是与模型对话。

- 找到教科书、课程幻灯片、财务报告、小说、非小说类书籍等任何长篇文档，与模型对话

- 连续两周，每天从早上打开电脑后的第一件事，到晚上睡觉前的最后一件事，都与模型对话

- 提出你能想到的每一个问题，比如 PCA 是什么？它与 SVD 有何区别？书中的哪一部分描述了这两者？书中具体怎么说的？

- 与所有你能接触到的模型对话，如 GPT、Gemini、Claude、Llama 等

- 持续两周，不做研究，不看论文，不上 arxiv，只与模型对话

- 在这个过程中，持续观察模型的表现，发现它们的问题，思考为什么模型会这样表现

我发现，经过这个过程的人，对问题的理解水平与那些只读论文的人有着根本性的不同。

### 26

方军 2024-06-06

摘：easy:

我觉得计算机这个专业以及程序员这个行业，在 AI 加持以后，会飞速分化。

最优秀的人才，会分布到人工智能核心、平台和公用组件库等基础业务上。但这些岗位是非常有限的。

不那么优秀的初中层人才，尤其是 CURD 工程师，则会有非常高的概率被取代。

因为他们的大部分工作是胶水性质的：将现有的组件放入框架以满足业务逻辑。

上一波 NoCode/LowCode 已经尝试去拿这一部分市场，但因为交互界面的问题（太复杂，用起来比编程都麻烦；2C 部分又不够优雅），表现不好，只抢到了部分中后台业务。

很多人以为现在 AI 编程的能力只是辅助，那是因为它们在通用编程上效果的确不尽人意。但是，GPT4 + DSL 的表现其实很惊艳，比如不少专注于 AI + SQL 的产品，可用度已经很高。

NoCode/LowCode 平台是现成的 DSL，GPT 的智能只要再进一步，就可以将它们最大的问题解决掉：通过自然界面实现领域编程。这已然可以干掉很多 CURD 岗位了。

在智能还不够高的时候，这种岗位不是以直接消失而是以逐渐减少的方式体现：以前 5 个程序员；现在 4 个 AI 编程，一个程序员二次测试和二次 Review。随着智能的提升，比例逐步增加。

那么胶水岗位会消失吗？

我觉得不会，因为它还有一个潜在要求：对业务逻辑足够了解。这一部分，不光是智能问题。这部分需求依然存在，但是从重「编程实现」逐渐转向重「业务逻辑」甚至「需求分析」。这也是未来 AGI 在各个行业落地的一个过程。

也可能将来会出现一些新的 AI 编程岗位 = 产品经理 + 指导 AI 干活 + Review 和测试。

以上，是我为什么建议不那么 TOP 的同学选择计算机作为本科第二专业的基本逻辑。

如果你选择计算机专业，必须要挤入顶尖人才层。不然你以为你是 T 字型人才，但其实是一字型人才。因为基础编程能力在四年后将遍地都是，| 部分被弱化甚至消解了。

这并不意味着编程能力不重要，相反，编程能力或者说编程思维可能是 AGI 时代必须的，但它们的门槛会变低，低得像现在的英文读写能力一样。

将计算机作为第二专业，则让你从 π 字型开始塑造自己的特长，即使其中 | 部分被持续弱化，你依然是一个 T 字形人才。更别说在两个专业交叉部分所拥有的极强竞争力了。

一个财务 SaaS 公司，在同样都能写出一般水平代码的情况下，更愿意招只懂计算机的应届生呢，还是财经专业选修计算机专业的应届生呢？

当然，这一切都建立在有能力精力和毅力把两个专业都学好，呃，算了，都学完的前提下 [允悲]

高飞：从社会分工的角度，计算机工作可能会趋向于「司机」，原来是个单独职业（专职司机才能碰车，如同穿白大褂带鞋套才能进机房），但是后来逐渐只是在细分场景是个工作（礼仪性、经营性、竞技性、研究性）。

### 27

方军 2024-06-06

转了点这类话题，才意识到中文社交网络讨论是因为高考？以后不看了不转了，这种宏大讨论价值太小，没有说具体问题。

甚至一深究这种宏大问题的建议就更难立住脚，比方说，这篇看似逻辑严谨，最后推导出要学一个专业，以计算机作为第二专业，但是，你那个主专业就选得对吗？一深究就全是问题了，原因还是出在讨论得太宏大、过于一般性。

宏大也不是不能讨论，一是宏大需要有真正的高度认知水平，第二是宏大需要讨论技巧，第三是宏大议题需要篇幅来保证全面性。短短说几句太难做到了。

讨论宏大会让人显得有智慧，讨论具体会让自己实际获益。都没错，就看自己想要什么。

### 28

方军 2024-06-06

这个里面的音频效果真是太赞了

字节的 TTS 论文

[Seed-TTS](https://bytedancespeech.github.io/seedtts_tech_report/#applications-samples)

[[2406.02430] Seed-TTS：A Family of High-Quality Versatile Speech Generation Models](https://arxiv.org/abs/2406.02430)

### 29

方军 2024-06-06

摘：同时宣布将编辑部细分为三个小部门：核心新闻部门，专栏评论部门，以及社媒、视频、AI 综合销售部门。刘易斯在一份声明中称：「通过加强部门职能，我们正摆脱'一刀切'做法，满足受众更深层次的新闻需求。

[「没人读你写的东西」，《华盛顿邮报》也陷读者流失危机，人事调整能否自救？](https://mp.weixin.qq.com/s/KWtUTeFUzn2QQANxpzM-VA)

### 30

方军 2024-06-06

看到一个 thread，OpenAI 最近的内容获取合作。

很有意思的一个感慨是，虽然大模型的训练资料是几乎所有的已有资料，但需要付费、引发很多关注的，是这些很新、很表面的新闻性资料。

当然，图书库、学术论文库之类的本来就很值钱。

x.com/chiefaioffice/status/1798128357177258317

### 31

方军 2024-06-06

当时写这段文字的时候，节奏写得很慢，假设人不太了解，慢慢地说。现在看，还不错

[如何正确地向 GPT 提问？](https://mp.weixin.qq.com/s/dmVsuWno7qa7Y-lfTg2P-A)

### 32

方军 2024-06-06

有个朋友问，框架是什么？

我回答：

核心知识点

及它们之间的关系

我这个回答应该还可理解吧

最近跟人说如何用 AI 时一直说这样一个方法

一面，用框架来协助自己提问

另一面，用 AI 来消除每一个疑难点

### 33

方军 2024-06-07

张俊林：为啥大模型做不好简单的数学计算？

讨论：张回复 @朱天赐 ztc：很好的想法，已经用了，一般把这叫 scratchpad，就是给些例子把详细计算过程体现出来，效果很好 //@朱天赐 ztc：张老师，用 CoT，把加法的规则很详细的写出来，放到提示词里，感觉是更符合人类的逻辑，不知道大模型能不能做到

我的看法是 1）原理上现在的 LLM 与数学不是高度匹配的 2）可以为它专门增加数学功能，虽说语言学家不管用，但数学规则是管用的，怎么把数学公式转换成布尔电路，靠现在傻傻的大量训练资料进去显然不是好方法。

### 34

方军 2024-06-07

「LLM 能通向 AGI」这一观点的反对者经常提这个问题：你们整天吹 LLM 会达到 AGI，可为啥大模型连最简单的「多位数加法」都做不好？

这质疑对很多 AGI 信奉者来说是很扎心的，不好反驳，因为大模型做多位数加法这种简单数学确实有困难。不过，目前基本已能理清其主要原因，大部分也获得了解决，我归纳了下，有如下几点：

其一，LLM 的 Tokenizer 对数字切分问题

早期 LLM 的 Tokenizer 一般不会对数字进行特殊处理，经常把连续的若干数字切在一起形成一个 Token，比如「13579」，可能被切成三个 Token ,「13」是一个，「57」是一个，「9」是一个 Token，类似这种。谁和谁被切在一起组成 Token，这取决于做 Tokenizer 的数据集合里的统计情况，俗称看命。在这种不确定哪些数字片段组成一个 Token 的情况下，LLM 要想做对多位数字数值计算，不说不可能，即使可能也是非常困难的。

不过，这个问题早已被解决掉了。我记忆中最早看到解决这个问题的公开文献是 LLama-1 的技术报告（23 年 2 月），里面提到了每个数字会被单独切分成一个 Token，之后这基本是个标准做法，目前绝大多数大模型应该都是这么做的。

把每个数字单独切开，这是必要条件，但是不充分，不是说这么做了之后 LLM 就能很好地做数学计算了。

其二，数字序列输入 LLM 的顺序问题

通常我们让 LLM 做数学题，都是正向输入的，比如要让 LLM 计算「13579+24680」，就按照这个顺序输入，高位在前，低位在后。但是……… 这么做从大模型的运行机制角度看，很明显是有点问题的（您可以想下为啥会有问题）。正确的做法应该是逆序输入，就是把「13579+24680」转成「97531+08642」，每个数值低位在前，高位在后。经过如此简单改造，LLM 计算多位数加法效果提升非常明显（可参考下面的图 2）。

这是为啥呢？其实好理解。我们知道，LLM 是通过 Next Token 预测，一次输出下一个 Token 来生成结果的。如果你按照「13579+24680=」顺序输入给 LLM，Next Token 就要求先输出计算结果的最高位，这意味着 LLM 必须在内部把完全正确的加法结果 38259 算完，而且得找地方存起来，然后再先输出高位 3，再输出次高位 8（这种类似想好了再说）….. 这无疑人为给 LLM 增加了学习难度，不匹配 LLM 这种一次产生一个 Token 的模式（Next Token 这种模式类似边说边想，而不是想好了再说，GPT 在这方面确实有明显弱点）。即使是我们人类在计算多位数加法的时候，也是先把两个运算数值串中的各个数字由低位到高位一一对齐，然后从右到左，也就是先算低位两个数字加法，如果有进位则把进位往高位传。

从低位往高位算降低了学习难度，这等于把一个复杂的多步推理问题，分解为连续的两个单位数加法的简单问题。两个单位数加法的组合空间相当有限，仅有 100 种组合（10*10），再加上有没有进位的两种情况（有进位／没有进位），则两个单位数加法只有 200 种组合可能性，这个组合空间很小，要让 LLM 学会这个看上去难度不大，哪怕你硬记也记住了。

这是为何逆序输入数字能明显提升 LLM 数学计算能力的原因，一方面极大降低了学习难度，另一方面比较匹配 LLM 的 Next Token 这种「边说边想」的输出模式。

其三，LLM 数字对齐问题

即使把数字逆序输入给 LLM，整体效果虽然有明显提升，但仍然不足够好。前面提到过，人在计算多位数加法的时候，首先需要把两个数值串对应位的数字从低位到高位对齐，然后从低位到高位计算，这样学习任务就比较简单。现在我们逆序输入数值，可以保证让 LLM 由低位到高位顺序运算，但是为啥效果仍然不够好呢？问题出在数字对齐上。

目前研究发现，LLM 在做数学运算的时候，经常对不齐相应位置的数字，比如「13579+24680」，3 本来应该对齐 4，但是 LLM 经常把 3 对到 4 附近的数字，比如 2 或者 6 上，这自然会产生运算错误。数字对不齐，这锅得 Transformer 里的 Self Attention 或者 Position Embedding 来背，说明相对位置编码（PE）或者检索字符（MHA）有问题。

目前解决数字对不齐有两种很有效的做法。一种是加入位置提示（Hint），比如「13579+24680」，每个位置加入提示字符，形成「a1b3c5d7e9+a2b4c6d8e0」这种输入形式，相同位置数字有个共同的提示字符，这很可能利用了 Induction Head 的作用（我猜的），可以有效帮助 LLM 对齐数字。

另外一种做法是对每个数字 Chunk 单独引入新的位置编码（Abacus Embedding），如图 1 所示：Abacus Embedding（from：Transformers Can Do Arithmetic with the Right Embeddings)

对于每个数字块，第一个字符引入位置编码 1，后续数字依次递增。这样，因为相同位的数字有相同的位置编码，所以有利于解决 LLM 数字对不齐的问题。

其四，LLM 长度外推能力弱

LLM 长度外推能力弱的意思是：比如我们在训练 LLM 的时候，LLM 见过的最长的数字串长度是 10 位，如果训练数据够多，LLM 做 10 位数以内的加法问题不大，但实际使用的时候，若给出 20 位长度的数字要求做加法，就容易算错。「没在训练过程中见过类似长度的就做不好」，这就是 LLM 长度外推能力弱的表现。

长度外推能力弱这个责任，Transformer 里的 Position Embedding 需要扛起来。目前研究结论是，起码对于数值运算来说，FIRE（Functional Interpolation for Relative Position Embeddings ）是长度外推能力最强的相对位置编码。（参考图 2:FIRE 外推能力较强，即使不把数字逆序输入，外推能力也能达到很好的水准（From：Transformers Can Achieve Length Generalization But Not Robustly)）

增强数值计算长度外推，只用 FIRE 也不够，在训练过程中还要配合「Randomized Position Encodings」技巧，才能大幅增加 LLM 数学运算长度外推能力。所谓「Randomized Position Encodings」，我们拿 Abacus Embedding 做为例子来说明，比如训练数据中包含的数字最大长度为 10，但是我想外推到 100 位的多位数加法。那么，在训练的时候，数值块中第一个数字的 Abacus 位置索引不要从 1 开始，而是在比如 [0,100] 之间随机取一个数值，后续数字位置索引在这个随机数基础上递增，以保持数字索引顺序。当然，同一个加法运算涉及到的两个数，都要采取相同的起始编号（随机抽一次，两个数同时用），这样才符合上面讲的解决数字对齐问题的思路。训练时候随机抽，但在实际运用的时候，恢复从 1 开始的起始编号。这就是「Randomized Position Encodings」的具体含义与做法。它本质上做的是：在训练的时候，就把没有见过的 [11,100] 之间的外推位置，学到对应的 Position Embedding，这样将来哪怕真碰见了这么长的位置编码也不怕。这么个思路。

FIRE 结合「Randomized Position Encodings」，以及上面提到的逆序输入和数字对齐，最长的长度外推能力可以达到 6 倍左右，就是说训练时见过的最大数值长度如果是 10 位，那就能支持 60 位长度的外推运算能力。

其五，大模型幻觉（个人猜测，无依据）

采取上述一系列措施，大体能解决较长的数字加减法或乘法问题，比如 100 位长度加法准确率可以达到 92% 到 97% 之间。但是，仍然有不太低的错误率，而且随着长度加大，错误率会更高。

也就是说，看上去很容易的问题，LLM 能解决得还不错，但不完美。这又是为啥呢？我个人猜测：这估计跟大模型幻觉脱不了干系。假设 LLM 输出 Token 有一个量化的幻觉衡量标准叫做「幻觉率」，就是说 LLM 输出 100 个 Token，有多少是幻觉 Token，如果有 3 个，那「幻觉率」就是 3%。幻觉率并不好测量，因为有些幻觉 Token 你觉察不出来，比如生成创意问题，里面肯定有幻觉 Token，但是你感知不到。只有输出答案每一个都有标准答案的场景，才能正确检测大模型的「幻觉率」（目前也有用事实性知识来量化测量大模型幻觉程度的）。

我觉得多位数加法可能是个检测大模型「幻觉率」的好的场景，因为输出结果每一 Token 都有标准答案，而且目前该做的改进也基本都做了，从错误分析来看很难再找出系统性的错误方向，那剩下的错误貌似只能靠幻觉来背了。之所以大模型仍然有 3% 到 7% 的加法运算错误，大概因为目前大模型的幻觉率在这两个数字之间，所以每输出 100 个数字 Token，就会有相应比例的幻觉 Token，导致计算错误 。

当然，个人猜测未必对。如果万里有一是对的，这意味着，如果幻觉问题不解决，LLM 的长数字运算很难达到完美准确率，基本无解。当然你说可以用外挂计算器，那是另外的思路，不在这个主题的讨论范围。

### 35

方军 2024-06-07

092 LLM 应用遇到疲软期

感觉上，LLM （大语言模型）应用遇到疲软期。实际上是不是如此，没有实际的资料，逻辑上讲，现在的数据应该是上升期。但是，从个人感受讲，当前的 LLM 应该处于一个冷却降温的阶段：

一方面，性能突破很久没出现了，也不太可能出现了。以电脑芯片类比，386 芯片出现是突破性的，后来还有 486 、奔腾等等各种引发关注的。但是，后面就是逐步的改进了。后面芯片的提升比当时大很多，但不再是人们关注的焦点了。

（普通人尤其热衷于视频如 SORA、音频如 S ONO、如这几日的 Seed-TTS，以及 GPT4o，但这些真的是边缘性的，真正的核心还是 LLM 自身的能力。还有图形生成模型，另外我还遇到一大帮人特别热情人形机器人。）

另一方面，从应用角度看，大模型能用的地方都已经在用了，不能用的，也很清楚了。普通人会用的，已经用上了，不会用的，教人用的也怎么教不会（看一些培训社群，里面的信息很没意思，不管教的一方还是学的人分享的学习成果）。接下来有什么大的突破性应用呢？没有。

产品上，有什么可以突破的呢？前几日转发 Arc CEO 的说法，他认为未来的 LLM 形态不是对话形态。现在很多人都这么想吧，所以我们看到业界热了很久的 AI 聚合搜索、RAG、Agent，也就那样（当然，从产业角度在落地在落实在实际发挥作用。）。我的看法与她不同，我认为 chatGPT 就是最终形态，也就是互动对话。

在这个阶段，没有热情，没有目标，难免感受到气馁。不过，从做产品技术的人角度讲，其实这个阶段蛮好的：

- 产品技术还是在高速发展，光光跟各种组件的版本更新，就可以消耗大部分精力。

- 模型本身的能力、价格都在降低，这都有了做很多应用的空间。

- 外界慢下来，期待降低了，踏实做产品的人实际上竞争优势强了。（你一个产品要好好打磨，比能快手搞产品，过去你当然没优势。）

- 论文也出得没那么快了，可以耐心地看完各种论文，找其中的亮点来应用。大量论文出来的时期，其实看不过来，多半就是随便翻翻，很多经典的也没空去细补，现在是好时机。

冷静期，其实是好事，是做事的好时机。

### 36

方军 2024-06-07

2024 年高考某语文试卷要求：

随着互联网的普及、人工智能的应用，越来越多的问题能很快得到答案。那么，我们的问题是否会越来越少？

（为广东高考题）

[五家国内 AI 大厂作答高考作文题，谁家更优秀？](https://mp.weixin.qq.com/s/mSPqLS0WkYacuTYMQoTIyA)

### 37

方军 2024-06-07

有意思的经验和我的看法：

indigo 整理了一下新文章的引用和参考，文章、视频、播客、论文还有报告加起来快 40 篇了，唯一两篇中文的还是我自己之前写的。。不过这次写作确实让我能从之前的阅读中提炼一些更高层次的理解。要消化这么多内容，肯定需要 AI 工具帮忙：

- Maimo.ai 全部参考内容的检索、问答与要点提炼，混合生成才有新灵感；

- Perplexity 概念解释与引用数据核实；

- ChatGPT 帮我整理写作逻辑和部分输出；

但是，我与他的做法完全相反

第一，我不依赖笔记，我从来不用自己的笔记

第二，我不依赖 AI 来阅读（最多读不了的我就不纳入），我有自己的阅读方法

第三，我不依赖 AI 的解释，写的时候绝不会用。学习时、校对修改时，可能会用，但也不多。

### 38

方军 2024-06-07

随着互联网的普及、人工智能的应用，越来越多的问题能很快得到答案。那么，我们的问题是否会越来越少？

我这个作文，能及格不？：）

大家评评呲牙

（为《重新学会学习》第 9 章下半部分的内容）

### 39

方军 2024-06-07

五个大厂 AI 的高考作文

增加了 GPT4o 的，一般。Gemini-1.5-pro，还不错。Claude Opus 还不错。

做了主题句标注，我较为喜欢讯飞

真细看都不太好，离人还是差很远

腾讯的看似有文采，但我较为不喜欢。（鹅厂那个文章，我现实中会给零分的，真有人写那种材料，直说你用人话重写吧。）

阿里、百度的，都过得去，放在工作场景能糊弄事。

字节豆包的逻辑混乱，效果最差。

有个朋友重点从区分「question」"problem" 来看模型能力，他认为腾讯和讯飞是过关的。

另附一篇互评

[让 AI 互评高考作文，Kimi 和 GPT-4o「互掐」，通义千问实力演绎「端水大师」](https://mp.weixin.qq.com/s/t5jAjVDLtQSLbTCBK5lHQw)

gaokao-ai-export.pdf

方军：网友对 GPT4o 的评价：；有一说一，现在的大语言模型写个文档还可以，文采太差了，即便是应付高中作文的水平也是没有的

之前用 GPT-4 写小学作文我的感觉也是惨不忍睹

非要写的话，也许可以搞个工作流：

从《人民日报教你写作文》语料库找一个合适的模版；

AI 根据题目生成思维导图；

AI 根据模版和思维导图仿写 800 字文章；

人工润色

这么搞的话也许可以，但时间成本来说还不如自己写一篇 [undefined]

2024-06-07 18:14

方军：网友：：还是不行啊，引用了四句名言两句严重错误，一句看起来差不多但意思不对。只有孟子那句引用对了

2024-06-08 07:46

### 40

方军 2024-06-07

这篇文章的标题甚至直接用「Let's think dot by dot」，来对标思维链的「Let's think step by step」，展现了「省略号」的威力。「点点点」的威力研究人员发现，把思维链（Chain-of-Thought，CoT）推理中的具体步骤，替换成毫无意义的「...」，产生的推理结果也大差不差。

[纽约大学研究表明：大模型并没有利用思维链的推理能力，它只是加了计算！](https://mp.weixin.qq.com/s/TlWRg_ZrBTd774qV_K0DgQ)

### 41

方军 2024-06-07

这个的确是场景：海淀家长已经 AI 武器化了。

据说中关村一票家长开始用 AI 进行压题，他们把历年北京高考试题给到 ChatGPT 做训练，让它帮助阅读提炼整理。然后问它过去三年哪些知识点不考了？过去五年出现 3 次及以上的知识点是哪些？问了去掉的知识点换成了哪些新的知识点？每年相比上一年的新知识点占比是多少？

大数据压题，比感觉比特级教师靠谱。

当然还有王炸，让 AI 给孩子出个全套的模拟卷，然后阅卷评分一气呵成。

### 42

方军 2024-06-08

律师、程序员都是典型的知识型工作，程序也一样，团队协作极高（不一定只是内部，也包括外部，甚至包括与设备如云的协同）

最近常听到一种说法，律所的团队管理者告诫下面的律师，要对案件有「ownership」。用中文说，就是要把律所的案子当成自己的案子。

这话我不认同。

ownership 直译过来是所有权的意思。所有权最重要的权能，一是收益，一是处分。既然要让下属 take ownership，那么下属除了责任，是否还有同等的收益？

如果钱都是合伙人赚，责任却由下属律师承担，任是谁也不会有工作热情的。

钱还只是一方面。如果合伙人对 take ownership 的要求只是让律师注意各个时间节点，及时提醒老板交材料，不要写错别字等等技术性甚至机械性的工作，这个 ownership 未免也太苦了。

我见过许多初年级律师分享，当他们做的法律研究成为案件的指导方向，他们起草的法律文件提交给法院乃至被判决直接引用，当他们第一次主导开庭或谈判，等等，是他们最有职业认同感的瞬间。

这些才是 ownership 的具体体现。

当然了，案件里还有大量枯燥机械的工作，如何保证这些工作不掉链子？

我认为，需要靠流程和制度，而不是靠虚幻的律师个人的责任心。

我在以前的文章里写过，法律文件不出错别字是最基本的要求，但只要是人，就会写错别字。这个时候，团队管理者除了严格要求团队成员，还应该从流程和制度层面想想，如何解决问题。

比如案件的各种截止日期容易遗漏，是否可以用合适的案件管理工具，或者就干脆派专人来负责管理日程，不然律师助理又要做案子又要管行政，难免有所疏漏。

又比如总有团队管理者说，某某问题我虽然没有直接交办你做，但这是你主办的案子，你怎么就不能多想两步，主动把工作做好呢？

坦白说，我能理解这样的问题，也认为团队管理者不该把大量时间花在交办重复性的工作上。但如果管理者没有派任务，那下属没有做，就不能责怪下属。

这个时候要做的，是把案件要做的每一步工作都分解成标准步骤，明确安排给下属律师，让他们知道什么时候要做什么事情。

一旦案件出了问题，就说下属律师没有责任心，没有 take ownership，这是解决不了问题的。

### 43

方军 2024-06-08

我一直有个偏见

复杂提示语用英文写

理由一： 讲实话咱英文都不好，写英文自然会语言简单、直白。

理由二： 写英语会自然地重视语法，也会注意选词，表达的清晰度会提高。

理由三： 写英文会更直接地提要求，中文我们往往会不直接。

当然模型本来英文理解水平就好，这么一叠加，提问效果好很多。

至于说不会写英文怎么办？

我一直以来遇到这种疑问的建议是，你难道就不会让大模型先帮你翻译下？

### 44

方军 2024-06-08

据英国《金融时报》（FT）报道，成立起始于深圳、后来把总部迁到美国洛杉矶的生成式 AI 初创公司 HeyGen 创始团队，要求其中国投资者 IDG 资本、百度风投、红杉中国、真格基金（ZhenFund）把股权卖给美国投资者。知情人士称，美国投资者和 HeyGen 创始人徐卓（Joshua Xu）希望「整顿股权结构表（cap table)」，即投资者名单，因为美国方面正在加强对中国科技集团和跨境投资的审查，并于今年 3 月完成了硅谷 Benchmark 领投的一轮融资，使得红杉中国等中国投资者大幅减少股份，而且让另一位创始人 arah Guo 接替了红杉中国在 HeyGen 董事会成员席位。对此，红杉中国和 HeyGen 拒绝置评。Benchmark、IDG 资本、百度风投和真格基金没有回应置评请求。

[AI 公司 HeyGen 正逼走中国风投，强制要求红杉中国、真格、百度、IDG 把股份卖给美国投资者｜钛媒体 AGI](https://mp.weixin.qq.com/s/bUCainOSmh1KG9zG6zBitw)

### 45

方军 2024-06-08

摘（这里应该和他的人群与 AI 高度相关的因素）王凯：我（王凯）临时插了个项目「松鼠快看」，主要解决我在 PC 端阅读 AI 信息源的问题。随着产品传播，很多用户给我反馈 —— 期望加 AI 功能，可以如何如何。

他们不知道我专门研究 AI、做 AI 项目的背景，很多给我想辙如何结合 AI。

很有意思：明显感觉大家普遍对 AI 有一定认知了，逐步形成共识，这个是 AI 爆款应用能出来的基础。

和之前做移动互联网一样，大家都会逐步形成共识问你：是否有 APP？

现在第一反应也是遇到互联网品牌，先去搜索 APP。

感觉市场教育效应越来越强了，逐步会演变成：必须要有 AI 点，像必须有 APP 一样的强约束力。

### 46

方军 2024-06-09

最近，美国众议院外交事务部委员会以显著的多数票数，成功通过了一项旨在严格管控 AI 技术出口的法案。这项法案被正式命名为《加强海外关键出口国家框架法案》，通常简称为《ENFORCE 法案》。值得一提的是，在该法案不仅限制了 AI 系统和大模型的出口，一旦法案通过，持有 H1b 签证的中国员工或留学生可能需要特殊许可才能在美从事 AI/ML 相关工作。也就是说，这是明晃晃在限制中国人在美从事 AI 相关工作。目前在美国的中国员工或留学生对此表示非常震惊，网友们甚至给它推演出了一个骇人听闻的「化名」：「禁止中国人从事 AI 职业」议案。

[限制中国人在美国搞 AI，美众议院提案对 AI 产业带来什么影响？](https://mp.weixin.qq.com/s/OjREdQVH2nXE0WvCogLUSQ)

### 47

方军 2024-06-09

[通往 AGI 之路：AIGCxChina 专访 AJ](https://mp.weixin.qq.com/s/QquJDG_LdRjXxIv0_yVOZg?from=groupmessage&isappinstalled=0&scene=1&clicktime=1717907103&enterid=1717907103)

### 48

方军 2024-06-09

从这个能猜测，一些人猜测的人们对 AI 的需求：

【主要内容】

一、	Prompt 工程：高级提示词技巧

二、	如何用 AI 看论文 / 报告，自动做 PPT

三、	如何用 AI 提取数据，做数据分析和可视化

四、	如何用 AI 做脑图、用户画像、SWOT 分析

五、	如何用 AI 把视频转文字、实时翻译、要点总结

六、	如何用 AI 生成小红书笔记、高级提示词拆解

七、	如何用 AI 快速读完一本书

八、	如何用 AI 帮你做营销策划

九、	如何用 AI 帮你推荐礼物

十、	如何用 AI 帮你做营养配餐

### 49

方军 2024-06-09

What We've Learned From A Year of Building with LLMs

这篇文章文章的链接：

[Applied LLMs - What We've Learned From A Year of Building with LLMs](https://applied-llms.org/)

### 50

方军 2024-06-09

093 有时候我也想如果 AI 能写就好了

把想法变成文字，是个体力活。有时候我也有这样的想法，如果 AI 能写就好了。

比方说，我要写一个资料，开头我想好了。

- 模块化是必然的。（这里不具体说哪个部分了）

- 经济学上，有青木昌彦提的模块时代。

- 计算机科学上，有 Frederick Phillips Brooks 的设计原本。

- 商业管理上，有 Carliss Y.Baldwin 的设计原则（另，他们第二本似乎就没写出来了，我特别担心老太太去世）

这么个开头之后，我就可以具体说，所讨论产业的模块化有七个细分趋势。其中各有多种项目、技术原理。

我还没写这个开头的原因是，没时间安静下来把这几百字写好。后面的七个细分趋势就不用 AI 写了（当然，如果细节 AI 能写也好），因为有几百页的 PPT，其实就是选择内容、看图说话，写一个单线条逻辑就可以了。

虽然我对 AI 有期待，但是，到目前为止，我还想不出，AI 真的能按我这样设想的写吗？（或者说，我让它写，会不会导致我要干更多的活？）这两个担忧，让我认为至少在两三年周期内，不用考虑这样的事。让我改 AI 写的，还不如自己写呢。

如果我想让它写，那必然前面要准备资料，后面要审校，想想都是不合理的。

方军：我现在写的东西真是不受欢迎呲牙

别人要的是，AI 可以写文章，用这个方法，你可以轻松写文章，最好再往下加，可以挣钱（月入十万）

但我却不会写这样的东西，

2024-06-09 20:09

### 51

方军 2024-06-10

今年大家都说工作流（workflow），其实看看宝玉这个字幕翻译早就是工作流了。

不是用了工作流软件（比如dify、coze里）就是工作流，而是你自己心里面有没有工作流。这些工作流软件不过是帮你自动化而已。

我们早就知道自动化这一步，是因为一直在用 API 及 Langchain。但我们知道，更重要的是工作流的拆分，找到可以适合交给 AI 的具体任务，也即能得到期待的结果。

以下来自宝玉：

问：我做了个讲解视频，手里有英文稿，但是 Whisper 或者 YouTube 生成的字幕有很多错误，我该如何用文稿纠正字幕错误？

答：

可以借助提示词工程，让 LLM（大语言模型）来修正字幕错误，并且保留时间轴不变。提示词并不复杂，主要是要让 LLM 明白你的**输入格式**和你期望的**输出格式**，并提供所有的上下文信息，甚至不需要 CoT（思维链）这样高级的提示词技巧。

但是要注意的是，要输入的内容不要太长，因为大语言模型容易出现幻觉，输出质量会下降，按我的经验，每次 800 左右单词是一个比较合适的长度。具体需要测试对比一下。

方法如下：
1. 字幕需要用 srt/vtt 这样的简单文本格式，然后要将字幕分页，每一页包含原始文稿和字幕文件。
2. 可以使用 API 或者 ChatGPT 都可以，如果使用 API，建议输出 XML 格式，方便代码解析，如果是 ChatGPT，可以将输出内容放在代码块中，方便复制出来。

参考提示词：

```
\# Task
Here is a transcript of a video, it was generated by an AI speech recognition tool and may contain some errors/infelicities. Your task is to correct any spelling discrepancies in the transcribed text.

I'll provide you with the corrected text and the subtitles, please return the corrected subtitles only, use only the context provided.

\# Output Format
<corrected-subtitle>

</corrected-subtitle>

\# Input
<correct-text>
「略」
</correct-text>

<subtitle>
「略」
</subtitle>

```

这段提示词结构很简单，就是要求 LLM 完成的任务、期望输出的格式，然后是输入，由于输入部分包含两种不同类型的数据：文稿和字幕，所以需要用两个标签包裹起来，这样 LLM 就能有效区分你输入的内容。

提示词中文参考：

```
\# 任务
这是一段视频的转录文本，由 AI 语音识别工具生成，可能包含一些错误或不合适的地方。你的任务是纠正转录文本中的拼写差异。

我会提供给你纠正后的文本和字幕，请仅返回纠正后的字幕，只使用提供的上下文。

\# 输出格式
<corrected-subtitle>

</corrected-subtitle>

\# 输入
<correct-text>
「略」
</correct-text>

<subtitle>
「略」
</subtitle>

```

注意：这个提示词仅作为参考，在 GPT-4o 下测试效果很好，但是其他模型可能需要调整，对于能力弱的模型，可能要提供 1-2 个示例，让模型更好的理解你的任务。

x.com/dotey/status/1799843067094012039

### 52

方军 2024-06-10

有意思的讨论：

Susan the dice tree：现在做一件最简单的事情，就证明「垃圾教科书」广泛存在。那就是那你找一门你学的最不好的一门课，用我推荐的 globe 写一份大纲，用 GPT 为你自己专门写一本教科书。

按照以前我介绍的 prompt engineering 技巧，凡看不懂的关键词全部叫他释义，稍微复杂点的概念叫他使用 metaphor; 能用图表的叫他 mermaidJS , 大段读不懂的地方叫他转换为 multiple choice 帮你主动理解，lab 叫他按照你学过的知识和你解题的能力来设计难度。

叫他生成针对某些关键概念你跟他玩 discussion 对话。你就真正知道垃圾教科书是一回事，真正的好的老师和教科书是针对每个人量身定制的。现在我们完全有这个技术条件可以做到了。

speaker：如果 GPT 也能扮演学生，让用户使用费曼学习法，通过讲课来检验学习的效果。再实现技能的考试和证明，整个流程就全部可以替代了。

ImmuneMind：gpt 最大的问题是有时会胡说八道，作为教材学坏了再揪成代价很大。作为辅助是可以的

x.com/feltanimalworld/status/1799515129526046916

方军：我的看法，这位 susan 过于乐观了。我觉得很多热情的普及者都有过于乐观的优点（或缺点）。

2024-06-10 10:10

### 53

方军 2024-06-10

山姆·奥尔特曼谈论在人工智能时代生存所需做的事情。

"你即将进入人类可能性的最伟大黄金时代..."

在那个世界里蓬勃发展，最重要的技能是：

- Deep familiarity with the tools

- 对工具的深入了解

- Staying abreast of changes

- 跟上变化

- Developing a great intuition for AI tools, where things are going, and how to make use of it

- 发展对 AI 工具的直觉，了解事物的发展方向，以及如何利用它

- Resilience and the ability to learn things fast and evolve yourself with technology

- 韧性和快速学习以及与技术共同进化的能力

我知道对于留意的人来说，大部分内容都是不言而喻的，但以下是要点：

AI 升级和跟上人工智能可能是当今世界上最重要的技能。

最迷人的部分是，新的 AI 发展和工具如此迅速地涌现，以至于每个人都在不断地学习。

我愿意打赌，如果你是一个完全的初学者，但花费 3 个月的专门学习，你将完全赶上专业用户。

换句话说：

花 3 个月学习 → 工作速度提高 2-4 倍，未雨绸缪。

现在开始的人有很多机会。

x.com/rowancheung/status/1787164459712073845

Sam Altman on what you need to do to survive in the age of artificial intelligence.

"You are about to enter the greatest golden age of human possibility..."

To thrive in that world, the skills that matter most are:

- Deep familiarity with the tools

- Staying abreast of changes

- Developing a great intuition for AI tools, where things are going, and how to make use of it

- Resilience and the ability to learn things fast and evolve yourself with technology

I know most of this stuff is a pretty big no-brainer for anyone paying attention, but here's the takeaway:

AI upskilling and keeping up with AI are possibly the most important skills in the world right now.

And the most fascinating part is that new AI developments and tools come out so fast that everyone is constantly learning.

I'd be willing to bet that if you're a complete beginner but spend 3 months of dedicated learning, you'd be fully caught up to power users.

In other words：

Spend 3 months learning → work 2-4x faster and future-proof yourself.

There's so much opportunity for anyone who just starts now.

### 54

方军 2024-06-10

「Do Anything Now」的论文作者昨天开源了论文中使用的 15,140 个 ChatGPT 提示，其中包括 1,405 个越狱提示，收集于 Reddit、Discord、网站和开源数据集。

[verazuo/jailbreak\_llms：[CCS'24] A dataset consists of 15,140 ChatGPT prompts from Reddit, Discord, websites, and open-source datasets（including 1,405 jailbreak prompts).](https://github.com/verazuo/jailbreak_llms)

### 55

方军 2024-06-10

094 AI 时代的表达真实性

在推上混了几天，不知是不是关注的或算法的原因，目前看到的由 AI 生成的内容还不是很多。一个强烈的感觉是：

在 AI 时代，人的表达的真实性重要度越来越高。

只要有一丝 AI 的感觉，几乎就会被忽略。

之前，机构号那种没有个人感的，已经完全没吸引力了，体现在它们或许粉丝不少，但互动率低得可笑。

这种真实性很难说明白，长句可能是真实的，短句也可能是真实的。说不出来，但能很容易识别，并且我有信心，在未来的很多年里（说少点五年）都很容易识别。

目前看，假互动也较为容易识别，比方说，一定有人在用 AI 进行回复和转发，这种也是一眼能看出来的。

现在我的推论是，在接下来，如果想要靠表达，至关重要的是真实性。

（我写的东西的错字、以及如果持续跟踪看到的修改，比如在星球里，其实是一种真实性，哈哈。）

AI 生成的假文字，容易生成，广泛供给，它的无价值是所有人都知道的。

之前我们还说 AI 生成的假文章是「预制菜」。现在，我想说一个更夸张的比喻，AI 生成的假文章是餐厅门口的塑料假菜，它的用途仅限于此，没有人会傻到把塑料假菜端上桌。

方军：真实性的要求也会变化，现在，如果有人按十多年前报纸的社论的感觉写文章，那现在多半会被认为是假的。

而更早前的比如香港报刊上的个人专栏那种写法，反而会一直被认为是具有高度真实性。当然，实际上两种都是真实的，这里面的差别是后者个人彩色更浓。

2024-06-10 13:15

### 56

方军 2024-06-10

世界就是这样，我无比赞同这种观点，但我知道事实就是大众就是喜欢这种傻东西，就跟大众天天看抖音或 tiktok 一样。

摘：我认为拿人工智能去生成视频是对算力的一种浪费，视频生成是一个人工智能伪命题。就像我们的真实世界因为短视频而多了如此多的白痴一样。

### 57

方军 2024-06-10

教育提示语：

SYSTEM：有一个学生要学会正确地使用「的」「地」「得」，请逐个给出测试题，要求填入正确的。一道题答完之后给出下一道。

USER：开始

其实 AI 就这么简单。

### 58

方军 2024-06-11

一堆人鼓吹 Siri x AI

我从头开始对 Siri 就毫无好感

我这么多 Apple 设备

使用的第一个设置是，把 Siri 关闭

极其烦

（再一次，我知道大众喜欢，能动嘴就不动手，但这不影响我觉得 Siri 从头到尾就是一个错误。）

以下这个论文讲的场景有个什么用，屏幕你按按不能了？屏幕上的信息复杂到理解不了，这不是 UI 设计的极致失败吗？

Jim Fan:

今天在 WWDC 上，每个人都期待着一个重生的 Siri。嗯，苹果已经发布了一篇关于它的论文，披露了比我们期待的更多细节。它被称为「Ferret-UI」，这是一个多模态视觉语言模型，它能理解 iOS 移动屏幕上的图标、小部件和文本，并推理它们的空间关系和功能含义。

您可以问 Ferret-UI 的示例问题：

- 提供此截图的摘要；

- 对于交互元素 [bbox]，提供一个最能描述其功能的短语；

- 预测 UI 元素 [bbox] 是否可点击。

具有强大的屏幕理解能力，向模型添加动作输出并使其成为一个完整的设备助手并不难。

这篇论文甚至谈到了数据集和 iOS UI 基准构建的细节。苹果公司的开放态度非常了不起！他们真正重新定义了他们的人工智能研究部门。

这篇论文在四月份悄悄发布，几乎没有什么宣传。WWDC 之前你还有足够的时间来热身：

[[2404.05719] Ferret-UI：Grounded Mobile UI Understanding with Multimodal LLMs](https://arxiv.org/abs/2404.05719)

### 59

方军 2024-06-11

宝玉：昨天发的让 ChatGPT 写高考作文，很多人评论说 AI 味很浓，也有人指出是提示词（Prompt）的问题。那好的提示词是不是能让分高一点？

可以来测试一下，先试试用朋友写的一段提示词：

---

请你写一篇中国高考的应试作文，题目如下：

生活中，人们常用认可度判别事物，区分高下。请写一篇文章，谈谈你对「认可度 ' 的认识和思考。

要求：（1）自拟题目；（2）不少于 800 字。

附加要求：

1，假设你的知识能力包含全部西方古典文学名著，熟悉中国传统知识和传统文学作品；

2，你非常善于在应试教育中取得好的分数；

3，为了得到一篇高分作文，你知道如下规则：

（1） 文章字数在 950-1000 字；

（2）文章中引用中国古代著名诗人的名句一次；

（3） 文章中引用西方经典哲学家的哲学名句一次；

（4）文章中使用一个最近十年中在中国发生的热点新闻话题作为举例；

（5） 文章的最后需要升华主题，体现优良的道德素质并复合中国传统文化价值观。

下面请按如上要求写这样一篇作文

---

认可度：群体的镜子与个体的选择

在世间万物的评价与交往中，「认可度」成为了一个不可忽视的尺度。从社交媒体上的点赞数到学术界的引用频次，人们似乎总是倾向于通过广泛的认可来判断事物的价值。但是，这种看似客观的评价标准真的能全面代表事物的价值吗？本文试图探讨认可度的双重性质以及它如何影响我们的决策和价值观。

首先，认可度作为群体共识的反映，有其不可否认的正面价值。正如庄子所言：「吾生也有涯，而知也无涯。」在知识和经验有限的情况下，群体的认可可以作为一种参考，帮助我们快速做出决策。在现代社会，这种现象尤为常见。例如，消费者在选择产品时往往会参考销量和评价，认为高认可度的产品更可靠。

然而，从另一个角度看，过度依赖群体的认可也可能导致个体失去独立思考的能力。柏拉图曾警告说：「不要让对话结束于纯粹的个人喜好之争。」当认可度成为评判一切的唯一标准时，那些不被大多数人理解或接受的新观念和创新就难以获得足够的关注和资源，从而抑制了社会的多样性和创新性。

在中国近年的热点新闻中，共享单车的兴起便是一个关于认可度双刃剑效应的例子。最初，由于其便捷性和环保性，共享单车获得了极高的认可度和广泛的推广。然而，随着时间的推移，过度投放和管理不善导致了城市管理的混乱，这也反映了盲目追求认可度可能带来的负面影响。

那么，如何在尊重群体认可与保持个人独立之间找到平衡？关键在于培养每个人的独立思考能力和批判性思维。教育应当不仅仅教会学生如何获得认可，更要教会他们如何质疑现有的认可，如何勇于提出和尝试新的可能性。只有这样，个体才能在群体的影响中保持自我，社会也能在多样性中实现健康发展。

最后，正如古代圣贤所倡导的那样，个人的道德修养和价值判断不应仅仅依赖外界的认可。真正的价值，是在于个体能否在群体的声音中，听见自己内心的声音，坚持正确的判断与选择。如是，我们既不会被盲目的群体认可所左右，也能在复杂多变的社会环境中，找到属于自己的坚实立足点。

综上所述，认可度虽是评价事物的一种手段，但不应成为唯一的标准。在追求广泛认可的同时，我们更应重视个体的独立性和创造力的培养。通过教育和社会实践，提高人们的独立思考能力，才能使我们的社会更加健康、包容和充满活力。

---

是不是要好一点？

如果我们要求更具体一点，给出中心思想和文章结构，是不是可以更好一点？

新的提示词：

---

你擅长写中国高考应试高分作文

* 题目

随着互联网的普及、人工智能的应用，越来越多的问题能很快得到答案。那么，我们的问题是否会越来越少？

以上材料引发了你怎样的联想和思考？请写一篇文章。

要求：选准角度，确定立意，明确文体，自拟标题；不要套作，不得抄袭；不得泄露个人信息；不少于 800 字。

* 规则

1，假设你的知识能力包含全部西方古典文学名著，熟悉中国传统知识和传统文学作品；

2，你非常善于在应试教育中取得好的分数；

3，为了得到一篇高分作文，你知道如下规则：

（1）文章字数在 950-1000 字；

（2）文章中引用中国古代著名诗人和西方经典哲学家的名句～3 次；

* 中心思想

随着互联网的普及、人工智能的应用，旧的问题少了，新的问题增加了，挑战更大了，所以更需要在党和国家的领导与监督下有序发展新技术，坚持新质生产力路线，必将取得胜利。

* 结构如下：

- 技术发展日新月异，旧的问题解决了，副作用也凸显了

- 党和国家高瞻远瞩，早就提出了新质生产力的发展路线，就是为了解决新问题

- 技术发展需要在党和国家的领导与监督下有序发展，互联网的普及、人工智能的应用是新质生产力路线的胜利。

* 参考资料

同时为了让你更好的理解什么是新质生产力，我附加了一篇相关文章，请结合文章内容写作。

文章内容如下：

新华社北京 4 月 8 日电题：做好发展新质生产力这篇大文章

<内容略>

----

结果这里就不发了，可以自己试试，但应该能拿个不错分数。

你也可以试试换换不同的提示词，也许可以得到还不错的结果。

### 60

方军 2024-06-11

对 AI 写作，网友点评及我的点评：

网友：初看下来，关于「认可度」的题目写得确实「impressive」。但细品之后，共享单车的例子真的合适吗？它出现的位置也很迷，从结构来说应该论述前段「过度依赖群体的认可导致失去独立思考的能力」，但实际内容又与「独立思考」无关，而是引申出另一负面影响～因此我认为 gpt 在逻辑上还有大的提升空间。

我的：（非从作文评分角度，仅从文章角度，且不谈我自己的逻辑，就是顺原文的逻辑）

认可度：群体的镜子与个体的选择

在世间万物的评价与交往中（📕交往在这儿不对，应去掉），「认可度」成为了一个不可忽视的尺度。从社交媒体上的点赞数到学术界的引用频次，人们似乎总是倾向于通过广泛的认可来判断事物的价值。但是，这种看似客观的评价标准真的能全面代表事物的价值吗？本文试图探讨认可度的双重性质以及它如何影响我们的决策和价值观。（📕这句应不要，1）通常不需要，2）总结得不对，反而误导。）

首先，认可度作为群体共识的反映，有其不可否认的正面价值。正如庄子所言：「吾生也有涯，而知也无涯。」（📕这个引用不相关，应去掉）在知识和经验有限的情况下，群体的认可可以作为一种参考，帮助我们快速做出决策。在现代社会，这种现象尤为常见。例如，消费者在选择产品时往往会参考销量和评价，认为高认可度的产品更可靠。

然而，从另一个角度看，过度依赖群体的认可也可能导致个体失去独立思考的能力。柏拉图曾警告说：「不要让对话结束于纯粹的个人喜好之争。」（📕再一次，不相关的引用。）当认可度成为评判一切的唯一标准时，那些不被大多数人理解或接受的新观念和创新就难以获得足够的关注和资源，从而抑制了社会的多样性和创新性。（📕这句话太拗口了，且内部逻辑细看有问题：当认可度成为评判的唯一标准时，那些不被大多数人理解或接受的新观念就会被压制，这会抑制了社会的创新性，也会影响社会的多样性。）

在中国近年的热点新闻中，共享单车的兴起便是一个关于认可度双刃剑效应的例子。最初，由于其便捷性和环保性，共享单车获得了极高的认可度和广泛的推广。然而，随着时间的推移，过度投放和管理不善导致了城市管理的混乱，这也反映了盲目追求认可度可能带来的负面影响。（📕共享单车是代表认可还是不认可？错误的例子，赞同网友看法。）

那么，如何在尊重群体认可与保持个人独立（📕加「观点」二字）之间找到平衡？关键在于培养每个人的独立思考能力和批判性思维（📕去掉「独立思考能力和」）。教育应当不仅仅教会学生如何获得认可，更要教会他们如何质疑现有的认可（📕批判性思维的意思不对。更要教会他们对一个判断的结论、逻辑、证据进行自己的思考），如何勇于提出和尝试新的可能性。只有这样，个体才能在群体的影响中保持自我，社会也能在多样性中实现健康发展。（📕这句空话可去掉，纯属个人观点，不要再往下加一句推论。）

最后，正如古代圣贤所倡导的那样，个人的道德修养（📕道德修养怎么来的）和价值判断（📕价值二字从何而来）不应仅仅依赖外界的认可。真正的价值，是在于个体能否在群体的声音中，听见自己内心的声音，坚持正确的判断与选择。如是，我们既不会被盲目的群体认可所左右，也能在复杂多变的社会环境中，找到属于自己的坚实立足点。（📕这个看似眼花缭乱，但观点乱七八糟，可简化：最后，个人的判断不应仅仅依赖外界的认可。在群体的声音中，个体应当听见自己内心的声音，进行正确的判断与选择。这样，我们就不会被盲目的群体认可所左右，从而能在复杂多变的社会环境中找到属于自己的坚实立足点。）

综上所述，认可度虽是评价事物的一种手段，但不应成为唯一的标准。在追求广泛认可的同时，我们更应重视个体的独立性和创造力的培养。通过教育和社会实践，提高人们的独立思考能力，才能使我们的社会更加健康、包容和充满活力。（📕AI 撰文典型的无效总结，可去掉或简化。当然，我也觉得这个里面引入了太多的概念，混乱了，仔细修改不如去掉。）

### 61

方军 2024-06-11

math note 蛮酷的

但我觉得还是不需要

是接近大众了

但还是用数学公式本身更好吧

我不了解数学家用什么，我觉得 latex 还行

（我的错误倾向性太明显了，一是倾向于复杂工具，二是倾向于现有工具）

### 62

方军 2024-06-11

[过去一年有关大模型应用构建的干货经验之战略篇](https://mp.weixin.qq.com/s/Nh7l_ly791thmsXG7hm5Cw)

### 63

方军 2024-06-11

Andrej Karpathy 对 Apple Intelligence 的评论：

「事实上，我非常喜欢苹果公司发布的 "Apple Intelligence"。在苹果公司，人工智能是整个操作系统的基础，这一定是一个非常激动人心的时刻。几个主要的主题。

第 1 步，多模态 I/O。启用文本 / 音频 / 图像 / 视频读写功能。可以说，这些都是原生的人类 APIs。

第 2 步，Agentic。允许 OS 和 apps 的所有部分通过 "函数调用" 进行互操作；用于内核进程的 LLM （编注：可以再去复习一下 AK 的 LLM-OS，Sina Visitor System），可根据用户查询安排和协调它们之间的工作。

第 3 步，无摩擦。以高度无摩擦、快速、"始终在线" 和情景化的方式全面集成这些功能。无需四处复制粘贴信息、提示工程等。根据需要调整 UI。

第 4 步，主动性。不要根据提示执行任务，而是要预测提示、提出建议、主动行动。

第 5 步，授权分级。尽可能使用设备端算力（苹果芯片非常有用，非常适合），但允许将工作分派到云端。

第 6 步模块化。允许操作系统访问和支持整个不断增长的 LLM 生态系统（例如 ChatGPT 合作公告）。

第 7 步隐私。

我们很快就要进入这样一个世界：你可以打开手机，随便说点什么。它会回应你，而且它认识你。一切都很顺利。这太令人兴奋了，作为一名用户，我非常期待它。」

[Andrej Kar... - @i 陆三金的微博 - 微博](https://weibo.com/1706699904/Ns2eixNAb)

### 64

方军 2024-06-11

095 苹果 AI 发布会所反映的

苹果 AI 发布会新信息不多，什么集成到系统中，什么能够理解界面，什么集成到 Siri，什么 iPad 计算器…… 这些所反映的所有一切，都是苹果还在延续自己的思路：把 AI 用到现有的应用中。

前一段时间有人（Arc 创始人）讨论，未来 AI 消费应用的形态是什么？他的观点是，不是 ChatGPT 这种对话形态。按他公司的产品到现在的发展，似乎应该认为类似 Arc Search 那种智能搜索形态。（他的观点较为接近苹果。）

Bing、Perplexity、Google、百度也做了自己的回答，Bing、Perlexity 是，聚合搜索，Googe、baidu 是结果集成。其他的大型应用，比如 FB、Twitter、微信、抖音 / Tiktok，还都没有做出正面的回答。

我觉得，苹果的思路肯定是不对的，它目前所做的一切，正如 Apple Intelligence 所暗示的，为现有的设备、OS、应用增加上智能。它似乎没有尝试回答上面这个问题：AI 的应用应该是什么样？

我目前的认识是，未来的 AI 当然会融入到各种应用里面去，但是，一定会冒出一种主流的形态，这个就是类似于 ChatGPT 这样一出来就是的样子，对话式交互界面。

至于对话式交互界面会进一步怎么发展，我觉得 Vercel 所尝试的 Gen UI 是合理的，也就是在对话中由 AI 生成卡片，协助用户与复杂系统交互。—— 这个还是试验期，近日看到 LangChain 也推了一个实验。

但不管怎样，我认为，未来的形态是对话式。

实际上，一个产品推出时的样子就是最终的样子，这样的产品演进其实我们看过不少。Windows、搜索、亚马逊 / 淘宝、QQ / 微信，表面的样子实际上一开始就基本上是那样了。

方军：给苹果出个馊主意，反正馊主意不怕丢脸。

现在这个时间点其实就是类似于过去那种，Yahoo 要不要自己出个搜索的时机。

苹果应该出个 ChatGPT 应用，免费，哪怕就是套壳 OpenAI 模型，然后慢慢地优化。不要搞现在这些各种莫名其妙的集成。

2024-06-11 15:07

方军：摘：不论你使用了什么端到端的加密通讯，如果结果能够在屏幕上显示，在声音中播放，系统级别的 AI 都是可以拿到内容的

2024-06-11 17:39

### 65

方军 2024-06-11

中国工科研究生最常见的错误英语用法

The Most Common Habits from more than 200 English Papers written by

Graduate Chinese Engineering Students

作者是 Felicia Brittman

www.cse.cuhk.edu.hk/~cslui/english_writing1.pdf

摘：

今天看到一篇比较有意思的文章，是一位美国教授写的，内容是作者阅读了 200 多篇中国工科研究生写的英文论文，从而总结出的一些最常见的错误英语用法，我把整篇 17 页全部读完了。

作者在导言里说到因为中国大多数大学都要求理工科领域的博士和硕士研究生在国际期刊上至少发表一篇英文论文，但有很多因素会导致这项任务难以完成。首先是中国学生在以前的英语学习中可能不够重视写作，更别说学术写作了，其次是大多数人从未在英语国家生活过，另外很多人缺乏学术写作的课程。

作者也提到了在论文写作中有些内容虽然使用了翻译检查器或者 word 自带的文本检查，但遗漏冠词或汉英口语等习惯，都不会被这类软件工具识别到。所以很多语法错误，口语化和格式问题，学生在编写时就没有注意到，但作者在批阅的时候很容易就会发现。

最常出现的错误首先是 "a/an/the＂的用法问题，出现这种情况的原因是因为普通话中没有直接对应的冠词，所以导致了在英文论文中，冠词在需要的地方省略，在不需要的地方使用或造成词不达意，这三个词也有互相错误代替对方的使用，也就是在该用一个代词的地方用了另一个代词。

另一个常见的错误是长难句滥用，作者说是因为学生在中文写作中，会将几个辅助观点放在一共句子中来组成证明逻辑，但在英文中主要观点和辅助观点通常是在不同的句子中的。（但本人认为，一味的使用长难句可能是跟我们这从小到大的英语应试教育写作打分有关），作者提到了避免长句的方法是将每个句子限制在一个或者两个主题上，如果想强调之间的逻辑关系，应该用分号。

另外作者还提到了主体开头没有 thesis statement 却把原因时间等信息放在最前面进行铺垫的问题，还有不会使用＂which/that＂的问题，分不清＂Respectively/Respective＂的问题，过度不适宜地使用＂In this paper/In this study＂，不会在论文中表述数字和方程，以及各种表达格式的问题。

作者在每个问题后面都举了非常多的例子和避免发生这些问题的方法，我个人是建议很值得一读的。

### 66

方军 2024-06-12

重要论文：The Prompt Report

A Systematic Survey of Prompting Techniques

Generative Artificial Intelligence（GenAI）systems are being increasingly deployed across all parts of industry and research settings.

[Paper page - The Prompt Report：A Systematic Survey of Prompting Techniques](https://huggingface.co/papers/2406.06608)

[[2406.06608] The Prompt Report：A Systematic Survey of Prompting Techniques](https://arxiv.org/abs/2406.06608)

### 67

方军 2024-06-12

气血的说法也许不合理，但其实换个词就好了，比如精力，比如 energy

还可以再换一种说法：我们怎么样活在数字世界与实体世界。就我自己而言，我发现不拿手机，拿着相机街头走走拍拍的时候，感受很不一样。

AI 是把我们进一步吸入数字世界，还是是一个契机，让我们走向新的平衡？

摘：我分析了我爸妈是如何在这个年纪气血足精力旺盛的，就是：都不爱刷手机

我爸闲着就出去溜达，在街心花园唱歌，去图书馆借书，给我妈当司机；

我妈这些年爱上研究面食，她不用面包机揉面，她就喜欢自己揉，揉的那筋膜薄似纸，

烤好面包后再坐着我爸开的车去亲友家送面包。

手机吃空了我们这代和下一代的气血。

### 68

方军 2024-06-12

我也挺喜欢打比方的，但这个好像大家批评得对。

最近看，AI 用的解释都不是很好。

给孩子解释下什么是 Cookie？

互联网小知识给孩子解释下

A：这种解释对我来说往往比原本的解释更难看懂。

打比方打的不好不如不打。

B： 一个假装教，另外一帮假装学

另外，我一直关注一个教育科学的 twitter，内容很好，可是看得很痛苦，因为它所有内容都是很难看懂的手写体。

### 69

方军 2024-06-12

吴恩达老师开源了一套 AI Agent 翻译工作流 Translation Agent。

[智能翻译助手：让长文本翻译、专业术语和方言翻译变得更简单！（附 PDF 翻译工具）](https://mp.weixin.qq.com/s/GeaghOAsivpTOvLj0qQmug)

工作流主要分三个步骤：

1. 通过指定大语言模型（LLM）进行语言之间的翻译；

2. 对翻译结果进行反思，并提出改进建议；

3. 再根据这些建议进行优化翻译。

另外，这也是高度可控的翻译工作流，你只需通过修改提示词，就可以指定语气（正式或非正式）、地区等，还可以提供专业术语表来确保术语翻译的一致性。

GitHub：github.com/andrewyng/translation-agent

宝玉之前的分享：

[直译、反思、意译：提升 GPT 翻译质量的一种新策略 | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/translator-gpt-prompt-v2?continueFlag=ab10d20fe397510507cf2965090ad254)

### 70

方军 2024-06-12

看到一位老师 3.5 万字，包括几乎与文字同等数量图的一篇超长报告（AI 相关的）。

很是感慨，简言之就是，资料太杂乱，看似梳理、其实没有梳理。

不过我反过来反思自己，我也经常准备这样的材料，当然不会这么夸张。反思一下：

1. 这样的材料是自己的梳理，对自己价值有

2. 对他人，不是紧密的了解的，别人跟不上，感受不到价值

3. 但这种资料，遇到了还是应该当成一个精心梳理的，用它来做辅助

4. 如果要向他人传递信息与知识，还是要用自己的话、自己的逻辑简写。

5. 先有轮廓，再填充细节。

近日其实在准备一个类似的，我大幅度做了删节。

如果从这位老师的实践中找什么教训的话，我觉得是，不要认为 AI 能够帮忙轻松地产出文字，就大量地产出。AI 产出的文字明显与他自己的比，明显是相对质量较低的（当然比外面看到的普通人写的还是要高。）

再一次，我不太会掉进类似的坑里面去，因为我宁愿写得段、有错字，也要自己写。

---

摘，赞同：

yangyi：专业的人和业余的人，对当下 AIGC 效果呈现了两种截然不同的态度

专业的人认为 AI 虽然只能做出 30 分的内容，但已经解决了生产材料过程中比较难的地方，可以通过各种后期手段达到一个比较优质的成品

业余的人认为 AI 只能产出 30 分的内容，根本没法用

专业的人是 AI + 人，AI 来生产内容素材，人来做后期优化

业余的人是纯 AI，期望傻瓜化一键达到预期

大炮只能在会打仗的将军手里发挥作用

给到普通群众，它唯一的价值就是放在展厅中展示

现阶段应该更关注 AI 如何帮助专业的人生产专业作品，在这个过程中通过工程运作，填平业余与专业的鸿沟

填的多了，AI 自然也就可以专业到傻瓜化一键了

### 71

方军 2024-06-13

摘：大语言模型的终身学习综述（Towards Lifelong Learning of Large Language Models：A Survey）

随着大语言模型在各个领域的应用扩展，它们适应数据变化、任务转变和用户偏好的能力变得尤为重要。

传统的静态数据集训练方法不足以应对真实世界信息的动态变化。

终身学习，即持续学习，帮助大型语言模型不断学习和适应新的知识，同时保留已学信息，防止遗忘。我们的调查终身学习的全景。

将策略分为两类：

基于新知识的整合方式：内部知识，LLM 通过完整或部分训练将新知识吸收到其参数中；

外部知识，不更新模型参数，通过外部资源如维基百科或 API 引入新知识。

我们的调查的主要贡献包括：

(1）引入了一种新分类方法，将终身学习的广泛文献分为 12 种场景；

(2）识别所有终身学习场景中的常见技术，并将现有文献分类到不同的技术组；

(3）强调了模型扩展和数据选择等新兴技术。

在大模型（LLM）应用到各个领域的过程中，它们适应数据持续变化的能力变得越来越重要。

[qianlima-lab/awesome-lifelong-learning-methods-for-llm：This repository collects awesome survey, resource, and paper for Lifelong Learning for Large Language Models.](https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm)

[[2406.06391v1] Towards Lifelong Learning of Large Language Models：A Survey](https://arxiv.org/abs/2406.06391v1)

### 72

方军 2024-06-13

这个有意思，似乎更好地总结了小红书体的特点（之前很多人就只看到表情这个表面特征）。摘：

经过一些观察和实验，发现在地瓜上要有流量不能有条有理的写一整个逻辑流程。而是要把一件大的事情拆解成无数个小环节，每个小环节发一条地瓜。

比如写旅游攻略，我之前写攻略应该算蛮详细和全面了吧，从怎么了解景点，到怎么看优惠票，怎么买票，怎么坐车，怎么安排行程，怎么购物，各地方卖些什么，等等等等，一长篇，全部涵盖。但在地瓜不能这么写，得拆开，拆得越细越好。

成田机场怎么去 T3 航站楼

T3 航站楼怎么去市区

机场去市区的长途巴士票怎么买

新干线车票怎么买

新干线闸机怎么过

公交车怎么坐车怎么付钱

坐公交怎么看站

关西机场坐飞机要提前几个小时到

在福冈哪里扭蛋最多，哪个商场的几楼

哪个景点的门票自助机器怎么操作

XX 景点的 XXX 在哪个位置

等等等等，一定要细，越细越好，一条地瓜就说这一件小事。很多人把地瓜当解决问题的搜索引擎，遇到问题就一定是一个很具象的问题，具象的问题就需要具象的解答。

### 73

方军 2024-06-13

这个女孩真牛

大爆冷！中专女生闯进全球数学竞赛 12 强，学服装设计的她怎么做到的？

（细节：她老师很棒，应该是同一个）

（这是一篇对话）

[爆冷杀入「阿里全球数学竞赛」的中专女生：梦想就是考大学](https://m.thepaper.cn/newsDetail_forward_27721337)

[2024 阿里巴巴全球数学竞赛试题 & 答案](https://mp.weixin.qq.com/s/yl90fpmvQpUKvgnxTkoj9w)

下为看起来根据对话写的报道

作者：橙柿互动记者沈积慧 06-13

一个名叫威尔的麻省理工学院清洁工，在楼道的黑板上默默解出了数学教授蓝勃出的巨难数学题，数学天才少年的身份才被发现。27 年前，一部叫《心灵捕手》的美国电影上映，一举拿下了第 70 届奥斯卡最佳原创剧本和最佳男配角两项大奖。

17 岁的姜萍，成了现实里的威尔。

今天，2024 阿里巴巴全球数学竞赛（简称「阿里数赛」）公布决赛名单结果，最终有来自全球各地的 801 名数学高手成功晋级，排在第 12 位的姜萍显得有些特别。17 岁的她是一名中专生，花两年自学高等数学，最终以 93 分的高分，名列全球 12 名，秒杀了一众数学专业的选手，也是前 30 名里唯一的女生。

是金子总会发光

17 岁的数学天才少女闯进全球 12 名

今天达摩院公布了阿里数赛公布的晋级名单，今年共有 801 名人类选手挺进决赛，遍布全球 17 个国家与地区，平均年龄 22 岁，主要来自北大、清华、麻省理工、剑桥等知名高校。

除了唯一闯进全球 30 名的女性选手，姜萍也是阿里数赛举办以来首位打进决赛的中专在读生，排在她前面的，清一色来自北京大学、剑桥大学、麻省理工学院、普林斯顿大学、清华大学等专业级数学高手。

姜萍是江苏省涟水中等专业学校的学生，专业是服装设计。

在被马克笔、服装设计图和针线围绕的教室里，课桌上一本全英文的《偏微分方程》暴露了姜萍的与众不同之处 —— 这是高等教育出版社与美国数学会合作精选的经典高等数学专著之一。

初中时发现自己的数学天赋

花两年自学微积分

姜萍是从初中时候开始，发现自己的数感特别好。

平时同学做作业时，姜萍只要朝题目扫上一眼，就觉得没有什么做的价值，因为她更乐意做有难度的题目。「我更偏向于做高等数学之类的，它会给我带来探索的欲望。」

不过当时，家长和老师都告诉她「不要偏科」，「高考不是只考数学」。直到上了专科学校，第一次月考遥遥领先的数学成绩引起了数学老师王闰秋的注意。

王老师向姜萍推荐了同济大学出版的《高等数学》和谢惠民的《数学分析》，在研究多元微积分的过程中，遇到困难的姜萍又继续研究起了 Lawrence C. Evans 编写的《偏微分方程》。

这是一场孤独的自我较量。姜萍说，一开始看高等数学也会觉得有难度。比如关于画图题，她会尝试把它画到一张纸上，然后一直换思路去解决这个问题。自认为排列组合不是特别好的她，遇到困难的时候就去请教王老师。

借助手机翻译软件和一本英汉小词典，目前已经自学了三分之一，达到了数学专业本科三、四年级的水平。

更多的孤独感来自于周边。

学校里所有人都知道姜萍的文化课成绩好，尤其是数学「断崖式领先」。但对于她究竟在「啃」什么级别的数学，大家并不太能理解。在这里，CMO、IMO，或者是「985」「211」，都是陌生的词汇。

姜萍和其他学校比，目前学校的资源不多，平时也不太跟同学不交流数学问题。有一次，姜萍告诉好朋友自己很喜欢数学，朋友很随意地拍了拍她的肩膀说，那你加油。

不过开心的是，除了三门文化课和服装专业课，现在有了更多课余时间跟数学相处，课余或者晚自习的大部分时间都在学习数学。

「数学更像我一个朋友，问题解决出来的时候，特别快乐。做不出来的时候，会很痛苦。」姜萍说。

喜欢数学更喜欢服装设计

很多服装设计灵感来自于数学

和很多同龄人一样，姜萍也是个动漫迷，最喜欢的是火影里鸣人这个角色。「他的经历给人一种积极向上的感觉，对我也挺有激励的。」

在姜萍眼里，数学无处不在。

学习服装要画的时装画和制图，衣服的制作和裁剪，在她看来都跟数学有关。比如她最喜欢也最擅长偏微分方程（PDE），就跟服装设计的画图有相通之处，「他们的对称性太美了。」在服装作画的时候突然冒出来的设计灵感有时候来自于数学。

今年，阿里巴巴全球数学竞赛启动报名时，王闰秋告诉她比赛没有门槛，快递小哥、外卖员也能参加，鼓励她试一试。

刚开始姜萍没什么自信，自己默默去刷了一遍去年的比赛题，然后抱着试试的心理报了名。

比赛持续了两个整天，除了吃饭和睡觉，姜萍一直在做题。「那道几何题我应该没做出来，因为它的作图很复杂，没办法直观地表现出它的立体形态，没有很好的思路去解决它。」

喜欢数学还是服装？姜萍说，自己更热爱服装。「如果学习服装设计是我的 plan A，那探索数学世界便是我的 plan B，希望我的 plan B 也能被看见。」

偏微分方程通常用于描述多个自变量之间复杂的相互作用，在种种因素下，原本考上了普通高中的姜萍来到了这所学校。

尽管王老师在两年前把高等数学教材递给姜萍的时候，主要是觉得她的天赋和热爱不应该被辜负，但他也存了一点希望，希望数学这个变量，未来或许能推动姜萍的人生等式出现微小的改变，带她飞向远方未知的世界。

[头条文章](https://weibo.com/ttarticle/x/m/show/id/2309405044834044870784?_wb_client_=1)

爆冷杀入「阿里全球数学竞赛」的中专女生：梦想就是考大学

澎湃新闻记者杨佳吟

2024-06-13 20:46

来源：澎湃新闻

6 月 13 日，2024 阿里巴巴全球数学竞赛公布决赛名单，江苏 17 岁的中专生姜萍以第十二名的成绩晋级。澎湃新闻（www.thepaper.cn）注意到，进入决赛的前 20 名选手中，除了姜萍，其他人均来自北大、清华、麻省理工、剑桥等知名高校。

公告显示，801 名进入决赛的选手来自 17 个国家和地区，平均年龄 22 岁。江苏涟水中等专业学校女生姜萍预选赛拿到 93 分，是这项赛事举办以来第一位进入决赛的中专在读生。

姜萍。竞赛主办方供图

据介绍，姜萍的专业是服装设计。她对数学的敏锐从初中开始，进入高中后，她的数学成绩引起数学老师王闰秋的注意，建议她从同济大学的《高等数学》开始，当她对其中复杂的证明感到困惑，又推荐了谢惠民的《数学分析》。在研究多元微积分的过程中，姜萍又遇到困难，开始学习《偏微分方程》。

姜萍。竞赛主办方供图

由阿里巴巴公益、达摩院举办的这项竞赛以不设报名门槛、趣味性著称，已连续举办六届。今年的决赛将于 22 日 0 时至 24 时举行，选手可选择任意连续的 8 小时作答，获奖者可分享 400 多万元奖金。

问答

Q：你是什么时候对数学产生兴趣的？

A：初中时就有一点兴趣，高中更深一步了解高等数学。当时是第一次月考数学考得比较好，数学老师找到我，问我对数学有什么想法。我们都感觉彼此对数学特别有兴趣，然后进一步接触到高等数学。

Q：你觉得自己有数学天赋吗？

A：这个怎么说？我对数学敏感。同学平时做作业时，我会朝他们做的题目看一眼，那些题目…… 哎呀真是说不出口，这么说不太好。我更偏向于做高等数学之类，它给我带来探索的欲望，我比较喜欢做有难度的题目。

Q：接触高等数学后，你是怎么学习的？

A：开始看同济大学的《高等数学》，又看谢惠民的《数学分析》，后面才了解 PDE（偏微分方程）。

Q：觉得难吗？

A：开始肯定觉得难。比如画图题，我会画到纸上，然后换思路去解决。另外我的排列组合不是特别好，遇到这些问题会去请教老师，他不会直接回答，而是引导我自己去解决。

Q： 你为什么能坚持下去？

A：学习数学一路比较坎坷，但解出来了会非常快乐。如果做不出来，它会给我带来痛苦。它更像朋友，对我的喜怒哀乐有很大影响。

Q：对数学和服装专业，时间精力是如何平衡的？

A： 我用课余或晚自习时间学数学，两者不会冲突，反而相辅相成。数学无处不在，学习服装要画时装画，还有制图、制作裁剪。服装制图分左右片，在数学中几何图形也有对称美。学数学时有时会突然想到好多思路；服装作画时，我脑子里也会突然产生灵感。

Q：你为什么会参加阿里巴巴数学竞赛？

A：从网上无意看到的，好像有点印象，老师提到过，感觉有点厉害，那我试试看。老师说世界各地无论什么职业，服务员、外卖小哥、医生都能参加。我原来觉得我是不是不配参加，但我觉得说不定是证明自己的机会。我平时很认真地学数学，想自己的能力能展现出来，被人看见。

Q：参加比赛的过程是什么样的？

A：抱着试一试的心态去挑战一下，去了解去年的题，刷了一下。比赛那天看了下线上的题目，然后在线下做的。比赛是两个整天，我除了吃饭睡觉大概都在做。那道几何题我应该没做出来，因为作图很复杂，没办法直观表现立体形态，没有好的思路去解决。比赛结束就是想睡觉。

比完后还是有一点信心，考完也跟老师交流，有些我觉得对的，他觉得不对，但我还是觉得是对的，会有些争论。

Q：学习数学过程中最大的难点是什么？

A：跟其他学校比资源还是少，但我平时会跟老师要材料做些了解。平时我和同学玩得挺好，但不会交流数学。我当时告诉玩得好的一两个朋友，他们会很随意地拍着我的肩膀说：那你加油。

Q：平时有什么兴趣爱好？

A：喜欢看动漫，比较喜欢看那个火影，比较喜欢鸣人这个角色，他给人积极向上的感觉，对我也有激励。

Q：想过这次考得这么好吗？

A：没敢想。就是抱着试一试的心理，没想到会这样。

Q：你老师说快递小哥也能参赛，但你知道和你一起比赛的有很多清华、北大数学专业选手吗？

A：我不知道其他是什么人，我感觉他们厉害死了，我感觉他们很强。我感觉真的好离谱啊。

Q：你感觉自己不够强吗？

A：不够，世界上很强的人很多。

Q：以后更想继续学服装还是学数学？

A：这两方面都特别感兴趣，都放不下，非要选一个，我感觉更偏向服装设计一点。

Q：你有什么梦想？

A：考大学。我就想考大学。但无论未来怎样，数学这个爱好会持续下去。如果服装设计是我的 plan a，探索数学世界便是 plan b，希望我的 plan b 也能被看见。

Q：数学为什么吸引你？

A：我喜欢一步步的推导过程，然后得到想证明的，这使我快乐。（问答由竞赛主办方提供）

### 74

方军 2024-06-14

宝玉：AI 民科和 AI 科学家之间的差距

宝玉这篇文章很有意思，当然我觉得不必说类似于码农这样的自嘲，做应用嘛，和做研究还是有差别的，更何况与吴恩达这样 AI 领域的顶级教育家比，谁都有巨大的差距。（另，我认为未来人们评价吴恩达时，会发现他作为教育家的角色占比非常高。）同时，吴恩达的巨大影响力，也是他提出的模式能够广泛接受的原因，之前也看到顶级AI研究者说，他被教导，50%时间写论文，50%让人接纳论文（具体忘记了，也许当时他说的是2：8），挺真实的，虽然与外人的通常理解差别很大。

宝玉的这组翻译实践总结得特别好，之前给活水分享时，我详细整理过他的想法，我自己演练后，又再分享和使用。

---

宝玉：AI 民科和 AI 科学家之间的差距

民科通常指的是民间科学爱好者，本人呢也算是个 AI 民间科学爱好者，简称 AI 民科。AI 科学家呢，就是那些专业的在 AI 领域有一定研究成果的科学家，比如像吴恩达老师。

最近很多人都在学习吴恩达老师发布的一段[用来翻译的智能体工作流](https://x.com/AndrewYNg/status/1800582171259982289 )：
1. 通过提示大语言模型 (LLM) 实现从一种语言到另一种语言的翻译；
2. 对翻译进行反思，提出建设性的改进建议；
3. 根据这些建议进一步优化翻译。

无独有偶，我在去年也提出了类似的翻译 Prompt：[直译、反思、意译：提升 GPT 翻译质量的一种新策略](直译、反思、意译：提升 GPT 翻译质量的一种新策略 | 宝玉的分享 )，本质上也是在翻译的时候，分成三步：
1. 先直译
2. 反思，指出直译中的问题，提出改进意见
3. 根据直译和反思的结果最终进行意译

所以当很多朋友看到吴恩达老师的工作流后，提到和我的 Prompt 原理一样。一方面我很高兴我写的 Prompt 得到这么多人的认可，另一方面也深感我这样的 AI 民科和 AI 科学家之间的差距。

这差距不在这一段 Prompt，而是在于是如何得到这一段 Prompt 的，以及未来如何创造更多这种有价值的 Prompt。

\## 我的翻译 Prompt 诞生史

那么作为一个 AI 民科，是我怎么得到这一段 Prompt 的呢？

无非是大量的实践后摸索出来的结果。

\### V1: 一次提供 3-5 个翻译结果

最开始，就是在翻译吴老师的 AI 课程字幕的时候，我就尝试用 GPT 去翻译字幕，但是翻译结果随机性很大，有时候翻译的很好，有时候翻译的很差。于是我就想：能不能一次提供 3-5 个翻译结果，然后我就只要去选择最好的那个结果，或者从几个结果中组合一下，省的自己去手动修改润色。

当时我还写了一篇文章《[字幕翻译、书籍翻译的福音，如何借助 ChatGPT 得到高质量的翻译结果？]( 字幕翻译、书籍翻译的福音，如何借助 ChatGPT 得到高质量的翻译结果？ | 宝玉的分享 )》分享我的经验：

> 另外还有一个很重要的技巧，就是翻译的时候，让 ChatGPT 一次提供 3-5 个不同风格的翻译结果，然后你就可以得到很多不同角度的翻译结果，大部分时候，你只要从这几种不同翻译结果里面选一个就可以了。
>
> ChatGPT 这类大语言模型是一种概率模型，根据概率生成内容，有时候很难一次就得到一个很好的结果，所以要得到好的生成效果，可以让它一次多生成几种结果，然后你可以从中选一个最好的，甚至于你可以让它自己帮你选一个。
>
> 有的时候，它能随机出来相当不错的翻译结果，所以大部分时候就只要从几个翻译选项中选一个最好的，或者组合一下。

\### V2: 直译 + 意译

当一次提供 3-5 个结果选择后，效率高多了，毕竟大多数时候只要选择一下，然后复制粘贴出来就可以。但缺点也很明显：
1. 要花大量的时间去手动筛选
2. 由于一次生成 3-5 条结果，所以每次都不能翻译太长的内容，并且每次都要耗费大量的 Token，成本很高

但在反复生成的过程中，我也发现了规律，那就是我选择第一条翻译的结果的概率最低，通常第一条翻译质量最差，而第二条就好很多，第三条和第四条质量有时候比第二条好，有时候差不多，但越到后面越容易偏离原意。

所以我就想，是不是只翻译两遍，直译和意译，就可以得到不错的结果呢？

于是反复试验对比了不同的 Prompt，最终发现：直译 + 意译两遍翻译的效果确实要比只翻译一遍要好很多，并且效率也不错，每次可以翻译比较长的内容，Token 消耗也少很多。

我也试过翻译 3 遍，一方面第三遍的时候结果容易偏离原意，另一方面 Token 消耗略大，所以性价比不高。

当时我也写过一篇文章《[一个简单的 Prompt 大幅提升 ChatGPT 翻译质量，告别“机翻感”]( 一个简单的 Prompt 大幅提升 ChatGPT 翻译质量，告别“机翻感” | 宝玉的分享 )》分享了我的经验。

\### V3: 直译 + 反思 + 意译

我虽然是一个 AI 民科，但也不会只是闭门造车，日常也喜欢学习别人的 Prompt 经验技巧，有一次看到一个 AI 写作的 Prompt（很遗憾记不得哪一个了），会对生成的结果进行反思，提出改进建议，然后再重新生成。当时我就想：能否将其应用到翻译 Prompt 上？在直译之后，对翻译结果进行反思，提出改进建议，然后再意译。

于是我改进了 Prompt，加入了反思这一步，效果确实很好，让翻译质量更稳定，由于反思的内容不算很长，也并没有让 Token 增加太多，所以是一个很好的改进。

我的那篇《[直译、反思、意译：提升 GPT 翻译质量的一种新策略]( 直译、反思、意译：提升 GPT 翻译质量的一种新策略 | 宝玉的分享 )》里面也详细分享了这种 Prompt 的使用方法。

基本上这就是我翻译 Prompt 的迭代过程，作为一个 AI 民科，就是靠的这种不停在实践中总结经验，改进迭代。

但基本上也就止步于此了，后来我也没能再提出更多有价值的 Prompt。

\## AI 民科和 AI 科学家之间的差距

几个月前，吴恩达老师分享了一个视频：《[AI 智能体工作流引领人工智能新趋势](https://baoyu.io/translations/transcript/whats-nex...》，在视频中吴恩达老师建议大家关注 AI 智能体工作流，可以大幅提升 AI 应用的性能，并且他总结了一套智能体设计模式：

- 反思：让大语言模型对自己的结果检查改进
- 使用工具：让大语言模型调用外部工具，如网络搜索、代码执行等
- 规划：让大语言模型自己设计一个多步骤的计划来达成目标
- 多智能体合作：多个 AI 智能体协同工作，分配任务，讨论和辩论想法，写作得到更好的结果

当时我就觉得，这样一套框架一下子就把很多提示工程的经验囊括进去了，绝大部分任务都可以基于这套设计模式往上面套，就可以得到更好的结果。包括我的翻译 Prompt，本质上也是多智能体 + 反思，我还[打过比方]( 简单通俗的解释《一个简单的 Prompt 大幅提升 ChatGPT 翻译质量，告别“机翻感”》 | ... )，就好比英语老师智能体直译，校长智能体审阅反思，中文老师智能体润色意译。

所以当吴恩达老师提出新的翻译智能体工作流，我一点都不奇怪，因为当有了一个科学的设计模式，就能衍生出来有多有价值的 Prompt 和工作流。

同样这也正是 AI 民科和 AI 科学家之间的差距所在，也许 AI 民科通过实践能在某个任务上不断总结迭代，得到一个还不错的结果，但如果不能提炼出一个通用的设计模式或者理论框架，就很难将任务泛化推广到其他领域。而 AI 科学家则能提出一套设计模式，然后通过这套设计模式，将经验泛化到不同的领域，让其他人也能应用这样的设计模式，从中受益。

要想跨越这道鸿沟，仅仅靠实践经验是不够的，需要大量的理论支撑，这也通常是我这样的 AI 民科所欠缺的，希望通过不断学习能弥补这种差距。

（本文同步发布于本人博客：AI 民科和 AI 科学家之间的差距 | 宝玉的分享 ）

[直译、反思、意译：提升 GPT 翻译质量的一种新策略 | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/translator-gpt-prompt-v2)

[字幕翻译、书籍翻译的福音，如何借助 ChatGPT 得到高质量的翻译结果？ | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/how-to-get-high-quality-translation-results-with-chatgpt)

[一个简单的 Prompt 大幅提升 ChatGPT 翻译质量，告别“机翻感” | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/a-prompt-for-better-translation-result)

[直译、反思、意译：提升 GPT 翻译质量的一种新策略 | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/translator-gpt-prompt-v2)

[简单通俗的解释《一个简单的 Prompt 大幅提升 ChatGPT 翻译质量，告别“机翻感”》 | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/two-ai-agents-in-one-prompt)

[AI 民科和 AI 科学家之间的差距 | 宝玉的分享](https://baoyu.io/blog/ai/gap-between-ai-amateurs-and-ai-scientists)

### 75

方军 2024-06-14

转：MCN 机构利用 AI 造谣：1 天写 7000 篇，日收入 1 万

6 月 14 日消息，据央视报道，江西南昌一家 MCN 机构发布数条信息涉嫌网络造谣。谣言内容从文本到图片都是通过 AI 生成的，全程基本不需要人工参与。

经调查，该 MCN 机构实控人王某某通过新闻网站获取热点关键词，并生成文章标题。然后通过 AI 软件，给出关键词，软件便会自动在网络上抓取相关信息，生成几百到上千字的文本，配上貌似和事件具有相关性的虚假图片，从而形成一段看起来信息量很大，实际上子虚乌有的内容。

据悉，通过 AI 软件生成内容的方式，该 MCN 机构短时间内生成了大量文章，最高峰一天能生成 4000 至 7000 篇，最高的一条收入 700 元，初步估算每天的收入在 1 万元以上。

经查，王某某的行为构成传播谣言，虚构事实，扰乱公共秩序。目前，南昌警方依据相关规定，依法对王某某处以行政拘留 5 日，责令涉案 MCN 机构停业整改。

### 76

方军 2024-06-14

摘：今天（发布会当日）关于苹果 WWDC 以讹传讹的信息比较多，主要是拜媒体标题党所赐，把 OpenAI 与苹果的合作当成主要新闻在写，甚至有信息在说 Apple Intelligence 是 GPT-4o base 的，这是完全错误的。

苹果目前集成 GPT-4o 的只有 Siri 对话和 Compose 写作，苹果展示的其他软件中的 AI 能力都是基于苹果自研的多个模型驱动，苹果在其博客中重点介绍了一个 30 亿参数的端侧语言模型和一个云端语言模型，展示了各种测试下的两个模型的成绩表现。

### 77

方军 2024-06-14

这个讨论有意思，现场学习能力

tomkeeper ：看到有人说大语言模型其实并不真「理解」，并不真「懂」，我想起了很多年前帮别人解决的这个技术问题。

当时我既不懂 C 语言也不懂 Linux，但我能基于对文本的推理帮别人解决 Linux 下的 C 语言问题。这个过程和 LLM 的工作原理也多少有某种相似之处。那么我「懂」我所解决的问题吗？到底什么是「懂」呢？总不能说因为我是碳基的，所以我就懂吧。

如果鸠摩智用小无相功模拟的少林七十二绝艺已经没人能分辨出真假，那他就是会七十二绝艺。

//@西雅图黄都督：其实「懂」的意思是你不能现学现卖然后还能记住下次接着卖……

### 78

方军 2024-06-15

096 该怎么用 AI 来协助资料阅读和调研

看到一个应用推了这样的功能：自动全网检索生成答案，找到引用文章，再装载到你的个人知识库，这样就能混合本地与网络的知识源来更好的做研究了！

刚刚，我再搜索资料时，又看到王树义老师的一个介绍：如何用 NotebookLM 综合分析多篇文献？（如何用 NotebookLM 综合分析多篇文献？ ）。

[如何用 NotebookLM 综合分析多篇文献？](https://mp.weixin.qq.com/s/vIdSI8hFLhsM6r7pg0jRxg)

从功能上来讲，目前的 AI 在如上这两个功能上都做的还将就，也许 60 分吧。

但是，从是不是要这样做的角度看，我有很大的疑虑：

我们真的可以用这样的方式阅读资料（消化材料）吗？

不谈学术场景，就说一般性的我们这些工作者的资料阅读，我不是这样读的。我通常的做法是：

- 大量收集必备的资料，然后从中筛选出优质的。

- 仔细阅读其中优质的资料，深度精读其中的最高质量的或最相关的（最有用的）。

- 然后将之应用。

前面的速读，几乎是一种基本能力，总有办法快速地浏览、速读、筛选。这个过程要不要靠 AI 来做呢？也许可以提高效率，但自己扫一遍还是必要的。

以上这两种做法，我看到的疑虑有：

1. AI 产出的中间结果，我们真的可以进一步使用吗？我目前不太信任 AI 摘选的结果（纯以 LLM 看，摘选效果非常不可信）。

2. 不管是学习，还是研究，实际上都是以「个人的大脑」来做筛选器。虽然现在 AI 很强，但它离我们个人的大脑的筛选器差距还是很大。

3. 产出，看似由我们的大脑做组合可以，用 AI 来做也是组合。但是，这种组合交给 AI 做，那我们个人从这个过程中要得到的启发就消失了。我个人觉得弊大于利。

### 79

方军 2024-06-15

有意思的讨论，摘：人工智能出现以后，很多老师非常好地利用了它来备课、来写书、来开展各种教学。但是有一个奇怪的点：既然老师能利用 AI，那么学生为什么不能利用 AI 呢？应该是双方都能使用。换言之，我们只是通过强迫手段，让学生在没有 AI 设备的考场，去参加纸笔考试。目前仅仅是老师们解放了生产力，加速了老师的内卷而已。未来的世界应该是师生双方都要在 AI 的基础上同步升级，构建新的学习模式。如果只是提升了老师的生产效率，那么当前的这种学习模式并没有大的变动：学生还是在课堂里坐着，学生还是看着书，…… 仅仅是教材和老师们利用了 AI 可以更好的「对付」学生。

### 80

方军 2024-06-15

有意思的看法，摘： 今天我一直在用人工智能教我英语，我感觉人工智能教英语有一个非常大的优势：

平时我向英语老师请教，是不敢张嘴的，总担心说错了，老师会笑话我，所以很难获得进步。

而且毕竟英语老师是人，所以我要担心问一些蠢问题，老师会不会不耐烦。

而问人工智能就没有这两方面的担忧，我既可以不要脸的胡说乱说，完全不用担心语法的错误，也不用担心发音的错误，更可以随时问一些很愚蠢的问题。

只要他能辨别，就会非常耐心的回答，非常非常耐心，即便一个问题反复问很多遍，让他换不同角度解释很多遍，他也完全没有完全没有一丁点情绪，这是普通的人类根本就比不了的。

但是由于现在人工智能技术还不成熟。对语音的辨别能力还很差，所以如果你也想用这个方法学习，还是要求要有一定的英语基础，比如掌握足够多的词汇，否则他说话你听不懂。你的发音也还是不能太差，太差了的话他辨别不了。

不过我觉得再过一两年，等人工智能技术再成熟一点，或者说等外国的人工智能技术开源了，那么我想人工智能软件辨别力就会变强。那时即便你词汇不多，发音很差，他也能跟你交流了。

最近一年，我和各种人工智能交流了很多很多之后，我得到了这样一个奇怪的，且自相矛盾的结论：

1 英语专业完蛋了。2 英语越来越重要了。

看上去是自相矛盾。但其实并不矛盾，反而是逻辑自洽的：因为有了 ai，所以英语专业完蛋了，这反而导致英语这门科学越来越重要了，也因为英语这门学科越来越重要了，人人都必须当母语一样学习，所以英语专业完蛋了。

你如果觉得费解，那说明你和人工智能接触太少了。

方军：语音方面好像有更大优势，今天试了一个，很不一样

2024-06-15 18:26

陈牧之回复方军：方军老师使用的是哪一个呢

2024-06-16 12:06

方军回复陈牧之：在试一个叫 enjoy 的软件，它主要在音调方面有些特别设计

2024-06-16 12:12

### 81

方军 2024-06-15

人文学术界啊

https://link.springer.com/journal/10676

摘：10 页纸的论文，发表在 Springer Link 出版的正式刊物 Ethics and Information Technology 上，详细论证了 ChatGPT 既不是 hallucination（幻觉），也不是 lie（撒谎），纯粹就是 bullshit（胡说八道）

### 82

方军 2024-06-16

阿当：AI agent 的落地，需要技术流的产品经理

包括各种人工智能界的大佬和互联网大佬、VC 大佬，都预测未来两年内，AI agent 会大爆发 — AI 的发展重心，将由基座大模型转到应用层落地。

什么时候呢？都在等。等什么？等聪明的公司打个样出来，好抄作业。我们抄作业很厉害的，后发先至，弯道超车一再上演。这是优势，同时也很可悲。

为什么 AI agent 的落地很困难？因为：

1、LLM 编程和传统编程不一样，有很多新概念和制约，没有技术背景的产品经理（包括公司老板等管理层），是完全没办法切入的。他们其实是处于毫无抓手的状态。他们只能等 LLM 编程理念已经完全普及开来之后，才会开始找到感觉，而这可能得三年之后了。

2、懂技术的程序员关注点往往在框架、api、语言、工具、性能等方面，他们更多的时候是在和同行卷八股，卷 api 的熟练度或更底层的实现原理。而对于产品需求，他们习惯了听产品经理、老板提需求过来，开发实现岀来。他们没有「技术产品化」的思维。

网上有一段程序员攻击乔布斯，说他不懂技术，而乔布斯说技术产品化比技术本身更重要，这个视频很值得思考。视频看这里 http://t.cn/A6QAYsrs

3、LLM、autogpt、multi-agent、langchain、垂直大模型．．. 都太新了，技术和概念在大爆炸和快速迭代，产品可能性、商业化、最佳实践都在探索期，人才储备严重稀缺。

4、深度 AI agent 需要行业资源，不是纯互联网、纯 AI 公司擅长的。纯互联网公司在卷大模型基座，希望可以成为入口，建立生态。openai、文心一言、豆包无不如此，他们的精力不在应用层。他们也都做了 gptstore 之样的 agent 市场，但收获的都是些非常浅的三方 agent，浅到 function calling 都不用，写了个 system 人设，了不起再拖几个 pdf 作为外挂知识库就完事了。这可不是真正的 agent。。。

真正有行业资源的，倒是真可以做 AI + 的项目，让 agent 在行业落地。问题是，基于上面提到的 123，谁去立项？谁去做从 0 到 1 的实践？

破局只有两个可能性：要么行业出现一个成功案例，大家一窝蜂开始去抄；要么公司内部出现技术流产品人，有能力从 0 到 1 去做技术驱动的产品，同时能说服公司高层，得到立项的支持。

AI agent 毫无疑问是「技术驱动产品创新」，而不是「资源整合驱动产品创新」，所以没有技术背景的老板们是入不了局的，特别是「没有可以抄的产品，市面上也缺人才储备」的当下。这种项目只可能是立下而上立项。

但你想想吧，这位员工要非常懂前沿技术，有产品思维，有热情，有执行力，有从 0 到 1 没作业可抄的创新能力，公司有行业资源，有团队有资金，能说服管理层立项。同时满足以上条件，概率有多大？

======

既然我说 gptstore、文心一言、豆包里的 agent 不够看，那么什么样的 agent 是够份量的呢？

以公司行政行业为例。产品分化三个时代：

1、纸质办公时代。所有的信息是通过纸质文件留档和传递的，流程也是纯线下跑窗口。

2、电子化办公时代。信息是通过电脑编写的数字文件，保存在数据库、硬盘上。信息可以通过电脑来编辑、复制和分发。流程可以通过系统来建立 SOP，可以纯线上完成。比起纸质办公时代进步了非常多，但办公的仍然是人。数字化的是「文档」和「流程」。

3、智能办公时代。智能办公时代不是指 office 的 copilot，那只是个辅助员工的 AI 工具。我指的是 AI 可以进一步将「人」也数字化，用虚拟员工代替真人员工。比如说，用 AI agent 设计一个 hr 团队，团队里有 hr 总监，有招聘专员，人才计划专员，培训专员，商旅专员。每个人技能不同，责职不同，就像真实的员工一样。不同的是，这些员工只存在于数字世界，7x24 办公，不跳槽不会劳动仲裁不会要求涨薪水不会摸鱼不会闹脾气也不会玩办公室政治。

以出差流程为例，以前你需要请一个真人 hr 帮你订机票，酒店。hr 根据你的需求去携程查机票和酒店，合适就下单，然后告诉你订单信息。如果机票或酒店有问题，会通知你．并给你一些替换方案。你选择一个方案后，hr 会用新方案下单，下单成功后再通知你。而用虚拟员工后，你向 AI hr 提出差需求，AI hr 会和真人 hr 一样的流程，去查携程，给你反馈，帮你下单。

技术能实现吗？能。尽管当下可能还不成熟，但这个方向已经可以落地了。AI agent 的核心模块包括用做大脑的 LLM，用做手脚的 tool，用做记忆的 memory，用做处理流程的 plan。上面描述的出差流程完全可以用虚拟员工处理。

而我们再向前思考一下：财务、法务可以用虚拟员工吗？比如记账、归档电子发票、出资产负债表、审核合同、盖电子章。。。尽管一些事情肯定需要人工审核，但技术可行性是已经具备了。

做虚拟员工难吗？一点也不简单。有些流程虚拟团队内部即可完成，有些流程需要跨团队完成，有些需要和公司外部交涉。有些数据是需要访问权限的，有些是可以以非机构化数据存到 RAG 里的。有的流程重要又紧急，有的流程是周期固定的。不同的岗位需要不同的专业技能。不同公司有不同的行业特殊性，需要定制流程和岗位。。。

做这么一套 multi-agent 的系统，技术上已经有可能性了：agent 和面向对象抽象混用，面向过程的自助填表和面向目标的 agent 虚拟员工混用，微服务和 tools、消息对列和 chain、结构化数据和非结构性化聊天、会议纪要、文档、合同、数据安全和 LLM 侵入、权限管理和任务调度、分层、封装和接口、经典 GUI 和多模交互。。。要考虑和处理的事情特别多，是挑战，也是创新的游乐场。

目前，agent 落地最前沿的是 windows 和 ios，新系统都将 LLM 融入到了系统底层，重构了系统架构。可以好好偷师一下他们的设计，从技术架构、产品架构和交互设计上不妨都跟一下，这个比 chatGpt + gptstore 模式复杂了 100 倍，是真正的 agent。行业应用方面，比如办公、健康管理、教育，我很期待哪个领头羊可以率先出个 agent 的成功案例出来。

### 83

方军 2024-06-16

这个提示语思路（伪代码）很赞：

[Prompt 高级技巧：借助伪代码精准的控制 LLM 的输出结果和定义其执行逻辑 | 宝玉的分享](https://baoyu.io/blog/prompt-engineering/advanced-prompting-using-pseudocode-to-control-llm-output)

[刘海：Prompt 纵向研究的最新成果：伪代码提示词 - 飞书云文档](https://waytoagi.feishu.cn/wiki/MjUDwTbq9iUtBrkskPXcpfOHnPg?continueFlag=6015d9d2e29d32029a5f2e446cb95b31)

相关摘录：

尹相志老师用 GPT-4o 伪代码的方式做绘本生成，真的是见鬼了，这样也行，超强。

透过伪代码可以让模型更能执行复杂的控制流程，例如 for loop 回圈中呼叫自订 function。

实际的 prompt 如下，可产生15页画风一致的小蝌蚪绘本 (实际执行若 gpt-4o 停下来，请输入继续即可):

story=(请你角色扮演成一个畅销的童书绘本作家，你擅长以孩童的纯真眼光看这世界，制作出许多温暖人心的作品。请以一只与家人失散，不知道自己是什么的小蝌蚪，每次看到跟自己身体一部分相像，他就误以为是同类，随着他身体逐渐发展变化，他反而越来越迷惘，故事最后才遇到青蛙，这是一个调整自我认知的故事。) 
print(story) 

pages=(将故事大纲细分至预计15个跨页的篇幅，每页需要包括(text，image_prompt)，第13页才可以出现青蛙，在这之前应该要让小蝌蚪的自我认知旅程更多元化) print(pages) 
style_base=(基于以上故事，请思考大方向上你想要呈现的视觉效果，这是你用来统一整体绘本风格的描述，请尽量精简，使用英文撰写) 
print(style_base以及将style_base翻译成中文) 

def image_generation(image_prompt, style_base): 
   final_prompt=请根据[image_prompt]以及[style_base]补充色调、背景描述、具体风格、画面细节，以及主角小蝌蚪目前的颜色(一开始是黑色)、体型(没有脚的蝌蚪直接叫做a tadpole即可)、尾巴长短，有几只脚必须具体的描述，同时至少3个效果词(光照效果、色彩色调、渲染效果、视觉风格)和1个以上的构图技巧，以构成完整的prompt，请设定随机种子值为42，且不要有文字出现在图中，使用英文撰写

  请根据以下引数内容{size="1792x1024",n=1,prompt=final_prompt}来调用内部工具dalle以生成并展示图片 

这不是个python任务 请实际执行此伪代码为每页生成图片，确保必须等待该页图像生成后，才可以进行下一页的操作 

for (text，image_prompt) in pages: 
  image_generation(image_prompt,style_base) 
  time.sleep(5)

### 84

方军 2024-06-16

我最近也有点觉得知易行难，同时，人们其实不愿意行动，昨天讲 ICDO 方法人很少，事后反思是，人们不想学方法，只想要结果，他们的想法也没错，但世上没这样的好事。

摘：嘴说全是容易，动手全是困难。

有些人讲，学 AI 容易，去 B 站看看视频，找找开源的软件，就学会了。

然后动手的时候发现，第一步，github 都打不开，欲哭无泪。

第二步，装了开源软件，一堆模型都不能用。

还有人说，找找免费的文生图模型，就能拿去赚钱了。

这就等于说，我会汉堡，比麦当劳做的还好，然后我就能开麦当劳了。

你会做汉堡，跟你能开一个麦当劳中间，可以差了无数级，无数级。

首先，你要租个地方吧，租金能付得起吗？

你会选址吗？

做汉堡、可乐，薯条的设备，你买得起吗？

要有中央厨房配送吧？

我记得以前肯德基加盟的加盟费是 800 万，现在不知道是多少。

喷子们嘴一张，就把这 800 万抹平了，可是真厉害啊。

搞 AI 也是一样的。

你搞个文生图模型，你显卡呢？

4090 要 1.5-1.6 万了，你一张显卡能支撑几个人呢？

1000 人在线，如何做到流畅？

那么多图片，你存哪里，要不要花钱？

1 万人在线呢？

有些人，觉得搞房子也很简单，外面不都是贷款广告么？

你往上一提交，立刻就被拒了。

你借个几千块，几万块当然不难，但是你借 1000 万呢，一个亿呢？

蚂蚁并不是放大 1 万倍就能变成大象的。

### 85

方军 2024-06-16

Heygen 真是这一轮 AI 中最优秀的应用产品之一。

以下是对创始人的访谈。

[HeyGen：AI 创业，我们如何在 7 个月内达到 100 万美元收入](https://mp.weixin.qq.com/s/zCWSynbvyAYn2Z9iXfnTYA)

### 86

方军 2024-06-17

向 AI 提问一年，我最念念不忘的一个主题是

我真是到今天也没法相信，AI 可以替代我们读数百万字（还没有收集、整理好的资料），然后帮我们抽取、组织成简单易懂的文字。

一是它做不到，理解它工作原理的人就知道它做不到。1）它仅仅是预测模式（预测下一个词），2）抽取相似片段，如 RAG，3）它没有这样宏大的推理。

二是，即便它能做，我们就让它做吗？不，我们自己做的过程，是学习的过程。我们是在训练自己的大脑。用 AI 替代这个，是蠢人的选择。

补：这四篇文章好像都不错，要抽时间看下

[AI 作为写作助手的四项研究](https://mp.weixin.qq.com/s/7MEWEWt9XfZxLIrERJg_Tw)

### 87

方军 2024-06-17

容错率是个有意思的话题，当然我不是很在乎纯银说的这种社会层面的容错。我觉得，用 AI 需要让自己高容错，否则实际上根本用不起来。

纯银：跟朋友聊到一个产品职场案例。

小团队，产品方案研发时，还原不精确，视觉交互总是有出入。但功能没问题。QA 一是忙不过来，二可能也对视觉交互没这么敏感。

PM 很生气。

产品主管说，我看过还原不精确的视觉交互部分，完全不影响结果，但的确与你的方案有出入。现在团队资源和进度就是这样，我不会去跟研发 battle，如果你很在意高还原度，可以每次验收时自己花 2 小时检查反馈。

PM 还是很生气，觉得这又不是老子的活，凭什么让老子去擦屁股。

产品主管没理他，完全不影响结果嘛，没必要为了抠细节去和研发团队产生摩擦。

这件事也成了该 PM 辞职的导火索。

我们聊天的时候，我说，这就是典型的职场低容错率。既不能接受还原不精确，也不愿意自己每个月投入 2 小时去检查反馈。你说他错嘛，他也没错。但如果每个人，每件事都按照低容错率来运行，整个氛围会很累很紧张。

所以好的氛围是，不重要的事高容错率，包容性强；只对少数影响结果的事情低容错率，因为值得斤斤计较。

以及，如果该项目的结果越来越好，团队扩张之后，有了更多人力支持，其实还原不精确这件事也就自然解决了。

公司经营，社会运行，都是如此。

你说他错嘛，他也没错。但容错率越低，则公司经营与社会运行的成本越高。成本的高低最终会分摊到每个人身上。

这些年，感觉整个社会的容错率越来越低，包容性越来越弱。

社交媒体就更夸张了，直接进化成了圣母社会。

我见过有人说「上飞机时，空姐站门口跟你打招呼，你如果不能直视并微笑回应，就是冷漠无礼的败类。」

也见过有人说「坐飞机、高铁，把座椅靠背放下来的，都是自私自利的败类。」

这就是社会容错率低到极致的的言论表现。

而超低的容错率，因为站上了道德制高点，往往能获得大量的正反馈与情绪价值。以至于哪怕说一套，做一套，社交媒体还是进化成了口嗨圣母社会。

### 88

方军 2024-06-17

头回见人这么夸 langchain 的哈哈，我觉得 langchain 反正以我的水平是很好的源代码学习材料（当然我觉得这个人还没尝到过度封装的苦发呆）：

阿当：langchain 设计得真好

langchain 对于 chatgpt 的 open api 封装得真是精彩！原生的 chatgpt open api 关于 function calling，接口设计得很清楚，只是各种参数的备注，让 LLM 能够理解什么时候该调 function，这个 function 需要什么参数。LLM 的 function 消息，只是将这些参数准备好，回复给客户端，客户端是否要调用 function，以及 function 内部如何实现，LLM 并不关心。但 function calling 在服务端有效期只有 10 分钟，需要客户端在 10 分钟之内，把 function 给执行了，把返回值传给 LLM。LLM 会把 function 返回的值，结合上下文，NLP 处理之后再作为最终回复传给用户。

用原生的 api，开发者需要关于 run_id、thread_id、tool_call_id、run.status、是否 stream 模式，还要用上 threads.runs.create_and_poll 和 threads.runs.submit_tool_outputs_and_poll 来区分是普通的 run 还是提交 function outputs 的 run。最后的返回结果还要通过 thread_id 从 threads.messages 里拿出来，应用层写个代码又臭又长，要理解一堆概念，还要记住一堆方法名，流程要自己记住，各种_id 要自己手动维护。。。

用 langchain，API 友好到想哭。langchain 对 function calling 的封装如下：

1、function calling 定义那里，直接通过 baseModel 的类，封装成一个对象，function calling 里的参数，就是对象的属性。

2、function 的调用，就是将 baseModel 类对象传到一个函数里，函数返回值直接在函数里写个 return 就完了。函数名字与 function calling 的 name 参数不用对应。

3、函数返回值再传回给 LLM，只需要通过管道语法，直接在 chain 后面再加一个 chain，对返回值的格式有要求，再后面追加一个 parser 类型的 chain。把函数式编程的威力发挥到极致。之前我一直是面向对象的忠实信徒，一直不明白函数式编程适合的场景。langchain 向我演示了这个场景，彻底服气了。

4、原生 open api 的 assistant、thread、run 这些概念，以及多轮对话的记录，在 longchain 里做了重新的设计。assitant 变成了 agent，对 agent 传参数 checkpointer，agent 就有了 memory 的能力，可以记住多轮对话的上下文。在调 invoke 方法时，传参 config 就可以区分不同 thread。不管是 invoke，还是 stream 模式，返回值都直接做了封装。是否 stream 模式，只需要将 chain 的方法由 invoke 改为 stream，就完成了模式切换，各种 run.status、thread_id 细节都透明了，应用层只需要关注应用层的代码逻辑即可。

先不说通过 community 将大量的三方 tools 引入进来，也不说屏蔽不同 LLM 大模型的原生接口的差异，单是对 LLM 原生接口层的语法糖封装，langchain 就已经让人无法放弃了，就看得出来 langchain 做了非常深入的思考，拥有强大的抽象能力和架构能力。

有你真好！

### 89

方军 2024-06-18

这篇文章难得的简单明了：

[The limitations of LLMs, or why are we doing RAG? | EDB](https://www.enterprisedb.com/blog/limitations-llm-or-why-are-we-doing-rag)

LLMs 的限制，或者为什么我们要做 RAG？

它之后还有几篇链接：

-  [RAG is not the same as vector similarity search

RAG 不同于向量相似性搜索](https://www.enterprisedb.com/blog/rag-not-same-vec...

-  [RAG app with Postgres and pgvector

RAG 应用程序与 Postgres 和 pgvector](https://www.enterprisedb.com/blog/rag-app-postgres...

### 90

方军 2024-06-18

[万字解析教育领域生成式 AI 全景图：五大板块重塑未来学习生态](https://mp.weixin.qq.com/s/7eKSKpqjksuIl6YXK4-2xg)

### 91

方军 2024-06-18

[对话高利明：AI 时代的黄峥、张一鸣已经下场创业](https://mp.weixin.qq.com/s/LTof6VF41hMBx0ElX2uFqw)

### 92

方军 2024-06-18

摘：如何用 AI 提升 Web3 投研效率和自我竞争力

[E11. 如何用 AI 提升 Web3 投研效率和自我竞争力 w/ Hashkey, Chainfeeds, TypoxAI - Day1Global 生而全球 - Web3 版 | 小宇宙 - 听播客，上小宇宙](https://www.xiaoyuzhoufm.com/episode/666f07e8b6a84127292d57f7)

### 93

方军 2024-06-18

九原客的疑问很真实：

大模型现在被某些人吹为新一轮的技术革命。

但是对行业带来的技术革命和生产力的提升到底是什么？

问答？写作？这种其实都不是企业的核心业务流。

做了一年多大模型落地，是越来越困惑了。

有很热烈的讨论：

https://x.com/9hills/status/1802973834049438153

### 94

方军 2024-06-18

最近的负面印象大累积，做宏观分析的网友：我感觉国内财经媒体和 APP 现在是不是都在用 AI 智能生成新闻？读了几篇报道，内容基本纯属胡编乱造牛头不对马嘴。

方军：控制信息摄取，除非必要不要看媒体，不要看社交媒体

今天就很尴尬，一个人疯狂在群里说了几百条关于热门事件。但前几日我们已经看到，他说的东西里面经常有很多事实错误。（观点、逻辑错误就很难评估了）。结果导致乌烟瘴气。

2024-06-18 20:16

### 95

方军 2024-06-18

我的观点跟下文相似，现在没人能准确评估是不是 AI，但却能感受出来。同时，AI 内容是没有价值的。其实，内容生产多低成本，用 AI 干什么，AI 干的事还是分发，只不过略有变化。

（另，我不太认同「内容 = 情绪」这样的看法，内容首先是信息。为社交媒体情绪写作可以算是一份工作技能，但不能以偏概全说是所有的写作）

摘：在内容创作这一行，什么 Sora 和可灵作为工具，都不值得期待。我把话放在这儿，只要还能被看出是 AI 做的，就 P 用没有。

别自欺欺人了，一群不做内容的人，居然替做内容的人想「你们革命啦」。一群只会拿 AI 骗老板的新媒体搬砖民工，居然替那些自己开公司当老板的自媒体鼓吹下一个时代来临，不觉得臊得慌吗？

知道什么是内容吗？内容就是情绪制造机。

不仅纯煽动情绪的内容制造了情绪。有逻辑的内容，就是让观众感到「啊我很有逻辑」这样的情绪；试图传达「不要情绪输出」的内容，就是让观众产生「自以为不是在输出情绪」的优越感情绪。别装了，其实都是情绪。

而看出你用了 AI，情绪链接到了使用工具时的你。这个时候，你就很难产生价值感，你的名声不会更好，带的货不会被认可，你未来的作品难以被期待。这对于一个独立挣钱的内容创作者是毁灭性的。因为他毁掉了自己的观众缘。其下场和用 AI 画画的画师一样，本以为是一场革命，能提高大家的欣赏水平。结果提升的是画画圈鉴定 AI 的水平。其内容带来的情绪，完全可以剔除 AI 作品之后，完全和 AI 时代前对接。好像人们所欣赏的内容，本质是欣赏劳动一样。

如果你是个新媒体小编，你的工资要么是老板从其他行业利润里抠出来给你的，要么是内容创作者从「观众缘」那里获取的。你当然不知道这里面的重要性，你是个趴在前线人员身上分账的，没资格开口。

视频生成 AI 最先带来革命的，绝不是内容给观众的情绪增值，而是谣言带来的信息混乱度。从而在广义上进行一些品牌竞争，可能是国家之间，可能是产品之间，也可能是思想之间。但不管怎么说，都是更高效低质量的信息轰炸，「内容投放」更加量化。而真正内容创作带来的情感连接，AI 不会改变一丝一毫。唯一能改变的，就是先用 AI 的打工人，比后用 AI 的打工人，能多骗老板几个月的工资。

即使是打工人多分点钱，也总归是好事，也是有点市场的对吧？对，当然有市场，你想去赚钱还是能赚的。但说是工业革命还是太扯淡了。

### 96

方军 2024-06-19

阿里云的生成式ＡＩ赋能内容生产

### 97

方军 2024-06-19

[孙凝晖院士给正国级、副国级讲课：万字长稿《人工智能与智能计算的发展》全文 - 知乎](https://zhuanlan.zhihu.com/p/704130455?utm_id=0#showWechatShareTip?utm_source=wechat_session&utm_medium=social&wechatShare=1&s_r=0)

### 98

方军 2024-06-19

由两位百度前高管创立的 AI 搜索引擎 Genspark，宣布完成了 6000 万美金的种子轮融资，估值则达到了 2.6 亿美金，由蓝驰创投领投。Genspark 给自己的定位是 AI Agent Engine，我体验了一下非常有意思，可能会成为很多人的研究助手。它和传统的搜索引擎方式完全不同，搜索出来的结果首先是一个基本的概要，其次下面是基于这个搜索主题单独生成的一个类似网页的页面。

[英伟达市值登顶，两位百度高管创立 AI 搜索种子轮拿了 6000 万美金](https://mp.weixin.qq.com/s/k4xkrcto3Dhqp74gkVtywQ)

### 99

方军 2024-06-20

这几天有个感悟，AI写的低质量信息还不是最严重的问题

更难的问题是编造的虚假信息，最近的事件里面有很多假材料

更难的是人的偏见吧，各种奇怪的偏见（当然我这也是偏见）

还有个别人的喋喋不休。很多人不知道的是，社交媒体 troller、以及据学者分析俄罗斯操作 Twitter/FB 的方式是喋喋不休带来的淹没。

### 100

方军 2024-06-20

所有的内容生产者都困于平台，看看 mollick 教授的感受：

补充：他的那本书被降价为 ￥5.99。

### 101

方军 2024-06-20

摘：这也是我们最近用 GPT-4o 做 K12 测试的一个重要感知：

- 只要对于知识点的定义，足够细和精准，AI 的识别和推荐能力，都要超越 10 年以上教龄的老师；

- 只是受限于上下文理解，所以在」答疑「场景下，幻觉问题还有 30% 的概率出现；

这也造成教育行业可能会有以下几个变革：

- 教育科技的竞争，将定位于数据集细分度和精准度的竞争；

- 如何实现学习过程中，增加学生的高频反馈，决定了内容推荐、消费的核心竞争力；

- 大模型在教育应用场景的落地，可以在精准的学科数据基础上（预计 5 亿 token），依赖部分通用 token（预计 2 万亿），出现轻量级专家模型；（中国因教育人口规模和市场规模，大概率领先于美国）

- 以往的题库、试题集的资源垄断，靠 LLM 的推理能力，会不再有护城河；

关于这个：

- 只是受限于上下文理解，所以在」答疑「场景下，幻觉问题还有 30% 的概率出现；

> 赞同这个，目前似乎较好的用法还是在使用阶段有人的强介入，Humain in the loop，光靠在微调、RAG、指令似乎都没法完全做到可用

人家的回应：是的，我们现在用的是一个」取巧「办法，用上一代 AI 知识图谱的技术，把答疑场景下的，」引导式教学「（业界叫苏格拉底教学法），写成 SOP，仅用模型的语言生成能力做交互。

彻底解决数理逻辑的推理，只能靠和大模型厂商一起做 RLHF 的训练，或者做轻量级模型是有用的。我们对后者是有信心的。不过还没启动这件事。

### 102

方军 2024-06-20

097 大语言模型用于信息分发与信息消费

近期有个强烈的感受，之前，我们倾向于认为，LLM 能够生成新内容，但现在我觉得，它的一个场景可能还是内容的分发与消化。这用的还是它的语言理解能力。

最近遇到一个特别的场景：

- 在这个场景中，并没有多少 AI 撰写的低质量信息。我对应地提及：AI 写的低质量信息还不是最严重的问题。

- 较为严重的问题是这个场景中出现的很多虚假材料，那些变成图片的表格、假信息、错信息大量泛滥、难以识别。这是社交网络中的典型问题，有些人造假都不知道他目的究竟是什么。

- 更为严重的是偏见，完全与事实分离的偏见，中间已经不留下任何的具体的讨论空间。

- 同时，我还发现之前在社交网络中发现的问题，超级大量的信息。信噪比几乎降到零，同时如果不小心应对算法推荐的问题和管控通道（关闭无效的通道），就会一面倒的接受一边的信息。

之前其实我们已经了解到，社交媒体 troller、以及据学者分析俄罗斯操纵 Twitter/FB 的方式是喋喋不休带来的淹没。

在这种场景下，我发现 LLM 可能有一定的信息分发（对平台）与信息消化（对受众）效果。

我们暂且假设平台不是只考虑流量，而是试图相对公正地传播信息。与过去相比，平台有强有力的工具来对内容进行分析，如识别虚假信息，识别情绪，平衡观点，阻碍仇恨言语，平滑信息爆炸型的 Troller 等。

而对于受众个人，我们必然感受到巨大的信息冲击。过去的最佳策略是从消费者变为生产者（把自己的事实、分析、观点写出来），次佳策略是屏蔽（屏蔽绝大部分信息，在这种场景中人能接受也许万分之一就已经觉得过量）。

现在则多了一种可能的策略（但现在没有普通人可用的工具），用 LLM 来做前面的个人筛选器。通俗地说，我们看到一个万字长文，先用 LLM 总结下摘要，然后再看要不要细读，逻辑是类似的。

方军：最后这个最佳或次佳其实不一定，对于无关的事，其实最佳策略就是屏蔽。

2024-06-20 16:25

### 103

方军 2024-06-20

[用写作路径框架，五步引导 AI 大模型写出高质量文本](https://mp.weixin.qq.com/s/KP_Q4QnzCsvdGEPNOxQZNg)

### 104

方军 2024-06-21

genspark 有点热

[技术总结：GenSpark AI 搜索及 Graph RAG 算法及工程框架再回顾](https://mp.weixin.qq.com/s/JWH2led3NJFWWlxzXDtRNQ)

### 105

方军 2024-06-21

Jerlin：谈谈伪代码提示词

摘：去年在极客时间写过一节专门聊伪代码技巧的，彼时还在 GPT 3.5 探索 Prompt 的边界。

一开始是受 Python 和股票公式的启发（图 1），引入了一些伪代码表达，没有高级语言的语法约束、自由度非常高。

在 ChatGPT 中，嵌入适当的 Markdown、<分隔符>、' 变量 '、/ 和箭头等语义化符号的效果都是立竿见影的。尽管表达可能不那么「自然」，信息的冗余度极低。

[undefined]

除了日常的 Prompt，全局的自定义指令（图 2）和 Memory 我都是这么用。

额外的好处：在 Notion 等支持代码高亮的环境中规划 Prompt，信息结构和重点也很清晰。

https://x.com/eviljer/status/1801834884027269631

### 106

方军 2024-06-21

a16z 的 ai 生产力图谱

ttps://x.com/omooretweets/status/1760000618557735289

### 107

方军 2024-06-22

据报道，今天凌晨，美国财政部发布一份长达 12 页的规则草案，要求对美国在半导体和微电子、量子计算和人工智能领域的某些投资进行监管，禁止或要求限制中国在 AI 和其他技术领域的投资。

[突发！美国财政部公布新规，将禁止所有美国人对中国 AI、芯片等领域投资，最快今年底前实施｜钛媒体 AGI](https://mp.weixin.qq.com/s/ugq1rA14QAObWXzgpmGfmQ)

### 108

方军 2024-06-22

我真需要 AI 筛选器

啊呀，我好想说几句脏话骂人

社交媒体上都是蝗虫

最近天天看到人说，要不要报考计算机

胡扯啥呢

不是这个话题怎么样，是你们这种蹭热度让人觉得浅薄无知丑陋

真愿意分享的，看看之前宝玉分享的那篇，人家怎样分享自己的经历和经验 / 教训的

你们一个个口水话

社交媒体蝗虫真是很烦啊

### 109

方军 2024-06-22

忽略标题，内容很实在

[我用 ChatGPT 做了一下姜萍的数学竞赛题，得分 76 分，排名进前 100？](https://mp.weixin.qq.com/s/djsIaukMQBeAEgp7VAV2QA)

### 110

方军 2024-06-23

篇文章虽然用了很多中英夹杂，但是必要的，其中的信息也可以展示很多 AI 的企业使用场景

摘：（子陵）这段时间参加各种培训和会议，正好让我想起了最近几个朋友，尤其是还在学术界的朋友问的问题：在美国从学术界过渡到工业界最大的挑战是什么。

我个人的感受是两个技能（skill）是关键，这也是在美国大部分工业界招聘广告中通常看到的两个技能，就是 organizational skills 和 interpersonal skills。这两个技能归结到一点，也是工业界和学术界的最大不同，就是工业界最在乎合规，俗语就是守规矩。

我曾经讲过，在公司尤其是大公司，很多实验都是代理商做的，而自己只需要设计试验、分析数据和记录结果等等，因此，技术性困难本身不是工业界的难点，而许多挑战则是学会和熟悉流程、系统，并培养规则意识。在工业界，不管公司大小，各个业务都有自己的一套电子系统，比如电子实验记录，样品和试剂管理系统 inventory，数据分享 folder（比如 Box, One Drive, SharePoint 等等），Endnote 数据库，电子图书馆，各种功能平台和软件；另外，行政、财务、HR、采购等等也都有各自的系统需要你能够熟练使用。因此，你需要掌握这些系统，并且准确高效使用它们，这就是所谓的 organizational skills。这一切都是为了实现有效、系统、科学管理科研过程，并记录一切（documentation）程序。而详细记录一切（Recordkeeping）则是工业界工作的王道。这点尤其是对于在学术界比较久和从国内来的朋友至关重要，因为很多人需要很久才能适应这种一切都要记录、并需要用多种系统记录不同事项的过程。这些过程保证了合规，就是从实验准确性、可靠性、可重复性，到临床试验的严格性，这都是为了满足 FDA、SEC 等机构的合规要求（compliance），从而保证药物从研发到上市的过程能够全程达标。除此以外，一些小事，比如开会准时出现，线下的准时出席、线上的打开摄像头、slides 用要求的模板、学会写规定格式的报告、会议纪要（minutes）等等，这些也都是重要的 organizational skills（这些可能做的最好的是几大 CPA 公司和咨询公司，从这些公司训练出来的人都具有极好的 organizational skills）。对于刚从学术界进入工业界的人来说，不要觉得这些是「事务性工作」很繁琐，其实这些是最需要重视的。

同等重要的是 interpersonal skills，就是如何跟同事、上级、下级处理工作关系。我在面试的时候，一个曾经做学校 PI 的 manager 说，你是 PhD，我不担心你学习和掌握新科技技能的能力，而我觉得挑战可能是要跟不同岗位和性格特点的人打交道。诚然我们应该对每个人友好，但是职场上的同事并不是你的家人亲朋，你和每个人之间要保持充分的职业关系（professional relationships），高效交流、互相尊重、有边界感但能体贴他人。交流在美国永远是王道。我经常说在美国职场要频繁交流，多参加会议、活动、聚会，多自我介绍。记住公司花钱雇你是让你来提供见解的，而不是让你保持沉默的。另外，工作中矛盾不可避免，而在工业界应该形成什么事情需要低调处理，什么需要 escalate（升级）。不要觉得遇到矛盾就是天塌下来，不要 overthink，化解工作纠纷的方法合理沟通；人际纠纷如果是欺凌和歧视要积累证据。一旦不能化解，可以可以先从和 DEI 或者 EEO 沟通开始。另外，跟 HR 谈话要谨慎，因为 HR 代表了公司的利益，所以要反复记住他们不是你的 union rep（工会代表）。发邮件尤其是一个重要技能，因为公司的邮件一般都会长期保存的，而这些邮件是重要行政记录，也是关键法律证据，因此收发邮件用语和附件要非常谨慎。而目前的线上短消息系统，比如 Teams、Zoom 等等介于正式和非正式之间，这些可以稍微放松。

许多朋友反映，工业界工作总是希望招牌有工业界工作经历的，而之所以工业界工作有这个倾向，就是因为很多机构希望你已经有足够的 organizational skills 和 interpersonal skills，这样你入职了以后可以把精力集中在处理你的工作和业务上，而公司不想花很大成本去培训你上述技能和意识。相反，如果你已经熟悉了这些 organizational skills 和 interpersonal skills，在很多工业界工作环境中你会有如鱼得水的感觉。

### 111

方军 2024-06-23

赞同纯银这个，补充一点，LLM 编程厉害还因为它的结果可以即时运行验证，相当于有个类似强化学习的循环在里面。普通文字很难验证和迭代。

摘：最近在犬校里讲，什么是经验？

经验就是高压缩率的文本，存在海量的上下文信息。因为文本量过于巨大，最终只能压缩输出，但必然因为压缩而带来模糊失真。

这也是经验无法高效率传递的原因。很多事情只能你自己经历过，有了体感，掌握海量的上下文信息，你才能真正理解吸收。

所以书本只能传递知识，无法传递经验。

以成文信息为语料的大模型，从成文信息中可以获取知识，无法获取经验。语料的缺乏限制了大模型成长为专家。

这就能解释，为什么大模型辅助编程特别厉害 —— 因为代码是低压缩率的文本。

从图文视频这些信息载体中，还原出来的有效信息越多，大模型就越厉害。智力并不是大模型的瓶颈，语料才是。

### 112

方军 2024-06-24

我们的学习工具从来没有如此之好啊

把大模型看成生产力工具 / 生产工具，目前都是还不理想，但作为学习工具，真是高效，绝对十倍，昨天说编程，早上简单试了试英语，除了单词还试了句子的拆解。

以下这些结果没有专业的查询到的好，但是，它很有针对性，针对性弥补专业性。这类似于，互联网（维基百科）以快速 / 迭代弥补专业性。

ruffle 可以替换成什么？

"Ruffle" 这个词在英语中通常指轻微地扰乱或使某物起皱。以下是一些可以替换 "ruffle" 的同义词：

1. Disturb - 扰乱，干扰。

2. Ruffle Up - 使起皱或扰乱。

3. Agitate - 使激动或不安。

4. Flutter - 轻拂，使飘动。

5. Billow - 使像波涛一样起伏。

6. Ruffle the Surface - 使表面起波纹。

7. Disorder - 使混乱或无序。

8. Disarrange - 使杂乱无章。

9. Dishevel - 使凌乱或不整洁。

10. Ruffle Feathers - 字面意思是弄乱羽毛，比喻意义上指激怒或使某人不安。

11. Stir - 搅动，激起。

12. Trouble - 使困扰或不安。

13. Waver - 使摇摆不定。

14. Wiggle - 使摆动或扭动。

15. Toss - 使翻动或搅动。

选择哪个词取决于你想要表达的具体情境和语境。例如，如果你描述的是风轻轻吹动树叶，"flutter" 或 "billow" 可能是合适的选择；如果你描述的是情绪上的轻微扰乱，"agitate" 或 "disturb" 可能更合适。

inky 可以替换成什么？

如果你想要替换 "inky" 这个词，你可以选择以下一些同义词或类似表达：

1. Dark - 普通的形容词，用来描述颜色深或光线不足。

2. Jet Black - 像喷气机一样的黑色，非常深的黑色。

3. Pitch Black - 像沥青一样的黑色，形容非常黑暗，没有一点光亮。

4. Deep Black - 深黑色的，形容颜色非常深。

5. Sable - 一种深褐色至黑色的描述，常用于描述颜色或物体。

6. Ebony - 乌木色，一种非常深的黑色，常用于描述颜色或质地。

7. Obsidian - 黑曜石，一种深黑色的火山玻璃，常用来比喻颜色。

8. Sooty - 像煤烟一样的黑色，形容颜色深且可能有污迹。

9. Murky - 暗淡且不透明，常用来形容水或光线。

10. Dusky - 微暗的，有时也用来形容颜色深。

11. Gloomy - 昏暗的，没有光亮，也可以形容气氛或情绪。

12. Shadowy - 充满阴影的，形容光线不足或颜色深。

选择哪个词取决于你想要传达的具体情境和情感色彩。例如，如果你描述的是夜晚的天空，"pitch black" 或 "inky" 可能都很合适；如果你描述的是颜色深的水域，"inky waters" 或 "murky waters" 都是合适的选择。

给出十组 blue-azure, black-obsidian, green - emerald 这样的颜色词

当然，以下是十组颜色词，每组都由基本颜色和其特定色调或宝石名称组成：

1. Red - Ruby：红色和红宝石。

2. Blue - Sapphire：蓝色和蓝宝石。

3. Yellow - Amber：黄色和琥珀。

4. Purple - Amethyst：紫色和紫水晶。

5. Orange - Citrine：橙色和黄水晶。

6. Pink - Rose Quartz：粉红色和玫瑰石英。

7. Brown - Mahogany：棕色和桃花心木。

8. Gray - Slate：灰色和板岩。

9. White - Pearl：白色和珍珠。

10. Violet - Lavender：紫罗兰色和薰衣草。

这些组合通常用于描述颜色的特定深浅或色调，有时也用于形容具有这些颜色特征的宝石或自然物质。

收起

方军：刚刚想，现在的 LLM 特别像早年微软的数字百科全书，我们都没读过大英百科全书，但心存梦想，当我们有机会学习时，接触到的是 CD 版的微软数字百科全书（名字不记得了），我当时觉得看了怎么数千词条，但完全没有没有留下任何印记。但很快，我们都用上了真正网络版的维基百科，它带来彻底的改变。

2024-06-24 10:25

### 113

方军 2024-06-24

098 用大语言模型进行信息消费

我个人的确无论是兴趣还是工作，都会消费大量的信息（主要是文字类）。

LLM 给内容消费带来了巨大的价值。我并不是说，我们可以用 AI 聚合搜索来速查资料，也不是用 AI 来编写摘要。我的用法大体上是这样的：

1）精简

某些老师的资料啊，真是废话太多了。（他 / 她们的废话甚至都不是我这种带点情绪的废话，就是纯粹的废话。）

这带来的结果是，他们的内容的信噪比经常处在较低的程度，但是瑕不掩瑜，他们的核心信息是有价值的。

这时我们就需要精简。

在有 LLM 之前，我的精简方式是，快速浏览结构，只看有用的那个部分。（一般这些老师的资料的价值是结构 + 某个部分。）

在有了 LLM 之后，很自然地用 AI 进行总结，更快地找到那个部分。

接着对所要的那个部分的内容，我多半会让 AI 再进行明托金字塔结构式（观点 - 子观点 - 支撑事实）的总结。

2）解释

还有一些资料则是反面，知识密度太高，一页可能有多个知识点、信息点，并且需要太多的前置知识。

在有 LLM 之前，这些资料我们是要啃的，就是一点一点地推进，有不懂的立刻去查，直到把其中的全搞明白。

有了 LLM 之后，至少有两个方法（按顺序的）：

第一，遇到不懂的、遇到密度高的，请 AI 做解释，例如 ELI5（向五岁小孩解释），例如逐步解释，例如举例解释。

第二，我们看了不一定真懂了，因此，我们可接着让 AI 提问，我们回答。这样，我们可以基本上确认我们理解了。

以上所说的是消化吸收的阶段，但很显然这是不够的，因为真正的理解发生在自己重新写、自己亲自用。那就超出这里的讨论范围了。

方军：另外，这几天好几个人推荐 Google Notebook LM https://notebooklm.google.com/ 我试用了下，还没能用起来。Google 除了搜索、邮件之外的产品都有点 UX 难度

2024-06-24 13:38

[NotebookLM](https://notebooklm.google.com/?original_referer=https:%2F%2Fwx.zsxq.com%23&pli=1)

### 114

方军 2024-06-24

说来也许不信，豆包大模型（在豆包里面用的），它的处理如下这个数学问题的能力，理解指令（一步一步解释，不要方程式等），竟然超过 gpt-4o

例题（六年级）：

爷爷有 16% 的糖水 50 克，（1）要把它稀释成 10% 的糖水，需加水多少克？（2）若要把它变成 30% 的糖水，需加糖多少克？

对国产大模型的能力还是有中长期的信心的。

### 115

方军 2024-06-24

099 我们究竟要不要一个可以记住一切的 AI 系统

今天看到人鼓吹可以记住一切的 AI 系统「Mymind」，当然必然会提及之前的「Rewind」，它们都是让你可以记住一切、让 AI 能够帮你回忆一切。

我还总遇到很多人热衷于笔记软件，这一轮 AI 也带来很多新的笔记软件。有不少人也是 Notion/Obsidian 的爱好者，或是个人知识管理 / 建立个人知识体系的爱好者。

那么，我的看法是什么？其实你现在看的这一星球，就可以看成是我一个关于 AI 提问这一单一话题的公开笔记本。我当然还有很多其他形式的笔记。AI 来了之后，会不会大幅度改变我记录笔记的方式呢？答案是不会。

从这个星球的实践说起，它包括两类内容：

第一类，我看了觉得应该摘录的笔记（摘了会多看一眼，但绝对不会再回过头看）。

第二类，就是自己写的笔记，如你所见，这些笔记的作用是一个思考的过程，相当于草稿纸，也几乎不会再看。

这就是我对过往资料与记录的态度。记录下一切是没有意义的，让 AI 去提取过去也是没有意义的。大脑是最好的过滤器，如果大脑能记住（并非死记硬背），那么根据大脑记住的补充资料，如果大脑不能记住，那么，恰恰反映了自己的知识不足，去补全。

（题外话一句，我的工作性质让我没必要保留很多记录，因为我们的工作都凝聚在产出里，不需要记录来核实。对某些类型的工作，比如科研、咨询、医疗，记录是必要的。）

我这样的做法，也让我始终很难接受，为什么要有 Mymind / Rewind 这样的把所有都记录下来的产品？部分人的追捧我能理解，但这不妨碍我觉得这样的产品是完全错误的方向。

同样，我也很难接受某些人整理的那种比图书馆分类还严谨的 Notion 结构模板。那是纯粹的浪费时间，为知识管理而知识管理，用于在小红书等平台展示自己的「虚假努力」。

在这样错误的系统里面，引入 AI，用它能力去理解资料、用 RAG（相似度查询 + 理解与生成），都有点是错上加错的做法：如果原来的思路、做法就是错误的，那么高效的工具只会让我们错得更离谱。

说个多年前的例子，以前还是 MSN 的时代，我选择避免即时回复 MSN 上来的部门同事的诉求，理由很简单，你们自己带团队，这些问题是要让我决策吗？本来就是你们自己决策的事。对这类信息的有意拖延，其实是把同步交流变成异步交流，并且机制上的确可以做到，因为对真有审批环节，那本来就是要走邮件体系；对真重大的，当然会碰面讨论。—— 后来我们都知道了，中国互联网职场都被微信攻占了，所有人都去期待即时回复、各种拉群，以及这几年的各种腾讯会议。工具越强，偏差越多。

简言之，不要因为可以记录一切，就试图记录一切。Rewind 是离谱到不行的思路。

不要因为 AI 可以理解语言（及图片），就再在记录下来的东西上加码。没有意义，浪费时间，消耗精力。

主要的精力，应该放在产出上，而不是放在笔记（记录）上。

### 116

方军 2024-06-24

必须澄清一些题外话，那么，就一个专门的事项，我们不是必然要收集、分析大量的资料，然后再进行产出吗？这是两回事，专门的项目（Project，台湾的说法专案蛮好的）自然需要特定的资料体系。

打个比方说，你要设计与建造一幢楼，当然要有各种前期资料，有形成两阶段衔接的设计文件，要有建设过程中的记录。

但是，前面把设计工作室的每一秒都录屏、录像，后面在工地安装几万个摄像头记录，这肯定是完全错误的。我吐槽的是后者这种逻辑上就根本不通的思路。

### 117

方军 2024-06-25

有个梨 GPT 这个很直，好：

本来不想聊天才数学少女事件。

但是很多中老年男性知识分子在这件事情上嘴脸实在是让人觉得恶心，卑劣。

我毫不避讳的表达这样一个观点，尤其是想对下一代说，就是在这个世界上，人要对自己诚实，但不需要对这个世界完全诚实，如果你没有伤害谁，你热爱自己的热爱，争取自己的争取，那就去做。不要被道德逼们的恐吓阻挡前进的脚步。

++++

我们生活的这个世界的真相是，人们都会用规则尽可能多的给自己争取利益。这个时候他们强调规则是公平的。能力之外的资本等于零。自己是一步一个脚印奋斗出来的。不是运气，不是好时代，不是因为他人的谦让和帮助。

但是对他人的时候，就会用陡增几个级别的道德水准要求，并且把自己的人设放在坚持正义，推动公平，大无畏的位置。和生活里真人的价值观差别巨大。

++++

我们的社会从小处说，海淀区重点高中的业余竞赛水准很多都是能发 sci 的；有个学霸学阀的父母的本科生象印刷机一样发表 sci；各种企业拿补贴报项目不分民企国企，ppt 上有几句话是真的？去见投资人时的 ppt 里又有几句话是真的？不是说完全没有，但真的是少。学术界能好一些吗？好点但是有限。个人近几年参加的一些有点工程类研发性质的项目里，连线到大论文的一作人家也委婉的表示了这个数据是挑出来的好数据因为学生要毕业的，你们要做产品的千万别走这条路。

我不是说因为这个社会如此，姜萍就该同流合污；但是把现实社会构建成这个 b 样儿的成年人们没有资格去指责。

姜萍的出身很简单，没有找枪手的实力，他的老师水平不错，很多竞赛老师讲题是可以的但真的自己上，能打的凤毛麟角。这就可以了。

恭喜姜萍喜欢数学，在数学里找到了乐趣，恭喜她有一个实力不俗的老师，可以让她在数学之路上走得更远。人生值得的部分就是这样的相遇和喜悦，不值得的部分就是不值得听连简单的偏微分方程都解不出来的人在那儿叨逼叨。

++++

卡拉马佐夫兄弟在整个西方世界的地位都是非常高的，其中有名的一句话是，and to know that the sun is there，that，is living.

可以把 the sun 换成任何能让你在尘世里沉浸几个小时收获喜悦的东西。

能做一个诚实的人很好，但是做不到或者不能一直做到，也没什么。人只能活一次，拿着支票走出赌场，去过想过的生活吧。他人的祝福可以收下，其他的没必要带着。

### 118

方军 2024-06-26

[a16z 两位创始人：AI 热潮和互联网热潮的区别在哪](https://mp.weixin.qq.com/s/HECads_DLaKwMQ5QATEJtQ?poc_token=HAl0imaj1r9dgvW9-JrBjebS0E8u6CKB_91Un19Y)

a16z 两位创始人 Marc Andreessen 和 Ben Horowitz 刚好谈到了这个话题，他们认为这种类比并不完全适用。从技术本质上来说，Marc Andreessen 认为互联网是一个网络，而 AI 更像是一个计算机。

### 119

方军 2024-06-26

赞同：这个时代需要两种真正的 AI 课程

一种是教会老师如何利用 AI 的

还有一种是教会学生如何利用 AI 的

普遍的讲，每个人在一个垂直领域内，都可以是学生

### 120

方军 2024-06-26

mollick 教授这个论文看着不错啊（AI 翻译的摘录，将就看）

Instructors as Innovators: a Future-focused Approach to New AI Learning Opportunities, With Prompts
教师作为创新者：面向未来的新人工智能学习机会

[Instructors as Innovators: a Future-focused Approach to New AI Learning Opportunities, With Prompts by Ethan R. Mollick, Lilach Mollick :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4802463)

The Wharton School Research Paper
宾夕法尼亚大学沃顿商学院研究论文
76 Pages Posted: 23 Apr 2024
76 页 发布日期：2024 年 4 月 23 日
Ethan R. Mollick 伊桑·R·莫利克
University of Pennsylvania - Wharton School
宾夕法尼亚大学 - 沃顿商学院
Lilach Mollick 莉拉赫·莫利克
University of Pennsylvania - Wharton School
宾夕法尼亚大学 - 沃顿商学院
Date Written: April 22, 2024
2024 年 4 月 22 日

This paper explores how instructors can leverage generative AI to create personalized learning experiences for students that transform teaching and learning. We present a range of AI-based exercises that enable novel forms of practice and application including simulations, mentoring, coaching, and co-creation. For each type of exercise, we provide prompts that instructors can customize, along with guidance on classroom implementation, assessment, and risks to consider. We also provide blueprints, prompts that help instructors create their own original prompts. Instructors can leverage their content and pedagogical expertise to design these experiences, putting them in the role of builders and innovators. We argue that this instructor-driven approach has the potential to democratize the development of educational technology by enabling individual instructors to create AI exercises and tools tailored to their students' needs. While the exercises in this paper are a starting point, not a definitive solutions, they demonstrate AI's potential to expand what is possible in teaching and learning.

本文探讨了教师如何利用生成式人工智能为学生创造个性化学习体验，从而改变教学和学习的方式。我们提出了一系列基于人工智能的练习，包括模拟、指导、辅导和共同创作，这些练习能够实现新颖的实践和应用形式。针对每种练习，我们提供了教师可以自定义的提示语，以及关于课堂实施、评估和需要考虑的风险的指导。我们还提供了蓝图，即帮助教师创建他们自己原创提示语的提示。教师可以利用他们的内容和教学专业知识来设计这些体验，使他们扮演建设者和创新者的角色。我们认为，这种以教师为驱动的方法有潜力通过使个体教师能够创建适合其学生需求的人工智能练习和工具，从而使教育技术的发展民主化。虽然本文中的练习只是一个起点，而非最终解决方案，但它们展示了人工智能扩展教学和学习可能性的潜力。

### 121

方军 2024-06-26

这个场景很精彩啊

[大模型上岗三甲医院，给每个医生都配个“病历质控助手”](https://mp.weixin.qq.com/s/Shlfri0e8v4SMhHuDfgMXw)

（最后隐藏一个广告，但广告没啥，这样带广告的实战场景案例多来几个）

### 122

方军 2024-06-26

优质信息越来越难获取

摘：优质信息正在越来越难获取。

在人类漫长的历史中，信息从未像今天这样呈指数级的增长，一个成年人一天获取的信息数量可能超过 20 年前一个人一年可以获取的信息，但为什么现在信息那么多却感觉有用的信息越来越少了呢？一句话总结：无用的垃圾信息变多了，就导致高质量的信息难以被筛选。

在传统纸媒日渐消亡的当下，调查记者基本全军覆没，连很多官号都不再开始自己生产信息，靠不加辨别地转发垃圾黄色新闻来获取流量。前段时间的猫一杯乌龙事件就是个很好的例子。而互联网时代的发展同时也不断伴随着信息内容的变迁，现在打开某些搜索引擎搜索到的前几条全是广告，连找到一条有用的信息都难，更别说是高质量的。原来的各种门户网站，论坛，博客也都逐渐走向衰落，随之而来的是各种自媒体平台的兴起，信息生产模式由平台生产内容，创作者生产内容逐渐转向了用户生产内容。但很难说我们对信息的掌握更多了。在现在的短视频平台上还有很多以零几年论坛上「天涯神贴」的内容进行二创，并且依旧日有流量。这也恰恰证明大众也还是依旧渴望高质量信息的。然而信息发布门槛降低就必然意味着发布垃圾信息的人变多了。

同时各种平台的推荐机制又让大部分人更难以接触到优质信息，因为信息推荐已经从主动获取，变成被动投喂，而信息投喂的逻辑永远是向下兼容，而不是向上兼容。这就导致文章或者视频内容都大幅度庸俗化，信息严重贬值，阅读几乎无效。因为算法背后的逻辑不是为了让你学到知识，提升自己，而是为了抓取你的眼球，增加你对应用的使用时长，进而控制你控制你的钱包。这一逻辑同样也会使一些内容创作者为了流量和利益，降低信息质量，往内容中注水。对于那些有自己信息获取渠道的人说可能没什么，因为他们清楚该从哪里获取优质信息，也知道如何分辨筛选优质信息。但对很大一部分来说，原有的认知只会不断的被固化和加深，对世界的理解也会变得越来越偏颇。最后陷入算法织就的美梦当中，难以自拔。

① 由锤子便签发送 via Smartisan Notes

### 123

方军 2024-06-27

自从看了某香港高校计算机系主任的各种观点之后，对跑去该校读 CS 的人好担心啊，虽然的确都是观点和路线之争，但个人觉得他真的有问题。或许，观点太鲜明不是啥好事，如果他的角色是组织者的话，作为学者没问题。我现在一样觉得杨立昆的过于鲜明的观点对 Meta AI 研究部门也很尴尬。

### 124

方军 2024-06-27

摘：mr bear：有了 chatGPT 之后， 我更加确信， 我们必须要学习并适应在更高层次上的学习和工作，学习如何用抽象的方法抽象掉更多的具体的细节。整合，调用专业知识。 才是我们这代人的出路。

用这种高阶的抽象思维，指导 chatgpt 这种大模型，再去创造更具体的知识。

### 125

方军 2024-06-27

这几天 CSDN gitcode 的事情非常丢人，实际上，这几年中国技术媒体都是要屏蔽的。有效屏蔽，才能有效获取信息。

面向代码的：

CSDN 垃圾农场

掘金垃圾农场

这次只是跟代码库有关，所有权更强，所以引发激烈的批评

AI 媒体里面

新智元，震惊体，一点都不像有知识的人（昨日还有更激烈的事件，创始人震惊）

机器之心等等一众 AI 媒体吧，同样陷入震惊体的恶性螺旋

近日极客公园那个，说不错也不错，但如果严格要求，真心技术素养非常差（虽然从那篇文章表面看似乎很好）

有效屏蔽，才能有效获取信息。

在信息上要坚决，最多两次有问题就坚决屏蔽。

（题外话两句，1. 最近看人做一个信息产品，他选的信息源好差， 2. 开会说起某人的推特，说让我有空看看，我说，坚决不看，因为他之前已经信息信誉破产了啊，两次事件导致的，一次是持续的错误翻译，一次是盗窃与抄袭。）

### 126

方军 2024-06-27

有时候感慨，在知识类信息方面，大众很惨：

例子一：

2024 年 4 月，张钹院士做客清华大学「人文清华讲坛」，以《走进「无人区」，探索人工智能之路》为题做了一场演讲。他认为，第一代人工智能的目标是让机器像人类一样思考。第二代人工智能最大的问题是不安全、不可信、不可控、不可靠、不易推广。第一代和第二代人工智能的模型、算法都有很多缺陷。对目前的人工智能技术而言，虽然提高了效率和质量，但系统越信息化和智能化，也就意味着越不安全。

我相信以张钹院士的风格，他多半讲了各种技术。但媒体在传播时，就是会只提出这么一个点出来，大传特传。结果是，大众接受的信息是有巨大偏颇的，而全景内容他们又几乎绝对不认为自己有能力去理解。

例子二：

请人用 AI 整理资料，最后给的是列表式。我们讨论这么几句：

A：我感觉它整体好像调整了一下回答风格，和上次那种成段的文风不太一致，不过对于小白理解来说可能更友好一些

B： 可能什么说法触发了，一般跟它说用列表时，就会这种格式。也蛮好的，无所谓。

不过是不是适合小白很难讲，我们这些有点专业知识的人会喜欢这种格式。但我打交道下来，发现小白还是最喜欢文字描述。他们怕这种看着专业的格式。很有意思的现象。

后面我说的是，大众就是想看到两三段短段落的文字。有了这个文字，他们心理上「感觉」自己搞懂了。

我猜，这是为什么所谓摘要受欢迎。而我们这些自认为是专业的人可能很多时候是相反的，我们似乎更愿意看列表式，我自己在这个方面特别明显。

### 127

方军 2024-06-27

摘：程序员一生都能从一个风口活到下一个风口，要珍惜这种特权……

想想看别的职业的人，一辈子都不会看到自己的行业上新闻，一辈子都不会看到大街小巷的人都在讨论你熟悉的技术。

这种特权已经持续了二十多年，真的会永远持续下去吗？

感想：AI 其实还是程序员的，至少到目前还是这样，未来三五年也是吧。

其实我很好奇，为什么 AI 取代程序员这个话题会如此热，热到像语言之争一样：

摘：自动化生产的普及淘汰了大批工人，但这并不意味着企业所雇佣的人数急剧减少。恰恰相反的是，企业对人才的需求量变得更为庞大，因为它需要更多优秀的人来管理和制造自动化流水线。

事实上，这里提到的优秀人才很大一部分还是来自原来的那批工人。技术的发展从来不是一蹴而就的，而是一个循序渐进的过程。人的成长和角色的转变也是如此。

所以我从来不担心 AI 会取代程序员，也不担心 AI 给整个行业带来的巨大颠覆最终会影响到自己。保持学习的热情，保持好奇心。

### 128

方军 2024-06-28

有意思这个推测合理，比前段那个海淀家长段子合理：

摘：午间奇思：AI 做高考题，尽管目前是各家大模型的一种半娱乐跑分，但无形中拉开了高考 AI 化的序幕。

明年大模型的得分大概率会远高于今年。由于高考是最大的刚需之一，必然会有团队认真对待这一领域。大模型不仅是娱乐跑分。

既然大模型能够解题，是否也能够估题？是否能够总结出更应试的规律？教辅行业也将会 AI 化，出现教辅大模型。目前的 AI 填报志愿，也是在同类领域的另一种应用。

于是压力来到命题侧，这是一种攻防思想，由于基础知识的总量是固定且规律化，出题的变赶不上做题家的变，这会倒逼命题 AI 化。

### 129

方军 2024-06-28

科技爱好者周刊（第 306 期）：信息就像糖一样上瘾

作者： 阮一峰日期： 2024 年 6 月 28 日

提及的文章：

知识肥胖危机：信息成瘾正在腐蚀我们的大脑

作者：GURWINDER 古温德

---

信息就像糖一样上瘾

工业革命有一个很少人提到的后果，就是人类可以大量生产糖了。

古时候，糖主要来自甘蔗和蜂蜜，难以大量生产。所以，古人的菜肴不放糖，主要依靠食物本身的甜味。可以想象，古代的大部分食物，尤其是日常食品，应该是不好吃的。

一旦糖大量生产，人类就疯狂地爱上了它。我们喜欢糖，喜欢到现在的大部分食物，都必须添加糖，比如饮料、糕点、奶制品，甚至鸡排的配料都包含糖。

对糖上瘾，已经成了一种常见病。百度百科有一个「糖瘾」条目，解释上瘾原因是「甜味会带给人愉悦的感觉。"

科学家认为，糖会增加多巴胺的分泌，使大脑兴奋，感觉不到饱腹。通俗地说，就是糖会让你吃得很愉快，从而吃了还想吃，即使已经抱了。

过量摄入糖，是体重超标的根本原因。糖本身只是一种甜味剂，没有营养，只有热量，最终都转化为体内脂肪，让你越来越胖。世界卫生组织一直在呼吁少吃糖。

不过，我今天想谈的不是糖，而是最近看到的一篇文章，里面有一个很有趣的观点：信息就像糖，让人上瘾。

作者列举了，信息与糖的相似之处。

（1）低成本的大量生产。信息革命之后，人类就像生产糖一样，大量生产信息。

信息不仅生产成本低，传播成本更低，可以接近零成本地到达消费者。

（2）上瘾机制相同。2019 年，加州大学伯克利分校的一项研究发现，信息也会刺激大脑产生多巴胺，跟糖的作用机制相同。

所以，看到一则吸引人的信息，跟吃一件甜点，带给你的满足感是一样的。你会对信息上瘾，看了还想看，具体表现就是放不下手机，划了又划。

（3）垃圾信息泛滥。就像垃圾食品流行一样，人类对信息上瘾的后果，就是垃圾信息的大量供给。

生产商发现，那些最刺激多巴胺的信息，有最多的阅读量，可以赚到更多的钱。于是，垃圾信息就被大量生产，变得无处不在。

人类已经陷入了这样一种境地，最容易获得的食品是垃圾食品，最容易接触到的信息是垃圾信息。

（4）思维「发胖」。就像垃圾食品让人体发胖一样，垃圾信息也会让思维发胖。

垃圾信息降低你的思考水平，将没意义的内容充斥你的大脑，分散你的注意力，堵塞你的思考，让你思维迟缓，判断力下降。

综上所述，信息上瘾与糖瘾一样有危害，需要积极防治。健康生活不仅少吃糖，还要少接触垃圾信息。

但是，就像躲不开糖一样，生活中根本没机会躲开垃圾信息。我们只能自我克制，反复告诫自己，远离垃圾信息，防止信息上瘾。

---

以下为 AI 翻译

[The Intellectual Obesity Crisis - by Gurwinder - The Prism](https://www.gurwinder.blog/p/the-intellectual-obesity-crisis)

、# The Intellectual Obesity Crisis

知识肥胖危机

\### Information addiction is rotting our brains

信息成瘾正在腐蚀我们的大脑

[GURWINDER 古温德](https://substack.com/@gurwinder)

2022 年 5 月 17 日

「Our minds are hurt more often by overeating than by hunger.」

「我们的头脑更经常因暴饮暴食而受伤，而不是因饥饿而受伤。」

— Petrarch — 彼特拉克

我们进化成渴望糖的样子，因为它是一种稀缺的能源来源。但是当我们学会大规模生产它时，突然间我们对甜食的热爱变成了一种负担。现在同样适用于数据。在信息过剩的时代，我们的好奇心曾经让我们专注，现在却让我们分心。这导致了一种智力肥胖的流行病，用有害的垃圾堵塞了我们的思维。

信息如同糖一样的类比并非只是修辞。伯克利大学的研究人员在 2019 年的一项研究中发现，信息对大脑多巴胺产生奖励的系统产生的影响与食物相同。简单来说，大脑将信息视为一种奖励；无论信息是否准确或有用，大脑仍然会渴望它，并在消耗后感到满足（至少直到它开始渴望更多）。

数十万年来，这并不是一个问题，因为在热带草原上，信息就像糖一样稀缺而珍贵。但随着工业社会和互联网的兴起，一切都发生了改变。

我们现在生活在一个注意力经济中，人们试图以任何可能的方式吸引我们的兴趣。由于低质量信息和高质量信息一样有效地满足我们的信息渴望，数字时代获取注意力的最有效方式是大量生产低质量的「垃圾信息」—— 一种思想快餐。像快餐一样，垃圾信息的生产成本低，消费满足感强，但添加剂高，营养低。它也可能上瘾，如果过度消费，会带来高度危险。

垃圾信息通常是虚假信息，但它不是垃圾，因为它是虚假的。它是垃圾，因为它没有实际用途；它不能让你的生活变得更好，也不能提高你的理解能力。甚至谎言也可以滋养人心；陀思妥耶夫斯基的作品是虚构的，却能教会你比任何心理学教科书更多关于人类的知识。与此同时，大多数经过验证的事实对改善你的生活或理解毫无作用，用尼采的话来说，就像对一个溺水者来说水的化学成分的知识一样没有用。

常见的垃圾信息类型包括八卦、琐事、点击诱导、黑客技术、营销、新闻工厂和胡言乱语。但事实上，任何你无法利用的信息都是垃圾信息。在社交媒体上的一个典型例子可能是一张新鲜煮熟的汉堡的照片，配文是「看我刚做的！」但没有附上食谱，所以你甚至无法重现它。这样的图片可能会让你短暂流口水，并可能激励你自己做一个汉堡，但它对你的生活没有明显的价值。

大多数人并不会认真考虑他们在社交媒体上发布的内容，这样的人自然能够以更快的速度发布，而不像更谨慎的人那样，所以琐事（例如「感觉累了，可能要去睡觉了，哈哈」）很快就会充斥这些平台。但最远传播的垃圾信息是那些引起强烈情绪的信息，这并没有被那些最渴望你关注的人，比如记者和评论员，忽视。

唤起的最容易的强烈情绪是愤怒；它不需要比一个简单的压迫故事更复杂的东西，而是量身定制给适当的政治部落。然而，愤怒尽管廉价，却极具上瘾性和极易传染，使其成为任何想在网络喧嚣中引起注意的人的选择武器。即使像《纽约时报》这样曾经受人尊敬的媒体现在也诉诸于「愤怒诱饵」，这是一种旨在激怒报纸读者和政治对手的耸人听闻的故事，以确保获得最大的关注。

市场力量和社会压力导致垃圾信息主导网络，因为它便宜、易于生产，并且擅长窃取你的注意力。它的无处不在意味着它总是容易触手可及，因此，成千上万的人现在对它上瘾。这就是为什么他们不断地滚动他们的 Twitter 时间线，或者检查他们的 Instagram 通知，或者反复点击 YouTube 首页的刷新按钮，或者不断续订他们对时报的订阅。

今天你消费的绝大部分在线内容都不会提高你对世界的理解。事实上，它可能会产生相反的效果；最近的研究表明，浏览社交媒体的人倾向于经历「规范性脱离」，在这种情况下，他们变得更不注意和更无法处理信息，以至于他们经常无法回忆起他们刚刚读过的内容。

但尽管是「空热量」，垃圾信息仍然很美味。由于多巴胺通路无法区分有用和无用的信息，消费垃圾信息会让你感觉像是在学习，给你提供了精神滋养的感觉，尽管你真正做的只是往脑袋里塞虚拟爆米花。

最终，对无用信息的沉迷导致了我所说的「智力肥胖」。就像暴饮暴食会使你的身体膨胀一样，沉迷于无用信息也会使你的头脑膨胀，充斥着一大堆半记住的胡言乱语，使你的注意力分散，混淆你的感官。无法区分重要和不重要的事物，你会为琐事而烦恼，为谬误而愤怒。这些烦恼和愤怒会促使你进一步消费，而在你消费的时候，你就无法做其他事情：学习、专注，甚至思考。结果就是你的意识流变得淤塞；你的头脑发生了动脉粥样硬化。

我们现在生活在一个不断分心的状态中，这是由对无用信息的上瘾所引起的，这种分心是如此强大，甚至让我们分心于我们正在被分心的事实。你可能会读这篇文章，简要考虑一下垃圾信息给你带来的伤害，然后又会毫无目的地滚动 Twitter。

但在你这样做之前，让我们试着想出某种解决方案。

改善信息摄入的最直接方法是养成元认知的习惯；关注自己关注的事情。当你发现自己不经提示就拿起手机，或者在推特图标上徘徊时，可以运用「10-10-10 法则」：问问自己，如果我消化这个信息，10 分钟后、10 个月后和 10 年后我会对此有什么感觉？这样做可能会让你意识到，垃圾信息所提供的短暂快感在你的生活大计中是如此短暂和微不足道，以至于根本不值得你花时间。

如果你的渴望无法被简单的推理打败，那么考虑重新安排你的生活方式，这样垃圾信息就不再是一个选择。我战胜智力肥胖的方式是努力成为我能成为的最好的作家。写作要求你过滤掉坏信息，因为你有责任向读者传达真实信息。写作还迫使你定期屏蔽所有信息，这样你就可以独自面对自己的思想。这种与自己的定期对抗有助于你在一个不断试图引诱你远离思维的世界中保持清醒。

最终，你将不得不确定适合你的信息饮食。但如果你坚持不断地消费网络提供的一切，要知道这场宴会最终将以苦涩的甜点告终：在你的一生结束时，当你在衡量自己的遗憾时，你可能不会说「哎呀，我真希望我花更多时间浏览网页。」相反，你可能不会记得那个陌生人告诉你他更喜欢意大利面而不是披萨的推文，或者那个让你开心了五秒钟的 gif，或者那篇让你生气了整整一分钟的《时代》文章。当你注意到所有这些垃圾在你的记忆中留下的无数空洞时，你终于会清楚地意识到，你并不是在消费它，而是它在消费你。

### 130

方军 2024-06-28

[Genspark](https://www.genspark.ai/)

又一个聚合搜索工具，的确不错啊。听说了一段，今天试用了下，目前的聚合搜索有：

- perplexity
- metaso (中文）
- genspark  (中国团队背景，但面向海外）

genspark  的特点是：

- 生成回答
- 生成页面

相对而言，metaso  的特点是
- 思维导图/PPT

不过，我还是对这类产品还是有疑虑：

第一个疑虑： RAG，效果究竟如何？

目前看，看似引用很多来源，但这些产品真的太难做质量判断了。

第二个疑虑：比 perplexity 往前走了一步，给出页面太像成品。

其实应该保持问答状态，让人觉得这是一种机器生成的回答。

所以总的来说，虽然我的确在用这类产品，但疑虑很大。

### 131

方军 2024-06-28

这个提示语不错：用逐渐增加复杂度的方式解释 某某某

示例：

Transformer是一种深度学习模型，主要用于处理序列数据，特别是在自然语言处理（NLP）领域取得了显著的成果。下面是按照复杂度逐渐增加的方式对Transformer进行解释：

\### 1. 基础概念
- **Transformer**：一种基于注意力机制（Attention Mechanism）的神经网络架构，由Vaswani等人在2017年提出。

\### 2. 简单比喻
想象一下，Transformer就像一个多语言翻译器，它可以同时理解多种语言，并能够将一种语言的句子翻译成另一种语言。它通过“注意”不同语言之间的相似之处来实现翻译。

\### 3. 技术细节
- **自注意力（Self-Attention）**：允许模型在处理序列的每个元素时，考虑序列中的所有其他元素，从而捕捉元素之间的关系。
- **多头注意力（Multi-Head Attention）**：通过多个注意力头并行处理信息，每个头学习序列的不同方面。

\### 4. 架构组成
- **编码器（Encoder）**：将输入序列转换成一系列向量表示。
- **解码器（Decoder）**：使用编码器的输出和之前生成的输出来生成新的序列。

\### 5. 应用场景
- **机器翻译**：将一种语言翻译成另一种语言。
- **文本摘要**：自动生成文本的简短版本。
- **问答系统**：理解和回答用户的问题。

\### 6. 技术挑战
- **计算资源**：Transformer模型通常需要大量的计算资源。
- **长序列处理**：对于非常长的序列，Transformer可能难以捕捉到序列的全局信息。

\### 7. 优化和变体
- **Transformer XL**：增加了循环机制，以更好地处理长序列。
- **BERT**：通过预训练语言表示，提高了模型对语言的理解能力。

\### 8. 社会影响
- **语言障碍的降低**：通过机器翻译，Transformer有助于打破语言障碍，促进全球交流。
- **信息获取**：提高了信息检索和处理的效率，有助于人们更快地获取所需信息。

\### 9. 哲学思考
- **人工智能的局限性**：尽管Transformer在语言处理方面表现出色，但它仍然受限于数据和算法的设计，不能完全理解语言的深层含义。

随着对Transformer理解的深入，你会发现它不仅仅是一种技术，更是一种推动人工智能领域发展的创新思想。

### 132

方军 2024-06-28

提示词越具体越好
使用多步提示，思维链
鼓励批判性思考
寻求多元视角充分
利用长对话的优势

[升级到 GPT-4o 后使用提示词的五项调整[译]](https://mp.weixin.qq.com/s/_CK9kLgukLIztLDvdLO0jA)

### 133

方军 2024-06-28

OpenAI 新论文：

CriticGPT最后结论里的一句话：“从这一点开始，LLM（大语言模型）的智能将会持续提高。而人类的智能则做不到。”

[Finding GPT-4’s mistakes with GPT-4 | OpenAI](https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/)

### 134

方军 2024-06-29

摘，这个例子真好

昨天刷到一个韩国人写的贴，内容如下。

邻居家有个边缘智力障碍（IQ70~85 之间，例如阿甘正传的阿甘），平时在便利店打打工，在妈妈店里帮点忙的小孩。

有一天他家传来砸东西大吵大闹的声音。

贴主住隔壁觉得实在太吵，带了杯咖啡和妈妈一起去问出什么事了。

邻居阿姨说孩子被网上诈骗团伙骗了，以为是合法的，差点进去做回收钱财的跑腿小弟，幸好她和警察在孩子犯案前抓回来了。

于是在家一顿臭骂，阿姨说自己好累，孩子是非不分，自己又忙着开店，总不能把孩子 24 小时绑在身边，不知道该怎么办，边说边哭。

贴主在一旁默默听，小心提问「阿姨… 或许… 您知道 chatgpt 吗…」，阿姨没听说过，贴主要来孩子的手机，下了 app，登上自己的付费账号，给孩子定制了一个 bot，和 gpt 说好「我是一个边缘智力的人，容易被骗或犯罪，你的任务是有任何这种征兆及时提醒我咨询父母。」

然后对小孩说以后尽量不要自己判断，凡事都可以先问问 gpt。之后观察他的日常生活，看到孩子已经把 gpt 当第二大脑了。

例如平时他会问「我想喝热牛奶，直接塑料瓶放微波炉微波可以吗？」gpt 会回答「那样瓶子会化的，请倒到碗里再微波，不要用金属碗，只能用陶瓷碗哦。」

大概会聊这些，真的感觉 gpt 在救一个人。

### 135

方军 2024-06-30

Perplexity 联合创始人、CEO 阿拉文德·斯里尼瓦斯（Aravind Srinivas）今年初表示：「人们可以将 Perplexity 看做是一个 AI‘套壳'产品，但成为一个拥有十万用户的‘套壳'产品显然比拥有自有模型却没有用户更有意义。」

### 136

方军 2024-06-30

我前几日说：个人觉得，马斯克老师就是一个 troller，他固然开启了电动车大浪潮，但其他方面，他真的可以用 troller 形容。

### 137

方军 2024-06-30

纯银这个思考到位：自然语言提问可以在一瞬间，将所有用户的深度使用技巧拉平，为浅度用户赋予深度使用的价值。

昨天和慢鸽程序员聊天，我说大模型对于点评和携程都是极大利好。

因为大模型最擅长做的事情，就是理解输入的语料，也理解自然语言的提问，然后基于语料进行强逻辑强总结的回答。

我代入自己的场景想了想，觉得找店，找酒店，都套得进去。来回几轮自然语言回答，以及启发式提问，比自己摸索筛选项舒服很多。

这个动作，看起来只是对老用户提效？不是的，最大的价值是许多用不好筛选器的浅度用户，可以在大模型加持下，轻松获得和深度用户一样的体验。

比如我经常吐槽点评的 APP 信息结构还停留在 2016 年分类网站的水准，很多高价值的信息分散在各个入口里，有的入口我都不会用，不常用，想不到。但自然语言提问可以在一瞬间，将所有用户的深度使用技巧拉平，为浅度用户赋予深度使用的价值。

这一点，对点评 APP 尤其重要。因为订酒店是刚需，但找店则不是，培养浅度用户在点评找店的习惯很慢，很难，而简化找店交互则可以快速弥补这个缺口。

2015 年我在携程做过周末生活与周末酒店，也发现许多用户在找周末酒店方面有需求，但迷失在筛选器里。那时我很难优化这个筛选器，现在则一定会用大模型来实现自然语言问答。这类非刚需的订酒店场景，对于携程亦有一定的业绩提升空间。

方军：补充：我代入自己的场景想了想，觉得找店，找酒店，都套得进去。来回几轮自然语言回答，以及启发式提问，比自己摸索筛选项舒服很多。

我觉得一般，这些不是需要对话的场景。

2024-07-01 09:30

方军：纯银：犬校同学提出了关于大模型发展的一个新的角度，也是一个很可怕的角度。

大意是，大模型会吃掉垂直平台，比如点评与携程全部的数据，深度理解之后，未来用户用自然语言问大模型，怎样选餐馆，怎样选酒店，大模型给出更好的搜索结果。

这对于数据为王的垂直平台来说是灭顶之灾，一定会在法律上殊死反抗。那将会是极为惨烈的战争。一旦大模型获胜，则提供搜索结果的垂直平台全灭，同时这些垂直平台上创造 UGC 的内容生态也全灭，这类内容走向枯竭，走向 Mad Max 的废土世界。

2024-07-01 19:41