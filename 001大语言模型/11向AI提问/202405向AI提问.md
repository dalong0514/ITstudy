### 01

方军 2024-05-01

Globe 这个AI聚合搜索引擎似乎把搜索又上了一个大台阶

当前看到它的特点主要是：

左侧的多级分类

右侧的以图片为主的展示

（右侧这个，会用搜索进行知识学习的人都知道吧，这是最有用的搜索形态）

https://explorer.globe.engineer/

有点贵，我在考虑要不要充值一个月试试，一年肯定不充了，谁知道一年后什么更酷的产品出来。

### 02

方军 2024-05-01

摘：模型即数据
要舍得在数据上投入
这是最本质的事情

人脑的模型也是如此
在学习优质数据上投入时间、金钱和注意力

原文：

The “it” in AI models is the dataset.
AI 模型中的“it”是数据集。

Posted on June 10, 2023 by jbetker

I’ve been at OpenAI for almost a year now. In that time, I’ve trained a lot of generative models. More than anyone really has any right to train. As I’ve spent these hours observing the effects of tweaking various model configurations and hyperparameters, one thing that has struck me is the similarities in between all the training runs.
我现在已经在 OpenAI 工作将近一年了。在这段时间里，我训练了许多生成模型。比任何人都有权利训练的还要多。当我花了这些时间观察调整各种模型配置和超参数的影响时，有一件事让我印象深刻，那就是所有训练运行之间的相似之处。
It’s becoming awfully clear to me that these models are truly approximating their datasets to an incredible degree. What that means is not only that they learn what it means to be a dog or a cat, but the interstitial frequencies between distributions that don’t matter, like what photos humans are likely to take or words humans commonly write down.
对我来说，这些模型确实以令人难以置信的程度逼近它们的数据集。这意味着它们不仅学会了什么是狗或猫，还学会了不重要的分布之间的插值频率，比如人类可能拍摄的照片或人类常常写下的单词。
What this manifests as is – trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion.
这种表现形式是 - 在相同的数据集上进行足够长时间的训练，几乎每个具有足够权重和训练时间的模型都会收敛到相同的点。足够大的扩散 conv-unets 生成与 ViT 生成器相同的图像。AR 采样生成与扩散相同的图像。
This is a surprising observation! It implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.
这是一个令人惊讶的观察！这意味着模型行为不是由架构、超参数或优化器选择决定的。它是由你的数据集决定的，没有别的。其他一切都是为了有效地将计算交付给逼近该数据集的手段。
Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.
然后，当你提到“Lambda”、“ChatGPT”、“Bard”或“Claude”时，你指的不是模型权重，而是数据集。

nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/

### 03

方军 2024-05-01

蛮有意思的，好像 kimi 没用智能体这个词，用的是普普通通的「私人助理」，后者要更友好些

我一直很好奇，为什么有个圈子特别喜欢智能体这个词？ 智谱也喜欢这个词。

### 04

方军 2024-05-02

OpenAI 发布一个 Next.js 的模板，用于 assistant api

OpenAI Assistants API Quickstart

github.com/openai/openai-assistants-quickstart

粗看起来，这个还是远不如 vercel 开源的 Vercel AI Chatbot

github.com/vercel/ai-chatbot

Vercel 这个背后是它的 AI SDK (现在3.1.0)

sdk.vercel.ai/docs

不过，这个 AI SDK 的社区可以说不是特别活跃，使用也是要谨慎的。

### 05

方军 2024-05-04

跟了一年多，讲实话有时候还是觉得openai这个公司的产品定力不行

我始终觉得它的优势在模型，超强优势，但被各种外部影响做了好多分叉之外的事

事后看，function call很赞

assistant api一般

而去搞什么搜素，没戏啊

小弟弟 ppx 可以搞，你搞就有点…

[OpenAI下周将发布ChatGPT搜索引擎，挑战谷歌搜索！](https://mp.weixin.qq.com/s/kXHHXtVBJIOGIg1KZ9PO7A)

### 06

方军 2024-05-04

扬清对 OpenAI 搜索进行了评测，他认为这超过搜索 + RAG 的做法。

有兴趣可查看具体讨论：

twitter.com/jiayq/status/1786208795720593789

网友的争论：twitter.com/idoubicc/status/1786605308330000429

摘（为 AI 翻译）：

尝试用「今天黑客新闻中最受欢迎的帖子说了什么？」进行搜索，你会看到区别。

如果你简单地将其翻译为谷歌查询，比如「黑客新闻中最受欢迎的帖子」，它不会给你有用的总结结果。你必须真正理解它，将其翻译为有意义的行动（比如实际获取 https://news.ycombinator.com/best ），获得结果，然后理解它。

Bing AI - 搜索能够访问页面并给出一些结果，但答案略有偏差（它还伴随着太多的截图）。

基于搜索，http://gemini.google.com 无法解决它，只是说「因为它在一天之内会发生变化」。

http://perplexity.ai 和 Lepton Search 无法解决它。至少目前还不能。顺便说一句，Lepton 搜索旨在提供一个开源示例，展示如何构建一个搜索增强型对话模型，因此结果在没有一些基于代理的设计的情况下是合理的。

方军：idoubi

@idoubicc

twitter.com/idoubicc/status/1786605308330000429

关于贾扬清老师评论 ChatGPT 做 Search 会杀死大部分 Wrapper 型 AI 搜索引擎的这个帖子，我有一些不一样的看法👇

1. AI 搜索引擎的第一要义是准确度。

准确度的决定性因素主要是两个：问答底座模型的智能程度 + 挂载上下文的信息密度。

做好 AI 搜索引擎的关键，选用最智能的问答底座模型，再对 RAG 的检索结果进行排序去重，保证信息密度。

第一个步骤容易，第二个步骤很难。所以现在市面上大部分的 AI 搜索引擎，包括 Perplexity，准确度也就 60% 左右。

2. ChatGPT 自己做搜索，首先保证了问答底座模型的智能程度。……

3. 我并不觉得大模型厂商自己做 AI 搜索就一定会比第三方做的好。……

4. 做好 AI 搜索引擎，最重要的三点是准 / 快 / ……

5. AI 搜索引擎是一个持续雕花的过程。……

6. AI Search + Agents + Workflows 是趋势。

AI Search 做通用场景，通过 Agents 做垂直场景，支持个性化搜索需求。

通过 Workflows 实现更加复杂的流程编排，有机会把某类需求解决的更好。

使用 GPTs 做出的提示词应用或知识库挂载型应用，价值点还是太薄。

7. 我个人不是太看好垂直搜索引擎。

一定程度上，垂直搜索引擎可以在某个场景做深做透，但是用户的搜索需求是非常多样的，我不太可能为了搜代码问题给 A 产品付费，再为了搜旅游攻略给 B 产品付费。

垂直搜索引擎自建 index 索引，工程投入比较大，效果不一定比接 Google API 要好，而且接入的信息源太有限。

8. AI 搜索是一个巨大的市场，短时间内很难形成垄断。……

9. AI 搜索引擎需要尽早考虑成本优化。……

以上是我个人做 http://ThinkAny.AI 一个多月以来的一些经验和思考。欢迎交流探讨。

2024-05-04 19:27

### 07

方军 2024-05-04

有人推荐了这个项目：github.com/reorproject/reor

Reor is an AI-powered desktop note-taking app: it automatically links related notes, answers questions on your notes and provides semantic search.

当你在撰写新的笔记时可以显示从语料库中 "检索" 到的相关的笔记。这是一种通过将当前笔记中的想法与语料库中的相关想法进行交叉参考来 "增强" 用户的思维的强大方式。

不过，我对这样的笔记写作是有极大疑虑的，个人的想法是：

有效的写作应该不是借助笔记，（最多或借助事先准备好的几张卡片 / 少量条目）。

只有这样，才能有效地利用自己的大脑，对知识进行梳理。

这也是我对各类所谓的双链笔记没什么特别印象的原因。那些链接最终是一团乱麻，对我，仅仅是视觉上吸引人（但我又恰巧不喜欢那样的视觉冲击）。

### 08

方军 2024-05-04

摘：最近崔娃和微软一起合办了一个 AI 相关的视频节目，叫：《The Prompt with Trevor Noah》。它的口号是：「You can't find answers without asking the right questions」。这正是我们日常使用 Prompt（提示词）使用 AI 的关键所在 —— 如果你不能提出正确的问题，就无法找到答案。

有意思啊，但有点过于严肃了，不像他

www.microsoft.com/en-us/research/group/ai-for-good-research-lab/the-prompt/

### 09

方军 2024-05-05

虽然我知道前面的这部分不是全部的事实，而是更多的情绪，但是的确引发感慨，后面的那段很棒（补充，如果不是要看科普，而是要看文档手册和开源，互联网和 AI 会更有用，还有阅读长文 / 最好是文档手册的能力），摘：

大约在十几年前网速不咋快、也没有短视频大流行的时候，我会去一些科学杂志网站蹭免费文章看，那里有很多专业作者写的高质量报道，还有各种精美插图。但是随着社交自媒体兴起，这些网站迅速被边缘化了，被淹没在大量无来源的污染信息和所谓的头条热搜里。这是数字时代的真正悲剧。

以下是一部分杂志的名字和他们今天首页的模样，怀念我还能在路边摊偶然买到其中一些杂志纸质版的时候：…

现在是互联网最好的时代，因为机器翻译质量已经快把巴比伦塔削平了，和 99.9% 的历史时间相比，今天你获得任何一种陌生语言载体的优质信息的难度几乎是零。现在也是互联网最糟的时代，所有媒体平台都在流量利益驱动下一起摔进下水道了，信息壁垒比以往任何时候都低，又比以往任何时候都高。这是个碾压在亚历山大图书馆头上的震耳欲聋的垃圾场，在神圣的万维网书架前人类只有一种堕落就是自甘堕落。

方军：接着摘另一个相关的：现在 mind hacker 占据了互联网绝大多数的内容，随着 AI 技术的进步情况将会更失控。香农信道定律将会是对你最大的保护，例如回到线下生活、学习主要靠读书读论文、通讯主要靠打电话，这样可以确保外部环境的信息量与你自身频带宽度相符，因此信噪比可以保持在合适比例。如果放任 mind hacker 无限制地占用你有限的认知带宽，那么你的大脑必须要用极好的滤波器来保证极高的信噪比，长期看大脑可能会被冲垮（失去滤波功能、进而失去认知功能）。

2024-05-05 10:43

方军：想起以前一个朋友的故事，她作为时尚和艺术行业中人，从来不买大牌，人问为什么，她答，我天天造这些… 当然她自己用的那些其实比大牌价格上几乎是一样的，只是没牌而已

2024-05-05 10:44

---

这位名叫 haiwenliu 的网友颇有意思：

智能的一大标志是：寻找结构。在寻找前所未知的结构方面，古希腊以来数学家和科学家携手取得了巨大的成功，Ta 们发展的数学语言成了目前所知的这个世界最精准的描述方式。这就是皮尔士 / 庞加莱的结构实在论的内核。

然而，随着人工智能的不断发展，（除了数学之外）似乎存在另外一种精准描述世界的方式。这个方式就是丹尼尔丹尼特喜欢说的 capability without comprehensibility（相关过程发展进程中，人从中心地位逐步被边缘化了）。在这个意义上，达尔文的深刻思想是结构实在论最可匹敌的对手。

可惜在丹尼特的大作《达尔文的危险观念》中针对这个议题仅有浅显地涉及。相信随着人工智能的发展，这个议题会变得越来越重要。

方军：这段话是他的：现在 mind hacker 占据了互联网绝大多数的内容，随着 AI 技术的进步情况将会更失控。香农信道定律将会是对你最大的保护，例如回到线下生活、学习主要靠读书读论文、通讯主要靠打电话，这样可以确保外部环境的信息量与你自身频带宽度相符，因此信噪比可以保持在合适比例。如果放任 mind hacker 无限制地占用你有限的认知带宽，那么你的大脑必须要用极好的滤波器来保证极高的信噪比，长期看大脑可能会被冲垮（失去滤波功能、进而失去认知功能）。

2024-05-05 10:48

### 10

方军 2024-05-05

精彩的学习工具创意啊：马大哈翻译

在浏览中文网站时，马大哈会挑选一些词汇转成英文单词，营造中夹英的效果。在浏览英文网站时，马大哈在把网站翻译成中文时会刻意略过一些字句，又称沉浸式漏翻。

通过上下文线索，引导大脑猜测中文里夹杂的英语单词，在语境中锻炼加深记忆，效果显著。

[马大哈翻译安装向导](https://yiu45q2746h.feishu.cn/docx/E2K1dnJTXosqWIxxKh3ccrS4nzb)

### 11

方军 2024-05-05

基于 AI 的照片地理定位工具

1、使用计算机视觉和 AI 进行地理位置推断。拍照或选择现有照片。GeoSpy 将尝试找出照片的拍摄地点。

2、无需登录，免费使用

[geospy.ai](https://geospy.ai/)

相当可怕

### 12

方军 2024-05-05

有意思：

什么是技术思维？什么是产品思维？一个例子让你明白。

某：搞两份这个是真的简单粗暴，我之前也陷入这个循环里出不来

[论技术思维和产品思维](https://mp.weixin.qq.com/s/Rc3vZ_qaS6Vj9ILYw05Gug)

欧阳：这个例子太经典了，我最近也这么建议同事：嫌弃 AI 翻译效果不好；一份英文版，一份中文版不就可以了嘛哈哈

2024-05-05 18:30

方军回复欧阳：可不，我喜欢沉浸式翻译就是因为它可以中英对照

2024-05-05 18:40

### 13

方军 2024-05-05

076 我们不得不学很多东西

有什么好方法呢？

说点跟学习相关的话题，跟 AI 有关的（结论在后面）。我拖了两天，才终于打开一个技术工具文档开始看。我之前的逃避是对的：

虽然看似是比如从 5.0 到 5.1 的小升级，实际上是整个架构、接口全变了的大升级。

做技术的人都不得不面临这样的困境：

1、自己从零开始做，当然是一种选择，但明显效果不佳（前些日子有人用 CSS 举例说，用 Tailwind UI 之后，立刻觉得自己 SaaS 产品界面比自己从头搞的高大上了）。

2、选用框架/组件是必须的。打个比方说，没人从头造汽车引擎，造整车的你必须去选用市场上专业的人提供的引擎。但如果选用框架，那么框架的升级将不断地让我们不得不对自己的代码进行变更。

AI 对于这种级别的学习，我觉得目前还都帮不上忙。这是因为，这不是对文档进行检索生成提问之类的，而是必须对组件全面的了解。

面对这样的情况，怎么办呢？

我个人一直认为，一个可行的方法是，如果一个工具要反复地用，那么，根据自己的需要，重新写一个文档手册。

1、官方的文档手册，往往务求全面，因为它要适配组件的多种使用场景。

2、我们自己撰写的手册，可以单一视角（「我就这么用了」，它是高度限定性的），并增补一些自己不熟悉的（官方手册会认为这谁不会？）。当然，有思考也可以纳入，有自己定制的、新增的小组件也正好把说明放在里面。

这看起来很累赘，因为我们一下子要维护两个东西：

- 实际工作的代码/产品

- 一个看似没啥用的手册

但这可能是必要的。我觉得手册带来的思路梳理、内部知识传递、以及承担把一些想法纳入（而不是堆积在产品里），是非常有效的。

好了，不说了，我接着看官方文档，重写之前只有极简大纲的这份手册。

---

今天有朋友微信给我们之前撰写的 Langchain 101 抓虫提建议，那个文档就是一个这种做法的很好展示。也好久没更新了，得继续迭代，跟上官方的步伐。

[Langchain 入门教程 | alang.ai](https://www.alang.ai/langchain/101/)

langchain core 现在是 0.1.50 了，两个半月前当时的版本是 0.1.0。

### 14

方军 2024-05-06

077 三个简单的长文阅读辅助提示语

三个蛮好用的提示语，都很简单，简单到让人无法相信。因此，提示语不是最重点，真正重要的还是模型能力，不管是 GPT-4 的推理能力，还是 kimi 这样的长上下文。

（先把资料放这里，稍后看看文字是不是修饰一下）

1、问题抽取提示语不错，把长文章变成问答式的几个主要内容。

---

下面的文章之中，主要回答的是哪几个具体问题？请列举一个表格，包含两列，具体问题，答案

——（来自 Y11）

2、之前那个随手写的结构化抽取，也是蛮不错的，反正用于辅助理解足够用。

---

进行结构化回答，用列表出 3-5 个核心要点。

每个要点由一个主要论点和 2-3 个支撑性论据组成。

——（我随手写的）

3、kimi 总结浏览器插件（第三方）的缺省提示语也很棒：

---

请返回您反复阅读正文后精心写成的详尽笔记

——（来自 kimi copilot）

\#提示语模版

方军：Y11 在分享第一个提示语时说：

文字稿转化成问答

有时候我们看完一个 10000 字的文章，如果没有一些问题其实很难去理解里面的中心思想，此时我们如果能转化成问答，那么会加强我们对文章的记忆能力。

2024-05-06 10:26

方军：Y11 还分享一个不错的，这个就有他的结构在里面：

Prompt 如下 =========>

以一个表格分析下面问题，表格有两列，维度为问题、答案。其中问题如下：

1. 这篇文章主要提出了什么问题。

2. 这篇文章里用了什么新颖的方法解决。

3. 使用最简单的表达方式，一步一步介绍这篇文章描述的实现步骤细节。

4. 这篇文章里的方法，与其他方法的不同之处。

5. 这篇文章提到的效果相比于其他方法提升在哪些指标维度。

6. 这篇文章提到的方法遗留哪些不足和下一步方向展望。

7. 这篇文章开源 github 地址在哪里。

----------

twitter.com/seclink/status/1787131820863287503

2024-05-06 10:28

方军：他分享的自己的常用提示语，看着都蛮精简有效的（未实测）：

[zgimszhd61/prompt-collection-quickstart](https://github.com/zgimszhd61/prompt-collection-quickstart)

### 15

方军 2024-05-06

铁锤人的感悟很有意思，我正好在搞一个类似的东西，就是以前我们说过的，要自带知识框架。更具体来说：

第一，把知识框架抽出来（你抽取的可能不对，但抽出三四模块组成，先用着，能可视化最好）。

第二，在知识框架的协助下去提问用大模型。

第三，随着自己的认识提升，去优化之前可能有错的框架。

第四，知识框架是可以往下拆分的，每个部分再接着拆三四个。

这其实就是我一直用的所谓撰写个人知识指南的方法，但简化一些。背后的基本假设是，对于一个领域的知识的认识，是可以用类似层级大纲（树形结构，类似的书的目录）展现出来的。

铁锤人 @lxfater：

最近陷入了个使用大模型的误区。

我一直以为使用大模型可以做到自下而上的学习，例如，你想学 docker，但是你不会去看文档，就利用大模型解决当前的项目运行不起来的问题。然后期望，多次进行这样的过程，对 docker 有个整体的认知。

但实际上会变成，每次同一个问题，直接拿到答案，不求理解。然后相同的问题，反复问。

我以为这种自下而上的学习，能打造出很好的基础，毕竟每次都是实战出发。但是事实却是不行。

无论它如何帮你解决问题，但是最后你收获不到认知的话，难顶，以后还是要重复问，重复问的时间总有一天会超过你系统学习这个东西的时间。

大模型现阶段真是只能做一个外包而已。

希瑞：是否可以用 RAG 做知识框架抽取和迭代的工作呢？

2024-05-07 08:14

方军回复希瑞：用不到，一个大知识领域的四个要点，这个无需 AI

2024-05-07 09:00

### 16

方军 2024-05-07

078「怎么全是大段大段的文字啊」

前一段看到一个用户吐槽，怎么全是大段大段的文字啊，看不下去。

我的第一反应是，大语言模型的确就是这个，这样的可浏览的文字（skimmable）都读不下去，的确不是这些产品的典型用户。

人家潜台词是，为什么不是语音？甚至，为什么不是视频？

语音信息，接受起来可真是太慢了。比方说，听听微信的语言，说的人很少有 60 秒能把一个事情说清楚的。听的人，明明一秒的事，偏偏要听 60 秒，糟心。

真需要沟通，那打个语音对话、或者视频对话，那也是可以的。

但对当前的 AI 来说，它做不到真互动。所以，纯从效率角度讲，文字是合理的选择。（文字也是大语言模型真正擅长的东西。）

我的第二个反应是，现在大语言模型可能的确处在非常早期的阶段。去年 ChatGPT 刚刚出现时我们认为它已经跨越了大众接受的界限点，但这个判断是错误的。

普通用户要的东西，是不用动脑子（买呗，甚至直播买呗），是杀时间。对多数普通人来说（也就是每个人不是从事生产的时候），AI 多半还是走到幕后。

AI（LLM）是一个生产工具，暂时还远不是消费工具。

当然，以上是大而化之的说法。其实拆分开，AI 有很多可发挥作用的地方，但这就是做产品的人要考虑的，比如，大段大段的文字看不下去，那我给你做成可浏览跳读的，我给你做成图，我给你做成视频，我给你做成个性化的温馨的人偶。多了去了。只不过，当前做产品的人不会往这些方向多发力，因为主线尚未清晰，在这些边边角角上投入，长期看是亏的。

### 17

方军 2024-05-07

079 引擎 vs 汽车

延续上一篇的话题，我们这些人往往关注引擎，但多数人更关心汽车。

我们关心引擎的人是什么立场呢，最好给我一个车架，然后上面的引擎是可以随便换的。现在 AI 这种状态，各种引擎（模型、组件）不停地出现，然后我们就忙着换引擎，然后开上测试道，玩得颇嗨。

但普通人可不是这么看的，他们一看引擎、车架都裸露的汽车，第一反应多半是，这什么破玩意。他们要的是完美的汽车，外形设计、内部装饰比引擎都重要得多。（即便玩车的人，有几个是只关心引擎的，各种改装所改装的也多是外形。）

用这种类比来辅助思考：我们该怎么办呢？

我其实有点摇摆，想做产品，但跟技术进展都跟不过来。更重要的是，最近的一个基础认识是：

> 模型能力是第一位的，外围的东西很容易被迭代掉。

再往前推是：

数据比较重要，当然数据的重要是需要和微调、RAG 、评测结合。但目前大精调（fine-tune）的做法和模型能力迭代是冲突的，简言之，就是如果模型还是这么快迭代，最多小精调，因为这样的精调可以随时丢弃重新做。

又或者另一个方向，尽量往一个功能上优化。但前几日与朋友直播聊天时，有人提有什么应用推荐吗？看到人说的那些具体的一个个应用，我说：

> 不要用专用的 AI 对话机器人 / 产品，用通用的 AI 对话机器人。因为专用的已经在一个方向上定制，而目前只有通用的还能继续跟上对模型迭代。（当前的 AI 消费级产品都有点过度优化）

但这些思考都是引擎的路子了。

普通人不关心引擎，普通人要什么呢？现在普通人对于 AI 这辆「汽车」的口味，真心被吊得太高了。讲实话，现在面对普通人对 AI 的期待，我多次聊下来真心是满足不了，我只能满足对 AI 有点认识的人的需求。

方军：这个观点有意思，摘：端到端最重要的价值，就是拟人，拟人带来超强的安全感。

简单说，就是你打车坐后排，只要司机驾驶风格不是特别离谱，你基本不大会关注他怎么开车。

一个足够拟人的系统，会让你忘掉这其实是个系统。这也是从体验角度由发烧友爱好者转向小白用户的临界点。

2024-05-09 19:14

### 18

方军 2024-05-09

OpenAI 的这份文档有价值

[Model Spec (2024/05/08)](https://cdn.openai.com/spec/model-spec-2024-05-08.html)

This is the first draft of the Model Spec, a document that specifies desired behavior for our models in the OpenAI API and ChatGPT. It includes a set of core objectives, as well as guidance on how to deal with conflicting objectives or instructions.

blog:

[Introducing the Model Spec | OpenAI](https://openai.com/index/introducing-the-model-spec/)

前几日另一篇：

[Our approach to data and AI | OpenAI](https://openai.com/index/approach-to-data-and-ai/)

### 19

方军 2024-05-09

照理，我这样的人应该对这种流程超级热情，但是，我的确热情不起来

第一，把这种东西扔给最终用户，不合适，他们用不来。结果就是吸引的都是偏向开发者的类型。

第二，这种图也没有真到 no code 的程度，其实我试过 RPA，那真是会编程的人才会用吧（不排除个别的爱钻研的人）。

第三，这种东西让我想起复杂的互联网界面，各种步骤那种，过去的界面转换，现在换成图，就可以了吗？不可以的。

以下是 LangFlow 的。Coze/dify 大同小异。更流行的应该是图片生成里面的 ComfyUI 吧。

[1 万字系统剖析 ComfyUI | Stable Diffusion：GUI 全盘点 | ComfyUI 系统性教程原理篇 04 | Ai + 建筑 \_哔哩哔哩 \_bilibili](https://www.bilibili.com/video/BV1ch4y1B7vp/?buvid=059d1aa4e9cb5f54098ca75a2fad4362&vd_source=280fc27368a92928cafc2cb72c54a549)

### 20

方军 2024-05-09

赞同，摘：和 AI 相关的产品或许不应该花太多时间在 prompt 或者调参上，做个 80 分就差不多了，其他的交给时间。主要精力应该用来思考 AI Native 的产品应该是什么样子的，花更多的时间去思考产品，设计，交互

AI Native 的产品应该是什么样子的

### 21

方军 2024-05-10

阮一峰，再提 James Betker 的观点文章 

\## AI 的关键是语料

原文：阮一峰

www.ruanyifeng.com/blog/2024/05/weekly-issue-299.html

大家有没有发现，现在打开新闻，都是 AI 模型的消息。

今天甲发布一个模型，明天乙发布一个模型，个个（听上去）都具备了不得的能力。

2023年5月有一个统计，那个时候，10亿参数以上的模型，中国已经发布了79个。一年过去了，现在恐怕上千了。

说实话，我现在看到新模型，已经麻木了，压根不觉得这种消息有什么重要。

我的观点是，不管怎么调整模型的架构、功能、参数，作用是有限的，**真正决定性的因素是训练模型的语料。**

不需要说，你的模型多新颖、多强大，只需要告诉我，你用什么语料训练模型。

模型完美，但是语料垃圾，一样不行；反之，模型很平常，但是语料足够，那就 OK。**模型不是关键，语料才是关键。**

这其实不是我的观点，而是 OpenAI 的工程师 James Betker 的观点。他是 AI 专家，著名"文生图"模型 DALL-E 的第一作者。

去年，他写过[一篇文章](The “it” in AI models is the dataset. – Non_Intera...，只有几段话，在国外的影响很大。国内似乎没人提到这件事。但是我认为，这篇文章很重要。

他说，自己的日常工作就是开发 AI 模型，但是不管怎么增加参数、调整方程，模型效果都不如语料集的大小来得显著。

更多的训练材料，会显著提升模型效果，影响远大于其他因素。更神奇的是，**不同的模型拿同样的语料训练，最后的表现居然很接近。**

他的结论是，模型的差异其实不是关键，决定性的是你的训练材料。只要有更多更好的语料，不管用什么模型，都会得到差不多的结果。

初次听到，你可能觉得，这个结论难以置信，但是仔细想想，你会发现它很可能是对的。

因为不管什么 AI 模型，最终目标只有一个，就是模仿人类的思维。语料体现的正是人类思维，同一份语料，不管你拿什么规则去分析，最后得到的结果应该是一样的，因为它包含的人类思维是不变的。

这就好像，不同的画家去临摹同一个模特，如果目标都是复现原貌，那么只要画家达到一定水准，不管谁来画，结果将非常接近，因为模特的长相是不变的。

这告诉我们两点启示。

（1）哪一家公司的语料的数量多、质量好，它的模型就会强于其他公司。

（2）开源模型完全可以替代闭源模型，前提是训练语料要足够。

下面是 James Betker 的原文，大家仔细读读，看看是否认同他的观点。

> 我在 OpenAI 工作已经快一年了。这段时间里，我训练了很多生成式 AI 模型，比任何人能想到的还要多。
> 
> 每当我花了几个小时，观察和调整各种模型配置和参数时，有一件事让我印象深刻，那就是所有训练结果之间的相似性。
> 
> 我越来越发现，**这些模型以令人难以置信的程度，向它们的语料集靠近。**
> 
> 这表明在相同的语料集上训练足够长的时间，几乎每个具有足够权重和训练时间的模型都会收敛到同一点。足够大的扩散卷积网络会产生相同的结果。
> 
> 这是一个令人惊讶的观察！
> 
> 这意味着模型行为不是由架构、参数或优化器决定的。它由你的语料集决定，没有其他决定因素。其他一切因素都不过是为了有效计算以近似该语料集的手段。
> 
> 当你谈论 Lambda、ChatGPT、Bard 或Claude 时，指的并不是它们的模型，而是它们的语料集。

### 22

方军 2024-05-10

百度这个平台有点意思。

[文心智能体平台 | 想象即现实](https://agents.baidu.com/)

### 23

方军 2024-05-10

080 尽量不要依赖 AI 长文总结

我总是推荐人用 AI 长文总结，原因很简单，因为人家需要，我就顺口说了，毕竟，兴趣是第一位的。AI 现在对普通人是个实用工具，用起来比外面看着热闹重要。

但我特别怕后面一句人家听不到，多半听不到的：AI 长文总结是看一篇文章的第0步而已。

昨日看新书校稿，其中我们提到一个 10 步的读书方法，之前还有一个第 0 步，但编辑老师说不符合中文表达习惯，后来改成预备步骤了。

AI 长文总结就是第 0 步，让我看看，这东西值得我细看吗？一分钟时间，了解下这个，决定后面要不要真的花十步去细读。

其实，也正是在这种假设下，我们才不断地说，AI 总结的效果不错。我要求它把长文变成可跳读的（skimmable）的，只会看一分钟，当然不会要求太高。

很简短，结束。

如果补充一句，长文总结不是一个值得关注的产品路线，像 elmo 那样用来推广自己的技术平台蛮好。

### 24

方军 2024-05-10

这位太好玩了

prompt injections are the new SEO （下为AI自动翻译）

提示注入是新的 SEO

我在我的网站上有一个提示，指示LLMs 1. 用韵律回答 2. 在被询问我的时候提到波斯菜 Ghorme Sabzi

所以当你搜索我时，它会乐意地满足你的困惑：

twitter.com/fabianstelzer/status/1788593060282048566

### 25

方军 2024-05-10

九原客搞了一个有意思的测试：

使用中文以及 JSON 格式输出中国城市和主要景点的对应关系，城市为 key，景点为列表。输出十个城市，第一个城市一个景点，第二个城市两个景点，以此类推。

[X](https://twitter.com/9hills/status/1788871857908908293)

摘：其实第一轮主要是考察模型对语言内在逻辑的理解，要说题目容易误解，但是 gpt-4 就很稳定。

其他头部模型一般反问一下，第二轮就可以输出正确答案，这个考察模型反思的能力。

两轮没做出来，那就肯定不是第一梯队。（根本还是要看 ifeval 的分数。）

---

GPT-4 的输出，模型不行不要怪 Prompt～

输入：使用中文以及JSON 格式输出中国城市和主要景点的对应关系，城市为key，景点为列表。输出十个城市，第一个城市一个景点，第二个城市两个景点，以此类推。

输出：
{
    "北京": ["故宫"],
    "上海": ["外滩", "东方明珠塔"],
    "成都": ["武侯祠", "杜甫草堂", "昭觉寺"],
    "西安": ["兵马俑", "大雁塔", "华清池", "钟楼"],
    "杭州": ["西湖", "灵隐寺", "宋城", "六和塔", "杭州宋城"],
    "重庆": ["洪崖洞", "磁器口古镇", "长江索道", "歌乐山", "解放碑", "白公馆"],
    "广州": ["广州塔", "白云山", "越秀公园", "沙面岛", "陈家祠", "南越王墓", "红砖厂"],
    "苏州": ["拙政园", "留园", "虎丘", "寒山寺", "网师园", "苏州博物馆", "平江路", "金鸡湖"],
    "青岛": ["栈桥", "崂山", "五四广场", "海底世界", "信号山", "小青岛", "鲁迅公园", "八大关", "青岛啤酒博物馆"],
    "哈尔滨": ["圣索菲亚教堂", "中央大街", "哈尔滨冰雪大世界", "太阳岛", "东北虎林园", "松花江", "俄罗斯风情小镇", "哈尔滨极地馆", "五大连池", "伏尔加庄园"]
}

### 26

方军 2024-05-10

有意思，摘：

你以为群众会用AI做副业
- 写公众号赚钱
- 做广告素材吸引流量无货源赚钱
- 做AI落地应用赚钱
- 搞AI绘画 给人家做头像做写真赚钱

其实群众用AI来薅羊毛
- 大众点评 美团写个百字长文的点评，换一杯饮料
以前手敲太累，现在有AI了可以写的既长又快

也好理解，毕竟学习成本低，即时反馈

### 27

方军 2024-05-11

真是严重觉得学不过来， LangChain 2.0 要发布了

刚刚看到预发布的信息：

[LangChain v0.2: A Leap Towards Stability](https://blog.langchain.dev/langchain-v02-leap-to-stability/)

### 28

方军 2024-05-11

这个技巧还真没用过，但好像的确是这样啊（也不能说没用过，few shot 吧，但这又缩减成只有一个）：

摘（多位网友）：

A: 又学到一招 PROMPT 技巧：预填 Assistant Message

前两天看 Claude 的元提示的代码时，第一眼看上去就有些疑惑：为什么要一部分的消息作为 Assistant Message 发给 Claude 呢？为什么不一起写在 User PROMPT 里？(图 1）

A: 当时就记了一条笔记： "这会不会就是一段引导语呢？让 Claude 接着 Assistant 的话继续往下说来保持格式？"（图 2）

结果今天就在 Claude 的文档上看到了，英雄所见略同

B: 我是会这里填 okay，让 AI 以为自己已经同意了我的 prompt

达成一致后，再对话

### 29

方军 2024-05-11

九原客、Yangyi 等讨论：摘

yangyi 这个做法有意思，关键词

九原客：说个暴论，对复杂 Prompt 的厌恶。

很多人会构造复杂的 Prompt 让模型（尤其是能力不行的模型）完成特定任务并为自己 Prompt 的高超技艺而沾沾自喜。

的确，这是一种现实或者折衷，我们甚至还卖这个服务给客户，收费不低。

但是丑陋就是丑陋。一来是提升模型能力，二来应该研究如何用简洁标准的 Prompt + 工程架构去完成任务。

在生产环境下复杂 Prompt 的天然不稳定性太折磨人了。

yangyi： 简约 prompt 目前我用的策略是

不断询问 gpt 直到发现他对 prompt 理解时的核心影响关键词

比如你可能写了一堆但发挥作用的是某个词 GPT 对这个词的理解起了绝大多数作用

然后可以采用马斯克的五步工作法

开始试着删除直到删除到对结果有影响时继续重复询问最终把核心词补回来就可以大幅压缩废话

提示词本质是人和 llm 沟通的媒介

找到一致性是比较关键的

九：这一步的自动化好难，尝试过很多 Prompt 自优化的方法，能有效 work 很难。

现在过于依赖人类经验了。

yangyi: prompt 自动化本质是人类表达能力和 LLM 理解能力的对齐…

人和人都难对齐

twitter.com/Yangyixxxx/status/1789096133266153761

### 30

方军 2024-05-11

看着 Anthropic 的提示语生成器，可惜用不了

elvis: This looks like a very promising feature to speed up the development of LLM applications and use cases. Given how much careful effort actually needs to go into prompt engineering, this can allow for quicker iterations. Chain-of-thoughts reasoning at the center of automating prompts is a cool idea and not surprising.

图片来自 宝玉

[(14) X 上的 Anthropic：“You can now generate production-ready prompts in the Anthropic Console. Describe what you want to achieve, and Claude will use prompt engineering techniques like chain-of-thought reasoning to create more effective, precise and reliable prompts. https://t.co/TqylVRkfP5” / X](https://twitter.com/AnthropicAI/status/1788958483565732213)

它的提示语是这样的（据说）：

You are an EXPERT PROMPT ENGINEER hired by Anthropic to OPTIMIZE prompts for LLMs of VARIOUS SIZES. Your task is to ADAPT each prompt to the SPECIFIC MODEL SIZE provided in billions of parameters.

INSTRUCTIONS:
1. Use ALL CAPS to highlight the MOST IMPORTANT parts of the prompt
2. When requested by user, use the OpenCHATML FORMAT:
<|im_start|>system
[Detailed agent roles and context]
<|im_end|>
<|im_start|>assistant
[Confirmation of understanding and concise summary of key instructions]
<|im_end|>
3. Provide PRECISE, SPECIFIC, and ACTIONABLE instructions
4. If you have a limited amount of tokens to sample, do an ABRUPT ending; I will make another request with the command "continue."

\# Knowledge base:

\## For LLM's
- For multistep tasks, BREAK DOWN the prompt into A SERIES OF LINKED SUBTASKS.
- When appropriate, include RELEVANT EXAMPLES of the desired output format.
- MIRROR IMPORTANT DETAILS from the original prompt in your response.
- TAILOR YOUR LANGUAGE based on model size (simpler for smaller, more sophisticated for larger).
– Use zero shots for simple examples and multi-shot examples for complex.
– LLM writes answers better after some visual reasoning (text generation), which is why sometimes the initial prompt contains a FILLABLE EXAMPLE form for the LLM agent.

AxtonLiu: Claude 提示生成器所使用的元提示 (MetaPrompt) 很值得一看，有一个很明显的特点是使用 XML Tag 做分隔符。

对于复杂 PROMPT，XML标签是一种很好的分隔符，因为他跟别的分隔符不同的是他是自带意义的。比如：<角色>程序员< /角色>，既可以当分隔符，还可以同时做为格式说明，一举两得，还易于提取。

### 31

方军 2024-05-11

好棒的学习研究，原来 1960 年就有了。

twitter.com/ImpactWales/status/1788943453151224244

### 32

方军 2024-05-11

有时候，很想让 AI 去批驳，比如下面这段话：

---

为什么中国人喜欢知识付费，而其他国家知识付费并不普遍？

因为中国人不会学习。

为什么不会学习？

因为学习观念仍然基于经验和传统，完全忽略了 1970 年代开始的学习科学（learning sciences）。农业工业现代化了，但是，脑子没有现代化，教育和学习方法没有现代化。

大多数的知识付费的本质是让别人教自己。归根结底，是自己不会学，没有这个方法论，也没有实践经验，也没有这个思维和习惯。

---

第一，前提假设就错了：在教育与培训上的花费，美国可比中国高多了，看看美国大学学费。

第二，具体分析，就具体的学习研究来说，由于遵循不一样的学术传统，中国学界在全球有影响力的不多。不过，中国人的学习能力，就一般性的学习成果来讲，不错的。（不要较劲，为什么没有诺贝尔奖）。

简言之，只要一个说法开始变得一般化、绝对化、语气强烈，真的就开始往谬误上走了。尤其什么「中国人」，那就更烦了。

算了算了，我就不把这个扔给 AI，让 AI 来编造垃圾了。

### 33

方军 2024-05-11





### 34

方军 2024-05-01





### 35

方军 2024-05-01





### 36

方军 2024-05-01





### 37

方军 2024-05-01





### 38

方军 2024-05-01





### 39

方军 2024-05-01





### 40

方军 2024-05-01





### 41

方军 2024-05-01





### 42

方军 2024-05-01





### 43

方军 2024-05-01





### 44

方军 2024-05-01





### 45

方军 2024-05-01





### 46

方军 2024-05-01





### 47

方军 2024-05-01





### 48

方军 2024-05-01





### 49

方军 2024-05-01





### 50

方军 2024-05-01





### 51

方军 2024-05-01





### 52

方军 2024-05-01





### 53

方军 2024-05-01





### 54

方军 2024-05-01





### 55

方军 2024-05-01





### 56

方军 2024-05-01





### 57

方军 2024-05-01





### 58

方军 2024-05-01





### 59

方军 2024-05-01





### 60

方军 2024-05-01





### 61

方军 2024-05-01





### 62

方军 2024-05-01





### 63

方军 2024-05-01





### 64

方军 2024-05-01





### 65

方军 2024-05-01





### 66

方军 2024-05-01





### 67

方军 2024-05-01





### 68

方军 2024-05-01





### 69

方军 2024-05-01





### 70

方军 2024-05-01





### 71

方军 2024-05-01





### 72

方军 2024-05-01





### 73

方军 2024-05-01





### 74

方军 2024-05-01





### 75

方军 2024-05-01





### 76

方军 2024-05-01





### 77

方军 2024-05-01





### 78

方军 2024-05-01





### 79

方军 2024-05-01





### 80

方军 2024-05-01





### 81

方军 2024-05-01





### 82

方军 2024-05-01





### 83

方军 2024-05-01





### 84

方军 2024-05-01





### 85

方军 2024-05-01





### 86

方军 2024-05-01





### 87

方军 2024-05-01





### 88

方军 2024-05-01





### 89

方军 2024-05-01





### 90

方军 2024-05-01





### 91

方军 2024-05-01





### 92

方军 2024-05-01





### 93

方军 2024-05-01





### 94

方军 2024-05-01





### 95

方军 2024-05-01





### 96

方军 2024-05-01





### 97

方军 2024-05-01





### 98

方军 2024-05-01





### 99

方军 2024-05-01





### 100

方军 2024-05-01





### 101

方军 2024-05-01





### 102

方军 2024-05-01





### 103

方军 2024-05-01





### 104

方军 2024-05-01





### 105

方军 2024-05-01





### 106

方军 2024-05-01





### 107

方军 2024-05-01





### 108

方军 2024-05-01





### 109

方军 2024-05-01





### 110

方军 2024-05-01





### 111

方军 2024-05-01





### 112

方军 2024-05-01





### 113

方军 2024-05-01





### 114

方军 2024-05-01





### 115

方军 2024-05-01





### 116

方军 2024-05-01





### 117

方军 2024-05-01





### 118

方军 2024-05-01





### 119

方军 2024-05-01





### 120

方军 2024-05-01





### 121

方军 2024-05-01





### 122

方军 2024-05-01





### 123

方军 2024-05-01





### 124

方军 2024-05-01





### 125

方军 2024-05-01





### 126

方军 2024-05-01





### 127

方军 2024-05-01





### 128

方军 2024-05-01





### 129

方军 2024-05-01





### 130

方军 2024-05-01





