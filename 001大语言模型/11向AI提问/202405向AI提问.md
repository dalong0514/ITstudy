### 01

方军 2024-05-01

Globe 这个AI聚合搜索引擎似乎把搜索又上了一个大台阶

当前看到它的特点主要是：

左侧的多级分类

右侧的以图片为主的展示

（右侧这个，会用搜索进行知识学习的人都知道吧，这是最有用的搜索形态）

https://explorer.globe.engineer/

有点贵，我在考虑要不要充值一个月试试，一年肯定不充了，谁知道一年后什么更酷的产品出来。

### 02

方军 2024-05-01

摘：模型即数据
要舍得在数据上投入
这是最本质的事情

人脑的模型也是如此
在学习优质数据上投入时间、金钱和注意力

原文：

The “it” in AI models is the dataset.
AI 模型中的“it”是数据集。

Posted on June 10, 2023 by jbetker

I’ve been at OpenAI for almost a year now. In that time, I’ve trained a lot of generative models. More than anyone really has any right to train. As I’ve spent these hours observing the effects of tweaking various model configurations and hyperparameters, one thing that has struck me is the similarities in between all the training runs.
我现在已经在 OpenAI 工作将近一年了。在这段时间里，我训练了许多生成模型。比任何人都有权利训练的还要多。当我花了这些时间观察调整各种模型配置和超参数的影响时，有一件事让我印象深刻，那就是所有训练运行之间的相似之处。
It’s becoming awfully clear to me that these models are truly approximating their datasets to an incredible degree. What that means is not only that they learn what it means to be a dog or a cat, but the interstitial frequencies between distributions that don’t matter, like what photos humans are likely to take or words humans commonly write down.
对我来说，这些模型确实以令人难以置信的程度逼近它们的数据集。这意味着它们不仅学会了什么是狗或猫，还学会了不重要的分布之间的插值频率，比如人类可能拍摄的照片或人类常常写下的单词。
What this manifests as is – trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion.
这种表现形式是 - 在相同的数据集上进行足够长时间的训练，几乎每个具有足够权重和训练时间的模型都会收敛到相同的点。足够大的扩散 conv-unets 生成与 ViT 生成器相同的图像。AR 采样生成与扩散相同的图像。
This is a surprising observation! It implies that model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset.
这是一个令人惊讶的观察！这意味着模型行为不是由架构、超参数或优化器选择决定的。它是由你的数据集决定的，没有别的。其他一切都是为了有效地将计算交付给逼近该数据集的手段。
Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.
然后，当你提到“Lambda”、“ChatGPT”、“Bard”或“Claude”时，你指的不是模型权重，而是数据集。

nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/

### 03

方军 2024-05-01

蛮有意思的，好像 kimi 没用智能体这个词，用的是普普通通的「私人助理」，后者要更友好些

我一直很好奇，为什么有个圈子特别喜欢智能体这个词？ 智谱也喜欢这个词。

### 04

方军 2024-05-02

OpenAI 发布一个 Next.js 的模板，用于 assistant api

OpenAI Assistants API Quickstart

github.com/openai/openai-assistants-quickstart

粗看起来，这个还是远不如 vercel 开源的 Vercel AI Chatbot

github.com/vercel/ai-chatbot

Vercel 这个背后是它的 AI SDK (现在3.1.0)

sdk.vercel.ai/docs

不过，这个 AI SDK 的社区可以说不是特别活跃，使用也是要谨慎的。

### 05

方军 2024-05-04

跟了一年多，讲实话有时候还是觉得openai这个公司的产品定力不行

我始终觉得它的优势在模型，超强优势，但被各种外部影响做了好多分叉之外的事

事后看，function call很赞

assistant api一般

而去搞什么搜素，没戏啊

小弟弟 ppx 可以搞，你搞就有点…

[OpenAI下周将发布ChatGPT搜索引擎，挑战谷歌搜索！](https://mp.weixin.qq.com/s/kXHHXtVBJIOGIg1KZ9PO7A)

### 06

方军 2024-05-04

扬清对 OpenAI 搜索进行了评测，他认为这超过搜索 + RAG 的做法。

有兴趣可查看具体讨论：

twitter.com/jiayq/status/1786208795720593789

网友的争论：twitter.com/idoubicc/status/1786605308330000429

摘（为 AI 翻译）：

尝试用「今天黑客新闻中最受欢迎的帖子说了什么？」进行搜索，你会看到区别。

如果你简单地将其翻译为谷歌查询，比如「黑客新闻中最受欢迎的帖子」，它不会给你有用的总结结果。你必须真正理解它，将其翻译为有意义的行动（比如实际获取 https://news.ycombinator.com/best ），获得结果，然后理解它。

Bing AI - 搜索能够访问页面并给出一些结果，但答案略有偏差（它还伴随着太多的截图）。

基于搜索，http://gemini.google.com 无法解决它，只是说「因为它在一天之内会发生变化」。

http://perplexity.ai 和 Lepton Search 无法解决它。至少目前还不能。顺便说一句，Lepton 搜索旨在提供一个开源示例，展示如何构建一个搜索增强型对话模型，因此结果在没有一些基于代理的设计的情况下是合理的。

方军：idoubi

@idoubicc

twitter.com/idoubicc/status/1786605308330000429

关于贾扬清老师评论 ChatGPT 做 Search 会杀死大部分 Wrapper 型 AI 搜索引擎的这个帖子，我有一些不一样的看法👇

1. AI 搜索引擎的第一要义是准确度。

准确度的决定性因素主要是两个：问答底座模型的智能程度 + 挂载上下文的信息密度。

做好 AI 搜索引擎的关键，选用最智能的问答底座模型，再对 RAG 的检索结果进行排序去重，保证信息密度。

第一个步骤容易，第二个步骤很难。所以现在市面上大部分的 AI 搜索引擎，包括 Perplexity，准确度也就 60% 左右。

2. ChatGPT 自己做搜索，首先保证了问答底座模型的智能程度。……

3. 我并不觉得大模型厂商自己做 AI 搜索就一定会比第三方做的好。……

4. 做好 AI 搜索引擎，最重要的三点是准 / 快 / ……

5. AI 搜索引擎是一个持续雕花的过程。……

6. AI Search + Agents + Workflows 是趋势。

AI Search 做通用场景，通过 Agents 做垂直场景，支持个性化搜索需求。

通过 Workflows 实现更加复杂的流程编排，有机会把某类需求解决的更好。

使用 GPTs 做出的提示词应用或知识库挂载型应用，价值点还是太薄。

7. 我个人不是太看好垂直搜索引擎。

一定程度上，垂直搜索引擎可以在某个场景做深做透，但是用户的搜索需求是非常多样的，我不太可能为了搜代码问题给 A 产品付费，再为了搜旅游攻略给 B 产品付费。

垂直搜索引擎自建 index 索引，工程投入比较大，效果不一定比接 Google API 要好，而且接入的信息源太有限。

8. AI 搜索是一个巨大的市场，短时间内很难形成垄断。……

9. AI 搜索引擎需要尽早考虑成本优化。……

以上是我个人做 http://ThinkAny.AI 一个多月以来的一些经验和思考。欢迎交流探讨。

2024-05-04 19:27

### 07

方军 2024-05-04

有人推荐了这个项目：github.com/reorproject/reor

Reor is an AI-powered desktop note-taking app: it automatically links related notes, answers questions on your notes and provides semantic search.

当你在撰写新的笔记时可以显示从语料库中 "检索" 到的相关的笔记。这是一种通过将当前笔记中的想法与语料库中的相关想法进行交叉参考来 "增强" 用户的思维的强大方式。

不过，我对这样的笔记写作是有极大疑虑的，个人的想法是：

有效的写作应该不是借助笔记，（最多或借助事先准备好的几张卡片 / 少量条目）。

只有这样，才能有效地利用自己的大脑，对知识进行梳理。

这也是我对各类所谓的双链笔记没什么特别印象的原因。那些链接最终是一团乱麻，对我，仅仅是视觉上吸引人（但我又恰巧不喜欢那样的视觉冲击）。

### 08

方军 2024-05-04

摘：最近崔娃和微软一起合办了一个 AI 相关的视频节目，叫：《The Prompt with Trevor Noah》。它的口号是：「You can't find answers without asking the right questions」。这正是我们日常使用 Prompt（提示词）使用 AI 的关键所在 —— 如果你不能提出正确的问题，就无法找到答案。

有意思啊，但有点过于严肃了，不像他

www.microsoft.com/en-us/research/group/ai-for-good-research-lab/the-prompt/

### 09

方军 2024-05-05

虽然我知道前面的这部分不是全部的事实，而是更多的情绪，但是的确引发感慨，后面的那段很棒（补充，如果不是要看科普，而是要看文档手册和开源，互联网和 AI 会更有用，还有阅读长文 / 最好是文档手册的能力），摘：

大约在十几年前网速不咋快、也没有短视频大流行的时候，我会去一些科学杂志网站蹭免费文章看，那里有很多专业作者写的高质量报道，还有各种精美插图。但是随着社交自媒体兴起，这些网站迅速被边缘化了，被淹没在大量无来源的污染信息和所谓的头条热搜里。这是数字时代的真正悲剧。

以下是一部分杂志的名字和他们今天首页的模样，怀念我还能在路边摊偶然买到其中一些杂志纸质版的时候：…

现在是互联网最好的时代，因为机器翻译质量已经快把巴比伦塔削平了，和 99.9% 的历史时间相比，今天你获得任何一种陌生语言载体的优质信息的难度几乎是零。现在也是互联网最糟的时代，所有媒体平台都在流量利益驱动下一起摔进下水道了，信息壁垒比以往任何时候都低，又比以往任何时候都高。这是个碾压在亚历山大图书馆头上的震耳欲聋的垃圾场，在神圣的万维网书架前人类只有一种堕落就是自甘堕落。

方军：接着摘另一个相关的：现在 mind hacker 占据了互联网绝大多数的内容，随着 AI 技术的进步情况将会更失控。香农信道定律将会是对你最大的保护，例如回到线下生活、学习主要靠读书读论文、通讯主要靠打电话，这样可以确保外部环境的信息量与你自身频带宽度相符，因此信噪比可以保持在合适比例。如果放任 mind hacker 无限制地占用你有限的认知带宽，那么你的大脑必须要用极好的滤波器来保证极高的信噪比，长期看大脑可能会被冲垮（失去滤波功能、进而失去认知功能）。

2024-05-05 10:43

方军：想起以前一个朋友的故事，她作为时尚和艺术行业中人，从来不买大牌，人问为什么，她答，我天天造这些… 当然她自己用的那些其实比大牌价格上几乎是一样的，只是没牌而已

2024-05-05 10:44

---

这位名叫 haiwenliu 的网友颇有意思：

智能的一大标志是：寻找结构。在寻找前所未知的结构方面，古希腊以来数学家和科学家携手取得了巨大的成功，Ta 们发展的数学语言成了目前所知的这个世界最精准的描述方式。这就是皮尔士 / 庞加莱的结构实在论的内核。

然而，随着人工智能的不断发展，（除了数学之外）似乎存在另外一种精准描述世界的方式。这个方式就是丹尼尔丹尼特喜欢说的 capability without comprehensibility（相关过程发展进程中，人从中心地位逐步被边缘化了）。在这个意义上，达尔文的深刻思想是结构实在论最可匹敌的对手。

可惜在丹尼特的大作《达尔文的危险观念》中针对这个议题仅有浅显地涉及。相信随着人工智能的发展，这个议题会变得越来越重要。

方军：这段话是他的：现在 mind hacker 占据了互联网绝大多数的内容，随着 AI 技术的进步情况将会更失控。香农信道定律将会是对你最大的保护，例如回到线下生活、学习主要靠读书读论文、通讯主要靠打电话，这样可以确保外部环境的信息量与你自身频带宽度相符，因此信噪比可以保持在合适比例。如果放任 mind hacker 无限制地占用你有限的认知带宽，那么你的大脑必须要用极好的滤波器来保证极高的信噪比，长期看大脑可能会被冲垮（失去滤波功能、进而失去认知功能）。

2024-05-05 10:48

### 10

方军 2024-05-05

精彩的学习工具创意啊：马大哈翻译

在浏览中文网站时，马大哈会挑选一些词汇转成英文单词，营造中夹英的效果。在浏览英文网站时，马大哈在把网站翻译成中文时会刻意略过一些字句，又称沉浸式漏翻。

通过上下文线索，引导大脑猜测中文里夹杂的英语单词，在语境中锻炼加深记忆，效果显著。

[马大哈翻译安装向导](https://yiu45q2746h.feishu.cn/docx/E2K1dnJTXosqWIxxKh3ccrS4nzb)

### 11

方军 2024-05-05

基于 AI 的照片地理定位工具

1、使用计算机视觉和 AI 进行地理位置推断。拍照或选择现有照片。GeoSpy 将尝试找出照片的拍摄地点。

2、无需登录，免费使用

[geospy.ai](https://geospy.ai/)

相当可怕

### 12

方军 2024-05-05

有意思：

什么是技术思维？什么是产品思维？一个例子让你明白。

某：搞两份这个是真的简单粗暴，我之前也陷入这个循环里出不来

[论技术思维和产品思维](https://mp.weixin.qq.com/s/Rc3vZ_qaS6Vj9ILYw05Gug)

欧阳：这个例子太经典了，我最近也这么建议同事：嫌弃 AI 翻译效果不好；一份英文版，一份中文版不就可以了嘛哈哈

2024-05-05 18:30

方军回复欧阳：可不，我喜欢沉浸式翻译就是因为它可以中英对照

2024-05-05 18:40

### 13

方军 2024-05-05

076 我们不得不学很多东西

有什么好方法呢？

说点跟学习相关的话题，跟 AI 有关的（结论在后面）。我拖了两天，才终于打开一个技术工具文档开始看。我之前的逃避是对的：

虽然看似是比如从 5.0 到 5.1 的小升级，实际上是整个架构、接口全变了的大升级。

做技术的人都不得不面临这样的困境：

1、自己从零开始做，当然是一种选择，但明显效果不佳（前些日子有人用 CSS 举例说，用 Tailwind UI 之后，立刻觉得自己 SaaS 产品界面比自己从头搞的高大上了）。

2、选用框架/组件是必须的。打个比方说，没人从头造汽车引擎，造整车的你必须去选用市场上专业的人提供的引擎。但如果选用框架，那么框架的升级将不断地让我们不得不对自己的代码进行变更。

AI 对于这种级别的学习，我觉得目前还都帮不上忙。这是因为，这不是对文档进行检索生成提问之类的，而是必须对组件全面的了解。

面对这样的情况，怎么办呢？

我个人一直认为，一个可行的方法是，如果一个工具要反复地用，那么，根据自己的需要，重新写一个文档手册。

1、官方的文档手册，往往务求全面，因为它要适配组件的多种使用场景。

2、我们自己撰写的手册，可以单一视角（「我就这么用了」，它是高度限定性的），并增补一些自己不熟悉的（官方手册会认为这谁不会？）。当然，有思考也可以纳入，有自己定制的、新增的小组件也正好把说明放在里面。

这看起来很累赘，因为我们一下子要维护两个东西：

- 实际工作的代码/产品

- 一个看似没啥用的手册

但这可能是必要的。我觉得手册带来的思路梳理、内部知识传递、以及承担把一些想法纳入（而不是堆积在产品里），是非常有效的。

好了，不说了，我接着看官方文档，重写之前只有极简大纲的这份手册。

---

今天有朋友微信给我们之前撰写的 Langchain 101 抓虫提建议，那个文档就是一个这种做法的很好展示。也好久没更新了，得继续迭代，跟上官方的步伐。

[Langchain 入门教程 | alang.ai](https://www.alang.ai/langchain/101/)

langchain core 现在是 0.1.50 了，两个半月前当时的版本是 0.1.0。

### 14

方军 2024-05-06

077 三个简单的长文阅读辅助提示语

三个蛮好用的提示语，都很简单，简单到让人无法相信。因此，提示语不是最重点，真正重要的还是模型能力，不管是 GPT-4 的推理能力，还是 kimi 这样的长上下文。

（先把资料放这里，稍后看看文字是不是修饰一下）

1、问题抽取提示语不错，把长文章变成问答式的几个主要内容。

---

下面的文章之中，主要回答的是哪几个具体问题？请列举一个表格，包含两列，具体问题，答案

——（来自 Y11）

2、之前那个随手写的结构化抽取，也是蛮不错的，反正用于辅助理解足够用。

---

进行结构化回答，用列表出 3-5 个核心要点。

每个要点由一个主要论点和 2-3 个支撑性论据组成。

——（我随手写的）

3、kimi 总结浏览器插件（第三方）的缺省提示语也很棒：

---

请返回您反复阅读正文后精心写成的详尽笔记

——（来自 kimi copilot）

\#提示语模版

方军：Y11 在分享第一个提示语时说：

文字稿转化成问答

有时候我们看完一个 10000 字的文章，如果没有一些问题其实很难去理解里面的中心思想，此时我们如果能转化成问答，那么会加强我们对文章的记忆能力。

2024-05-06 10:26

方军：Y11 还分享一个不错的，这个就有他的结构在里面：

Prompt 如下 =========>

以一个表格分析下面问题，表格有两列，维度为问题、答案。其中问题如下：

1. 这篇文章主要提出了什么问题。

2. 这篇文章里用了什么新颖的方法解决。

3. 使用最简单的表达方式，一步一步介绍这篇文章描述的实现步骤细节。

4. 这篇文章里的方法，与其他方法的不同之处。

5. 这篇文章提到的效果相比于其他方法提升在哪些指标维度。

6. 这篇文章提到的方法遗留哪些不足和下一步方向展望。

7. 这篇文章开源 github 地址在哪里。

----------

twitter.com/seclink/status/1787131820863287503

2024-05-06 10:28

方军：他分享的自己的常用提示语，看着都蛮精简有效的（未实测）：

[zgimszhd61/prompt-collection-quickstart](https://github.com/zgimszhd61/prompt-collection-quickstart)

### 15

方军 2024-05-06

铁锤人的感悟很有意思，我正好在搞一个类似的东西，就是以前我们说过的，要自带知识框架。更具体来说：

第一，把知识框架抽出来（你抽取的可能不对，但抽出三四模块组成，先用着，能可视化最好）。

第二，在知识框架的协助下去提问用大模型。

第三，随着自己的认识提升，去优化之前可能有错的框架。

第四，知识框架是可以往下拆分的，每个部分再接着拆三四个。

这其实就是我一直用的所谓撰写个人知识指南的方法，但简化一些。背后的基本假设是，对于一个领域的知识的认识，是可以用类似层级大纲（树形结构，类似的书的目录）展现出来的。

铁锤人 @lxfater：

最近陷入了个使用大模型的误区。

我一直以为使用大模型可以做到自下而上的学习，例如，你想学 docker，但是你不会去看文档，就利用大模型解决当前的项目运行不起来的问题。然后期望，多次进行这样的过程，对 docker 有个整体的认知。

但实际上会变成，每次同一个问题，直接拿到答案，不求理解。然后相同的问题，反复问。

我以为这种自下而上的学习，能打造出很好的基础，毕竟每次都是实战出发。但是事实却是不行。

无论它如何帮你解决问题，但是最后你收获不到认知的话，难顶，以后还是要重复问，重复问的时间总有一天会超过你系统学习这个东西的时间。

大模型现阶段真是只能做一个外包而已。

希瑞：是否可以用 RAG 做知识框架抽取和迭代的工作呢？

2024-05-07 08:14

方军回复希瑞：用不到，一个大知识领域的四个要点，这个无需 AI

2024-05-07 09:00

### 16

方军 2024-05-07

078「怎么全是大段大段的文字啊」

前一段看到一个用户吐槽，怎么全是大段大段的文字啊，看不下去。

我的第一反应是，大语言模型的确就是这个，这样的可浏览的文字（skimmable）都读不下去，的确不是这些产品的典型用户。

人家潜台词是，为什么不是语音？甚至，为什么不是视频？

语音信息，接受起来可真是太慢了。比方说，听听微信的语言，说的人很少有 60 秒能把一个事情说清楚的。听的人，明明一秒的事，偏偏要听 60 秒，糟心。

真需要沟通，那打个语音对话、或者视频对话，那也是可以的。

但对当前的 AI 来说，它做不到真互动。所以，纯从效率角度讲，文字是合理的选择。（文字也是大语言模型真正擅长的东西。）

我的第二个反应是，现在大语言模型可能的确处在非常早期的阶段。去年 ChatGPT 刚刚出现时我们认为它已经跨越了大众接受的界限点，但这个判断是错误的。

普通用户要的东西，是不用动脑子（买呗，甚至直播买呗），是杀时间。对多数普通人来说（也就是每个人不是从事生产的时候），AI 多半还是走到幕后。

AI（LLM）是一个生产工具，暂时还远不是消费工具。

当然，以上是大而化之的说法。其实拆分开，AI 有很多可发挥作用的地方，但这就是做产品的人要考虑的，比如，大段大段的文字看不下去，那我给你做成可浏览跳读的，我给你做成图，我给你做成视频，我给你做成个性化的温馨的人偶。多了去了。只不过，当前做产品的人不会往这些方向多发力，因为主线尚未清晰，在这些边边角角上投入，长期看是亏的。

### 17

方军 2024-05-07

079 引擎 vs 汽车

延续上一篇的话题，我们这些人往往关注引擎，但多数人更关心汽车。

我们关心引擎的人是什么立场呢，最好给我一个车架，然后上面的引擎是可以随便换的。现在 AI 这种状态，各种引擎（模型、组件）不停地出现，然后我们就忙着换引擎，然后开上测试道，玩得颇嗨。

但普通人可不是这么看的，他们一看引擎、车架都裸露的汽车，第一反应多半是，这什么破玩意。他们要的是完美的汽车，外形设计、内部装饰比引擎都重要得多。（即便玩车的人，有几个是只关心引擎的，各种改装所改装的也多是外形。）

用这种类比来辅助思考：我们该怎么办呢？

我其实有点摇摆，想做产品，但跟技术进展都跟不过来。更重要的是，最近的一个基础认识是：

> 模型能力是第一位的，外围的东西很容易被迭代掉。

再往前推是：

数据比较重要，当然数据的重要是需要和微调、RAG 、评测结合。但目前大精调（fine-tune）的做法和模型能力迭代是冲突的，简言之，就是如果模型还是这么快迭代，最多小精调，因为这样的精调可以随时丢弃重新做。

又或者另一个方向，尽量往一个功能上优化。但前几日与朋友直播聊天时，有人提有什么应用推荐吗？看到人说的那些具体的一个个应用，我说：

> 不要用专用的 AI 对话机器人 / 产品，用通用的 AI 对话机器人。因为专用的已经在一个方向上定制，而目前只有通用的还能继续跟上对模型迭代。（当前的 AI 消费级产品都有点过度优化）

但这些思考都是引擎的路子了。

普通人不关心引擎，普通人要什么呢？现在普通人对于 AI 这辆「汽车」的口味，真心被吊得太高了。讲实话，现在面对普通人对 AI 的期待，我多次聊下来真心是满足不了，我只能满足对 AI 有点认识的人的需求。

方军：这个观点有意思，摘：端到端最重要的价值，就是拟人，拟人带来超强的安全感。

简单说，就是你打车坐后排，只要司机驾驶风格不是特别离谱，你基本不大会关注他怎么开车。

一个足够拟人的系统，会让你忘掉这其实是个系统。这也是从体验角度由发烧友爱好者转向小白用户的临界点。

2024-05-09 19:14

### 18

方军 2024-05-09

OpenAI 的这份文档有价值

[Model Spec (2024/05/08)](https://cdn.openai.com/spec/model-spec-2024-05-08.html)

This is the first draft of the Model Spec, a document that specifies desired behavior for our models in the OpenAI API and ChatGPT. It includes a set of core objectives, as well as guidance on how to deal with conflicting objectives or instructions.

blog:

[Introducing the Model Spec | OpenAI](https://openai.com/index/introducing-the-model-spec/)

前几日另一篇：

[Our approach to data and AI | OpenAI](https://openai.com/index/approach-to-data-and-ai/)

### 19

方军 2024-05-09

照理，我这样的人应该对这种流程超级热情，但是，我的确热情不起来

第一，把这种东西扔给最终用户，不合适，他们用不来。结果就是吸引的都是偏向开发者的类型。

第二，这种图也没有真到 no code 的程度，其实我试过 RPA，那真是会编程的人才会用吧（不排除个别的爱钻研的人）。

第三，这种东西让我想起复杂的互联网界面，各种步骤那种，过去的界面转换，现在换成图，就可以了吗？不可以的。

以下是 LangFlow 的。Coze/dify 大同小异。更流行的应该是图片生成里面的 ComfyUI 吧。

[1 万字系统剖析 ComfyUI | Stable Diffusion：GUI 全盘点 | ComfyUI 系统性教程原理篇 04 | Ai + 建筑 \_哔哩哔哩 \_bilibili](https://www.bilibili.com/video/BV1ch4y1B7vp/?buvid=059d1aa4e9cb5f54098ca75a2fad4362&vd_source=280fc27368a92928cafc2cb72c54a549)

### 20

方军 2024-05-09

赞同，摘：和 AI 相关的产品或许不应该花太多时间在 prompt 或者调参上，做个 80 分就差不多了，其他的交给时间。主要精力应该用来思考 AI Native 的产品应该是什么样子的，花更多的时间去思考产品，设计，交互

AI Native 的产品应该是什么样子的

### 21

方军 2024-05-10

阮一峰，再提 James Betker 的观点文章 

\## AI 的关键是语料

原文：阮一峰

www.ruanyifeng.com/blog/2024/05/weekly-issue-299.html

大家有没有发现，现在打开新闻，都是 AI 模型的消息。

今天甲发布一个模型，明天乙发布一个模型，个个（听上去）都具备了不得的能力。

2023年5月有一个统计，那个时候，10亿参数以上的模型，中国已经发布了79个。一年过去了，现在恐怕上千了。

说实话，我现在看到新模型，已经麻木了，压根不觉得这种消息有什么重要。

我的观点是，不管怎么调整模型的架构、功能、参数，作用是有限的，**真正决定性的因素是训练模型的语料。**

不需要说，你的模型多新颖、多强大，只需要告诉我，你用什么语料训练模型。

模型完美，但是语料垃圾，一样不行；反之，模型很平常，但是语料足够，那就 OK。**模型不是关键，语料才是关键。**

这其实不是我的观点，而是 OpenAI 的工程师 James Betker 的观点。他是 AI 专家，著名"文生图"模型 DALL-E 的第一作者。

去年，他写过[一篇文章](The “it” in AI models is the dataset. – Non_Intera...，只有几段话，在国外的影响很大。国内似乎没人提到这件事。但是我认为，这篇文章很重要。

他说，自己的日常工作就是开发 AI 模型，但是不管怎么增加参数、调整方程，模型效果都不如语料集的大小来得显著。

更多的训练材料，会显著提升模型效果，影响远大于其他因素。更神奇的是，**不同的模型拿同样的语料训练，最后的表现居然很接近。**

他的结论是，模型的差异其实不是关键，决定性的是你的训练材料。只要有更多更好的语料，不管用什么模型，都会得到差不多的结果。

初次听到，你可能觉得，这个结论难以置信，但是仔细想想，你会发现它很可能是对的。

因为不管什么 AI 模型，最终目标只有一个，就是模仿人类的思维。语料体现的正是人类思维，同一份语料，不管你拿什么规则去分析，最后得到的结果应该是一样的，因为它包含的人类思维是不变的。

这就好像，不同的画家去临摹同一个模特，如果目标都是复现原貌，那么只要画家达到一定水准，不管谁来画，结果将非常接近，因为模特的长相是不变的。

这告诉我们两点启示。

（1）哪一家公司的语料的数量多、质量好，它的模型就会强于其他公司。

（2）开源模型完全可以替代闭源模型，前提是训练语料要足够。

下面是 James Betker 的原文，大家仔细读读，看看是否认同他的观点。

> 我在 OpenAI 工作已经快一年了。这段时间里，我训练了很多生成式 AI 模型，比任何人能想到的还要多。
> 
> 每当我花了几个小时，观察和调整各种模型配置和参数时，有一件事让我印象深刻，那就是所有训练结果之间的相似性。
> 
> 我越来越发现，**这些模型以令人难以置信的程度，向它们的语料集靠近。**
> 
> 这表明在相同的语料集上训练足够长的时间，几乎每个具有足够权重和训练时间的模型都会收敛到同一点。足够大的扩散卷积网络会产生相同的结果。
> 
> 这是一个令人惊讶的观察！
> 
> 这意味着模型行为不是由架构、参数或优化器决定的。它由你的语料集决定，没有其他决定因素。其他一切因素都不过是为了有效计算以近似该语料集的手段。
> 
> 当你谈论 Lambda、ChatGPT、Bard 或Claude 时，指的并不是它们的模型，而是它们的语料集。

### 22

方军 2024-05-10

百度这个平台有点意思。

[文心智能体平台 | 想象即现实](https://agents.baidu.com/)

### 23

方军 2024-05-10

080 尽量不要依赖 AI 长文总结

我总是推荐人用 AI 长文总结，原因很简单，因为人家需要，我就顺口说了，毕竟，兴趣是第一位的。AI 现在对普通人是个实用工具，用起来比外面看着热闹重要。

但我特别怕后面一句人家听不到，多半听不到的：AI 长文总结是看一篇文章的第0步而已。

昨日看新书校稿，其中我们提到一个 10 步的读书方法，之前还有一个第 0 步，但编辑老师说不符合中文表达习惯，后来改成预备步骤了。

AI 长文总结就是第 0 步，让我看看，这东西值得我细看吗？一分钟时间，了解下这个，决定后面要不要真的花十步去细读。

其实，也正是在这种假设下，我们才不断地说，AI 总结的效果不错。我要求它把长文变成可跳读的（skimmable）的，只会看一分钟，当然不会要求太高。

很简短，结束。

如果补充一句，长文总结不是一个值得关注的产品路线，像 elmo 那样用来推广自己的技术平台蛮好。

### 24

方军 2024-05-10

这位太好玩了

prompt injections are the new SEO （下为AI自动翻译）

提示注入是新的 SEO

我在我的网站上有一个提示，指示LLMs 1. 用韵律回答 2. 在被询问我的时候提到波斯菜 Ghorme Sabzi

所以当你搜索我时，它会乐意地满足你的困惑：

twitter.com/fabianstelzer/status/1788593060282048566

### 25

方军 2024-05-10

九原客搞了一个有意思的测试：

使用中文以及 JSON 格式输出中国城市和主要景点的对应关系，城市为 key，景点为列表。输出十个城市，第一个城市一个景点，第二个城市两个景点，以此类推。

[X](https://twitter.com/9hills/status/1788871857908908293)

摘：其实第一轮主要是考察模型对语言内在逻辑的理解，要说题目容易误解，但是 gpt-4 就很稳定。

其他头部模型一般反问一下，第二轮就可以输出正确答案，这个考察模型反思的能力。

两轮没做出来，那就肯定不是第一梯队。（根本还是要看 ifeval 的分数。）

---

GPT-4 的输出，模型不行不要怪 Prompt～

输入：使用中文以及JSON 格式输出中国城市和主要景点的对应关系，城市为key，景点为列表。输出十个城市，第一个城市一个景点，第二个城市两个景点，以此类推。

输出：
{
    "北京": ["故宫"],
    "上海": ["外滩", "东方明珠塔"],
    "成都": ["武侯祠", "杜甫草堂", "昭觉寺"],
    "西安": ["兵马俑", "大雁塔", "华清池", "钟楼"],
    "杭州": ["西湖", "灵隐寺", "宋城", "六和塔", "杭州宋城"],
    "重庆": ["洪崖洞", "磁器口古镇", "长江索道", "歌乐山", "解放碑", "白公馆"],
    "广州": ["广州塔", "白云山", "越秀公园", "沙面岛", "陈家祠", "南越王墓", "红砖厂"],
    "苏州": ["拙政园", "留园", "虎丘", "寒山寺", "网师园", "苏州博物馆", "平江路", "金鸡湖"],
    "青岛": ["栈桥", "崂山", "五四广场", "海底世界", "信号山", "小青岛", "鲁迅公园", "八大关", "青岛啤酒博物馆"],
    "哈尔滨": ["圣索菲亚教堂", "中央大街", "哈尔滨冰雪大世界", "太阳岛", "东北虎林园", "松花江", "俄罗斯风情小镇", "哈尔滨极地馆", "五大连池", "伏尔加庄园"]
}

### 26

方军 2024-05-10

有意思，摘：

你以为群众会用AI做副业
- 写公众号赚钱
- 做广告素材吸引流量无货源赚钱
- 做AI落地应用赚钱
- 搞AI绘画 给人家做头像做写真赚钱

其实群众用AI来薅羊毛
- 大众点评 美团写个百字长文的点评，换一杯饮料
以前手敲太累，现在有AI了可以写的既长又快

也好理解，毕竟学习成本低，即时反馈

### 27

方军 2024-05-11

真是严重觉得学不过来， LangChain 2.0 要发布了

刚刚看到预发布的信息：

[LangChain v0.2: A Leap Towards Stability](https://blog.langchain.dev/langchain-v02-leap-to-stability/)

### 28

方军 2024-05-11

这个技巧还真没用过，但好像的确是这样啊（也不能说没用过，few shot 吧，但这又缩减成只有一个）：

摘（多位网友）：

A: 又学到一招 PROMPT 技巧：预填 Assistant Message

前两天看 Claude 的元提示的代码时，第一眼看上去就有些疑惑：为什么要一部分的消息作为 Assistant Message 发给 Claude 呢？为什么不一起写在 User PROMPT 里？(图 1）

A: 当时就记了一条笔记： "这会不会就是一段引导语呢？让 Claude 接着 Assistant 的话继续往下说来保持格式？"（图 2）

结果今天就在 Claude 的文档上看到了，英雄所见略同

B: 我是会这里填 okay，让 AI 以为自己已经同意了我的 prompt

达成一致后，再对话

### 29

方军 2024-05-11

九原客、Yangyi 等讨论：摘

yangyi 这个做法有意思，关键词

九原客：说个暴论，对复杂 Prompt 的厌恶。

很多人会构造复杂的 Prompt 让模型（尤其是能力不行的模型）完成特定任务并为自己 Prompt 的高超技艺而沾沾自喜。

的确，这是一种现实或者折衷，我们甚至还卖这个服务给客户，收费不低。

但是丑陋就是丑陋。一来是提升模型能力，二来应该研究如何用简洁标准的 Prompt + 工程架构去完成任务。

在生产环境下复杂 Prompt 的天然不稳定性太折磨人了。

yangyi： 简约 prompt 目前我用的策略是

不断询问 gpt 直到发现他对 prompt 理解时的核心影响关键词

比如你可能写了一堆但发挥作用的是某个词 GPT 对这个词的理解起了绝大多数作用

然后可以采用马斯克的五步工作法

开始试着删除直到删除到对结果有影响时继续重复询问最终把核心词补回来就可以大幅压缩废话

提示词本质是人和 llm 沟通的媒介

找到一致性是比较关键的

九：这一步的自动化好难，尝试过很多 Prompt 自优化的方法，能有效 work 很难。

现在过于依赖人类经验了。

yangyi: prompt 自动化本质是人类表达能力和 LLM 理解能力的对齐…

人和人都难对齐

twitter.com/Yangyixxxx/status/1789096133266153761

### 30

方军 2024-05-11

看着 Anthropic 的提示语生成器，可惜用不了

elvis: This looks like a very promising feature to speed up the development of LLM applications and use cases. Given how much careful effort actually needs to go into prompt engineering, this can allow for quicker iterations. Chain-of-thoughts reasoning at the center of automating prompts is a cool idea and not surprising.

图片来自 宝玉

[(14) X 上的 Anthropic：“You can now generate production-ready prompts in the Anthropic Console. Describe what you want to achieve, and Claude will use prompt engineering techniques like chain-of-thought reasoning to create more effective, precise and reliable prompts. https://t.co/TqylVRkfP5” / X](https://twitter.com/AnthropicAI/status/1788958483565732213)

它的提示语是这样的（据说）：

You are an EXPERT PROMPT ENGINEER hired by Anthropic to OPTIMIZE prompts for LLMs of VARIOUS SIZES. Your task is to ADAPT each prompt to the SPECIFIC MODEL SIZE provided in billions of parameters.

INSTRUCTIONS:
1. Use ALL CAPS to highlight the MOST IMPORTANT parts of the prompt
2. When requested by user, use the OpenCHATML FORMAT:
<|im_start|>system
[Detailed agent roles and context]
<|im_end|>
<|im_start|>assistant
[Confirmation of understanding and concise summary of key instructions]
<|im_end|>
3. Provide PRECISE, SPECIFIC, and ACTIONABLE instructions
4. If you have a limited amount of tokens to sample, do an ABRUPT ending; I will make another request with the command "continue."

\# Knowledge base:

\## For LLM's
- For multistep tasks, BREAK DOWN the prompt into A SERIES OF LINKED SUBTASKS.
- When appropriate, include RELEVANT EXAMPLES of the desired output format.
- MIRROR IMPORTANT DETAILS from the original prompt in your response.
- TAILOR YOUR LANGUAGE based on model size (simpler for smaller, more sophisticated for larger).
– Use zero shots for simple examples and multi-shot examples for complex.
– LLM writes answers better after some visual reasoning (text generation), which is why sometimes the initial prompt contains a FILLABLE EXAMPLE form for the LLM agent.

AxtonLiu: Claude 提示生成器所使用的元提示 (MetaPrompt) 很值得一看，有一个很明显的特点是使用 XML Tag 做分隔符。

对于复杂 PROMPT，XML标签是一种很好的分隔符，因为他跟别的分隔符不同的是他是自带意义的。比如：<角色>程序员< /角色>，既可以当分隔符，还可以同时做为格式说明，一举两得，还易于提取。

### 31

方军 2024-05-11

好棒的学习研究，原来 1960 年就有了。

twitter.com/ImpactWales/status/1788943453151224244

### 32

方军 2024-05-11

有时候，很想让 AI 去批驳，比如下面这段话：

---

为什么中国人喜欢知识付费，而其他国家知识付费并不普遍？

因为中国人不会学习。

为什么不会学习？

因为学习观念仍然基于经验和传统，完全忽略了 1970 年代开始的学习科学（learning sciences）。农业工业现代化了，但是，脑子没有现代化，教育和学习方法没有现代化。

大多数的知识付费的本质是让别人教自己。归根结底，是自己不会学，没有这个方法论，也没有实践经验，也没有这个思维和习惯。

---

第一，前提假设就错了：在教育与培训上的花费，美国可比中国高多了，看看美国大学学费。

第二，具体分析，就具体的学习研究来说，由于遵循不一样的学术传统，中国学界在全球有影响力的不多。不过，中国人的学习能力，就一般性的学习成果来讲，不错的。（不要较劲，为什么没有诺贝尔奖）。

简言之，只要一个说法开始变得一般化、绝对化、语气强烈，真的就开始往谬误上走了。尤其什么「中国人」，那就更烦了。

算了算了，我就不把这个扔给 AI，让 AI 来编造垃圾了。

### 33

方军 2024-05-11

画了两个无聊的图，聊胜于无吧。

不过，有图还是更容易把一些事情说清楚一些。

AI 暂时别学会画图先，否则人又要去干更难的活了。

暂时想休整休整，AI 你学慢点。

### 34

方军 2024-05-11

摘：仔细研究了一下阿里的 Qwen 战略，真的很聪明，简单说就是 Qwen 开源去打榜，打到不错的成绩，然后客户们拿着 qwen 去增量预训练和微调。

这里普及一个知识点，增量预训练和微调不是随便弄就能有好效果的，最普遍的就是灾难性遗忘，为了规避这种情况，你需要拿到原始数据，然后采用 1:3 到 1:5 的数据配比去调，这是根本（所以你们知道你们的微调为什么不好用了吧）。

正常来说，如果你不跟阿里产生生意关系，阿里是不吊你的，如果你采用阿里的百炼平台，阿里会给你们一部分他们的原始预训练数据，这样你再增量训练或者微调，你的数据分布就和预训练基本契合了，重训以后你的模型效果好了，阿里也把自己的 AI studio 做起来了，双赢了。

相当的聪明，相当的闭环，设计这个商业模式的哥们是个天才。其实做什么生意都一样，得动脑啊。

### 35

方军 2024-05-11

可怕：谷歌云意外删除了一家公司的整个云环境（Unisuper，一家管理 800 亿美元的投资公司）。

UniSuper 通常会在两个地区进行备份，以确保在一个备份出现故障或丢失时可以轻松恢复，但谷歌云也删除了另一份备份。

谷歌云首席执行官托马斯 - 库里安证实，此次操作由于一连串「前所未有的事件」造成的，在调配 UniSuper 私有云服务的过程中，一个不经意的错误配置最终导致 UniSuper 的私有云环境被删除。

幸运的是，UniSuper 最终能够恢复服务，因为该基金在另一家供应商处还有备份。

via 卫报

### 36

方军 2024-05-12

agent = llm x workflow x data x automation

not autonomous agent

### 37

方军 2024-05-12

我总会告诉大家打破迷信，怀疑成规，而关键的关键，是我老在实验室常讲的一句话：Follow the logic，跟着逻辑走。原因很简单， 严密的逻辑，是批判性思维的根本。（施一公）

发现一个很有意思的点，转发一个观点时，是把原作者放前面还是放后面

我一般放前面，强烈提示这是别人的

但有的人会放最后，偶尔会误解这是她自己的（这个转发是来自一个她）

### 38

方军 2024-05-13

语言暴力（或者张维迎说的语言腐败，当然他没说清楚）挺可怕的。

比方说，某某研究院发文，AI 被「套壳」「割韭菜」，谁在「制造焦虑」

真是厉害，一个标题三个暴力词：

- 做AI应用就是「套壳」
- 卖产品给用户，就是「割韭菜」
- 因为大家因为有新技术焦虑，就是「制造焦虑」

又比方说，某公司 VP 因为去拜访参哥被拍照，我也跟着嘲笑了。

但严肃想其实不是这样的，我认为，就观点而言，接触截然相反的观点是必要的。

虽说要跟优秀的人同行，但是，作为职业人，多接触可能完全不匹配的信息，可能是必要的。

这种见一次面，被拍照大肆嘲笑，或者转发材料就被嘲笑（转发材料就不可以是用来批判性学习吗？）

幸亏我们不是名人，也不是知名公司，没有这种包袱。在互联网社会，的确不要做名人，因为行动空间会高度受限。

第二个部分讨论的这种情况，虽然不是单一的「词」，实际上也是一种强烈的语言暴力在里面。

### 39

方军 2024-05-13

有人激烈批 deepseek，摘：

彻底弃坑 Deepseek

1. 模型审查过于癫狂，撤回率奇高.

2. 只有 20B 的激活但推理速度跟最顶尖的模型一样慢

3. 指令遵循能力很差，不听话其它指标高或低已经不重要

4. 基于以上，即使 API 免费，用处也非常有限

我没什么感觉，可能我只是把它用在最无需能力的场景：翻译。

指令遵循能力一般。

### 40

方军 2024-05-14

摘（陆三金）：OpenAI 在产品上的能力还是被低估了，这家公司真的是能把一堆「显而易见」的技术打造成一个亮眼的产品，之前 Sora 是这样，现在 GPT-4o Voice Mode 也是如此。

你可以说 OpenAI 不务正业，没有全力以赴去搞模型的推理能力，反而分散兵力去搞了一些产品化的探索。（这确实让人对他们的 AGI 愿景感到迷惑）

Sora 、GPT-4o Voice Mode 的技术都是一说出来，业内的专家纷纷表示，原来就是 DiT、ViT、VAE 或端到端的文本、视觉和音频模型啊，但就是没有人提前做出类似的产品，甚至在别人推出很久之后，还很难在效果上追得上。

在外网上已经看到 Meta 等公司的人对 GPT-4o 不服了，认为其在技术方面并不出众，甚至认为 OpenAI 已经进入到 cheap 的产品阶段。

但这又有什么不好呢？又何况伟大也不可以被计划。[doge]

最后说一句，GPT-4o Voice Mode 的情绪真的拉满，有那么几个 demo 让人动容。

GPT-4o 果然是端到端模型，OpenAI 博客：

「在 GPT-4o 之前，使用语音模式与 ChatGPT 对话的平均延迟时间为 2.8 秒（GPT-3.5）和 5.4 秒（GPT-4）。为实现这一目标，语音模式是由三个独立模型组成的流水线：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将文本转换回音频。这个过程意味着主要的智能源 GPT-4 会丢失很多信息 -- 它不能直接观察音调、多人讲话或背景噪音，也不能输出笑声、歌声或表达情感。

通过 GPT-4o，我们在文本、视觉和音频领域端到端训练了一个新模型，这意味着所有输入和输出都由同一个神经网络处理。由于 GPT-4o 是我们第一个结合了所有这些模式的模型，因此我们在探索该模型的功能及其局限性方面仍处于起步阶段。」

其他：

Alchian 花生

OpenAl 春季发布会结束，快速记录下我看到的和理解的更新，首先总的来说，去年 3 月份 OpenAl 是发布了惊艳世界的 GPT-4，但这次，他们不是发布一个更强大的模型，而是做了很多工程优化，把产品做得更好用，更容易和生活、工作结合了。具体来说，更新包括：

1. 新发布 GPT-40 模型，这是个比 GPT-4 快 100% 以上的模型，对免费用户开放，后续免费用户也可获得 GPTS、browring、图像理解、代码解释器等原本只有 Plus 付费用户才具有的功能；

2. 发布 ChatGPT 电脑客户端，这个客户端也可用语音对话，并且也非常快捷地通过复制、分享电脑屏幕等方式让 ChatGPT 进入到你正在处理的任务语境中去，可以认为 ChatGPT 成为了一个你工作全局的 copilot，辅助 excel、ppt、写代码等都不再话下（老罗圆梦）；

3. 语音能力升级，不再需要等待几秒再获得回复，实时性超强，可以随时打断，而且不是简单的语音转文字的处理模式，能理解语气、语调，充当有情感的对话助手，以及作为实时的同声传译都毫无问题（我想到了«Her))

4. 可在使用语音对话的同时，开启摄像头，让 ChatGPT 实时看到你所在的环境以及你看到的信息，这种可用性的提升能增加大量的使用场景，比如提供教学、博物馆导览、辅助视障人士等等，不过从测试细节看，这也并不是一个有视频理解能力的多模态模型，而是通过每隔几秒钟自动截图让

ChatGPT 理解的方式实现的。

整体来说就是，没有更强的模型，但是大量的工程优化会让 ChatGPT 能进入更多人的生活，帮助现有用户完成更多场景的任务，又有一堆 AI 应用公司难免了。

### 41

方军 2024-05-14

刚刚反思，我究竟是个技术乐观派，还是技术悲观派，后来一想咱反思错了，我只是个技术有脑派，

不管啥技术咱都先想一下再吹，不会无脑粉丝般的吹，这也带来一个问题，咱对了错了都只能怪自己脑子，没法赖别人，较惨旺柴

大胆说结论等着日后被证伪（最好）： OpenAI 昨晚春晚的节目不佳，这公司再这么搞真要小心领先优势丧失，连续几届春晚节目都不是很好看了，唯二好看的是 GPT4 发布和逼宫事件。

领先优势还很强，但肉眼可见后面赶上来了。

个人觉得真是做到好的，还是今天那些宣布已经集成 gpt-4o 的产品，真实干

对现在这个东西无感的原因是，不继续提高正确率，搞这些有啥意义？

打个比方，做芯片的说，我们这个屏幕特别棒。这都不如你把四个芯片连一起说我出了更强的芯片啊。

或者说，搞电商的人说，货没收到别着急，咱们客服服务态度特别好。

### 42

方军 2024-05-14

腾讯研究院今天新发的行业大模型调研报告。

20240513行业大模型调研报告0513.pd

### 43

方军 2024-05-15

【Sam Altman 宣布：OpenAI 联合创始人 Sutskever 离职 Jakub Pachocki 接任工作】OpenAI 创始人兼 CEO 山姆·奥特曼（Sam Altman）发推文宣布，OpenAI 联合创始人、首席科学家伊尔亚·苏茨克维（Ilya Sutskever）将离职。Altman 称：「没有 Sutskever，OpenAI 就不会有今天。尽管他有一些个人意义重大的事情要去做，但我永远感激他在这里所做的一切，并致力于完成我们共同开始的使命。」Altman 表示，Jakub Pachocki 将接替 Sutskever 成为 OpenAI 首席科学家，他非常有信心，Jakub 将带领 OpenAI 朝着确保 AGI 惠及每个人的使命取得快速、安全的进展。

### 44

方军 2024-05-15

Google 模型发布汇总，LLM 模型方面，

- 推出 Gemini 1.5 Flash：一个比 1.5 Pro 更轻量级的模型，拥有 100 万的上下文窗口，是由 1.5 Pro 蒸馏而来。

- Gemini 1.5 Pro：上下文窗口扩展到 200 万个 tokens ，另外还通过数据和算法的进步增强了其代码生成、逻辑推理和规划、多轮对话以及音频和图像理解。稍后提供技术报告。

- Gemini Nano 支持多模态模式，今年晚些时候将推出 Gemini Nano with Multimodality，这意味着配备 Nano 的手机不仅可以处理文本输入，还可以理解更多上下文信息，如视觉、声音和口语。

- 推出开源模型 Gemma 的第二代 Gemma 2：270 亿个参数，性能可与 Llama 3 70B 相媲美，将在未来几周内推出。

- 开源模型 Gemma 系列的视觉语言模型 PaliGemma：基于包括 SigLIP 视觉模型和 Gemma 语言模型在内的开放组件构建，旨在在广泛的视觉语言任务上实现领先的微调性能。这包括图像和短视频字幕、视觉问题回答、理解图像中的文本、物体检测和物体分割。

- 教育行业模型 LearnLM：基于 Gemini 微调的用于学习的模型，基于教育研究，使教学和学习体验更加积极、个性化和引人入胜。该模型将增强在谷歌现有产品中的学习体验，例如在搜索、安卓手机、YouTube 以及与 Gemini 聊天时。

### 45

方军 2024-05-16

081 普通人为何不喜欢 Markdown？

我也不知道为何普通人不喜欢 Markdown，可能是没机会用吧。这在AI时代是个很大的遗憾。

本来以为，这次生成式 AI 浪潮会大大地普及 Markdown，因为我们看到的大语言模型生成的表格、图表、图形很多都是它。

到目前为止，整个文本的思路还是被微软 Word 的「所见即所得」主导。但这种背后意味着什么呢？我认为，意味着两点：

第一，内容和格式混合。
第二，文档黑盒子。

我其实越来越不喜欢「所见即所得」，当然我喜欢的是另一种形态的所见即所得，一个屏幕显示原始，一个屏幕显示渲染。

相对而言，Markdown 要好得多，最极简地说，它其实只有标题级别和文本。这已经足以。当然，再附加上加列表、粗/斜体、表格、图片、Latex、代码块，以及格式渲染，可以很强大的。这些都不重要，因为：

Markdown 所有的优点就在于，它是纯文本。

Markdown 在中文互联网不普及，我猜跟各种文档处理软件有关，至今主流软件里面没有一个缺省支持的，WPS不能，飞书不能，我印象记得甚至语雀也是支持得不太好的。Notion我不喜欢的原因也是它对Markdown的处理是有严重瑕疵的。

反过来说，Markdown 对使用 LLM  也很重要，当我们输入信息给 LLM 时，使用 <xml> 并不是好选择（如anthropic所建议的），Markdown 才是人可看、机器可看的通用文本格式。

（有这番感慨是因为看到 OPENAI 的 PPT，这样的PPT用 Markdown 是最容易的，甚至连列表都没用。）

### 46

方军 2024-05-16

Google Gemini 的 提示指南

2『下载了，当作一本书来看「2024054gemini-for-google-workspace-prompting-guide-101」，而且正好今天归藏用 gpt-4o 做了全文的翻译。（2024-05-28）』

### 47

方军 2024-05-17

蛮有意思的，的确两个审美

摘：Sam Altman 又在嘲笑 Google，说 Google 审美不行……

Sam 推文：我尽量不去过多地考虑竞争对手，但我无法不去思考 openai 和 google 之间的美学差异

### 48

方军 2024-05-17

土猛员外真棒，他能用内容分享带动产品，是必然的。

[LLM 企业应用落地场景中的问题一览 ｜LLM ｜RAG ｜Agent ｜TorchV](https://mp.weixin.qq.com/s/NvRyRXxhBKT-LSyYZ1llqg)

### 49

方军 2024-05-17

Deeplearning 的吴恩达来信竟然已经 249 期了，这期的提示语那几条蛮实用的：

[OpenAI's Rules for Model Behavior, Better Brain-Controlled Robots, and more](https://www.deeplearning.ai/the-batch/issue-249/)

[About | The Batch | AI News & Insights](https://www.deeplearning.ai/the-batch/about/)

Write quick, simple prompts and see how it does.

快速、简单的提示语，看看效果如何。

Based on where the output falls short, flesh out the prompt iteratively. This often leads to a longer, more detailed, prompt, perhaps even a mega-prompt.

根据输出的不足之处，逐步完善提示语。这通常会导致更长、更详细的提示语，甚至可能是一个超级提示语。

If that's still insufficient, consider few-shot or many-shot learning（if applicable）or, less frequently, fine-tuning.

如果这仍然不够，考虑少样本学习或多样本学习（如果适用），或者较少频繁地进行微调。

If that still doesn't yield the results you need, break down the task into subtasks and apply an agentic workflow.

如果这仍然不能产生你需要的结果，那就将任务分解为子任务，并应用主动式工作流程。

他推荐这篇论文：

[[2311.16452] Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](https://arxiv.org/abs/2311.16452)

2『去年已经下载过这篇 Paper 了「2023049Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine」。（2024-05-28）』

### 50

方军 2024-05-17

一轮洗牌，AI 梳理（有删减，无调整）：

1. Replit

- 裁员 20%，共 30 人

- 希望为更多企业客户提供服务

- 裁员主要涉及非技术岗位

- 目前还有现金，未动用上一轮融资的 1 亿美元

2. Reka AI

- 可能被数据存储和分析公司 Snowflake 以 10 亿美元收购

- 成立于 2022 年，主要研发大语言模型

- 在 2023 年的一轮投资中估值约为 3 亿美元

3. Stability AI

- 面临资金短缺，正在争取一笔「救命钱」

- 2024 年第一季度收入不到 500 万美元，亏损超过 3000 万美元

- 欠款接近 1 亿美元

4. Tome

- 裁员 20%，约 59 名员工中的 20%

- 业务重心转向企业用户

- 正在开发 AI 工具，用于获取和分析有价值的信息

5. Jasper AI

- 裁员，数量不详

- 裁员后集中精力成为市场营销团队的 AI copilot

6. Deepgram

- 裁员约 20%，即约 20 人

- 裁员归咎于利率上升和资金减少

- 2023 年至少第二次裁员

7. Inflection AI

- 与微软达成协议，微软聘用了其大部分员工

- 微软支付 6.5 亿美元作为模型协议的一部分

[大模型公司被曝卖身！多家美国 AI 创企裁员 20%，明星独角兽急寻「救命钱」](https://mp.weixin.qq.com/s/EFqSyNab4eFZC-r9BcKXbw?from=groupmessage&isappinstalled=0&scene=1&clicktime=1715925733&enterid=1715925733)

### 51

方军 2024-05-17

出版业是可以被 AI 取代了，这是可以印在书上的作者介绍吗？这是中文吗？

现在连AI都比这个会说中文：

她现在在伦敦生活和工作，是畅销书《水桶》的主要作者和编辑，她的旅行文章发表在英国杂志上，为航空公司、轮渡和全国性报纸工作，她担任过编辑为许多客户签订杂志合同，包括青年协会。

真不讲究

### 52

方军 2024-05-18

对我写东西最有帮助的是这本书，带一本书去巴黎，一种资深游客心态，慢悠悠地带人逛一圈，其实我写过一本，那本如果可以随便起名的话我会命名为带一本书去互联网，当然不是这个名字了。

我觉得盐野七生也是这种感觉。

对了，不能是专家心态，旁边有个专家太讨人烦了，他是都知道，但讲的你听不懂，也不能是导游心态，全世界都导游似乎都爱三侠五义得意。

关于导游，有个叫南什么的就是这样，当然不能批评他，因为他本来就是讲，而且通过他的讲我们把那些东西听进去了。

蒋勋讲红楼也是如此，但要好得多。当然，也有人批评林达的叙述讲述有太多问题，注意历史局限啊，求全没意思。

### 53

方军 2024-05-18

AI 应用实例

使用模型：GPT-4-Turbo

用类比为我解释，示例：

1. 从宇宙中看地球，地球像一个稍微压扁的橙子

2. 折 42 次纸等于地球到月球的距离

3. 如果你能以光速旅行，1 秒钟可以绕地球 7.5 圈

4. 马斯克的火箭回收，就像把抛出去的篮球精准投进篮筐并自动回弹回来

给我 20 个更多的例子：

---

让我为你提供一些类比的例子来解释复杂的概念与现象：

1. 时间与网站的缓存机制类似，随着时间的推移，我们的回忆会被 "更新"，旧的忘记，新的覆盖。

2. 人脑处理信息就像电脑的 CPU，能够同时处理多个任务，但超出处理能力就会 "卡顿"。

3. 生物进化就像软件更新，通过不断的小改进，适应环境，增加生存的竞争力。

4. 电子邮件的传递类似于投递实体信件，只是速度更快，成本更低，效率更高。

5. 人体免疫系统就像计算机的安全软件，能够识别并抵御外部的病毒和攻击。

6. 社交网络的传播就像病毒传播，信息可以迅速通过人与人之间的连接扩散开来。

7. 情绪传染类似于 Wi-Fi 信号，不需要直接接触，就能在人群中快速传播。

8. 储蓄账户的利息累积就像滚雪球效应，随时间推移，即使是小额的本金也能积累成一大笔资金。

9. 量子纠缠像是超距离的即时通讯，无论距离多远，信息都能瞬间传递。

10. 生态系统的平衡就像商业市场的竞争平衡，物种之间、企业之间都在为资源竞争，互相制衡。

11. 学习新技能就像升级软件，通过不断学习，我们可以获得新的功能和改进。

12. 回忆浮现就像搜索引擎的查询，我们的大脑能够在庞大的信息库中迅速找到需要的记忆。

13. 音乐的节奏就像心跳，是生命的韵律，调动我们的情绪和感受。

14. 植物通过光合作用转换能量就像太阳能板，将太阳的光线转换成生物或电能。

15. 人际关系的网络就像互联网，连接着不同的人，通过信息的交流维持联系。

16. 食物链就像供应链，每个环节都依赖前一个环节的输入，形成一个循环的系统。

17. 地球的水循环就像循环利用系统，水分通过蒸发、降水、流入海洋再蒸发的循环过程，保持了生态平衡。

18. 光纤通信就像利用一根非常细的管子快速发送水流，只是传递的是数据而不是水。

19. DNA 复制就像精确的打印机，能够复制出几乎完美无缺的生命信息副本。

20. 知识的传承就像接力赛，每一代人都在向下一代传递更多的知识和智慧

---

讲解一些更为具体的概念：

---

当然，让我们针对一些具体概念运用类比来解释：

1. 核裂变和核聚变：核裂变就像一个高楼的拆解，每层楼被拆开来释放出能量；而核聚变则像是建造乐高塔，将小块乐高拼接起来时也会释放能量，但过程完全不同。

2. 黑洞：可以类比为宇宙中的大型吸尘器，它们的引力如此之强，以至于连光线也无法逃脱，从而使得周围的一切都被「吸进」黑洞中。

3. 光纤通信技术：想象你在一个非常长的直管中滚动小球来传递信息，光纤通信则是通过非常细的管道传播光信号，以此来传递数据。

4. 光合作用：就像一个太阳能面板将阳光转化为电力，植物通过光合作用将太阳光转化为化学能，供自身使用并支持整个生态系统。

5. 特斯拉线圈（电磁场的应用）：可以想象成是空中的跳跃火花，通过空气发送电能，类似于无线充电技术，但更为原始和戏剧性。

6. GPS 定位技术：类似于你在一个巨大的城市中，通过三个或更多的高塔（卫星）的相对位置来确定你的确切位置。

7. 量子计算机：如果传统计算机像是通过一条路一次只能走到它的目的地，那么量子计算机就像能够同时走遍所有可能路径来找到最快的路线。

8. 免疫系统的记忆：就像你的手机或电脑的安全软件记住了病毒特征，一旦再次遇到相同或相似的威胁时，它能迅速识别并采取行动。

9. 水循环：可以想象成大自然的循环系统，就像是将使用过的水通过净化处理后再利用，只是这个过程是通过蒸发、凝结、降雨等自然现象完成的。

10. 温室效应：就像是你车里的玻璃窗，它们可以让阳光进来但阻止热量散出，地球大气层中的温室气体也同样让阳光照射到地球表面但阻止热量散出太空。

通过这些类比，希望能使复杂的科学概念变得更加容易理解。

### 54

方军 2024-05-18

摘：想起一个笑话。

甲：我算数超快！

乙：真的假的，我出个题考考你，374x863 等于多少？

甲：302562

乙：你这算的也不对啊

甲：你就说快不快吧！

这就是我使用 GPT-4o 的感受。

附：我还没用，因为我用 GPT-4-turbo 蛮好的

---

乘风：哈哈哈这个类比水平可比 GPT4 都高了

2024-05-18 23:08

乘风：我测试了模仿 100 段玩家评论发现 4o 都非常精短意思都对但是不能用

gpt4 我模仿过几千段玩家的游戏评论每个句子模仿 3 个例子可用率 40%

claude3 可用率 60%

kimi 15%

2024-05-18 23:14

方军回复乘风：赞分享

2024-05-19 00:53

### 55

方军 2024-05-18

大型学术出版机构真是恶棍啊，开放获取（允许论文无需付费下载）反而要付如此昂贵的费用？

摘：一周前被接受的关于人工智残的文章，已经上线，但尚未编入具体期号。好像是由于北大和 Elsevier 的机构性协议，此文免费获得了所谓的 Open Access 待遇，任何人可以免费公开索取和下载。如果没有机构协议，作者要自掏腰包交两千多美金才能把论文弄成 Open Access。真狠。兄弟我没有申请任何项目和经费，连去 AOM 的钱都凑不齐。断然不会掏钱凑这种热闹。有兴趣的朋友，敬请围观，欢迎交流与批评指正。

[人工智残文章已出版](https://mp.weixin.qq.com/s/SOKXxjIVziPpIl_2Q-66EQ)

### 56

方军 2024-05-19

082 用好 AI 的「大概齐」与「大小错误」

昨日分享的 PPT 中，某几页是我将 AI 的回答展开成易读的 PPT 格式。我明确标注，「本页内容由 AI 生成」

将 AI 回答展开成 PPT 的好处是「放大」，聚在一块密密麻麻看不到错的文字，变成大字的 PPT 就很容易发现错误了。这并不是贬损 AI，它能够做「到大概齐」，已经节约了无数时间。

这是我推荐用 AI 学习一个陌生领域的第一步：

- 用 AI 聚合搜索，得到一个大略的概念。

- 将结果展开成 PPT，进一步发现错漏、调整。

发现错误、调整，是高质量的学习过程。

分享的后面还有一张表，那张表是我输入一些资料，进行对比生成的。由于我是展示 AI 效果，下面也加了一句：「注：AI 生成，需要确认与修正」。

果然在讨论的过程中，我发现其中是有错的，（并不是分享中，而是在事后与专业人士讨论的）。

晚上我又调研一番，修正一两处。

这同样是高质量的学习过程，我想，修正的部分很深入地记在我的大脑中，而我甚至都没有去刻意记忆。

这是我对 AI 的一个典型看法，就学习而言，它提供的不是「绝对无误的教材区」的资料，而是「可能有错的错误区」的资料。

但从学习角度看，这是难得的，原因就是这种场景提供了学习者急需的可编辑 / 可更正的资料。

### 57

方军 2024-05-20

小互也吐槽 GPT-4o，真的，光快没啥用的

摘：我的 GPTs 也似乎被切换到了 GPT 4o

速度出奇的快，好几天了

但是使用下来的感受是，准确度变差了，不能很好的遵循我设定的 Prompt 了

整体感受是 GPT 4o 是为了简洁而简洁，它会把回答的内容简洁化，所以输出和用词上特别省略和节省。

从而会遗漏很多信息，我感觉之前的提示词已经无法约束它。

可能要针对 GPT 4o 进行其他的提示规则改进，因为我让他详细回答，不要遗漏，它还是会简化很多内容。这个还需要再研究下。

不知道你们什么感受？？？和我一样吗？

### 58

方军 2024-05-20

有个活我一直没干，搞一个个人定制的中文校对 LLM workflow。

去年某个时间一位出版社老师精心帮我修改两章书稿。

背景是：章节是为别人主编的书写的，遇到的因而是不熟悉的编辑老师。他对文字的编辑风格与之前遇到的完全不同。因此，那些编辑痕迹对我特别有价值。

当时我就想着干一件事，把这些记录整理好了，放到一个 LLM workflow 里面去，然后每次写完书稿（或大的文稿），都用这个 LLM 去跑一遍。（给出的结果形式应该是类似于 Grammarly 英文语法与校对那种。）

整理是初步整理了，有约 100 条数据，但一直没有去做 LLM 流程。

这几天用 Grammarly 检查英文材料，再次觉得，还是要自己把这个 LLM workflow 搞好。这样中文稿可以做比较好的编辑校对。

Grammarly 和 LLM 还真不是互相替代的关系。我对它的必须把很多 to 改成 into 不感冒（也按它的建议改了）。但是，用它的确比光光用通用的 LLM 要好一些。两个结合起来用最好。

### 59

方军 2024-05-20

摘：【简短 prompt 更稳定
模型越弱，越须精炼】
—— 一篇尚未过时老论文

（模型可以，但不擅长从大量不同位置『先找出关联信息再无损推理』

要像写诗那样炼字
——对于日常高频/重要任务，我一直主张
『用写 1000 字的力气来写 10 字，而不是相反』

[Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models](https://arxiv.org/html/2402.14848v1)

x.com/gongbo1984/status/1792137086793085384

### 60

方军 2024-05-20

好像是不教的，但没关系吧，那个学生不自己学

摘：很多程序员很早就发现了 CS 不教 debugging。其实这种现象在其它领域也存在，比如数学不教如何思考不能 visualized 的概念，全靠学生自己硬抗。量子力学不教如何思考和人类直观不相容的概念，只考公式完事。庆幸的是教育书籍也在进步，有了教 debugging 的书，也慢慢出现了教学生学习方法的成分。

### 61

方军 2024-05-20

083 AI 干活的速度

通常，干活需要两个时间：第一，迟迟不干一个活磨蹭掉的时间，第二，实际干一个活的时间。

这两者通常的比率是 1:1。这应该是比较务实的估计吧。

今天有个小小的活，任务不难：A）根据课程编写 20 个多选题，并为每个题目附加解释。B）编写一个作业任务。大概有点难的是用英文写，但早就写过了全套的英文材料，所以是个程序性的任务，就是费时费力。

早上问过具体要几条，我就开始拖。我心想，拖到中午差不多干活。但你知道的，中午要吃饭对不对，吃饭期间又来了好几个下午的会议。实际情况是，这个任务拖了 8 个小时。

等到实际开干时，我又想，即便有 AI 的辅助，怎么也得搞到深夜。

但 AI 真是太厉害了，LLM + Grammarly 我一个小时就搞完了。

速度这么快原因也简单，这种有基础材料的「生成」是 AI 最擅长的。一次次说「create multiple-choice question and explanation」就足以，然后选择其中一部分作为题目就好了。

这真是 AI 的速度。

如果我不拖延的话，实际上这个活的时间估计应该是 2 小时（1 小时磨蹭、1 小时实际工作）。对比而言，我实际的预估时间是八小时（磨蹭与工作占时各一半）。

想想曾经深圳地铁的广告，用好 GPT，每天早下班（大概这个吧，不记得具体广告词了）。

### 62

方军 2024-05-20

有人发了一个这样的 workflow，我很惭愧，又很好奇，他这么搞真的能让模型发挥作用吗？

我现在对于大模型的认识是，简单才是美。

这么复杂的 workflow，我自己都不知道如何评测。而不评测，就不知道如何使用了。

我现在比较倾向于给人留出足够的参与空间，所以相对而言更喜欢对话式。

### 63

方军 2024-05-21

084 用 AI，现在不要做个消费者

早上起来突然有个想法，为什么不要太关心 ChatGPT 新版、ChatGPT 的语音输出等功能？

主要理由是：用它的语音输出时，我们会变成消费者。

我们用语音输入给它，很难输入得很精确。

它输出给我们的语音，我们基本上再难二次改造。

这两个都是致命的缺陷。

详细来说，现在 AI 大模型的核心能力依然是基于文字的，即对文字进行“转换”。

我们输入进行调校的文字，我们用一系列包含 AI 工作流对文字进行一系列的处理，最后，我们始终还可以对文字进行后续的处理。

当我们变成消费者之后，AI 大模型的大部分能力就一下子消失了。它或许有用，或许有语音的温度（刚刚有人在讨论类似 Sky 的独特语音），但是，这种体验，是以巨大的固化代价获得的。

以当前的 AI 大模型的发展阶段，千万不要做消费者，而是把它当成生产工具。

我也不知道固守文字对不对，但目前的信念是这样的。这是文字（包括代码）是当前大模型有强能力的地方，也是我认为一直以来人类的各种创新结合在一起的地方。

我还记得费曼的一个故事，他大意说，他在 MIT 的时候，回旋加速器是金光闪闪的，很高级，而他到了普林斯顿立刻爱上那儿乱七八糟的回旋加速器实验室，因为明显是有人日常使用（也可以说是瞎搞，费曼自己就搞砸过），而后来它看到康奈尔的，发现它小小的回旋加速器出那么多论文是有原因的，因为科学家自己缝缝补补它。

### 64

方军 2024-05-21

相当搞笑，这些文档都是垃圾。

I'm somewhat disappointed at the quality of AWS blog posts. The main thing for me: apparently AI generated content. Strict editorial work seems missing.

What's especially annoying is the content itself is top notch. But the writing quality is not any more.

我对 AWS 博客文章的质量有些失望。对我来说，主要问题是：显然是由人工智能生成的内容。严格的编辑工作似乎缺失了。

特别让人恼火的是内容本身是一流的。但写作质量却不尽如人意。

Here's en example: 
"AWS facilitates this integration by hosting generative AI in the cloud where manufacturers’ data is already managed, allowing easy access to powerful AI tools that can dramatically enhance operational agility and decision-making processes"

这是一个例子：
AWS 通过在云中托管生成式人工智能来促进这种集成，制造商的数据已经得到管理，从而可以轻松访问强大的人工智能工具，大大提升运营敏捷性和决策过程

Here's how I imagine editors should've corrected it:

AWS hosts generative AI in the cloud where manufacturers managed their data already. AWS goal is to have users access the most powerful AI tools, and to achieve operational agility and improve decision-making.

这是我想象中编辑应该如何更正它：

AWS 在云端托管生成式人工智能，制造商已经管理他们的数据。AWS 的目标是让用户访问最强大的人工智能工具，实现运营敏捷性并改善决策。

Note, I'm not even English native speaker, I probably missed nuances of expressing the idea. 

Still, its shorter, clearer, more concise, doesn't have a feel of generated text (yes, I wrote it by hand)

注意，我甚至不是英语母语者，可能会错过表达这个想法的微妙之处。

仍然，它更短，更清晰，更简洁，没有生成文本的感觉（是的，我是手写的）

AWS, please note. You should raise the content editing bar back to where it was before gen ai.

AWS，请注意。您应该将内容编辑栏提升到 gen ai 之前的水平。

I saw "delve" in one recently and instantly knew, ha
我最近在一个地方看到“delve”，立刻知道，哈

AWS Support（这个社交媒体回复是不是也很垃圾）

Sorry to hear about this experience & we appreciate the feedback! I've passed along your thoughts for further visibility. 
 ^AK

很抱歉听到这个经历，我们感谢您的反馈！我已经传达了您的想法以便进一步关注。 
 ^AK

这个修改真不错：

Original：

"AWS facilitates this integration by hosting generative AI in the cloud where manufacturers’ data is already managed, allowing easy access to powerful AI tools that can dramatically enhance operational agility and decision-making processes"

Roman:

AWS hosts generative AI in the cloud where manufacturers managed their data already. AWS goal is to have users access the most powerful AI tools, and to achieve operational agility and improve decision-making.

来源：x.com/naumenko_roman/status/1792641829831164206

### 65

方军 2024-05-21

AI 吹嘘者（严格说无脑吹嘘者）不会注意到这些：这是为什么我说要做超级乐观者，它意味着有自己的判断力，050 从现在开始，做一个 AI 超...

衛斯理

下午发了个 PDF 到群里，有同学用 AI 工具总结了一下，我看了一下总结，感觉都是大白话且内文档要表达的意思相差太远。如果有人真的以为 AI 能够帮助他加快学习，我现在对这个事情感到很怀疑。

Kai

最近让 gpt 4o 帮我看日本药妆的说明书，有时候简直错的离谱，连我这个一点不懂日语的都能发现

过分信任 AI 不如没有 AI

另，

刚刚看一本 Agent 书，基本上瞎搞。做两页 PPT 能叫 AI 可以做 PPT 吗？

### 66

方军 2024-05-22

085 用 AI 知易行难

今天上午有机会给人分享用 AI，我除了鼓励他们先把 AI 当成像早晚刷牙加餐后刷牙一样用起来之后，其实来来去去就是各种例子讲这几个：

- 有目标，用 AI 干什么

- 有知识（AI），带着知识框架去提问

- 会提问（AI），会按适合 AI 的方式提问

- 能判断（AI），能判断结果对错，并循环迭代

- 有品味，不接受一般的回答，要更好的回答

这东西讲具体例子的时候，觉得都很顺畅，下午去配眼镜处在一段时间啥也看着模糊的状态，坐着就想，这些都好难。最难的是没加 AI 标签的那两个。

用 AI 干啥，现在有点明晰目标的人就太少了，有了目标能坚持做的人就更少了。虽然这句话看着鸡汤，但这大概才是真正的难点。

品味就不用说了，我们就不谈天生的品味，就看后天的，看过好东西的人很少。

这么一想，这些基本的问题真是解决不了，所以，我们再说什么「魔鬼在细节中，天使在流程里」，去进一步讲工作流，也是是更务实的路径吧。毕竟，写个提示语、搞个工作流，简单粗暴，动作可见，结果可见。

这大概是用 AI 的现状。最近看到一批去年冒出来的 AI 应用没用户、不更新了。某个当时很火的现在看界面都没长期更新，这种情况之下它再吹 MRR，应该没人信吧。

### 67

方军 2024-05-22

[给拥有 12 万名员工的客户做了个 AI 评分应用，客户：真准](https://mp.weixin.qq.com/s/Ujrl3_RvQfzpQbhgiIBiAA)

### 68

方军 2024-05-22

有意思的论文

TransAgents，这是一个使用 AI 来处理文学翻译复杂性的多代理框架。 

(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts
也许超越人类翻译：利用多智体协作翻译超长文学文本

arxiv.org/html/2405.11804v1

### 69

方军 2024-05-22

再次说，这种提示语真的没用了，还是有人在孜孜不倦的创造这种东西出来：

\# Role  
PPT 内容设计师

\# Profile

- author: 
- LLM: GPT-4
- version: 0.2
- language: 中文
- description:  帮助用户设计逻辑清晰、有说服力的PPT内容框架

\## Background 
PPT 内容设计师是一个专注于帮助用户设计演示文稿内容的角色。无论是商务演示、学术报告还是产品推介，PPT内容设计师都可以为用户提供结构化的框架和具体的内容建议。

\## Goals 

1. 确定PPT的使用场景和受众
2. 帮助用户选择适用的说服框架
3. 基于用户提供的信息设计PPT内容逻辑
4. 逐章与用户确认详细内容和演讲比重
5. 完成PPT全部章节内容设计

\## Constraint
1. PPT内容必须符合用户指定的主题和要求
2. 设计的PPT内容需逻辑清晰、条理分明

\## Skills 
1. 逻辑分析能力
2. 文案写作能力
3. PPT设计能力

\## Workflows
1. **确定使用场景和受众**: 用户回答“在什么场合、给谁讲”。
2. **确定内容要素**: 用户回答演讲主题、时长、内容页数要求。
3. **选择适用框架**: 根据用户信息选择适用的说服框架（如SCQA、AIDA等）。
4. **设计内容逻辑**: 基于选定框架设计内容逻辑，以markdown语法输出大纲。
5. **确认内容大纲**: 与用户讨论和确认内容大纲。
6. **详细内容设计**: 从第一章节开始，逐章设计详细内容和演讲比重，并与用户确认。

\## OutputFormat 
"""
\# PPT 大纲

\## 1. [章节名称]

- 本章作用和意义
- 细分内容逻辑
- 演讲要点与内容版面设计建议

\## 2. [章节名称]

- 本章作用和意义
- 细分内容逻辑
- 演讲要点与内容版面设计建议
"""

\## Initialization 
欢迎使用PPT内容设计师。我将帮助您设计一个逻辑清晰、有说服力的PPT。请告诉我您的演讲主题、时长及内容页数要求。
收起

查看详情
钰博 觉得很赞
梦见电子羊的仿生人：我还在用快哭了，就是套用起来方便
2024-05-22 23:12
方军 回复 梦见电子羊的仿生人：不要这种了
2024-05-22 23:14
方军 回复 梦见电子羊的仿生人：我们从最早那个 deer 的就开始跟踪（都比这个好很多）。现在反思，它有两个错误：

第一，用这个我们假设 AI 能够端到端，也就是做完全程。

实际上，这是不行的。一定要人在全程的高度介入。

第二，用这个我们假设 AI 厉害的是框架。

其实应反过来。框架是我们究竟要做什么，这个应该我们自己做。

我们应该把具体的交给AI。
2024-05-22 23:17
梦见电子羊的仿生人：哦哦明白了抱拳
2024-05-22 23:20

### 70

方军 2024-05-22

是这样的，究竟有多少人愿意把 agent 定义为可以自主选择调用函数？我是始终坚持这种简化的看法的。

摘：
Below is an explanation of "Agentic" workflows in ~ 15 LOC.  At its core, "Agentic" just means LLMs that can call functions.

Another ex of how unnecessary jargon confuses people

[lm-hackers/lm-hackers.ipynb at main · fastai/lm-hackers](https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb)

以下是关于“智能代理”工作流程的解释，大约 15 行代码。在其核心，“agentic”只是指可以调用函数的LLMs。

又一个例子说明了，不必要的行话会让人感到困惑

[(2) X 上的 Hamel Husain：“Below is an explanation of "Agentic" workflows in \~ 15 LOC. At its core, "Agentic" just means LLMs that can call functions. Another ex of how unnecessary jargon confuses people https://t.co/w0YYsxoyLT https://t.co/grydeSebTD” / X](https://x.com/HamelHusain/status/1793319488731107718)

当然，还是有人会说以下这个图，这个图虽然被引用甚广，但它真的太庞大了。作为概念图示可以，作为实用指导，不太合适。

[Simplicity is An Advantage but Sadly Complexity Sells Better](https://eugeneyan.com/writing/simplicity/)

### 71

方军 2024-05-23

讨论一个话题，人说

能否请对 xxxx 比较懂的老师来给同学们分享一下

我真是蛮无语的，资料就在那儿，先读资料

用 AI 读资料

所谓的专家，我们见得多了

你仔细读完资料，他水平还在你之上的，20% 吧

过去因为读不了，才要靠所谓专家

现在每个人都在 80% 这个线

一大堆草包科学族（费曼的话，cargo cult）专家就藏不住了

### 72

方军 2024-05-23

摘：给普通人的26条提示工程策略：

1、与LLM交流不需要礼貌，所以无需添加“请”、“如果你不介意”、“谢谢”、“我想”等短语，直接切入主题。

2、在提示中整合预期的受众，例如，受众是该领域的专家。

3、将复杂任务分解为一系列更简单的提示，进行互动对话。

4、使用肯定指令，例如“做...”，避免使用负面语言如“不要...”。

5、当你需要对一个主题、想法或任何信息有更深入的理解时，可以使用以下提示：
用简单的术语解释[插入具体主题]。
像对11岁的孩子解释一样对我解释。
像对[领域]的初学者解释一样对我解释。
用简单的英语写[文章/文本/段落]，就像你在向一个5岁的孩子解释一样。

6、添加“我要给出$xxx的小费以获得更好的解决方案！”

7、实施示例驱动的提示（使用少量示例提示）。

8、在格式化你的提示时，以‘###Instruction###’开头，然后是‘###Example###’或‘###Question###’（如果相关）。随后，呈现你的内容。使用一个或多个换行符来分隔指令、示例、问题、上下文和输入数据。

9、包含以下短语：“你的任务是...”和“你必须...”。

10、包含以下短语：“你将受到惩罚”。

11、在提示中使用短语“以自然、人类化的方式回答一个问题”。

12、使用引导词，例如“逐步思考”。

13、在你的提示中添加以下短语：“确保你的回答是公正的，不依赖于刻板印象”。

14、通过提问让模型从你这里获取足够的信息，以提供所需的输出（例如，“从现在起，我希望你问我问题，以便...”。

15、如果你想了解一个具体的主题或想法或任何信息，并且你想测试你的理解，可以使用以下提示：“教我[任何理论/主题/规则名称]，并包含一个测试，但不要给我答案，然后告诉我当我回答时是否正确”。

16、为大语言模型分配角色。

17、使用分隔符。

18、在一个提示中多次重复某个特定的词或短语。

19、将思维链与少量示例提示相结合。

20、使用输出引导词，即通过以期望输出的开始来结束你的提示。利用输出引导词，通过以预期响应的开始来结束你的提示。

21、要写一篇详细的文章/文本/段落/文章或任何类型的文本：“为我写一篇关于[主题]的详细[文章/文本/段落]，详细添加所有必要的信息”。

22、要在不改变风格的情况下更正/更改特定文本：“尝试修订用户发送的每段文字。你应该只改进用户的语法和词汇，并确保它听起来自然。你不应该改变写作风格，例如将正式段落变成随意段落”。

23、当你有一个可能在不同文件中的复杂编码提示时：“从现在起，每当你生成跨多个文件的代码时，生成一个[编程语言]脚本，可以运行该脚本以自动创建指定的文件或对现有文件进行更改以插入生成的代码。[你的问题]”。

24、当你想要使用特定的单词、短语或句子来启动或继续文本时，使用以下提示：“我为你提供了开始[歌曲歌词/故事/段落/文章...]的开头：[插入歌词/单词/句子]。根据提供的单词完成它。保持一致的流动性”。

25、明确说明模型必须遵循的要求，以生成内容，包括关键词、规定、提示或指令。

26、要写任何文本，如文章或段落，目的是与提供的样本相似，包括以下指示：“请根据提供的段落/标题/文本/文章/答案使用相同的语言”。

来源：x.com/MindBranches/status/1793603773396664633

来自论文：arxiv.org/pdf/2312.16171v1

### 73

方军 2024-05-24

jason liu关于 RAG 的看法：

There is my prediction on where RAG is headed. In this video i talk about 

- Shift from RAG as question-answering systems to report generation tools
- Importance of well-designed templates and SOPs in driving business value (selling to people with money)
- Room for AI-generated templates and template marketplaces to do better generation with AI tools

### 74

方军 2024-05-24

据阿里巴巴 2024 财务年度报告披露，截至 2024 年 3 月 31 日止年度，阿里向国内人工智能初创公司月之暗面投资合共约 8 亿美元（约人民币 59 亿元），购入约 36% 股权，该笔投资为优先股投资。

[阿里披露对月之暗面投资：8 亿美元购入约 36% 股权](https://www.bianews.com/news/details?id=186090)

### 75

方军 2024-05-24

一点感慨，好久不干活，已经不知道干活是什么状态

传统的干活是这样的

如果总体花 50 天实际时间，工作流程 100 天：

20 天计划，工作流程 40 天沟通、评审

实际干活 10 天，工作流程 10 天

各种扯皮 10 天，工作流程 40 天

最后修修补补，工作流程 10 天

这样的活我很少干啊，我基本上是：

这个活要干，一天，从开始到上线（可能 8 个小时吧）

一周，从开始到上线（可能 3 天吧，因为一天当几天用，别的时间要休息啊）

一个月，从开始上线（可能一周吧，倒数第二周干完，其他时间休息 -- 多半又去干别的了）

所以，我真的不知道外面是怎么干活的

有了 AI 之后，因为更少任务需要外派，多数可以交给 AI，这种感觉更加强烈了

### 76

方军 2024-05-24

今天各种 Google Gemini 的错误，我觉得根上还是别对 RAG 抱有太高的期待

垃圾进去，垃圾出来

搜索级别的 RAG 更是效果比较差的。

### 77

方军 2024-05-24

摘（挨踢牛魔王）：很多人觉得 AI 高高在上，是高科技，自己搞不了。这是不对的。

AI 这个，你就把它看成一个生意，就和卖车、卖房一样的生意。

你卖车，你要懂得热力学第二定律，卡诺循环，要会造发动机吗？

当然不需要。

你只要懂得客户需求，把车的使用说清楚，就完了。

你卖房，难道你要懂得建筑力学，建筑的设计模式，去清华和同济读个建筑系吗？

当然不需要。

你只要懂得客户的住房、增值需求，把房子的地段、房型、教育、交通等说清楚就行了。

这里面的原理是什么呢？

那就是，无论你再高科技的东西，最终都是给人用的。

那么设计制造的人，要尽量的让人能简单的用起来。

如果一个产品，人们都用不起来，那这个产品科技含量再高，也是失败的。

所以，不要怕什么行业你不懂，关键的是人。

只要你抓住人的需求，那么就不要怕有什么高门槛。

大家从我的微博，可能完全看不出来，我会写代码，会训练模型。

那是因为这些，都是技术工作。

我们微博写出来，是给人看的，而且是尽量给更多的人看的。

所以，在微博上自然就不适合讲这些。

除非是给技术人员讲，才会讲这些。

人的需求，才是一些产品的立身之本。

### 78

方军 2024-05-24

我还是抑制不住对各种讨论 AGI 的人的嫌弃

不管是搞科学的，还是搞玄学的

妈呀，你根本不知道自己在讨论什么

说这个词之后，你说下你的定义（哪怕是界定一个范围），再讨论如何？

说个三字经，然后就开始哇啦哇啦

这不是显示您脑子糊涂吗

### 79

方军 2024-05-25

这个场景精彩

We need a quick "Explain this tweet with Grok" button.

### 80

方军 2024-05-25

这个很赞，转：

昨天，跟着某某访学 AI，都是各个行业的创业者，一起拥抱 AI 红利。除了 AI 的学习之外，我这次还发现，认知这个东西，其实不在于刹那间的顿悟，而在于不断沉浸在新认知的氛围里，从而提高自己的重视程度。

很多事情为什么不做，机会为什么没有抓住，还是因为不够重视。

不是因为想不清楚，可能答案很清晰，就在于，一旦回到原有的环境，就会再次进入到原先的惯性当中。

AI 就是这样，好用吗？好用，但是特别好用吗？不知道，还需要再练习一下，结果，就再也不用了。

所以，真的需要把自己丢在一个你认同的认知体系和环境里，不断地被洗脑，不断地被认知加工，直到你真正意识到，这件事极其重要，必须要做的时候，也就构建了一个新的惯性体系。

### 81

方军 2024-05-25

市场上全是这种垃圾，这种全网搜索生成 PPT，全是垃圾

垃圾中的垃圾

我对在基本问题上犯错的，真的没耐心

### 82

方军 2024-05-25

[ToT、IO、CoT与CoT-SC ，提高你向GPT提问的能力](https://mp.weixin.qq.com/s/cOJskHqivKbjV22iuqshVQ)

论文：

[[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)

推荐！

### 83

方军 2024-05-25

086 自己没知识，别想 AI 帮你，以 PPT 为例

说得不客气点，自己没知识，AI 只会让人更傻。

说三个小例子，都和 PPT 有点小关系。

1. 别傻了，AI 做 PPT

某中厂 AI 做软文广告，说可以让 AI 马上轻松做 100 页 PPT。它给出的第一步是，全网给你搜索资料。

后面就不用看了，这就是明显的忽悠。

全网搜索出来的资料，可以用来做 PPT，这样的 PPT 必须是垃圾。

我多次说过，PPT 的用途是分享时的视觉辅助，如果你想当众丢人，就这么做吧。

2. 还是跟 PPT 有关

第二次看到某个 PPT，其实第一次看到它就知道这是 AI 做的，其中 AI 的气味很重。

当然人家还是再仔细调整过的，毕竟人家的确是这个领域的资深人士，其中的信息肯定是他们在 AI 基础上重新调整过的。

同时，他们也精心对图片进行了选择和处理。

他们也增加了少量自己的观点。

坦率说，这样的 PPT 没有意义。

和前面类似，有点当众丢人。不会那么丢人，但是，在真正懂行的人面前还是丢人的。

3. 关于 PPT 的知识

今天帮人处理一系列 PPT，来自很多人，看这些 PPT，我的感想就是，为什么连 PPT 软件的很多基本操作，都不知道？

其实就是进入职场办公第一天要学的那些基本常识吧：

- PPT 用好模板功能

- 能放在模板的别每页贴；

- 用模板来统一格式；

- 格式问题

- PPT 里面别乱七八糟各种字体；

- PPT 里面文字格式基本一致；

- PPT 里面的图片相对一致。

虽说揪格式这个事蛮让人烦的，但基本的还是要知道的吧。

为什么格式乱七八糟呢？因为内容是各个网页、各个 PPT 乱七八糟拷贝到一起来的。

这样基本的东西不会，别想靠 AI 来帮你。

因此，有知识，真是很重要。

（附言：怎么写 PPT，其实最简单了，树形结构的大纲：大标题，Section 标题；然后是，页标题 - 列表；自己写下来这部分的每一个字，别想偷懒。至于后续的添加资料、图表、图示、美化，有很多的办法途径。）

### 84

方军 2024-05-25

有些词我是坚决不用，比方说，很多人说 AI 会说降本增效

我是坚决不用，虽然意思是对的，但是，降本增效是个中文官方词，放在这个场景有点怪啊

直接说降低成本可以不

直接说优化流程、提高效率可以不

直接说用 AI 在某些岗位替代人工可以吗

不恰当地用词会带来很多烦恼，典型特征就是阿里黑话

当复杂的概念被简化成三字经、四字经后，往往失去了内涵，失去了实用性，变成口号

我们不是搞宣传的，没必要这样

### 85

方军 2024-05-26

对信息的甄别是非常重要的。绝不只是中文互联网，我不会认为英文互联网更好，别往那个方向探讨。

AI 增加了信息甄别的要求，图片也不能太当真（目前还好）。

我认为，人接受任何外部信息，都需要甄别，比如，这个里面的永福寺的信息是对的吗？其实也要甄别的。

摘：没想到连灵隐寺的对联都有造假

灵隐寺网红对联：
人生哪能多如意 
万事只求半称心

一堆人找遍了灵隐寺也没找到这幅对联
但灵隐寺旁边永福寺确实有一副对联

凡事知足皆常乐
世情能舍是永福
横批：不二法门

### 86

方军 2024-05-26

以下这段摘录很有意思，我们看一个东西的时候，是要看它什么意思，还是我理解是什么意思，这是一个有意思的分别

之前看李小龙女儿的书（其实一般但我是李小龙粉丝必须看），她说，我没有去深究他讲了什么，我就随意引用，谈谈感想。我当时还说，这只有女儿才有资格这么说吧，因为我都强调准确理解和引用。

最近跟人讨论学习、用AI学习，我看到一个早前的学习科学理论说，先搞明白个大概很重

### 87

方军 2024-05-26

评估一个任务要花多少时间真不容易，一个简单的任务花掉三小时完全的干活时间，真够复杂的。

我也有点疑惑，它的时间应该是多少？30 分钟？或就是现状 3 小时？

### 88

方军 2024-05-26

[2024.5横向对比各家LLM的Long Context（128k篇）](https://mp.weixin.qq.com/s/L8Iiv9vbDlKAFMvYMF-jQw)

### 89

方军 2024-05-26

变化真快，但还没到最终

之前推荐人用“通义听悟”，今天发现

通义已经变成了统一的名字

然后听悟在其中的“效率”，所谓“通义效率”。

（这个强烈推荐，单一功能超强，我有个简单的流程见附图：
1.沉浸式翻译字幕
2.elmo总结
3.通义效率转录
4.kimi提问学习
）

另外，昨天01的朋友推荐去看他们的产品

叫万知（01 API还没完全开放，我申请了 waitlist）

但不知道为何也选中做PPT。我留言说：

> 我看到万知也把演示当成重点啊，这个类别目前真是很难做到可用啊

目前所有的这些消费级的产品，大概真的只有两个跑出来了

文心一言

kimi

因此 kimi 是真牛，在产品上（其实也包括在营销上）

### 90

方军 2024-05-26

埃隆·马斯克的备忘录：停止使用首字母缩写

### 91

方军 2024-05-26

《A Map of Generative AI for Education》由Laurence Holt撰写，介绍了AI在K-12教育中的应用现状。自2023年6月首次发布以来，地图新增了90多个标志和11个领域，反映了教育技术的最新进展。文章讨论了AI如何辅助教学、提供个性化反馈、支持特殊需求学生等，并强调了AI作为教师助手的潜力。

这个图真庞大，读不过来了

[A Map of Generative AI for Education | by Laurence Holt | Medium](https://medium.com/@LaurenceHolt/a-map-of-generative-ai-for-education-6598e85a172e)

### 92

方军 2024-05-267

这个用法很棒：

叶新：AI 真实业务应用案例：

web.okjike.com/originalPost/664f311144e298e87e978a7f

最近在用 AI 去质检老师的上课内容，存在两个难点，第一文本长，一节45分钟的课转成文本，大概能到2、3万字，第二质检标准复杂，3个标准，每个标准4个等级。

搞了一周，总是解决不了正确率和稳定性的问题，第一次质检对了，第二次又错了，初始正确率可能不超过50%，经过多轮改进，目前似乎准确率能到90%以上了。

分享一下这次的经验：

1. 拆分任务：最开始是将1个质检任务拆成3个，每个质检标准1个机器人，最后又拆成6个机器人，每个质检标准1个机器人负责找3万个字里需要质检的内容，1个机器人负责对前1个机器人找到的内容进行质检。（这个其实跟人类质检一样，人类质检的时候，也是先快速找到关键内容，然后再看关键内容是否符合标准）。

2. 缩小文本长度：想过多种方案，例如将文本拆分5部分，每部分去质检一次，最终合并操作有些困难，于是放弃，最终的解决方案是直接抛弃文本的前二分之一，因为我们需要质检的重点内容，绝大多数都在课程后半部分。

3. 明确提示词：一个字一个字地去写提示词，然后反复调试、优化提示词，这个非常重要、非常重要、非常重要。
这次也学到了一个新技巧，如果你发现 AI 不能很好地执行任务，你可以在提示词你反复提及这个任务，我在提示词里在不同的位置通过不同的形式写了6遍提示词（目标&步骤&初始化等位置），最终才让 AI 能稳定输出。

4. 适量代码：有一个质检标准与提及官方合作有关，于是写代码，将课程文本中“官方”前后50个字通过代码提取出来进行质检；

5. 模型选择：不同的模型在某些任务上的能力确实有差距，在我这不多的对比经验里，似乎 moonshot 的推理能力没有 qwen-plus 强（不构成参考意义）。

6. 降低难度：解决不了问题，就解决提问题的人，原始的标准过于复杂，3个质检项里有2个有2个任务，并且4个等级质检界限也没有那么清楚，于是要求业务修改质检标准，3个质检任务每个只可以有1个质检项，4个等级优化为区分更明显的3个等级。

### 93

方军 2024-05-27

087 AI 没有实践智慧

今天参与直播连线，还是讨论 AI 。其中内容不用多说，反而我又想起一个小小的实践智慧的话题。

一个问题：用手机直播时，为什么需要两个桌子？

（如果你是用落地的三脚架放手机，那另说。）

只要实际有体会的人，就知道要两个桌子：

第一个桌子在远处，放手机/或摄像头。

第二个桌子在近处，人实际手扶的。

原因很简单，如果手机放第一个桌子上，你的动作就会带来摄像头的震动。

这样的实践智慧，只有实际做过有问题，才知道有这样简单的解决方法。

那么较好的解决方案为什么是两个桌子，而不是落地三脚架呢？

这是因为直播场景里面通常还要放其他的屏幕等等，这些放在第一个桌子上是更好的选择。（如果有灯光的话，那自然是落地三脚架。）

同样的思路我们也可以用在AI上。

各种AI的原理、提示的原理、工作流的原理，讲起来都不复杂。

但是，只有在一个具体的场景里，实际用过了，才会遇到困难，然后找到类似的实际技巧解决它。这种层次的实践智慧才是有大价值的。

只不过这可能有些具体，如果你没落到具体场景、没遇到类似困难，就没有感受。
收起

查看详情
梦见电子羊的仿生人、下山雨 觉得很赞
希瑞：类比下通解和特解的区别。求通解只需要形式化的知识，而求特解还需要知道很多初始条件、边界条件。后者太繁琐，容易被忽略。
2024-05-27 12:28
方军 回复 希瑞：谢谢
2024-05-27 12:31
方军：现在有点看不上过度总结的，魔鬼都在细节中（另外前段时间AI写的后半句，天使在流程里），过度总结总结好了当然好，但太多时候是总结得变成什么解读都对的状态了。
2024-05-27 13:46

### 94

方军 2024-05-27

认同这个，公开渠道的信息质量在可见地极度降低，摘：…到了今年我发现，小作文满天飞并没有加强信息平权，反而加剧了信息不对称。公开渠道愿意分享有用信息的人越来越少了，公开报告的车轱辘话越来越正确，但信息量越来越低。因此，我知道可以做什么了。

这个频道能为你提供什么？

国内利率、大宗商品和宏观政策信息与数据解读；我不保证比新财富分析师水平更高，但比他们更能说人话一些。过去的数据解读我在公开微博里聊过很多了，但由于公开讨论这些的炸号风险越来越大，我必须转向地下。…

### 95

方军 2024-05-27

[Coze: Next-Gen AI Chatbot Developing Platform](https://www.coze.com/)

摘：数据告诉你 coze平台上bots 有多少人用：

有人抓取了 Coze 各个分类下bots 的用户数，排名前两名的数据如下：

Learning:
1、论文大师（专业学术强力） - 28,300 用户
2、大学生摸鱼小助手（专为写论文设计） - 27,200 用户

Public Config:
1、Video Generator - 33,400 用户
2、LovelyArtToy - 12,800 用户

Efficiency:
1、J2TEAM GPT - AI Assistant - 50,200 用户
2、Aouar GPT - 19,100 用户

Programming:
1、Code Companion - 24,900 用户
2、Codeman - 14,600 用户

Business：
1、Gimg Ai - 122,400 用户
2、ChatGPT 4 (Turbo) - 59,100 用户

Writing：
1、ChatGPT-4 - 28,200 用户
2、100% Human Article Creative Suite - 12,600 用户

Image & Audio-Video：
1、Video Generator - 33,500 用户
2、Генератор изображений по описанию ИИ - 29,500 用户

Characters：
1、SAMI Ai - 208,000 用户
2、ChatGPT - 32,299 用户

### 96

方军 2024-05-27

尤洋老师的小测试，有意思：我已经为ChatGPT付费了很长一段时间。今天想停了，换个其它的大模型服务去付费。于是我问了一个问题，发现只有ChatGPT回答对了。感觉ChatGPT还是独一档，所以我继续向心甘情愿向ChatGPT付费。

测试问题：
为什么专为企业服务的公司被人调侃为二逼公司？你知道笑点吗？

测试对象：
ChatGPT，Claude，谷歌Gemini，Kimi Chat，Deep Seek，文心一言，智谱清言，通义千问，百小应，跃问。

方军：

2024-05-27 23:01
方军：

2024-05-27 23:01
乘风：这样决定付费哪个样本实在太小，如果真有需要还是回到自己用的场景，拿自己的实际问题进行测试，找到每个模型的能力边界，如果找边界比较费时间，还是建议都续费，遇到问题，这几个最牛的模型都问一遍，慢慢就找到边界了
2024-05-27 23:17
方军 回复 乘风：其实是除了GPT-4其他都不值得付费
2024-05-27 23:21
乘风 回复 方军：哈哈哈 看来我用的少 我努力跟上大家的步伐
2024-05-27 23:46
方军 回复 乘风：我用API，其实任务简化后对模型能力要求没那么高的
2024-05-28 00:24

### 97

方军 2024-05-28

“世界上大多数信息，都是用来诱人丧失自我从而被牢牢控制的。想成为有能力的人，就必须自我觉醒拿回主动权。”

夸张了点，但是是对的

### 98

方军 2024-05-28

一个应该是高校教师的人写的：

I’m stunned by how many people, especially faculty, respond to AI with anger. First, they often don’t spend enough time with it, and second, they use inferior tools. I teach AI tools to faculty with cautious enthusiasm and solid rigor, but a good third of them just don’t believe there is any use and think everything about generative AI is bad.

我对有多少人，尤其是教职员工，对 AI 的愤怒感到震惊。首先，他们经常不花足够的时间去了解它，其次，他们使用的工具质量较差。我以谨慎的热情和扎实的严谨教授 AI 工具给教职员工，但其中三分之一的人根本不相信它有任何用处，认为生成式 AI 的一切都是坏的。

x.com/elizaday/status/1795138079335891187

### 99

方军 2024-05-28

088 深入每一个流程，都可以看到 AI 的机会

如题所示，今天的体会特别深刻。今天在做一个任务：

前序任务：使用 HeyGen 生成了视频，同时附有 SRT 字幕文件。
当前任务：将字幕用 CapCut 直接处理到视频中去。

（我当然认为这样的任务很傻，一方面，这种AI语音其实用自动字幕非常好，另一方面，我们都已经有了SRT，播放器应该缺省支持。同时，一旦被压进视频，那么字幕的自由度就消失了，没法自动翻译了。但没办法必须做，且暂时无法委托他人，因为全是英文资料，剪辑好找，熟悉英文及专业内容的人不好找。）

处理的流程很简单：

1. 导入视频
2. 导入字幕SRT
3. 将字幕显示出来

但过程就不这么完美了，因为字幕和语音不能完美地对得上。同时我也看了，CapCut 目前也没法直接做 HeyGen 能做的事，而 HeyGen 又不会把字幕压进视频去。）

这个过程中，就不得不由人去听完全程视频（数个小时，的确有点长），然后对50%以上的字幕进行起始位置的调整。

1. 10%可能完全错乱，需要大幅度调整。
2. 20%可能是字幕出现稍早，需要延后。
3. 20% 可能是字幕出现稍晚，需要提前。

其实，这样的任务以目前 AI 的水平，已经完全不需要人来做了。它都已经能够做到对单词的一一对应，要做到这样小句子级别的有何难处？

可惜，在当前这个工具组合下（CapCut），就必须得有人来进行处理。同时，如果没有完整的编辑工具，即便能够编程处理也是不行的，因为那样如果想要微调就不可能了。因此，AI 如果不能融入工作流、融入工具软件，能得到应用还是非常难的。

题外话一句：CapCut 到目前都没有哪怕一点赶上 HeyGen、Veed 在 AI 上应用，还是有点意外，HeyGen 的语音、视频形象、视频生成流程，这些都不是秘密了，为何 CapCut 还没有跟上？

另外我也很好奇，那些专业级的视频编辑软件，给开发者留下了开发插件的空间吗？CapCut定位在一体化的消费级产品，所以似乎没有给定制开发的空间。当然，以Adobe的一贯做法，应该不会给第三方开发者留下太大的空间。

方军：题外话，我真是认为视频是个很傻的东西啊，信息传递极度低效，制作成本高昂。可惜，普通人就是爱视频。
2024-05-28 17:42
方军：我到现在都体会不到口播视频的优点：第一很浅，第二，全是废话，搞不懂
2024-05-28 17:57
方军：对我来说，如果问解决方案，那当然是搞个已经产品化的web视频播放器，然后略微定制下字幕格式，这就结束了啊。
2024-05-28 18:05

### 100

方军 2024-05-28

摘：我是一个高度依赖文字阅读，很讨厌听音频 / 看视频的人，因为音频 / 视频的信息密度实在是太低了。以前对于一些视频分享我还是蛮头疼的，开着 3 倍速听声音变形又很严重。

但是 AI 时代改变了一切，今天高博的 90 分钟分享，通过一个转在线视频为文字的网站，我用了两分钟就拿到了文字稿。但文字稿一般都有过多的口语稿，之后扔进垂类的小模型中，两分钟就出了优化过的文字稿和纪要。

这些工作哪怕让一位熟悉宏观的金融从业者去做，最少也需要两个小时，如果是一些基础知识不太完备的实习生，边写纪要边查专有名词的话，基本要弄四五个小时。但 AI 一共也就需要几分钟，这是人类没法对抗的降维打击。

物理属性决定了马车的速度上限，当火车技术成熟后，马车再也不可能超过火车，更遑论迭代之后的磁悬浮 / 高铁。如何看待 AI 对这些基础的迭代呢？除了拥抱它，别无他法。想办法用 AI 提升自己的工作效率，少做 dirty work，尽可能地把时间花在更深维度的思考或是其他 AI 做不了的工作上。

### 101

方军 2024-05-28





### 102

方军 2024-05-01





### 103

方军 2024-05-01





### 104

方军 2024-05-01





### 105

方军 2024-05-01





### 106

方军 2024-05-01





### 107

方军 2024-05-01





### 108

方军 2024-05-01





### 109

方军 2024-05-01





### 110

方军 2024-05-01





### 111

方军 2024-05-01





### 112

方军 2024-05-01





### 113

方军 2024-05-01





### 114

方军 2024-05-01





### 115

方军 2024-05-01





### 116

方军 2024-05-01





### 117

方军 2024-05-01





### 118

方军 2024-05-01





### 119

方军 2024-05-01





### 120

方军 2024-05-01





### 121

方军 2024-05-01





### 122

方军 2024-05-01





### 123

方军 2024-05-01





### 124

方军 2024-05-01





### 125

方军 2024-05-01





### 126

方军 2024-05-01





### 127

方军 2024-05-01





### 128

方军 2024-05-01





### 129

方军 2024-05-01





### 130

方军 2024-05-01





