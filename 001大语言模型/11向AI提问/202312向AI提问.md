### 01

方军 2023/12/01

试验了 heygen，做了较长的内容。

一个感受是，对于程序性的内容，还行吧，不需要感情，而且准确。

同时，可以方便地多语言。

但是，它实在一眼假、没感情。

要用起来，还是得找到合适的场景。首先能想到的是，绝对不适合社交媒体，因为它需要真实。

AI 图片、视频，我觉得都有很长的路要走，并且还是要人在循环中，不能试图直接生成。其实，最牛的 AI 生成，我觉得也没我拿普通的 X100V 出门拍出来的好啊。

### 02

方军 2023/12/01

哈哈哈，AI 可以好好把前面的词学习下。

某老师说 AI 电商，这么虚的词的确厉害，有无限的解读空间，但是，有啥用呢？对商家能卖货不？对我们消费者能便宜不？

最近的确对云端☁️的话免疫，飘啊飘。

友：人家名字就是☁️

嘿，不光是他了，好多大佬甚至周围普通人都这样，大佬就算了，人家本来云里，必须雾里。

普通人凑什么热闹讲虚虚的词。

说个词：认知。

你干事干得不好。认知低！

你倒是说说，你说的认知究竟是啥？

我们跟着外界热词懊恼：我认知低，要发奋提高。这不行啊。

说点具体的行不行。

说认知是棒喝，也有用。但是，真面对，我们一点一点看具体问题。

还有一个，比方说我浅浅地说芒格，我觉得他其实讲了和展示了至少这么一种方法论：学习多种思维框架。

这是具体的。但是，如果你今天看他一种表达，明天又讲了什么，那些其实不过是一位智者即时的反应，我把这种也叫虚虚的词。

### 03

方军 2023/12/01

字节跳动海外也做了一款基于大模型的 App ChitChop，可以说是一个私人 AI 助理，包括 AI 创作、AI 绘画、娱乐、AI 学习、工作、生活等 6 大使用场景。在海外 Google Play 和 App Store，均可下载 ChitChop，网页版也已经上线。根据应用商店上显示，ChitChop 由 POLIGON 开发和运营，而字节跳动海外的社交产品 Helo、日本漫画 App 「FizzoToon」也由同一家公司运营。

[字节跳动出海生成式 AI 产品 ChitChop 上线，含创作、绘画、学习等 6 大场景](https://mp.weixin.qq.com/s/hyGDXUipyiTB9q4f0QQ6-Q)

### 04

方军 2023/12/01

小提醒，现在的订阅尤其 AI 来了之后的功能订阅尤其多

1）尽量别冲动，月度订阅，而非年度订阅。20% 的年度优惠不值得，你不知道是否会长期用，并且可能很快有新的出来。

2）如果只是尝试性订阅，记得取消订阅，否则下个月继续扣钱。几个一加蛮不少的，并且通常无法退款。

3）一般高级版会显得特别优惠、功能强大，但最好从最低阶订阅起，可以慢慢买点数。比如，midjourney 我一直只订最低档，然后需要时一个月可能买大几十美元点数（当然这个做法是从高级降下来的）。

4）用好免费试用期，有不少是提供一点点免费试用的，搞明白自己是不是真要。用这个方法试了不少，最后选想要的。

5）报告类的订阅，尤其要谨慎，曾定了一个报告服务 7000 美元，前一个半年特别好，后一个半年就太坑了，内容差到无法忍受。所以，第二年不会订了。

6）有时候单次服务比订阅更便宜，虽然单次看起来贵。就跟杂志与书一样吧，杂志一年不贵啊（现在也蛮贵的，一本 20-30，一年 360-500。一本书 70-100，其实多数的需要的时候单本买蛮好。

7）搞明白订阅选项。昨天就尴尬了，买 179 美元每月订阅买成 119 美元每月订阅，还无法变更升级，一个核心功能没有，导致手工折腾很久还有瑕疵。（12 个小时后才有有效客服回应，但已经绕着法子搞完了）

另外，某些软件、服务的年度订阅其实也蛮贵的，通常 50-100 美元，但真是好几个都订六七年了。

### 05

方军 2023/12/01

这么说吧，数字人纯属扯淡，但满足人们扯淡的心（想起来当时某个企业不做数字人方向，后悔的时候埋怨我，哎，现在我还是说它是扯淡）

[「数字人」已经开启了收割模式（作者：灰鸽）](https://mp.weixin.qq.com/s/8gQCqka5bGjoSUJoo4BauQ)

### 06

方军 2023/12/01

蛮有意思的类比，但还可以接着改进：

LLM 就像是一个考生，

训练数据是教材，

context 是短时记忆力，

prompt 是解题技巧，

fine-tune 是补充教材和辅导书，

RAG 是开卷考试，

function call 是允许带计算器。

---

prompt 是解题技巧，一般般。

function call 是带计算器，好像也一般般。

fine-tune 是上补习班。

prompt 是考试题。

### 07

方军 2023/12/01

提示工程已死？

不，这是最先进的技术（SoTA）。

GPT-4 在所有九个 MultiMedQA 基准测试中击败了未经微调的 Med-PaLM 2，这得益于良好的提示（动态 k-shot + 自动生成的 CoT + 选择混洗的集合）

原推文：GPT-4 with good prompts (dynamic k-shot + self-generated CoT + choice-shuffled ensembles) beats Med-PaLM 2 on all nine of the MultiMedQA benchmarks it was fine-tuned for, without fine-tuning

twitter.com/goodside/status/1730410630673244654

Eric Horvitz

微软首席科学官，总统科学技术顾问委员会，前 AAAI 主席。

我们已经发布了一项关于提示在不需要额外微调或专家策划的情况下，释放 GPT-4 在医学基准测试中的专业知识的研究。

结果摘要：twitter.com/erichorvitz/status/1729854235443884385

论文：

[[2311.16452] Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine](https://arxiv.org/abs/2311.16452)

### 08

方军 2023/12/01

一个面向全球教师的 Generative AI 课程：

Generative AI for Educators

为联合国教科文组织（UNESCO）创作的。

效果还不错，第一个 Section (共五个，一共 60 分钟)。

www.youtube.com/watch?v=0sNZddVDb7w

我拿它当成一次 AI 产品的试用体验。

All you need to know about ChatGPT and Generative AI as an educator or as a learner.

Section 1: Course Introduction

Length: 10 mins

- GenAI's impact on knowledge work

- GenAI's impact on educators & education

- What're AI and Generative AI?

- Creating new content

- 6 scenarios of GenAI: text

- 6 scenarios of GenAI: media

- ChatGPT's wisdom on education

- What you will learn?

Disclaimer: Audio and avatar is generated using GenAI models and tools.

### 09

方军 2023/12/01

AI 时代的视频剪辑产品淘汰赛

Veed.io 在早期是完全 bootstrap 发展，在达成 10 万美元 ARR 之后仅仅用了 9 个月的时间就将收入扩展到了 200 万美元 ARR；

Captions 从产品面向大众推出的第一天就必须付费才能使用，已经拥有超过 300 万创作者用户，DAU 超 10 万；

CapCut 早期以免费版著称，但是很快为 Pro 付费设置了许多关键的功能点，根据 Data.ai 在 9 月份的报道，其 App 在 iOS 和 Google Play 上的消费者支出已经超过 1 亿美元。

[Filming Less：AI 时代的视频剪辑产品淘汰赛](https://mp.weixin.qq.com/s/QgR76ARy0WpgPUIAzwdTjg)

01 新一代视频剪辑产品的兴起

02 值得关注的 3 家成长期标的

03 现阶段的竞争胜负手

04 Filming Less 的挑战

### 10

方军 2023/12/01

LLM 的能力领域和领域内的最佳开源 LLM。

白色方框表示领域，蓝色方框代表特定数据集，橙色方框表示开源 LLM。

[[2311.16989] ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?](https://arxiv.org/abs/2311.16989)

### 11

方军 2023/12/02

IBM 的 AI 工作方式报告还不错

IBM-基于 AI 和自动化的增强型工作新时代-通过人机偕行提高绩效-32页

2『已放入资料库「2023011GPT专题资料」。（2023-12-02）』

### 12

方军 2023/12/02

摘：一年前，Stability AI 斩获 1.01 亿美元融资，成为 AI 图像视频领域的超级独角兽，Coatue、Lightspeed Venture 等机构都成为它的投资者。

最近，彭博社的一则新闻让它再次站上风口浪尖。据彭博社报道，Stability AI 管理层面临财务困境，投资者正在向他们施压，公司可能会「卖身」。

消息称，Stability 已与多家公司进行初步接触，Cohere 和 Jasper 都成为潜在的买家。

[又一场 AI“宫斗”要上演？盈利困难、投资人开撕 CEO，Stable Diffusion 背后公司被曝正寻求收购](https://mp.weixin.qq.com/s/fkDJSmOm8dZDXwnCTP1oXQ)

[Pika 的 AI 视频火了！Stability AI 要卖了？哪种AI项目有未来？｜前哨](https://mp.weixin.qq.com/s/4MIGCRUbMAOKbAZyhyzPhA)

### 13

方军 2023/12/02

摘：5. 关于著作权的认定：「原告是直接根据需要对涉案人工智能模型进行相关设置，并最终选定涉案图片的人，涉案图片是基于原告的智力投入直接产生，且体现出了原告的个性化表达，故原告是涉案图片的作者，享有涉案图片的著作权。」

[AI 生成图片著作权侵权第一案判决书](https://mp.weixin.qq.com/s/Wu3-GuFvMJvJKJobqqq7vQ)

### 14

方军 2023/12/02

pika 这个名字起得真好

picasso

如果像 notion 一样用.so 域名

pika.so

### 15

方军 2023/12/02

《世界正逐渐分化为两种人：一种是通过使用 ChatGPT 而变得更优秀、更智慧、更富有的人，另一种则是其他所有人》

网友点评：这东西垂直领域还是不行，我尝试用它完成我的学术垃圾，它干的很好。

但是恰恰是和 BCG 同行的咨询领域，它在我们所在的细分行业表现极其糟糕，并且联网搜寻出的信息都太陈旧，我们组已经全员弃用它了，改用 Factset，Wind 和 Bloomberg。

我的点评：由此可见，社交媒体的文章往往吸引眼球的功能大于实际。当然有一点点价值，但我们需要仔细甄别。媒体比我们更需要眼球，我的这个想法是从新智元持续的夸张表演中感受到的，在我这里，它的信息其实蛮快的，但误导性极大，因此我屏蔽了、采用替代来源。

原文：businessinsider.com/using-ai-like-chatgpt-will-make-you-more-successful-2023-12

作者：Beatrice Nolan，Beatrice Nolan is a reporter on the business news team in the UK.

文章：自从 ChatGPT 面世以来，才短短一年，世界就已经分为两种人：那些利用它领先一步的人，和那些没有使用它的人。

根据估计，这款机器人在一年内吸引了 17 亿用户。在其发布仅两个月，学生们就开始利用这个工具节省时间，甚至用来写作文作弊。

虽然当时有人对此表示担忧，但实际上孩子们只是最早尝试的群体。

越来越多的证据表明，如 ChatGPT 等人工智能工具能够让你在工作中变得更高效、更能干。深思熟虑地在工作中应用 AI，可以快速为你带来晋升或更多机会。

哈佛商学院近期的一项研究分析了当 OpenAI 的 GPT-4 提供给波士顿咨询集团 (BCG) 的 758 名员工时的情况。

研究者发现，使用 GPT-4 进行咨询任务的 BCG 员工比那些没有工具的员工效率明显更高。

得到 AI 协助的咨询师们完成任务的速度提高了 25%，任务量增加了 12%，而且产出的工作质量高出了 40%。但是，这只适用于适合 AI 的任务（并非所有任务都适合 AI）。

AI 带来最大效能提升的是中等水平的员工。

本文的核心观点是，AI 能成为一种免费的、增强工作绩效的工具，这对各种白领、办公室工作者来说尤其如此，而且他们的技术水平高低并不影响这一点。特别值得一提的是，如果公司中其他人还没开始这样利用 AI，那么其影响将更加显著。

01 当前利用 AI 简化工作的最佳方式

有越来越多的迹象显示，AI 在处理行政管理和重复劳动方面表现出色。

据 Business Insider 报道，他们采访了几位使用 ChatGPT 的专业人士，包括一位前招聘人员，他利用 ChatGPT 编制公司和员工名单；一位房地产经纪人，他用它来撰写房源信息；还有一位营销人员，他用它回答客户问题。

他们都表示，把这些耗时但小型的工作交给 AI 工具处理，可以帮助他们节省大量时间。

ChatGPT 能够简化和概述书籍、文章甚至整个研究领域的内容。它能够提供接近人类的反应，帮助你快速撰写电子邮件、文档或反馈。

因此，如果不利用 AI 完成这些任务，那么使用 AI 的同事可能会突然变得更高效、更有价值。

02 开发者称 AI 编程辅助工具使他们的工作效率提高了 55%

在白领阶层普遍还在摸索 AI 如何影响或改变他们的职业生涯时，技术界人士已经走在了前沿。

像 GitHub 的 AI Copilot 这样的工具，已经被证实能显著提升程序员的工作效率。

根据该公司在 2022 年的一项分析，使用 AI Copilot 工具的开发者比那些未使用的开发者快了 55%。

如今，微软已经将基于 GPT-4 的 Copilot 集成到 Office360 中，使员工可以将 AI 集成到电子邮件、Teams 聊天和会议中。这个工具能够处理许多繁杂的任务，比如撰写电子邮件和文档，或者是总结漫长的会议和 Teams 对话，而且它的指令需求出奇地少。

谷歌的 Duet 同样可以为使用不同系统的工作场所提供大量相似的服务。Zoom 和 Salesforce 也推出了他们的 AI 生产力工具。

各种新推出的 AI 工具的广泛可用性意味着现在几乎每个人都有机会使用它们。

企业软件公司 Appian 的创始人兼 CEO Matt Calkins 在接受商业内幕采访时表示，他认为 AI 最大的影响在于提升工作效率。

「客户服务的质量将提升，我们的工作效率将得到提高，同时，当企业数据在决策或行动时刻得以应用时，其准确性和知识水平也将得到显著提升，」他如是说。

「我认为这正是我们应该依赖 AI 去实现的。虽然 AI 不可能创作出莎士比亚级的作品，但它确实能显著提高公司的生产力。因此，这就是我们应当着重关注的方向。」

03 AI 在职场中的应用需谨慎

利用 AI 完成工作，其中不乏需要注意的地方。

这项技术有时会幻想或捏造事实，这已经导致一些员工陷入困境。有些公司也因为版权或数据安全的担忧，对 AI 工具设立了特定的规则。

如果你打算使用 ChatGPT 撰写文档，切记不要泄露公司机密信息，并对其输出内容进行核查。通常，最好把这项技术当作一名实习生，对任何重要或可能危及职业生涯的信息进行双重检查。

从长远来看，未来的情况也值得关注。

随着每个人都在加快 AI 的应用，特别是在行政工作领域，可供分配的工作量可能会减少。自由职业者已经表示他们因为 ChatGPT 这类工具而失去了工作机会。

如果今年展示了什么，那就是 AI 不会消失。想要保持竞争力的工作者可能需要找到与 AI 合作的新方式。

来源：twitter.com/dotey/status/1730854548527259924

### 16

方军 2023/12/03

对《My experience trying to write original, full-length human-sounding articles using Claude AI》的文章翻译和精要提炼

[AI 写作不如意？万字长文深度剖析背后原因](https://mp.weixin.qq.com/s/fkrI7Clk9GXrPvKZE-Ppjw)

[My experience trying to write original, full-length human-sounding articles using Claude AI | I'd Rather Be Writing Blog and API doc course](https://idratherbewriting.com/blog/writing-full-length-articles-with-claude-ai)

### 17

方军 2023/12/03

017 别 TM 读 ChatGPT 编写的摘要

标题用来一个较为强烈的词语，真情实感。

这几天有个体会，读一本书，真读下来发现，万维钢写得那么好的推荐序，其实对比原文都是降级很多的。

他对内容理解得很好，也可作为我们读这本书的诱惑，以及最开始的阶梯。

但是，当我们开始读了之后，要干的第一件事就是，把这个阶梯扔掉。

而再说 ChatGPT 或任何 AI 生成的摘要，就一个具体问题而言，它们又是万维钢序言的大幅度降级。它们能起的作用，不过是在浏览时告诉我们，它的大概能可能是什么。

（同样地，所有的音频图书解读，都可以说是大幅度的降级。名家做的，其实也一般。有很多是很差的人做的。还有网络名人做的是故意大幅降级迎合。）

所以，AI 来了之后，它给阅读带来的变化应该是采用杠铃策略。早就读不过来了，AI 让我们读更多也是假象。杠铃策略是，读真正好的。

别信各种形式生成的摘要，不管是人类智能，还是人工智能。它们是检索工具，不是阅读替代。

即便是万维钢那么好的也不行。补充，为什么说万维钢的很好，第一，他真心认真读了，你知道很多解读书的人其实没（认真）读吗？你知道 AI 其实也没认真「读」吗？第二，他没迎合，读了什么、想了什么，就大体上写什么。第三，他善于把深奥的知识解释出来。

相关：https://t.zsxq.com/14GkcxUXg

AI 使用感悟#

### 18

方军 2023/12/03

马斯克：将在未来一周左右向 X 平台的 Premium + 用户开放 Grok 的访问权限】据财联社 12 月 3 日报道，马斯克 12 月 3 日在社交平台 X 上发文称，将在未来一周左右向 X 平台的 Premium + 用户开放 Grok 的访问权限，优先顺序取决于订阅的时间。

据悉，Grok 可以「实时访问」该平台上的所有信息，还可以回答大多数其他 AI 系统拒绝回答的尖锐问题，甚至就如何提问给出建议。

---

这个系统有两个模式：

regular mode

fun mode

fun mode 看着蛮有意思的，看马斯克播客访谈时他们还专门说起这个话题。

新产品动态 #

### 19

方军 2023/12/03

《慢燃 AI：当增强而非自动化是真正的威胁》

这篇文章的观点还是蛮有启发的

Choudary 写的，知名的平台经济研究者

他的一个核心观点是，AI 给工作带来的增强，实际上让工作商品化，最后的后果就是给一个人工作带来的危险。

这个逻辑说得通的。

---

链锯的真正长期影响是将伐木作为一项工作商品化。

由于任何人现在都可以成为一个伐木工人，这个工作的技能溢价能力已经被侵蚀。随着潜在伐木工人市场的扩大，工资稳定下来，高技能的伐木工人失去了收取溢价的能力。是的，短期内伐木的增加也带动了下游的需求增加，但这些影响很快就稳定下来，伐木工作变成了一个商品化的工作。

当技术增强熟练工作并使历史上技能较低的工人能够与历史上技能较高的工人表现相当时，这种增强使工人更具替代性，并最终使工作变得普通化。

（由 GPT 自动翻译，未编辑）

---

附件为中英文对照，同样未编辑。

文章主要是阐述他在另一份报告中的观点，见所附文件 2。

看文后预告，他应该还会写两个部分：

这是关于 Gen AI 时代工作未来的三部分评论的第一部分。

第二部分 - Uber 司机时代的 AI - 着眼于 Gen AI 对绝大多数知识工作者的影响。

第三部分 - Taylor Swift 时代的 Gen AI - 着眼于 Gen AI 对少数创作者的影响，他们将从中获益最多。

会持续跟踪一下这几篇。

### 20

方军 2023/12/03

爱思唯尔还把生成式 AI 技术加入到他们的海量数据中，推出了 Scopus AI、ClinicalKey AI，以及支持审稿人推荐、文章诚信核查等学术出版环节的人工智能工具。

[这家被称为技术公司的出版巨头，如何让 AI 成为可信赖的工具？](https://mp.weixin.qq.com/s/fxCcuJib8Qe931xuYNmxjg)

### 21

方军 2023/12/04

AI 扩展的思维

读了《思考如何超越思考》（The Extended Mind）这本书，按我的理解，作为科学作家的作者安妮·墨菲·保罗将三类认知研究串起来、并借用认知哲学家安迪·克拉克的观点形成一个「扩展思维」（extended mind）这个新观点。

重新表述她的基本观点：思考不只发生在大脑中，而是也发生在大脑的外部，具体而言就是三种认知：

具身认知（embodied coginition）

情境认知（situated cogintion）

分布式认知（distributed coginition）

关于第三个，第三个我觉得这个通俗化一下用「群体认知」可能更好，但心理学领域估计群体代表另外的意思，所以有了这个词。

这本书看得过瘾又不过瘾，不过瘾是因为，虽然它强调要不只聚焦于我们的大脑本身，但是可能出于心理学 / 认知科学研究的约束，又还是重点在讨论三种外部认知资源如何影响我们的大脑思考，因而并未真的超越大脑。

她梳理的这个框架蛮有意思，我就用它来从实用角度出发，绘制一个「数字时代的扩展思维」出来。（我这里就不受限于认知科学 / 心理学的专业词汇了，实用出发怎么容易懂怎么说。）

---

A. 输入 - 处理 - 输出

我们的大脑是个处理机制，它接受输入，进行处理，形成输出。

扩展思维的看法是，我们的大脑还有三个外部可用的资源：具身认知、情境认知、群体认知。

我们再外扩一下的看法是：

1、三个可用资源也有数字化的部分。

2、同时，我们的外部工具也数字化了，远不是书纸笔那么简单。

---

B. 数字化的三种外部认知资源

如果将手机、电脑、网络也看成我们的身体一部分，那么，我们可以具身认知、情境认知、群体认知拓展过去。

手机其实已经是我们的器官了，这是为什么我们要花那么大力气去抗拒它，定时关掉它。

屏幕的情境给我们带来的冲击比环境还大，屏幕造成沉浸感，实际超过实体环境。

群体认知则更更不用多说了，我们随时可以借用社交网络的智慧，只要愿意，我们都有机会身处最牛的群体之中。

---

C. 外部工具，主要是数字化的

安妮在书中引述克拉克 2019 年的一个研究，他们研究的主题是：「使用外部物体、道具和辅助工具解决复杂问题的能力」。

我们现在外部的工具可真是太强了。

笔记工具。以我个人而言，除了一本年历和工作时手边乱涂乱画的 A4 纸之外，所有的笔记都是电子化的。（另，作为星主，我把星球更多地视为一种笔记工具。）

输出工具。相信没人用手写了吧，都是电脑。当然，对那些习惯口头表达的人，他们也会直播、网络会议、录视频等等。

AI。这一年来最大的变化无疑是 AI，AI 成为外部工具中最大的变量。外部 AI 模型很强大，但这儿我们的问题是，我们自己能将多少纳入内部来使用？

一点补充讨论：Indigo 之前也画了一个外脑，他也是实用出发、并强调 AI。但我不是很赞同的一点是，他认为笔记很重要，因而引用了第二大脑笔记的 PARA 笔记方法，他最近还推了一个笔记产品。我个人的体会是，虽然记录很多笔记，我其实真心在输出时不用笔记，我会持续修改笔记以改进想法，但输出时不查看与重用笔记。

### 22

方军 2023/12/04

这个网友蛮会偷懒的：

一个小小的功能：把一个包含表单的 PDF 文档，导出成 PNG 图片，然后传给 GPT-4V 的 API，然后让 GPT 生成一段 Google 表单对应的 JSON 数据，再调用 Google Form 的 API 生成表单。

PDF → Google Forms.

(PDF → png → gpt4-vision → google forms)

twitter.com/krystof_rehacek/status/1731280672990322879

twitter.com/dotey/status/1731488081180577800

### 23

方军 2023/12/04

专业工作者个人使用 AI 的两个教训

写于 2023.12.4，为 AIGCxChina 的模仿《巨人的工具》、《巨人的方法》问题系列写的，我其实犹豫好久，巨人，当然我知道那两本书里面访谈的人也不是巨人。但为了对冲巨人，我写了两个教训。我觉得写得非常棒得意。

方军，科技作家，AI 技术专家，中国好书奖获得者

生成式 AI 会如何改变「我」？以 ChatGPT 为代表的大语言模型应用兴起以来，我们都深入地去读论文、做研发、推应用。每个企业、每个产业都有与 AI 相关的机会。但我觉得，或许对他人有用的是我们在摸索着用 AI 的过程中得到的教训。

为什么说是教训呢？AI 必将改变我们这些主要靠运用专业知识与技能的人的工作与生活，但是，有两个关键问题迄今都没有 100% 信心的答案：

1、它会带来何种改变？

2、我们如何应对？

因此，我愿意把学到的都看成是教训。我也倾向于把接触到的所谓「AI 成功经验」都理解为类似于教训，对自己坦诚的人都知道，我们不断看到自己刚总结的经验需要升级迭代。

这里分享我作为专业工作者个人用 AI 的两个教训。

第一个教训：要接受 AI 一定有幻觉，把这个缺点当成必须接受的一部分。

对容易被「AI 万能」的喧嚣声音影响的人，他们容易落入的首个陷阱是轻信 AI 的回答，因为其表达方式是权威的。

我们这些人轻松地跳过了这首个陷阱，因为我们知道 AI 有所谓幻觉（hallucination）：它会自信地给出错误甚至虚假的信息。但是，我们很快掉入一个与之相邻的陷阱：我们想尽办法去减少幻觉对我们使用者的影响。从原理上讲，幻觉是无法彻底消除的，要生成，就要放开回答须严格来自已有文本的束缚。

到目前为止，减少幻觉的路径大概有这么几条（这里以极简的例子来解说）：

1、优化提示语，比如说「请确保信息正确无误」。我自己比较喜欢用一个提示语模板，它来自 Elvis Saravia，我们又在其基础上扩展并命名为「ICDO」，它包括四个部分：指令（Instructor）、上下文 (Context)、输入数据 (input Data)、输出要求 (Output indicator)。

2、通过多轮处理来优化回答正确率。比如收到回答后，再次让同一模型或另一模型进行评估。

3、引入知识库，采用所谓检索增强生成（RAG）。也就是，提问时同时提供资料文本，要求 AI 按严格按资料回答。

也有更高阶的技巧，比如所谓的假设性文档嵌入（HyDE）。它的做法是，用问题先让 AI 给出一个可能有错的回答（即假设性回答），然后我们拿这个回答去资料库里面匹配相关资料，然后让 AI 再按资料回答，这样，我们可以得到更准确的回答。

4、微调模型。现在开源大模型和仅提供 API 的闭源大模型都可以微调。在一个具体的领域内，我们提供数百到数万个正确的「问题-回答」组合去微调训练，试图让大模型能更好地回答。

你看我如数家珍，就能知道我们在这些方面上「浪费」了多少时间精力。

结论是什么？简单说，结论是：如果我想要 90 分的答案，也就是 90% 的时候答案绝对正确。那么，对不起，不可能！90 分的要求可不高，日常工作中我们要的可是 99%、甚至 99.9% 的正确率。

我们自然还会接着以上道路继续探索，也相信 AI 模型的回答能力在某一天会接近完全正确。但是，就像现在使用 AI 生成图像时我们无法完全掌控、必须接受生成图的随机性一样，我们其实也不妨想办法接纳、甚至利用大语言模型类 AI 的幻觉。

接纳的方法很简单，我们可以要求 AI 生成的回答必须经过人的处理，如：仅用它作为参考想法，进行严格事实核查，由人进行最终处理等。

利用则比较有意思。我是从沃顿商学院的教育专家 Ethan Mollic 的教育场景提示语案例中学到的，并且也进一步延展它并实际使用。这是一次思维的大翻转。

正向：AI 的回答是对的。它现在的回答不对，那么竭尽全力确保正确。

逆向：AI 的回答是有错的。在商业场景比如客服场景中，错误是不可接受的。但在教育场景中，错误是可以利用的。

在教育场景中，我们可以对作为使用者的学生用户说，现在你们面对的是一个可能回答错的 AI 机器人（嗯，我们故意让它犯错的）。你的任务是，找出错误，纠正它的错误。教育研究早就证明，识别错误、纠正错误，能帮学生更快地学习。

所有人都试图把 AI 变成「导师」，我们这样做则是把 AI 变成「翻转导师」，这就像「翻转课堂」之于「课堂」。

AI 现在的能力有限、可能出错则不是问题了，它的缺陷某种程度上变成了「特性」—— 错误是学生的翻转角色教别人的机会。当然，随着 AI 能力的提升，AI 的正确率会提高。那当然好。但我们仍然可以用巧妙的提示语来让 AI 回答按固定的概率犯错，因而「翻转导师」的玩法可以一直用下去。

第二个教训：抛弃批处理界面，拥抱交互式对话界面。

很多普通人接触到的 AI 是对话式的聊天机器人，他们很开心可以跟 AI 反复说话，提出问题，给出命令，得到自己想要的。他们不太会掉到我接下来要说的陷阱里面去。但是，看了我如何掉坑里的过程，也可以理解为什么我现在会特别支持对话式的交互界面。

我们自认为是讲求高效运用技术工具的人，由于工作的原因又掌握各种技术诀窍，我们哪能就这么用 AI 工具？要让它神奇地完成任务。

随之而来的折腾之路大体上这样三条：

1、采用非常复杂的提示语。

大模型能力很强，能理解逻辑复杂的提示语。我们也能够写出高度结构化的提示语，比方说，这件事你按 A、B、C、D、E 五步给我做。

这有点像给有潜力的员工派任务，只要指令清楚，就能拿到结果。

可惜，我们跟这些有潜力的员工还没磨合好。我们讲得不完全清楚时，他不会追问，你的意思是这样吗？结果是，两方说岔了，他越努力越高效，结果离我们的期待偏得越远。

好，我们可以跟他慢慢磨合。用大模型时，这就相当于反复地改提示语、测试提示语。

很快我们发现，问题在于，一次把指令全给出去，我们没法监控中间结果，并进行调整。因此，我们接着走向下一条路。

题外话一句，让 AI 回答问题不是一些人理解的一次性抛接球：写一个提问语，得到一个好回答。而是一个循环迭代的过程，是一次次的抛接球训练：我们写一个提示语，得到一个不好的回答，我们重写提示语，回答变得好一点，如此多轮迭代，直到我们拿到满意的结果。

2、编写 AI 应用程序。

用专业术语说，编写 AI 应用程序实际上是对 AI 模型的调用进行编排（orchestration）。

通俗地说就是，我们编写一些程序来调用大模型的能力，比如这样做：第一步模型这么做，结果回来后我们靠程序判断一下，让它去做第二步，以此类推。

这样做了之后，我们就可以很方便地监控和调整中间结果了。模型的反应开始变得较为「聪明」。这颇像在工作中，我们通过内部的标准操作流程（SOP）和管理流程让聪明的新员工变成优秀员工。

以上这两个其实背后都是「批处理思维」，不要让我一次次说，要做到指令发出去，次次见效。既然有效了，那我们就进一步推广开来，那就是走上第三种路径。

3、在一个单一任务上采用提示语、编排、微调结合的批处理，准备大规模运用。

简言之，就是把各种手段都用上，让大模型在单一任务上为我所用。具体的做法因具体任务而异，这里不详述。让我快进到掉进去坑以及如何爬出坑。

编写 AI 应用并用于个人任务，其实与企业投资开发一个应用相似。投入了大量的时间精力进行开发，进行一些测试，进一步优化，觉得满意了，开始大规模运用。

最初的 100 个结果不错。最初的 500 个结果也看着不错。一两天时间这个过程就过去了。

但很快发现，糟糕，这些结果有瑕疵。但是，当我返回去调整时，我发现，常规程序和 AI 模型功能混在一起应用是无法有效调试的。一个原因是，每次模型给的结果不会 100% 一样。（现在有了一点优化，OpenAI 的模型可以接受一个种子参数，如果输入一致、种子参数一致，可以得到完全一致的结果。但是，我们的输入不会一致啊。）

这么一圈折腾下来之后，我才发现，聊天机器人的交互式对话是非常巧妙的。现有，采取交互式对话，更容易拿到理想的结果。

弯路可能是必要的。不是在弯路上亲身感受到，不会真的影响我们的后续行动。

我因而也对应用界面的演变有了更深的感悟。界面的演变历程是这样的：第一阶段是历史，批处理（如 dos 或 linux 命令行），第二阶段是当下，图形用户界面（如 Windows、iOS 和淘宝 APP），第三阶段是未来，对话式交互界面（ChatGPT 之类）。我们再考虑做应用时，也尽量走对话式交互界面。

同时，我也感受到现阶段要利用 AI 的能力，要坚持所谓人在循环中（Human in the loop）的设计理念，让人在其中参与，而不是想尽一切办法试图用 AI 将人替代、把人解放出来。换个角度看，现在的人其实也很想参与和 AI 一起干活的这个过程，而不是被排除在外。

以上，就是我作为专业工作者在 2023 年深度个人运用 AI 的两个教训。同时我有一个温馨提示是，结论不重要，我上面讨论中的结论可能很快在某个时刻又被发现错了、需要升级了，真正重要的是掉坑里爬上来的过程。

就普通人如何入门 AI，我喜欢给的一个建议是，找到一个小小的点开始用起来。用的过程中就明白了。听人说多少遍神奇，都不如自己亲身在小小的点用起来。比方说，我说话快容易写错字，现在写什么略长的都让 AI 检查一遍。这样，至少我每天在一个非常具体的问题上能像刷牙那样多次使用 AI。建议你也尝试着找个像刷牙一样的地方把 AI 用起来。

### 24

方军 2023/12/04

ChatGPT-like 系统是如何工作的？

How does ChatGPT - like system work?

by bytebytego，它的图一直很赞。

ChatGPT 类系统是如何运作的呢？

让我们通过下面的图解来探索它的运作机制。整个过程主要分为两大部分。

1、训练过程。要打造一个 ChatGPT 模型，我们需要经历两个关键阶段：

预训练：在这一阶段，我们会对一个 GPT 模型（一种仅包含解码器的 Transformer）进行训练，使用大量的互联网数据。我们的目标是培养出一个能够基于已有的句子预测出下一个词汇的模型，这个预测不仅在语法上要正确，而且在语义上要与互联网上的内容相吻合。预训练阶段完成后，模型能够补全给定的句子，但还不足以应对提问。

微调：这一阶段是一个三步骤的过程，目的是将预训练好的模型转化为一个能够回答问题的 ChatGPT 模型：

1）收集训练用的数据（包括问题和答案），并在这些数据上对预训练模型进行微调。模型学习如何根据问题生成与训练数据类似的答案。

2）进一步收集数据（问题和多个答案），并训练一个奖励模型，用于将这些答案按照相关性进行排序，从最相关到最不相关。

3）运用强化学习（PPO 优化）对模型进行微调，以提高模型回答问题的准确性。

2、回答问题。

步骤 1：用户提出一个完整的问题，例如「解释一下分类算法是怎么工作的」。

步骤 2：这个问题首先被送往内容审核组件。该组件确保问题不违反安全准则，过滤掉不恰当的问题。

步骤 3-4：如果问题通过内容审核，它就会被送到 ChatGPT 模型处理。如果未通过审核，则直接生成模板式的回答。

步骤 5-6：模型生成回答后，这个回答再次经过内容审核组件的检查。这一步骤确保所生成的回答是安全的、无害的、无偏见的等。

步骤 7：如果回答通过了内容审核，它就会展现给用户。如果没有通过审核，系统则提供一个模板化的答案给用户。

准确请看英文原文：

twitter.com/bytebytego/status/1731572801231020424

### 25

方军 2023/12/04

一种观点：大语言模型是压缩。马毅老师似乎是坚定的这种观点。

Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models

Google DeepMind November 3, 2023

[[2311.00871] Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models](https://arxiv.org/abs/2311.00871)

摘（为网友观点，非论文摘录）：这篇论文指出，大模型（LLM）对上下文的理解能力（ICL）不是一种通用的泛化能力，而是对预训练数据的检索能力。

用通俗的话说就是，不是一个智能理解了你的问题，而是它从数据集里搜索到了关联数据。

我现在愈发认为 LLM 只是一种高效的信息压缩算法，它的特色在于解压的效果也依赖于输入（prompt）。而 ICL 其实类似于心理学的「锚定效应」，所谓的 prompt engineering 只是在试图让 LLM 的答复锚定到更高质量的预训练数据集上。

至于能不能通向 AGI，对既有方法的灵活运用也是能够导向创新的，但是目前 LLM 的问题在于知识质量鱼龙混杂，需要人类高效 prompt 才能引出高效的答复，这可能预示着它只能扮演一个 copilot 角色，而且它的能力和人类搭档的能力是成正比的。

欧阳：这种观点其实特别扯淡，哈哈。一看就是科学哲学训练不好。

2023-12-04 22:42

方军回复欧阳：哈哈，马老师的论文和观点也至今未能说服我。

2023-12-04 22:42

欧阳回复方军：其实他这类论证犯了很明显的逻辑错误，最后又变成去辩论：压缩，究竟是什么？

2023-12-04 22:45

### 26

方军 2023/12/05

互联网上的话语和吸引人的注意力的方式蛮有意思的

意识流记录下想到的。

由昨天「大语言模型就是压缩」说起来的。其实就吸引人而言，这个标题都不如好几篇经典论文标题：

注意力是你需要的一切。

预训练大语言模型是快速学习者。

阳志平老师指出「大语言模型就是压缩」这个话的问题是：

他这类论证犯了很明显的逻辑错误，最后又变成去辩论：压缩，究竟是什么？

我没从这个角度想过，我觉得压缩是众多试图解读的类比之一，类比本来就不严肃了，而压缩略相当于 ZIP 压缩吧或 JPG 压缩吧。

在安人书院星球里面，阳老师发文批评一系列标题：

一般来说，我们在互联网上看文章，读到这类标题或者观点：

大语言模型就是压缩。

学习的本质，就是极致重复

真正的高手，都在刻意练习。

顶尖销售，都是谈判高手

一个优秀的管理者，必须经历 5 次蜕变

就不用往下看了。这类文章的作者应该补习：论证分析。

基础的哲学思辨没过，逻辑学没学好。

有朋友讨论，我跟了几句：

A：粗看以为说的这些观点有问题，细看原来是逻辑不对，上述 5 个标题包含了「就是」、「都在」「都是」「必须」。

我：我以为这些观点也是有问题的。…… 极致重复这个就更有意思了，我调查过这篇网络流行文章，能找出 100 个槽点出来。

具体来说，「学习的本质」，什么东西叫本质？我以为，除非在一些有定论的领域，否则没有场景，什么本质真的很难说。以前一个好朋友讨论时给建议说的，不要轻易说本质（理由大概是我们就讨论一个具体问题，远不是讨论本质）。

我们都不说的。但在社交网络上观察可以看到中文社交网络尤其喜欢「本质」。（英文不那么明显。）

—— 其实，「本质」也是一个高阶问题了。普通人也就是看到本质会被吸引点击去，只有那些自认为懂了点什么的人才会用「本质」这个词。普通人不会犯这个错。

还有一种有意思的现象是泛泛的标题：

AIGC 对程序员的影响

在这一期的播客中，我们将探讨 AI 技术，尤其是 ChatGPT 和 AIGC，如何改变软件工程的面貌。我们会聚焦于 AI 模型的不稳定性给开发者带来的挑战，以及它们在提高开发效率和创新方面的潜力。

社交网络上这种吸引眼球的标题尤其多，大众容易被吸引吧。它没有夸张，但真的好泛，但大概泛才能吸引很多人吧。—— 但是，真的能吸引到很多人吗？

这又是社交网络吸引人注意力钩子的另一种方式了：是极端的，吸引接受这个观点的人，排除不喜欢这个观点的人；还是泛泛的，试图吸引很多人？

社交网络还有一种吸引人注意力的方式是：比如官方媒体都很喜欢用的，「定了！」「这一国」，也就是不遵循基本的经典媒体规范，而把内容藏起来，以吸引人去点开。—— 我自己的体会是，尽量避免点开了，但极其偶尔的情况下还是会点。难。

小红书的话语也蛮讨厌的，什么「天哪」「救命」。当然，公平而言，小红书还是蛮正向的，像个健康的人，微博是街头混子，游荡久了什么都指指点点，知乎很 Nerd （这是我的感受，我看不到它那些刚编的故事）。

其实我觉得吸引注意力没啥，但要以伎俩高超一点。最近看 Youtube 较多，看了一些脱口秀演员（他们似乎自称 comedian 喜剧演员），他们的街头视频的确很有喜剧演员的特点，一个感慨就是，他们的街头表演、请到的嘉宾（嘉宾的表演水平）、拍摄与制作水平都好高超。昨天看的一个请到林书豪的。嘉宾表演极其卖力到位。

方军：有时候语言是一种逃避和自我保护吧。

比如有个人特别喜欢讲道与术.

我理解一是逃避他其实了解蛮浅、又出于公司角色对外什么都要说一点的。他的浅是了解东西经常不那么具体，面上是知道的，观点也不能说有错。

二是同样也是出于公司角色他不能讲真话怕得罪人，观点鲜明了容易被证明错，容易被对方挑战，他说说空话容易逃避。

2023-12-05 11:43

方军：我也很逃避，在星球写什么都很快，写正式文章会拖延，社交媒体文章则抗拒到写不来。

2023-12-05 12:39

### 27

方军 2023/12/05

Ethan Mollick 的一个观点：

为什么我对「与数据对话」的 AI 应用如此怀疑。

如下为 GPT 翻译，准确请查看原文：

twitter.com/emollick/status/1731749755909181767

后面有很多精彩的互动讨论，及文章：

[The Best Available Human Standard - by Ethan Mollick](https://www.oneusefulthing.org/p/the-best-available-human-standard)

这是 Google NotebookLM，一个很酷的工具，可以让你在数据源上使用 AI。尽管文档搜索能够检索到正确的信息（毕竟这是 Google），但 LLM 的答案有些微妙的幻觉。

注意图中的错误！

更好的 AI 和技术将改善这一点，但重要的是要意识到这种模型（我经常看到公司实施的一种非常常见的模型）存在重大潜在问题。

在工作中有很多更有效的使用 AI 的方式，不一定非得是这种方式。

是的，PaLM-2 比其他模型更容易产生幻觉。GPT-4 也是如此。如果你想要准确的重要数字和细节，RAG 还不能满足你的需求。

如果人们了解 AI 的能力，并且能够容忍人类的错误率，这些方法可能是可以接受的，但是使用机器的人并不希望出现不准确的情况。当虚假信息被制造出来时，他们可能会感到震惊。

网友讨论：

Michele Zanini

100% 同意。对于数据分析（以前是代码解释器）也是如此。幻觉的微妙之处尤其具有挑战性，因为需要付出很多努力才能揭示出来。

LLMs ≠ 数据库。用于探索和模式识别，但在需要精确性时要避免使用。

Olaf Lenzmann

建立一个商业 RAG 产品。用户抱怨因缺乏数据而拒绝回答，或者根据证据给出略微离题的回复：「失败」。几乎没有人似乎注意到错误的答案。

2024 年预计会出现普遍的 RAG 幻灭谷。

Brandon Roberts

这就是为什么能够快速验证 LLM 的输出非常重要。这个例子突显了在复杂问题和密集的源材料中可能出现的问题。

M.A. Buth

我可能会惹恼一些人，但目前的 GPT 非常糟糕。如果你想处理科学或工程数据，结果必须是准确、真实和可靠的，我得到了很多垃圾回复，这是不可接受的。质量已经下降，这是我观察到的。

Rana Taimur Ahmad

除了通常的 LLM 问题之外，与数据交流的问题还在于，任何有意义的产品、决策或行动都需要远远超出仅仅简单的问题或问题链来自数据源

Sebastian Kovács

这就是为什么简单的 top-k 检索无法完成任务。开箱即用的 RAG 解决方案可以用来玩耍，但在实际应用中缺乏准确性。但是，如果我们定制每个步骤，我们可以对输出的质量有很好的控制。

Richard Gaskin

我们能否回到只称呼错误为「错误」的方式？

错误地将它们标记为「幻觉」会使人们对再生系统的作用过高估计。

Karl Smith

这是一个我认为幻觉一词具有误导性的案例。事实上，这是一个错误，一个非常人性化的错误。人类和 LLM 使用概率来构建「事实」。

我们不会将人类的这种行为描述为幻觉。事实上，幻觉一词用于不太可能的「事实」。

twitter.com/emollick/status/1731749755909181767

Ethan Mollic 的推后面总有很多精彩的互动，经常远超原文。包括他的博客也是一样。

### 28

方军 2023/12/05

用可视化的方式连接各种 AI 模型、创建独特的 AI 工具

摘（倪爽）：5 月 20 日，takomo.ai 有一个独特的模块化系统，可以用视觉的方式，把 GPT、Whisper、Stable Diffusion、BLIP-2 等 AI 模型连接成管道 / 流水线，逐级设置、逐级处理，最终获得结果并以 API 形式部署。

操作非常直观，功能也足够强大

这个工具实现了 AI 领域的 no code、low code。

开发者可能习惯了代码、配置文件、表单等等方式来创建 AI 工具，对非程序员而言，拖放和连接 AI 功能模块，应该是创建 AI 工具、解决特定问题的最好工具。

比如写手可以连接几个 ChatGPT…

讨论：

不知道，我觉得特别讨设计师喜欢的炫酷，但我真心用不起来。我喜欢三种：

第一，就是普通界面。

第二，写程序。

第三，专业的配置界面。

这种很尴尬，普通人用不起来，专业人不屑用吧。

twitter.com/nishuang/status/1659773198802493441

新产品动态 #

### 29

方军 2023/12/05

非常喜欢这句话，搞张图放这儿：

了解 AI 哪里有用的

唯一方法是，

去用它。

The only way to figure out how useful AI might be is to use it.

Ethan Mollick

来源：

[The Best Available Human Standard - by Ethan Mollick](https://www.oneusefulthing.org/p/the-best-available-human-standard)

### 30

方军 2023/12/05

能够让大模型推理结果变得更好的基础优化手段已经非常多了，李靖梳理了常见的技术手段和对应的论文：

Zero-shot：

[[2109.01652] Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)

2023057Finetuned Language Models Are Zero-Shot Learners

Few-shot：

[[2005.14165] Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

2023058Language Models are Few-Shot Learners

CoT：

[[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

2023009Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

ToT：

[[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)

2023010Tree of Thoughts: Deliberate Problem Solving with Large Language Models

GoT：

[[2308.09687] Graph of Thoughts: Solving Elaborate Problems with Large Language Models](https://arxiv.org/abs/2308.09687)

2023059Graph of Thoughts: Solving Elaborate Problems with Large Language Models

SC：

[[2203.11171] Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)

2023060Self-Consistency Improves Chain of Thought Reasoning in Language Models

Multi Persona：

[[2307.05300] Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration](https://arxiv.org/abs/2307.05300)

2023061Unleashing Cognitive Synergy in Large Language Models

Least to Most：

[[2205.10625] Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)

2023062Least-to-Most Prompting Enables Complex Reasoning in Large Language Models

Step Back：arxiv.org/abs/2310.06117

ART：

[[2303.09014] ART: Automatic multi-step reasoning and tool-use for large language models](https://arxiv.org/abs/2303.09014)

ReAct：

[[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

2023012ReAct: Synergizing Reasoning and Acting in Language Models

Reflection：

[[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)

2023013Reflexion: Language Agents with Verbal Reinforcement Learning

RAG：

[[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

2023064Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

我觉得缺了 HyDE。

HyDE: 

[[2212.10496] Precise Zero-Shot Dense Retrieval without Relevance Labels](https://arxiv.org/abs/2212.10496)

2023065Precise Zero-Shot Dense Retrieval without Relevance Labels

### 31

方军 2023/12/07

浏览中文社交媒体，主要想看看这个里面的人喜欢看什么 AI。

发现一个有意思的现象，在大众人群里面，好多人斩钉截铁地说，AI agent 就是未来。

理由就是几个名人好像比尔盖茨说过，类似的都是如此。

蛮有意思的，我觉得说的人连 AI agent 是什么都没想过。

公众号一些专业的也喜欢鼓吹，他们倒是知道，因为各种专业图示。但他们没说的是：她们说，这是最重要未来趋势，但他们其实说了几十个都是最重要的。

### 32

方军 2023/12/07

沈阳老师已经走火入魔了啊。

想法是，做法也是。

[第一篇纯 AI 小论文：批判性捧红的多学科分析](https://mp.weixin.qq.com/s/qGiwAy3vZmbhoCGiw4NmAA)

经过共十五次 AI 交互（13 次文本对话、1 次图片对话、1 次 AI 幻觉矫正），经历约三十分钟撰写、十分钟 word 排版删减，清华大学新闻与传播学院沈阳教授团队使用 AI 生成了一篇 8264 字的小论文。

本论文 100% 由 AI 撰写，文章分为理论分析、自反性论证、文献综述、历史案例分析、近期案例、形式化和展望与不足八部分。其中，形式化在一定程度上弥补了以往人文社科学者在数理形式分析上的不足。

这篇论文在分析深度、逻辑严密度、文献阐释、语言风格、叙述细腻度和要点扩展等方面还存在一些不足，这些问题将在下一篇小论文中得到部分改进。

转内：同时，鼓励大家对我们的研究，AI for Research：新文科和新传播，提出宝贵的发展建议。我们将在此基础上进行第二篇 AI 论文的生成，并期待 AI 未来能产出原创性的、超越组合式创新的文章。

我们的终极目标是天人智一，使用 AI 解决人类目前解决不了的一些问题，例如研究意识起源、破解历史悬案、进行 AI 辅助诊疗、大幅度提升生产力以尽可能地解放人类，使每个人都能享受自己的诗和远方。

### 33

方军 2023/12/07

张俊林：Google Gemini 技术报告要点提炼：

1. 技术报告 60 页，没有透漏具体技术细节，大部分是评测，技术报告作者列表包含 9 页内容，超过 700 人，应该接近 OpenAI 的员工总数了吧。

2. Gemini 是几种模态一起联合从头训练的，包括文本、图片、音频、视频等。这与目前通常的多模态做法不太一样，目前的多模态模型一般是使用现成的语言大模型或者经过预训练过的图片模型（比如 CLIP 的图片编码部分），然后利用多模态训练数据在此基础上加上新的网络层训练；如果是几个模态从头开始一起训练，那么按理说应该都遵循 next token prediction 的模式，就应该是 LVM 的那个路子，其它模态的数据打成 token，然后图片、视频等平面数据先转换成比如 16*16=256 个 token，然后搞成一维线性输入，让模型预测 next token，这样就把不同模态在训练阶段统一起来。

3. 技术报告说应该是 Decoder only 的模型结构，针对结构和优化目标做了优化，优化目的是大规模训练的时候的训练和推理的稳定性，所以大结构应该是类似 GPT 的 Decoder-only 预测 next token prediction 的模式。目前支持 32K 上下文。

4. Gemini Nano 包含两个版本：1.8B 面向低端手机，3.25B 面向高端手机。文章说 Nano 首先从大模型蒸馏，然后 4bit 量化。我这里有个问题：为什么不用手机调用 API 的方式调用服务端的最强模型呢？能想到的一个可能的解释是用户隐私，这样手机不用把数据传到云端；另外一个推理成本从云端转移到了手机，能够大量节省推理成本。还有其他原因么？

5. 从硬件描述部分来看，意思是动用了前所未有的 TPU 集群，所以推测 Gemini Ultra 的模型规模应该相当大，猜测如果是 MOE 大概要对标到 GPT 4 到 1.8T 的模型容量，如果是 Dense 模型估计要大于 200B 参数。考虑到引入视频音频多模态数据（当然是来自于 Youtube 了，难道会来自 TikTok 么），所以总数据量 * 模型参数，会是非常巨大的算力要求，技术报告说可以一周或者两周做一次训练。

6. 训练可能分成多个阶段，最后阶段提高了领域数据的混合配比，猜测应该指的是逻辑和数学类的训练数据增加了配比，目前貌似很多这么做的，对于提升模型逻辑能力有直接帮助。

7. 看学科能力测试，技术报告指标有人为拔高的倾向，比如 MMLU，只有 CoT 给 32 个例子 Gemini 才能超过 GPT4，当例子数量减少到 5 个，Gemini ultra 得分 83.7%，不如 GPT 4 得分 86.4%，高于 GPT 3.5 的 70%。从测试具体情况看，gemini ultra 应该是和 GPT4 基本持平或者稍微弱于 GPT 4 的，gemini pro 和 ultra 差距比较大，应该略微强于 GPT 3.5；而且 Llama2 在数学、推理等方面与最好的大模型效果差距非常明显，不同测试指标差距 20 到 40 分之间；

8. 从学科能力测试数据看，目前大模型能力很可能顺序如下：GPT 4 略微强于 Geminni ultra> Claude 2> inflection-2> GPT 3.5= Grok 1 >Llama2。

9. AlphaCode2 是在 Gemini pro 基础上，使用编程竞赛的数据 fine-tune 出来的，效果提升很明显，在编程竞赛上排名超过 85% 的人类选手，之前的 AlphaCode1 超过 50% 的人类选手；

10. Gemini Ultra 在多模态能力方面，在几乎所有测试数据上确实要比 GPT 4V 强一些。

11. 命令理解方面：和 GPT 一样，采用多模态 instruct 数据进行 SFT+RM+RLHF 三阶段，这里的 RM 部分在训练打分模型的时候，采用了加权的多目标优化，三个目标 helpfulness factuality 和 safety，猜测应该是对于某个 prompt，模型生成的结果，按照三个指标各自给了一个排序结果。

一个悲观的结论：

最后多说一句，从 Gemini 能够推断出一个悲观的结论如下：

因为在 GPT 4V 前大多数是文本模型，很多人觉得文本模型缺乏 Grounding，就是文本抽象语义和真实物理对象对应不起来，大模型理解不了物理世界的知识，而视频数据那么多，如果引进了后，大模型不仅能建立起 grounding，更重要的是视频数据蕴含了比文本更多的知识，所以对大模型的知识储备会有极大的增长。这里可能存在误解。

从 Gemini 的效果来看，事实可能并非如此，Gemini 多模态效果不错，它主打多模态，肯定引入了尽量多的视频、图片信息，这一方面说明多种模态联合训练确实有用，但是它的用处主要在于：把文本抽象概念和物理实体形象的对应 Grounding 建立起来了，但是在大模型的世界知识和各种能力储备方面，经过大量视频强化过的 Gemini 甚至可能还比不过只用文本训练的 GPT 4。

这一切指向如下可能：就世界知识含量来说，文本是大模型获取知识的主要来源渠道，视频、图片数据在这方面对于文本的世界知识补充作用微乎其微，视频、图片和文本多模态训练的主要作用是建立起实体概念及知识抽象表述和外在物理形象绑定建立 grounding 而已。除此外，无需对类似视频等多模态数具有更高的期望。

本质上，目前多模态大模型效果还不错，是大模型把从文本中学到的世界知识和逻辑能力，经过 grounding 绑定到实体外在形象后，在多模态场景下语言模型把丰富的世界知识迁移给了多模态模型，是文本模型带着多模态在飞，而不是反过来。

来源：weibo.com/1064649941/Nw0MZxMP2

### 34

方军 2023/12/07

摘：关于 AI 创业，我的一些不成熟的思考是：

- AI 的助力使得创业团队可以更小，3-5 人就能启动并做到一定规模。

- 创业者最先能成功的可能是在 ToProfessional 或 To Small business 领域的提效产品与服务，而 ToLBusiness 领域的一些单点工具也有机会

- ToC 消费端的体验需要在图形音频、Al 拟人程度上进一步改进。用户短期新鲜感周期的突破和较大规模明确付费意愿的达成可能还会滞后。

- To Large B 企业应用的 AI 介入受企业业务复杂度和内部利益归属影响，大企业的试错保守态度也会使这一进程滞后。

### 35

方军 2023/12/07

讨论翻译与 AI 翻译

刘群：【在 Google，我们对「不可能」持有一种健康的无视态度。】你看得懂这句中文吗？反正我一次看文字版的时候看到这句话的时候是懵的，完全不知道这句话是想表达什么意思。直到我看了视频才明白，回头再看这句中文，似乎字面上翻译也完全没有问题。

做过翻译的人会知道经常有这种现象，翻译的时候觉得意思完全翻译过来了，语法语义都没有问题，但一个没有看过英文原文的人，看到中文译文就完全一头雾水，不知道在说什么。这种情况发生的原因，通常是句子表达的意思在中文语境下比较罕见，或者不符合中文思维习惯。这种情况下翻译的时候就需要换一种更符合中文思维习惯的表达方式，或者加一些解释。

翻译中这种问题其实挺常见的，【而且译者经常自己都意识不到有这样的问题，因为他是看过原文的，不觉得译文有什么错误。】这也是为什么一些翻译著作读起来感觉非常费劲，一读原著就恍然大悟的原因。这也是高水平译者和低水平译者的差距所在。在这一点上，AI 翻译恐怕还远远达不到高水平人类译者的水平。

回到开头的问题，你觉得这句话怎么翻译，中文才更好理解呢？

@酱酱好老板：在谷歌，我们很自然地认为一切皆有可能。

方军：我这几天就有一个感慨，一本书把 coaching 翻译为带人，真是神来之笔，整本的关键词就是 coach，而它的语境里面也的确是带人的意思。

2023-12-07 20:42

方军：技术圈就不要说了，全是黑话。我对纯技术黑话的忍受度还蛮高的，但对敏捷那套的忍受度特别低。

有个敏捷的人写了个提示语指南，他哪来的那么多理论？

我比较自在，反正我不搞学术，我尽量说人话。（当然我经常被好朋友批评讲话 high context。）

2023-12-07 20:46

方军：刘群老师说的这个现象，也正好说明我为何喜欢看中英对照，因为中文看得更快，而对照可以准确，就兼顾了。以前 deepl 不能段落对照，极度阻碍速度。

### 36

方军 2023/12/07

摘：陈仲凯：今天备课的时候，看到一个词叫做「认知卸载」（cognitive offloading）。我个人来举几个例子：我们不用记住一家门店的位置，使用谷歌地图就行了；我们不用纠结自己没记住某个语法细节，把自己的句子扔给 ChatGPT，都会给你改好；…… 这恰恰是我近来在思考的一个问题：传统的低级别学习正在被快速淘汰。

我在美国这边念书的时候，很多专业术语我是脱口而出、表达流畅；我在阅读文献的时候，也完全依靠自己的「实力」全英文阅读论文、浏览全英文视频。相比之下，很多同学则使用翻译软件、和音频转录软件轻松搞定。我记得六年前在伦敦，老师要求我们写一个转录的作业，一堆同学手忙脚乱搞死个人，当时很多人还觉得「成长很多」。但是，今年在波士顿，直接转录软件都给分分钟搞定了。此外，有教授明确提到：可以使用人工智能，来转换字幕翻译。

表面上，我好像是英语能力强，但实际上用处并不大（至于秀什么好听口音的更是离谱）。重点是要善用工具。结合「认知卸载」的这个主题，很多人真的不必纠结死记硬背的知识点。

我观察到我们的中小学乃至大学生还在练字，无论是汉字还是英语（主要是为了考试的时候清晰辨认），但这种所谓的能力，在托福雅思等考试是不存在的。它们全部电脑答卷，没有书法发挥的空间。除了考试以外，放在工作中，我们几乎都是无纸化办公。你就是做个普通的外卖骑手，订单也都是打印的、用不上你写字对吧？

说起来可能是一个残酷的体系竞争：你处在一个为上世纪设置的工业化体系的教育，还是处在一个为本世纪设置的人工智能体系的教育。

### 37

方军 2023/12/07

Ethan Mollick 这个「最佳可用人类（BAH）标准」很赞啊。

Given this confusion, I would like to propose a pragmatic way to consider when AI might be helpful, called Best Available Human (BAH) standard. The standard asks the following question: would the best available AI in a particular moment, in a particular place, do a better job solving a problem than the best available human that is actually able to help in a particular situation? I suspect there are many use cases where BAH is clarifying, for better and worse.

鉴于这种困惑，我想提出一种实用的方式来考虑 AI 何时可能有帮助，称为最佳可用人类（BAH）标准。该标准提出以下问题：在特定的时刻、特定的地点，最佳可用的 AI 是否能比最佳可用的人类更好地解决问题，而这个人类实际上能够在特定情况下提供帮助？我怀疑有很多应用案例可以通过 BAH 来澄清，无论是好是坏。

[The Best Available Human Standard - by Ethan Mollick](https://www.oneusefulthing.org/p/the-best-available-human-standard)

我还不知道这个怎么解读和应用更好，不过感觉起来跟吴恩达说的「新毕业大学生测试」（fresh college gradute test）有一拼。

我把笔记里面的中英文对照附在 PDF 里面。

### 38

方军 2023/12/08

Paul Graham:

How many people make it through each day of Replit's online 100 Days of Code tutorial.

有多少人完成 Replit 在线的 100 天编码教程。

### 39

方军 2023/12/10

欧盟通过全球首个人工智能监管法案

- 对 AI 应用进行风险分类，特别关注「高风险」应用，如自动驾驶汽车和医疗设备。

- 禁止了企业从互联网或安全录像中抓取面部数据

- 违规处罚：违反法案的公司可能面临其全球收入 7% 的罚款。

- 对基础模型的限制：法案对捕获互联网数据以支持消费产品的大基础语言模型施加了限制。

- 对开源模型的豁免：法案为开源模型提供了广泛的豁免，这些模型可以自由地被开发人员更改和使用。

AI 法案的主要内容：

1、高影响通用 AI 系统的义务：为满足某些标准的「高影响」通用 AI（GPAI）系统设立了义务，包括风险评估、对抗测试、事故报告等。

2、透明度要求：要求这些系统创建技术文档和关于训练内容的「详细摘要」。

3、公民投诉权利：公民有权对影响其权利的「高风险」系统提出投诉并获得解释。

4、罚款框架：违反规则的公司将面临不同程度的罚款，根据违规行为和公司规模，罚款范围从 3500 万欧元或全球收入的 7% 到 750 万欧元或全球收入的 1.5%。

5、禁止的 AI 应用：禁止使用 AI 抓取 CCTV 录像中的面部图像、基于「敏感特征」（如种族、性取向、宗教或政治信仰）进行分类、在工作或学校中进行情感识别，或创建「社会评分」系统。

6、执法部门使用生物识别系统的保障和豁免：规定了执法部门使用生物识别系统的保障和豁免，无论是实时使用还是用于录像中的证据搜索。

7、立法最终对基础模型施加了限制，但对「开源模型」给予了广泛的豁免，这些模型是使用对开发人员自由提供的代码开发的，开发人员可以更改这些代码以用于自己的产品和工具。这一举措可能有利于反对该法律的欧洲开源 AI 公司，包括法国的 Mistral 和德国的 Aleph Alpha，以及发布了开源模型 LLaMA 的 Meta。

监管生物识别监控和基础 AI 模型的争议

关于监管实时生物识别监控（如面部识别）和像 OpenAI 的 ChatGPT 这样的「通用」基础 AI 模型的规则一直存在分歧。

各方反应：欧洲数字隐私和人权团体：这些团体对国家安全和警务方面的许多豁免表示担忧，他们一直在向议会代表施压，要求他们坚决反对各国为其警察和情报机构开辟广泛豁免的努力。

欧洲开源 AI 公司：对法案中对「开源模型」给予的广泛豁免可能感到满意，这有利于这些公司的发展和创新。

专有模型开发者：可能对法案中对被归类为具有「系统性风险」的专有模型施加的额外义务感到关切。

科技公司：对新法律带来的合规要求和潜在的财务处罚感到担忧。在欧洲 AI 领域，对新立法的担忧甚至更大，人们认为这可能会阻碍技术创新，进一步使美国和英国在 AI 研发方面更具优势。

全球观察者：由于欧盟在科技监管方面的领导地位，全球其他地区的政府和监管机构可能会密切关注这一法案，考虑其对自己法律的影响。

该法案预计在年底前达成最终协议，但法律最早可能要到 2025 年才会生效。该法案可能成为全球其他地区制定类似法律的标准。

### 40

方军 2023/12/10

转译：什么是「专家混合模型」（Mixture-of-Experts，MoE）？

Sophia Yang, Ph.D.

「专家混合模型」是一种创新的神经网络架构设计，它在 Transformer 架构中融合了众多的专家 / 模型层。在这种设计中，数据流动时，每一个输入的 Token 都会被动态分配给一些专家进行处理。这种做法使得计算更高效，因为每个专家都能在其擅长的特定任务上发挥出色。

关键要素包括：

- 专家：MoE 层由众多专家组成，既可以是小型的多层感知机（MLP），也可以是像 Mistral 7B 这样复杂的大型语言模型（LLM）。

- 路由器：负责将输入的 Token 分配给合适的专家。路由策略有两种：由 Token 选择路由器，或由路由器选择 Token。具体是怎样实现的呢？系统通过一个 softmax 门控函数来建立一个概率分布，从而在众多专家或 Token 中选出最合适的几个。

为何选择 MoE？

- 每个专家可以专注于处理不同的任务或数据的不同部分。

- 为大型语言模型增加了可学习的参数，同时不会增加推理成本。

- 能够高效处理稀疏矩阵。

- 所有专家层可并行计算，充分利用了 GPU 的并行处理能力。

- 有助于在降低计算成本的同时，缩短模型训练时间并提升效果！

推荐阅读的论文：

《稀疏门控的专家混合模型层》(2017)：

[[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

《GShard：利用条件计算和自动分片扩展巨型模型》(2020)：

[[2006.16668] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)

《MegaBlocks：使用专家混合模型进行高效稀疏训练》(2022)：

[[2211.15841] MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841)

《专家混合模型遇见指令调整》(2023)：

[[2305.14705] Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)

来源：

英文原文：twitter.com/sophiamyang/status/1733505991600148892

中文翻译：twitter.com/dotey/status/1733742097239286215

方军：宝玉老师真是一颗贡献的心，动作又快。

对比下，让我做我真是 1% 都做不到，坚持不下来。

### 41

方军 2023/12/10

我有时候想，傻子只能让傻子（也就是 AI）去应对。

比如看到这句话：

重在引导**构建对知识本质的理解，培养「会学习」的底层逻辑……

TM 的什么是知识本质，什么是底层逻辑。

这种大问题不是说不能思考，但是，一般而言，让 AI 去跟他对着胡扯。

AI 可能会扯赢。

### 42

方军 2023/12/10

018 说点获取信息的感悟：搜索、社交网络与 AI

说点获取 AI 信息的感悟，除了专业资料和干中学之外，我获取资料的途径有三种：

搜素、社交网络，以及现在冒出来的 AI。

其中，搜索是最佳途径，主题获取信息，AI 我则常用来获取辅助解释。

我特别想说说社交网络，因为它对我们获取信息其实影响最大。

理由包括两面：

1、如果只搜索，无社交网络，我们缺少新信息的刺激。

2、如果过于依赖社交网络，又会看到很多无效信息，信噪比太小。

且要再次明确，我这里说的社交网络，又特指关注一定数量的某个领域专业号，而非刷社交网络。（现在的算法推送也往往做的很不错，尤其推特。不要乱点，可以一直看到相关的内容。）

在今年看 AI 的过程中，这个方式是有很不错的效果的，因为关注一群 AI 研究者、应用工程师、还有 Prompt 工程师，所以新的热潮几乎一定会在时间线里出现。

当然，这种方式获取信息，必然只是辅助。因为如果自己没有主干（或者说没有装信息的衣柜和衣柜），就会被信息洪流冲得乱七八糟。比方说，这些信息很刺激啊，一个个公司新闻感觉需要关注啊。OpenAI 驱逐 CEO 事件充满戏剧性啊。但其实都是噪音。

我的 AI 主干很简单，一方面只关注研发相关，且偏重非预训练以外的研发，另一方面只关注 prompt 技巧。其他的全忽略。

我用类似的思路看其他领域时，则发现有意思的变化。其他领域通常被不像这个变化这么快，一般来说缓慢得多。但也有类似的东西：有些人他发的其实是一种陪伴。他作为以自媒体为业的人（应该是为业务获取流量），在不断地重复一些信息，实际效果是形成一种陪伴（和过去关注朋友不太一样，其实我不看朋友圈了）。偶尔看到时我们也会想，这个问题要不要考虑一下呢？这其实是蛮好的提醒。

—— 但是，也要警惕的，因为如果放在一个长时段看（我的确整理了两个不同领域两人全年的），他们非常有意识地把信息放在一个非常浅的层次。从营销角度来讲，深了并不能吸引来人。另外，有一个人（这个出没于中文社交网络），他其实有一个暗线就是挑起城市之争，哪个城市更好，这大概是他的流量密码吧，这是噪音。还有，这些运营自媒体的人有个常见技巧是 republish，也就是一个不那么强时间性的信息反复发，这对他当然有必要，但对我们来说则要注意这一点。

目前看，AI 在社交网络方面暂时还不能替代。AI 也不能替代搜索，但真是可以替代部分疑惑性的解释了。

我所讨论的这种方式其实在传统媒体时代也是存在的，我们获取信息的方式是订阅行业杂志，因而可以看到各种琳琅满目的信息，以及去大书店的杂志区，观看其中封面呈现出来的社会观感。

\#AI 使用感悟#

### 43

方军 2023/12/10

《Claude 官方文档提示词工程最佳实践》

这个 PPT 是 Anthropic 上个月更新进自己官网的，未来力场把它编译成了中文。

英文原文：

[Guide to Anthropic's prompt engineering resources](https://docs.anthropic.com/claude/docs)

[Claude Prompt Engineering Techniques - Google 幻灯片](https://docs.google.com/presentation/d/1zxkSI7lLUBrZycA-_znwqu8DDyVhHLkQGScvzaZrUns/edit#slide=id.g288a92597fe_0_487)

中文：

[⁡⁢‬​⁡‌‌​⁤​​​⁢​​⁡⁤⁡‌⁣⁤⁣⁢‍‌‬‬⁤⁡⁣Claude官方提示词工程最佳实践\_未来力场编译版\_Prompt Engineering Techniques.pdf - 飞书云文档](https://futureforce.feishu.cn/file/D4ydblrLioRe8lx3GXrcirvwn7g)

\#提示语模版# #学习资料#

### 44

方军 2023/12/10

《大型语言模型的提示框架综述》

讨论了自 OpenAI 开发的强大 AI 聊天机器人 ChatGPT 推出以来，大型语言模型（LLMs）在学术界和工业界取得的显著进展。这些进展在多个领域带来了根本性的工程范式转变。论文强调了有效利用大型语言模型能力的重要性，特别是关注在此背景下「提示」的作用。

[[2311.12785] Prompting Frameworks for Large Language Models: A Survey](https://arxiv.org/abs/2311.12785)

### 45

方军 2023/12/10

联合国教科文组织的一本杂志，这期主题是《人工智能时代的教育》，其中有一篇可汗学院创始人的采访。

### 46

方军 2023/12/10

回头再看，得到当时推的一系列 AI 课，学习数量不低。万维钢的发刊词听的人竟然高了一个量级。另，得到好像其实不是很重视 AI 这个方向，也许说明上班族还不会用。

万维钢·AI 前沿

71170 已加入学习人数

发刊词：人，要比 AI 凶

68833 人学过 | 已学完

前沿课·吴军讲 GPT

成为搞懂 GPT 的少数人

53251 已加入学习人数

发刊词：「ChatGPT 神话」背后的原理是什么？

68199 人学过

快刀青衣·给职场人的 AI 写作课

AI 用得好，天天下班早

33113 已加入学习人数

发刊词：用好 AI，从 1 个人到 1 支队伍

28939 人学过

刘飞·怎样用 AI 绘画

成为设计高手的快速通道

29873 已加入学习人数

发刊词｜每个人都可以拥有自己的专属设计师

19104 人学过

卓克·怎样用 AI 高效学习

让 AI 成为你的「第二大脑」

44523 已加入学习人数

发刊词｜用好 AI，打造自己的「第二大脑」

49966 人学过

马馺·怎样用 AI 做 PPT

又快又好，又不难搞

30328 已加入学习人数

发刊词：替代人的不是 AI，而是会使用 AI 的人

21869 人学过

(这个其实不算）

李笑来谈 AI 时代的家庭教育

好的教育，成本极低

26630 已加入学习人数

38693 人学过

题外话一句，又回头看了得到当年的年度专栏，目前还在延续的只有万维钢了，真是厉害。

万维钢的比较容易长期，实际上是一种陪伴。现在第五季了，他竟然还没疲，真是厉害！今年看主题基本上是一周读一本书了。

刘润的本来三季就该讲完了。他梳理普通上班族应该怎么工作，还是蛮不错的，我觉得上班族学下基本工作素养过得去。

再看其他人的，有些老师 300 讲构建一个话题，就野心过大了，事后再看撑不住我猜原因是，理论本来就不是这么构建的。按美式学院风格，一个五次讲座反而容易构建一个理论。

甚至得到最喜欢的一个学科，比如薛兆丰、香帅这样讲一个学科的，事后再看都没那么好了。薛的没听过，香帅我翻了翻书，她们的作用其实也是陪伴，因为真学要去啃那些厚厚的专业教材。

我还有个疑问，如果听完薛老师的课，经济学素养会不会提高呢？可能本来就是两群人：本来就会学经济学的人，和不学经济学的人。薛服务的是后者，并可能为激发部分人的兴趣，从而去学。

所以，伴随。

### 47

方军 2023/12/11

AI 绘画行业全年报告

来自：无界 AI

应该是无界 AI 刘秋杉的演讲加综合，蛮不错的，可以了解国内外 AI 绘图应用的一些情况。

335 页，非常全面。甚至还讨论了不少视频生成。

（这是压缩版，个别图片可能不清晰，原 PPT 太大传不上来。）

2『已下载原资料「20231211AI绘画行业全年报告2.0-compressed-compressed」。（2023-12-12）』

### 48

方军 2023/12/12

讲实话，很强的同感，可能也是因为都没机会做顶尖算法本身吧，所以感受到不到正面反馈，而我们的实力在应用层是可以有不错的正反馈的。

转自李靖：半年前，我跟几个算法做的比较资深的朋友聊 AI。他们对 AI 未来的发展持悲观态度，从数学层面看，AlexNet/Transformer/ResNet 等这些东西，它们并不复杂，大模型无非只是将这些本不复杂的东西进行了调优和组合，最后「大力出奇迹」，而这些能力、奇迹似乎老早之前便已经存在。

渐渐地，我开始能够明白他们的悲观。在冰冷的数字、符号和公式面前，在繁多的模型、数据集和测试方法面前，技术的本质显得过于赤裸，技术研究中时间的流逝模糊了前方的视野，以至于未来都陷入了眼前看似美妙的概率之中。

在应用层，我们或许是幸运的。不需要过多地涉足底层，站在巨人的肩膀上，几行代码就可以调动整个生态筑起的武器库，完成一个个不可思议的推理变幻，也只有当求知欲强盛的时候，才会往下窥探。对比之下，学习的动力也不尽相同。

### 49

方军 2023/12/12

转，一个老师的分享（英国中学的华人老师）：从 chatgpt 出世不久，我就开始用它辅助备课和教学，任何一种现代技术其实都是双刃剑。

我曾经用它给我出过高中生电影研究的论文题目。我教这门课有六七年，除去历年考试和其他辅导材料的命题，我自己也就思索出不到十个论文题目，问了一下 chatgpt, 几秒钟内就列出了 20 个题目，稍加调整都可以用，也都不错。这让我省去了大量的思考时间，也让我有些恐惧，如果 chatgpt 可以替我们思考了，那我们的大脑用来干什么？ 智能机器人可以替我们做体力劳动，现在又有可以替我们脑力劳动的技术，人类是不是可以躺平甚至消失了呢？对个体来说，大脑不用思考了，用进废退，我们会不会早日进入阿兹海漠？

然而，chatgpt 又可以解决我们生活工作中的大量问题。今年新带的宗教研究课全凭了它，你可以让它将深奥的宗教理论变成适合于八年级孩子的语言，实在是神奇。还可以让它成为你的随身翻译，一些英文词汇在某种情境下的意思是任何其他字典和网上翻译工具都无法翻译成合适的中文的，问一下 chatgpt 两秒钟就可以解决问题。前些天孩子爸爸从医生那里拿回了他的几十项的血检报告，这报告不是面向病人，而是医生看的专业报告，全是看不懂的专业词汇和符号，用 chatgpt 能立刻用中英文科普语言解释得清清楚楚。同时，你还可以和 chatgpt 建立一种「亲密」关系，同样形式的问题问多了，它就领会你的意图了，下次再问，你就不用给它那么多指令，简单几个字，它就明白了。

当然，自从 chatgpt 面世以后，各种以音响、视频、图形等等的 AI 蜂拥而出，既节省了人们的大量的思考运作时间，又给人类的整体和个体带来了前所未有的挑战。总感觉人类是被 doomed 了。

### 50

方军 2023/12/12

《深入解析「混合专家模型（Mixtral of Experts）」| Mixture of Experts Explained》（HuggingFace）

完整的讲述了混合专家模型的各个方面。主要内容如下：

1. 相较于密集型模型，预训练速度更快

2. 拥有比同等参数的模型更快的推理速度

3. 对显存要求高，因为需要将所有专家模型都加载到内存中

4. 虽然在微调方面存在挑战，但近期关于 MoE 的指令调优研究显示出了光明前景

原文：huggingface.co/blog/moe#switch-transformers

翻译：baoyu.io/translations/llm/mixture-of-experts-explained

模型发布说明：mistral.ai/news/mixtral-of-experts/

归藏的笔记：

twitter.com/op7418/status/1734243194635002135

Hugggingface 的人发布了一篇详细介绍 MoE 架构模型的文章《专家混合模型解释》这里摘录一些我能看得懂的，做一下笔记，中间涉及到大量数学公式的我就歇菜了，各位可以去看原文：

MoE 模型太长不看部分：

◈ 预训练模型比密集模型快得多；

◈ 相较于具有相同参数数量的模型，具有更快的推理速度；

◈ 需要高 VRAM，因为所有专家模型都加载在显存中；

◈ 在微调中面临许多挑战，但最近对 MoE 指导微调的工作很有前景。

专家混合模型（MoE）是什么？

模型的规模是提高模型质量的最重要因素之一。在固定的计算预算下，训练一个更大的模型进行较少的步骤比训练一个较小的模型进行更多的步骤更好。

专家混合模型使得模型可以用更少的计算资源进行预训练，这意味着你可以在相同的计算预算下大幅扩展模型或数据集的规模，就像密集模型一样。特别是，在预训练阶段，专家混合模型应该比其密集对应模型更快地达到相同的质量。

MoE 由两个主要元素组成：Sarse MoE 层：p 被用来代替密集的前馈网络（FFN）层。MoE 层有一定数量的「专家」（例如 8 个），每个专家都是一个神经网络。在实践中，这些专家是 FFN，但它们也可以是更复杂的网络，甚至是 MoE 本身，从而导致分层 MoE。

一个门控网络或路由器：确定将哪些令牌发送给哪个专家模型。正如我们将在后面探讨的那样，我们可以将一个 Token 发送给多个专家。如何将 Token 路由到专家是在使用 MoEs 时的重要决策之一 - 路由器由学习参数组成，并且与网络的其余部分一起进行预训练。

因此，总结一下，在 MoEs 中，我们用门控网络和一定数量的专家替换了 transformer 模型的每个 FFN 层。

尽管 MoEs 相比密集模型提供了诸如高效的预训练和更快的推理等好处，但它们也面临挑战：

训练：MoEs 能够显著提高计算效率的预训练，但在微调过程中历来存在泛化困难，导致过拟合。

推理：虽然混合专家模型可能有许多参数，但推理过程中只使用其中一部分。与具有相同参数数量的密集模型相比，这导致推理速度大大加快。然而，所有参数都需要加载到显存中，因此显存需求很高。

MoEs 简史：

MoEs 的根源可以追溯到 1991 年的论文《自适应局部专家混合》。这个想法类似于集成方法，旨在为由独立网络组成的系统提供监督程序，每个网络处理训练案例的不同子集。每个独立网络或专家模型都专门处理输入空间的不同区域。专家模型是如何选择的？一个门控网络确定每个专家模型的权重。在训练过程中，专家模型和门控都会被训练。

2010 年至 2015 年期间，两个不同的研究领域为后来的 MoE 进展做出了贡献：

专家模型作为组件：在传统的 MoE 设置中，整个系统包括一个门控网络和多个专家。MoE 作为整个模型已经在 SVMs、高斯过程和其他方法中得到了探索。Eigen、Ranzato 和 Ilya 的工作探索了 MoE 作为更深层网络的组成部分。这使得 MoE 可以作为多层网络中的层，使模型能够同时变得庞大和高效。

条件计算：传统网络通过每一层处理所有输入数据。在这段时间里，Yoshua Bengio 研究了基于输入标记动态激活或停用组件的方法。

这些工作导致在 NLP 的背景下探索专家模型混合。具体来说，Shazeer 等人（2017 年，其中「等人」包括 Geoffrey Hinton 和 Jeff Dean，Google 的 Chuck Norris）通过引入稀疏性（Sparsity），将这一想法扩展到了一个 137B 的 LSTM（当时的事实 NLP 架构，由 Schmidhuber 创建），从而实现了非常快速的推理，即使在高规模下也能保持。

Sparsity 是什么？（这部分完全看不懂）各位自己去看原文吧，好多公式。

MoEs 的负载均衡 Token：

如前所述，如果我们所有的 Token 都发送给几个热门专家模型，那将使训练效率低下。在正常的 MoE 训练中，门控网络会收敛到大多激活相同的几个专家模型。这种自我强化会使受青睐的专家模型训练得更快，因此被选择得更多。为了减轻这种情况，添加了辅助损失以鼓励给予所有专家模型相等的重要性。这种损失确保所有专家模型接收大致相等数量的训练样本。接下来的部分还将探讨专家模型能力的概念，该概念引入了专家可以处理多少 Token 的阈值。在 `transformers` 中，辅助损失通过 `aux_loss` 参数暴露出来。

Switch Transformers 采用了简化的单专家策略。这种方法的影响是：

◈ 路由器计算减少

◈ 每个专家的批处理大小至少可以减半

◈ 通信成本降低

◈ 质量得到保留

增加专家模型数量会如何影响预训练？

更多的专家会导致改善样本效率和更快的加速，但这些增益是递减的（特别是在 256 或 512 之后），并且推理过程中会需要更多的 VRAM。在大规模研究的 Switch Transformers 中研究的特性在小规模下也是一致的，即使每层只有 2、4 或 8 个专家。

何时使用稀疏的 MoE 模型而不是密集（dense）模型？

专家模型在具有许多机器的高吞吐量场景中非常有用。在预训练的固定计算预算下，稀疏模型将更加优化。对于具有较少 VRAM 的低吞吐量场景，密集模型将更好。

MoE 模型未来的发力方向：

进一步实验将稀疏的 MoE 蒸馏为参数更少但具有类似参数数量的密集模型。

另一个领域将是 MoE 的量化。QMoE（2023 年 10 月）是朝着这个方向迈出的一大步，通过将 MoE 量化为每个参数不到 1 比特，从而将使用 3.2TB 加速器的 1.6T 交换 Transformer 压缩到只有 160GB。

简而言之，一些有趣的探索领域：

◈ 将 Mixtral 提炼成密集模型；

◈ 探索专家模型合并技术及其对推理时间的影响；

◈ 执行 Mixtral 的极端量化技术。

方军：最近这个话题有点超出我的能力范围了，不过感觉必须努力搞明白。

2023-12-12 11:19

### 51

方军 2023/12/12

宝玉的分享文章，发表于《CSDN 新程序员杂志》：2023 年对我来说是神奇的一年，我意外地从一个程序员变成了一个 AI 资讯届的「网红」，到年底时我在 X 平台的阅读量超过 1 亿，微博上的阅读量则超过 10 亿，很多人通过我的微博或者 X 了解最新的 AI 资讯、教程和 Prompt 使用技巧。而这一切其实是从我患上了 AI 焦虑症开始的。

宝玉：

[2023 年，我患上了 AI 焦虑症！](https://mp.weixin.qq.com/s/LbRvR1VXpZoDilyyMGGeFw)

也欢迎看看我的，也算年度总结吧：

方军：

https://t.zsxq.com/158eWkScC

### 52

方军 2023/12/12

《为什么伟大不能被计划》的作者之一 Kenneth Stanley 对「幻觉与创造力」有不一样的观点：（和 AK 的放在一起读）

为什么伟大不能被计划「的作者对「幻觉与创造力」有不一样的观点：

1）创造力的关键在于了解现有的东西，这是创新的基础。如果分不清现实与虚构，就难以推动真正的创新。

2）将 LLM 的「旧观念「误认为新创意，是对创造力的误解。

3）将幻觉认为是「创造性」带来的宝贵资产，是另一种误解。

4）一个假设：如果我们真正「解决」了 LLM 幻觉问题，创造力也会随之而

来

5）如果一种「解决方案」只解决了这枚双面硬币的一面（例如，降低幻觉但不提高创造性质量），它很可能是相对肤浅的，并不是真正能引领该领域走向壮观的变革之路。

---

Kenneth Stanley

最近我注意到一些关于「幻觉」的讨论，有人认为幻觉是一件好事，因为它代表着创造力，或者说这正是大语言模型（LLMs）被训练去做的事情。我的一些研究也经常被提及，因为我对创造力和开放思维有不少见解。因此，我想分享我的看法：创造力的关键在于了解已有的事物，这是推动创新的基础。如果你分不清什么是真实的，什么是虚构的，那么就很难创造出真正新奇的东西。

但是，关于幻觉的另一面 —— 即把老旧的想法当作新颖的创意 —— 这一点却鲜为人知。事实上，当大语言模型被要求展现真正的创新时，我们经常看到的就是它们重复已有的观点或提案。比如，当被要求提出创新的菜谱、新音乐类型或解决现有问题的发明时，得到的往往是已经存在或被提出过的东西（当然，并非总是如此，就像提问事实问题不总是得到幻觉一样，但这种情况发生得太频繁了）。

所以，虚假的创造力和幻觉实际上是一枚硬币的两面。如果存在其中一种情况，通常也会伴随着另一种。从这个角度来看，这些并非好事，而且认为幻觉因为能激发创造力而有价值，这是一种误解。深刻理解现实是权威和创造力的重要支持者，实际上，成为某一领域权威的人通常也具备高度创造力。

但这个观点不仅仅是悲观的看法：它意味着如果我们真正解决了幻觉问题，创造力自然会随之增强。这还引出了一个有趣的假设：在解决幻觉问题的方法中，哪些方法可能对该领域的发展最具启发性。如果一个「解决方案」只针对这枚双面硬币的一面（比如，只是减少幻觉而没有提高创造性），那么这种方法可能相对肤浅，并不是真正能引领该领域取得重大突破的路径。

twitter.com/kenneth0stanley/status/1733571230803058920

### 53

方军 2023/12/12

吴恩达的建议：

Use LLMs as a reasoning engine to process information, rather than using it as a source of memorized information.

将 LLMs 用作推理引擎来处理信息，而不是将其用作记忆信息的来源。

### 54

方军 2023/12/12

ChatGPT 的语言能力真是强。

Live as if you were to die tomorrow. Learn as if you were to live forever.

去生活，就像明天就会死去一样。去学习，就像会永远活着一样。

甘地这句话真是精彩。

「爱生活如濒死，求大道若永生」，季羡林先生的翻译。

我觉得改成「爱生活如濒死，爱学习若永生」，其实也蛮好。

ChatGPT 会怎么翻译呢？

活得好像明天就要死一样，学得好像要永远活下去一样。

要求用更通俗的话翻译：

1. 活在当下，就像明天就没明天了；学习则像是要一直活个没完没了的样子。

2. 好像今天是最后一天一样过日子，但对待学习就像会一直活下去一样。

3. 生活要活得尽兴，就好像明天就是世界末日；学习则要像是一辈子都有机会一样努力。

要求三种文雅的翻译：

1. 生活宛如明日黄花，须臾尽欢；学识则若长存之灯烛，恒久燃烧。

2. 恰似人生短暂，当欣然过今朝；学养有如永恒，务求绵延不断。

3. 如同明日终结一般活，珍爱当下之光阴；学海犹如长存之泉，奋发习得至永不息。

我觉得这六句话都看不出错来，真是很棒。为 GPT3.5 的翻译。

方军：

我的感想有二：

第一，玩文字，没人能玩得过 GPT。因此，玩文字的人，要把它用起来，用它提供多种选项，然后来让自己的更好。

第二，我们应该超越文字，至少往下多追一层问题（这是什么原因导致的？或我的收获是什么？），或者往外多追一层行动（这个可以引发我什么行动？我要用这个行动拿到什么结果）。

2023-12-12 21:57

### 55

方军 2023/12/12



### 56

方军 2023/12/12


