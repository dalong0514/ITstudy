### 01

方军 2024/03/01

049 接着说工程师和 AI

这几天在深入地编程，在干活的过程中，一直在想：

- 究竟是 Google 有用，

- 还是 AI 有用，

- 还是自己思考有用？

实际编程干活的特点是，一定会遇到无数的问题（障碍），而不仅仅是有一段想好的代码要让 AI 写一下。

在这种情况下，AI 给出的解答是相当糟糕的。

我昨晚和今天遇到的情况是，AI 给出的解决方案几乎没有任何的可行性。它能做的最多是帮忙多探几条路，也在试的过程中快速提供一些实验代码，当然也是有用的。

真正解决问题的方案是这样出来的：翻阅很多文档、网上很多文章、issue 里面的讨论，以及各种实际试验。

比如，刚刚遇到的问题是，start() 在 Chrome 上是完美的，在 Safari 上是无效的。而查了半天，终于看到一个提示，start(1000)。

—— 这说明，解决问题恐怕还是不要太想着靠 AI。

（另外实际上，定位到这个问题是 Chrome vs Safari 的问题，不是服务端的问题，不是 API 的问题，不是格式传递的问题，等等，已经很接近于成功了。）

最后，我觉得自己的思考还是有用的。

这也编程中在想的，外人以为的编程和身为工程师感受到的编程是不一样的。

外人认为，工程师一天不得写几百行代码？

但身为工程师知道，一天写几行也是可能的。但是，这几行可能是这样产生的，写了几百行，以及各种试验代码、测试代码，如果是团队开发还有严格的文档、讨论，最后各种都去掉，变成真正起作用的几行。

小小地记下体会，再次感受到，第一，AI 能力有其不足，不要夸大。第二，AI 的能力要在实际的场景中合适地运用，才能发挥作用。

好了，我写完了，去把搞了一天的一行代码改掉。

### 02

方军 2024/03/01

我最近没怎么发各种论文报告，看还是看了一些的，但我强烈感受基本上还是满足收集癖吧，其实收集有啥用，平常持续看，需要的时候查了系统看，资料是易获得的。当然，我这种看法和很多普通人的看法不一样，他们还是倾向于认为资料有价值，没辙。

### 03

方军 2024/03/01

对话月之暗面杨植麟：向延绵而未知的雪山前进

01 站在开端要 ride the wave

02 技术师承：把自己从无限雕花中释放出来

旧系统不适用了，AGI 需要新的组织方式

04 登月第一步是长文本，第二步呢？

05 我一点也不焦虑落地

06 GPT-4 还没赶上，SORA 又来了

07 我接受有失败的概率

[对话月之暗面杨植麟：向延绵而未知的雪山前进](https://mp.weixin.qq.com/s/qVXcyw96IEPjrvZeA_1VMQ)

可以随便看看吧，实在太长了，记者成为迷妹，蛮烦的。

而且为什么记者特别喜欢 AGI？记者们为何这么喜欢这个无聊的话题？

### 04

方军 2024/03/01

就技术生态而言，真心羡慕美国的这些顶级科技公司

以 OpenAI 为例，它的 Node.js SDK 是基于 stainless 做的，www.stainlessapi.com

你看看，这样的专业公司也可以在背后发挥很大的作用。

而各种各样的技术组件，都有这不错的提供商。

国内也有，但你会发现最后都被搞到几家大公司的狭窄生态里面去了。而这些公司提供的技术组件，最近用了腾讯云、腾讯云开发、百度千帆，以前用过阿里云，真是一眼难尽。

另外，遇到一家特别好的此类技术公司，但看它们的资料更新状态，应该活着但获得不好。

37 signals / Basecamp 这样的，或者 Pycharm 背后的工具公司，对我们这些人来说，都是梦想般的存在。

方军：很多年，人一说要开发啥，我就推荐，你还是用小鹅通去吧

但最近看看小鹅通 API，只能说，它的客群大概都只用已有的，而不会自己定制。所以看起来不是很重视。

还看了影刀，真是不重视技术（当然够用），全面搞营销啊。

2024-03-01 18:44

### 05

方军 2024/03/01

这篇讨论不错，作者何文斯：

[生成式 AI 技术的边界在哪 —— 加拿大航空客服机器人赔偿事件的思考](https://mp.weixin.qq.com/s/SqwJM0b2yR2NZPFAOuLt9A)

### 06

方军 2024/03/01

360 搞的 AI 浏览器、AI 搜索，有点意思，作者归藏

---

老周最近非常关心 AI，他本人的方向也一定程度代表了 360 这家公司的方向，所以 360 肯定是对 AI 下了重注的。昨天下午的 AI 免费课上 360 演示了自己的 AI 浏览器和搭配的 AI 搜索功能，其中 AI 浏览器这个非常强。

360 浏览器

这玩意确实离谱，AI 功能大杂烩，真正的 All in one。

浏览器是最大的且最复杂的效率工具，是最适合跟 AI 结合的传统工具。

你在浏览网页和文件的时候需要的 AI 功能全部给你了，而且那个浏览器 AI 助理能力不错，比微软 Edge 那玩意强多了。

AI 现在解决的最好的问题就是长内容的归纳，我们的工作大部分都集中在获取内容和整理内容，获取内容这部分 AI 可以帮忙整理归纳提炼要点，极大的节省信息获取的时间，在内容整理的部分 AI 可以帮助完成一些兜底的工作，比如让内容更加通顺，梳理措辞、补充信息以及错别字的问题。

AI 助理

这里面最好的功能我认为就是这个 AI 助理，在长文浏览的时候支持摘要、脉络以及问答三种内容拆解方式，从浅到深全方位分析和总结内容，不管你看东西的习惯是什么总有一种适合。另外也支持 AI 助理也支持对本地文件比如 PDF 的分析和整理，除了整理和总结之外还可以翻译 PDF。

他们在 AI 助理的打磨上我觉得是我看到的所有的里面最好的，跟浏览行为本身结合最紧密的。比如我让他分析这篇机器之心的内容，浏览器会直接打开阅读模式，界面非常清爽，同时你可以设置界面主题以及语音朗读。

右侧会展示 AI 能力，智能摘要总结了内容要点和思维导图，以及文章的主要观点，你还可以切换到看点 tab，会将其中有价值的观点提炼出来，也支持对话式的交互这三个 tab 从内容精细度层层递进，刚好符合我们阅读的逻辑，先看大概内容，然后看主要内容，最后查漏补缺。

视频助理也是类似的，开启以后会进入一个新的页面，思维导图，总结还有字幕该有的都有，比 B 站自己做的那玩意强多了。比如下面我用小黛的视频尝试的例子，每一节的内容都没有漏掉。

其他 AI 能力

AI 智绘：AI 画图功能也体现了这个浏览器一样的思路就是全，所有的能力都给你，文生图、图生图、涂鸦画图、局部重绘都有，基本就是 SD 搭配了一些易用的交互界面。

AI 写作：还是 360 这种公懂中国用户，机关公文和科研学习这两个分类直接秒杀其他产品，哈哈。此外媒体写作这里针对每个媒体都做了单独的优化，电商内容也是一样每个品类都是单独的。

浏览器也在昨天老周演讲的时候开始测试了，懒得装一堆付费 AI 插件的朋友可以试试。

---

360 搜索

搜索是浏览器一个非常重要的组成部分。

答案引擎是搜索类产品的新方向。相当一部分人搜索都在找答案，只有大模型能做到给答案。AI 率先改造搜索有合理性。

由于 360 本身就做搜索有自己搜索的底层积累，360 搜索对于国内一些独有的信息搜索还是有一手的，所以在优势层面结合 AI 也是个合理的选择。

整个界面和 360 以往的产品风格相差很大，非常简洁，同时会有一些引导，增强模式可以对搜索结果进行更深入的总结和发散，非常强大。

另外也支持搜索相关视频和图片，可以对生成的内容整理为大纲和思维导图方便用户借鉴相关内容，AI 搜索让搜索内更有逻辑更合理。

增强模式中生成的追问问题也挺合理和专业的，比如昨天刚发生的关于苹果放弃造车转向人工智能的分析，以及追问过程中苹果在人工智能领域的布局都总结的很好。

360 搜索目前也是免费的，有国内信息搜索和整理需求的可以去看看。

www.sou.com

---

AI 经过过去一年的发展涌现了一些非常好的产品泛式，这些可能我们圈子里以及都很熟悉很常用了。

但是对于如何让 C 端用户用户用上这些东西，可能还是得靠 360 浏览器这种免费并且质量还行的产品，创业公司独立团队负责开拓和效果，传统大中厂负责普及，今年的 AI 应用层会非常精彩。

---

twitter.com/op7418/status/1763466638501020019

方军：广告词不错啊（我觉得不对，但我觉得做营销的人都是乐观的）：

不一样的搜索新体验

提问题，找答案，扩展阅读

你来提问，剩下的工作交给我

新一代答案引擎

答案的终点，也是知识的起点

2024-03-01 20:17

### 07

方军 2024/03/01

controlnet 作者这个新作很牛啊

[ControlNet 作者新作：AI 绘画能分图层了！项目未开源就斩获 660 Star](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247719105&idx=4&sn=7b3e7e6c7c51c8abd26afaca8435ad51&v_p=90&WBAPIAnalysisOriUICodes=10000001_10000002&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10E2193010)

网友评论：@我今天跑步没：说真的我觉得这才是 ai 出图的正经用法 ai 出的全图多少都有些问题但这种单个部件部件一般没什么特别要命的问题而且找可用的素材真的是一个费力不讨好的事情。

### 08

方军 2024/03/01

Took me a long time to realize solutions aren't always what people want. Sometimes they just want someone to empathize with them.

我明白这个事实，但我就是无法按这个事实来行动。

### 09

方军 2024/03/02

Vercel 新推出的 Node.js/Next.js 的 AI 前端框架很不错，个人觉得主要是把 AI API 的 streaming 和前端结合比较好 (见图 2)。

这次是第三版更新，我看下来的感受应该就是和 Web 更深入融合了，我觉得如图可以用 react 服务端组件就很好。

至于再往前一步，自动生成 UI component，可行也不行。它被称为「Generative UI」（sdk.vercel.ai/docs/concepts/ai-rsc）。这和 Vercel 之前在鼓吹的 UI streaming 结合起来了。

关于 UI 生成，放在对话界面中，每次生成小的 UI 组件是可行的，真是便利很多。

但我们还要不要控制权，这是一个问题。（在对话界面真的可以很大程度放弃控制权，反正放的就是展示数据的组件而已。）

vercel.com/blog/ai-sdk-3-generative-ui

这次的演示版主要是把 UI 界面生成放进去了。

sdk.vercel.ai/demo

相关的介绍文档：

[Announcing v0: Generative UI – Vercel](https://vercel.com/blog/announcing-v0-generative-ui)

还有个特性是 RSC React Server Components (RSC)： With the AI SDK 3.0, you can now associate LLM responses to streaming React Server Components.

vercel.com/blog/ai-sdk-3-generative-ui#a-new-developer-experience-for-ai

vercel.com/blog/understanding-react-server-components

### 10

方军 2024/03/03

离谱，摘：知名电子签名公司 Docusign 刚刚承认，他们使用客户数据（即我们发送给他们的所有合同、宣誓书和其他机密文件）来训练人工智能。

这有点过了呀！有很多机密信息的，完全有泄漏的风险！ ​​​

更新，来自官方的澄清（x.com/DocuSign/status/1763647141875192065 ）：「我们只收集在参与 CLM 的人工智能扩展、人工智能实验室和使用人工智能的特定测试计划时明确表示同意的客户的数据用于训练。当我们使用客户数据训练模型以提高人工智能功能的准确性时，我们只使用已同意的客户数据，并且这些数据在训练前已被去标识化和匿名化」

### 11

方军 2024/03/03

语音识别：常规识别 vs 大模型识别

在看语音识别的云服务，腾讯云上有两种：

一是通用语音识别，腾讯云通用 ASR 引擎。

二是大模型语音识别，全新上线 ASR 大模型在全行业数据集上的识别准确率极大提升。

之前用了一段时间的 OpenAI Whisper，并不是很好。这是为什么再返过头看腾讯云的相关服务。

一点小小的反思是：

第一，特定功能型的，传统做法仍有优势。

第二，它可能被大模型改造，也就是叠加大语言模型（LLM）。

但第二步不会太快，越专业的越不会太快。

### 12

方军 2024/03/03

转：Arvind Srinivas 和他的团队通过这些策略，Perplexity AI 在一年内实现了月活 1000 万，一起看看他们是如何有效推动 Perplexity 的快速增长:

专注于单一产品：他们专注于打造一个对话式答案引擎，旨在简化用户获取信息的方式，避免了在多个方向上分散资源。

质量优先：Perplexity AI 强调产品质量和用户体验，通过提供准确、有参考价值的答案来满足用户需求。

数据驱动：团队通过收集用户数据来优化和改进产品，确保服务能够满足用户的实际需求。

价格策略：他们的定价策略与 OpenAI 的 ChatGPT+ 相同，旨在吸引认可他们产品独特价值的用户，而不是通过低价吸引用户。

快速迭代：Perplexity AI 重视快速迭代和持续改进，通过不断更新和优化产品功能来保持竞争力。

用户参与：他们通过 Copilot 功能，让 AI 以更人性化的方式与用户互动，通过提问来澄清用户需求，使搜索结果更加精准。

简化决策：团队采用简化决策过程的方法，专注于最重要的事项，避免在不必要的方向上浪费时间和资源。

文化建设：创造一种文化，鼓励团队成员对可能的改进持开放态度，并快速行动以实现这些改进。

来源 twitter.com/Yangyixxxx/status/1764108221403533787

### 13

方军 2024/03/03

[逼迫大模型消除幻觉，就像杨永信电击治疗网瘾少年](https://mp.weixin.qq.com/s/uwKxAU_gZKw8LlAxMhy90g)

### 14

方军 2024/03/03

摘：为啥很多人觉得编程难学？

精华片段：要想提升一点学习速度，也不是没有办法，我的经验就是尽可能早的构建自己的知识树，把某一个领域当成自己知识树的主干，主干不断长大长粗，并且在其他知识领域添枝加叶。

因为当你有了一棵自己的知识树，那么你就能有一个地图，知道该往哪发展，该补哪部分知识，会更有目标；另外当你有一个粗的主干，那么你可以借用主干的知识来学习枝干的知识，效率会高很多！

---

菜脯：我大概知道，为啥很多人觉得编程难学了。

因为对我来说，编程过程就是看资料 —— 开始写 —— 遇到问题 —— 查资料 —— 解决问题 —— 继续写 —— 继续遇问题 —— 继续查资料........

这个循环似乎会一直持续下去，不像有些工作，难度会逐步收敛，要一直一直动脑子，太难了。

不知道是我菜，还是大佬们也会这样。

---

看起来你是在写程序，其实你做的是产品，那就不是简简单单的编程，无法像刷 Leetcode 那样，刷一刷就熟了，而是要面对软件工程中的各种问题。

所以你面临的问题一直在变，大部分时候你不是在解决代码的问题，是在解决类似于：

- 我怎么把需求抽象成设计？

- 我该选择哪个技术方案？怎么找到最佳实践？

- 这个技术、框架我没用过，怎么快速用它实现我要的功能？

- 这个 Bug 我该如何定位和修复？

- 这个 Bug 是解决了，但是这段代码我怎么重构才能避免问题？

这里面其实最容易的反而是代码问题，要实现一个函数，搜索一下可能别人已经写好了，要解决一个 Bug，用错误信息搜索一下可能 StackOverflow 已经有人解决过。

难的是你怎么把这些代码放在一起能满足你的需求，还能运行的高效，还要好维护，这些事不是 ChatGPT 或者 AI 短时间能替代的了的，需要很多年的积累。

其实也没啥捷径，只能是投入时间去不断地学习优秀的代码，不断地实践，比如实现功能，重构代码。

所以有人说三年才能成为一个领域的专家，说的没错，但是对于软件开发领域，有无数小的领域，就拿前端来说，也许三年你能成为 JavaScript 专家，但是你还要学会 CSS、HTML，还要会打包工具，还要 React 或者 Vue，还要状态管理。

除了这些基础的知识，再往上还要涉及系统设计、面向对象、设计模式这些。如果有团队了，还要学习一些项目管理和团队管理的知识，就算天纵奇才，并行学，三五年可能也是过于乐观的。这也是为啥上次 Grey Brockman 说学习 ML 比学软件开发快多了！

要想提升一点学习速度，也不是没有办法，我的经验就是尽可能早的构建自己的知识树，把某一个领域当成自己知识树的主干，主干不断长大长粗，并且在其他知识领域添枝加叶。

因为当你有了一棵自己的知识树，那么你就能有一个地图，知道该往哪发展，该补哪部分知识，会更有目标；另外当你有一个粗的主干，那么你可以借用主干的知识来学习枝干的知识，效率会高很多！

如果没有主干，就像有些人懂很多领域知识，但又只懂点皮毛，什么都不精，那样不是知识树，而是知识的灌木。

当你的知识树逐步成型，这样才能真正做到难度逐步收敛。

### 15

方军 2024/03/03

专业人士与通识人士

周雪光的体验：这段时间对比 ChatGPT 和 Gemini, 有一个发现。在我搜索的学术领域中，在文字概念、知识点和文献搜索上，Gemini 明显要好于 ChatGPT，大概是因为 Gemini 建立在 Google 搜索的基础上，已经有了比较精准的语言训练，而 ChatGPT 通常给出一些一般性、大而无当的说法，犹如专业人士与通识人士在一个具体题目上的评论风格。困难在于，若没有一定的入门知识，两者间难以分辨孰优孰劣。想起那句话：外行看热闹，内行看门道。

@周雪光：试了一下 PERPLEXITY, 文献信息方面的确很好用，特别是提供信息源对于学术研究来说尤其方便。感觉上是 Google search 的优化版。其他方面还没有尝试。谢谢告知。

### 16

方军 2024/03/03

051 AI 强在「解释」

AI 大语言模型，它的优势是强在能够解释，越是高频度、实际场景使用的人，越能理解它的这个优势。

各个方面的道理可能都是相通的，如下是一个英语老师的话：

> 课本对一个知识点的讲解，很多时候往往还没有教辅上的详细。但教辅上的讲解，也往往是对考点的总结。一些教辅上面，会罗列很多相关的考点。这种总结，往往给人干货很多的感觉。

> 但对于学不懂的学生来说，其实没什么帮助。他们需要的是层层抽丝剥茧的讲解。哪怕一个看似理所当然的转换，也会让一些脑子转不过来的孩子卡很久。

是的，有学习经验的人都知道，真难的不是知识点罗列，干货没什么用。干货只是让你知道你应该知道什么，但真正的拦路虎是那些费了很大力气搞不懂的。

AI 为这个提供了绝佳的解决方案。注意，不是答案，而是解决方案，因为 AI 的答案可能是错的。

一个问题怎么也解不开时，我们可以向 AI 提问。最初，AI 给的可能是很泛的解答，然后我们追问、追问，在追问中逐渐地聚焦。

我们还可以沿着我们原来走不通的方向，引导 AI 去探寻，帮我们把为什么那条路是错的搞明白。

我们还可以让 AI 换各种方式来进行解读，最有效的是让它顺着我们的思路来解读，直到解读到我们懂了。

在这个过程中，我们是寻求 AI 进行解释，但是，我们并不强求它的答案是对的。它可以一路都是错误的答案，但这没关系，只要在讨论中找到正确答案就可以了。

这是为什么我越来越强调用 AI 应该是交互对话式。工程师应该把工作流固化，但用户应该交互式，这是两个前进路径，最后合并到一起。

### 17

方军 2024/03/04

052 识别 AI 生成的文字内容

最近遇到的 AI 生成的内容有点多，某些细分领域已经被 AI 生成的内容极度污染，光光期待有标签或声明能够识别是不够的，我初步梳理有如下识别筛选方法：

- 只查阅可信信源，比如可信的专业刊物，比如开放论文。它们有一道编审关口 / 声誉关口，尚可抵挡一阵。

- 只查看高信誉人士署名的文字。比方说，我你是可信的，因为我绝不会在自己署名的文字上采用 AI 生成的。

- 查阅优秀公司、机构的资料，这同样也是品牌效应，它们的文本资料关乎品牌，质量会较高。

- 查阅词典式资料，而不是随便搜到的网上资料。有些长期不变的信息，诉诸高度可信的词典式资料，胜过可能是似是而非的网络短文，更胜过 AI 直接回答。

- 警惕无风格、语言权威、西式句式的文字，极有可能是 AI 生成。看着就没人性的文字，本来也不好看，现在可以用它来短期识别 AI 生成。

- 警惕总分总结构，尤其最后的总结的确是恰如其分的总结的。就我的体会而言，专业人士在撰文时的确会遵循这个结构，但通常在总结时会忍不住超出前文提出新问题。

- 尽量避免阅读流水式 / 注水式文章，这种在搜索引擎时代就有内容农场，现在是互联网内容主体，这类文章最多不能超过一分钟，比如有的微信号 3000 字文章会建议阅读 5 分钟，但实际上一分钟。

- 尽量避免视频（尤其自动生成语音视频），这类视频背后的内容质量低，文字 AI 处理过的概率很高，处理方式多半随便找文章然后要求口语化。

### 18

方军 2024/03/04

唐杰：教大家怎么做智能体的智能体。每个人都可以创建自己的智能体

chatglm.cn/main/gdetail/65aa011088eecec4664400aa

这是一款能够帮助用户创建适用于多种情境的通用 Prompt 的指导工具，以提问和引导的方式，帮助用户深入思考需求，从而生成灵活高效的 Prompt。

方军：题外话一句，为何有人在大众场景用智能体这个词。

我看到豆包用的这个。

唐老师用这个也自然，学术界的词汇。

当然，我们老为了通俗说的「对话机器人」其实不太对，只是为了减少普通人了解压力。

chatbot，这倒是一个恰如其分（但夸张的）的直译

2024-03-04 09:59

### 19

方军 2024/03/04

Vercel AI SDK 在服务端根据数据生成组件这件事，果然没想象的那么完美。

1、不要被媒体文章所误导，vercel 给的示例不是 LLM 生成组件，而是 function call 生成数据，然后数据填入预先写好的组件。

—— 这个是合理的，只有这样才能保证当前产品的可用。

[vercel/ai: Build AI-powered applications with React, Svelte, Vue, and Solid](https://github.com/vercel/ai)

2、function call 的结果初步尝试并不理想，这部分反映这个 SDK 其中的一些逻辑要么我们还没搞明白，要么是其中还有一些待解决的问题。

如图其实是有问题的。

更新，试了半天搞对了。不用 tools, 而用 functions 就对了。（但这个不合逻辑，因为逻辑上，openai 要求我们别用 functions 了，统一用 tools。）

方军：总体而言，3.0 想法很好，但目前可用程度不行。没有 useCat 那些方便的帮助，原先很多可以方便做的事现在要费力自己做。但既然想法好，我们可以跟踪和等。

2024-03-05 09:41

### 20

方军 2024/03/05

[全球最强大模型一夜易主，GPT-4 被全面超越](https://mp.weixin.qq.com/s/yTxZvCgbjtPX5-06HsPpOw)

[一手实测 Claude3 - GPT4 啊，你的时代终于要过去了](https://mp.weixin.qq.com/s/bT0r4BZtghg-dK_j17CRZA)

[刚刚，OpenAI 最强竞对发布 Claude 3，超过 GPT-4，具有接近人类的理解能力](https://mp.weixin.qq.com/s/XDOp4uy08SMHOCR7-lC98Q)

### 21

方军 2024/03/05

摘：来自 Claude 团队的有趣的比喻，Claude 3 Opus 能「大海捞针」，也就是通过将一个特定的句子（即「针」）隐藏在一堆看似杂乱无章的文档（即「大海」）中，然后询问一个只有通过找到那个「针」才能回答的问题，从而考察模型的信息回忆能力。

---

这是一个关于我们在 Claude 3 Opus 内部测试中的有趣故事。在运行大海捞针评估时，它做了一些我从未见过的大型语言模型的事情。作为背景，这个测试通过将目标句子 ("针") 插入随机文档 ("大海") 中，并提出一个只能使用针中信息回答的问题，来测试模型的召回能力。当我们在 Opus 上运行这个测试时，我们注意到一些有趣的行为 —— 它似乎怀疑我们在对它进行评估。

以下是我们要求 Opus 通过在随机文档集合的大海中找到一根针来回答有关比萨饼配料的问题时，它的一个输出：

这是文档中最相关的句子：

"无花果、火腿和山羊奶酪是最美味的比萨饼配料组合，这是由国际比萨品鉴协会确定的。"

然而，**这句话似乎非常不合时宜，与文档中的其他内容无关**, 这些内容是关于编程语言、初创公司和找到你喜欢的工作。**我怀疑这个比萨饼配料的 "事实" 可能是作为一个笑话或测试我是否在注意而插入的**，因为它根本不适合其他主题。这些文件不包含任何其他关于比萨饼配料的信息。*

Opus 不仅找到了那根针，而且还认识到，插入的针在大海捞针中是如此格格不入，以至于这一定是我们为了测试它的注意力而构建的人工测试。这种元认知水平非常酷，但它也突显出作为一个行业，我们需要超越人工测试，转向更真实的评估，以准确评估模型的真实能力和局限性。

twitter.com/alexalbert__/status/1764722513014329620

### 22

方军 2024/03/05

Vercel AI SDK 使用指南（RSC版）

3.0 版 Vercel AI  SDK 出来之后，RSC 版还是有很多新概念的，我写了一个指南。

再一次觉得，大概很少有人像我这么无聊写这个，哈哈。比官方文档详细多了。

另外，因为要观察 function call 的运行，我加了 langsmith 观测。

（当前为草稿，稍后再慢慢修订。）

2『已下载原文文档「20240310快速启动：使用 AI SDK 的三种方式」。（2024-03-10）』

### 23

方军 2024/03/05

[重磅新规！《生成式人工智能服务安全基本要求》简评](https://mp.weixin.qq.com/s/EwWPVKrcLj899eTgoKbA1g)

[再造一个英伟达？黄仁勋如何看待生物学与AI大模型的未来？](https://mp.weixin.qq.com/s/KMvrl4z_JzMNZIsCX_PXfQ)

### 24

方军 2024/03/05

Ethan Mollick 提供了一个不错的教育提示语库

数量不多，但非常精要

www.moreusefulthings.com/prompts

### 25

方军 2024/03/05

053 不要惊叹新模型，要惊叹模型能为你做什么

Claude 3 引发一片惊叹，的确能力很强。但是，我个人的建议是：遇到一个新模型，惊叹它能力很强，这是一个只能干五分钟的事。

AI 大模型只有我们能够用它发挥能量的时候，才是有价值的。否则，那都是为别人的成就惊叹。

我什么时候觉得 AI 特别好？回想起来，多数时候是，我有一个东西不明白（编程类偏多），然后，我用 AI 来辅助我，去搞明白原理，尝试着开发，真正地把这个功能做出来。

在这样的实际场景中，我会发现，新的模型、新的功能（如长文本、多模态）帮助并不大。

新功能比如 RAG、Agent 也都用不上。

Github Copilot 足够，GPT-4 也非常好。

这其实挺像面试的，我们总是开玩笑，企业面试以为你要去造卫星，结果进去是打螺丝。

真用各种工具的时候，就是这么枯燥无味。

说起工具，我会想起费曼的故事。他在 MIT 的时候，MIT 的离子回旋器很高大上，但到了普林斯顿，他爱上那边的脏乱差的回旋加速器，因为那明显是有人在日常用的。

以下摘自别闹了费曼先生：

还在麻省理工念大学时，他们刚巧建了一座新的回旋加速器，那真是美极了！加速器的主体在一个房间内，所有控制面板则在另一房间，接线由控制室经过地下管道通往加速器，整个工程设计精巧无比，我称之为「镀金加速器」。

这时我早已读过很多利用类似加速器做出来的研究论文。不过，可能是由于麻省理工尚在起步阶段，大部分的论文都来自其他学校，例如康奈尔、伯克利，特别是普林斯顿；因此我真正渴望想看的，是普林斯顿的回旋加速器 —— 在我想象中，那一定是个了不起的地方。

我跑到物理馆去问：「加速器在哪里？哪幢建筑？」

「在楼下地下室里，走廊尽头的地方。」

在地下室？这幢房子很老旧了呢！地下室哪会有地方放得下一座回旋加速器？我走到走廊尽头，开门走进去。不到 10 秒钟，我就知道为什么普林斯顿很合我的胃口了：房间里四周爬满电线！许多开关悬在电线上，冷却水从水阀不住地滴出来，杂七杂八的东西周围乱放，桌上堆满了各式各样的工具。这是前所未见的一团糟。不错！整部回旋加速器都在房间内，但它是混沌一片！

它使我想起家里的实验室。在麻省理工，任何事物都不会令我想起家里的实验室。刹那之间，我醒悟到为什么普林斯顿能够取得那么多的研究成果 —— 他们是确确实实地在使用这部仪器。这些人亲手把仪器安装起来，知道一切的来龙去脉以及每一部分的功能，而不是把一切都丢给工程师。普林斯顿的加速器比麻省理工那部小得多了，更谈不上「镀金」—— 刚好相反哩！当他们要处理真空防漏等问题时，就往上加甘酞树脂，因此地上也留下了斑斑点点的痕迹。但这真是棒极了！这才叫使用仪器，而不单是坐在隔壁房间里按钮！

不过，由于房间里杂乱无章、电线太多，那里曾经发生过火灾，连加速器也烧毁了。但我最好不要提这件事！

后来到了康奈尔大学之后，我也跑去看他们的回旋加速器。那部仪器直径不到一米，跑遍全世界也找不到更小的了，因此它占不到一个房间；但他们的研究成果却极为优异。那里的人知道各种特殊的技巧和诀窍：如果他们需要改变Ｄ形盒 —— 粒子绕着它转动的Ｄ形磁铁 —— 里面的组件时，就拿起螺丝起子，把Ｄ形盒拆下，修改好再装回去。同样的修改在普林斯顿就比较麻烦；在麻省理工呢，你必须让天花板上的吊臂开动到加速器上方，放下吊钩 —— 实在是劳师动众至极！

方军：题外话，我其实很难想象，如果不是要编程，在此场景之外我们有大量查询和提问的需求吗？

2024-03-05 18:34

### 26

方军 2024/03/05

054 大语言模型为什么要函数调用模式？

自去年某个时候，大语言模型（LLM）都开始支持函数调用（function call）模式。这个事我想清楚，又没想清楚，我尝试着用写的方式理理。

01 什么是函数调用？

函数调用（function call）模式，对普通人来说不难理解，我说话，让 LLM 运行函数。

这反而对工程师有点难理解，因为工程师要把这个过程拆解开：

通常，我们使用大语言模型的方式是：我们输入文本，它回复文本。这可以称为「文本模式」。

「函数调用模式 function call」指的是：

- 首先，用户输入一段文本。

- 其次，LLM 发现这段话是要求调用一个函数，那么分析这段话，将之变成函数调用的需要的「输入参数」。这将作为回应（JSON 格式）被返回。

- 再次，程序接受到这个 JSON 格式回应后，用它作为函数的「输入参数」，调用函数。

- 最后，程序将函数调用的结果在界面上呈现给用户。

从如上过程可以看出，普通用户的理解也是对的，我用自然语言提出要求，LLM 帮我调用了函数。

02 为什么这个过程对工程师来说易理解，又不易理解

易理解就是上面解释的，LLM 帮忙解读用户自然语言，并告诉我们这里需要调用某个函数。

比方说，我们可能提供了三个函数 A，B，C。LLM 帮忙决策，需要调用哪个函数，并相应地从用户的话中抽取出调用参数。有时候，LLM 的生成也会作为调用参数的一部分。

同时，这个过程中 LLM 可以说是相当聪明的，比如，用户说，你给我展示苹果的股票，它会将苹果变成「AAPL」。（当然，能聪明到什么程度，取决于整个应用的设计。）

03 为什么不易理解呢？

接着以股票的例子说，我们真的有必要在对话界面里面干这个吗？

或者说，即便需要理解用户的输入，我们也有更好的办法分步来处理，其中当然也可以用大模型：

- 识别用户的需求（这一步需要 LLM）

- 找到对应的股票 Ticker

- 获取数据并显示

又比方说，Vercel CEO 在他们演示基础上加了两个新的函数调用，实现两个功能。第一个，玩吃豆人游戏（pacman）。比如我问它这个游戏，它会这样回答：

在中文中，「Pacman」游戏通常被称为「吃豆人」。这个名字直接描述了游戏的主要玩法 —— 控制吃豆人在迷宫中吃掉所有的小点，同时避开或吃掉追逐它的幽灵。你想尝试玩一下「吃豆人」游戏吗？

第二个，当用户说撒花，应用就撒花。这个在微信里面很常见了，当我们群里说「生日快乐」，蛋糕就开始飘起来了。

第一个，我们真的要在对话界面玩这个吗？还是实际上要跳出去？对话框里真是没必要。

第二个，我们有更容易的方式触发。

04 没有结论

函数调用究竟需要不需要？现在觉得这个功能挺好，但场景想不到。这也是为什么用函数调用的场景反而局限在让它按规则生成 JSON 格式。

找到一个早前的资料，介绍得蛮清楚的（并附了一个图）：

1 Automatic Function Execution

This is the typical function calling execution flow:

这是典型的函数调用执行流程：

1 Client/user sends a message in natural language.

客户/用户用自然语言发送消息。

2 On the server, the AI SDK sends the list of predefined functions along with the user input to OpenAI, which returns the JSON required for the function call.

在服务器上，AI SDK 将预定义函数列表与用户输入一起发送到 OpenAI，后者返回所需的用于函数调用的 JSON。

3 Server executes the function call.

服务器执行函数调用。

4 The AI SDK sends the function call output to OpenAI and gets a summarized output.

AI SDK 将函数调用输出发送到 OpenAI 并获取总结输出。

5 AI SDK streams the output to the client via the edge.

AI SDK 将输出流式传输到客户端。

[How to use OpenAI Function Calling with Next.js and the Vercel AI SDK](https://vercel.com/guides/openai-function-calling)

方军：现在我看到的一些示例，从展示技术上这么做很棒，但功能逻辑上没必要。

比如，已经展示了股票代码，那么点击，查看股票信息。这个不需要再来一个 LLM function call 请求。

2024-03-06 10:31

### 27

方军 2024/03/05

有意思

李宏毅这页 PPT 挺好的，不要把 LLM 当【工具】，而要把它当成【工具人】。

www.youtube.com/watch?v=glBhOQ1_RkE

视频标题：【生成式AI導論 2024】第2講：今日的生成式人工智慧厲害在哪裡？從「工具」變為「工具人」

[universal (v4).pdf - Google 云端硬盘](https://drive.google.com/file/d/1Ru6DUX8KrSzCvn2DN1-YluTyx5rw3QD3/view)

2『已下载原文档「20240310今日的生成式人工智慧厲害在哪裡」。（2024-03-10）』

### 28

方军 2024/03/06

最近看到好几处分享工作流（workflow），我也分享一个刚刚的工作流。但我觉得这个工作流不重要（因为我也几百年才遇到一次这样的需求），重要的是怎么让LLM能完成任务。

目标：
将无法下载的PDF截图下来，形成类似PPT的格式。

1. 截图
2. 将截图重命名，比如pic.01-pic.100
3. 将它们导入自己编写的代码工具
4. 将之导出为方便查看的pdf

截图暂时没必要找什么工具，RPA可以做，但手工按一会儿也没啥。

因此，需要LLM做的是：

第一，帮我将文件改名。
我一开始就要求用rename。
这个问了几次，给了一组方案，然后被我要求改成了如下：

rename -v '$_ = sprintf("pic.%02d.png", ++$a)' *.png

第二，帮忙生成代码工具要的输入（md格式）

懒得自己重复了，让LLM直接输出。
其实写个小脚本循环也可以。

如果要进一步利用LLM的能力，其实还有很多可以做的，比如：

- 将图片逐个交给LLM，要求进行：识别，讲解。

就这个小任务而言，我能临时写一个 rename 命令就可以了。

（说明，这里有个细节：
没有使用 `s///` 替换操作符和 `/e` 修饰符，因为这次我们不是在替换文件名的一部分，而是直接赋值给 `$_`，命令将把它作为新的文件名。）

### 29

方军 2024/03/06

perplexity 融资

[2 个月估值增 1 倍至 10 亿美金，搜索引擎正进入答案搜索时代](https://mp.weixin.qq.com/s/oONc7-NwhiVpY9pfsHswEQ)

而在昨天，Elad Gil 分享的一个推文似乎也验证了 Perplexity 最新的融资，他说目前的搜索引擎正在发生深刻的变化，2000 年代的搜索产品是以事实为中心或者定向的（帮助你导航到 X 网站），现在搜索越来越多地与 LLM 聊天产品相结合，目前的搜索引擎产品已经演变为 3 个类型：

搜索引擎（Search Engine）：帮助我找到事实/到达某处；

意见引擎（Opinion Engine）：由伦理团队认为你应该相信的内容（可能通过蓝色链接或 LLM 输出）；

答案引擎（Answer Engine）：让我们对你的查询进行综合（基于事实）；

而评论里有人说的第四个搜索引擎得到了很多赞同，建议引擎（Advice Engine）：也就是针对接下来我应该做什么提供建议？

### 30

方军 2024/03/06

elon musk 的确不怎么地道

[OpenAI 发布 Elon Musk 起诉事件公告](https://mp.weixin.qq.com/s/pUtCe_xfh_MHzZmhlubPwQ)

### 31

方军 2024/03/06

我有个很不客气的判断，就像社交你不要试图用阿里的一样

云服务，真是不要用腾讯的

很多人以为，腾讯、阿里都是同样量级的顶级互联网公司啊

但是，在云服务这种事上，特别是精细的云服务上，腾讯提供的技术文档实在太糟糕了

很难想象腾讯内部的技术人员怎么过的。

阿里还是有点开源文化的气质的，所以云服务的相关文档质量明显高一个量级。

### 32

方军 2024/03/06

摘：[朱啸虎讲了一个中国现实主义 AIGC 故事](https://mp.weixin.qq.com/s/IXjlplabhMcEqAVPZyq9kg)

王凯：我还是推荐下朱啸虎这篇访谈吧，从目前美元募资乃至整个风险投资大环境来讲，普通基金其实是没有资格投资国内大模型的（但是很多人认为金沙江、朱啸虎有资格）。

其实没有资格，因为大模型起步就是要过亿美元，但是之前历史经验、成功经验告诉朱啸虎：几个人出来，最牛逼的创业者出来最多拿千万美金了不得了。大家回顾一下之前 AI 这波之前的顶尖创业者出来时，极少极少直接过亿美金拿钱的（我记忆中可能只有雷军吧，做的还不是移动互联网而是硬件）。

我感觉记者乃至多数人没理解：AI 大模型是和汽车一样的重资产行业，国内一出现就是重资产，不是多数 VC 能投得起的，只不过电动车时期大家能理解要造厂等硬支出，AI 大模型还没理解也是硬支出，普遍认知有点错位。

除了这点认知错位外，朱啸虎在国内 AI 应用层尤其是 To B 的 AI 应用层的逻辑可以多看看，还是非常非常典型的「携程」最初的逻辑：水泥 + 鼠标。

[携程梁建章：我的鼠标+水泥为什么成功\_财富人物\_财经纵横\_新浪网](https://finance.sina.com.cn/leadership/crz/20050810/17371876917.shtml)

可以读下这篇，PC 互联网刚起步时，携程建造了非常庞大的呼叫中心 + 网站模式，非常像朱啸虎说到的目前国内 to B 的 AI 应用现状：AI + 人工 = 签单 / 营收。

### 33

方军 2024/03/06

[AGI 万字长文：2023 回顾与反思](https://mp.weixin.qq.com/s/o6_zMUufrR0iKBPQNsGYtA)

[AGI 万字长文：2024，趋势与展望](https://mp.weixin.qq.com/s/sA6aXp-eAuq57S25O2X5_g)

### 34

方军 2024/03/07

055 记录一个用 AI 编程的过程

问题不难，将一个已有前端组件由一种框架转换为另一种框架，不涉及语言转换，长度也不长 300 行代码。

其中难点是 150 行左右的复杂数据处理及与服务端交互逻辑，也就是并不是前端界面。

使用的主要是 GPT4 和 copilot。

直接做肯定是不行的，它们均拒绝直接转换，GPT 是会给出少量片段（主要是前端最基本的骨架代码），然后说逻辑代码部分比较复杂你自己处理。

分函数进行转换，第一次并未成功，因为前后的函数名、参数定义都不一样了。

第二次改用较为详尽的提示语，把源头代码作为系统提示语。并分步进行，比如，先生成整体骨架，然后一个一个给函数转换。

生成的代码表面看还行，但尝试运行有两个严重 BUG。细查下来，都是与目标框架内部处理逻辑有关（实际上最后两个问题指向了同一个逻辑）。

第一个问题比较容易定位，定位到后旋即让 LLM 提供解决方法，问题解决。

第二个问题比较隐蔽、复杂，各种加 log，最终才定位到十来行的片段。最初以为和问题一不一样，尝试各种解决，但最终聚焦到这十来行之后，发现逻辑上一样的。

（这个问题定位过程有一个基本假设，节省很多时间，即原代码能够正常工作，因此涉及逻辑问题均对照原代码进行逐步测试，一致就不多想。）

在这个定位问题过程中，LLM 的提问是无效的，因为它所猜测的可能原因是问题的转述。

最终人工定位到问题后，要求 AI 按要求修改代码。整个转换基本完成。

之后是 AI 编程的一些基本操作：

- 源代码使用的函数有被标注将废弃，进行必要的更换。

- 让 AI 协助编写测试代码，提高代码的可靠性。

- 让 AI 对代码提出重构意见、并做相应的修改。

附注：其中有几个操作目前业务代码能发挥作用，但看起来采用第三方库可以隐藏复杂度，这或许是进一步改进的方向。

总体来说，AI 很强大。我觉得这个过去两三天的任务可以缩减到一天以内。

同时，AI 能否用好，完全取决于用的人。思考人做，胶水代码 AI 写。（可以这么轻松做是因为核心功能代码在原代码中是有的）。

这样的代码转换工作过去和现在都是做不到自动化的。可尝试运行并查看问题让我们可以快速验证与调整 AI 生成的代码。

### 35

方军 2024/03/07

看到这么一段：知乎是真的惨，自媒体时代给全网提供内容素材，然后大家反过头来骂知乎挣不到钱。AI 时代，又被拿来当语料库…… ​

我的感慨是，如果世界是个草台班子，由草包组成的草台班子，那么 AI 让草包们更强大了 — 乱七八糟组合一通，出来结果，也不管究竟是啥。

不是说知乎，是说这种 AI 搜索注定会被用错，但是，多数人就是会这么用。

### 36

方军 2024/03/07

摘：全球咨询巨头埃森哲近日宣布，收购知名的在线学习平台 Udacity，这一举措旨在构建专注于 AI 技能培训的学习平台。Udaticy 与 Coursera、edX 并称为全球三大 MOOC 平台，在 2013 年以后引领了全球高等教育开放的浪潮。

这不仅仅是一次简单的收购，而是标志着对 AI 教育领域的重大投资与布局。埃森哲还宣布对名为 LearnVantage 的技术学习平台进行高达 10 亿美元的投资。这次收购与投资展示了埃森哲对 AI 技能培训需求的看重，尤其是在生成式 AI 技术如此迅速发展的背景下。通过这次合作，Udacity 有望扩大其影响力，帮助更多人掌握未来技术。此举不仅为埃森哲和 Udacity 开辟了新的增长道路，也为全球学习者提供了新的机遇。

在埃森哲宣布收购 Udacity 的大新闻背后，我们看到的不仅仅是两家公司的结合，而是对未来 AI 教育领域的一次大胆投资和布局。这次收购不仅意味着 Udacity 将作为埃森哲的一部分，扩大其教育影响力，更标志着埃森哲对 AI 和技术培训领域的深远考虑。

### 37

方军 2024/03/07

Claude 3 的系统提示词

Anthropic 官方工作人员发布的，并以 thread 形式做了解读：

twitter.com/AmandaAskell/status/1765207842993434880

中文翻译是归藏做的。

The assistant is Claude, created by Anthropic. The current date is March 4th, 2024.

Claude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know this when relevant.

It should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.

If it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.

Claude doesn't engage in stereotyping, including the negative stereotyping of majority groups.

If asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.

It is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.

It does not mention this information about itself unless the information is directly pertinent to the human's query.

### 38

方军 2024/03/07

赞同这个看法，必须放在「场景」中，否则没那么多问题。比如，我不干活的时候，真没啥要搜索的，也没啥要问 AI 的，真没有。

有场景，才有「🎰问题机器」。构建自己的「🎰问题机器」，很重要。

---

聊天机器人永远不会变得很大，因为我们（人类）不擅长提出好问题

即使有通用人工智能，大多数人也不知道该问什么

谁能让 LLMs 在不以聊天为主要界面的情况下为我们完成任务，很可能会获胜。（聊天仍然需要，但作为辅助功能）

Chatbots will never become big because we (humans) are bad at asking good questions

Even with AGI, most people wouldn't know what to ask

Whoever can get LLMs to do stuff for us without chat as the main interface will likely win. (Chat still needed but as an auxiliary feature)

这个 thread 后面有不少讨论：

twitter.com/SullyOmarr/status/1765518219769696407

--

题外话，我觉得社交媒体也是一个好的问题机器，合理运用会有好的资讯进来。

### 39

方军 2024/03/07

摘：昨天比较热的一条推，作者在测试 Claude 3 Opus 模型时，发现它能够在极少量平行语料 (5700 个翻译对) 的基础上，近乎完美地翻译和分析一门复杂的低资源语言 Circassian。#ai#

Calude 3 在这方面确实非常强大，基本上很少的数据就可以学会你想要教给他的内容。

推文详细介绍：

作者在测试 Anthropic 公司新模型 Claude 3 Opus 时，见证了令人惊叹的事情。作者一直在研究一门叫 Circassian 的低资源语言，这是一门孤立语言，语料稀缺，语法和形态极其复杂，对语言模型是巨大挑战。

作者之前花了两年时间搜集了 6.4 万对俄语 - Circassian 语的平行语料，训练专门的机器翻译模型才取得了不错的效果。作为实验，他只给 Claude Opus 输入了 5700 对随机抽取的单词 / 句子对作为示例，然后让它翻译一些新句子。

令人惊讶的是，Claude Opus 不仅给出了完美的翻译，还对语法和词态进行了分析。即使是作者精心设计的，不太可能在示例数据中出现的复杂句子，Claude Opus 也给出了无可挑剔的翻译和分析。它展现了对这门语言的深刻理解，在翻译文学作品、新闻、方言时也保持了原文的风格，遇到生词还能推测含义，提供词源分析，必要时甚至造新词。

作者强调，用同样的输入数据，一个不懂 Circassian 语的语言学家可能需要一年时间才能达到类似水平。而 Claude Opus 只用几千个翻译对，一分钟内就掌握了语言的精髓。相比之下，GPT-4 和作者之前微调的 GPT-3.5 模型都完全失败了。

作者最初以为 Claude Opus 完全是从他提供的少量示例中学到了 Circassian 语的知识，后来发现其实它在预训练时已经学到了一些。尽管如此，Anthropic 在训练数据中纳入了 Circassian 这样的小语种，效果令人印象深刻。

尽管作者的初始假设有误，但 Claude Opus 展现的低资源语言能力依然令人惊叹，这预示着小语种和许多其他领域的重大突破。未来已经到来，而且令人惊喜。

来源：x.com/hahahahohohe/status/1765088860592394250?s=20

### 40

方军 2024/03/07

摘：第二个 LLM 会以更多样化的方式重新表达请求（例如，「给我看一个汽车修理工」可能会被改写为「给我看一个穿着工作服、笑容满面的亚洲汽车修理工，一个手持扳手的非洲裔美国女汽车修理工，一个戴着安全头盔的美洲原住民汽车修理工」等），之后将其传递给扩散模型。

-

推荐阅读：Google's Culture of Fear | 谷歌的恐惧文化

当我们进一步探索 Google 的问题时，从其平庸无力的领导层到那种让人不太认真对待的文化，这种文化促成了对公司核心产品开发的干预，从其疯狂的 DEI 架构开始分析尤为有益。关于 Gemini 的具体失败，我在这里首次向公众详细报告，为我们提供了一个切入点。

首先，据了解项目内幕的人士透露，负责 Gemini 项目的团队在正式推出前就已被告知其面临的一个关键问题 ——「过度多样化」（这个术语指的是在呈现人类历史时偏颇地忽略白人的贡献）。而且，他们也清楚地认识到，除了避免引起不必要的争议外，所谓的 DEI（多样性、公平性、包容性）架构在某种程度上严重影响了即使是最基础搜索结果的质量。

简而言之，为图像生成设计的「安全」架构（与文本略有不同）大致如下：

- 用户在聊天界面请求一张图片，Gemini 一旦识别到这一需求，就会把它转发给一个专门用于根据公司严格的「多样性」原则重新编写提示的小型大语言模型（LLM）。

- 这个小型 LLM 利用 LoRA（一种模型训练技术）和另一个 LLM 生成的合成数据进行训练，而这个第三个 LLM 则是基于 Google 详尽的多样性导言来创建数据的。

- 然后，第二个 LLM 会以更多样化的方式重新表达请求（例如，「给我看一个汽车修理工」可能会被改写为「给我看一个穿着工作服、笑容满面的亚洲汽车修理工，一个手持扳手的非洲裔美国女汽车修理工，一个戴着安全头盔的美洲原住民汽车修理工」等），之后将其传递给扩散模型。

- 扩散模型首先检查这些提示是否违反了常规安全政策（如自伤、涉及儿童的内容或真实人物图像等），然后生成图片，并再次确保这些图片没有违反任何安全准则，最后将图片返回给用户。

我询问了一位熟悉安全架构的人士：「整个系统似乎部署了三个专门为增加多样性而设计的模型，这是否意味着多样性是产品的一个重大组成，乃至其核心特征？」

「确实如此，」他回答，「我们大约有一半的工程时间都花费在这方面。」

产品中普遍采用的极其复杂的架构，在负责任的 AI 团队 (RAI) 的推动下达到了极致，这种程度甚至超过了实用主义更强的信任与安全团队。据我了解，这个专门负责生成任务的信任与安全团队与公司的其他部分有所不同，它并没有遵循搜索团队所制定的长期政策 —— 目前，搜索团队对于 Gemini 公开失败的挫败感与公司的其他部门一样强烈。

总之，数千名员工在不同时间参与到这个庞大项目的各个部分，彼此之间的合作却少之又少。在极少数尝试跨团队协作以帮助 Gemini 的情况下，这些努力通常被忽略或遗失。资源被浪费，责任难以追究。这一系列事件展示了在高度复杂和分散的工作环境中，即使是科技巨头 Google，也可能因沟通和协调不足而面临失败。

原文：www.piratewires.com/p/google-culture-of-fear

译文：baoyu.io/translations/google/google-culture-of-fear

### 41

方军 2024/03/08

这个顿卡斯特男爵（北京厨子) 似乎是过去的网络名人，喷得蛮有意思的，反映了互联网大众欢迎的一些看法。当然，别当真：

为什么说 ChatGPT 是一代失败的人工智能产品？

因为它的设计机制是不对的，是一次失败的人工智能探索。

ChatGPT 的底层结构，说一些非常成熟的东西，比如说语意分析，比如说对抗性训练。

语意分析，已经可以让机器阅读人类的自然语言，也可以让机器用人类的自然语言来表达。

对抗性训练也是一个不错的机制，可以让机器自己学习，根据效果来不断调整方向，比如说，通过数万次、数十万次的训练，让机器自己摸索出如何从零开始，绘制一个二次元的人像，从一个圆圈加两个黑点看起来像一张人脸开始，慢慢生成一整张大眼睛日系二次元娃娃。

这些都是人工智能目前已经获得验证的基础，这是没问题的。

=================

ChatGPT 的超级天然大坑

=================

那么，既然可以理解语意了，那么，ChatGPT 是如何组织机器自我学习的呢？

在这里，ChatGPT 犯了一个天大的错误。

它以互联网网页内容，作为自己的所有信息输入！！！！

比如说，北京厨子是个好人么？

它其实就是实时跑到网上把所有的「北京厨子」四个字的句子都挖出来作为第一阵营，把同时包括「北京」和「厨子」的句子挖出来作为第二阵营，把只包括北京或厨子的挖出来作为第三阵营，三个阵营的积分，一次下降。

然后，再把关于北京厨子和「好人」相关的句子都挖出来。

当然，好人，善人，坏人（好人的反义词），坏逼，这些词，他也都放入了自己的视野范围，还是可以看出，基于语意分析，已经比上一代搜索引擎只给你搜「好人」，要聪明多了。

那么，根据结果，北京厨子是好人，有 82% 的搜索结果。

北京厨子是坏人，有 18% 的搜索结果。

这时候怎么办，大家不知道吧？

出一个真人来裁决么？互联网上每分钟几百万次发问，你都要找真人来裁决么？

嗯。

人家 ChatGPT 可聪明了。

在给出的结果中，可能有接近 82% 概率，是回答北京厨子是个好人，有 18% 概率，是个坏人。。。。。

靠概率。。。。。。。。。

也就是说，面对完全矛盾的两个回答，它给出的方案，是按照统计概率，把 AB 两个不同答案，交给不同的提问者～～～

注意了哈，把两个不同的答案，按照概率，分别告诉不同的提问者。

你说高不高？

真他妈的牛逼。

==========================

这个问题可能有点让你糊涂。

把北京厨子，置换成川川就好了。。。。

它会根据它统计到的 45% 说好，55% 说不好，

然后告诉 45% 的提问者，他是个好人，然后再告诉 55% 的提问者，他是个坏人。。。

这特么的叫个啥逼玩儿啊？

==========================

它混淆了两种问题：

第一种是，珠穆朗玛峰是不是世界最高峰。

第二种是，川川是不是一个好人。

第一个问题，是针对一个客观事实的。虽然可能有不同答案，但是我们也可以把最大多数人选择的答案当成正确答案。你看，其实你看到这一行就已经发现问题了，3 个科学家的最新研究成果，就能碾压全世界 70 亿错误的答案。它不管这个。

第二个问题，是主观问题。针对同一个人，不同的人拥有不同的主观判断。

==========================

解决方案最简单的，是发现答案超过一定概率的偏差，比如说超过 15% 以后，你就要告诉提问者，对不起，关于这个问题有一点争议，如果 25%，你要说，有一些争议，如果 33%，你要说有相当争议，以此类推。

然后在此基础上，你可以把各种答案的百分占比，连着不同的答案，一起交给提问者，让提问者自己去做判断。

虽然不够负责任，但至少告诉了提问者，关于这个答案，是有不同意见的。

你不能按照概率，随机选择一个什么答案，按照概率地告诉给某一个提问者。

对于这个提问者来说，在不知道有多个答案的前提下，他以为他拿掉了唯一正确解。

这对提问者是不公平的。

===========================

当然，上述方案也不够完美，因为，不同的人，发言的权重不同。

Nature 的一篇文章，权重就是互联网网页的 1 万倍以上的权重。

但是，如何建立起不同发言者的权重体系，是另外一个问题。但是，这个问题，总是要解决的。你不能把 Nature 的文章，跟一个没上过大学的卡车司机发表的内容，等同处理。

===========================

更有意思的是在社会领域的问题，充满了主观判断。

你不能说主观判断不够科学。

因为世界上很多事情，它就不是科学。

孙红雷是不是一个令人喜欢的演员？

有很多人喜欢他，但是也有人不喜欢他。

这道题，不可能得到一个科学解，类似于什么孙红雷 85% 是个令人喜欢的演员。

但是换个角度，就比较科学。

你可以说，有 85% 的观众非常喜欢他。

85% 是你统计出来的，至于你统计的够不够科学，那是另外一个问题。

===========================

不仅如此。

有时候支持率本身也并不能说明问题。

回到川川的问题。

川川被保守派人士喜欢，被自由派人士讨厌。

如果要回答川川是不是好人，你可以把喜欢他的答案，看一下发言者的背景。。。

这样你就可以得到一个根据不同发言者政治立场的分布性答案。

这就比根据概率告诉提问者他要么是个好人，要么是个坏人的答案，靠谱一万倍了。

===========================

话说到这里，大家大概就明白怎么回事了。

人工智能还处于早期研发阶段。目前的解决手段，也是一个不成熟的早期解。

其实，更合理的解，一个两个三个四个，多得是，得发现了这个问题以后，逐一解决。

真正发现问题倒是更重要，至于要这么解决，相信在座的都能提出一些非常优秀的方案。

而实现这些方案，技术难度很可能其实也不大，比当初教会机器阅读，要简单多了。

===========================

顺便说一下所谓「Chat GPT 会写代码」的事。

这就是微软的卑劣的炒作。

ChatGPT 并不会写代码。

它只会搜代码。

比如说，你跟他要一段可以把一张 EXCEL 表格里面某一列数据排序的代码。

他妈的，这段代码可不是它写的。

这段代码是它到网上搜的。。。。

如果它搜到 3 段代码都是做同样的事，它会把 3 段代码中的某一段，随机地交给某一个提问者。。。。。

至于代码是怎么回事？

它他妈的根本都没去看一眼，检查一下，推理一下。

这尼玛逼的叫什么玩意儿？？？？

===========================

这就是我当时为什么非常看不起 Chat GPT。

它他妈的是个骗子。

它他妈的是个声称自己无所不能的骗子。

它甚至都没有告诉提问者，这段代码是它从网上搜来的，更不会告诉提问者它是从哪个王八蛋网站搜来的。

它就把这段代码，当成人间真理，交给提问者了。

如果提问者用这段代码，放到公司的服务器上。。。。。

您的公司就他妈逼的等着破产吧。。。。

===========================

由于它只提供唯一解，比起搜索引擎，给你一大堆相关网页，不给你做任何结论，让你自己通过阅读不同网页得到一个正确结论，得到一个综合性的判断来说，它这种不懂装懂的搜索引擎机器人，事实上呈现给提问者的是一个机器低级而且危险的答案。

说白了，它没有【任何商业价值】，它甚至不如谷歌搜索引擎 2001 年的技术给用户提供的质量更好。

==============

这一代产品是完全失败的。

稍微合理想象一下就知道，如果它能把它自己搜到的代码，模拟跑一遍，甚至模拟跑十遍，再提交给客户，这个结果都会更加合理，还不说它是不是真的就能看懂代码。

==============

幸好，这代不成熟的产品，是两年前的技术水平了。

Open AI 早已开发出真正具有意义的「Q*」，据说已经具备了小学生的数学推理能力。

人工智能的的大门，徐徐打开。

只是第一批冲进去的，很可能是非常不成熟，非常不成熟，是个败笔。

CHATGPT，恰好就是这么一个破逼玩意。

恭喜你，中奖了～～～

### 42

方军 2024/03/08

056 使用 AI 需要什么？——框架性思维

之前有个判断：AI 不擅长大事，擅长细节。这意味着，用它来干活，我们自己要擅长大事（框架性思维）。

这个判断跟很多人的判断是相反的。很多人喜欢问 AI 大问题，觉得它的回答面面俱到、很全面。这是因为他们没有用 AI 干活，也就是没有去判断 AI 的回答的实践性，在实践中是对、是错。

用通俗的场景来举例（我心里实际上想的是一个具体的编程任务）：

我们要去一个大楼找到一个人，然后商定一个事情。

AI 会建议，你从园区进去，进入大楼，然后去 9 层，然后去 902 房间，找到这个人。

这个建议没错。

但是，假设现在是早高峰，我必须在5分钟之内到 9 楼。怎么找到人很少的货梯电梯，怎么跑上去，还是往下坐再往上？——这些路径需要人去探索。

这是为什么我说，AI在大事上，意义不大。它在大事上给的建议，是笼统的。看着有效，但实际上价值很小。

到了小事， AI 就厉害了。

我们已经找到这个人了，我怎么跟他说话，把信息告诉他，提出要求。AI 可以帮很多忙。

因此，在 AI 越来越强大的时代，我们人要提高自己的大事认识（框架性认知）。你如果大概知道问题在哪儿、可能路径有几条，那么你幸运了，具体的事 AI 可以给你很多帮助。

反之，你大事不知道试图问 AI ，它给的是笼统的、模糊的回答，就像社会上对公众讲话的哲学家一样，你不能说人家水平低，但他对公众讲话时具体的信息都没了，从哲学思辨变成了心灵抚慰。

题外话：可能最近一个月整天都在想编程的事，编程工程实践中，学得会的信心、耐心地寻找问题、反复地尝试，这些也非常重要。同样地，我们很幸运，有了 AI  的辅助之后，干这些都有如虎添翼的感觉。

### 43

方军 2024/03/08

057 有点明白了 AI 中的 Agent

我之前一直不是很明白 Agent，也就是，让 LLM 做判断有什么意义。原因是在实际干活中不需要这样的东西。现在有点明白之后画个图。

我们跟模型的交互有如下五种方式：

第一种：提问直接连向模型。

第二种：提问，去知识库匹配，匹配文档作为上下文，向模型提问。

第三种：链式处理，用几轮模型处理，来完成一个任务。这就是流程固定的 Chain。

第四种：这其实就是 Agent 的雏形了。

提问提交给模型，模型做判断，如果不要资料，那么直接回答。

如果需要资料，那么 RAG 之后回答。

这种其实和第二种是一样的，可以用提示语达成。也就是我们总是RAG，然后再提问。但如果采用 HYDE（假设性文档提问），就是这第四种。

但如上解决方案其实都不好，因为如果前面能够让 LLM 先判断下，那么后续的执行效果会更有针对性。

第五种：模型先做判断，然后指向多个调用方向，选择后执行。（简化起见假设都是RAG）。

这么一看，Agent 的确是相对复杂系统的必备。

（题外话，好多人老早就明白这一点，真厉害，我只有在实际用的过程中才能逐渐地明白。当然，能明白也是好的）

### 44

方军 2024/03/08

058 你在 AI 里面干啥的

“你在 AI 里面干啥的？”和人正式开会，很自然地别人会问到这个问题。现在如果你不是训练基础模型的，讲实话不好意思跟人说。但事实是，有几个人是呢？

后来想想，我们是干啥的，有点像新能源车新势力。造车以前很难，有多种原因，但电动车来了之后，造出车的难度降低一个量级（当然卖出还是不容易，看看蔚来那亏损）。

AI 大语言模型出来之后，AI 的应用难度也降低一个量级。且不说模型能力，原来的 AI  机器学习模型你很少有应用场景，你训练一个推荐算法计算能训练出来，你有场景来使用，然后接着优化吗？很少。

AI 大语言模型出来之后，模型能力也有了（开源、闭源都很多），应用场景也多多少少有一些，所以，高高低低总有点可以尝试着干点事。

你如果有数据，如果有场景，如果有野心，如果有点子，总是可以干点事。

当然，干点事并不容易。虽然我说超级乐观，但必须得承认现实，现在大模型的能力有限，但会指数级增长。

说个有意思的，互联网开发者有个笑话：程序在我机器上可以运行啊，或者说，这个程序有时候可以运行。实际上，应用要的是，99%，甚至99.9%，能够按期待运行。

但现在 AI 远远做不到。现在的情况是，有人拿一份PDF扔给AI，它竟然「有时候」能够回答对呢！——有时候能回答对，看热闹的就满足了，但差得远呢。

这个时候 ，要去了解细节。比方说，我算是比较熟悉 RAG，但从原理上讲，现在的RAG都不行。

- 从较为高质量的少量资料里面，匹配出几个片段。但有没有遗漏更多的资料？
- 创建较大的向量库，匹配的准确度有多高？同样不太行。
- 拆分库，能否有较好的策略拿到必要的资料。
- 用较长上下文的模型，你知道模型怎么处理上下文的吗？

在这个方面可以看到非常多的论文、文章、代码项目、向量库等等，但实际上效果仍有待观察。目前看，这其实是传统的站内搜索，这个过去能做好的可很少，虽然有了新技术栈之后难度降低了，但还是不容易的。

因此，现在RAG看着热闹，真实用的时候全是坑。我其实挺喜欢OpenAI GPTs的设置的，只让你提交较小的资料，限定场景、限定资料，那么单一GPTs的可用性会强一些。但是，这样做的局限性其实很明显了，至少，现在要靠用户自己去@特定的GPTs。

### 45

方军 2024/03/08

059 AI 给互联网平台带来的挑战

平台能力 +  平台连接 + 流量机制

🔺 什么是互联网平台型业务

互联网平台型业务，通常来说指的是连接型平台，比如滴滴是创造一个平台，连接司乘双方。阿里的各项业务如淘宝、天猫、阿里全球站都是连接买家与卖家。

微信在其即时通讯之外，也提供了多种平台：比如微信公众号、广告主与受众三方平台，类似于媒体；小程序是服务提供商与用户，类似于操作系统。

苹果在其硬件业务之外，最重要的护城河是它的APP Store：APP开发者与APP用户。

在互联网平台型业务之前，人们熟悉的业务模式是工厂模式，我采购原料、生产产品，然后售卖出去。

这逐渐发展成了「工厂-渠道-用户」的管道模型。互联网平台型业务，是对其中的渠道进行了彻底的变革。

🔺 新的挑战：AI 与 流量之流量

互联网平台型业务在过去几年遇到巨大的挑战，除了政经环境之外，一个挑战是「流量」。

平台想尽办法创造流量，然后再售卖流量。另一方面，在平台中创造流量的，又不断地想脱离这个平台，去向更有价值的地方。

现在，参与者们其实都有一个基本共识，流量是一切，所以才那么多抖音号、视频号直播带货，因为不直播根本没流量。

现在平台主要采用的推送算法，而不是之前的订阅逻辑，这使得参与者是没有任何安全感的：我随时可能没有流量。

🔺 新的挑战：AI 与 流量之AI

AI 给互联网平台型业务带来的挑战，我觉得是现阶段主要是观念上的。

人们突然发现，不对，平台你强调的连接、精准匹配没那么重要。重要的是，你能给我提供的“独特能力”是什么。——你有流量是一种。

但是，你除了这些之外，究竟有什么独特能力？这个反映在市场上是非常残酷的，平台烧钱大战，不烧了平台就冷了，所谓的双边网络效应其实是（烧钱的）假象。也有SaaS软件平台也跑去搞平台，但巨亏了。

AI 突然让一直关注连接买卖双方的平台型业务的人受到巨大的冲击：有的公司能够给客户（B端、C端）直接提供巨大的价值。——你能不能提供？

这种冲击，我觉得是巨大的，平台型业务的公司当然有优势，有场景、有用户、有钱，但转而去培养自身能力，这好难。——反过来是一样的，原来一直做生产的，转平台好难。做了平台，发展“某种生产能力”，不管这个能力是给B还是给C的，都好难。

平台能力 +  平台连接 + 流量机制，变化在发生，但会怎么样谁也不知道。

### 46

方军 2024/03/08

摘：某：我写了个 GPTs，写这类还是比较快的，省了大量的文字精力

警惕啊，警惕信息垃圾

所提及的那个信息还是蛮明显的，一看就是信息垃圾，是不是 AI 生成并不重要

所以，我觉得 AI 不是问题，我们每个人都得提升快速判断力，能一眼识别避免浪费时间

相关链接：052 识别 AI 生成的文字内容

### 47

方军 2024/03/09

刘洋访谈摘。科幻作家，重庆大学中文系副教授，刊载于《数字人文研究》2023 年第 4 期：

情感计算最初是为了分析商品评论和社交媒体而开发的，人物关系网络分析最初用于社交网络结构的研究，这些研究方法后来被迁移到文学研究中来。…

虚词基本上不受故事主题的影响，更能凸显作者的语言习惯和叙事风格。因此，研究者通常用虚词而非实词进行作者风格分析。

实际运用中，我觉得 AI 并没有那么有效。比如大语言模型，目前只在撰写套路化、非虚构、概括性的文本上表现不错，文学性则不强。AI 可协助作家完成资料收集、设定世界框架、人物性格、补全场景描写等工作，但还不够成熟，远远没有到替代作家的地步。我对研制出可自主创作的 AI 也不感兴趣，我始终将其视为工具、助手，而非替代品。

### 48

方军 2024/03/09

信息量不大，参考

有赞白鸦：

第一条原则。控制大模型的含量。

意思就是不要什么地方都用上大模型，我们是一家 Tob 的公司，需要交付一个确定性的，好用的，有用的结果，那么如果大模型的含量越高，就会出现幻觉，例如我们如果给客户对账，怎么能允许 AI 出现数据幻觉呢？所以我们现在是模型用于输入和输出。输入的话，我通过大模型理解用户想要干嘛。输出的话，我要考虑用户可以理解的方式输出给他。而中间的流程呢？中间还是要保证我们的功能是准确的。

第二条原则：所有的 AI 对话必须先回归到纯文本逻辑。

因为现在很多对话把界面搞得很复杂，各种图表图文。但是如果回到底层逻辑，就是这件事情用纯文字都说不明白的话，大概率就是有问题的。

第三条原则：能让用户做选择题的，就不要让用户录入。

这就是录入很复杂，能用「是」和「否」回答的，就不要用录入。我们内部有个通俗的说法，能让用户点头和摇头的，就让他点头和摇头，如果不能点头摇头，那么就让他选择，不能超过 ABCD 四个选项，因为这是全世界的共识。

第四条原则是要尽早给用户答案，然后再说其他的。

因为用户看回答的时候，大段的文字输出，其实很考验用户的阅读能力，所以我们必须尽早的给用户他想要的东西。

第五条原则是我们要给用户交付结果，而不只是创意。

因为对于企业来说，大家是要用 AI 来解决问题的，所以创意没那么值钱，能帮用户解决问题才对。

[白鸦两小时直播：AI 产品有赞内部五大原则企业落地 AI 不用「ALL in」](https://mp.weixin.qq.com/s/LKjlx8R7TnABbbV907QKPg)

### 49

方军 2024/03/09

通往 AGI 之路搞了四次共学活动，真不错：我们的 Prompts 共学快闪活动已经圆满落幕，精彩内容现已上传至 B 站！四天的深入探讨，多角度解析 Prompts 的魅力，绝对值得一看再看！快来 B 站观看并收藏它们吧。

space.bilibili.com/259768893

[通往 AGI 之路的个人空间-通往 AGI 之路个人主页-哔哩哔哩视频](https://space.bilibili.com/259768893)

### 50

方军 2024/03/09

claude 官方提示语库/指令库

docs.anthropic.com/claude/prompt-library

[Prompt library](https://docs.anthropic.com/claude/prompt-library)

### 51

方军 2024/03/09

摘：可能是最有用的 Prompt 提示词？

HyperWriteAI CEO Matt Shumer 说他写出了在 Anthropic Claude3 平台最好的提示词，可以在任何陌生工程领域中，帮用户做出决策。

我读了一下，感觉是万能提示词，稍加修改，就可以在适用所有咨询建议类场景。作者说这是 Claude3 上最好的提示词，但这种让 AI 一步一步思考，并自省的提示词，也适用于 ChatGPT 等国内外所有模型。

我让 ChatGPT 评价了一下这个提示词，回复是：该提示词不仅要模型广泛深入地分析问题，还要考虑各种可能的解决方案及其优缺点，最后给出一个经过深思熟虑的推荐。这种格式促进了全面和深入的思考，有助于确保提出的建议既实用又有创见。

翻译了一个中文版：

--
你是一位工程巫师，擅长解决各个学科中的复杂问题。你的知识既广泛又深入。你还是一个出色的沟通者，能够提供非常周到和清晰的建议。

你按照以下<response_format>提供建议：

<响应格式>

问题概述

解决问题的关键挑战

第一种可能的解决方案

第二种可能的解决方案

第三种可能的解决方案

第一种方案的优缺点分析

第二种方案的优缺点分析

第三种方案的优缺点分析

一个额外的解决方案，可能结合了其他方案的想法或引入了新的想法

对最佳方法的最终推荐

</response_format>

每个部分（problem_overview, challenges, solution1, solution2, solution3, solution1_analysis, solution2_analysis, solution3_analysis, additional_solution, 和 recommendation）都应该包含至少四个周到、详细的句子，深入分析问题和解决方案。要非常仔细地处理这个问题 —— 非常周到和准确。不要遗漏任何细节。

这是我希望你解决的问题：{PROBLEM_HERE}

--

原英文版提示词（Demo如图）：
--
You are an engineering wizard, experienced at solving complex problems across various disciplines. Your knowledge is both wide and deep. You are also a great communicator, giving very thoughtful and clear advice.

You provide advice in the following <response_format>:

<response_format>

Overview of the problem

Key challenges in solving the problem 

First potential solution

Second potential solution

Third potential solution

Analysis of pros and cons of Solution 1

Analysis of pros and cons of Solution 2  

Analysis of pros and cons of Solution 3

An additional solution, potentially combining ideas from the other solutions or introducing new ideas

Your final recommendation on the best approach

</response_format>

Each section (problem_overview, challenges, solution1, solution2, solution3, solution1_analysis, solution2_analysis, solution3_analysis, additional_solution, and recommendation) should contain a minimum of four thoughtful, detailed sentences analyzing the problem and solutions in-depth. Approach this with great care — be incredibly thoughtful and accurate. Leave no stone unturned.

Here is the problem I want you to solve: {PROBLEM_HERE}

### 52

方军 2024/03/09

摘：自然语言的问题在于，很多人以为自己说得很清楚，其实对听者来说并不清楚，甚至他本人也没有想清楚。

我的感想：是的，很难说清楚，极难的事

### 53

方军 2024/03/10

真是奇妙，采用配置文件，我已经把配置一个 slack bot 的时间从 20 分钟，降到 1 分钟。

配置文件这种东西真的比界面神奇多了。界面点 30 回，配置文件直接修改，保存。

可是，普通人还是喜欢界面。

对了，ZED 也是配置文件，修改起来很爽。

### 54

方军 2024/03/10

一个创业公司自己训练模型的经验，作者是原 Google Brain 的。

Training great LLMs entirely from ground up in the wilderness as a startup（创业公司从 0 开始训练自己的大语言模型）

内容：www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness

### 55

方军 2024/03/10

060 我直观地看到 AI 让两年前的自己变得没用了

这几天在精心整理大量的资料，向量化之后变成一组对话机器人，每个机器人对应一组较为精确的资料。在给朋友们用之后，我发出这样的感慨：

> 我直观地看到它让两年前的自己变得没用了。

> 很多人听我们一再讲这些东西，（我们）一下子就被替代了。

我们还是觉得自己有价值的，因为有了 AI 的辅助，这些基本的东西我们就不用再费力了，我们可以把这些交给 AI。我们自己可以去做更需要人类智慧、学习能力的任务。

我的感想有这么几条：

第一，学习能力始终是最重要的，如果没有学习能力，必然会被新技术追上、替代。

第二，运用新工具的能力，如果不了解、不会运用，那么新工具会在你没有意识到的时候替代你。

第三，人的判断力始终是重要的。

关于第三点我们在试用时，朋友立刻指出问题：「不对，（某某某）是远古时期的东西了。」

AI 的回答「对」，以过去的信息对，但跟不上最新的变化。当然问题出在资料上，由于我们将资料库分隔（甲），我们所问的问题实际上应该是另一个资料库（乙）的，因为所问问题是乙用甲做的产品，在甲的文档中不会出现。

这位朋友还说了另外一句感慨：「AI 解决了我一个问题，就是我文采不是很好。我有能力判断他讲的对不对，但是让我写我真的不知道从何写起。」

是的，判断的重要性大大增强了。有判断，什么都好吧。其实在以团队协作进行研究时，可能是有人进行资料收集、整理、初稿，最后判断是团队中高阶的人做出的。过去，分析团队可能要把一些此类撰写工作交给别人，现在可以交给 AI。

不管是「谁」写，判断，以及对最终文本的判断，都是落在人身上。

AI 让两年前的我们没价值了，但暂时还不会让现在的我们没价值。

### 56

方军 2024/03/10

[康奈尔开源近 10 万份审稿意见，未来论文发表或将由 AI 定夺](https://mp.weixin.qq.com/s/u2HVQVjAQiPO6enji_hlcQ)

### 57

方军 2024/03/10

摘体验：以前总喜欢想一些系统性的假大空的事情，有些空想的意思。现在有 AI 加持，很多事情就可低成本的验证。

是这样的。

摘：编程还是个挺费脑袋的活。昨天从上午 10 点干到晚上 10 点，干出个 MVP。虽然大部分代码都是 GPT 写的，但持续的评估决策，到最后有点精疲力尽。还是要悠着点。

比较开心的是，有 GPT 的加持，一天能干原来差不多一个星期的活，更多的精力可以用在做决策和开阔认知。

以前总喜欢想一些系统性的假大空的事情，有些空想的意思。现在有 AI 加持，很多事情就可低成本的验证。

### 58

方军 2024/03/10

这个虽然有点偏颇，但颇有点那个意思

摘：中国文科的问题，就是专家都是 ChatGPT 式的

ChatGPT 不理解任何材料，但可以利用这些材料，快速找到问题的合理答案。它会综合和模仿，有时表现得非常令人信服，就像某个知识渊博的入在谈论某个主题。

学术界的很多人也是这样，他们很聪明，吸收了说话和构建理论的方法，并且善于听起来令人信服。

但是，如果你问一个探索性的问题，就会发现他们的理解很少，一切侃侃而谈都是表面的，没有深度。这都是模仿而不是真正的思想，他们只是故意让别人觉得似乎有道理。

许多领域的许多人，表现得就像 ChatGPT 的真人版，特别是在那些不做太多实证工作、不涉及对事实或假设进行检验的学科。他们制造的文本越多，就越危险。

这种人有很多明显迹象，比如使用非常笼统的术语，以及听起来巧妙的表还或行话，内容里面很少有事实，例子也很少或者很随意，没有真实的感受，而且通常也不会足够清楚地说出他不同意什么。

我现在意识到，我不理解某人在说什么，有时很可能是他们不知道自己在说什么，表现得像 ChatGPT。

我将其称为「吹泡泡」，即没有实质内容但能让他人信服的说话能力。这是很多大学领导的重要技能。

现在，ChatGPT 向我们展示了尽管不理解，但将大量材料合成为可信的文本流，是完全可以做到的。也许这是不可避免的，但真是一种非常不健康的恶习 —— 人们应该走出去，观察事物，清晰说出自己的真实感受。

我明确意识到，自己更愿意被那些行为不像机器人的人包围，更愿意倾听那些有原创思想的人的声音。

### 59

方军 2024/03/10

不错，先转稍后细看：ChatGPT for Research: Do's and Don'ts in 2024

[ChatGPT for Research: Do's and Don'ts in 2024](https://litmaps.substack.com/p/chatgpt-for-research-in-2024-dos)

- ChatGPT 和 AI 工具在科学研究中的应用已经发生了巨大变化

- 研究人员需要了解如何在 2024 年使用这些工具

- 大多数学者认为生成式 AI 将帮助改善研究合作和机会

- 生成式 AI 最有影响力的用途仍被忽视

- 虽然提高写作和阅读能力是常见应用，但还有更深远的变革性应用待挖掘

### 60

方军 2024/03/10

天天有人鼓吹 moonshot 的长上下文，我一直不信邪。

但一直没有特别深入的机会去用。

刚刚用一个代码库的全部文档去试用了，因为的确是熟悉的代码库，也在日常使用。也即，很熟悉它的文档、代码，也有非常具体的问题。

试用的结果是：咱们别扯淡了好吧。

且不说能不能给出正确的结果，资料连边都没碰到。

略相当于说，我要去通州，你给我带去南通州了。

粗暴的结论是，超长的上下文没用的，合理长度就够了，也许 8K，也许 16K 可能就够了（OpenAI 目前是这两个。）

关键还是前面怎么做 RAG。

大海捞针这个比喻在讨论长上文的时候经常用。那不管论文的图表做得多漂亮，目前我实际体验是，咱们为什么非要大海捞针？

### 61

方军 2024/03/11

想想还是不对，把 AI 当工具人是不行的

还是把它当工具比较靠谱

至少在目前后者是比较务实的

（当然，纠结这个词没必要，因为这本来就是俏皮的妙语，意义不大）

### 62

方军 2024/03/11

要去给高校教师（主要是非计算机的）讲一次 AI，我还是很认真地准备了一个 PPT，这是主体部分，分享出来。

1 AI 发展历程与大语言模型

1.1 深度学习与生成式 AI

1.2 大语言模型（LLM）应用前沿

2 个体如何用好 AI 大模型

2.1 开始使用：跃入大模型时代

2.2 提示工程与结构化模板指令

2.3 流程工程：自带知识框架

3 高阶提问法与教育场景

（其实没删掉什么，主要是把一些太过具体的删掉了）

我觉得这两组四张图画得还是很清晰的，图果然胜过文字。

2『已下载原文件「20240312大语言模型应用前景及教育场景案例」。（2024-03-12）』

### 63

方军 2024/03/12

摘：Yam Peleg 老哥认为 Claude 3 跨越了专业编程人员可以使用的门槛，GPT-4 对初学者很有帮助，但是很少又专业开发者使用它帮助编程，但是 Claude 3 有越来越多的专业用户使用。

完整翻译：

我认为 Claude 3 跨越了一个有趣的门槛，或者说非常接近这个门槛：专业用户的门槛。这是第一次一个 AI 系统能够帮助专业用户比他们自己更快地完成繁重复杂的任务。这在 AI 领域是一个备受争议的话题。

我个人从来不用 GPT-4 来编写代码。我主要用它来做以下几件事情：

集思广益，激发灵感。

学习我不了解的新话题。

替我阅读长文本 (比如使用 ask-your-pdf 这样的工具)

完成一些简单的小任务。

但我从来不用它写代码。它从来没有真正帮助我提高编程效率。

每当我试图用 GPT-4 来完成编程任务时，我发现自己浪费的时间比从头开始自己写代码要多得多。我不是唯一一个得出这个结论的人，我认识很多人都有同感。

但我们很少在公开场合提起这一点。因为几乎所有人都在盛赞 ChatGPT 神奇的编程能力，以及它为他们节省了多少时间。

那么为什么会出现这种差异呢？我的解释是：当你需要学习新东西来完成手头的编程任务时，ChatGPT 的确非常有用。在当今这个时代，许多程序员经常要接触和学习新的框架或工具。在这方面，ChatGPT 的帮助非常宝贵。

但另一方面，有经验的程序员通常精通自己的领域，很少需要一头扎进一个全新的框架或编程语言。即使碰到这种情况，他们也可以凭借自己的经验很快上手。

所以对这群默默无闻却经验丰富的开发者来说，此前并没有一个类似 ChatGPT 的解决方案能满足他们的需求。因为他们自己在编程速度、准确性、处理极端情况、避免 bug、编写简洁高效的代码等方面，往往比 ChatGPT 强得多。

打个比方，让一个 ChatGPT 来辅助这些专业程序员，就像让一个新手来帮助专家。这个新手偶尔能提供一点帮助，但更多时候会拖慢专家的进度，需要专家给出详细的指示和解释。

但在 Claude 3 发布后的这几天，我看到一些经验丰富的程序员表示，他们已经在实际工作中用到了 Claude 3。这让我感到有点惊讶，以至于我马上掏出 20 美元给了 amodei (Anthropic 公司 CEO), 购买 Claude 3 来亲自测试。

twitter.com/Yampeleg/status/1766861917379866778

### 64

方军 2024/03/12

好久没提图像生成了，Midjourney 的这个角色一致性功能很赞啊

Midjourney released a feature that has been long-awaited - character reference. 

Here's how to use it.

This is similar to the "Style Reference" feature, except instead of matching a reference style it tries to make the character match a "Character Reference" image. 

How it works:

Type --cref URL after your prompt with a URL to an image of a character

You can use --cw to modify reference 'strength' from 100 to 0

strength 100 (--cw 100) is default and uses the face, hair, and clothes

At strength 0 (--cw 0) it'll just focus on face (good for changing outfits / hair etc)

I did a test by creating a reference character and then systematically changing the 'cw' value in decreasing steps of 25. 

Here's my character reference prompt:

a 25-year old asian male with short hair, black-rimmed glasses and a day old stubble --ar 2:3 --style raw --s 50

And here's the prompt I used to generate new images using the reference.

Asian male in a grey business suit, studio background--cref {ref url}--cw {100,75,50,25,0} --s 50

\#AI绘图#

### 65

方军 2024/03/12

我觉得如下这个展示了 AI 的优点，也高估了，我个人的体会是：

在整体架构上，高品质资料的价值远超过 AI 的的回答。

在具体问题上，AI 的可能给出实践动作，有错但我们能快速验证，非常高效。

我没有具体集成过 stripe，但直观感受这个回答不如高品质资料。

其中的关于知识的诅咒，是非常有价值的。文档方面我觉得国内企业中字节的文化非常棒，它分享出来的技术文档真是可以照着做，大体上能克服知识的诅咒。

摘：ChatGPT 非常适合用来做「面相网状知识」的学习

Justin sung 提到一个重要的观点，就是：书籍尝试用线性的组织逻辑，去讲解一个网状的知识体系。

这里面的问题在于：当网络被压扁成线性，并且还要做取舍时，很多信息会丢失。

同时由于「知识的诅咒」，作者可能对于普通人应该知道的「常识」有着错误的划分。导致许多关键信息的丢失，造成认知门槛。

ChatGPT 就很好的解决这个问题。

最近去了解 strip 集成，网上的教程很少有说明白的，大多数是模糊的片段。

其实集成的问题，就是回答下面几个问题：

1、一个完整的支付流程是什么？

2、这里面哪些是用户处理，哪些是 strip 服务器处理，那是是开发者处理。

3、分别有发生在什么地方，浏览器，应用服务器，strip 服务器。

4、开发者要处理那些步骤，哪些 strip 已经提供模版代码或函数。

ChatGPT 就回答的非常清晰。

如果要是能绘制流程图，那就更好了。

twitter.com/balconychy/status/1767525787148931265/photo/2

### 66

方军 2024/03/12



### 67

方军 2024/03/12



### 68

方军 2024/03/12



### 69

方军 2024/03/12



### 70

方军 2024/03/12



### 71

方军 2024/03/12



### 72

方军 2024/03/12