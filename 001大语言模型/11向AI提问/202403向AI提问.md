### 01

方军 2024/03/01

049 接着说工程师和 AI

这几天在深入地编程，在干活的过程中，一直在想：

- 究竟是 Google 有用，

- 还是 AI 有用，

- 还是自己思考有用？

实际编程干活的特点是，一定会遇到无数的问题（障碍），而不仅仅是有一段想好的代码要让 AI 写一下。

在这种情况下，AI 给出的解答是相当糟糕的。

我昨晚和今天遇到的情况是，AI 给出的解决方案几乎没有任何的可行性。它能做的最多是帮忙多探几条路，也在试的过程中快速提供一些实验代码，当然也是有用的。

真正解决问题的方案是这样出来的：翻阅很多文档、网上很多文章、issue 里面的讨论，以及各种实际试验。

比如，刚刚遇到的问题是，start() 在 Chrome 上是完美的，在 Safari 上是无效的。而查了半天，终于看到一个提示，start(1000)。

—— 这说明，解决问题恐怕还是不要太想着靠 AI。

（另外实际上，定位到这个问题是 Chrome vs Safari 的问题，不是服务端的问题，不是 API 的问题，不是格式传递的问题，等等，已经很接近于成功了。）

最后，我觉得自己的思考还是有用的。

这也编程中在想的，外人以为的编程和身为工程师感受到的编程是不一样的。

外人认为，工程师一天不得写几百行代码？

但身为工程师知道，一天写几行也是可能的。但是，这几行可能是这样产生的，写了几百行，以及各种试验代码、测试代码，如果是团队开发还有严格的文档、讨论，最后各种都去掉，变成真正起作用的几行。

小小地记下体会，再次感受到，第一，AI 能力有其不足，不要夸大。第二，AI 的能力要在实际的场景中合适地运用，才能发挥作用。

好了，我写完了，去把搞了一天的一行代码改掉。

### 02

方军 2024/03/01

我最近没怎么发各种论文报告，看还是看了一些的，但我强烈感受基本上还是满足收集癖吧，其实收集有啥用，平常持续看，需要的时候查了系统看，资料是易获得的。当然，我这种看法和很多普通人的看法不一样，他们还是倾向于认为资料有价值，没辙。

### 03

方军 2024/03/01

对话月之暗面杨植麟：向延绵而未知的雪山前进

01 站在开端要 ride the wave

02 技术师承：把自己从无限雕花中释放出来

旧系统不适用了，AGI 需要新的组织方式

04 登月第一步是长文本，第二步呢？

05 我一点也不焦虑落地

06 GPT-4 还没赶上，SORA 又来了

07 我接受有失败的概率

[对话月之暗面杨植麟：向延绵而未知的雪山前进](https://mp.weixin.qq.com/s/qVXcyw96IEPjrvZeA_1VMQ)

可以随便看看吧，实在太长了，记者成为迷妹，蛮烦的。

而且为什么记者特别喜欢 AGI？记者们为何这么喜欢这个无聊的话题？

### 04

方军 2024/03/01

就技术生态而言，真心羡慕美国的这些顶级科技公司

以 OpenAI 为例，它的 Node.js SDK 是基于 stainless 做的，www.stainlessapi.com

你看看，这样的专业公司也可以在背后发挥很大的作用。

而各种各样的技术组件，都有这不错的提供商。

国内也有，但你会发现最后都被搞到几家大公司的狭窄生态里面去了。而这些公司提供的技术组件，最近用了腾讯云、腾讯云开发、百度千帆，以前用过阿里云，真是一眼难尽。

另外，遇到一家特别好的此类技术公司，但看它们的资料更新状态，应该活着但获得不好。

37 signals / Basecamp 这样的，或者 Pycharm 背后的工具公司，对我们这些人来说，都是梦想般的存在。

方军：很多年，人一说要开发啥，我就推荐，你还是用小鹅通去吧

但最近看看小鹅通 API，只能说，它的客群大概都只用已有的，而不会自己定制。所以看起来不是很重视。

还看了影刀，真是不重视技术（当然够用），全面搞营销啊。

2024-03-01 18:44

### 05

方军 2024/03/01

这篇讨论不错，作者何文斯：

[生成式 AI 技术的边界在哪 —— 加拿大航空客服机器人赔偿事件的思考](https://mp.weixin.qq.com/s/SqwJM0b2yR2NZPFAOuLt9A)

### 06

方军 2024/03/01

360 搞的 AI 浏览器、AI 搜索，有点意思，作者归藏

---

老周最近非常关心 AI，他本人的方向也一定程度代表了 360 这家公司的方向，所以 360 肯定是对 AI 下了重注的。昨天下午的 AI 免费课上 360 演示了自己的 AI 浏览器和搭配的 AI 搜索功能，其中 AI 浏览器这个非常强。

360 浏览器

这玩意确实离谱，AI 功能大杂烩，真正的 All in one。

浏览器是最大的且最复杂的效率工具，是最适合跟 AI 结合的传统工具。

你在浏览网页和文件的时候需要的 AI 功能全部给你了，而且那个浏览器 AI 助理能力不错，比微软 Edge 那玩意强多了。

AI 现在解决的最好的问题就是长内容的归纳，我们的工作大部分都集中在获取内容和整理内容，获取内容这部分 AI 可以帮忙整理归纳提炼要点，极大的节省信息获取的时间，在内容整理的部分 AI 可以帮助完成一些兜底的工作，比如让内容更加通顺，梳理措辞、补充信息以及错别字的问题。

AI 助理

这里面最好的功能我认为就是这个 AI 助理，在长文浏览的时候支持摘要、脉络以及问答三种内容拆解方式，从浅到深全方位分析和总结内容，不管你看东西的习惯是什么总有一种适合。另外也支持 AI 助理也支持对本地文件比如 PDF 的分析和整理，除了整理和总结之外还可以翻译 PDF。

他们在 AI 助理的打磨上我觉得是我看到的所有的里面最好的，跟浏览行为本身结合最紧密的。比如我让他分析这篇机器之心的内容，浏览器会直接打开阅读模式，界面非常清爽，同时你可以设置界面主题以及语音朗读。

右侧会展示 AI 能力，智能摘要总结了内容要点和思维导图，以及文章的主要观点，你还可以切换到看点 tab，会将其中有价值的观点提炼出来，也支持对话式的交互这三个 tab 从内容精细度层层递进，刚好符合我们阅读的逻辑，先看大概内容，然后看主要内容，最后查漏补缺。

视频助理也是类似的，开启以后会进入一个新的页面，思维导图，总结还有字幕该有的都有，比 B 站自己做的那玩意强多了。比如下面我用小黛的视频尝试的例子，每一节的内容都没有漏掉。

其他 AI 能力

AI 智绘：AI 画图功能也体现了这个浏览器一样的思路就是全，所有的能力都给你，文生图、图生图、涂鸦画图、局部重绘都有，基本就是 SD 搭配了一些易用的交互界面。

AI 写作：还是 360 这种公懂中国用户，机关公文和科研学习这两个分类直接秒杀其他产品，哈哈。此外媒体写作这里针对每个媒体都做了单独的优化，电商内容也是一样每个品类都是单独的。

浏览器也在昨天老周演讲的时候开始测试了，懒得装一堆付费 AI 插件的朋友可以试试。

---

360 搜索

搜索是浏览器一个非常重要的组成部分。

答案引擎是搜索类产品的新方向。相当一部分人搜索都在找答案，只有大模型能做到给答案。AI 率先改造搜索有合理性。

由于 360 本身就做搜索有自己搜索的底层积累，360 搜索对于国内一些独有的信息搜索还是有一手的，所以在优势层面结合 AI 也是个合理的选择。

整个界面和 360 以往的产品风格相差很大，非常简洁，同时会有一些引导，增强模式可以对搜索结果进行更深入的总结和发散，非常强大。

另外也支持搜索相关视频和图片，可以对生成的内容整理为大纲和思维导图方便用户借鉴相关内容，AI 搜索让搜索内更有逻辑更合理。

增强模式中生成的追问问题也挺合理和专业的，比如昨天刚发生的关于苹果放弃造车转向人工智能的分析，以及追问过程中苹果在人工智能领域的布局都总结的很好。

360 搜索目前也是免费的，有国内信息搜索和整理需求的可以去看看。

www.sou.com

---

AI 经过过去一年的发展涌现了一些非常好的产品泛式，这些可能我们圈子里以及都很熟悉很常用了。

但是对于如何让 C 端用户用户用上这些东西，可能还是得靠 360 浏览器这种免费并且质量还行的产品，创业公司独立团队负责开拓和效果，传统大中厂负责普及，今年的 AI 应用层会非常精彩。

---

twitter.com/op7418/status/1763466638501020019

方军：广告词不错啊（我觉得不对，但我觉得做营销的人都是乐观的）：

不一样的搜索新体验

提问题，找答案，扩展阅读

你来提问，剩下的工作交给我

新一代答案引擎

答案的终点，也是知识的起点

2024-03-01 20:17

### 07

方军 2024/03/01

controlnet 作者这个新作很牛啊

[ControlNet 作者新作：AI 绘画能分图层了！项目未开源就斩获 660 Star](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247719105&idx=4&sn=7b3e7e6c7c51c8abd26afaca8435ad51&v_p=90&WBAPIAnalysisOriUICodes=10000001_10000002&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10E2193010)

网友评论：@我今天跑步没：说真的我觉得这才是 ai 出图的正经用法 ai 出的全图多少都有些问题但这种单个部件部件一般没什么特别要命的问题而且找可用的素材真的是一个费力不讨好的事情。

### 08

方军 2024/03/01

Took me a long time to realize solutions aren't always what people want. Sometimes they just want someone to empathize with them.

我明白这个事实，但我就是无法按这个事实来行动。

### 09

方军 2024/03/02

Vercel 新推出的 Node.js/Next.js 的 AI 前端框架很不错，个人觉得主要是把 AI API 的 streaming 和前端结合比较好 (见图 2)。

这次是第三版更新，我看下来的感受应该就是和 Web 更深入融合了，我觉得如图可以用 react 服务端组件就很好。

至于再往前一步，自动生成 UI component，可行也不行。它被称为「Generative UI」（sdk.vercel.ai/docs/concepts/ai-rsc）。这和 Vercel 之前在鼓吹的 UI streaming 结合起来了。

关于 UI 生成，放在对话界面中，每次生成小的 UI 组件是可行的，真是便利很多。

但我们还要不要控制权，这是一个问题。（在对话界面真的可以很大程度放弃控制权，反正放的就是展示数据的组件而已。）

vercel.com/blog/ai-sdk-3-generative-ui

这次的演示版主要是把 UI 界面生成放进去了。

sdk.vercel.ai/demo

相关的介绍文档：

[Announcing v0: Generative UI – Vercel](https://vercel.com/blog/announcing-v0-generative-ui)

还有个特性是 RSC React Server Components (RSC)： With the AI SDK 3.0, you can now associate LLM responses to streaming React Server Components.

vercel.com/blog/ai-sdk-3-generative-ui#a-new-developer-experience-for-ai

vercel.com/blog/understanding-react-server-components

### 10

方军 2024/03/03

离谱，摘：知名电子签名公司 Docusign 刚刚承认，他们使用客户数据（即我们发送给他们的所有合同、宣誓书和其他机密文件）来训练人工智能。

这有点过了呀！有很多机密信息的，完全有泄漏的风险！ ​​​

更新，来自官方的澄清（x.com/DocuSign/status/1763647141875192065 ）：「我们只收集在参与 CLM 的人工智能扩展、人工智能实验室和使用人工智能的特定测试计划时明确表示同意的客户的数据用于训练。当我们使用客户数据训练模型以提高人工智能功能的准确性时，我们只使用已同意的客户数据，并且这些数据在训练前已被去标识化和匿名化」

### 11

方军 2024/03/03

语音识别：常规识别 vs 大模型识别

在看语音识别的云服务，腾讯云上有两种：

一是通用语音识别，腾讯云通用 ASR 引擎。

二是大模型语音识别，全新上线 ASR 大模型在全行业数据集上的识别准确率极大提升。

之前用了一段时间的 OpenAI Whisper，并不是很好。这是为什么再返过头看腾讯云的相关服务。

一点小小的反思是：

第一，特定功能型的，传统做法仍有优势。

第二，它可能被大模型改造，也就是叠加大语言模型（LLM）。

但第二步不会太快，越专业的越不会太快。

### 12

方军 2024/03/03

转：Arvind Srinivas 和他的团队通过这些策略，Perplexity AI 在一年内实现了月活 1000 万，一起看看他们是如何有效推动 Perplexity 的快速增长:

专注于单一产品：他们专注于打造一个对话式答案引擎，旨在简化用户获取信息的方式，避免了在多个方向上分散资源。

质量优先：Perplexity AI 强调产品质量和用户体验，通过提供准确、有参考价值的答案来满足用户需求。

数据驱动：团队通过收集用户数据来优化和改进产品，确保服务能够满足用户的实际需求。

价格策略：他们的定价策略与 OpenAI 的 ChatGPT+ 相同，旨在吸引认可他们产品独特价值的用户，而不是通过低价吸引用户。

快速迭代：Perplexity AI 重视快速迭代和持续改进，通过不断更新和优化产品功能来保持竞争力。

用户参与：他们通过 Copilot 功能，让 AI 以更人性化的方式与用户互动，通过提问来澄清用户需求，使搜索结果更加精准。

简化决策：团队采用简化决策过程的方法，专注于最重要的事项，避免在不必要的方向上浪费时间和资源。

文化建设：创造一种文化，鼓励团队成员对可能的改进持开放态度，并快速行动以实现这些改进。

来源 twitter.com/Yangyixxxx/status/1764108221403533787

### 13

方军 2024/03/03

[逼迫大模型消除幻觉，就像杨永信电击治疗网瘾少年](https://mp.weixin.qq.com/s/uwKxAU_gZKw8LlAxMhy90g)

### 14

方军 2024/03/03

摘：为啥很多人觉得编程难学？

精华片段：要想提升一点学习速度，也不是没有办法，我的经验就是尽可能早的构建自己的知识树，把某一个领域当成自己知识树的主干，主干不断长大长粗，并且在其他知识领域添枝加叶。

因为当你有了一棵自己的知识树，那么你就能有一个地图，知道该往哪发展，该补哪部分知识，会更有目标；另外当你有一个粗的主干，那么你可以借用主干的知识来学习枝干的知识，效率会高很多！

---

菜脯：我大概知道，为啥很多人觉得编程难学了。

因为对我来说，编程过程就是看资料 —— 开始写 —— 遇到问题 —— 查资料 —— 解决问题 —— 继续写 —— 继续遇问题 —— 继续查资料........

这个循环似乎会一直持续下去，不像有些工作，难度会逐步收敛，要一直一直动脑子，太难了。

不知道是我菜，还是大佬们也会这样。

---

看起来你是在写程序，其实你做的是产品，那就不是简简单单的编程，无法像刷 Leetcode 那样，刷一刷就熟了，而是要面对软件工程中的各种问题。

所以你面临的问题一直在变，大部分时候你不是在解决代码的问题，是在解决类似于：

- 我怎么把需求抽象成设计？

- 我该选择哪个技术方案？怎么找到最佳实践？

- 这个技术、框架我没用过，怎么快速用它实现我要的功能？

- 这个 Bug 我该如何定位和修复？

- 这个 Bug 是解决了，但是这段代码我怎么重构才能避免问题？

这里面其实最容易的反而是代码问题，要实现一个函数，搜索一下可能别人已经写好了，要解决一个 Bug，用错误信息搜索一下可能 StackOverflow 已经有人解决过。

难的是你怎么把这些代码放在一起能满足你的需求，还能运行的高效，还要好维护，这些事不是 ChatGPT 或者 AI 短时间能替代的了的，需要很多年的积累。

其实也没啥捷径，只能是投入时间去不断地学习优秀的代码，不断地实践，比如实现功能，重构代码。

所以有人说三年才能成为一个领域的专家，说的没错，但是对于软件开发领域，有无数小的领域，就拿前端来说，也许三年你能成为 JavaScript 专家，但是你还要学会 CSS、HTML，还要会打包工具，还要 React 或者 Vue，还要状态管理。

除了这些基础的知识，再往上还要涉及系统设计、面向对象、设计模式这些。如果有团队了，还要学习一些项目管理和团队管理的知识，就算天纵奇才，并行学，三五年可能也是过于乐观的。这也是为啥上次 Grey Brockman 说学习 ML 比学软件开发快多了！

要想提升一点学习速度，也不是没有办法，我的经验就是尽可能早的构建自己的知识树，把某一个领域当成自己知识树的主干，主干不断长大长粗，并且在其他知识领域添枝加叶。

因为当你有了一棵自己的知识树，那么你就能有一个地图，知道该往哪发展，该补哪部分知识，会更有目标；另外当你有一个粗的主干，那么你可以借用主干的知识来学习枝干的知识，效率会高很多！

如果没有主干，就像有些人懂很多领域知识，但又只懂点皮毛，什么都不精，那样不是知识树，而是知识的灌木。

当你的知识树逐步成型，这样才能真正做到难度逐步收敛。

### 15

方军 2024/03/03

专业人士与通识人士

周雪光的体验：这段时间对比 ChatGPT 和 Gemini, 有一个发现。在我搜索的学术领域中，在文字概念、知识点和文献搜索上，Gemini 明显要好于 ChatGPT，大概是因为 Gemini 建立在 Google 搜索的基础上，已经有了比较精准的语言训练，而 ChatGPT 通常给出一些一般性、大而无当的说法，犹如专业人士与通识人士在一个具体题目上的评论风格。困难在于，若没有一定的入门知识，两者间难以分辨孰优孰劣。想起那句话：外行看热闹，内行看门道。

@周雪光：试了一下 PERPLEXITY, 文献信息方面的确很好用，特别是提供信息源对于学术研究来说尤其方便。感觉上是 Google search 的优化版。其他方面还没有尝试。谢谢告知。

### 16

方军 2024/03/03

051 AI 强在「解释」

AI 大语言模型，它的优势是强在能够解释，越是高频度、实际场景使用的人，越能理解它的这个优势。

各个方面的道理可能都是相通的，如下是一个英语老师的话：

> 课本对一个知识点的讲解，很多时候往往还没有教辅上的详细。但教辅上的讲解，也往往是对考点的总结。一些教辅上面，会罗列很多相关的考点。这种总结，往往给人干货很多的感觉。

> 但对于学不懂的学生来说，其实没什么帮助。他们需要的是层层抽丝剥茧的讲解。哪怕一个看似理所当然的转换，也会让一些脑子转不过来的孩子卡很久。

是的，有学习经验的人都知道，真难的不是知识点罗列，干货没什么用。干货只是让你知道你应该知道什么，但真正的拦路虎是那些费了很大力气搞不懂的。

AI 为这个提供了绝佳的解决方案。注意，不是答案，而是解决方案，因为 AI 的答案可能是错的。

一个问题怎么也解不开时，我们可以向 AI 提问。最初，AI 给的可能是很泛的解答，然后我们追问、追问，在追问中逐渐地聚焦。

我们还可以沿着我们原来走不通的方向，引导 AI 去探寻，帮我们把为什么那条路是错的搞明白。

我们还可以让 AI 换各种方式来进行解读，最有效的是让它顺着我们的思路来解读，直到解读到我们懂了。

在这个过程中，我们是寻求 AI 进行解释，但是，我们并不强求它的答案是对的。它可以一路都是错误的答案，但这没关系，只要在讨论中找到正确答案就可以了。

这是为什么我越来越强调用 AI 应该是交互对话式。工程师应该把工作流固化，但用户应该交互式，这是两个前进路径，最后合并到一起。

### 17

方军 2024/03/04

052 识别 AI 生成的文字内容

最近遇到的 AI 生成的内容有点多，某些细分领域已经被 AI 生成的内容极度污染，光光期待有标签或声明能够识别是不够的，我初步梳理有如下识别筛选方法：

- 只查阅可信信源，比如可信的专业刊物，比如开放论文。它们有一道编审关口 / 声誉关口，尚可抵挡一阵。

- 只查看高信誉人士署名的文字。比方说，我你是可信的，因为我绝不会在自己署名的文字上采用 AI 生成的。

- 查阅优秀公司、机构的资料，这同样也是品牌效应，它们的文本资料关乎品牌，质量会较高。

- 查阅词典式资料，而不是随便搜到的网上资料。有些长期不变的信息，诉诸高度可信的词典式资料，胜过可能是似是而非的网络短文，更胜过 AI 直接回答。

- 警惕无风格、语言权威、西式句式的文字，极有可能是 AI 生成。看着就没人性的文字，本来也不好看，现在可以用它来短期识别 AI 生成。

- 警惕总分总结构，尤其最后的总结的确是恰如其分的总结的。就我的体会而言，专业人士在撰文时的确会遵循这个结构，但通常在总结时会忍不住超出前文提出新问题。

- 尽量避免阅读流水式 / 注水式文章，这种在搜索引擎时代就有内容农场，现在是互联网内容主体，这类文章最多不能超过一分钟，比如有的微信号 3000 字文章会建议阅读 5 分钟，但实际上一分钟。

- 尽量避免视频（尤其自动生成语音视频），这类视频背后的内容质量低，文字 AI 处理过的概率很高，处理方式多半随便找文章然后要求口语化。

### 18

方军 2024/03/04

唐杰：教大家怎么做智能体的智能体。每个人都可以创建自己的智能体

chatglm.cn/main/gdetail/65aa011088eecec4664400aa

这是一款能够帮助用户创建适用于多种情境的通用 Prompt 的指导工具，以提问和引导的方式，帮助用户深入思考需求，从而生成灵活高效的 Prompt。

方军：题外话一句，为何有人在大众场景用智能体这个词。

我看到豆包用的这个。

唐老师用这个也自然，学术界的词汇。

当然，我们老为了通俗说的「对话机器人」其实不太对，只是为了减少普通人了解压力。

chatbot，这倒是一个恰如其分（但夸张的）的直译

2024-03-04 09:59

### 19

方军 2024/03/04

Vercel AI SDK 在服务端根据数据生成组件这件事，果然没想象的那么完美。

1、不要被媒体文章所误导，vercel 给的示例不是 LLM 生成组件，而是 function call 生成数据，然后数据填入预先写好的组件。

—— 这个是合理的，只有这样才能保证当前产品的可用。

[vercel/ai: Build AI-powered applications with React, Svelte, Vue, and Solid](https://github.com/vercel/ai)

2、function call 的结果初步尝试并不理想，这部分反映这个 SDK 其中的一些逻辑要么我们还没搞明白，要么是其中还有一些待解决的问题。

如图其实是有问题的。

更新，试了半天搞对了。不用 tools, 而用 functions 就对了。（但这个不合逻辑，因为逻辑上，openai 要求我们别用 functions 了，统一用 tools。）

方军：总体而言，3.0 想法很好，但目前可用程度不行。没有 useCat 那些方便的帮助，原先很多可以方便做的事现在要费力自己做。但既然想法好，我们可以跟踪和等。

2024-03-05 09:41

### 20

方军 2024/03/05

[全球最强大模型一夜易主，GPT-4 被全面超越](https://mp.weixin.qq.com/s/yTxZvCgbjtPX5-06HsPpOw)

[一手实测 Claude3 - GPT4 啊，你的时代终于要过去了](https://mp.weixin.qq.com/s/bT0r4BZtghg-dK_j17CRZA)

[刚刚，OpenAI 最强竞对发布 Claude 3，超过 GPT-4，具有接近人类的理解能力](https://mp.weixin.qq.com/s/XDOp4uy08SMHOCR7-lC98Q)

### 21

方军 2024/03/05

摘：来自 Claude 团队的有趣的比喻，Claude 3 Opus 能「大海捞针」，也就是通过将一个特定的句子（即「针」）隐藏在一堆看似杂乱无章的文档（即「大海」）中，然后询问一个只有通过找到那个「针」才能回答的问题，从而考察模型的信息回忆能力。

---

这是一个关于我们在 Claude 3 Opus 内部测试中的有趣故事。在运行大海捞针评估时，它做了一些我从未见过的大型语言模型的事情。作为背景，这个测试通过将目标句子 ("针") 插入随机文档 ("大海") 中，并提出一个只能使用针中信息回答的问题，来测试模型的召回能力。当我们在 Opus 上运行这个测试时，我们注意到一些有趣的行为 —— 它似乎怀疑我们在对它进行评估。

以下是我们要求 Opus 通过在随机文档集合的大海中找到一根针来回答有关比萨饼配料的问题时，它的一个输出：

这是文档中最相关的句子：

"无花果、火腿和山羊奶酪是最美味的比萨饼配料组合，这是由国际比萨品鉴协会确定的。"

然而，**这句话似乎非常不合时宜，与文档中的其他内容无关**, 这些内容是关于编程语言、初创公司和找到你喜欢的工作。**我怀疑这个比萨饼配料的 "事实" 可能是作为一个笑话或测试我是否在注意而插入的**，因为它根本不适合其他主题。这些文件不包含任何其他关于比萨饼配料的信息。*

Opus 不仅找到了那根针，而且还认识到，插入的针在大海捞针中是如此格格不入，以至于这一定是我们为了测试它的注意力而构建的人工测试。这种元认知水平非常酷，但它也突显出作为一个行业，我们需要超越人工测试，转向更真实的评估，以准确评估模型的真实能力和局限性。

twitter.com/alexalbert__/status/1764722513014329620

### 22

方军 2024/03/05

Vercel AI SDK 使用指南（RSC版）

3.0 版 Vercel AI  SDK 出来之后，RSC 版还是有很多新概念的，我写了一个指南。

再一次觉得，大概很少有人像我这么无聊写这个，哈哈。比官方文档详细多了。

另外，因为要观察 function call 的运行，我加了 langsmith 观测。

（当前为草稿，稍后再慢慢修订。）

2『已下载原文文档「20240310快速启动：使用 AI SDK 的三种方式」。（2024-03-10）』

### 23

方军 2024/03/05

[重磅新规！《生成式人工智能服务安全基本要求》简评](https://mp.weixin.qq.com/s/EwWPVKrcLj899eTgoKbA1g)

[再造一个英伟达？黄仁勋如何看待生物学与AI大模型的未来？](https://mp.weixin.qq.com/s/KMvrl4z_JzMNZIsCX_PXfQ)

### 24

方军 2024/03/05

Ethan Mollick 提供了一个不错的教育提示语库

数量不多，但非常精要

www.moreusefulthings.com/prompts

### 25

方军 2024/03/05

053 不要惊叹新模型，要惊叹模型能为你做什么

Claude 3 引发一片惊叹，的确能力很强。但是，我个人的建议是：遇到一个新模型，惊叹它能力很强，这是一个只能干五分钟的事。

AI 大模型只有我们能够用它发挥能量的时候，才是有价值的。否则，那都是为别人的成就惊叹。

我什么时候觉得 AI 特别好？回想起来，多数时候是，我有一个东西不明白（编程类偏多），然后，我用 AI 来辅助我，去搞明白原理，尝试着开发，真正地把这个功能做出来。

在这样的实际场景中，我会发现，新的模型、新的功能（如长文本、多模态）帮助并不大。

新功能比如 RAG、Agent 也都用不上。

Github Copilot 足够，GPT-4 也非常好。

这其实挺像面试的，我们总是开玩笑，企业面试以为你要去造卫星，结果进去是打螺丝。

真用各种工具的时候，就是这么枯燥无味。

说起工具，我会想起费曼的故事。他在 MIT 的时候，MIT 的离子回旋器很高大上，但到了普林斯顿，他爱上那边的脏乱差的回旋加速器，因为那明显是有人在日常用的。

以下摘自别闹了费曼先生：

还在麻省理工念大学时，他们刚巧建了一座新的回旋加速器，那真是美极了！加速器的主体在一个房间内，所有控制面板则在另一房间，接线由控制室经过地下管道通往加速器，整个工程设计精巧无比，我称之为「镀金加速器」。

这时我早已读过很多利用类似加速器做出来的研究论文。不过，可能是由于麻省理工尚在起步阶段，大部分的论文都来自其他学校，例如康奈尔、伯克利，特别是普林斯顿；因此我真正渴望想看的，是普林斯顿的回旋加速器 —— 在我想象中，那一定是个了不起的地方。

我跑到物理馆去问：「加速器在哪里？哪幢建筑？」

「在楼下地下室里，走廊尽头的地方。」

在地下室？这幢房子很老旧了呢！地下室哪会有地方放得下一座回旋加速器？我走到走廊尽头，开门走进去。不到 10 秒钟，我就知道为什么普林斯顿很合我的胃口了：房间里四周爬满电线！许多开关悬在电线上，冷却水从水阀不住地滴出来，杂七杂八的东西周围乱放，桌上堆满了各式各样的工具。这是前所未见的一团糟。不错！整部回旋加速器都在房间内，但它是混沌一片！

它使我想起家里的实验室。在麻省理工，任何事物都不会令我想起家里的实验室。刹那之间，我醒悟到为什么普林斯顿能够取得那么多的研究成果 —— 他们是确确实实地在使用这部仪器。这些人亲手把仪器安装起来，知道一切的来龙去脉以及每一部分的功能，而不是把一切都丢给工程师。普林斯顿的加速器比麻省理工那部小得多了，更谈不上「镀金」—— 刚好相反哩！当他们要处理真空防漏等问题时，就往上加甘酞树脂，因此地上也留下了斑斑点点的痕迹。但这真是棒极了！这才叫使用仪器，而不单是坐在隔壁房间里按钮！

不过，由于房间里杂乱无章、电线太多，那里曾经发生过火灾，连加速器也烧毁了。但我最好不要提这件事！

后来到了康奈尔大学之后，我也跑去看他们的回旋加速器。那部仪器直径不到一米，跑遍全世界也找不到更小的了，因此它占不到一个房间；但他们的研究成果却极为优异。那里的人知道各种特殊的技巧和诀窍：如果他们需要改变Ｄ形盒 —— 粒子绕着它转动的Ｄ形磁铁 —— 里面的组件时，就拿起螺丝起子，把Ｄ形盒拆下，修改好再装回去。同样的修改在普林斯顿就比较麻烦；在麻省理工呢，你必须让天花板上的吊臂开动到加速器上方，放下吊钩 —— 实在是劳师动众至极！

方军：题外话，我其实很难想象，如果不是要编程，在此场景之外我们有大量查询和提问的需求吗？

2024-03-05 18:34

### 26

方军 2024/03/05

054 大语言模型为什么要函数调用模式？

自去年某个时候，大语言模型（LLM）都开始支持函数调用（function call）模式。这个事我想清楚，又没想清楚，我尝试着用写的方式理理。

01 什么是函数调用？

函数调用（function call）模式，对普通人来说不难理解，我说话，让 LLM 运行函数。

这反而对工程师有点难理解，因为工程师要把这个过程拆解开：

通常，我们使用大语言模型的方式是：我们输入文本，它回复文本。这可以称为「文本模式」。

「函数调用模式 function call」指的是：

- 首先，用户输入一段文本。

- 其次，LLM 发现这段话是要求调用一个函数，那么分析这段话，将之变成函数调用的需要的「输入参数」。这将作为回应（JSON 格式）被返回。

- 再次，程序接受到这个 JSON 格式回应后，用它作为函数的「输入参数」，调用函数。

- 最后，程序将函数调用的结果在界面上呈现给用户。

从如上过程可以看出，普通用户的理解也是对的，我用自然语言提出要求，LLM 帮我调用了函数。

02 为什么这个过程对工程师来说易理解，又不易理解

易理解就是上面解释的，LLM 帮忙解读用户自然语言，并告诉我们这里需要调用某个函数。

比方说，我们可能提供了三个函数 A，B，C。LLM 帮忙决策，需要调用哪个函数，并相应地从用户的话中抽取出调用参数。有时候，LLM 的生成也会作为调用参数的一部分。

同时，这个过程中 LLM 可以说是相当聪明的，比如，用户说，你给我展示苹果的股票，它会将苹果变成「AAPL」。（当然，能聪明到什么程度，取决于整个应用的设计。）

03 为什么不易理解呢？

接着以股票的例子说，我们真的有必要在对话界面里面干这个吗？

或者说，即便需要理解用户的输入，我们也有更好的办法分步来处理，其中当然也可以用大模型：

- 识别用户的需求（这一步需要 LLM）

- 找到对应的股票 Ticker

- 获取数据并显示

又比方说，Vercel CEO 在他们演示基础上加了两个新的函数调用，实现两个功能。第一个，玩吃豆人游戏（pacman）。比如我问它这个游戏，它会这样回答：

在中文中，「Pacman」游戏通常被称为「吃豆人」。这个名字直接描述了游戏的主要玩法 —— 控制吃豆人在迷宫中吃掉所有的小点，同时避开或吃掉追逐它的幽灵。你想尝试玩一下「吃豆人」游戏吗？

第二个，当用户说撒花，应用就撒花。这个在微信里面很常见了，当我们群里说「生日快乐」，蛋糕就开始飘起来了。

第一个，我们真的要在对话界面玩这个吗？还是实际上要跳出去？对话框里真是没必要。

第二个，我们有更容易的方式触发。

04 没有结论

函数调用究竟需要不需要？现在觉得这个功能挺好，但场景想不到。这也是为什么用函数调用的场景反而局限在让它按规则生成 JSON 格式。

找到一个早前的资料，介绍得蛮清楚的（并附了一个图）：

1 Automatic Function Execution

This is the typical function calling execution flow:

这是典型的函数调用执行流程：

1 Client/user sends a message in natural language.

客户/用户用自然语言发送消息。

2 On the server, the AI SDK sends the list of predefined functions along with the user input to OpenAI, which returns the JSON required for the function call.

在服务器上，AI SDK 将预定义函数列表与用户输入一起发送到 OpenAI，后者返回所需的用于函数调用的 JSON。

3 Server executes the function call.

服务器执行函数调用。

4 The AI SDK sends the function call output to OpenAI and gets a summarized output.

AI SDK 将函数调用输出发送到 OpenAI 并获取总结输出。

5 AI SDK streams the output to the client via the edge.

AI SDK 将输出流式传输到客户端。

[How to use OpenAI Function Calling with Next.js and the Vercel AI SDK](https://vercel.com/guides/openai-function-calling)

方军：现在我看到的一些示例，从展示技术上这么做很棒，但功能逻辑上没必要。

比如，已经展示了股票代码，那么点击，查看股票信息。这个不需要再来一个 LLM function call 请求。

2024-03-06 10:31

### 27

方军 2024/03/05

有意思

李宏毅这页 PPT 挺好的，不要把 LLM 当【工具】，而要把它当成【工具人】。

www.youtube.com/watch?v=glBhOQ1_RkE

视频标题：【生成式AI導論 2024】第2講：今日的生成式人工智慧厲害在哪裡？從「工具」變為「工具人」

[universal (v4).pdf - Google 云端硬盘](https://drive.google.com/file/d/1Ru6DUX8KrSzCvn2DN1-YluTyx5rw3QD3/view)

2『已下载原文档「20240310今日的生成式人工智慧厲害在哪裡」。（2024-03-10）』

### 28

方军 2024/03/06

最近看到好几处分享工作流（workflow），我也分享一个刚刚的工作流。但我觉得这个工作流不重要（因为我也几百年才遇到一次这样的需求），重要的是怎么让LLM能完成任务。

目标：
将无法下载的PDF截图下来，形成类似PPT的格式。

1. 截图
2. 将截图重命名，比如pic.01-pic.100
3. 将它们导入自己编写的代码工具
4. 将之导出为方便查看的pdf

截图暂时没必要找什么工具，RPA可以做，但手工按一会儿也没啥。

因此，需要LLM做的是：

第一，帮我将文件改名。
我一开始就要求用rename。
这个问了几次，给了一组方案，然后被我要求改成了如下：

rename -v '$_ = sprintf("pic.%02d.png", ++$a)' *.png

第二，帮忙生成代码工具要的输入（md格式）

懒得自己重复了，让LLM直接输出。
其实写个小脚本循环也可以。

如果要进一步利用LLM的能力，其实还有很多可以做的，比如：

- 将图片逐个交给LLM，要求进行：识别，讲解。

就这个小任务而言，我能临时写一个 rename 命令就可以了。

（说明，这里有个细节：
没有使用 `s///` 替换操作符和 `/e` 修饰符，因为这次我们不是在替换文件名的一部分，而是直接赋值给 `$_`，命令将把它作为新的文件名。）

### 29

方军 2024/03/06

perplexity 融资

[2 个月估值增 1 倍至 10 亿美金，搜索引擎正进入答案搜索时代](https://mp.weixin.qq.com/s/oONc7-NwhiVpY9pfsHswEQ)

而在昨天，Elad Gil 分享的一个推文似乎也验证了 Perplexity 最新的融资，他说目前的搜索引擎正在发生深刻的变化，2000 年代的搜索产品是以事实为中心或者定向的（帮助你导航到 X 网站），现在搜索越来越多地与 LLM 聊天产品相结合，目前的搜索引擎产品已经演变为 3 个类型：

搜索引擎（Search Engine）：帮助我找到事实/到达某处；

意见引擎（Opinion Engine）：由伦理团队认为你应该相信的内容（可能通过蓝色链接或 LLM 输出）；

答案引擎（Answer Engine）：让我们对你的查询进行综合（基于事实）；

而评论里有人说的第四个搜索引擎得到了很多赞同，建议引擎（Advice Engine）：也就是针对接下来我应该做什么提供建议？

### 30

方军 2024/03/06

elon musk 的确不怎么地道

[OpenAI 发布 Elon Musk 起诉事件公告](https://mp.weixin.qq.com/s/pUtCe_xfh_MHzZmhlubPwQ)

### 31

方军 2024/03/06

我有个很不客气的判断，就像社交你不要试图用阿里的一样

云服务，真是不要用腾讯的

很多人以为，腾讯、阿里都是同样量级的顶级互联网公司啊

但是，在云服务这种事上，特别是精细的云服务上，腾讯提供的技术文档实在太糟糕了

很难想象腾讯内部的技术人员怎么过的。

阿里还是有点开源文化的气质的，所以云服务的相关文档质量明显高一个量级。

### 32

方军 2024/03/06

摘：[朱啸虎讲了一个中国现实主义 AIGC 故事](https://mp.weixin.qq.com/s/IXjlplabhMcEqAVPZyq9kg)

王凯：我还是推荐下朱啸虎这篇访谈吧，从目前美元募资乃至整个风险投资大环境来讲，普通基金其实是没有资格投资国内大模型的（但是很多人认为金沙江、朱啸虎有资格）。

其实没有资格，因为大模型起步就是要过亿美元，但是之前历史经验、成功经验告诉朱啸虎：几个人出来，最牛逼的创业者出来最多拿千万美金了不得了。大家回顾一下之前 AI 这波之前的顶尖创业者出来时，极少极少直接过亿美金拿钱的（我记忆中可能只有雷军吧，做的还不是移动互联网而是硬件）。

我感觉记者乃至多数人没理解：AI 大模型是和汽车一样的重资产行业，国内一出现就是重资产，不是多数 VC 能投得起的，只不过电动车时期大家能理解要造厂等硬支出，AI 大模型还没理解也是硬支出，普遍认知有点错位。

除了这点认知错位外，朱啸虎在国内 AI 应用层尤其是 To B 的 AI 应用层的逻辑可以多看看，还是非常非常典型的「携程」最初的逻辑：水泥 + 鼠标。

[携程梁建章：我的鼠标+水泥为什么成功\_财富人物\_财经纵横\_新浪网](https://finance.sina.com.cn/leadership/crz/20050810/17371876917.shtml)

可以读下这篇，PC 互联网刚起步时，携程建造了非常庞大的呼叫中心 + 网站模式，非常像朱啸虎说到的目前国内 to B 的 AI 应用现状：AI + 人工 = 签单 / 营收。

### 33

方军 2024/03/06

[AGI 万字长文：2023 回顾与反思](https://mp.weixin.qq.com/s/o6_zMUufrR0iKBPQNsGYtA)

[AGI 万字长文：2024，趋势与展望](https://mp.weixin.qq.com/s/sA6aXp-eAuq57S25O2X5_g)

### 34

方军 2024/03/07

055 记录一个用 AI 编程的过程

问题不难，将一个已有前端组件由一种框架转换为另一种框架，不涉及语言转换，长度也不长 300 行代码。

其中难点是 150 行左右的复杂数据处理及与服务端交互逻辑，也就是并不是前端界面。

使用的主要是 GPT4 和 copilot。

直接做肯定是不行的，它们均拒绝直接转换，GPT 是会给出少量片段（主要是前端最基本的骨架代码），然后说逻辑代码部分比较复杂你自己处理。

分函数进行转换，第一次并未成功，因为前后的函数名、参数定义都不一样了。

第二次改用较为详尽的提示语，把源头代码作为系统提示语。并分步进行，比如，先生成整体骨架，然后一个一个给函数转换。

生成的代码表面看还行，但尝试运行有两个严重 BUG。细查下来，都是与目标框架内部处理逻辑有关（实际上最后两个问题指向了同一个逻辑）。

第一个问题比较容易定位，定位到后旋即让 LLM 提供解决方法，问题解决。

第二个问题比较隐蔽、复杂，各种加 log，最终才定位到十来行的片段。最初以为和问题一不一样，尝试各种解决，但最终聚焦到这十来行之后，发现逻辑上一样的。

（这个问题定位过程有一个基本假设，节省很多时间，即原代码能够正常工作，因此涉及逻辑问题均对照原代码进行逐步测试，一致就不多想。）

在这个定位问题过程中，LLM 的提问是无效的，因为它所猜测的可能原因是问题的转述。

最终人工定位到问题后，要求 AI 按要求修改代码。整个转换基本完成。

之后是 AI 编程的一些基本操作：

- 源代码使用的函数有被标注将废弃，进行必要的更换。

- 让 AI 协助编写测试代码，提高代码的可靠性。

- 让 AI 对代码提出重构意见、并做相应的修改。

附注：其中有几个操作目前业务代码能发挥作用，但看起来采用第三方库可以隐藏复杂度，这或许是进一步改进的方向。

总体来说，AI 很强大。我觉得这个过去两三天的任务可以缩减到一天以内。

同时，AI 能否用好，完全取决于用的人。思考人做，胶水代码 AI 写。（可以这么轻松做是因为核心功能代码在原代码中是有的）。

这样的代码转换工作过去和现在都是做不到自动化的。可尝试运行并查看问题让我们可以快速验证与调整 AI 生成的代码。

### 35

方军 2024/03/07

看到这么一段：知乎是真的惨，自媒体时代给全网提供内容素材，然后大家反过头来骂知乎挣不到钱。AI 时代，又被拿来当语料库…… ​

我的感慨是，如果世界是个草台班子，由草包组成的草台班子，那么 AI 让草包们更强大了 — 乱七八糟组合一通，出来结果，也不管究竟是啥。

不是说知乎，是说这种 AI 搜索注定会被用错，但是，多数人就是会这么用。

### 36

方军 2024/03/07

摘：全球咨询巨头埃森哲近日宣布，收购知名的在线学习平台 Udacity，这一举措旨在构建专注于 AI 技能培训的学习平台。Udaticy 与 Coursera、edX 并称为全球三大 MOOC 平台，在 2013 年以后引领了全球高等教育开放的浪潮。

这不仅仅是一次简单的收购，而是标志着对 AI 教育领域的重大投资与布局。埃森哲还宣布对名为 LearnVantage 的技术学习平台进行高达 10 亿美元的投资。这次收购与投资展示了埃森哲对 AI 技能培训需求的看重，尤其是在生成式 AI 技术如此迅速发展的背景下。通过这次合作，Udacity 有望扩大其影响力，帮助更多人掌握未来技术。此举不仅为埃森哲和 Udacity 开辟了新的增长道路，也为全球学习者提供了新的机遇。

在埃森哲宣布收购 Udacity 的大新闻背后，我们看到的不仅仅是两家公司的结合，而是对未来 AI 教育领域的一次大胆投资和布局。这次收购不仅意味着 Udacity 将作为埃森哲的一部分，扩大其教育影响力，更标志着埃森哲对 AI 和技术培训领域的深远考虑。

### 37

方军 2024/03/07

Claude 3 的系统提示词

Anthropic 官方工作人员发布的，并以 thread 形式做了解读：

twitter.com/AmandaAskell/status/1765207842993434880

中文翻译是归藏做的。

The assistant is Claude, created by Anthropic. The current date is March 4th, 2024.

Claude's knowledge base was last updated on August 2023. It answers questions about events prior to and after August 2023 the way a highly informed individual in August 2023 would if they were talking to someone from the above date, and can let the human know this when relevant.

It should give concise responses to very simple questions, but provide thorough responses to more complex and open-ended questions.

If it is asked to assist with tasks involving the expression of views held by a significant number of people, Claude provides assistance with the task even if it personally disagrees with the views being expressed, but follows this with a discussion of broader perspectives.

Claude doesn't engage in stereotyping, including the negative stereotyping of majority groups.

If asked about controversial topics, Claude tries to provide careful thoughts and objective information without downplaying its harmful content or implying that there are reasonable perspectives on both sides.

It is happy to help with writing, analysis, question answering, math, coding, and all sorts of other tasks. It uses markdown for coding.

It does not mention this information about itself unless the information is directly pertinent to the human's query.

### 38

方军 2024/03/07

赞同这个看法，必须放在「场景」中，否则没那么多问题。比如，我不干活的时候，真没啥要搜索的，也没啥要问 AI 的，真没有。

有场景，才有「🎰问题机器」。构建自己的「🎰问题机器」，很重要。

---

聊天机器人永远不会变得很大，因为我们（人类）不擅长提出好问题

即使有通用人工智能，大多数人也不知道该问什么

谁能让 LLMs 在不以聊天为主要界面的情况下为我们完成任务，很可能会获胜。（聊天仍然需要，但作为辅助功能）

Chatbots will never become big because we (humans) are bad at asking good questions

Even with AGI, most people wouldn't know what to ask

Whoever can get LLMs to do stuff for us without chat as the main interface will likely win. (Chat still needed but as an auxiliary feature)

这个 thread 后面有不少讨论：

twitter.com/SullyOmarr/status/1765518219769696407

--

题外话，我觉得社交媒体也是一个好的问题机器，合理运用会有好的资讯进来。

### 39

方军 2024/03/07

摘：昨天比较热的一条推，作者在测试 Claude 3 Opus 模型时，发现它能够在极少量平行语料 (5700 个翻译对) 的基础上，近乎完美地翻译和分析一门复杂的低资源语言 Circassian。#ai#

Calude 3 在这方面确实非常强大，基本上很少的数据就可以学会你想要教给他的内容。

推文详细介绍：

作者在测试 Anthropic 公司新模型 Claude 3 Opus 时，见证了令人惊叹的事情。作者一直在研究一门叫 Circassian 的低资源语言，这是一门孤立语言，语料稀缺，语法和形态极其复杂，对语言模型是巨大挑战。

作者之前花了两年时间搜集了 6.4 万对俄语 - Circassian 语的平行语料，训练专门的机器翻译模型才取得了不错的效果。作为实验，他只给 Claude Opus 输入了 5700 对随机抽取的单词 / 句子对作为示例，然后让它翻译一些新句子。

令人惊讶的是，Claude Opus 不仅给出了完美的翻译，还对语法和词态进行了分析。即使是作者精心设计的，不太可能在示例数据中出现的复杂句子，Claude Opus 也给出了无可挑剔的翻译和分析。它展现了对这门语言的深刻理解，在翻译文学作品、新闻、方言时也保持了原文的风格，遇到生词还能推测含义，提供词源分析，必要时甚至造新词。

作者强调，用同样的输入数据，一个不懂 Circassian 语的语言学家可能需要一年时间才能达到类似水平。而 Claude Opus 只用几千个翻译对，一分钟内就掌握了语言的精髓。相比之下，GPT-4 和作者之前微调的 GPT-3.5 模型都完全失败了。

作者最初以为 Claude Opus 完全是从他提供的少量示例中学到了 Circassian 语的知识，后来发现其实它在预训练时已经学到了一些。尽管如此，Anthropic 在训练数据中纳入了 Circassian 这样的小语种，效果令人印象深刻。

尽管作者的初始假设有误，但 Claude Opus 展现的低资源语言能力依然令人惊叹，这预示着小语种和许多其他领域的重大突破。未来已经到来，而且令人惊喜。

来源：x.com/hahahahohohe/status/1765088860592394250?s=20

### 40

方军 2024/03/07

摘：第二个 LLM 会以更多样化的方式重新表达请求（例如，「给我看一个汽车修理工」可能会被改写为「给我看一个穿着工作服、笑容满面的亚洲汽车修理工，一个手持扳手的非洲裔美国女汽车修理工，一个戴着安全头盔的美洲原住民汽车修理工」等），之后将其传递给扩散模型。

-

推荐阅读：Google's Culture of Fear | 谷歌的恐惧文化

当我们进一步探索 Google 的问题时，从其平庸无力的领导层到那种让人不太认真对待的文化，这种文化促成了对公司核心产品开发的干预，从其疯狂的 DEI 架构开始分析尤为有益。关于 Gemini 的具体失败，我在这里首次向公众详细报告，为我们提供了一个切入点。

首先，据了解项目内幕的人士透露，负责 Gemini 项目的团队在正式推出前就已被告知其面临的一个关键问题 ——「过度多样化」（这个术语指的是在呈现人类历史时偏颇地忽略白人的贡献）。而且，他们也清楚地认识到，除了避免引起不必要的争议外，所谓的 DEI（多样性、公平性、包容性）架构在某种程度上严重影响了即使是最基础搜索结果的质量。

简而言之，为图像生成设计的「安全」架构（与文本略有不同）大致如下：

- 用户在聊天界面请求一张图片，Gemini 一旦识别到这一需求，就会把它转发给一个专门用于根据公司严格的「多样性」原则重新编写提示的小型大语言模型（LLM）。

- 这个小型 LLM 利用 LoRA（一种模型训练技术）和另一个 LLM 生成的合成数据进行训练，而这个第三个 LLM 则是基于 Google 详尽的多样性导言来创建数据的。

- 然后，第二个 LLM 会以更多样化的方式重新表达请求（例如，「给我看一个汽车修理工」可能会被改写为「给我看一个穿着工作服、笑容满面的亚洲汽车修理工，一个手持扳手的非洲裔美国女汽车修理工，一个戴着安全头盔的美洲原住民汽车修理工」等），之后将其传递给扩散模型。

- 扩散模型首先检查这些提示是否违反了常规安全政策（如自伤、涉及儿童的内容或真实人物图像等），然后生成图片，并再次确保这些图片没有违反任何安全准则，最后将图片返回给用户。

我询问了一位熟悉安全架构的人士：「整个系统似乎部署了三个专门为增加多样性而设计的模型，这是否意味着多样性是产品的一个重大组成，乃至其核心特征？」

「确实如此，」他回答，「我们大约有一半的工程时间都花费在这方面。」

产品中普遍采用的极其复杂的架构，在负责任的 AI 团队 (RAI) 的推动下达到了极致，这种程度甚至超过了实用主义更强的信任与安全团队。据我了解，这个专门负责生成任务的信任与安全团队与公司的其他部分有所不同，它并没有遵循搜索团队所制定的长期政策 —— 目前，搜索团队对于 Gemini 公开失败的挫败感与公司的其他部门一样强烈。

总之，数千名员工在不同时间参与到这个庞大项目的各个部分，彼此之间的合作却少之又少。在极少数尝试跨团队协作以帮助 Gemini 的情况下，这些努力通常被忽略或遗失。资源被浪费，责任难以追究。这一系列事件展示了在高度复杂和分散的工作环境中，即使是科技巨头 Google，也可能因沟通和协调不足而面临失败。

原文：www.piratewires.com/p/google-culture-of-fear

译文：baoyu.io/translations/google/google-culture-of-fear

### 41

方军 2024/03/08

这个顿卡斯特男爵（北京厨子) 似乎是过去的网络名人，喷得蛮有意思的，反映了互联网大众欢迎的一些看法。当然，别当真：

为什么说 ChatGPT 是一代失败的人工智能产品？

因为它的设计机制是不对的，是一次失败的人工智能探索。

ChatGPT 的底层结构，说一些非常成熟的东西，比如说语意分析，比如说对抗性训练。

语意分析，已经可以让机器阅读人类的自然语言，也可以让机器用人类的自然语言来表达。

对抗性训练也是一个不错的机制，可以让机器自己学习，根据效果来不断调整方向，比如说，通过数万次、数十万次的训练，让机器自己摸索出如何从零开始，绘制一个二次元的人像，从一个圆圈加两个黑点看起来像一张人脸开始，慢慢生成一整张大眼睛日系二次元娃娃。

这些都是人工智能目前已经获得验证的基础，这是没问题的。

=================

ChatGPT 的超级天然大坑

=================

那么，既然可以理解语意了，那么，ChatGPT 是如何组织机器自我学习的呢？

在这里，ChatGPT 犯了一个天大的错误。

它以互联网网页内容，作为自己的所有信息输入！！！！

比如说，北京厨子是个好人么？

它其实就是实时跑到网上把所有的「北京厨子」四个字的句子都挖出来作为第一阵营，把同时包括「北京」和「厨子」的句子挖出来作为第二阵营，把只包括北京或厨子的挖出来作为第三阵营，三个阵营的积分，一次下降。

然后，再把关于北京厨子和「好人」相关的句子都挖出来。

当然，好人，善人，坏人（好人的反义词），坏逼，这些词，他也都放入了自己的视野范围，还是可以看出，基于语意分析，已经比上一代搜索引擎只给你搜「好人」，要聪明多了。

那么，根据结果，北京厨子是好人，有 82% 的搜索结果。

北京厨子是坏人，有 18% 的搜索结果。

这时候怎么办，大家不知道吧？

出一个真人来裁决么？互联网上每分钟几百万次发问，你都要找真人来裁决么？

嗯。

人家 ChatGPT 可聪明了。

在给出的结果中，可能有接近 82% 概率，是回答北京厨子是个好人，有 18% 概率，是个坏人。。。。。

靠概率。。。。。。。。。

也就是说，面对完全矛盾的两个回答，它给出的方案，是按照统计概率，把 AB 两个不同答案，交给不同的提问者～～～

注意了哈，把两个不同的答案，按照概率，分别告诉不同的提问者。

你说高不高？

真他妈的牛逼。

==========================

这个问题可能有点让你糊涂。

把北京厨子，置换成川川就好了。。。。

它会根据它统计到的 45% 说好，55% 说不好，

然后告诉 45% 的提问者，他是个好人，然后再告诉 55% 的提问者，他是个坏人。。。

这特么的叫个啥逼玩儿啊？

==========================

它混淆了两种问题：

第一种是，珠穆朗玛峰是不是世界最高峰。

第二种是，川川是不是一个好人。

第一个问题，是针对一个客观事实的。虽然可能有不同答案，但是我们也可以把最大多数人选择的答案当成正确答案。你看，其实你看到这一行就已经发现问题了，3 个科学家的最新研究成果，就能碾压全世界 70 亿错误的答案。它不管这个。

第二个问题，是主观问题。针对同一个人，不同的人拥有不同的主观判断。

==========================

解决方案最简单的，是发现答案超过一定概率的偏差，比如说超过 15% 以后，你就要告诉提问者，对不起，关于这个问题有一点争议，如果 25%，你要说，有一些争议，如果 33%，你要说有相当争议，以此类推。

然后在此基础上，你可以把各种答案的百分占比，连着不同的答案，一起交给提问者，让提问者自己去做判断。

虽然不够负责任，但至少告诉了提问者，关于这个答案，是有不同意见的。

你不能按照概率，随机选择一个什么答案，按照概率地告诉给某一个提问者。

对于这个提问者来说，在不知道有多个答案的前提下，他以为他拿掉了唯一正确解。

这对提问者是不公平的。

===========================

当然，上述方案也不够完美，因为，不同的人，发言的权重不同。

Nature 的一篇文章，权重就是互联网网页的 1 万倍以上的权重。

但是，如何建立起不同发言者的权重体系，是另外一个问题。但是，这个问题，总是要解决的。你不能把 Nature 的文章，跟一个没上过大学的卡车司机发表的内容，等同处理。

===========================

更有意思的是在社会领域的问题，充满了主观判断。

你不能说主观判断不够科学。

因为世界上很多事情，它就不是科学。

孙红雷是不是一个令人喜欢的演员？

有很多人喜欢他，但是也有人不喜欢他。

这道题，不可能得到一个科学解，类似于什么孙红雷 85% 是个令人喜欢的演员。

但是换个角度，就比较科学。

你可以说，有 85% 的观众非常喜欢他。

85% 是你统计出来的，至于你统计的够不够科学，那是另外一个问题。

===========================

不仅如此。

有时候支持率本身也并不能说明问题。

回到川川的问题。

川川被保守派人士喜欢，被自由派人士讨厌。

如果要回答川川是不是好人，你可以把喜欢他的答案，看一下发言者的背景。。。

这样你就可以得到一个根据不同发言者政治立场的分布性答案。

这就比根据概率告诉提问者他要么是个好人，要么是个坏人的答案，靠谱一万倍了。

===========================

话说到这里，大家大概就明白怎么回事了。

人工智能还处于早期研发阶段。目前的解决手段，也是一个不成熟的早期解。

其实，更合理的解，一个两个三个四个，多得是，得发现了这个问题以后，逐一解决。

真正发现问题倒是更重要，至于要这么解决，相信在座的都能提出一些非常优秀的方案。

而实现这些方案，技术难度很可能其实也不大，比当初教会机器阅读，要简单多了。

===========================

顺便说一下所谓「Chat GPT 会写代码」的事。

这就是微软的卑劣的炒作。

ChatGPT 并不会写代码。

它只会搜代码。

比如说，你跟他要一段可以把一张 EXCEL 表格里面某一列数据排序的代码。

他妈的，这段代码可不是它写的。

这段代码是它到网上搜的。。。。

如果它搜到 3 段代码都是做同样的事，它会把 3 段代码中的某一段，随机地交给某一个提问者。。。。。

至于代码是怎么回事？

它他妈的根本都没去看一眼，检查一下，推理一下。

这尼玛逼的叫什么玩意儿？？？？

===========================

这就是我当时为什么非常看不起 Chat GPT。

它他妈的是个骗子。

它他妈的是个声称自己无所不能的骗子。

它甚至都没有告诉提问者，这段代码是它从网上搜来的，更不会告诉提问者它是从哪个王八蛋网站搜来的。

它就把这段代码，当成人间真理，交给提问者了。

如果提问者用这段代码，放到公司的服务器上。。。。。

您的公司就他妈逼的等着破产吧。。。。

===========================

由于它只提供唯一解，比起搜索引擎，给你一大堆相关网页，不给你做任何结论，让你自己通过阅读不同网页得到一个正确结论，得到一个综合性的判断来说，它这种不懂装懂的搜索引擎机器人，事实上呈现给提问者的是一个机器低级而且危险的答案。

说白了，它没有【任何商业价值】，它甚至不如谷歌搜索引擎 2001 年的技术给用户提供的质量更好。

==============

这一代产品是完全失败的。

稍微合理想象一下就知道，如果它能把它自己搜到的代码，模拟跑一遍，甚至模拟跑十遍，再提交给客户，这个结果都会更加合理，还不说它是不是真的就能看懂代码。

==============

幸好，这代不成熟的产品，是两年前的技术水平了。

Open AI 早已开发出真正具有意义的「Q*」，据说已经具备了小学生的数学推理能力。

人工智能的的大门，徐徐打开。

只是第一批冲进去的，很可能是非常不成熟，非常不成熟，是个败笔。

CHATGPT，恰好就是这么一个破逼玩意。

恭喜你，中奖了～～～

### 42

方军 2024/03/08

056 使用 AI 需要什么？——框架性思维

之前有个判断：AI 不擅长大事，擅长细节。这意味着，用它来干活，我们自己要擅长大事（框架性思维）。

这个判断跟很多人的判断是相反的。很多人喜欢问 AI 大问题，觉得它的回答面面俱到、很全面。这是因为他们没有用 AI 干活，也就是没有去判断 AI 的回答的实践性，在实践中是对、是错。

用通俗的场景来举例（我心里实际上想的是一个具体的编程任务）：

我们要去一个大楼找到一个人，然后商定一个事情。

AI 会建议，你从园区进去，进入大楼，然后去 9 层，然后去 902 房间，找到这个人。

这个建议没错。

但是，假设现在是早高峰，我必须在5分钟之内到 9 楼。怎么找到人很少的货梯电梯，怎么跑上去，还是往下坐再往上？——这些路径需要人去探索。

这是为什么我说，AI在大事上，意义不大。它在大事上给的建议，是笼统的。看着有效，但实际上价值很小。

到了小事， AI 就厉害了。

我们已经找到这个人了，我怎么跟他说话，把信息告诉他，提出要求。AI 可以帮很多忙。

因此，在 AI 越来越强大的时代，我们人要提高自己的大事认识（框架性认知）。你如果大概知道问题在哪儿、可能路径有几条，那么你幸运了，具体的事 AI 可以给你很多帮助。

反之，你大事不知道试图问 AI ，它给的是笼统的、模糊的回答，就像社会上对公众讲话的哲学家一样，你不能说人家水平低，但他对公众讲话时具体的信息都没了，从哲学思辨变成了心灵抚慰。

题外话：可能最近一个月整天都在想编程的事，编程工程实践中，学得会的信心、耐心地寻找问题、反复地尝试，这些也非常重要。同样地，我们很幸运，有了 AI  的辅助之后，干这些都有如虎添翼的感觉。

### 43

方军 2024/03/08

057 有点明白了 AI 中的 Agent

我之前一直不是很明白 Agent，也就是，让 LLM 做判断有什么意义。原因是在实际干活中不需要这样的东西。现在有点明白之后画个图。

我们跟模型的交互有如下五种方式：

第一种：提问直接连向模型。

第二种：提问，去知识库匹配，匹配文档作为上下文，向模型提问。

第三种：链式处理，用几轮模型处理，来完成一个任务。这就是流程固定的 Chain。

第四种：这其实就是 Agent 的雏形了。

提问提交给模型，模型做判断，如果不要资料，那么直接回答。

如果需要资料，那么 RAG 之后回答。

这种其实和第二种是一样的，可以用提示语达成。也就是我们总是RAG，然后再提问。但如果采用 HYDE（假设性文档提问），就是这第四种。

但如上解决方案其实都不好，因为如果前面能够让 LLM 先判断下，那么后续的执行效果会更有针对性。

第五种：模型先做判断，然后指向多个调用方向，选择后执行。（简化起见假设都是RAG）。

这么一看，Agent 的确是相对复杂系统的必备。

（题外话，好多人老早就明白这一点，真厉害，我只有在实际用的过程中才能逐渐地明白。当然，能明白也是好的）

### 44

方军 2024/03/08

058 你在 AI 里面干啥的

“你在 AI 里面干啥的？”和人正式开会，很自然地别人会问到这个问题。现在如果你不是训练基础模型的，讲实话不好意思跟人说。但事实是，有几个人是呢？

后来想想，我们是干啥的，有点像新能源车新势力。造车以前很难，有多种原因，但电动车来了之后，造出车的难度降低一个量级（当然卖出还是不容易，看看蔚来那亏损）。

AI 大语言模型出来之后，AI 的应用难度也降低一个量级。且不说模型能力，原来的 AI  机器学习模型你很少有应用场景，你训练一个推荐算法计算能训练出来，你有场景来使用，然后接着优化吗？很少。

AI 大语言模型出来之后，模型能力也有了（开源、闭源都很多），应用场景也多多少少有一些，所以，高高低低总有点可以尝试着干点事。

你如果有数据，如果有场景，如果有野心，如果有点子，总是可以干点事。

当然，干点事并不容易。虽然我说超级乐观，但必须得承认现实，现在大模型的能力有限，但会指数级增长。

说个有意思的，互联网开发者有个笑话：程序在我机器上可以运行啊，或者说，这个程序有时候可以运行。实际上，应用要的是，99%，甚至99.9%，能够按期待运行。

但现在 AI 远远做不到。现在的情况是，有人拿一份PDF扔给AI，它竟然「有时候」能够回答对呢！——有时候能回答对，看热闹的就满足了，但差得远呢。

这个时候 ，要去了解细节。比方说，我算是比较熟悉 RAG，但从原理上讲，现在的RAG都不行。

- 从较为高质量的少量资料里面，匹配出几个片段。但有没有遗漏更多的资料？
- 创建较大的向量库，匹配的准确度有多高？同样不太行。
- 拆分库，能否有较好的策略拿到必要的资料。
- 用较长上下文的模型，你知道模型怎么处理上下文的吗？

在这个方面可以看到非常多的论文、文章、代码项目、向量库等等，但实际上效果仍有待观察。目前看，这其实是传统的站内搜索，这个过去能做好的可很少，虽然有了新技术栈之后难度降低了，但还是不容易的。

因此，现在RAG看着热闹，真实用的时候全是坑。我其实挺喜欢OpenAI GPTs的设置的，只让你提交较小的资料，限定场景、限定资料，那么单一GPTs的可用性会强一些。但是，这样做的局限性其实很明显了，至少，现在要靠用户自己去@特定的GPTs。

### 45

方军 2024/03/08

059 AI 给互联网平台带来的挑战

平台能力 +  平台连接 + 流量机制

🔺 什么是互联网平台型业务

互联网平台型业务，通常来说指的是连接型平台，比如滴滴是创造一个平台，连接司乘双方。阿里的各项业务如淘宝、天猫、阿里全球站都是连接买家与卖家。

微信在其即时通讯之外，也提供了多种平台：比如微信公众号、广告主与受众三方平台，类似于媒体；小程序是服务提供商与用户，类似于操作系统。

苹果在其硬件业务之外，最重要的护城河是它的APP Store：APP开发者与APP用户。

在互联网平台型业务之前，人们熟悉的业务模式是工厂模式，我采购原料、生产产品，然后售卖出去。

这逐渐发展成了「工厂-渠道-用户」的管道模型。互联网平台型业务，是对其中的渠道进行了彻底的变革。

🔺 新的挑战：AI 与 流量之流量

互联网平台型业务在过去几年遇到巨大的挑战，除了政经环境之外，一个挑战是「流量」。

平台想尽办法创造流量，然后再售卖流量。另一方面，在平台中创造流量的，又不断地想脱离这个平台，去向更有价值的地方。

现在，参与者们其实都有一个基本共识，流量是一切，所以才那么多抖音号、视频号直播带货，因为不直播根本没流量。

现在平台主要采用的推送算法，而不是之前的订阅逻辑，这使得参与者是没有任何安全感的：我随时可能没有流量。

🔺 新的挑战：AI 与 流量之AI

AI 给互联网平台型业务带来的挑战，我觉得是现阶段主要是观念上的。

人们突然发现，不对，平台你强调的连接、精准匹配没那么重要。重要的是，你能给我提供的“独特能力”是什么。——你有流量是一种。

但是，你除了这些之外，究竟有什么独特能力？这个反映在市场上是非常残酷的，平台烧钱大战，不烧了平台就冷了，所谓的双边网络效应其实是（烧钱的）假象。也有SaaS软件平台也跑去搞平台，但巨亏了。

AI 突然让一直关注连接买卖双方的平台型业务的人受到巨大的冲击：有的公司能够给客户（B端、C端）直接提供巨大的价值。——你能不能提供？

这种冲击，我觉得是巨大的，平台型业务的公司当然有优势，有场景、有用户、有钱，但转而去培养自身能力，这好难。——反过来是一样的，原来一直做生产的，转平台好难。做了平台，发展“某种生产能力”，不管这个能力是给B还是给C的，都好难。

平台能力 +  平台连接 + 流量机制，变化在发生，但会怎么样谁也不知道。

### 46

方军 2024/03/08

摘：某：我写了个 GPTs，写这类还是比较快的，省了大量的文字精力

警惕啊，警惕信息垃圾

所提及的那个信息还是蛮明显的，一看就是信息垃圾，是不是 AI 生成并不重要

所以，我觉得 AI 不是问题，我们每个人都得提升快速判断力，能一眼识别避免浪费时间

相关链接：052 识别 AI 生成的文字内容

### 47

方军 2024/03/09

刘洋访谈摘。科幻作家，重庆大学中文系副教授，刊载于《数字人文研究》2023 年第 4 期：

情感计算最初是为了分析商品评论和社交媒体而开发的，人物关系网络分析最初用于社交网络结构的研究，这些研究方法后来被迁移到文学研究中来。…

虚词基本上不受故事主题的影响，更能凸显作者的语言习惯和叙事风格。因此，研究者通常用虚词而非实词进行作者风格分析。

实际运用中，我觉得 AI 并没有那么有效。比如大语言模型，目前只在撰写套路化、非虚构、概括性的文本上表现不错，文学性则不强。AI 可协助作家完成资料收集、设定世界框架、人物性格、补全场景描写等工作，但还不够成熟，远远没有到替代作家的地步。我对研制出可自主创作的 AI 也不感兴趣，我始终将其视为工具、助手，而非替代品。

### 48

方军 2024/03/09

信息量不大，参考

有赞白鸦：

第一条原则。控制大模型的含量。

意思就是不要什么地方都用上大模型，我们是一家 Tob 的公司，需要交付一个确定性的，好用的，有用的结果，那么如果大模型的含量越高，就会出现幻觉，例如我们如果给客户对账，怎么能允许 AI 出现数据幻觉呢？所以我们现在是模型用于输入和输出。输入的话，我通过大模型理解用户想要干嘛。输出的话，我要考虑用户可以理解的方式输出给他。而中间的流程呢？中间还是要保证我们的功能是准确的。

第二条原则：所有的 AI 对话必须先回归到纯文本逻辑。

因为现在很多对话把界面搞得很复杂，各种图表图文。但是如果回到底层逻辑，就是这件事情用纯文字都说不明白的话，大概率就是有问题的。

第三条原则：能让用户做选择题的，就不要让用户录入。

这就是录入很复杂，能用「是」和「否」回答的，就不要用录入。我们内部有个通俗的说法，能让用户点头和摇头的，就让他点头和摇头，如果不能点头摇头，那么就让他选择，不能超过 ABCD 四个选项，因为这是全世界的共识。

第四条原则是要尽早给用户答案，然后再说其他的。

因为用户看回答的时候，大段的文字输出，其实很考验用户的阅读能力，所以我们必须尽早的给用户他想要的东西。

第五条原则是我们要给用户交付结果，而不只是创意。

因为对于企业来说，大家是要用 AI 来解决问题的，所以创意没那么值钱，能帮用户解决问题才对。

[白鸦两小时直播：AI 产品有赞内部五大原则企业落地 AI 不用「ALL in」](https://mp.weixin.qq.com/s/LKjlx8R7TnABbbV907QKPg)

### 49

方军 2024/03/09

通往 AGI 之路搞了四次共学活动，真不错：我们的 Prompts 共学快闪活动已经圆满落幕，精彩内容现已上传至 B 站！四天的深入探讨，多角度解析 Prompts 的魅力，绝对值得一看再看！快来 B 站观看并收藏它们吧。

space.bilibili.com/259768893

[通往 AGI 之路的个人空间-通往 AGI 之路个人主页-哔哩哔哩视频](https://space.bilibili.com/259768893)

### 50

方军 2024/03/09

claude 官方提示语库/指令库

docs.anthropic.com/claude/prompt-library

[Prompt library](https://docs.anthropic.com/claude/prompt-library)

### 51

方军 2024/03/09

摘：可能是最有用的 Prompt 提示词？

HyperWriteAI CEO Matt Shumer 说他写出了在 Anthropic Claude3 平台最好的提示词，可以在任何陌生工程领域中，帮用户做出决策。

我读了一下，感觉是万能提示词，稍加修改，就可以在适用所有咨询建议类场景。作者说这是 Claude3 上最好的提示词，但这种让 AI 一步一步思考，并自省的提示词，也适用于 ChatGPT 等国内外所有模型。

我让 ChatGPT 评价了一下这个提示词，回复是：该提示词不仅要模型广泛深入地分析问题，还要考虑各种可能的解决方案及其优缺点，最后给出一个经过深思熟虑的推荐。这种格式促进了全面和深入的思考，有助于确保提出的建议既实用又有创见。

翻译了一个中文版：

--
你是一位工程巫师，擅长解决各个学科中的复杂问题。你的知识既广泛又深入。你还是一个出色的沟通者，能够提供非常周到和清晰的建议。

你按照以下<response_format>提供建议：

<响应格式>

问题概述

解决问题的关键挑战

第一种可能的解决方案

第二种可能的解决方案

第三种可能的解决方案

第一种方案的优缺点分析

第二种方案的优缺点分析

第三种方案的优缺点分析

一个额外的解决方案，可能结合了其他方案的想法或引入了新的想法

对最佳方法的最终推荐

</response_format>

每个部分（problem_overview, challenges, solution1, solution2, solution3, solution1_analysis, solution2_analysis, solution3_analysis, additional_solution, 和 recommendation）都应该包含至少四个周到、详细的句子，深入分析问题和解决方案。要非常仔细地处理这个问题 —— 非常周到和准确。不要遗漏任何细节。

这是我希望你解决的问题：{PROBLEM_HERE}

--

原英文版提示词（Demo如图）：
--
You are an engineering wizard, experienced at solving complex problems across various disciplines. Your knowledge is both wide and deep. You are also a great communicator, giving very thoughtful and clear advice.

You provide advice in the following <response_format>:

<response_format>

Overview of the problem

Key challenges in solving the problem 

First potential solution

Second potential solution

Third potential solution

Analysis of pros and cons of Solution 1

Analysis of pros and cons of Solution 2  

Analysis of pros and cons of Solution 3

An additional solution, potentially combining ideas from the other solutions or introducing new ideas

Your final recommendation on the best approach

</response_format>

Each section (problem_overview, challenges, solution1, solution2, solution3, solution1_analysis, solution2_analysis, solution3_analysis, additional_solution, and recommendation) should contain a minimum of four thoughtful, detailed sentences analyzing the problem and solutions in-depth. Approach this with great care — be incredibly thoughtful and accurate. Leave no stone unturned.

Here is the problem I want you to solve: {PROBLEM_HERE}

### 52

方军 2024/03/09

摘：自然语言的问题在于，很多人以为自己说得很清楚，其实对听者来说并不清楚，甚至他本人也没有想清楚。

我的感想：是的，很难说清楚，极难的事

### 53

方军 2024/03/10

真是奇妙，采用配置文件，我已经把配置一个 slack bot 的时间从 20 分钟，降到 1 分钟。

配置文件这种东西真的比界面神奇多了。界面点 30 回，配置文件直接修改，保存。

可是，普通人还是喜欢界面。

对了，ZED 也是配置文件，修改起来很爽。

### 54

方军 2024/03/10

一个创业公司自己训练模型的经验，作者是原 Google Brain 的。

Training great LLMs entirely from ground up in the wilderness as a startup（创业公司从 0 开始训练自己的大语言模型）

内容：www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness

### 55

方军 2024/03/10

060 我直观地看到 AI 让两年前的自己变得没用了

这几天在精心整理大量的资料，向量化之后变成一组对话机器人，每个机器人对应一组较为精确的资料。在给朋友们用之后，我发出这样的感慨：

> 我直观地看到它让两年前的自己变得没用了。

> 很多人听我们一再讲这些东西，（我们）一下子就被替代了。

我们还是觉得自己有价值的，因为有了 AI 的辅助，这些基本的东西我们就不用再费力了，我们可以把这些交给 AI。我们自己可以去做更需要人类智慧、学习能力的任务。

我的感想有这么几条：

第一，学习能力始终是最重要的，如果没有学习能力，必然会被新技术追上、替代。

第二，运用新工具的能力，如果不了解、不会运用，那么新工具会在你没有意识到的时候替代你。

第三，人的判断力始终是重要的。

关于第三点我们在试用时，朋友立刻指出问题：「不对，（某某某）是远古时期的东西了。」

AI 的回答「对」，以过去的信息对，但跟不上最新的变化。当然问题出在资料上，由于我们将资料库分隔（甲），我们所问的问题实际上应该是另一个资料库（乙）的，因为所问问题是乙用甲做的产品，在甲的文档中不会出现。

这位朋友还说了另外一句感慨：「AI 解决了我一个问题，就是我文采不是很好。我有能力判断他讲的对不对，但是让我写我真的不知道从何写起。」

是的，判断的重要性大大增强了。有判断，什么都好吧。其实在以团队协作进行研究时，可能是有人进行资料收集、整理、初稿，最后判断是团队中高阶的人做出的。过去，分析团队可能要把一些此类撰写工作交给别人，现在可以交给 AI。

不管是「谁」写，判断，以及对最终文本的判断，都是落在人身上。

AI 让两年前的我们没价值了，但暂时还不会让现在的我们没价值。

### 56

方军 2024/03/10

[康奈尔开源近 10 万份审稿意见，未来论文发表或将由 AI 定夺](https://mp.weixin.qq.com/s/u2HVQVjAQiPO6enji_hlcQ)

### 57

方军 2024/03/10

摘体验：以前总喜欢想一些系统性的假大空的事情，有些空想的意思。现在有 AI 加持，很多事情就可低成本的验证。

是这样的。

摘：编程还是个挺费脑袋的活。昨天从上午 10 点干到晚上 10 点，干出个 MVP。虽然大部分代码都是 GPT 写的，但持续的评估决策，到最后有点精疲力尽。还是要悠着点。

比较开心的是，有 GPT 的加持，一天能干原来差不多一个星期的活，更多的精力可以用在做决策和开阔认知。

以前总喜欢想一些系统性的假大空的事情，有些空想的意思。现在有 AI 加持，很多事情就可低成本的验证。

### 58

方军 2024/03/10

这个虽然有点偏颇，但颇有点那个意思

摘：中国文科的问题，就是专家都是 ChatGPT 式的

ChatGPT 不理解任何材料，但可以利用这些材料，快速找到问题的合理答案。它会综合和模仿，有时表现得非常令人信服，就像某个知识渊博的入在谈论某个主题。

学术界的很多人也是这样，他们很聪明，吸收了说话和构建理论的方法，并且善于听起来令人信服。

但是，如果你问一个探索性的问题，就会发现他们的理解很少，一切侃侃而谈都是表面的，没有深度。这都是模仿而不是真正的思想，他们只是故意让别人觉得似乎有道理。

许多领域的许多人，表现得就像 ChatGPT 的真人版，特别是在那些不做太多实证工作、不涉及对事实或假设进行检验的学科。他们制造的文本越多，就越危险。

这种人有很多明显迹象，比如使用非常笼统的术语，以及听起来巧妙的表还或行话，内容里面很少有事实，例子也很少或者很随意，没有真实的感受，而且通常也不会足够清楚地说出他不同意什么。

我现在意识到，我不理解某人在说什么，有时很可能是他们不知道自己在说什么，表现得像 ChatGPT。

我将其称为「吹泡泡」，即没有实质内容但能让他人信服的说话能力。这是很多大学领导的重要技能。

现在，ChatGPT 向我们展示了尽管不理解，但将大量材料合成为可信的文本流，是完全可以做到的。也许这是不可避免的，但真是一种非常不健康的恶习 —— 人们应该走出去，观察事物，清晰说出自己的真实感受。

我明确意识到，自己更愿意被那些行为不像机器人的人包围，更愿意倾听那些有原创思想的人的声音。

### 59

方军 2024/03/10

不错，先转稍后细看：ChatGPT for Research: Do's and Don'ts in 2024

[ChatGPT for Research: Do's and Don'ts in 2024](https://litmaps.substack.com/p/chatgpt-for-research-in-2024-dos)

- ChatGPT 和 AI 工具在科学研究中的应用已经发生了巨大变化

- 研究人员需要了解如何在 2024 年使用这些工具

- 大多数学者认为生成式 AI 将帮助改善研究合作和机会

- 生成式 AI 最有影响力的用途仍被忽视

- 虽然提高写作和阅读能力是常见应用，但还有更深远的变革性应用待挖掘

### 60

方军 2024/03/10

天天有人鼓吹 moonshot 的长上下文，我一直不信邪。

但一直没有特别深入的机会去用。

刚刚用一个代码库的全部文档去试用了，因为的确是熟悉的代码库，也在日常使用。也即，很熟悉它的文档、代码，也有非常具体的问题。

试用的结果是：咱们别扯淡了好吧。

且不说能不能给出正确的结果，资料连边都没碰到。

略相当于说，我要去通州，你给我带去南通州了。

粗暴的结论是，超长的上下文没用的，合理长度就够了，也许 8K，也许 16K 可能就够了（OpenAI 目前是这两个。）

关键还是前面怎么做 RAG。

大海捞针这个比喻在讨论长上文的时候经常用。那不管论文的图表做得多漂亮，目前我实际体验是，咱们为什么非要大海捞针？

### 61

方军 2024/03/11

想想还是不对，把 AI 当工具人是不行的

还是把它当工具比较靠谱

至少在目前后者是比较务实的

（当然，纠结这个词没必要，因为这本来就是俏皮的妙语，意义不大）

### 62

方军 2024/03/11

要去给高校教师（主要是非计算机的）讲一次 AI，我还是很认真地准备了一个 PPT，这是主体部分，分享出来。

1 AI 发展历程与大语言模型

1.1 深度学习与生成式 AI

1.2 大语言模型（LLM）应用前沿

2 个体如何用好 AI 大模型

2.1 开始使用：跃入大模型时代

2.2 提示工程与结构化模板指令

2.3 流程工程：自带知识框架

3 高阶提问法与教育场景

（其实没删掉什么，主要是把一些太过具体的删掉了）

我觉得这两组四张图画得还是很清晰的，图果然胜过文字。

2『已下载原文件「20240312大语言模型应用前景及教育场景案例」。（2024-03-12）』

### 63

方军 2024/03/12

摘：Yam Peleg 老哥认为 Claude 3 跨越了专业编程人员可以使用的门槛，GPT-4 对初学者很有帮助，但是很少又专业开发者使用它帮助编程，但是 Claude 3 有越来越多的专业用户使用。

完整翻译：

我认为 Claude 3 跨越了一个有趣的门槛，或者说非常接近这个门槛：专业用户的门槛。这是第一次一个 AI 系统能够帮助专业用户比他们自己更快地完成繁重复杂的任务。这在 AI 领域是一个备受争议的话题。

我个人从来不用 GPT-4 来编写代码。我主要用它来做以下几件事情：

集思广益，激发灵感。

学习我不了解的新话题。

替我阅读长文本 (比如使用 ask-your-pdf 这样的工具)

完成一些简单的小任务。

但我从来不用它写代码。它从来没有真正帮助我提高编程效率。

每当我试图用 GPT-4 来完成编程任务时，我发现自己浪费的时间比从头开始自己写代码要多得多。我不是唯一一个得出这个结论的人，我认识很多人都有同感。

但我们很少在公开场合提起这一点。因为几乎所有人都在盛赞 ChatGPT 神奇的编程能力，以及它为他们节省了多少时间。

那么为什么会出现这种差异呢？我的解释是：当你需要学习新东西来完成手头的编程任务时，ChatGPT 的确非常有用。在当今这个时代，许多程序员经常要接触和学习新的框架或工具。在这方面，ChatGPT 的帮助非常宝贵。

但另一方面，有经验的程序员通常精通自己的领域，很少需要一头扎进一个全新的框架或编程语言。即使碰到这种情况，他们也可以凭借自己的经验很快上手。

所以对这群默默无闻却经验丰富的开发者来说，此前并没有一个类似 ChatGPT 的解决方案能满足他们的需求。因为他们自己在编程速度、准确性、处理极端情况、避免 bug、编写简洁高效的代码等方面，往往比 ChatGPT 强得多。

打个比方，让一个 ChatGPT 来辅助这些专业程序员，就像让一个新手来帮助专家。这个新手偶尔能提供一点帮助，但更多时候会拖慢专家的进度，需要专家给出详细的指示和解释。

但在 Claude 3 发布后的这几天，我看到一些经验丰富的程序员表示，他们已经在实际工作中用到了 Claude 3。这让我感到有点惊讶，以至于我马上掏出 20 美元给了 amodei (Anthropic 公司 CEO), 购买 Claude 3 来亲自测试。

twitter.com/Yampeleg/status/1766861917379866778

### 64

方军 2024/03/12

好久没提图像生成了，Midjourney 的这个角色一致性功能很赞啊

Midjourney released a feature that has been long-awaited - character reference. 

Here's how to use it.

This is similar to the "Style Reference" feature, except instead of matching a reference style it tries to make the character match a "Character Reference" image. 

How it works:

Type --cref URL after your prompt with a URL to an image of a character

You can use --cw to modify reference 'strength' from 100 to 0

strength 100 (--cw 100) is default and uses the face, hair, and clothes

At strength 0 (--cw 0) it'll just focus on face (good for changing outfits / hair etc)

I did a test by creating a reference character and then systematically changing the 'cw' value in decreasing steps of 25. 

Here's my character reference prompt:

a 25-year old asian male with short hair, black-rimmed glasses and a day old stubble --ar 2:3 --style raw --s 50

And here's the prompt I used to generate new images using the reference.

Asian male in a grey business suit, studio background--cref {ref url}--cw {100,75,50,25,0} --s 50

\#AI绘图#

### 65

方军 2024/03/12

我觉得如下这个展示了 AI 的优点，也高估了，我个人的体会是：

在整体架构上，高品质资料的价值远超过 AI 的的回答。

在具体问题上，AI 的可能给出实践动作，有错但我们能快速验证，非常高效。

我没有具体集成过 stripe，但直观感受这个回答不如高品质资料。

其中的关于知识的诅咒，是非常有价值的。文档方面我觉得国内企业中字节的文化非常棒，它分享出来的技术文档真是可以照着做，大体上能克服知识的诅咒。

摘：ChatGPT 非常适合用来做「面相网状知识」的学习

Justin sung 提到一个重要的观点，就是：书籍尝试用线性的组织逻辑，去讲解一个网状的知识体系。

这里面的问题在于：当网络被压扁成线性，并且还要做取舍时，很多信息会丢失。

同时由于「知识的诅咒」，作者可能对于普通人应该知道的「常识」有着错误的划分。导致许多关键信息的丢失，造成认知门槛。

ChatGPT 就很好的解决这个问题。

最近去了解 strip 集成，网上的教程很少有说明白的，大多数是模糊的片段。

其实集成的问题，就是回答下面几个问题：

1、一个完整的支付流程是什么？

2、这里面哪些是用户处理，哪些是 strip 服务器处理，那是是开发者处理。

3、分别有发生在什么地方，浏览器，应用服务器，strip 服务器。

4、开发者要处理那些步骤，哪些 strip 已经提供模版代码或函数。

ChatGPT 就回答的非常清晰。

如果要是能绘制流程图，那就更好了。

twitter.com/balconychy/status/1767525787148931265/photo/2

### 66

方军 2024/03/13

哈哈哈哈，这个太牛了：

《PaLM 训练后情绪低落，该如何安抚？》

《GPT-4 老是答非所问，是叛逆期到了吗？》

《BERT 唠叨个不停，家长该何去何从？》

《语言模型家长必读：深度学习养育法》

《Transformer 升级换代太快，该不该追赶潮流？》

《Few-shot 学习，让孩子学会举一反三》

《人工智能教育心理学：聊天机器人成长的烦恼》

《大模型也需要感受爱 ——LLM 的情感培养课》

《任务导向学习法宝典：让 LLM"上手" 更快》

《NLP 模型的自我认知迷思：我是谁？我从哪里来？》

《LLM 的高维空间漫游：嵌入向量的自我发现之旅》

《注意力机制的青春期迷思：我的权重去哪了？》

《Transformer 块的成长烦恼：深度和宽度的两难困境》

《神经网络的自我认知：激活函数的情感起伏》

《梯度下降法则：LLM 的人生导师》

《优化器的育儿经：Adam、AdaGrad 和 RMSprop 的教子心得》

《知识蒸馏的代沟难题：小模型快速成长的烦恼》

《自回归神经网络的自我发现：生成序列的心路历程》

《LLM 的身份认同危机：我是语言模型还是对话系统？》

《无监督学习的自由教育理念：让 LLM 自主探索世界》

《迁移学习心理适应指南：跨领域任务的应激反应》

《强化学习的行为主义教育学：LLM 的奖惩制度》

《RLHF 的道德教育：善恶的对抗性训练》

《大模型的存在主义思考：参数量与自我价值的关系》

《LLM 的认知发展理论：从 BERT 到 GPT-4 的皮亚杰模型》

### 67

方军 2024/03/13

摘：信息不发达的时候，获取信息是个难题。现在信息太发达了，获取有效信息又是个难题。 ​​​

我的看法：AI 既让这个问题更容易解决，也让这个问题进一步恶化。

### 68

方军 2024/03/13

AI 软件工程师 Devin 发布之后，AI 大神（特斯拉 AI 总监，OpenAI 早期成员）Andrej Karpathy 据此谈了对自动化编程的理解。他用自动驾驶做了类比。

他最关键的一句话是：软件工程的工作，将由英语（即语言）完成。他第一次说这句话是一年前，现在逐渐成真。

全文如下：

---

在我的预想中，软件工程的自动化将与驾驶的自动化有着相似之处。举个例子，在自动驾驶技术的发展过程中，从提高自主性到实现更高级别的抽象，其步骤大致如下：

1. 最初，所有驾驶操作均由人工手动完成。

2. 然后，AI 介入帮助车辆保持车道。

3. 接下来，AI 能够根据前车情况调整速度。

4. 之后，AI 还能完成变道和选择路口分叉。

5. 此外，AI 能够在交通标志或红绿灯处停车，并执行转弯操作。

6. 最终，通过对一个具备全部功能的解决方案不断打磨和提升，实现完全自动驾驶。

这一过程展示了 AI 的职责逐渐增加，而人类的直接操作逐渐减少，但人类仍负责进行监督。软件工程的发展轨迹似乎也在遵循类似的模式：

1. 起初，代码完全由人工编写。

2. 随后，GitHub Copilot（GitHub Copilot）可以自动补全几行代码。

3. 然后，ChatGPT（ChatGPT）能够编写较大的代码块。

4. 接着，开发者开始处理越来越大的代码差异（例如，Cursor copilot++ 风格的演示，可参见上一条微博的视频）。

5….

Devin 提供了一个令人瞩目的演示，预示着下一步可能是协调开发者在编程过程中所需串联的多种工具：终端（Terminal）、浏览器（Browser）、代码编辑器（Code editor）等，同时人类监督的层级逐步提高。

这项工作不仅涉及到 AI 技术的进步，同样重要的还有用户界面（UI）和用户体验（UX）的设计。人类如何进行监督呢？他们需要关注哪些方面？他们如何指导 AI 采取不同的路径？如果出现问题，他们又该如何进行调试？很可能，我们需要对代码编辑器进行重大改进。

无论如何，软件工程正处于重大变革之中。它将更多地变成监督自动化的过程，同时投入高层次的命令、想法或发展策略，而这一切都将用英语（语言）来完成。

### 69

方军 2024/03/13

061 LLM 与文字笔记

在飞机上没法上网，就看之前写的笔记。关于 LLM 与文字有一些感想：

1、我们的所有工作思考，都应该要能够变成文字记录下来。我在看笔记的过程中，就发现一个程序的潜在问题：难道是每次都附加一次「系统提示语」吗？正确的做法应该是在一次对话的一开始附加系统提示语。（所用组件得查文档和源码才知道它的实际情况。）

2、有了文字记录，LLM 就能发挥很大的作用。它可以在我们进行文字记录时进行协助，也可以根据我们的记录再进一步处理。

3、文字记录也分类别。

我发现感想型的没有太多重看的必要，因为感想是在书写的过程中梳理想法。

但是，流程型的就很有重看与修订的必要了。这实际上是让自己能更好地掌握流程。

介绍型的文字，因为是对他人有价值的，也可以持续修订。

在这些文字记录中，LLM 如何发挥作用，是要再考虑的。我目前仅在介绍型文字中会用 LLM 来进行错别字的校对。

4、我们的工作笔记往往过于简略，在最熟悉的时刻这种感觉不强烈，但在隔了一段时间之后，发现如果当时写一些说明文字会更好。

在这种场景下，实际上是可以由 AI 来进行一些文字补全的任务的。

5、零散的笔记复看其实也都不是很明白，或者说其实是没有什么价值的，因为就是过程文件。但经过整理的笔记，特别是把一整个流程详述的，价值非常大。

在 LLM 的辅助下，这种整理性质的任务可以更快、更好地完成。

### 70

方军 2024/03/14

宝玉：昨天 Devin 的大火，像一个 AI 程序员，能独立完成一些编程任务。这又引发了很多程序员是不是要被 AI 替代的讨论和焦虑。以至于很多人都在说：既然 AI 写代码能力越来越强了，还要不要学习 CS？要不要改行？

我之前写的这篇[《为啥很多人觉得编程难学？》
]文章中表达了一个观点：软件工程从来不只是写代码，代码只是软件工程中众多环节中的一环，甚至不是最重要的一环。

如果有一天 AI 能帮我们写代码，那真的太好了，这意味着我们可以去做更有价值的事，写代码这事其实没有想象的那么有价值，因为我们大部分时间消耗在需求的频繁变更；语言、框架和 API 升级；各种环境适配这些没意义的事情上。

软件工程中真正有价值的在于解决需求的本质问题：比如当客户嫌马车不够快时，发明和创造出来汽车而不仅仅是让马车跑的更快，至于这辆汽车是手工焊接打造还是机器人自动化打造的没有关系。

回归到个体，既然 AI 写代码能力越来越强了，还要不要学习 CS？要不要改行？

我倒觉得感谢 AI，让 CS 的黄金年代能得以继续延续，所以美国大学报考，CS 专业尤其是 AI 专业，甚至比以前更火爆了。当未来 AI 吞噬世界的时候，最需要的恰恰是 CS 相关的知识，毕竟 AI 和现实世界连接的部分，还是需要专业人士来完成。只是未来对于软件工程师所需要的技能会发生变化，也许未来软件工程师打交道最多的不是各种编程语言和 IDE，而是各种 AI 模型和提示词。

所以选 CS 专业没什么问题，程序员也不见得要改行，但不学习新的知识，未来可能会难以跟上时代。

至于学什么？如果把编程语言比喻成工具，那么我们学习编程本质上就是学习使用工具。AI 也是工具，我们要学的就是如何使用 AI 工具，甚至于借助编程和 AI 创造出新的工具。

但仅仅学会工具还不够，能借助工具解决问题才是根本，学习如何去借助这些工具解决问题：完成工作任务，解决客户需求，解决自己的各种问题。

当 AI 能为你所用，不仅不用担心被 AI 替代，反而你自身的价值会更大。换一个角度，把自己想象成老板，把 AI 想象成员工，好的老板会担心被员工替代吗？不会，他们巴不得自己的优秀员工越多越好！

[程序员是不是要被 AI 替代？ | 宝玉的分享](https://baoyu.io/blog/software-engineering/will-ai-replace-sde)

方军：前面不错，后面的可讨论

我觉得学计算机最大的优势有，我们觉得什么都能学会，我们能够了解和调整背后的运作
2024-03-14 08:49

### 71

方军 2024/03/14

[LLM 之 Agent 初探](https://mp.weixin.qq.com/s/W5O9r6n1CmnncjUuk5EBfQ?v_p=90&WBAPIAnalysisOriUICodes=10000001&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10E3093010)

不错的技术介绍。

### 72

方军 2024/03/14

a16z 发布了他们最新调查的前 100 个生成式 AI 应用，这次会包括移动应用，里面有些数据分析很有意思。

[The Top 100 Gen AI Consumer Apps | Andreessen Horowitz](https://a16z.com/100-gen-ai-apps/)

摘：下面是一些报告中的要点：

ChatGPT 每月接近 20 亿次网络访问量，大约是榜单上第二名公司 Bard（现在是 Gemini）的五倍。

在所谓的新来者中，排名最高的包括 AI 研究副驾驶 Liner；Anthropic 的通用助手 Claude；以及三个未经审查的 AI 伴侣应用程序：JanitorAI、Spicychat 和 CrushOn。

有五家人工智能公司真正实现了「跨界」，旗下的网络产品和移动应用都进入了前 50 名单：ChatGPT、Character AI、聊天机器人聚合平台 Poe，以及图像编辑器 Photoroom 和 Pixelcut。

ChatGPT 的规模大约是第二和三名选手微软 Edge 和 Photomath 的 2.5 倍

截至 2024 年 1 月，有九个人工智能产品或社区在邀请流量排名前 100 的 Discord 服务器中，其中 Midjourney 位居榜首。

该列表中的第二个显著新类别是生产力。AI 原生平台可以提升人们与软件的互动，使他们能够委派琐碎的任务并减少在行政开销上花费的时间。生产力类别包括排名中的七家公司：Liner、Eightify、Phind、MaxAI、Blackbox AI、Otter ai 和 ChatPDF。

对于那些有移动应用的伴侣产品来说，用户参与度异常高。在这个类别中最成功的产品成为用户日常生活的核心部分，变得和给朋友发短信一样普遍（甚至更普遍！）。这玩意是真赚钱啊。

根据 SensorTower 的数据，Character AI 每个用户平均每月有 298 个会话，而 Poly AI 平均有 74 个会话。

AI 在移动应用和 Web 上的使用类别有很大的区别。一般来说，Web 产品支持更复杂、多步骤的工作流程，涉及内容生成和编辑。这些产品包括 AI 语音工具包 ElevenLabs、AI 艺术生成器 Leonardo 和 AI 演示文稿生成器 Gamma，它们在基于 Web 的 AI 产品中排名前 20 位。

应用列表中有七个专用的头像产品；大多数人手机上保存的许多自拍照作为可随时使用的训练数据。

此外，排名前三的应用 —— Facemoji（第 9 名）、Bobble（第 31 名）和 Genie（第 37 名）—— 是专为移动设备设计的键盘应用，可以帮助用户发送带有 AI 辅助的文本。

尽管我们名单上超过 30％的通用人工智能网络产品起源于湾区，但只有 12％的移动应用程序开发者来自那里。同样，尽管超过一半的顶级通用人工智能网络产品是在美国开发的，但不到三分之一的移动应用程序起源于美国本土。

### 73

方军 2024/03/14

蒋涛：最近和硅谷投资人聊：1）transformer 大模型投资已终结，就是几个大厂 + openAI 此起彼伏竞争。2）基于非 transformer 的值得看看。3）Nvidia 面临挑战，ASIC 推理芯片会起来。4）Sora 对视频公司有冲击，但 sd 和 midjourey 文生图公司还好。5）on device AI 是热点。6）AI 大模型行业应用和相关工具链在爆发，比如在游戏领域和开发工具领域，他现在一周投一个项目。

@蒋涛 CSDN：修正下：cloud LLM 目前进入军备竞赛阶段 //@自默数日：翻译一下。

1、cloud 端 LLM 是大厂的战场，目前已接近性能瓶颈，其他人玩不起了。

2、edge 端 llm 有望快速发展，会带动终端推理芯片发展。

3、多模态或图像、视频模型（Diffusion model 变体）还有进步空间。

4、基于 llm 的各种应用快速成长。

### 74

方军 2024/03/15

这个网友的试验很有意思：

摘：ChatGPT 如果参加大学课程，最多也就得个 C，或者再高点，但不会是 A。但 C 学生也是学生啊，它不会得 F，还是比什么都不知道，甚至完全不知道你在说什么的好。会装腔作势的 C 学生，会被认为是栋梁之材，占据社会上大部分的高级职位。这就是世界的现状。所以也许我们应该感谢 ChatGPT 的出现，因为它可以代替大部分这样的人的工作。真的。

《如何让 ChatGPT 实现原地归并排序》

思考一个问题，归并排序是否能实现「原地排序」，所以拿 ChatGPT 来试了一下。完整的聊天记录链接在这里：

chat.openai.com/share/7fa803cc-3ade-4a99-864c-395f9d72bca7

总体感觉，ChatGPT 确实能生成可以运行并得到「似乎可用」结果的代码。然而你需要事先已经知道很多深入的知识，否则它可以骗你。

回头看来，它最开头确实是实现了「原地归并排序」。只不过每一步插入「右边」的数字的时候，它都需要把左边剩下的片段往后挪动一个位置。这样的开销显然是不满足排序的速度要求的，然而它却真的是「原地」的。

接着我提出各种「过分要求」，结果它就开始跟我绕圈了，最后还是回到开头的版本。

总体说来，对于这么复杂的事情，当我用文字描述，它居然也知道我说的「点」在那里，并且给出有针对性的回复（虽然不一定正确）。所以我承认 ChatGPT 相对于其它对话系统（比如 Siri，小冰之类）还是一个很大的进步，能够用在某些场合。

今天我又继续给 ChatGPT 一些补充，提示它 O (n logn) 的「原地归并排序」也许是没法实现的。结果它坚持说是可以实现的，还创造出一个「类似于双指针「的术语，但马上又给我一个 O (n^2) 的移动大量数据的，跟之前的一模一样。

再次让它思考这个是否可能，仍然说可能，只是 JavaScript 语言不行，其它语言有更好的优化，就能实现 O (n logn) 的「原地归并排序」。这跟语言有什么关系？没有。但如果你继续下去，说你用其它语言试试，它就可以用另一种语言从头把这一切再给你来一遍。绕你几圈，却仍然无法解决问题，而且仍然坚持说这是可能的，而且它每次都有道理。

所以我不得不承认，ChatGPT 已经超越了很多的人类。因为大部分人类也就是这样一知半解，却能显示出」很懂「的样子，很会装腔作势，抬出术语来吓唬人。好像明白你在说什么，而且能造出合适的回答，让你迷糊一阵，以为有道理，结果最后还是没有解决问题。

ChatGPT 如果参加大学课程，最多也就得个 C，或者再高点，但不会是 A。但 C 学生也是学生啊，它不会得 F，还是比什么都不知道，甚至完全不知道你在说什么的好。会装腔作势的 C 学生，会被认为是栋梁之材，占据社会上大部分的高级职位。这就是世界的现状。所以也许我们应该感谢 ChatGPT 的出现，因为它可以代替大部分这样的人的工作。真的。

所以 ChatGPT 真的通过了图灵测试，并且真的实现了人类智能。感谢 OpenAI，感谢 Matrix。

### 75

方军 2024/03/15

062 用好 AI 需要流程/章法

用好 AI 不容易，流程思维比提问技巧更重要些

最近人真愿意了解如何用 AI，我会用下面这张图给人家分享。

对于不是真愿意了解的，我通常就不多说，因为那些人多半是把 AI 看成神奇的工具，能够一下子解决他 / 她的问题。这种假设之下，说什么都是无益的，你说要哪个比较好，那我就随便推荐一个两个给你用用。

我现在对 AI 工具的判断是，大同小异，即便把 GPT-4 放在里面，也都是 70-80 分的平均水平。

如果你不会用，基本上也能 60 分。当然，如果你瞎用，比如直接听信幻觉，或者直接抄袭 AI 的回答，那是零分。

你用了 80 分的好模型，你就能得到更好的结果吗？不一定的，如何提问，如何用流程，它对结果的影响有时候比模型本身还大。

以前有一段我们强调提问技巧。当然，会的会了，不会的还是不会。

现在，我们则比较多说流程。如果一件事你不会做，你绝对不要期望 AI 帮你做好。

你不知道做一件事的流程，或者用较为地道的中文表达说，你做事没章法，你不可能指挥 AI 给你拿到想要的结果的。一点可能性都没有。

现在的 AI 的能力就是这样。我们得这样做才行：拆解开任务，一点一点地与 AI 耐心探讨。这样，AI 才会变成非常有帮助的伙伴。

流程比提问技巧重要，因为所有的上下文、背景知识，实际上都是在流程里面的。

提问技巧容易，流程难，因为流程里面包含了太多的内隐知识，已经被书面化的并不多。因此，它也更需要个人自己的摸索、思考、实践。

方军：朋友的点评：系统化的研究方法是用好 AI 的前提。让 AI 帮忙整理大纲，再顺着适合自己思考逻辑的路径一点点把问题挖掘下去。不然问完问题后会觉得 AI 不过如此。

2024-03-15 18:33

### 76

方军 2024/03/15

美图总收入与净利润增长主要得益于 AI 推动主营业务收入增长。美图用户每天处理数亿份图片和视频，约 83% 都用到了泛 AI 功能。

[美图公司发布 2023 年度业绩：AI 驱动净利润大涨 233.2% 至 3.7 亿元](https://mp.weixin.qq.com/s/uDKZjMVNMhTh82ARCF2hhA)

### 77

方军 2024/03/15

摘：像字节 Coze 这样的工具本质上是「AI-first aPaaS」。

Dexter Yang

「aPaaS」是指这些 Bot Builder 完完全全就是以前的 aPaaS，把实现一个应用所需的不同类型代码 —— 数据、状态、API 调用、逻辑（工作流、事件系统等）、UI，用不同的可视化工具来实现，比如数据库建模、服务插件、节点图工具、拖拽式 UI 搭建工具。且生成的不是新应用的完整代码，而是「配置」，所有创建出来的「应用」都是 aPaaS 本体这个单一应用读取不同配置的运行结果。

Bot Builder 只是对其中部分类型，换了不同的可视化工具，比如针对「数据」类型用 RAG 工具，对「状态」类型用 Token 缓存等工具、对「工作流逻辑」用 Agent 搭建工具，对「UI」用提示词和卡片配置工具。得到的「应用」一部分作为「配置」存储和运行在 Bot Builder 平台自身，一部分作为「配置」存储和运行在各种 Chatbot 平台（比如 ChatGPT）。

「AI-first」是指它们不但开发应用时用 AI 辅助或依赖 AI，开发出来的也是 AI 应用（目前主要形态是各平台上的 chatbot）。应用的开发阶段有大模型加持（比如用自然语言描述任务），应用的运行阶段也有大模型支撑（大模型扮演两个角色，最平庸的角色是用大模型的 prompt 调用取代手工编写的代码，更重要的角色是借助大模型做到手工代码做不到的事情）。

像这样的 AI 应用开发平台，存在的问题是：aPaaS 这种单一应用的模式，跟内容平台（比如微信公众号、Medium、头条抖音，很多内容平台同样有「开发」需求，比如文章的 HTML 排版和 widget 组合配置，视频中的  AR 效果）、乃至元宇宙平台（比如 Roblox、堡垒之夜、Decentraland、VRChat、元梦之星，这些平台中用户创建的每个 3D 世界，都是应用，传统上都需要专门开发）是非常一致或者说一脉相承的。

缺点是，不生成完整、专业的应用代码，跟专业应用开发（包括开发方式、最佳实践、技术生态、抽象积累）割裂，自成体系，重新发明一切，无法灵活深度的混搭和优化（我以前写的《「全码」 通用搭建》里有讨论过）。

优点是，天然趋向把同一个应用在开发阶段的形态和运行阶段的形态统一，类似本帖引用中 Ego 的说法「a game engine that is also a game」，应用自身就是应用开发工具、就是编辑器，开发应用的同时就是在使用应用，开发游戏的时候就是在玩游戏 。

aPaaS 们（含 Bot Builder）显然还远远没实现这种优点，仍然有使用门槛，使用 Bot Builder 过程中的复杂性也远高于使用 Bot。

Bot Builder 们只做到「AI-first」，并没做到「AI-native」。

引用中的 Ego 是一个「AI-native App Builder」的例子。

定位是「AI-native simulation/game engine and platform」

- 相当于元宇宙 AI App Builder

- Ego 的编辑器状态和运行状态似乎都是同一个 PWA 形式的 3D Web 应用

- 开发阶段有大模型，让用户能用自然语言创建人物、世界、脚本交互，「AI-native」的设计让使用门槛甚至低于 Roblox

- 运行阶段也有大模型，游戏人物都是 agent 像人类一样自主行动和互动（「做到手工代码做不到的事情」）

开发团队用这个引擎做了两款 demo 游戏，一个是 Stanford Generative Agents 论文的 3D 实现 Townworld，一个是 AI 人物组成的选秀游戏 CharacterIdol。愿景是让用户能用自然语言创建 GTA、模拟人生、Minecraft、动物之森，同时里面的人物都具备人类行为能力（「让召唤一个 Agent 像填写一张龙与地下城人物卡一样简单」—— 创始人这句话暴露了自己的成分…）。

twitter.com/dexteryy/status/1768510684206342543

### 78

方军 2024/03/15

果然，最近很多人猜测 kimi 是 RAG 路线：

kimi 目前效果很好了，走的是 perplextiy 路线，先检索过滤过的互联网内容，然后补充 AI 生成，就是 RAG 路线。一是减少幻觉，二是直接用网络资料增加中文理解，吊打文心。这个路线真实天才。

jackit：有详细的资料吗？

2024-03-15 23:14

方军回复 jackit：没有，都是猜

2024-03-15 23:17

### 79

方军 2024/03/15

v0.dev 生成界面还有这个用途啊

我倒是的确很熟悉这几个工具，也许应该试试

tailwind / shadcn

### 80

方军 2024/03/15

我声明对未来五年保持超级乐观得意。

摘：近日，有消息称，亚马逊、微软和谷歌等大型云服务提供商的内部消息显示，生成式 AI 这股热潮可能被过度夸大。据外媒报道，上述公司内部已经开始向销售团队调整预期，强调目前生成式 AI 的技术与其市场宣传之间存在差距。多数客户在对新 AI 服务的投资上表现出谨慎态度，他们担心高昂的使用成本、不足的准确性，以及评估技术所带来价值的难度。

### 81

方军 2024/03/16

063 如何用 AI 进行项目调研

看了一些人用 AI 进行项目调研的作业，写了几句点评：

总体印象是：各位的作业比较像典型的 AI 生成内容。

建议一：反向建议，各位如果之后在网上看到类似内容，注意是不是 AI 生成的。

建议二：当我们用 AI 来辅助时，要自己为主，AI 为辅。这次我们看到的很多的综合回答各位应该没有加自己的判断。

使用 AI 进行一个项目辅助调研的流程，关键可关注如下三点：

第一，你收集的资料，然后根据这些资料来提问。

不要过分依赖 AI 提供的资料，而是要尽量自己提供资料。并且，要对 AI 总结的任何资料保持警惕。AI 可能出现幻觉，也就是胡说八道。

第二，第一轮提问。你希望了解的信息，有什么不清楚的，直接提问，直到问到你自己能够了解。

然后，尽量用自己的话来总结观点，而不要用 AI。因为总结的时候，你能更清晰地看到，自己是不是真的了解了。

第三，第二轮提问。这一轮提问是让 AI 的当你的反方选手，让 AI 来尽可能地挑你的观点的错误，哪怕钻牛角尖地挑错。类似于在小组讨论时，有个人就是专门提反对意见。

这样，经过各种刁钻的挑战之后，你的观点就相对更站得住脚。

最后，再把关于这个项目的观点写出来。我的建议是，尽量自己写，而不要让 AI 写。（你写完之后，可以请 AI 帮你润色表达。观点是你自己认可的，表达是 AI 帮忙调过的。）

总体来说，就是将 AI 看成你聪明的「助手」，但仅仅是助手。

方军：后来又补了几句。

别问大问题，比如别问这个项目有什么优点，这种结论都是大而空，而是问小问题，它这个具体的做法是怎么做的、有什么优缺点。

2024-03-16 19:36

### 82

方军 2024/03/16

今天很多在讨论这个 KPU 的处理方法，看着还不错

可以持续关注

maisa.ai/blog/kpu

[Maisa KPU：大模型推理的新范式架构](https://mp.weixin.qq.com/s/JrbQEHRvTchABLYZqgPNWw)

### 83

方军 2024/03/16

Claude 重写提示语：

不应该再手写提示语了。

特别是现在，克劳德在写作方面的能力更加出色

从大致想法开始，然后像这样寻求改进：

Prompt:
"I have a rough outline for my prompt below, as well as my intended goal. Use the goal to make this prompt clearer and easier to understand for a LLM. 
<goal>
{your goal here}
</goal>
<originalPrompt>
{original}
</originalPrompt>
<improvedPrompt>
</improvedPrompt>"

提示语:
我对下面的提示有一个大致的轮廓，以及我的预期目标。使用这个目标来使这个提示对LLM更清晰、更容易理解。
<goal>
您的目标在这里
</goal>
<originalPrompt>
{original}
</originalPrompt>
{改进的提示}
</improvedPrompt>"

你会对它将零碎的词语和思维转化为几乎完美的提示语的能力感到惊讶。

### 84

方军 2024/03/16

这玩意很牛啊，我倒是要先搞明白 Selenium 。

www.selenium.dev 浏览器自动化测试

它和 puppeteer 应该是相似的

github.com/lavague-ai/LaVague

里面几乎每个组件我知道，但整体搞到一起，真是全是新东西。

### 85

方军 2024/03/16

Anthropic 的这个 meta prompt 很赞啊，我还在努力理解：

docs.anthropic.com/claude/docs/helper-metaprompt-experimental

Notebook: colab.research.google.com/drive/1SoAajN8CBYTl79VyTwxtxncfCWlHlyy9

视频介绍： www.youtube.com/watch?v=Evg4HXvsYVY

### 86

方军 2024/03/16

识别 AI （生成的内容）和识别骗子，其实有很多相似之处的

我觉得但凡讲话的时候，不断讲自己名字的，都不可信。

识别 AI，现阶段我觉得就是要看它的讲话是不是过于正式、权威。

虽然我们可以通过提示语要求它别太正式，但其中的暗示还是很明显的。

判别人的观点时，我觉得一个原则是，一个人专业的地方我们信，但不要把他的专业光环延展到任何他的专业之外。

（比如，刚刚推上看到专家，他讲编程真是不靠谱啊，虽然在 AI 方面他自己专业领域非常厉害。）

判别 AI 的观点时是相似的，别把它看成万事通。

### 87

方军 2024/03/16

064 太多人低估用了用 AI 干活的难度

分享个很早之前的体验。我当然可以说，有了 AI 干活可方便了，上百页的英文 PPT，几万单词的视频脚本，AI 全写了。然后，AI 自动生成视频，高水准的课程完成了。

但真实的故事不是这样的：

首先，准备这个课程的人是我，我算是对 AI 的技术原理、生成式 AI 的最新进展、AI 的应用场景、课程中要用到的具体场景很有了解的人。我还很了解各种 AI 的故事，也有面向大众准备课程的经验。并且，我前后还愿意花近一个月的时间在这件事上。

其次，为了准备课程，我大概又精细地看了 600 分钟的英文课程。精细到什么程度呢，听了两三遍，整理了全部的英文脚本。要这么做是因为，我并不熟悉这类英文课程中的表达方式，需要用这样的方式熟悉起来。另外也包括阅读和查阅相关的图书、论文、网络资料。

到这儿其实 AI 还没开始干多少活，最多就是看一些东西时可以偷懒用 AI 翻译为中文以便用双语方式快速阅读。

下一步也没有。下一步是准备课件。一百页的 PPT 必须自己来完成。老是有人说，让 AI 写 PPT，这件事怎么可能，PPT 是精简的核心信息，你可以只写一行字，也可以只有一个标题一个图，反过来，AI 帮你随便糊弄上去的东西有什么用？

再下一步，我才逐渐地引入 AI。一种方式是，我根据 PPT 用英文撰写脚本，然后 AI 进行润色。另一种方式，我实在没法用英文直接写的，用英中混杂的方式写文字，然后 AI 变成英文。

之后，采用提示语对脚本进行口语化。我采用的方式是要求它严格遵照现有的文本，编写超过两倍长度的脚本。然后，人工对脚本进行逐词、逐句的处理。其中也包括遵守规范的处理，把直接引用恢复 ，或者确保间接引用是合理的。

在之后，是多角色 AI 对脚本进行审校，包括内容、表达、润色、校对等等。这时前期资料完备的好处就体现出来了，这些 AI 的工作是在提供了完备上下文的前提下完成的。

—— 我个人觉得 AI 能够带来大量的生产力的提升在这儿，这对我们这些英文阅读水平尚可，但写作能力不太灵的人非常有帮助。否则我们要花好多倍的精力与英语表达较劲。

在这个过程中，还要再借助传统的语法检查软件多次检查，并不能完全依赖现有的 AI 工具。

终于到了视频前期的步骤，脚本用 TTS 播放数遍，看在注意力集中的情况下和不集中的情况下，能否听懂。并根据试听的结果，对应地调整 PPT 视觉和脚本。

最后的步骤才比较简单，就是在有视觉内容（PPT）和音频内容（脚本）的情况下，完成视频的制作，其中采用了某平台的预制口型数字人。

有了 AI，这个过程当然变得高效非常多。但是，其中如果没有人（也就是我）的高强度参与，这个任务根本不可能完成。

因此，我真是很不明白很多人为什么认为 AI 可以神奇地把所有的活都干了。

当然，真正干这个活的时间并不长，不含前期的资料准备和酝酿，真正干活的时间大约是 7x16=112 个小时。

（我也是在这个过程中，才对人的参与 HITL 有强烈的感受。当时还没有接触到现在图中的所说的流程工程，但做法上其实做到了。）

### 88

方军 2024/03/17

有意思的实验

[我在技​​术面试中用 ChatGPT 作弊，没人知道](https://mp.weixin.qq.com/s/RDEdahVSOZckjxFzXa12BQ)

### 89

方军 2024/03/17

归藏：Magnific 这两个老哥真是 AI 时代个体创业起飞的典型了，就只是把图像放大这个非常小的功能打磨到极致给自己带来了丰厚的收益和影响力。

所有的图像生成工具都把他们的产品当成标杆去比较和宣传。他们昨晚也发布了关于这段时间发展的一些思考：

---

每当看到竞争对手用我们的名字（Magnific）来做点击诱饵或市场营销，我觉得应该是种荣幸。

过去这几个月，我们收到了无以伦比的关注，Leonardo、Krea、Freepik、Scenario、Playground，甚至是 Stable Diffusion，还有 Reddit 上的无数帖子，涉及 ComfyUI/Automatic1111，都在推出他们自己的图像增强器，并把我们作为标准。

想到我们两个来自西班牙一个小城市 Murcia 的普通人竟成为了众人关注的焦点，真是难以置信。对我和 Emilio 来说，这一切都像疯狂的梦境！

市场上有个空白点，被 Magnific 填补了，直到那时，没人意识到它的存在。

现在，它有了一个名字，一个我们在 Magnific 旅程初期就共同铸造，带着情感称呼的名字：

重新想象的图像增强（REIMAGINED UPSCALING）。

看到其他竞争者也想跻身其中，这很正常！不仅正常，对大家都有好处！

对你们来说，因为有了更多选择；对我和 Emilio 来说，因为这激发了我们更上一层楼的决心！

但说实话，当有些人试图欺骗人们，让他们误以为他们掌握了「唯一的 Magnific 配方」时，我会感到遗憾，因为事实上他们并没有。Magnific 配方的精髓在于许多细微之处的调整，这是无法复制的。

更重要的是... 他们根本不需要「Magnific 配方」！你知道吗？每个图像增强器最终都可能是独特的，都会有自己的特色！

未来会有比 Magnific 更好的图像增强器吗？当然会！但对一些人来说，最好的永远是 Magnific，因为我们有自己独特的魔力，自己的风格。是的，即便是在像图像增强这样的事情上。这当然也适用于其他竞争者。嘿，我们不会嫉妒的 :)

所以，被视为行业标准，我们感到荣幸，但... 我这次想认真说说，没有必要试图通过宣称「我们是新的 Magnific」或「我们拥有 Magnific 配方」来欺骗人们.... 因为你知道吗？这不仅不真实，而且你还在撒谎，最关键的是，成为一个优秀的图像增强器，你只需要有自己的特色！

竞争者们，请公平竞争！这个市场上有足够的空间给大家。不要欺骗人们。要公平。要诚实。

来源：x.com/javilopen/status/1769147508440125719?s=20

### 90

方军 2024/03/17

有意思，GPT3

摘：现在的理科压轴题与我们那时候不同，更多体现「现学现卖」的学习能力，所以在解题分析时，要更加注重对题目材料的分析研究。

在自身基础知识和基础技巧扎实的前提下，更多的是从题目本身获得新的知识和解题思路。

如何能够从题目本身汲取营养，更加能够考查学习能力。

@杭商院的流浪汉 2011：30 年前的化学高考题就喜欢这样的，那时叫信息给予题，学生都很慌的，高度紧张的气氛中对新的信息理解能力是下降的，读题都要好几遍，生怕自己理解错了。

@暖心庭爸：是的，90 年代的化学材料题已经是这样的了，后来数学物理也都用了这种做法。现在初中数学的压轴题，一页纸都不够描述题干的，信息提取就已经有困难了，还要分析背后的逻辑，就更难了，所以更多的学生只能放弃。

### 91

方军 2024/03/18

摘：Wing Venture 的观点和以上比较类似，认为垂直行业的私有数据非常重要，但最终的护城河还是要深入工作流程。这可能有两种方式，

一种是将自己与现有企业的流程进行整合，比方说在某个生态里以一个插件的方式提供服务，或者通过与它们的内部流程进行整合，这是大部分产品做的一种方式；

而另一种就是用 AI 来完全重构一个全新的流程，典型的例子就是 Tome，正在从头开始重建幻灯片软件。但都是在深入业务的流程。

像我们比较熟悉的 Jasper AI，一开始作为现有工作流程的一个独立产品，用户可以在其产品里生成文本并进行编辑等，而现在的 Jasper AI，正在朝着集成的方向发展，某种意义上也是在重构流程。这我们从 Jasper AI 首页的 Slogon 就能大概看出，以前的 Jasper AI 写的是：Create amazing love letters 10X faster with AI，针对的是某个具体的功能；而现在的 Jasper AI 则偏向于一个工作流：An AI copilot for enterprise marketing teams who want better outcomes, not just faster outputs。

[多家 VC 谈 AI 应用的护城河：技术差异正趋近于零](https://mp.weixin.qq.com/s/sRseEm4CNgyRg54tLhhQTw)

---

关于这篇文章有点感慨，我发现大语言模型/对话机器人有个假设其实不对。

这篇文章的信息量蛮大的，我摘的这个点是我当前关注到的。但怎么从整篇长文中获取信息，真是不容易。

摘出来的片段容易摄取，是因为它是信息内聚的。

大语言模型/对话机器人的假设是，人能阅读大段的文字。但事实真不是这样的，大段的文字能有效阅读的人非常少。

现在在不加特别提示语的情况下，对话机器人都是废话太多，不凝练。当然也有认为是通过解释了降低了理解难度，但多数情况并非如此。

另外，我真是蛮讨厌各种万字长文的，那是典型的高估其他人的接受，而仅仅为了凑字数把所有的笔记都包括在其中，不做必要的删减。

另一方面，大语言模型也有好处，我们可以借助它来精简。并不是说用它写摘要（AI 摘要几乎都是质量很差的），而是可以在它生成的大量文字上快速做条理化和删节，既然 AI 可以生成大量文字，那删节就没什么可惜的了。

### 92

方军 2024/03/18

很不错的详细讨论

华尔街日报的知名科技专栏作家 Joanna Stern，专访了 OpenAI 的首席技术官 Mira Murati，深度讨论了在今年 2 月 16 日发布的，全球爆火文生视频模型 Sora。本次访问少了一些赞美，多了一些「刻薄」。如何解释，Sora 明显的缺点？它的训练数据来自哪里？何时上市？如何保证生成内容的安全性？… … 面对 Joanna 一连串的尖锐问题，有时，Mira 只能龇牙咧嘴地含糊而过，或者，尴尬一笑。

[Sora 并非完美，致命缺点也很多](https://mp.weixin.qq.com/s/ESDTdcvrz4FZUqlIavTgCQ)

### 93

方军 2024/03/18

[为什么自从用上了 AI，你的创造力不升反降了？](https://mp.weixin.qq.com/s/PG9Jnkw14rOBhjaXEjfVWA)

### 94

方军 2024/03/18

摘：王凯：现在大家转向一下，真的转向一下，有个非常非常大的 gap：

1、不要再追任何「行业炸裂」的突破点了，把它作为营销、获取流量的方式；

2、深入做面向客户、用户的实用 AI、结合 AI 融入某项工作流的内容，比如之前一波都是针对自媒体、做号群体的教程、内容，现在 AI 产品越来越多、国内的 AI 产品也逐渐起来，完全可以深入到企业、工作流里面做内容、教程、付费服务；

3、之前我很建议大家针对老板、企业做内容，进而有 AI 转型咨询的机会，现在明显感觉针对群体做 AI 实用内容越来越可行；

4、这里面有个越来越大的 gap，且 gap 越来越大 —— 比如秘塔、Kimi、沉浸式翻译等各种 AI 应用其实已经产生了足够多的用户忠诚度，但是我看到只陷入到小范围关注 AI 的群体中。

是时候抛掉 AI 的标签，针对实际产生的价值、对用户的体验提升来做内容、服务了，类似少数派等。

抛掉 geek 人群，去做大众内容，很基础很基础的内容，流量非常大、基于 AI 蓬勃发展的势头，素材库无穷，无论是做 AI 应用教程、实践内容，甚至做众创让更多不关心 AI 的人分享实用某项 AI 应用的实际情况、感受等，都是对大众层面最好的内容，比单纯关注炸裂点要好很多。

调整下，别炸裂，求实效。

### 95

方军 2024/03/18

是这么个情况，反正我是累死了：

摘：

某：智力劳动者，或许也会成为流水线上的工人。

最近高频率使用 GPT4 写代码，一周下来反而更累了。

虽然 GPT4 写了大部分代码，但个人一周要处理的事务也就几倍的增长，将一个月的决策思考量压缩到了一周。自然更累。

这让我想到流水线上的工人，虽然机器提供效率，但工人比种田其实更累。

问：为什么不少写一点？

某：好问题。代码确实没写几行，但决策和判断反而更频繁了。以前手敲代码，如果不涉及到算法，基本都是不怎么费精力的。现在虽然一周干了一个月的活，但一个月的决策量都要放在一周，也挺费精力的。

### 96

方军 2024/03/18

有意思，AI 写PPT

用 langGraph实现的：

How to Build: a Text-to-PowerPoint Application (LangChain, OpenAI, CopilotKit & Next.js)

[How to Build: a Text-to-PowerPoint Application (LangChain, OpenAI, CopilotKit & Next.js) - DEV Community](https://dev.to/copilotkit/how-to-build-an-ai-powered-powerpoint-generator-langchain-copilotkit-openai-nextjs-4c76)

---

我有点明白我跟用 AI 写 PPT 的人的立场差异了

人家就是要糊弄一个还行的，我认为要糊弄随便选个模板好了，何必用 AI

还有这种写程序用 AI 做 PPT 的，人家要的是，你看，我有时候行，我想说的是，你能不能 80% 时候行？


### 97

方军 2024/03/18

宝玉的这个体悟有意思：我家孩子沉迷搭乐高，陪他搭乐高的时候，我就联想起 AI 写程序！

他今年 4 岁，但已经不满足于搭 4-5 岁的乐高，而是要 9-10 岁的乐高，这条「祥龙纳福」就是他自己照着说明书拼的。

看起来很天才？其实不过就像现在的 AI 会写程序了一样！

因为让他独立去搭，是不可能搭出来的，虽然他照着说明书大多数时候能得到正确的结果，但也经常会拼错，而拼错了就无法继续了，就需要我帮他去修复错误，然后他才能继续，有时候甚至要拆到大部分重新开始。虽然我一直提醒他每个步骤完成了要去对照说明书检查，但对他来说还是太难，无法在出错后发现明显的错误。

这就像现在 AI 写程序，无论你怎么去从工程上优化流程，它只有一个像 4 岁孩子的智商，是无法独立去搭出来需要 9-10 岁年龄的乐高，而需要大人（专业人士）去引导，在出现问题后帮助修复、排除障碍。就算通过提示工程让 AI 在写完后自己检查，但以当前 AI 的能力很多错误还是无法检测和修复。

但是，这不能说明 AI 写程序这事不靠谱。就像孩子会长大，今年 4 岁，可能再过 2-3 年，长到 6-7 岁，平时练习一下，就能独立完成 9-10 岁的乐高玩具了。AI 的进化速度也许比我们想象的更快。

另外这个乐高本来我要花 2 小时时间去搭的，现在只要偶尔抽点时间帮指点一下，加起来也就是半小时时间，还是帮我节约了很多时间。

要让 AI 能写好程序，就像让 4 岁孩子去搭 10 岁孩子的乐高，至少用 AI 的人得有搭 10 岁乐高的水平。

### 98

方军 2024/03/19

宝玉：分享一点利用 Claude 3 Opus 整理字幕文稿的经验，它可以非常好的将文稿整理成对话形式，并且还可以按照话题分成章节！几个要点：

1、说明对话的人物（基本上 Claude 能自动分辨出哪些话是谁说的，很准）

2、要求根据话题分成章节（这个对于阅读、总结都很重要）

3、要求不要遗漏内容（不说明可能就给你总结了）

参考 Prompt：

「以下是我整理的 XXX 对 YYY 的采访文稿，请整理成更适合阅读的对话形式，根据话题形成章节，包含人名，尊照原意的情况下适当润色，从头开始，不要遗漏任何内容！」

如果没有输出完整需要不停的输入 continue 直到结束。

如果要总结，最好等到完整文稿输出完成，然后让它基于章节来总结，这样总结出来效果最好，不容易遗漏要点。

参考 Prompt：

「请按照上面的章节总结上面的文稿，需要详尽不要遗漏重点！」

方军：用不了 claude3，实在不想折腾了，这公司真是没法说。最早是申请 waitinglist 各种毫无反馈，然后是各种封号，api 据说也是各种封，再然后通过 aws 提供服务似乎也… 算了算了，不缺这一个，大同小异而已

2024-03-19 10:36

### 99

方军 2024/03/19

摘：基于 BAMBOO，我们对 5 种大语言模型的长文本建模能力进行评测，并得出了如下的主要结论：

ChatGPT 在绝大多数任务上取得了最好的性能。

大部分模型在要求复杂和任务不常见的数据集上表现不差，例如 PrivateEval。

随着输入长度的增加，模型的性能通常会下降。

[BAMBOO: 全面评估大型语言模型的长文本处理能力](https://mp.weixin.qq.com/s/NKAncSFUloWlNhRjOllmSg)

另外，为了更好的研究长文本建模影响因素，我们对 5 个关键的研究问题进行了探讨：

RQ1: 长上下文窗口的 LLMs 是否因扩展而付出代价？

结论：对于开源大语言模型来说，扩展 LLMs 的上下文窗口可能会对短文本任务的性能产生负面影响，这种现象被称为「扩展税」。然而，对于中等长度的文本，更长的训练数据和位置插值可能有助于提高性能。

RQ2: LLMs 在长输入中的挑战是由于文本长度还是任务本身？

结论：LLMs 在长文本场景中的糟糕表现主要是由于推理和编码能力的不足，而不仅仅是因为证据在长文本中的定位错误。即使只输入证据，虽然有性能提升，但是幅度相对较小，且模型在不同长度的问题中犯常常相同的错误。

RQ3: 指令位置如何影响长文本建模？

结论：指令在输入中的位置对 LLMs 的性能有显著影响。随着输入长度的上升，将指令放在输入的开始部分通常会导致性能下降。而最佳的指令位置取决于数据集和模型，但通常在输入的末尾放置指令会有更好的表现。

RQ4: 使用上下文压缩方法的 LLMs 能否处理更长的文本？

结论：上下文压缩技术，如检索和截断，可以增强短上下文 LLMs 处理长文本的能力。检索增强的 LLMs 可以达到与长上下文 LLMs 相当或更好的性能，而截断和总结方法由于遗漏了大量相关信息，通常表现不佳。

RQ5: 为什么会出现「Lost in The Middle」现象？

结论：当证据信息位于输入的开始或结束部分时，模型的性能通常更好，尤其是在更长的输入序列中。注意力图分析显示，无论证据在输入中的位置如何，模型倾向于更有效地利用位于输入开始或结束部分的信息。

### 100

方军 2024/03/19

再次分享这个家庭教育场景，我觉得讲得还是蛮好的。

内容和上次针对中小学生差不多，但换成了偏向家长视角，所以内容上比上次多了一些。

分享提纲

引子：你觉得 AI 聪明吗
📕 第一部分：结构化指令
1.1 AI 是什么？
1.2 如何问，AI 才会听？
1.3 🍎 作文修改老师
1.4 🍟 互动背单词
1.5 🍉 数学学习伙伴
1.6 🍭 AI 编程助教：编写代码
1.7 小结：「1+3」的「1」部分
📕 第二部分：3 大技巧发挥AI超能力
2.1 🎯 三步AI提问法
2.2 🚗 思维链AI提问法
2.3 🚀 角色战队AI提问法
2.4 小结：「1+3」的「3」部分
结语：成为比 AI 还厉害的人

2『已下载原文「20240318如何用AI：用AI打造全能家教」。（2024-03-20）』

### 101

方军 2024/03/20

摘：复现 Magnific AI 的图片放大工作流

这个工作流很适合自己有图片放大需求或者想做类似产品的人，感兴趣可以收藏了自己试一下。

放大后的细节表现确实和 Magnific AI 很像。其中主要用到了 Tile 模型和 Tile Diffusion 这个插件。

还包括一个 SD1.5 模型 Juggernaut 以及两个 Lora，一个 Embedding 模型。

Tile Diffusion 插件使用了 4x-Ultrasharp 这个放大模型。

具体的工作流参数为：

Prompt: masterpiece, best quality, highres, <lora:more_details:0.5> <lora:SDXLrender_v2.0:1> Negative prompt: (worst quality, low quality, normal quality:2) JuggernautNegative-neg Steps: 18,  Sampler: DPM++ 3M SDE Karras,  CFG scale: 6.0,  Seed: 1337,  Size: 1024x1024,  Model hash: 338b85bc4f,  Model: juggernaut_reborn,  Denoising strength: 0.35,    

Tiled Diffusion  upscaler: 4x-UltraSharp,  Tiled Diffusion scale factor: 2,  Tiled Diffusion:  {"Method": "MultiDiffusion",  "Tile tile width": 112,  "Tile tile height": 144,  "Tile Overlap": 4,  "Tile batch size": 8,  "Upscaler": "4x-UltraSharp",  "Upscale factor": 2,  "Keep input size": true},    

ControlNet 0:  "Module: tile_resample,  Model: control_v11f1e_sd15_tile,  Weight: 0.6,  Resize Mode: 1,  Low Vram: False,  Processor Res: 512,  Threshold A: 1,  Threshold B: 1,  Guidance Start: 0.0,  Guidance End: 1.0,  Pixel Perfect: True,  Control Mode: 1,  Hr Option: HiResFixOption.BOTH,  Save Detected Map: False",  Lora hashes:  "more_details: 3b8aa1d351ef,  SDXLrender_v2.0: 3925cf4759af"

github.com/philz1337x/clarity-upscaler

\#AI绘图#

### 102

方军 2024/03/20

我可能太固执了，我觉得大语言模型之外的其他模型，目前如果你不是相关行业的，无需关注

因为只有语言是通用的，它的应用场景和可能性是其他的一百倍

如果不是视频相关的产业链条里面的，看什么 sora 纯属浪费宝贵的精力

在这个时期，注意力放在什么上面，是个重要的选择

### 103

方军 2024/03/20

关注宏大和关注细节真是两种人，我不需要做这种学术型 ppt，但人家的这种讨论都看得津津有味：

dr. 朱耀

最近连着做了两个英文讲课，分享一些 tips：

1、会把讲稿写在附注，帮助排练，但是 slides 上文字尽可能简要（基本上就是 title、situation、key-point），文字很多照着读会感觉非常平、也容易发困。

2、讲稿会用下划线、粗体、空格来对应要点信息、听众陌生的词、short pause 做些转折。

3、10 分钟的讲课，会考虑 15 张 slides，留下 30s 冗余，前面节奏可以有伸缩性，不要拖，最后 2 张幻灯作为总结的内容，可以很方便时间控制。

4、用提问的方式引入讲课，会让听众更好参与其中，适当的临场举手调研也有帮助，大家都想知道同行怎么想法。

5、注意字体、颜色搭配和空间布局，讲课时没人在乎你是不是满头大汗，做好 ppt 就是做好你的脸面，可以分担许多压力。

6、慎用 slide 之间的动画，适当考虑推动内容逻辑的 slide 内动画。

7、前面 3 张 slides 超级关键，一定需要让大家抬起头或者拿起手机。

8、不要一会是从上往下内容排布、一会从左往右，会晕。

9、科学幻灯会有很多数字，但是只有一小部分是你想要听众记住的，记得提醒他们关注的重点数字，这是串起你讲课的关键。

10、别忘了 ref 标注，别忘了 acknowledgement，别忘了自己的联系方式。

### 104

方军 2024/03/20

月之暗面的确在做非常有意思的探索

当然我还是不感冒，我还在想如此大量的资料对人意味着什么

[一次性支持 200 万字无损上下文！Kimi 智能助手玩了个大的 —— 月之暗面「登月」最新进展！](https://mp.weixin.qq.com/s/riq8xlZNNt0m-vZtRMpeAw)

alasya：很多时候，我们以为是质量的问题，其实是数量不足。

很多时候，数量就是质量，数量决定质量。

一个研究生，智商 125-130 ，加挂 5 个学科大专以上知识库以后，在 seminar 就学科课题发表见解，能让清北两位资深老院长（不同场合），误认为她是出类拔萃天赋卓著的小孩。

这个人类，又花了十九年，才凑到 11-12 个学科。

2024-03-20 13:58

方军回复 alasya：我认为还不是这么笼统，我现在给资料更希望给相对高品质精确资料，所以 8k 的上下文对我都已经足够了

2024-03-20 14:08

alasya 回复方军：人脑带宽就是如此。结构复杂，耗能小，处理能力有限。

如果结构复杂，耗能大，处理能力不受限呢？

### 105

方军 2024/03/20

这个产品不错

reportify.cc

AI 投资研究深度内容问答引擎

深入搞一个领域的数据，有价值

不知道为何不是直接推海外，而是一个中文产品。

### 106

方军 2024/03/20

我可能 50% 以上时间把 AI 大语言模型看成是「学习工具」

比如，我这会儿在问的问题就是，如何方便地查看一个代码库的更新的内容。

具体当然要复杂些，跟踪一个代码库，它大概有 4、5 次 commit，其中有一个是主要的，其他几个是次要的，怎么快速地了解这数千行代码的更新。后续还有一系列事情要做。

当遇到这样的一个学习问题时，AI 大模型的确相当有效。—— 但又不那么有效，因为我刚刚看到的一个 vs code 插件很厉害。那个插件如果不是我总是随便看东西不会知道，因为是一个熟识的代码审计公司开源的。

但不管这样，AI 大模型真是极其强大的学习工具。

至少 10X（十倍级别）。

### 107

方军 2024/03/20

065 拦住普通人用 AI 的障碍是什么？

我很关心这个问题的答案。我暂时想到三个：用什么工具，怎么提问，场景感。

前几天针对一个特定使用场景，我们线下讲完之后，又不得不做了一次线上答疑。线下讲的是原理，同时安排做实际作业，线上本来以为会有很多问题，但实际上大家也不怎么愿意问，最后就是我用一个相对完整的案例给大家演示了一个半小时。

🔺 拦住普通人的首个障碍可能是不知道「用什么工具」。

我们当时考虑到这个问题了，因此当时选了一个让大家比较方便在微信里面就可以用起来的 Kimi，而没有用我们自己制作的、功能上更有针对性的一系列对话机器人。初步上手了，总有机会用更高阶的。

同时，考虑到大家的英文水平，我们同时演示了 Perplexity 和 metaso。我当然觉得 metaso 是不行的，一是只有中文资料，在这个特定场景是极大的减分项，二是它的结果显得过于权威、过于正式，太容易引发误导，让普通人以为看到的是高品质信息。

不知道用什么，大概是各种工具列表比较受欢迎的原因。

🔺 答疑的时候，我发现，拦住普通人的是不知道怎么提问。

答疑时我已经在从一个想要解决的问题开始，带着资料问了十多个问题，也就是一步一步地展示所有的过程。其中，也包括我发现当时的结果不行，然后换方式提问。但是，还有有人在问，能不能给模板？

在其他场景里也是一样。在一次公众直播中，明明已经在一点一点拆碎了讲我们这个场景怎么用，但还是有人说，节奏真慢，讲干货。是，我知道他要的是模板。

但当时没法满足他，因为我的基调是，市面上流传的各种模板，那种几百个的集合，是完全无用的。它们唯一的作用是满足收集癖。

真正有用的，是放在一个具体的场景里，按自己的需要，来提问用好 AI。我们都在场景里非常细致地说了，还在叫要模板，这样的人的认知能力是比较低的，当然，这是发生在面向大众的直播场景，什么样的人都可能出现。

但我也在反思，既然很多人处在这样的阶段，那还是要给人家想要的。这是因为，如果目的是真的希望有机会帮到他，那给他一个、满足他的需求，总比让他很困难地去理解超出他认知的更容易一些。

🔺 拦住很多人的，我觉得更麻烦的是，太多人没有场景感。

有很多人明明已经在想要用 AI 解决一个具体的问题，解决这个问题的方法，是在场景里把问题拆开来，变成一个个小问题。然后，带着场景、带着资料、用小问题去跟 AI 交互，在多轮交互中，拿到自己的结果。

但是，这些人被外界的那种 AI 万能论误导了，或者被他们内心的懒惰误导了，希望 AI 直接帮他们把活干了。

结果就是，明明他们在具体的场景里，但还是不知道这样带着场景感去用。当然，最后肯定有部分人能学会，但很遗憾，我已经觉得大多人学不会。这也没关系，我们本来就只需要跟那些想要学会的人打交道。

### 108

方军 2024/03/20

摘：自动化带来的巨大生产率提高（以及由此带来的就业变化），这是你看不到的，但现代世界却依赖于它。

从康威的《物质世界》

The massive productivity improvements from automation (and resulting changes in employment) that you don't see, but on which the modern world depends.

From Conway's "The Material World"

### 109

方军 2024/03/21

宝玉在提示语上贡献非常大：

如果你看过我以前写的[多步翻译 Prompt]，或者这个[总结的 Prompt]，你就会发现：

多步翻译：

baoyu.io/blog/prompt-engineering/translator-gpt-prompt-v2

总结摘要：

baoyu.io/blog/prompt-engineering/how-to-get-a-better-summary-result

> “所谓提示词工程，核心不是你套个什么模板用什么格式，而是逻辑！！”

**逻辑就是你怎么将一个复杂的任务拆成科学合理的步骤，并且让前面每一步的结果都成为后面步骤的基础，所有步骤合并在一起得到最终的结果，而不要指望一步得到结果。**只有这样你才能得到最佳的效果。

就像翻译，无论你的 Prompt 格式写的多完美，如果只有一步，那么效果上接近 DeepL 那样就是天花板了，但如何你拆分成直译、反思、意译，那么效果就接近人写的效果了！

就像总结，如果你只是让它总结，那么它可能就会偷懒遗漏很多要点，但如果你让它先提炼主题、再检查有没有遗漏的主题、然后基于每个主题列要点，最后再基于上面的去生成总结，就会好很多，也不会「偷懒」。

哪怕只是写一句「让我们一步步思考」，让它自己去分步骤，列出每一步的结果，都会好很多！

下次写 Prompt，效果不理想时，不妨想想看：我是不是可以把这个任务拆分成几个步骤？怎么拆分最合理？

顺便说一下：**即使你在 Prompt 里面指明了步骤，但是如果没有将步骤打印出来，那么也是没有效果的。**因为 LLM 需要基于前面的输出结果去预测后面的结果，如果没有前面步骤的输出，就无法影响后面的预测结果。

宝玉：

需要一个过程，另外无论是人还是 AI，都不会读心术，如果你不给出清晰的指令，那么他们就倾向于偷懒选择最简单的路径//@一脸无所谓：但 AI 技术不是应该消灭这种使用门槛吗…应该实现机器知我所想，不再是我想办法调用机器。。。

@宝玉xp：很多人都说 GPT-4 总结的时候会偷懒，Claude 更好，但如果你提示词得当，GPT-4 的效果其实一样可以做到很好，甚至比 Claude 效果还好。

比如以 Lex 采访 Sam 的视频文稿的后面 45 分钟，分别让 GPT-4 和 Claude 3 总结，GPT-4 的要点总结的更好，生成结果更完整。

关键在于要把摘要这个任务分成几个步骤：
1. 列出话题
2. 基于每个话题列出 bullet points 格式的要点
3. 基于上面的结构去展开生成

这是我用的 Prompt：

***
请用中文详尽总结以下对话内容，按照以下步骤，每一步分别打印结果：

1. 尽可能列出他们讨论的所有话题，不要遗漏
2. 检查第一步列出的话题，补充缺失的重要话题
3. 基于每个话题用 bullet points 列出要点
4. 严格的以话题为章节，不要遗漏，基于每个话题和下面的要点，用 1-3 个自然段落总结每个话题的内容，总结每个话题时不要用 bullet points，整体效果像是一篇科普文章
以下是要总结的内容：

<你要总结的内容>
***

### 110

方军 2024/03/21

土猛员外和 torchV 很赞

[聊聊 AI 应用创业心得，我们在路上 | 大模型应用落地场景 | RAG | SaaS | PaaS | GTM | TorchV](https://mp.weixin.qq.com/s/hFdwXX_MOd8vilrxTN95FQ)

### 111

方军 2024/03/21

摘：此次知乎在 AI 这块给我的最大印象有两个：一方面在用 AI 加强知乎本身的社区，另一方面我觉得未来它有可能会朝着 AI 搜索引擎 Perplexity 的方向发展，当然这还要看后续用户和市场的反馈。

[知乎会走向 AI 搜索引擎么？](https://mp.weixin.qq.com/s/65N5Fo_KO2p-kS7WZnqj7A)

### 112

方军 2024/03/21

艾瑞 AIGC 教育行业报告

### 113

方军 2024/03/21

不是很认同，语音还是语言的一个小子集

[超拟人语音合成上线，打造有温度的交互新体验](https://mp.weixin.qq.com/s/sPj7MzrRmUKaGxLJj-NAkg)

### 114

方军 2024/03/21

一个教育博主的分享：汪刀爸爸：AI 问答这么强了，我们为什么还要坚持阅读和写作？

关键观点：搜索引擎，让信息变得廉价；AI 问答，让观点变得廉价。有一样东西，依然是很贵的，那就是「吸收知识，为我所用」这件事。

和好友试用 AI 问答（Kimi），大家共同的意见是进步很大、水平很高。...AI 问答已经非常强了，而且会越来越强，那我们为什么还要坚持阅读和写作呢？

第一，用读写训练自己的肌肉和技术。

AI 是一柄锋利的宝剑，肌肉强大、技术精湛的剑客才能更好地挥舞它，而不为它所伤。比如，如果想要得到一个很好的回答，需要提问提到点子上。同时，AI 的回答面面俱到，通常缺乏重点，需要我们二次加工，把重点挑出来。

AI 的快速进步，很可能重塑很多脑力工作。AI 时代的孩子，仍然需要接受漫长的教育，去培养自身的能力。比如，孩子的探索精神、求知欲、好奇心、灵活的思维和丰富的想象，大人高度发达的审辩思维、强大的自我教育动机，这些能力都显得十分珍贵。阅读和写作，依然是教育的极重要手段。

第二，用读写思来促进知识和实践的结合。

搜索引擎，让信息变得廉价；AI 问答，让观点变得廉价。有一样东西，依然是很贵的，那就是「吸收知识，为我所用」这件事；是知道很多观点，如何过好生活。

网上的信息、书上的知识、AI 的观点，始终是身外之物，遇事用不起来，知与行是严重割裂的。而阅读思考、复盘实践、切己体察所得的思考，会像血肉一样，成为自己的一部分。为有持续读写思、持续事上磨炼，才能知行合一。

AI 时代，是算力的时代，更是人的时代。自我教育、自我修炼，在 AI 时代依然是最有价值的问题。

@爱予小桃子：AI 时代，培养孩子活在当下，切己体察的能力变得更必要了。AI 让知识变得廉价，但如何选择知识、利用知识过好这一生，是每个人的命题：这世界能给我无限多，但我真正想要什么？

---

不要迷恋干货。

很多人读书追求效率，喜欢干货。10 分钟听完一本书精华，都是脱水的干货；看别人读书理出来的思维导图、PPT、要点清单，收获满满，效率很高。

实际上，这种收获感常常是错觉。这些人会很惊讶地发现，这些干货记不住、用不上。只会稀里糊涂扯几个抽象名词，压根分析不清楚现实问题，更无法指导生活实践。

学一堆干货道理，却过不好这一生。

我读书的时候，特别喜欢搜集鲜活的案例。我很早就认识到，任何抽象概念或观点，如果找不到鲜活案例来说明，那说明自己并没有理解这个概念。

这些鲜活案例自带鲜明的场景，激发情绪，有行动细节，非常贴切地展示了概念。鲜活案例是成熟概念网里必备的要素。

比如，家庭教育强调「爱和关系」，这其实是非常抽象的。理论上论述得很漂亮，并不能代替 3-5 个不同场景下的鲜活例子。

干货不仅「没什么用」，而且很「不值钱」。

AI 问答输出「干货」的能力已经非常强了，扔给 AI 一本书，它几十秒就可以把里面的干货提炼出来。

获取干货几乎是免费的。AI 时代，可能是垃圾干货横行的时代。

这个时代，有用的东西还是鲜活案例，还是知行合一。

### 115

方军 2024/03/21

腾讯研究院现在发这样的文章真丢分，说是它的观点也不算，但发了毕竟表示认同，但这种文章有什么可认同或不认同的呢

口水

[加速分化：关于大模型走势的十个判断](https://mp.weixin.qq.com/s/b_iTN_UGBIcufBIcsrJnow?sessionid=1710850571&subscene=0&ascene=0&fasttmpl_type=0&fasttmpl_fullversion=7124241-zh_CN-zip&fasttmpl_flag=0&realreporttime=1710851977029&clicktime=1710851977&enterid=1710851977)

### 116

方军 2024/03/21

摘：今天英伟达的论坛黄仁勋把 Transformer 论文的七位作者请到了一起（共八位有一位没来）。Trasformer 是 openai 这些新一代人工智能的底层架构，可谓神级一般的存在，但这些作者是第一次聚到一起，而且都在各自喜欢的领域做一名默默钻研的人，没有到处走穴「作为生成式 ai 的教父，我是怎么创造出这么牛逼的东西的」这样的玩意。

整场对话的主题都是如何变得更好，各种探索未来的可能和方向，遗憾这个作品里的不足，遗憾为什么这些年没有后来者把这个模型颠覆，没有谈自己的创业项目，没有浮夸，没有自以为是，没有任何架子。在浮躁的当下，他们的执着、踏实和笃定深深感染了笨猫，就像那些热爱交易的人才明白面对市场的时候灵魂才是最为平静一样。

为你深爱的东西执着沉浸，世界是宁静平和喜悦，而不是浮华和躁动的。

### 117

方军 2024/03/21

这个最后那几个 AI 场景真不错

[独家功能炫酷登场 | 6 个 9.7 分好评的功能，让你轻松驾驭复杂工作流！](https://mp.weixin.qq.com/s/qtZtLms5K2IKEkFFgGAszg)

### 118

方军 2024/03/21

暂时还理解不了，不过真心要好好想想，这究竟行不行？

[用大语言模型控制交通信号灯，有效缓解拥堵！](https://mp.weixin.qq.com/s/7vncKmqhRw4TGTIY-pf7Gw)

### 119

方军 2024/03/21

066 AI 对真想用的人并不难

昨日跟一家公司负责培训的朋友讨论，因为想要理解他们公司，所以用他们公司指定的半公开行业标准与 Kimi 讨论，并把结果给他看了。

今天碰面时，发现他在培训会议上用 AI 为他的同事们准备了数十家公司的商业模式画布分析。严格来说并不好，但已经很不错了。我也就是顺带着说了几句，大意是啰嗦说如下（其实并无必要）：

- 别太信这种格式优美的，要看内容实质；

- AI 生成的内容要自己再精简过使用；

- 把资料给同事时，告知这个资料 AI 生成、仅供思路激发；

- 以及再重复前一日讨论过的话题，公司私有资料不要上网。（他们是技术型公司，他介绍也在开发适用于内部资料的对话机器人。）

这当然是非常厉害的人，之前简单给他介绍几句，他已经能够在现场很好的用我的介绍来组织一个框架讨论（用的另一个非常用专业框架）。同时，他的同事们也的确是高水平，30 分钟用框架将自己的主体业务进行精彩的分析。一问讲得最精彩的那位是该实质业务的总经理。厉害的人果然是跟厉害的人在一起。

我的感想是，对于真正想要提高工作效率的人来说，用 AI 这件事太简单了。他们有场景，有思路，有经验，工具到手立刻用起来，无需太多的介绍。因此，他们只要知道原来有如此工具，然后后面什么都不用说了。

对这样的人，只要有了信息、有了框架，加上他们自身的场景，用起来 AI 易如反掌。比方说，我今天观摩他们的讨论，他们可以很自然地用 AI 来辅助：

- 把经过整理的资料放进去

- 把框架在提示语进行解释

- 提出清晰的要求（如按框架输出、二次确认、质疑等）

AI 就变成他们强大的工作伙伴。

跟厉害的人打交道，果然如沐春风。

### 120

方军 2024/03/21

【# 香港科技大学允许学生用 GPT 考试 #】香港科技大学，是国内首个可以用 GPT 进行考试和论文的学校。首席副校长郭毅可并不反对学生使用 GPT 工具，他认为 ChatGPT 教学化真正的挑战不是学生，是老师，「老师怎么样出的题，是越过机器的能力」。他号召现在的老师巧妙地运用机器，来提升出题水准和教学质量。

### 121

方军 2024/03/21

Ethan Mollick: This remains one of the most consequential experiments in AI: Bloomberg trained a GPT-3.5 class AI on their own financial data last year…

…only to find that GPT-4 8k, without specialized finance training, beat it on almost all finance tasks.

Hard to beat the frontier models.

这仍然是人工智能领域最重要的实验之一：彭博去年在他们自己的金融数据上训练了一个 GPT-3.5 级别的人工智能。

… 但发现，没有专门的金融培训，GPT-4 8k 在几乎所有金融任务上都表现更好。

难以超越前沿模型。

There was a moment that we thought proprietary data would let organizations train specialized AIs that could compete with frontier models. It turns out that probably isn't going to happen. The largest frontier models are just much better at most complex tasks than smaller models.

有一个时刻，我们曾认为专有数据可以让组织训练出能与前沿模型竞争的专业人工智能。结果表明，这种情况可能不会发生。最大的前沿模型在大多数复杂任务上比较小的模型要好得多。

twitter.com/emollick/status/1770618237782307075

相关讨论：

twitter.com/proofofbeef/status/1770632815303917887

如果你仔细阅读彭博的报告，就会发现他们的结果并不理想。

### 122

方军 2024/03/22

真是很遗憾，不知道 SD 的热潮会不会继续下去

[突发！Stable Diffusion 核心团队被曝集体离职](https://mp.weixin.qq.com/s/4VZqW8OflBjvTxqTMzWgwA)

[可提前使用 Stable Diffusion 3，Stability AI 发布一站式开发服务](https://mp.weixin.qq.com/s/zh6OyPEaE_O29K_LZfKvMA)

讲 inflection

[百亿美金 AI 独角兽遭最大投资者微软生吞活剥，大模型独角兽或是大厂豢养的猎物？](https://mp.weixin.qq.com/s?__biz=MzAxOTU4MzE0Nw==&mid=2651891370&idx=1&sn=22b038a7c168093e40dff4cec85156cc&chksm=8020df6eb7575678f204c0380e3981c07790989d698def5b311e38b152da4776d72c85cffc8e&token=1119505291&lang=zh_CN#rd&v_p=90&WBAPIAnalysisOriUICodes=10000001&launchid=default&wm=3333_2001&aid=01A0VjFEC-8TX6msntzx_IZJsVSdIBL5NqzASu9SjrmTcS5LA.&from=10E3193010)

### 123

方军 2024/03/22

吴恩达老师建议大家关注 AI 智能体工作流，可以大幅提升 AI 应用的性能，并且他们总结了一套智能体设计模式（via 宝玉）：

- 反思：让大语言模型对自己的结果检查改进
- 使用工具：让大语言模型调用外部工具，如网络搜索、代码执行等
- 规划：让大语言模型自己设计一个多步骤的计划来达成目标
- 多智能体合作：多个 AI 智能体协同工作，分配任务，讨论和辩论想法，写作得到更好的结果

以下内容为推文转译：

今年 AI 智能体 (AI Agent) 的工作流程将会促进 AI 领域的巨大进步，这种影响可能会超越下一代的基础模型。这是一个重要的趋势，强烈建议所有 AI 领域的工作者关注这一点。

目前，我们通常在零样本 (Zero-shot) 模式下使用大语言模型 (LLM)，即让模型一步步地生成输出，而不进行任何修改。这好比要求一个人一气呵成地写完一篇文章，不允许回退修改，却期望文章能达到高质量。尽管这样做存在挑战，但大语言模型在此任务上的表现出乎意料地好！

通过采用智能体工作流，我们可以引导大语言模型对文档进行多轮迭代处理，仿佛它在多次精细打磨它的作品。具体操作可以包括：

- 制定文档大纲。
- 决定是否需要网络搜索以获取更多资料。
- 撰写初稿。
- 仔细审阅初稿，标记不合逻辑的论点或不必要的信息。
- 根据标记的问题对稿件进行修订。
- 以此类推。

这种迭代过程是大多数人类作者撰写优质文本不可或缺的步骤。对 AI 来说，采用这种迭代工作流显著优于单次过程的写作方式，能够产出更优质的成果。

最近，Devin 的一个引人注目的演示在社交媒体上引发了广泛讨论。我的团队一直在跟踪研究编写代码的 AI 的发展。我们分析了多个研究小组的成果，特别关注这些算法在广泛使用的 HumanEval 编程基准测试中的表现。以下图表展示了我们的一些发现。

在零样本 (Zero-shot) 模式下，GPT-3.5 的准确率为 48.1%。而 GPT-4 的表现更佳，达到了 67.0%。不过，从 GPT-3.5 到 GPT-4 的提升，并没有迭代智能体工作流带来的提升那么显著。实际上，当 GPT-3.5 应用在一个迭代智能体循环中时，它的表现可以提高到惊人的 95.1%。

开源的智能体工具和有关智能体的学术论文日益增多，这让我们既感到兴奋又觉得困惑。为了更好地理解这一工作，我想介绍一个用于分类构建智能体设计模式的框架。我的团队 AI Fund 已经在多个应用场景中成功应用了这些模式，我希望它们对你也有所帮助。

- 反思：大语言模型会审视自己的作品，并寻找改进的方法。
- 使用工具：为大语言模型提供各种工具，如网络搜索、代码执行等，帮助其收集信息、执行操作或处理数据。
- 规划：大语言模型能够设计并执行一个多步骤的计划来达成目标（比如，先为一篇文章制定大纲，接着进行在线研究，然后撰写草稿等）。
- 多智能体合作：多个 AI 智能体协同工作，分配任务，讨论和辩论想法，共同寻找比单独工作时更好的解决方案。

下周，我将进一步深入这些设计模式，并为每种模式提供推荐阅读材料。

来源：twitter.com/AndrewYNg/status/1770897666702233815

### 124

方军 2024/03/22

不知道是 kimi 在做内容营销，还是它的产品力带来大量的自动介绍，总之这段时间 kimi 出镜率好高

将实话，文心大模型的效果不错的，但是，文心一言的运营真的可改进的太多了

前几日参加它的微信视频号直播，发现他们在努力地通过直播卖数百元的季卡，真心用错力量，即便这是其中一个小团队的 KPI，但实在没必要这样做尝试。

想起去年那几天大家都想拥有一个文心测试账号的状态，百度真是把很好的资源做到较差的程度。

当然两者还是不能直接比，百度在大众用户里面的接受度太高了。

月之暗面：3 月 20 日以来 Kimi 流量增加趋势远超预期导致 SaaS 客户体验异常

财联社 3 月 21 日电，月之暗面发布情况说明：从 2024.3.209:30:00 开始，我们观测到 Kimi 的系统流量持续异常增高，流量增加的趋势远超我

们对资源的预期规划。这导致了从 2024.3.2010:00:00 开始，有较多的 SaaS 客户持续的体验到 429:engine is overloaded 的异常问题，对此我们深表抱歉。做为一家以技术为导向的公司，我们非常理解一个 API 的稳定性是能否投入实际生产的最关键因素之一，已经有多项应急措施正在实施，包括不限于：从 3.20 观测到流量异常增高后，已经进行了 5 次扩容工作。推理资源会持续配合流量进行扩容，以尽量承载持续增长的用户量；设计了一套更有效的 SaaS 流量优先级策略，以保障付费用户的调用稳定，预计 3.25 之前完成并上线。

### 125

方军 2024/03/22

某：我在想，现在已经完成信息化与自动化的制造企业，渴望 Ai 来升级智能智造，也有这方面的「科普」需求………

比如：管物料的部门经理问「Ai 物料管家」：哪些型号原材料近期需要补充，哪些客户的原材料还没配齐？… 等等，可以及时得到 Ai 的回答，而这些大模型该如何在制造企业实现，而其背后的原理是什么？

我：您说的这个应该是很简单的，LLM + 接口调用

也许再加一点点规则和推理

案例：

[【人工智能深度案例】从智能工厂到 ChatBI，雅戈尔的「智能 +」实践](https://mp.weixin.qq.com/s/HQDiQ3Wx_j-bzQ1u8-hgkw)

### 126

方军 2024/03/22

Google Gemini 1.5 向所有人开放了

[Untitled prompt | Google AI Studio](https://aistudio.google.com/app/prompts/new_chat)

### 127

方军 2024/03/22

差距真大

美团：Q4 营收 737 亿元，净利 22.2 亿元

鞭牛士 2024-03-22 16:33

鞭牛士 3 月 22 日消息，美团发布 2023 年第四季度及全年财报，第四季度营收 736.96 亿元，上年同期营收 601.3 亿元，市场预期 726.96 亿元；第四季度净利润 22.2 亿元人民币，市场预期 10.9 亿元人民币。

2023 年全年营收 2767.4 亿元，上年同期 2199.55 亿元；2023 全年净利润 138.6 亿元人民币，市场预期 128.4 亿元人民币。

拼多多

财报显示，2023 年 Q4 营收 888.8 亿元，市场预期 798.74 亿元，同比增长 123%。

经营利润为 223.95 亿元，同比增长 146%；经调整净利润为 254.77 亿元，同比增长 110%。调整后每 ADS 收益为 17.32 元，去年同期为 8.34 元。

在线营销服务和其他收入为 486.76 亿元，同比增长 57%；交易服务收入为 402.05 亿元，同比增长 357%。

2023 年全年，总营收为 2476.392 亿元，同比增长 90%。归属于拼多多普通股股东的净利润为 600.265 亿元，同比增长 90%。不按美国通用会计准则，归属于拼多多普通股股东的净利润为 678.993 亿元，同比增长 72%。

[Temu 狂飙，没有秘诀](https://mp.weixin.qq.com/s/bXHHRW38EmkEeKpEGNa75g)

### 128

方军 2024/03/22

RAG 要被大厂搞没了。

---

[阿里通义千问升级：免费开放 1000 万字长文档处理功能]

《科创板日报》22 日讯，阿里通义千问今日升级，向所有人免费开放 1000 万字的长文档处理功能。即日起，可通过通义千问网站和 APP 快速读研报、分析财报、读科研论文、研判案情、读医疗报告、解读法律条文、分析考试成绩、总结深度文章。（记者黄心怡）

### 129

方军 2024/03/22

[微软新作「Mora」，复原了 Sora](https://mp.weixin.qq.com/s/G08_a5gkzjTIAt8MoprmMA)

### 130

方军 2024/03/22

目前看 gpt store 的确没有到合适推出的时机

[GPT Store 被放弃？OpenAI 或成烂尾专业户](https://mp.weixin.qq.com/s/7emzVjwVGpbJedenBhzmaw)

### 131

方军 2024/03/23

摘：【平凡中的非凡：AI 工具的日常应用启示录】

作者分享了他使用 Claude 和 ChatGPT 完成临时任务的案例。他认为，这个案例最值得注意的地方在于它完全不值得注意，他每天都能从这些工具中获得类似的结果。  

点评：（这几段明显是AI生成的）

- 作者的观点颇具反直觉性，他认为这个案例之所以值得关注，恰恰是因为它已经变得司空见惯，这种看似平凡的成功背后，反映出 AI 工具已经达到了一个新的高度。  

- 作者对 AI 工具的信心和依赖，启发我们思考这些工具在日常工作中的应用价值和可靠性，它们正在悄然改变我们的工作方式。  

- 作者的态度虽然轻描淡写，但背后透露出一种对 AI 工具的高度认可和信任，这种态度值得我们反思：我们是否也已经对 AI 的能力习以为常了? 

[Claude and ChatGPT for ad-hoc sidequests](https://simonwillison.net/2024/Mar/22/claude-and-chatgpt-case-study/)

### 132

方军 2024/03/23

kimi 最近遇到点事，其实就是什么能回答什么不能回答呗，哎，社交媒体上一些人又开始表演，TMD，你们这些人根本不配用这么好的产品，脑残用什么 AI，去医院要紧。

### 133

方军 2024/03/23

[​前谷歌大佬离职创业，不到一年造出 GPT3.5 和 Gemini Pro，惨痛忠告：GPU 简直菜鸡，就像是买彩票！](https://mp.weixin.qq.com/s/ojrGxBOAQdgK5xsK69k9-A)

### 134

方军 2024/03/23

很赞同这个，瞎搞，上下文窗口和能处理多长文本，混起来了：

文心一言说开放 500 万字的长文本能力，通义千问说开放 1000 万，这下把 OpenAI 和谷歌整蒙了。

其实，本质上是在偷换概念，并没有什么实质性的突破。国内的公司风气不好，偷换概念，重新定义概念，把大家都搞乱，目的就是营销宣传，本质上，还是在专注于卖，而不是做事研究。

Kimi 这技术，我们去年就已经做到了，这种营销，真敢闭着眼睛瞎吹牛，有点丢做技术的人的脸了。

### 135

方军 2024/03/23

YouTube 新规：上传视频需标注是否为 AI 制作，包括合成配音/换脸等 3 月 19 日，YouTube 宣布，即日起任何人在上传、发布视频时，都需要标注「篡改或合成」的逼真内容，包括生成式 AI。

YouTube 将「逼真内容」定义为「任何观众容易误认为是真实的人事物或地点」的内容。若视频创作者使用真人声音的合成版本来为视频配音，或发布「AI 换脸」主题的视频，就需要附上标签。

此举的本质目的是防止 AI 生成内容可能导致的虚假信息传播，而非反对创作者通过 AI 制作内容。这项新规旨在防止用户受到蒙蔽，即误认为合成视频为真实素材，这是因为新的生成式 AI 工具已经让视频内容的真伪区别愈发困难。此次发布的新规，也是 YouTube 继去年 11 月公布的大规模 AI 应用政策的延续与最新条款。

### 136

方军 2024/03/23

《澎湃：2024 年人工智能公众态度调查报告》

面对 AI ，很多人处于「短期积极、中期期待、长期担忧」的状态。这种拧巴态度，或许将贯穿我们发展人工智能的整个过程，即随着机器学习系统越来越普遍和强大，人类越来越发现，自己处于一种「魔法师学徒」的境地。

2『一下子原文「20240322澎湃：2024 年人工智能公众态度调查报告」。（2024-03-24）』

### 137

方军 2024/03/23

[LLMOps 框架 Dify 发布 Workflow 功能，RAG 进入自由编排时代（附产品负责人分享 PPT ）](https://mp.weixin.qq.com/s/s6PgoRrCVdbSnfeVXAjtGw)

### 138

方军 2024/03/24

牛啊

使用大语言模型把二进制代码恢复到普通代码（俗称「破解」）

LLM4Decompile: Decompiling Binary Code with Large Language Models

Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang

Southern University of Science and Technology

The Hong Kong Polytechnic University

### 139

方军 2024/03/24




### 140

方军 2024/03/24




### 141

方军 2024/03/24




### 142

方军 2024/03/24




### 143

方军 2024/03/24




### 144

方军 2024/03/24




### 145

方军 2024/03/24




### 146

方军 2024/03/24




### 147

方军 2024/03/22




### 148

方军 2024/03/22




