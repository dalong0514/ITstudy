[assertEquals vs. assertEqual in python](https://stackoverflow.com/questions/930995/assertequals-vs-assertequal-in-python)

Is there a difference between assertEquals and assertEqual in the python unittest.TestCase?And if there is not, why are there two functions? Only for convenience? 

2009-05-31 01:07:03Z

Is there a difference between assertEquals and assertEqual in the python unittest.TestCase?And if there is not, why are there two functions? Only for convenience? Good question!Actually, in Python 2.6, both assertEqual and assertEquals are convenience aliases to failUnlessEqual. The source declares them thus: In Python 3, to your point, failUnlessEqual is explicitly deprecated. assertEquals carries this comment :-)So, the upshot appears to be that you should use whatever you like for Python 2.x, but tend toward assertEqual for Python 3.A 3.3 update: From 26.3.7.1.1. Deprecated aliases :Not just for Python 3.x, since Python 2.7 assertEquals has been deprecated as well:From 25.3.7.1.1. Deprecated aliasesI think this was tension between the "only one obvious way to do it" vs. "alias to make the overall code flow semantically".  Personally I found I like to readover but likedover the other two (assertEquals(a, b) bothers my sense of grammar).The "only one obvious way to do it" has taken precedence going forward.I don't find any mention of assertEquals in http://docs.python.org/library/unittest.html.  However, when I import TestCase and then do a "help(TestCase)", it's listed.  I think it's just a synonym for convenience.I know it doesn't answer the specific question, but if you got here while searching for:You just need to change the call to .assertEqual() (remove the 's' in equalS)It's almost the same except that assertEquals is repricated. It's recommended to use assertEqual as in here :Online source : https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Lib/lib2to3/fixes/fix_asserts.py

How to make an immutable object in Python?

Lennart Regebro

[How to make an immutable object in Python?](https://stackoverflow.com/questions/4828080/how-to-make-an-immutable-object-in-python)

Although I have never needed this, it just struck me that making an immutable object in Python could be slightly tricky. You can't just override __setattr__, because then you can't even set attributes in the __init__. Subclassing a tuple is a trick that works:But then you have access to the a and b variables through self[0] and self[1], which is annoying.Is this possible in Pure Python? If not, how would I do it with a C extension?(Answers that work only in Python 3 are acceptable).Update: So subclassing tuple is the way to do it in Pure Python, which works well except for the additional possibility of accessing the data by [0], [1] etc. So, to complete this question all that is missing is howto do it "properly" in C, which I suspect would be quite simple, by just not implementing any geititem or setattribute, etc. But instead of doing it myself, I offer a bounty for that, because I'm lazy. :)

2011-01-28 12:14:06Z

Although I have never needed this, it just struck me that making an immutable object in Python could be slightly tricky. You can't just override __setattr__, because then you can't even set attributes in the __init__. Subclassing a tuple is a trick that works:But then you have access to the a and b variables through self[0] and self[1], which is annoying.Is this possible in Pure Python? If not, how would I do it with a C extension?(Answers that work only in Python 3 are acceptable).Update: So subclassing tuple is the way to do it in Pure Python, which works well except for the additional possibility of accessing the data by [0], [1] etc. So, to complete this question all that is missing is howto do it "properly" in C, which I suspect would be quite simple, by just not implementing any geititem or setattribute, etc. But instead of doing it myself, I offer a bounty for that, because I'm lazy. :)Yet another solution I just thought of:  The simplest way to get the same behaviour as your original code isIt does not solve the problem that attributes can be accessed via [0] etc., but at least it's considerably shorter and provides the additional advantage of being compatible with pickle and copy.namedtuple creates a type similar to what I described in this answer, i.e. derived from tuple and using __slots__.  It is available in Python 2.6 or above.The easiest way to do this is using __slots__:Instances of A are immutable now, since you can't set any attributes on them.If you want the class instances to contain data, you can combine this with deriving from tuple:Edit: If you want to get rid of indexing either, you can override __getitem__():Note that you can't use operator.itemgetter for the properties in thise case, since this would rely on Point.__getitem__() instead of tuple.__getitem__().  Fuerthermore this won't prevent the use of tuple.__getitem__(p, 0), but I can hardly imagine how this should constitute a problem.I don't think the "right" way of creating an immutable object is writing a C extension.  Python usually relies on library implementers and library users being consenting adults, and instead of really enforcing an interface, the interface should be clearly stated in the documentation.  This is why I don't consider the possibility of circumventing an overridden __setattr__() by calling object.__setattr__() a problem.  If someone does this, it's on her own risk.You could use Cython to create an extension type for Python:It works both Python 2.x and 3.If you don't mind indexing support then collections.namedtuple suggested by @Sven Marnach is preferrable:Another idea would be to completely disallow __setattr__ and use object.__setattr__ in the constructor:Of course you could use object.__setattr__(p, "x", 3) to modify a Point instance p, but your original implementation suffers from the same problem (try tuple.__setattr__(i, "x", 42) on an Immutable instance).You can apply the same trick in your original implementation: get rid of __getitem__(), and use tuple.__getitem__() in your property functions.You could create a @immutable decorator that either overrides the __setattr__ and change the __slots__ to an empty list, then decorate the __init__ method with it.Edit: As the OP noted, changing the __slots__ attribute only prevents the creation of new attributes, not the modification.Edit2: Here's an implementation:Edit3: Using __slots__ breaks this code, because if stops the creation of the object's __dict__. I'm looking for an alternative.Edit4: Well, that's it. It's a but hackish, but works as an exercise :-)I don't think it is entirely possible except by using either a tuple or a namedtuple. No matter what, if you override __setattr__() the user can always bypass it by calling object.__setattr__() directly. Any solution that depends on __setattr__ is guaranteed not to work.The following is about the nearest you can get without using some sort of tuple:but it breaks if you try hard enough:but Sven's use of namedtuple is genuinely immutable.UpdateSince the question has been updated to ask how to do it properly in C, here's my answer on how to do it properly in Cython:First immutable.pyx:and a setup.py to compile it (using the command setup.py build_ext --inplace:Then to try it out:For Python 3.7+ you can use a Data Class with a frozen=True option, which is a very pythonic and maintainable way to do what you want. It would look something like that: As type hinting is required for dataclasses' fields, I have used Any from the typing module.  Before Python 3.7 it was frequent to see namedtuples being used as immutable objects. It can be tricky in many ways, one of them is that the __eq__ method between namedtuples does not consider the objects' classes. For example:As you see, even if the types of obj1 and obj2 are different, even if their fields' names are different, obj1 == obj2 still gives True. That's because the __eq__ method used is the tuple's one, which compares only the values of the fields given their positions. That can be a huge source of errors, specially if you are subclassing these classes.In addition to the excellent other answers I like to add a method for python 3.4 (or maybe 3.3). This answer builds upon several previouse answers to this question.In python 3.4, you can use properties without setters to create class members that cannot be modified. (In earlier versions assigning to properties without a setter was possible.)You can use it like this:which will print "constant"But calling instance.a=10 will cause:Explaination: properties without setters are a very recent feature of python 3.4 (and I think 3.3). If you try to assign to such a property, an Error will be raised.

Using slots I restrict the membervariables to __A_a (which is __a).Problem: Assigning to _A__a is still possible (instance._A__a=2). But if you assign to a private variable, it is your own fault...This answer among others, however, discourages the use of __slots__. Using other ways to prevent attribute creation might be preferrable.Here's an elegant solution:Inherit from this class, initialize your fields in the constructor, and you'e all set.I've made immutable classes by overriding __setattr__, and allowing the set if the caller is __init__:This isn't quite enough yet, since it allows anyone's ___init__ to change the object, but you get the idea.If you are interested in objects with behavior, then namedtuple is almost your solution.  As described at the bottom of the namedtuple documentation, you can derive your own class from namedtuple; and then, you can add the behavior you want.For example (code taken directly from the documentation):This will result in:This approach works for both Python 3 and Python 2.7 (tested on IronPython as well).

The only downside is that the inheritance tree is a bit weird; but this is not something you usually play with.I needed this a little while ago and decided to make a Python package for it. The initial version is on PyPI now:To use:Full docs here: https://github.com/theengineear/immutableHope it helps, it wraps a namedtuple as has been discussed, but makes instantiation much simpler.This way doesn't stop object.__setattr__ from working, but I've still found it useful:you may need to override more stuff (like __setitem__) depending on the use case.Classes which inherit from the following Immutable class are immutable, as are their instances, after their __init__ method finishes executing.  Since it's pure python, as others have pointed out, there's nothing stopping someone from using the mutating special methods from the base object and type, but this is enough to stop anyone from mutating a class/instance by accident.It works by hijacking the class-creation process with a metaclass.As of Python 3.7, you can use the @dataclass decorator in your class and it will be immutable like a struct! Though, it may or may not add a __hash__() method to your class. Quote:Here the example from the docs linked above:      You can override setattr and still use init to set the variable. You would use super class setattr. here is the code.The third party attr module provides this functionality.Edit: python 3.7 has adopted this idea into the stdlib with @dataclass.attr implements frozen classes by overriding __setattr__ and has a minor performance impact at each instantiation time, according to the documentation.If you're in the habit of using classes as datatypes, attr may be especially useful as it takes care of the boilerplate for you (but doesn't do any magic). In particular, it writes nine dunder (__X__) methods for you (unless you turn any of them off), including repr, init, hash and all the comparison functions.attr also provides a helper for __slots__.So, I am writing respective of python 3:I) with the help of data class decorator and set frozen=True.

we can create immutable objects in python.for this need to import data class from data classes lib and needs to set frozen=Trueex.from dataclasses import dataclasso/p:Source: https://realpython.com/python-data-classes/An alternative approach is to create a wrapper which makes an instance immutable.This is useful in situations where only some instances have to be immutable (like default arguments of function calls).Can also be used in immutable factories like:Also protects from object.__setattr__, but fallable to other tricks due to Python's dynamic nature.I used the same idea as Alex: a meta-class and an "init marker", but in combination with over-writing __setattr__:Note: I'm calling the meta-class directly to make it work both for Python 2.x and 3.x.It does work also with slots ...:... and multiple inheritance:Note, however, that mutable attributes stay to be mutable:One thing that's not really included here is total immutability... not just the parent object, but all the children as well. tuples/frozensets may be immutable for instance, but the objects that it's part of may not be. Here's a small (incomplete) version that does a decent job of enforcing immutability all the way down:You can just override setAttr in the final statement of init. THen you can construct but not change. Obviously you can still override by usint object.setAttr but in practice most languages have some form of reflection so immutablility is always a leaky abstraction. Immutability is more about preventing clients from accidentally violating the contract of an object. I use:=============================The original solution offered was incorrect, this was updated based on the comments using the solution from hereThe original solution is wrong in an interesting way, so it is included at the bottom.===============================Output :======================================Original Implementation:It was pointed out in the comments, correctly, that this does not in fact work, as it prevents the creation of more than one object as you are overriding the class setattr method, which means a second cannot be created as self.a = will fail on the second initialisation.The basic solution below addresses the following scenario:The idea is to override __setattr__ method and replace its implementation each time the object frozen status is changed.So we need some method (_freeze) which stores those two implementations and switches between them when requested.This mechanism may be implemented inside the user class or inherited from a special Freezer class as shown below:I have an open source library where I'm doing things in a functional way so moving data around in an immutable object is helpful. However, I don't want to have to transform my data object for the client to interact with them. So, I came up with this - it gives you a dict like object thats immutable + some helper methods.Credit to Sven Marnach in his answer for the basic implementation of restricting property updating and deleting.

how do I insert a column at a specific column index in pandas?

HappyPy

[how do I insert a column at a specific column index in pandas?](https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas)

Can I insert a column at a specific column index in pandas? This will put column n as the last column of df, but isn't there a way to tell df to put n at the beginning?

2013-09-07 13:59:01Z

Can I insert a column at a specific column index in pandas? This will put column n as the last column of df, but isn't there a way to tell df to put n at the beginning?see docs: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.insert.htmlusing loc = 0 will insert at the beginningYou could try to extract columns as list, massage this as you want, and reindex your dataframe:EDIT: this can be done in one line ; however, this looks a bit ugly. Maybe some cleaner proposal may come...If you want a single value for all rows:Edit:You can also:

Python csv string to array

Drew LeSueur

[Python csv string to array](https://stackoverflow.com/questions/3305926/python-csv-string-to-array)

Anyone know of a simple library or function to parse a csv encoded string and turn it into an array or dictionary?I don't think I want the built in csv module because in all the examples I've seen that takes filepaths, not strings.

2010-07-22 05:08:48Z

Anyone know of a simple library or function to parse a csv encoded string and turn it into an array or dictionary?I don't think I want the built in csv module because in all the examples I've seen that takes filepaths, not strings.You can convert a string to a file object using io.StringIO and then pass that to the csv module:simpler version with split() on newlines:Or you can simply split() this string into lines using \n as separator, and then split() each line into values, but this way you must be aware of quoting, so using csv module is preferred.On Python 2 you have to import StringIO asinstead.Simple - the csv module works with lists, too:The official doc for csv.reader() https://docs.python.org/2/library/csv.html  is very helpful, which says To parse a CSV file:As others have already pointed out, Python includes a module to read and write CSV files. It works pretty well as long as the input characters stay within ASCII limits. In case you want to process other encodings, more work is needed.The Python documentation for the csv module implements an extension of csv.reader, which uses the same interface but can handle other encodings and returns unicode strings. Just copy and paste the code from the documentation. After that, you can process a CSV file like this:Per the documentation:Just turn your string into a single element list.Importing StringIO seems a bit excessive to me when this example is explicitly in the docs.https://docs.python.org/2/library/csv.html?highlight=csv#csv.readerThus, a StringIO.StringIO(), str.splitlines() or even a generator are all good.Here's an alternative solution:Here's the documentationUse this to have a csv loaded into a listPanda is quite powerful and smart library reading CSV in PythonA simple example here, I have example.zip file with four files in it.Once you have data you can manipulate to play with a list or other formats.

Creating Threads in python

chrissygormley

[Creating Threads in python](https://stackoverflow.com/questions/2905965/creating-threads-in-python)

I have a script and I want one function to run at the same time as the other.The example code I have looked at:I am having trouble getting this working. I would prefer to get this going using a threaded function rather than a class.This is the working script:

2010-05-25 15:15:01Z

I have a script and I want one function to run at the same time as the other.The example code I have looked at:I am having trouble getting this working. I would prefer to get this going using a threaded function rather than a class.This is the working script:You don't need to use a subclass of Thread to make this work - take a look at the simple example I'm posting below to see how:Here I show how to use the threading module to create a thread which invokes a normal function as its target.  You can see how I can pass whatever arguments I need to it in the thread constructor.There are a few problems with your code:If you really want to do this with only functions, you have two options:With threading:With thread:Doc for thread.start_new_threadI tried to add another join(), and it seems worked. Here is codeYou can use the target argument in the Thread constructor to directly pass in a function that gets called instead of run.Did you override the run() method?  If you overrided __init__, did you make sure to call the base threading.Thread.__init__()?After starting the two threads, does the main thread continue to do work indefinitely/block/join on the child threads so that main thread execution does not end before the child threads complete their tasks?And finally, are you getting any unhandled exceptions?Python 3 has the facility of Launching parallel tasks. This makes our work easier. It has for thread pooling and Process pooling. The following gives an insight:ThreadPoolExecutor ExampleAnother Example

Logging uncaught exceptions in Python

Jacob Marble

[Logging uncaught exceptions in Python](https://stackoverflow.com/questions/6234405/logging-uncaught-exceptions-in-python)

How do you cause uncaught exceptions to output via the logging module rather than to stderr?I realize the best way to do this would be:But my situation is such that it would be really nice if logging.exception(...) were invoked automatically whenever an exception isn't caught.

2011-06-04 02:53:22Z

How do you cause uncaught exceptions to output via the logging module rather than to stderr?I realize the best way to do this would be:But my situation is such that it would be really nice if logging.exception(...) were invoked automatically whenever an exception isn't caught.As Ned pointed out, sys.excepthook is invoked every time an exception is raised and uncaught.  The practical implication of this is that in your code you can override the default behavior of sys.excepthook to do whatever you want (including using logging.exception).As a straw man example:Override sys.excepthook:Commit obvious syntax error (leave out the colon) and get back custom error information:For more information about sys.excepthook: http://docs.python.org/library/sys.html#sys.excepthookHere's a complete small example that also includes a few other tricks:The method sys.excepthook will be invoked if an exception is uncaught: http://docs.python.org/library/sys.html#sys.excepthookWhy not:Here is the output with sys.excepthook as seen above:Here is the output with the sys.excepthook commented out:The only difference is that the former has ERROR:root:Unhandled exception: at the beginning of the first line.To build on Jacinda's answer, but using a logger object:Wrap your app entry call in a try...except block so you'll be able to catch and log (and perhaps re-raise) all uncaught exceptions. E.g. instead of:Do this:Maybe you could do something at the top of a module that redirects stderr to a file, and then logg that file at the bottomAlthough @gnu_lorien's answer gave me good starting point, my program crashes on first exception.I came with a customised (and/or) improved solution, which silently logs Exceptions of functions that are decorated with @handle_error.To answer the question from Mr.Zeus discussed in the comment section of the accepted answer, I use this to log uncaught exceptions in an interactive console (tested with PyCharm 2018-2019). I found out sys.excepthook does not work in a python shell so I looked deeper and found that I could use sys.exc_info instead. However, sys.exc_info takes no arguments unlike sys.excepthook that takes 3 arguments. Here, I use both sys.excepthook and sys.exc_info to log both exceptions in an interactive console and a script with a wrapper function. To attach a hook function to both functions, I have two different interfaces depending if arguments are given or not.Here's the code:The logging setup can be found in gnu_lorien's answer.In my case (using python 3) when using @Jacinda 's answer the content of the traceback was not printed. Instead, it just prints the object itself: <traceback object at 0x7f90299b7b90>.  Instead I do:

How to step through Python code to help debug issues? [closed]

Blankman

[How to step through Python code to help debug issues? [closed]](https://stackoverflow.com/questions/4929251/how-to-step-through-python-code-to-help-debug-issues)

In Java/C# you can easily step through code to trace what might be going wrong, and IDE's make this process very user friendly.Can you trace through python code in a similar fashion?

2011-02-08 03:21:42Z

In Java/C# you can easily step through code to trace what might be going wrong, and IDE's make this process very user friendly.Can you trace through python code in a similar fashion?Yes! There's a Python debugger called pdb just for doing that!You can launch a Python program through pdb by using pdb myscript.py or python -m pdb myscript.py.There are a few commands you can then issue, which are documented on the pdb page.Some useful ones to remember are:If you don't want to use a command line debugger, some IDEs like Pydev, Wing IDE or PyCharm have a GUI debugger. Wing and PyCharm are commercial products, but Wing has a free "Personal" edition, and PyCharm has a free community edition.First step is to make the Python interpreter to enter into the debugging mode.A. From the Command LineMost straight forward way, running from command line, of python interpreterB. Within the InterpreterWhile developing early versions of modules and to experiment it more iteratively.C. From Within Your ProgramFor a big project and long-running module, can start the debugging from inside the program using 

import pdb and set_trace()

like this :Step-by-Step debugging to go into more internalNote:**All these commands should be execute from **pdbFor in-depth knowledge, refer:-https://pymotw.com/2/pdb/https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/There is a module called 'pdb' in python. At the top of your python script you doand you will enter into debugging mode. You can use 's' to step, 'n' to follow next line similar to what you would do with 'gdb' debugger.Starting in Python 3.7, you can use the breakpoint() built-in function to enter the debugger: By default, breakpoint() will import pdb and call pdb.set_trace(). However, you can control debugging behavior via sys.breakpointhook() and use of the environment variable PYTHONBREAKPOINT.See PEP 553 for more information.ipdb adds IPython functionality to pdb, offering the following HUGE improvements:Much like pdg, ipdb is still far from perfect and completely rudimentary if compared to GDB, but it is already a huge improvement over pdb.Usage is analogous to pdb, just install it with:and then add to the line you want to step debug from:You likely want to add a shortcut for that from your editor, e.g. for Vim snipmate I have:so I can type just ipd<tab> and it expands to the breakpoint. Then removing it is easy with dd since everything is contained in a single line.context=21 increases the number of context lines as explained at: How can I make ipdb show more lines of context while debugging?Alternatively, you can also debug programs from the start with:but you generally don't want to do that because:Or alternatively, as in raw pdb 3.2+ you can set some breakpoints from the command line:although -c c is broken for some reason: https://github.com/gotcha/ipdb/issues/156python -m module debugging has been asked at: How to debug a Python module run with python -m from the command line? and since Python 3.7 can be done with:Serious missing features of both pdb and ipdb compared to GDB:ipdb specific annoyances:Tested in Ubuntu 16.04, ipdb==0.11, Python 3.5.2.There exist breakpoint() method nowadays, which replaces import pdb; pdb.set_trace().It also has several new features, such as possible environment variables.If you come from Java/C# background I guess your best bet would be to use Eclipse with Pydev. This gives you a fully functional IDE with debugger built in. I use it with django as well.If you want an IDE with integrated debugger, try PyScripter.https://wiki.python.org/moin/PythonDebuggingToolspudb is a good drop-in replacement for pdbProgrammatically stepping and tracing through python code is possible too (and its easy!). Look at the sys.settrace() documentation for more details. Also here is a tutorial to get you started.Visual Studio with PTVS could be an option for you: http://www.hanselman.com/blog/OneOfMicrosoftsBestKeptSecretsPythonToolsForVisualStudioPTVS.aspxPyCharm is an IDE for Python that includes a debugger. Watch this YouTube video for an introduction on using PyCharm's debugger to step through code.PyCharm Tutorial - Debug python code using PyCharmNote: This is not intended to be an endorsement or review. PyCharm is a commercial product that one needs to pay for, but the company does provide a free license to students and teachers, as well as a "lightweight" Community version that is free and open-source. Python Tutor is an online single-step debugger. You can put in code on the edit page then click "Visualize Execution" to start it running.

How to loop backwards in python? [duplicate]

snakile

[How to loop backwards in python? [duplicate]](https://stackoverflow.com/questions/3476732/how-to-loop-backwards-in-python)

I'm talking about doing something like:I can think of some ways to do so in python (creating a list of range(1,n+1) and reverse it, using while and --i, ...) but I wondered if there's a more elegant way to do it. Is there?EDIT:

Some suggested I use xrange() instead of range() since range returns a list while xrange returns an iterator. But in Python 3 (which I happen to use) range() returns an iterator and xrange doesn't exist.

2010-08-13 12:25:08Z

I'm talking about doing something like:I can think of some ways to do so in python (creating a list of range(1,n+1) and reverse it, using while and --i, ...) but I wondered if there's a more elegant way to do it. Is there?EDIT:

Some suggested I use xrange() instead of range() since range returns a list while xrange returns an iterator. But in Python 3 (which I happen to use) range() returns an iterator and xrange doesn't exist.range() and xrange() take a third parameter that specifies a step. So you can do the following.Which givesBut for iteration, you should really be using xrange instead. So,This works on basically everything that has a defined order, including xrange objects and lists.All of these three solutions give the same results if the input is a string:1.2.3.To reverse a string without using reversed or [::-1], try something like:

Create a .csv file with values from a Python list

Fortilan

[Create a .csv file with values from a Python list](https://stackoverflow.com/questions/2084069/create-a-csv-file-with-values-from-a-python-list)

I am trying to create a .csv file with the values from a Python list. When I print the values in the list they are all unicode (?), i.e. they look something like this If I iterate through the values in the list i.e. for v in mylist: print v they appear to be plain text.And I can put a , between each with print ','.join(mylist)And I can output to a file, i.e. But I want to output to a CSV and have delimiters around the values in the list e.g.I can't find an easy way to include the delimiters in the formatting, e.g. I have tried through the join statement. How can I do this?

2010-01-18 05:34:08Z

I am trying to create a .csv file with the values from a Python list. When I print the values in the list they are all unicode (?), i.e. they look something like this If I iterate through the values in the list i.e. for v in mylist: print v they appear to be plain text.And I can put a , between each with print ','.join(mylist)And I can output to a file, i.e. But I want to output to a CSV and have delimiters around the values in the list e.g.I can't find an easy way to include the delimiters in the formatting, e.g. I have tried through the join statement. How can I do this?Edit: this only works with python 2.x.To make it work with python 3.x replace wb with w (see this SO answer)Here is a secure version of Alex Martelli's:For another approach, you can use DataFrame in pandas:

And it can easily dump the data to csv just like the code below:The best option I've found was using the savetxt from the numpy module:In case you have multiple lists that need to be stackedUse python's csv module for reading and writing comma or tab-delimited files. The csv module is preferred because it gives you good control over quoting.For example, here is the worked example for you:Produces:You could use the string.join method in this case.Split over a few of lines for clarity - here's an interactive sessionOr as a single lineHowever, you may have a problem is your strings have got embedded quotes. If this is the case you'll need to decide how to escape them. The CSV module can take care of all of this for you, allowing you to choose between various quoting options (all fields, only fields with quotes and seperators, only non numeric fields, etc) and how to esacpe control charecters (double quotes, or escaped strings). If your values are simple, string.join will probably be OK but if you're having to manage lots of edge cases, use the module available.To create and write into a csv fileThe below example demonstrate creating and writing a csv file.

to make a dynamic file writer we need to import a package import csv, then need to create an instance of the file with file reference

Ex:- with open("D:\sample.csv","w",newline="") as file_writerhere if the file does not exist with the mentioned file directory then python will create a same file in the specified directory, and "w" represents write, if you want to read a file then replace "w" with "r" or to append to existing file then "a". newline="" specifies that it removes an extra empty row for every time you create row so to eliminate empty row we use newline="", create some field names(column names) using list like fields=["Names","Age","Class"], then apply to writer instance like

writer=csv.DictWriter(file_writer,fieldnames=fields)

here using Dictionary writer and assigning column names, to write column names to csv we use writer.writeheader() and to write values we use writer.writerow({"Names":"John","Age":20,"Class":"12A"}) ,while writing file values must be passed using dictionary method , here the key is column name and value is your respective key valueLets say that your list is AThen you can code the following ad you will have it as a csv file (columns only!)This solutions sounds crazy, but works smooth as honeyThe file is being written by csvwriter hence csv properties are maintained i.e. comma separated. 

The delimiter helps in the main part by moving list items to next line, each time.you should use the CSV module for sure , but the chances are , you need to write unicode . For those Who need to write unicode , this is the class from example page , that you can use as a util module: Here is another solution that does not require the csv module.Example :However, if the initial list contains some ", they will not be escaped. If it is required, it is possible to call a function to escape it like that :

How can I display an image from a file in Jupyter Notebook?

zach

[How can I display an image from a file in Jupyter Notebook?](https://stackoverflow.com/questions/11854847/how-can-i-display-an-image-from-a-file-in-jupyter-notebook)

I would like to use an IPython notebook as a way to interactively analyze some genome charts I am making with Biopython's GenomeDiagram module. While there is extensive documentation on how to use matplotlib to get graphs inline in IPython notebook, GenomeDiagram uses the ReportLab toolkit which I don't think is supported for inline graphing in IPython. I was thinking, however, that a way around this would be to write out the plot/genome diagram to a file and then open the image inline which would have the same result with something like this:However, I can't figure out how to do this - or know if it's possible. So does anyone know if images can be opened/displayed in IPython?

2012-08-07 22:11:04Z

I would like to use an IPython notebook as a way to interactively analyze some genome charts I am making with Biopython's GenomeDiagram module. While there is extensive documentation on how to use matplotlib to get graphs inline in IPython notebook, GenomeDiagram uses the ReportLab toolkit which I don't think is supported for inline graphing in IPython. I was thinking, however, that a way around this would be to write out the plot/genome diagram to a file and then open the image inline which would have the same result with something like this:However, I can't figure out how to do this - or know if it's possible. So does anyone know if images can be opened/displayed in IPython?Courtesy of this post, you can do the following:(official docs)If you are trying to display an Image in this way inside a loop, then you need to wrap the Image constructor in a display method.Note, until now posted solutions only work for png and jpg!If you want it even easier without importing further libraries or you want to display an animated or not animated GIF File in your Ipython Notebook. Transform the line where you want to display it to markdown and use this nice short hack!This will import and display a .jpg image in Jupyter (tested with Python 2.7 in Anaconda environment)in Anaconda this is done by typingCourtesy of this page, I found this worked when the suggestions above didn't:A cleaner Python3 version that use standard numpy, matplotlib and PIL. Merging the answer for opening from URL.When using GenomeDiagram with Jupyter (iPython), the easiest way to display images is by converting the GenomeDiagram to a PNG image. This can be wrapped using an IPython.display.Image object to make it display in the notebook.[See Notebook]

Formatting floats in Python without trailing zeros

TarGz

[Formatting floats in Python without trailing zeros](https://stackoverflow.com/questions/2440692/formatting-floats-in-python-without-trailing-zeros)

How can I format a float so that it doesn't contain trailing zeros? In other words, I want the resulting string to be as short as possible.For example:

2010-03-14 00:27:39Z

How can I format a float so that it doesn't contain trailing zeros? In other words, I want the resulting string to be as short as possible.For example:Me, I'd do ('%f' % x).rstrip('0').rstrip('.') -- guarantees fixed-point formatting rather than scientific notation, etc etc.  Yeah, not as slick and elegant as %g, but, it works (and I don't know how to force %g to never use scientific notation;-).You could use %g to achieve this:or, for Python 2.6 or better:From the docs for format: g causes (among other things)What about trying the easiest and probably most effective approach?

The method normalize() removes all the rightmost trailing zeros.Works in Python 2 and Python 3.-- Updated --The only problem as @BobStein-VisiBone pointed out, is that numbers like 10, 100, 1000... will be displayed in exponential representation. This can be easily fixed using the following function instead:After looking over answers to several similar questions, this seems to be the best solution for me:My reasoning:%g doesn't get rid of scientific notation.15 decimal places seems to avoid strange behavior and has plenty of precision for my needs.I could have used format(inputValue, '.15f'). instead of '%.15f' % inputValue, but that is a bit slower (~30%).I could have used Decimal(inputValue).normalize(), but this has a few issues as well. For one, it is A LOT slower (~11x). I also found that although it has pretty great precision, it still suffers from precision loss when using normalize().Most importantly, I would still be converting to Decimal from a float which can make you end up with something other than the number you put in there. I think Decimal works best when the arithmetic stays in Decimal and the Decimal is initialized with a string.I'm sure the precision issue of Decimal.normalize() can be adjusted to what is needed using context settings, but considering the already slow speed and not needing ridiculous precision and the fact that I'd still be converting from a float and losing precision anyway, I didn't think it was worth pursuing.I'm not concerned with the possible "-0" result since -0.0 is a valid floating point number and it would probably be a rare occurrence anyway, but since you did mention you want to keep the string result as short as possible, you could always use an extra conditional at very little extra speed cost.Here's a solution that worked for me. It's a blend of the solution by PolyMesh and use of the new .format() syntax.Output:You can simply use format() to achieve this:format(3.140, '.10g') where 10 is the precision you want.While formatting is likely that most Pythonic way, here is an alternate solution using the more_itertools.rstrip tool.The number is converted to a string, which is stripped of trailing characters that satisfy a predicate.  The function definition fmt is not required, but it is used here to test assertions, which all pass. Note: it works on string inputs and accepts optional predicates.See also details on this third-party library, more_itertools.If you can live with 3. and 3.0 appearing as "3.0", a very simple approach that right-strips zeros from float representations:(thanks @ellimilial for pointing out the exceptions)Using the QuantiPhy package is an option. Normally QuantiPhy is used when 

working with numbers with units and SI scale factors, but it has a variety of 

nice number formatting options.And it will not use e-notation in this situation:An alternative you might prefer is to use SI scale factors, perhaps with units.OP would like to remove superflouous zeros and make the resulting string as short as possible.I find the %g exponential formatting shortens the resulting string for very large and very small values. The problem comes for values that don't need exponential notation, like 128.0, which is neither very large or very small.Here is one way to format numbers as short strings that uses %g exponential notation only when Decimal.normalize creates strings that are too long. This might not be the fastest solution (since it does use Decimal.normalize) For float you could use this:Test it:For Decimal see solution here: https://stackoverflow.com/a/42668598/5917543Use %g with big enough width, for example '%.99g'.

It will print in fixed-point notation for any reasonably big number.EDIT: it doesn't workYou can use max() like this:print(max(int(x), x))You can achieve that in most pythonic way like that:python3:Handling %f and you should put , where:

.2f == .00 floats.Example:print "Price: %.2f" % prices[product]Price: 1.50

Listing contents of a bucket with boto3

Amelio Vazquez-Reina

[Listing contents of a bucket with boto3](https://stackoverflow.com/questions/30249069/listing-contents-of-a-bucket-with-boto3)

How can I see what's inside a bucket in S3 with boto3? (i.e. do an "ls")?Doing the following:returns:How do I see its contents?

2015-05-14 23:22:55Z

How can I see what's inside a bucket in S3 with boto3? (i.e. do an "ls")?Doing the following:returns:How do I see its contents?One way to see the contents would be:This is similar to an 'ls' but it does not take into account the prefix folder convention and will list the objects in the bucket.  It's left up to the reader to filter out prefixes which are part of the Key name. In Python 2:In Python 3:I'm assuming you have configured authentication separately. If you want to pass the ACCESS and SECRET keys (which you should not do, because it is not secure):In order to handle large key listings (i.e. when the directory list is greater than 1000 items), I used the following code to accumulate key values (i.e. filenames) with multiple listings (thanks to Amelio above for the first lines).  Code is for python3:My s3 keys utility function is essentially an optimized version of @Hephaestus's answer:In my tests (boto3 1.9.84), it's significantly faster than the equivalent (but simpler) code:As S3 guarantees UTF-8 binary sorted results, a start_after optimization has been added to the first function.A more parsimonious way, rather than iterating through via a for loop you could also just print the original object containing all files inside your S3 bucket:ObjectSummary:There are two identifiers that are attached to the ObjectSummary:boto3 S3: ObjectSummaryMore on Object Keys from AWS S3 Documentation:Here is some example code that demonstrates how to get the bucket name and the object key.Example:I just did it like this, including the authentication method:With little modification to @Hephaeastus 's code in one of the above comments, wrote the below method to list down folders and objects (files) in a given path. Works similar to s3 ls command. This lists down all objects / folders in a given path. Folder_path can be left as None by default and method will list the immediate contents of the root of the bucket.

Python Flask, how to set content type

Tampa

[Python Flask, how to set content type](https://stackoverflow.com/questions/11773348/python-flask-how-to-set-content-type)

I am using Flask and I return an XML file from a get request. How do I set the content type to xml ?e.g.

2012-08-02 08:03:16Z

I am using Flask and I return an XML file from a get request. How do I set the content type to xml ?e.g.Try like this:The actual Content-Type is based on the mimetype parameter and the charset (defaults to UTF-8).Response (and request) objects are documented here: http://werkzeug.pocoo.org/docs/wrappers/As simple as thisHope it helpsUpdate:

Use this method because it will work with both python 2.x and python 3.xand secondly it also eliminates multiple header problem.I like and upvoted @Simon Sapin's answer.  I ended up taking a slightly different tack, however, and created my own decorator:and use it thus:I think this is slightly more comfortable.Use the make_response method to get a response with your data. Then set the mimetype attribute. Finally return this response:If you use Response directly, you lose the chance to customize the responses by setting app.response_class. The make_response method uses the app.responses_class to make the response object. In this you can create your own class, add make your application uses it globally:Usually you don’t have to create the Response object yourself because make_response() will take care of that for you.One more thing, it seems that no one mentioned the after_this_request, I want to say something:after_this_requestso we can do it with after_this_request, the code should look like this:You can try the following method(python3.6.2)：case one：case two：I am using Flask .And if you want to  return json,you can write this:

How can I convert JSON to CSV?

little_fish

[How can I convert JSON to CSV?](https://stackoverflow.com/questions/1871524/how-can-i-convert-json-to-csv)

I have a JSON file I want to convert to a CSV file. How can I do this with Python?I tried:However, it did not work. I am using Django and the error I received is:I then tried the following:I then get the error:Sample json file:

2009-12-09 04:06:37Z

I have a JSON file I want to convert to a CSV file. How can I do this with Python?I tried:However, it did not work. I am using Django and the error I received is:I then tried the following:I then get the error:Sample json file:First, your JSON has nested objects, so it normally cannot be directly converted to CSV. You need to change that to something like this:Here is my code to generate CSV from that:You will get output as:With the pandas library, this is as easy as using two commands!To convert a JSON string to a  pandas object (either a series or dataframe). Then, assuming the results were stored as df:Which can either return a string or write directly to a csv-file.Based on the verbosity of previous answers, we should all thank pandas for the shortcut.I am assuming that your JSON file will decode into a list of dictionaries. First we need a function which will flatten the JSON objects: The result of running this snippet on your JSON object:isAfter applying this function to each dict in the input array of JSON objects:and finding the relevant column names:it's not hard to run this through the csv module:I hope this helps!JSON can represent a wide variety of data structures -- a JS "object" is roughly like a Python dict (with string keys), a JS "array" roughly like a Python list, and you can nest them as long as the final "leaf" elements are numbers or strings.CSV can essentially represent only a 2-D table -- optionally with a first row of "headers", i.e., "column names", which can make the table interpretable as a list of dicts, instead of the normal interpretation, a list of lists (again, "leaf" elements can be numbers or strings).So, in the general case, you can't translate an arbitrary JSON structure to a CSV.  In a few special cases you can (array of arrays with no further nesting; arrays of objects which all have exactly the same keys).  Which special case, if any, applies to your problem?  The details of the solution depend on which special case you do have.  Given the astonishing fact that you don't even mention which one applies, I suspect you may not have considered the constraint, neither usable case in fact applies, and your problem is impossible to solve. But please do clarify!A generic solution which translates any json list of flat objects to csv.Pass the input.json file as first argument on command line.This code should work for you, assuming that your JSON data is in a file called data.json.It'll be easy to use csv.DictWriter(),the detailed implementation can be like this:Note that this assumes that all of your JSON objects have the same fields. Here is the reference which may help you.I was having trouble with Dan's proposed solution, but this worked for me:Where "test.json" contained the following:As mentioned in the previous answers the difficulty in converting json to csv is because a json file can contain nested dictionaries and therefore be a multidimensional data structure verses a csv which is a 2D data structure. However, a good way to turn a multidimensional structure to a csv is to have multiple csvs that tie together with primary keys.In your example, the first csv output has the columns "pk","model","fields" as your columns. Values for "pk", and "model" are easy to get but because the "fields" column contains a dictionary, it should be its own csv and because "codename" appears to the be the primary key, you can use as the input for "fields" to complete the first csv. The second csv contains the dictionary from the "fields" column with codename as the the primary key that can be used to tie the 2 csvs together.Here is a solution for your json file which converts a nested dictionaries to 2 csvs.I know it has been a long time since this question has been asked but I thought I might add to everyone else's answer and share a blog post that I think explain the solution in a very concise way.Here is the linkAlec's answer is great, but it doesn't work in the case where there are multiple levels of nesting. Here's a modified version that supports multiple levels of nesting. It also makes the header names a bit nicer if the nested object already specifies its own key (e.g. Firebase Analytics / BigTable / BigQuery data):This works relatively well.

It flattens the json to write it to a csv file.

Nested elements are managed :)That's for python 3enjoy.My simple way to solve this:Create a new Python file like: json_to_csv.pyAdd this code: After add this code, save the file and run at the terminal:I hope this help you.SEEYA!It is not a very smart way to do it, but I have had the same problem and this worked for me:Surprisingly, I found that none of the answers posted here so far correctly deal with all possible scenarios (e.g., nested dicts, nested lists, None values, etc).This solution should work across all scenarios:Try thisModified Alec McGail's answer to support JSON with lists insideThanks!This code works for any given json fileSince the data appears to be in a dictionary format, it would appear that you should actually use csv.DictWriter() to actually output the lines with the appropriate header information. This should allow the conversion to be handled somewhat easier. The fieldnames parameter would then set up the order properly while the output of the first line as the headers would allow it to be read and processed later by csv.DictReader().For example, Mike Repass usedHowever just change the initial setup to

    output = csv.DictWriter(filesetting, fieldnames=data[0].keys())Note that since the order of elements in a dictionary is not defined, you might have to create fieldnames entries explicitly. Once you do that, the writerow will work. The writes then work as originally shown.Unfortunately I have not enouthg reputation to make a small contribution to the amazing @Alec McGail answer.

I was using Python3 and I have needed to convert the map to a list following the  @Alexis R comment. Additionaly I have found the csv writer was adding a extra CR to the file (I have a empty line for each line with data inside the csv file). The solution was very easy following the @Jason R. Coombs answer to this thread:

CSV in Python adding an extra carriage returnYou need to simply add the lineterminator='\n' parameter to the csv.writer. It will be: csv_w = csv.writer( out_file, lineterminator='\n' )You can use this code to convert a json file to csv file

After reading the file, I am converting the object to pandas dataframe and then saving this to a CSV fileI might be late to the party, but I think, I have dealt with the similar problem. I had a json file which looked like thisI only wanted to extract few keys/values from these json file. So, I wrote the following code to extract the same.I hope this will help. For details on how this code work you can check hereIf we consider the below example for converting the json format file to csv formatted file.The below code will convert the json file ( data3.json ) to csv file ( data3.csv ).The above mentioned code has been executed in the locally installed pycharm and it has successfully converted the json file to the csv file. Hope this help to convert the files.

How do I mock an open used in a with statement (using the Mock framework in Python)?

Daryl Spitzer

[How do I mock an open used in a with statement (using the Mock framework in Python)?](https://stackoverflow.com/questions/1289894/how-do-i-mock-an-open-used-in-a-with-statement-using-the-mock-framework-in-pyth)

How do I test the following code with mocks (using mocks, the patch decorator and sentinels provided by Michael Foord's Mock framework):

2009-08-17 19:26:50Z

How do I test the following code with mocks (using mocks, the patch decorator and sentinels provided by Michael Foord's Mock framework):The way to do this has changed in mock 0.7.0 which finally supports mocking the python protocol methods (magic methods), particularly using the MagicMock:http://www.voidspace.org.uk/python/mock/magicmock.htmlAn example of mocking open as a context manager (from the examples page in the mock documentation):mock_open is part of mock framework and is very simple to use. patch used as context returns the object used to replace the patched one: you can use it to make your test simpler.Use builtins instead of __builtin__.mock is not part of unittest and you should patch __builtin__If you would use patch as decorator using mock_open()'s result as the new patch's argument can be a little bit weird. In this case is better to use the new_callable patch's argument and remember that every extra arguments that patch doesn't use will be passed to new_callable function as described in patch documentation.For instance decorated version for Python 3.x is:Remember that in this case patch will add the mock object as argument of you test function.With the latest versions of mock, you can use the really useful mock_open helper:To use mock_open for a simple file read() (the original mock_open snippet already given on this page is geared more for write):Note as per docs for mock_open, this is specifically for read(), so won't work with common patterns like for line in f, for example.Uses python 2.6.6 / mock 1.0.1I might be a bit late to the game, but this worked for me when calling open in another module without having to create a new file.test.pyMyObj.pyBy patching the open function inside the __builtin__ module to my mock_open(), I can mock writing to a file without creating one.Note: If you are using a module that uses cython, or your program depends on cython in any way, you will need to import cython's __builtin__ module by including import __builtin__ at the top of your file. You will not be able to mock the universal __builtin__ if you are using cython.The top answer is useful but I expanded on it a bit.If you want to set the value of your file object (the f in as f) based on the arguments passed to open() here's one way to do it:Basically, open() will return an object and with will call __enter__() on that object.To mock properly, we must mock open() to return a mock object. That mock object should then mock the __enter__() call on it (MagicMock will do this for us) to return the mock data/file object we want (hence mm.__enter__.return_value). Doing this with 2 mocks the way above allows us to capture the arguments passed to open() and pass them to our do_something_with_data method.I passed an entire mock file as a string to open() and my do_something_with_data looked like this:This transforms the string into a list so you can do the following as you would with a normal file:This worked for a patch to read a json config. The mocked object is the io.TextIOWrapper object returned by the open() function

How do I print the key-value pairs of a dictionary in python

oaklander114

[How do I print the key-value pairs of a dictionary in python](https://stackoverflow.com/questions/26660654/how-do-i-print-the-key-value-pairs-of-a-dictionary-in-python)

I want to output my key value pairs from a python dictionary as such:I thought I could maybe do it like this:but obviously that's not how it goes as the keys() and values() don't take an argument.

2014-10-30 18:27:21Z

I want to output my key value pairs from a python dictionary as such:I thought I could maybe do it like this:but obviously that's not how it goes as the keys() and values() don't take an argument.Your existing code just needs a little tweak. i is the key, so you would just need to use it:You can also get an iterator that contains both keys and values. In Python 2, d.items() returns a list of (key, value) tuples, while d.iteritems() returns an iterator that provides the same:In Python 3, d.items() returns the iterator; to get a list, you need to pass the iterator to list() yourself.A little intro to dictionaryPrint keys,values method oneAnother methodYou can get keys using iterYou can get value of key of dictionary using get(key, [value]):If key is not present in dictionary,when default value given, will return value.Or, for Python 3:The dictionary:Another one line solution:Output: (but, since no one has suggested something like this before, I suspect it is not good practice)You can access your keys and/or values by calling items() on your dictionary.If you want to sort the output by dict key you can use the collection package.It works on python 3In addition to ways already mentioned..  can use 'viewitems', 'viewkeys', 'viewvalues'Or or using itemgetterA simple dictionary:To print a specific (key, value) pair in Python 3 (pair at index 1 in this example):Output:Here is a one liner solution to print all pairs in a tuple:Output:

Calculate difference in keys contained in two Python dictionaries

hughdbrown

[Calculate difference in keys contained in two Python dictionaries](https://stackoverflow.com/questions/1165352/calculate-difference-in-keys-contained-in-two-python-dictionaries)

Suppose I have two Python dictionaries - dictA and dictB. I need to find out if there are any keys which are present in dictB but not in dictA. What is the fastest way to go about it?Should I convert the dictionary keys into a set and then go about?Interested in knowing your thoughts... Thanks for your responses.Apologies for not stating my question properly.

My scenario is like this - I have a dictA which can be the same as dictB or may have some keys missing as compared to dictB or else the value of some keys might be different which has to be set to that of dictA key's value.Problem is the dictionary has no standard and can have values which can be dict of dict.SaySo 'key2' value has to be reset to the new value and 'key13' has to be added inside the dict.

The key value does not have a fixed format. It can be a simple value or a dict or a dict of dict.

2009-07-22 13:43:13Z

Suppose I have two Python dictionaries - dictA and dictB. I need to find out if there are any keys which are present in dictB but not in dictA. What is the fastest way to go about it?Should I convert the dictionary keys into a set and then go about?Interested in knowing your thoughts... Thanks for your responses.Apologies for not stating my question properly.

My scenario is like this - I have a dictA which can be the same as dictB or may have some keys missing as compared to dictB or else the value of some keys might be different which has to be set to that of dictA key's value.Problem is the dictionary has no standard and can have values which can be dict of dict.SaySo 'key2' value has to be reset to the new value and 'key13' has to be added inside the dict.

The key value does not have a fixed format. It can be a simple value or a dict or a dict of dict.You can use set operations on the keys:Here is a class to find all the possibilities: what was added, what was removed, which key-value pairs are the same, and which key-value pairs are changed.Here is some sample output:Available as a github repo:

https://github.com/hughdbrown/dictdifferIn case you want the difference recursively, I have written a package for python:

https://github.com/seperman/deepdiffInstall from PyPi:ImportingSame object returns emptyType of an item has changedValue of an item has changedItem added and/or removedString differenceString difference 2Type changeList differenceList difference 2:List difference ignoring order or duplicates: (with the same dictionaries as above)List that contains dictionary:Sets:Named Tuples:Custom objects:Object attribute added:not sure whether its "fast" or not, but normally, one can do thisAs Alex Martelli wrote, if you simply want to check if any key in B is not in A, any(True  for k in dictB if k not in dictA) would be the way to go.To find the keys that are missing:So those two solutions are pretty much the same speed. If you really mean exactly what you say (that you only need to find out IF "there are any keys" in B and not in A, not WHICH ONES might those be if any), the fastest way should be:If you actually need to find out WHICH KEYS, if any, are in B and not in A, and not just "IF" there are such keys, then existing answers are quite appropriate (but I do suggest more precision in future questions if that's indeed what you mean;-).Use set():The top answer by hughdbrown suggests using set difference, which is definitely the best approach:The problem with this code is that it builds two lists just to create two sets, so it's wasting 4N time and 2N space. It's also a bit more complicated than it needs to be.Usually, this is not a big deal, but if it is:In Python 2, keys() returns a list of the keys, not a KeysView. So you have to ask for viewkeys() directly.For dual-version 2.7/3.x code, you're hopefully using six or something similar, so you can use six.viewkeys(dictb):In 2.4-2.6, there is no KeysView. But you can at least cut the cost from 4N to N by building your left set directly out of an iterator, instead of building a list first:So you really don't need to compare the keys, but the items. An ItemsView is only a Set if the values are hashable, like strings. If they are, it's easy:Although the question isn't directly asking for a recursive diff, some of the example values are dicts, and it appears the expected output does recursively diff them. There are already multiple answers here showing how to do that.There is an other question in stackoverflow about this argument and i have to admit that there is a simple solution explained: the datadiff library of python helps printing the difference between two dictionaries. Here's a way that will work, allows for keys that evaluate to False, and still uses a generator expression to fall out early if possible. It's not exceptionally pretty though.EDIT:THC4k posted a reply to my comment on another answer.  Here's a better, prettier way to do the above:Not sure how that never crossed my mind...This is an old question and asks a little bit less than what I needed so this answer actually solves more than this question asks. The answers in this question helped me solve the following:All this combined with JSON makes for a pretty powerful configuration storage support.The solution (also on github):what about standart (compare FULL Object)PyDev->new PyDev Module->Module: unittestIf on Python ≥ 2.7:Here is a solution for deep comparing 2 dictionaries keys:here's a solution that can compare more than two dicts:usage example:My recipe of symmetric difference between two dictionaries:And result is:As mentioned in other answers, unittest produces some nice output for comparing dicts, but in this example we don't want to have to build a whole test first.Scraping the unittest source, it looks like you can get a fair solution with just this:soResults in:Where:Like in unittest, the only caveat is that the final mapping can be thought to be a diff, due to the trailing comma/bracket.@Maxx has an excellent answer, use the unittest tools provided by Python:Then, anywhere in your code you can call:The resulting output looks like the output from diff, pretty-printing the dictionaries with + or - prepending each line that is different.Not sure if it is still relevant but I came across this problem, my situation i just needed to return a dictionary of the changes for all nested dictionaries etc etc. Could not find a good solution out there but I did end up writing a simple function to do this. Hope this helps, If you want a built-in solution for a full comparison with arbitrary dict structures, @Maxx's answer is a good start.Based on ghostdog74's answer,will print differ value of dictaTry this to find de intersection, the keys that is in both dictionarie, if you want the keys not found on second dictionarie, just use the not in...

How to check if variable is string with python 2 and 3 compatibility

Randall Hunt

[How to check if variable is string with python 2 and 3 compatibility](https://stackoverflow.com/questions/11301138/how-to-check-if-variable-is-string-with-python-2-and-3-compatibility)

I'm aware that I can use: isinstance(x, str) in python-3.x but I need to check if something is a string in python-2.x as well. Will isinstance(x, str) work as expected in python-2.x? Or will I need to check the version and use isinstance(x, basestr)?Specifically, in python-2.x:and python-3.x does not have u"foo"

2012-07-02 21:03:29Z

I'm aware that I can use: isinstance(x, str) in python-3.x but I need to check if something is a string in python-2.x as well. Will isinstance(x, str) work as expected in python-2.x? Or will I need to check the version and use isinstance(x, basestr)?Specifically, in python-2.x:and python-3.x does not have u"foo"If you're writing 2.x-and-3.x-compatible code, you'll probably want to use six:The most terse approach I've found without relying on packages like six, is:then, assuming you've been checking for strings in Python 2 in the most generic manner,will now also work for Python 3+.What about this, works in all cases?This is @Lev Levitsky's answer, re-written a bit.The try/except test is done once, and then defines a function that always works and is as fast as possible.EDIT: Actually, we don't even need to call isinstance(); we just need to evaluate basestring and see if we get a NameError:I think it is easier to follow with the call to isinstance(), though.The future library adds (to Python 2) compatible names, so you can continue writing Python 3. You can simple do the following:To install it, just execute pip install future.As a caveat, it only support python>=2.6,>=3.3, but it is more modern than six, which is only recommended if using python 2.5Maybe use a workaround likeYou can get the class of an object by calling object.__class__, so in order to check if object is the default string type:And You can place the following in the top of Your code so that strings enclosed by quotes are in unicode in python 2:You can try this at the beginning of your code:and later in the code:Be careful! In python 2, str and bytes are essentially the same. This can cause a bug if you are trying to distinguish between the two. type(string) == strreturns true if its a string, and false if not

Add x and y labels to a pandas plot

Everaldo Aguiar

[Add x and y labels to a pandas plot](https://stackoverflow.com/questions/21487329/add-x-and-y-labels-to-a-pandas-plot)

Suppose I have the following code that plots something very simple using pandas:How do I easily set x and y-labels while preserving my ability to use specific colormaps? I noticed that the plot() wrapper for pandas DataFrames doesn't take any parameters specific for that.

2014-01-31 18:23:09Z

Suppose I have the following code that plots something very simple using pandas:How do I easily set x and y-labels while preserving my ability to use specific colormaps? I noticed that the plot() wrapper for pandas DataFrames doesn't take any parameters specific for that.The df.plot() function returns a matplotlib.axes.AxesSubplot object. You can set the labels on that object.Or, more succinctly: ax.set(xlabel="x label", ylabel="y label").Alternatively, the index x-axis label is automatically set to the Index name, if it has one. so df2.index.name = 'x label' would work too.You can use do it like this:Obviously you have to replace the strings 'xlabel' and 'ylabel' with what you want them to be.If you label the columns and index of your DataFrame, pandas will automatically supply appropriate labels:In this case, you'll still need to supply y-labels manually (e.g., via plt.ylabel as shown in the other answers).It is possible to set both labels together with axis.set function. Look for the example:For cases where you use pandas.DataFrame.hist:Note that you get an ARRAY of plots, rather than a plot.  Thus to set the x label you will need to do something like thiswhat about ...pandas uses matplotlib for basic dataframe plots. So, if you are using pandas for basic plot you can use matplotlib for plot customization. However, I propose an alternative method here using seaborn which allows more customization of the plot while not going into the basic level of matplotlib. Working Code:

How do I check the difference, in seconds, between two dates?

Alex

[How do I check the difference, in seconds, between two dates?](https://stackoverflow.com/questions/4362491/how-do-i-check-the-difference-in-seconds-between-two-dates)

There has to be an easier way to do this. I have objects that want to be refreshed every so often, so I want to record when they were created, check against the current timestamp, and refresh as necessary. datetime.datetime has proven to be difficult, and I don't want to dive into the ctime library. Is there anything easier for this sort of thing? 

2010-12-06 01:28:43Z

There has to be an easier way to do this. I have objects that want to be refreshed every so often, so I want to record when they were created, check against the current timestamp, and refresh as necessary. datetime.datetime has proven to be difficult, and I don't want to dive into the ctime library. Is there anything easier for this sort of thing? if you want to compute differences between two known dates, use total_seconds like this:86400.00would that work for you?(7 will be whatever amount of time you waited a bit above)I find datetime.datetime to be fairly useful, so if there's a complicated or awkward scenario that you've encountered, please let us know.EDIT: Thanks to @WoLpH for pointing out that one is not always necessarily looking to refresh so frequently that the datetimes will be close together. By accounting for the days in the delta, you can handle longer timestamp discrepancies:We have function total_seconds() with Python 2.7

Please see below code for python 2.6Here's the one that is working for me.Hope this helps!

How to print a percentage value in python?

zjm1126

[How to print a percentage value in python?](https://stackoverflow.com/questions/5306756/how-to-print-a-percentage-value-in-python)

this is my code:and it shows:but I want to get 33%What can I do?

2011-03-15 02:10:44Z

this is my code:and it shows:but I want to get 33%What can I do?format supports a percentage floating point precision type:If you don't want integer division, you can import Python3's division from __future__:There is a way more convenient 'percent'-formatting option for the .format() format method:Just for the sake of completeness, since I noticed no one suggested this simple approach:Details:You are dividing integers then converting to float. Divide by floats instead.As a bonus, use the awesome string formatting methods described here: http://docs.python.org/library/string.html#format-specification-mini-languageTo specify a percent conversion and precision.A great gem!Just to add Python 3 f-string solutionThen you'd want to do this instead:The .0 denotes them as floats and int() rounds them to integers afterwards again.

Reading and writing environment variables in Python? [duplicate]

user749632

[Reading and writing environment variables in Python? [duplicate]](https://stackoverflow.com/questions/5971635/reading-and-writing-environment-variables-in-python)

My python script which calls many python functions and shell scripts. I want to set a environment variable in Python (main calling function) and all the daughter processes including the shell scripts to see the environmental variable set.I need to set some environmental variables like this:1 is a number, not a string. Additionally, how can I read the value stored in an environment variable? (Like DEBUSSY/FSDB in another python child script.)

2011-05-11 23:16:45Z

My python script which calls many python functions and shell scripts. I want to set a environment variable in Python (main calling function) and all the daughter processes including the shell scripts to see the environmental variable set.I need to set some environmental variables like this:1 is a number, not a string. Additionally, how can I read the value stored in an environment variable? (Like DEBUSSY/FSDB in another python child script.)Try using the os module.See the Python docs on os.environ. Also, for spawning child processes, see Python's subprocess docs.First things first :) reading books is an excellent approach to problem solving; it's the difference between band-aid fixes and long-term investments in solving problems. Never miss an opportunity to learn. :DYou might choose to interpret the 1 as a number, but environment variables don't care. They just pass around strings:(From environ(3posix).)You access environment variables in python using the os.environ dictionary-like object:If you want to pass global variables into new scripts, you can create a python file that is only meant for holding global variables (e.g. globals.py). When you import this file at the top of the child script, it should have access to all of those variables.If you are writing to these variables, then that is a different story. That involves concurrency and locking the variables, which I'm not going to get into unless you want.Use os.environ[str(DEBUSSY)] for both reading and writing (http://docs.python.org/library/os.html#os.environ).As for reading, you have to parse the number from the string yourself of course.

random.seed(): What does it do?

Ahaan S. Rungta

[random.seed(): What does it do?](https://stackoverflow.com/questions/22639587/random-seed-what-does-it-do)

I am a bit confused on what random.seed() does in Python. For example, why does the below trials do what they do (consistently)? I couldn't find good documentation on this.

2014-03-25 15:46:46Z

I am a bit confused on what random.seed() does in Python. For example, why does the below trials do what they do (consistently)? I couldn't find good documentation on this.Pseudo-random number generators work by performing some operation on a value. Generally this value is the previous number generated by the generator. However, the first time you use the generator, there is no previous value.Seeding a pseudo-random number generator gives it its first "previous" value. Each seed value will correspond to a sequence of generated values for a given random number generator. That is, if you provide the same seed twice, you get the same sequence of numbers twice.Generally, you want to seed your random number generator with some value that will change each execution of the program. For instance, the current time is a frequently-used seed. The reason why this doesn't happen automatically is so that if you want, you can provide a specific seed to get a known sequence of numbers.All the other answers don't seem to explain the use of random.seed(). 

Here is a simple example (source):You try this. Let's say 'random.seed' gives a value to random value generator ('random.randint()') which generates these values on the basis of this seed. One of the must properties of random numbers is that they should be reproducible. Once you put same seed you get the same pattern of random numbers. So you are generating them right from the start again. You give a different seed it starts with a different initial (above 3).You have given a seed now it will generate random numbers between 1 and 10 one after another. So you can assume one set of numbers for one seed value.                    A random number is generated by some operation on previous value.If there is no previous value then the current time as previous value automatically. We can provide this previous value by own using random.seed(x) where x could be any number or string etc.Hence random.random() is not actually perfect random number, it could be predicted via random.seed(x).Hence, generating a random number is not actually random, because it runs on algorithms. Algorithms always give the same output based on the same input. This means, it depends on the value of the seed. So, in order to make it more random, time is automatically assigned to seed().Execute the above program multiple times...1st attempt: prints 5 random integers in the range of 1 - 1002nd attempt: prints same 5 random numbers appeared in the above execution.3rd attempt: same.....So onExplanation: Every time we are running the above program we are setting seed to 10, then random generator takes this as a reference variable. And then by doing some predefined formula, it generates a random number.Hence setting seed to 10 in the next execution again sets reference number to 10 and again the same behavior starts...As soon as we reset the seed value it gives the same plants.Note: Change the seed value and run the program, you'll see a different random sequence than the previous one.In this case, random is actually pseudo-random. Given a seed, it will generate numbers with an equal distribution. But with the same seed, it will generate the same number sequence every time. If you want it to change, you'll have to change your seed. A lot of people like to generate a seed based on the current time or something.Imho, it is used to generate same random course result when you use random.seed(samedigit) again.Set the seed(x) before generating a set of random numbers and use the same seed to generate the same set of random numbers. Useful in case of reproducing the issues.Here is my understanding.

Every time we set a seed value, a "label" or " reference" is generated. The next random.function call is attached to this "label", so next time you call the same seed value and random.function, it will give you the same result.Here is a small test that demonstrates that feeding the seed() method with the same argument will cause the same pseudo-random result:

django test app error - Got an error creating the test database: permission denied to create database

Andrius

[django test app error - Got an error creating the test database: permission denied to create database](https://stackoverflow.com/questions/14186055/django-test-app-error-got-an-error-creating-the-test-database-permission-deni)

When I try to test any app with command (I noticed it when I tried to deploy myproject using fabric, which uses this command):I get this error:syncdb command seems to work. My database settings in settings.py:

2013-01-06 19:51:21Z

When I try to test any app with command (I noticed it when I tried to deploy myproject using fabric, which uses this command):I get this error:syncdb command seems to work. My database settings in settings.py:When Django runs the test suite, it creates a new database, in your case test_finance. The postgres user with username django does not have permission to create a database, hence the error message.When you run migrate or syncdb, Django does not try to create the finance database, so you don't get any errors.You can add the createdb permission to the django user by running the following command in the postgres shell as a superuser (hat tip to this stack overflow answer).Note: The username used in the ALTER USER <username> CREATEDB; command needs to match the database user in your Django settings files. In this case, the original poster, had the user as django the above answer.I have found interesting solution to your problem. 

In fact for MySQL you can grant privileges for non-existing database.

So you can add name 'test_finance' for your test database in your settings:start MySQL shell as the root user:and now grant all privileges to this non-existing database in MySQL:Now Django will start tests without any problems.In the case of Postgres, the user must have createdb permission.See this documentation: https://docs.djangoproject.com/en/2.0/topics/testing/overview/#the-test-databaseIf database is mysql then these two changes will get the things done.1.Open mysite/mysite/settings.pyYour database settings should have an additional TEST block as shown with projectname_test.2.Type the below command using mysql command prompt or mysql workbench to give all privilages to the user specified in settings.pyNow you can run  python manage.py test polls.In my case, GRANT PRIVILEGES solutions didn't work with Python 3.7.2, Django 2.1.7 and MySQL 5.6.23... I don't know why.So I decided to use SQLite as a TEST database...After that, TESTS car run without troubles:If you are using docker-compose what worked for me was the following:orMy settings looks like this:and my docker-compose.yml looks as follows:Wow so combining all of the answers here with a little tweaking finally got me to a working solution for docker-compose, django, and postgres...First the postgres command given by noufal valapra is not correct (or maybe just not current), it should be:In the case of a docker-compose setup, this will go in the init.sql file, this is what mine looks like:Then the Dockerfile for postgres looks like this:Then the Django settings.py has this entry:and the docker-compose looks like this:version: '3.6'services:Maybe you put your test in suspended mode or as a backgrounded job. Try with fg command in bash shell.

How can I break up this long line in Python?

Gattster

[How can I break up this long line in Python?](https://stackoverflow.com/questions/2058925/how-can-i-break-up-this-long-line-in-python)

How would you go about formatting a long line such as this? I'd like to get it to no more than 80 characters wide:Is this my best option?

2010-01-13 17:41:07Z

How would you go about formatting a long line such as this? I'd like to get it to no more than 80 characters wide:Is this my best option?That's a start.  It's not a bad practice to define your longer strings outside of the code that uses them.  It's a way to separate data and behavior.  Your first option is to join string literals together implicitly by making them adjacent to one another:Or with line ending continuations, which is a little more fragile, as this works:But this doesn't:See the difference?  No?  Well you won't when it's your code either.The downside to implicit joining is that it only works with string literals, not with strings taken from

variables, so things can get a little more hairy when you refactor.  Also, you can only interpolate formatting on the combined string as a whole.Alternatively, you can join explicitly using the concatenation operator (+):Explicit is better than implicit, as the zen of python says, but this creates three strings instead of one, and uses twice as much memory: there are the two you have written, plus one which is the two of them joined together, so you have to know when to ignore the zen.  The upside is you can apply formatting to

any of the substrings separately on each line, or to the whole lot from outside the parentheses.Finally, you can use triple-quoted strings:This is often my favorite, though its behavior is slightly different as the newline and any leading whitespace on subsequent lines will show up in your final string.  You can eliminate the newline with an escaping backslash.  This has the same problem as the same technique above, in that correct code only differs from incorrect code by invisible whitespace.Which one is "best" depends on your particular situation, but the answer is not simply aesthetic, but one of subtly different behaviors.  Consecutive string literals are joined by the compiler, and parenthesized expressions are considered to be a single line of code:Personally I dislike hanging open blocks, so I'd format it as:In general I wouldn't bother struggle too hard to make code fit exactly within a 80-column line. It's worth keeping line length down to reasonable levels, but the hard 80 limit is a thing of the past.You can use textwrap module to break it in multiple linesFrom the documentation:For anyone who is also trying to call .format() on a long string, and is unable to use some of the most popular string wrapping techniques without breaking the subsequent .format( call, you can do str.format("", 1, 2) instead of "".format(1, 2). This lets you break the string with whatever technique you like. For example:can beOtherwise, the only possibility is using line ending continuations, which I personally am not a fan of.

Can anyone explain python's relative imports?

carl

[Can anyone explain python's relative imports?](https://stackoverflow.com/questions/1918539/can-anyone-explain-pythons-relative-imports)

I can't for the life of me get python's relative imports to work. I have created a simple example of where it does not function:The directory structure is:/start.py contains just: import sub.relative/sub/relative.py contains just from .. import parentAll other files are blank.When executing the following on the command line:I get:I am using Python 2.6. Why is this the case? How do I make this sandbox example work?

2009-12-16 23:27:05Z

I can't for the life of me get python's relative imports to work. I have created a simple example of where it does not function:The directory structure is:/start.py contains just: import sub.relative/sub/relative.py contains just from .. import parentAll other files are blank.When executing the following on the command line:I get:I am using Python 2.6. Why is this the case? How do I make this sandbox example work?You are importing from package "sub". start.py is not itself in a package even if there is a __init__.py present.You would need to start your program from one directory over parent.py:With start.py:Now pkg is the top level package and your relative import should work.If you want to stick with your current layout you can just use import parent. Because you use start.py to launch your interpreter, the directory where start.py is located is in your python path. parent.py lives there as a separate module.You can also safely delete the top level __init__.py, if you don't import anything into a script further up the directory tree.If you are going to call relative.py directly and i.e. if you really want to import from a top level module you have to explicitly add it to the sys.path list.

Here is how it should work:  If you think the above can cause some kind of inconsistency you can use this instead:  sys.path[0] refers to the path that the entry point was ran from.Checking it out in python3:Example1:If we run it like this(just to make sure PYTHONPATH is empty):Output:If we change import in sub/relative.pyIf we run it like this:Output:Example2:Run it like:Output:If we change import in sub/start.py:Run it like:Output:Run it like:Output:Also it's better to use import from root folder, i.e.:Run it like:Output:

Python: finding an element in a list [duplicate]

Himadri Choudhury

[Python: finding an element in a list [duplicate]](https://stackoverflow.com/questions/604802/python-finding-an-element-in-a-list)

What is a good way to find the index of an element in a list in Python?

Note that the list may not be sorted.Is there a way to specify what comparison operator to use?

2009-03-03 01:45:02Z

What is a good way to find the index of an element in a list in Python?

Note that the list may not be sorted.Is there a way to specify what comparison operator to use?The best way is probably to use the list method .index. For the objects in the list, you can do something like:with any special processing you need.You can also use a for/in statement with enumerate(arr)Example of finding the index of an item that has value > 100.SourceFrom Dive Into Python:If you just want to find out if an element is contained in the list or not:Here is another way using list comprehension (some people might find it debatable). It is very approachable for simple tests, e.g. comparisons on object attributes (which I need a lot):Of course this assumes the existence (and, actually, uniqueness) of a suitable element in the list.assuming you want to find a value in a numpy array,

I guess something like this might work:There is the index method, i = array.index(value), but I don't think you can specify a custom comparison operator. It wouldn't be hard to write your own function to do so, though:The index method of a list will do this for you. If you want to guarantee order, sort the list first using sorted(). Sorted accepts a cmp or key parameter to dictate how the sorting will happen:Or:I use function for returning index for the matching element (Python 2.6):Then use it via lambda function for retrieving needed element by any required equation e.g. by using element name.If i need to use it in several places in my code i just define specific find function e.g. for finding element by name:And then it is quite easy and readable:I found this by adapting some tutos. Thanks to google, and to all of you ;)A very simple use:P.S. scuse my englishhow's this one?Usage:

Simple way to measure cell execution time in ipython notebook

colinfang

[Simple way to measure cell execution time in ipython notebook](https://stackoverflow.com/questions/32565829/simple-way-to-measure-cell-execution-time-in-ipython-notebook)

I would like to get the time spent on the cell execution in addition to the original output from cell.To this end, I tried %%timeit -r1 -n1 but it doesn't expose the variable defined within cell.%%time works for cell which only contains 1 statement.What's the best way to do it?I have been using Execute Time in Nbextension for quite some time now. It is great.

2015-09-14 13:15:57Z

I would like to get the time spent on the cell execution in addition to the original output from cell.To this end, I tried %%timeit -r1 -n1 but it doesn't expose the variable defined within cell.%%time works for cell which only contains 1 statement.What's the best way to do it?I have been using Execute Time in Nbextension for quite some time now. It is great.Use cell magic and this project on github by Phillip Cloud: Load it by putting this at the top of your notebook or put it in your config file if you always want to load it by default:If loaded, every output of subsequent cell execution will include the time in min and sec it took to execute it.The only way I found to overcome this problem is by executing the last statement with print.Do not forget that cell magic starts with %% and line magic starts with %.Notice that any changes performed inside the cell are not taken into consideration in the next cells, something that is counter intuitive when there is a pipeline:

%time and %timeit now come part of ipython's built-in magic commandsAn easier way is to use ExecuteTime plugin in jupyter_contrib_nbextensions package.I simply added %%time at the beginning of the cell and got the time.  You may use the same on Jupyter Spark cluster/ Virtual environment using the same. Just add %%time at the top of the cell and you will get the output. On spark cluster using Jupyter, I added to the top of the cell and I got output like below:- This is not exactly beautiful but without extra softwareThen you can run it like:Sometimes the formatting is different in a cell when using print(res), but jupyter/ipython comes with a display. See an example of the formatting difference using pandas below.The display statement can preserve the formatting. 

You can use timeit magic function for that.Or on the cellCheck more IPython magic functions at https://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynbyou may also want to look in to python's profiling magic command %prunwhich gives something like -

then

will return I find it useful when working with large chunks of code.When in trouble what means what:?%timeit or ??timeitTo get the details:

Python - Check If Word Is In A String

The Woo

[Python - Check If Word Is In A String](https://stackoverflow.com/questions/5319922/python-check-if-word-is-in-a-string)

I'm working with Python v2, and I'm trying to find out if you can tell if a word is in a string.I have found some information about identifying if the word is in the string - using .find, but is there a way to do an IF statement. I would like to have something like the following:Thanks for any help.

2011-03-16 01:10:17Z

I'm working with Python v2, and I'm trying to find out if you can tell if a word is in a string.I have found some information about identifying if the word is in the string - using .find, but is there a way to do an IF statement. I would like to have something like the following:Thanks for any help.What is wrong with:but keep in mind that this matches a sequence of characters, not necessarily a whole word - for example, 'word' in 'swordsmith' is True. If you only want to match whole words, you ought to use regular expressions:If you want to find out whether a whole word is in a space-separated list of words, simply use:This elegant method is also the fastest. Compared to Hugh Bothwell's and daSong's approaches:Edit: A slight variant on this idea for Python 3.6+, equally fast:find returns an integer representing the index of where the search item was found.  If it isn't found, it returns -1.You can split string to the words and check the result list.This small function compares all search words in given text. If all search words are found in text, returns length of search, or False otherwise.Also supports unicode string search.usage:If matching a sequence of characters is not sufficient and you need to match whole words, here is a simple function that gets the job done. It basically appends spaces where necessary and searches for that in the string:This assumes that commas and other punctuations have already been stripped out.As you are asking for a word and not for a string, I would like to present a solution which is not sensitive to prefixes / suffixes and ignores case:If your words might contain regex special chars (such as +), then you need re.escape(word)Advanced way to check the exact word, that we need to find in a long string:Using regex is a solution, but it is too complicated for that case.You can simply split text into list of words. Use split(separator, num) method for that. It returns a list of all the words in the string, using separator as the separator. If separator is unspecified it splits on all whitespace (optionally you can limit the number of splits to num).This will not work for string with commas etc. For example:If you also want to split on all commas etc. use separator argument like this:You could just add a space before and after "word".This way it looks for the space before and after "word".

python design patterns [closed]

Ted Smith

[python design patterns [closed]](https://stackoverflow.com/questions/606448/python-design-patterns)

I am looking for any resources that gives examples of Best Practices, Design patterns and the SOLID principles using Python.  

2009-03-03 13:47:28Z

I am looking for any resources that gives examples of Best Practices, Design patterns and the SOLID principles using Python.  Some overlap in theseIntermediate and Advanced Software Carpentry in PythonCode Like a Pythonista: Idiomatic PythonPython Idioms and EfficiencyGoogle Developers Day US - Python Design PatternsAnother resource is by example at the Python Recipes.  A good number do not follow best practices but you can find some patterns in there that are usefulTypein a Python console.Although this is usually treated as a (fine!) joke, it contains a couple of valid python-specific axioms.Bruce Eckel's "Thinking in Python" leans heavily on Design Patterns You can get started here and here.  For a more in depth look at design pattners you should look at Design Patterns: Elements of Reusable Object-Oriented Software.  The source code is not in Python, but it doesn't need to be for you to understand the patterns. Something you can use to simplify your code when calling attributes on objects that might or might not exist is to use the Null Object Design Pattern (to which I was introduced in Python Cookbook).This object just eats the lack of attribute error, and you can avoid checking for their existence.It's nothing more thanWith this, if you do Null("any", "params", "you", "want").attribute_that_doesnt_exists() it won't explode, but just silently become the equivalent of pass.Normally you'd do something likeWith this, you just do:and forget about it. Beware that extensive use of the Null object can potentially hide bugs in your code.You may also wish to read this article (select the .pdf file), which discusses Design Patterns in dynamic object oriented languages (i.e. Python). To quote the page:

How do I get python's pprint to return a string instead of printing?

drue

[How do I get python's pprint to return a string instead of printing?](https://stackoverflow.com/questions/521532/how-do-i-get-pythons-pprint-to-return-a-string-instead-of-printing)

In other words, what's the sprintf equivalent to pprint?  

2009-02-06 18:25:53Z

In other words, what's the sprintf equivalent to pprint?  The pprint module has a command named pformat, for just that purpose.From the documentation:Example:Assuming you really do mean pprint from the pretty-print library,  then you want

the pprint.pformat method.If you just mean  print, then you want str()Are you looking for pprint.pformat?Something like this:

How can I recover the return value of a function passed to multiprocessing.Process?

blz

[How can I recover the return value of a function passed to multiprocessing.Process?](https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce)

In the example code below, I'd like to recover the return value of the function worker.  How can I go about doing this?  Where is this value stored?Example Code:Output:I can't seem to find the relevant attribute in the objects stored in jobs.

2012-05-02 13:44:34Z

In the example code below, I'd like to recover the return value of the function worker.  How can I go about doing this?  Where is this value stored?Example Code:Output:I can't seem to find the relevant attribute in the objects stored in jobs.Use shared variable to communicate. For example like this:I think the approach suggested by @sega_sai is the better one. But it really needs a code example, so here goes:Which will print the return values:If you are familiar with map (the Python 2 built-in) this should not be too challenging. Otherwise have a look at sega_Sai's link.Note how little code is needed. (Also note how processes are re-used).This example shows how to use a list of multiprocessing.Pipe instances to return strings from an arbitrary number of processes:Output:This solution uses fewer resources than a multiprocessing.Queue which usesor a multiprocessing.SimpleQueue which usesIt is very instructive to look at the source for each of these types.For some reason, I couldn't find a general example of how to do this with Queue anywhere (even Python's doc examples don't spawn multiple processes), so here's what I got working after like 10 tries:Queue is a blocking, thread-safe queue that you can use to store the return values from the child processes. So you have to pass the queue to each process. Something less obvious here is that you have to get() from the queue before you join the Processes or else the queue fills up and blocks everything.Update for those who are object-oriented (tested in Python 3.4):For anyone else who is seeking how to get a value from a Process using Queue:It seems that you should use the multiprocessing.Pool class instead and use the methods .apply() .apply_async(), map()http://docs.python.org/library/multiprocessing.html?highlight=pool#multiprocessing.pool.AsyncResultYou can use the exit built-in to set the exit code of a process. It can be obtained from the exitcode attribute of the process:Output:The pebble package has a nice abstraction leveraging multiprocessing.Pipe which makes this quite straightforward:Example from: https://pythonhosted.org/Pebble/#concurrent-decoratorsA simple solution:Output:If you are using Python 3, you can use concurrent.futures.ProcessPoolExecutor as a convenient abstraction:Output:Thought I'd simplify the simplest examples copied from above, working for me on Py3.6. Simplest is multiprocessing.Pool:You can set the number of processes in the pool with, e.g., Pool(processes=5). However it defaults to CPU count, so leave it blank for CPU-bound tasks. (I/O-bound tasks often suit threads anyway, as the threads are mostly waiting so can share a CPU core.) Pool also applies chunking optimization.(Note that the worker method cannot be nested within a method. I initially defined my worker method inside the method that makes the call to pool.map, to keep it all self-contained, but then the processes couldn't import it, and threw "AttributeError: Can't pickle local object outer_method..inner_method". More here. It can be inside a class.)(Appreciate the original question specified printing 'represent!' rather than time.sleep(), but without it I thought some code was running concurrently when it wasn't.)Py3's ProcessPoolExecutor is also two lines (.map returns a generator so you need the list()):With plain Processes:Use SimpleQueue if all you need is put and get. The first loop starts all the processes, before the second makes the blocking queue.get calls. I don't think there's any reason to call p.join() too.I modified vartec's answer a bit since I needed to get the error codes from the function.  (Thanks vertec!!! its an awesome trick)This can also be done with a manager.list but I think is better to have it in a dict and store a list within it.  That way, way we keep the function and the results since we can't be sure of the order in which the list will be populated.

Python jinja2 shorthand conditional

Ahmed Nuaman

[Python jinja2 shorthand conditional](https://stackoverflow.com/questions/14214942/python-jinja2-shorthand-conditional)

Say I have this:In PHP, say, I can write a shorthand conditional, like:Is there then a way I can translate this to work in a jinja2 template:

2013-01-08 12:27:33Z

Say I have this:In PHP, say, I can write a shorthand conditional, like:Is there then a way I can translate this to work in a jinja2 template:Yes, it's possible to use inline if-expressions:Alternative way (but it's not python style. It's JS style)

Why can't Python's raw string literals end with a single backslash?

cdleary

[Why can't Python's raw string literals end with a single backslash?](https://stackoverflow.com/questions/647769/why-cant-pythons-raw-string-literals-end-with-a-single-backslash)

Technically, any odd number of backslashes, as described in the documentation.It seems like the parser could just treat backslashes in raw strings as regular characters (isn't that what raw strings are all about?), but I'm probably missing something obvious.

2009-03-15 12:54:53Z

Technically, any odd number of backslashes, as described in the documentation.It seems like the parser could just treat backslashes in raw strings as regular characters (isn't that what raw strings are all about?), but I'm probably missing something obvious.The reason is explained in the part of that section which I highlighted in bold:So raw strings are not 100% raw, there is still some rudimentary backslash-processing.The whole misconception about python's raw strings is that most of people think that backslash (within a raw string) is just a regular character as all others. It is NOT. The key to understand is this python's tutorial sequence:So any character following a backslash is part of raw string. Once parser enters a raw string (non Unicode one) and encounters a backslash it knows there are 2 characters (a backslash and a char following it).This way:and:Last case shows that according to documentation now a parser cannot find closing quote as the last quote you see above is part of the string i.e. backslash cannot be last here as it will 'devour' string closing char.That's the way it is! I see it as one of those small defects in python!I don't think there's a good reason for it, but it's definitely not parsing; it's really easy to parse raw strings with \ as a last character.The catch is, if you allow \ to be the last character in a raw string then you won't be able to put " inside a raw string. It seems python went with allowing " instead of allowing \ as the last character.However, this shouldn't cause any trouble.If you're worried about not being able to easily write windows folder pathes such as c:\mypath\ then worry not, for, you can represent them as r"C:\mypath", and, if you need to append a subdirectory name, don't do it with string concatenation, for it's not the right way to do it anyway! use os.path.joinAnother trick is to use chr(92) as it evaluates to "\". I recently had to clean a string of backslashes and the following did the trick:I realize that this does not take care of the "why" but the thread attracts many people looking for a solution to an immediate problem.In order for you to end a raw string with a slash I suggest you can use this trick:Since \" is allowed inside the raw string.  Then it can't be used to identify the end of the string literal. Why not stop parsing the string literal when you encounter the first "?If that was the case, then \" wouldn't be allowed inside the string literal.  But it is.The reason for why r'\' is syntactical incorrect is that although the string expression is raw the used quotes (single or double) always have to be escape since they would mark the end of the quote otherwise. So if you want to express a single quote inside single quoted string, there is no other way than using \'. Same applies for double quotes.But you could use:Another user who has since deleted their answer (not sure if they'd like to be credited) suggested that the Python language designers may be able to simplify the parser design by using the same parsing rules and expanding escaped characters to raw form as an afterthought (if the literal was marked as raw).I thought it was an interesting idea and am including it as community wiki for posterity.Comming from C it pretty clear to me that a single \ works as escape character allowing you to put special characters such as newlines, tabs and quotes into strings.That does indeed disallow \ as last character since it will escape the " and make the parser choke. But as pointed out earlier \ is legal.some tips :1) if you need to manipulate backslash for path then standard python module os.path is your friend. for example : 2) if you want to build strings with backslash in it BUT without backslash at the END of your string then raw string is your friend (use 'r' prefix before your literal string). for example : 3) if you need to prefix a string in a variable X with a backslash then you can do this :4) if you need to create a string with a backslash at the end then combine tip 2 and 3 :now lilypond_statement contains "\DisplayLilyMusic \upper"long live python ! :)n3onI encountered this problem and found a partial solution which is good for some cases. Despite python not being able to end a string with a single backslash, it can be serialized and saved in a text file with a single backslash at the end. Therefore if what you need is saving a text with a single backslash on you computer, it is possible:BTW it is not working with json if you dump it using python's json library. Finally, I work with Spyder, and I noticed that if I open the variable in spider's text editor by double clicking on its name in the variable explorer, it is presented with a single backslash and can be copied to the clipboard that way (it's not very helpful for most needs but maybe for some..). 

Python「raise from」usage

darkfeline

[Python「raise from」usage](https://stackoverflow.com/questions/24752395/python-raise-from-usage)

What's the difference between raise and raise from in Python?which yieldsandwhich yields

2014-07-15 07:33:02Z

What's the difference between raise and raise from in Python?which yieldsandwhich yieldsThe difference is that when you use from, the __cause__ attribute is set and the message states that the exception was directly caused by. If you omit the from then no __cause__ is set, but the __context__ attribute may be set as well, and the traceback then shows the context as during handling something else happened.Setting the __context__ happens if you used raise in an exception handler; if you used raise anywhere else no __context__ is set either.If a __cause__ is set, a __suppress_context__ = True flag is also set on the exception; when __suppress_context__ is set to True, the __context__ is ignored when printing a traceback.When raising from a exception handler where you don't want to show the context (don't want a during handling another exception happened message), then use raise ... from None to set __suppress_context__ to True.In other words, Python sets a context on exceptions so you can introspect where an exception was raised, letting you see if another exception was replaced by it. You can also add a cause to an exception, making the traceback explicit about the other exception (use different wording), and the context is ignored (but can still be introspected when debugging). Using raise ... from None lets you suppress the context being printed.See the raise statement documenation:Also see the Built-in Exceptions documentation for details on the context and cause information attached to exceptions.

How can I filter a date of a DateTimeField in Django?

Xidobix

[How can I filter a date of a DateTimeField in Django?](https://stackoverflow.com/questions/1317714/how-can-i-filter-a-date-of-a-datetimefield-in-django)

I am trying to filter a DateTimeField comparing with a date. I mean:I get an empty queryset list as an answer because (I think) I am not considering time, but I want "any time".Is there an easy way in Django for doing this?I have the time in the datetime setted, it is not 00:00.

2009-08-23 03:52:07Z

I am trying to filter a DateTimeField comparing with a date. I mean:I get an empty queryset list as an answer because (I think) I am not considering time, but I want "any time".Is there an easy way in Django for doing this?I have the time in the datetime setted, it is not 00:00.Such lookups are implemented in django.views.generic.date_based as follows:Because it is quite verbose there are plans to improve the syntax using __date operator. Check "#9596 Comparing a DateTimeField to a date is too hard" for more details.// edit after commentsdoest not work because it creates a datetime object with time values set to 0, so the time in database doesn't match.Here are the results I got with ipython's timeit function:contains seems to be faster.the above is what I've used. Not only does it work, it also has some inherent logical backing.Now Django has __date queryset filter to query datetime objects against dates in development version. Thus, it will be available in 1.9 soon.As of Django 1.9, the way to do this is by using __date on a datetime object.For example:

MyObject.objects.filter(datetime_attr__date=datetime.date(2009,8,22))This produces the same results as using __year, __month, and __day and seems to work for me:assuming active_on is a date object, increment it by 1 day then do  range Here is an interesting technique-- I leveraged the startswith procedure as implemented with Django on MySQL to achieve the result of only looking up a datetime through only the date. Basically, when Django does the lookup in the database it has to do a string conversion for the DATETIME MySQL storage object, so you can filter on that, leaving out the timestamp portion of the date-- that way %LIKE% matches only the date object and you'll get every timestamp for the given date. This will perform the following query:The LIKE BINARY in this case will match everything for the date, no matter the timestamp. Including values like:Hopefully this helps everyone until Django comes out with a solution!Hm.. My solution is working:There's a fantastic blogpost that covers this here: Comparing Dates and Datetimes in the Django ORMThe best solution posted for Django>1.7,<1.9 is to register a transform:Then you can use it in your filters like this:EDITThis solution is definitely back end dependent. From the article:See the article Django DocumentationIts very similar to the JZ answerIn Django 1.7.6 works:

How to get item's position in a list?

Sean

[How to get item's position in a list?](https://stackoverflow.com/questions/364621/how-to-get-items-position-in-a-list)

I am iterating over a list and I want to print out the index of the item if it meets a certain condition. How would I do this?Example:  

2008-12-13 01:20:32Z

I am iterating over a list and I want to print out the index of the item if it meets a certain condition. How would I do this?Example:  Hmmm.  There was an answer with a list comprehension here, but it's disappeared.Here:Example:Update:Okay, you want a generator expression, we'll have a generator expression.  Here's the list comprehension again, in a for loop:Now we'll construct a generator...and niftily enough, we can assign that to a variable, and use it from there...And to think I used to write FORTRAN.What about the following?If you are not sure whether the element to look for is actually in the list, you can add a preliminary check, likeoror the "pythonic way", which I don't like so much because code is less clear, but sometimes is more efficient,Use enumerate:xrange instead of range as requested (see comments).Here is another way to do this:If your list got large enough and you only expected to find the value in a sparse number of indices, consider that this code could execute much faster because you don't have to iterate every value in the list.If you expect to find the value a lot you should probably just append "index" to a list and print the list at the end to save time per iteration.Try the below:I think that it might be useful to use the curselection() method from thte Tkinter library:This method works on Tkinter listbox widgets, so you'll need to construct one of them instead of a list.This will return a position like this:('0',) (although later versions of Tkinter may return a list of ints instead)Which is for the first position and the number will change according to the item position.For more information, see this page:

http://effbot.org/tkinterbook/listbox.htmGreetings.Why complicate things?I guess that it's exacly what you want. ;-)

'id' will be always the index of the values on the list.

APT command line interface-like yes/no input?

h3.

[APT command line interface-like yes/no input?](https://stackoverflow.com/questions/3041986/apt-command-line-interface-like-yes-no-input)

Is there any short way to achieve what the APT (Advanced Package Tool) command line interface does in Python?I mean, when the package manager prompts a yes/no question followed by [Yes/no], the script accepts YES/Y/yes/y or Enter (defaults to Yes as hinted by the capital letter).The only thing I find in the official docs is input and raw_input...I know it's not that hard to emulate, but it's annoying to rewrite :|

2010-06-15 01:13:54Z

Is there any short way to achieve what the APT (Advanced Package Tool) command line interface does in Python?I mean, when the package manager prompts a yes/no question followed by [Yes/no], the script accepts YES/Y/yes/y or Enter (defaults to Yes as hinted by the capital letter).The only thing I find in the official docs is input and raw_input...I know it's not that hard to emulate, but it's annoying to rewrite :|As you mentioned, the easiest way is to use raw_input() (or simply input() for Python 3). There is no built-in way to do this. From Recipe 577058:Usage example:I'd do it this way:There is a function strtobool in Python's standard library: http://docs.python.org/2/distutils/apiref.html?highlight=distutils.util#distutils.util.strtoboolYou can use it to check user's input and transform it to True or False value.A very simple (but not very sophisticated) way of doing this for a single choice would be:You could also write a simple (slightly improved) function around this:Note: On Python 2, use raw_input instead of input.You can use click's confirm method.This will print:Should work for Python 2/3 on Linux, Mac or Windows.Docs: http://click.pocoo.org/5/prompts/#confirmation-promptsas mentioned by @Alexander Artemenko, here's a simple solution using strtoboolI know this has been answered a bunch of ways and this may not answer OP's specific question (with the list of criteria) but this is what I did for the most common use case and it's far simpler than the other responses:You can also use prompter.Shamelessly taken from the README:I modified fmark's answer to by python 2/3 compatible more pythonic.See ipython's utility module if you are interested in something with more error handlingon 2.7, is this too non-pythonic?it captures any variation of Yes at least.Doing the same with python 3.x, where raw_input() doesn't exist:For Python 3, I'm using this function:The strtobool function converts a string into a bool. If the string cant be parsed it will raise a ValueError.In Python 3 raw_input has been renamed to input.You could try something like the code below to be able to work with choices from the variable 'accepted' show here:Here is the code ..As a programming noob, I found a bunch of the above answers overly complex, especially if the goal is to have a simple function that you can pass various yes/no questions to, forcing the user to select yes or no. After scouring this page and several others, and borrowing all of the various good ideas, I ended up with the following:How about this: This is what I use:This is how I'd do it.OutputHere's my take on it, I simply wanted to abort if the user did not affirm the action.

Find the most common element in a list

hoju

[Find the most common element in a list](https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list)

What is an efficient way to find the most common element in a Python list?My list items may not be hashable so can't use a dictionary.

Also in case of draws the item with the lowest index should be returned. Example:

2009-10-05 06:35:44Z

What is an efficient way to find the most common element in a Python list?My list items may not be hashable so can't use a dictionary.

Also in case of draws the item with the lowest index should be returned. Example:With so many solutions proposed, I'm amazed nobody's proposed what I'd consider an obvious one (for non-hashable but comparable elements) -- [itertools.groupby][1].  itertools offers fast, reusable functionality, and lets you delegate some tricky logic to well-tested standard library components.  Consider for example:This could be written more concisely, of course, but I'm aiming for maximal clarity.  The two print statements can be uncommented to better see the machinery in action; for example, with prints uncommented:emits:As you see, SL is a list of pairs, each pair an item followed by the item's index in the original list (to implement the key condition that, if the "most common" items with the same highest count are > 1, the result must be the earliest-occurring one).groupby groups by the item only (via operator.itemgetter). The auxiliary function, called once per grouping during the max computation, receives and internally unpacks a group - a tuple with two items (item, iterable) where the iterable's items are also two-item tuples, (item, original index) [[the items of SL]].Then the auxiliary function uses a loop to determine both the count of entries in the group's iterable, and the minimum original index; it returns those as combined "quality key", with the min index sign-changed so the max operation will consider "better" those items that occurred earlier in the original list.This code could be much simpler if it worried a little less about big-O issues in time and space, e.g....:same basic idea, just expressed more simply and compactly... but, alas, an extra O(N) auxiliary space (to embody the groups' iterables to lists) and O(N squared) time (to get the L.index of every item). While premature optimization is the root of all evil in programming, deliberately picking an O(N squared) approach when an O(N log N) one is available just goes too much against the grain of scalability!-)Finally, for those who prefer "oneliners" to clarity and performance, a bonus 1-liner version with suitably mangled names:-).A simpler one-liner:Borrowing from here, this can be used with Python 2.7:Works around 4-6 times faster than Alex's solutions, and is 50 times faster than the one-liner proposed by newacct.To retrieve the element that occurs first in the list in case of ties:What you want is known in statistics as mode, and Python of course has a built-in function to do exactly that for you:Note that if there is no "most common element" such as cases where the top two are tied, this will raise StatisticsError, because statistically speaking, there is no mode in this case.If they are not hashable, you can sort them and do a single loop over the result counting the items (identical items will be next to each other). But it might be faster to make them hashable and use a dict.This is an O(n) solution.(reversed is used to make sure that it returns the lowest index item)Sort a copy of the list and find the longest run.  You can decorate the list before sorting it with the index of each element, and then choose the run that starts with the lowest index in the case of a tie.A one-liner:Simple one line solutionIt will return most frequent element with its frequency.You probably don't need this anymore, but this is what I did for a similar problem. (It looks longer than it is because of the comments.)Building on Luiz's answer, but satisfying the "in case of draws the item with the lowest index should be returned" condition:Example:Without the requirement about the lowest index, you can use collections.Counter for this:Here:I have a vague feeling there is a method somewhere in the standard library that will give you the count of each element, but I can't find it.This is the obvious slow solution (O(n^2)) if neither sorting nor hashing is feasible, but equality comparison (==) is available:But making your items hashable or sortable (as recommended by other answers) would almost always make finding the most common element faster if the length of your list (n) is large. O(n) on average with hashing, and O(n*log(n)) at worst for sorting.I needed to do this in a recent program. I'll admit it, I couldn't understand Alex's answer, so this is what I ended up with.I timed it against Alex's solution and it's about 10-15% faster for short lists, but once you go over 100 elements or more (tested up to 200000) it's about 20% slower.Hi this is a very simple solution with big O(n)Where number the element in the list that repeats most of the time

python re.sub group: number after \number

zhigang

[python re.sub group: number after \number](https://stackoverflow.com/questions/5984633/python-re-sub-group-number-after-number)

How can I replace foobar with foo123bar? This doesn't work:This works:I think it's a common issue when having something like \number. Can anyone give me a hint on how to handle this?

2011-05-12 21:22:24Z

How can I replace foobar with foo123bar? This doesn't work:This works:I think it's a common issue when having something like \number. Can anyone give me a hint on how to handle this?The answer is:Relevant excerpt from the docs:

Windows Scipy Install: No Lapack/Blas Resources Found

tjb305

[Windows Scipy Install: No Lapack/Blas Resources Found](https://stackoverflow.com/questions/28190534/windows-scipy-install-no-lapack-blas-resources-found)

I am trying to install python and a series of packages onto a 64bit windows 7 desktop. I have installed Python 3.4, have Microsoft Visual Studio C++ installed, and have successfully installed numpy, pandas and a few others. I am getting the following error when trying to install scipy;I am using pip install offline, the install command I am using is;I have read the posts on here about requiring a compiler which if I understand correctly is the VS C++ compiler. I am using the 2010 version as I am using Python 3.4. This has worked for other packages.Do I have to use the window binary or is there a way I can get pip install to work?Many thanks for the help

2015-01-28 11:00:07Z

I am trying to install python and a series of packages onto a 64bit windows 7 desktop. I have installed Python 3.4, have Microsoft Visual Studio C++ installed, and have successfully installed numpy, pandas and a few others. I am getting the following error when trying to install scipy;I am using pip install offline, the install command I am using is;I have read the posts on here about requiring a compiler which if I understand correctly is the VS C++ compiler. I am using the 2010 version as I am using Python 3.4. This has worked for other packages.Do I have to use the window binary or is there a way I can get pip install to work?Many thanks for the helpThe solution to the absence of BLAS/LAPACK libraries for SciPy installations on Windows 7 64-bit is described here:http://www.scipy.org/scipylib/building/windows.htmlInstalling Anaconda is much easier, but you still don't get Intel MKL or GPU support without paying for it (they are in the MKL Optimizations and Accelerate add-ons for Anaconda - I'm not sure if they use PLASMA and MAGMA either). With MKL optimization, numpy has outperformed IDL on large matrix computations by 10-fold. MATLAB uses the Intel MKL library internally and supports GPU computing, so one might as well use that for the price if they're a student ($50 for MATLAB + $10 for the Parallel Computing Toolbox). If you get the free trial of Intel Parallel Studio, it comes with the MKL library, as well as C++ and FORTRAN compilers that will come in handy if you want to install BLAS and LAPACK from MKL or ATLAS on Windows:http://icl.cs.utk.edu/lapack-for-windows/lapack/Parallel Studio also comes with the Intel MPI library, useful for cluster computing applications and their latest Xeon processsors. While the process of building BLAS and LAPACK with MKL optimization is not trivial, the benefits of doing so for Python and R are quite large, as described in this Intel webinar:https://software.intel.com/en-us/articles/powered-by-mkl-accelerating-numpy-and-scipy-performance-with-intel-mkl-pythonAnaconda and Enthought have built businesses out of making this functionality and a few other things easier to deploy. However, it is freely available to those willing to do a little work (and a little learning).For those who use R, you can now get MKL optimized BLAS and LAPACK for free with R Open from Revolution Analytics.EDIT: Anaconda Python now ships with MKL optimization, as well as support for a number of other Intel library optimizations through the Intel Python distribution. However, GPU support for Anaconda in the Accelerate library (formerly known as NumbaPro) is still over $10k USD! The best alternatives for that are probably PyCUDA and scikit-cuda, as copperhead (essentially a free version of Anaconda Accelerate) unfortunately ceased development five years ago. It can be found here if anybody wants to pick up where they left off.The following link should solve all problems with Windows and SciPy; just choose the appropriate download. I was able to pip install the package with no problems.  Every other solution I have tried gave me big headaches.Source: http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipyCommand: This assumes you have installed the following already:It will be successful installed.Sorry to necro, but this is the first google search result. This is the solution that worked for me:This was the order I got everything working. The second point is the most important one. Scipy needs Numpy+MKL, not just vanilla Numpy.If you are working with Windows and Visual Studio 2015Enter the following commandsMy 5  cents; You can just install the entire (pre-compiled) SciPy from 

https://github.com/scipy/scipy/releasesGood Luck!Simple and Fast Installation of Scipy in WindowsFor python27

1、Install numpy + mkl（download link:http://www.lfd.uci.edu/~gohlke/pythonlibs/）

2、install scipy (the same site)

OK!Intel now provides a Python distribution for Linux / Windows / OS X for free called "Intel distribution for Python". Its a complete Python distribution (e.g. python.exe is included in the package) which includes some pre-installed modules compiled against Intel's MKL (Math Kernel Library) and thus optimized for faster performance. The distribution includes the modules NumPy, SciPy, scikit-learn, pandas, matplotlib, Numba, tbb, pyDAAL, Jupyter, and others. The drawback is a bit of lateness in upgrading to more recent versions of Python. For example as of today (1 May 2017) the distribution provides CPython 3.5 while the 3.6 version is already out. But if you don't need the new features they should be perfectly fine.I was also getting same error while installing scikit-fuzzy. I resolved error as follows:choose file according to python version like amd64 for python3 and other win32 file for the python27 I hope, It will work for youSolutions:Refer: Using resources at  http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy will solve the problem. However, you should be careful about versions compatibility. After trying for several times, finally I decided to uninstall python and then installed a fresh version of python along with numpy and then installed scipy and this resolved my problem.  install intel's distribution of python https://software.intel.com/en-us/intel-distribution-for-pythonbetter of for distribution of python should contain them initiallydo this, it solved for me 

        pip install -U scikit-learn

How to do a less than or equal to filter in Django queryset?

Finglish

[How to do a less than or equal to filter in Django queryset?](https://stackoverflow.com/questions/10040143/how-to-do-a-less-than-or-equal-to-filter-in-django-queryset)

I am attempting to filter users by a custom field in each users profile called profile. This field is called level and is an integer between 0-3.If I filter using equals, I get a list of users with the chosen level as expected:When I try to filter using less than:I get the error:Is there away to filter by < or >, or am I barking up the wrong tree.

2012-04-06 06:49:32Z

I am attempting to filter users by a custom field in each users profile called profile. This field is called level and is an integer between 0-3.If I filter using equals, I get a list of users with the chosen level as expected:When I try to filter using less than:I get the error:Is there away to filter by < or >, or am I barking up the wrong tree.Less than or equal:Greater than or equal:Likewise, lt for less than and gt for greater than. You can find them all in the documentation.

How to modify list entries during for loop?

Alex

[How to modify list entries during for loop?](https://stackoverflow.com/questions/4081217/how-to-modify-list-entries-during-for-loop)

Now I know that it is not safe to modify the list during an iterative looping. However, suppose I have a list of strings, and I want to strip the strings themselves. Does replacement of mutable values count as modification? 

2010-11-02 19:04:29Z

Now I know that it is not safe to modify the list during an iterative looping. However, suppose I have a list of strings, and I want to strip the strings themselves. Does replacement of mutable values count as modification? It's considered poor form. Use a list comprehension instead, with slice assignment if you need to retain existing references to the list.Since the loop below only modifies elements already seen, it would be considered acceptable:Which is different from:in that it doesn't require the creation of a temporary list and an assignment of it to replace the original, although it does require more indexing operations.Caution: Although you can modify entries this way, you can't change the number of items in the list without risking the chance of encountering problems. Here's an example of what I mean—deleting an entry messes-up the indexing from that point on:(The result is wrong because it didn't delete all the items it should have.) UpdateSince this is a fairly popular answer, here's how to effectively delete entries "in-place" (even though that's not exactly the question):One more for loop variant, looks cleaner to me than one with enumerate():Modifying each element while iterating a list is fine, as long as you do not change add/remove elements to list. You can use list comprehension:or just do the C-style for loop:No you wouldn't alter the "content" of the list, if you could mutate strings that way. But in Python they are not mutable. Any string operation returns a new string.If you had a list of objects you knew were mutable, you could do this as long as you don't change the actual contents of the list.Thus you will need to do a map of some sort. If you use a generator expression it [the operation] will be done as you iterate and you will save memory.You can do something like this:It's called a list comprehension, to make it easier for you to loop inside a list.The answer given by Jemshit Iskenderov  and Ignacio Vazquez-Abrams is really good. It can be further illustrated with this example: imagine thata) A list with two vectors is given to you;b) you would like to traverse the list and reverse the order of each one of the arraysLet's say you haveYou will getOn the other hand, if you do The result isIt is not clear from your question what the criteria for deciding what strings to remove is, but if you have or can make a list of the strings that you want to remove , you could do the following:which changes my_strings to ['a', 'c', 'e']

How to sort with lambda in Python

Niklas

[How to sort with lambda in Python](https://stackoverflow.com/questions/3766633/how-to-sort-with-lambda-in-python)

In Python, I am trying to sort by date with lambda.  I can't understand my error message. The message is:The line I have is

2010-09-22 05:46:41Z

In Python, I am trying to sort by date with lambda.  I can't understand my error message. The message is:The line I have isUseOn Python 2.x, the sorted function takes its arguments in this order:so without the key=, the function you pass in will be considered a cmp function which takes 2 arguments.It will print as following:Python lists have two built-in ways to sort data:Based on your requirement you can choose among these two:if you want to keep original list ,you can use sorted function or if you don't need original list you can  use sort function.Before going on sort or sorted ,we need to understand lambda.A lambda is an anonymous function and an anonymous function is a function that is defined without a name, this post seems to explain it pretty nicely.https://www.programiz.com/python-programming/anonymous-functionLambda functions are nice for calling in-line because they only have one expression which is evaluated and returned. They syntax for a lambda is:lambda arguments: expressionlet's see how to use sorted function:output:

[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]Here we can see list student_tuples having tuples is sorted based on key parameter provided that is student[2].

django MultiValueDictKeyError error, how do I deal with it

dotty

[django MultiValueDictKeyError error, how do I deal with it](https://stackoverflow.com/questions/5895588/django-multivaluedictkeyerror-error-how-do-i-deal-with-it)

I'm trying to save a object to my database, but it's throwing a MultiValueDictKeyError error.The problems lies within the form, the is_private is represented by a checkbox. If the check box is NOT selected, obviously nothing is passed. This is where the error gets chucked.How do I properly deal with this exception, and catch it? The line is

2011-05-05 09:40:14Z

I'm trying to save a object to my database, but it's throwing a MultiValueDictKeyError error.The problems lies within the form, the is_private is represented by a checkbox. If the check box is NOT selected, obviously nothing is passed. This is where the error gets chucked.How do I properly deal with this exception, and catch it? The line isUse the MultiValueDict's get method. This is also present on standard dicts and is a way to fetch a value while providing a default if it does not exist.Generally,Choose what is best for you:If is_private key is present in request.POST the is_private variable will be equal to it, if not, then it will be equal to False.You get that because you're trying to get a key from a dictionary when it's not there. You need to test if it is in there first.try:or depending on the values you're using.Why didn't you try to define is_private in your models as default=False?Another thing to remember is that request.POST['keyword'] refers to the element identified by the specified html name attribute keyword.So, if your form is:then, request.POST['keyword'] and request.POST['results'] will contain the value of the input elements keyword and results, respectively.For me, this error occurred in my django project because of the following:As can be seen in no. 3 above,in the last url pattern, I was incorrectly calling views.count whereas I needed to call views.about. 

This line fulltext = request.GET['fulltext'] in count function (which was mistakenly called because of wrong entry in urlpatterns) of views.py threw the multivaluedictkeyerror exception.Then I changed the last url pattern in urls.py  to the correct one i.e. path('about/',views.about,name="about"), and everything worked fine.Apparently, in general a newbie programmer in django can make the mistake I made of wrongly calling another view function for a url, which might be expecting different set of parameters or passing different set of objects in its render call, rather than the intended behavior.Hope this helps some newbie programmer to django.You can handle this error by putting :

pyplot axes labels for subplots

farqwag25

[pyplot axes labels for subplots](https://stackoverflow.com/questions/6963035/pyplot-axes-labels-for-subplots)

I have the following plot:I want to be able to create axes labels and titles not just for each of the two subplots, but also common labels that span both subplots.  For example, since both plots have identical axes, I only need one set of x and y- axes labels.  I do want different titles for each subplot though.I tried a few things but none of them worked right

2011-08-05 21:55:47Z

I have the following plot:I want to be able to create axes labels and titles not just for each of the two subplots, but also common labels that span both subplots.  For example, since both plots have identical axes, I only need one set of x and y- axes labels.  I do want different titles for each subplot though.I tried a few things but none of them worked right

You can create a big subplot that covers the two subplots and then set the common labels.Another way is using fig.text() to set the locations of the common labels directly.One simple way using subplots:Wen-wei Liao's answer is good if you are not trying to export vector graphics or that you have set up your matplotlib backends to ignore colorless axes; otherwise the hidden axes would show up in the exported graphic.My answer suplabel here is similar to the fig.suptitle which uses the fig.text function.  Therefore there is no axes artist being created and made colorless.

However, if you try to call it multiple times you will get text added on top of each other (as fig.suptitle does too).  Wen-wei Liao's answer doesn't, because fig.add_subplot(111) will return the same Axes object if it is already created.My function can also be called after the plots have been created.Here is a solution where you set the ylabel of one of the plots and adjust the position of it so it is centered vertically. This way you avoid problems mentioned by KYC.The methods in the other answers will not work properly when the yticks are large. The ylabel will either overlap with ticks, be clipped on the left or completely invisible/outside of the figure. I've modified Hagne's answer so it works with more than 1 column of subplots, for both xlabel and ylabel, and it shifts the plot to keep the ylabel visible in the figure.It works for the following example, while Hagne's answer won't draw ylabel (since it's outside of the canvas) and KYC's ylabel overlaps with the tick labels:Alternatively, if you are fine with colorless axis, I've modified Julian Chen's solution so ylabel won't overlap with tick labels. Basically, we just have to set ylims of the colorless so it matches the largest ylims of the subplots so the colorless tick labels sets the correct location for the ylabel.Again, we have to shrink the plot to prevent clipping. Here I've hard coded the amount to shrink, but you can play around to find a number that works for you or calculate it like in the method above.plt.setp() will do the job:

Django 1.7 throws django.core.exceptions.AppRegistryNotReady: Models aren't loaded yet

doubleo

[Django 1.7 throws django.core.exceptions.AppRegistryNotReady: Models aren't loaded yet](https://stackoverflow.com/questions/25537905/django-1-7-throws-django-core-exceptions-appregistrynotready-models-arent-load)

This is the traceback on my windows system. And my manage.py looks like this:I get this error when i am trying to use registration app in Django 1.7

2014-08-27 22:24:57Z

This is the traceback on my windows system. And my manage.py looks like this:I get this error when i am trying to use registration app in Django 1.7This is what solved it for us and these folks:Our project started with Django 1.4, we went to 1.5 and then to 1.7. Our wsgi.py looked like this:When I updated to the 1.7 style WSGI handler:Everything works now.Running these commands solved my problem (credit to this answer):However I'm not sure why I need this. Comments would be appreciated.The issue is in your registration app. It seems django-registration calls get_user_module() in models.py at a module level (when models are still being loaded by the application registration process). This will no longer work:I'd change this models file to only call get_user_model() inside methods (and not at module level) and in FKs use something like:BTW, the call to django.setup() shouldn't be required in your manage.py file, it's called for you in execute_from_command_line. (source)Just encountered the same issue. The problem is because of django-registration incompatible with django 1.7 user model. A simple fix is to change these lines of code, at your installed django-registration module::to::Mine is at .venv/local/lib/python2.7/site-packages/registration/models.py (virtualenv)This works for me for Django 1.9 . The Python script to execute was in the root of the Django project.Set PROJECT_NAME and APP_NAME to yoursAnother option is that you have a duplicate entry in INSTALLED_APPS. That threw this error for two different apps I tested. Apparently it's not something Django checks for, but then who's silly enough to put the same app in the list twice. Me, that's who.Do you have a Python virtual environment that you need to enter before you run manage.py?I ran into this error myself, and that was the problem.I ran into this issue when I use djangocms and added a plugin (in my case: djangocms-cascade). Of course I had to add the plugin to the INSTALLED_APPS. But the order is here important.To place 'cmsplugin_cascade' before 'cms' solved the issue.install django-registration-redux==1.1 instead django-registration, if you using django 1.7./manage.py migrateThis solved my issueYour manage.py is "wrong"; I don't know where you got it from, but that's not a 1.7 manage.py - were you using some funky pre-release build or something?Reset your manage.py to the conventional, as below, and things Should Just Work:

Converting XML to JSON using Python?

Pete Karl II

[Converting XML to JSON using Python?](https://stackoverflow.com/questions/191536/converting-xml-to-json-using-python)

I've seen a fair share of ungainly XML->JSON code on the web, and having interacted with Stack's users for a bit, I'm convinced that this crowd can help more than the first few pages of Google results can.So, we're parsing a weather feed, and we need to populate weather widgets on a multitude of web sites.  We're looking now into Python-based solutions.This public weather.com RSS feed is a good example of what we'd be parsing (our actual weather.com feed contains additional information because of a partnership w/them).In a nutshell, how should we convert XML to JSON using Python?

2008-10-10 14:19:44Z

I've seen a fair share of ungainly XML->JSON code on the web, and having interacted with Stack's users for a bit, I'm convinced that this crowd can help more than the first few pages of Google results can.So, we're parsing a weather feed, and we need to populate weather widgets on a multitude of web sites.  We're looking now into Python-based solutions.This public weather.com RSS feed is a good example of what we'd be parsing (our actual weather.com feed contains additional information because of a partnership w/them).In a nutshell, how should we convert XML to JSON using Python?There is no "one-to-one" mapping between XML and JSON, so converting one to the other necessarily requires some understanding of what you want to do with the results.That being said, Python's standard library has several modules for parsing XML (including DOM, SAX, and ElementTree).  As of Python 2.6, support for converting Python data structures to and from JSON is included in the json module.So the infrastructure is there.xmltodict (full disclosure: I wrote it) can help you convert your XML to a dict+list+string structure, following this "standard". It is Expat-based, so it's very fast and doesn't need to load the whole XML tree in memory.Once you have that data structure, you can serialize it to JSON:You can use the xmljson library to convert using different XML JSON conventions.For example, this XML:translates via the BadgerFish convention into this:and via the GData convention into this (attributes are not supported):... and via the Parker convention into this (attributes are not supported):It's possible to convert from XML to JSON and from JSON to XML using the same

conventions:Disclosure: I wrote this library. Hope it helps future searchers.If some time you get only response code instead of all data then error like json parse will be there so u need to convert it as textHere's the code I built for that. There's no parsing of the contents, just plain conversion.There is a method to transport XML-based markup as JSON which allows it to be losslessly converted back to its original form. See http://jsonml.org/. It's a kind of XSLT of JSON. I hope you find it helpful You may want to have a look at http://designtheory.org/library/extrep/designdb-1.0.pdf. This project starts off with an XML to JSON conversion of a large library of XML files. There was much research done in the conversion, and the most simple intuitive XML -> JSON mapping was produced (it is described early in the document). In summary, convert everything to a JSON object, and put repeating blocks as a list of objects.objects meaning key/value pairs (dictionary in Python, hashmap in Java, object in JavaScript)There is no mapping back to XML to get an identical document, the reason is, it is unknown whether a key/value pair was an attribute or an <key>value</key>, therefore that information is lost. If you ask me, attributes are a hack to start; then again they worked well for HTML.Well, probably the simplest way is just parse the XML into dictionaries and then serialize that with simplejson. I'd suggest not going for a direct conversion. Convert XML to an object, then from the object to JSON.In my opinion, this gives a cleaner definition of how the XML and JSON correspond.It takes time to get right and you may even write tools to help you with generating some of it, but it would look roughly like this:To anyone that may still need this. Here's a newer, simple code to do this conversion.I found for simple XML snips, use regular expression would save troubles.  For example:To do it by XML parsing, as @Dan said, there is not one-for-all solution because the data is different. My suggestion is to use lxml. Although not finished to json, lxml.objectify give quiet good results:While the built-in libs for XML parsing are quite good I am partial to lxml.But for parsing RSS feeds, I'd recommend Universal Feed Parser, which can also parse Atom.

Its main advantage is that it can digest even most malformed feeds.Python 2.6 already includes a JSON parser, but a newer version with improved speed is available as simplejson.With these tools building your app shouldn't be that difficult.When I do anything with XML in python I almost always use the lxml package.  I suspect that most people use lxml.  You could use xmltodict but you will have to pay the penalty of parsing the XML again.To convert XML to json with lxml you:I use the following class in my projects.  Use the toJson method.The output from the built in main is:Which is a transformation of this xml:jsonpickle or if you're using feedparser, you can try feed_parser_to_json.pyMy answer addresses the specific (and somewhat common) case where you don't really need to convert the entire xml to json, but what you need is to traverse/access specific parts of the xml, and you need it to be fast, and simple (using json/dict-like operations).For this, it is important to note that parsing an xml to etree using lxml is super fast.  The slow part in most of the other answers is the second pass: traversing the etree structure (usually in python-land), converting it to json.Which leads me to the approach I found best for this case: parsing the xml using lxml, and then wrapping the etree nodes (lazily), providing them with a dict-like interface.Here's the code:This implementation is not complete, e.g., it doesn't cleanly support cases where an element has both text and attributes, or both text and children (only because I didn't need it when I wrote it...)  It should be easy to improve it, though.In my specific use case, where I needed to only process specific elements of the xml, this approach gave a suprising and striking speedup by a factor of 70 (!) compared to using @Martin Blech's xmltodict and then traversing the dict directly.As a bonus, since our structure is already dict-like, we get another alternative implementation of xml2json for free. We just need to pass our dict-like structure to json.dumps.  Something like:If your xml includes attributes, you'd need to use some alphanumeric attr_prefix (e.g. "ATTR_"), to ensure the keys are valid json keys.I haven't benchmarked this part.This stuff here is actively maintained and so far is my favorite: xml2json in pythoncheck out lxml2json (disclosure: I wrote it) https://github.com/rparelius/lxml2jsonit's very fast, lightweight (only requires lxml), and one advantage is that you have control over whether certain elements are converted to lists or dictsYou can use declxml. It has advanced features like multi attributes and complex nested support. You just need to write a simple processor for it. Also with the same code, you can convert back to JSON as well. It is fairly straightforward and the documentation is awesome.Link: https://declxml.readthedocs.io/en/latest/index.htmlPrepare data in Python:

To create JSON first you need to prepare data in python. We can use List and Dictionary in Python to prepare the data.Python List  <==> JSON ArrayPython Dictionary <==> JSON Object (Key Value Format) 

Check this for more detailshttps://devstudioonline.com/article/create-json-and-xml-in-pythonTo represent data in JSON format In json we repesent a data in key and value formatTo represent data in XML format 

How do I remove leading whitespace in Python?

James Wanchai

[How do I remove leading whitespace in Python?](https://stackoverflow.com/questions/959215/how-do-i-remove-leading-whitespace-in-python)

I have a text string that starts with a number of spaces, varying between 2 & 4.What is the simplest way to remove the leading whitespace? (ie. remove everything before a certain character?)

2009-06-06 07:55:12Z

I have a text string that starts with a number of spaces, varying between 2 & 4.What is the simplest way to remove the leading whitespace? (ie. remove everything before a certain character?)The lstrip() method will remove leading whitespaces, newline and tab characters on a string beginning:EditAs balpha pointed out in the comments, in order to remove only spaces from the beginning of the string, lstrip(' ') should be used:Related question:The function strip will remove whitespace from the beginning and end of a string.will set my_str to "text".If you want to cut the whitespaces before and behind the word, but keep the middle ones. 

You could use: To remove everything before a certain character, use a regular expression:to remove everything up to the first 'a'. [^a] can be replaced with any character class you like, such as word characters.The question doesn't address multiline strings, but here is how you would strip leading whitespace from a multiline string using python's standard library textwrap module. If we had a string like:if we print(s) we would get output like:and if we used textwrap.dedent:

How to run multiple Python versions on Windows

Bilal Basharat

[How to run multiple Python versions on Windows](https://stackoverflow.com/questions/4583367/how-to-run-multiple-python-versions-on-windows)

I had two versions of Python installed on my machine (versions 2.6 and 2.5). I want to run 2.6 for one project and 2.5 for another. How can I specify which I want to use?I am working on Windows XP SP2.

2011-01-03 09:30:33Z

I had two versions of Python installed on my machine (versions 2.6 and 2.5). I want to run 2.6 for one project and 2.5 for another. How can I specify which I want to use?I am working on Windows XP SP2.Running a different copy of Python is as easy as starting the correct executable. You mention that you've started a python instance, from the command line, by simply typing python. What this does under Windows, is to trawl the %PATH% environment variable, checking for an executable, either batch file (.bat), command file (.cmd) or some other executable to run, that matches the name given. When it finds the correct file to run, it does it.Now, if you've installed two python versions 2.5 and 2.6, the path will have both of their directories in it, something like PATH=c:\python\2.5;c:\python\2.6 but Windows will stop examining the path when it finds a match.What you really need to do is to explicitly call one or both of the applications, such as c:\python\2.5\python.exe or c:\python\2.6\python.exe.The other alternative is to create a shortcut to the respective python.exe calling one of them python25 and the other python26; you can then simply run python25 on your command line.Adding two more solutions to the problem:#! c:\[path to Python 2.5]\python.exe - for scripts you want to be run with Python 2.5

#! c:\[path to Python 2.6]\python.exe - for scripts you want to be run with Python 2.6or instead of running python command run pylauncher command (py) specyfing which version of Python you want;py -2.6 – version 2.6

py -2 – latest installed version 2.x

py -3.4 – version 3.4

py -3 – latest installed version 3.x  virtualenv -p c:\[path to Python 2.5]\python.exe [path where you want to have virtualenv using Python 2.5 created]\[name of virtualenv]virtualenv -p c:\[path to Python 2.6]\python.exe [path where you want to have virtualenv using Python 2.6 created]\[name of virtualenv]for examplevirtualenv -p c:\python2.5\python.exe c:\venvs\2.5virtualenv -p c:\python2.6\python.exe c:\venvs\2.6then you can activate the first and work with Python 2.5 like this

c:\venvs\2.5\activate

and when you want to switch to Python 2.6 you do  From Python 3.3 on, there is the official Python launcher for Windows (http://www.python.org/dev/peps/pep-0397/). Now, you can use the #!pythonX to determine the wanted version of the interpreter also on Windows. See more details in my another comment or read the PEP 397.Summary: The py script.py launches the Python version stated in #! or Python 2 if #! is missing. The py -3 script.py launches the Python 3.As per @alexander you can make a set of symbolic links like below.  Put them somewhere which is included in your path so they can be easily invokedAs long as c:\bin or where ever you placed them in is in your path you can now goFor example for 3.6 version type py -3.6. 

 If you have also 32bit and 64bit versions, you can just type py -3.6-64 or py -3.6-32.When you install Python, it will not overwrite other installs of other major versions. So installing Python 2.5.x will not overwrite Python 2.6.x, although installing 2.6.6 will overwrite 2.6.5.So you can just install it. Then you call the Python version you want. For example:for Python 2.5 on windows andfor Python 2.6 on windows, or or on Windows Unix (including Linux and OS X).When you install on Unix (including Linux and OS X) you will get a generic python command installed, which will be the last one you installed. This is mostly not a problem as most scripts will explicitly call /usr/local/bin/python2.5 or something just to protect against that. But if you don't want to do that, and you probably don't you can install it like this:Note the "altinstall" that means it will install it, but it will not replace the python command.On Windows you don't get a global python command as far as I know so that's not an issue.Here's a quick hack:I strongly recommend the pyenv-win project.Thanks to kirankotari's work, now we have a Windows version of pyenv.cp c:\python27\bin\python.exe as python2.7.execp c:\python34\bin\python.exe as python3.4.exethey are all in the system path, choose the version you want to runUsing a batch file to switch, easy and efficient on windows 7. I use this:In the environment variable dialog (C:\Windows\System32\SystemPropertiesAdvanced.exe),In the section user variablesIn the section system variablesI created batch files for every python installation (exmple for 3.4 x64Name =          SetPathPython34x64  !!!  ToExecuteAsAdmin.bat     ;-) just to remember.Content of the file = To switch between versions, I execute the batch file in admin mode.  !!!!!  The changes are effective for the SUBSEQUENT command prompt windows OPENED. !!!So I have exact control on it.The easiest way to run multiple versions of python on windows is described below as follows:-1)Download the latest versions of python from python.org/downloads by selecting the relevant version for your system.2)Run the installer and select Add python 3.x to the path to set path automatically in python 3 (you just have to click the checkbox). For python 2 open up your python 2 installer, select whatever preferences you want but just remember to set Add python.exe to path to Will be installed on local hard drive, Now just click next and wait for the installer to finish.3)When both the installations are complete. Right click on my computer--Go to properties--Select advanced system settings--Go to environment variables--Click on new under System variables and add a new system variable with variable name as PY_PYTHON and set this variable value to 3. Now click on OK and you should be done.4)Now to test this open the command prompt. Once you are in there type python or py, It should open up python3.5)Now exit out of python3 by typing exit(). Now type py -2 it should open python 2.If none of this works then restart the computer and if the problem still persists then uninstall everything and repeat the steps.Thanks.You can create different python development environments graphically from Anaconda Navigator. 

I had same problem while working with different python versions so I used anaconda navigator to create different python development environments and used different python versions in each environments.Here is the help documentation for this.https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/Using the Rapid Environment Editor  you can push to the top the directory of the desired Python installation. For example, to start python from the c:\Python27 directory, ensure that c:\Python27 directory is before or on top of the c:\Python36 directory in the Path environment variable. From my experience, the first python executable found in the Path environment is being executed. For example, I have MSYS2 installed with Python27 and since I've added C:\MSYS2 to the path before C:\Python36, the python.exe from the C:\MSYS2.... folder is being executed.Just call the correct executable

'str' object has no attribute 'decode'. Python 3 error?

Martijn Pieters

['str' object has no attribute 'decode'. Python 3 error?](https://stackoverflow.com/questions/28583565/str-object-has-no-attribute-decode-python-3-error)

Here is my code:at this point I get the error message Python 3 doesn't have decode anymore, am I right? how can I fix this? Also, in:I am selecting only the 1st email. How do I select all?

2015-02-18 12:20:05Z

Here is my code:at this point I get the error message Python 3 doesn't have decode anymore, am I right? how can I fix this? Also, in:I am selecting only the 1st email. How do I select all?You are trying to decode an object that is already decoded. You have a str, there is no need to decode from UTF-8 anymore.Simply drop the .decode('utf-8') part:As for your fetch() call, you are explicitly asking for just the first message. Use a range if you want to retrieve more messages. See the documentation:Begin with Python 3, all string is unicode object.the code before are same. So I think you should remove the .decode('utf-8'). Because you have already get the unicode object.Use it by this Method:I'm not familiar with the library, but if your problem is that you don't want a byte array, one easy way is to specify an encoding type straight in a cast:For Python3It s already decoded in Python3, Try directly it should work.

Python initializing a list of lists [duplicate]

Amir

[Python initializing a list of lists [duplicate]](https://stackoverflow.com/questions/12791501/python-initializing-a-list-of-lists)

I intend to initialize a list of list with length of n.However, this somehow links the lists together.I expect to have something like: Any ideas?

2012-10-09 01:02:00Z

I intend to initialize a list of list with length of n.However, this somehow links the lists together.I expect to have something like: Any ideas?The problem is that they're all the same exact list in memory. When you use the [x]*n syntax, what you get is a list of n many x objects, but they're all references to the same object. They're not distinct instances, rather, just n references to the same instance.To make a list of 3 different lists, do this:This gives you 3 separate instances of [], which is what you want[[]]*n is similar toWhile [[] for i in range(3)] is similar to:

Convert a python UTC datetime to a local datetime using only python standard library?

Nitro Zark

[Convert a python UTC datetime to a local datetime using only python standard library?](https://stackoverflow.com/questions/4563272/convert-a-python-utc-datetime-to-a-local-datetime-using-only-python-standard-lib)

I have a python datetime instance that was created using datetime.utcnow() and persisted in database.For display, I would like to convert the datetime instance retrieved from the database to local datetime using the default local timezone (i.e., as if the datetime was created using datetime.now()).How can I convert the UTC datetime to a local datetime using only python standard library (e.g., no pytz dependency)?It seems one solution would be to use datetime.astimezone( tz ), but how would you get the default local timezone?

2010-12-30 14:14:41Z

I have a python datetime instance that was created using datetime.utcnow() and persisted in database.For display, I would like to convert the datetime instance retrieved from the database to local datetime using the default local timezone (i.e., as if the datetime was created using datetime.now()).How can I convert the UTC datetime to a local datetime using only python standard library (e.g., no pytz dependency)?It seems one solution would be to use datetime.astimezone( tz ), but how would you get the default local timezone?In Python 3.3+:In Python 2/3:Using pytz (both Python 2/3):Note: it takes into account DST and the recent change of utc offset for MSK timezone.I don't know whether non-pytz solutions work on Windows.You can't do it with only the standard library as the standard library doesn't have any timezones. You need pytz or dateutil.Or well, you can do it without pytz or dateutil by implementing your own timezones. But that would be silly.You can't do it with standard library. Using  pytz module you can convert any naive/aware datetime object to any other  time zone. Lets see some examples using Python 3.To convert a naive object to any other time zone, first you  have to convert  it into aware datetime object. You can use the replace method for converting a naive datetime object to an aware datetime object.  Then to convert an aware datetime object to any other timezone you can use astimezone method.The variable pytz.all_timezones gives you the list of all available time zones in pytz module.Because now method returns current date and time, so you have to make the datetime object timezone aware first. The localize  function  converts a naive datetime object into a timezone-aware datetime object. Then you can use the astimezone method to convert it into another timezone.I think I figured it out: computes number of seconds since epoch, then converts to a local timzeone using time.localtime, and then converts the time struct back into a datetime...It applies the summer/winter DST correctly:The standard Python library does not come with any tzinfo implementations at all. I've always considered this a surprising shortcoming of the datetime module.The documentation for the tzinfo class does come with some useful examples. Look for the large code block at the end of the section.Building on Alexei's comment. This should work for DST too.A simple (but maybe flawed) way that works in Python 2 and 3:Its advantage is that it's trivial to write an inverse functionThe easiest way I have found is to get the time offset of where you are, then subtract that from the hour.This works for me, in Python 3.5.2.Here is another way to change timezone in datetime format (I know I wasted my energy on this but I didn't see this page so I don't know how) without min. and sec. cause I don't need it for my project:This is a terrible way to do it but it avoids creating a definition.  It fulfills the requirement to stick with the basic Python3 library.Convert and revert works with timestamp when replacing timezone in Python3 , my datetime objects is timezone aware only import from standard datetime, may be hassle free rewritten for Python2!Test result.Use timedelta to switch between timezones. All you need is the offset in hours between timezones. Don't have to fiddle with boundaries for all 6 elements of a datetime object. timedelta handles leap years, leap centuries, etc., too, with ease. You must first  Then if offset is the timezone delta in hours:   timeout = timein + timedelta(hours = offset)  where timein and timeout are datetime objects. e.g.   timein + timedelta(hours = -8)  converts from GMT to PST.So, how to determine offset? Here is a simple function provided you only have a few possibilities for conversion without using datetime objects that are timezone "aware" which some other answers nicely do. A bit manual, but sometimes clarity is best.After looking at the numerous answers and playing around with the tightest code I can think of (for now) it seems best that all applications, where time is important and mixed timezones must be accounted for, should make a real effort to make all datetime objects "aware". Then it would seem the simplest answer is:to convert to GMT for example. Of course, to convert to/from any other timezone you wish, local or otherwise, just use the appropriate timezone string that pytz understands (from pytz.all_timezones). Daylight savings time is then also taken into account.

Asserting successive calls to a mock method

Jonathan

[Asserting successive calls to a mock method](https://stackoverflow.com/questions/7242433/asserting-successive-calls-to-a-mock-method)

Mock has a helpful assert_called_with() method. However, as far as I understand this only checks the last call to a method.

If I have code that calls the mocked method 3 times successively, each time with different parameters, how can I assert these 3 calls with their specific parameters?

2011-08-30 11:29:34Z

Mock has a helpful assert_called_with() method. However, as far as I understand this only checks the last call to a method.

If I have code that calls the mocked method 3 times successively, each time with different parameters, how can I assert these 3 calls with their specific parameters?assert_has_calls is another approach to this problem.From the docs:Example:Source: https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_has_callsUsually, I don't care about the order of the calls, only that they happened. In that case, I  combine assert_any_call with an assertion about call_count.I find doing it this way to be easier to read and understand than a large list of calls passed into a single method.If you do care about order or you expect multiple identical calls, assert_has_calls might be more appropriate.Since I posted this answer, I've rethought my approach to testing in general. I think it's worth mentioning that if your test is getting this complicated, you may be testing inappropriately or have a design problem. Mocks are designed for testing inter-object communication in an object oriented design. If your design is not objected oriented (as in more procedural or functional), the mock may be totally inappropriate. You may also have too much going on inside the method, or you might be testing internal details that are best left unmocked. I developed the strategy mentioned in this method when my code was not very object oriented, and I believe I was also testing internal details that would have been best left unmocked.You can use the Mock.call_args_list attribute to compare parameters to previous method calls. That in conjunction with Mock.call_count attribute should give you full control.I always have to look this one up time and time again, so here is my answer.  Suppose we have a heavy duty class (which we want to mock): here is some code that uses two instances of the  HeavyDuty class: Now, here is a test case for the heavy_work function:We are mocking the HeavyDuty class with MockHeavyDuty. To assert method calls coming from every HeavyDuty instance we have to refer to MockHeavyDuty.return_value.assert_has_calls,  instead of MockHeavyDuty.assert_has_calls.   In addition, in the list of expected_calls we have to specify which method name we are interested in asserting calls for.  So our list is made of calls to call.do_work, as opposed to simply call. Exercising the test case shows us it is successful:If we modify the heavy_work function, the test fails and produces a helpful error message:To contrast with the above, here is an example that shows how to mock multiple calls to a function: 

There are two main differences.  The first one is that when mocking a function we setup our expected calls using call,  instead of using call.some_method.  The second one is that we call assert_has_calls on mock_work_function, instead of on mock_work_function.return_value.

How can I disable logging while running unit tests in Python Django?

shreddd

[How can I disable logging while running unit tests in Python Django?](https://stackoverflow.com/questions/5255657/how-can-i-disable-logging-while-running-unit-tests-in-python-django)

I am using a simple unit test based test runner to test my Django application.My application itself is configured to use a basic logger in settings.py using:And in my application code using:However, when running unittests, I'd like to disable logging so that it doesn't clutter my test result output. Is there a simple way to turn off logging in a global way, so that the application specific loggers aren't writing stuff out to the console when I run tests?

2011-03-10 04:59:55Z

I am using a simple unit test based test runner to test my Django application.My application itself is configured to use a basic logger in settings.py using:And in my application code using:However, when running unittests, I'd like to disable logging so that it doesn't clutter my test result output. Is there a simple way to turn off logging in a global way, so that the application specific loggers aren't writing stuff out to the console when I run tests?will disable all logging calls with levels less severe than or equal to CRITICAL. Logging can be re-enabled withSince you are in Django, you could add these lines to your settings.py:That way you don't have to add that line in every setUp() on your tests.You could also do a couple of handy changes for your test needs this way.There is another "nicer" or "cleaner" way to add specifics to your tests and that is making your own test runner.Just create a class like this:And now add to your settings.py file:This lets you do one really handy modification that the other approach doesn't, which is to make Django just tests the applications that you want. You can do that by changing the test_labels adding this line to the test runner:The other answers prevent "writing stuff out to the console" by globally setting the logging infrastructure to ignore anything. This works but I find it too blunt an approach. My approach is to perform a configuration change which does only what's needed to prevent logs to get out on the console. So I add a custom logging filter to my settings.py:And I configure the Django logging to use the filter:End result: when I'm testing, nothing goes to the console, but everything else stays the same.I design code that contains logging instructions that are triggered only in specific circumstances and that should output the exact data I need for diagnosis if things go wrong. Therefore I test that they do what they are supposed to do and thus completely disabling logging is not viable for me. I don't want to find once the software is in production that what I thought would be logged is not logged.Moreover, some test runners (Nose, for instance) will capture logs during testing and output the relevant part of the log together with a test failure. It is useful in figuring out why a test failed. If logging is completely turned off, then there's nothing that can be captured.I like Hassek's custom test runner idea.  It should be noted that DjangoTestSuiteRunner is no longer the default test runner in Django 1.6+, it has been replaced by the DiscoverRunner.  For default behaviour, the test runner should be more like:I've found that for tests within unittest or similar a framework, the most effective way to safely disable unwanted logging in unit tests is to enable/disable in the setUp/tearDown methods of a particular test case.  This lets one target specifically where logs should be disabled.  You could also do this explicitly on the logger of the class you're testing.There is some pretty and clean method to suspend logging in tests with unittest.mock.patch method.foo.py:

tests.py:

And python3 -m unittest tests will produce no logging output.I am using a simple method decorator to disable logging only in a particular test method.And then I use it as in the following example:Sometimes you want the logs and sometimes not. I have this code in my settings.pySo if you run your test with the --no-logs options you'll get only the critical logs:It's very helpful if you want speedup the tests on your continuous integration flow.If you don't want it repeatedly turn it on/off in setUp() and tearDown() for unittest (don't see the reason for that), you could just do it once per class:In cases where I wish to temporarily suppress a specific logger, I've written a little context manager that I've found useful:You then use it like:This has the advantage that the logger is re-enabled (or set back to its prior state) once the with completes.In my case I have a settings file  settings/test.py created specifically for testing purposes, here's what it looks like:I put an environment variable DJANGO_SETTINGS_MODULE=settings.test to /etc/environment.If you have different initaliser modules for test, dev and production then you can disable anything or redirect it in the initialser. I have local.py, test.py and production.py that all inherit from common.ycommon.py does all the main config including this snippet :Then in test.py I have this:This replaces the console handler with a FileHandler and means still get logging but I do not have to touch the production code base.If you're using pytest:Since pytest captures log messages and only displays them for failed tests, you typically don't want to disable any logging. Instead, use a separate settings.py file for tests (e.g., test_settings.py), and add to it:This tells Django to skip configuring the logging altogether. The LOGGING setting will be ignored and can be removed from the settings.With this approach, you don't get any logging for passed tests, and you get all available logging for failed tests. The tests will run using the logging that was set up by pytest. It can be configured to your liking in the pytest settings (e.g., tox.ini). To include debug level log messages, use log_level = DEBUG (or the corresponding command line argument).

How do I get PyLint to recognize numpy members?

Alphadelta14

[How do I get PyLint to recognize numpy members?](https://stackoverflow.com/questions/20553551/how-do-i-get-pylint-to-recognize-numpy-members)

I am running PyLint on a Python project. PyLint makes many complaints about being unable to find numpy members. How can I avoid this while avoiding skipping membership checks.From the code:Which, when ran, I get the expected:However, pylint gives me this error:For versions, I am using pylint 1.0.0 (astroid 1.0.1, common 0.60.0) and trying to work with numpy 1.8.0 .

2013-12-12 20:27:12Z

I am running PyLint on a Python project. PyLint makes many complaints about being unable to find numpy members. How can I avoid this while avoiding skipping membership checks.From the code:Which, when ran, I get the expected:However, pylint gives me this error:For versions, I am using pylint 1.0.0 (astroid 1.0.1, common 0.60.0) and trying to work with numpy 1.8.0 .If using Visual Studio Code with Don Jayamanne's excellent Python extension, add a user setting to whitelist numpy:I had the same issue here, even with the latest versions of all related packages (astroid 1.3.2, logilab_common 0.63.2, pylon 1.4.0).The following solution worked like a charm: I added numpy to the list of ignored modules by modifying my pylintrc file, in the [TYPECHECK] section:Depending on the error, you might also need to add the following line (still in the [TYPECHECK] section):I was getting the same error for a small numpy project I was working on and decided that ignoring the numpy modules would do just fine. I created a .pylintrc file with:$ pylint --generate-rcfile > ~/.pylintrcand following paduwan's and j_houg's advice I modified the following sectors:andand it "fixed" my issue.In recent versions of pylint you can add --extension-pkg-whitelist=numpy to your pylint command. They had fixed this problem in an earlier version in an unsafe way. Now if you want them to look more carefully at a package outside of the standard library, you must explicitly whitelist it. See here.Since this is the top result in google and it gave me the impression that you have to ignore those warnings in all files:The problem has actually been fixed in the sources of pylint/astroid last month https://bitbucket.org/logilab/astroid/commits/83d78af4866be5818f193360c78185e1008fd29e

but are not yet in the Ubuntu packages. To get the sources, justwhereby the last step will most likely require a sudo and of course you need mercurial to clone.For ignoring all the errors generated by numpy.core‘s attributes, we can now use:As another solution, add this option to ~/.pylintrc or /etc/pylintrc file:For mentioned in question code by now this seems reduntant, but still matters for another modules, ie. netifaces and etc.If you don't want to add more config, please add this code to your config file, instead of 'whitelist'.There have been many different bugs reported about this over the past few years i.e. https://bitbucket.org/logilab/pylint/issue/58/false-positive-no-member-on-numpy-importsI'd suggest disabling for the lines where the complaints occur.Probably, it's confused with numpy's abstruse method of methods import. Namely, zeros is in fact numpy.core.multiarray.zeros, imported in numpy with statementin turn imported withand in numeric you'll findI guess I would be confused in place of PyLint! See this bug for PyLint side of view.In Extension to j_hougs answer, you can now add the modules in question to this line in .pylintrc, which is already prepared empty on generation:you can generate a sample .pylintrc by doing:and then edit the mentioned lineThis has finally been resolved in Pylint 1.8.2. Works out of the box, no pylintrc tweaks needed!I had to add this at the top of any file where I use numpy a lot. Just in case someone in eclipse is having trouble with Pydev and pylint...This is the pseudo-solution I have come up with for this problem.Then, in your code, instead of calling numpy functions as np.array and np.zeros and so on, 

you would write np_array, np_zeros, etc. 

Advantages of this approach vs. other approaches suggested in other answers:The clear disadvantage is that you have to explicitely import every numpy function you use. 

The approach could be elaborated on further. 

You could define your own module, call it say, numpy_importer as followsThen, your application code could import this module only (instead of numpy) as and use the names as usual: np.zeros, np.array etc.The advantage of this is that you will have a single module in which all numpy related imports are done once and for all, and then you import it with that single line, wherever you want. Still you have to be careful that numpy_importer does not import names that don´t exist in numpy as those errors won't be caught by pylint.  This seems to work on at least Pylint 1.1.0:I had this problem with numpy, scipy, sklearn, nipy, etc., and I solved it by wrapping epylint like so:$ cat epylint.pyThis script simply runs epylint, then scrapes its output to filter out false-positive warnings and errors. You can extend it by added more elif cases.N.B.: If this applies to you, then you'll want to modify your pychechers.sh so it likes like this(Of course, you have to make epylint.py executable first)Here is a link to my .emacs https://github.com/dohmatob/mydotemacs. Hope this is useful to someone.I had the same problem with a different module (kivy.properties) which is a wrapped C module like numpy.Using VSCode V1.38.0, the accepted solution stopped all linting for the project.  So, while it did indeed remove the false-positive no-name-in-module, it didn't really improve the situation.The best workaround for me was to use the --ignored-modules argument on the offending module.  Trouble is, passing any argument via python.linting.pylintArgs wipes out the default VSCode settings, so you need to re-set those also.  That left me with the following settings.json file:A little bit of copy paste from the previous answer to summarize what is working (at least for me: debian-jessie)Basic command to run:

    # ONLY if you do not already have a .pylintrc file in your home

    $ pylint --generate-rcfile > .pylintrcThen open the file and add the packages you want after extension-pkg-whitelist= separated by comma. You can have the same behavior using the option --extension-pkg-whitelist=numpy from the command line.If you ignore some packages in the [TYPECHECK] section that means that pylint will never show error related to that packages. In practice, pylint will not tell you anything about those packages.This solution worked for me Basically, go to Select the gear icon from bottom left=>Setting=>Workspace Setting =>Extension=>Python Configuration=>Click on any Settings.json => add this in the file

"python.linting.pylintArgs" : [ "--extension-pkg-whitelist=numpy" ]

I am using VS 1.27.2I've been working on a patch to pylint to solve the issue with dynamic members in libraries such as numpy. It adds a "dynamic-modules" option which forces to check if members exist during runtime by making a real import of the module. See Issue #413 in logilab/pylint. There is also a pull request, see link in one of the comments.A quick answer: update Pylint to 1.7.1 (use conda-forge provided Pylint 1.7.1 if you use conda to manage packages)I found a similar issue in pylint GitHub here and someone replied everything getting OK after updating to 1.7.1.I'm not sure if this is a solution, but in VSCode once I wrote explicitly in my user settings to enable pylint, all modules were recognized.Lately (since something changed in spyder or pylint or ?), I have been getting E1101 errors ("no member") from spyder's static code analysis on astropy.constants symbols.  No idea why.My simplistic solution for all users on a Linux or Unix system (Mac is probably similar) is to create an /etc/pylintrc as follows:Of course, this could, instead, be put in a personal $HOME/.pylintrc file.

And, I could have updated an existing file.

How to send an email with Python?

cloud311

[How to send an email with Python?](https://stackoverflow.com/questions/6270782/how-to-send-an-email-with-python)

This code works and sends me an email just fine:  However if I try to wrap it in a function like this:and call it I get the following errors:  Can anyone help me understand why?  

2011-06-07 19:48:05Z

This code works and sends me an email just fine:  However if I try to wrap it in a function like this:and call it I get the following errors:  Can anyone help me understand why?  I recommend that you use the standard packages email and smtplib together to send email. Please look at the following example (reproduced from the Python documentation). Notice that if you follow this approach, the "simple" task is indeed simple, and the more complex tasks (like attaching binary objects or sending plain/HTML multipart messages) are accomplished very rapidly.For sending email to multiple destinations, you can also follow the example in the Python documentation:As you can see, the header To in the MIMEText object must be a string consisting of email addresses separated by commas. On the other hand, the second argument to the sendmail function must be a list of strings (each string is an email address).So, if you have three email addresses: person1@example.com, person2@example.com, and person3@example.com, you can do as follows (obvious sections omitted):the ",".join (to) part makes a single string out of the list, separated by commas.From your questions I gather that you have not gone through the Python tutorial - it is a MUST if you want to get anywhere in Python - the documentation is mostly excellent for the standard library.Well, you want to have an answer that is up-to-date and modern.Here is my answer:When I need to mail in python, I use the mailgun API wich get's a lot of the headaches with sending mails sorted out. They have a wonderfull app/api that allows you to send 10,000 emails per month for free.Sending an email would be like this:You can also track events and lots more, see the quickstart guide.I hope you find this useful!I'd like to help you with sending emails by advising the yagmail package (I'm the maintainer, sorry for the advertising, but I feel it can really help!). The whole code for you would be:Note that I provide defaults for all arguments, for example if you want to send to yourself, you can omit TO, if you don't want a subject, you can omit it also.Furthermore, the goal is also to make it really easy to attach html code or images (and other files).Where you put contents you can do something like:Wow, how easy it is to send attachments! This would take like 20 lines without yagmail ;) Also, if you set it up once, you'll never have to enter the password again (and have it safely stored). In your case you can do something like:which is much more concise!I'd invite you to have a look at the github or install it directly with pip install yagmail.There is indentation problem. The code below will work:Here is an example on Python 3.x, much simpler than 2.x:call this function:If you use 126/163, 网易邮箱，you need to set"客户端授权密码", like below:ref: https://stackoverflow.com/a/41470149/2803344

https://docs.python.org/3/library/email.examples.html#email-examplesWhile indenting your code in the function (which is ok), you did also indent the lines of the raw message string. But leading white space implies folding (concatenation) of the header lines, as described in sections 2.2.3 and 3.2.3 of RFC 2822 - Internet Message Format:In the function form of your sendmail call, all lines are starting with white space and so are "unfolded" (concatenated) and you are trying to sendOther than our mind suggests, smtplib will not understand the To: and Subject: headers any longer, because these names are only recognized at the beginning of a line. Instead smtplib will assume a very long sender email address: This won't work and so comes your Exception.The solution is simple: Just preserve the message string as it was before.  This can be done by a function (as Zeeshan suggested) or right away in the source code:Now the unfolding does not occur and you sendwhich is what works and what was done by your old code.Note that I was also preserving the empty line between headers and body to accommodate section 3.5 of the RFC (which is required) and put the include outside the function according to the Python style guide PEP-0008 (which is optional).It's probably putting tabs into your message. Print out message before you pass it to sendMail.Make sure you have granted permission for both Sender and Receiver to send email and receive email from Unknown sources(External Sources) in Email Account.As far your code is concerned, there doesn't seem to be anything fundamentally wrong with it except that, it is unclear how you're actually calling that function. All I can think of is that when your server is not responding then you will get this SMTPServerDisconnected error. If you lookup the getreply() function in smtplib (excerpt below), you will get an idea.check an example at https://github.com/rreddy80/sendEmails/blob/master/sendEmailAttachments.py that also uses a function call to send an email, if that's what you're trying to do (DRY approach).Thought I'd put in my two bits here since I have just figured out how this works. It appears that you don't have the port specified on your SERVER connection settings, this effected me a little bit when I was trying to connect to my SMTP server that isn't using the default port: 25. According to the smtplib.SMTP docs, your ehlo or helo request/response should automatically be taken care of, so you shouldn't have to worry about this (but might be something to confirm if all else fails).Another thing to ask yourself is have you allowed SMTP connections on your SMTP server itself? For some sites like GMAIL and ZOHO you have to actually go in and activate the IMAP connections within the email account. Your mail server might not allow SMTP connections that don't come from 'localhost' perhaps? Something to look into. The final thing is you might want to try and initiate the connection on TLS. Most servers now require this type of authentication.You'll see I've jammed two TO fields into my email. The msg['TO'] and msg['FROM'] msg dictionary items allows the correct information to show up in the headers of the email itself, which one sees on the receiving end of the email in the To/From fields (you might even be able to add a Reply To field in here. The TO and FROM fields themselves are what the server requires. I know I've heard of some email servers rejecting emails if they don't have the proper email headers in place. This is the code I've used, in a function, that works for me to email the content of a *.txt file using my local computer and a remote SMTP server (ZOHO as shown): 

Is there a label/goto in Python?

user46646

[Is there a label/goto in Python?](https://stackoverflow.com/questions/438844/is-there-a-label-goto-in-python)

Is there a goto or any equivalent in Python to be able to jump to a specific line of code?

2009-01-13 12:53:23Z

Is there a goto or any equivalent in Python to be able to jump to a specific line of code?No, Python does not support labels and goto, if that is what you're after. It's a (highly) structured programming language.Python offers you the ability to do some of the things you could do with a goto using first class functions.  For example:Could be done in python like this:Granted, that isn't the best way to substitute for goto.  But without knowing exactly what you're trying to do with the goto, it's hard to give specific advice.@ascobol:Your best bet is to either enclose it in a function or use an exception.  For the function:For the exception:Using exceptions to do stuff like this may feel a bit awkward if you come from another programming language.  But I would argue that if you dislike using exceptions, Python isn't the language for you.  :-)I recently wrote a function decorator that enables goto in Python, just like that:I'm not sure why one would like to do something like that though. That said, I'm not too serious about it. But I'd like to point out that this kind of meta programming is actual possible in Python, at least in CPython and PyPy, and not only by misusing the debugger API as that other guy did. You have to mess with the bytecode though.I found this in the official python Design and History FAQ.It's very nice that this is even mentioned in the official FAQ, and that a nice solution sample is provided. I really like python because its community is treating even goto like this ;)To answer the @ascobol's question using @bobince's suggestion from the comments:The indent for the else block is correct. The code uses obscure else after a loop Python syntax. See Why does python use 'else' after for and while loops?A working version has been made: http://entrian.com/goto/.Note: It was offered as an April Fool's joke. (working though)Needless to say. Yes its funny, but DONT use it.Labels for break and continue were proposed in PEP 3136 back in 2007, but it was rejected.  The Motivation section of the proposal illustrates several common (if inelegant) methods for imitating labeled break in Python.It is technically feasible to add a 'goto' like statement to python with some work. We will use the "dis" and "new" modules, both very useful for scanning and modifying python byte code.The main idea behind the implementation is to first mark a block of code as using "goto" and "label" statements. A special "@goto" decorator will be used for the purpose of marking "goto" functions. Afterwards we scan that code for these two statements and apply the necessary modifications to the underlying byte code. This all happens at source code compile time.Hope this answers the question.you can use User-defined Exceptions to emulate gotoexample:I was looking for some thing similar toSo my approach was to use a boolean to help breaking out from the nested for loops:Link: goto-statementfoo.pyThere is now. gotoI think this might be useful for what you are looking for.I wanted the same answer and I didnt want to use goto. So I used the following example (from learnpythonthehardway)I have my own way of doing gotos.

I use separate python scripts.If I want to loop:file1.py file2.pyfile3.py(NOTE: This technique only works on Python 2.x versions)For a forward Goto, you could just add:This only helps for simple scenarios though (i.e. nesting these would get you into a mess)In lieu of a python goto equivalent I use the break statement in the following fashion for quick tests of my code. This assumes you have structured code base. The test variable is initialized at the start of your function and I just move the "If test: break" block to the end of the nested if-then block or loop I want to test, modifying the return variable at the end of the code to reflect the block or loop variable I'm testing.Though there isn't any code equivalent to goto/label in Python, you could still get such functionality of goto/label using loops.  Lets take a code sample shown below where goto/label can be used in a arbitrary language other than python.Now the same functionality of the above code sample can be achieved in python by using a while loop as shown below.  no there is an alternative way to implement goto statement

pandas loc vs. iloc vs. ix vs. at vs. iat?

scribbles

[pandas loc vs. iloc vs. ix vs. at vs. iat?](https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-ix-vs-at-vs-iat)

Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in Pandas. I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options. 

2015-02-27 04:12:39Z

Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in Pandas. I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options. loc: only work on index

iloc: work on position

ix: You can get data from dataframe without it being in the index

at: get scalar values. It's a very fast loc

iat: Get scalar values. It's a very fast ilochttp://pyciencia.blogspot.com/2015/05/obtener-y-filtrar-datos-de-un-dataframe.htmlNote: As of pandas 0.20.0, the .ix indexer is deprecated in favour of the more strict .iloc and .loc indexers.Updated for pandas 0.20 given that ix is deprecated.  This demonstrates not only how to use loc, iloc, at, iat, set_value, but how to accomplish, mixed positional/label based indexing.loc - label based

Allows you to pass 1-D arrays as indexers.  Arrays can be either slices (subsets) of the index or column, or they can be boolean arrays which are equal in length to the index or columns.  Special Note: when a scalar indexer is passed, loc can assign a new index or column value that didn't exist before.iloc - position based

Similar to loc except with positions rather that index values.  However, you cannot assign new columns or indices.at - label based

Works very similar to loc for scalar indexers.  Cannot operate on array indexers.  Can! assign new indices and columns.  Advantage over loc is that this is faster.

Disadvantage is that you can't use arrays for indexers.iat - position based

Works similarly to iloc.  Cannot work in array indexers.  Cannot! assign new indices and columns.Advantage over iloc is that this is faster.

Disadvantage is that you can't use arrays for indexers.set_value - label based

Works very similar to loc for scalar indexers.  Cannot operate on array indexers.  Can! assign new indices and columnsAdvantage Super fast, because there is very little overhead!

Disadvantage There is very little overhead because pandas is not doing a bunch of safety checks.  Use at your own risk.  Also, this is not intended for public use.set_value with takable=True - position based

Works similarly to iloc.  Cannot work in array indexers.  Cannot! assign new indices and columns.Advantage Super fast, because there is very little overhead!

Disadvantage There is very little overhead because pandas is not doing a bunch of safety checks.  Use at your own risk.  Also, this is not intended for public use.There are two primary ways that pandas makes selections from a DataFrame.The documentation uses the term position for referring to integer location. I do not like this terminology as I feel it is confusing. Integer location is more descriptive and is exactly what .iloc stands for. The key word here is INTEGER - you must use integers when selecting by integer location.Before showing the summary let's all make sure that ...There are three primary indexers for pandas. We have the indexing operator itself (the brackets []), .loc, and .iloc. Let's summarize them:I almost never use .at or .iat as they add no additional functionality and with just a small performance increase. I would discourage their use unless you have a very time-sensitive application. Regardless, we have their summary:In addition to selection by label and integer location, boolean selection also known as boolean indexing exists. We will first focus on the differences between .loc and .iloc. Before we talk about the differences, it is important to understand that DataFrames have labels that help identify each column and each row. Let's take a look at a sample DataFrame:All the words in bold are the labels. The labels, age, color, food, height, score and state are used for the columns. The other labels, Jane, Nick, Aaron, Penelope, Dean, Christina, Cornelia are used as labels for the rows. Collectively, these row labels are known as the index.The primary ways to select particular rows in a DataFrame are with the .loc and .iloc indexers. Each of these indexers can also be used to simultaneously select columns but it is easier to just focus on rows for now. Also, each of the indexers use a set of brackets that immediately follow their name to make their selections.We will first talk about the .loc indexer which only selects data by the index or column labels. In our sample DataFrame, we have provided meaningful names as values for the index. Many DataFrames will not have any meaningful names and will instead, default to just the integers from 0 to n-1, where n is the length(number of rows) of the DataFrame.There are many different inputs you can use for .loc three out of them are  Selecting a single row with .loc with a stringTo select a single row of data, place the index label inside of the brackets following .loc.This returns the row of data as a Series  Selecting multiple rows with .loc with a list of stringsThis returns a DataFrame with the rows in the order specified in the list:Selecting multiple rows with .loc with slice notationSlice notation is defined by a start, stop and step values. When slicing by label, pandas includes the stop value in the return. The following slices from Aaron to Dean, inclusive. Its step size is not explicitly defined but defaulted to 1.Complex slices can be taken in the same manner as Python lists.Let's now turn to .iloc. Every row and column of data in a DataFrame has an integer location that defines it. This is in addition to the label that is visually displayed in the output. The integer location is simply the number of rows/columns from the top/left beginning at 0.There are many different inputs you can use for .iloc three out of them are  Selecting a single row with .iloc with an integerThis returns the 5th row (integer location 4) as a SeriesSelecting multiple rows with .iloc with a list of integersThis returns a DataFrame of the third and second to last rows:Selecting multiple rows with .iloc with slice notationOne excellent ability of both .loc/.iloc is their ability to select both rows and columns simultaneously. In the examples above, all the columns were returned from each selection. We can choose columns with the same types of inputs as we do for rows. We simply need to separate the row and column selection with a comma.For example, we can select rows Jane, and Dean with just the columns height, score and state like this:This uses a list of labels for the rows and slice notation for the columnsWe can naturally do similar operations with .iloc using only integers..ix was used to make selections simultaneously with labels and integer location which was useful but confusing and ambiguous at times and thankfully it has been deprecated. In the event that you need to make a selection with a mix of labels and integer locations, you will have to make both your selections labels or integer locations. For instance, if we want to select rows Nick and Cornelia along with columns 2 and 4, we could use .loc by converting the integers to labels with the following:Or alternatively, convert the index labels to integers with the get_loc index method.The .loc indexer can also do boolean selection. For instance, if we are interested in finding all the rows where age is above 30 and return just the food and score columns we can do the following:You can replicate this with .iloc but you cannot pass it a boolean series. You must convert the boolean Series into a numpy array like this:It is possible to use .loc/.iloc for just column selection. You can select all the rows by using a colon like this:Most people are familiar with the primary purpose of the DataFrame indexing operator, which is to select columns. A string selects a single column as a Series and a list of strings selects multiple columns as a DataFrame.Using a list selects multiple columnsWhat people are less familiar with, is that, when slice notation is used, then selection happens by row labels or by integer location. This is very confusing and something that I almost never use but it does work.The explicitness of .loc/.iloc for selecting rows is highly preferred. The indexing operator alone is unable to select rows and columns simultaneously.Selection with .at is nearly identical to .loc but it only selects a single 'cell' in your DataFrame. We usually refer to this cell as a scalar value. To use .at, pass it both a row and column label separated by a comma.Selection with .iat is nearly identical to .iloc but it only selects a single scalar value. You must pass it an integer for both the row and column locationsLet's start with this small df:We'll so have With this we have:Thus we cannot use .iat for subset, where we must use .iloc only.But let's try both to select from a larger df and let's check the speed ...So with .loc we can manage subsets and with .at only a single scalar, but .at is faster than .loc:-)

Using javadoc for Python documentation [closed]

JF Dion

[Using javadoc for Python documentation [closed]](https://stackoverflow.com/questions/5334531/using-javadoc-for-python-documentation)

I am currently beginning with Python and I have a strong PHP background and in PHP I have took the habit of using javadoc as a documentation template.I was wondering if javadoc has its place as docstring documentation in Python. What are the established conventions and/or official guildelines here?E.g. is something like this too elaborate to fit in the Python mindset or should I try to be as concise as possible?And if I am a bit too exhaustive should I go with something like this instead (where most of the documentation doesn't get printed through the __doc__ method)?

2011-03-17 03:22:52Z

I am currently beginning with Python and I have a strong PHP background and in PHP I have took the habit of using javadoc as a documentation template.I was wondering if javadoc has its place as docstring documentation in Python. What are the established conventions and/or official guildelines here?E.g. is something like this too elaborate to fit in the Python mindset or should I try to be as concise as possible?And if I am a bit too exhaustive should I go with something like this instead (where most of the documentation doesn't get printed through the __doc__ method)?Have a look at the reStructuredText (also known as "reST") format, which is a plaintext/docstring markup format, and probably the most popular in the Python world. And you should certainly look at Sphinx, a tool to generate documentation from reStructuredText (used for eg. the Python documentation itself). Sphinx includes the possibility to extract documentation from the docstrings in your code (see sphinx.ext.autodoc), and recognizes reST field lists following certain conventions. This has probably become (or is becoming) the most popular way to do it.Your example could look as follows:Or extended with type information:Follow Google Python Style Guide. Note that Sphinx can also parse this format using the Napolean extension, which will come packaged with Sphinx 1.3 (this is also compatible with PEP257):Example taken from the Napolean documentation linked above.A comprehensive example on all types of docstrings here.The standard for python documentation strings is described in Python Enhancement Proposal 257.The appropriate comment for your method would be something likeTake a look at Documenting Python, a page "aimed at authors and potential authors of documentation for Python."In short, reStructuredText is what's used for documenting Python itself. The developer's guide contains a reST primer, style guide, and general advice for writing good documentation.

What is the purpose of the -m switch?

Charles Brunet

[What is the purpose of the -m switch?](https://stackoverflow.com/questions/7610001/what-is-the-purpose-of-the-m-switch)

Could you explain to me what the difference is between callingandIt seems in both cases mymod1.py is called and sys.argv is So what is the -m switch for?

2011-09-30 11:58:29Z

Could you explain to me what the difference is between callingandIt seems in both cases mymod1.py is called and sys.argv is So what is the -m switch for?The first line of the Rationale section of PEP 338 says:So you can specify any module in Python's search path this way, not just files in the current directory. You're correct that python mymod1.py mymod2.py args has exactly the same effect. The first line of the Scope of this proposal section states:With -m more is possible, like working with modules which are part of a package, etc. That's what the rest of PEP 338 is about. Read it for more info.One more thing I think worth mention, when executepython interpretor will looking for a __main__.py file in the package path to execute. It's equivalent to:It will execute the content after:if the file doesn't exist, this package can not be executed directly.

Using property() on classmethods

Mark Roddy

[Using property() on classmethods](https://stackoverflow.com/questions/128573/using-property-on-classmethods)

I have a class with two class methods (using the classmethod() function) for getting and setting what is essentially a static variable.  I tried to use the property() function with these, but it results in an error.  I was able to reproduce the error with the following in the interpreter:I can demonstrate the class methods, but they don't work as properties:Is it possible to use the property() function with classmethod decorated functions?

2008-09-24 17:37:11Z

I have a class with two class methods (using the classmethod() function) for getting and setting what is essentially a static variable.  I tried to use the property() function with these, but it results in an error.  I was able to reproduce the error with the following in the interpreter:I can demonstrate the class methods, but they don't work as properties:Is it possible to use the property() function with classmethod decorated functions?A property is created on a class but affects an instance.  So if you want a classmethod property, create the property on the metaclass.But since you're using a metaclass anyway, it will read better if you just move the classmethods in there.or, using Python 3's metaclass=... syntax, and the metaclass defined outside of the foo class body, and the metaclass responsible for setting the initial value of _var:Reading the Python 2.2 release notes, I find the following.NOTE: The below method doesn't actually work for setters, only getters.Therefore, I believe the prescribed solution is to create a ClassProperty as a subclass of property.However, the setters don't actually work:foo._var is unchanged, you've simply overwritten the property with a new value.You can also use ClassProperty as a decorator:I hope this dead-simple read-only @classproperty decorator would help somebody looking for classproperties.No. However, a classmethod is simply a bound method (a partial function) on a class accessible from instances of that class.Since the instance is a function of the class and you can derive the class from the instance, you can can get whatever desired behavior you might want from a class-property with property:This code can be used to test - it should pass without raising any errors:And note that we didn't need metaclasses at all - and you don't directly access a metaclass through its classes' instances anyways.You can actually create a classproperty decorator in just a few lines of code by subclassing property (it's implemented in C, but you can see equivalent Python here):Then treat the decorator as if it were a classmethod combined with property:And this code should work without errors:But I'm not sure how well-advised this would be. An old mailing list article suggests it shouldn't work.The downside of the above is that the "class property" isn't accessible from the class, because it would simply overwrite the data descriptor from the class __dict__.However, we can override this with a property defined in the metaclass __dict__. For example:And then a class instance of the metaclass could have a property that accesses the class's property using the principle already demonstrated in the prior sections:And now we see both the instanceand the class have access to the class property.Old question, lots of views, sorely in need of a one-true Python 3 way. Luckily, it's easy with the metaclass kwarg:Then, >>> Foo.varThere is no reasonable way to make this "class property" system to work in Python.Here is one unreasonable way to make it work. You can certainly make it more seamless with increasing amounts of metaclass magic.The knot of the issue is that properties are what Python calls "descriptors". There is no short and easy way to explain how this sort of metaprogramming works, so I must point you to the descriptor howto.You only ever need to understand this sort of things if you are implementing a fairly advanced framework. Like a transparent object persistence or RPC system, or a kind of domain-specific language.However, in a comment to a previous answer, you say that you It seems to me, what you really want is an Observer design pattern.Setting it only on the meta class doesn't help if you want to access the class property via an instantiated object, in this case you need to install a normal property on the object as well (which dispatches to the class property). I think the following is a bit more clear:Half a solution, __set__ on the class does not work, still. The solution is a custom property class implementing both a property and a staticmethodDo you have access to at least one instance of the class? I can think of a way to do it then:Give this a try, it gets the job done without having to change/add a lot of existing code.The property function needs two callable arguments. give them lambda wrappers (which it passes the instance as its first argument) and all is well.Here's a solution which should work for both access via the class and access via an instance which uses a metaclass.This also works with a setter defined in the metaclass.After searching different places, I found a method to define a classproperty

valid with Python 2 and 3.Hope this can help somebody :)Here's my suggestion.  Don't use class methods.  Seriously.  What's the reason for using class methods in this case?  Why not have an ordinary object of an ordinary class?If you simply want to change the value, a property isn't really very helpful is it?  Just set the attribute value and be done with it.A property should only be used if there's something to conceal -- something that might change in a future implementation.  Maybe your example is way stripped down, and there is some hellish calculation you've left off.  But it doesn't look like the property adds significant value.The Java-influenced "privacy" techniques (in Python, attribute names that begin with _) aren't really very helpful.  Private from whom?  The point of private is a little nebulous when you have the source (as you do in Python.)The Java-influenced EJB-style getters and setters (often done as properties in Python) are there to facilitate Java's primitive introspection as well as to pass muster with the static language compiler.  All those getters and setters aren't as helpful in Python.

What is the reason for performing a double fork when creating a daemon?

Shabbyrobe

[What is the reason for performing a double fork when creating a daemon?](https://stackoverflow.com/questions/881388/what-is-the-reason-for-performing-a-double-fork-when-creating-a-daemon)

I'm trying to create a daemon in python. I've found the following question, which has some good resources in it which I am currently following, but I'm curious as to why a double fork is necessary. I've scratched around google and found plenty of resources declaring that one is necessary, but not why.Some mention that it is to prevent the daemon from acquiring a controlling terminal. How would it do this without the second fork? What are the repercussions?

2009-05-19 07:25:18Z

I'm trying to create a daemon in python. I've found the following question, which has some good resources in it which I am currently following, but I'm curious as to why a double fork is necessary. I've scratched around google and found plenty of resources declaring that one is necessary, but not why.Some mention that it is to prevent the daemon from acquiring a controlling terminal. How would it do this without the second fork? What are the repercussions?Looking at the code referenced in the question, the justification is:So it is to ensure that the daemon is re-parented onto init (just in case the process kicking off the daemon is long lived), and removes any chance of the daemon reacquiring a controlling tty. So if neither of these cases apply, then one fork should be sufficient. "Unix Network Programming - Stevens" has a good section on this.I was trying to understand the double fork and stumbled upon this question here. After a lot of research this is what I figured out. Hopefully it will help clarify things better for anyone who has the same question.In Unix every process belongs to a group which in turn belongs to a session. Here is the hierarchy…Session (SID) →  Process Group (PGID) → Process (PID)The first process in the process group becomes the process group leader and the first process in the session becomes the session leader. Every session can have one TTY associated with it. Only a session leader can take control of a TTY. For a process to be truly daemonized (ran in the background) we should ensure that the session leader is killed so that there is no possibility of the session ever taking control of the TTY. I ran Sander Marechal's python example daemon program from this site on my Ubuntu. Here are the results with my comments. Note that the process is the session leader after Decouple#1, because it's PID = SID. It could still take control of a TTY.Note that Fork#2 is no longer the session leader PID != SID. This process can never take control of a TTY. Truly daemonized.I personally find terminology fork-twice to be confusing. A better idiom might be fork-decouple-fork.Additional links of interest:Strictly speaking, the double-fork has nothing to do with re-parenting the daemon as a child of init. All that is necessary to re-parent the child is that the parent must exit. This can be done with only a single fork. Also, doing a double-fork by itself doesn't re-parent the daemon process to init; the daemon's parent must exit. In other words, the parent always exits when forking a proper daemon so that the daemon process is re-parented to init.So why the double fork? POSIX.1-2008 Section 11.1.3, "The Controlling Terminal", has the answer (emphasis added):This tells us that if a daemon process does something like this ...... then the daemon process might acquire /dev/console as its controlling terminal, depending on whether the daemon process is a session leader, and depending on the system implementation. The program can guarantee that the above call will not acquire a controlling terminal if the program first ensures that it is not a session leader. Normally, when launching a daemon, setsid is called (from the child process after calling fork) to dissociate the daemon from its controlling terminal. However, calling setsid also means that the calling process will be the session leader of the new session, which leaves open the possibility that the daemon could reacquire a controlling terminal. The double-fork technique ensures that the daemon process is not the session leader, which then guarantees that a call to open, as in the example above, will not result in the daemon process reacquiring a controlling terminal.The double-fork technique is a bit paranoid. It may not be necessary if you know that the daemon will never open a terminal device file. Also, on some systems it may not be necessary even if the daemon does open a terminal device file, since that behavior is implementation-defined. However, one thing that is not implementation-defined is that only a session leader can allocate the controlling terminal. If a process isn't a session leader, it can't allocate a controlling terminal. Therefore, if you want to be paranoid and be certain that the daemon process cannot inadvertently acquire a controlling terminal, regardless of any implementation-defined specifics, then the double-fork technique is essential.Taken from Bad CTK:"On some flavors of Unix, you are forced to do a double-fork on startup, in order to go into daemon mode. This is because single forking isn’t guaranteed to detach from the controlling terminal."According to "Advanced Programming in the Unix Environment", by Stephens and Rago, the second fork is more a recommendation, and it is done to guarantee that the daemon does not acquire a controlling terminal on System V-based systems. One reason is that the parent process can immediately wait_pid() for the child, and then forget about it.  When then grand-child dies, it's parent is init, and it will wait() for it - and taking it out of the zombie state.  The result is that the parent process doesn't need to be aware of the forked children, and it also makes it possible to fork long running processes from libs etc.The daemon() call has the parent call _exit() if it succeeds. The original motivation may have been to allow the parent to do some extra work while the child is daemonizing.It may also be based on a mistaken belief that it's necessary in order to ensure the daemon has no parent process and is reparented to init - but this will happen anyway once the parent dies in the single fork case.So I suppose it all just boils down to tradition in the end - a single fork is sufficient as long as the parent dies in short order anyway.A decent discussion of it appear to be at http://www.developerweb.net/forum/showthread.php?t=3025Quoting mlampkin from there:It might be easier to understand in this way:

Flatten nested dictionaries, compressing keys

A Timmes

[Flatten nested dictionaries, compressing keys](https://stackoverflow.com/questions/6027558/flatten-nested-dictionaries-compressing-keys)

Suppose you have a dictionary like:How would you go about flattening that into something like:

2011-05-17 07:23:01Z

Suppose you have a dictionary like:How would you go about flattening that into something like:Basically the same way you would flatten a nested list, you just have to do the extra work for iterating the dict by key/value, creating new keys for your new dictionary and creating the dictionary at final step.There are two big considerations that the original poster needs to consider:(Performance is not likely an issue, but I'll elaborate on the second point in case anyone else cares: In implementing this, there are numerous dangerous choices. If you do this recursively and yield and re-yield, or anything equivalent which touches nodes more than once (which is quite easy to accidentally do), you are doing potentially O(N^2) work rather than O(N). This is because maybe you are calculating a key a then a_1 then a_1_i..., and then calculating a then a_1 then a_1_ii..., but really you shouldn't have to calculate a_1 again. Even if you aren't recalculating it, re-yielding it (a 'level-by-level' approach) is just as bad. A good example is to think about the performance on {1:{1:{1:{1:...(N times)...{1:SOME_LARGE_DICTIONARY_OF_SIZE_N}...}}}})Below is a function I wrote flattenDict(d, join=..., lift=...) which can be adapted to many purposes and can do what you want. Sadly it is fairly hard to make a lazy version of this function without incurring the above performance penalties (many python builtins like chain.from_iterable aren't actually efficient, which I only realized after extensive testing of three different versions of this code before settling on this one).To better understand what's going on, below is a diagram for those unfamiliar with reduce(left), otherwise known as "fold left". Sometimes it is drawn with an initial value in place of k0 (not part of the list, passed into the function). Here, J is our join function. We preprocess each kn with lift(k).This is in fact the same as functools.reduce, but where our function does this to all key-paths of the tree.Demonstration (which I'd otherwise put in docstring):Performance:... sigh, don't think that one is my fault...[unimportant historical note due to moderation issues]Regarding the alleged duplicate of Flatten a dictionary of dictionaries (2 levels deep) of lists in Python:That question's solution can be implemented in terms of this one by doing sorted( sum(flatten(...),[]) ). The reverse is not possible: while it is true that the values of flatten(...) can be recovered from the alleged duplicate by mapping a higher-order accumulator, one cannot recover the keys. (edit: Also it turns out that the alleged duplicate owner's question is completely different, in that it only deals with dictionaries exactly 2-level deep, though one of the answers on that page gives a general solution.)Or if you are already using pandas, You can do it with json_normalize() like so:Output:Here is a kind of a "functional", "one-liner" implementation. It is recursive, and based on a conditional expression and a dict comprehension.Test:If you're using pandas there is a function hidden in pandas.io.json._normalize1 called nested_to_record which does this exactly.1 In pandas versions 0.24.x and older use pandas.io.json.normalize (without the _)Code:Results:I am using python3.2, update for your version of python.How about a functional and performant solution in Python3.5?This is even more performant:In use:This is not restricted to dictionaries, but every mapping type that implements .items(). Further ist faster as it avoides an if condition. Nevertheless credits go to Imran:My Python 3.3 Solution using generators:Simple function to flatten nested dictionaries. For Python 3, replace .iteritems() with .items()The idea/requirement was:

Get flat dictionaries with no keeping parent keys.Example of usage:Keeping parent keys is simple as well.This is similar to both imran's and ralu's answer. It does not use a generator, but instead employs recursion with a closure:Davoud's solution is very nice but doesn't give satisfactory results when the nested dict also contains lists of dicts, but his code be adapted for that case:The answers above work really well. Just thought I'd add the unflatten function that I wrote:Note: This doesn't account for '_' already present in keys, much like the flatten counterparts.Here's an algorithm for elegant, in-place replacement. Tested with Python 2.7 and Python 3.5. Using the dot character as a separator.Example:Output:I published this code here along with the matching unflatten_json function.If you want to flat nested dictionary and want all unique keys list then here is the solution:Utilizing recursion, keeping it simple and human readable:Call is simple:orif we want to change the default separator.A little breakdown:When the function is first called, it is called only passing the dictionary we want to flatten. The accumulator parameter is here to support recursion, which we see later. So, we instantiate accumulator to an empty dictionary where we will put all of the nested values from the original dictionary.As we iterate over the dictionary's values, we construct a key for every value. The parent_key argument will be None for the first call, while for every nested dictionary, it will contain the key pointing to it, so we prepend that key.In case the value v the key k is pointing to is a dictionary, the function calls itself, passing the nested dictionary, the accumulator (which is passed by reference, so all changes done to it are done on the same instance) and the key k so that we can construct the concatenated key. Notice the continue statement. We want to skip the next line, outside of the if block, so that the nested dictionary doesn't end up in the accumulator under key k.So, what do we do in case the value v is not a dictionary? Just put it unchanged inside the accumulator.Once we're done we just return the accumulator, leaving the original dictionary argument untouched.NOTEThis will work only with dictionaries that have strings as keys. It will work with hashable objects implementing the __repr__ method, but will yield unwanted results.Using generators:Using dict.popitem() in straightforward nested-list-like recursion:I always prefer access dict objects via .items(), so for flattening dicts I use the following recursive generator flat_items(d). If you like to have dict again, simply wrap it like this: flat = dict(flat_items(d))Variation of this Flatten nested dictionaries, compressing keys with max_level and custom reducer.I was thinking of a subclass of UserDict to automagically flat the keys.‌

The advantages it that keys can be added on the fly, or using standard dict instanciation, without surprise:‌If you do not mind recursive functions, here is a solution. I have also taken the liberty to include an exclusion-parameter in case there are one or more values you wish to maintain.Code:Usage:Output:I actually wrote a package called cherrypicker recently to deal with this exact sort of thing since I had to do it so often!I think the following code would give you exactly what you're after:You can install the package with:...and there's more docs and guidance at https://cherrypicker.readthedocs.io.Other methods may be faster, but the priority of this package is to make such tasks quick and easy. If you do have a large list of objects to flatten though, you can also tell CherryPicker to use parallel processing to speed things up.Just use python-benedict, it is a dict subclass that offers many features, included a flatten method. It's possible to install it using pip: pip install python-benedict https://github.com/fabiocaccamo/python-benedict#flatten

Python concatenate text files

JJ Beck

[Python concatenate text files](https://stackoverflow.com/questions/13613336/python-concatenate-text-files)

I have a list of 20 file names, like ['file1.txt', 'file2.txt', ...]. I want to write a Python script to concatenate these files into a new file. I could open each file by f = open(...), read line by line by calling f.readline(), and write each line into that new file. It doesn't seem very "elegant" to me, especially the part where I have to read//write line by line. Is there a more "elegant" way to do this in Python?

2012-11-28 19:54:46Z

I have a list of 20 file names, like ['file1.txt', 'file2.txt', ...]. I want to write a Python script to concatenate these files into a new file. I could open each file by f = open(...), read line by line by calling f.readline(), and write each line into that new file. It doesn't seem very "elegant" to me, especially the part where I have to read//write line by line. Is there a more "elegant" way to do this in Python?This should do itFor large files:For small files:… and another interesting one that I thought of:Sadly, this last method leaves a few open file descriptors, which the GC should take care of anyway. I just thought it was interestingUse shutil.copyfileobj.It automatically reads the input files chunk by chunk for you, which is more more efficient and reading the input files in and will work even if some of the input files are too large to fit into memory:That's exactly what fileinput is for:For this use case, it's really not much simpler than just iterating over the files manually, but in other cases, having a single iterator that iterates over all of the files as if they were a single file is very handy. (Also, the fact that fileinput closes each file as soon as it's done means there's no need to with or close each one, but that's just a one-line savings, not that big of a deal.)There are some other nifty features in fileinput, like the ability to do in-place modifications of files just by filtering each line.As noted in the comments, and discussed in another post, fileinput for Python 2.7 will not work as indicated. Here slight modification to make the code Python 2.7 compliantWhat's wrong with UNIX commands ? (given you're not working on Windows) : ls | xargs cat | tee output.txt does the job ( you can call it from python with subprocess if you want)I don't know about elegance, but this works:An alternative to @inspectorG4dget answer (best answer to date 29-03-2016). I tested with 3 files of 436MB. @inspectorG4dget solution: 162 secondsThe following solution : 125 secondsThe idea is to create a batch file and execute it, taking advantage of "old good technology". Its semi-python but works faster. Works for windows. If you have a lot of files in the directory then glob2 might be a better option to generate a list of filenames rather than writing them by hand.A simple benchmark shows that the shutil performs better.Check out the .read() method of the File object:http://docs.python.org/2/tutorial/inputoutput.html#methods-of-file-objectsYou could do something like:or a more 'elegant' python-way:which, according to this article: http://www.skymind.com/~ocrow/python_string/ would also be the fastest.If the files are not gigantic:If the files are too big to be entirely read and held in RAM, the algorithm must be a little different to read each file to be copied in a loop by chunks of fixed length, using read(10000) for example.

Ruby equivalent of virtualenv?

dbr

[Ruby equivalent of virtualenv?](https://stackoverflow.com/questions/486995/ruby-equivalent-of-virtualenv)

Is there something similar to the Python utility virtualenv?Basically it allows you to install Python packages into a sandboxed environment, so easy_install django doesn't go in your system-wide site-packages directory, it would go in the virtualenv-created directory.For example:Is there something like this for RubyGems?

2009-01-28 09:24:14Z

Is there something similar to the Python utility virtualenv?Basically it allows you to install Python packages into a sandboxed environment, so easy_install django doesn't go in your system-wide site-packages directory, it would go in the virtualenv-created directory.For example:Is there something like this for RubyGems?RVM works closer to how virtualenv works since it lets you sandbox different ruby versions and their gems, etc.Neither sandbox, RVM, nor rbenv manage the versions of your app's gem dependencies. The tool for that is bundler.No one seems to have mentioned rbenv.I think you'll like sandbox.I'll mention the way I do this with Bundler (which I use with RVM - RVM to manage the rubies and a default set of global gems, Bundler to handle project specific gems)Running this command in the root of a project will install the gems listed from your Gemfile, put the libs in ./vendor, and any executables in ./bin and all requires (if you use bundle console or the Bundler requires) will reference these exes and libs.Works for me.If you only need to install gems as non-root, try setting the GEM_HOME environment variable.  Then just run gem.For example:I recommend direnv. It is an environment switcher for the shell.Before each prompt it checks for the existence of an ".envrc" file in the current and parent directories. If the file exists (and authorized), it is loaded into a bash sub-shell and all exported variables are then captured by direnv and then made available the current shell.Here is how to use direnv with ruby-installAdd this to the ~/.direnvrcInstall ruby-install (brew install ruby-install) and install a bunch of rubies.And then make a couple of symlinks for convenience:And finally in any project's .envrc:use ruby 2.0This will put all gems under the project's .direnv/ruby directory (makes opening gems easier). bundler will put wrapper binaries in  .direnv/bin (no more bundle exec!). It's also possible to use rbenv by adding the use rbenv command in any .envrc file. This will activate rbenv which in turn will put the ruby wrappers in the PATH.Note that it's not necessary to install rbenv in the .bashrc or .zshrc for this to work.Here is the most complicated .envrc that I use on ruby projects:rvm is used to select the right ruby version for youlayout commands automatically set some of the usual environment variables. For now only the ruby layout exists. What it does is set the GEM_HOME environment variable and it's bin directory to your path. Because it depends on the ruby version, make sure to call it after "rvm". Since each ruby layout directories have their own GEM_HOME, you don't need to use rvm's gemsets.PATH_add prepends and expands the given relative path. In that case, I use this to segregate the bundler binstubs from my own bin scripts with bundle install --binstubs .direnv/bundler-binIf you want to find out what those commands exactly do, for now: cat direnv stdlib | lessMineshaft is a project that I've been working on for some time and am continuing development work on.It offers the ability to both create virtual environments akin to how virtualenv works and can also install Ruby globally as well.

Why should we NOT use sys.setdefaultencoding(「utf-8」) in a py script?

mlzboy

[Why should we NOT use sys.setdefaultencoding(「utf-8」) in a py script?](https://stackoverflow.com/questions/3828723/why-should-we-not-use-sys-setdefaultencodingutf-8-in-a-py-script)

I have seen few py scripts which use this at the top of the script. In what cases one should use it?

2010-09-30 07:46:08Z

I have seen few py scripts which use this at the top of the script. In what cases one should use it?As per the documentation: This allows you to switch from the default ASCII to other encodings such as UTF-8, which the Python runtime will use whenever it has to decode a string buffer to unicode.  This function is only available at Python start-up time, when Python scans the environment. It has to be called in a system-wide module, sitecustomize.py, After this module has been evaluated, the setdefaultencoding() function is removed from the sys module. The only way to actually use it is with a reload hack that brings the attribute back. Also, the use of sys.setdefaultencoding() has always been discouraged, and it has become a no-op in py3k. The encoding of py3k is hard-wired to "utf-8" and changing it raises an error.I suggest some pointers for reading:The answer is NEVER! (unless you really know what you're doing)9/10 times the solution can be resolved with a proper understanding of encoding/decoding.1/10 people have an incorrectly defined locale or environment and need to set:in their environment to fix console printing problems.sys.setdefaultencoding("utf-8") (struck through to avoid re-use) changes the default encoding/decoding used whenever Python 2.x needs to convert a Unicode() to a str() (and vice-versa) and the encoding is not given. I.e:In Python 2.x, the default encoding is set to ASCII and the above examples will fail with:(My console is configured as UTF-8, so "€" = '\xe2\x82\xac', hence exception on \xe2)orsys.setdefaultencoding("utf-8") will allow these to work for me, but won't necessarily work for people who don't use UTF-8. The default of ASCII ensures that assumptions of encoding are not baked into codesys.setdefaultencoding("utf-8") also has a side effect of appearing to fix sys.stdout.encoding, used when printing characters to the console. Python uses the user's locale (Linux/OS X/Un*x) or codepage (Windows) to set this. Occasionally, a user's locale is broken and just requires PYTHONIOENCODING to fix the console encoding.Example:People have been developing against Python 2.x for 16 years on the understanding that the default encoding is ASCII. UnicodeError exception handling methods have been written to handle string to Unicode conversions on strings that are found to contain non-ASCII.From https://anonbadger.wordpress.com/2015/06/16/why-sys-setdefaultencoding-will-break-code/Changing what should be a constant will have dramatic effects on modules you depend upon. It's better to just fix the data coming in and out of your code.While the setting of defaultencoding to UTF-8 isn't the root cause in the following example, it shows how problems are masked and how, when the input encoding changes, the code breaks in an unobvious way: 

UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 3131: invalid start byteon shell works , sending to sdtout not ,

so that is one workaround, to write to stdout .I made other approach, which is not run if sys.stdout.encoding  is not define, or in others words , need export PYTHONIOENCODING=UTF-8 first to write to stdout.                                                         

so, using same example:will work

Correct way to define Python source code encoding

Oli

[Correct way to define Python source code encoding](https://stackoverflow.com/questions/728891/correct-way-to-define-python-source-code-encoding)

PEP 263 defines how to declare Python source code encoding.Normally, the first 2 lines of a Python file should start with:But I have seen a lot of files starting with:=> encoding instead of coding.So what is the correct way of declaring the file encoding?Is encoding permitted because the regex used is lazy? Or is it just another form of declaring the file encoding?I'm asking this question because the PEP does not talk about encoding, it just talks about coding.

2009-04-08 07:35:05Z

PEP 263 defines how to declare Python source code encoding.Normally, the first 2 lines of a Python file should start with:But I have seen a lot of files starting with:=> encoding instead of coding.So what is the correct way of declaring the file encoding?Is encoding permitted because the regex used is lazy? Or is it just another form of declaring the file encoding?I'm asking this question because the PEP does not talk about encoding, it just talks about coding.Check the docs here: "If a comment in the first or second line of the Python script matches the regular expression coding[=:]\s*([-\w.]+), this comment is processed as an encoding declaration""The recommended forms of this expression arewhich is recognized also by GNU Emacs, andwhich is recognized by Bram Moolenaar’s VIM."So, you can put pretty much anything before the "coding" part, but stick to "coding" (with no prefix) if you want to be 100% python-docs-recommendation-compatible.More specifically, you need to use whatever is recognized by Python and the specific editing software you use (if it needs/accepts anything at all). E.g. the coding form is recognized (out of the box) by GNU Emacs but not Vim (yes, without a universal agreement, it's essentially a turf war).PEP 263:So, "encoding: UTF-8" matches. PEP provides some examples: Just copy paste below statement on the top of your program.It will solve character encoding problemsPEP 263 itself mentions the regex it follows:So, as already summed up by other answers, it'll match coding with any prefix, but if you'd like to be as PEP-compliant as it gets (even though, as far as I can tell, using encoding instead of coding does not violate PEP 263 in any way) — stick with 'plain' coding, with no prefixes.If I'm not mistaken, the original proposal for source file encodings was to use a regular expression for the first couple of lines, which would allow both.I think the regex was something along the lines of coding: followed by something.I found this: http://www.python.org/dev/peps/pep-0263/

Which is the original proposal, but I can't seem to find the final spec stating exactly what they did.I've certainly used encoding: to great effect, so obviously that works.Try changing to something completely different, like duhcoding: ... to see if that works just as well.I suspect it is similar to Ruby - either method is okay.This is largely because different text editors use different methods (ie, these two) of marking encoding.With Ruby, as long as the first, or second if there is a shebang line contains a string that matches:and ignoring any whitespace and other fluff on those lines.  (It can often be a = instead of :, too).

Step-by-step debugging with IPython

Amelio Vazquez-Reina

[Step-by-step debugging with IPython](https://stackoverflow.com/questions/16867347/step-by-step-debugging-with-ipython)

From what I have read, there are two ways to debug code in Python:Is there any way to combine the best of both worlds? I.e.An example of this type of "enhanced debugging" can be found  in MATLAB, where the user always has full access to the MATLAB engine/shell, and she can still step-by-step through her code, define conditional breakpoints, etc. From what I have discussed with other users, this is the debugging feature that people miss the most when moving from MATLAB to IPython.I don't want to make the question too specific, but I work mostly in Emacs, so I wonder if there is any way to bring this functionality into it. Ideally, Emacs (or the editor) would allow the programmer to set breakpoints anywhere on the code and communicate with the interpreter or debugger to have it stop in the location of your choice, and bring to a full IPython interpreter on that location.

2013-05-31 23:21:04Z

From what I have read, there are two ways to debug code in Python:Is there any way to combine the best of both worlds? I.e.An example of this type of "enhanced debugging" can be found  in MATLAB, where the user always has full access to the MATLAB engine/shell, and she can still step-by-step through her code, define conditional breakpoints, etc. From what I have discussed with other users, this is the debugging feature that people miss the most when moving from MATLAB to IPython.I don't want to make the question too specific, but I work mostly in Emacs, so I wonder if there is any way to bring this functionality into it. Ideally, Emacs (or the editor) would allow the programmer to set breakpoints anywhere on the code and communicate with the interpreter or debugger to have it stop in the location of your choice, and bring to a full IPython interpreter on that location.Nobody mentioned IPython's %pdb magic yet. Just call %pdb in IPython and when an error occurs, you're automatically dropped to ipdb. While you don't have the stepping immediately, you're in ipdb afterwards.This makes debugging individual functions easy, as you can just load a file with %load and then run a function. You could force an error with an assert at the right position.%pdb is a line magic. Call it as %pdb on, %pdb 1, %pdb off or %pdb 0. If called without argument it works as a toggle.What about ipdb.set_trace() ? In your code :import ipdb; ipdb.set_trace()update: now in Python 3.7, we can write breakpoint(). It works the same, but it also obeys to the PYTHONBREAKPOINT environment variable. This feature comes from this PEP.This allows for full inspection of your code, and you have access to commands such as c (continue), n (execute next line), s (step into the method at point) and so on.See the ipdb repo and a list of commands. IPython is now called (edit: part of) Jupyter.ps: note that an ipdb command takes precedence over python code. So in order to write list(foo) you'd need print list(foo).Also, if you like the ipython prompt (its emacs and vim modes, history, completions,…) it's easy to get the same for your project since it's based on the python prompt toolkit.For anyone in Emacs, this thread shows how to accomplish everything described in the OP (and more) using The combination of these two packages is extremely powerful and allows one to recreate exactly the behavior described in the OP and do even more.More info on the wiki article of RealGUD for ipdb.After having tried many different methods for debugging Python, including everything mentioned in this thread, one of my preferred ways of debugging Python with IPython is with embedded shells. Add the following on a script to your PYTHONPATH, so that the method ipsh() becomes available.Then, whenever I want to debug something in my code, I place ipsh() right at the location where I need to do object inspection, etc. For example, say I want to debug my_function belowand then I invoke my_function(2) in one of the following ways:Regardless of how I invoke it, the interpreter stops at the line that says ipsh(). Once you are done, you can do Ctrl-D and Python will resume execution (with any variable updates that you made). Note that, if you run the code from a regular IPython  the IPython shell (case 2 above), the new IPython shell will be nested inside the one from which you invoked it, which is perfectly fine, but it's good to be aware of. Eitherway, once the interpreter stops on the location of ipsh, I can inspect the value of a (which be 2), see what functions and objects are defined, etc.The solution above can be used to have Python stop anywhere you want in your code, and then drop you into  a fully-fledged IPython interpreter. Unfortunately it does not let you add or remove breakpoints once you invoke the script, which is highly frustrating. In my opinion, this is the only thing that is preventing IPython from becoming a great debugging tool for Python. A workaround is to place ipsh() a priori at the different locations where you want the Python interpreter to launch an IPython shell (i.e. a breakpoint). You can then "jump" between different pre-defined, hard-coded "breakpoints" with Ctrl-D, which would exit the current embedded IPython shell and stop again whenever the interpreter hits the next call to ipsh(). If you go this route, one way to exit  "debugging mode" and ignore all subsequent breakpoints, is to use ipshell.dummy_mode = True which will make Python ignore any subsequent instantiations of the ipshell object that we created above. You can start IPython session from pudb and go back to the debugging session as you like.BTW, ipdb is using IPython behind the scenes and you can actually use IPython functionality such as TAB completion and magic commands (the one starts with %).  If you are OK with ipdb you can start it from IPython using commands such as %run and %debug.  ipdb session is actually better than plain IPython one in the sense you can go up and down in the stack trace etc.  What is missing in ipdb for "object inspection"?Also, python.el bundled with Emacs >= 24.3 has nice ipdb support.Looks like the approach in @gaborous's answer is deprecated.The new approach seems to be:Prefixing an "!" symbol to commands you type in pdb seems to have the same effect as doing something in an IPython shell. This works for accessing help for a certain function, or even variable names. Maybe this will help you to some extent. For example,But !help(numpy.transpose) will give you the expected help page on numpy.transpose. Similarly for variable names, say you have a variable l, typing "l" in pdb lists the code, but !l prints the value of l.Did you try this tip?One option is to use an IDE like Spyder which should allow you to interact with your code while debugging (using an IPython console, in fact). In fact, Spyder is very MATLAB-like, which I presume was intentional. That includes variable inspectors, variable editing, built-in access to documentation, etc.the right, easy, cool, exact answer for the question is to use %run macro with -d flag.If you type exit() in embed() console the code continue and go to the next embed() line.The Pyzo IDE has similar capabilities as the OP asked for. You don't have to start in debug mode. Similarly to MATLAB, the commands are executed in the shell. When you set up a break-point in some source code line, the IDE stops the execution there and you can debug and issue regular IPython commands as well.It does seem however that step-into doesn't (yet?) work well (i.e. stopping in one line and then stepping into another function) unless you set up another break-point.Still, coming from MATLAB, this seems the best solution I've found.Not explicitly said before is that you can call IPython.embed() from within ipdb.  ipdb is great for stepping through code and for introspection, but it doesn't handle other things, like multiple lines, well.  IPython handles multi-line statements well.To induce the debugger, call the following in your code:This allows for introspection and stepping.If you need IPython functionality, call the following in ipdb>:Regarding Emacs, here's my idiosyncratic solution which I hope inspires you.I use Emacs with M-x shell.  I have a yassnippet defined for ipdb which induces the debugger.  The bm package highlights the line so that I don't forget to remove it from the source code:I import IPython in my .pdbrc file:This allows me to call embed() from any ipdb instance (when IPython is installed).Running from inside Emacs' IPython-shell and breakpoint set via pdb.set_trace() should work.Checked with python-mode.el, M-x ipython RET etc.From python 3.2, you have the interact command, which gives you access to the full python/ipython command space.

What is the purpose「pip install --user …」?

Rob Truxal

[What is the purpose「pip install --user …」?](https://stackoverflow.com/questions/42988977/what-is-the-purpose-pip-install-user)

From pip install --help:The documentation for site.USER_BASE is a terrifying wormhole of interesting *NIX subject matter that I don't understand.  What is the purpose of --user in plain english? Why would intalling the package to ~/.local/ matter? Why not just put an executable somewhere in my $PATH?

2017-03-23 23:44:40Z

From pip install --help:The documentation for site.USER_BASE is a terrifying wormhole of interesting *NIX subject matter that I don't understand.  What is the purpose of --user in plain english? Why would intalling the package to ~/.local/ matter? Why not just put an executable somewhere in my $PATH?pip defaults to installing Python packages to a system directory (such as /usr/local/lib/python3.4). This requires root access.--user makes pip install packages in your home directory instead, which doesn't require any special privileges.--user installs in site.USER_SITE.For my case, it was /Users/.../Library/Python/2.7/bin. So I have added that to my PATH (in ~/.bash_profile file):Other answers mention site.USER_SITE as where Python packages get placed. If you're looking for binaries, these go in {site.USER_BASE}/bin.If you want to add this directory to your shell's search path, use:Best way to is install virtualenv and not require the --user confusion. You will get more flexibility and not worry about clobbering the different python versions and projects everytime you pip install a package.https://virtualenv.pypa.io/en/stable/On macOS, the reason for using the --user flag is to make sure we don't corrupt the libraries the OS relies on.  A conservative approach for many macOS users is to avoid installing or updating pip with a command that requires sudo.  Thus, this includes installing to /usr/local/bin... Ref: Installing python for Neovim (https://github.com/zchee/deoplete-jedi/wiki/Setting-up-Python-for-Neovim)I'm not all clear why installing into /usr/local/bin is a risk on a Mac given the fact that the system only relies on python binaries in /Library/Frameworks/ and /usr/bin.  I suspect it's because as noted above, installing into /usr/local/bin requires sudo which opens the door to making a costly mistake with the system libraries.  Thus, installing into ~/.local/bin is a sure fire way to avoid this risk.Ref: Using python on a Mac (https://docs.python.org/2/using/mac.html)Finally, to the degree there is a benefit of installing packages into the /usr/local/bin, I wonder if it makes sense to change the owner of the directory from root to user?  This would avoid having to use sudo while still protecting against making system-dependent changes.*  Is this a security default a relic of how Unix systems were more often used in the past (as servers)? Or at minimum, just a good way to go for Mac users not hosting a server?*Note: Mac's System Integrity Protection (SIP) feature also seems to protect the user from changing the system-dependent libraries.- EJust a warning: According to this issue, --user is currently not valid inside a virtual env's pip, since a user location doesn't really make sense for a virtual environment.So do not use pip install --user some_pkg inside a virtual environment, otherwise, virtual environment's pip will be confused. See this answer for more details.pip <command> --user changes the scope of the current pip command to work on the current user account's local python package install location, rather than the system-wide package install location, which is the default.This only really matters on a multi-user machine. Anything installed to the system location will be visible to all users, so installing to the user location will keep that package installation separate from other users (they will not see it, and would have to install it themselves separately to use it). Because there can be version conflicts, installing a package with dependencies needed by other packages can cause problems, so it's best not to push all packages a given user uses to the system install location.The --user option in an active venv/virtualenv environment will install to the local user python location (same as without a virtual environment).Packages are installed to the virtual environment by default, but if you use --user it will force it to install outside the virtual environments, in the users python script directory (in Windows, this currently is c:\users\<username>\appdata\roaming\python\python37\scripts for me with Python 3.7).However, you won't be able to access a system or user install from within virtual environment (even if you used --user while in a virtual environment). Although, if you install a virtual environment with the --system-site-packages argument, you will have access to the system script folder for python. I believe this included the user python script folder as well, but I'm unsure.You can find the location of the user install folder for python with python -m site --user-base. I'm finding conflicting information in Q&A's, the documentation and actually using this command on my PC as to what the defaults are, but they are underneath the user home directory (~ shortcut in *nix, and c:\users\<username> typically for Windows).The --user option is not a valid for every command. For example pip uninstall will find and uninstall packages wherever they were installed (in the user folder, virtual environment folder, etc.) and the --user option is not valid.Things installed with pip install --user will be installed in a local location that will only be seen by the current user account, and will not require root access (on *nix) or administrator access (on Windows).The --user option modifies all pip commands that accept it to see/operate on the user install folder, so if you use pip list -- user it will only show you packages installed with pip install --user.

Accessing class variables from a list comprehension in the class definition

Mark Lodato

[Accessing class variables from a list comprehension in the class definition](https://stackoverflow.com/questions/13905741/accessing-class-variables-from-a-list-comprehension-in-the-class-definition)

How do you access other class variables from a list comprehension within the class definition?  The following works in Python 2 but fails in Python 3:Python 3.2 gives the error:Trying Foo.x doesn't work either.  Any ideas on how to do this in Python 3?A slightly more complicated motivating example:In this example, apply() would have been a decent workaround, but it is sadly removed from Python 3.

2012-12-16 21:42:22Z

How do you access other class variables from a list comprehension within the class definition?  The following works in Python 2 but fails in Python 3:Python 3.2 gives the error:Trying Foo.x doesn't work either.  Any ideas on how to do this in Python 3?A slightly more complicated motivating example:In this example, apply() would have been a decent workaround, but it is sadly removed from Python 3.Class scope and list, set or dictionary comprehensions, as well as generator expressions do not mix.In Python 3, list comprehensions were given a proper scope (local namespace) of their own, to prevent their local variables bleeding over into the surrounding scope (see Python list comprehension rebind names even after scope of comprehension. Is this right?). That's great when using such a list comprehension in a module or in a function, but in classes, scoping is a little, uhm, strange.This is documented in pep 227:and in the class compound statement documentation:Emphasis mine; the execution frame is the temporary scope.Because the scope is repurposed as the attributes on a class object, allowing it to be used as a nonlocal scope as well leads to undefined behaviour; what would happen if a class method referred to x as a nested scope variable, then manipulates Foo.x as well, for example? More importantly, what would that mean for subclasses of Foo? Python has to treat a class scope differently as it is very different from a function scope.Last, but definitely not least, the linked Naming and binding section in the Execution model documentation mentions class scopes explicitly:So, to summarize: you cannot access the class scope from functions, list comprehensions or generator expressions enclosed in that scope; they act as if that scope does not exist. In Python 2, list comprehensions were implemented using a shortcut, but in Python 3 they got their own function scope (as they should have had all along) and thus your example breaks. Other comprehension types have their own scope regardless of Python version, so a similar example with a set or dict comprehension would break in Python 2.There's one part of a comprehension or generator expression that executes in the surrounding scope, regardless of Python version. That would be the expression for the outermost iterable. In your example, it's the range(1):Thus, using x in that expression would not throw an error:This only applies to the outermost iterable; if a comprehension has multiple for clauses, the iterables for inner for clauses are evaluated in the comprehension's scope:This design decision was made in order to throw an error at genexp creation time instead of iteration time when creating the outermost iterable of a generator expression throws an error, or when the outermost iterable turns out not to be iterable. Comprehensions share this behavior for consistency.You can see this all in action using the dis module. I'm using Python 3.3 in the following examples, because it adds qualified names that neatly identify the code objects we want to inspect. The bytecode produced is otherwise functionally identical to Python 3.2.To create a class, Python essentially takes the whole suite that makes up the class body (so everything indented one level deeper than the class <name>: line), and executes that as if it were a function:The first LOAD_CONST there loads a code object for the Foo class body, then makes that into a function, and calls it. The result of that call is then used to create the namespace of the class, its __dict__. So far so good.The thing to note here is that the bytecode contains a nested code object; in Python, class definitions, functions, comprehensions and generators all are represented as code objects that contain not only bytecode, but also structures that represent local variables, constants, variables taken from globals, and variables taken from the nested scope. The compiled bytecode refers to those structures and the python interpreter knows how to access those given the bytecodes presented.The important thing to remember here is that Python creates these structures at compile time; the class suite is a code object (<code object Foo at 0x10a436030, file "<stdin>", line 2>) that is already compiled.Let's inspect that code object that creates the class body itself; code objects have a co_consts structure:The above bytecode creates the class body. The function is executed and the resulting locals() namespace, containing x and y is used to create the class (except that it doesn't work because x isn't defined as a global). Note that after storing 5 in x, it loads another code object; that's the list comprehension; it is wrapped in a function object just like the class body was; the created function takes a positional argument, the range(1) iterable to use for its looping code, cast to an iterator. As shown in the bytecode, range(1) is evaluated in the class scope.From this you can see that the only difference between a code object for a function or a generator, and a code object for a comprehension is that the latter is executed immediately when the parent code object is executed; the bytecode simply creates a function on the fly and executes it in a few small steps.Python 2.x uses inline bytecode there instead, here is output from Python 2.7:No code object is loaded, instead a FOR_ITER loop is run inline. So in Python 3.x, the list generator was given a proper code object of its own, which means it has its own scope.However, the comprehension was compiled together with the rest of the python source code when the module or script was first loaded by the interpreter, and the compiler does not consider a class suite a valid scope. Any referenced variables in a list comprehension must look in the scope surrounding the class definition, recursively. If the variable wasn't found by the compiler, it marks it as a global. Disassembly of the list comprehension code object shows that x is indeed loaded as a global:This chunk of bytecode loads the first argument passed in (the range(1) iterator), and just like the Python 2.x version uses FOR_ITER to loop over it and create its output.Had we defined x in the foo function instead, x would be a cell variable (cells refer to nested scopes):The LOAD_DEREF will indirectly load x from the code object cell objects:The actual referencing looks the value up from the current frame data structures, which were initialized from a function object's .__closure__ attribute. Since the function created for the comprehension code object is discarded again, we do not get to inspect that function's closure. To see a closure in action, we'd have to inspect a nested function instead:So, to summarize:If you were to create an explicit scope for the x variable, like in a function, you can use class-scope variables for a list comprehension:The 'temporary' y function can be called directly; we replace it when we do with its return value. Its scope is considered when resolving x:Of course, people reading your code will scratch their heads over this a little; you may want to put a big fat comment in there explaining why you are doing this.The best work-around is to just use __init__ to create an instance variable instead:and avoid all the head-scratching, and questions to explain yourself. For your own concrete example, I would not even store the namedtuple on the class; either use the output directly (don't store the generated class at all), or use a global:In my opinion it is a flaw in Python 3. I hope they change it.Old Way (works in 2.7, throws NameError: name 'x' is not defined in 3+):NOTE: simply scoping it with A.x would not solve itNew Way (works in 3+): Because the syntax is so ugly I just initialize all my class variables in the constructor typically The accepted answer provides excellent information, but there appear to be a few other wrinkles here -- differences between list comprehension and generator expressions. A demo that I played around with:This is a bug in Python. Comprehensions are advertised as being equivalent to for loops, but this is not true in classes. At least up to Python 3.6.6, in a comprehension used in a class, only one variable from outside the comprehension is accessible inside the comprehension, and it must be used as the outermost iterator. In a function, this scope limitation does not apply.To illustrate why this is a bug, let's return to the original example. This fails:But this works:The limitation is stated at the end of this section in the reference guide.Since the outermost iterator is evaluated in the surrounding scope we can use zip together with itertools.repeat to carry the dependencies over to the comprehension's scope:One can also use nested for loops in the comprehension and include the dependencies in the outermost iterable:For the specific example of the OP:

Sorting a Python list by two fields

half full

[Sorting a Python list by two fields](https://stackoverflow.com/questions/5212870/sorting-a-python-list-by-two-fields)

I have the following list created from a sorted csvI would actually like to sort the list by two criteria: first by the value in field 1 and then by the value in field 2. How do I do this?

2011-03-06 19:36:08Z

I have the following list created from a sorted csvI would actually like to sort the list by two criteria: first by the value in field 1 and then by the value in field 2. How do I do this?like this:No need to import anything when using lambda functions.

The following sorts list by the first element, then by the second element.Python has a stable sort, so provided that performance isn't an issue the simplest way is to sort it by field 2 and then sort it again by field 1.That will give you the result you want, the only catch is that if it is a big list (or you want to sort it often) calling sort twice might be an unacceptable overhead.Doing it this way also makes it easy to handle the situation where you want some of the columns reverse sorted, just include the 'reverse=True' parameter when necessary.Otherwise you can pass multiple parameters to itemgetter or manually build a tuple. That is probably going to be faster, but has the problem that it doesn't generalise well if some of the columns want to be reverse sorted (numeric columns can still be reversed by negating them but that stops the sort being stable).So if you don't need any columns reverse sorted, go for multiple arguments to itemgetter, if you might, and the columns aren't numeric or you want to keep the sort stable go for multiple consecutive sorts.Edit: For the commenters who have problems understanding how this answers the original question, here is an example that shows exactly how the stable nature of the sorting ensures we can do separate sorts on each key and end up with data sorted on multiple criteria:This is a runnable example, but to save people running it the output is:Note in particular how in the second step the reverse=True parameter keeps the firstnames in order whereas simply sorting then reversing the list would lose the desired order for the third sort key.We can also use .sort with lambda 2 times because python sort is in place and stable. This will first sort the list according to the second element, x[1]. Then, it will sort the first element, x[0] (highest priority).This is equivalent to doing the following:

employees.sort(key = lambda x:(x[0], x[1]))In ascending order you can use:or in descending order you can use:Sorting list of dicts using below will sort list in descending order on first column as salary and second column as ageOutput: [{'salary': 123, 'age': 25}, {'salary': 123, 'age': 23}]

Why is Python running my module when I import it, and how do I stop it?

Dasmowenator

[Why is Python running my module when I import it, and how do I stop it?](https://stackoverflow.com/questions/6523791/why-is-python-running-my-module-when-i-import-it-and-how-do-i-stop-it)

I have a Python program I'm building that can be run in either of 2 ways: the first is to call "python main.py" which prompts the user for input in a friendly manner and then runs the user input through the program.  The other way is to call "python batch.py -file-" which will pass over all the friendly input gathering and run an entire file's worth of input through the program in a single go.The problem is that when I run "batch.py" it imports some variables/methods/etc from "main.py", and when it runs this code:at the first line of the program, it immediately errors because it tries to run the code in "main.py".How can I stop Python from running the code contained in the "main" module which I'm importing?

2011-06-29 16:11:12Z

I have a Python program I'm building that can be run in either of 2 ways: the first is to call "python main.py" which prompts the user for input in a friendly manner and then runs the user input through the program.  The other way is to call "python batch.py -file-" which will pass over all the friendly input gathering and run an entire file's worth of input through the program in a single go.The problem is that when I run "batch.py" it imports some variables/methods/etc from "main.py", and when it runs this code:at the first line of the program, it immediately errors because it tries to run the code in "main.py".How can I stop Python from running the code contained in the "main" module which I'm importing?Because this is just how Python works - keywords such as class and def are not declarations. Instead, they are real live statements which are executed. If they were not executed your module would be .. empty :-)Anyway, the idiomatic approach is:See What is if __name__ == "__main__" for?It does require source control over the module being imported, however.Happy coding.Due to the way Python works, it is necessary for it to run your modules when it imports them.To prevent code in the module from being executed when imported, but only when run directly, you can guard it with this if:You may want to put this code in a main() method, so that you can either execute the file directly, or import the module and call the main(). For example, assume this is in the file foo.py.This program can be run either by going python foo.py, or from another Python script:Use the if __name__ == '__main__' idiom -- __name__ is a special variable whose value is '__main__' if the module is being run as a script, and the module name if it's imported. So you'd do something likeUnfortunately, you don't. That is part of how the import syntax works and it is important that it does so -- remember def is actually something executed, if Python did not execute the import, you'd be, well, stuck without functions.Since you probably have access to the file, though, you might be able to look and see what causes the error. It might be possible to modify your environment to prevent the error from happening.Put the code inside a function and it won't run until you call the function.  You should have a main function in your main.py.  with the statement:Then, if you call python main.py the main() function will run.  If you import main.py, it will not.  Also, you should probably rename main.py to something else for clarity's sake.You may write your "main.py" like this:There was a Python enhancement proposal PEP 299 which aimed to replace if __name__ == '__main__': idiom with def __main__:, but it was rejected. It's still a good read to know what to keep in mind when using if __name__ = '__main__':.Although you cannot use import without running the code; there is quite a swift way in which you can input your variables; by using numpy.savez, which stores variables as numpy arrays in a .npz file. Afterwards you can load the variables using numpy.load. See a full description in the scipy documentationPlease note this is only the case for variables and arrays of variable, and not for methods, etc. Try just importing the functions needed from main.py? So,It could be that you've named a function in batch.py the same as one in main.py, and when you import main.py the program runs the main.py function instead of the batch.py function; doing the above should fix that. I hope.

Remove the first character of a string

Hossein

[Remove the first character of a string](https://stackoverflow.com/questions/4945548/remove-the-first-character-of-a-string)

I would like to remove the first character of a string.For example, my string starts with a : and I want to remove that only. There are several occurrences of : in the string that shouldn't be removed.I am writing my code in Python.

2011-02-09 13:33:29Z

I would like to remove the first character of a string.For example, my string starts with a : and I want to remove that only. There are several occurrences of : in the string that shouldn't be removed.I am writing my code in Python.python 2.xpython 3.xboth printsYour problem seems unclear. You say you want to remove "a character from a certain position" then go on to say you want to remove a particular character.If you only need to remove the first character you would do:If you want to remove a character at a particular position, you would do:If you need to remove a particular character, say ':', the first time it is encountered in a string then you would do:Depending on the structure of the string, you can use lstrip:But this would remove all colons at the beginning, i.e. if you have ::foo, the result would be foo. But this function is helpful if you also have strings that do not start with a colon and you don't want to remove the first character then.deleting a char:it deletes all the chars that are in indexes; you can use it in your case with del_char(your_string, [0])

Getting a list of values from a list of dicts

SuperString

[Getting a list of values from a list of dicts](https://stackoverflow.com/questions/7271482/getting-a-list-of-values-from-a-list-of-dicts)

I have a list of dicts like this:I want ['apple', 'banana', 'cars']Whats the best way to do this?

2011-09-01 14:04:53Z

I have a list of dicts like this:I want ['apple', 'banana', 'cars']Whats the best way to do this?Assuming every dict has a value key, you can write (assuming your list is named l)If value might be missing, you can useHere's another way to do it using map() and lambda functions:where l is the list.

I see this way "sexiest", but I would do it using the list comprehension.Update:

In case that 'value' might be missing as a key use:Update: I'm also not a big fan of lambdas, I prefer to name things... this is how I would do it with that in mind:I would even go further and create a sole function that explicitly says what I want to achieve:With Python 3, since map returns an iterator, use list to return a list, e.g. list(map(operator.itemgetter('value'), l)).For a very simple case like this, a comprehension, as in Ismail Badawi's answer is definitely the way to go.But when things get more complicated, and you need to start writing multi-clause or nested comprehensions with complex expressions in them, it's worth looking into other alternatives. There are a few different (quasi-)standard ways to specify XPath-style searches on nested dict-and-list structures, such as JSONPath, DPath, and KVC. And there are nice libraries on PyPI for them.Here's an example with the library named dpath, showing how it can simplify something just a bit more complicated:Or, using jsonpath-ng:This one may not look quite as simple at first glance, because find returns match objects, which include all kinds of things besides just the matched value, such as a path directly to each item. But for more complex expressions, being able to specify a path like '*.[*].value' instead of a comprehension clause for each * can make a big difference. Plus, JSONPath is a language-agnostic specification, and there are even online testers that can be very handy for debugging.I think as simple as below would give you what you are looking for.You can do this with a combination of map and lambda as well but list comprehension looks more elegant and pythonic.For a smaller input list comprehension is way to go but if the input is really big then i guess generators are the ideal way.Here is a comparison of all possible solutions for bigger inputAs you can see, generators are a better solution in comparison to the others, map is also slower compared to generator for reason I will leave up to OP to figure out.Follow the example --Get key values from list of dictionaries in python?Ex:for d in data: Output:A very simple way to do it is:Output:

How do I use method overloading in Python?

user1335578

[How do I use method overloading in Python?](https://stackoverflow.com/questions/10202938/how-do-i-use-method-overloading-in-python)

I am trying to implement method overloading in Python: but the output is second method 2; similarly:givesHow do I make this work?

2012-04-18 04:47:50Z

I am trying to implement method overloading in Python: but the output is second method 2; similarly:givesHow do I make this work?It's method overloading not method overriding. And in Python, you do it all in one function:You can't have two methods with the same name in Python -- and you don't need to.See the Default Argument Values section of the Python tutorial. See "Least Astonishment" and the Mutable Default Argument for a common mistake to avoid.Edit: See PEP 443 for information about the new single dispatch generic functions in Python 3.4.You can also use pythonlangutil:In Python, you don't do things that way. When people do that in languages like Java, they generally want a default value (if they don't, they generally want a method with a different name). So, in Python, you can have default values.As you can see, you can use this to trigger separate behaviour rather than merely having a default value.You can't, never need to and don't really want to.In Python, everything is an object. Classes are things, so they are objects. So are methods.There is an object called A which is a class. It has an attribute called stackoverflow. It can only have one such attribute.When you write def stackoverflow(...): ..., what happens is that you create an object which is the method, and assign it to the stackoverflow attribute of A. If you write two definitions, the second one replaces the first, the same way that assignment always behaves.You furthermore do not want to write code that does the wilder of the sorts of things that overloading is sometimes used for. That's not how the language works.Instead of trying to define a separate function for each type of thing you could be given (which makes little sense since you don't specify types for function parameters anyway), stop worrying about what things are and start thinking about what they can do.You not only can't write a separate one to handle a tuple vs. a list, but also don't want or need to.All you do is take advantage of the fact that they are both, for example, iterable (i.e. you can write for element in container:). (The fact that they aren't directly related by inheritance is irrelevant.)I write my answer in Python 3.2.1.How it works:Usage:I think the word you're looking for is "overloading".  There is no method overloading in python.  You can however use default arguments, as follows.When you pass it an argument it will follow the logic of the first condition and execute the first print statement.  When you pass it no arguments, it will go into the else condition and execute the second print statement.I write my answer in Python 2.7:In Python, method overloading is not possible; if you really want access the same function with different features, I suggest you to go for method overriding.While @agf was right with the answer in the past now with PEP-3124 we got our syntax sugar. See typing documentation for details  on the @overload decorator but note that this is really just syntax sugar and IMHO this is all people have been arguing about ever since. Personally I agree that having multiple functions with different signatures makes it more readable then having a single function with 20+ arguments all set to a default value (None most of the time) and then having to fiddle around using endless if, elif, else chains to find out what the caller actually wants our function to do with the provided set of arguments. This was long overdue following the Python Zen and arguably also Straight from the official Python documentation linked above: In Python, overloading is not an applied concept. However, if you are trying to create a case where, for instance, you want one initializer to be performed if passed an argument of type foo and another initializer for an argument of type bar then, since everything in Python is handled as object, you can check the name of the passed object's class type and write conditional handling based on that.This concept can be applied to multiple different scenarios through different methods as needed.In Python, you'd do this with a default argument.Just came across this https://github.com/bintoro/overloading.py for anybody who may be interested.From the linked repository's readme:Python does not support method overloading like Java or C++. We may overload the methods but can only use the latest defined method.We need to provide optional arguments or *args in order to provide different number of args on calling.Courtesy from https://www.geeksforgeeks.org/python-method-overloading/Python 3.x includes standard typing library which allows for method overloading with the use of @overload decorator. Unfortunately, this is to make the code more readable, as the @overload decorated methods will need to be followed by a non-decorated method that handles different arguments. 

More can be found here here but for your example:In MathMethod.py fileIn Main.py fileWe can overload method by using multipledispatch 

Difference between exit(0) and exit(1) in Python

seeker

[Difference between exit(0) and exit(1) in Python](https://stackoverflow.com/questions/9426045/difference-between-exit0-and-exit1-in-python)

What's the difference between exit(0) and exit(1) in Python?I tried looking around but didn't find a specific question on these lines. If it's already been answered, a link would be sufficient.

2012-02-24 05:49:34Z

What's the difference between exit(0) and exit(1) in Python?I tried looking around but didn't find a specific question on these lines. If it's already been answered, a link would be sufficient.0 and 1 are the exit codes.exit(0) means a clean exit without any errors / problemsexit(1) means there was some issue / error / problem and that is why the program is exiting.This is not Python specific and is pretty common. A non-zero exit code is treated as an abnormal exit, and at times, the error code indicates what the problem was. A zero error code means a successful exit.This is useful for other programs, shell, caller etc. to know what happened with your program and proceed accordingly.This determines the exit status of the program when it finishes running (generally, 0 for success and 1 for error).It is not unique to Python, and the exact effect depends on your operating system and how the program is called (though 99% of the time, if you're just running Python scripts, it doesn't matter).The standard convention for all C programs, including Python, is for exit(0) to indicate success, and exit(1) or any other non-zero value (in the range 1..255) to indicate failure.  Any value outside the range 0..255 is treated modulo 256 (the exit status is stored in an 8-bit value).  Sometimes, that will be treated as signed (so you might see -128, -127, etc) but more usually it is treated as unsigned.This status is available to the code that invoked Python.  This convention applies across platforms, though the meaning of non-zero exit status can vary on different platforms.The number you pass to the exit() function is simply your program's return code, which is given to the operating system. From your program's point of view, there is no difference: execution will end in both cases, and the value supplied to the function will be given to the OS. But some tools and scripts take into account the program's exit code. Most tools return 0 when they succeed and nonzero to indicate an error.So, if your program will be run from a script, an automated tool or from some other software that takes into account the return code (such as an IDE), you must be careful on what you return.When in doubt, just return 0 to indicate everything is OK.exit(0): This causes the program to exit with a successful termination.exit(1): This causes the program to exit with a system-specific meaning.On many systems, exit(1) signals some sort of failure, however there

is no guarantee.As I recall, the C standard only recognizes three standard exit

values:

What's the difference between lists enclosed by square brackets and parentheses in Python?

qazwsx

[What's the difference between lists enclosed by square brackets and parentheses in Python?](https://stackoverflow.com/questions/8900166/whats-the-difference-between-lists-enclosed-by-square-brackets-and-parentheses)

Are they both valid? Is one preferred for some reason?

2012-01-17 19:02:34Z

Are they both valid? Is one preferred for some reason?Square brackets are lists while parentheses are tuples.A list is mutable, meaning you can change its contents:while tuples are not:The other main difference is that a tuple is hashable, meaning that you can use it as a key to a dictionary, among other things. For example:Note that, as many people have pointed out, you can add tuples together. For example:However, this does not mean tuples are mutable. In the example above, a new tuple is constructed by adding together the two tuples as arguments. The original tuple is not modified. To demonstrate this, consider the following:Whereas, if you were to construct this same example with a list, y would also be updated:One interesting difference :A comma must be included in a tuple even if it contains only a single value. e.g. (1,) instead of (1).  They are not lists, they are a list and a tuple. You can read about tuples in the Python tutorial.  While you can mutate lists, this is not possible with tuples.The first is a list, the second is a tuple. Lists are mutable, tuples are not.Take a look at the Data Structures section of the tutorial, and the Sequence Types section of the documentation.Comma-separated items enclosed by  ( and ) are tuples, those enclosed by [ and ] are lists.Another way brackets and parentheses differ is that square brackets can describe a list comprehension, e.g. [x for x in y]Whereas the corresponding parenthetic syntax specifies a tuple generator: (x for x in y)You can get a tuple comprehension using: tuple(x for x in y)See: Why is there no tuple comprehension in Python?

sqlalchemy unique across multiple columns

Ominus

[sqlalchemy unique across multiple columns](https://stackoverflow.com/questions/10059345/sqlalchemy-unique-across-multiple-columns)

Let's say that I have a class that represents locations. Locations "belong" to customers. Locations are identified by a unicode 10 character code. The "location code" should be unique among the locations for a specific customer.So if i have two customers, customer "123" and customer "456". They both can have a location called "main" but neither could have two locations called main.I can handle this in the business logic but I want to make sure there is no way to easily add the requirement in sqlalchemy. The unique=True option seems to only work when applied to a specific field and it would cause the entire table to only have a unique code for all locations.

2012-04-07 23:55:16Z

Let's say that I have a class that represents locations. Locations "belong" to customers. Locations are identified by a unicode 10 character code. The "location code" should be unique among the locations for a specific customer.So if i have two customers, customer "123" and customer "456". They both can have a location called "main" but neither could have two locations called main.I can handle this in the business logic but I want to make sure there is no way to easily add the requirement in sqlalchemy. The unique=True option seems to only work when applied to a specific field and it would cause the entire table to only have a unique code for all locations.Extract from the documentation of the Column:As these belong to a Table and not to a mapped Class, one declares those in the table definition, or if using declarative as in the __table_args__:

Parsing XML with namespace in Python via 'ElementTree'

Sudar

[Parsing XML with namespace in Python via 'ElementTree'](https://stackoverflow.com/questions/14853243/parsing-xml-with-namespace-in-python-via-elementtree)

I have the following XML which I want to parse using Python's ElementTree:I want to find all owl:Class tags and then extract the value of all rdfs:label instances inside them. I am using the following code:Because of the namespace, I am getting the following error.I tried reading the document at http://effbot.org/zone/element-namespaces.htm but I am still not able to get this working since the above XML has multiple nested namespaces.Kindly let me know how to change the code to find all the owl:Class tags.

2013-02-13 12:08:37Z

I have the following XML which I want to parse using Python's ElementTree:I want to find all owl:Class tags and then extract the value of all rdfs:label instances inside them. I am using the following code:Because of the namespace, I am getting the following error.I tried reading the document at http://effbot.org/zone/element-namespaces.htm but I am still not able to get this working since the above XML has multiple nested namespaces.Kindly let me know how to change the code to find all the owl:Class tags.ElementTree is not too smart about namespaces. You need to give the .find(), findall() and iterfind() methods an explicit namespace dictionary. This is not documented very well:Prefixes are only looked up in the namespaces parameter you pass in. This means you can use any namespace prefix you like; the API splits off the owl: part, looks up the corresponding namespace URL in the namespaces dictionary, then changes the search to look for the XPath expression {http://www.w3.org/2002/07/owl}Class instead. You can use the same syntax yourself too of course:If you can switch to the lxml library things are better; that library supports the same ElementTree API, but collects namespaces for you in a .nsmap attribute on elements.Here's how to do this with lxml without having to hard-code the namespaces or scan the text for them (as Martijn Pieters mentions):UPDATE:5 years later I'm still running into variations of this issue.  lxml helps as I showed above, but not in every case.  The commenters may have a valid point regarding this technique when it comes merging documents, but I think most people are having difficulty simply searching documents.Here's another case and how I handled it:xmlns without a prefix means that unprefixed tags get this default namespace.  This means when you search for Tag2, you need to include the namespace to find it.  However, lxml creates an nsmap entry with None as the key, and I couldn't find a way to search for it.  So, I created a new namespace dictionary like thisNote: This is an answer useful for Python's ElementTree standard library without using hardcoded namespaces.To extract namespace's prefixes and URI from XML data you can use ElementTree.iterparse function, parsing only namespace start events (start-ns):Then the dictionary can be passed as argument to the search functions:I've been using similar code to this and have found it's always worth reading the documentation... as usual!findall() will only find elements which are direct children of the current tag. So, not really ALL.It might be worth your while trying to get your code working with the following, especially if you're dealing with big and complex xml files so that that sub-sub-elements (etc.) are also included.

If you know yourself where elements are in your xml, then I suppose it'll be fine! Just thought this was worth remembering.ref: https://docs.python.org/3/library/xml.etree.elementtree.html#finding-interesting-elements

"Element.findall() finds only elements with a tag which are direct children of the current element. Element.find() finds the first child with a particular tag, and Element.text accesses the element’s text content. Element.get() accesses the element’s attributes:"To get the namespace in its namespace format, e.g. {myNameSpace}, you can do the following:This way, you can use it later on in your code to find nodes, e.g using string interpolation (Python 3).My solution is based on @Martijn Pieters' comment:So the trick here is to use different dictionaries for serialization and for searching.Now, register all namespaces for parsing and writing:For searching (find(), findall(), iterfind()) we need a non-empty prefix. Pass these functions a modified dictionary (here I modify the original dictionary, but this must be made only after the namespaces are registered).Now, the functions from the find() family can be used with the default prefix:butdoes not use any prefixes for elements in the default namespace.

Python: zip-like function that pads to longest length?

Mark Harrison

[Python: zip-like function that pads to longest length?](https://stackoverflow.com/questions/1277278/python-zip-like-function-that-pads-to-longest-length)

Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input?

2009-08-14 11:04:09Z

Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input?In Python 3 you can use itertools.zip_longestYou can pad with a different value than None by using the fillvalue parameter:With Python 2 you can either use itertools.izip_longest (Python 2.6+), or you can use map with None. It is a little known feature of map (but map changed in Python 3.x, so this only works in Python 2.x).For Python 2.6x use itertools module's izip_longest.For Python 3 use zip_longest instead (no leading i).non itertools Python 3 solution:non itertools My Python 2 solution:Im using a 2d array but the concept is the similar using python 2.x:

Python - doctest vs. unittest [closed]

Sean

[Python - doctest vs. unittest [closed]](https://stackoverflow.com/questions/361675/python-doctest-vs-unittest)

I'm trying to get started with unit testing in Python and I was wondering if someone could explain the advantages and disadvantages of doctest and unittest.  What conditions would you use each for?

2008-12-12 01:50:19Z

I'm trying to get started with unit testing in Python and I was wondering if someone could explain the advantages and disadvantages of doctest and unittest.  What conditions would you use each for?Both are valuable.  I use both doctest and nose taking the place of unittest. I use doctest for cases where the test is giving an example of usage that is actually useful as documentation.  Generally I don't make these tests comprehensive, aiming solely for informative.  I'm effectively using doctest in reverse:  not to test my code is correct based on my doctest, but to check that my documentation is correct based on the code.The reason is that I find comprehensive doctests will clutter your documentation far too much, so you will either end up with either unusable docstrings, or incomplete testing.For actually testing the code, the goal is to thoroughly test every case, rather than illustrate what is does by example, which is a different goal which I think is better met by other frameworks.I use unittest almost exclusively.Once in a while, I'll put some stuff in a docstring that's usable by doctest.95% of the test cases are unittest.Why?  I like keeping docstrings somewhat shorter and more to the point.  Sometimes test cases help clarify a docstring.  Most of the time, the application's test cases are too long for a docstring.Another advantage of doctesting is that you get to make sure your code does what your documentation says it does.  After a while, software changes can make your documentation and code do different things.  :-)I work as a bioinformatician, and most of the code I write is "one time, one task" scripts, code that will be run only once or twice and that execute a single specific task.In this situation, writing big unittests may be overkill, and doctests are an useful compromise. They are quicker to write, and since they are usually incorporated in the code, they allow to always keep an eye on how the code should behave, without having to have another file open. That's useful when writing small script.Also, doctests are useful when you have to pass your script to a researcher that is not expert in programming. Some people find it very difficult to understand how unittests are structured; on the other hand, doctests are simple examples of usage, so people can just copy and paste them to see how to use them.So, to resume my answer: doctests are useful when you have to write small scripts, and when you have to pass them or show them to researchers that are not computer scientists.If you're just getting started with the idea of unit testing, I would start with doctest because it is so simple to use. It also naturally provides some level of documentation. And for more comprehensive testing with doctest, you can place tests in an external file so it doesn't clutter up your documentation.I would suggest unittest if you're coming from a background of having used JUnit or something similar, where you want to be able to write unit tests in generally the same way as you have been elsewhere.I use unittest exclusively; I think doctest clutters up the main module too much.  This probably has to do with writing thorough tests.Using both is a valid and rather simple option. The doctest module provides the DoctTestSuite and DocFileSuite methods which create a unittest-compatible testsuite from a module or file, respectively.So I use both and typically use doctest for simple tests with functions that require little or no setup (simple types for arguments). I actually think a few doctest tests help document the function, rather than detract from it.But for more complicated cases, and for a more comprehensive set of test cases, I use unittest which provides more control and flexibility.I don't use doctest as a replacement for unittest. Although they overlap a bit, the two modules don't have the same function:The widely documented benefits of test driven development I get from unittest. doctest solves the far more subtle danger of having outdated comments misleading the maintenance of the code.I almost never use doctests. I want my code to be self documenting, and the docstrings provide the documentation to the user. IMO adding hundreds of lines of tests to a module makes the docstrings far less readable. I also find unit tests easier to modify when needed. Doctest can some times lead to wrong result. Especially when output contains escape sequences. For examplegivesAlso doesn't check the type of the output. It just compares the output strings. For example it have made some type rational which prints just like integer if it is a whole number. Then suppose you have function which return rational. So, a doctest won't differentiate if the output is rational whole number or a integer number.I prefer the discovery based systems ("nose" and "py.test", using the former currently).doctest is nice when the test is also good as a documentation, otherwise they tend to clutter the code too much.

Setting different color for each series in scatter plot on matplotlib

Yotam

[Setting different color for each series in scatter plot on matplotlib](https://stackoverflow.com/questions/12236566/setting-different-color-for-each-series-in-scatter-plot-on-matplotlib)

Suppose I have three data sets:I can scatter plot this:How can I do this with 10 sets? I searched for this and could find any reference to what I'm asking.Edit: clarifying (hopefully) my question  If I call scatter multiple times, I can only set the same color on each scatter. Also, I know I can set a color array manually but I'm sure there is a better way to do this. 

My question is then, "How can I automatically scatter-plot my several data sets, each with a different color. If that helps, I can easily assign a unique number to each data set. 

2012-09-02 14:02:12Z

Suppose I have three data sets:I can scatter plot this:How can I do this with 10 sets? I searched for this and could find any reference to what I'm asking.Edit: clarifying (hopefully) my question  If I call scatter multiple times, I can only set the same color on each scatter. Also, I know I can set a color array manually but I'm sure there is a better way to do this. 

My question is then, "How can I automatically scatter-plot my several data sets, each with a different color. If that helps, I can easily assign a unique number to each data set. I don't know what you mean by 'manually'.  You can choose a colourmap and make a colour array easily enough:Or you can make your own colour cycler using itertools.cycle and specifying the colours you want to loop over, using next to get the one you want.  For example, with 3 colours:Come to think of it, maybe it's cleaner not to use zip with the first one too:The normal way to plot plots with points in different colors in matplotlib is to pass a list of colors as a parameter.E.g.:When you have a list of lists and you want them colored per list.

I think the most elegant way is that suggesyted by @DSM,

just do a loop making multiple calls to scatter.But if for some reason you wanted to do it with just one call, you can make a big list of colors, with a list comprehension and a bit of flooring division:If you have only one type of collections (e.g. scatter with no error bars) you can also change the colours after that you have plotted them, this sometimes is easier to perform.The output gives you differnent colors even when you have many different scatter plots in the same subplot.You can always use the plot() function like so:This question is a bit tricky before Jan 2013 and matplotlib 1.3.1 (Aug 2013), which is the oldest stable version you can find on matpplotlib website. But after that it is quite trivial.Because present version of matplotlib.pylab.scatter support assigning: array of colour name string, array of float number with colour map, array of RGB or RGBA.this answer is dedicate to @Oxinabox's endless passion for correcting the 2013 version of myself in 2015.you have two option of using scatter command with multiple colour in a single call.the code is also inspired by the source code of pyplot.scatter, I just duplicated what scatter does without trigger it to draw. the command pyplot.scatter return a PatchCollection Object, in the file "matplotlib/collections.py" a private variable _facecolors in Collection class and a method set_facecolors. so whenever you have a scatter points to draw you can do this:This works for me:for each series, use a random rgb colour generatorA MUCH faster solution for large dataset and limited number of colors is the use of Pandas and the groupby function:

How to sort mongodb with pymongo

WildBill

[How to sort mongodb with pymongo](https://stackoverflow.com/questions/8109122/how-to-sort-mongodb-with-pymongo)

I'm trying to use the sort feature when querying my mongoDB, but it is failing.  The same query works in the MongoDB console but not here.  Code is as follows:The error I get is as follows:I found a link elsewhere that says I need to place a 'u' infront of the key if using pymongo, but that didn't work either.  Anyone else get this to work or is this a bug.

2011-11-13 02:13:58Z

I'm trying to use the sort feature when querying my mongoDB, but it is failing.  The same query works in the MongoDB console but not here.  Code is as follows:The error I get is as follows:I found a link elsewhere that says I need to place a 'u' infront of the key if using pymongo, but that didn't work either.  Anyone else get this to work or is this a bug..sort(), in pymongo, takes key and direction as parameters.So if you want to sort by, let's say, id then you should .sort("_id", 1)For multiple fields:You can try this:This also works:I'm using this in my code, please comment if i'm doing something wrong here, thanks.Why python uses list of tuples instead dict?In python you cannot guarantee that dictionary will be interpreted in the order you declared. So, in mongo shell you could do .sort({'field1':1,'field2':1}) and the interpreter should sort field1 at first level and field 2 at second level.If this sintax was used in python, there is a chance to sort field2 at first level. With tuple there is no risk.Python uses key,direction. You can use the above way.So in your case you can do thisTLDR: Aggregation pipeline is faster as compared to conventional .find().sort().Now moving to the real explanation. There are two ways to perform sorting operations in MongoDB:As suggested by many .find().sort() is the simplest way to perform the sorting.However, this is a slow process compared to the aggregation pipeline. Coming to the aggregation pipeline method. The steps to implement simple aggregation pipeline intended for sorting are:NOTE: In my experience, the aggregation pipeline works a bit faster than the .find().sort() method.Here's an example of the aggregation pipeline.Try this method yourself, compare the speed and let me know about this in the comments.Edit: Do not forget to use allowDiskUse=True while sorting on multiple fields otherwise it will throw an error.

Encoding an image file with base64

rectangletangle

[Encoding an image file with base64](https://stackoverflow.com/questions/3715493/encoding-an-image-file-with-base64)

I want to encode an image into a string using the base64 module. I've ran into a problem though. How do I specify the image I want to be encoded? I tried using the directory to the image, but that simply leads to the directory being encoded. I want the actual image file to be encoded. EDITI tried this snippet:but I get the following error:What am I doing wrong? 

2010-09-15 07:24:48Z

I want to encode an image into a string using the base64 module. I've ran into a problem though. How do I specify the image I want to be encoded? I tried using the directory to the image, but that simply leads to the directory being encoded. I want the actual image file to be encoded. EDITI tried this snippet:but I get the following error:What am I doing wrong? I'm not sure I understand your question. I assume you are doing something along the lines of:You have to open the file first of course, and read its contents - you cannot simply pass the path to the encode function.Edit:

Ok, here is an update after you have edited your original question.First of all, remember to use raw strings (prefix the string with 'r') when using path delimiters on Windows, to prevent accidentally hitting an escape character. Second, PIL's Image.open either accepts a filename, or a file-like (that is, the object has to provide read, seek and tell methods). That being said, you can use cStringIO to create such an object from a memory buffer:With python 2.x, you can trivially encode using .encode:As I said in your previous question, there is no need to base64 encode the string, it will only make the program slower. Just use the reprNow the image is stored as a variable called image_data in a file called image.py

Start a fresh interpreter and import the image_dataThe first answer will print a string with prefix b'.

That means your string will be like this b'your_string' To solve this issue please add the following line of code.Borrowing from what Ivo van der Wijk and gnibbler have developed earlier, this is a dynamic solutionYou can then decide to compile the output image file with Cython to make it cool. With this method, you can bundle all your graphics into one module.

UnicodeDecodeError: 'utf8' codec can't decode byte 0xa5 in position 0: invalid start byte

Dipak Ingole

[UnicodeDecodeError: 'utf8' codec can't decode byte 0xa5 in position 0: invalid start byte](https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s)

I am using Python-2.6 CGI scripts but found this error in server log while doing json.dumps(),​Here ,​__get​data() function returns dictionary {} .Before posting this question I have referred this of question os SO.Following line is hurting JSON encoder,I got a temporary fix for itBut I am not sure is it correct way to do it.

2014-03-06 05:47:37Z

I am using Python-2.6 CGI scripts but found this error in server log while doing json.dumps(),​Here ,​__get​data() function returns dictionary {} .Before posting this question I have referred this of question os SO.Following line is hurting JSON encoder,I got a temporary fix for itBut I am not sure is it correct way to do it.The error is because there is some non-ascii character in the dictionary and it can't be encoded/decoded. One simple way to avoid this error is to encode such strings with encode() function as follows (if a is the string with non-ascii character):Try the below code snippet:I switched this simply by defining a different codec package in the read_csv() command:encoding = 'unicode_escape'Eg:Your string has a non ascii character encoded in it.Not being able to decode with utf-8 may happen if you've needed to use other encodings in your code. For example:In this case, the encoding is windows-1252 so you have to do:Now that you have Unicode, you can safely encode into utf-8.On read csv, I added an encoding method:Set default encoder at the top of your codeAs of 2018-05 this is handled directly with decode, at least for Python 3. I'm using the below snippet for invalid start byte and invalid continuation byte type errors. Adding errors='ignore' fixed it for me.Inspired by @aaronpenne and @SoumyaanshSimple Solution:Following line is hurting JSON encoder,I got a temporary fix for itMarking this as correct as a temporary fix (Not sure so).If the above methods are not working for you, you may want to look into changing the encoding of the csv file itself.Using Excel:Using Notepad:By doing this, you should be able to import csv files without encountering the UnicodeCodeError.This solution worked for me:After trying all the aforementioned workarounds, if it still throws the same error, you can try exporting the file as CSV (a second time if you already have).

Especially if you're using scikit learn, it is best to import the dataset as a CSV file.I spent hours together, whereas the solution was this simple. Export the file as a CSV to the directory where Anaconda or your classifier tools are installed and try.You may use any standard encoding of your specific usage and input.utf-8 is the default.iso8859-1 is also popular for Western Europe.e.g: bytes_obj.decode('iso8859-1') see: docsInstead of looking for ways to decode a5 (Yen ¥) or 96 (en-dash –), tell MySQL that your client is encoded "latin1", but you want "utf8" in the database.See details in Trouble with UTF-8 characters; what I see is not what I stored

Python + Django page redirect

Kyle Hayes

[Python + Django page redirect](https://stackoverflow.com/questions/523356/python-django-page-redirect)

How do I accomplish a simple redirect (e.g. cflocation in ColdFusion, or header(location:http://) for PHP) in Django?

2009-02-07 07:08:03Z

How do I accomplish a simple redirect (e.g. cflocation in ColdFusion, or header(location:http://) for PHP) in Django?It's simple:More info in the official Django docsUpdate: Django 1.0There is apparently a better way of doing this in Django now using generic views.Example -There is more in the generic views documentation.

Credit - Carles Barrobés.Update #2: Django 1.3+In Django 1.5 redirect_to no longer exists and has been replaced by RedirectView. Credit to YonatanDepending on what you want (i.e. if you do not want to do any additional pre-processing), it is simpler to just use Django's redirect_to generic view:See documentation for more advanced examples.For Django 1.3+ use:There's actually a simpler way than having a view for each redirect - you can do it directly in urls.py:A target can be a callable as well as a string, which is what I'm using here.Since Django 1.1, you can also use the simpler redirect shortcut:It also takes an optional permanent=True keyword argument.If you want to redirect a whole subfolder, the url argument in RedirectView is actually interpolated, so you can do something like this in urls.py:The ?P<path> you capture will be fed into RedirectView.  This captured variable will then be replaced in the url argument you gave, giving us /new_path/yay/mypath if your original path was /old/yay/mypath.You can also do ….as_view(url='…', query_string=True) if you want to copy the query string over as well.With Django version 1.3, the class based approach is:This example lives in in urls.pyBeware.  I did this on a development server and wanted to change it later. I had to clear my caches to change it.  In order to avoid this head-scratching in the future, I was able to make it temporary like so:You can do this in the Admin section. It's explained in the documentation.https://docs.djangoproject.com/en/dev/ref/contrib/redirects/page_path = define in urls.pyThis should work in most versions of django, I am using it in 1.6.5:You can still use the name of the url pattern instead of a hard coded url with this solution.  The location_id parameter from the url is passed down to the lambda function.

Django optional url parameters

Darwin Tech

[Django optional url parameters](https://stackoverflow.com/questions/14351048/django-optional-url-parameters)

I have a Django URL like this:views.py:The problem is that I want the project_id parameter to be optional.I want /project_config/ and /project_config/12345abdce/ to be equally valid URL patterns, so that if project_id is passed, then I can use it.As it stands at the moment, I get a 404 when I access the URL without the project_id parameter.

2013-01-16 03:46:35Z

I have a Django URL like this:views.py:The problem is that I want the project_id parameter to be optional.I want /project_config/ and /project_config/12345abdce/ to be equally valid URL patterns, so that if project_id is passed, then I can use it.As it stands at the moment, I get a 404 when I access the URL without the project_id parameter.There are several approaches.One is to use a non-capturing group in the regex:    (?:/(?P<title>[a-zA-Z]+)/)?

Making a Regex Django URL Token OptionalAnother, easier to follow way is to have multiple rules that matches your needs, all pointing to the same view.Keep in mind that in your view you'll also need to set a default for the optional URL parameter, or you'll get an error:You can use nested routesDjango <1.8Django >=1.8This is a lot more DRY (Say you wanted to rename the product kwarg to product_id, you only have to change line 4, and it will affect the below URLs.Edited for Django 1.8 and aboveEven simpler is to use:The "(a|b)" means a or b, so in your case it would be one or more word characters (\w+) or nothing.So it would look like:Django > 2.0 version:The approach is essentially identical with the one given in Yuji 'Tomita' Tomita's Answer. Affected, however, is the syntax:Using path() you can also pass extra arguments to a view with the optional argument kwargs that is of type dict. In this case your view would not need a default for the attribute project_id:For how this is done in the most recent Django version, see the official docs about URL dispatching.Thought I'd add a bit to the answer.If you have multiple URL definitions then you'll have to name each of them separately. So you lose the flexibility when calling reverse since one reverse will expect a parameter while the other won't.Another way to use regex to accommodate the optional parameter:Django = 2.2

Installing SciPy and NumPy using pip

eran

[Installing SciPy and NumPy using pip](https://stackoverflow.com/questions/11114225/installing-scipy-and-numpy-using-pip)

I'm trying to create required libraries in a package I'm distributing. It requires both the SciPy and NumPy libraries.

While developing, I installed both usingwhich installed SciPy 0.9.0 and NumPy 1.5.1, and it worked fine.I would like to do the same using pip install - in order to be able to specify dependencies in a setup.py of my own package.The problem is, when I try:it works fine.But thenfails miserably, withHow do I get it to work?

2012-06-20 06:48:00Z

I'm trying to create required libraries in a package I'm distributing. It requires both the SciPy and NumPy libraries.

While developing, I installed both usingwhich installed SciPy 0.9.0 and NumPy 1.5.1, and it worked fine.I would like to do the same using pip install - in order to be able to specify dependencies in a setup.py of my own package.The problem is, when I try:it works fine.But thenfails miserably, withHow do I get it to work?I am assuming Linux experience in my answer; I found that there are three prerequisites to getting pip install scipy to proceed nicely.Go here: Installing SciPYFollow the instructions to download, build and export the env variable for BLAS and then LAPACK. Be careful to not just blindly cut'n'paste the shell commands - there will be a few lines you need to select depending on your architecture, etc., and you'll need to fix/add the correct directories that it incorrectly assumes as well.The third thing you may need is to yum install numpy-f2py or the equivalent.Oh, yes and lastly, you may need to yum install gcc-gfortran as the libraries above are Fortran source.This worked for me on Ubuntu 14.04:you need the libblas and liblapack dev packages if you are using Ubuntu.Since the previous instructions for installing with yum are broken here are the updated instructions for installing on something like fedora. I've tested this on "Amazon Linux AMI 2016.03"I was working on a project that depended on numpy and scipy. In a clean installation of Fedora 23, using a python virtual environment for Python 3.4 (also worked for Python 2.7), and with the following in my setup.py (in the setup() method)I found I had to run the following to get pip install -e . to work:and The redhat-rpm-config is for scipy's use of redhat-hardened-cc1 as opposed to the regular cc1What operating system is this? The answer might depend on the OS involved. However, it looks like you need to find this BLAS library and install it. It doesn't seem to be in PIP (you'll have to do it by hand thus), but if you install it, it ought let you progress your SciPy install.On windows, using python 3.5, I managed to install scipy by using conda not pip:in my case, upgrading pip did the trick. Also, I've installed scipy with -U parameter (upgrade all packages to the last available version)

What's the correct way to sort Python `import x` and `from x import y` statements?

ThiefMaster

[What's the correct way to sort Python `import x` and `from x import y` statements?](https://stackoverflow.com/questions/20762662/whats-the-correct-way-to-sort-python-import-x-and-from-x-import-y-statement)

The python style guide suggests to group imports like this:However, it does not mention anything how the two different ways of imports should be laid out:There are multiple ways to sort them (let's assume all those import belong to the same group):PEP8 does not mention the preferred order for this and the "cleanup imports" features some IDEs have probably just do whatever the developer of that feature preferred.I'm looking for another PEP clarifying this or a relevant comment/email from the BDFL (or another Python core developer). Please don't post subjective answers stating your own preference.

2013-12-24 14:37:57Z

The python style guide suggests to group imports like this:However, it does not mention anything how the two different ways of imports should be laid out:There are multiple ways to sort them (let's assume all those import belong to the same group):PEP8 does not mention the preferred order for this and the "cleanup imports" features some IDEs have probably just do whatever the developer of that feature preferred.I'm looking for another PEP clarifying this or a relevant comment/email from the BDFL (or another Python core developer). Please don't post subjective answers stating your own preference.Imports are generally sorted alphabetically and described in various places beside PEP 8.Alphabetically sorted modules are quicker to read and searchable. After all python is all about readability.

Also It is easier to verify that something is imported, and avoids duplicated importsThere is nothing available in PEP 8 regarding sorting.So its all about choice what you use.According to few references from reputable sites and repositories also popularity, Alphabetical ordering is the way.for eg like this:ORReddit official repository also states that, In general PEP-8 import ordering should be used. However there are a few additions which isReferences:PS: the isort utility automatically sorts your imports.According to the CIA's internal coding conventions (part of the WikiLeaks Vault 7 leak), python imports should be grouped into three groups:Imports should be ordered lexicographically within these groups, ignoring case:The PEP 8 says nothing about it indeed. There's no convention for this point, and it doesn't mean the Python community need to define one absolutely. A choice can be better for a project but the worst for another... It's a question of preferences for this, since each solutions has pro and cons. But if you want to follow conventions, you have to respect the principal order you quoted:For example, Google recommend in this page that import should be sorted lexicographically, in each categories (standard/third parties/yours). But at Facebook, Yahoo and whatever, it's maybe another convention...I highly recommend reorder-python-imports. It follows the 2nd option of the accepted answer and also integrates into pre-commit, which is super helpful.All import x statements should be sorted by the value of x and all from x import y statements should be sorted by the value of x in alphabetical order and the sorted groups of from x import y statements must follow the sorted group of import x statements.

How to validate IP address in Python? [duplicate]

krupan

[How to validate IP address in Python? [duplicate]](https://stackoverflow.com/questions/319279/how-to-validate-ip-address-in-python)

What's the best way to validate that an IP entered by the user is valid?  It comes in as a string.

2008-11-25 23:40:45Z

What's the best way to validate that an IP entered by the user is valid?  It comes in as a string.Don't parse it.  Just ask.The IPy module (a module designed for dealing with IP addresses) will throw a ValueError exception for invalid addresses.However, like Dustin's answer, it will accept things like "4" and "192.168" since, as mentioned, these are valid representations of IP addresses.If you're using Python 3.3 or later, it now includes the ipaddress module:For Python 2, you can get the same functionality using ipaddress if you install python-ipaddress:This module is compatible with Python 2 and provides a very similar API to that of the ipaddress module included in the Python Standard Library since Python 3.3. More details here. In Python 2 you will need to explicitly convert the IP address string to unicode: ipaddress.ip_address(u'127.0.0.1').From Python 3.4 on, the best way to check if an IPv6 or IPv4 address is correct, is to use the Python Standard Library module ipaddress - IPv4/IPv6 manipulation library s.a. https://docs.python.org/3/library/ipaddress.html for complete documentation.Example : 

For other versions: Github, phihag / Philipp Hagemeister,"Python 3.3's ipaddress for older Python versions", https://github.com/phihag/ipaddressThe backport from phihag is available e.g. in Anaconda Python 2.7 & is included in Installer. s.a. https://docs.continuum.io/anaconda/pkg-docsTo install with pip:s.a.: ipaddress 1.0.17, "IPv4/IPv6 manipulation library", "Port of the 3.3+ ipaddress module", https://pypi.python.org/pypi/ipaddress/1.0.17IPv4:IPv6:The IPv6 version uses "(?:(?<=::)|(?<!::):)", which could be replaced with "(?(?<!::):)" on regex engines that support conditionals with look-arounds. (i.e. PCRE, .NET)Edit:Edit2:I found some links discussing how to parse IPv6 addresses with regex:Edit3:Finally managed to write a pattern that passes all tests, and that I am also happy with.I hope it's simple and pythonic enough:I have to give a great deal of credit to Markus Jarderot for his post - the majority of my post is inspired from his.I found that Markus' answer still fails some of the IPv6 examples in the Perl script referenced by his answer.Here is my regex that passes all of the examples in that Perl script:I also put together a Python script to test all of those IPv6 examples; it's here on Pastebin because it was too large to post here.You can run the script with test result and example arguments in the form of "[result]=[example]", so like:or you can simply run all of the tests by specifying no arguments, so like:Anyway, I hope this helps somebody else!I think this would do it...Consider IPv4 address as "ip".I came up with this noob simple version I only needed to parse IP v4 addresses. My solution based on Chills strategy follows:

Get protocol + host name from URL

Gerard

[Get protocol + host name from URL](https://stackoverflow.com/questions/9626535/get-protocol-host-name-from-url)

In my Django app, I need to get the host name from the referrer in request.META.get('HTTP_REFERER') along with its protocol so that from URLs like:I should get:I looked over other related questions and found about urlparse, but that didn't do the trick since

2012-03-08 23:12:38Z

In my Django app, I need to get the host name from the referrer in request.META.get('HTTP_REFERER') along with its protocol so that from URLs like:I should get:I looked over other related questions and found about urlparse, but that didn't do the trick sinceYou should be able to do it with urlparse (docs: python2, python3):https://github.com/john-kurkowski/tldextractThis is a more verbose version of urlparse.  It detects domains and subdomains for you.From their documentation:ExtractResult is a namedtuple, so it's simple to access the parts you want.Pure string operations :):That's all, folks.if you think your url is valid then this will work all the timeIs there anything wrong with pure string operations:If you prefer having a trailing slash appended, extend this script a bit like so:That can probably be optimized a bit ...Here is a slightly improved version:OutputFiddle: https://pyfiddle.io/fiddle/23e4976e-88d2-4757-993e-532aa41b7bf0/?i=trueThis is a bit obtuse, but uses urlparse in both directions:that odd ("",) * 4 bit is because urlparse expects a sequence of exactly len(urlparse.ParseResult._fields) = 6I know it's an old question, but I too encountered it today.

Solved this with an one-liner:to get domain/hostname and Origin**Origin is used in XMLHttpRequest headersIt could be solved by re.search()The standard library function urllib.parse.urlsplit() is all you need. Here is an example for Python3:If it contains less than 3 slashes thus you've it got and if not then we can find the occurrence between it:

How to limit the maximum value of a numeric field in a Django model?

sampablokuper

[How to limit the maximum value of a numeric field in a Django model?](https://stackoverflow.com/questions/849142/how-to-limit-the-maximum-value-of-a-numeric-field-in-a-django-model)

Django has various numeric fields available for use in models, e.g. DecimalField and PositiveIntegerField. Although the former can be restricted to the number of decimal places stored and the overall number of characters stored, is there any way to restrict it to storing only numbers within a certain range, e.g. 0.0-5.0 ?Failing that, is there any way to restrict a PositiveIntegerField to only store, for instance, numbers up to 50?Update: now that Bug 6845 has been closed, this StackOverflow question may be moot. - sampablokuper

2009-05-11 17:29:57Z

Django has various numeric fields available for use in models, e.g. DecimalField and PositiveIntegerField. Although the former can be restricted to the number of decimal places stored and the overall number of characters stored, is there any way to restrict it to storing only numbers within a certain range, e.g. 0.0-5.0 ?Failing that, is there any way to restrict a PositiveIntegerField to only store, for instance, numbers up to 50?Update: now that Bug 6845 has been closed, this StackOverflow question may be moot. - sampablokuperYou could also create a custom model field type - see http://docs.djangoproject.com/en/dev/howto/custom-model-fields/#howto-custom-model-fieldsIn this case, you could 'inherit' from the built-in IntegerField and override its validation logic.The more I think about this, I realize how useful this would be for many Django apps. Perhaps a IntegerRangeField type could be submitted as a patch for the Django devs to consider adding to trunk.This is working for me:Then in your model class, you would use it like this (field being the module where you put the above code):OR for a range of negative and positive (like an oscillator range):What would be really cool is if it could be called with the range operator like this:But, that would require a lot more code since since you can specify a 'skip' parameter - range(1, 50, 2) - Interesting idea though...You can use Django's built-in validators—Edit: When working directly with the model, make sure to call the model full_clean method before saving the model in order to trigger the validators. This is not required when using ModelForm since the forms will do that automatically.I had this very same problem; here was my solution:There are two ways to do this. One is to use form validation to never let any number over 50 be entered by a user. Form validation docs.If there is no user involved in the process, or you're not using a form to enter data, then you'll have to override the model's save method to throw an exception or limit the data going into the field.Here is the best solution if you want some extra flexibility and don't want to change your model field. Just add this custom validator:And it can be used as such:The two parameters are max and min, and it allows nulls. You can customize the validator if you like by getting rid of the marked if statement or change your field to be blank=False, null=False in the model.  That will of course require a migration.Note: I had to add the validator because Django does not validate the range on PositiveSmallIntegerField, instead it creates a smallint (in postgres) for this field and you get a DB error if the numeric specified is out of range.Hope this helps :) More on Validators in Django.PS. I based my answer on BaseValidator in django.core.validators, but everything is different except for the code.

Image library for Python 3

banx

[Image library for Python 3](https://stackoverflow.com/questions/3896286/image-library-for-python-3)

What is python-3 using instead of PIL for manipulating Images?

2010-10-09 15:56:23Z

What is python-3 using instead of PIL for manipulating Images?The "friendly PIL fork" Pillow works on Python 2 and 3. Check out the Github project for support matrix and so on.Christoph Gohlke managed to build PIL (for Windows only) for python versions up to 3.3: http://www.lfd.uci.edu/~gohlke/pythonlibs/I tried his version of PIL with Python 3.2, and image open/create/pixel manipulation/save all work.Qt works very well with graphics. In my opinion it is more versatile than PIL.You get all the features you want for graphics manipulation, but there's also vector graphics and even support for real printers. And all of that in one uniform API, QPainter.To use Qt you need a Python binding for it: PySide or PyQt4.

They both support Python 3.Here is a simple example that loads a JPG image, draws an antialiased circle of radius 10 at coordinates (20, 20) with the color of the pixel that was at those coordinates and saves the modified image as a PNG file:But please note that this solution is quite 'heavyweight', because Qt is a large framework for making GUI applications.As of March 30, 2012, I have tried and failed to get the sloonz fork on GitHub to open images.  I got it to compile ok, but it didn't actually work.  I also tried building gohlke's library, and it compiled also but failed to open any images.  Someone mentioned PythonMagick above, but it only compiles on Windows.  See PythonMagick on the wxPython wiki.PIL was last updated in 2009, and while it's website says they are working on a Python 3 port, it's been 3 years, and the mailing list has gone cold.To solve my Python 3 image manipulation problem, I am using subprocess.call() to execute ImageMagick shell commands.  This method works.See the subprocess module documentation.You can use my package mahotas on Python 3. It is numpy-based rather than PIL based.You want the Pillow library, here is how to install it on Python 3:If that does not work for you (it should), try normal pip:Depending on what is needed, scikit-image may be the best choice, with manipulations going way beyond PIL and the current version of Pillow.  Very well-maintained, at least as much as Pillow.  Also, the underlying data structures are from Numpy and Scipy, which makes its code incredibly interoperable. Examples that pillow can't handle:You can see its power in the gallery.   This paper provides a great intro to it.  Good luck!If you are on Python3 you can also use the library PILasOPENCV which works in Python 2 and 3. Function api calls are the same as in PIL or pillow but internally it works with OpenCV and numpy to load, save and manipulate images. Have a look at https://github.com/bunkahle/PILasOPENCV or install it with pip install PILasOPENCV. Not all PIL functions have been simulated but the most common functions work.

UnicodeEncodeError: 'charmap' codec can't encode - character maps to <undefined>, print function [duplicate]

Carlos Eugenio Thompson Pinzón

[UnicodeEncodeError: 'charmap' codec can't encode - character maps to <undefined>, print function [duplicate]](https://stackoverflow.com/questions/14630288/unicodeencodeerror-charmap-codec-cant-encode-character-maps-to-undefined)

I am writing a Python (Python 3.3) program to send some data to a webpage using POST method.  Mostly for debugging process I am getting the page result and displaying it on the screen using print() function.The code is like this:the HTTPResponse .read() method returns a bytes element encoding the page (which is a well formated UTF-8 document)  It seemed okay until I stopped using IDLE GUI for Windows and used the Windows console instead.  The returned page has a U+2014 character (em-dash) which the print function translates well in the Windows GUI (I presume Code Page 1252) but does not in the Windows Console (Code Page 850).  Given the strict default behavior I get the following error:I could fix it using this quite ugly code:Now it replace the offending character "—" with a ?.  Not the ideal case (a hyphen should be a better replacement) but good enough for my purpose.There are several things I do not like from my solution.The problem is not the emdash (I can think of several ways to solve that particularly problem) but I need to write robust code.  I am feeding the page with data from a database and that data can come back.  I can anticipate many other conflicting cases: an 'Á' U+00c1 (which is possible in my database) could translate into CP-850 (DOS/Windows Console encodign for Western European Languages) but not into CP-437 (encoding for US English, which is default in many Windows instalations).So, the question:Is there a nicer solution that makes my code agnostic from the output interface encoding?

2013-01-31 16:18:30Z

I am writing a Python (Python 3.3) program to send some data to a webpage using POST method.  Mostly for debugging process I am getting the page result and displaying it on the screen using print() function.The code is like this:the HTTPResponse .read() method returns a bytes element encoding the page (which is a well formated UTF-8 document)  It seemed okay until I stopped using IDLE GUI for Windows and used the Windows console instead.  The returned page has a U+2014 character (em-dash) which the print function translates well in the Windows GUI (I presume Code Page 1252) but does not in the Windows Console (Code Page 850).  Given the strict default behavior I get the following error:I could fix it using this quite ugly code:Now it replace the offending character "—" with a ?.  Not the ideal case (a hyphen should be a better replacement) but good enough for my purpose.There are several things I do not like from my solution.The problem is not the emdash (I can think of several ways to solve that particularly problem) but I need to write robust code.  I am feeding the page with data from a database and that data can come back.  I can anticipate many other conflicting cases: an 'Á' U+00c1 (which is possible in my database) could translate into CP-850 (DOS/Windows Console encodign for Western European Languages) but not into CP-437 (encoding for US English, which is default in many Windows instalations).So, the question:Is there a nicer solution that makes my code agnostic from the output interface encoding?I see three solutions to this:Based on Dirk Stöcker's answer, here's a neat wrapper function for Python 3's print function. Use it just like you would use print.As an added bonus, compared to the other answers, this won't print your text as a bytearray ('b"content"'), but as normal strings ('content'), because of the last decode step.For debugging purposes, you could use print(repr(data)).To display text, always print Unicode. Don't hardcode the character encoding of your environment such as Cp850 inside your script. To decode the HTTP response, see A good way to get the charset/encoding of an HTTP response in Python.To print Unicode to Windows console, you could use win-unicode-console package.I dug deeper into this and found the best solutions are here.http://blog.notdot.net/2010/07/Getting-unicode-right-in-PythonIn my case I solved "UnicodeEncodeError: 'charmap' codec can't encode character "original code:New code:If you are using Windows command line to print the data, you should use This worked for me!If you use Python 3.6 (possibly 3.5 or later), it doesn't give that error to me anymore.  I had a similar issue, because I was using v3.4, but it went away after I uninstalled and reinstalled. 

Why is apt-get function not working in terminal on mac osx 10.9?

user2800761

[Why is apt-get function not working in terminal on mac osx 10.9?](https://stackoverflow.com/questions/19688424/why-is-apt-get-function-not-working-in-terminal-on-mac-osx-10-9)

I was watching this and as you can see the first command I am told to put in is:When I do this, it outputs:I have no idea why this is the case. How can I resolve this so I am following the tutorial correctly?

2013-10-30 16:25:49Z

I was watching this and as you can see the first command I am told to put in is:When I do this, it outputs:I have no idea why this is the case. How can I resolve this so I am following the tutorial correctly?Mac OS X doesn't have apt-get. There is a package manager called Homebrew that is used instead.This command would be:Use Homebrew to install packages that you would otherwise use apt-get for.The page I linked to has an up-to-date way of installing homebrew, but at present, you can install Homebrew as follows:After that, usage of Homebrew is brew install <package>.One of the prerequisites for Homebrew are the XCode command line tools.A package manager (like apt-get or brew) just gives your system an easy and automated way to install packages or libraries.  Different systems use different programs.  apt and its derivatives are used on Debian based linux systems. Red Hat-ish Linux systems use rpm (or at least they did many, many, years ago).  yum is also a package manager for RedHat based systems. Alpine based systems use apk.As of 25 April 2016, homebrew opts the user in to sending analytics by default. This can be opted out of in two ways:Setting an environment variable:Running the following command:the analytics status can then be checked with the command:As Homebrew is my favorite for MacOS although it is possible to have apt-get on MacOS using Fink.MacPorts is another package manager for OSX: https://www.macports.org/.

Installation instructions are at https://www.macports.org/install.php after which one issues sudo port install pythonXX, where XX is 27 or 35.You can also use curl command for installing things, whereever apt-get is mentioned with a url.. e.g. curl -O http://www.magentocommerce.com/downloads/assets/1.8.1.0/magento-1.8.1.0.tar.gz

How can I convert a tensor into a numpy array in TensorFlow?

mathetes

[How can I convert a tensor into a numpy array in TensorFlow?](https://stackoverflow.com/questions/34097281/how-can-i-convert-a-tensor-into-a-numpy-array-in-tensorflow)

How to convert a tensor into a numpy array when using Tensorflow with Python bindings?

2015-12-04 20:55:54Z

How to convert a tensor into a numpy array when using Tensorflow with Python bindings?Any tensor returned by Session.run or eval is a NumPy array.Or:Or, equivalently:EDIT: Not any tensor returned by Session.run or eval() is a NumPy array. Sparse Tensors for example are returned as SparseTensorValue:To convert back from tensor to numpy array you can simply run .eval() on the transformed tensor.Eager Execution is enabled by default, so just call .numpy() on the Tensor object.It is worth noting (from the docs),Bold emphasis mine. A copy may or may not be returned, and this is an implementation detail.If Eager Execution is disabled, you can build a graph and then run it through tf.compat.v1.Session:See also TF 2.0 Symbols Map for a mapping of the old API to the new one.You need to: Code:This worked for me. You can try it in a ipython notebook. Just don't forget to add the following line:Maybe you can try，this method:I have faced and solved the tensor->ndarray conversion in the specific case of tensors representing (adversarial) images, obtained with cleverhans library/tutorials.I think that my question/answer (here) may be an helpful example also for other cases.I'm new with TensorFlow, mine is an empirical conclusion:It seems that tensor.eval() method may need, in order to succeed, also the value for input placeholders. 

Tensor may work like a function that needs its input values (provided into feed_dict) in order to return an output value, e.g.Please note that the placeholder name is x in my case, but I suppose you should find out the right name for the input placeholder. 

x_input is a scalar value or array containing input data.In my case also providing sess was mandatory.My example also covers the matplotlib image visualization part, but this is OT.A simple example could be,n

now if we want this tensor a to be converted into a numpy arrayAs simple as that!

Numpy - add row to array

Darren J. Fitzpatrick

[Numpy - add row to array](https://stackoverflow.com/questions/3881453/numpy-add-row-to-array)

How does one add rows to a numpy array?I have an array A:I wish to add rows to this array from another array X if the first element of each row in X meets a specific condition.Numpy arrays do not have a method 'append' like that of lists, or so it seems.If A and X were lists I would merely do:Is there a numpythonic way to do the equivalent?Thanks,

S ;-)

2010-10-07 12:09:13Z

How does one add rows to a numpy array?I have an array A:I wish to add rows to this array from another array X if the first element of each row in X meets a specific condition.Numpy arrays do not have a method 'append' like that of lists, or so it seems.If A and X were lists I would merely do:Is there a numpythonic way to do the equivalent?Thanks,

S ;-)What is X? If it is a 2D-array, how can you then compare its row to a number: i < 3?EDIT after OP's comment:add to A all rows from X where the first element < 3:well u can do this :As this question is been 7 years before, in the latest version which I am using is numpy version 1.13, and python3, I am doing the same thing with adding a row to a matrix, remember to put a double bracket to the second argument, otherwise, it will raise dimension error. In here I am adding on matrix Awith a row same usage in np.r_Just to someone's intersted, if you would like to add a column, array = np.c_[A,np.zeros(#A's row size)]following what we did before on matrix A, adding a column to itYou can also do this:If no calculations are necessary after every row, it's much quicker to add rows in python, then convert to numpy. Here are timing tests using python 3.6 vs. numpy 1.14, adding 100 rows, one at a time:So, the simple solution to the original question, from seven years ago, is to use vstack() to add a new row after converting the row to a numpy array. But a more realistic solution should consider vstack's poor performance under those circumstances. If you don't need to run data analysis on the array after every addition, it is better to buffer the new rows to a python list of rows (a list of lists, really), and add them as a group to the numpy array using vstack() before doing any data analysis.I use 'np.vstack' which is faster, EX: If you can do the construction in a single operation, then something like the vstack-with-fancy-indexing answer is a fine approach. But if your condition is more complicated or your rows come in on the fly, you may want to grow the array. In fact the numpythonic way to do something like this - dynamically grow an array - is to dynamically grow a list:Lists are highly optimized for this kind of access pattern; you don't have convenient numpy multidimensional indexing while in list form, but for as long as you're appending it's hard to do better than a list of row arrays.You can use numpy.append() to append a row to numpty array and reshape to a matrix later on.

Take multiple lists into dataframe

jfalkson

[Take multiple lists into dataframe](https://stackoverflow.com/questions/30522724/take-multiple-lists-into-dataframe)

How do I take multiple lists and put them as different columns in a python dataframe? I tried this solution but had some trouble.Attempt 1:Attempt 2:How do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe? 

2015-05-29 06:37:49Z

How do I take multiple lists and put them as different columns in a python dataframe? I tried this solution but had some trouble.Attempt 1:Attempt 2:How do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe? I think you're almost there, try removing the extra square brackets around the lst's (Also you don't need to specify the column names when you're creating a dataframe from a dict like this):If you need a more performant solution you can use np.column_stack rather than zip as in your first attempt, this has around a 2x speedup on the example here, however comes at bit of a cost of readability in my opinion:Adding to Aditya Guru's answer here. There is no need of using map. You can do it simply by:This will set the column's names as 0,1,2. To set your own column names, you can pass the keyword argument columns to the method above.Just adding that using the first approach it can be done as -Adding one more scalable solution.Adding to above answers, we can create on the flyhope it helps !@oopsi used pd.concat() but didn't include the column names.  You could do the following, which, unlike the first solution in the accepted answer, gives you control over the column order (avoids dicts, which are unordered):

How to read a text file into a list or an array with Python

user2037744

[How to read a text file into a list or an array with Python](https://stackoverflow.com/questions/14676265/how-to-read-a-text-file-into-a-list-or-an-array-with-python)

I am trying to read the lines of a text file into a list or array in python.  I just need to be able to individually access any item in the list or array after it is created.The text file is formatted as follows:Where the ... is above, there actual text file has hundreds or thousands more items.I'm using the following code to try to read the file into a list:The output I get is:Apparently it is reading the entire file into a list of just one item, rather than a list of individual items.  What am I doing wrong?

2013-02-03 19:19:51Z

I am trying to read the lines of a text file into a list or array in python.  I just need to be able to individually access any item in the list or array after it is created.The text file is formatted as follows:Where the ... is above, there actual text file has hundreds or thousands more items.I'm using the following code to try to read the file into a list:The output I get is:Apparently it is reading the entire file into a list of just one item, rather than a list of individual items.  What am I doing wrong?python's file.readlines() method returns a list of the lines in the file: Now you should be able to iterate through the array of lines x.If you want to use the file and not have to remember to close it afterward, do this:You will have to split your string into a list of values using split()So,You can also use numpy loadtxt likeSo you want to create a list of lists... We need to start with an empty listnext, we read the file content, line by lineA common use case is that of columnar data, but our units of storage are the

rows of the file, that we have read one by one, so you may want to transpose

your list of lists.  This can be done with the following idiomAnother common use is to give a name to each columnso that you can operate on homogeneous data itemsMost of what I've written can be speeded up using the csv module, from the standard library.  Another third party module is pandas, that lets you automate most aspects of a typical data analysis (but has a number of dependencies).Update While in Python 2 zip(*list_of_lists) returns a different (transposed) list of lists, in Python 3 the situation has changed and zip(*list_of_lists) returns a zip object that is not subscriptable.If you need indexed access you can usethat gives you a list of lists in both versions of Python.On the other hand, if you don't need indexed access and what you want is just to build a dictionary indexed by column names, a zip object is just fine...This question is asking how to read the comma-separated value contents from a file into an iterable list: 0,0,200,0,53,1,0,255,...,0.The easiest way to do this is with the csv module as follows:Now, you can easily iterate over spamreader like this:See documentation for more examples.If your file contains numerical values then numpy's loadtxt method seems to be the best approach. You can read the array as follows:You can index values as array in x and file.readlines() is inconvenient because it inserts '\n' in every line and indexing may become erroneous. 

python: SyntaxError: EOL while scanning string literal

l--''''''---------''''''''''''

[python: SyntaxError: EOL while scanning string literal](https://stackoverflow.com/questions/3561691/python-syntaxerror-eol-while-scanning-string-literal)

I have the above-mentioned error in s1="some very long string............" Does anyone know what I am doing wrong?

2010-08-24 23:04:26Z

I have the above-mentioned error in s1="some very long string............" Does anyone know what I am doing wrong?You are not putting a " before the end of the line. Use """ if you want to do this:I had this problem - I eventually worked out that the reason was that I'd included \ characters in the string. If you have any of these, "escape" them with \\ and it should work fine.(Assuming you don't have/want line breaks in your string...)How long is this string really?I suspect there is a limit to how long a line read from a file or from the commandline can be, and because the end of the line gets choped off the parser sees something like s1="some very long string.......... (without an ending ") and thus throws a parsing error?You can split long lines up in multiple lines by escaping linebreaks in your source like this:In my situation, I had \r\n in my single-quoted dictionary strings. I replaced all instances of \r with \\r and \n with \\n and it fixed my issue, properly returning escaped line breaks in the eval'ed dict.I faced a similar problem. I had a string which contained path to a folder in Windows e.g. C:\Users\ The problem is that \ is an escape character and so in order to use it in strings you need to add one more \. Incorrect: C:\Users\Correct:   C:\\\Users\\\I too had this problem, though there were answers here I want to an important point to this

after 

/ there should not be empty spaces.Be Aware of itI also had this exact error message, for me the problem was fixed by adding an " \"It turns out that my long string, broken into about eight lines with " \" at the very end, was missing a " \" on one line.Python IDLE didn't specify a line number that this error was on, but it red-highlighted a totally correct variable assignment statement, throwing me off.  The actual misshapen string statement (multiple lines long with " \") was adjacent to the statement being highlighted.  Maybe this will help someone else.In my case, I use Windows so I have to use double quotes instead of single.I was getting this error in postgresql function. I had a long SQL which I broke into multiple lines with \ for better readability. However, that was the problem. I removed all and made them in one line to fix the issue. I was using pgadmin III.In my case with Mac OS X, I had the following statement:  I was getting the error:  After I change to:  It worked...  DavidYou can try this:Your variable(s1) spans multiple lines. In order to do this (i.e you want your string to span multiple lines), you have to use triple quotes(""").Most previous answers are correct and my answer is very similar to aaronasterling, you could also do 3 single quotations 

s1='''some very long string............'''

What is the inverse function of zip in python? [duplicate]

user17151

[What is the inverse function of zip in python? [duplicate]](https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python)

I've used the zip() function from the numpy library to sort tuples and now I have a list containing all the tuples. I had since modified that list and now I would like to restore the tuples so I can use my data. How can I do this?

2012-11-29 21:26:25Z

I've used the zip() function from the numpy library to sort tuples and now I have a list containing all the tuples. I had since modified that list and now I would like to restore the tuples so I can use my data. How can I do this?should give you the unzipped list.*zipped_list unpacks the zipped_list object. it then passes all the tuples from the zipped_list object to zip, which just packs them back up as they were when you passed them in.so if:then zipped_list = zip(a,b) gives you:and *zipped_list gives you backzipping that with zip(*zipped_list) gives you back the two collections:

Outputting difference in two Pandas dataframes side by side - highlighting the difference

sky

[Outputting difference in two Pandas dataframes side by side - highlighting the difference](https://stackoverflow.com/questions/17095101/outputting-difference-in-two-pandas-dataframes-side-by-side-highlighting-the-d)

I am trying to highlight exactly what changed between two dataframes.Suppose I have two Python Pandas dataframes:My goal is to output an HTML table that:I suppose I could do a row by row and column by column comparison, but is there an easier way?

2013-06-13 19:08:16Z

I am trying to highlight exactly what changed between two dataframes.Suppose I have two Python Pandas dataframes:My goal is to output an HTML table that:I suppose I could do a row by row and column by column comparison, but is there an easier way?The first part is similar to Constantine, you can get the boolean of which rows are empty*:Then we can see which entries have changed:Here the first entry is the index and the second the columns which has been changed.* Note: it's important that df1 and df2 share the same index here. To overcome this ambiguity, you can ensure you only look at the shared labels using df1.index & df2.index, but I think I'll leave that as an exercise.It is possible to use the DataFrame style property to highlight the background color of the cells where there is a difference.Using the example data from the original questionThe first step is to concatenate the DataFrames horizontally with the concat function and distinguish each frame with the keys parameter:It's probably easier to swap the column levels and put the same column names next to each other:Now, its much easier to spot the differences in the frames. But, we can go further and use the style property to highlight the cells that are different. We define a custom function to do this which you can see in this part of the documentation.This will highlight cells that both have missing values. You can either fill them or provide extra logic so that they don't get highlighted.This answer simply extends @Andy Hayden's, making it resilient to when numeric fields are nan, and wrapping it up into a function.So with your data (slightly edited to have a NaN in the score column):Output:printsI have faced this issue, but found an answer before finding this post :Based on unutbu's answer, load your data......define your diff function...Then you can simply use a Panel to conclude :By the way, if you're in IPython Notebook, you may like to use a colored diff function

to give colors depending whether cells are different, equal or left/right null :If your two dataframes have the same ids in them, then finding out what changed is actually pretty easy. Just doing frame1 != frame2 will give you a boolean DataFrame where each True is data that has changed. From that, you could easily get the index of each changed row by doing changedids = frame1.index[np.any(frame1 != frame2,axis=1)].A different approach using concat and drop_duplicates:Output:After fiddling around with @journois's answer, I was able to get it to work using MultiIndex instead of Panel due to Panel's deprication.First, create some dummy data:Then, define your diff function, in this case I'll use the one from his answer report_diff stays the same:Then, I'm going to concatenate the data into a MultiIndex dataframe:And finally I'm going to apply the report_diff down each column group:This outputs:And that is all!Extending answer of @cge, which is pretty cool for more readability of result:Full demonstration example:Here is another way using select and merge:Here is the same thing from a Jupyter screenshot:A function that finds asymmetrical  difference between two data frames is implemented below:

(Based on set difference for pandas)

GIST: https://gist.github.com/oneryalcin/68cf25f536a25e65f0b3c84f9c118e03Example:

How do I tell matplotlib that I am done with a plot?

Stefano Borini

[How do I tell matplotlib that I am done with a plot?](https://stackoverflow.com/questions/741877/how-do-i-tell-matplotlib-that-i-am-done-with-a-plot)

The following code plots to two PostScript (.ps) files, but the second one contains both lines.How can I tell matplotlib to start afresh for the second plot?

2009-04-12 14:40:55Z

The following code plots to two PostScript (.ps) files, but the second one contains both lines.How can I tell matplotlib to start afresh for the second plot?You can use figure to create a new plot, for example, or use close after the first plot.There is a clear figure command, and it should do it for you:If you have multiple subplots in the same figureclears the current axes.As stated from David Cournapeau, use figure().Or subplot(121) / subplot(122) for the same plot, different position.Just enter plt.hold(False) before the first plt.plot, and you can stick to your original code.If you're using Matplotlib interactively, for example in a web application, (e.g. ipython) you maybe looking forinstead of plt.close() or plt.clf().If none of them are working then check this.. say if you have x and y arrays of  data along respective axis. Then check in which cell(jupyter) you have initialized x and y to empty. This is because , maybe you are appending data to x and y without re-initializing them. So plot has old data too. So check that..

How can I time a code segment for testing performance with Pythons timeit?

Mestika

[How can I time a code segment for testing performance with Pythons timeit?](https://stackoverflow.com/questions/2866380/how-can-i-time-a-code-segment-for-testing-performance-with-pythons-timeit)

I've a python script which works just as it should, but I need to write the execution time. I've googled that I should use timeit but I can't seem to get it to work.My Python script looks like this:What I need is the time it takes to execute the query and write it to the file results_update.txt. The purpose is to test an update statement for my database with different indexes and tuning mechanisms.

2010-05-19 14:24:46Z

I've a python script which works just as it should, but I need to write the execution time. I've googled that I should use timeit but I can't seem to get it to work.My Python script looks like this:What I need is the time it takes to execute the query and write it to the file results_update.txt. The purpose is to test an update statement for my database with different indexes and tuning mechanisms.You can use time.time() or time.clock() before and after the block you want to time.This method is not as exact as timeit (it does not average several runs) but it is straightforward. time.time() (in Windows and Linux) and time.clock() (in Linux) are not precise enough for fast functions (you get total = 0). In this case or if you want to average the time elapsed by several runs, you have to manually call the function multiple times (As I think you already do in you example code and timeit does automatically when you set its number argument)In Windows, as Corey stated in the comment, time.clock() has much higher precision (microsecond instead of second) and is preferred over time.time().If you are profiling your code and can use IPython, it has the magic function %timeit.%%timeit operates on cells.Quite apart from the timing, this code you show is simply incorrect: you execute 100 connections (completely ignoring all but the last one), and then when you do the first execute call you pass it a local variable query_stmt which you only initialize after the execute call.First, make your code correct, without worrying about timing yet: i.e. a function that makes or receives a connection and performs 100 or 500 or whatever number of updates on that connection, then closes the connection. Once you have your code working correctly is the correct point at which to think about using timeit on it!Specifically, if the function you want to time is a parameter-less one called foobar you can use timeit.timeit (2.6 or later -- it's more complicated in 2.5 and before):You'd better specify the number of runs because the default, a million, may be high for your use case (leading to spending a lot of time in this code;-).Focus on one specific thing. Disk I/O is slow, so I'd take that out of the test if all you are going to tweak is the database query.And if you need to time your database execution, look for database tools instead, like asking for the query plan, and note that performance varies not only with the exact query and what indexes you have, but also with the data load (how much data you have stored).That said, you can simply put your code in a function and run that function with timeit.timeit():This would disable the garbage collection, repeatedly call the function_to_repeat() function, and time the total duration of those calls using timeit.default_timer(), which is the most accurate available clock for your specific platform.You should move setup code out of the repeated function; for example, you should connect to the database first, then time only the queries. Use the setup argument to either import or create those dependencies, and pass them into your function:would grab the globals function_to_repeat, var1 and var2 from your script and pass those to the function each repetition. I see the question has already been answered, but still want to add my 2 cents for the same.I have also faced similar scenario in which I have to test the execution times for several approaches and hence written a small script, which calls timeit on all functions written in it.The script is also available as github gist here.Hope it will help you and others.Here's a simple wrapper for steven's answer. This function doesn't do repeated runs/averaging, just saves you from having to repeat the timing code everywhere :)

Basic http file downloading and saving to disk in python?

arvindch

[Basic http file downloading and saving to disk in python?](https://stackoverflow.com/questions/19602931/basic-http-file-downloading-and-saving-to-disk-in-python)

I'm new to Python and I've been going through the Q&A on this site, for an answer to my question. However, I'm a beginner and I find it difficult to understand some of the solutions. I need a very basic solution.Could someone please explain a simple solution to 'Downloading a file through http' and 'Saving it to disk, in Windows', to me?I'm not sure how to use shutil and os modules, either.The file I want to download is under 500 MB and is an .gz archive file.If someone can explain how to extract the archive and utilise the files in it also, that would be great!Here's a partial solution, that I wrote from various answers combined:Could someone point out errors (beginner level) and explain any easier methods to do this?Thanks!

2013-10-26 04:49:27Z

I'm new to Python and I've been going through the Q&A on this site, for an answer to my question. However, I'm a beginner and I find it difficult to understand some of the solutions. I need a very basic solution.Could someone please explain a simple solution to 'Downloading a file through http' and 'Saving it to disk, in Windows', to me?I'm not sure how to use shutil and os modules, either.The file I want to download is under 500 MB and is an .gz archive file.If someone can explain how to extract the archive and utilise the files in it also, that would be great!Here's a partial solution, that I wrote from various answers combined:Could someone point out errors (beginner level) and explain any easier methods to do this?Thanks!A clean way to download a file is:This downloads a file from a website and names it file.gz. This is one of my favorite solutions, from Downloading a picture via urllib and python.This example uses the urllib library, and it  will directly retrieve the file form a source.As mentioned here:EDIT: If you still want to use requests, take a look at this question or this one.I use wget.Simple and good library if you want to example?wget module support python 2 and python 3 versionsFour methods using wget, urllib and request.testRequest  - 4469882 function calls (4469842 primitive calls) in 20.236 secondstestRequest2 - 8580 function calls (8574 primitive calls) in 0.072 secondstestUrllib   - 3810 function calls (3775 primitive calls) in 0.036 secondstestwget     - 3489 function calls in 0.020 secondsFor Python3+ URLopener is deprecated.

And when used you will get error as below:So, try:Exotic Windows SolutionI started down this path because ESXi's wget is not compiled with SSL and I wanted to download an OVA from a vendor's website directly onto the ESXi host which is on the other side of the world.I had to disable the firewall(lazy)/enable https out by editing the rules(proper)created the python script:ESXi libraries are kind of paired down but the open source weasel installer seemed to use urllib for https... so it inspired me to go down this pathAnother clean way to save the file is this:

Python strptime() and timezones?

victorhooi

[Python strptime() and timezones?](https://stackoverflow.com/questions/3305413/python-strptime-and-timezones)

I have a CSV dumpfile from a Blackberry IPD backup, created using IPDDump.

The date/time strings in here look something like this

(where EST is an Australian time-zone):I need to be able to parse this date in Python. At first, I tried to use the strptime() function from datettime.However, for some reason, the datetime object that comes back doesn't seem to have any tzinfo associated with it.I did read on this page that apparently datetime.strptime silently discards tzinfo, however, I checked the documentation, and I can't find anything to that effect documented here.I have been able to get the date parsed using a third-party Python library, dateutil, however I'm still curious as to how I was using the in-built strptime() incorrectly?  Is there any way to get strptime() to play nicely with timezones?

2010-07-22 02:42:08Z

I have a CSV dumpfile from a Blackberry IPD backup, created using IPDDump.

The date/time strings in here look something like this

(where EST is an Australian time-zone):I need to be able to parse this date in Python. At first, I tried to use the strptime() function from datettime.However, for some reason, the datetime object that comes back doesn't seem to have any tzinfo associated with it.I did read on this page that apparently datetime.strptime silently discards tzinfo, however, I checked the documentation, and I can't find anything to that effect documented here.I have been able to get the date parsed using a third-party Python library, dateutil, however I'm still curious as to how I was using the in-built strptime() incorrectly?  Is there any way to get strptime() to play nicely with timezones?The datetime module documentation says:See that [0:6]? That gets you (year, month, day, hour, minute, second). Nothing else. No mention of timezones.Interestingly, [Win XP SP2, Python 2.6, 2.7] passing your example to time.strptime doesn't work but if you strip off the " %Z" and the " EST" it does work. Also using "UTC" or "GMT" instead of "EST" works. "PST" and "MEZ" don't work. Puzzling.It's worth noting this has been updated as of version 3.2 and the same documentation now also states the following:Note that this doesn't work with %Z, so the case is important. See the following example:I recommend using python-dateutil.  Its parser has been able to parse every date format I've thrown at it so far.and so on.  No dealing with strptime() format nonsense... just throw a date at it and it Does The Right Thing.Update: Oops.  I missed in your original question that you mentioned that you used dateutil, sorry about that.  But I hope this answer is still useful to other people who stumble across this question when they have date parsing questions and see the utility of that module.Your time string is similar to the time format in rfc 2822 (date format in email, http headers). You could parse it using only stdlib:See solutions that yield timezone-aware datetime objects for various Python versions: parsing date with timezone from an email.In this format,  EST is semantically equivalent to -0500. Though, in general, a timezone abbreviation is not enough, to identify a timezone uniquely.Ran into this exact problem.What I ended up doing: 

How do you catch this exception?

boatcoder

[How do you catch this exception?](https://stackoverflow.com/questions/26270042/how-do-you-catch-this-exception)

This code is in django/db/models/fields.py  It creates/defines an exception?This is in django/db/models/fields/related.py it raises the said exception above:The problem is that this code:won't catch that exceptionand Raises an AttributeError: 'module' object has no attribute 'RelatedObjectDoesNotExist'which is probably why.

2014-10-09 03:54:44Z

This code is in django/db/models/fields.py  It creates/defines an exception?This is in django/db/models/fields/related.py it raises the said exception above:The problem is that this code:won't catch that exceptionand Raises an AttributeError: 'module' object has no attribute 'RelatedObjectDoesNotExist'which is probably why.If your related model is called Foo you can just do:Django is amazing when its not terrifying. RelatedObjectDoesNotExist is a property that returns a type that is figured out dynamically at runtime. That type uses self.field.rel.to.DoesNotExist as a base class. According to Django documentation:This is the magic that makes that happen. Once the model has been built up, self.field.rel.to.DoesNotExist is the does-not-exist exception for that model.If you don't want to import the related model class, you can:orwhere related_field is the field name.To catch this exception in general, you can doThe RelatedObjectDoesNotExist exception is created dynamically at runtime.  Here is the relevant code snippet for the ForwardManyToOneDescriptor and ReverseOneToOneDescriptor descriptors:So the exception inherits from <model name>.DoesNotExist and AttributeError.  In fact, the complete MRO for this exception type is:The basic takeaway is you can catch <model name>.DoesNotExist, ObjectDoesNotExist (import from django.core.exceptions) or AttributeError, whatever makes the most sense in your context.tdelaney's answer is great for regular code paths, but if you need to know how to catch this exception in tests:

Format a datetime into a string with milliseconds

Jurudocs

[Format a datetime into a string with milliseconds](https://stackoverflow.com/questions/7588511/format-a-datetime-into-a-string-with-milliseconds)

I want to have a datetime string from the date with milliseconds. This code is typical for me and I'm eager to learn how to shorten it.

2011-09-28 19:28:56Z

I want to have a datetime string from the date with milliseconds. This code is typical for me and I'm eager to learn how to shorten it.To get a date string with milliseconds (3 decimal places behind seconds), use this:Note: For Python3, print requires parentheses:With Python 3.6 you can use:Output:More info here: https://docs.python.org/3/library/datetime.html#datetime.datetime.isoformathttp://docs.python.org/library/datetime.html#strftime-strptime-behavior@Cabbi raised the issue that on some systems, the microseconds format %f may give "0", so it's not portable to simply chop off the last three characters.The following code carefully formats a timestamp with milliseconds:Example Output:To get the exact output that the OP wanted, we have to strip punctuation characters:Example Output:Probably like this :I assume you mean you're looking for something that is faster than datetime.datetime.strftime(), and are essentially stripping the non-alpha characters from a utc timestamp.You're approach is marginally faster, and I think you can speed things up even more by slicing the string:resultUse of translate() and slicing method run in same time

translate() presents the advantage to be usable in one lineComparing the times on the basis of the first one:I dealt with the same problem but in my case it was important that the millisecond was rounded and not truncatedThe problem with datetime.utcnow() and other such solutions is that they are slow.More efficient solution may look like this one:Where prec would be 3 in your case (milliseconds).The function works up to 9 decimal places (please note number 9 in the 2nd formatting string).If you'd like to round the fractional part, I'd suggest building "%.9f" dynamically with desired number of decimal places.

pythonic way to do something N times without an index variable?

Manuel Aráoz

[pythonic way to do something N times without an index variable?](https://stackoverflow.com/questions/2970780/pythonic-way-to-do-something-n-times-without-an-index-variable)

Every day I love python more and more. Today, I was writing some code like:I had to do something N times. But each time didn't depend on the value of i (index variable).

I realized that I was creating a variable I never used (i), and I thought "There surely is a more pythonic way of doing this without the need for that useless index variable."So... the question is: do you know how to do this simple task in a more (pythonic) beautiful way?

2010-06-04 00:39:15Z

Every day I love python more and more. Today, I was writing some code like:I had to do something N times. But each time didn't depend on the value of i (index variable).

I realized that I was creating a variable I never used (i), and I thought "There surely is a more pythonic way of doing this without the need for that useless index variable."So... the question is: do you know how to do this simple task in a more (pythonic) beautiful way?A slightly faster approach than looping on xrange(N) is:Use the _ variable, as I learned when I asked this question, for example:I just use for _ in range(n), it's straight to the point. It's going to generate the entire list for huge numbers in Python 2, but if you're using Python 3 it's not a problem.since function is first-class citizen, you can write small wrapper (from Alex answers)then you can pass function as argument.The _ is the same thing as x. However it's a python idiom that's used to indicate an identifier that you don't intend to use. In python these identifiers don't takes memor or allocate space like variables do in other languages.  It's easy to forget that.  They're just names that point to objects, in this case an integer on each iteration.I found the various answers really elegant (especially Alex Martelli's) but I wanted to quantify performance first hand, so I cooked up the following script:I also came up with an alternative solution that builds on Martelli's one and uses map() to call the payload function. OK, I cheated a bit in that I took the freedom of making the payload accept a parameter that gets discarded: I don't know if there is a way around this. Nevertheless, here are the results:so using map yields an improvement of approximately 30% over the standard  for loop and an extra 19% over Martelli's.Assume that you've defined do_something as a function, and you'd like to perform it N times.

Maybe you can try the following:What about a simple while loop?You already have the variable; why not use it?

How to assert two list contain the same elements in Python? [duplicate]

satoru

[How to assert two list contain the same elements in Python? [duplicate]](https://stackoverflow.com/questions/12813633/how-to-assert-two-list-contain-the-same-elements-in-python)

When writing test cases, I often need to assert that two list contain the same elements without regard to their order.I have been doing this by converting the lists to sets.Is there any simpler way to do this?EDIT:As @MarkDickinson pointed out, I can just use TestCase.assertItemsEqual.Notes that TestCase.assertItemsEqual is new in Python2.7.

If you are using an older version of Python, you can use unittest2 - a backport of new features of Python 2.7.

2012-10-10 06:56:45Z

When writing test cases, I often need to assert that two list contain the same elements without regard to their order.I have been doing this by converting the lists to sets.Is there any simpler way to do this?EDIT:As @MarkDickinson pointed out, I can just use TestCase.assertItemsEqual.Notes that TestCase.assertItemsEqual is new in Python2.7.

If you are using an older version of Python, you can use unittest2 - a backport of new features of Python 2.7.As of Python 3.2 unittest.TestCase.assertItemsEqual(doc) has been replaced by unittest.TestCase.assertCountEqual(doc) which does exactly what you are looking for, as you can read from the python standard library documentation. The method is somewhat misleadingly named but it does exactly what you are looking for.Here a simple example which compares two lists having the same elements but in a different order.Here a little example script.Side Note : Please make sure that the elements in the lists you are comparing are sortable.Slightly faster version of the implementation (If you know that most couples lists will have different lengths):Comparing: GivenIn Python >= 3.0In Python >= 2.7, the above function was named:In Python < 2.7Via six module (Any Python version)Converting your lists to sets will tell you that they contain the same elements. But this method cannot confirm that they contain the same number of all elements. For example, your method will fail in this case:You are likely better off sorting the two lists and comparing them:Note that this does not alter the structure/contents of the two lists. Rather, the sorting creates two new listsNeeds ensure library but you can compare list by:This will not raise assert exception. Documentation of thin is really thin so i would recommend to look at ensure's codes on github 

Negation in Python

David Mulder

[Negation in Python](https://stackoverflow.com/questions/6117733/negation-in-python)

I'm trying to create a directory if the path doesn't exist, but the ! (not) operator doesn't work. I'm not sure how to negate in Python... What's the correct way to do this?

2011-05-24 22:38:35Z

I'm trying to create a directory if the path doesn't exist, but the ! (not) operator doesn't work. I'm not sure how to negate in Python... What's the correct way to do this?The negation operator in Python is not. Therefore just replace your ! with not.For your example, do this:For your specific example (as Neil said in the comments), you don't have to use the subprocess module, you can simply use os.mkdir() to get the result you need, with added exception handling goodness.Example:Python prefers English keywords to punctuation. Use not x, i.e. not os.path.exists(...). The same thing goes for && and || which are and and or in Python.try instead:Combining the input from everyone else (use not, no parens, use os.mkdir) you'd get...

Does SQLAlchemy have an equivalent of Django's get_or_create?

FogleBird

[Does SQLAlchemy have an equivalent of Django's get_or_create?](https://stackoverflow.com/questions/2546207/does-sqlalchemy-have-an-equivalent-of-djangos-get-or-create)

I want to get an object from the database if it already exists (based on provided parameters) or create it if it does not.Django's get_or_create (or source) does this.  Is there an equivalent shortcut in SQLAlchemy?I'm currently writing it out explicitly like this:

2010-03-30 14:57:31Z

I want to get an object from the database if it already exists (based on provided parameters) or create it if it does not.Django's get_or_create (or source) does this.  Is there an equivalent shortcut in SQLAlchemy?I'm currently writing it out explicitly like this:That's basically the way to do it, there is no shortcut readily available AFAIK.You could generalize it ofcourse:Following the solution of @WoLpH, this is the code that worked for me (simple version):With this, I'm able to get_or_create any object of my model.  Suppose my model object is :To get or create my object I write :I've been playing with this problem and have ended up with a fairly robust solution:I just wrote a fairly expansive blog post on all the details, but a few quite ideas of why I used this.EDIT: I've changed session.commit() to session.flush() as explained in this blog post. Note that these decisions are specific to the datastore used (Postgres in this case).EDIT 2: I’ve updated using a {} as a default value in the function as this is typical Python gotcha. Thanks for the comment, Nigel! If your curious about this gotcha, check out this StackOverflow question and this blog post.A modified version of erik's excellent answerNote that if using MySQL, the transaction isolation level must be set to READ COMMITTED rather than REPEATABLE READ for this to work. Django's get_or_create (and here) uses the same stratagem, see also the Django documentation.This SQLALchemy recipe does the job nice and elegant.The first thing to do is to define a function that is given a Session to work with, and associates a dictionary with the Session() which keeps track of current unique keys.An example of utilizing this function would be in a mixin:And finally creating the unique get_or_create model:The recipe goes deeper into the idea and provides different approaches but I've used this one with great success.The closest semantically is probably:not sure how kosher it is to rely on a globally defined Session in sqlalchemy, but the Django version doesn't take a connection so...The tuple returned contains the instance and a boolean indicating if the instance was created (i.e. it's False if we read the instance from the db).Django's get_or_create is often used to make sure that global data is available, so I'm committing at the earliest point possible.Depending on the isolation level you adopted, none of the above solutions would work.

The best solution I have found is a RAW SQL in the following form:This is transactionally safe whatever the isolation level and the degree of parallelism are.Beware: in order to make it efficient, it would be wise to have an INDEX for the unique column.I slightly simplified @Kevin. solution to avoid wrapping the whole function in an if/else statement. This way there's only one return, which I find cleaner:

Append a NumPy array to a NumPy array

Mohit

[Append a NumPy array to a NumPy array](https://stackoverflow.com/questions/9775297/append-a-numpy-array-to-a-numpy-array)

I have a numpy_array. Something like [ a b c ].And then I want to append it into another NumPy array (just like we create a list of lists). How do we create an array of NumPy arrays containing NumPy arrays?I tried to do the following without any luck

2012-03-19 17:55:24Z

I have a numpy_array. Something like [ a b c ].And then I want to append it into another NumPy array (just like we create a list of lists). How do we create an array of NumPy arrays containing NumPy arrays?I tried to do the following without any luckor this:Well, the error message says it all:  NumPy arrays do not have an append() method.  There's a free function numpy.append() however:This will create a new array instead of mutating M in place.  Note that using numpy.append() involves copying both arrays.  You will get better performing code if you use fixed-sized NumPy arrays.You may use numpy.append()... This will not create two separate arrays but will append two arrays into a single dimensional array.Sven said it all, just be very cautious because of automatic type adjustments when append is called.As you see based on the contents the dtype went from int64 to float32, and then to S1I found this link while looking for something slightly different, how to start appending array objects to an empty numpy array, but tried all the solutions on this page to no avail.Then I found this question and answer: How to add a new row to an empty numpy arrayThe gist here:Then you can use concatenate to add rows like so:arr = np.concatenate( ( arr, [[x, y, z]] ) , axis=0)See also https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.htmlActually one can always create an ordinary list of numpy arrays and convert it later.I had the same issue, and I couldn't comment on @Sven Marnach answer (not enough rep, gosh I remember when Stackoverflow first started...) anyway.Adding a list of random numbers to a 10 X 10 matrix.Using np.zeros() an array is created with 1 x 10 zeros.Then a list of 10 random numbers is created using np.random and assigned to randomList.

The loop stacks it 10 high. We just have to remember to remove the first empty entry.So in a function:a 7 x 7 array using random numbers 0 - 1000If I understand your question, here's one way.  Say you have:so here's some code... Which leads to: 

How do I configure PyCharm to run py.test tests?

Joe White

[How do I configure PyCharm to run py.test tests?](https://stackoverflow.com/questions/6397063/how-do-i-configure-pycharm-to-run-py-test-tests)

I want to start writing unit tests for my Python code, and the py.test framework sounds like a better bet than Python's bundled unittest. So I added a "tests" directory to my project, and added test_sample.py to it. Now I want to configure PyCharm to run all the tests in my "tests" directory.PyCharm allegedly supports py.test in its test runner. You're supposed to be able to create a run/debug configuration to run your tests, and PyCharm allegedly has a "create configuration" dialog box specifically for py.test. But that's the complete extent of their documentation on the subject, and I can't find this alleged dialog box anywhere.If I right-click the directory in the Project tool window, I'm supposed to see a "Create <name>" menu item, but the only menu item starting with "Create" is "Create Run Configuration". Okay, maybe the documentation is just wrong, and "Create Run Configuration" does sound promising. Unfortunately, the only two items in its submenu are "Unittests in C:\mypath..." and "Doctests in C:\mypath...", neither of which applies -- I'm using neither unittest nor doctest. There is no menu item for py.test.If I open my test_sample.py and right-click in the editor window, I do get the promised "Create <name>" menu items: there's "Create 'Unittests in test_sa...'...", followed by "Run 'Unittests in test_sa...'" and "Debug 'Unittests in test_sa...'". So again, it's all specific to the unittest framework; nothing for py.test.If I do try the menu items that say "unittest", I get a dialog box with options for "Name", "Type", a "Tests" group box with "Folder" and "Pattern" and "Script" and "Class" and "Function", etc. This sounds exactly like what's documented as the dialog to add a configuration for Python Unit Test, and not like the "Name" and "Test to run" and "Keywords" options that are supposed to show up in the configuration for py.test dialog. There's nothing inside the dialog to switch which test framework I'm adding.I'm using PyCharm 1.5.2 on Windows with Python 3.1.3 and pytest 2.0.3. I can successfully run py.test on my tests from the command line, so it's not something simple like pytest not being installed properly.How do I configure PyCharm to run my py.test tests?

2011-06-18 15:37:12Z

I want to start writing unit tests for my Python code, and the py.test framework sounds like a better bet than Python's bundled unittest. So I added a "tests" directory to my project, and added test_sample.py to it. Now I want to configure PyCharm to run all the tests in my "tests" directory.PyCharm allegedly supports py.test in its test runner. You're supposed to be able to create a run/debug configuration to run your tests, and PyCharm allegedly has a "create configuration" dialog box specifically for py.test. But that's the complete extent of their documentation on the subject, and I can't find this alleged dialog box anywhere.If I right-click the directory in the Project tool window, I'm supposed to see a "Create <name>" menu item, but the only menu item starting with "Create" is "Create Run Configuration". Okay, maybe the documentation is just wrong, and "Create Run Configuration" does sound promising. Unfortunately, the only two items in its submenu are "Unittests in C:\mypath..." and "Doctests in C:\mypath...", neither of which applies -- I'm using neither unittest nor doctest. There is no menu item for py.test.If I open my test_sample.py and right-click in the editor window, I do get the promised "Create <name>" menu items: there's "Create 'Unittests in test_sa...'...", followed by "Run 'Unittests in test_sa...'" and "Debug 'Unittests in test_sa...'". So again, it's all specific to the unittest framework; nothing for py.test.If I do try the menu items that say "unittest", I get a dialog box with options for "Name", "Type", a "Tests" group box with "Folder" and "Pattern" and "Script" and "Class" and "Function", etc. This sounds exactly like what's documented as the dialog to add a configuration for Python Unit Test, and not like the "Name" and "Test to run" and "Keywords" options that are supposed to show up in the configuration for py.test dialog. There's nothing inside the dialog to switch which test framework I'm adding.I'm using PyCharm 1.5.2 on Windows with Python 3.1.3 and pytest 2.0.3. I can successfully run py.test on my tests from the command line, so it's not something simple like pytest not being installed properly.How do I configure PyCharm to run my py.test tests?Please go to File | Settings | Tools | Python Integrated Tools and change the default test runner to py.test. Then you'll get the py.test option to create tests instead of the unittest one.PyCharm 2017.3I think you need to use the Run/Debug Configuration item on the toolbar. Click it and 'Edit Configurations' (or alternatively use the menu item Run->Edit Configurations). In the 'Defaults' section in the left pane there is a 'py.test' item which I think is what you want.I also found that the manual didn't match up to the UI for this. Hope I've understood the problem correctly and that helps.Here is how I made it work with pytest 3.7.2 (installed via pip) and pycharms 2017.3:It's poorly documented to be sure.   Once you get add a new configuration from defaults, you will be in the realm of running the "/Applications/PyCharm CE.app/Contents/helpers/pycharm/pytestrunner.py" script.   It's not documented and has its own ideas of command line arguments.You can:Oddly, you will find it hard to find any discussion as JetBrains does a good job of bombing Google algorithms with its own pages.find this thread when I hit the same question and found the solution

pycharm version:2017.1.2

go to "Preferences" -> "Tools" -> "Python Integrated Tools" and set the default test runner from right side panel as py.test

solve my problemI'm using 2018.2I do Run -> Edit Configurations... 

Then click the + in the upper left of the modal dialog.

Select "python tests" -> py.test

Then I give it a name like "All test with py.test"I select Target: module name

and put in the module where my tests are (that is 'tests' for me) or the module where all my code is if my tests are mixed in with my code. This was tripping me up.I set the Python interpreter.I set the working directory to the project directory.In pycharm 2019.2, you can simply do this to run all tests:For a higher integration of pytest into pycharm, see https://www.jetbrains.com/help/pycharm/pytest.htmlWith a special Conda python setup which included the pip install for py.test plus usage of the Specs addin (option --spec) (for Rspec like nice test summary language), I had to do ; 1.Edit the default py.test to  include option= --spec , which means use the plugin: https://github.com/pchomik/pytest-spec2.Create new test configuration, using py.test. Change its python interpreter to use ~/anaconda/envs/ your choice of interpreters, eg py27  for my namings.3.Delete the 'unittests' test configuration.4.Now the default test config is py.test with my lovely Rspec style outputs. I love it! Thank you everyone!p.s. Jetbrains' doc on run/debug configs is here: https://www.jetbrains.com/help/pycharm/2016.1/run-debug-configuration-py-test.html?search=py.testWith 2018.3 it appears to automatically detect that I'm using pytest, which is nice, but it still doesn't allow running from the top level of the project. I had to run pytest for each tests directory individually. However, I found that I could choose one of the configurations and manually edit it to run at the root of the project and that this worked. I have to manually choose it in the Configurations drop-down - can't right click on the root folder in the Project pane. But at least it allows me to run all tests at once.Enable Pytest for you project  

how to use python to execute a curl command

Qiang Fu

[how to use python to execute a curl command](https://stackoverflow.com/questions/25491090/how-to-use-python-to-execute-a-curl-command)

I want to execute a curl command in python.Usually, I just need enter the command in terminal and press return key. However, I don't know how it works in python. The command shows below:There is a request.json file to be sent to get response.I searched a lot and got confused. I tried to write a piece of code, although I could not fully understand. It didn't work.The error message is 'Parse Error'.Can anyone tell me how to fix it? or how to get response from the sever correctly?

2014-08-25 17:20:21Z

I want to execute a curl command in python.Usually, I just need enter the command in terminal and press return key. However, I don't know how it works in python. The command shows below:There is a request.json file to be sent to get response.I searched a lot and got confused. I tried to write a piece of code, although I could not fully understand. It didn't work.The error message is 'Parse Error'.Can anyone tell me how to fix it? or how to get response from the sever correctly?For sake of simplicity, maybe you should consider using the Requests library.An example with json response content would be something like:If you look for further information, in the Quickstart section, they have lots of working examples.EDIT:For your specific curl translation:Just use this website. It'll convert any curl command into Python, Node.js, PHP, R, or Go.Example:Becomes this in Python,maybe?if you are trying to send a file ahh thanks @LukasGraf now i better understand what his original code is doingits python implementation be likecheck this link, it will help convert cURl command to python,php and nodejsThere is a nice website https://curl.trillworks.com/ that does the conversion for you. It does convert from cURL into Python, Node.js, R, PHP, Go.My answer is WRT python 2.6.2. I apologize for not providing the required parameters 'coz it's confidential.Some background: I went looking for exactly this question because I had to do something to retrieve content, but all I had available was an old version of python with inadequate SSL support. If you're on an older MacBook, you know what I'm talking about. In any case, curl runs fine from a shell (I suspect it has modern SSL support linked in) so sometimes you want to do this without using requests or urllib2.You can use the subprocess module to execute curl and get at the retrieved content:Python 3's subprocess module also contains .run() with a number of useful options. I'll leave it to someone who is actually running python 3 to provide that answer.This could be achieve with the below mentioned psuedo code approachImport os

import requests 

Data = os.execute(curl URL)

R= Data.json()

Why are Python's arrays slow?

Valentin Lorentz

[Why are Python's arrays slow?](https://stackoverflow.com/questions/36778568/why-are-pythons-arrays-slow)

I expected array.array to be faster than lists, as arrays seem to be unboxed.However, I get the following result:What could be the cause of such a difference?

2016-04-21 19:16:49Z

I expected array.array to be faster than lists, as arrays seem to be unboxed.However, I get the following result:What could be the cause of such a difference?The storage is "unboxed", but every time you access an element Python has to "box" it (embed it in a regular Python object) in order to do anything with it.  For example, your sum(A) iterates over the array, and boxes each integer, one at a time, in a regular Python int object.  That costs time.  In your sum(L), all the boxing was done at the time the list was created.So, in the end, an array is generally slower, but requires substantially less memory.Here's the relevant code from a recent version of Python 3, but the same basic ideas apply to all CPython implementations since Python was first released.Here's the code to access a list item:There's very little to it:  somelist[i] just returns the i'th object in the list (and all Python objects in CPython are pointers to a struct whose initial segment conforms to the layout of a struct PyObject).And here's the __getitem__ implementation for an array with type code l:The raw memory is treated as a vector of platform-native C long integers; the i'th C long is read up; and then PyLong_FromLong() is called to wrap ("box") the native C long in a Python long object (which, in Python 3, which eliminates Python 2's distinction between int and long, is actually shown as type int).This boxing has to allocate new memory for a Python int object, and spray the native C long's bits into it.  In the context of the original example, this object's lifetime is very brief (just long enough for sum() to add the contents into a running total), and then more time is required to deallocate the new int object.This is where the speed difference comes from, always has come from, and always will come from in the CPython implementation.To add to Tim Peters' excellent answer, arrays implement the buffer protocol, while lists do not.  This means that, if you are writing a C extension (or the moral equivalent, such as writing a Cython module), then you can access and work with the elements of an array much faster than anything Python can do.  This will give you considerable speed improvements, possibly well over an order of magnitude.  However, it has a number of downsides:Going straight to C extensions may be using a sledgehammer to swat a fly, depending on your use case.  You should first investigate NumPy and see if it is powerful enough to do whatever math you're trying to do.  It will also be much faster than native Python, if used correctly.Tim Peters answered why this is slow, but let's see how to improve it.Sticking to your example of sum(range(...)) (factor 10 smaller than your example to fit into memory here):This way also numpy needs to box/unbox, which has additional overhead. To make it fast one has to stay within the numpy c code:So from the list solution to the numpy version this is a factor 16 in runtime.Let's also check how long creating those data structures takesClear winner: NumpyAlso note that creating the data structure takes about as much time as summing, if not more. Allocating memory is slow.Memory usage of those:So these take 8 bytes per number with varying overhead. For the range we use 32bit ints are sufficient, so we can safe some memory.But it turns out that adding 64bit ints is faster than 32bit ints on my machine, so this is only worth it if you are limited by memory/bandwidth.

On localhost, how do I pick a free port number?

Anton L.

[On localhost, how do I pick a free port number?](https://stackoverflow.com/questions/1365265/on-localhost-how-do-i-pick-a-free-port-number)

I'm trying to play with inter-process communication and since I could not figure out how to use named pipes under Windows I thought I'll use network sockets. Everything happens locally. The server is able to launch slaves in a separate process and listens on some port. The slaves do their work and submit the result to the master. How do I figure out which port is available? I assume I cannot listen on port 80 or 21?I'm using Python, if that cuts the choices down.Thanks!

2009-09-02 00:07:14Z

I'm trying to play with inter-process communication and since I could not figure out how to use named pipes under Windows I thought I'll use network sockets. Everything happens locally. The server is able to launch slaves in a separate process and listens on some port. The slaves do their work and submit the result to the master. How do I figure out which port is available? I assume I cannot listen on port 80 or 21?I'm using Python, if that cuts the choices down.Thanks!Do not bind to a specific port, or bind to port 0, e.g. sock.bind(('', 0)).  The OS will then pick an available port for you.  You can get the port that was chosen using sock.getsockname()[1], and pass it on to the slaves so that they can connect back.For the sake of snippet of what the guys have explained above:Bind the socket to port 0. A random free port from 1024 to 65535 will be selected. You may retrieve the selected port with getsockname() right after bind().You can listen on whatever port you want; generally, user applications should listen to ports 1024 and above (through 65535).  The main thing if you have a variable number of listeners is to allocate a range to your app - say 20000-21000, and CATCH EXCEPTIONS.  That is how you will know if a port is unusable (used by another process, in other words) on your computer.  However, in your case, you shouldn't have a problem using a single hard-coded port for your listener, as long as you print an error message if the bind fails.Note also that most of your sockets (for the slaves) do not need to be explicitly bound to specific port numbers - only sockets that wait for incoming connections (like your master here) will need to be made a listener and bound to a port.  If a port is not specified for a socket before it is used, the OS will assign a useable port to the socket.  When the master wants to respond to a slave that sends it data, the address of the sender is accessible when the listener receives data.I presume you will be using UDP for this?

Understanding the main method of python [duplicate]

Eran Morad

[Understanding the main method of python [duplicate]](https://stackoverflow.com/questions/22492162/understanding-the-main-method-of-python)

I am new to Python, but I have experience in other OOP languages. My course does not explain the main method in python. Please tell me how main method works in python ? I am confused because I am trying to compare it to Java. How is main executed and why do I need this strange if to execute main. My code is terminated without output when I remove the if.The minimal code - 

2014-03-18 22:14:32Z

I am new to Python, but I have experience in other OOP languages. My course does not explain the main method in python. Please tell me how main method works in python ? I am confused because I am trying to compare it to Java. How is main executed and why do I need this strange if to execute main. My code is terminated without output when I remove the if.The minimal code - The Python approach to "main" is almost unique to the language(*).The semantics are a bit subtle.  The __name__ identifier is bound to the name of any module as it's being imported.  However, when a file is being executed then __name__ is set to "__main__" (the literal string: __main__).This is almost always used to separate the portion of code which should be executed from the portions of code which define functionality.  So Python code often contains a line like:Using this convention one can have a file define classes and functions for use in other programs, and also include code to evaluate only when the file is called as a standalone script.It's important to understand that all of the code above the if __name__ line is being executed, evaluated, in both cases.  It's evaluated by the interpreter when the file is imported or when it's executed.  If you put a print statement before the if __name__ line then it will print output every time any other code attempts to import that as a module.  (Of course, this would be anti-social. Don't do that).I, personally, like these semantics.  It encourages programmers to separate functionality (definitions) from function (execution) and encourages re-use.Ideally almost every Python module can do something useful if called from the command line.  In many cases this is used for managing unit tests.  If a particular file defines functionality which is only useful in the context of other components of a system then one can still use __name__ == "__main__" to isolate a block of code which calls a suite of unit tests that apply to this module.(If you're not going to have any such functionality nor unit tests than it's best to ensure that the file mode is NOT executable).Summary: if __name__ == '__main__': has two primary use cases:It's fairly common to def main(*args) and have if __name__ == '__main__': simply call main(*sys.argv[1:]) if you want to define main in a manner that's similar to some other programming languages.  If your .py file is primarily intended to be used as a module in other code then you might def test_module() and calling test_module() in your if __name__ == '__main__:' suite.In Python, execution does NOT have to begin at main. The first line of "executable code" 

is executed first. Output - More on main() - http://ibiblio.org/g2swap/byteofpython/read/module-name.htmlA module's __name__Every module has a name and statements in a module can find out the name of its module. This is especially handy in one particular situation - As mentioned previously, when a module is imported for the first time, the main block in that module is run. What if we want to run the block only if the program was used by itself and not when it was imported from another module? This can be achieved using the name attribute of the module.Using a module's __name__Output - How It Works - Every Python module has it's __name__ defined and if this is __main__, it implies that the module is being run standalone by the user and we can do corresponding appropriate actions.Python does not have a defined entry point like Java, C, C++, etc. Rather it simply executes a source file line-by-line. The if statement allows you to create a main function which will be executed if your file is loaded as the "Main" module rather than as a library in another module.To be clear, this means that the Python interpreter starts at the first line of a file and executes it. Executing lines like class Foobar: and def foobar() creates either a class or a function and stores them in memory for later use.If you import the module (.py) file you are creating now from another python script it will not execute the code within If you run the script directly from the console, it will be executed.Python does not use or require a main() function. Any code that is not protected by that guard will be executed upon execution or importing of the module.This is expanded upon a little more at python.berkely.edu

How Python web frameworks, WSGI and CGI fit together

Eli Bendersky

[How Python web frameworks, WSGI and CGI fit together](https://stackoverflow.com/questions/219110/how-python-web-frameworks-wsgi-and-cgi-fit-together)

I have a Bluehost account where I can run Python scripts as CGI. I guess it's the simplest CGI, because to run I have to define the following in .htaccess:Now, whenever I look up web programming with Python, I hear a lot about WSGI and how most frameworks use it. But I just don't understand how it all fits together, especially when my web server is given (Apache running at a host's machine) and not something I can really play with (except defining .htaccess commands).How are WSGI, CGI, and the frameworks all connected? What do I need to know, install, and do if I want to run a web framework (say web.py or CherryPy) on my basic CGI configuration? How to install WSGI support?

2008-10-20 16:43:57Z

I have a Bluehost account where I can run Python scripts as CGI. I guess it's the simplest CGI, because to run I have to define the following in .htaccess:Now, whenever I look up web programming with Python, I hear a lot about WSGI and how most frameworks use it. But I just don't understand how it all fits together, especially when my web server is given (Apache running at a host's machine) and not something I can really play with (except defining .htaccess commands).How are WSGI, CGI, and the frameworks all connected? What do I need to know, install, and do if I want to run a web framework (say web.py or CherryPy) on my basic CGI configuration? How to install WSGI support?How WSGI, CGI, and the frameworks are all connected?Apache listens on port 80.  It gets an HTTP request.  It parses the request to find a way to respond.  Apache has a LOT of choices for responding.   One way to respond is to use CGI to run a script.  Another way to respond is to simply serve a file.  In the case of CGI, Apache prepares an environment and invokes the script through the CGI protocol.  This is a standard Unix Fork/Exec situation -- the CGI subprocess inherits an OS environment including the socket and stdout.  The CGI subprocess writes a response, which goes back to Apache; Apache sends this response to the browser.CGI is primitive and annoying.  Mostly because it forks a subprocess for every request, and subprocess must exit or close stdout and stderr to signify end of response.WSGI is an interface that is based on the CGI design pattern.  It is not necessarily CGI -- it does not have to fork a subprocess for each request.  It can be CGI, but it doesn't have to be.WSGI adds to the CGI design pattern in several important ways.  It parses the HTTP Request Headers for you and adds these to the environment.  It supplies any POST-oriented input as a file-like object in the environment.  It also provides you a function that will formulate the response, saving you from a lot of formatting details.What do I need to know / install / do if I want to run a web framework (say web.py or cherrypy) on my basic CGI configuration?Recall that forking a subprocess is expensive.  There are two ways to work around this.Note that mod_wsgi can work in either mode: embedded or daemon.When you read up on mod_fastcgi, you'll see that Django uses flup to create a WSGI-compatible interface from the information provided by mod_fastcgi.  The pipeline works like this.Django has several "django.core.handlers" for the various interfaces.For mod_fastcgi, Django provides a manage.py runfcgi that integrates FLUP and the handler.For mod_wsgi, there's a core handler for this.How to install WSGI support?Follow these instructions.https://code.google.com/archive/p/modwsgi/wikis/IntegrationWithDjango.wikiFor background see thishttp://docs.djangoproject.com/en/dev/howto/deployment/#howto-deployment-indexI think Florian's answer answers the part of your question about "what is WSGI", especially if you read the PEP.As for the questions you pose towards the end:WSGI, CGI, FastCGI etc. are all protocols for a web server to run code, and deliver the dynamic content that is produced. Compare this to static web serving, where a plain HTML file is basically delivered as is to the client.CGI, FastCGI and SCGI are language agnostic. You can write CGI scripts in Perl, Python, C, bash, whatever. CGI defines which executable will be called, based on the URL, and how it will be called: the arguments and environment. It also defines how the return value should be passed back to the web server once your executable is finished. The variations are basically optimisations to be able to handle more requests, reduce latency and so on; the basic concept is the same.WSGI is Python only. Rather than a language agnostic protocol, a standard function signature is defined:That is a complete (if limited) WSGI application. A web server with WSGI support (such as Apache with mod_wsgi) can invoke this function whenever a request arrives.The reason this is so great is that we can avoid the messy step of converting from a HTTP GET/POST to CGI to Python, and back again on the way out. It's a much more direct, clean and efficient linkage.It also makes it much easier to have long-running frameworks running behind web servers, if all that needs to be done for a request is a function call. With plain CGI, you'd have to start your whole framework up for each individual request.To have WSGI support, you'll need to have installed a WSGI module (like mod_wsgi), or use a web server with WSGI baked in (like CherryPy). If neither of those are possible, you could use the CGI-WSGI bridge given in the PEP.You can run WSGI over CGI as Pep333 demonstrates as an example. However every time there is a request a new Python interpreter is started and the whole context (database connections, etc.) needs to be build which all take time.The best if you want to run WSGI would be if your host would install mod_wsgi and made an appropriate configuration to defer control to an application of yours.Flup is another way to run with WSGI for any webserver that can speak FCGI, SCGI or AJP. From my experience only FCGI really works, and it can be used in Apache either via mod_fastcgi or if you can run a separate Python daemon with mod_proxy_fcgi.WSGI is a protocol much like CGI, which defines a set of rules how webserver and Python code can interact, it is defined as Pep333. It makes it possible that many different webservers can use many different frameworks and applications using the same application protocol. This is very beneficial and makes it so useful.If you are unclear on all the terms in this space, and lets face it, its a confusing acronym-laden one, there's also a good background reader in the form of an official python HOWTO which discusses CGI vs. FastCGI vs. WSGI and so on: http://docs.python.org/howto/webservers.htmlIt's a simple abstraction layer for Python, akin to what the Servlet spec is for Java.  Whereas CGI is really low level and just dumps stuff into the process environment and standard in/out, the above two specs model the http request and response as constructs in the language.  My impression however is that in Python folks have not quite settled on de-facto implementations so you have a mix of reference implementations, and other utility-type libraries that provide other things along with WSGI support (e.g. Paste).  Of course I could be wrong, I'm a newcomer to Python.  The "web scripting" community is coming at the problem from a different direction (shared hosting, CGI legacy, privilege separation concerns) than Java folks had the luxury of starting with (running a single enterprise container in a dedicated environment against statically compiled and deployed code).

Counting array elements in Python [duplicate]

UnkwnTech

[Counting array elements in Python [duplicate]](https://stackoverflow.com/questions/187455/counting-array-elements-in-python)

How can I count the number of elements in an array, because contrary to logic array.count(string) does not count all the elements in the array, it just searches for the number of occurrences of string.

2008-10-09 14:12:55Z

How can I count the number of elements in an array, because contrary to logic array.count(string) does not count all the elements in the array, it just searches for the number of occurrences of string.The method len() returns the number of elements in the list.Syntax:Eg:Output:len is a built-in function that calls the given container object's __len__ member function to get the number of elements in the object.  Functions encased with double underscores are usually "special methods" implementing one of the standard interfaces in Python (container, number, etc).  Special methods are used via syntactic sugar (object creation, container indexing and slicing, attribute access, built-in functions, etc.).Using obj.__len__() wouldn't be the correct way of using the special method, but I don't see why the others were modded down so much.If you have a multi-dimensional array, len() might not give you the value you are looking for. For instance:This code block will return true, telling you the size of the array is 2. However, there are in fact 10 elements in this 2D array. In the case of multi-dimensional arrays, len() gives you the length of the first dimension of the array i.e. To get the number of elements in a multi-dimensional array of arbitrary shape:Or,if you want to be oopy; "len(myArray)" is a lot easier to type! :)Before I saw this, I thought to myself, "I need to make a way to do this!"And then I thought, "There must be a simpler way to do this." and I was right.len(arrayName)

How to loop through all but the last item of a list?

David Sykes

[How to loop through all but the last item of a list?](https://stackoverflow.com/questions/914715/how-to-loop-through-all-but-the-last-item-of-a-list)

I would like to loop through a list checking each item against the one following it.Is there a way I can loop through all but the last item using for x in y? I would prefer to do it without using indexes if I can.Notefreespace answered my actual question, which is why I accepted the answer, but SilentGhost answered the question I should have asked.Apologies for the confusion.

2009-05-27 08:59:35Z

I would like to loop through a list checking each item against the one following it.Is there a way I can loop through all but the last item using for x in y? I would prefer to do it without using indexes if I can.Notefreespace answered my actual question, which is why I accepted the answer, but SilentGhost answered the question I should have asked.Apologies for the confusion.If y is a generator, then the above will not work.the easiest way to compare the sequence item with the following:If you want to get all the elements in the sequence pair wise, use this approach (the pairwise function is from the examples in the itertools module).If you need to compare the last value to some special value, chain that value to the endif you meant comparing nth item with n+1 th item in the list you could also do withnote there is no hard coding going on there. This should be ok unless you feel otherwise.To compare each item with the next one in an iterator without instantiating a list:This answers what the OP should have asked, i.e. traverse a list comparing consecutive elements (excellent SilentGhost answer), yet generalized for any group (n-gram): 2, 3, ... n:zip(*(l[start:] for start in range(0, n)))Examples:Explanations:Note:AFAIK, this code is as lazy as it can be. Not tested.

Python: json.loads returns items prefixing with 'u'

janeh

[Python: json.loads returns items prefixing with 'u'](https://stackoverflow.com/questions/13940272/python-json-loads-returns-items-prefixing-with-u)

I'll be receiving a JSON encoded string form Obj-C, and I am decoding a dummy string (for now) like the code below. My output comes out with character 'u' prefixing each item: How is JSON adding this unicode char? What's the best way to remove it? 

2012-12-18 19:40:28Z

I'll be receiving a JSON encoded string form Obj-C, and I am decoding a dummy string (for now) like the code below. My output comes out with character 'u' prefixing each item: How is JSON adding this unicode char? What's the best way to remove it? The u- prefix just means that you have a Unicode string.  When you really use the string, it won't appear in your data.  Don't be thrown by the printed output.For example, try this:You won't see a u.Everything is cool, man. The 'u' is a good thing, it indicates that the string is of type Unicode in python 2.x. http://docs.python.org/2/howto/unicode.html#the-unicode-typeThe d3 print below is the one you are looking for (which is the combination of dumps and loads) :)Having:Prints:Unicode is an appropriate type here. The JSONDecoder docs describe the conversion table and state that json string objects are decoded into Unicode objectshttps://docs.python.org/2/library/json.html#encoders-and-decoders"encoding determines the encoding used to interpret any str objects decoded by this instance (UTF-8 by default)."The u prefix means that those strings are unicode rather than 8-bit strings. The best way to not show the u prefix is to switch to Python 3, where strings are unicode by default. If that's not an option, the str constructor will convert from unicode to 8-bit, so simply loop recursively over the result and convert unicode to str. However, it is probably best just to leave the strings as unicode.Those 'u' characters being appended to an object signifies that the object is encoded in "unicode". If you want to remove those 'u' chars from your object you can do this:Let's checkout from python shellI kept running into this problem when trying to capture JSON data in the log with the Python logging library, for debugging and troubleshooting purposes. Getting the u character is a real nuisance when you want to copy the text and paste it into your code somewhere.As everyone will tell you, this is because it is a Unicode representation, and it could come from the fact that you’ve used json.loads() to load in the data from a string in the first place. If you want the JSON representation in the log, without the u prefix, the trick is to use json.dumps() before logging it out. For example:Just replace the u' with a single quote...Try this:mail_accounts[0].encode("ascii")

Any way to clear python's IDLE window?

devoured elysium

[Any way to clear python's IDLE window?](https://stackoverflow.com/questions/1432480/any-way-to-clear-pythons-idle-window)

I know there's a similar topic about python console, but I do not know if they are the same. I tried system("clear") and it didn't work here.How do I clear python's IDLE window?

2009-09-16 11:49:36Z

I know there's a similar topic about python console, but I do not know if they are the same. I tried system("clear") and it didn't work here.How do I clear python's IDLE window?The "cls" and "clear" are commands which will clear a terminal (ie a DOS prompt, or terminal window).  From your screenshot, you are using the shell within IDLE, which won't be affected by such things.  Unfortunately, I don't think there is a way to clear the screen in IDLE.  The best you could do is to scroll the screen down lots of lines, eg:Though you could put this in a function:And then call it when needed as cls()os.system('clear') works on linux. If you are running windows try os.system('CLS') instead.You need to import os first like this: ctrl + L clears the screen on Ubuntu Linux.Most of the answers, here do clearing the DOS prompt screen, with clearing commands, which is not the question. Other answers here, were printing blank lines to show a clearing effect of the screen. The simplest answer of this question is An extension for clearing the shell can be found in Issue6143 as a "feature request". This extension is included with IdleX.That does is perfectly.  No '0' printed either.There does not appear to be a way to clear the IDLE 'shell' buffer.The way to execute commands in Python 2.4+ is to use the subprocess module. You can use it in the same way that you use os.system.If you're executing this in the python console, you'll need to do something to hide the return value (for either os.system or subprocess.call), like assigning it to a variable:File -> New WindowIn the new window**Run -> Python ShellThe problem with this method is that it will clear all the things you defined, such as variables.Alternatively, you should just use command prompt. open up command prompttype "cd c:\python27"type "python example.py" , you have to edit this using IDLE when it's not in interactive mode. If you're in python shell, file -> new window.Note that the example.py needs to be in the same directory as C:\python27, or whatever directory you have python installed.Then from here, you just press the UP arrow key on your keyboard. You just edit example.py, use CTRL + S, then go back to command prompt, press the UP arrow key, hit enter.If the command prompt gets too crowded, just type "clr"The "clr" command only works with command prompt, it will not work with IDLE.I like to use:It seems like there is no direct way for clearing the IDLE console.One way I do it is use of exit() as the last command in my python script (.py). When I run the script, it always opens up a new console and prompt before exiting.Upside : Console is launched fresh each time the script is executed.

Downside : Console is launched fresh each time the script is executed.You can make an AutoHotKey script.To set ctrl-r to a hotkey to clear the shell:Just install AutoHotKey, put the above in a file called idle_clear.ahk, run the file, and your hotkey is active.None of these solutions worked for me on Windows 7 and within IDLE.  Wound up using PowerShell, running Python within it and exiting to call "cls" in PowerShell to clear the window.  CONS: Assumes Python is already in the PATH variable.  Also, this does clear your Python variables (though so does restarting the shell).PROS: Retains any window customization you've made (color, font-size).It seems it is impossible to do it without any external library.An alternative way if you are using windows and don't want to open and close the shell everytime you want to clear it is by using windows command prompt.I would recommend you to use Thonny IDE for Python. It's shell has "Clear Shell" option and you can also track variables created in a separate list. It's debugging is very good even comparing with modern IDEs. You can also write code in python file along with access to shell at the same place.And its lightweight!"command + L" for MAC OS X."control + L" for Ubuntu Clears the last line on the interactive session As mark.ribau said, it seems that there is no way to clear the Text widget in idle. One should edit the EditorWindow.py module and add a method and a menu item in the EditorWindow class that does something like:and perhaps some more tag management of which I'm unaware of.The best way to do it on windows is using the command prompt 'cmd' and access python directory the command prompt could be found on the start menu >run>cmduse thisYour screenshot shows you're using IDLE. cls/clear won't work for you.This works for me in Windows:print chr(12)Whenever you wish to clear console call the function cls, but note that this function will clear the screen in Windows only if you are running Python Script using cmd prompt, it will not clear the buffer if you running by IDLE.If you are using the terminal ( i am using ubuntu ) , then just use the combination of CTRL+l from keyboard to clear the python script, even it clears the terminal script...There is no need to write your own function to do this! Python has a built in clear function. Type the following in the command prompt:If using IPython for Windows, it's

When should iteritems() be used instead of items()?

Jon Clements

[When should iteritems() be used instead of items()?](https://stackoverflow.com/questions/13998492/when-should-iteritems-be-used-instead-of-items)

Is it legitimate to use items() instead of iteritems() in all places? Why was iteritems() removed from Python 3? Seems like a terrific and useful method. What's the reasoning behind it?Edit: To clarify, I want to know what is the correct idiom for iterating over a dictionary in a generator-like way (one item at a time, not all into memory) in a way that is compatible with both Python 2 and Python 3?

2012-12-21 23:27:24Z

Is it legitimate to use items() instead of iteritems() in all places? Why was iteritems() removed from Python 3? Seems like a terrific and useful method. What's the reasoning behind it?Edit: To clarify, I want to know what is the correct idiom for iterating over a dictionary in a generator-like way (one item at a time, not all into memory) in a way that is compatible with both Python 2 and Python 3?In Python 2.x - .items() returned a list of (key, value) pairs. In Python 3.x, .items() is now an itemview object, which behaves different - so it has to be iterated over, or materialised... So, list(dict.items()) is required for what was dict.items() in Python 2.x.Python 2.7 also has a bit of a back-port for key handling, in that you have viewkeys, viewitems and viewvalues methods, the most useful being viewkeys which behaves more like a set (which you'd expect from a dict).Simple example:Will give you a list of the common keys, but again, in Python 3.x - just use .keys() instead.Python 3.x has generally been made to be more "lazy" - i.e. map is now effectively itertools.imap, zip is itertools.izip, etc.dict.iteritems was removed because dict.items now does the thing dict.iteritems did in python 2.x and even improved it a bit by making it an itemview.The six library helps with writing code that is compatible with both python 2.5+ and python 3. It has an iteritems method that will work in both python 2 and 3. Example:As the dictionary documentation for python 2 and python 3 would tell you, in python 2 items returns a list, while iteritems returns a iterator.In python 3, items returns a view, which is pretty much the same as an iterator.If you are using python 2, you may want to user iteritems if you are dealing with large dictionaries and all you want to do is iterate over the items (not necessarily copy them to a list)Just as @Wessie noted, dict.iteritems, dict.iterkeys and dict.itervalues (which return an iterator in Python2.x) as well as dict.viewitems, dict.viewkeys and dict.viewvalues (which return view objects in Python2.x) were all removed in Python3.xAnd dict.items, dict.keys and dict.values used to return a copy of the dictionary's list in Python2.x now return view objects in Python3.x, but they are still not the same as iterator.If you want to return an iterator in Python3.x, use iter(dictview) :You cannot use items instead iteritems in all places in Python. For example, the following code:will break if you use items:The same is true for viewitems, which is available in Python 3.Also, since items returns a copy of the dictionary’s list of (key, value) pairs, it is less efficient, unless you want to create a copy anyway.In Python 2, it is best to use iteritems for iteration. The 2to3 tool can replace it with items if you ever decide to upgrade to Python 3.

How do I abort the execution of a Python script? [duplicate]

Ray Vega

[How do I abort the execution of a Python script? [duplicate]](https://stackoverflow.com/questions/179369/how-do-i-abort-the-execution-of-a-python-script)

I have a simple Python script that I want to stop executing if a condition is met.For example:Essentially, I am looking for something that behaves equivalently to the 'return' keyword in the body of a function which allows the flow of the code to exit the function and not execute the remaining code.

2008-10-07 16:46:36Z

I have a simple Python script that I want to stop executing if a condition is met.For example:Essentially, I am looking for something that behaves equivalently to the 'return' keyword in the body of a function which allows the flow of the code to exit the function and not execute the remaining code.To exit a script you can use,You can also provide an exit status value, usually an integer.Exits with zero, which is generally interpreted as success.  Non-zero codes are usually treated as errors.  The default is to exit with zero.Prints "aa! errors!" and exits with a status code of 1.There is also an _exit() function in the os module.  The sys.exit() function raises a SystemExit exception to exit the program, so try statements and cleanup code can execute.  The os._exit() version doesn't do this.  It just ends the program without doing any cleanup or flushing output buffers, so it shouldn't normally be used.  The Python docs indicate that os._exit() is the normal way to end a child process created with a call to os.fork(), so it does have a use in certain circumstances.You could put the body of your script into a function and then you could return from that function.You can either use:or:The optional parameter can be an exit code or an error message. Both methods are identical. I used to prefer sys.exit, but I've lately switched to raising SystemExit, because it seems to stand out better among the rest of the code (due to the raise keyword).TryIt is like the perlif this is what you are looking for. It terminates the execution of the script even it is called from an imported module / def /functionexit() should do the trickexit() should do it.If the entire program should stop use sys.exit() otherwise just use an empty return.

How do I get current URL in Selenium Webdriver 2 Python?

user2276896

[How do I get current URL in Selenium Webdriver 2 Python?](https://stackoverflow.com/questions/15985339/how-do-i-get-current-url-in-selenium-webdriver-2-python)

I'm trying to get the current url after a series of navigations in Selenium. I know there's a command called getLocation for ruby, but I can't find the syntax for Python.

2013-04-13 07:20:01Z

I'm trying to get the current url after a series of navigations in Selenium. I know there's a command called getLocation for ruby, but I can't find the syntax for Python.Use current_url element. Example:According to this documentation (a place full of goodies:)):  or, see official documentation:

https://seleniumhq.github.io/docs/site/en/webdriver/browser_manipulation/#get-current-urlSelenium2Library has get_location():Another way to do it would be to inspect the url bar in chrome to find the id of the element, have your WebDriver click that element, and then send the keys you use to copy and paste using the keys common function from selenium, and then printing it out or storing it as a variable, etc. 

Random row selection in Pandas dataframe

John

[Random row selection in Pandas dataframe](https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe)

Is there a way to select random rows from a DataFrame in Pandas.In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.I have also looked at the slicing documentation and there seems to be nothing equivalent.Now using version 20. There is a sample method.df.sample(n)

2013-04-10 10:52:47Z

Is there a way to select random rows from a DataFrame in Pandas.In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.I have also looked at the slicing documentation and there seems to be nothing equivalent.Now using version 20. There is a sample method.df.sample(n)Something like this?Note: As of Pandas v0.20.0, ix has been deprecated in favour of loc for label based indexing.With pandas version 0.16.1 and up, there is now a DataFrame.sample method built-in:For either approach above, you can get the rest of the rows by doing:As of v0.20.0, you can use pd.DataFrame.sample, which can be used to return a random sample of a fixed number rows, or a percentage of rows:For reproducibility, you can specify an integer random_state, equivalent to using np.ramdom.seed. So, instead of setting, for example, np.random.seed = 0, you can:The best way to do this is with the sample function from the random module,Actually this will give you repeated indices np.random.random_integers(0, len(df), N) where N is a large number.Below line will randomly select n number of rows out of the total existing row numbers from the dataframe df without replacement.df=df.take(np.random.permutation(len(df))[:n])

Call a function with argument list in python

SurDin

[Call a function with argument list in python](https://stackoverflow.com/questions/817087/call-a-function-with-argument-list-in-python)

I'm trying to call a function inside another function in python, but can't find the right syntax. What I want to do is something like this:In this case first call will work, and second won't.

What I want to modify is the wrapper function and not the called functions.

2009-05-03 13:43:33Z

I'm trying to call a function inside another function in python, but can't find the right syntax. What I want to do is something like this:In this case first call will work, and second won't.

What I want to modify is the wrapper function and not the called functions.To expand a little on the other answers:In the line:The * next to args means "take the rest of the parameters given and put them in a list called args". In the line:The * next to args here means "take this list called args and 'unwrap' it into the rest of the parameters.So you can do the following:In wrapper2, the list is passed explicitly, but in both wrappers args contains the list [1,2,3].The simpliest way to wrap a function ... is to manually write a wrapper that would call func() inside itself:In Python function is an object, so you can pass it's name as an argument of another function and return it. You can also write a wrapper generator for any function anyFunc():Please also note that in Python when you don't know or don't want to name all the arguments of a function, you can refer to a tuple of arguments, which is denoted by its name, preceded by an asterisk in the parentheses after the function name:For example you can define a function that would take any number of arguments:Python provides for even further manipulation on function arguments. You can allow a function to take keyword arguments. Within the function body the keyword arguments are held in a dictionary. In the parentheses after the function name this dictionary is denoted by two asterisks followed by the name of the dictionary:A similar example that prints the keyword arguments dictionary:You can use *args and **kwargs syntax for variable length arguments.What do *args and **kwargs mean?And from the official python tutorial http://docs.python.org/dev/tutorial/controlflow.html#more-on-defining-functionsThe literal answer to your question (to do exactly what you asked, changing only the wrapper, not the functions or the function calls) is simply to alter the lineto read This tells Python to take the list given (in this case, args) and pass its contents to the function as positional arguments.This trick works on both "sides" of the function call, so a function defined like this:would be able to accept as many positional arguments as you throw at it, and place them all into a list called args.I hope this helps to clarify things a little.  Note that this is all possible with dicts/keyword arguments as well, using ** instead of *.You need to use arguments unpacking..A small addition to previous answers, since I couldn't find a solution for a problem, which is not worth opening a new question, but led me here.Here is a small code snippet, which combines lists, zip() and *args, to provide a wrapper that can deal with an unknown amount of functions with an unknown amount of arguments.Keep in mind, that zip() does not provide a safety check for lists of unequal length, see zip iterators asserting for equal length in python.

How do lexical closures work?

Eli Bendersky

[How do lexical closures work?](https://stackoverflow.com/questions/233673/how-do-lexical-closures-work)

While I was investigating a problem I had with lexical closures in Javascript code, I came along this problem in Python:Note that this example mindfully avoids lambda. It prints "4 4 4", which is surprising. I'd expect "0 2 4". This equivalent Perl code does it right:"0 2 4" is printed.Can you please explain the difference ?Update: The problem is not with i being global. This displays the same behavior:As the commented line shows, i is unknown at that point. Still, it prints "4 4 4".

2008-10-24 14:08:25Z

While I was investigating a problem I had with lexical closures in Javascript code, I came along this problem in Python:Note that this example mindfully avoids lambda. It prints "4 4 4", which is surprising. I'd expect "0 2 4". This equivalent Perl code does it right:"0 2 4" is printed.Can you please explain the difference ?Update: The problem is not with i being global. This displays the same behavior:As the commented line shows, i is unknown at that point. Still, it prints "4 4 4".Python is actually behaving as defined. Three separate functions are created, but  they each have the closure of the environment they're defined in - in this case, the global environment (or the outer function's environment if the loop is placed inside another function). This is exactly the problem, though - in this environment, i is mutated, and the closures all refer to the same i.Here is the best solution I can come up with - create a function creater and invoke that instead. This will force different environments for each of the functions created, with a different i in each one.This is what happens when you mix side effects and functional programming. The functions defined in the loop keep accessing the same variable i while its value changes. At the end of the loop, all the functions point to the same variable, which is holding the last value in the loop: the effect is what reported in the example.In order to evaluate i and use its value, a common pattern is to set it as a parameter default: parameter defaults are evaluated when the def statement is executed, and thus the value of the loop variable is frozen.The following works as expected:Here's how you do it using the functools library (which I'm not sure was available at the time the question was posed).Outputs 0 2 4, as expected.look at this:It means they all point to the same i variable instance, which will have a value of 2 once the loop is over.A readable solution:What is happening is that the variable i is captured, and the functions are returning the value it is bound to at the time it is called.  In functional languages this kind of situation never arises, as i wouldn't be rebound.  However with python, and also as you've seen with lisp, this is no longer true.The difference with your scheme example is to do with the semantics of the do loop.  Scheme is effectively creating a new i variable each time through the loop, rather than reusing an existing i binding as with the other languages.  If you use a different variable created external to the loop and mutate it, you'll see the same behaviour in scheme.  Try replacing your loop with:Take a look here for some further discussion of this.[Edit] Possibly a better way to describe it is to think of the do loop as a macro which performs the following steps:ie. the equivalent to the below python:The i is no longer the one from the parent scope but a brand new variable in its own scope (ie. the parameter to the lambda) and so you get the behaviour you observe.  Python doesn't have this implicit new scope, so the body of the for loop just shares the i variable.I'm still not entirely convinced why in some languages this works one way, and in some another way. In Common Lisp it's like Python:Prints "6 6 6" (note that here the list is from 1 to 3, and built in reverse").

While in Scheme it works like in Perl:Prints "6 4 2"And as I've mentioned already, Javascript is in the Python/CL camp. It appears there is an implementation decision here, which different languages approach in distinct ways. I would love to understand what is the decision, exactly.The problem is that all of the local functions bind to the same environment and thus to the same i variable. The solution (workaround) is to create separate environments (stack frames) for each function (or lambda):The variable i is a global, whose value is 2 at each time the function f is called.I would be inclined to implement the behavior you're after as follows:Response to your update:  It's not the globalness of i per se which is causing this behavior, it's the fact that it's a variable from an enclosing scope which has a fixed value over the times when f is called.  In your second example, the value of i is taken from the scope of the kkk function, and nothing is changing that when you call the functions on flist.The reasoning behind the behavior has already been explained, and multiple solutions have been posted, but I think this is the most pythonic (remember, everything in Python is an object!):Claudiu's answer is pretty good, using a function generator, but piro's answer is a hack, to be honest, as it's making i into a "hidden" argument with a default value (it'll work fine, but it's not "pythonic").

python list by value not by reference [duplicate]

d.putto

[python list by value not by reference [duplicate]](https://stackoverflow.com/questions/8744113/python-list-by-value-not-by-reference)

Let's take an exampleI wanted to append value in list 'b' but the value of list 'a' have also changed.

I think I have little idea why its like this (python passes lists by reference).

My question is "how can I pass it by value so that appending 'b' does't change values in 'a' ?"

2012-01-05 14:28:51Z

Let's take an exampleI wanted to append value in list 'b' but the value of list 'a' have also changed.

I think I have little idea why its like this (python passes lists by reference).

My question is "how can I pass it by value so that appending 'b' does't change values in 'a' ?"As answered in the official Python FAQ:To copy a list you can use list(a) or a[:]. In both cases a new object is created.

These two methods, however, have limitations with collections of mutable objects as inner objects keep their references intact:If you want a full copy of your objects you need copy.deepcopyIn terms of performance my favorite answer would be:Check how the related alternatives compare with each other in terms of performance:Also, you can do:This will work for any sequence, even those that don't support indexers and slices...If you want to copy a one-dimensional list, useHowever, if a is a 2-dimensional list, this is not going to work for you. That is, any changes in a will also be reflected in b. In that case, useAs mentioned by phihag in his answer,will work for your case since slicing a list creates a new memory id of the list (meaning you are no longer referencing the same object in your memory and the changes you make to one will not be reflected in the other.)However, there is a slight problem.  If your list is multidimensional, as in lists within lists, simply slicing will not solve this problem.  Changes made in the higher dimensions, i.e. the lists within the original list, will be shared between the two.Do not fret, there is a solution.  The module copy has a nifty copying technique that takes care of this issue.will copy a list with a new memory id no matter how many levels of lists it contains!To create a copy of a list do this:When you do b = a you simply create another pointer to the same memory of a,

that's why when you append to b , a changes too.You need to create copy of a and that's done like this:I found that we can use extend() to implement the function of copy()I would recommend the following solution:This will copy all the elements from a to b. The copy will be value copy, not reference copy.b = list(a)See http://henry.precheur.org/python/copy_list.

Retrieving parameters from a URL

niteshb

[Retrieving parameters from a URL](https://stackoverflow.com/questions/5074803/retrieving-parameters-from-a-url)

Given a URL like the following, how can I parse the value of the query parameters? For example, in this case I want the value of def.I am using Django in my environment; is there a method on the request object that could help me?I tried using self.request.get('def') but it is not returning the value ghi as I had hoped.

2011-02-22 06:27:16Z

Given a URL like the following, how can I parse the value of the query parameters? For example, in this case I want the value of def.I am using Django in my environment; is there a method on the request object that could help me?I tried using self.request.get('def') but it is not returning the value ghi as I had hoped.Python 2:Python 3:parse_qs returns a list of values, so the above code will print ['ghi'].Here's the Python 3 documentation.I'm shocked this solution isn't on here already. Use: This will "get" the variable from the "GET" dictionary, and return the 'variable_name' value if it exists, or a None object if it doesn't exist.for Python > 3.4There is a new library called furl. I find this library to be most pythonic for doing url algebra. 

To install:Code:I know this is a bit late but since I found myself on here today, I thought that this might be a useful answer for others.With parse_qsl(), "Data are returned as a list of name, value pairs."The url you are referring is a query type and I see that the request object supports a method called arguments to get the query arguments. You may also want try self.request.get('def') directly to get your value from the object..Hope this helpsThe urlparse module provides everything you need:urlparse.parse_qs()There's not need to do any of that. Only withNotice that I'm not specifying the method (GET, POST, etc). This is well documented and this is an exampleThe fact that you use Django templates doesn't mean the handler is processed by Django as wellIn pure Python:Btw, I was having issues using parse_qs() and getting empty value parameters and learned that you have to pass a second optional parameter 'keep_blank_values' to return a list of the parameters in a query string that contain no values. It defaults to false. Some crappy written APIs require parameters to be present even if they contain no valuesThere is a nice library w3lib.urlThis one is simple. The variable parameters will contain a dictionary of all the parameters.I see there isn't an answer for users of Tornado:This method must work inside an handler that is derived from: None is the answer this method will return when the requested key can't be found. This saves you some exception handling.

Iterating through directories with Python

Wolf

[Iterating through directories with Python](https://stackoverflow.com/questions/19587118/iterating-through-directories-with-python)

I need to iterate through the subdirectories of a given directory and search for files. If I get a file I have to open it and change the content and replace it with my own lines.I tried this:but I am getting an error. What am I doing wrong? 

2013-10-25 10:17:39Z

I need to iterate through the subdirectories of a given directory and search for files. If I get a file I have to open it and change the content and replace it with my own lines.I tried this:but I am getting an error. What am I doing wrong? The actual walk through the directories works as you have coded it. If you replace the contents of the inner loop with a simple print statement you can see that each file is found:If you still get errors when running the above, please provide the error message.Updated for Python3Another way of returning all files in subdirectories is to use the pathlib module, introduced in Python 3.4, which provides an object oriented approach to handling filesystem paths (Pathlib is also available on Python 2.7 via the pathlib2 module on PyPi):Since Python 3.5, the glob module also supports recursive file finding:The file_list from either of the above approaches can be iterated over without the need for a nested loop:As of 2019, glob.iglob(path/**, recursive=True) seems the most pythonic solution, i.e.:Output:Notes:

1 - glob.iglob2 - If recursive is True, the pattern '**' will match any files and

zero or more directories and subdirectories.3 - If the directory contains files starting with . they won’t be matched by default. For example, consider a directory containing card.gif and .card.gif:.

How do I pick 2 random items from a Python set? [duplicate]

Thierry Lam

[How do I pick 2 random items from a Python set? [duplicate]](https://stackoverflow.com/questions/1262955/how-do-i-pick-2-random-items-from-a-python-set)

