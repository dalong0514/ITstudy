
How can I add new keys to a dictionary?

lfaraone

[How can I add new keys to a dictionary?](https://stackoverflow.com/questions/1024847/how-can-i-add-new-keys-to-a-dictionary)

Is it possible to add a key to a Python dictionary after it has been created?It doesn't seem to have an .add() method.

2009-06-21 22:07:39Z

Is it possible to add a key to a Python dictionary after it has been created?It doesn't seem to have an .add() method.To add multiple keys simultaneously, use dict.update():For adding a single key, the accepted answer has less computational overhead.I feel like consolidating info about Python dictionaries:This uses a new featrue called dictionary unpacking.The update operator |= now works for dictionaries:The merge operator | now works for dictionaries:Feel free to add more!Yes it is possible, and it does have a method that implements this, but you don't want to use it directly.To demonstrate how and how not to use it, let's create an empty dict with the dict literal, {}:To update this dict with a single new key and value, you can use the subscript notation (see Mappings here) that provides for item assignment: my_dict is now:We can also update the dict with multiple values efficiently as well using the update method.  We may be unnecessarily creating an extra dict here, so we hope our dict has already been created and came from or was used for another purpose:my_dict is now:Another efficient way of doing this with the update method is with keyword arguments, but since they have to be legitimate python words, you can't have spaces or special symbols or start the name with a number, but many consider this a more readable way to create keys for a dict, and here we certainly avoid creating an extra unnecessary dict:and my_dict is now:So now we have covered three Pythonic ways of updating a dict. There's another way of updating a dict that you shouldn't use, which uses the __setitem__ method. Here's an example of how one might use the __setitem__ method to add a key-value pair to a dict, and a demonstration of the poor performance of using it:So we see that using the subscript notation is actually much faster than using __setitem__. Doing the Pythonic thing, that is, using the language in the way it was intended to be used, usually is both more readable and computationally efficient.If you want to add a dictionary within a dictionary you can do it this way. Example: Add a new entry to your dictionary & sub dictionaryOutput:NOTE: Python requires that you first add a sub  before adding entries.The orthodox syntax is d[key] = value, but if your keyboard is missing the square bracket keys you could do:In fact, defining __getitem__ and __setitem__ methods is how you can make your own class support the  square bracket syntax. See https://python.developpez.com/cours/DiveIntoPython/php/endiveintopython/object_oriented_framework/special_class_methods.phpYou can create one:Gives:This popular question addresses functional methods of merging dictionaries a and b.Here are some of the more straightforward methods (tested in Python 3)...Note: The first method above only works if the keys in b are strings.To add or modify a single element, the b dictionary would contain only that one element...This is equivalent to...Let's pretend you want to live in the immutable world and do NOT want to modify the original but want to create a new dict that is the result of adding a new key to the original.In Python 3.5+ you can do:The Python 2 equivalent is:After either of these:params is still equal to {'a': 1, 'b': 2}andnew_params is equal to {'a': 1, 'b': 2, 'c': 3}There will be times when you don't want to modify the original (you only want the result of adding to the original). I find this a refreshing alternative to the following:orReference: https://stackoverflow.com/a/2255892/514866So many answers and still everybody forgot about the strangely named, oddly behaved, and yet still handy dict.setdefault()Thisbasically just does this:e.g.If you're not joining two dictionaries, but adding new key-value pairs to a dictionary, then using the subscript notation seems like the best way.However, if you'd like to add, for example, thousands of new key-value pairs, you should consider using the update() method.I think it would also be useful to point out Python's collections module that consists of many useful dictionary subclasses and wrappers that simplify the addition and modification of data types in a dictionary, specifically defaultdict:This is particularly useful if you are working with dictionaries that always consist of the same data types or structures, for example a dictionary of lists. If the key does not yet exist, defaultdict assigns the value given (in our case 10) as the initial value to the dictionary (often used inside loops). This operation therefore does two things: it adds a new key to a dictionary (as per question), and assigns the value if the key doesn't yet exist. With the standard dictionary, this would have raised an error as the += operation is trying to access a value that doesn't yet exist:Without the use of defaultdict, the amount of code to add a new element would be much greater and perhaps looks something like: defaultdict can also be used with complex data types such as list and set:Adding an element automatically initialises the list.Here's another way that I didn't see here: You can use the dictionary constructor and implicit expansion to reconstruct a dictionary. Moreover, interestingly, this method can be used to control the positional order during dictionary construction (post Python 3.6). In fact, insertion order is guaranteed for Python 3.7 and above!The above is using dictionary comprehension.first to check whether the key already existsthen you can add the new key and valueadd dictionary key, value class.

How to install pip on Windows?

Colonel Panic

[How to install pip on Windows?](https://stackoverflow.com/questions/4750806/how-to-install-pip-on-windows)

pip is a replacement for easy_install. But should I install pip using easy_install on Windows?  Is there a better way?

2012-09-18 11:45:33Z

pip is a replacement for easy_install. But should I install pip using easy_install on Windows?  Is there a better way?Good news! Python 3.4 (released March 2014) and Python 2.7.9 (released December 2014) ship with Pip. This is the best feature of any Python release. It makes the community's wealth of libraries accessible to everyone. Newbies are no longer excluded from using community libraries by the prohibitive difficulty of setup. In shipping with a package manager, Python joins Ruby, Node.js, Haskell, Perl, Go—almost every other contemporary language with a majority open-source community. Thank you, Python.If you do find that pip is not available when using Python 3.4+ or Python 2.7.9+, simply execute e.g.:Of course, that doesn't mean Python packaging is problem solved. The experience remains frustrating. I discuss this in the Stack Overflow question Does Python have a package/module management system?.And, alas for everyone using Python 2.7.8 or earlier (a sizable portion of the community). There's no plan to ship Pip to you. Manual instructions follow.Flying in the face of its 'batteries included' motto, Python ships without a package manager. To make matters worse, Pip was—until recently—ironically difficult to install.Per https://pip.pypa.io/en/stable/installing/#do-i-need-to-install-pip:Download get-pip.py, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:You possibly need an administrator command prompt to do this. Follow Start a Command Prompt as an Administrator (Microsoft TechNet).This installs the pip package, which (in Windows) contains ...\Scripts\pip.exe that path must be in PATH environment variable to use pip from the command line (see the second part of 'Alternative Instructions' for adding it to your PATH,The official documentation tells users to install Pip and each of its dependencies from source. That's tedious for the experienced and prohibitively difficult for newbies.For our sake, Christoph Gohlke prepares Windows installers (.msi) for popular Python packages. He builds installers for all Python versions, both 32 and 64 bit. You need to:For me, this installed Pip at C:\Python27\Scripts\pip.exe. Find pip.exe on your computer, then add its folder (for example, C:\Python27\Scripts) to your path (Start / Edit environment variables). Now you should be able to run pip from the command line. Try installing a package:There you go (hopefully)! Solutions for common problems are given below:If you work in an office, you might be behind an HTTP proxy. If so, set the environment variables http_proxy and https_proxy. Most Python applications (and other free software) respect these. Example syntax:If you're really unlucky, your proxy might be a Microsoft NTLM proxy. Free software can't cope. The only solution is to install a free software friendly proxy that forwards to the nasty proxy. http://cntlm.sourceforge.net/Python modules can be partly written in C or C++. Pip tries to compile from source. If you don't have a C/C++ compiler installed and configured, you'll see this cryptic error message.You can fix that by installing a C++ compiler such as MinGW or Visual C++. Microsoft actually ships one specifically for use with Python. Or try Microsoft Visual C++ Compiler for Python 2.7.Often though it's easier to check Christoph's site for your package.-- Outdated -- use distribute, not setuptools as described here. --

-- Outdated #2 -- use setuptools as distribute is deprecated.As you mentioned pip doesn't include an independent installer, but you can install it with its predecessor easy_install.So:You are done. Now you can use pip install package to easily install packages as in Linux :)2014 UPDATE:1) If you have installed Python 3.4 or later, pip is included with Python and should already be working on your system.2) If you are running a version below Python 3.4 or if pip was not installed with Python 3.4 for some reason, then you'd probably use pip's official installation script get-pip.py. The pip installer now grabs setuptools for you, and works regardless of architecture (32-bit or 64-bit).The installation instructions are detailed here and involve:I'll leave the two sets of old instructions below for posterity.OLD Answers:For Windows editions of the 64 bit variety - 64-bit Windows + Python used to require a separate installation method due to ez_setup, but I've tested the new distribute method on 64-bit Windows running 32-bit Python and 64-bit Python, and you can now use the same method for all versions of Windows/Python 2.7X:OLD Method 2 using distribute:The last step will not work unless you're either in the directory easy_install.exe is located in (C:\Python27\Scripts would be the default for Python 2.7), or you have that directory added to your path.OLD Method 1 using ez_setup:from the setuptools page --After this, you may continue with:2016+ Update: These answers are outdated or otherwise wordy and difficult.If you've got Python 3.4+ or 2.7.9+, it will be installed by default on Windows.  Otherwise, in short:The new binaries pip.exe (and the deprecated easy_install.exe) will be found in the "%ProgramFiles%\PythonXX\Scripts" folder (or similar), which is often not in your PATH variable.  I recommend adding it.Python 3.4, which  was released in March 2014, comes with pip included:

http://docs.python.org/3.4/whatsnew/3.4.html

So, since the release of Python 3.4, the up-to-date way to install pip on Windows is to just install Python.The recommended way to use it is to call it as a module, especially with multiple python distributions or versions installed, to guarantee packages go to the correct place:

python -m pip install --upgrade packageXYZhttps://docs.python.org/3/installing/#work-with-multiple-versions-of-python-installed-in-parallelWhen I have to use Windows, I use ActivePython, which automatically adds everything to your PATH and includes a package manager called PyPM which provides binary package management making it faster and simpler to install packages.pip and easy_install aren't exactly the same thing, so there are some things you can get through pip but not easy_install and vice versa.My recommendation is that you get ActivePython Community Edition and don't worry about the huge hassle of getting everything set up for Python on Windows. Then, you can just use pypm.In case you want to use pip you have to check the PyPM option in the ActiveState installer. After installation you only need to logoff and log on again, and pip will be available on the commandline, because it is contained in the ActiveState installer PyPM option and the paths have been set by the installer for you already. PyPM will also be available, but you do not have to use it.The up-to-date way is to use Windows' package manager Chocolatey.Once this is installed, all you have to do is open a command prompt and run the following the three commands below, which will install Python 2.7, easy_install and pip. It will automatically detect whether you're on x64 or x86 Windows.All of the other Python packages on the Chocolatey Gallery can be found here.Update March 2015Python 2.7.9 and later (on the Python 2 series), and Python 3.4 and later include pip by default, so you may have pip already.If you don't, run this one line command on your prompt (which may require administrator access):It will install pip. If Setuptools is not already installed, get-pip.py will install it for you too.As mentioned in comments, the above command will download code from the Pip source code repository at GitHub, and dynamically run it at your environment. So be noticed that this is a shortcut of the steps download, inspect and run, all with a single command using Python itself. If you trust Pip, proceed without doubt.Be sure that your Windows environment variable PATH includes Python's folders (for Python 2.7.x default install: C:\Python27 and C:\Python27\Scripts, for Python 3.3x: C:\Python33 and C:\Python33\Scripts, and so on).I've built Windows installers for both distribute and pip here (the goal being to use pip without having to either bootstrap with easy_install or save and run Python scripts):On Windows, simply download and install first distribute, then pip from the above links. The distribute link above does contain stub .exe installers, and these are currently 32-bit only. I haven't tested the effect on 64-bit Windows.The process to redo this for new versions is not difficult, and I've included it here for reference.In order to get the stub .exe files, you need to have a Visual C++ compiler (it is apparently compilable with MinGW as well)The following works for Python 2.7. Save this script and launch it:  

https://raw.github.com/pypa/pip/master/contrib/get-pip.py  

Pip is installed, then add the path to your environment : FinallyAlso you need Microsoft Visual C++ 2008 Express to get the good compiler and avoid these kind of messages when installing packages:If you have a 64-bit version of Windows 7, you may read 64-bit Python installation issues on 64-bit Windows 7 to successfully install the Python executable package (issue with registry entries).To install pip globally on Python 2.x, easy_install appears to be the best solution as Adrián states.However the installation instructions for pip recommend using virtualenv since every virtualenv has pip installed in it automatically.  This does not require root access or modify your system Python installation.Installing virtualenv still requires easy_install though.2018 update: Python 3.3+ now includes the venv module for easily creating virtual environments like so:python3 -m venv /path/to/new/virtual/environmentSee documentation for different platform methods of activating the environment after creation, but typically one of:For latest Python Download - I have python 3.6 on windows. You don't have to wonder everything you need is there , take  a breath i will show you how to do it.So, PIP is found under the folder in above screen "SCRIPTS"

Lets add Python and PIP in environment variable path. 

Almost Done , Let test with CMD to install goole package using pip.BYE BYE! To use pip, it is not mandatory that you need to install pip in the system directly. You can use it through virtualenv. What you can do is follow these steps:We normally need to install Python packages for one particular project. So, now create a project folder, let’s say myproject.Now create a virtual environment, let’s say myvirtualenv as follows, inside the myproject folder:It will show you:Now your virtual environment, myvirtualenv, is created inside your project folder. You might notice, pip is now installed inside you virtual environment. All you need to do is activate the virtual environment with the following command.You will see the following at the command prompt:Now you can start using pip, but make sure you have activated the virtualenv looking at the left of your prompt.This is one of the easiest way to install pip i.e. inside virtual environment, but you need to have virtualenv.py file with you.For more ways to install pip/virtualenv/virtualenvwrapper, you can refer to thegauraw.tumblr.com.I just wanted to add one more solution for those having issues installing setuptools from Windows 64-bit. The issue is discussed in this bug on python.org and is still unresolved as of the date of this comment. A simple workaround is mentioned and it works flawlessly. One registry change did the trick for me.Link: http://bugs.python.org/issue6792#Solution that worked for me...:Add this registry setting for 2.6+ versions of Python:This is most likely the registry setting you will already have for Python 2.6+:Clearly, you will need to replace the 2.6 version with whatever version of Python you are running.Updated at 2016 : Pip should already be included in Python 2.7.9+ or 3.4+, but if for whatever reason it is not there, you can use the following one-liner.PS:Can't believe there are so many lengthy (perhaps outdated?) answers out there. Feeling thankful to them but, please up-vote this short answer to help more new comers!The best way I found so far, is just two lines of code:It was tested on Windows 8 with PowerShell, Cmd, and Git Bash (MinGW).And you probably want to add the path to your environment. It's somewhere like C:\Python33\Scripts.Here how to install pip with easy way.PythonXY comes with pip included, among others.I use the cross-platform Anaconda package manager from continuum.io on Windows and it is reliable.  It has virtual environment management and a fully featured shell with common utilities (e.g. conda, pip).conda also comes with binaries for libraries with non-Python dependencies, e.g. pandas, numpy, etc.  This proves useful particularly on Windows as it can be  hard to correctly compile C dependencies.I had some issues installing in different ways when I followed instructions here. I think it's very tricky to install in every Windows environment in the same way. In my case I need Python 2.6, 2.7 and 3.3 in the same machine for different purposes so that's why I think there're more problems.

But the following instructions worked perfectly for me, so might be depending on your environment you should try this one:http://docs.python-guide.org/en/latest/starting/install/win/Also, due to the different environments I found incredible useful to use Virtual Environments, I had websites that use different libraries and it's much better to encapsulate them into a single folder, check out the instructions, briefly if PIP is installed you just install VirtualEnv:Into the folder you have all your files runAnd seconds later you have a virtual environment with everything in venv folder, to activate it run venv/Scripts/activate.bat (deactivate the environment is easy, use deactivate.bat). Every library you install will end up in venv\Lib\site-packages and it's easy to move your whole environment somewhere.The only downside I found is some code editors can't recognize this kind of environments, and you will see warnings in your code because imported libraries are not found. Of course there're tricky ways to do it but it would be nice editors keep in mind Virtual Environments are very normal nowadays.Hope it helps.Guide link: http://www.pip-installer.org/en/latest/installing.html#install-pipNote: Make sure scripts path like this (C:\Python27\Scripts) is added int %PATH% environment variable as well.It's very simple:(Make sure your Python and Python script directory (for example, C:\Python27 and C:\Python27\Scripts) are in the PATH.)Working as of Feb 04 2014 :):If you have tried installing pip through the Windows installer file from http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip as suggested by @Colonel Panic, you might have installed the pip package manager successfully, but you might be unable to install any packages with pip. You might also have got the same SSL error as I got when I tried to install Beautiful Soup 4 if you look in the pip.log file:The problem is an issue with an old version of OpenSSL being incompatible with pip 1.3.1 and above versions. The easy workaround for now, is to install pip 1.2.1, which does not require SSL:Installing Pip on Windows:Now try to install any package using pip.For example, to install the requests package using pip, run this from cmd:Whola! requests will be successfully installed and you will get a success message.pip is already installed if you're using Python 2 >=2.7.9 or Python 3 >=3.4 binaries downloaded from python.org, but you'll need to upgrade pip.On Windows upgrade can be done easily Go to Python command line and run below Python commandpython -m pip install -U pipInstalling with get-pip.pyDownload get-pip.py in the same folder or any other folder of your choice. I am assuming you will download it in the same folder from you have python.exe file and run this command Pip's installation guide is pretty clean and simple.Using this you should be able to get started with Pip in under two minutes.if you even have other problems with pip version you can try this Old answer (still valid)Have you tried ?it's probably the easiest to install pip on any system.Simple CMD wayUse CURL to download get-pip.pyExecute downloaded python fileThen add C:\Python37\Scripts path to your environment variable. Assumes that there is a Python37 folder in your C drive, that folder name may varied according to the installed python versionNow you can install python packages by runningJust download setuptools-15.2.zip (md5), from here https://pypi.python.org/pypi/setuptools#windows-simplified , and run ez_setup.py.Alternatively, you can get pip-Win which is an all-in-one installer for pip and virtualenv on Windows and its GUI.Now, it is bundled with Python. You don't need to install it.This is how you can check whether pip is installed or not.

In rare case, if it is not installed, download get-pip.py file and run it with python as

Converting string into datetime

Oli

[Converting string into datetime](https://stackoverflow.com/questions/466345/converting-string-into-datetime)

I've got a huge list of date-times like this as strings:I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. This is going through Django's ORM so I can't use SQL to do the conversion on insert.

2009-01-21 18:00:29Z

I've got a huge list of date-times like this as strings:I'm going to be shoving these back into proper datetime fields in a database so I need to magic them into real datetime objects. This is going through Django's ORM so I can't use SQL to do the conversion on insert.datetime.strptime is the main routine for parsing strings into datetimes. It can handle all sorts of formats, with the format determined by a format string you give it:The resulting datetime object is timezone-naive.Links:Notes:Use the third party dateutil library:It can handle most date formats, including the one you need to parse. It's more convenient than strptime as it can guess the correct format most of the time.It's very useful for writing tests, where readability is more important than performance.You can install it with:Check out strptime in the time module.  It is the inverse of strftime.I have put together a project that can convert some really neat expressions. Check out timestring. Remember this and you didn't need to get confused in datetime conversion again.String to datetime object = strptimedatetime object to other formats = strftimeJun 1 2005  1:33PMis equals to%b %d %Y %I:%M%pso you need strptime i-e converting string to OutputWhat if you have different format of dates you can use panda or dateutil.parseOutPutIn Python >= 3.7.0,to convert YYYY-MM-DD string to datetime object, datetime.fromisoformat could be used.Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.Python 3.2+:Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.TimingsAnd here is how to convert the OP's original date-time examples:There are many options for converting from the strings to Pandas Timestamps using to_datetime, so check the docs if you need anything special.Likewise, Timestamps have many properties and methods that can be accessed in addition to .dateI personally like the solution using the parser module, which is the second Answer to this question and is beautiful, as you don't have to construct any string literals to get it working. BUT, one downside is that it is 90% slower than the accepted answer with strptime.As long as you are not doing this a million times over and over again, I still  think the parser method is more convenient and will handle most of the time formats automatically.Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.Django Timezone aware datetime object example.This conversion is very important for Django and Python when you have USE_TZ = True:Create a small utility function like:This is versatile enough:It would do the helpful for converting string to datetime and also with time zonearrow offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.See http://arrow.readthedocs.io/en/latest/ for more.You can use easy_date to make it easy:If you want only date format then you can manually convert it by passing your individual fields like:You can pass your split string values to convert it into date type like:You will get the resulting value in date format.See my answer.In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from strptime()) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From my solutionit shows "Start Date Time" Column and "Last Login Time" both are "object = strings" in data-frameBy using parse_dates option in read_csv mention you can convert your string datetime into pandas datetime format.You can also check out dateparserInstall:This is, I think, the easiest way you can parse dates.Sample Code:Output:

Is there a way to run Python on Android?

e-satis

[Is there a way to run Python on Android?](https://stackoverflow.com/questions/101754/is-there-a-way-to-run-python-on-android)

We are working on an S60 version and this platform has a nice Python API..However, there is nothing official about Python on Android, but since Jython exists, is there a way to let the snake and the robot work together??

2008-09-19 13:21:12Z

We are working on an S60 version and this platform has a nice Python API..However, there is nothing official about Python on Android, but since Jython exists, is there a way to let the snake and the robot work together??One way is to use Kivy:Kivy Showcase app  There is also the new Android Scripting Environment (ASE/SL4A) project. It looks awesome, and it has some integration with native Android components. Note: no longer under "active development", but some forks may be.Yes! : Android Scripting EnvironmentAn example via Matt Cutts via SL4A -- "here’s a barcode scanner written in six lines of Python code:Pygame is a 2D game engine for Python (on desktop) that is popular with new programmers. The Pygame Subset for Android describes itself as...The examples include a complete game packaged as an APK, which is pretty interesting.My blog has instructions and a patch for cross compiling Python 2.7.2 for Android.I've also open sourced Ignifuga, my 2D Game Engine. It's Python/SDL based, and it cross compiles for Android. Even if you don't use it for games, you might get useful ideas from the code or builder utility (named Schafer, after Tim... you know who).SL4A does what you want. You can easily install it directly onto your device from their site, and do not need root.It supports a range of languages. Python is the most mature. By default, it uses Python 2.6, but there is a 3.2 port you can use instead. I have used that port for all kinds of things on a Galaxy S2 and it worked fine.SL4A provides a port of their android library for each supported language. The library provides an interface to the underlying Android API through a single Android object.Each language has pretty much the same API. You can even use the JavaScript API inside webviews.For user interfaces, you have three options:You can mix options, so you can have a webview for the main interface, and still use native dialogues.There is a third party project named QPython. It builds on SL4A, and throws in some other useful stuff.QPython gives you a nicer UI to manage your installation, and includes a little, touchscreen code editor, a Python shell, and a PIP shell for package management. They also have a Python 3 port. Both versions are available from the Play Store, free of charge. QPython also bundles libraries from a bunch of Python on Android projects, including Kivy, so it is not just SL4A.Note that QPython still develop their fork of SL4A (though, not much to be honest). The main SL4A project itself is pretty much dead.As a Python lover and Android programmer, I'm sad to say this is not a good way to go. There are two problems:One problem is that there is a lot more than just a programming language to the Android development tools. A lot of the Android graphics involve XML files to configure the display, similar to HTML. The built-in java objects are integrated with this XML layout, and it's a lot easier than writing your code to go from logic to bitmap.The other problem is that the G1 (and probably other Android devices for the near future) are not that fast. 200 MHz processors and RAM is very limited. Even in Java, you have to do a decent amount of rewriting-to-avoid-more-object-creation if you want to make your app perfectly smooth. Python is going to be too slow for a while still on mobile devices.I wanted to add to what @JohnMudd has written about Kivy. It has been years since the situation he described, and Kivy has evolved substantially.The biggest selling point of Kivy, in my opinion, is its cross-platform compatibility. You can code and test everything using any desktop environment (Windows/*nix etc.), then package your app for a range of different platforms, including Android, iOS, MacOS and Windows (though apps often lack the native look and feel).With Kivy's own KV language, you can code and build the GUI interface easily (it's just like Java XML, but rather than TextView etc., KV has its own ui.widgets for a similar translation), which is in my opinion quite easy to adopt.Currently Buildozer and python-for-android are the most recommended tools to build and package your apps. I have tried them both and can firmly say that they make building Android apps with Python a breeze. Their guides are well documented too.iOS is another big selling point of Kivy. You can use the same code base with few changes required via kivy-ios Homebrew tools, although Xcode is required for the build, before running on their devices (AFAIK the iOS Simulator in Xcode currently doesn't work for the x86-architecture build). There are also some dependency issues which must be manually compiled and fiddled around with in Xcode to have a successful build, but they wouldn't be too difficult to resolve and people in Kivy Google Group are really helpful too.With all that being said, users with good Python knowledge should have no problem picking up the basics quickly.If you are using Kivy for more serious projects, you may find existing modules unsatisfactory. There are some workable solutions though. With the (work in progress) pyjnius for Android, and pyobjus, users can now access Java/Objective-C classes to control some of the native APIs.Not at the moment and you would be lucky to get Jython to work soon. If you're planning to start your development now you would be better off with just sticking to Java for now on.Using SL4A (which has already been mentioned by itself in other answers) you can run a full-blown web2py instance (other python web frameworks are likely candidates as well).  SL4A doesn't allow you to do native UI components (buttons, scroll bars, and the like), but it does support WebViews.  A WebView is basically nothing more than a striped down web browser pointed at a fixed address. I believe the native Gmail app uses a WebView instead of going the regular widget route.  This route would have some interesting features:You can use the Termux app, which provides a POSIX environment for Android, to install Python.Note that apt install python will install Python3 on Termux. For Python2, you need to use apt install python2.I use the QPython app. It's free and includes a code editor, an interactive interpreter and a package manager, allowing you to create and execute Python programs directly on your device.From the Python for android site:Yet another attempt: https://code.google.com/p/android-python27/This one embed directly the Python interpretter in your app apk.Chaquopy is a plugin for Android Studio's Gradle-based build system. It focuses on close integration with the standard Android development tools.This is a commercial product, but it's free for open-source use and will always remain that way.(I am the creator of this product.)Here are some tools listed in official python websiteThere is an app called QPython3 in playstore which can be used for both editing and running python script.Playstore linkAnother app called Termux in which you can install python using commandPlaystore LinkIf you want develop apps , there is Python Android Scripting Layer (SL4A) .The Scripting Layer for Android, SL4A, is an open source application that allows programs written in a range of interpreted languages to run on Android. It also provides a high level API that allows these programs to interact with the Android device, making it easy to do stuff like accessing sensor data, sending an SMS, rendering user interfaces and so on.You can also check PySide for Android, which is actually Python bindings for the Qt 4.There's a platform called PyMob where apps can be written purely in Python and the compiler tool-flow (PyMob) converts them in native source codes for various platforms. Also check python-for-androidpython-for-android is an open source build tool to let you package Python code into standalone android APKs. These can be passed around, installed, or uploaded to marketplaces such as the Play Store just like any other Android app. This tool was originally developed for the Kivy cross-platform graphical framework, but now supports multiple bootstraps and can be easily extended to package other types of Python apps for Android.Try Chaquopy

A Python SDK for AndroidAnddd... BeeWareBeeWare allows you to write your app in Python and release it on multiple platforms. No need to rewrite the app in multiple programming languages. It means no issues with build tools, environments, compatibility, etc.You can run your Python code using sl4a. sl4a supports Python, Perl, JRuby, Lua, BeanShell, JavaScript, Tcl, and shell script.You can learn sl4a Python Examples.You can use QPython:It has a Python Console, Editor, as well as Package Management / Installershttp://qpython.com/It's an open source project with both Python 2 and Python 3 implementations. You can download the source and the Android .apk files directly from github.QPython 2: https://github.com/qpython-android/qpython/releasesQPython 3: https://github.com/qpython-android/qpython3/releasesAnother option if you are looking for 3.4.2 or 3.5.1 is this archive on GitHub.  Python3-Android 3.4.2 or Python3-Android 3.5.1It currently supports Python 3.4.2 or 3.5.1 and the 10d version of the NDK.  It can also support 3.3 and 9c, 11c and 12It's nice in that you simply download it, run make and you get the .so or the .aI currently use this to run raw Python on android devices. With a couple modifications to the build files you can also make x86 and armeabi 64 bitDidn't see this posted here, but you can do it with Pyside and Qt now that Qt works on Android thanks to Necessitas.It seems like quite a kludge at the moment but could be a viable route eventually...http://qt-project.org/wiki/PySide_for_Android_guideOne more option seems to be pyqtdeploy which citing the docs is:According to Deploying PyQt5 application to Android via pyqtdeploy and Qt5 it is actively developed, although it is difficult to find examples of working Android apps or tutorial on how to cross-compile all the required libraries to Android. It is an interesting project to keep in mind though!Take a look at BeeWare. At the moment of answering this question it is still in early development. It's aim is to be able to create native apps with Python for all supported operating systems, including Android.Check out enaml-native which takes the react-native concept and applies it to python. It lets users build apps with native Android widgets and provides APIs to use android and java libraries from python. It also integrates with android-studio and shares a few of react's nice dev features like code reloading and remote debugging.

How do I get a substring of a string in Python?

Joan Venge

[How do I get a substring of a string in Python?](https://stackoverflow.com/questions/663171/how-do-i-get-a-substring-of-a-string-in-python)

Is there a way to substring a string in Python, to get a new string from the third character to the end of the string?Maybe like myString[2:end]?If leaving the second part means 'till the end', and if you leave the first part, does it start from the start?

2009-03-19 17:29:41Z

Is there a way to substring a string in Python, to get a new string from the third character to the end of the string?Maybe like myString[2:end]?If leaving the second part means 'till the end', and if you leave the first part, does it start from the start?Python calls this concept "slicing" and it works on more than just strings. Take a look here for a comprehensive introduction.Just for completeness as nobody else has mentioned it.  The third parameter to an array slice is a step.  So reversing a string is as simple as:Or selecting alternate characters would be:The ability to step forwards and backwards through the string maintains consistency with being able to array slice from the start or end.Substr() normally (i.e. PHP and Perl) works this way: So the parameters are beginning and LENGTH.But Python's behaviour is different; it expects beginning and one after END (!). This is difficult to spot by beginners. So the correct replacement for Substr(s, beginning, LENGTH) isA common way to achieve this is by string slicing. MyString[a:b] gives you a substring from index a to (b - 1).One example seems to be missing here: full (shallow) copy.This is a common idiom for creating a copy of sequence types (not of interned strings), [:]. Shallow copies a list, see Python list slice syntax used for no obvious reason.Yes, this actually works if you assign, or bind, the name,end, to constant singleton, None:Slice notation has 3 important arguments:Their defaults when not given are None - but we can pass them explicitly:Yes, for example:Note that we include start in the slice, but we only go up to, and not including, stop.When step is None, by default the slice uses 1 for the step. If you step with a negative integer, Python is smart enough to go from the end to the beginning.I explain slice notation in great detail in my answer to Explain slice notation Question. You've got it right there except for "end". It's called slice notation. Your example should read:If you leave out the second parameter it is implicitly the end of the string.I would like to add two points to the discussion:If myString contains an account number that begins at offset 6 and has length 9, then you can extract the account number this way: acct = myString[6:][:9].If the OP accepts that, they might want to try, in an experimental fashion,It works - no error is raised, and no default 'string padding' occurs.Maybe I missed it, but I couldn't find a complete answer on this page to the original question(s) because variables are not further discussed here. So I had to go on searching.Since I'm not yet allowed to comment, let me add my conclusion here. I'm sure I was not the only one interested in it when accessing this page:  If you leave the first part, you get   And if you left the : in the middle as well you got the simplest substring, which would be the 5th character (count starting with 0, so it's the blank in this case):Using hardcoded indexes itself can be a mess.In order to avoid that, Python offers a built-in object slice().If we want to know how many money I got left.Normal solution:Using slices:Using slice you gain readability.

How to print colored text in terminal in Python?

aboSamoor

[How to print colored text in terminal in Python?](https://stackoverflow.com/questions/287871/how-to-print-colored-text-in-terminal-in-python)

How can I output colored text to the terminal, in Python?

What is the best Unicode symbol to represent a solid block?

2008-11-13 18:58:10Z

How can I output colored text to the terminal, in Python?

What is the best Unicode symbol to represent a solid block?This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some python code from the blender build scripts:To use code like this, you can do something like or, with Python3.6+:This will work on unixes including OS X, linux and windows (provided you use ANSICON, or in Windows 10 provided you enable VT100 emulation). There are ansi codes for setting the color, moving the cursor, and more.If you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the "curses" module, which handles a lot of the complicated parts of this for you. The Python Curses HowTO is a good introduction.If you are not using extended ASCII (i.e. not on a PC), you are stuck with the ascii characters below 127, and '#' or '@' is probably your best bet for a block. If you can ensure your terminal is using a IBM extended ascii character set, you have many more options. Characters 176, 177, 178 and 219 are the "block characters".Some modern text-based programs, such as "Dwarf Fortress", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the Dwarf Fortress Wiki see (user-made tilesets).The Text Mode Demo Contest has more resources for doing graphics in text mode.Hmm.. I think got a little carried away on this answer. I am in the midst of planning an epic text-based adventure game, though. Good luck with your colored text!I'm surprised no one has mentioned the Python termcolor module. Usage is pretty simple:Or in Python 3:It may not be sophisticated enough, however, for game programming and the "colored blocks" that you want to do...The answer is Colorama for all cross-platform coloring in Python.A Python 3.6 example screenshot:

Print a string that starts a color/style, then the string, then end the color/style change with '\x1b[0m':Get a table of format options for shell text with following code:Define a string that starts a color and a string that ends the color, then print your text with the start string at the front and the end string at the end.This produces the following in bash, in urxvt with a Zenburn-style color scheme:Through experimentation, we can get more colors:Note: \33[5m and \33[6m are blinking.This way we can create a full color collection:Here is the code to generate the test:You want to learn about ANSI escape sequences. Here's a brief example:For more info see http://en.wikipedia.org/wiki/ANSI_escape_codeFor a block character, try a unicode character like \u2588:Putting it all together:My favorite way is with the Blessings library (full disclosure: I wrote it). For example:To print colored bricks, the most reliable way is to print spaces with background colors. I use this technique to draw the progress bar in nose-progressive:You can print in specific locations as well:If you have to muck with other terminal capabilities in the course of your game, you can do that as well. You can use Python's standard string formatting to keep it readable:The nice thing about Blessings is that it does its best to work on all sorts of terminals, not just the (overwhelmingly common) ANSI-color ones. It also keeps unreadable escape sequences out of your code while remaining concise to use. Have fun!sty is similar to colorama, but it's less verbose, supports 8bit and 24bit (rgb) colors, allows you to register your own styles, supports muting, is really flexible, well documented and more.Examples:prints:Demo:

generated a class with all the colors using a for loop to iterate every combination of color up to 100, then wrote a class with python colors. Copy and paste as you will, GPLv2 by me:Try this simple code I'm responding because I have found out a way to use ANSI codes on Windows 10, so that you can change the colour of text without any modules that aren't built in:The line that makes this work is os.system('color'), but to make sure you don't raise errors if the person is not on Windows you could use this script:Note: Although this gives the same options as other Windows options, Windows does not full support ANSI codes, even with this trick. Not all the text decoration colours work and all the 'bright' colours (Codes 90-97 and 100-107) display the same as the regular colours (Codes 30-37 and 40-47)tl;dr: Add os.system('color') after the imports.Python Version: 3.6.7On Windows you can use module 'win32console' (available in some Python distributions) or module 'ctypes' (Python 2.5 and up) to access the Win32 API.To see complete code that supports both ways, see the color console reporting code from Testoob.ctypes example:Stupidly simple based on @joeld's answerThen justI have wrapped @joeld answer into a module with global functions that I can use anywhere in my code.file: log.py     use as follows:For Windows you cannot print to console with colors unless you're using the win32api.For Linux it's as simple as using print, with the escape sequences outlined here:ColorsFor the character to print like a box, it really depends on what font you are using for the console window. The pound symbol works well, but it depends on the font:I ended up doing this, I felt it was cleanest: Building on @joeld answer, using https://pypi.python.org/pypi/lazyme  pip install -U lazyme :Screenshot:Some updates to the color_print with new formatters, e.g.:Note: italic, fast blinking and strikethrough may not work on all terminals, doesn't work on Mac / Ubuntu. E.g. Screenshot:note how well the with keyword mixes with modifiers like these that need to be reset (using Python 3 and Colorama):

Try online You could use CLINT:Get it from GitHub.You can use the Python implementation of the curses library:

http://docs.python.org/library/curses.htmlAlso, run this and you'll find your box:If you are programming a game perhaps you would like to change the background color and use only spaces? For example:If you are using Windows, then here you go!Here's a curses example:https://raw.github.com/fabric/fabric/master/fabric/colors.pyasciimatics provides a portable support for building text UI and animations:Asciicast:One easier option would be to use the cprint function from termcolor package.It also supports %s, %d format of printingwhile i find this answer useful, i modified it a bit. this Github Gist is the resultusagein addition you can wrap common usages:If you are using Django snapshot:(I generally use colored output for debugging on runserver terminal so I added it.)

You can test if it is installed in your machine:

 $ python -c "import django; print django.VERSION"

To install it check: How to install Django

Give it a Try!!

How do I sort a list of dictionaries by a value of the dictionary?

Mario F

[How do I sort a list of dictionaries by a value of the dictionary?](https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary)

I have a list of dictionaries and want each item to be sorted by a specific property values.Take into consideration the array below,When sorted by name, should become

2008-09-16 14:39:44Z

I have a list of dictionaries and want each item to be sorted by a specific property values.Take into consideration the array below,When sorted by name, should becomeIt may look cleaner using a key instead a cmp:or as J.F.Sebastian and others suggested,For completeness (as pointed out in comments by fitzgeraldsteele), add reverse=True to sort descendingTo sort the list of dictionaries by key='name':To sort the list of dictionaries by key='age':my_list will now be what you want.(3 years later) Edited to add:The new key argument is more efficient and neater.  A better answer now looks like:...the lambda is, IMO, easier to understand than operator.itemgetter, but YMMV.If you want to sort the list by multiple keys you can do the following:It is rather hackish, since it relies on converting the values into a single string representation for comparison, but it works as expected for numbers including negative ones (although you will need to format your string appropriately with zero paddings if you are using numbers)'key' is used to sort by an arbitrary value and 'itemgetter' sets that value to each item's 'name' attribute.I guess you've meant:This would be sorted like this:You could use a custom comparison function, or you could pass in a function that calculates a custom sort key. That's usually more efficient as the key is only calculated once per item, while the comparison function would be called many more times.You could do it this way:But the standard library contains a generic routine for getting items of arbitrary objects: itemgetter. So try this instead:Using Schwartzian transform from Perl,dogivesMore on Perl Schwartzian transformYou have to implement your own comparison function that will compare the dictionaries by values of name keys. See Sorting Mini-HOW TO from PythonInfo Wikisometime we need to use lower() for exampleHere is the alternative general solution - it sorts elements of dict by keys and values.

The advantage of it - no need to specify keys, and it would still work if some keys are missing in some of dictionaries.Using the pandas package is another method, though it's runtime at large scale is much slower than the more traditional methods proposed by others:Here are some benchmark values for a tiny list and a large (100k+) list of dicts:If you do not need the original list of dictionaries, you could modify it in-place with sort() method using a custom key function.Key function:The list to be sorted:Sorting it in-place:If you need the original list, call the sorted() function passing it the list and the key function, then assign the returned sorted list to a new variable:Printing data_one and new_data.Let's say I have a dictionary D with elements below. To sort just use key argument in sorted to pass custom function as below :Check this out.I have been a big fan of filter w/ lambda however it is not best option if you considering time complexityFirst optionSecond optionFast comparison of exec times If performance is a concern, I would use operator.itemgetter instead of lambda as built-in functions perform faster than hand-crafted functions. The itemgetter function seems to perform approximately 20% faster than lambda based on my testing.From https://wiki.python.org/moin/PythonSpeed:Here is a comparison of sorting speed using lambda vs itemgetter.Both techniques sort the list in the same order (verified by execution of the final statement in the code block) but one is a little faster.You may use the following code

How to print without newline or space?

Andrea Ambu

[How to print without newline or space?](https://stackoverflow.com/questions/493386/how-to-print-without-newline-or-space)

I'd like to do it in  python. What I'd like to do in this example in c:In C:Output:In Python:In Python print will add a \n or space, how can I avoid that? Now, it's just an example, don't tell me I can first build a string then print it. I'd like to know how to "append" strings to stdout.

2009-01-29 20:58:25Z

I'd like to do it in  python. What I'd like to do in this example in c:In C:Output:In Python:In Python print will add a \n or space, how can I avoid that? Now, it's just an example, don't tell me I can first build a string then print it. I'd like to know how to "append" strings to stdout.In Python 3, you can use the sep= and end= parameters of the print function:To not add a newline to the end of the string:To not add a space between all the function arguments you want to print:You can pass any string to either parameter, and you can use both parameters at the same time.If you are having trouble with buffering, you can flush the output by adding flush=True keyword argument:From Python 2.6 you can either import the print function from Python 3 using the __future__ module:which allows you to use the Python 3 solution above.However, note that the flush keyword is not available in the version of the print function imported from __future__ in Python 2; it only works in Python 3, more specifically 3.3 and later. In earlier versions you'll still need to flush manually with a call to sys.stdout.flush(). You'll also have to rewrite all other print statements in the file where you do this import.Or you can use sys.stdout.write()You may also need to callto ensure stdout is flushed immediately.It should be as simple as described at this link by Guido Van Rossum:Re: How does one print without a c/r ?http://legacy.python.org/search/hypermail/python-1992/0115.htmlYes, append a comma after the last argument to print. For instance,

this loop prints the numbers 0..9 on a line separated by spaces. Note

the parameterless "print" that adds the final newline:Note: The title of this question used to be something like "How to printf in python?"Since people may come here looking for it based on the title, Python also supports printf-style substitution:And, you can handily multiply string values:Use the python3-style print function for python2.6+   (will also break any existing keyworded print statements in the same file.)To not ruin all your python2 print keywords, create a separate printf.py fileThen, use it in your fileMore examples showing printf styleHow to print on the same line:The new (as of Python 3.0) print function has an optional end parameter that lets you modify the ending character. There's also sep for separator.Using functools.partial to create a new function called printfEasy way to wrap a function with default parameters.You can just add , in the end of print function so it won't print on new line.In Python 3+, print is a function. When you callPython translates it toYou can change end to whatever you want.You can try:python 2.6+:python 3:python <= 2.5:if extra space is OK after each print, in python 2misleading in python 2 - avoid:You can do the same in python3 as follows :and execute it with python filename.py or python3 filename.pyi recently had the same problem..i solved it by doing:this works on both unix and windows ... have not tested it

on macosx ...hth@lenooh satisfied my query. I discovered this article while searching for 'python suppress newline'. I'm using IDLE3 on Raspberry Pi to develop Python 3.2 for PuTTY. I wanted to create a progress bar on the PuTTY command line. I didn't want the page scrolling away. I wanted a horizontal line to re-assure the user from freaking out that the program hasn't cruncxed to a halt nor been sent to lunch on a merry infinite loop - as a plea to 'leave me be, I'm doing fine, but this may take some time.' interactive message - like a progress bar in text.The print('Skimming for', search_string, '\b! .001', end='') initializes the message by preparing for the next screen-write, which will print three backspaces as ⌫⌫⌫ rubout and then a period, wiping off '001' and extending the line of periods. After search_string parrots user input, the \b! trims the exclamation point of my search_string text to back over the space which print() otherwise forces, properly placing the punctuation. That's followed by a space and the first 'dot' of the 'progress bar' which I'm simulating. Unnecessarily, the message is also then primed with the page number (formatted to a length of three with leading zeros) to take notice from the user that progress is being processed and which will also reflect the count of periods we will later build out to the right.The progress bar meat is in the sys.stdout.write('\b\b\b.'+format(page, '03')) line. First, to erase to the left, it backs up the cursor over the three numeric characters with the '\b\b\b' as ⌫⌫⌫ rubout and drops a new period to add to the progress bar length. Then it writes three digits of the page it has progressed to so far. Because sys.stdout.write() waits for a full buffer or the output channel to close, the sys.stdout.flush() forces the immediate write. sys.stdout.flush() is built into the end of print() which is bypassed with print(txt, end='' ). Then the code loops through its mundane time intensive operations while it prints nothing more until it returns here to wipe three digits back, add a period and write three digits again, incremented.The three digits wiped and rewritten is by no means necessary - it's just a flourish which exemplifies sys.stdout.write() versus print(). You could just as easily prime with a period and forget the three fancy backslash-b ⌫ backspaces (of course not writing formatted page counts as well) by just printing the period bar longer by one each time through - without spaces or newlines using just the sys.stdout.write('.'); sys.stdout.flush() pair.Please note that the Raspberry Pi IDLE3 Python shell does not honor the backspace as ⌫ rubout but instead prints a space, creating an apparent list of fractions instead.—(o=8> wizMany of these answers seem a little complicated. In Python 3.X you simply do this,The default value of end is "\n". We are simply changing it to a space or you can also use end="".You will notice that all the above answers are correct. But I wanted to make a shortcut to always writing the " end='' " parameter in the end.You could define a function likeIt would accept all the number of parameters. Even it will accept all the other parameters like file, flush ,etc and with the same name.you want to print something in for loop right;but you don't want it print in new line every time..

for example:but you want it to print like this:

hi hi hi hi hi hi right????

just add a comma after print "hi"Example:for i in range (0,5):

   print "hi",

OUTPUT:

hi hi hi hi hi

This worked in both 2.7.8 & 2.5.2 (Canopy and OSX terminal, respectively) -- no module imports or time travel required.Or have a function like:Then now:Outputs:There are general two ways to do this:Print without newline in Python 3.xAppend nothing after the print statement and remove '\n' by using end=''  as:Another Example in Loop:Print without newline in Python 2.xAdding a trailing comma says that after print ignore \n.Another Example in Loop:Hope this will help you.

You can visit this link ....you do not need to import any library. Just use the delete character:this removes the newline and the space (^_^)*

How to select rows from a DataFrame based on column values?

szli

[How to select rows from a DataFrame based on column values?](https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values)

How to select rows from a DataFrame based on values in some column in Python Pandas?In SQL, I would use:I tried to look at pandas documentation but did not immediately find the answer.

2013-06-12 17:42:05Z

How to select rows from a DataFrame based on values in some column in Python Pandas?In SQL, I would use:I tried to look at pandas documentation but did not immediately find the answer.To select rows whose column value equals a scalar, some_value, use ==:To select rows whose column value is in an iterable, some_values, use isin:Combine multiple conditions with &: Note the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses is parsed as which results in a Truth value of a Series is ambiguous error.To select rows whose column value does not equal some_value, use !=:isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:For example,yieldsIf you have multiple values you want to include, put them in a

list (or more generally, any iterable) and use isin:yieldsNote, however, that if you wish to do this many times, it is more efficient to

make an index first, and then use df.loc:yieldsor, to include multiple values from the index use df.index.isin:yieldsThe pandas equivalent to isMultiple conditions:orIn the above code it is the line df[df.foo == 222] that gives the rows based on the column value, 222 in this case.Multiple conditions are also possible:But at that point I would recommend using the query function, since it's less verbose and yields the same result:There are several ways to select rows from a pandas data frame:Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column 'A' == 'foo'(Note on performance: For each base type, we can keep things simple by using the pandas API or we can venture outside the API, usually into numpy, and speed things up.)Setup

The first thing we'll need is to identify a condition that will act as our criterion for selecting rows.  We'll start with the OP's case column_name == some_value, and include some other common use cases.Borrowing from @unutbu:... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.We can then use this mask to slice or index the data frameThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.Positional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.Label indexing can be very handy, but in this case, we are again doing more work for no benefitpd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient.  More so than the standard approach and of similar magnitude as my best suggestion.My preference is to use the Boolean mask Actual improvements can be made by modifying how we create our Boolean mask.mask alternative 1

Use the underlying numpy array and forgo the overhead of creating another pd.Series  I'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the maskEvaluating the mask with the numpy array is ~ 30 times faster.  This is partly due to numpy evaluation often being faster.  It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.Next, we'll look at the timing for slicing with one mask versus the other.The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.mask alternative 2

We could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe—you must take care of the dtypes when doing so!Instead of df[mask] we will do thisIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.However, if the data frame is not of mixed type, this is a very useful way to do it.GivenVersusWe cut the time in half.mask alternative 3

@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.However, as before, we can utilize numpy to improve performance while sacrificing virtually nothing.  We'll use np.in1dTiming

I'll include other concepts mentioned in other posts as well for reference.

Code Below  Each Column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.You'll notice that fastest times seem to be shared between mask_with_values and mask_with_in1dFunctions  Testing  Special Timing

Looking at the special case when we have a single non-object dtype for the entire data frame.

Code Below  Turns out, reconstruction isn't worth it past a few hundred rows.Functions  Testing  I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val')Reproduced from http://pandas.pydata.org/pandas-docs/version/0.17.0/indexing.html#indexing-queryYou can also access variables in the environment by prepending an @.Faster results can be achieved using numpy.where. For example, with unubtu's setup -Timing comparisons:Here is a simple example  August 2019 updated answerSince pandas >= 0.25.0 we can use the query method to filter dataframes with pandas methods and even column names which have spaces. Normally the spaces in column names would give an error, but now we can solve that using a backtick (`) see GitHub:Using .query with method str.endswith:OutputAlso we can use local variables by prefixing it with an @ in our query:OutputFor selecting only specific columns out of multiple columns for a given value in pandas:Options:or To append to this famous question (though a bit too late): You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g.Run this gives:You can also use .apply:It actually works row-wise (i.e., applies the function to each row).The output is The results is the same as using as mentioned by @unutbu

Why is reading lines from stdin much slower in C++ than Python?

Vaughn Cato

[Why is reading lines from stdin much slower in C++ than Python?](https://stackoverflow.com/questions/9371238/why-is-reading-lines-from-stdin-much-slower-in-c-than-python)

I wanted to compare reading lines of string input from stdin using Python and C++ and was shocked to see my C++ code run an order of magnitude slower than the equivalent Python code. Since my C++ is rusty and I'm not yet an expert Pythonista, please tell me if I'm doing something wrong or if I'm misunderstanding something.(TLDR answer: include the statement: cin.sync_with_stdio(false) or just use fgets instead.TLDR results: scroll all the way down to the bottom of my question and look at the table.)C++ code:Python Equivalent:Here are my results:I should note that I tried this both under Mac OS X v10.6.8 (Snow Leopard) and Linux 2.6.32 (Red Hat Linux 6.2). The former is a MacBook Pro, and the latter is a very beefy server, not that this is too pertinent.Tiny benchmark addendum and recapFor completeness, I thought I'd update the read speed for the same file on the same box with the original (synced) C++ code. Again, this is for a 100M line file on a fast disk. Here's the comparison, with several solutions/approaches:

2012-02-21 03:24:19Z

I wanted to compare reading lines of string input from stdin using Python and C++ and was shocked to see my C++ code run an order of magnitude slower than the equivalent Python code. Since my C++ is rusty and I'm not yet an expert Pythonista, please tell me if I'm doing something wrong or if I'm misunderstanding something.(TLDR answer: include the statement: cin.sync_with_stdio(false) or just use fgets instead.TLDR results: scroll all the way down to the bottom of my question and look at the table.)C++ code:Python Equivalent:Here are my results:I should note that I tried this both under Mac OS X v10.6.8 (Snow Leopard) and Linux 2.6.32 (Red Hat Linux 6.2). The former is a MacBook Pro, and the latter is a very beefy server, not that this is too pertinent.Tiny benchmark addendum and recapFor completeness, I thought I'd update the read speed for the same file on the same box with the original (synced) C++ code. Again, this is for a 100M line file on a fast disk. Here's the comparison, with several solutions/approaches:By default, cin is synchronized with stdio, which causes it to avoid any input buffering.  If you add this to the top of your main, you should see much better performance:Normally, when an input stream is buffered, instead of reading one character at a time, the stream will be read in larger chunks.  This reduces the number of system calls, which are typically relatively expensive.  However, since the FILE* based stdio and iostreams often have separate implementations and therefore separate buffers, this could lead to a problem if both were used together.  For example:If more input was read by cin than it actually needed, then the second integer value wouldn't be available for the scanf function, which has its own independent buffer.  This would lead to unexpected results.To avoid this, by default, streams are synchronized with stdio.  One common way to achieve this is to have cin read each character one at a time as needed using stdio functions.  Unfortunately, this introduces a lot of overhead.  For small amounts of input, this isn't a big problem, but when you are reading millions of lines, the performance penalty is significant.Fortunately, the library designers decided that you should also be able to disable this feature to get improved performance if you knew what you were doing, so they provided the sync_with_stdio method.Just out of curiosity I've taken a look at what happens under the hood, and I've used dtruss/strace on each test.C++syscalls sudo dtruss -c ./a.out < inPythonsyscalls sudo dtruss -c ./a.py < inI'm a few years behind here, but:In 'Edit 4/5/6' of the original post, you are using the construction:This is wrong in a couple of different ways:A better construction would be:In this statement it is the shell which opens big_file, passing it to your program (well, actually to `time` which then executes your program as a subprocess) as an already-open file descriptor.  100% of the file reading is strictly the responsibility of the program you're trying to benchmark.  This gets you a real reading of its performance without spurious complications.I will mention two possible, but actually wrong, 'fixes' which could also be considered (but I 'number' them differently as these are not things which were wrong in the original post):A. You could 'fix' this by timing only your program:B. or by timing the entire pipeline:These are wrong for the same reasons as #2: they're still using `cat` unnecessarily.  I mention them for a few reasons:But I say that last thing with some hesitation.  If we examine the last result in 'Edit 5' ---- this claims that `cat` consumed 74% of the CPU during the test; and indeed 1.34/1.83 is approximately 74%.  Perhaps a run of:would have taken only the remaining .49 seconds!  Probably not: `cat` here had to pay for the read() system calls (or equivalent) which transferred the file from 'disk' (actually buffer cache), as well as the pipe writes to deliver them to `wc`.  The correct test would still have had to do those read() calls; only the write-to-pipe and read-from-pipe calls would have been saved, and those should be pretty cheap.Still, I predict you would be able to measure the difference between `cat file | wc -l` and `wc -l < file` and find a noticeable (2-digit percentage) difference.  Each of the slower tests will have paid a similar penalty in absolute time; which would however amount to a smaller fraction of its larger total time.In fact I did some quick tests with a 1.5 gigabyte file of garbage, on a Linux 3.13 (Ubuntu 14.04) system, obtaining these results (these are actually 'best of 3' results; after priming the cache, of course): Notice that the two pipeline results claim to have taken more CPU time (user+sys) than realtime.  This is because I'm using the shell (Bash)'s built-in 'time' command, which is cognizant of the pipeline; and I'm on a multi-core machine where separate processes in a pipeline can use separate cores, accumulating CPU time faster than realtime.  Using /usr/bin/time I see smaller CPU time than realtime -- showing that it can only time the single pipeline element passed to it on its command line.  Also, the shell's output gives milliseconds while /usr/bin/time only gives hundreths of a second.So at the efficiency level of `wc -l`, the `cat` makes a huge difference: 409 / 283 = 1.453 or 45.3% more realtime, and 775 / 280 = 2.768, or a whopping 177% more CPU used!  On my random it-was-there-at-the-time test box.I should add that there is at least one other significant difference between these styles of testing, and I can't say whether it is a benefit or fault; you have to decide this yourself:When you run `cat big_file | /usr/bin/time my_program`, your program is receiving input from a pipe, at precisely the pace sent by `cat`, and in chunks no larger than written by `cat`.When you run `/usr/bin/time my_program < big_file`, your program receives an open file descriptor to the actual file.  Your program -- or in many cases the I/O libraries of the language in which it was written -- may take different actions when presented with a file descriptor referencing a regular file.  It may use mmap(2) to map the input file into its address space, instead of using explicit read(2) system calls.  These differences could have a far larger effect on your benchmark results than the small cost of running the `cat` binary.Of course it is an interesting benchmark result if the same program performs significantly differently between the two cases.  It shows that, indeed, the program or its I/O libraries are doing something interesting, like using mmap().  So in practice it might be good to run the benchmarks both ways; perhaps discounting the `cat` result by some small factor to "forgive" the cost of running `cat` itself.I reproduced the original result on my computer using g++ on a Mac.Adding the following statements to the C++ version just before the while loop brings it inline with the Python version:sync_with_stdio improved speed to 2 seconds, and setting a larger buffer brought it down to 1 second.getline, stream operators, scanf, can be convenient if you don't care about file loading time or if you are loading small text files. But, if the performance is something you care about, you should really just buffer the entire file into memory (assuming it will fit).Here's an example:If you want, you can wrap a stream around that buffer for more convenient access like this:Also, if you are in control of the file, consider using a flat binary data format instead of text. It's more reliable to read and write because you don't have to deal with all the ambiguities of whitespace. It's also smaller and much faster to parse.The following code was faster for me than the other code posted here so far:

(Visual Studio 2013, 64-bit, 500 MB file with line length uniformly in [0, 1000)).It beats all my Python attempts by more than a factor 2.By the way, the reason the line count for the C++ version is one greater than the count for the Python version is that the eof flag only gets set when an attempt is made to read beyond eof. So the correct loop would be:In your second example (with scanf()) reason why this is still slower might be because scanf("%s") parses string and looks for any space char (space, tab, newline).Also, yes, CPython does some caching to avoid harddisk reads.A first element of an answer: <iostream> is slow. Damn slow. I get a huge performance boost with scanf as in the below, but it is still two times slower than Python.Well, I see that in your second solution you switched from cin to scanf, which was the first suggestion I was going to make you (cin is sloooooooooooow). Now, if you switch from scanf to fgets, you would see another boost in performance: fgets is the fastest C++ function for string input.BTW, didn't know about that sync thing, nice. But you should still try fgets.

How to iterate over rows in a DataFrame in Pandas?

Roman

[How to iterate over rows in a DataFrame in Pandas?](https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas)

I have a DataFrame from pandas:Output:Now I want to iterate over the rows of this frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. For example:Is it possible to do that in pandas?I found this similar question. But it does not give me the answer I need. For example, it is suggested there to use:orBut I do not understand what the row object is and how I can work with it.

2013-05-10 07:04:49Z

I have a DataFrame from pandas:Output:Now I want to iterate over the rows of this frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. For example:Is it possible to do that in pandas?I found this similar question. But it does not give me the answer I need. For example, it is suggested there to use:orBut I do not understand what the row object is and how I can work with it.DataFrame.iterrows is a generator which yield both index and rowFirst consider if you really need to iterate over rows in a DataFrame. See this answer for alternatives.If you still need to iterate over rows, you can use methods below. Note some  important caveats which are not mentioned in any of the other answers.itertuples() is supposed to be faster than iterrows()But be aware, according to the docs (pandas 0.24.2 at the moment):See pandas docs on iteration for more details.Iteration in pandas is an anti-pattern, and is something you should only do when you have exhausted every other option. You should not use any function with "iter" in its name for more than a few thousand rows or you will have to get used to a lot of waiting.Do you want to print a DataFrame? Use DataFrame.to_string().Do you want to compute something? In that case, search for methods in this order (list modified from here):iterrows and itertuples (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for. Appeal to Authority

The docs page on iteration has a huge red warning box that says:A good number of basic operations and computations are "vectorised" by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on Essential Basic Functionality to find a suitable vectorised method for your problem.If none exists, feel free to write your own using custom cython extensions.List comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you're trying to perform elementwise transformation on your code. There is a good amount of evidence to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common pandas tasks.The formula is simple,If you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw  python.Let's demonstrate the difference with a simple example of adding two pandas columns A + B. This is a vectorizable operaton, so it will be easy to contrast the performance of the methods discussed above.Benchmarking code, for your reference.I should mention, however, that it isn't always this cut and dry. Sometimes the answer to "what is the best method for an operation" is "it depends on your data". My advice is to test out different approaches on your data before settling on one.* Pandas string methods are "vectorized" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.A common trend I notice from new users is to ask questions of the form "how can I iterate over my df to do X?". Showing code that calls iterrows() while doing something inside a for loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is not the right thing to do.The aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs vectorization, but I want new users to be informed when developing solutions to their problems with this library.You should use df.iterrows(). Though iterating row-by-row is not especially efficient since Series objects have to be created.While iterrows() is a good option, sometimes itertuples() can be much faster:You can also use df.apply() to iterate over rows and access multiple columns for a function.docs: DataFrame.apply()You can use the df.iloc function as follows:I was looking for How to iterate on rows AND columns and ended here so :You can write your own iterator that implements namedtupleThis is directly comparable to pd.DataFrame.itertuples.  I'm aiming at performing the same task with more efficiency.For the given dataframe with my function:Or with pd.DataFrame.itertuples:A comprehensive test

We test making all columns available and subsetting the columns.  If you really have to iterate a pandas dataframe, you will probably want to avoid using iterrows(). There are different methods and the usual iterrows() is far from being the best. itertuples() can be 100 times faster.In short: Generate a random dataframe with a million rows and 4 columns:1) The usual iterrows() is convenient but damn slow:2) The default itertuples() is already much faster but it doesn't work with column names such as My Col-Name is very Strange (you should avoid this method if your columns are repeated or if a column name cannot be simply converted to a python variable name).:3) The default itertuples() using name=None is even faster but not really convenient as you have to define a variable per column.4) Finally, the named itertuples() is slower than the previous point but you do not have to define a variable per column and it works with column names such as My Col-Name is very Strange.Output:This article is a very interesting comparison between iterrows and itertuplesTo loop all rows in a dataframe you can use:To loop all rows in a dataframe and use values of each row conveniently, namedtuples can be converted to ndarrays. For example:Iterating over the rows:results in:Please note that if index=True, the index is added as the first element of the tuple, which may be undesirable for some applications.Sometimes a useful pattern is:Which results in:There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:Note the usage of double brackets. This returns a DataFrame with a single row.For both viewing and modifying values, I would use iterrows(). In a for loop and by using tuple unpacking (see the example: i, row), I use the row for only viewing the value and use i with the loc method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.Here the row in the loop is a copy of that row, and not a view of it. Therefore, you should NOT write something like row['A'] = 'New_Value', it will not modify the DataFrame. However, you can use i and loc and specify the DataFrame to do the work.You can also do numpy indexing for even greater speed ups. It's not really iterating but works much better than iteration for certain applications.You may also want to cast it to an array. These indexes/selections are supposed to act like Numpy arrays already but I ran into issues and needed to castThere are so many ways to iterate over the rows in pandas dataframe. One very simple and intuitive way is :This example uses iloc to isolate each digit in the data frame. Some libraries (e.g. a Java interop library that I use) require values to be passed in a row at a time, for example, if streaming data. To replicate the streaming nature, I 'stream' my dataframe values one by one, I wrote the below, which comes in handy from time to time.Which can be used:And preserves the values/ name mapping for the rows being iterated. Obviously, is a lot slower than using apply and Cython as indicated above, but is necessary in some circumstances.

How to know if an object has an attribute in Python

Lucas Gabriel Sánchez

[How to know if an object has an attribute in Python](https://stackoverflow.com/questions/610883/how-to-know-if-an-object-has-an-attribute-in-python)

Is there a way in Python to determine if an object has some attribute?  For example:How can you tell if a has the attribute property before using it?

2009-03-04 14:45:59Z

Is there a way in Python to determine if an object has some attribute?  For example:How can you tell if a has the attribute property before using it?Try hasattr():EDIT: See zweiterlinde's answer below, who offers good advice about asking forgiveness! A very pythonic approach! The general practice in python is that, if the property is likely to be there most of the time, simply call it and either let the exception propagate, or trap it with a try/except block. This will likely be faster than hasattr. If the property is likely to not be there most of the time, or you're not sure, using hasattr will probably be faster than repeatedly falling into an exception block.As Jarret Hardie answered, hasattr will do the trick.  I would like to add, though, that many in the Python community recommend a strategy of "easier to ask for forgiveness than permission" (EAFP) rather than "look before you leap" (LBYL).  See these references:EAFP vs LBYL (was Re: A little disappointed so far)

EAFP vs. LBYL @Code Like a Pythonista: Idiomatic Pythonie:... is preferred to:You can use hasattr() or catch AttributeError, but if you really just want the value of the attribute with a default if it isn't there, the best option is just to use getattr():I think what you are looking for is hasattr. However, I'd recommend something like this if you want to detect python properties-The disadvantage here is that attribute errors in the properties __get__ code are also caught.Otherwise, do-Docs:http://docs.python.org/library/functions.html

Warning:

The reason for my recommendation is that hasattr doesn't detect properties.

Link:http://mail.python.org/pipermail/python-dev/2005-December/058498.htmlAccording to pydoc, hasattr(obj, prop) simply calls getattr(obj, prop) and catches exceptions. So, it is just as valid to wrap the attribute access with a try statement and catch AttributeError as it is to use hasattr() beforehand.I would like to suggest avoid this:The user @jpalecek mentioned it: If an AttributeError occurs inside doStuff(), you are lost.Maybe this approach is better:Depending on the situation you can check with isinstance what kind of object you have, and then use the corresponding attributes. With the introduction of abstract base classes in Python 2.6/3.0 this approach has also become much more powerful (basically ABCs allow for a more sophisticated way of duck typing).One situation were this is useful would be if two different objects have an attribute with the same name, but with different meaning. Using only hasattr might then lead to strange errors.One nice example is the distinction between iterators and iterables (see this question). The __iter__ methods in an iterator and an iterable have the same name but are semantically quite different! So hasattr is useless, but isinstance together with ABC's provides a clean solution.However, I agree that in most situations the hasattr approach (described in other answers) is the most appropriate solution.Hope you expecting hasattr(), but try to avoid hasattr() and please prefer getattr(). getattr() is faster than hasattr()using hasattr():same here i am using getattr to get property if there is no property it return noneEDIT:This approach has serious limitation. It should work if the object is an iterable one. Please check the comments below.If you are using Python 3.6 or higher like me there is a convenient alternative to check whether an object has a particular attribute:However, I'm not sure which is the best approach right now. using hasattr(), using getattr() or using in. Comments are welcome.Here's a very intuitive approach :This is super simple, just use dir(object)

This will return a list of every available function and attribute of the object.You can check whether object contains attribute by using hasattr builtin method.For an instance if your object is a and you want to check for attribute stuffThe method signature itself is hasattr(object, name) -> bool which mean if object has attribute which is passed to second argument in hasattr than it gives boolean True or False according to the presence of name attribute in object.Another possible option, but it depends if what you mean by before:output:This allows you to even check for None-valued attributes.But!  Be very careful you don't accidentally instantiate and compare undefined multiple places because the is will never work in that case.

How do I check if a string is a number (float)?

Daniel Goldberg

[How do I check if a string is a number (float)?](https://stackoverflow.com/questions/354038/how-do-i-check-if-a-string-is-a-number-float)

What is the best possible way to check if a string can be represented as a number in Python? The function I currently have right now is:Which, not only is ugly and slow, seems clunky. However I haven't found a better method because calling float in the main function is even worse. 

2008-12-09 20:03:42Z

What is the best possible way to check if a string can be represented as a number in Python? The function I currently have right now is:Which, not only is ugly and slow, seems clunky. However I haven't found a better method because calling float in the main function is even worse. I'd dispute both.A regex or other string parsing method would be uglier and slower.  I'm not sure that anything much could be faster than the above.  It calls the function and returns.  Try/Catch doesn't introduce much overhead because the most common exception is caught without an extensive search of stack frames.The issue is that any numeric conversion function has two kinds of resultsC (as an example) hacks around this a number of ways.  Python lays it out clearly and explicitly.I think your code for doing this is perfect.In case you are looking for parsing (positive, unsigned) integers instead of floats, you can use the isdigit() function for string objects.String Methods - isdigit(): Python2, Python3There's also something on Unicode strings, which I'm not too familiar with

Unicode - Is decimal/decimalTL;DR The best solution is s.replace('.','',1).isdigit()I did some benchmarks comparing the different approachesIf the string is not a number, the except-block is quite slow. But more importantly, the try-except method is the only approach that handles scientific notations correctly.Float notation ".1234" is not supported by:

- is_number_regex  Scientific notation "1.000000e+50" is not supported by:

- is_number_regex

- is_number_repl_isdigit

Scientific notation "1e50" is not supported by:

- is_number_regex

- is_number_repl_isdigit  where the following functions were testedThere is one exception that you may want to take into account: the string 'NaN'If you want is_number to return FALSE for 'NaN' this code will not work as Python converts it to its representation of a number that is not a number (talk about identity issues):Otherwise, I should actually thank you for the piece of code I now use extensively. :)G.how about this:which will return true only if there is one or no '.' in the string of digits.will return falseedit: just saw another comment ...

adding a .replace(badstuff,'',maxnum_badstuff) for other cases can be done. if you are passing salt and not arbitrary condiments (ref:xkcd#974) this will do fine :PIt may take some getting used to, but this is the pythonic way of doing it.  As has been already pointed out, the alternatives are worse.  But there is one other advantage of doing things this way:  polymorphism.The central idea behind duck typing is that "if it walks and talks like a duck, then it's a duck."  What if you decide that you need to subclass string so that you can change how you determine if something can be converted into a float?  Or what if you decide to test some other object entirely?  You can do these things without having to change the above code.Other languages solve these problems by using interfaces.  I'll save the analysis of which solution is better for another thread.  The point, though, is that python is decidedly on the duck typing side of the equation, and you're probably going to have to get used to syntax like this if you plan on doing much programming in Python (but that doesn't mean you have to like it of course).One other thing you might want to take into consideration: Python is pretty fast in throwing and catching exceptions compared to a lot of other languages (30x faster than .Net for instance).  Heck, the language itself even throws exceptions to communicate non-exceptional, normal program conditions (every time you use a for loop).  Thus, I wouldn't worry too much about the performance aspects of this code until you notice a significant problem.Updated after Alfe pointed out you don't need to check for float separately as complex handles both:Previously said: Is some rare cases you might also need to check for complex numbers (e.g. 1+2i), which can not be represented by a float:For int use this:But for float we need some tricks ;-). Every float number has one point...Also for negative numbers just add lstrip():And now we get a universal way:In C# there are two different functions that handle parsing of scalar values:float.parse():Note: If you're wondering why I changed the exception to a TypeError, here's the documentation.float.try_parse():Note: You don't want to return the boolean 'False' because that's still a value type. None is better because it indicates failure. Of course, if you want something different you can change the fail parameter to whatever you want.To extend float to include the 'parse()' and 'try_parse()' you'll need to monkeypatch the 'float' class to add these methods.If you want respect pre-existing functions the code should be something like:SideNote: I personally prefer to call it Monkey Punching because it feels like I'm abusing the language when I do this but YMMV.Usage:And the great Sage Pythonas said to the Holy See Sharpisus, "Anything you can do I can do better; I can do anything better than you."For strings of non-numbers, try: except: is actually slower than regular expressions.  For strings of valid numbers, regex is slower.  So, the appropriate method depends on your input. If you find that you are in a performance bind, you can use a new third-party module called fastnumbers that provides a function called isfloat.  Full disclosure, I am the author.  I have included its results in the timings below.As you can seeI know this is particularly old but I would add an answer I believe covers the information missing from the highest voted answer that could be very valuable to any who find this:For each of the following methods connect them with a count if you need any input to be accepted. (Assuming we are using vocal definitions of integers rather than 0-255, etc.)x.isdigit()

works well for checking if x is an integer.x.replace('-','').isdigit()

works well for checking if x is a negative.(Check - in first position)x.replace('.','').isdigit()

works well for checking if x is a decimal.x.replace(':','').isdigit()

works well for checking if x is a ratio.x.replace('/','',1).isdigit()

works well for checking if x is a fraction.This answer provides step by step guide having function with examples to find the string is:You may use str.isdigit() to check whether given string is positive integer. Sample Results:str.isdigit() returns False if the string is a negative number or a float number. For example:If you want to also check for the negative integers and float, then you may write a custom function to check for it as:Sample Run:The above functions will return True for the "NAN" (Not a number) string because for Python it is valid float representing it is not a number. For example:In order to check whether the number is "NaN", you may use math.isnan() as:Or if you don't want to import additional library to check this, then you may simply check it via comparing it with itself using ==. Python returns False when nan float is compared with itself. For example:Hence, above function is_number can be updated to return False for "NaN" as:Sample Run:PS: Each operation for each check depending on the type of number comes with additional overhead. Choose the version of is_number function which fits your requirement.Casting to float and catching ValueError is probably the fastest way, since float() is specifically meant for just that. Anything else that requires string parsing (regex, etc) will likely be slower due to the fact that it's not tuned for this operation. My $0.02.You can use Unicode strings, they have a method to do just what you want:Or:http://www.tutorialspoint.com/python/string_isnumeric.htmhttp://docs.python.org/2/howto/unicode.htmlI wanted to see which method is fastest. Overall the best and most consistent results were given by the check_replace function. The fastest results were given by the check_exception function, but only if there was no exception fired - meaning its code is the most efficient, but the overhead of throwing an exception is quite large.Please note that checking for a successful cast is the only method which is accurate, for example, this works with check_exception but the other two test functions will return False for a valid float:Here is the benchmark code:Here are the results with Python 2.7.10 on a 2017 MacBook Pro 13:Here are the results with Python 3.6.5 on a 2017 MacBook Pro 13:Here are the results with PyPy 2.7.13 on a 2017 MacBook Pro 13:So to put it all together, checking for Nan, infinity and complex numbers (it would seem they are specified with j, not i, i.e. 1+2j) it results in:The input may be as follows:a="50"

b=50

c=50.1

d="50.1"The input of this function can be everything!Finds whether the given variable is numeric. Numeric strings consist of optional sign, any number of digits, optional decimal part and optional exponential part. Thus +0123.45e6 is a valid numeric value. Hexadecimal (e.g. 0xf4c3b00c) and binary (e.g. 0b10100111001) notation is not allowed.is_numeric functiontest:is_float functionFinds whether the given variable is float. float strings consist of optional sign, any number of digits, ...test:what is ast?use str.isdigit() methoddetect int value:detect float:I did some speed test. Lets say that if the string is likely to be a number the try/except strategy is the fastest possible.If the string is not likely to be a number and you are interested in Integer check, it worths to do some test (isdigit plus heading '-'). 

If you are interested to check float number, you have to use the try/except code whitout escape.I needed to determine if a string cast into basic types (float,int,str,bool). After not finding anything on the internet I created this:ExampleYou can capture the type and use it RyanN suggestsBut this doesn't quite work, because for sufficiently large floats, x-1 == x returns true. For example, 2.0**54 - 1 == 2.0**54I think your solution is fine, but there is a correct regexp implementation.There does seem to be a lot of regexp hate towards these answers which I think is unjustified, regexps can be reasonably clean and correct and fast.  It really depends on what you're trying to do.  The original question was how can you "check if a string can be represented as a number (float)" (as per your title).  Presumably you would want to use the numeric/float value once you've checked that it's valid, in which case your try/except makes a lot of sense.  But if, for some reason, you just want to validate that a string is a number then a regex also works fine, but it's hard to get correct.  I think most of the regex answers so far, for example, do not properly parse strings without an integer part (such as ".7") which is a float as far as python is concerned.  And that's slightly tricky to check for in a single regex where the fractional portion is not required.  I've included two regex to show this.It does raise the interesting question as to what a "number" is.  Do you include "inf" which is valid as a float in python?  Or do you include numbers that are "numbers" but maybe can't be represented in python (such as numbers that are larger than the float max).There's also ambiguities in how you parse numbers.  For example, what about "--20"?  Is this a "number"?  Is this a legal way to represent "20"?  Python will let you do "var = --20" and set it to 20 (though really this is because it treats it as an expression), but float("--20") does not work.Anyways, without more info, here's a regex that I believe covers all the ints and floats as python parses them.Some example test values:Running the benchmarking code in @ron-reiter's answer shows that this regex is actually faster than the normal regex and is much faster at handling bad values than the exception, which makes some sense.  Results:I also used the function you mentioned, but soon I notice that strings as "Nan", "Inf" and it's variation are considered as number. So I propose you improved version of your function, that will return false on those type of input and will not fail "1e3" variants:This code handles the exponents, floats, and integers, wihtout using regex.User helper function:then Here's my simple way of doing it. Let's say that I'm looping through some strings and I want to add them to an array if they turn out to be numbers.Replace the myvar.apppend with whatever operation you want to do with the string if it turns out to be a number. The idea is to try to use a float() operation and use the returned error to determine whether or not the string is a number.You can generalize the exception technique in a useful way by returning more useful values than True and False.  For example this function puts quotes round strings but leaves numbers alone.  Which is just what I needed for a quick and dirty filter to make some variable definitions for R. I was working on a problem that led me to this thread, namely how to convert a collection of data to strings and numbers in the most intuitive way.  I realized after reading the original code that what I needed was different in two ways:1 - I wanted an integer result if the string represented an integer2 - I wanted a number or a string result to stick into a data structureso I adapted the original code to produce this derivative:Try this.use following it handles all cases:-

Meaning of @classmethod and @staticmethod for beginner? [duplicate]

Rostyslav Dzinko

[Meaning of @classmethod and @staticmethod for beginner? [duplicate]](https://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner)

Could someone explain to me the meaning of @classmethod and @staticmethod in python? I need to know the difference and the meaning. As far as I understand, @classmethod tells a class that it's a method which should be inherited into subclasses, or... something. However, what's the point of that? Why not just define the class method without adding @classmethod or @staticmethod or any @ definitions?tl;dr: when should I use them, why should I use them, and how should I use them?I'm pretty advanced with C++, so using more advanced programming concepts shouldn't be a problem. Feel free giving me a corresponding C++ example if possible.

2012-08-29 13:37:33Z

Could someone explain to me the meaning of @classmethod and @staticmethod in python? I need to know the difference and the meaning. As far as I understand, @classmethod tells a class that it's a method which should be inherited into subclasses, or... something. However, what's the point of that? Why not just define the class method without adding @classmethod or @staticmethod or any @ definitions?tl;dr: when should I use them, why should I use them, and how should I use them?I'm pretty advanced with C++, so using more advanced programming concepts shouldn't be a problem. Feel free giving me a corresponding C++ example if possible.Though classmethod and staticmethod are quite similar, there's a slight difference in usage for both entities: classmethod must have a reference to a class object as the first parameter, whereas staticmethod can have no parameters at all.Let's assume an example of a class, dealing with date information (this will be our boilerplate):This class obviously could be used to store information about certain dates (without timezone information; let's assume all dates are presented in UTC).Here we have __init__, a typical initializer of Python class instances, which receives arguments as a typical instancemethod, having the first non-optional argument (self) that holds a reference to a newly created instance.Class MethodWe have some tasks that can be nicely done using classmethods.Let's assume that we want to create a lot of Date class instances having date information coming from an outer source encoded as a string with format 'dd-mm-yyyy'. Suppose we have to do this in different places in the source code of our project.So what we must do here is:This will look like:For this purpose, C++ can implement such a feature with overloading, but Python lacks this overloading. Instead, we can use classmethod. Let's create another "constructor".Let's look more carefully at the above implementation, and review what advantages we have here:Static methodWhat about staticmethod? It's pretty similar to classmethod but doesn't take any obligatory parameters (like a class method or instance method does).Let's look at the next use case.We have a date string that we want to validate somehow. This task is also logically bound to the Date class we've used so far, but doesn't require instantiation of it.Here is where staticmethod can be useful. Let's look at the next piece of code:So, as we can see from usage of staticmethod, we don't have any access to what the class is---it's basically just a function,  called syntactically like a method, but without access to the object and its internals (fields and another methods), while classmethod does.Rostyslav Dzinko's answer is very appropriate. I thought I could highlight one other reason you should choose @classmethod over @staticmethod when you are creating an additional constructor.In the example above, Rostyslav used the @classmethod from_string as a Factory to create Date objects from otherwise unacceptable parameters. The same can be done with @staticmethod as is shown in the code below:Thus both new_year and millenium_new_year are instances of the Date class.But, if you observe closely, the Factory process is hard-coded to create Date objects no matter what. What this means is that even if the Date class is subclassed, the subclasses will still create plain Date objects (without any properties of the subclass). See that in the example below:datetime2 is not an instance of DateTime? WTF? Well, that's because of the @staticmethod decorator used.In most cases, this is undesired. If what you want is a Factory method that is aware of the class that called it, then @classmethod is what you need.Rewriting Date.millenium as (that's the only part of the above code that changes):ensures that the class is not hard-coded but rather learnt. cls can be any subclass. The resulting object will rightly be an instance of cls.

Let's test that out:The reason is, as you know by now, that @classmethod was used instead of @staticmethod@classmethod means: when this method is called, we pass the class as the first argument instead of the instance of that class (as we normally do with methods). This means you can use the class and its properties inside that method rather than a particular instance.@staticmethod means:  when this method is called, we don't pass an instance of the class to it (as we normally do with methods). This means you can put a function inside a class but you can't access the instance of that class (this is useful when your method does not use the instance).@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It’s definition is immutable via inheritance.@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance, can be overridden by subclass. That’s because the first argument for @classmethod function must always be cls (class).here is good link to this topic.You don't need either decorator. But on the principle that you should minimize the number of arguments to functions (see Clean Coder), they are useful for doing just that.For both instance methods and class methods, not accepting at least one argument is a TypeError, but not understanding the semantics of that argument is a user error.(Define some_function's, e.g.:and this will work.)A dotted lookup on an instance is performed in this order - we look for:Note, a dotted lookup on an instance is invoked like this:and methods are callable attributes:The argument, self, is implicitly given via the dotted lookup.You must access instance methods from instances of the class.The argument, cls, is implicitly given via dotted lookup.You can access this method via an instance or the class (or subclasses).No arguments are implicitly given. This method works like any function defined (for example) on a modules' namespace, except it can be looked upEach of these are progressively more restrictive in the information they pass the method versus instance methods.Use them when you don't need the information.This makes your functions and methods easier to reason about and to unittest.Which is easier to reason about?oror The functions with fewer arguments are easier to reason about. They are also easier to unittest.These are akin to instance, class, and static methods. Keeping in mind that when we have an instance, we also have its class, again, ask yourself, which is easier to reason about?:Here are a couple of my favorite builtin examples:The str.maketrans static method was a function in the string module, but it is much more convenient for it to be accessible from the str namespace.The dict.fromkeys class method returns a new dictionary instantiated from an iterable of keys:When subclassed, we see that it gets the class information as a class method, which is very useful:Use static methods when you don't need the class or instance arguments, but the function is related to the use of the object, and it is convenient for the function to be in the object's namespace.Use class methods when you don't need instance information, but need the class information perhaps for its other class or static methods, or perhaps itself as a constructor. (You wouldn't hardcode the class so that subclasses could be used here.)One would use @classmethod when he/she would want to change the behaviour of the method based on which subclass is calling the method. remember we have a reference to the calling class in a class method.While using static you would want the behaviour to remain unchanged across subclasses Example:A little compilation@staticmethod

A way to write a method inside a class without reference to the object it is being called on. So no need to pass implicit argument like self or cls.

It is written exactly the same how written outside the class, but it is not of no use in python because if you need to encapsulate a method inside a class since this method needs to be the part of that class @staticmethod is comes handy in that case.@classmethod

It is important when you want to write a factory method and by this custom attribute(s) can be attached in a class. This attribute(s) can be overridden in the inherited class.A comparison between these two methods can be as belowI'm a beginner on this site, I have read all above answers, and got the information what I want. However, I don't have the right to upvote.  So I want to get my start on StackOverflow with the answer as I understand it.@classmethod@classmethod may be compared with __init__. 

You could think it is another __init__(). It is the way python realize class constructor overloading in c++. notice they both has a reference for class as first argument in definitioin while __init__ use self but construct_from_func use cls conventionally.@staticmethod@staticmethod may be compared with object methodA slightly different way to think about it that might be useful for someone... A class method is used in a superclass to define how that method should behave when it's called by different child classes. A static method is used when we want to return the same thing regardless of the child class that we are calling.In short, @classmethod turns a normal method to a factory method.Let's explore it with an example:Without a @classmethod,you should labor to create instances one by one and they are scattered.As for example with @classmethodTest it:See? Instances are successfully created inside a class definition and they are collected together.In conclusion, @classmethod decorator convert a conventional method to a factory method,Using classmethods makes it possible to add as many alternative constructors as necessary.Class method can modify the class state,it bound to the class and it contain cls as parameter.Static method can not modify the class state,it bound to the class and it does't know class or instance

How to leave/exit/deactivate a Python virtualenv

Apreche

[How to leave/exit/deactivate a Python virtualenv](https://stackoverflow.com/questions/990754/how-to-leave-exit-deactivate-a-python-virtualenv)

I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command. How do I exit all virtual machines and work on my real machine again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on "nothing", and if so, what is it? If such a command does not exist, how would I go about creating it?

2009-06-13 14:15:36Z

I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command. How do I exit all virtual machines and work on my real machine again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on "nothing", and if so, what is it? If such a command does not exist, how would I go about creating it?Usually, activating a virtualenv gives you a shell function named:which puts things back to normal.I have just looked specifically again at the code for virtualenvwrapper, and, yes, it too supports deactivate as the way to escape from all virtualenvs.If you are trying to leave an Anaconda environment, the command depends upon your version of conda. Recent versions (like 4.6) install a conda function directly in your shell, in which case you run:Older conda versions instead implement deactivation using a stand-alone script:I defined an alias, workoff, as the opposite of workon:It is easy to remember:Use:If this doesn't work, try Anyone who knows how Bash source works will think that's odd, but some wrappers/workflows around virtualenv implement it as a complement/counterpart to source activate. Your mileage may vary.To activate a Python virtual environment:To deactivate:I found that when within a Miniconda3 environment I had to run:Neither deactivate nor source deactivate worked for me.You can use virtualenvwrapper in order to ease the way you work with virtualenv.Installing virtualenvwrapper:If you are using a standard shell, open your ~/.bashrc or ~/.zshrc if you use Oh My Zsh. Add these two lines:To activate an existing virtualenv, use command workon:In order to deactivate your virtualenv:Here is my tutorial, step by step on how to install virtualenv and virtualenvwrapper.Since the deactivate function created by sourcing ~/bin/activate cannot be discovered by the usual means of looking for such a command in ~/bin, you may wish to create one that just executes the function deactivate.The problem is that a script named deactivate containing a single command deactivate will cause an endless loop if accidentally executed while not in the venv. A common mistake.This can be avoided by only executing deactivate if the function exists (i.e. has been created by sourcing activate).Use deactivate.Note, (my_env) is gone.I use zsh-autoenv which is based off autoenv. Here is an example:So when I leave the dtree directory, the virtual environment is automatically exited."Development tree utiles" is just a name… No hidden mean linking to the Illuminati in here.Using the deactivate feature provided by the venv's activate script requires you to trust the deactivation function to be properly coded to cleanly reset all environment variables back to how they were before— taking into account not only the original activation, but also any switches, configuration, or other work you may have done in the meantime.It's probably fine, but it does introduce a new, non-zero risk of leaving your environment modified afterwards.However, it's not technically possible for a process to directly alter the environment variables of its parent, so we can use a separate sub-shell to be absolutely sure our venvs don't leave any residual changes behind:$ bash --init-file PythonVenv/bin/activate$ exit OR [CTRL]+[D]I had the same problem while working on an installer script. I took a look at what the bin/activate_this.py did and reversed it.Example:I am not 100% sure if it works as intended. I may have missed something completely.

How to check if the string is empty?

Joan Venge

[How to check if the string is empty?](https://stackoverflow.com/questions/9573244/how-to-check-if-the-string-is-empty)

Does Python have something like an empty string variable where you can do:Regardless, what's the most elegant way to check for empty string values? I find hard coding "" every time for checking an empty string not as good.

2012-03-05 20:09:23Z

Does Python have something like an empty string variable where you can do:Regardless, what's the most elegant way to check for empty string values? I find hard coding "" every time for checking an empty string not as good.Empty strings are "falsy" which means they are considered false in a Boolean context, so you can just do this:This is the preferred way if you know that your variable is a string.  If your variable could also be some other type then you should use myString == "".  See the documentation on Truth Value Testing for other values that are false in Boolean contexts.From PEP 8, in the「Programming Recommendations」section:So you should use:or:Just to clarify, sequences are evaluated to False or True in a Boolean context if they are empty or not. They are not equal to False or True.The most elegant way would probably be to simply check if its true or falsy, e.g.:However, you may want to strip white space because:You should probably be a bit more explicit in this however, unless you know for sure that this string has passed some kind of validation and is a string that can be tested this way.I would test noneness before stripping. Also, I would use the fact that empty strings are False (or Falsy). This approach is similar to Apache's StringUtils.isBlank or Guava's Strings.isNullOrEmptyThis is what I would use to test if a string is either None OR Empty OR Blank:And, the exact opposite to test if a string is not None NOR Empty NOR Blank:More concise forms of the above code:I once wrote something similar to Bartek's answer and javascript inspired:Test:The only really solid way of doing this is the following:All other solutions have possible problems and edge cases where the check can fail.len(myString)==0 can fail if myString is an object of a class that inherits from str and overrides the __len__() method.Similarly myString == "" and myString.__eq__("") can fail if myString overrides __eq__() and __ne__().For some reason "" == myString also gets fooled if myString overrides __eq__().myString is "" and "" is myString are equivalent. They will both fail if myString is not actually a string but a subclass of string (both will return False). Also, since they are identity checks, the only reason why they work is because Python uses String Pooling (also called String Internment) which uses the same instance of a string if it is interned (see here: Why does comparing strings using either '==' or 'is' sometimes produce a different result?). And "" is interned from the start in CPythonThe big problem with the identity check is that String Internment is (as far as I could find) that it is not standardised which strings are interned. That means, theoretically "" is not necessary interned and that is implementation dependant.The only way of doing this that really cannot be fooled is the one mentioned in the beginning: "".__eq__(myString). Since this explicitly calls the __eq__() method of the empty string it cannot be fooled by overriding any methods in myString and solidly works with subclasses of str.Also relying on the falsyness of a string might not work if the object overrides it's __bool__() method.This is not only theoretical work but might actually be relevant in real usage since I have seen frameworks and libraries subclassing str before and using myString is "" might return a wrong output there.Also, comparing strings using is in general is a pretty evil trap since it will work correctly sometimes, but not at other times, since string pooling follows pretty strange rules.That said, in most cases all of the mentioned solutions will work correctly. This is post is mostly academic work.Test empty or blank string (shorter way):If you want to differentiate between empty and null strings, I would suggest using if len(string), otherwise, I'd suggest using simply if string as others have said.  The caveat about strings full of whitespace still applies though, so don't forget to strip.if stringname: gives a false when the string is empty. I guess it can't be simpler than this.Doing this: foo == "" is very bad practice. "" is a magical value. You should never check against magical values (more commonly known as magical numbers)What you should do is compare to a descriptive variable name.One may think that "empty_string" is a descriptive variable name. It isn't.Before you go and do empty_string = "" and think you have a great variable name to compare to. This is not what "descriptive variable name" means.A good descriptive variable name is based on its context.

You have to think about what the empty string is.You are building a form where a user can enter values. You want to check if the user wrote something or not.A good variable name may be not_filled_inThis makes the code very readableYou are parsing CSV files and want the empty string to be parsed as None(Since CSV is entirely text based, it cannot represent None without using predefined keywords)A good variable name may be CSV_NONEThis makes the code easy to change and adapt if you have a new CSV file that represents None with another string than ""There are no questions about if this piece of code is correct. It is pretty clear that it does what it should do.Compare this toThe first question here is, Why does the empty string deserve special treatment?This would tell future coders that an empty string should always be considered as None.This is because it mixes business logic (What CSV value should be None) with code implementation (What are we actually comparing to)There needs to be a separation of concern between the two.Responding to @1290. Sorry, no way to format blocks in comments. The None value is not an empty string in Python, and neither is (spaces). The answer from Andrew Clark is the correct one: if not myString. The answer from @rouble is application-specific and does not answer the OP's question. You will get in trouble if you adopt a peculiar definition of what is a "blank" string. In particular, the standard behavior is that str(None) produces 'None', a non-blank string.However if you must treat None and (spaces) as "blank" strings, here is a better way:Examples:Meets the @rouble requirements while not breaking the expected bool behavior of strings.How about this?  Perhaps it's not "the most elegant", but it seems pretty complete and clear:I find this elegant as it makes sure it is a string and checks its length:Another easy way could be to define a simple function:  You may have a look at this Assigning empty value or string in PythonThis is about comparing strings that are empty. So instead of testing for emptiness with not, you may test is your string is equal to empty string with "" the empty string...for those who expect a behaviour like the apache StringUtils.isBlank or Guava Strings.isNullOrEmpty :When you are reading file by lines and want to determine, which line is empty, make sure you will use .strip(), because there is new line character in "empty" line:This expression is True for strings that are empty. Non-empty strings, None and non-string objects will all produce False, with the caveat that objects may override __str__ to thwart this logic by returning a falsy value.If you just use it is not possible to difference a variable which is boolean False from an empty string '':However, if you add a simple condition to your script, the difference is made:In case this is useful to someone, here is a quick function i built out to replace blank strings with N/A's in lists of lists (python 2).This is useful for posting lists of lists to a mysql database that does not accept blanks for certain fields (fields marked as NN in schema.  in my case, this was due to a composite primary key).I did some experimentation with strings like '', ' ', '\n', etc. I want isNotWhitespace to be True if and only if the variable foo is a string with at least one non-whitespace character. I'm using Python 3.6. Here's what I ended up with:Wrap this in a method definition if desired.As prmatta posted above, but with mistake.

String formatting: % vs. .format

NorthIsUp

[String formatting: % vs. .format](https://stackoverflow.com/questions/5082452/string-formatting-vs-format)

Python 2.6 introduced the str.format() method with a slightly different syntax from the existing % operator. Which is better and for what situations?

2011-02-22 18:46:42Z

Python 2.6 introduced the str.format() method with a slightly different syntax from the existing % operator. Which is better and for what situations?To answer your first question... .format just seems more sophisticated in many ways. An annoying thing about % is also how it can either take a variable or a tuple. You'd think the following would always work:yet, if name happens to be (1, 2, 3), it will throw a TypeError. To guarantee that it always prints, you'd need to dowhich is just ugly. .format doesn't have those issues. Also in the second example you gave, the .format example is much cleaner looking.Why would you not use it? To answer your second question, string formatting happens at the same time as any other operation - when the string formatting expression is evaluated. And Python, not being a lazy language, evaluates expressions before calling functions, so in your log.debug example, the expression "some debug info: %s"%some_infowill first evaluate to, e.g. "some debug info: roflcopters are active", then that string will be passed to log.debug(). Something that the modulo operator ( % ) can't do, afaik:resultVery useful.Another point: format(), being a function, can be used as an argument in other functions: Results in:Assuming you're using Python's logging module, you can pass the string formatting arguments as arguments to the .debug() method rather than doing the formatting yourself:which avoids doing the formatting unless the logger actually logs something.As of Python 3.6 (2016) you can use f-strings to substitute variables:Note the f" prefix. If you try this in Python 3.5 or earlier, you'll get a SyntaxError.See https://docs.python.org/3.6/reference/lexical_analysis.html#f-stringsPEP 3101 proposes the replacement of the % operator with the new, advanced string formatting in Python 3, where it would be the default.But please be careful, just now I've discovered one issue when trying to replace all % with .format in existing code: '{}'.format(unicode_string) will try to encode unicode_string and will probably fail.Just look at this Python interactive session log:s is just a string (called 'byte array' in Python3) and u is a Unicode string (called 'string' in Python3):When you give a Unicode object as a parameter to % operator it will produce a Unicode string even if the original string wasn't Unicode:but the .format function will raise "UnicodeEncodeError":and it will work with a Unicode argument fine only if the original string was Unicode.or if argument string can be converted to a string (so called 'byte array')Yet another advantage of .format (which I don't see in the answers): it can take object properties.Or, as a keyword argument:This is not possible with % as far as I can tell.% gives better performance than format from my test.Test code:Python 2.7.2:Result:Python 3.5.2ResultIt looks in Python2, the difference is small whereas in Python3, % is much faster than format.Thanks @Chris Cogdon for the sample code.Edit 1:Tested again in Python 3.7.2 in July 2019.Result:There is not much difference. I guess Python is improving gradually.Edit 2:After someone mentioned python 3's f-string in comment, I did a test for the following code under python 3.7.2 :Result:It seems f-string is still slower than % but better than format.As I discovered today, the old way of formatting strings via % doesn't support Decimal, Python's module for decimal fixed point and floating point arithmetic, out of the box.Example (using Python 3.3.5):Output:There surely might be work-arounds but you still might consider using the format() method right away. If your python >= 3.6, F-string formatted literal is your new friend.It's more simple, clean, and better performance.As a side note, you don't have to take a performance hit to use new style formatting with logging.  You can pass any object to logging.debug, logging.info, etc. that implements the __str__ magic method.  When the logging module has decided that it must emit your message object (whatever it is), it calls str(message_object) before doing so.  So you could do something like this:This is all described in the Python 3 documentation (https://docs.python.org/3/howto/logging-cookbook.html#formatting-styles).  However, it will work with Python 2.6 as well (https://docs.python.org/2.6/library/logging.html#using-arbitrary-objects-as-messages).One of the advantages of using this technique, other than the fact that it's formatting-style agnostic, is that it allows for lazy values e.g. the function expensive_func above.  This provides a more elegant alternative to the advice being given in the Python docs here: https://docs.python.org/2.6/library/logging.html#optimization.One situation where % may help is when you are formatting regex expressions. For example, raises IndexError. In this situation, you can use:This avoids writing the regex as '{type_names} [a-z]{{2}}'. This can be useful when you have two regexes, where one is used alone without format, but the concatenation of both is formatted.I would add that since version 3.6, we can use fstrings like the followingWhich giveEverything is converted to stringsResult:you can pass function, like in others formats methodGiving for exampleFor python version >= 3.6 (see PEP 498)Python 3.6.7 comparative:Output:But one thing is that also if you have nested curly-braces, won't work for format but % will work.Example:

Converting integer to string?

Hick

[Converting integer to string?](https://stackoverflow.com/questions/961632/converting-integer-to-string)

I want to convert an integer to a string in Python. I am typecasting it in vain:When I try to convert it to string, it's showing an error like int doesn't have any attribute called str.

2009-06-07 10:22:38Z

I want to convert an integer to a string in Python. I am typecasting it in vain:When I try to convert it to string, it's showing an error like int doesn't have any attribute called str.Links to the documentation:Conversion to a string is done with the builtin str() function, which basically calls the __str__() method of its parameter.Try this:There is not typecast and no type coercion in Python. You have to convert your variable in an explicit way.To convert an object in string you use the str() function. It works with any object that has a method  called __str__() defined. In factis equivalent toThe same if you want to convert something to int, float, etc.To manage non-integer inputs:In Python => 3.6 you can use f formatting:The most decent way in my opinion is ``.Can use %s or .format(OR)For someone who wants to convert int to string in specific digits, the below method is recommended.For more details, you can refer to Stack Overflow question Display number with leading zeros.With the introduction of f-strings in Python 3.6, this will also work:It is actually faster than calling str(), at the cost of readability.In fact, it's faster than %x string formatting and .format()!For Python 3.6 you can use the f-strings new feature to convert to string and it's faster compared to str() function, it is used like that: Python provides the str() function for that reason.For more detailed answer you can check this article: Converting Python Int to String and Python String to Int

Delete an element from a dictionary

richzilla

[Delete an element from a dictionary](https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary)

Is there a way to delete an item from a dictionary in Python?Additionally, how can I delete an item from a dictionary to return a copy (i.e., not modifying the original)?

2011-04-30 21:20:57Z

Is there a way to delete an item from a dictionary in Python?Additionally, how can I delete an item from a dictionary to return a copy (i.e., not modifying the original)?The del statement removes an element:However, this mutates the existing dictionary so the contents of the dictionary changes for anybody else who has a reference to the same instance. To return a new dictionary, make a copy of the dictionary:The dict() constructor makes a shallow copy. To make a deep copy, see the copy module.Note that making a copy for every dict del/assignment/etc. means you're going from constant time to linear time, and also using linear space. For small dicts, this is not a problem. But if you're planning to make lots of copies of large dicts, you probably want a different data structure, like a HAMT (as described in this answer).pop mutates the dictionary.If you want to keep the original you could just copy it.I think your solution is best way to do it. But if you want another solution, you can create a new dictionary with using the keys from old dictionary without including your specified key, like this:The del statement is what you're looking for. If you have a dictionary named foo with a key called 'bar', you can delete 'bar' from foo like this:Note that this permanently modifies the dictionary being operated on. If you want to keep the original dictionary, you'll have to create a copy beforehand:The dict call makes a shallow copy. If you want a deep copy, use copy.deepcopy.Here's a method you can copy & paste, for your convenience:There're a lot of nice answers, but I want to emphasize one thing.You can use both dict.pop() method and a more generic del statement to remove items from a dictionary. They both mutate the original dictionary, so you need to make a copy (see details below).And both of them will raise a KeyError if the key you're providing to them is not present in the dictionary:andYou have to take care of this:by capturing the exception:andby performing a check:andbut with pop() there's also a much more concise way - provide the default return value:Unless you use pop() to get the value of a key being removed you may provide anything, not necessary None.

Though it might be that using del with in check is slightly faster due to pop() being a function with its own complications causing overhead. Usually it's not the case, so pop() with default value is good enough.As for the main question, you'll have to make a copy of your dictionary, to save the original dictionary and have a new one without the key being removed.Some other people here suggest making a full (deep) copy with copy.deepcopy(), which might be an overkill, a "normal" (shallow) copy, using copy.copy() or dict.copy(), might be enough. The dictionary keeps a reference to the object as a value for a key. So when you remove a key from a dictionary this reference is removed, not the object being referenced. The object itself may be removed later automatically by the garbage collector, if there're no other references for it in the memory. Making a deep copy requires more calculations compared to shallow copy, so it decreases code performance by making the copy, wasting memory and providing more work to the GC, sometimes shallow copy is enough.However, if you have mutable objects as dictionary values and plan to modify them later in the returned dictionary without the key, you have to make a deep copy.With shallow copy:With deep copy:Result: d = {1: 2, '2': 3}A dict is the wrong data structure to use for this.Sure, copying the dict and popping from the copy works, and so does building a new dict with a comprehension, but all that copying takes time—you've replaced a constant-time operation with a linear-time one. And all those copies alive at once take space—linear space per copy.Other data structures, like hash array mapped tries, are designed for exactly this kind of use case: adding or removing an element returns a copy in logarithmic time, sharing most of its storage with the original.1Of course there are some downsides. Performance is logarithmic rather than constant (although with a large base, usually 32-128). And, while you can make the non-mutating API identical to dict, the "mutating" API is obviously different. And, most of all, there's no HAMT batteries included with Python.2The pyrsistent library is a pretty solid implementation of HAMT-based dict-replacements (and various other types) for Python. It even has a nifty evolver API for porting existing mutating code to persistent code as smoothly as possible. But if you want to be explicit about returning copies rather than mutating, you just use it like this:That d3 = d1.remove('a') is exactly what the question is asking for.If you've got mutable data structures like dict and list embedded in the pmap, you'll still have aliasing issues—you can only fix that by going immutable all the way down, embedding pmaps and pvectors.1. HAMTs have also become popular in languages like Scala, Clojure, Haskell because they play very nicely with lock-free programming and software transactional memory, but neither of those is very relevant in Python.2. In fact, there is an HAMT in the stdlib, used in the implementation of contextvars. The earlier withdrawn PEP explains why. But this is a hidden implementation detail of the library, not a public collection type.Simply call del d['key'].However, in production, it is always a good practice to check if 'key' exists in d.No, there is no other way thanHowever, often creating copies of only slightly altered dictionaries is probably not a good idea because it will result in comparatively large memory demands. It is usually better to log the old dictionary(if even necessary) and then modify it.this doesn't do any error handling, it assumes the key is in the dict, you might want to check that first and raise if its notHere a top level design approach:I'm passing the dictionary and the key I want into my function, validates if it's a dictionary and if the key is okay, and if both exist, removes the value from the dictionary and prints out the left-overs.Output: {'B': 55, 'A': 34}Hope that helps!Below code snippet will help you definitely, I have added comments in each line which will help you in understanding the code.or you can also use dict.pop()or the better approach isHere's another variation using list comprehension:The approach is based on an answer from this post:

Efficient way to remove keys with empty strings from a dictThe following code will make a copy of dict species and delete items which are not in trans_HI

How can you profile a Python script?

Chris Lawlor

[How can you profile a Python script?](https://stackoverflow.com/questions/582336/how-can-you-profile-a-python-script)

Project Euler and other coding contests often have a maximum time to run or people boast of how fast their particular solution runs. With Python, sometimes the approaches are somewhat kludgey - i.e., adding timing code to __main__.What is a good way to profile how long a Python program takes to run?

2009-02-24 16:01:26Z

Project Euler and other coding contests often have a maximum time to run or people boast of how fast their particular solution runs. With Python, sometimes the approaches are somewhat kludgey - i.e., adding timing code to __main__.What is a good way to profile how long a Python program takes to run?Python includes a profiler called cProfile. It not only gives the total running time, but also times each function separately, and tells you how many times each function was called, making it easy to determine where you should make optimizations.You can call it from within your code, or from the interpreter, like this:Even more usefully, you can invoke the cProfile when running a script:To make it even easier, I made a little batch file called 'profile.bat':So all I have to do is run:And I get this:EDIT: Updated link to a good video resource from PyCon 2013 titled 

Python Profiling

Also via YouTube.A while ago I made pycallgraph which generates a visualisation from your Python code. Edit: I've updated the example to work with 3.3, the latest release as of this writing.After a pip install pycallgraph and installing GraphViz you can run it from the command line:Or, you can profile particular parts of your code:Either of these will generate a pycallgraph.png file similar to the image below:It's worth pointing out that using the profiler only works (by default) on the main thread, and you won't get any information from other threads if you use them.  This can be a bit of a gotcha as it is completely unmentioned in the profiler documentation.If you also want to profile threads, you'll want to look at the threading.setprofile() function in the docs.You could also create your own threading.Thread subclass to do it:and use that ProfiledThread class instead of the standard one.  It might give you more flexibility, but I'm not sure it's worth it, especially if you are using third-party code which wouldn't use your class.The python wiki is a great page for profiling resources:

http://wiki.python.org/moin/PythonSpeed/PerformanceTips#Profiling_Codeas is the python docs:

http://docs.python.org/library/profile.htmlas shown by Chris Lawlor cProfile is a great tool and can easily be used to print to the screen:or to file:PS> If you are using Ubuntu, make sure to install python-profileIf you output to file you can get nice visualizations using the following toolsPyCallGraph : a tool to create call graph images 

  install:run:view:You can use whatever you like to view the png file, I used gimp

Unfortunately I often get dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.257079 to fitwhich makes my images unusably small.  So I generally create svg files:PS> make sure to install graphviz (which provides the dot program):Alternative Graphing using gprof2dot via @maxy / @quodlibetor :@Maxy's comment on this answer helped me out enough that I think it deserves its own answer: I already had cProfile-generated .pstats files and I didn't want to re-run things with pycallgraph, so I used gprof2dot, and got pretty svgs:and BLAM!It uses dot (the same thing that pycallgraph uses) so output looks similar. I get the impression that gprof2dot loses less information though:I ran into a handy tool called SnakeViz when researching this topic. SnakeViz is a web-based profiling visualization tool. It is very easy to install and use. The usual way I use it is to generate a stat file with %prun and then do analysis in SnakeViz.The main viz technique used is Sunburst chart as shown below, in which the hierarchy of function calls is arranged as layers of arcs and time info encoded in their angular widths.The best thing is you can interact with the chart. For example, to zoom in one can click on an arc, and the arc and its descendants will be enlarged as a new sunburst to display more details.Simplest and quickest way to find where all the time is going.Draws a pie chart in a browser. Biggest piece is the problem function. Very simple.I think that cProfile is great for profiling, while kcachegrind is great for visualizing the results. The pyprof2calltree in between handles the file conversion.To install the required tools (on Ubuntu, at least):The result:Also worth mentioning is the GUI cProfile dump viewer RunSnakeRun.  It allows you to sort and select, thereby zooming in on the relevant parts of the program.  The sizes of the rectangles in the picture is proportional to the time taken.  If you mouse over a rectangle it highlights that call in the table and everywhere on the map.  When you double-click on a rectangle it zooms in on that portion.  It will show you who calls that portion and what that portion calls.The descriptive information is very helpful.  It shows you the code for that bit which can be helpful when you are dealing with built-in library calls.  It tells you what file and what line to find the code.Also want to point at that the OP said 'profiling' but it appears he meant 'timing'.  Keep in mind programs will run slower when profiled.A nice profiling module is the line_profiler (called using the script kernprof.py).  It can be downloaded here.My understanding is that cProfile only gives information about total time spent in each function.  So individual lines of code are not timed.  This is an issue in scientific computing since often one single line can take a lot of time.  Also, as I remember, cProfile didn't catch the time I was spending in say numpy.dot.line_profiler (already presented here) also inspired  pprofile, which is described as:It provides line-granularity as line_profiler, is pure Python, can be used as a standalone command or a module, and can even generate callgrind-format files that can be easily analyzed with [k|q]cachegrind.There is also vprof, a Python package described as:I recently created tuna for visualizing Python runtime and import profiles; this may be helpful here.Install withCreate a runtime profileor an import profile (Python 3.7+ required)Then just run tuna on the fileThere's a lot of great answers but they either use command line or some external program for profiling and/or sorting the results.I really missed some way I could use in my IDE (eclipse-PyDev) without touching the command line or installing anything. So here it is.See docs or other answers for more info.Following Joe Shaw's answer about multi-threaded code not to work as expected, I figured that the runcall method in cProfile is merely doing self.enable() and self.disable() calls around the profiled function call, so you can simply do that yourself and have whatever code you want in-between with minimal interference with existing code.In Virtaal's source there's a very useful class and decorator that can make profiling (even for specific methods/functions) very easy. The output can then be viewed very comfortably in KCacheGrind.cProfile is great for quick profiling but most of the time it was ending for me with the errors. Function runctx solves this problem by initializing correctly the environment and variables, hope it can be useful for someone:If you want to make a cumulative profiler,

meaning to run the function several times in a row and watch the sum of the results.you can use this cumulative_profiler decorator:it's python >= 3.6 specific, but you can remove nonlocal for it work on older versions.Exampleprofiling the function baz baz ran 5 times and printed this:specifying the amount of timesThe terminal-only (and simplest) solution, in case all those fancy UI's fail to install or to run:

ignore cProfile completely and replace it with pyinstrument, that will collect and display the tree of calls right after execution.Install:  Profile and display result:  Works with python2 and 3.My way is to use yappi (https://github.com/sumerc/yappi). It's especially useful combined with an RPC server where (even just for debugging) you register method to start, stop and print profiling information, e.g. in this way: Then when your program work you can start profiler at any time by calling the startProfiler RPC method and dump profiling information to a log file by calling printProfiler (or modify the rpc method to return it to the caller) and get such output:It may not be very useful for short scripts but helps to optimize server-type processes especially given the printProfiler method can be called multiple times over time to profile and compare e.g. different program usage scenarios. In newer versions of yappi, the following code will work:A new tool to handle profiling in Python is PyVmMonitor: http://www.pyvmmonitor.com/It has some unique features such asNote: it's commercial, but free for open source.gprof2dot_magicMagic function for gprof2dot to profile any Python statement as a DOT graph in JupyterLab or Jupyter Notebook.GitHub repo: https://github.com/mattijn/gprof2dot_magicinstallationMake sure you've the Python package gprof2dot_magic.Its dependencies gprof2dot and graphviz will be installed as wellusageTo enable the magic function, first load the gprof2dot_magic moduleand then profile any line statement as a DOT graph as such:https://github.com/amoffat/Inspect-ShellYou could use that (and your wristwatch).To add on to https://stackoverflow.com/a/582337/1070617,I wrote this module that allows you to use cProfile and view its output easily. More here: https://github.com/ymichael/cprofilevAlso see: http://ymichael.com/2014/03/08/profiling-python-with-cprofile.html on how to make sense of the collected statistics.It would depend on what you want to see out of profiling. Simple time 

metrics can be given by (bash). Even '/usr/bin/time' can output detailed metrics by using '--verbose' flag.To check time metrics given by each function and to better understand how much time is spent on functions, you can use the inbuilt cProfile in python. Going into more detailed metrics like performance, time is not the only metric. You can worry about memory, threads etc.

Profiling options:

1. line_profiler is another profiler used commonly to find out timing metrics line-by-line.

2. memory_profiler is a tool to profile memory usage.

3. heapy (from project Guppy) Profile how objects in the heap are used. These are some of the common ones I tend to use. But if you want to find out more, try reading this book

It is a pretty good book on starting out with performance in mind. You can move onto advanced topics on using Cython and JIT(Just-in-time) compiled python. There's also a statistical profiler called statprof. It's a sampling profiler, so it adds minimal overhead to your code and gives line-based (not just function-based) timings. It's more suited to soft real-time applications like games, but may be have less precision than cProfile.The version in pypi is a bit old, so can install it with pip by specifying the git repository:You can run it like this:See also https://stackoverflow.com/a/10333592/320036I just developed my own profiler inspired from pypref_time:https://github.com/modaresimr/auto_profilerBy adding a decorator it will show a tree of time-consuming functions @Profiler(depth=4, on_disable=show)When i'm not root on the server, I use 

lsprofcalltree.py and run my program like this:Then I can open the report with any callgrind-compatible software, like qcachegrind

Delete column from pandas DataFrame

John

[Delete column from pandas DataFrame](https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe)

When deleting a column in a DataFrame I use:And this works great. Why can't I use the following?As you can access the column/Series as df.column_name, I expect this to work.

2012-11-16 06:26:40Z

When deleting a column in a DataFrame I use:And this works great. Why can't I use the following?As you can access the column/Series as df.column_name, I expect this to work.As you've guessed, the right syntax is It's difficult to make del df.column_name work simply as the result of syntactic limitations in Python. del df[name] gets translated to df.__delitem__(name) under the covers by Python.The best way to do this in pandas is to use drop:where 1 is the axis number (0 for rows and 1 for columns.)To delete the column without having to reassign df you can do:Finally, to drop by column number instead of by column label, try this to delete, e.g. the 1st, 2nd and 4th columns:Also working with "text" syntax for the columns:Use:This will delete one or more columns in-place. Note that inplace=True was added in pandas v0.13 and won't work on older versions. You'd have to assign the result back in that case:Delete first, second and fourth columns:Delete first column:There is an optional parameter inplace so that the original

data can be modified without creating a copy.Column selection, addition, deletionDelete column column-name:print df:df.drop(df.columns[[0]], axis=1, inplace=True)

print df:three = df.pop('three')

print df:The actual question posed, missed by most answers here is:At first we need to understand the problem, which requires us to dive into python magic methods.As Wes points out in his answer del df['column'] maps to the python magic method df.__delitem__('column') which is implemented in pandas to drop the columnHowever, as pointed out in the link above about python magic methods:You could argue that del df['column_name'] should not be used or encouraged, and thereby del df.column_name should not even be considered.However, in theory, del df.column_name could be implemeted to work in pandas using the magic method __delattr__. This does however introduce certain problems, problems which the del df['column_name'] implementation already has, but in lesser degree.What if I define a column in a dataframe called "dtypes" or "columns".Then assume I want to delete these columns.del df.dtypes would make the __delattr__ method confused as if it should delete the "dtypes" attribute or the "dtypes" column.You cannot do del df.column_name because pandas has a quite wildly grown architecture that needs to be reconsidered in order for this kind of cognitive dissonance not to occur to its users.Don't use df.column_name, It may be pretty, but it causes cognitive dissonanceThere are multiple ways of deleting a column.Columns are sometimes attributes but sometimes not.Does del df.dtypes delete the dtypes attribute or the dtypes column?A nice addition is the ability to drop columns only if they exist. This way you can cover more use cases, and it will only drop the existing columns from the labels passed to it:Simply add errors='ignore', for example.:from version 0.16.1 you can do It's good practice to always use the [] notation. One reason is that attribute notation (df.column_name) does not work for numbered indices:Pandas version 0.21 has changed the drop method slightly to include both the index and columns parameters to match the signature of the rename and reindex methods. Personally, I prefer using the axis parameter to denote columns or index because it is the predominant keyword parameter used in nearly all pandas methods. But, now you have some added choices in version 0.21.In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:A lot of effort to find a marginally more efficient solution.  Difficult to justify the added complexity while sacrificing the simplicity of df.drop(dlst, 1, errors='ignore')Preamble

Deleting a column is semantically the same as selecting the other columns.  I'll show a few additional methods to consider.  I'll also focus on the general solution of deleting multiple columns at once and allowing for the attempt to delete columns not present.  Using these solutions are general and will work for the simple case as well.Setup

Consider the pd.DataFrame df and list to delete dlstThe result should look like:Since I'm equating deleting a column to selecting the other columns, I'll break it into two types:We start by manufacturing the list/array of labels that represent the columns we want to keep and without the columns we want to delete.Columns from Labels

For the sake of comparing the selection process, assume:Then we can evaluate  Which all evaluate to:We can construct an array/list of booleans for slicingColumns from Boolean

For the sake of comparison  Which all evaluate to:Robust Timing  Functions  Testing  This is relative to the time it takes to run df.drop(dlst, 1, errors='ignore').  It seems like after all that effort, we only improve performance modestly.If fact the best solutions use reindex or reindex_axis on the hack list(set(df.columns.values.tolist()).difference(dlst)).  A close second and still very marginally better than drop is np.setdiff1d.The dot syntax works in JavaScript, but not in Python.Another way of Deleting a Column in Pandas DataFrameif you're not looking for In-Place deletion then you can create a new DataFrame by specifying the columns using DataFrame(...) function  asCreate a new DataFrame asYou get a result as good as what you get with del / drop

Pythonic way to create a long multi-line string

Pablo Mescher

[Pythonic way to create a long multi-line string](https://stackoverflow.com/questions/10660435/pythonic-way-to-create-a-long-multi-line-string)

I have a very long query. I would like to split it in several lines in Python. A way to do it in JavaScript would be using several sentences and joining them with a + operator (I know, maybe it's not the most efficient way to do it, but I'm not really concerned about performance in this stage, just code readability). Example:I tried doing something similar in Python, but it didn't work, so I used \ to split the long string. However, I'm not sure if this is the only/best/pythonicest way of doing it. It looks awkward. 

Actual code:

2012-05-18 22:21:09Z

I have a very long query. I would like to split it in several lines in Python. A way to do it in JavaScript would be using several sentences and joining them with a + operator (I know, maybe it's not the most efficient way to do it, but I'm not really concerned about performance in this stage, just code readability). Example:I tried doing something similar in Python, but it didn't work, so I used \ to split the long string. However, I'm not sure if this is the only/best/pythonicest way of doing it. It looks awkward. 

Actual code:Are you talking about multi-line strings? Easy, use triple quotes to start and end them.You can use single quotes too (3 of them of course at start and end) and treat the resulting string s just like any other string.NOTE: Just as with any string, anything between the starting and ending quotes becomes part of the string, so this example has a leading blank (as pointed out by @root45). This string will also contain both blanks and newlines.I.e.,:Finally, one can also construct long lines in Python like this:which will not include any extra blanks or newlines (this is a deliberate example showing what the effect of skipping blanks will result in):No commas required, simply place the strings to be joined together into a pair of parenthesis and be sure to account for any needed blanks and newlines.If you don't want a multiline string but just have a long single line string, you can use parentheses, just make sure you don't include commas between the string segments, then it will be a tuple.In a SQL statement like what you're constructing, multiline strings would also be fine.  But if the extra whitespace a multiline string would contain would be a problem, then this would be a good way to achieve what you want.Breaking lines by \ works for me.  Here is an example:I found myself happy with this one:I find that when building long strings, you are usually doing something like building an SQL query, in which case this is best:What Levon suggested is good, but might be vulnerable to mistakes:You can also concatenate variables in when using """ notation:EDIT: Found a better way, with named params and .format():This approach uses:This way looks the most pythonic to me.Update: 1/29/2019 Incorporate @ShadowRanger's suggestion to use inspect.cleandoc instead of textwrap.dedentIn Python >= 3.6 you can use Formatted string literals (f string)For example:if the value of condition should be a string, you can do like this:I find textwrap.dedent the best for long strings as described here:I personally find the following to be the best (simple, safe and Pythonic) way to write raw SQL queries in Python, especially when using Python's sqlite3 module:Others have mentioned the parentheses method already, but I'd like to add that with parentheses, inline comments are allowed.When using backslash line continuations (\ ), comments are not allowed. You'll receive a SyntaxError: unexpected character after line continuation character error.Based on the example from https://docs.python.org/3/library/re.html#re.VERBOSE, I usually use something like this:If you want to remove annoying blank spaces in each line, you could do as follows:Your actual code shouldn't work, you are missing whitespaces at the end of "lines" (eg: role.descr as roleFROM...)There is triplequotes for multiline string:It will contain the line breaks and extra spaces, but for SQL that's not a problem.You can also place the sql-statement in a seperate file action.sql and load it in the py file withSo the sql-statements will be separated from the python code. If there are parameters in the sql statement which needs to be filled from python, you can use string formating (like %s or {field})"À la" Scala way (but I think is the most pythonic way as OQ demands):If you want final str without jump lines, just put \n at the start of the first argument of the second replace:Note: the white line between "...templates." and "Also, ..." requires a whitespace after the |.Hey try something like this hope it works, like in this format it will return you a continuous line like you have successfully enquired about this property`tl;dr: Use """\ and """ to wrap the string, as inFrom the official python documentation:I use a recursive function to build complex SQL Queries. This technique can generally be used to build large strings while maintaining code readability.P.S: Have a look at the awesome python-sqlparse library to pretty print SQL queries if needed.

http://sqlparse.readthedocs.org/en/latest/api/#sqlparse.formatAnother option that I think is more readable when the code (e.g variable) is indented and the output string should be a one liner (no newlines):I like this approach because it privileges reading. In cases where we have long strings there is no way! Depending on the level of indentation you are in and still limited to 80 characters per line... Well... No need to say anything else. In my view the python style guides are still very vague. I took the @Eero Aaltonen approach because it privileges reading and common sense. I understand that style guides should help us and not make our lives a mess. Thanks! From the official python documentation:For defining a long string inside a dict,

keeping the newlines but omitting the spaces, I ended up defining the string in a constant like this:Generally, I use list and join for multi-line comments/string.you can use any string to join all this list element like '\n'(newline) or ','(comma) or ''(space)Cheers..!!

Proper way to declare custom exceptions in modern Python?

Nelson

[Proper way to declare custom exceptions in modern Python?](https://stackoverflow.com/questions/1319615/proper-way-to-declare-custom-exceptions-in-modern-python)

What's the proper way to declare custom exception classes in modern Python? My primary goal is to follow whatever standard other exception classes have, so that (for instance) any extra string I include in the exception is printed out by whatever tool caught the exception.By "modern Python" I mean something that will run in Python 2.5 but be 'correct' for the Python 2.6 and Python 3.* way of doing things. And by "custom" I mean an Exception object that can include extra data about the cause of the error: a string, maybe also some other arbitrary object relevant to the exception.I was tripped up by the following deprecation warning in Python 2.6.2:It seems crazy that BaseException has a special meaning for attributes named message. I gather from PEP-352 that attribute did have a special meaning in 2.5 they're trying to deprecate away, so I guess that name (and that one alone) is now forbidden? Ugh.I'm also fuzzily aware that Exception has some magic parameter args, but I've never known how to use it. Nor am I sure it's the right way to do things going forward; a lot of the discussion I found online suggested they were trying to do away with args in Python 3.Update: two answers have suggested overriding __init__, and __str__/__unicode__/__repr__. That seems like a lot of typing, is it necessary?

2009-08-23 21:29:29Z

What's the proper way to declare custom exception classes in modern Python? My primary goal is to follow whatever standard other exception classes have, so that (for instance) any extra string I include in the exception is printed out by whatever tool caught the exception.By "modern Python" I mean something that will run in Python 2.5 but be 'correct' for the Python 2.6 and Python 3.* way of doing things. And by "custom" I mean an Exception object that can include extra data about the cause of the error: a string, maybe also some other arbitrary object relevant to the exception.I was tripped up by the following deprecation warning in Python 2.6.2:It seems crazy that BaseException has a special meaning for attributes named message. I gather from PEP-352 that attribute did have a special meaning in 2.5 they're trying to deprecate away, so I guess that name (and that one alone) is now forbidden? Ugh.I'm also fuzzily aware that Exception has some magic parameter args, but I've never known how to use it. Nor am I sure it's the right way to do things going forward; a lot of the discussion I found online suggested they were trying to do away with args in Python 3.Update: two answers have suggested overriding __init__, and __str__/__unicode__/__repr__. That seems like a lot of typing, is it necessary?Maybe I missed the question, but why not:Edit: to override something (or pass extra args), do this:That way you could pass dict of error messages to the second param, and get to it later with e.errorsPython 3 Update: In Python 3+, you can use this slightly more compact use of super():With modern Python Exceptions, you don't need to abuse .message, or override .__str__() or .__repr__() or any of it. If all you want is an informative message when your exception is raised, do this:That will give a traceback ending with MyException: My hovercraft is full of eels.If you want more flexibility from the exception, you could pass a dictionary as the argument:However, to get at those details in an except block is a bit more complicated. The details are stored in the args attribute, which is a list. You would need to do something like this:It is still possible to pass in multiple items to the exception and access them via tuple indexes, but this is highly discouraged (and was even intended for deprecation a while back). If you do need more than a single piece of information and the above method is not sufficient for you, then you should subclass Exception as described in the tutorial.This is fine, unless your exception is really a type of a more specific exception:Or better (maybe perfect), instead of pass give a docstring:From the docsThat means that if your exception is a type of a more specific exception, subclass that exception instead of the generic Exception (and the result will be that you still derive from Exception as the docs recommend). Also, you can at least provide a docstring (and not be forced to use the pass keyword):Set attributes you create yourself with a custom __init__. Avoid passing a dict as a positional argument, future users of your code will thank you. If you use the deprecated message attribute, assigning it yourself will avoid a DeprecationWarning:There's really no need to write your own __str__ or __repr__. The builtin ones are very nice, and your cooperative inheritance ensures that you use it.Again, the problem with the above is that in order to catch it, you'll either have to name it specifically (importing it if created elsewhere) or catch Exception, (but you're probably not prepared to handle all types of Exceptions, and you should only catch exceptions you are prepared to handle). Similar criticism to the below, but additionally that's not the way to initialize via super, and you'll get a DeprecationWarning if you access the message attribute:It also requires exactly two arguments to be passed in (aside from the self.) No more, no less. That's an interesting constraint that future users may not appreciate. To be direct - it violates Liskov substitutability.I'll demonstrate both errors:Compared to: see how exceptions work by default if one vs more attributes are used (tracebacks omitted):so you might want to have a sort of "exception template", working as an exception itself, in a compatible way:this can be done easily with this subclassand if you don't like that default tuple-like representation, just add __str__ method to the ExceptionTemplate class, like:and you'll haveAs of Python 3.8 (2018, https://docs.python.org/dev/whatsnew/3.8.html), the recommended method is still:Please don't forget to document, why a custom exception is neccessary!If you need to, this is the way to go for exceptions with more data:and fetch them like:payload=None is important to make it pickle-able. Before dumping it, you have to call error.__reduce__(). Loading will work as expected.You maybe should investigate in finding a solution using pythons return statement if you need much data to be transferred to some outer structure. This seems to be clearer/more pythonic to me. Advanced exceptions are heavily used in Java, which can sometimes be annoying, when using a framework and having to catch all possible errors.You should override __repr__ or __unicode__ methods instead of using message, the args you provide when you construct the exception will be in the args attribute of the exception object.No, "message" is not forbidden. It's just deprecated. You application will work fine with using message. But you may want to get rid of the deprecation error, of course.When you create custom Exception classes for your application, many of them do not subclass just from Exception, but from others, like ValueError or similar. Then you have to adapt to their usage of variables.And if you have many exceptions in your application it's usually a good idea to have a common custom base class for all of them, so that users of your modules can doAnd in that case you can do the __init__ and __str__ needed there, so you don't have to repeat it for every exception. But simply calling the message variable something else than message does the trick.In any case, you only need the __init__ or __str__ if you do something different from what Exception itself does. And because if the deprecation, you then need both, or you get an error. That's not a whole lot of extra code you need per class. ;)See a very good article "The definitive guide to Python exceptions". The basic principles are:There is also information on organizing (in modules) and wrapping exceptions, I recommend to read the  guide.Try this ExampleTo define your own exceptions correctly, there are a few best practices that you need to follow:

Extracting extension from filename in Python

Alex

[Extracting extension from filename in Python](https://stackoverflow.com/questions/541390/extracting-extension-from-filename-in-python)

Is there a function to extract the extension from a filename?

2009-02-12 14:11:50Z

Is there a function to extract the extension from a filename?Yes. Use os.path.splitext(see Python 2.X documentation or Python 3.X documentation):Unlike most manual string-splitting attempts, os.path.splitext will correctly treat /a/b.c/d as having no extension instead of having extension .c/d, and it will treat .bashrc as having no extension instead of having extension .bashrc:New in version 3.4.I'm surprised no one has mentioned pathlib yet, pathlib IS awesome!If you need all the suffixes (eg if you have a .tar.gz), .suffixes will return a list of them!To get only the text of the extension, without the dot.One option may be splitting from dot:No error when file doesn't have an extension:But you must be careful:worth adding a lower in there so you don't find yourself wondering why the JPG's aren't showing up in your list.Any of the solutions above work, but on linux I have found that there is a newline at the end of the extension string which will prevent matches from succeeding. Add the strip() method to the end. For example:  With splitext there are problems with files with double extension (e.g. file.tar.gz, file.tar.bz2, etc..)but should be: .tar.gzThe possible solutions are hereYou can find some great stuff in pathlib module (available in python 3.x).Although it is an old topic, but i wonder why there is none mentioning a very simple api of python called rpartition in this case:to get extension of a given file absolute path, you can simply type:example:will give you:  'csv'Just join all pathlib suffixes.Surprised this wasn't mentioned yet:Benefits:As function:You can use a split on a filename:This does not require additional libraryThis is a direct string representation techniques :

I see a lot of solutions mentioned, but I think most are looking at split.

Split however does it at every occurrence of "." .

What you would rather be looking for is partition.Another solution with right split:Even this question is already answered I'd add the solution in Regex. This is The Simplest Method to get both Filename & Extension in just a single line.Unlike other solutions, you don't need to import any package for this.For funsies... just collect the extensions in a dict, and track all of them in a folder.  Then just pull the extensions you want. A true one-liner, if you like regex. 

And it doesn't matter even if you have additional "." in the middleSee here for the result: Click HereThat will give you the file name up to the first ".", which would be the most common.

Measure time elapsed in Python

gilbert8

[Measure time elapsed in Python](https://stackoverflow.com/questions/7370801/measure-time-elapsed-in-python)

What I want is to start counting time somewhere in my code and then get the passed time, to measure the time it took to execute few function. I think I'm using the timeit module wrong, but the docs are just confusing for me.

2011-09-10 09:21:02Z

What I want is to start counting time somewhere in my code and then get the passed time, to measure the time it took to execute few function. I think I'm using the timeit module wrong, but the docs are just confusing for me.If you just want to measure the elapsed wall-clock time between two points, you could use  time.time():This gives the execution time in seconds.Another option since 3.3 might be to use perf_counter or process_time, depending on your requirements. Before 3.3 it was recommended to use time.clock (thanks Amber). However, it is currently deprecated:Use timeit.default_timer instead of timeit.timeit. The former provides the best clock available on your platform and version of Python automatically:timeit.default_timer is assigned to time.time() or time.clock() depending on OS. On Python 3.3+ default_timer is time.perf_counter() on all platforms. See Python - time.clock() vs. time.time() - accuracy?See also:Since time.clock() is deprecated as of Python 3.3, you will want to use time.perf_counter() for system-wide timing, or time.process_time() for process-wide timing, just the way you used to use time.clock():The new function process_time will not include time elapsed during sleep.Given a function you'd like to time,test.py:the easiest way to use timeit is to call it from the command line:Do not try to use time.time or time.clock (naively) to compare the speed of functions. They can give misleading results.PS. Do not put print statements in a function you wish to time; otherwise the time measured will depend on the speed of the terminal.It's fun to do this with a context-manager that automatically remembers the start time upon entry to a with block, then freezes the end time on block exit. With a little trickery, you can even get a running elapsed-time tally inside the block from the same context-manager function. The core library doesn't have this (but probably ought to). Once in place, you can do things like:Here's contextmanager code sufficient to do the trick:And some runnable demo code:Note that by design of this function, the return value of elapsed() is frozen on block exit, and further calls return the same duration (of about 6 seconds in this toy example). Measuring time in seconds:Output:I prefer this. timeit doc is far too confusing. Note, that there isn't any formatting going on here, I just wrote hh:mm:ss into the printout so one can interpret time_elapsedIt's 2020 already, let's do it with a conciser way:Advantages of using this approch instead of others:Please refer to this link for more detailed instructions.Using time.time to measure execution gives you the overall execution time of your commands including running time spent by other processes on your computer. It is the time the user notices, but is not good if you want to compare different code snippets / algorithms / functions / ...More information on timeit:If you want a deeper insight into profiling:Update: I used http://pythonhosted.org/line_profiler/ a lot during the last year and find it very helpfull and recommend to use it instead of Pythons profile module.Here are my findings after going through many good answers here as well as a few other articles.First, if you are debating between timeit and time.time, the timeit has two advantages: Now the problem is that timeit is not that simple to use because it needs setup and things get ugly when you have a bunch of imports. Ideally, you just want a decorator or use with block and measure time. Unfortunately, there is nothing built-in available for this so you have two options:Option 1: Use timebudget libraryThe timebudget is a versatile and very simple library that you can use just in one line of code after pip install.Option 2: Use code module directlyI created below little utility module.Now you can time any function just by putting a decorator in front of it:If you want to time portion of code then just put it inside with block:Advantages:There are several half-backed versions floating around so I want to point out few highlights:Here is a tiny timer class that returns "hh:mm:ss" string: Usage: The python cProfile and pstats modules offer great support for measuring time elapsed in certain functions without having to add any code around the existing functions.For example if you have a python script timeFunctions.py:To run the profiler and generate stats for the file you can just run:What this is doing is using the cProfile module to profile all functions in timeFunctions.py and collecting the stats in the timeStats.profile file. Note that we did not have to add any code to existing module (timeFunctions.py) and this can be done with any module.Once you have the stats file you can run the pstats module as follows:This runs the interactive statistics browser which gives you a lot of nice functionality. For your particular use case you can just check the stats for your function. In our example checking stats for both functions shows us the following:The dummy example does not do much but give you an idea of what can be done. The best part about this approach is that I dont have to edit any of my existing code to get these numbers and obviously help with profiling.Here's another context manager for timing code -Usage: or, if you need the time valuebenchmark.py:Adapted from http://dabeaz.blogspot.fr/2010/02/context-manager-for-timing-benchmarks.htmlUse profiler module. It gives a very detailed profile.it outputs something like:I've found it very informative.(With Ipython only) you can use %timeit to measure average processing time:and then:    the result is something like:I like it simple (python 3):Output is microseconds for a single execution:Explanation:

timeit executes the anonymous function 1 million times by default and the result is given in seconds. Therefore the result for 1 single execution is the same amount but in microseconds on average.For slow operations add a lower number of iterations or you could be waiting forever:Output is always in seconds for the total number of iterations:on python3:elegant and short.One more way to use timeit:Kind of a super later response, but maybe it serves a purpose for someone. This is a way to do it which I think is super clean.Keep in mind that "print" is a function in Python 3 and not Python 2.7. However, it works with any other function. Cheers!We can also convert time into human-readable time.You can use timeit.Here is an example on how to test naive_func that takes parameter using Python REPL:You don't need wrapper function if function doesn't have any parameters.                                                                                      I made a library for this, if you want to measure a function you can just do it like this https://github.com/Karlheinzniebuhr/pythonbenchmark It just takes those 2 lines of code in a Jupyter notebook, and it generates a nice interactive diagram. For example: Here is the code. Again, the 2 lines starting with % are the only extra lines of code needed to use snakeviz: It also seems possible to run snakeviz outside notebooks. More info on the snakeviz website.This unique class-based approach offers a printable string representation, customizable rounding, and convenient access to the elapsed time as a string or a float. It was developed with Python 3.7.Usage:The only way I can think of is using time.time().Hope that will help.The timeit module is good for timing a small piece of Python code. It can be used at least in three forms: 1- As a command-line module2- For a short code, pass it as arguments.3- For longer code as:Time can also be measured by %timeit magic function as follow:n 1 is for running function only 1 time.In addition to %timeit in ipython you can also use %%timeit for multi-line code snippets:Also it can be used in jupyter notebook the same way, just put magic %%timeit at the beginning of cell.

Is there a way to create multiline comments in Python?

Dungeon Hunter

[Is there a way to create multiline comments in Python?](https://stackoverflow.com/questions/7696924/is-there-a-way-to-create-multiline-comments-in-python)

I have recently started studying Python, but I couldn't find how to implement multi-line comments. Most languages have block comment symbols likeI tried this in Python, but it throws an error, so this probably is not the correct way. Does Python actually have a multiline comment feature?

2011-10-08 12:51:13Z

I have recently started studying Python, but I couldn't find how to implement multi-line comments. Most languages have block comment symbols likeI tried this in Python, but it throws an error, so this probably is not the correct way. Does Python actually have a multiline comment feature?You can use triple-quoted strings. When they're not a docstring (the first thing in a class/function/module), they are ignored.(Make sure to indent the leading ''' appropriately to avoid an IndentationError.)Guido van Rossum (creator of Python) tweeted this as a "pro tip".However, Python's style guide, PEP8, favors using consecutive single-line comments, and this is also what you'll find in many projects. Text editors usually have a shortcut to do this easily.Python does have a multiline string/comment syntax in the sense that unless used as docstrings, multiline strings generate no bytecode -- just like #-prepended comments. In effect, it acts exactly like a comment.On the other hand, if you say this behavior must be documented in the official documentation to be a true comment syntax, then yes, you would be right to say it is not guaranteed as part of the language specification.In any case, your text editor should also be able to easily comment-out a selected region (by placing a # in front of each line individually). If not, switch to a text editor that does.Programming in Python without certain text editing features can be a painful experience. Finding the right editor (and knowing how to use it) can make a big difference in how the Python programming experience is perceived.Not only should the text editor be able to comment-out selected regions, it should also be able to shift blocks of code to the left and right easily, and it should automatically place the cursor at the current indentation level when you press Enter. Code folding can also be useful.To protect against link decay, here is the content of Guido van Rossum's tweet:From the accepted answer...This is simply not true. Unlike comments, triple-quoted strings are still parsed and must be syntactically valid, regardless of where they appear in the source code.If you try to run this code...You'll get either......on Python 2.x or......on Python 3.x.The only way to do multi-line comments which are ignored by the parser is...In Python 2.7 the multiline comment is:In case you are inside a class you should tab it properly.For example:AFAIK, Python doesn't have block comments. For commenting individual lines, you can use the # character.If you are using Notepad++, there is a shortcut for block commenting. I'm sure others like gVim and Emacs have similar features.I think it doesn't, except that a multiline string isn't processed. However, most, if not all Python IDEs have a shortkey for 'commenting out' multiple lines of code.If you put a comment inin the middle of a script, Python/linters won't recognize that. Folding will be messed up, as the above comment is not part of the standard recommendations. It's better to useIf you use Vim, you can plugins like commentary.vim, to automatically comment out long lines of comments by pressing Vjgcc. Where Vj selects two lines of code, and gcc comments them out.If you don’t want to use plugins like the above you can use search and replace likeThis will replace the first character on the current and next line with #.There is no such feature as a multi-line comment. # is the only way to comment a single line of code.

Many of you answered ''' a comment ''' this as their solution.It seems to work, but internally ''' in Python takes the lines enclosed as a regular strings which the interpreter does not ignores like comment using #.Check the official documentation hereUnfortunately stringification can not always be used as commenting out! So it is safer to stick to the standard prepending each line with a #.Here is an example:Well, you can try this (when running the quoted, the input to the first question should quoted with '):Whatever enclosed between """ will be commented.If you are looking for single-line comments then it's #.Multiline comment in Python:For me, both ''' and """ worked.Example:Example:The inline comments in Python starts with a hash character.Note that a hash character within a string literal is just a hash character.A hash character can also be used for single or multiple lines comments.Enclose the text with triple double quotes to support docstring.Enclose the text with triple single quotes for block comments.On Python 2.7.13:Single:Multiline:Visual Studio Code universal official multi-line comment toggle.

macOS: Select code-block and then ⌘+/

Windows: Select code-block and then Ctrl+/Yes, it is fine to use both:andBut, the only thing you all need to remember while running in an IDE, is you have to 'RUN' the entire file to be accepted as multiple lines codes. Line by line 'RUN' won't work properly and will show an error.For commenting out multiple lines of code in Python is to simply use a # single-line comment on every line:For writing「proper」multi-line comments in Python is to use multi-line strings with the """ syntax

Python has the documentation strings (or docstrings) feature. It gives programmers an easy way of adding quick notes with every Python module, function, class, and method.Also, mention that you can access docstring by a class object like thisYou can use the following. This is called DockString.I would advise against using """ for multi line comments!Here is a simple example to highlight what might be considered an unexpected behavior:Now have a look at the output:The multi line string was not treated as comment, but it was concatenated with 'clearly I'm also a string' to form a single string.If you want to comment multiple lines do so according to PEP 8 guidelines:Output:Using PyCharm IDE.Select all lines then press Ctrl + /A multiline comment doesn't actually exist in Python. The below example consists of an unassigned string, which is validated by Python for syntactical errors.A few text editors, like Notepad++, provide us shortcuts to comment out a written piece of code or words.Also, Ctrl + K is a shortcut in Notepad++ to block comment. It adds a # in front of every line under the selection. Ctrl + Shift + K is for block uncomment.Among other answers, I find the easiest way is to use the IDE comment functions which use the Python comment support of #.I am using Anaconda Spyder and it has:It would comment/uncomment a single/multi line/s of code with #.I find it the easiest.For example, a block comment:Select the lines that you want to comment and then use Ctrl + ? to comment or uncomment the Python code in the Sublime Text editor.For single line you can use Shift + #.

How do I trim whitespace from a string?

Brian

[How do I trim whitespace from a string?](https://stackoverflow.com/questions/761804/how-do-i-trim-whitespace-from-a-string)

How do I remove leading and trailing whitespace from a string in Python?For example:

2009-04-17 19:16:06Z

How do I remove leading and trailing whitespace from a string in Python?For example:Just one space, or all consecutive spaces?  If the second, then strings already have a .strip() method:If you need only to remove one space however, you could do it with:Also, note that str.strip() removes other whitespace characters as well (e.g. tabs and newlines).  To remove only spaces, you can specify the character to remove as an argument to strip, i.e.:As pointed out in answers above will remove all the leading and trailing whitespace characters such as \n, \r, \t, \f, space.For more flexibility use the followingMore details are available in the docsstrip is not limited to whitespace characters either:This will remove all leading and trailing whitespace in myString:You want strip():I wanted to remove the too-much spaces in a string (also in between the string, not only in the beginning or end). I made this, because I don't know how to do it otherwise:This replaces double spaces in one space until you have no double spaces any moreI could not find a solution to what I was looking for so I created some custom functions. You can try them out.If you want to trim specified number of spaces from left and right, you could do this:

fatal error: Python.h: No such file or directory

Mohanad Y.

[fatal error: Python.h: No such file or directory](https://stackoverflow.com/questions/21530577/fatal-error-python-h-no-such-file-or-directory)

I am trying to build a shared library using a C extension file but first I have to generate the output file using the command below:After executing the command, I get this error message:I have tried all the suggested solutions over the internet but the problem still exists. I have no problem with Python.h. I managed to locate the file on my machine.

2014-02-03 15:00:16Z

I am trying to build a shared library using a C extension file but first I have to generate the output file using the command below:After executing the command, I get this error message:I have tried all the suggested solutions over the internet but the problem still exists. I have no problem with Python.h. I managed to locate the file on my machine.Looks like you haven't properly installed the header files and static libraries for python dev.  Use your package manager to install them system-wide.  For apt (Ubuntu, Debian...):For yum (CentOS, RHEL...):For dnf (Fedora...):For zypper (openSUSE...):For apk (Alpine...):For apt-cyg (Cygwin...):On Ubuntu, I was running Python 3 and I had to install If you want to use a version of Python that is not linked to python3, install the associated python3.x-dev package.  For example:Two things you have to do.Install development package for Python, in case of Debian/Ubuntu/Mint it's done with command:Second thing is that include files are not by default in the include path, nor is Python library linked with executable by default. You need to add these flags (replace Python's version accordingly):In other words your compile command ought to be:If you are using a Raspberry Pi:For Python 3.7 and Ubuntu in particular, I needed . 

I think at some point names were changed from pythonm.n-dev to this. on Fedora run this for Python 2:and for Python 3:If you are using tox to run tests on multiple versions of Python, you may need to install the Python dev libraries for each version of Python you are testing on.You need to install the package python2-devel or python3-devel, depending on the Python version you're using. You can quickly install it using the 32-bit or 64-bit setup.exe (depending on your installation) from Cygwin.com.Example (modify setup.exe's filename and Python's major version if you need):You can also check my other answer for a few more options to install Cygwin's packages from the command-line.In AWS API (centOS) its For me, changing it to this worked:I found the file /usr/include/python2.7/Python.h, and since /usr/include is already in the include path, then python2.7/Python.h should be sufficient.You could also add the include path from command line instead - gcc -I/usr/lib/python2.7 (thanks @erm3nda).Make sure that the Python dev files come with your OS.You should not hard code the library and include paths. Instead, use pkg-config, which will output the correct options for your specific system:You may add it to your gcc line:AWS EC2 install running python34:sudo yum install python34-develIn my case, what fixed it in Ubuntu was to install the packages libpython-all-dev (or libpython3-all-dev if you use Python 3).If you use a virtualenv with a 3.6 python (edge right now), be sure to install the matching python 3.6 dev sudo apt-get install python3.6-dev, otherwise executing sudo python3-dev will install the python dev 3.3.3-1, which won't solve the issue.It's not the same situation, but it also works for me and now I can use SWIG with Python3.5:I was trying to compile:With Python 2.7 works fine, not with my version 3.5:After run in my Ubuntu 16.04 installation:Now I can compile without problems Python3.5:I also encountered this error when I was installing coolprop in ubuntu. For ubuntu 16.04 with python 3.6 If ever this doesn't work try installing/updating gcc lib.try apt-file. It is difficult to remember the package name where the missing file resides. It is generic and useful for any package files.For example:Now you can make an expert guess as to which one to choose from.I managed to solve this issue and generate the .so file in one command For the OpenSuse comrades out there:For CentOS 7:   I followed the instructions here for installing python3.6 on several VMs: https://www.digitalocean.com/community/tutorials/how-to-install-python-3-and-set-up-a-local-programming-environment-on-centos-7

and was then able to build mod_wsgi and get it working with a python3.6 virtualenvIf you're using Python 3.6 on Amazon Linux (based on RHEL, but the RHEL answers given here didn't work):This error occurred when I attempted to install ctds on CentOS 7 with Python3.6. I did all the tricks mentioned here including yum install python34-devel.  The problem was Python.h was found in /usr/include/python3.4m but not in /usr/include/python3.6m.  I tried to use --global-option to point to include dir (pip3.6 install --global-option=build_ext --global-option="--include-dirs=/usr/include/python3.4m" ctds).  This resulted in a lpython3.6m not found when linking ctds.Finally what worked was fixing the development environment for Python3.6 needs to correct with the include and libs.Python.h needs to be in your include path for gcc.  Whichever version of python is used, for example if it's 3.6, then it should be in /usr/include/python3.6m/Python.h typically.Sure python-dev or libpython-all-dev are the first thing to (apt )install, but if that doesn't help as was my case, I advice you to install the foreign Function Interface packages by sudo apt-get install libffi-dev and sudo pip install cffi.This should help out especially if you see the error as/from c/_cffi_backend.c:2:20: fatal error: Python.h: No such file or directory.It often appear when you trying to remove python3.5 and install python3.6.So when using python3 (which python3 -V => python3.6) to install some packages required python3.5 header will appear this error.Resolve by install python3.6-dev module.The python-config program can be named after the Python versions - on Debian, Ubuntu for example these can be named python3-config or python3.6-config.Sometimes even after installing python-dev the error persists,

Check for the error if it is 'gcc' missing.First download as stated in https://stackoverflow.com/a/21530768/8687063, then install gccFor apt (Ubuntu, Debian...):For yum (CentOS, RHEL...):For dnf (Fedora...):For zypper (openSUSE...):For apk (Alpine...):This means that Python.h isn't in your compiler's default include paths. Have you installed it system-wide or locally? What's your OS?You could use the -I<path> flag to specify an additional directory where your compiler should look for headers. You will probably have to follow up with -L<path> so that gcc can find the library you'll be linking with using -l<name>.

Does Django scale? [closed]

Roee Adler

[Does Django scale? [closed]](https://stackoverflow.com/questions/886221/does-django-scale)

I'm building a web application with Django. The reasons I chose Django were:Now that I'm getting closer to thinking about publishing my work, I start being concerned about scale. The only information I found about the scaling capabilities of Django is provided by the Django team (I'm not saying anything to disregard them, but this is clearly not objective information...).My questions:

2009-05-20 05:07:55Z

I'm building a web application with Django. The reasons I chose Django were:Now that I'm getting closer to thinking about publishing my work, I start being concerned about scale. The only information I found about the scaling capabilities of Django is provided by the Django team (I'm not saying anything to disregard them, but this is clearly not objective information...).My questions:There are, of course, many more sites and bloggers of interest, but I have got to stop somewhere!Blog post about Using Django to build high-traffic site michaelmoore.com described as a top 10,000 website.  Quantcast stats and compete.com stats.(*) The author of the edit, including such reference, used to work as outsourced developer in that project.We're doing load testing now.  We think we can support 240 concurrent requests (a sustained rate of 120 hits per second 24x7) without any significant degradation in the server performance.  That would be 432,000 hits per hour.  Response times aren't small (our transactions are large) but there's no degradation from our baseline performance as the load increases.We're using Apache front-ending Django and MySQL.  The OS is Red Hat Enterprise Linux (RHEL).  64-bit.  We use mod_wsgi in daemon mode for Django.  We've done no cache or database optimization other than to accept the defaults.  We're all in one VM on a 64-bit Dell with (I think) 32Gb RAM. Since performance is almost the same for 20 or 200 concurrent users, we don't need to spend huge amounts of time "tweaking".  Instead we simply need to keep our base performance up through ordinary SSL performance improvements, ordinary database design and implementation (indexing, etc.), ordinary firewall performance improvements, etc.What we do measure is our load test laptops struggling under the insane workload of 15 processes running 16 concurrent threads of requests.Not sure about the number of daily visits but here are a few examples of large Django sites:Here is a link to list of high traffic Django sites on Quora.In the US, it was Mahalo. I'm told they handle roughly 10 million uniques a month. Now, in 2019, Mahalo is powered by Ruby on Rails.Abroad, the Globo network (a network of news, sports, and entertainment sites in Brazil); Alexa ranks them in to top 100 globally (around 80th currently).Other notable Django users include PBS, National Geographic, Discovery, NASA (actually a number of different divisions within NASA), and the Library of Congress.Yes -- but only if you've written your application right, and if you've got enough hardware. Django's not a magic bullet.Yes (but see above).Technology-wise, easily: see soclone for one attempt. Traffic-wise, compete pegs StackOverflow at under 1 million uniques per month. I can name at least dozen Django sites with more traffic than SO.Scaling Web apps is not about web frameworks or languages, is about your architecture.

It's about how you handle you browser cache, your database cache, how you use non-standard persistence providers (like CouchDB), how tuned is your database and a lot of other stuff...Playing devil's advocate a little bit:You should check the DjangoCon 2008 Keynote, delivered by Cal Henderson, titled "Why I hate Django" where he pretty much goes over everything Django is missing that you might want to do in a high traffic website. At the end of the day you have to take this all with an open mind because it is perfectly possible to write Django apps that scale, but I thought it was a good presentation and relevant to your question.The largest django site I know of is the Washington Post, which would certainly indicate that it can scale well.Good design decisions probably have a bigger performance impact than anything else. Twitter is often cited as a site which embodies the performance issues with another dynamic interpreted language based web framework, Ruby on Rails - yet Twitter engineers have stated that the framework isn't as much an issue as some of the database design choices they made early on. Django works very nicely with memcached and provides some classes for managing the cache, which is where you would resolve the majority of your performance issues. What you deliver on the wire is almost more important than your backend in reality - using a tool like yslow is critical for a high performance web application. You can always throw more hardware at your backend, but you can't change your users bandwidth.I was at the EuroDjangoCon conference the other week, and this was the subject of a couple of talks - including from the founders of what was the largest Django-based site, Pownce (slides from one talk here). The main message is that it's not Django you have to worry about, but things like proper caching, load balancing, database optimisation, etc.Django actually has hooks for most of those things - caching, in particular, is made very easy.I'm sure you're looking for a more solid answer, but the most obvious objective validation I can think of is that Google pushes Django for use with its App Engine framework. If anybody knows about and deals with scalability on a regular basis, it's Google. From what I've read, the most limiting factor seems to be the database back-end, which is why Google uses their own...As stated in High Performance   Django Book

 and Go through this Cal Henderson See further details as mentioned below:It’s not uncommon to hear people say「Django doesn’t scale」. Depending on how you look at it, the statement is either completely true or patently false. Django, on its own, doesn’t scale.The same can be said of Ruby on Rails, Flask, PHP, or any other language used by a database-driven dynamic website.The good news, however, is that Django interacts beautifully with a suite of caching and

load balancing tools that will allow it to scale to as much traffic as you can throw at it.Contrary to what you may have read online, 

it can do so without replacing core components often labeled as「too slow」such as the database ORM or the template layer.Disqus serves over 8 billion page views per month. Those are some huge numbers. These teams have proven Django most certainly does scale.

 Our experience here at Lincoln Loop backs it up.We’ve built big Django sites capable of spending the day on the Reddit homepage without breaking a sweat.Django’s scaling success stories are almost too numerous to list at this point.It backs Disqus, Instagram, and Pinterest. Want some more proof? Instagram was able to sustain over 30 million users on Django with only 3 engineers (2 of which had no back-end developmentToday we use many web apps and sites for our needs. Most of them are highly useful. I will show you some of them used by python or django.Washington PostThe Washington Post’s website is a hugely popular online news source to accompany their daily paper. Its’ huge amount of views and traffic can be easily handled by the Django web framework.

Washington Post - 52.2 million unique visitors (March, 2015)NASAThe National Aeronautics and Space Administration’s official website is the place to find news, pictures, and videos about their ongoing space exploration. This Django website can easily handle huge amounts of views and traffic.

2 million visitors monthlyThe GuardianThe Guardian is a British news and media website owned by the Guardian Media Group. It contains nearly all of the content of the newspapers The Guardian and The Observer. This huge data is handled by Django.

The Guardian (commenting system) - 41,6 million unique visitors (October, 2014)YouTubeWe all know YouTube as the place to upload cat videos and fails. As one of the most popular websites in existence, it provides us with endless hours of video entertainment. The Python programming language powers it and the features we love.DropBoxDropBox started the online document storing revolution that has become part of daily life. We now store almost everything in the cloud. Dropbox allows us to store, sync, and share almost anything using the power of Python.Survey MonkeySurvey Monkey is the largest online survey company. They can handle over one million responses every day on their rewritten Python website.QuoraQuora is the number one place online to ask a question and receive answers from a community of individuals. On their Python website relevant results are answered, edited, and organized by these community members.BitlyA majority of the code for Bitly URL shortening services and analytics are all built with Python. Their service can handle hundreds of millions of events per day.RedditReddit is known as the front page of the internet. It is the place online to find information or entertainment based on thousands of different categories. Posts and links are user generated and are promoted to the top through votes. Many of Reddit’s capabilities rely on Python for their functionality.HipmunkHipmunk is an online consumer travel site that compares the top travel sites to find you the best deals. This Python website’s tools allow you to find the cheapest hotels and flights for your destination.Click here for more: 

25-of-the-most-popular-python-and-django-websites, 

What-are-some-well-known-sites-running-on-DjangoI think we might as well add Apple's App of the year for 2011, Instagram, to the list which uses django intensively.Yes it can. It could be Django with Python or Ruby on Rails. It will still scale. There are few different techniques. First, caching is not scaling. You could have several application servers balanced with nginx as the front in addition to hardware balancer(s).

To scale on the database side you can go pretty far with read slave in MySQL / PostgreSQL if you go the RDBMS way.Some good examples of heavy traffic websites in Django could be:You can feel safe.Here's a list of some relatively high-profile things built in Django:I imagine a number of these these sites probably gets well over 100k+ hits per day. Django can certainly do 100k hits/day and more. But YMMV in getting your particular site there depending on what you're building.There are caching options at the Django level (for example caching querysets and views in memcached can work wonders) and beyond (upstream caches like Squid). Database Server specifications will also be a factor (and usually the place to splurge), as is how well you've tuned it. Don't assume, for example, that Django's going set up indexes properly. Don't assume that the default PostgreSQL or MySQL configuration is the right one.Furthermore, you always have the option of having multiple application servers running Django if that is the slow point, with a software or hardware load balancer in front.Finally, are you serving static content on the same server as Django? Are you using Apache or something like nginx or lighttpd? Can you afford to use a CDN for static content? These are things to think about, but it's all very speculative. 100k hits/day isn't the only variable: how much do you want to spend? How much expertise do you have managing all these components? How much time do you have to pull it all together?The developer advocate for YouTube gave a talk about scaling Python at PyCon 2012, which is also relevant to scaling Django.YouTube has more than a billion users, and YouTube is built on Python.I have been using Django for over a year now, and am very impressed with how it manages to combine modularity, scalability and speed of development. Like with any technology, it comes with a learning curve. However, this learning curve is made a lot less steep by the excellent documentation from the Django community. Django has been able to handle everything I have thrown at it really well. It looks like it will be able to scale well into the future.BidRodeo Penny Auctions is a moderately sized Django powered website. It is a very dynamic website and does handle a good number of page views a day. Note that if you're expecting 100K users per day, that are active for hours at a time (meaning max of 20K+ concurrent users), you're going to need A LOT of servers.  SO has ~15,000 registered users, and most of them are probably not active daily.  While the bulk of traffic comes from unregistered users, I'm guessing that very few of them stay on the site more than a couple minutes (i.e. they follow google search results then leave).  For that volume, expect at least 30 servers ... which is still a rather heavy 1,000 concurrent users per server.What's the "largest" site that's built on Django today? (I measure size mostly by user traffic)

Pinterest

disqus.com

More here: https://www.shuup.com/en/blog/25-of-the-most-popular-python-and-django-websites/Can Django deal with 100,000 users daily, each visiting the site for a couple of hours?

Yes but use proper architecture, database design, use of cache, use load balances and multiple servers or nodesCould a site like Stack Overflow run on Django?

Yes just need to follow the answer mentioned in the 2nd questionAnother example is rasp.yandex.ru, Russian transport timetable service. Its attendance satisfies your requirements.If you have a site with some static content, then putting a Varnish server in front will dramatically increase your performance. Even a single box can then easily spit out 100 Mbit/s of traffic.Note that with dynamic content, using something like Varnish becomes a lot more tricky.My experience with Django is minimal but I do remember in The Django Book they have a chapter where they interview people running some of the larger Django applications.  Here is a link.  I guess it could provide some insights.It says curse.com is one of the largest Django applications with around 60-90 million page views in a month.I develop high traffic sites using Django for the national broadcaster in Ireland. It works well for us. Developing a high performance site is more than about just choosing a framework. A framework will only be one part of a system that is as strong as it's weakest link. Using the latest framework 'X' won't solve your performance issues if the problem is slow database queries or a badly configured server or network.Even-though there have been a lot of great answers here, I just feel like pointing out, that nobody have put emphasis on..It depends on the application If you application is light on writes, as in you are reading a lot more data from the DB than you are writing. Then scaling django should be fairly trivial, heck, it comes with some fairly decent output/view caching straight out of the box. Make use of that, and say, redis as a cache provider, put a load balancer in front of it, spin up n-instances and you should be able to deal with a VERY large amount of traffic.Now, if you have to do thousands of complex writes a second? Different story. Is Django going to be a bad choice? Well, not necessarily, depends on how you architect your solution really, and also, what your requirements are.Just my two cents :-)You can definitely run a high-traffic site in Django. Check out this pre-Django 1.0 but still relevant post here: http://menendez.com/blog/launching-high-performance-django-site/Check out this micro news aggregator called EveryBlock.It's entirely written in Django. In fact they are the people who developed the Django  framework itself.The problem is not to know if django can scale or not. The right way is to understand and know which are the network design patterns and tools to put under your django/symfony/rails project to scale well.Some ideas can be :Hope it help a bit. This is my tiny rock to the mountain.If you want to use Open source then there are many options for you. But python is best among them as it has many libraries and a super awesome community.

These are a few reasons which might change your mind:Conclusion is a framework or language won't do everything for you. A better architecture, designing and strategy will give you a scalable website. Instagram is the biggest example, this small team is managing such huge data. Here is one blog about its architecture must read it.I don't think the issue is really about Django scaling. I really suggest you look into your architecture that's what will help you with your scaling needs.If you get that wrong there is no point on how well Django performs. Performance != Scale. You can have a system that has amazing performance but does not scale and vice versa.Is your application database bound? If it is then your scale issues lay there as well. How are you planning on interacting with the database from Django? What happens when you database cannot process requests as fast as Django accepts them? What happens when your data outgrows one physical machine. You need to account for how you plan on dealing with those circumstances.Moreover, What happens when your traffic outgrows one app server? how you handle sessions in this case can be tricky, more often than not you would probably require a shared nothing architecture. Again that depends on your application.In short languages is not what determines scale, a language is responsible for performance(again depending on your applications, different languages perform differently). It is your design and architecture that makes scaling a reality. I hope it helps, would be glad to help further if you have questions.Spreading the tasks evenly, in short optimizing each and every aspect including DBs, Files, Images, CSS etc. and balancing the load with several other resources is necessary once your site/application starts growing. OR you make some more space for it to grow. Implementation of latest technologies like CDN, Cloud are must with huge sites. Just developing and tweaking an application won't give your the cent percent satisfation, other components also play an important role.

In Python, how do I determine if an object is iterable?

willem

[In Python, how do I determine if an object is iterable?](https://stackoverflow.com/questions/1952464/in-python-how-do-i-determine-if-an-object-is-iterable)

Is there a method like isiterable? The only solution I have found so far is to callBut I am not sure how fool-proof this is.

2009-12-23 12:13:55Z

Is there a method like isiterable? The only solution I have found so far is to callBut I am not sure how fool-proof this is.Use the Abstract Base Classes. They need at least Python 2.6 and work only for new-style classes.However, iter() is a bit more reliable as described by the documentation:I'd like to shed a little bit more light on the interplay of iter, __iter__ and __getitem__ and what happens behind the curtains. Armed with that knowledge, you will be able to understand why the best you can do isI will list the facts first and then follow up with a quick reminder of what happens when you employ a for loop in python, followed by a discussion to illustrate the facts.In order to follow along, you need an understanding of what happens when you employ a for loop in Python. Feel free to skip right to the next section if you already know.When you use for item in o for some iterable object o, Python calls iter(o) and expects an iterator object as the return value. An iterator is any object which implements a __next__ (or next in Python 2) method and an __iter__ method. By convention, the __iter__ method of an iterator should return the object itself (i.e. return self). Python then calls next on the iterator until StopIteration is raised. All of this happens implicitly, but the following demonstration makes it visible:Iteration over a DemoIterable:On point 1 and 2: getting an iterator and unreliable checksConsider the following class:Calling iter with an instance of BasicIterable will return an iterator without any problems because BasicIterable implements __getitem__.However, it is important to note that b does not have the __iter__ attribute and is not considered an instance of Iterable or Sequence:This is why Fluent Python by Luciano Ramalho recommends calling iter and handling the potential TypeError as the most accurate way to check whether an object is iterable. Quoting directly from the book:On point 3: Iterating over objects which only provide __getitem__, but not __iter__Iterating over an instance of BasicIterable works as expected: Python

constructs an iterator that tries to fetch items by index, starting at zero, until an IndexError is raised. The demo object's __getitem__ method simply returns the item which was supplied as the argument to __getitem__(self, item) by the iterator returned by iter.Note that the iterator raises StopIteration when it cannot return the next item and that the IndexError which is raised for item == 3 is handled internally. This is why looping over a BasicIterable with a for loop works as expected:Here's another example in order to drive home the concept of how the iterator returned by iter tries to access items by index. WrappedDict does not inherit from dict, which means instances won't have an __iter__ method.Note that calls to __getitem__ are delegated to dict.__getitem__ for which the square bracket notation is simply a shorthand.On point 4 and 5: iter checks for an iterator when it calls __iter__:When iter(o) is called for an object o, iter will make sure that the return value of __iter__, if the method is present, is an iterator. This means that the returned object

must implement __next__ (or next in Python 2) and __iter__. iter cannot perform any sanity checks for objects which only

provide __getitem__, because it has no way to check whether the items of the object are accessible by integer index.Note that constructing an iterator from FailIterIterable instances fails immediately, while constructing an iterator from FailGetItemIterable succeeds, but will throw an Exception on the first call to __next__.On point 6: __iter__ winsThis one is straightforward. If an object implements __iter__ and __getitem__, iter will call __iter__. Consider the following classand the output when looping over an instance:On point 7: your iterable classes should implement __iter__You might ask yourself why most builtin sequences like list implement an __iter__ method when __getitem__ would be sufficient.After all, iteration over instances of the class above, which delegates calls to __getitem__ to list.__getitem__ (using the square bracket notation), will work fine:The reasons your custom iterables should implement __iter__ are as follows:This isn't sufficient: the object returned by __iter__ must implement the iteration protocol (i.e. next method). See the relevant section in the documentation.In Python, a good practice is to "try and see" instead of "checking".In Python <= 2.5, you can't and shouldn't - iterable was an "informal" interface.But since Python 2.6 and 3.0 you can leverage the new ABC (abstract base class) infrastructure along with some builtin ABCs which are available in the collections module:Now, whether this is desirable or actually works, is just a matter of conventions. As you can see, you can register a non-iterable object as Iterable - and it will raise an exception at runtime. Hence, isinstance acquires a "new" meaning - it just checks for "declared" type compatibility, which is a good way to go in Python.On the other hand, if your object does not satisfy the interface you need, what are you going to do? Take the following example:If the object doesn't satisfy what you expect, you just throw a TypeError, but if the proper ABC has been registered, your check is unuseful. On the contrary, if the __iter__ method is available Python will automatically recognize object of that class as being Iterable.So, if you just expect an iterable, iterate over it and forget it. On the other hand, if you need to do different things depending on input type, you might find the ABC infrastructure pretty useful.Don't run checks to see if your duck really is a duck to see if it is iterable or not, treat it as if it was and complain if it wasn't.Since Python 3.5 you can use the typing module from the standard library for type related things:The best solution I've found so far:hasattr(obj, '__contains__')which basically checks if the object implements the in operator.Advantages (none of the other solutions has all three):Notes: You could try this:If we can make a generator that iterates over it (but never use the generator so it doesn't take up space), it's iterable. Seems like a "duh" kind of thing. Why do you need to determine if a variable is iterable in the first place?I found a nice solution here:According to the Python 2 Glossary, iterables areOf course, given the general coding style for Python based on the fact that it's「Easier to ask for forgiveness than permission.」, the general expectation is to useBut if you need to check it explicitly, you can test for an iterable by hasattr(object_in_question, "__iter__") or hasattr(object_in_question, "__getitem__"). You need to check for both, because strs don't have an __iter__ method (at least not in Python 2, in Python 3 they do) and because generator objects don't have a __getitem__ method.I often find convenient, inside my scripts, to define an iterable function.

(Now incorporates Alfe's suggested simplification):so you can test if any object is iterable in the very readable formas you would do with thecallable functionEDIT: if you have numpy installed, you can simply do: from numpy import iterable, 

which is simply something likeIf you do not have numpy, you can simply implement this code, or the one above.pandas has a built-in function like that:This will say yes to all manner of iterable objects, but it will say no to strings in Python 2. (That's what I want for example when a recursive function could take a string or a container of strings.  In that situation, asking forgiveness may lead to obfuscode, and it's better to ask permission first.)Many other strategies here will say yes to strings. Use them if that's what you want.Note: is_iterable() will say yes to strings of type bytes and bytearray.The O.P. hasattr(x, '__iter__') approach will say yes to strings in Python 3 and no in Python 2 (no matter whether '' or b'' or u''). Thanks to @LuisMasuelli for noticing it will also let you down on a buggy __iter__.It's always eluded me as to why python has callable(obj) -> bool but not iterable(obj) -> bool...

surely it's easier to do hasattr(obj,'__call__') even if it is slower.Since just about every other answer recommends using try/except TypeError, where testing for exceptions is generally considered bad practice among any language, here's an implementation of iterable(obj) -> bool I've grown more fond of and use often:For python 2's sake, I'll use a lambda just for that extra performance boost...

(in python 3 it doesn't matter what you use for defining the function, def has roughly the same speed as lambda)Note that this function executes faster for objects with __iter__ since it doesn't test for __getitem__.Most iterable objects should rely on __iter__ where special-case objects fall back to __getitem__, though either is required for an object to be iterable.

(and since this is standard, it affects C objects as well)The easiest way, respecting the Python's duck typing, is to catch the error (Python knows perfectly what does it expect from an object to become an iterator):Notes:The isiterable func at the following code returns True if object is iterable. if it's not iterable returns FalseexampleInstead of checking for the __iter__ attribute, you could check for the __len__ attribute, which is implemented by every python builtin iterable, including strings. None-iterable objects would not implement this for obvious reasons. However, it does not catch user-defined iterables that do not implement it, nor do generator expressions, which iter can deal with. However, this can be done in a line, and adding a simple or expression checking for generators would fix this problem. (Note that writing type(my_generator_expression) == generator would throw a NameError. Refer to this answer instead.)(This makes it useful for checking if you can call len on the object though.)Kinda late to the party but I asked myself this question and saw this then thought of an answer. I don't know if someone already posted this. But essentially, I've noticed that all iterable types have "getitem" in their dict. This is how you would check if an object was an iterable without even trying. (Pun intended)Not really "correct" but can serve as quick check of most common types like strings, tuples, floats, etc...

Find all files in a directory with extension .txt in Python

usertest

[Find all files in a directory with extension .txt in Python](https://stackoverflow.com/questions/3964681/find-all-files-in-a-directory-with-extension-txt-in-python)

How can I find all the files in a directory having the extension .txt in python?

2010-10-19 01:09:13Z

How can I find all the files in a directory having the extension .txt in python?You can use glob:or simply os.listdir:or if you want to traverse directory, use os.walk:Use glob.Something like that should do the jobSomething like this will work: You can simply use pathlibs glob 1:or in a loop:If you want it recursive you can use .glob('**/*.txt)1The pathlib module was included in the standard library in python 3.4. But you can install back-ports of that module even on older Python versions (i.e. using conda or pip): pathlib and pathlib2.I like os.walk():Or with generators:Here's more versions of the same that produce slightly different results:path.py is another alternative: https://github.com/jaraco/path.pyFast method using os.scandir in a recursive function.  Searches for all files with a specified extension in folder and sub-folders.If you are searching over directories which contain 10,000s files, appending to a list becomes inefficient.  'Yielding' the results is a better solution.  I have also included a function to convert the output to a Pandas Dataframe.Python has all tools to do this: To get all '.txt' file names inside 'dataPath' folder as a list in a Pythonic wayTry this this will find all your files recursively:I did a test (Python 3.6.4, W7x64) to see which solution is the fastest for one folder, no subdirectories, to get a list of complete file paths for files with a specific extension.To make it short, for this task os.listdir() is the fastest and is 1.7x as fast as the next best: os.walk() (with a break!), 2.7x as fast as pathlib, 3.2x faster than os.scandir() and 3.3x faster than glob.

Please keep in mind, that those results will change when you need recursive results. If you copy/paste one method below, please add a .lower() otherwise .EXT would not be found when searching for .ext.Results:This code makes my life simpler.Use fnmatch: https://docs.python.org/2/library/fnmatch.htmlTo get an array of ".txt" file names from a folder called "data" in the same directory I usually use this simple line of code:I suggest you to use fnmatch and the upper method. In this way you can find any of the following:.Here's one with extend()Functional solution with sub-directories:In case the folder contains a lot of files or memory is an constraint, consider using generators:Option A: IterateOption B: Get allA copy-pastable solution similar to the one of ghostdog:use Python OS module to find files with specific extension.the simple example is here :Many users have replied with os.walk answers, which includes all files but also all directories and subdirectories and their files.Or for a one off where you don't need a generator:If you are going to use matches for something else, you may want to make it a list rather than a generator expression:A simple method by using for loop  : Though this can be made more generalised .

How to install packages using pip according to the requirements.txt file from a local directory?

kakarukeys

[How to install packages using pip according to the requirements.txt file from a local directory?](https://stackoverflow.com/questions/7225900/how-to-install-packages-using-pip-according-to-the-requirements-txt-file-from-a)

Here is the problemI have a requirements.txt that looks like:I have a local archive directory containing all the packages + others.I have created a new virtualenv withupon activating it, I tried to install the packages according to requirements.txt from the local archive directory.I got some output that seems to indicate that the installation is fineBut later check revealed none of the package is installed properly. I cannot import the package, and none is found in the site-packages directory of my virtualenv. So what went wrong?

2011-08-29 03:53:04Z

Here is the problemI have a requirements.txt that looks like:I have a local archive directory containing all the packages + others.I have created a new virtualenv withupon activating it, I tried to install the packages according to requirements.txt from the local archive directory.I got some output that seems to indicate that the installation is fineBut later check revealed none of the package is installed properly. I cannot import the package, and none is found in the site-packages directory of my virtualenv. So what went wrong?This works for me:--no-index - Ignore package index (only looking at --find-links URLs instead). -f, --find-links <URL> - If a URL or path to an html file, then parse for links to archives. 

If a local path or file:// URL that's a directory, then look for archives in the directory listing.I've read the above, realize this is an old question, but it's totally unresolved and still at the top of my google search results so here's an answer that works for everyone:For virtualenv to install all files in the requirements.txt file.I had a similar problem. I tried this:(-U = update if it had already installed)But the problem continued. I realized that some of generic libraries for development were missed.I don't know if this would help you.For further details please check the help option.We can find the option '-r'Further information on some commonly used pip install options: (This is the help option on pip install command)Also the above is the complete set of options. Please use pip install --help for complete list of options.Short answeror in another form:ExplanationHere, -r is short form of --requirement and it asks the pip to install from the given requirements file.pip will start installation only after checking the availability of all listed items in the requirements file and it won't start installation even if one requirement is unavailable.One workaround to install the available packages is installing listed packages one by one. Use the following command for that. A red color warning will be shown to notify you about the unavailable packages.To ignore comments (lines starting with a #) and blank lines, use:Often, you will want a fast install from local archives, without probing PyPI.First, download the archives that fulfill your requirements:Then, install using –find-links and –no-index:I work with a lot of systems that have been mucked by developers "following directions they found on the internet". It is extremely common that your pip and your python are not looking at the same paths/site-packages. For this reason, when I encounter oddness I start by doing this:That is a happy system.Below is an unhappy system. (Or at least it's a blissfully ignorant system that causes others to be unhappy.)It is unhappy because pip is (python3.6 and) using /usr/local/lib/python3.6/site-packages while python is (python2.7 and) using /usr/local/lib/python2.7/site-packagesWhen I want to make sure I'm installing requirements to the right python, I do this:You've heard, "If it ain't broke, don't try to fix it." The DevOps version of that is, "If you didn't break it and you can work around it, don't try to fix it."Installing requirements.txt file inside virtual env with python 3:I had the same issue. I was trying to install requirements.txt file inside a virtual environament. I found the solution.Initially, I created my virtual env in this way:Activate the environment using:Now I installed the requirements.txt using:Installation was successful and I was able to import the modules.try thisfirst of all, create a virtual environmentin python 3.6in python 2.7then activate the environment and install all the packages available in the requirement.txt file.OR

Terminating a Python script

Teifion

[Terminating a Python script](https://stackoverflow.com/questions/73663/terminating-a-python-script)

I am aware of the die() command in PHP which stops a script early.How can I do this in Python?

2008-09-16 15:35:55Z

I am aware of the die() command in PHP which stops a script early.How can I do this in Python?details from the sys module documentation:Note that this is the 'nice' way to exit.  @glyphtwistedmatrix below points out that if you want a 'hard exit', you can use os._exit(errorcode), though it's likely os-specific to some extent (it might not take an errorcode under windows, for example), and it definitely is less friendly since it doesn't let the interpreter do any cleanup before the process dies.A simple way to terminate a Python script early is to use the built-in function quit(). There is no need to import any library, and it is efficient and simple.Example:Another way is:You can also use simply exit().Keep in mind that sys.exit(), exit(), quit(), and os._exit(0) kill the Python interpreter. Therefore, if it appears in a script called from another script by execfile(), it stops execution of both scripts. See "Stop execution of a script called with execfile" to avoid this.While you should generally prefer sys.exit because it is more "friendly" to other code, all it actually does is raise an exception.If you are sure that you need to exit a process immediately, and you might be inside of some exception handler which would catch SystemExit, there is another function - os._exit - which terminates immediately at the C level and does not perform any of the normal tear-down of the interpreter; for example, hooks registered with the "atexit" module are not executed.I've just found out that when writing a multithreadded app, raise SystemExit and sys.exit() both kills only the running thread. On the other hand, os._exit() exits the whole process. This was discussed here.The example below has 2 threads. Kenny and Cartman. Cartman is supposed to live forever, but Kenny is called recursively and should die after 3 seconds. (recursive calling is not the best way, but I had other reasons)If we also want Cartman to die when Kenny dies, Kenny should go away with os._exit, otherwise, only Kenny will die and Cartman will live forever.As a parameter you can pass an exit code, which will be returned to OS. Default is 0.I'm a total novice but surely this is cleaner and more controlled...than...EditThe point being that the program ends smoothly and peacefully, rather than "I'VE STOPPED !!!!"In Python 3.5, I tried to incorporate similar code without use of modules (e.g. sys, Biopy) other than what's built-in to stop the script and print an error message to my users. Here's my example:Later on, I found it is more succinct to just throw an error:There are a few standard ways to do it:I was dissatisfied that (on repl.it at least,) none of these completely shut down the interpreter; it could always run code in the repl, even after exit, etc. the only way I could think of that always stops the interpreter from proceeding is by simulating a system exit by sleeping.This is obviously not recommended.

Correct way to write line to file?

Yaroslav Bulatov

[Correct way to write line to file?](https://stackoverflow.com/questions/6159900/correct-way-to-write-line-to-file)

I'm used to doing print >>f, "hi there"However, it seems that print >> is getting deprecated. What is the recommended way to do the line above?Update:

Regarding all those answers with "\n"...is this universal or Unix-specific? IE, should I be doing "\r\n" on Windows?

2011-05-28 05:44:53Z

I'm used to doing print >>f, "hi there"However, it seems that print >> is getting deprecated. What is the recommended way to do the line above?Update:

Regarding all those answers with "\n"...is this universal or Unix-specific? IE, should I be doing "\r\n" on Windows?This should be as simple as:From The Documentation:Some useful reading:You should use the print() function which is available since Python 2.6+For Python 3 you don't need the import, since the  print() function is the default.The alternative would be to use:Quoting from Python documentation regarding newlines:The python docs recommend this way:So this is the way I usually do it :)Statement from docs.python.org:Regarding os.linesep:Here is an exact unedited Python 2.7.1 interpreter session on Windows:On Windows:As expected, os.linesep does NOT produce the same outcome as '\n'. There is no way that it could produce the same outcome. 'hi there' + os.linesep is equivalent to 'hi there\r\n', which is NOT equivalent to 'hi there\n'.It's this simple: use \n which will be translated automatically to os.linesep. And it's been that simple ever since the first port of Python to Windows.There is no point in using os.linesep on non-Windows systems, and it produces wrong results on Windows.DO NOT USE os.linesep!I do not think there is a "correct" way.I would use:In memoriam Tim Toady.In Python 3 it is a function, but in Python 2 you can add this to the top of the source file:Then you do If you are writing a lot of data and speed is a concern you should probably go with f.write(...). I did a quick speed comparison and it was considerably faster than print(..., file=f) when performing a large number of writes.On average write finished in 2.45s on my machine, whereas print took about 4 times as long (9.76s). That being said, in most real-world scenarios this will not be an issue.If you choose to go with print(..., file=f) you will probably find that you'll want to suppress the newline from time to time, or replace it with something else. This can be done by setting the optional end parameter, e.g.;Whichever way you choose I'd suggest using with since it makes the code much easier to read.Update: This difference in performance is explained by the fact that write is highly buffered and returns before any writes to disk actually take place (see this answer), whereas print (probably) uses line buffering. A simple test for this would be to check performance for long writes as well, where the disadvantages (in terms of speed) for line buffering would be less pronounced.The performance difference now becomes much less pronounced, with an average time of 2.20s for write and 3.10s for print. If you need to concatenate a bunch of strings to get this loooong line performance will suffer, so use-cases where print would be more efficient are a bit rare.Since 3.5 you can also use the pathlib for that purpose:When you said Line it means some serialized characters which are ended to '\n' characters. Line should be last at some point so we should consider '\n' at the end of each line. Here is solution:in append mode after each write the cursor move to new line, if you want to use 'w' mode you should add '\n' characters at the end of write() function:One can also use the io module as in:You can also try filewriterpip install filewriterWrites into my_file.txtTakes an iterable or an object with __str__ support. When I need to write new lines a lot, I define a lambda that uses a print function:This approach has the benefit that it can utilize all the features that are available with the print function.Update: As is mentioned by Georgy in the comment section, it is possible to improve this idea further with the partial function: IMHO, this is a more functional and less cryptic approach. To write text in a file in the flask can be used:

How do I parse XML in Python?

randombits

[How do I parse XML in Python?](https://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python)

I have many rows in a database that contains XML and I'm trying to write a Python script to count instances of a particular node attribute.My tree looks like:How can I access the attributes "1" and "2" in the XML using Python?

2009-12-16 05:09:24Z

I have many rows in a database that contains XML and I'm trying to write a Python script to count instances of a particular node attribute.My tree looks like:How can I access the attributes "1" and "2" in the XML using Python?I suggest ElementTree.  There are other compatible implementations of the same API, such as lxml, and cElementTree in the Python standard library itself; but, in this context, what they chiefly add is even more speed -- the ease of programming part depends on the API, which ElementTree defines.First build an Element instance root from the XML, e.g. with the XML function, or by parsing a file with something like:Or any of the many other ways shown at ElementTree. Then do something like:And similar, usually pretty simple, code patterns.minidom is the quickest and pretty straight forward.XML:Python:Output:You can use BeautifulSoup:There are many options out there. cElementTree looks excellent if speed and memory usage are an issue. It has very little overhead compared to simply reading in the file using readlines.The relevant metrics can be found in the table below, copied from the cElementTree website:As pointed out by @jfs, cElementTree comes bundled with Python:I suggest xmltodict for simplicity.It parses your XML to an OrderedDict;lxml.objectify is really simple.Taking your sample text:Output:Python has an interface to the expat XML parser.It's a non-validating parser, so bad XML will not be caught. But if you know your file is correct, then this is pretty good, and you'll probably get the exact info you want and you can discard the rest on the fly.I might suggest declxml.Full disclosure: I wrote this library because I was looking for a way to convert between XML and Python data structures without needing to write dozens of lines of imperative parsing/serialization code with ElementTree.With declxml, you use processors to declaratively define the structure of your XML document and how to map between XML and Python data structures. Processors are used to for both serialization and parsing as well as for a basic level of validation.Parsing into Python data structures is straightforward:Which produces the output:You can also use the same processor to serialize data to XMLWhich produces the following outputIf you want to work with objects instead of dictionaries, you can define processors to transform data to and from objects as well.Which produces the following outputJust to add another possibility, you can use untangle, as it is a simple xml-to-python-object library. Here you have an example:Installation:Usage:Your XML file (a little bit changed):Accessing the attributes with untangle:The output will be:More information about untangle can be found in "untangle".Also, if you are curious, you can find a list of tools for working with XML and Python in "Python and XML". You will also see that the most common ones were mentioned by previous answers.Here a very simple but effective code using cElementTree. This is from "python xml parse".XML:Python code:Output:I find the Python xml.dom and xml.dom.minidom quite easy. Keep in mind that DOM isn't good for large amounts of XML, but if your input is fairly small then this will work fine.These are some pros of the two most used libraries I would have benefit to know before choosing between them.This will print the value of the foobar attribute.There's no need to use a lib specific API if you use python-benedict. Just initialize a new instance from your XML and manage it easily since it is a dict subclass.Installation is easy: pip install python-benedictIt supports and normalizes I/O operations with many formats: Base64, CSV, JSON, TOML, XML, YAML and query-string.It is well tested and open-source on GitHub.If the source is an xml file, say like this sampleyou may try the following codeOutput would be

How can I get a list of locally installed Python modules?

Léo Léopold Hertz 준영

[How can I get a list of locally installed Python modules?](https://stackoverflow.com/questions/739993/how-can-i-get-a-list-of-locally-installed-python-modules)

I would like to get a list of Python modules, which are in my Python installation (UNIX server).How can you get a list of Python modules installed in your computer?

2009-04-11 12:34:18Z

I would like to get a list of Python modules, which are in my Python installation (UNIX server).How can you get a list of Python modules installed in your computer?My 50 cents for getting a pip freeze-like list from a Python script:As a (too long) one liner:Giving:This solution applies to the system scope or to a virtual environment scope, and covers packages installed by setuptools, pip and (god forbid) easy_install.I added the result of this call to my flask server, so when I call it with http://example.com/exampleServer/environment I get the list of packages installed on the server's virtualenv. It makes debugging a whole lot easier.I have noticed a strange behaviour of this technique - when the Python interpreter is invoked in the same directory as a setup.py file, it does not list the package installed by setup.py.We have behave's setup.py in /tmp/behave:behave==1.2.5a1 is missing from the second example, because the working directory contains behave's setup.py file.I could not find any reference to this issue in the documentation. Perhaps I shall open a bug for it.in a Python shell/prompt.Now, these methods I tried myself, and I got exactly what was advertised:  All the modules.Alas, really you don't care much about the stdlib, you know what you get with a python install.  Really, I want the stuff that I installed. What actually, surprisingly, worked just fine was:Which returned:I say "surprisingly" because the package install tool is the exact place one would expect to find this functionality, although not under the name 'freeze' but python packaging is so weird, that I am flabbergasted that this tool makes sense.  Pip 0.8.2, Python 2.7.  Since pip version 1.3, you've got access to:Which seems to be syntactic sugar for "pip freeze".  It will list all of the modules particular to your installation or virtualenv, along with their version numbers.  Unfortunately it does not display the current version number of any module, nor does it wash your dishes or shine your shoes.I just use this to see currently used modules:which shows all modules running on your python.For all built-in modules use:Which is a dict containing all modules and import objects.In normal shell just useAs of pip 10, the accepted answer will no longer work.  The development team has removed access to the get_installed_distributions routine.  There is an alternate function in the setuptools for doing the same thing.  Here is an alternate version that works with pip 10:Please let me know if it will or won't work in previous versions of pip, too.If we need to list the installed packages in the Python shell, we can use the help command as followsI normally use pip list to get a list of packages (with version).This works in a virtual environment too, of course. To show what's installed in only the virtual environment (not global packages), use pip list --local.Here's documentation showing all the available pip list options, with several good examples.on windows, Enter this in cmdVery simple searching using pkgutil.iter_modulesI ran into a custom installed python 2.7 on OS X. It required X11 to list modules installed (both using help and pydoc).To be able to list all modules without installing X11 I ran pydoc as http-server, i.e.:Then it's possible to direct Safari to http://localhost:12345/ to see all modules.Try theseorIn terminal or IPython, type:thenWarning: Adam Matan discourages this use in pip > 10.0. Also, read @sinoroc's comment belowThis was inspired by Adam Matan's answer (the accepted one):which then prints out a table in the form ofwhich lets you then easily discern which packages you installed with and without sudo.A note aside: I've noticed that when I install a packet once via sudo and once without, one takes precedence so that the other one isn't being listed (only one location is shown). I believe that only the one in the local directory is then listed. This could be improved.Aside from using pip freeze I have been installing yolk in my virtual environments.This solution is primary based on modules importlib and pkgutil and work with CPython 3.4 and CPython 3.5, but has no support for the CPython 2.ExplanationFull codeUsageFor the CPython3.5 (truncated)For the CPython3.4 (truncated)In case you have an anaconda python distribution installed, you could also usein addition to solutions described above.For the second purpose, example code:pip freeze does it all finding packages however one can simply write the following command to list all paths where python packages are.There are many way to skin a cat.There are many ideas, initially I am pondering on these two:piphelp('modules')I need an easy approach, using basic libraries and compatible with old python 2.xAnd I see the light: listmodules.pyHidden in the documentation source directory in 2.5 is a small script that lists all available modules for a Python installation.Pros:I needed to find the specific version of packages available by default in AWS Lambda. I did so with a mashup of ideas from this page. I'm sharing it for posterity.What I discovered is that the provided boto3 library was way out of date and it wasn't my fault that my code was failing. I just needed to add boto3 and botocore to my project. But without this I would have been banging my head thinking my code was bad.What I discovered was also different from what they officially publish. At the time of writing this:InstallationCodeSample Output:Here is a python code solution that will return a list of modules installed. One can easily modify the code to include version numbers.For anyone wondering how to call pip list from a Python program you can use the following:From the shellIf that's not helpful, you can do this.And see what that produces.

「Large data」work flows using pandas

Zelazny7

[「Large data」work flows using pandas](https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas)

I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I'm not talking about "big data" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.My first thought is to use HDFStore to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:What are some best-practice workflows for accomplishing the following:Real-world examples would be much appreciated, especially from anyone who uses pandas on "large data".Edit -- an example of how I would like this to work:I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.Edit -- Responding to Jeff's questions specifically:It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).

2013-01-10 16:20:32Z

I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I'm not talking about "big data" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.My first thought is to use HDFStore to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:What are some best-practice workflows for accomplishing the following:Real-world examples would be much appreciated, especially from anyone who uses pandas on "large data".Edit -- an example of how I would like this to work:I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.Edit -- Responding to Jeff's questions specifically:It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).I routinely use tens of gigabytes of data in just this fashion

e.g. I have tables on disk that I read via queries, create data and append back.It's worth reading the docs and late in this thread for several suggestions for how to store your data.Details which will affect how you store your data, like:

Give as much detail as you can; and I can help you develop a structure.Ensure you have pandas at least 0.10.1 installed.Read iterating files chunk-by-chunk and multiple table queries.Since pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it's easy to select a small group of fields (which will work with a big table, but it's more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):

(The following is pseudocode.)Reading in the files and creating the storage (essentially doing what append_to_multiple does):Now you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn't necessary).This is how you get columns and create new ones:When you are ready for post_processing:About data_columns, you don't actually need to define ANY data_columns; they allow you to sub-select rows based on the column. E.g. something like:They may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).You also might want to:Let me know when you have questions!I think the answers above are missing a simple approach that I've found very useful. When I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)Example: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the endOne of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)The other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formatsThis approach doesn't cover all scenarios, but is very useful in a lot of themThere is now, two years after the question, an 'out-of-core' pandas equivalent: dask. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you're looking for here, but doing scientific computing on a notebook with 4GB of RAM isn't reasonable.I know this is an old thread but I think the Blaze library is worth checking out.  It's built for these types of situations.From the docs:Blaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.Edit: By the way, it's supported by ContinuumIO and Travis Oliphant, author of NumPy.This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (dict of attributes).  Many people form a collection and you can have many collections (people, stock market, income).pd.dateframe -> pymongo Note: I use the chunksize in read_csv to keep it to 5 to 10k records(pymongo drops the socket if larger)querying: gt = greater than....find() returns an iterator so I commonly use ichunked to chop into smaller iterators.  How about a join since I normally get 10 data sources to paste together:then (in my case sometimes I have to agg on aJoinDF first before its "mergeable".)And you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).On smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a dict lookup as you create documents.Now you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...You can also use the two methods built into MongoDB (MapReduce and aggregate framework). See here for more info about the aggregate framework, as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn't need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.My basic workflow is to first get a CSV file from the database. I gzip it, so it's not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn't use any memory, since it's only operating row-by-row. Then I "transpose" the row-oriented HDF5 file into a column-oriented HDF5 file.The table transpose looks like:Reading it back in then looks like:Now, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.This generally works for me, but it's a bit clunky, and I can't use the fancy pytables magic.Edit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can't handle tables. Or, at least, I've been unable to get it to load heterogeneous tables.One trick I found helpful for large data use cases is to reduce the volume of the data by reducing float precision to 32-bit. It's not applicable in all cases, but in many applications 64-bit precision is overkill and the 2x memory savings are worth it. To make an obvious point even more obvious:As noted by others, after some years an 'out-of-core' pandas equivalent has emerged: dask. Though dask is not a drop-in replacement of pandas and all of its functionality it stands out for several reasons:Dask is a flexible parallel computing library for analytic computing that is optimized for dynamic task scheduling for interactive computational workloads of

「Big Data」collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments and scales from laptops to clusters.and to add a simple code sample:replaces some pandas code like this:and, especially noteworthy, provides through the concurrent.futures interface a general infrastructure for the submission of custom tasks:One more variationMany of the operations done in pandas can also be done as a db query (sql, mongo)Using a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)Later, you can perform post processing using pandas.The advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.And although the query language and pandas are different, it's usually not complicated to translate part of the logic from one to another.It is worth mentioning here Ray as well,

it's a distributed computation framework, that has it's own implementation for pandas in a distributed way.  Just replace the pandas import, and the code should work as is:can read more details here:https://rise.cs.berkeley.edu/blog/pandas-on-ray/Consider Ruffus if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. I recently came across a similar issue. I found simply reading the data in chunks and appending it as I write it in chunks to the same csv works well. My problem was adding a date column based on information in another table, using the value of certain columns as follows. This may help those confused by dask and hdf5 but more familiar with pandas like myself. I'd like to point out the Vaex package.Have a look at the documentation: https://vaex.readthedocs.io/en/latest/

The API is very close to the API of pandas.At the moment I am working "like" you, just on a lower scale, which is why I don't have a PoC for my suggestion.However, I seem to find success in using pickle as caching system and outsourcing execution of various functions into files - executing these files from my commando / main file; For example i use a prepare_use.py to convert object types, split a data set into test, validating and prediction data set.How does your caching with pickle work?

I use strings in order to access pickle-files that are dynamically created, depending on which parameters and data sets were passed (with that i try to capture and determine if the program was already run, using .shape for data set, dict for passed parameters). 

Respecting these measures, i get a String to try to find and read a .pickle-file and can, if found, skip processing time in order to jump to the execution i am working on right now.Using databases I encountered similar problems, which is why i found joy in using this solution, however - there are many constraints for sure - for example storing huge pickle sets due to redundancy.

Updating a table from before to after a transformation can be done with proper indexing - validating information opens up a whole other book (I tried consolidating crawled rent data and stopped using a database after 2 hours basically - as I would have liked to jump back after every transformation process)I hope my 2 cents help you in some way.Greetings.

Removing duplicates in lists

Neemaximo

[Removing duplicates in lists](https://stackoverflow.com/questions/7961363/removing-duplicates-in-lists)

Pretty much I need to write a program to check if a list has any duplicates and if it does it removes them and returns a new list with the items that weren't duplicated/removed. This is what I have but to be honest I do not know what to do.

2011-11-01 00:45:24Z

Pretty much I need to write a program to check if a list has any duplicates and if it does it removes them and returns a new list with the items that weren't duplicated/removed. This is what I have but to be honest I do not know what to do.The common approach to get a unique collection of items is to use a set. Sets are unordered collections of distinct objects. To create a set from any iterable, you can simply pass it to the built-in set() function. If you later need a real list again, you can similarly pass the set to the list() function.The following example should cover whatever you are trying to do:As you can see from the example result, the original order is not maintained. As mentioned above, sets themselves are unordered collections, so the order is lost. When converting a set back to a list, an arbitrary order is created.If order is important to you, then you will have to use a different mechanism. A very common solution for this is to rely on OrderedDict to keep the order of keys during insertion:Starting with Python 3.7, the built-in dictionary is guaranteed to maintain the insertion order as well, so you can also use that directly if you are on Python 3.7 or later (or CPython 3.6):Note that this may have some overhead of creating a dictionary first, and then creating a list from it. If you don’t actually need to preserve the order, you’re often better off using a set, especially because it gives you a lot more operations to work with. Check out this question for more details and alternative ways to preserve the order when removing duplicates.Finally note that both the set as well as the OrderedDict/dict solutions require your items to be hashable. This usually means that they have to be immutable. If you have to deal with items that are not hashable (e.g. list objects), then you will have to use a slow approach in which you will basically have to compare every item with every other item in a nested loop.In Python 2.7, the new way of removing duplicates from an iterable while keeping it in the original order is:In Python 3.5, the OrderedDict has a C implementation. My timings show that this is now both the fastest and shortest of the various approaches for Python 3.5.In Python 3.6, the regular dict became both ordered and compact.  (This feature is holds for CPython and PyPy but may not present in other implementations).  That gives us a new fastest way of deduping while retaining order:In Python 3.7, the regular dict is guaranteed to both ordered across all implementations.  So, the shortest and fastest solution is:It's a one-liner: list(set(source_list)) will do the trick.A set is something that can't possibly have duplicates.Update: an order-preserving approach is two lines:Here we use the fact that OrderedDict remembers the insertion order of keys, and does not change it when a value at a particular key is updated. We insert True as values, but we could insert anything, values are just not used. (set works a lot like a dict with ignored values, too.)If you don't care about the order, just do this:A set is guaranteed to not have duplicates.To make a new list  retaining the order of first elements of duplicates in Lnewlist=[ii for n,ii in enumerate(L) if ii not in L[:n]]for example if L=[1, 2, 2, 3, 4, 2, 4, 3, 5] then newlist will be [1,2,3,4,5]This checks each new element has not appeared previously in the list before adding it. 

Also it does not need imports. A colleague have sent the accepted answer as part of his code to me for a codereview today.

While I certainly admire the elegance of the answer in question, I am not happy with the performance.

I have tried this solution (I use set to reduce lookup time)To compare efficiency, I used a random sample of 100 integers - 62 were uniqueHere are the results of the measurementsWell, what happens if set is removed from the solution?The result is not as bad as with the OrderedDict, but still more than 3 times of the original solutionThere are also solutions using Pandas and Numpy. They both return numpy array so you have to use the function .tolist() if you want a list.Using Pandas function unique():Using numpy function unique().Note that numpy.unique() also sort the values. So the list t2 is returned sorted. If you want to have the order preserved use as in this answer:The solution is not so elegant compared to the others, however, compared to pandas.unique(), numpy.unique() allows you also to check if nested arrays are unique along one selected axis.Another way of doing:Simple and easy:Output:I had a dict in my list, so I could not use the above approach. I got the error:So if you care about order and/or some items are unhashable. Then you might find this useful:Some may consider list comprehension with a side effect to not be a good solution. Here's an alternative:All the order-preserving approaches I've seen here so far either use naive  comparison (with O(n^2) time-complexity at best) or heavy-weight OrderedDicts/set+list combinations that are limited to hashable inputs. Here is a hash-independent O(nlogn) solution:Update added the key argument, documentation and Python 3 compatibility.If you want to preserve the order, and not use any external modules here is an easy way to do this:Note: This method preserves the order of appearance, so, as seen above, nine will come after one because it was the first time it appeared. This however, is the same result as you would get with doingbut it is much shorter, and runs faster.This works because each time the fromkeys function tries to create a new key, if the value already exists it will simply overwrite it. This wont affect the dictionary at all however, as fromkeys creates a dictionary where all keys have the value None, so effectively it eliminates all duplicates this way.Try using sets:Reduce variant with ordering preserve:Assume that we have list:Reduce variant (unefficient):5 x faster but more sophisticatedExplanation:You could also do this:The reason that above works is that index method returns only the first index of an element. Duplicate elements have higher indices. Refer to here:In this answer, will be two sections: Two unique solutions, and a graph of speed for specific solutions.Most of these answers only remove duplicate items which are hashable, but this question doesn't imply it doesn't just need hashable items, meaning I'll offer some solutions which don't require hashable items.collections.Counter is a powerful tool in the standard library which could be perfect for this. There's only one other solution which even has Counter in it. However, that solution is also limited to hashable keys.To allow unhashable keys in Counter, I made a Container class, which will try to get the object's default hash function, but if it fails, it will try its identity function. It also defines an eq and a hash method. This should be enough to allow unhashable items in our solution. Unhashable objects will be treated as if they are hashable. However, this hash function uses identity for unhashable objects, meaning two equal objects that are both unhashable won't work. I suggest you overriding this, and changing it to use the hash of an equalivent mutable type (like using hash(tuple(my_list)) if my_list is a list).I also made two solutions. Another solution which keeps the order of the items, using a subclass of both OrderedDict and Counter which is named 'OrderedCounter'. Now, here are the functions:remd is non-ordered sorting, oremd is ordered sorting. You can clearly tell which one is faster, but I'll explain anyways. The non-ordered sorting is slightly faster. It keeps less data, since it doesn't need order.Now, I also wanted to show the speed comparisons of each answer. So, I'll do that now.For removing duplicates, I gathered 10 functions from a few answers. I calculated the speed of each function and put it into a graph using matplotlib.pyplot.I divided this into three rounds of graphing. A hashable is any object which can be hashed, an unhashable is any object which cannot be hashed. An ordered sequence is a sequence which preserves order, an unordered sequence does not preserve order. Now, here are a few more terms:Unordered Hashable was for any method which removed duplicates, which didn't necessarily have to keep the order. It didn't have to work for unhashables, but it could.Ordered Hashable was for any method which kept the order of the items in the list, but it didn't have to work for unhashables, but it could.Ordered Unhashable was any method which kept the order of the items in the list, and worked for unhashables.On the y-axis is the amount of seconds it took. On the x-axis is the number the function was applied to.We generated sequences for unordered hashables and ordered hashables with the following comprehension: [list(range(x)) + list(range(x)) for x in range(0, 1000, 10)]For ordered unhashables: [[list(range(y)) + list(range(y)) for y in range(x)] for x in range(0, 1000, 10)]Note there is a 'step' in the range because without it, this would've taken 10x as long. Also because in my personal opinion, I thought it might've looked a little easier to read.Also note the keys on the legend are what I tried to guess as the most vital parts of the function. As for what function does the worst or best? The graph speaks for itself.With that settled, here are the graphs.

(Zoomed in)

(Zoomed in)

(Zoomed in)

Best approach of removing duplicates from a list is using set() function, available in python, again converting that set into listYou can use the following function: Example: Usage:['this', 'is', 'a', 'list', 'with', 'dupicates', 'in', 'the']There are many other answers suggesting different ways to do this, but they're all batch operations, and some of them throw away the original order. That might be okay depending on what you need, but if you want to iterate over the values in the order of the first instance of each value, and you want to remove the duplicates on-the-fly versus all at once, you could use this generator:This returns a generator/iterator, so you can use it anywhere that you can use an iterator.Output:If you do want a list, you can do this:Output:Without using set One more better approach could be,and the order remains preserved.This one cares about the order without too much hassle (OrderdDict & others). Probably not the most Pythonic way, nor shortest way, but does the trick:below code is simple for removing duplicate in listit returns [1,2,3,4]Here's the fastest pythonic solution comaring to others listed in replies.Using implementation details of short-circuit evaluation allows to use list comprehension, which is fast enough. visited.add(item) always returns None as a result, which is evaluated as False, so the right-side of or would always be the result of such an expression.Time it yourselfUsing set :Using unique :Unfortunately. Most answers here either do not preserve the order or are too long. Here is a simple, order preserving answer.This will give you x with duplicates removed but preserving the order. You can use set to remove duplicates:But note the results will be unordered. If that's an issue:Very simple way in Python 3:In python, it is very easy to process the complicated cases like this and only by python's built-in type. Let me show you how to do ! Method 1: General Case The way (1 line code) to remove duplicated element in list and still keep sorting orderYou will get the resultMethod 2: Special Case The special case to process unhashable (3 line codes)You will get the result : Because tuple is hashable and you can convert data between list and tuple easily     

How to put the legend out of the plot

pottigopi

[How to put the legend out of the plot](https://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot)

I have a series of 20 plots (not subplots) to be made in a single figure.  I want the legend to be outside of the box.  At the same time, I do not want to change the axes, as the size of the figure gets reduced.  Kindly help me for the following queries:

2011-01-15 16:10:03Z

I have a series of 20 plots (not subplots) to be made in a single figure.  I want the legend to be outside of the box.  At the same time, I do not want to change the axes, as the size of the figure gets reduced.  Kindly help me for the following queries:You can make the legend text smaller by creating font properties:There are a number of ways to do what you want.  To add to what @inalis and @Navi already said, you can use the bbox_to_anchor keyword argument to place the legend partially outside the axes and/or decrease the font size. Before you consider decreasing the font size (which can make things awfully hard to read), try playing around with placing the legend in different places:So, let's start with a generic example:If we do the same thing, but use the bbox_to_anchor keyword argument we can shift the legend slightly outside the axes boundaries:Similarly, you can make the legend more horizontal and/or put it at the top of the figure (I'm also turning on rounded corners and a simple drop shadow):Alternatively, you can shrink the current plot's width, and put the legend entirely outside the axis of the figure:And in a similar manner, you can shrink the plot vertically, and put the a horizontal legend at the bottom:Have a look at the matplotlib legend guide. You might also take a look at plt.figlegend().A legend is positioned inside the bounding box of the axes using the loc argument to plt.legend.

E.g. loc="upper right" places the legend in the upper right corner of the bounding box, which by default extents from (0,0) to (1,1) in axes coordinates (or in bounding box notation (x0,y0, width, height)=(0,0,1,1)).To place the legend outside of the axes bounding box, one may specify a tuple (x0,y0) of axes coordinates of the lower left corner of the legend.However, a more versatile approach would be to manually specify the bounding box into which the legend should be placed, using the bbox_to_anchor argument. One can restrict oneself to supply only the (x0,y0) part of the bbox. This creates a zero span box, out of which the legend will expand in the direction given by the loc argument. E.g.plt.legend(bbox_to_anchor=(1.04,1), loc="upper left")places the legend outside the axes, such that the upper left corner of the legend is at position (1.04,1) in axes coordinates. Further examples are given below, where additionally the interplay between different arguments like mode and ncols are shown. Details about how to interpret the 4-tuple argument to bbox_to_anchor, as in l4, can be found in this question. The mode="expand" expands the legend horizontally inside the bounding box given by the 4-tuple. For a vertically expanded legend, see this question.Sometimes it may be useful to specify the bounding box in figure coordinates instead of axes coordinates. This is shown in the example l5 from above, where the bbox_transform argument is used to put the legend in the lower left corner of the figure. Having placed the legend outside the axes often leads to the undesired situation that it is completely or partially outside the figure canvas.Solutions to this problem are:Comparisson between the cases discussed above:  A figure legend

One may use a legend to the figure instead of the axes, matplotlib.figure.Figure.legend. This has become especially useful for matplotlib version >=2.1, where no special arguments are neededto create a legend for all artists in the different axes of the figure. The legend is placed using the loc argument, similar to how it is placed inside an axes, but in reference to the whole figure - hence it will be outside the axes somewhat automatically. What remains is to adjust the subplots such that there is no overlap between the legend and the axes. Here the point "Adjust the subplot parameters"  from above will be helpful. An example:Legend inside dedicated subplot axes

An alternative to using bbox_to_anchor would be to place the legend in its dedicated subplot axes (lax). 

Since the legend subplot should be smaller than the plot, we may use gridspec_kw={"width_ratios":[4,1]} at axes creation.

We can hide the axes lax.axis("off") but still put a legend in. The legend handles and labels need to obtained from the real plot via h,l = ax.get_legend_handles_labels(), and can then be supplied to the legend in the lax subplot, lax.legend(h,l). A complete example is below.This produces a plot which is visually pretty similar to the plot from above:We could also use the first axes to place the legend, but use the bbox_transform of the legend axes,In this approach, we do not need to obtain the legend handles externally, but we need to specify the bbox_to_anchor argument.Just call legend() call after the plot() call like this:Results would look something like this:Short answer: you can use bbox_to_anchor + bbox_extra_artists + bbox_inches='tight'.Longer answer:

You can use bbox_to_anchor to manually specify the location of the legend box, as some other people have pointed out in the answers. However, the usual issue is that the legend box is cropped, e.g.:In order to prevent the legend box from getting cropped, when you save the figure you can use the parameters bbox_extra_artists and bbox_inches to ask savefig to include cropped elements in the saved image:fig.savefig('image_output.png', bbox_extra_artists=(lgd,), bbox_inches='tight')Example (I only changed the last line to add 2 parameters to fig.savefig()):I wish that matplotlib would natively allow outside location for the legend box as Matlab does:To place the legend outside the plot area, use loc and bbox_to_anchor keywords of legend(). For example, the following code will place the legend to the right of the plot area:For more info, see the legend guideIn addition to all the excellent answers here, newer versions of matplotlib and pylab can automatically determine where to put the legend without interfering with the plots, if possible.This will automatically place the legend away from the data if possible!

However, if there is no place to put the legend without overlapping the data, then you'll want to try one of the other answers; using loc="best" will never put the legend outside of the plot.Short Answer: Invoke draggable on the legend and interactively move it wherever you want:Long Answer: If you rather prefer to place the legend interactively/manually rather than programmatically, you can toggle the draggable mode of the legend so that you can drag it to wherever you want. Check the example below:Not exactly what you asked for, but I found it's an alternative for the same problem.

Make the legend semi-transparant, like so:

Do this with:As noted, you could also place the legend in the plot, or slightly off it to the edge as well. Here is an example using the Plotly Python API, made with an IPython Notebook. I'm on the team. To begin, you'll want to install the necessary packages:Then, install Plotly:This creates your graph, and allows you a chance to keep the legend within the plot itself. The default for the legend if it is not set is to place it in the plot, as shown here.  For an alternative placement, you can closely align the edge of the graph and border of the legend, and remove border lines for a closer fit. You can move and re-style the legend and graph with code, or with the GUI. To shift the legend, you have the following options to position the legend inside the graph by assigning x and y values of <= 1. E.g : In this case, we choose the upper right, legendstyle = {"x" : 1, "y" : 1}, also described in the documentation:Something along these lines worked for me. Starting with a bit of code taken from Joe, this method modifies the window width to automatically fit a legend to the right of the figure.You can also try figlegend.  It is possible to create a legend independent of any Axes object.  However, you may need to create some "dummy" Paths to make sure the formatting for the objects gets passed on correctly.Here is an example from the matplotlib tutorial found here. This is one of the more simpler examples but I added transparency to the legend and added plt.show() so you can paste this into the interactive shell and get a result:The solution that worked for me when I had huge legend was to use extra empty image layout. 

In following example I made 4 rows and at the bottom I plot image with offset for legend (bbox_to_anchor) at the top it does not get cut.Here's another solution, similar to adding bbox_extra_artists and bbox_inches, where you don't have to have your extra artists in the scope of your savefig call. I came up with this since I generate most of my plot inside functions.Instead of adding all your additions to the bounding box when you want to write it out, you can add them ahead of time to the Figure's artists. Using something similar to Franck Dernoncourt's answer above:Here's the generated plot.don't know if you already sorted out your issue...probably yes, but...

I simply used the string 'outside' for the location, like in matlab.

I imported pylab from matplotlib.

see the code as follow:Click to see the plot

What is the use of「assert」in Python?

Hossein

[What is the use of「assert」in Python?](https://stackoverflow.com/questions/5142418/what-is-the-use-of-assert-in-python)

I have been reading some source code and in several places I have seen the usage of assert. What does it mean exactly? What is its usage?

2011-02-28 13:11:45Z

I have been reading some source code and in several places I have seen the usage of assert. What does it mean exactly? What is its usage?The assert statement exists in almost every programming language. It helps detect problems early in your program, where the cause is clear, rather than later as a side-effect of some other operation.When you do...... you're telling the program to test that condition, and immediately trigger an error if the condition is false.In Python, it's roughly equivalent to this:Try it in the Python shell:Assertions can include an optional message, and you can disable them when running the interpreter.To print a message if the assertion fails:Do not use parenthesis to call assert like a function. It is a statement. If you do assert(condition, message) you'll be running the assert with a (condition, message) tuple as first parameter.As for disabling them, when running python in optimized mode, where __debug__ is False, assert statements will be ignored. Just pass the -O flag:See here for the relevant documentation.Watch out for the parentheses. As has been pointed out above, in Python 3, assert is still a statement, so by analogy with print(..), one may extrapolate the same to assert(..) or raise(..) but you shouldn't.This is important because:won't work, unlikeThe reason the first one will not work is that bool( (False, "Houston we've got a problem") ) evaluates to True.In the statement assert(False), these are just redundant parentheses around False, which evaluate to their contents. But with assert(False,) the parentheses are now a tuple, and a non-empty tuple evaluates to True in a boolean context.As other answers have noted, assert is similar to throwing an exception if a given condition isn't true. An important difference is that assert statements get ignored if you compile your code with the optimization option -O. The documentation says that assert expression can better be described as being equivalent to This can be useful if you want to thoroughly test your code, then release an optimized version when you're happy that none of your assertion cases fail - when optimization is on, the __debug__ variable becomes False and the conditions will stop getting evaluated. This feature can also catch you out if you're relying on the asserts and don't realize they've disappeared.Others have already given you links to documentation.You can try the following in a interactive shell:The first statement does nothing, while the second raises an exception. This is the first hint: asserts are useful to check conditions that should be true in a given position of your code (usually, the beginning (preconditions) and the end of a function (postconditions)).Asserts are actually highly tied to programming by contract, which is a very useful engineering practice:http://en.wikipedia.org/wiki/Design_by_contract.The goal of an assertion in Python is to inform developers about unrecoverable errors in a program.Assertions are not intended to signal expected error conditions, like「file not found」, where a user can take corrective action (or just try again).Another way to look at it is to say that assertions are internal self-checks in your code. They work by declaring some conditions as impossible in your code. If these conditions don’t hold that means there’s a bug in the program.If your program is bug-free, these conditions will never occur. But if one of them does occur the program will crash with an assertion error telling you exactly which「impossible」condition was triggered. This makes it much easier to track down and fix bugs in your programs.Here’s a summary from a tutorial on Python’s assertions I wrote:The assert statement has two forms.The simple form, assert <expression>, is equivalent toThe extended form, assert <expression1>, <expression2>, is equivalent toFrom docs:Here you can read more: http://docs.python.org/release/2.5.2/ref/assert.htmlAssertions are a systematic way to check that the internal state of a program is as the programmer expected, with the goal of catching bugs. See the example below.Here is a simple example, save this in file (let's say b.py)and the result when $python b.pyif the statement after assert  is true then the program continues , but if the statement after assert is false then the program gives an error. Simple as that.e.g.:The assert statement exists in almost every programming language. It helps detect problems early in your program, where the cause is clear, rather than later as a side-effect of some other operation. They always expect a True condition.When you do something like:You're telling the program to test that condition and immediately trigger an error if it is false.In Python, assert expression, is equivalent to:You can use the extended expression to pass an optional message:Try it in the Python interpreter:There are some caveats to seen before using them mainly for those who deem to toggles between the assert and if statements. The aim to use assert is on occasions when the program verifies a condition and return a value that should stop the program immediately instead of taking some alternative way to bypass the error:As you may have noticed, the assert statement uses two conditions. Hence, do not use parentheses to englobe them as one for obvious advice. If you do such as:Example:You will be running the assert with a (condition, message) which represents a tuple as the first parameter, and this happens cause non-empty tuple in Python is always True. However, you can do separately without problem:Example:If you are wondering regarding when use assert statement. Take an example used in real life:* When your program tends to control each parameter entered by the user or whatever else:* Another case is on math when 0 or non-positive as a coefficient or constant on a certain equation:* or even a simple example of a boolean implementation:The utmost importance is to not rely on the assert statement to execute data processing or data validation because this statement can be turned off on the Python initialization with -O or -OO flag – meaning value 1, 2, and 0 (as default), respectively – or PYTHONOPTIMIZE environment variable.Value 1:* asserts are disabled;* bytecode files are generated using .pyo extension instead of .pyc;* sys.flags.optimize is set to 1 (True);* and, __debug__ is set to False;Value 2: disables one more stuff* docstrings are disabled;Therefore, using the assert statement to validate a sort of expected data is extremely dangerous, implying even to some security issues. Then, if you need to validate some permission I recommend you raise AuthError instead. As a preconditional effective, an assert is commonly used by programmers on libraries or modules that do not have a user interact directly.As summarized concisely on the C2 Wiki:You can use an assert statement to document your understanding of the code at a particular program point. For example, you can document assumptions or guarantees about inputs (preconditions), program state (invariants), or outputs (postconditions).Should your assertion ever fail, this is an alert for you (or your successor) that your understanding of the program was wrong when you wrote it, and that it likely contains a bug.For more information, John Regehr has a wonderful blog post on the Use of Assertions, which applies to the Python assert statement as well.If you ever want to know exactly what a reserved function does in python, type in help(enter_keyword)Make sure if you are entering a reserved keyword that you enter it as a string. Python assert is basically a debugging aid which test condition for internal self-check of your code.

Assert makes debugging really easy when your code gets into impossible edge cases. Assert check those impossible cases.Let's say there is a function to calculate price of item after discount :here, discounted_price can never be less than 0 and greater than actual price. So, in case the above condition is violated assert raises an Assertion Error, which helps the developer to identify that something impossible had happened.Hope it helps :)My short explanation is:A related tutorial about this:format :

  assert Expression[,arguments]

When assert encounters a statement,Python evaluates the expression.If the statement is not true,an exception is raised(assertionError).

If the assertion fails, Python uses ArgumentExpression as the argument for the AssertionError. AssertionError exceptions can be caught and handled like any other exception using the try-except statement, but if not handled, they will terminate the program and produce a traceback.

Example:When the above code is executed, it produces the following result:Can be used to ensure parameters are passed in the function call.Basically the assert keyword meaning is that if the condition is not true then it through an assertionerror else it continue for example in python.code-1OUTPUT: code-2OUTPUT: 

Why are Python lambdas useful? [closed]

meade

[Why are Python lambdas useful? [closed]](https://stackoverflow.com/questions/890128/why-are-python-lambdas-useful)

I'm trying to figure out Python lambdas.  Is lambda one of those "interesting" language items that in real life should be forgotten? I'm sure there are some edge cases where it might be needed, but given the obscurity of it, the potential of it being redefined in future releases (my assumption based on the various definitions of it) and the reduced coding clarity - should it be avoided?  This reminds me of overflowing (buffer overflow) of C types - pointing to the top variable and overloading to set the other field values.  It feels like sort of a techie showmanship but maintenance coder nightmare.

2009-05-20 20:40:01Z

I'm trying to figure out Python lambdas.  Is lambda one of those "interesting" language items that in real life should be forgotten? I'm sure there are some edge cases where it might be needed, but given the obscurity of it, the potential of it being redefined in future releases (my assumption based on the various definitions of it) and the reduced coding clarity - should it be avoided?  This reminds me of overflowing (buffer overflow) of C types - pointing to the top variable and overloading to set the other field values.  It feels like sort of a techie showmanship but maintenance coder nightmare.Are you talking about lambda functions? LikeThose things are actually quite useful.  Python supports a style of programming called functional programming where you can pass functions to other functions to do stuff. Example:sets mult3 to [3, 6, 9], those elements of the original list that are multiples of 3. This is shorter (and, one could argue, clearer) thanOf course, in this particular case, you could do the same thing as a list comprehension:(or even as range(3,10,3)), but there are many other, more sophisticated use cases where you can't use a list comprehension and a lambda function may be the shortest way to write something out.I use lambda functions on a regular basis. It took me a while to get used to them, but eventually I came to understand that they're a very valuable part of the language.lambda is just a fancy way of saying function. Other than its name, there is nothing obscure, intimidating or cryptic about it. When you read the following line, replace lambda by function in your mind:It just defines a function of x. Some other languages, like R, say it explicitly:You see? It's one of the most natural things to do in programming. The two-line summary:A lambda is part of a very important abstraction mechanism which deals with higher order functions. To get proper understanding of its value, please watch high quality lessons from Abelson and Sussman, and read the book SICPThese are relevant issues in modern software business, and becoming ever more popular.I doubt lambda will go away.

See Guido's post about finally giving up trying to remove it. Also see an outline of the conflict.You might check out this post for more of a history about the deal behind Python's functional features:

http://python-history.blogspot.com/2009/04/origins-of-pythons-functional-features.htmlMy own two cents: Rarely is lambda worth it as far as clarity goes. Generally there is a more clear solution that doesn't include lambda.lambdas are extremely useful in GUI programming. For example, lets say you're creating a group of buttons and you want to use a single paramaterized callback rather than a unique callback per button. Lambda lets you accomplish that with ease:(Note: although this question is specifically asking about lambda, you can also use functools.partial to get the same type of result)The alternative is to create a separate callback for each button which can lead to duplicated code.In Python, lambda is just a way of defining functions inline,and....are the exact same.There is nothing you can do with lambda which you cannot do with a regular function—in Python functions are an object just like anything else, and lambdas simply define a function:I honestly think the lambda keyword is redundant in Python—I have never had the need to use them (or seen one used where a regular function, a list-comprehension or one of the many builtin functions could have been better used instead)For a completely random example, from the article "Python’s lambda is broken!":I would argue, even if that did work, it's horribly and "unpythonic", the same functionality could be written in countless other ways, for example:Yes, it's not the same, but I have never seen a cause where generating a group of lambda functions in a list has been required. It might make sense in other languages, but Python is not Haskell (or Lisp, or ...)Edit:There are a few cases where lambda is useful, for example it's often convenient when connecting up signals in PyQt applications, like this:Just doing w.textChanged.connect(dothing) would call the dothing method with an extra event argument and cause an error. Using the lambda means we can tidily drop the argument without having to define a wrapping function.I find lambda useful for a list of functions that do the same, but for different circumstances. Like the Mozilla plural rules.If you'd have to define a function for all of those you'd go mad by the end of it. Also it wouldn't be nice with function names like plural_rule_1, plural_rule_2, etc. And you'd need to eval() it when you're depending on a variable function id.Pretty much anything you can do with lambda you can do better with either named functions or list and generator expressions.Consequently, for the most part you should just one of those in basically any situation (except maybe for scratch code written in the interactive interpreter).I've been using Python for a few years and I've never run in to a case where I've needed lambda. Really, as the tutorial states, it's just for syntactic sugar.I can't speak to python's particular implementation of lambda, but in general lambda functions are really handy.  They're a core technique (maybe even THE technique) of functional programming, and they're also very useuful in object-oriented programs.  For certain types of problems, they're the best solution, so certainly shouldn't be forgotten!I suggest you read up on closures and the map function (that links to python docs, but it exists in nearly every language that supports functional constructs) to see why it's useful.Lambda function it's a non-bureaucratic way to create a function.That's it. For example, let's supose you have your main function and need to square values. Let's see the traditional way and the lambda way to do this:Traditional way:The lambda way:See the difference?Lambda functions go very well with lists, like lists comprehensions or map. In fact, list comprehension it's a "pythonic" way to express yourself using lambda. Ex:Let's see what each elements of the syntax means:That's convenient uh? Creating functions like this. Let's rewrite it using lambda:Now let's use map, which is the same thing, but more language-neutral. Maps takes 2 arguments: (i) one function(ii) an iterableAnd gives you a list where each element it's the function applied to each element of the iterable.So, using map we would have:If you master lambdas and mapping, you will have a great power to manipulate data and in a concise way. Lambda functions are neither obscure nor take away code clarity. Don't confuse something hard with something new. Once you start using them, you will find it very clear.One of the nice things about lambda that's in my opinion understated is that it's way of deferring an evaluation for simple forms till the value is needed. Let me explain. Many library routines are implemented so that they allow certain parameters to be callables (of whom lambda is one). The idea is that the actual value will be computed only at the time when it's going to be used (rather that when it's called). An (contrived) example might help to illustrate the point. Suppose you have a routine which which was going to do log a given timestamp. You want the routine to use the current time minus 30 minutes. You'd call it like soNow suppose the actual function is going to be called only when a certain event occurs and you want the timestamp to be computed only at that time. You can do this like soAssuming the log_timestamp can handle callables like this, it will evaluate this when it needs it and you'll get the timestamp at that time. There are of course alternate ways to do this (using the operator module for example) but I hope I've conveyed the point. Update: Here is a slightly more concrete real world example. Update 2: I think this is an example of what is called a thunk.As stated above, the lambda operator in Python defines an anonymous function, and in Python functions are closures. It is important not to confuse the concept of closures with the operator lambda, which is merely syntactic methadone for them.When I started in Python a few years ago, I used lambdas a lot, thinking they were cool, along with list comprehensions. However, I wrote and have to maintain a big website written in Python, with on the order of several thousand function points. I've learnt from experience that lambdas might be OK to prototype things with, but offer nothing over inline functions (named closures) except for saving a few key-stokes, or sometimes not.Basically this boils down to several points:That's enough reason to round them up and convert them to named closures. However, I hold two other grudges against anonymous closures.The first grudge is simply that they are just another unnecessary keyword cluttering up the language.The second grudge is deeper and on the paradigm level, i.e. I do not like that they promote a functional-programming style, because that style is less flexible than the message passing, object oriented or procedural styles, because the lambda calculus is not Turing-complete (luckily in Python, we can still break out of that restriction even inside a lambda). The reasons I feel lambdas promote this style are:I try hard to write lambda-free Python, and remove lambdas on sight. I think Python would be a slightly better language without lambdas, but that's just my opinion.Lambdas are actually very powerful constructs that stem from ideas in functional programming, and it is something that by no means will be easily revised, redefined or removed in the near future of Python. They help you write code that is more powerful as it allows you to pass functions as parameters, thus the idea of functions as first-class citizens.Lambdas do tend to get confusing, but once a solid understanding is obtained, you can write clean elegant code like this:The above line of code returns a list of the squares of the numbers in the list. Ofcourse, you could also do it like:It is obvious the former code is shorter, and this is especially true if you intend to use the map function (or any similar function that takes a function as a parameter) in only one place. This also makes the code more intuitive and elegant. Also, as @David Zaslavsky mentioned in his answer, list comprehensions are not always the way to go especially if your list has to get values from some obscure mathematical way.From a more practical standpoint, one of the biggest advantages of lambdas for me recently has been in GUI and event-driven programming. If you take a look at callbacks in Tkinter, all they take as arguments are the event that triggered them. E.g.Now what if you had some arguments to pass? Something as simple as passing 2 arguments to store the coordinates of a mouse-click. You can easily do it like this:Now you can argue that this can be done using global variables, but do you really want to bang your head worrying about memory management and leakage especially if the global variable will just be used in one particular place? That would be just poor programming style.In short, lambdas are awesome and should never be underestimated. Python lambdas are not the same as LISP lambdas though (which are more powerful), but you can really do a lot of magical stuff with them. Lambdas are deeply linked to functional programming style in general. The idea that you can solve problems by applying a function to some data, and merging the results, is what google uses to implement most of its algorithms.  Programs written in functional programming style, are easily parallelized  and hence are becoming more and more important with modern multi-core machines.

So in short, NO you should not forget them.First congrats that managed to figure out lambda. In my opinion this is really powerful construct to act with. The trend these days towards functional programming languages is surely an indicator that it neither should be avoided nor it will be redefined in the near future.You just have to think a little bit different. I'm sure soon you will love it. But be careful if you deal only with python. Because the lambda is not a real closure, it is "broken" somehow: pythons lambda is brokenNote that this isn't a condemnation of anything.  Everybody has a different set of things that don't come easily.No.It's not obscure.  The past 2 teams I've worked on, everybody used this feature all the time.I've seen no serious proposals to redefine it in Python, beyond fixing the closure semantics a few years ago.It's not less clear, if you're using it right.  On the contrary, having more language constructs available increases clarity.Lambda is like buffer overflow?  Wow.  I can't imagine how you're using lambda if you think it's a "maintenance nightmare".I use lambdas to avoid code duplication. It would make the function easily comprehensible

Eg:I replace that with a temp lambdaI started reading David Mertz's book today 'Text Processing in Python.' While he has a fairly terse description of Lambda's the examples in the first chapter combined with the explanation in Appendix A made them jump off the page for me (finally) and all of a sudden I understood their value.  That is not to say his explanation will work for you and I am still at the discovery stage so I will not attempt to add to these responses other than the following:

I am new to Python

I am new to OOP

Lambdas were a struggle for me

Now that I read Mertz, I think I get them and I see them as very useful as I think they allow a cleaner approach to programming.  He reproduces the Zen of Python, one line of which is Simple is better than complex. As a non-OOP programmer reading code with lambdas (and until last week list comprehensions) I have thought-This is simple?.  I finally realized today that actually these features make the code much more readable, and understandable than the alternative-which is invariably a loop of some sort.  I also realized that like financial statements-Python was not designed for the novice user, rather it is designed for the user that wants to get educated.  I can't believe how powerful this language is.  When it dawned on me (finally) the purpose and value of lambdas I wanted to rip up about 30 programs and start over putting in lambdas where appropriate.A useful case for using lambdas is to improve the readability of long list comprehensions.

In this example loop_dic is short for clarity but imagine loop_dic being very long. If you would just use a plain value that includes i instead of the lambda version of that value you would get a NameError.Instead of I can give you an example where I actually needed lambda serious.  I'm making a graphical program, where the use right clicks on a file and assigns it one of three options.  It turns out that in Tkinter (the GUI interfacing program I'm writing this in), when someone presses a button, it can't be assigned to a command that takes in arguments.  So if I chose one of the options and wanted the result of my choice to be:Then no big deal.  But what if I need my choice to have a particular detail.  For example, if I choose choice A, it calls a function that takes in some argument that is dependent on the choice A, B or C, TKinter could not support this.  Lamda was the only option to get around this actually...I use it quite often, mainly as a null object or to partially bind parameters to a function.Here are examples:let say that I have the following APIThen, when I wan't to quickly dump the recieved data to a file I do that:   I use lambda to create callbacks that include parameters. It's cleaner writing a lambda in one line than to write a method to perform the same functionality.For example:as opposed to:I'm a python beginner, so to getter a clear idea of lambda I compared it with a 'for' loop; in terms of efficiency.

Here's the code (python 2.7) -Lambda is a procedure constructor. You can synthesize programs at run-time, although Python's lambda is not very powerful. Note that few people understand that kind of programming.

How to read a text file into a string variable and strip newlines?

klijo

[How to read a text file into a string variable and strip newlines?](https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines)

I use the following code segment to read a file in python:Input file is:and when I print data I getAs I see data is in list form. How do I make it string? And also how do I remove the "\n", "[", and "]" characters from it?

2011-12-03 16:47:54Z

I use the following code segment to read a file in python:Input file is:and when I print data I getAs I see data is in list form. How do I make it string? And also how do I remove the "\n", "[", and "]" characters from it?You could use:Use read(), not readline():You can read from a file in one line:Please note that this does not close the file explicitly.CPython will close the file when it exits as part of the garbage collection.But other python implementations won't. To write portable code, it is better to use with or close the file explicitly. Short is not always better. See https://stackoverflow.com/a/7396043/362951To join all lines into a string and remove new lines I normally use :In Python 3.5 or later, using pathlib you can copy text file contents into a variable and close the file in one line:and then you can use str.replace to remove the newlines:join() will join a list of strings, and rstrip() with no arguments will trim whitespace, including newlines, from the end of strings.This can be done using the read() method :Or as the default mode itself is 'r' (read) so simply use,I have fiddled around with this for a while and have prefer to use use read in combination with rstrip. Without rstrip("\n"), Python adds a newline to the end of the string, which in most cases is not very useful.It's hard to tell exactly what you're after, but something like this should get you started:I'm surprised nobody mentioned splitlines() yet.Variable data is now a list that looks like this when printed:Note there are no newlines (\n).At that point, it sounds like you want to print back the lines to console, which you can achieve with a for loop:This is a one line, copy-pasteable solution that also closes the file object:You can also strip each line and concatenate into a final string.This would also work out just fine.you can compress this into one into two lines of code!!!if your file reads:python outputpython3: Google "list comphrension" if the square bracket syntax is new to you.Have you tried this?I don't feel that anyone addressed the [ ] part of your question. When you read each line into your variable, because there were multiple lines before you replaced the \n with '' you ended up creating a list. If you have a variable of x and print it out just byx or print(x)or str(x)You will see the entire list with the brackets. If you call each element of the (array of sorts)x[0]

then it omits the brackets. If you use the str() function you will see just the data and not the '' either.

str(x[0])Maybe you could try this? I use this in my programs.Regular expression works too: This works:

Change your file to:Then:This creates a list named words that equals:That got rid of the "\n". To answer the part about the brackets getting in your way, just do this:Or:This returns:This code will help you to read the first line and then using the list and split option you can convert the first line word separated by space to be stored in a list.Than you can easily access any word, or even store it in a string.You can also do the same thing with using a for loop.To remove line breaks using Python you can use replace function of a string.This example removes all 3 types of line breaks:Example file is:You can try it using this replay scenario:https://repl.it/repls/AnnualJointHardwareTry the following: Caution: It does not remove the \n. It is just for viewing the text as if there were no \n

Is there a simple way to delete a list element by value?

zjm1126

[Is there a simple way to delete a list element by value?](https://stackoverflow.com/questions/2793324/is-there-a-simple-way-to-delete-a-list-element-by-value)

The above shows the following error:So I have to do this:But is there not a simpler way to do this?

2010-05-08 07:48:28Z

The above shows the following error:So I have to do this:But is there not a simpler way to do this?To remove an element's first occurrence in a list, simply use list.remove:Mind that it does not remove all occurrences of your element. Use a list comprehension for that.Usually Python will throw an Exception if you tell it to do something it can't so you'll have to do either:or:An Exception isn't necessarily a bad thing as long as it's one you're expecting and handle properly.You can dobut above need to search 6 in list a 2 times, so try except would be fasterConsider:To take out all occurrences, you could use the filter function in python.

For example, it would look like:So, it would keep all elements of a != 2.To just take out one of the items use Here's how to do it inplace (without list comprehension):If you know what value to delete, here's a simple way (as simple as I can think of, anyway):You'll get

[0, 0, 2, 3, 4]Another possibility is to use a set instead of a list, if a set is applicable in your application.IE if your data is not ordered, and does not have duplicates, thenis error-free.Often a list is just a handy container for items that are actually unordered.  There are questions asking how to remove all occurences of an element from a list.  If you don't want dupes in the first place, once again a set is handy.doesn't change my_set from above.As stated by numerous other answers, list.remove() will work, but throw a ValueError if the item wasn't in the list. With python 3.4+, there's an interesting approach to handling this, using the suppress contextmanager:Finding a value in a list and then deleting that index (if it exists) is easier done by just using list's remove method:If you do this often, you can wrap it up in a function:This example is fast and will delete all instances of a value from the list:In one line:sometimes it usefull.Even easier:If your elements are distinct, then a simple set difference will do.We can also use .pop:Overwrite the list by indexing everything except the elements you wish to removeWith a for loop and a condition:And if you want to remove some, but not all:Say for example, we want to remove all 1's from x. This is how I would go about it:Now, this is a practical use of my method:And this is my method in a single line:Both yield this as an output:Hope this helps. 

PS, pleas note that this was written in version 3.6.2, so you might need to adjust it for older versions.Maybe your solutions works with ints, but It Doesnt work for me with dictionarys.In one hand, remove() has not worked for me. But maybe it works with basic Types. I guess the code bellow is also the way to remove items from objects list.In the other hand, 'del' has not worked properly either. In my case, using python 3.6: when I try to delete an element from a list in a 'for' bucle with 'del' command, python changes the index in the process and bucle stops prematurely before time. It only works if You delete element by element in reversed order. In this way you dont change the pending elements array index when you are going through itThen, Im used:where 'list' is like [{'key1':value1'},{'key2':value2}, {'key3':value3}, ...]Also You can do more pythonic using enumerate:This removes all instances of "-v" from the array sys.argv, and does not complain if no instances were found:You can see the code in action, in a file called speechToText.py:this is my answer, just use while and for

Difference between del, remove and pop on lists

sachin irukula

[Difference between del, remove and pop on lists](https://stackoverflow.com/questions/11520492/difference-between-del-remove-and-pop-on-lists)

Is there any difference between the above three methods to remove an element from a list?

2012-07-17 10:21:41Z

Is there any difference between the above three methods to remove an element from a list?Yes, remove removes the first matching value, not a specific index:del removes the item at a specific index:and pop removes the item at a specific index and returns it.Their error modes are different too:Use del to remove an element by index, pop() to remove it by index if you need the returned value, and remove() to delete an element by value.  The latter requires searching the list, and raises ValueError if no such value occurs in the list.When deleting index i from a list of n elements, the computational complexities of these methods areSince no-one else has mentioned it, note that del (unlike pop) allows the removal of a range of indexes because of list slicing:This also allows avoidance of an IndexError if the index is not in the list:Already answered quite well by others. This one from my end :)Evidently, pop is the only one which returns the value, and remove is the only one which searches the object, while del limits itself to a simple deletion. pop    - Takes Index and returns Valueremove - Takes value, removes first occurrence, and returns nothing delete - Takes index, removes value at that index, and returns nothingMany best explanations are here but I will try my best to simplify more.Among all these methods, reverse & pop are postfix while delete is prefix.remove(): It used to remove first occurrence of elementremove(i) => first occurrence of i value pop(): It used to remove element if:unspecified pop() => from end of listspecified pop(index) => of indexdelete(): Its a prefix method. Keep an eye on two different syntax for same method: [] and (). It possesses power to:1.Delete index  del a[index] => used to delete index and its associated value just like pop.2.Delete values in range [index 1:index N] del a[0:3] => multiple values in range3.Last but not list, to delete whole list in one shot del (a) => as said above.Hope this clarifies the confusion if any.Any operation/function on different data structures is defined for particular actions. Here in your case i.e. removing an element, delete, Pop and remove. (If you consider sets, Add another operation - discard)

Other confusing case is while adding. Insert/Append. 

For Demonstration, Let us Implement deque. deque is a hybrid linear data structure, where you can add elements / remove elements from both ends.(Rear and front Ends) In here, see the operations:Operations have to return something. So, pop -  With and without an index. 

If I don't want to return the value:

del self.items[0]Delete by value not Index: let us consider the case of sets.While pop and delete both take indices to remove an element as stated in above comments. A key difference is the time complexity for them. The time complexity for pop() with no index is O(1) but is not the same case for deletion of last element.If your use case is always to delete the last element, it's always preferable to use pop() over delete(). For more explanation on time complexities, you can refer to https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt The remove operation on a list is given a value to remove. It searches the list to find an item with that value and deletes the first matching item it finds. It is an error if there is no matching item, raises a ValueError.The del statement can be used to delete an entire list. If you have a specific list item as your argument to del (e.g. listname[7] to specifically reference the 8th item in the list), it'll just delete that item. It is even possible to delete a "slice" from a list. It is an error if there index out of range, raises a IndexError.The usual use of pop is to delete the last item from a list as you use the list as a stack. Unlike del, pop returns the value that it popped off the list. You can optionally give an index value to pop and pop from other than the end of the list (e.g listname.pop(0) will delete the first item from the list and return that first item as its result). You can use this to make the list behave like a queue, but there are library routines available that can provide queue operations with better performance than pop(0) does. It is an error if there index out of range, raises a IndexError.See collections.deque for more details.Remove basically works on the value .

Delete and pop work on the indexRemove basically removes the first matching value.

Delete deletes the item from a specific index

Pop basically takes an index and returns the value at that index. Next time you print the list the value doesnt appear.You can also use remove to remove a value by index as well.  n would then refer to [1, 5]

Checking whether a variable is an integer or not [duplicate]

Hulk

[Checking whether a variable is an integer or not [duplicate]](https://stackoverflow.com/questions/3501382/checking-whether-a-variable-is-an-integer-or-not)

How do I check whether a variable is an integer?

2010-08-17 10:15:08Z

How do I check whether a variable is an integer?If you need to do this, dounless you are in Python 2.x in which case you wantDo not use type. It is almost never the right answer in Python, since it blocks all the flexibility of polymorphism. For instance, if you subclass int, your new class should register as an int, which type will not do:This adheres to Python's strong polymorphism: you should allow any object that behaves like an int, instead of mandating that it be one.The classical Python mentality, though, is that it's easier to ask forgiveness than permission. In other words, don't check whether x is an integer; assume that it is and catch the exception results if it isn't:This mentality is slowly being overtaken by the use of abstract base classes, which let you register exactly what properties your object should have (adding? multiplying? doubling?) by making it inherit from a specially-constructed class. That would be the best solution, since it will permit exactly those objects with the necessary and sufficient attributes, but you will have to read the docs on how to use it.All proposed answers so far seem to miss the fact that a double (floats in python are actually doubles) can also be an integer (if it has nothing after the decimal point).  I use the built-in is_integer() method on doubles to check this.Example (to do something every xth time in a for loop):Edit:You can always convert to a float before calling this method. The three possibilities:Otherwise, you could check if it is an int first like Agostino said:Here's a summary of the different methods mentioned here:and here's how they apply to a variety of numerical types that have integer value:You can see they aren't 100% consistent.  Fraction and Rational are conceptually the same, but one supplies a .index() method and the other doesn't.  Complex types don't like to convert to int even if the real part is integral and imaginary part is 0.(np.int8|16|32|64(5) means that np.int8(5), np.int32(5), etc. all behave identically)If you really need to check then it's better to use abstract base classes rather than concrete classes. For an integer that would mean:This doesn't restrict the check to just int, or just int and long, but also allows other user-defined types that behave as integers to work.See here for more.Note that this does not help if you're looking for int-like attributes. In this case you may also want to check for long:I've seen checks of this kind against an array/index type in the Python source, but I don't think that's visible outside of C.Token SO reply: Are you sure you should be checking its type? Either don't pass a type you can't handle, or don't try to outsmart your potential code reusers, they may have a good reason not to pass an int to your function.Why not try something like:Rather than over complicate things, why not just a simpleA simple method I use in all my software is this. It checks whether the variable is made up of numbers.it's really astounding to see such a heated discussion coming up when such a basic, valid and, i believe, mundane question is being asked. some people have pointed out that type-checking against int (and long) might loose cases where a big decimal number is encountered. quite right.some people have pointed out that you should 'just do x + 1 and see whether that fails. well, for one thing, this works on floats too, and, on the other hand, it's easy to construct a class that is definitely not very numeric, yet defines the + operator in some way.i am at odds with many posts vigorously declaring that you should not check for types. well, GvR once said something to the effect that in pure theory, that may be right, but in practice, isinstance often serves a useful purpose (that's a while ago, don't have the link; you can read what GvR says about related issues in posts like this one).what is funny is how many people seem to assume that the OP's intent was to check whether the type of a given x is a numerical integer type—what i understood is what i normally mean when using the OP's words: whether x represents an integer number. and this can be very important: like ask someone how many items they'd want to pick, you may want to check you get a non-negative integer number back. use cases like this abound. it's also, in my opinion, important to see that (1) type checking is but ONE—and often quite coarse—measure of program correctness, because (2) it is often bounded values that make sense, and out-of-bounds values that make nonsense. sometimes just some intermittent values make sense—like considering all numbers, only those real (non-complex), integer numbers might be possible in a given case. funny non-one seems to mention checking for x == math.floor( x ). if that should give an error with some big decimal class, well, then maybe it's time to re-think OOP paradigms. there is also PEP 357 that considers how to use not-so-obviously-int-but-certainly-integer-like values to be used as list indices. not sure whether i like the solution.Found a related question here on SO itself.Python developers prefer to not check types but do a type specific operation and catch a TypeError exception. But if you don't know the type then you have the following.If you want to check that a string consists of only digits, but converting to an int won't help, you can always just use regex.In this case, if x were "hello", converting it to a numeric type would throw a ValueError, but data would also be lost in the process. Using a regex and catching an AttributeError would allow you to confirm numeric characters in a string with, for instance, leading 0's.If you didn't want it to throw an AttributeError, but instead just wanted to look for more specific problems, you could vary the regex and just check the match:That actually shows you where the problem occurred without the use of exceptions. Again, this is not for testing the type, but rather the characters themselves. This gives you much more flexibility than simply checking for types, especially when converting between types can lose important string data, like leading 0's.why not just check if the value you want to check is equal to itself cast as an integer as shown below?You can also use str.isdigit. Try looking up help(str.isdigit)If the variable is entered like a string (e.g. '2010'): Before using this I worked it out with try/except and checking for (int(variable)), but it was longer code. I wonder if there's any difference in use of resources or speed.Here is a simple example how you can determine an integerIf you just need the value, operator.index (__index__ special method) is the way to go in my opinion. Since it should work for all types that can be safely cast to an integer. I.e. floats fail, integers, even fancy integer classes that do not implement the Integral abstract class work by duck typing.operator.index is what is used for list indexing, etc. And in my opinion it should be used for much more/promoted.In fact I would argue it is the only correct way to get integer values if you want to be certain that floating points, due to truncating problems, etc. are rejected and it works with all integral types (i.e. numpy, etc.) even if they may not (yet) support the abstract class. This is what __index__ was introduced for!If you want to check with no regard for Python version (2.x vs 3.x), use six (PyPI) and it's integer_types attribute:Within six (a very light-weight single-file module), it's simply doing this:If you are reading from a file and you have an array or dictionary with values of multiple datatypes, the following will be useful.

Just check whether the variable can be type casted to int(or any other datatype you want to enforce) or not. In the presence of numpy check like .... (slow) or .... in order to match all type variants like np.int8, np.uint16, ...(Drop long in PY3)Recognizing ANY integer-like object from anywhere is a tricky guessing game. Checking for truth and non-exception may be a good bet. Similarly, checking for signed integer type exclusively:use the int function to helpIf you have not int you can do just this:A simple way to do this is to directly check if the remainder on division by 1 is 0 or not.A more general approach that will attempt to check for both integers and integers given as strings will beIt is very simple to check in python. You can do like this:Suppose you want to check a variable is integer or not!I was writing a program to check if a number was square and I encountered this issue, the 

code I used was:To tell if the number was an integer I converted the float number you get from square rooting the user input to a rounded integer (stored as the value ), if those two numbers were equal then the first number must have been an integer, allowing the program to respond. This may not be the shortest way of doing this but it worked for me. Call this function:To test whether a value is an integer (of any kind), you can to do this :source : http://python3porting.com/differences.htmlyou can do this by:and it will return 'this works'... but if you change name to int(1) then it will return 'this does not work' because it is now a string...

you can also try:and the same thing will happenThere is another option to do the type check. For example:

How do I get the row count of a pandas DataFrame?

yemu

[How do I get the row count of a pandas DataFrame?](https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe)

I'm trying to get the number of rows of dataframe df with Pandas, and here is my code.Both the code snippets give me this error:What am I doing wrong?

2013-04-11 08:14:08Z

I'm trying to get the number of rows of dataframe df with Pandas, and here is my code.Both the code snippets give me this error:What am I doing wrong?You can use the .shape property or just len(DataFrame.index). However, there are notable performance differences ( len(DataFrame.index) is fastest):EDIT: As @Dan Allen noted in the comments len(df.index) and df[0].count() are not interchangeable as count excludes NaNs,Suppose df is your dataframe then:Or, more succinctly, Use len(df). This works as of pandas 0.11 or maybe even earlier.__len__() is currently (0.12) documented with Returns length of index. Timing info, set up the same way as in root's answer:Due to one additional function call it is a bit slower than calling len(df.index) directly, but this should not play any role in most use cases.This table summarises the different situations in which you'd want to count something in a DataFrame (or Series, for completeness), along with the recommended method(s).Below, I show examples of each of the methods described in the table above. First, the setup - It seems silly to compare the performance of constant time operations, especially when the difference is on the level of "seriously, don't worry about it". But this seems to be a trend with other answers, so I'm doing the same for completeness.Of the 3 methods above, len(df.index) (as mentioned in other answers) is the fastest. Analogous to len(df.index), len(df.columns) is the faster of the two methods (but takes more characters to type).s.size and len(s.index) are about the same in terms of speed. But I recommend len(df). The methods described here only count non-null values (meaning NaNs are ignored). Calling DataFrame.count will return non-NaN counts for each column:For Series, use Series.count to similar effect:For DataFrames, use DataFrameGroupBy.size to count the number of rows per group.Similarly, for Series, you'll use SeriesGroupBy.size.In both cases, a Series is returned. This makes sense for DataFrames as well since all groups share the same row-count.Similar to above, but use GroupBy.count, not GroupBy.size. Note that size always returns a Series, while count returns a Series if called on a specific column, or else a DataFrame.The following methods return the same thing:Meanwhile, for count, we have...called on the entire GroupBy object, v/s, Called on a specific column.len() is your friend, short answer for row counts is len(df). Alternatively, you can access all rows by df.index and all columns by 

df.columns, and as you can use the len(anyList) for getting the count of list, hence you can use

len(df.index) for getting the number of rows, and len(df.columns) for the column count.Alternatively, you can use df.shape which returns the number of rows and columns together, if you want to access the number of rows only use df.shape[0] and for the number of columns only use: df.shape[1].Apart from above answers use can use df.axes to get the tuple with row and column indexes and then use len() function:...building on Jan-Philip Gehrcke's answer. The reason why len(df) or len(df.index) is faster than df.shape[0]. Look at the code. df.shape is a @property that runs a DataFrame method calling len twice.And beneath the hood of len(df)len(df.index) will be slightly faster than len(df) since it has one less function call, but this is always faster than df.shape[0]I come to pandas from R background, and I see that pandas is more complicated when it comes to selecting row or column.

I had to wrestle with it for a while, then I found some ways to deal with:getting the number of columns:getting the number of rows:In case you want to get the row count in the middle of a chained operation, you can use:Example:This can be useful if you don't want to put a long statement inside a len() function.You could use __len__() instead but __len__() looks a bit weird. For dataframe df, a printed comma formatted row count used while exploring data:    Example:An alternative method to finding out the amount of rows in a dataframe which I think is the most readable variant is pandas.Index.size.Do note that as I commented on the accepted answer:

What are「named tuples」in Python?

Denilson Sá Maia

[What are「named tuples」in Python?](https://stackoverflow.com/questions/2970608/what-are-named-tuples-in-python)

Reading the changes in Python 3.1, I found something... unexpected:I never heard about named tuples before, and I thought elements could either be indexed by numbers (like in tuples and lists) or by keys (like in dicts). I never expected they could be indexed both ways.Thus, my questions are:

2010-06-03 23:50:16Z

Reading the changes in Python 3.1, I found something... unexpected:I never heard about named tuples before, and I thought elements could either be indexed by numbers (like in tuples and lists) or by keys (like in dicts). I never expected they could be indexed both ways.Thus, my questions are:Named tuples are basically easy-to-create, lightweight object types.  Named tuple instances can be referenced using object-like variable dereferencing or the standard tuple syntax.  They can be used similarly to struct or other common record types, except that they are immutable.  They were added in Python 2.6 and Python 3.0, although there is a recipe for implementation in Python 2.4.For example, it is common to represent a point as a tuple (x, y).  This leads to code like the following:Using a named tuple it becomes more readable:However, named tuples are still backwards compatible with normal tuples, so the following will still work:Thus, you should use named tuples instead of tuples anywhere you think object notation will make your code more pythonic and more easily readable.  I personally have started using them to represent very simple value types, particularly when passing them as parameters to functions.  It makes the functions more readable, without seeing the context of the tuple packing.Furthermore, you can also replace ordinary immutable classes that have no functions, only fields with them.  You can even use your named tuple types as base classes:However, as with tuples, attributes in named tuples are immutable:If you want to be able change the values, you need another type.  There is a handy recipe for mutable recordtypes which allow you to set new values to attributes.I am not aware of any form of "named list" that lets you add new fields, however.  You may just want to use a dictionary in this situation. Named tuples can be converted to dictionaries using pt1._asdict() which returns {'x': 1.0, 'y': 5.0} and can be operated upon with all the usual dictionary functions.  As already noted, you should check the documentation for more information from which these examples were constructed.namedtuple is a factory function for making a tuple class. With that class we can create tuples that are callable by name also.A named tuple is a tuple.It does everything a tuple can.But it's more than just a tuple. It's a specific subclass of a tuple that is programmatically created to your specification, with named fields and a fixed length. This, for example, creates a subclass of tuple, and aside from being of fixed length (in this case, three), it can be used everywhere a tuple is used without breaking. This is known as Liskov substitutability.New in Python 3.6, we can use a class definition with typing.NamedTuple to create a namedtuple:The above is the same as the below, except the above additionally has type annotations and a docstring. The below is available in Python 2+:This instantiates it:We can inspect it and use its attributes:To understand named tuples, you first need to know what a tuple is. A tuple is essentially an immutable (can't be changed in-place in memory) list.Here's how you might use a regular tuple:You can expand a tuple with iterable unpacking:Named tuples are tuples that allow their elements to be accessed by name instead of just index! You make a namedtuple like this:You can also use a single string with the names separated by spaces, a slightly more readable use of the API:You can do everything tuples can do (see above) as well as do the following:A commenter asked:The types you create with namedtuple are basically classes you can create with easy shorthand. Treat them like classes. Define them on the module level, so that pickle and other users can find them.The working example, on the global module level:And this demonstrates the failure to lookup the definition:Use them when it improves your code to have the semantics of tuple elements expressed in your code. You can use them instead of an object if you would otherwise use an object with unchanging data attributes and no functionality. You can also subclass them to add functionality, for example:It would probably be a regression to switch from using named tuples to tuples. The upfront design decision centers around whether the cost from the extra code involved is worth the improved readability when the tuple is used. There is no extra memory used by named tuples versus tuples. You're looking for either a slotted object that implements all of the functionality of a statically sized list or a subclassed list that works like a named tuple (and that somehow blocks the list from changing in size.) A now expanded, and perhaps even Liskov substitutable, example of the first:And to use, just subclass and define __slots__:namedtuples are a great feature, they are perfect container for data. When you have to "store" data you would use tuples or dictionaries, like:or:The dictionary approach is overwhelming, since dict are mutable and slower than tuples. On the other hand, the tuples are immutable and lightweight but lack readability for a great number of entries in the data fields.namedtuples are the perfect compromise for the two approaches, the have great readability, lightweightness and immutability (plus they are polymorphic!).named tuples allow backward compatibility with code that checks for the version like thiswhile allowing future code to be more explicit by using this syntaxis one of the easiest ways to clean up your code and make it more readable. It self-documents what is happening in the tuple. Namedtuples instances are just as memory efficient as regular tuples as they do not have per-instance dictionaries, making them faster than dictionaries. Without naming each element in the tuple, it would read like this:It is so much harder to understand what is going on in the first example. With a namedtuple, each field has a name. And you access it by name rather than position or index. Instead of p[1], we can call it p.saturation. It's easier to understand. And it looks cleaner.Creating an instance of the namedtuple is easier than creating a dictionary.The syntaxYou can still access namedtuples by their position, if you so choose. p[1] == p.saturation. It still unpacks like a regular tuple.All the regular tuple methods are supported. Ex: min(), max(), len(), in, not in, concatenation (+), index, slice, etc. And there are a few additional ones for namedtuple. Note: these all start with an underscore. _replace, _make, _asdict._replace

Returns a new instance of the named tuple replacing specified fields with new values.The syntaxExampleNotice: The field names are not in quotes; they are keywords here.

Remember: Tuples are immutable - even if they are namedtuples and have the _replace method. The _replace produces a new instance; it does not modify the original or replace the old value. You can of course save the new result to the variable. p = p._replace(hue=169)_makeMakes a new instance from an existing sequence or iterable.The syntaxExampleWhat happened with the last one? The item inside the parenthesis should be the iterable. So a list or tuple inside the parenthesis works, but the sequence of values without enclosing as an iterable returns an error._asdictReturns a new OrderedDict which maps field names to their corresponding values.The syntaxExampleReference: https://www.reddit.com/r/Python/comments/38ee9d/intro_to_namedtuple/There is also named list which is similar to named tuple but mutable

https://pypi.python.org/pypi/namedlistWhat is namedtuple ?As the name suggests, namedtuple is a tuple with name.  In standard tuple, we access the elements using the index, whereas namedtuple allows user to define name for elements.  This is very handy especially processing csv (comma separated value) files and working with complex and large dataset, where the code becomes messy with the use of indices (not so pythonic).How to use them ?Reading Interesting Scenario in CSV Processing :In Python inside there is a good use of container called a named tuple, it can be used to create a definition of class and has all the features of the original tuple.Using named tuple will be directly applied to the default class template to generate a simple class, this method allows a lot of code to improve readability and it is also very convenient when defining a class.Another way (a new way) to use named tuple is using NamedTuple from typing package: Type hints in namedtupleLet's use the example of the top answer in this post to see how to use it. (1) Before using the named tuple, the code is like this:(2) Now we use the named tupleinherit the NamedTuple class and define the variable name in the new class. test is the name of the class.create instances from the class and assign values to themuse the variables from the instances to calculateTry this:Basically, namedtuples are easy to create, lightweight object types. 

They turn tuples into convenient containers for simple tasks. 

With namedtuples, you don’t have to use integer indices for accessing members of a tuple.Examples:Code 1:Code 2:Everyone else has already answered it, but I think I still have something else to add.Namedtuple could be intuitively deemed as a shortcut to define a class.See a cumbersome and conventional  way to define a class .As for namedtuple

differentiate null=True, blank=True in django

user993563

[differentiate null=True, blank=True in django](https://stackoverflow.com/questions/8609192/differentiate-null-true-blank-true-in-django)

When we add a database field in django we generally write:The same is done with ForeignKey, DecimalField etc. What is the basic difference in having in respect to different (CharField, ForeignKey, ManyToManyField, DateTimeField) fields. What are the advantages/disadvantages of using 1/2/3?

2011-12-22 20:11:03Z

When we add a database field in django we generally write:The same is done with ForeignKey, DecimalField etc. What is the basic difference in having in respect to different (CharField, ForeignKey, ManyToManyField, DateTimeField) fields. What are the advantages/disadvantages of using 1/2/3?null=True sets NULL (versus NOT NULL) on the column in your DB. Blank values for Django field types such as DateTimeField or ForeignKey will be stored as NULL in the DB.blank determines whether the field will be required in forms. This includes the admin and your custom forms. If blank=True then the field will not be required, whereas if it's False the field cannot be blank.The combo of the two is so frequent because typically if you're going to allow a field to be blank in your form, you're going to also need your database to allow NULL values for that field. The exception is CharFields and TextFields, which in Django are never saved as NULL. Blank values are stored in the DB as an empty string ('').A few examples:Obviously, Those two options don't make logical sense to use (though there might be a use case for null=True, blank=False if you want a field to always be required in forms, optional when dealing with an object through something like the shell.)CHAR and TEXT types are never saved as NULL by Django, so null=True is unnecessary. However, you can manually set one of these fields to None to force set it as NULL. If you have a scenario where that might be necessary, you should still include null=True.This is how the ORM maps blank & null fields for Django 1.8The database fields created for PostgreSQL 9.4 are : The database fields created for MySQL 5.6 are : As said in Django Model Field reference: LinkIt's crucial to understand that the options in a Django model field definition serve (at least) two purposes: defining the database tables, and defining the default format and validation of model forms. (I say "default" because the values can always be overridden by providing a custom form.) Some options affect the database, some options affect forms, and some affect both.When it comes to null and blank, other answers have already made clear that the former affects the database table definition and the latter affects model validation. I think the distinction can be made even clearer by looking at use cases for all four possible configurations:Simply null=True defines database should accept NULL values, on other hand blank=True defines on form validation this field should accept blank values or not(If blank=True it accept form without a value in that field and blank=False[default value] on form validation it will show This field is required error.null=True/False related to databaseblank=True/False related to form validationYou may have your answer however till this day it's difficult to judge whether to put null=True or blank=True or both to a field. I personally think it's pretty useless and confusing to provide so many options to developers. Let the handle the nulls or blanks however they want.I follow this table, from Two Scoops of Django: Here is an example of the field with blank= True and null=Truedescription = models.TextField(blank=True, null= True)In this case: 

blank = True: tells our form that it is ok to leave the description field blankandnull = True: tells our database that it is ok to record a null value in our db field and not give an error.Here, is the main difference of null=True and blank=True:The default value of both null and blank is False. Both of these values work at field level i.e., whether we want to keep a field null or blank.null=True will set the field’s value to NULL i.e., no data. It is basically for the databases column value.blank=True determines whether the field will be required in forms. This includes the admin and your own custom forms.title = models.CharField(blank=True) // title can be kept blank.

 In the database ("") will be stored.

null=True blank=True This means that the field is optional in all circumstances.Means there is no constraint of database for the field to be filled, so you can have an object with null value for the filled that has this option.Means there is no constraint of validation in django forms. so when you fill a modelForm for this model you can leave field with this option unfilled.The default values of null and blank are False.Null: It is database-related. Defines if a given database column will accept null values or not.Blank: It is validation-related. It will be used during forms validation, when calling form.is_valid().That being said, it is perfectly fine to have a field with null=True and blank=False. Meaning on the database level the field can be NULL, but in the application level it is a required field.Now, where most developers get it wrong: Defining null=True for string-based fields such as CharField and TextField. Avoid doing that. Otherwise, you will end up having two possible values for「no data」, that is: None and an empty string. Having two possible values for「no data」is redundant. The Django convention is to use the empty string, not NULL.When we save anything in Django admin two steps validation happens, on Django level and on Database level. We can't save text in a number field.Database has data type NULL, it's nothing. When Django creates columns in the database it specifies that they can't be empty. And if you will try to save NULL you will get the database error.Also on Django-Admin level, all fields are required by default, you can't save blank field, Django will throw you an error.So, if you want to save blank field you need to allow it on Django and Database level.

blank=True - will allow empty field in admin panel

null=True - will allow saving NULL to the database column.There's one point where null=True would be necessary even on a CharField or TextField and that is when the database has the unique flag set for the column.In other words, if you've a unique Char/TextField in Django, you'll need to use this:For non-unique CharField or TextField, you'll be better off skipping the null=True otherwise some fields will get set as NULL while others as "" , and you'll have to check the field value for NULL everytime. null is for database and blank is for fields validation that you want to show on user interface like textfield to get the last name of person.

If lastname=models.charfield (blank=true) it didnot ask user to enter last name as this is the optional field now.

If lastname=models.charfield (null=true)  then it means that if this field doesnot get any value from user then it will store in database as an empty string " ".null - default is False 

       if True, Django will store empty as null in the database.blank - default is False

        if true that field is allowed to be blankmore, goto

https://docs.djangoproject.com/en/3.0/topics/db/models/This table below demonstrates the main differences:In Very simple words,Blank is different than null. null is purely database-related, whereas blank is validation-related(required in form). If null=True, Django will store empty values as NULL in the database. If a field has blank=True, form validation will allow entry of an empty value. If a field has blank=False, the field will be required.

How do I install a Python package with a .whl file?

e9t

[How do I install a Python package with a .whl file?](https://stackoverflow.com/questions/27885397/how-do-i-install-a-python-package-with-a-whl-file)

I'm having trouble installing a Python package on my Windows machine, and would like to install it with Christoph Gohlke's Window binaries. (Which, to my experience, alleviated much of the fuss for many other package installations). However, only .whl files are available.http://www.lfd.uci.edu/~gohlke/pythonlibs/#jpypeBut how do I install .whl files?

2015-01-11 08:48:34Z

I'm having trouble installing a Python package on my Windows machine, and would like to install it with Christoph Gohlke's Window binaries. (Which, to my experience, alleviated much of the fuss for many other package installations). However, only .whl files are available.http://www.lfd.uci.edu/~gohlke/pythonlibs/#jpypeBut how do I install .whl files?I just used the following which was quite simple. First open a console then cd to where you've downloaded your file like some-package.whl and useNote: if pip.exe is not recognized, you may find it in the "Scripts" directory from where python has been installed. If pip is not installed, this page can help:

How do I install pip on Windows?Note: for clarification

If you copy the *.whl file to your local drive (ex. C:\some-dir\some-file.whl) use the following command line parameters --  First, make sure you have updated pip to enable wheel support:Then, to install from wheel, give it the directory where the wheel is downloaded. For example, to install package_name.whl:I am in the same boat as the OP.Using a Windows command prompt, from directory:seemed to work.Changing directory to where the whl was located, it just tells me 'pip is not recognized'. Going back to C:\Python34\Scripts>, then using the full command above to provide the 'where/its/downloaded' location, it says Requirement 'scikit_image-...-win32.whl' looks like a filename, but the filename does not exist.So I dropped a copy of the .whl in Python34/Scripts, ran the exact same command over again (with the --find-links= still going to the other folder), and this time it worked.There are several file versions on the great Christoph Gohlke's site.Something I have found important when installing wheels from this site is to first run this from the Python console:so that you know which version you should install for your computer. Picking the wrong version may fail the installing of the package (especially if you don't use the right CPython tag, for example, cp27).You have to run pip.exe from the command prompt on my computer. 

I type C:/Python27/Scripts/pip2.exe install numpyOn Windows you can't just upgrade using pip install --upgrade pip, because the pip.exe is in use and there would be an error replacing it. Instead, you should upgrade pip like this:Then check the pip version:If it shows 6.x series, there is wheel support.Only then, you can install a wheel package like this:To be able to install wheel files with a simple doubleclick on them you can do one the following:1) Run two commands in command line under administrator privileges:2) Alternatively, they can be copied into a wheel.bat file and executed with 'Run as administrator' checkbox in the properties.PS pip.exe is assumed to be in the PATH.Update:(1) Those can be combined in one line:(2) Syntax for .bat files is slightly different:Also its output can be made more verbose:see my blog post for details.In-case if you unable to install specific package directly using PIP.You can download a specific .whl (wheel) package from - https://www.lfd.uci.edu/~gohlke/pythonlibs/ CD (Change directory) to that downloaded package and install it manually by -

pip install PACKAGENAME.whl

ex:

pip install ad3‑2.1‑cp27‑cp27m‑win32.whlEDIT: THIS NO LONGER IS A PART OF PIPTo avoid having to download such files, you can try:For more information, see this.I downloaded NumPy from here

    https://pypi.python.org/pypi/numpyPS.: I installed it on Windows 10.You can install the .whl file, using pip install filename. Though to use it in this form, it should be in the same directory as your command line, otherwise specify the complete filename, along with its address like pip install C:\Some\PAth\filename.Also make sure the .whl file is of the same platform as you are using, do a python -V to find out which version of Python you are running and if it is win32 or 64, install the correct version according to it.What I did was first updating the pip by using the command: 

pip install --upgrade pip and then I also installed wheel by using command: pip install wheel and then it worked perfectly Fine.Hope it works for you I guess.New Python users on Windows often forget to add Python's \Scripts directory to the PATH variable during the installation. I recommend to use the Python launcher and execute pip as a script with the -m switch. Then you can install the wheels for a specific Python version (if more than one are installed) and the Scripts directory doesn't have to be in the PATH. So open the command line, navigate (with the cd command) to the folder where the .whl file is located and enter:Replace 3.6 by your Python version or just enter -3 if the desired Python version appears first in the PATH. And with an active virtual environment: py -m pip install your_whl_file.whl.Of course you can also install packages from PyPI in this way, e.g.I would be suggesting you the exact way how to install .whl file.

Initially I faced many issues but then I solved it, Here is my trick to install .whl files.Follow The Steps properly in order to get a module imported3.Now, enter the command written belowThank you:)Download the package (.whl).Put the file inside the script folder of python directoryUse the command prompt to install the package.On the MacOS, with pip installed via MacPorts into the MacPorts python2.7, I had to use @Dunes solution:Where python was replaced by the MacPorts python in my case, which is python2.7 or python3.5 for me.The -m option is "Run library module as script" according to the manpage.(I had previously run sudo port install py27-pip  py27-wheel to install pip and wheel into my python 2.7 installation first.)

list comprehension vs. lambda + filter

Agos

[list comprehension vs. lambda + filter](https://stackoverflow.com/questions/3013449/list-comprehension-vs-lambda-filter)

I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this:But then I thought, wouldn't it be better to write it like this?It's more readable, and if needed for performance the lambda could be taken out to gain something.  Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way™ entirely and should do it in yet another way (such as using itemgetter instead of the lambda)?

2010-06-10 10:14:00Z

I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this:But then I thought, wouldn't it be better to write it like this?It's more readable, and if needed for performance the lambda could be taken out to gain something.  Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way™ entirely and should do it in yet another way (such as using itemgetter instead of the lambda)?It is strange how much beauty varies for different people. I find the list comprehension much clearer than filter+lambda, but use whichever you find easier.There are two things that may slow down your use of filter.The first is the function call overhead: as soon as you use a Python function (whether created by def or lambda) it is likely that filter will be slower than the list comprehension. It almost certainly is not enough to matter, and you shouldn't think much about performance until you've timed your code and found it to be a bottleneck, but the difference will be there.The other overhead that might apply is that the lambda is being forced to access a scoped variable (value). That is slower than accessing a local variable and in Python 2.x the list comprehension only accesses local variables. If you are using Python 3.x the list comprehension runs in a separate function so it will also be accessing value through a closure and this difference won't apply.The other option to consider is to use a generator instead of a list comprehension:Then in your main code (which is where readability really matters) you've replaced both list comprehension and filter with a hopefully meaningful function name.This is a somewhat religious issue in Python. Even though Guido considered removing map, filter and reduce from Python 3, there was enough of a backlash that in the end only reduce was moved from built-ins to functools.reduce.Personally I find list comprehensions easier to read. It is more explicit what is happening from the expression [i for i in list if i.attribute == value] as all the behaviour is on the surface not inside the filter function.I would not worry too much about the performance difference between the two approaches as it is marginal. I would really only optimise this if it proved to be the bottleneck in your application which is unlikely.Also since the BDFL wanted filter gone from the language then surely that automatically makes list comprehensions more Pythonic ;-)Since any speed difference is bound to be miniscule, whether to use filters or list comprehensions comes down to a matter of taste. In general I'm inclined to use comprehensions (which seems to agree with most other answers here), but there is one case where I prefer filter. A very frequent use case is pulling out the values of some iterable X subject to a predicate P(x):but sometimes you want to apply some function to the values first:

As a specific example, considerI think this looks slightly better than using filter. But now considerIn this case we want to filter against the post-computed value. Besides the issue of computing the cube twice (imagine a more expensive calculation), there is the issue of writing the expression twice, violating the DRY aesthetic. In this case I'd be apt to useAlthough filter may be the "faster way", the "Pythonic way" would be not to care about such things unless performance is absolutely critical (in which case you wouldn't be using Python!).I thought I'd just add that in python 3, filter() is actually an iterator object, so you'd have to pass your filter method call to list() in order to build the filtered list. So in python 2:lists b and c have the same values, and were completed in about the same time as filter() was equivalent [x for x in y if z]. However, in 3, this same code would leave list c containing a filter object, not a filtered list. To produce the same values in 3:The problem is that list() takes an iterable as it's argument, and creates a new list from that argument. The result is that using filter in this way in python 3 takes up to twice as long as the [x for x in y if z] method because you have to iterate over the output from filter() as well as the original list. An important difference is that list comprehension will return a list while the filter returns a filter, which you cannot manipulate like a list (ie: call len on it, which does not work with the return of filter).My own self-learning brought me to some similar issue.That being said, if there is a way to have the resulting list from a filter, a bit like you would do in .NET when you do lst.Where(i => i.something()).ToList(), I am curious to know it.EDIT: This is the case for Python 3, not 2 (see discussion in comments).I find the second way more readable. It tells you exactly what the intention is: filter the list.

PS: do not use 'list' as a variable namegenerally filter is slightly faster if using a builtin function.I would expect the list comprehension to be slightly faster in your case Filter is just that. It filters out the elements of a list. You can see the definition mentions the same(in the official docs link I mentioned before). Whereas, list comprehension is something that produces a new list after acting upon something on the previous list.(Both filter and list comprehension creates new list and not perform operation in place of the older list. A new list here is something like a list with, say, an entirely new data type. Like converting integers to string ,etc)In your example, it is better to use filter than list comprehension, as per the definition. However, if you want, say other_attribute from the list elements, in your example is to be retrieved as a new list, then you can use list comprehension.This is how I actually remember about filter and list comprehension. Remove a few things within a list and keep the other elements intact, use filter. Use some logic on your own at the elements and create a watered down list suitable for some purpose, use list comprehension.In addition to the accepted answer, there is a corner case when you should use filter instead of a list comprehension. If the list is unhashable you cannot directly process it with a list comprehension. A real world example is if you use pyodbc to read results from a database. The fetchAll() results from cursor is an unhashable list. In this situation, to directly manipulating on the returned results, filter should be used:If you use list comprehension here you will get the error:Here's a short piece I use when I need to filter on something after the list comprehension.  Just a combination of filter, lambda, and lists (otherwise known as the loyalty of a cat and the cleanliness of a dog).In this case I'm reading a file, stripping out blank lines, commented out lines, and anything after a comment on a line:It took me some time to get familiarized with the higher order functions filter and map. So i got used to them and i actually liked filter as it was explicit that it filters by keeping whatever is truthy and I've felt cool that I knew some functional programming terms. Then I read this passage (Fluent Python Book): And now I think, why bother with the concept of  filter / map if you can achieve it with already widely spread idioms like list comprehensions. Furthermore maps and filters are kind of functions. In this case I prefer using Anonymous functions lambdas. Finally, just for the sake of having it tested, I've timed both methods (map and listComp) and I didn't see any relevant speed difference that would justify making arguments about it. Curiously on Python 3, I see filter performing faster than list comprehensions.I always thought that the list comprehensions would be more performant.

Something like:

[name for name in brand_names_db if name is not None]

The bytecode generated is a bit better.But they are actually slower:My take

How do I remove/delete a folder that is not empty?

Amara

[How do I remove/delete a folder that is not empty?](https://stackoverflow.com/questions/303200/how-do-i-remove-delete-a-folder-that-is-not-empty)

I am getting an 'access is denied' error when I attempt to delete a folder that is not empty. I used the following command in my attempt: os.remove("/folder_name"). What is the most effective way of removing/deleting a folder/directory that is not empty?

2008-11-19 20:15:38Z

I am getting an 'access is denied' error when I attempt to delete a folder that is not empty. I used the following command in my attempt: os.remove("/folder_name"). What is the most effective way of removing/deleting a folder/directory that is not empty?Standard Library Reference: shutil.rmtree.By design, rmtree fails on folder trees containing read-only files. If you want the folder to be deleted regardless of whether it contains read-only files, then useFrom the python docs on os.walk():from python 3.4 you may use :where pth is a pathlib.Path instance. Nice, but may not be the fastest.From docs.python.org:If ignore_errors is set, errors are ignored; otherwise, if onerror is set, it is called to handle the error with arguments (func, path, exc_info) where func is os.listdir, os.remove, or os.rmdir; path is the argument to that function that caused it to fail; and exc_info is a tuple returned by sys.exc_info(). If ignore_errors is false and onerror is None, an exception is raised.enter code hereBase on kkubasik's answer, check if folder exists before remove, more robustif you are sure, that you want to delete the entire dir tree, and are no more interested in contents of dir, then crawling for entire dir tree is stupidness... just call native OS command from python to do that. It will be faster, efficient and less memory consuming. or *nix  In python, the code will look like..Just some python 3.5 options to complete the answers above. (I would have loved to find them here).Delete folder if emptyDelete also folder if it contains this filedelete folder if it contains only .srt or .txt file(s)Delete folder if its size is less than 400kb :I'd like to add a "pure pathlib" approach:This relies on the fact that Path is orderable, and longer paths will always sort after shorter paths, just like str. Therefore, directories will come before files. If we reverse the sort, files will then come before their respective containers, so we can simply unlink/rmdir them one by one with one pass.Benefits:If you don't want to use the shutil module you can just use the os module.To delete a folder even if it might not exist (avoiding the race condition in Charles Chow's answer) but still have errors when other things go wrong (e.g. permission problems, disk read error, the file isn't a directory)For Python 3.x:The Python 2.7 code is almost the same:Ten years later and using Python 3.7 and Linux there are still different ways to do this:Essentially it's using Python's subprocess module to run the bash script $ rm -rf '/path/to/your/dir as if you were using the terminal to accomplish the same task. It's not fully Python, but it gets it done. The reason I included the pathlib.Path example is because in my experience it's very useful when dealing with many paths that change. The extra steps of importing the pathlib.Path module and converting the end results to strings is often a lower cost to me for development time. It would be convenient if Path.rmdir() came with an arg option to explicitly handle non-empty dirs.You can use os.system command for simplicity:As obvious, it actually invokes system terminal to accomplish this task.With os.walk I would propose the solution which consists of 3 one-liner Python calls:The first script chmod's all sub-directories, the second script chmod's all files. Then the third script removes everything with no impediments.I have tested this from the "Shell Script" in a Jenkins job (I did not want to store a new Python script into SCM, that's why searched for a one-line solution) and it worked for Linux and Windows.I have found a very easy way to Delete any folder(Even NOT Empty) or file on WINDOWS OS.For Windows, if directory is not empty, and you have read-only files or you get errors like Try this, os.system('rmdir /S /Q "{}"'.format(directory))It's equivalent for rm -rf in Linux/Mac.

Getting key with maximum value in dictionary?

ricafeal

[Getting key with maximum value in dictionary?](https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary)

I have a dictionary: keys are strings, values are integers.Example: I'd like to get 'b' as an answer, since it's the key with a higher value.I did the following, using an intermediate list with reversed key-value tuples:Is that one the better (or even more elegant) approach?

2008-11-06 10:49:58Z

I have a dictionary: keys are strings, values are integers.Example: I'd like to get 'b' as an answer, since it's the key with a higher value.I did the following, using an intermediate list with reversed key-value tuples:Is that one the better (or even more elegant) approach?You can use operator.itemgetter for that:And instead of building a new list in memory use stats.iteritems(). The key parameter to the max() function is a function that computes a key that is used to determine how to rank items.Please note that if you were to have another key-value pair 'd': 3000 that this method will only return one of the two even though they both have the maximum value. If using Python3:I have tested MANY variants, and this is the fastest way to return the key of dict with the max value:To give you an idea, here are some candidate methods:The test dictionary:And the test results under Python 3.2:And under Python 2.7:You can see that f1 is the fastest under Python 3.2 and 2.7 (or, more completely, keywithmaxval at the top of this post)If you need to know only a key with the max value you can do it without iterkeys or iteritems because iteration through dictionary in Python is iteration through it's keys.EDIT:From comments, @user1274878 :Yep...The optional key argument describes how to compare elements to get maximum among them:Returned values will be compared.Python dict is a hash table. A key of dict is a hash of an object declared as a key. Due to performance reasons iteration though a dict implemented as iteration through it's keys.Therefore we can use it to rid operation of obtaining a keys list.The stats variable available through __closure__ attribute of the lambda function as a pointer to the value of the variable defined in the parent scope.Example:if you wanna find the max value with its key, maybe follwing could be simple, without any relevant functions.the output is the key which has the max value.Here is another one:The function key simply returns the value that should be used for ranking and max() returns the demanded element right away.If you don't care about value (I'd be surprised, but) you can do:I like the tuple unpacking better than a [0] subscript at the end of the expression.

I never like the readability of lambda expressions very much, but find this one better than the operator.itemgetter(1) IMHO.Given that more than one entry my have the max value. I would make a list of the keys that have the max value as their value.This will give you 'b' and any other max key as well.Note: For python 3 use stats.items() instead of stats.iteritems()To get the maximum key/value of the dictionary stats:>>> max(stats.items(), key = lambda x: x[0])

('c', 100)>>> max(stats.items(), key = lambda x: x[1])

('b', 3000)Of course, if you want to get only the key or value from the result, you can use tuple indexing. For Example, to get the key corresponding to the maximum value:>>> max(stats.items(), key = lambda x: x[1])[0]

'b'ExplanationThe dictionary method items() in Python 3 returns a view object of the dictionary. When this view object is iterated over, by the max function, it yields the dictionary items as tuples of the form (key, value).>>> list(stats.items())

[('c', 100), ('b', 3000), ('a', 1000)]When you use the lambda expression lambda x: x[1], in each iteration, x  is one of these tuples (key, value). So, by choosing the right index, you select whether you want to compare by keys or by values.Python 2For Python 2.2+ releases, the same code will work. However, it is better to use iteritems() dictionary method instead of items() for performance.NotesPer the iterated solutions via comments in the selected answer... In Python 3:In Python 2:I got here looking for how to return mydict.keys() based on the value of mydict.values(). Instead of just the one key returned, I was looking to return the top x number of values.This solution is simpler than using the max() function and you can easily change the number of values returned:If you want the single highest ranking key, just use the index:If you want the top two highest ranking keys, just use list slicing:With collections.Counter you could doIf appropriate, you could simply start with an empty collections.Counter and add to itI was not satisfied with any of these answers. max always picks the first key with the max value. The dictionary could have multiple keys with that value.Posting this answer in case it helps someone out.

See the below SO postWhich maximum does Python pick in the case of a tie?A heap queue is a generalised solution which allows you to extract the top n keys ordered by value:Note dict.__getitem__ is the method called by the syntactic sugar dict[]. As opposed to dict.get, it will return KeyError if a key is not found, which here cannot occur.max((value, key) for key, value in stats.items())[1]You can use: To return the key, value pair use:+1 to @Aric Coady's simplest solution. 

And also one way to random select one of keys with max value in the dictionary:I tested the accepted answer AND @thewolf's fastest solution against a very basic loop and the loop was faster than both:results:How about:For scientific python users, here is a simple solution using Pandas:In the case you have more than one key with the same value, for example:You could get a collection with all the keys with max value as follow:

Why does Python code run faster in a function?

thedoctar

[Why does Python code run faster in a function?](https://stackoverflow.com/questions/11241523/why-does-python-code-run-faster-in-a-function)

This piece of code in Python runs in  (Note: The timing is done with the time function in BASH in Linux.)However, if the for loop isn't placed within a function, then it runs for a much longer time:Why is this?

2012-06-28 09:18:34Z

This piece of code in Python runs in  (Note: The timing is done with the time function in BASH in Linux.)However, if the for loop isn't placed within a function, then it runs for a much longer time:Why is this?You might ask why it is faster to store local variables than globals. This is a CPython implementation detail.Remember that CPython is compiled to bytecode, which the interpreter runs. When a function is compiled, the local variables are stored in a fixed-size array (not a dict) and variable names are assigned to indexes. This is possible because you can't dynamically add local variables to a function. Then retrieving a local variable is literally a pointer lookup into the list and a refcount increase on the PyObject which is trivial.Contrast this to a global lookup (LOAD_GLOBAL), which is a true dict search involving a hash and so on. Incidentally, this is why you need to specify global i if you want it to be global: if you ever assign to a variable inside a scope, the compiler will issue STORE_FASTs for its access unless you tell it not to.By the way, global lookups are still pretty optimised. Attribute lookups foo.bar are the really slow ones!Here is small illustration on local variable efficiency.Inside a function, the bytecode isAt top level, the bytecode isThe difference is that STORE_FAST is faster (!) than STORE_NAME.  This is because in a function, i is a local but at toplevel it is a global.To examine bytecode, use the dis module.  I was able to disassemble the function directly, but to disassemble the toplevel code I had to use the compile builtin.Aside from local/global variable store times, opcode prediction makes the function faster.As the other answers explain, the function uses the STORE_FAST opcode in the loop. Here's the bytecode for the function's loop:Normally when a program is run, Python executes each opcode one after the other, keeping track of the a stack and preforming other checks on the stack frame after each opcode is executed. Opcode prediction means that in certain cases Python is able to jump directly to the next opcode, thus avoiding some of this overhead.In this case, every time Python sees FOR_ITER (the top of the loop), it will "predict" that STORE_FAST is the next opcode it has to execute. Python then peeks at the next opcode and, if the prediction was correct, it jumps straight to STORE_FAST. This has the effect of squeezing the two opcodes into a single opcode.On the other hand, the STORE_NAME opcode is used in the loop at the global level. Python does *not* make similar predictions when it sees this opcode. Instead, it must go back to the top of the evaluation-loop which has obvious implications for the speed at which the loop is executed.To give some more technical detail about this optimization, here's a quote from the ceval.c file (the "engine" of Python's virtual machine):We can see in the source code for the FOR_ITER opcode exactly where the prediction for STORE_FAST is made:The PREDICT function expands to if (*next_instr == op) goto PRED_##op i.e. we just jump to the start of the predicted opcode. In this case, we jump here:The local variable is now set and the next opcode is up for execution. Python continues through the iterable until it reaches the end, making the successful prediction each time.The Python wiki page has more information about how CPython's virtual machine works.

How to copy a dictionary and only edit the copy

MadSc13ntist

[How to copy a dictionary and only edit the copy](https://stackoverflow.com/questions/2465921/how-to-copy-a-dictionary-and-only-edit-the-copy)

Can someone please explain this to me? This doesn't make any sense to me.I copy a dictionary into another and edit the second and both are changed. Why is this happening?

2010-03-17 21:07:06Z

Can someone please explain this to me? This doesn't make any sense to me.I copy a dictionary into another and edit the second and both are changed. Why is this happening?Python never implicitly copies objects. When you set dict2 = dict1, you are making them refer to the same exact dict object, so when you mutate it, all references to it keep referring to the object in its current state.If you want to copy the dict (which is rare), you have to do so explicitly withorWhen you assign dict2 = dict1, you are not making a copy of dict1, it results in dict2 being just another name for dict1.To copy the mutable types like dictionaries, use copy / deepcopy of the copy module.While dict.copy() and dict(dict1) generates a copy, they are only shallow copies. If you want a deep copy, copy.deepcopy(dict1) is required. An example:Regarding shallow vs deep copies, from the Python copy module docs:On python 3.5+ there is an easier way to achieve a shallow copy by using the ** unpackaging operator. Defined by Pep 448.** unpackages the dictionary into a new dictionary that is then assigned to dict2. We can also confirm that each dictionary has a distinct id. If a deep copy is needed then copy.deepcopy() is still the way to go. The best and the easiest ways to create a copy of a dict in both Python 2.7 and 3 are... To create a copy of simple(single-level) dictionary:1. Using dict() method, instead of generating a reference that points to the existing dict.2. Using the built-in update() method of python dictionary. To create a copy of nested or complex dictionary:Use the built-in copy module, which provides a generic shallow and deep copy operations. This module is present in both Python 2.7 and 3.*You can also just make a new dictionary with a dictionary comprehension.  This avoids importing copy.Of course in python >= 2.7 you can do:But for backwards compat., the top method is better.In addition to the other provided solutions, you can use ** to integrate the dictionary into an empty dictionary, e.g.,shallow_copy_of_other_dict = {**other_dict}. Now you will have a "shallow" copy of other_dict. Applied to your example:Pointer: Difference between shallow and deep copysAssignment statements in Python do not copy objects, they create bindings between a target and an object. so, dict2 = dict1, it results another binding between dict2and the object that dict1 refer to.if you want to copy a dict, you can use the copy module.

The copy module has two interface:The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances):A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.For example, in python 2.7.9:and the result is:You can copy and edit the newly constructed copy in one go by calling the dict constructor with additional keyword arguments:This confused me too, initially, because I was coming from a C background.In C, a variable is a location in memory with a defined type. Assigning to a variable copies the data into the variable's memory location.But in Python, variables act more like pointers to objects. So assigning one variable to another doesn't make a copy, it just makes that variable name point to the same object.Every variable in python (stuff like dict1 or str or __builtins__ is a pointer to some hidden platonic "object" inside the machine.If you set dict1 = dict2,you just point dict1 to the same object (or memory location, or whatever analogy you like) as dict2. Now, the object referenced by dict1 is the same object referenced by dict2.You can check: dict1 is dict2 should be True. Also, id(dict1) should be the same as id(dict2).You want dict1 = copy(dict2), or dict1 = deepcopy(dict2).The difference between copy and deepcopy? deepcopy will make sure that the elements of dict2 (did you point it at a list?) are also copies.I don't use deepcopy much - it's usually poor practice to write code that needs it (in my opinion).dict1 is a symbol that references an underlying dictionary object. Assigning dict1 to dict2 merely assigns the same reference. Changing a key's value via the dict2 symbol changes the underlying object, which also affects dict1. This is confusing.It is far easier to reason about immutable values than references, so make copies whenever possible:This is syntactically the same as:dict2 = dict1 does not copy the dictionary. It simply gives you the programmer a second way (dict2) to refer to the same dictionary.There are many ways to copy Dict object, I simply useAs others have explained, the built-in dict does not do what you want. But in Python2 (and probably 3 too) you can easily create a ValueDict class that copies with = so you can be sure that the original will not change.Please refer to the lvalue modification pattern discussed here: Python 2.7 - clean syntax for lvalue modification. The key observation is that str and int behave as values in Python (even though they're actually immutable objects under the hood). While you're observing that, please also observe that nothing is magically special about str or int. dict can be used in much the same ways, and I can think of many cases where ValueDict makes sense.i ran into a peculiar behavior when trying to deep copy dictionary property of class w/o assigning it to variablenew = copy.deepcopy(my_class.a) doesn't work i.e. modifying new modifies my_class.abut if you do old = my_class.a and then new = copy.deepcopy(old) it works perfectly i.e. modifying new does not affect my_class.aI am not sure why this happens, but hope it helps save some hours! :)the following code, which is on dicts which follows json syntax more than 3 times faster than deepcopyCopying by using a for loop: because, dict2 = dict1, dict2 holds the reference to dict1. Both dict1 and dict2 points to the same location in the memory. This is just a normal case while working with mutable objects in python. When you are working with mutable objects in python you must be careful as it is hard to debug. Such as the following example.This example intention is to get all the user ids including blocked ids.

That we got from ids variable but we also updated the value of my_users unintentionally. when you extended the ids with blocked_ids my_users got updated because ids refer to my_users.You can use directly:where object dict2 is an independent copy of dict1, so you can modify dict2 without affecting dict1.This works for any kind of object.

How to make a class JSON serializable

Sergey

[How to make a class JSON serializable](https://stackoverflow.com/questions/3768895/how-to-make-a-class-json-serializable)

How to make a Python class serializable?  A simple class:What should I do to be able to get output of:Without the error

2010-09-22 11:52:19Z

How to make a Python class serializable?  A simple class:What should I do to be able to get output of:Without the errorDo you have an idea about the expected output? For e.g. will this do?In that case you can merely call json.dumps(f.__dict__). If you want more customized output then you will have to subclass JSONEncoder and implement your own custom serialization. For a trivial example, see below.Then you pass this class into the json.dumps() method as cls kwarg:If you also want to decode then you'll have to supply a custom object_hook to the JSONDecoder class. For e.g.Here is a simple solution for a simple feature:Instead of a JSON serializable class, implement a serializer method:So you just call it to serialize:will output:For more complex classes you could consider the tool jsonpickle:(link to jsonpickle on PyPi)Most of the answers involve changing the call to json.dumps(), which is not always possible or desirable (it may happen inside a framework component for example).If you want to be able to call  json.dumps(obj) as is, then a simple solution is inheriting from dict:This works if your class is just basic data representation, for trickier things you can always set keys explicitly.I like Onur's answer but would expand to include an optional toJSON() method for objects to serialize themselves:Another option is to wrap JSON dumping in its own class:Or, even better, subclassing FileItem class from a JsonSerializable class:Testing:Just add to_json method to your class like this:And add this code (from this answer), to somewhere at the top of everything:This will monkey-patch json module when it's imported so

JSONEncoder.default() automatically checks for a special "to_json()"

method and uses it to encode the object if found.Just like Onur said, but this time you don't have to update every json.dumps() in your project.I came across this problem the other day and implemented a more general version of an Encoder for Python objects that can handle nested objects and inherited fields:Example:Result:If you're using Python3.5+, you could use jsons. It will convert your object (and all its attributes recursively) to a dict.Or if you wanted a string:Or if your class implemented jsons.JsonSerializable:if use standard json, u need to define a default functionjson is limited in terms of objects it can print, and jsonpickle (you may need a pip install jsonpickle) is limited in terms it can't indent text. If you would like to inspect the contents of an object whose class you can't change, I still couldn't find a straighter way than:Note: that still they can't print the object methods. This class can do the trick, it converts object to standard json .usage:working in python2.7 and python3.jaraco gave a pretty neat answer. I needed to fix some minor things, but this works:Note that we need two steps for loading. For now, the __python__ property

is not used.Using the method of AlJohri, I check popularity of approaches:Serialization (Python -> JSON):Deserialization (JSON -> Python):This has worked well for me: and thenandIf you don't mind installing a package for it, you can use json-tricks:After that you just need to import dump(s) from json_tricks instead of json, and it'll usually work:which'll giveAnd that's basically it!This will work great in general. There are some exceptions, e.g. if special things happen in __new__, or more metaclass magic is going on.Obviously loading also works (otherwise what's the point):This does assume that module_name.test_class.MyTestCls can be imported and hasn't changed in non-compatible ways. You'll get back an instance, not some dictionary or something, and it should be an identical copy to the one you dumped.If you want to customize how something gets (de)serialized, you can add special methods to your class, like so:which serializes only part of the attributes parameters, as an example.And as a free bonus, you get (de)serialization of numpy arrays, date & times, ordered maps, as well as the ability to include comments in json.Disclaimer: I created json_tricks, because I had the same problem as you.jsonweb seems to be the best solution for me. See http://www.jsonweb.info/en/latest/Here is my 3 cents ...

This demonstrates explicit json serialization for a tree-like python object.

Note: If you actually wanted some code like this you could use the twisted 

FilePath class.I ran into this problem when I tried to store Peewee's model into PostgreSQL JSONField.After struggling for a while, here's the general solution.The key to my solution is going through Python's source code and realizing that the code documentation (described here) already explains how to extend the existing json.dumps to support other data types.Suppose you current have a model that contains some fields that are not serializable to JSON and the model that contains the JSON field originally looks like this:Just define a custom JSONEncoder like this:And then just use it in your JSONField like below:The key is the default(self, obj) method above. For every single ... is not JSON serializable complaint you receive from Python, just add code to handle the unserializable-to-JSON type (such as Enum or datetime)For example, here's how I support a class inheriting from Enum:Finally, with the code implemented like above, you can just convert any Peewee models to be a JSON-seriazable object like below:Though the code above was (somewhat) specific to Peewee, but I think:Any questions, please post in the comments section. Thanks!This is a small library that serializes an object with all its children to JSON and also parses it back:https://github.com/Toubs/PyJSONSerialization/I came up with my own solution. Use this method, pass any document (dict,list, ObjectId etc) to serialize.I chose to use decorators to solve the datetime object serialization problem.

Here is my code:By importing the above module, my other modules use json in a normal way (without specifying the default keyword) to serialize data that contains date time objects. The datetime serializer code is automatically called for json.dumps and json.dump.I liked Lost Koder's method the most. I ran into issues when trying to serialize more complex objects whos members/methods aren't serializable. Here's my implementation that works on more objects:If you are able to install a package, I'd recommend trying dill, which worked just fine for my project. A nice thing about this package is that it has the same interface as pickle, so if you have already been using pickle in your project you can simply substitute in dill and see if the script runs, without changing any code. So it is a very cheap solution to try!(Full anti-disclosure: I am in no way affiliated with and have never contributed to the dill project.)Install the package:Then edit your code to import dill instead of pickle:Run your script and see if it works. (If it does you may want to clean up your code so that you are no longer shadowing the pickle module name!)Some specifics on datatypes that dill can and cannot serialize, from the project page:I see no mention here of serial versioning or backcompat, so I will post my solution which I've been using for a bit. I probably have a lot more to learn from, specifically Java and Javascript are probably more mature than me here but here goeshttps://gist.github.com/andy-d/b7878d0044a4242c0498ed6d67fd50feTo add another option: You can use the attrs package and the asdict method.and to convert backclass looks like thisIn addition to the Onur's answer, You possibly want to deal with datetime type like below. (in order to handle: 'datetime.datetime' object has no attribute 'dict' exception.)Usage:First we need to make our object JSON-compliant, so we can dump it using the standard JSON module. I did it this way:There are many approaches to this problem.  'ObjDict'  (pip install objdict) is another.  There is an emphasis on providing javascript like objects which can also act like dictionaries to best handle data loaded from JSON, but there are other features which can be useful as well.  This provides another alternative solution to the original problem.

Multiprocessing vs Threading Python [duplicate]

John

[Multiprocessing vs Threading Python [duplicate]](https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python)

I am trying to understand the advantages of multiprocessing over threading. I know that multiprocessing gets around the Global Interpreter Lock, but what other advantages are there, and can threading not do the same thing?

2010-06-15 11:12:45Z

I am trying to understand the advantages of multiprocessing over threading. I know that multiprocessing gets around the Global Interpreter Lock, but what other advantages are there, and can threading not do the same thing?The threading module uses threads, the multiprocessing module uses processes. The difference is that threads run in the same memory space, while processes have separate memory. This makes it a bit harder to share objects between processes with multiprocessing. Since threads use the same memory, precautions have to be taken or two threads will write to the same memory at the same time. This is what the global interpreter lock is for.Spawning processes is a bit slower than spawning threads.Here are some pros/cons I came up with.Threading's job is to enable applications to be responsive. Suppose you have a database connection and you need to respond to user input. Without threading, if the database connection is busy the application will not be able to respond to the user. By splitting off the database connection into a separate thread you can make the application more responsive. Also because both threads are in the same process, they can access the same data structures - good performance, plus a flexible software design.Note that due to the GIL the app isn't actually doing two things at once, but what we've done is put the resource lock on the database into a separate thread so that CPU time can be switched between it and the user interaction. CPU time gets rationed out between the threads.Multiprocessing is for times when you really do want more than one thing to be done at any given time. Suppose your application needs to connect to 6 databases and perform a complex matrix transformation on each dataset. Putting each job in a separate thread might help a little because when one connection is idle another one could get some CPU time, but the processing would not be done in parallel because the GIL means that you're only ever using the resources of one CPU. By putting each job in a Multiprocessing process, each can run on it's own CPU and run at full efficiency.The key advantage is isolation. A crashing process won't bring down other processes, whereas a crashing thread will probably wreak havoc with other threads.Python documentation quotesThe canonical version of this answer is now at the dupliquee question: What are the differences between the threading and multiprocessing modules?I've highlighted the key Python documentation quotes about Process vs Threads and the GIL at: What is the global interpreter lock (GIL) in CPython?Process vs thread experimentsI did a bit of benchmarking in order to show the difference more concretely.In the benchmark, I timed CPU and IO bound work for various numbers of threads on an 8 hyperthread CPU. The work supplied per thread is always the same, such that more threads means more total work supplied.The results were:Plot data.Conclusions:Test code:GitHub upstream + plotting code on same directory.Tested on Ubuntu 18.10, Python 3.6.7, in a Lenovo ThinkPad P51 laptop with CPU: Intel Core i7-7820HQ CPU (4 cores / 8 threads), RAM: 2x Samsung M471A2K43BB1-CRC (2x 16GiB), SSD: Samsung MZVLB512HAJQ-000L7 (3,000 MB/s).Visualize which threads are running at a given timeThis post https://rohanvarma.me/GIL/ taught me that you can run a callback whenever a thread is scheduled with the target= argument of threading.Thread and the same for multiprocessing.Process.This allows us to view exactly which thread runs at each time. When this is done, we would see something like (I made this particular graph up): which would show that:Another thing not mentioned is that it depends on what OS you are using where speed is concerned. In Windows processes are costly so threads would be better in windows but in unix processes are faster than their windows variants so using processes in unix is much safer plus quick to spawn.As mentioned in the question, Multiprocessing in Python is the only real way to achieve true parallelism. Multithreading cannot achieve this because the GIL prevents threads from running in parallel. As a consequence, threading may not always be useful in Python, and in fact, may even result in worse performance depending on what you are trying to achieve. For example, if you are performing a CPU-bound task such as decompressing gzip files or 3D-rendering (anything CPU intensive) then threading may actually hinder your performance rather than help. In such a case, you would want to use Multiprocessing as only this method actually runs in parallel and will help distribute the weight of the task at hand. There could be some overhead to this since Multiprocessing involves copying the memory of a script into each subprocess which may cause issues for larger-sized applications.However, Multithreading becomes useful when your task is IO-bound. For example, if most of your task involves waiting on API-calls, you would use Multithreading because why not start up another request in another thread while you wait, rather than have your CPU sit idly by.TL;DROther answers have focused more on the multithreading vs multiprocessing aspect, but in python Global Interpreter Lock (GIL) has to be taken into account. When more number (say k) of threads are created, generally they will not increase the performance by  k  times, as it will still be running as a single threaded application. GIL is a global lock which locks everything out and allows only single thread execution  utilizing only a single core. The performance does increase in places where C extensions like numpy, Network, I/O are being used, where a lot of background work is done and GIL is released.  So when threading is used, there is only a single operating system level thread while python creates pseudo-threads which are completely managed by threading itself but are essentially running as a single process. Preemption takes place between these pseudo threads. If the CPU runs at maximum capacity, you may want to switch to multiprocessing.

Now in case of self-contained instances of execution, you can instead opt for pool. But in case of overlapping data, where you may want processes communicating you should use multiprocessing.Process.MULTIPROCESSINGMULTITHREADINGExample of Multi-threading and Multiprocessing using PythonPython 3 has the facility of Launching parallel tasks. This makes our work easier. It has for thread pooling and Process pooling. The following gives an insight:ThreadPoolExecutor ExampleProcessPoolExecutorThreads share the same memory space to guarantee that two threads don't share the same memory location so special precautions must be taken the CPython interpreter handles this using a mechanism called GIL, or the Global Interpreter Lockwhat is GIL(Just I want to Clarify GIL it's repeated above)?In CPython, the global interpreter lock, or GIL, is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary mainly because CPython's memory management is not thread-safe.For the main question, we can compare using Use Cases, How?1-Use Cases for Threading: in case of GUI programs threading can be used to make the application responsive  For example, in a text editing program, one thread can take care of recording the user inputs, another can be responsible for displaying the text, a third can do spell-checking, and so on. Here, the program has to wait for user interaction. which is the biggest bottleneck. Another use case for threading is programs that are IO bound or network bound, such as web-scrapers.2-Use Cases for Multiprocessing: Multiprocessing outshines threading in cases where the program is CPU intensive and doesn’t have to do any IO or user interaction.For More Details visit this link and link or you need in-depth knowledge for threading visit here for Multiprocessing visit hereProcess may have multiple threads. These threads may share memory and are the units of execution within a process. Processes run on the CPU, so  threads are residing under each process. Processes are individual entities which run independently. If you want to share data or state between each process, you may use a memory-storage tool such as Cache(redis, memcache), Files, or a Database.As I learnd in university most of the answers above are right. In PRACTISE on different platforms (always using python) spawning multiple threads ends up like spawning one process. The difference is the multiple cores share the load instead of only 1 core processing everything at 100%. So if you spawn for example 10 threads on a 4 core pc, you will end up getting only the 25% of the cpus power!! And if u spawn 10 processes u will end up with the cpu processing at 100% (if u dont have other limitations). Im not a expert in all the new technologies. Im answering with own real experience background

What does the 'b' character do in front of a string literal?

Jesse Webb

[What does the 'b' character do in front of a string literal?](https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal)

Apparently, the following is valid syntaxI would like to know:I found a related question right here on SO, but that question is about PHP though, and it states the b is used to indicate the string is binary, as opposed to Unicode, which was needed for code to be compatible from version of PHP < 6, when migrating to PHP 6. I don't think this applies to Python.I did find this documentation on the Python site about using a u character in the same syntax to specify a string as Unicode. Unfortunately, it doesn't mention the b character anywhere in that document.Also, just out of curiosity, are there more symbols than the b and u that do other things?

2011-06-07 18:14:52Z

Apparently, the following is valid syntaxI would like to know:I found a related question right here on SO, but that question is about PHP though, and it states the b is used to indicate the string is binary, as opposed to Unicode, which was needed for code to be compatible from version of PHP < 6, when migrating to PHP 6. I don't think this applies to Python.I did find this documentation on the Python site about using a u character in the same syntax to specify a string as Unicode. Unfortunately, it doesn't mention the b character anywhere in that document.Also, just out of curiosity, are there more symbols than the b and u that do other things?To quote the Python 2.x documentation:The Python 3 documentation states:Python 3.x makes a clear distinction between the types:If you're familiar with Java or C#, think of str as String and bytes as byte[].  If you're familiar with SQL, think of str as NVARCHAR and bytes as BINARY or BLOB.  If you're familiar with the Windows registry, think of str as REG_SZ and bytes as REG_BINARY.  If you're familiar with C(++), then forget everything you've learned about char and strings, because A CHARACTER IS NOT A BYTE.  That idea is long obsolete.You use str when you want to represent text.You use bytes when you want to represent low-level binary data like structs.You can encode a str to a bytes object.And you can decode a bytes into a str.But you can't freely mix the two types.The b'...' notation is somewhat confusing in that it allows the bytes 0x01-0x7F to be specified with ASCII characters instead of hex numbers.But I must emphasize, a character is not a byte.Pre-3.0 versions of Python lacked this kind of distinction between text and binary data.  Instead, there was:In order to ease the 2.x-to-3.x transition, the b'...' literal syntax was backported to Python 2.6, in order to allow distinguishing binary strings (which should be bytes in 3.x) from text strings (which should be str in 3.x).  The b prefix does nothing in 2.x, but tells the 2to3 script not to convert it to a Unicode string in 3.x.So yes, b'...' literals in Python have the same purpose that they do in PHP.The r prefix creates a raw string (e.g., r'\t' is a backslash + t instead of a tab), and triple quotes '''...''' or """...""" allow multi-line string literals.The b denotes a byte string.Bytes are the actual data. Strings are an abstraction.If you had multi-character string object and you took a single character, it would be a string, and it might be more than 1 byte in size depending on encoding.If took 1 byte with a byte string, you'd get a single 8-bit value from 0-255 and it might not represent a complete character if those characters due to encoding were > 1 byte.TBH I'd use strings unless I had some specific low level reason to use bytes.From server side, if we send any response, it will be sent in the form of byte type, so it will appear in the client as b'Response from server'In order get rid of b'....' simply use below code:Server file:Client file:then it will print Response from serverHere's an example where the absence of b would throw a TypeError exception in Python 3.xAdding a b prefix would fix the problem.It turns it into a bytes literal (or str in 2.x), and is valid for 2.6+.The r prefix causes backslashes to be "uninterpreted" (not ignored, and the difference does matter).In addition to what others have said, note that a single character in unicode can consist of multiple bytes. The way unicode works is that it took the old ASCII format (7-bit code that looks like 0xxx xxxx) and added multi-bytes sequences where all bytes start with 1 (1xxx xxxx) to represent characters beyond ASCII so that Unicode would be backwards-compatible with ASCII.You can use JSON to convert it to dictionary{"key":"value"}FLASK: This is an example from flask. Run this on terminal line:In flask/routes.py{'key':'value'}

How to import other Python files?

Tamer

[How to import other Python files?](https://stackoverflow.com/questions/2349991/how-to-import-other-python-files)

How do I import other files in Python?For example, in main.py I have:Although this gives me all the definitions in extra.py, when maybe all I want is a single definition:What do I add to the import statement to just get gap from extra.py?

2010-02-28 03:40:11Z

How do I import other files in Python?For example, in main.py I have:Although this gives me all the definitions in extra.py, when maybe all I want is a single definition:What do I add to the import statement to just get gap from extra.py?importlib is recent addition in Python to programmatically import a module. It just a wrapper around __import__ 

See https://docs.python.org/3/library/importlib.html#module-importlibUpdate: Answer below is outdated. Use the more recent alternative above.Don't just hastily pick the first import strategy that works for you or else you'll have to rewrite the codebase later on when you find it doesn't meet your needs.  I'll start out explaining the easiest example #1, then I'll move toward the most professional and robust example #7Example 1, Import a python module with python interpreter:Example 2, Use execfile or (exec in Python 3) in a script to execute the other python file in place:Example 3, Use from ... import ... functionality:Example 4, Import riaa.py if it's in a different file location from where it is importedExample 5, use os.system("python yourfile.py")Example 6, import your file via piggybacking the python startuphook:Update: This example used to work for both python2 and 3, but now only works for python2.  python3 got rid of this user startuphook feature set because it was abused by low-skill python library writers, using it to impolitely inject their code into the global namespace, before all user-defined programs.  If you want this to work for python3, you'll have to get more creative.  If I tell you how to do it, python developers will disable that feature set as well, so you're on your own.  See: https://docs.python.org/2/library/user.htmlPut this code into your home directory in ~/.pythonrc.pyPut this code into your main.py (can be anywhere):Run it, you should get this:If you get an error here: ModuleNotFoundError: No module named 'user' then it means you're using python3, startuphooks are disabled there by default.Credit for this jist goes to: https://github.com/docwhat/homedir-examples/blob/master/python-commandline/.pythonrc.py  Send along your up-boats.Example 7, Most Robust: Import files in python with the bare import command:If you want to see my post on how to include ALL .py files under a directory see here: https://stackoverflow.com/a/20753073/445131To import a specific Python file at 'runtime' with a known name:...You do not have many complex methods to import a python file from one folder to another. Just create a __init__.py file to declare this folder is a python package and then go to your host file where you want to import just type from root.parent.folder.file import variable, class, whateverImport doc .. -- Link for reference The __init__.py files are required to make Python treat the directories as containing packages, this is done to prevent directories with a common name, such as string, from unintentionally hiding valid modules that occur later on the module search path.__init__.py can just be an empty file, but it can also execute initialization code for the package or set the __all__ variable.and Here are the two simple ways I have understood by now and make sure your "file.py" file which you want to import as a library is present in your current directory only.First case: You want to import file A.py in file B.py, these two files are in the same folder, like this:You can do this in file B.py:or or Then you will be able to use all the functions of file A.py in file B.pySecond case: You want to import file folder/A.py in file B.py, these two files are not in the same folder, like this:You can do this in file B:ororThen you will be able to use all the functions of file A.py in file B.pySummary:

In the first case, file A.py is a module that you imports in file B.py, you used the syntax import module_name. In the second case, folder is the package that contains the module A.py, you used the syntax import package_name.module_name.For more info on packages and modules, consult this link.the best way to import .py files is by way of __init__.py. the simplest thing to do, is to create an empty file named __init__.py in the same directory that your.py file is located.this post by Mike Grouchy is a great explanation of __init__.py and its use for making, importing, and setting up python packages.How I import is import the file and use shorthand of it's name.Don't forget that your importing file MUST BE named with .py extensionI'd like to add this note I don't very clearly elsewhere; inside a module/package, when loading from files, the module/package name must be prefixed with the mymodule. Imagine mymodule being layout like this:When loading somefile.py/otherstuff.py from __init__.py the contents should look like:If the function defined is in a file x.py:In the file where you are importing the function, write this:This is useful if you do not wish to import all the functions in a file.In case the module you want to import is not in a sub-directory, then try the following and run app.py from the deepest common parent directory:Directory Structure:In app.py, append path of client to sys.path:Optional (If you load e.g. configs) (Inspect seems to be the most robust one for my use cases)RunThis solution works for me in cli, as well as PyCharm.There are couple of ways of including your python script with name abc.pyIn case your python script gets updated and you don't want to upload - use these statements for auto refresh. Bonus :)Just to import python file in another python filelets say I have helper.py python file which has a display function like,Now in app.py, you can use the display function,The output,I'm working sundar gsvNOTE: No need to specify the .py extension.This may sound crazy but you can just create a symbolic link to the file you want to import if you're just creating a wrapper script to it.You can also do this: from filename import somethingexample: from client import Client

Note that you do not need the .py .pyw .pyui extension.This is how I did to call a function from a python file, that is flexible for me to call any functions.There are many ways, as listed above, but I find that I just want to import he contents of a file, and don't want to have to write lines and lines and have to import other modules. So, I came up with a way to get the contents of a file, even with the dot syntax (file.property) as opposed to merging the imported file with yours.

First of all, here is my file which I'll import, data.py Note: You could use the .txt extension instead.

In mainfile.py, start by opening and getting the contents.Now you have the contents as a string, but trying to access data.testString will cause an error, as data is an instance of the str class, and even if it does have a property testString it will not do what you expected.

Next, create a class. For instance (pun intended), ImportedFileAnd put this into it (with the appropriate indentation):

And finally, re-assign data like so:And that's it! Just access like you would for any-other module, typing print(data.testString) will print to the console A string literal to import and test with.

If, however, you want the equivalent of from mod import * just drop the class, instance assignment, and de-dent the exec.Hope this helps:)

-BenjiOne very unknown feature of Python is the ability to import zip files:The file __init__.py of the package contains the following:We can write another script which can import a package from the zip archive. It is only necessary to add the zip file to the sys.path.

What is the purpose and use of **kwargs?

Federer

[What is the purpose and use of **kwargs?](https://stackoverflow.com/questions/1769403/what-is-the-purpose-and-use-of-kwargs)

What are the uses for **kwargs in Python?I know you can do an objects.filter on a table and pass in a **kwargs argument.  Can I also do this for specifying time deltas i.e. timedelta(hours = time1)? How exactly does it work? Is it classes as 'unpacking'? Like a,b=1,2?

2009-11-20 09:40:57Z

What are the uses for **kwargs in Python?I know you can do an objects.filter on a table and pass in a **kwargs argument.  Can I also do this for specifying time deltas i.e. timedelta(hours = time1)? How exactly does it work? Is it classes as 'unpacking'? Like a,b=1,2?You can use **kwargs to let your functions take an arbitrary number of keyword arguments ("kwargs" means "keyword arguments"):You can also use the **kwargs syntax when calling functions by constructing a dictionary of keyword arguments and passing it to your function:The Python Tutorial contains a good explanation of how it works, along with some nice examples.<--Update-->For people using Python 3, instead of iteritems(), use items()** unpacks dictionaries.Thisis the same asIt's useful if you have to construct parameters:This lets you use the function like this:kwargs is just a dictionary that is added to the parameters.A dictionary can contain key, value pairs. And that are the kwargs. Ok, this is how.The what for is not so simple.For example (very hypothetical) you have an interface that just calls other routines to do the job:Now you get a new method "drive":But wait a minute, there is a new parameter "vehicle" -- you did not know it before. Now you must add it to the signature of the myDo-function.Here you can throw kwargs into play -- you just add kwargs to the signature:This way you don't need to change the signature of your interface function every time some of your called routines might change.This is just one nice example you could find kwargs helpful.On the basis that a good sample is sometimes better than a long discourse I will write two functions using all python variable argument passing facilities (both positional and named arguments). You should easily be able to see what it does by yourself:And here is the output:Motif: *args and **kwargs serves as a placeholder for the arguments that need to be passed to a function callusing *args and **kwargs to call a function  Now we'll use *args to call the above defined function  arg1: two

arg2: 3

arg3: 5Now, using **kwargs to call the same functionarg1: 5

arg2: two

arg3: 3  Bottomline : *args has no intelligence, it simply interpolates the passed args to the parameters(in left-to-right order) while **kwargs behaves intelligently by placing the appropriate value @ the required placeSo being said that let me explain "named arguments" first and then "arbitrary number of named arguments" kwargs.Named argumentsArbitrary number of named arguments kwargsPassing tuple and dict variables for custom argsTo finish it up, let me also note that we can pass Thus the same above call can be made as follows:Finally note * and ** in function calls above. If we omit them, we may get ill results.Omitting * in tuple args:prints Above tuple ('custom param1', 'custom param2', 'custom param3') is printed as is. Omitting dict args:gives As an addition, you can also mix different ways of usage when calling kwargs functions:gives this output:Note that **kwargs has to be the last argumentkwargs are a syntactic sugar to pass name arguments as dictionaries(for func), or dictionaries as named arguments(to func)Here's a simple function that serves to explain the usage:Any arguments that are not specified in the function definition will be put in the args list, or the kwargs list, depending on whether they are keyword arguments or not:If you add a keyword argument that never gets passed to a function, an error will be raised:Here is an example that I hope is helpful:When you run the program, you get:The key take away here is that the variable number of named arguments in the call translate into a dictionary in the function. This is the simple example to understand about python unpacking,eg1:In Java, you use constructors to overload classes and allow for multiple input parameters. In python, you can use kwargs to provide similar behavior.java example: https://beginnersbook.com/2013/05/constructor-overloading/python example:just another way to think about it.

How do I get the full path of the current file's directory?

Shubham

[How do I get the full path of the current file's directory?](https://stackoverflow.com/questions/3430372/how-do-i-get-the-full-path-of-the-current-files-directory)

I want to get the current file's directory path.

I tried:But how can I retrieve the directory's path?For example:

2010-08-07 12:17:52Z

I want to get the current file's directory path.

I tried:But how can I retrieve the directory's path?For example:For the directory of the script being run:For the current working directory:For the directory of the script being run:If you mean the current working directory:Note that before and after file is two underscores, not just one. Also note that if you are running interactively or have loaded code from something other than a file (eg: a database or online resource), __file__ may not be set since there is no notion of "current file". The above answer assumes the most common scenario of running a python script that is in a file. Using Path is the recommended way since Python 3:Documentation: pathlibNote: If using Jupyter Notebook, __file__ doesn't return expected value, so Path().absolute() has to be used.In Python 3.x I do:Explanation:Using pathlib is the modern way to work with paths. If you need it as a string later for some reason, just do str(path).You can use os and os.path library easily as followsos.path.dirname returns upper directory from current one. 

It lets us change to an upper level without passing any file argument and without knowing absolute path.Try this:USEFUL PATH PROPERTIES IN PYTHON:OUTPUT:

ABSOLUTE PATH IS THE PATH WHERE YOUR PYTHON FILE IS PLACEDAbsolute path : D:\Study\Machine Learning\Jupitor Notebook\JupytorNotebookTest2\Udacity_Scripts\Matplotlib and seaborn Part2File path : D:\Study\Machine Learning\Jupitor Notebook\JupytorNotebookTest2\Udacity_Scripts\Matplotlib and seaborn Part2\data\fuel_econ.csvisfileExist : Trueisadirectory : FalseFile extension : .csvI found the following commands will all return the full path of the parent directory of a Python 3.6 script. Python 3.6 Script:Explanation links: .resolve(), .absolute(), Path(file).parent().absolute() IPython has a magic command %pwd to get the present working directory. It can be used in following way:On IPython Jupyter Notebook %pwd can be used directly as following:To keep the migration consistency across platforms (macOS/Windows/Linux), try:I have made a function to use when running python under IIS in CGI in order to get the current folder:System: MacOSVersion: Python 3.6 w/ Anacondaimport os

 rootpath = os.getcwd()

 os.chdir(rootpath)Let's assume you have the following directory structure: -main/

    fold1

    fold2

    fold3...

Remove all whitespace in a string in Python

co2f2e

[Remove all whitespace in a string in Python](https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string-in-python)

I want to eliminate all the whitespace from a string, on both ends, and in between words.I have this Python code:But that only eliminates the whitespace on both sides of the string. How do I remove all whitespace?

2011-11-25 13:51:21Z

I want to eliminate all the whitespace from a string, on both ends, and in between words.I have this Python code:But that only eliminates the whitespace on both sides of the string. How do I remove all whitespace?If you want to remove leading and ending spaces, use str.strip(): If you want to remove all space characters, use str.replace():(NB this only removes the「normal」ASCII space character ' ' U+0020 but not any other whitespace)If you want to remove duplicated spaces, use str.split():To remove only spaces use str.replace:To remove all whitespace characters (space, tab, newline, and so on) you can use split then join:or a regular expression:If you want to only remove whitespace from the beginning and end you can use strip:You can also use lstrip to remove whitespace only from the beginning of the string, and rstrip to remove whitespace from the end of the string.An alternative is to use regular expressions and match these strange white-space characters too. Here are some examples:Remove ALL spaces in a string, even between words:Remove spaces in the BEGINNING of a string:Remove spaces in the END of a string:Remove spaces both in the BEGINNING and in the END of a string:Remove ONLY DUPLICATE spaces:     (All examples work in both Python 2 and Python 3)Whitespace includes space, tabs, and CRLF. So an elegant and one-liner string function we can use is str.translate:OR if you want to be thorough:For removing whitespace from beginning and end, use strip.MaK already pointed out the "translate" method above. And this variation works with Python 3 (see this Q&A).Be careful:strip does a rstrip and lstrip (removes leading and trailing spaces, tabs, returns and form feeds, but it does not remove them in the middle of the string).If you only replace spaces and tabs you can end up with hidden CRLFs that appear to match what you are looking for, but are not the same.In addition, strip has some variations:Remove spaces in the BEGINNING and END of a string:Remove spaces in the BEGINNING of a string:Remove spaces in the END of a string:All three string functions strip lstrip, and rstrip can take parameters of the string to strip, with the default being all white space. This can be helpful when you are working with something particular, for example, you could remove only spaces but not newlines:Or you could remove extra commas when reading in a string list:

Import a module from a relative path

Jude Allred

[Import a module from a relative path](https://stackoverflow.com/questions/279237/import-a-module-from-a-relative-path)

How do I import a Python module given its relative path?For example, if dirFoo contains Foo.py and dirBar, and dirBar contains Bar.py, how do I import Bar.py into Foo.py?Here's a visual representation:Foo wishes to include Bar, but restructuring the folder hierarchy is not an option.

2008-11-10 21:28:48Z

How do I import a Python module given its relative path?For example, if dirFoo contains Foo.py and dirBar, and dirBar contains Bar.py, how do I import Bar.py into Foo.py?Here's a visual representation:Foo wishes to include Bar, but restructuring the folder hierarchy is not an option.Assuming that both your directories are real Python packages (do have the __init__.py file inside them), here is a safe solution for inclusion of modules relatively to the location of the script.I assume that you want to do this, because you need to include a set of modules with your script. I use this in production in several products and works in many special scenarios like: scripts called from another directory or executed with python execute instead of opening a new interpreter.As a bonus, this approach does let you force Python to use your module instead of the ones installed on the system.Warning! I don't really know what is happening when current module is inside an egg file. It probably fails too.Be sure that dirBar has the __init__.py file -- this makes a directory into a Python package.You could also add the subdirectory to your Python path so that it imports as a normal script.Just do simple things to import the .py file from a different folder.Let's say you have a directory like:Then just keep an empty file in lib folder as namedAnd then useKeep the __init__.py file in every folder of the hierarchy of the import module.If you structure your project this way:Then from Foo.py you should be able to do:Or:Per Tom's comment, this does require that the src folder is accessible either via site_packages or your search path. Also, as he mentions, __init__.py is implicitly imported when you first import a module in that package/directory. Typically __init__.py is simply an empty file.The easiest method is to use sys.path.append().However, you may be also interested in the imp module.

It provides access to internal import functions.This can be used to load modules dynamically when you don't know a module's name.I've used this in the past to create a plugin type interface to an application, where the user would write a script with application specific functions, and just drop thier script in a specific directory.Also, these functions may be useful:This is the relevant PEP:http://www.python.org/dev/peps/pep-0328/In particular, presuming dirFoo is a directory up from dirBar...In dirFoo\Foo.py:The easiest way without any modification to your script is to set PYTHONPATH environment variable. Because sys.path is initialized from these locations:Just run:You sys.path will contains above path, as show below:In my opinion the best choice is to put __ init __.py in the folder and call the file withIt is not recommended to use sys.path.append() because something might gone wrong if you use the same file name as the existing python package. I haven't test that but that will be ambiguous.If you are just tinkering around and don't care about deployment issues, you can use a symbolic link (assuming your filesystem supports it) to make the module or package directly visible in the folder of the requesting module.orNote: A "module" is any file with a .py extension and a "package" is any folder that contains the file __init__.py (which can be an empty file).  From a usage standpoint, modules and packages are identical -- both expose their contained "definitions and statements" as requested via the import command.See: http://docs.python.org/2/tutorial/modules.htmlinstead of:just in case there could be another dirBar installed and confuse a foo.py reader.For this case to import Bar.py into Foo.py, first I'd turn these folders into Python packages like so:Then I would do it like this in Foo.py:If I wanted the namespacing to look like Bar.whatever, orIf I wanted the namespacing dirBar.Bar.whatever. This second case is useful if you have more modules under the dirBar package.Add an __init__.py file:Then add this code to the start of Foo.py:Relative sys.path example:Based on this answer.Well, as you mention, usually you want to have access to a folder with your modules relative to where your main script is run, so you just import them.Solution:I have the script in D:/Books/MyBooks.py and some modules (like oldies.py). I need to import from subdirectory D:/Books/includes:Place a print('done') in oldies.py, so you verify everything is going OK.  This way always works because by the Python definition sys.path as initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter.If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH.Simply you can use: from Desktop.filename import somethingExample: the code:But make sure you make an empty file  called "__init__.py" in that directoryAnother solution would be to install the py-require package and then use the following in Foo.pyHere's a way to import a file from one level above, using the relative path.Basically, just move the working directory up a level (or any relative location), add that to your path, then move the working directory back where it started.I'm not experienced about python, so if there is any wrong in my words, just tell me. If your file hierarchy arranged like this:module_1.py defines a function called func_1(), module_2.py:and you run python module_2.py in cmd, it will do run what func_1() defines. That's usually how we import same hierarchy files. But when you write from .module_1 import func_1 in module_2.py, python interpreter will say No module named '__main__.module_1'; '__main__' is not a package. So to fix this, we just keep the change we just make, and move both of the module to a package, and make a third module as a caller to run module_2.py.main.py:But the reason we add a . before module_1 in module_2.py is that if we don't do that and run main.py, python interpreter will say No module named 'module_1', that's a little tricky, module_1.py is right beside module_2.py. Now I let func_1() in module_1.py do something:that __name__ records who calls func_1. Now we keep the . before module_1 , run main.py, it will print package_1.module_1, not module_1. It indicates that the one who calls func_1() is at the same hierarchy as main.py, the . imply that module_1 is at the same hierarchy as module_2.py itself. So if there isn't a dot, main.py will recognize module_1 at the same hierarchy as itself, it can recognize package_1, but not what "under" it.Now let's make it a bit complicated. You have a config.ini and a module defines a function to read it at the same hierarchy as 'main.py'.And for some unavoidable reason, you have to call it with module_2.py, so it has to import from upper hierarchy.module_2.py:Two dots means import from upper hierarchy (three dots access upper than upper,and so on). Now we run main.py, the interpreter will say:ValueError:attempted relative import beyond top-level package. The "top-level package" at here is main.py. Just because config.py is beside main.py, they are at same hierarchy, config.py isn't "under" main.py, or it isn't "leaded" by main.py, so it is beyond main.py. To fix this, the simplest way is:I think that is coincide with the principle of arrange project file hierarchy, you should arrange modules with different function in different folders, and just leave a top caller in the outside, and you can import how ever you want.This also works, and is much simpler than anything with the sys module:Call me overly cautious, but I like to make mine more portable because it's unsafe to assume that files will always be in the same place on every computer. Personally I have the code look up the file path first. I use Linux so mine would look like this:That is of course unless you plan to package these together. But if that's the case you don't really need two separate files anyway.

Shuffling a list of objects

utdiscant

[Shuffling a list of objects](https://stackoverflow.com/questions/976882/shuffling-a-list-of-objects)

I have a list of objects and I want to shuffle them. I thought I could use the random.shuffle method, but this seems to fail when the list is of objects. Is there a method for shuffling objects or another way around this?This will fail.

2009-06-10 16:56:59Z

I have a list of objects and I want to shuffle them. I thought I could use the random.shuffle method, but this seems to fail when the list is of objects. Is there a method for shuffling objects or another way around this?This will fail.random.shuffle should work.  Here's an example, where the objects are lists:Note that shuffle works in place, and returns None.As you learned the in-place shuffling was the problem. I also have problem frequently, and often seem to forget how to copy a list, too. Using sample(a, len(a)) is the solution, using len(a) as the sample size. See https://docs.python.org/3.6/library/random.html#random.sample for the Python documentation.Here's a simple version using random.sample() that returns the shuffled result as a new list.It took me some time to get that too. But the documentation for shuffle is very clear:So you shouldn't print(random.shuffle(b)). Instead do random.shuffle(b) and then print(b).If you happen to be using numpy already (very popular for scientific and financial applications) you can save yourself an import.http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.htmlIt works fine for me.  Make sure to set the random method.If you have multiple lists, you might want to define the permutation (the way you shuffle the list / rearrange the items in the list) first and then apply it to all lists:If your lists are numpy arrays, it is simpler:I've created the small utility package mpu which has the consistent_shuffle function:Note that mpu.consistent_shuffle takes an arbitrary number of arguments. So you can also shuffle three or more lists with it.This alternative may be useful for some applications where you want to swap the ordering function.In some cases when using numpy arrays, using random.shuffle created duplicate data in the array. An alternative is to use numpy.random.shuffle. If you're working with numpy already, this is the preferred method over the generic random.shuffle. numpy.random.shuffleExampleUsing random.shuffle:Using numpy.random.shuffle:'print func(foo)' will print the return value of 'func' when called with 'foo'.

'shuffle' however has None as its return type, as the list will be modified in place, hence it prints nothing.

Workaround:If you're more into functional programming style you might want to make the following wrapper function:For one-liners, userandom.sample(list_to_be_shuffled, length_of_the_list) with an example:outputs:

[2, 9, 7, 8, 3, 0, 4, 1, 6, 5]One can define a function called shuffled (in the same sense of sort vs sorted)shuffle  is in place, so do not print result, which is None, but the list. You can go for this:if you want to go back to two lists, you then split this long list into two.you could build a function that takes a list as a parameter and returns a shuffled version of the list:you can either use shuffle or sample . both of which come from random module.ORMake sure you are not naming your source file random.py, and that there is not a file in your working directory called random.pyc..  either could cause your program to try and import your local random.py file instead of pythons random module.  The shuffling process is "with replacement", so the occurrence of each item may change! At least when when items in your list is also list. E.g.,After, The number of [0] may be 9 or 8, but not exactly 10.Plan: Write out the shuffle without relying on a library to do the heavy lifting. Example: Go through the list from the beginning starting with element 0; find a new random position for it, say 6, put 0’s value in 6 and 6’s value in 0. Move on to element 1 and repeat this process, and so on through the rest of the listIt works fine. I am trying it here with functions as list objects:It prints out:

foo1 foo2 foo3 

foo2 foo3 foo1

(the foos in the last row have a random order)

Get unique values from a list in python [duplicate]

savitha

[Get unique values from a list in python [duplicate]](https://stackoverflow.com/questions/12897374/get-unique-values-from-a-list-in-python)

I want to get the unique values from the following list:The output which I require is:This code works:is there a better solution I should use?

2012-10-15 14:05:37Z

I want to get the unique values from the following list:The output which I require is:This code works:is there a better solution I should use?First declare your list properly, separated by commas. You can get the unique values by converting the list to a set.If you use it further as a list, you should convert it back to a list by doing:Another possibility, probably faster would be to use a set from the beginning, instead of a list. Then your code should be:As it has been pointed out, sets do not maintain the original order. If you need that, you should look for an ordered set implementation (see this question for more).To be consistent with the type I would use:If we need to keep the elements order, how about this:And one more solution using reduce and without the temporary used var.UPDATE - March, 2019And a 3rd solution, which is a neat one, but kind of slow since .index is O(n).UPDATE - Oct, 2016Another solution with reduce, but this time without .append which makes it more human readable and easier to understand.NOTE: Have in mind that more human-readable we get, more unperformant the script is.ANSWERING COMMENTS Because @monica asked a good question about "how is this working?". For everyone having problems figuring it out. I will try to give a more deep explanation about how this works and what sorcery is happening here ;)So she first asked:Well it's actually workingThe problem is that we are just not getting the desired results inside the unique variable, but only inside the used variable. This is because during the list comprehension .append modifies the used variable and returns None.So in order to get the results into the unique variable, and still use the same logic with .append(x) if x not in used, we need to move this .append call on the right side of the list comprehension and just return x on the left side.But if we are too naive and just go with:We will get nothing in return. Again, this is because the .append method returns None, and it this gives on our logical expression the following look: This will basically always:And in both cases (False/None), this will be treated as falsy value and we will get an empty list as a result.But why this evaluates to None when x is not in used? Someone may ask. Well it's because this is how Python's short-circuit operators works.So when x is not in used (i.e. when its True) the next part or the expression will be evaluated (used.append(x)) and its value (None) will be returned. But that's what we want in order to get the unique elements from a list with duplicates, we want to .append them into a new list only when we they came across for a fist time. So we really want to evaluate used.append(x) only when x is not in used, maybe if there is a way to turn this None value into a truthy one we will be fine, right? Well, yes and here is where the 2nd type of short-circuit operators come to play.We know that .append(x) will always be falsy, so if we just add one or next to him, we will always get the next part. That's why we write:so we can evaluate used.append(x) and get True as a result, only when the first part of the expression (x not in used) is True.Similar fashion can be seen in the 2nd approach with the reduce method.where we:What type is your output variable?Python sets are what you need. Declare output like this:and you're ready to go adding elements with output.add(elem) and be sure they're unique.Warning: sets DO NOT preserve the original order of the list.A Python list:To get unique items, just transform it into a set (which you can transform back again into a list if required):Maintaining order:Order doesn't matter:   Options to remove duplicates may include the following generic data structures:Here is a summary on quickly getting either one in Python.GivenCodeOption 1 - A set (unordered):Option 2 - Python doesn't have ordered sets, but here are some ways to mimic one (insertion ordered):The last option is recommended if using Python 3.6+.Note: listed elements must be hashable.  See details on the latter example in this blog post.  Furthermore, see R. Hettinger's post on the same technique; the order preserving dict is extended from one of his early implementations.  See also more on total ordering.Getting unique elements from ListReferenceIf you are using numpy in your code (which might be a good choice for larger amounts of data), check out numpy.unique:(http://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html)As you can see, numpy supports not only numeric data, string arrays are also possible. Of course, the result is a numpy array, but it doesn't matter a lot, because it still behaves like a sequence:If you really want to have a vanilla python list back, you can always call list().However, the result is automatically sorted, as you can see from the above code fragments. Check out numpy unique without sort if retaining list order is required.set - unordered collection of unique elements. List of elements can be passed to set's constructor. So, pass list with duplicate elements, we get set with unique elements and transform it back to list then get list with unique elements. I can say nothing about performance and memory overhead, but I hope, it's not so important with small lists.Simply and short.Same order unique list using only a list compression.enumerates gives the index i and element e as a tuple.my_list.index returns the first index of e. If the first index isn't i then the current iteration's e is not the first e in the list.EditI should note that this isn't a good way to do it, performance-wise. This is just a way that achieves it using only a list compression.By using basic property of Python Dictionary:Output will be:First thing, the example you gave is not a valid list.Suppose if above is the example list. Then you can use the following recipe as give the itertools example doc  that can return the unique values and preserving the order as you seem to require. The iterable here is the example_list set can help you filter out the elements from the list that are duplicates. It will work well for str, int or tuple elements, but if your list contains dict or other list elements, then you will end up with TypeError exceptions.Here is a general order-preserving solution to handle some (not all) non-hashable types:As a bonus, Counter is a simple way to get both the unique values and the count for each value:To get unique values from your list use code below:IMPORTANT:

Approach above won't work if any of items in a list is not hashable which is case for mutable types, for instance list or dict.That means that you have to be sure that trends list would always contains only hashable items otherwise you have to use more sophisticated code:I am surprised that nobody so far has given a direct order-preserving answer:It will generate the values so it works with more than just lists, e.g. unique(range(10)). To get a list, just call list(unique(sequence)), like this:It has the requirement that each item is hashable and not just comparable, but most stuff in Python is and it is O(n) and not O(n^2), so will work just fine with a long list.In addition to the previous answers, which say you can convert your list to set, you can do that in this way toooutput will bethough order will not be preserved. Another simpler answer could be (without using sets)If you want to get unique elements from a list and keep their original order, then you may employ OrderedDict data structure from Python's standard library:In fact, if you are using Python ≥ 3.6, you can use plain dict for that:It's become possible after the introduction of "compact" representation of dicts. Check it out here. Though this "considered an implementation detail and should not be relied upon".You can use sets. Just to be clear, I am explaining what is the difference between a list and a set. 

sets are unordered collection of unique elements.Lists are ordered collection of elements.

So,But: Do not use list/set in naming the variables. It will cause error: 

EX: Instead of use list instead of unicode_list in the above one. My solution to check contents for uniqueness but preserve the original order:

Edit:

Probably can be more efficient by using dictionary keys to check for existence instead of doing a whole file loop for each line, I wouldn't use my solution for large sets. use set to de-duplicate a list, return as listSet is a collection of un-ordered and unique elements. So, you can use set as below to get a unique list:I know this is an old question, but here's my unique solution: class inheritance!:Then, if you want to uniquely append items to a list you just call appendunique on a UniqueList. Because it inherits from a list, it basically acts like a list, so you can use functions like index() etc. And because it returns true or false, you can find out if appending succeeded (unique item) or failed (already in the list).To get a unique list of items from a list, use a for loop appending items to a UniqueList (then copy over to the list).Example usage code:Prints:Copying to list:Prints:For long arraysTry this function, it's similar to your code but it's a dynamic range.Use the following function:

How do you test that a Python function throws an exception?

Daryl Spitzer

[How do you test that a Python function throws an exception?](https://stackoverflow.com/questions/129507/how-do-you-test-that-a-python-function-throws-an-exception)

How does one write a unittest that fails only if a function doesn't throw an expected exception?

2008-09-24 20:00:35Z

How does one write a unittest that fails only if a function doesn't throw an expected exception?Use TestCase.assertRaises (or TestCase.failUnlessRaises) from the unittest module, for example:Since Python 2.7 you can use context manager to get ahold of the actual Exception object thrown:http://docs.python.org/dev/library/unittest.html#unittest.TestCase.assertRaisesIn Python 3.5, you have to wrap context.exception in str, otherwise you'll get a TypeErrorThe code in my previous answer can be simplified to:And if afunction takes arguments, just pass them into assertRaises like this:Use the self.assertRaises method as a context manager:The best practice approach is fairly easy to demonstrate in a Python shell. The unittest libraryIn Python 2.7 or 3:In Python 2.6, you can install a backport of 2.7's unittest library, called unittest2, and just alias that as unittest:Now, paste into your Python shell the following test of Python's type-safety:Test one uses assertRaises as a context manager, which ensures that the error is properly caught and cleaned up, while recorded. We could also write it without the context manager, see test two. The first argument would be the error type you expect to raise, the second argument, the function you are testing, and the remaining args and keyword args will be passed to that function. I think it's far more simple, readable, and maintainable to just to use the context manager.To run the tests:In Python 2.6, you'll probably need the following:And your terminal should output the following:And we see that as we expect, attempting to add a 1 and a '1' result in a TypeError.For more verbose output, try this:Your code should follow this pattern (this is a unittest module style test):On Python < 2.7 this construct is useful for checking for specific values in the expected exception. The unittest function assertRaises only checks if an exception was raised.from: http://www.lengrand.fr/2011/12/pythonunittest-assertraises-raises-error/First, here is the corresponding (still dum :p) function in file dum_function.py :Here is the test to be performed (only this test is inserted):We are now ready to test our function! Here is what happens when trying to run the test :The TypeError is actullay raised, and generates a test failure. The problem is that this is exactly the behavior we wanted :s.To avoid this error, simply run the function using lambda in the test call :The final output :Perfect !... and for me is perfect too!!Thansk a lot Mr. Julien Lengrand-LambertYou can build your own contextmanager to check if the exception was raised.And then you can use raises like this:If you are using pytest, this thing is implemented already. You can do pytest.raises(Exception):Example:And the result:I use doctest[1] almost everywhere because I like the fact that I document and test my functions at the same time.Have a look at this code:If you put this example in a module and run it from the command line both test cases are evaluated and checked.[1] Python documentation: 23.2 doctest -- Test interactive Python examplesHave a look at the assertRaises method of the unittest module.I just discovered that the Mock library provides an assertRaisesWithMessage() method (in its unittest.TestCase subclass), which will check not only that the expected exception is raised, but also that it is raised with the expected message:There are a lot of answers here. The code shows how we can create an Exception, how we can use that exception in our methods, and finally, how you can verify in a unit test, the correct exceptions being raised.You can use assertRaises from the unittest moduleWhile all the answers are perfectly fine, I was looking for a way to test if a function raised an exception without relying on unit testing frameworks and having to write test classes.I ended up writing the following:And it fails on the right line :

Convert hex string to int in Python

Matt

[Convert hex string to int in Python](https://stackoverflow.com/questions/209513/convert-hex-string-to-int-in-python)

How do I convert a hex string to an int in Python? I may have it as "0xffff" or just "ffff".

2008-10-16 17:28:03Z

How do I convert a hex string to an int in Python? I may have it as "0xffff" or just "ffff".Without the 0x prefix, you need to specify the base explicitly, otherwise there's no way to tell:With the 0x prefix, Python can distinguish hex and decimal automatically.(You must specify 0 as the base in order to invoke this prefix-guessing behavior; omitting the second parameter means to assume base-10.)int(hexString, 16) does the trick, and works with and without the 0x prefix:For any given string s:To convert a string to an int, pass the string to int along with the base you are converting from. Both strings will suffice for conversion in this way:If you pass 0 as the base, int will infer the base from the prefix in the string. Without the hexadecimal prefix, 0x, int does not have enough information with which to guess:If you're typing into source code or an interpreter, Python will make the conversion for you:This won't work with ffff because Python will think you're trying to write a legitimate Python name instead:Python numbers start with a numeric character, while Python names cannot start with a numeric character.Adding to Dan's answer above: if you supply the int() function with a hex string, you will have to specify the base as 16 or it will not think you gave it a valid value. Specifying base 16 is unnecessary for hex numbers not contained in strings.The worst way:Is using eval in Python a bad practice?Or ast.literal_eval (this is safe, unlike eval):Demo:The formatter option '%x' % seems to work in assignment statements as well for me. (Assuming Python 3.0 and later)Example If you are using the python interpreter, you can just type 0x(your hex value) and the interpreter will convert it automatically for you.Handles hex, octal, binary, int, and floatUsing the standard prefixes (i.e. 0x, 0b, 0, and 0o)  this function will convert any suitable string to a number.  I answered this here: https://stackoverflow.com/a/58997070/2464381 but here is the needed function.In Python 2.7, int('deadbeef',10) doesn't seem to work. The following works for me:with '0x' prefix, you might also use eval functionFor example

How to check if type of a variable is string?

c_pleaseUpvote

[How to check if type of a variable is string?](https://stackoverflow.com/questions/4843173/how-to-check-if-type-of-a-variable-is-string)

Is there a way to check if the type of a variable in python is string. like:for integer values?

2011-01-30 13:32:19Z

Is there a way to check if the type of a variable in python is string. like:for integer values?In Python 2.x, you would dobasestring is the abstract superclass of str and unicode. It can be used to test whether an object is an instance of str or unicode.In Python 3.x, the correct test isThe bytes class isn't considered a string type in Python 3.I know this is an old topic, but being the first one shown on google and given that I don't find any of the answers satisfactory, I'll leave this here for future reference:six is a Python 2 and 3 compatibility library which already covers this issue. You can then do something like this:Inspecting the code, this is what you find:In Python 3.x or Python 2.7.6you can do:or:hope this helps!The type module also exists if you are checking more than ints and strings. 

http://docs.python.org/library/types.htmlEdit based on better answer below.  Go down about 3 answers and find out about the coolness of basestring.Old answer:

Watch out for unicode strings, which you can get from several places, including all COM calls in Windows.since basestring isn't defined in Python3, this little trick might help to make the code compatible:after that you can run the following test on both Python2 and Python3Python 2 / 3 including unicodehttp://python-future.org/overview.htmlAlso I want notice that if you want to check whether the type of a variable is a specific kind, you can compare the type of the variable to the type of a known object.For string you can use thisLots of good suggestions provided by others here, but I don't see a good cross-platform summary.  The following should be a good drop in for any Python program:In this function, we use isinstance(object, classinfo) to see if our input is a str in Python 3 or a basestring in Python 2.Alternative way for Python 2, without using basestring:But still won't work in Python 3 since unicode isn't defined (in Python 3).So,You have plenty of options to check whether your variable is string or not:This order is for purpose. returns Truereturns TrueHere is my answer to support both Python 2 and Python 3 along with these requirements:If you do not want to depend on external libs, this works both for Python 2.7+ and Python 3 (http://ideone.com/uB4Kdc):You can simply use the isinstance function to make sure that the input data is of format string or unicode. Below examples will help you to understand easily.This is how I do it:I've seen: 

Usage of __slots__?

Jeb

[Usage of __slots__?](https://stackoverflow.com/questions/472000/usage-of-slots)

What is the purpose of __slots__ in Python — especially with respect to when I would want to use it, and when not?

2009-01-23 05:37:23Z

What is the purpose of __slots__ in Python — especially with respect to when I would want to use it, and when not?The special attribute __slots__ allows you to explicitly state which instance attributes you expect your object instances to have, with the expected results:The space savings is from Small caveat, you should only declare a particular slot one time in an inheritance tree. For example:Python doesn't object when you get this wrong (it probably should), problems might not otherwise manifest, but your objects will take up more space than they otherwise should. Python 3.8:This is because the Base's slot descriptor has a slot separate from the Wrong's. This shouldn't usually come up, but it could:The biggest caveat is for multiple inheritance - multiple "parent classes with nonempty slots" cannot be combined. To accommodate this restriction, follow best practices: Factor out all but one or all parents' abstraction which their concrete class respectively and your new concrete class collectively will inherit from - giving the abstraction(s) empty slots (just like abstract base classes in the standard library). See section on multiple inheritance below for an example.There are a lot of details if you wish to keep reading.The creator of Python, Guido van Rossum, states that he actually created __slots__ for faster attribute access.  It is trivial to demonstrate measurably significant faster access:andThe slotted access is almost 30% faster in Python 3.5 on Ubuntu.In Python 2 on Windows I have measured it about 15% faster.Another purpose of __slots__ is to reduce the space in memory that each object instance takes up.  My own contribution to the documentation clearly states the reasons behind this: SQLAlchemy attributes a lot of memory savings to __slots__.To verify this, using the Anaconda distribution of Python 2.7 on Ubuntu Linux, with guppy.hpy (aka heapy) and sys.getsizeof, the size of a class instance without __slots__ declared, and nothing else, is 64 bytes. That does not include the __dict__. Thank you Python for lazy evaluation again, the __dict__ is apparently not called into existence until it is referenced, but classes without data are usually useless. When called into existence, the __dict__ attribute is a minimum of 280 bytes additionally. In contrast, a class instance with __slots__ declared to be () (no data) is only 16 bytes, and 56 total bytes with one item in slots, 64 with two.For 64 bit Python, I illustrate the memory consumption in bytes in Python 2.7 and 3.6, for __slots__ and __dict__ (no slots defined) for each point where the dict grows in 3.6 (except for 0, 1, and 2 attributes):So, in spite of smaller dicts in Python 3, we see how nicely __slots__ scale for instances to save us memory, and that is a major reason you would want to use __slots__. Just for completeness of my notes, note that there is a one-time cost per slot in the class's namespace of 64 bytes in Python 2, and 72 bytes in Python 3, because slots use data descriptors like properties, called "members".To deny the creation of a __dict__, you must subclass object:now:Or subclass another class that defines __slots__and now:but:To allow __dict__ creation while subclassing slotted objects, just add '__dict__' to the __slots__ (note that slots are ordered, and you shouldn't repeat slots that are already in parent classes):andOr you don't even need to declare __slots__ in your subclass, and you will still use slots from the parents, but not restrict the creation of a __dict__:And:However, __slots__ may cause problems for multiple inheritance:Because creating a child class from parents with both non-empty slots fails:If you run into this problem, You could just remove __slots__ from the parents, or if you have control of the parents, give them empty slots, or refactor to abstractions:and now:So with '__dict__' in slots we lose some of the size benefits with the upside of having dynamic assignment and still having slots for the names we do expect.When you inherit from an object that isn't slotted, you get the same sort of semantics when you use __slots__ - names that are in __slots__ point to  slotted values, while any other values are put in the instance's __dict__.Avoiding __slots__ because you want to be able to add attributes on the fly is actually not a good reason - just add "__dict__" to your __slots__ if this is required.You can similarly add __weakref__ to __slots__ explicitly if you need that feature.The namedtuple builtin make immutable instances that are very lightweight (essentially, the size of tuples) but to get the benefits, you need to do it yourself if you subclass them:usage:And trying to assign an unexpected attribute raises an AttributeError because we have prevented the creation of __dict__:You can allow __dict__ creation by leaving off __slots__ = (), but you can't use non-empty __slots__ with subtypes of tuple.Even when non-empty slots are the same for multiple parents, they cannot be used together:Using an empty __slots__ in the parent seems to provide the most flexibility, allowing the child to choose to prevent or allow (by adding '__dict__' to get dynamic assignment, see section above) the creation of a __dict__:You don't have to have slots - so if you add them, and remove them later, it shouldn't cause any problems.Going out on a limb here: If you're composing mixins or using abstract base classes, which aren't intended to be instantiated, an empty __slots__ in those parents seems to be the best way to go in terms of flexibility for subclassers.To demonstrate, first, let's create a class with code we'd like to use under multiple inheritanceWe could use the above directly by inheriting and declaring the expected slots:But we don't care about that, that's trivial single inheritance, we need another class we might also inherit from, maybe with a noisy attribute:Now if both bases had nonempty slots, we couldn't do the below. (In fact, if we wanted, we could have given AbstractBase nonempty slots a and b, and left them out of the below declaration - leaving them in would be wrong):And now we have functionality from both via multiple inheritance, and can still deny __dict__ and __weakref__ instantiation:You may be able to tease out further caveats from the rest of the __slots__ documentation (the 3.7 dev docs are the most current), which I have made significant recent contributions to.The current top answers cite outdated information and are quite hand-wavy and miss the mark in some important ways.I quote:Abstract Base Classes, for example, from the collections module, are not instantiated, yet __slots__ are declared for them. Why?If a user wishes to deny __dict__ or __weakref__ creation, those things must not be available in the parent classes.__slots__ contributes to reusability when creating interfaces or mixins.It is true that many Python users aren't writing for reusability, but when you are, having the option to deny unnecessary space usage is valuable.When pickling a slotted object, you may find it complains with a misleading TypeError: This is actually incorrect. This message comes from the oldest protocol, which is the default. You can select the latest protocol with the -1 argument. In Python 2.7 this would be 2 (which was introduced in 2.3), and in 3.6 it is 4.in Python 2.7:in Python 3.6So I would keep this in mind, as it is a solved problem.The first paragraph is half short explanation, half predictive. Here's the only part that actually answers the questionThe second half is wishful thinking, and off the mark:Python actually does something similar to this, only creating the __dict__ when it is accessed, but creating lots of objects with no data is fairly ridiculous.The second paragraph oversimplifies and misses actual reasons to avoid __slots__. The below is not a real reason to avoid slots (for actual reasons, see the rest of my answer above.):It then goes on to discuss other ways of accomplishing that perverse goal with Python, not discussing anything to do with __slots__.The third paragraph is more wishful thinking. Together it is mostly off-the-mark content that the answerer didn't even author and contributes to ammunition for critics of the site.Create some normal objects and slotted objects:Instantiate a million of them:Inspect with guppy.hpy().heap():Access the regular objects and their __dict__ and inspect again:This is consistent with the history of Python, from Unifying types and classes in Python 2.2Quoting Jacob Hallen:You would want to use __slots__ if you are going to instantiate a lot (hundreds, thousands) of objects of the same class. __slots__ only exists as a memory optimization tool.It's highly discouraged to use __slots__ for constraining attribute creation.Pickling objects with __slots__ won't work with the default (oldest) pickle protocol; it's necessary to specify a later version.Some other introspection features of python may also be adversely affected.Each python object has a __dict__ atttribute which is a dictionary containing all other attributes. e.g. when you type self.attr python is actually doing self.__dict__['attr']. As you can imagine using a dictionary to store attribute takes some extra space & time for accessing it.However, when you use __slots__, any object created for that class won't have a __dict__ attribute. Instead, all attribute access is done directly via pointers.So if want a C style structure rather than a full fledged class you can use __slots__ for compacting size of the objects & reducing attribute access time. A good example is a Point class containing attributes x & y. If you are going to have a lot of points, you can try using __slots__ in order to conserve some memory.In addition to the other answers, here is an example of using __slots__:So, to implement __slots__, it only takes an extra line (and making your class a new-style class if it isn't already). This way you can reduce the memory footprint of those classes 5-fold, at the expense of having to write custom pickle code, if and when that becomes necessary.Slots are very useful for library calls to eliminate the "named method dispatch" when making function calls.  This is mentioned in the SWIG documentation.  For high performance libraries that want to reduce function overhead for commonly called functions using slots is much faster.Now this may not be directly related to the OPs question.  It is related more to building extensions than it does to using the slots syntax on an object.  But it does help complete the picture for the usage of slots and some of the reasoning behind them.An attribute of a class instance has 3 properties: the instance, the name of the attribute, and the value of the attribute.In regular attribute access, the instance acts as a dictionary and the name of the attribute acts as the key in that dictionary looking up value.instance(attribute) --> valueIn __slots__ access, the name of the attribute acts as the dictionary and the instance acts as the key in the dictionary looking up value.attribute(instance) --> valueIn flyweight pattern, the name of the attribute acts as the dictionary and the value acts as the key in that dictionary looking up the instance.attribute(value) --> instanceA very simple example of __slot__ attribute.If I don't have __slot__ attribute  in my class, I can add new attributes to  my objects. If you look at example above, you can see that obj1 and obj2 have their own x and y attributes and python has also created  a dict attribute for each object (obj1 and obj2).Suppose if my class Test has thousands of such objects? Creating an additional attribute dict for each object will cause lot of overhead (memory, computing power etc.) in my code. Now in the following example my class Test contains __slots__ attribute. Now I can't add new attributes to my objects (except attribute x) and python doesn't create a dict attribute anymore. This eliminates overhead for each object, which can become significant if you have many objects.Another somewhat obscure use of __slots__ is to add attributes to an object proxy from the ProxyTypes package, formerly part of the PEAK project. Its ObjectWrapper allows you to proxy another object, but intercept all interactions with the proxied object. It is not very commonly used (and no Python 3 support), but we have used it to implement a thread-safe blocking wrapper around an async implementation based on tornado that bounces all access to the proxied object through the ioloop, using thread-safe concurrent.Future objects to synchronise and return results.By default any attribute access to the proxy object will give you the result from the proxied object. If you need to add an attribute on the proxy object, __slots__ can be used.You have — essentially — no use for __slots__.  For the time when you think you might need __slots__, you actually want to use Lightweight or Flyweight design patterns. These are cases when you no longer want to use purely Python objects. Instead, you want a Python object-like wrapper around an array, struct, or numpy array.The class-like wrapper has no attributes — it just provides methods that act on the underlying data. The methods can be reduced to class methods. Indeed, it could be reduced to just functions operating on the underlying array of data.The original question was about general use cases not only about memory.

So it should be mentioned here that you also get better performance when instantiating large amounts of objects - interesting e.g. when parsing large documents into objects or from a database.Here is a comparison of creating object trees with a million entries, using slots and without slots. As a reference also the performance when using plain dicts for the trees (Py2.7.10 on OSX):Test classes (ident, appart from slots):testcode, verbose mode:

How can I check file size in Python?

5YrsLaterDBA

[How can I check file size in Python?](https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python)

I am writing a Python script in Windows. I want to do something based on the file size. For example, if the size is greater than 0, I will send an email to somebody, otherwise continue to other things. How do I check the file size?

2010-01-20 18:58:29Z

I am writing a Python script in Windows. I want to do something based on the file size. For example, if the size is greater than 0, I will send an email to somebody, otherwise continue to other things. How do I check the file size?Use os.stat, and use the st_size member of the resulting object:Output is in bytes.Using os.path.getsize:The output is in bytes.The other answers work for real files, but if you need something that works for "file-like objects", try this:It works for real files and StringIO's, in my limited testing. (Python 2.7.3.) The "file-like object" API isn't really a rigorous interface, of course, but the API documentation suggests that file-like objects should support seek() and tell().EditAnother difference between this and os.stat() is that you can stat() a file even if you don't have permission to read it. Obviously the seek/tell approach won't work unless you have read permission.Edit 2 At Jonathon's suggestion, here's a paranoid version. (The version above leaves the file pointer at the end of the file, so if you were to try to read from the file, you'd get zero bytes back!)Result:Using pathlib (added in Python 3.4 or a backport available on PyPI):This is really only an interface around os.stat, but using pathlib provides an easy way to access other file related operations.There is a bitshift trick I use if I want to to convert from bytes to any other unit. If you do a right shift by 10 you basically shift it by an order (multiple). Strictly sticking to the question, the Python code (+ pseudo-code) would be:

pg_config executable not found

user1448207

[pg_config executable not found](https://stackoverflow.com/questions/11618898/pg-config-executable-not-found)

I am having trouble installing psycopg2. I get the following error when I try to pip install psycopg2:But the problem is pg_config is actually in my PATH; it runs without any problem:I tried adding the pg_config path to the setup.cfg file and building it using the source files I downloaded from their website (http://initd.org/psycopg/) and I get the following error message!But it is actually THERE!!!I am baffled by these errors. Can anyone help please?By the way, I sudo all the commands. Also I am on RHEL 5.5.

2012-07-23 19:09:40Z

I am having trouble installing psycopg2. I get the following error when I try to pip install psycopg2:But the problem is pg_config is actually in my PATH; it runs without any problem:I tried adding the pg_config path to the setup.cfg file and building it using the source files I downloaded from their website (http://initd.org/psycopg/) and I get the following error message!But it is actually THERE!!!I am baffled by these errors. Can anyone help please?By the way, I sudo all the commands. Also I am on RHEL 5.5.pg_config is in postgresql-devel (libpq-dev in Debian/Ubuntu, libpq-devel on Cygwin/Babun.)On Mac OS X, I solved it using the homebrew package managerHave you installed python-dev?

If you already have, try also installing libpq-devFrom the article: How to install psycopg2 under virtualenvAlso on OSX. Installed Postgress.app from http://postgresapp.com/ but had the same issue.I found pg_config in that app's contents and added the dir to $PATH.It was at /Applications/Postgres.app/Contents/Versions/latest/bin. So this worked: export PATH="/Applications/Postgres.app/Contents/Versions/latest/bin:$PATH".On alpine, the library containing pg_config is postgresql-dev. To install, run:You should add python requirements used in Postgres on Ubuntu. Run:Just to sum up, I also faced exactly same problem. After reading a lot of stackoverflow posts and online blogs, the final solution which worked for me is this:1) PostgreSQL(development or any stable version) should be installed before installing psycopg2.2) The pg_config file (this file normally resides in the bin folder of the PostgreSQL installation folder) PATH had to be explicitly setup before installing psycopg2. In my case, the installation PATH for PostgreSQL is:so in order to explicitly set the PATH of pg_config file, I entered following command in my terminal:This command ensures that when you try to pip install psycopg2, it would find the PATH to pg_config automatically this time.I have also posted a full error with trace and its solution on my blog which you may want to refer. Its for Mac OS X but the pg_config PATH problem is generic and applicable to Linux also. This is what worked for me on CentOS, first install:On Ubuntu just use the equivilent apt-get packages.And now include the path to your postgresql binary dir with you pip install, this should work for either Debain or RHEL based Linux:Make sure to include the correct path. Thats all :)sudo apt-get install libpq-dev works for me on Ubuntu 15.4On Linux Mint sudo apt-get install libpq-dev worked for me.UPDATE /etc/yum.repos.d/CentOS-Base.repo, [base] and [updates] sections

ADD exclude=postgresql*For those running OS X, this solution worked for me:1) Install Postgres.app:http://www.postgresql.org/download/macosx/2) Then open the Terminal and run this command, replacing where it says {{version}} with the Postgres version number:export PATH=$PATH:/Applications/Postgres.app/Contents/Versions/{{version}}/bine.g.export PATH=$PATH:/Applications/Postgres.app/Contents/Versions/9.4/binTry to add it to PATH:Ali's solution worked for me but I was having trouble finding the bin folder location. A quick way to find the path on Mac OS X is to open psql (there's a quick link in the top menu bar). This will open a separate terminal window and on the second line the path of your Postgres installation will appear like so:Your pg_config file is in that bin folder. Therefore, before installing psycopg2 set the path of the pg_config file:or for newer version:Then install psycopg2. You need to upgrade your pip before installing psycopg2. Use this commandI'm going to leave this here for the next unfortunate soul who can't get around this problem despite all the provided solutions. Simply use sudo pip3 install psycopg2-binaryJust solved the problem in Cent OS 7 by:make sure your PostgreSql version matches the right version above.You can install pre-compiled binaries on any platform with pip or conda:orPlease be advised that the psycopg2-binary pypi page recommends building from source in production:To use the package built from sources, use pip install psycopg2. That process will require several dependencies (documentation) (emphasis mine):On Windows,

You may want to install the Windows port of Psycopg, which is recommended in psycopg's documentation.On Mac OS X and If you are using Postgres App (http://postgresapp.com/):No need to specify version of Postgres in this command. It will be always pointed to latest.and do P.S: If Changes doesn't reflect you may need to restart the Terminal/Command promptSourceInstalling python-psycopg2 solved it for me on Arch Linux:On MacOS, the simplest solution will be to symlink the correct binary, that is under the Postgres package.This is fairly harmless, and all the applications will be able to use it system wide, if required.sudo yum install postgresql-devel   (centos6X)pip install psycopg2==2.5.2Here, for OS X completeness: if you install PostgreSQL from MacPorts, pg_config will be in /opt/local/lib/postgresql94/bin/pg_config. When you installed MacPorts, it already added /opt/local/bin to your PATH. So, this will fix the problem:

$ sudo ln -s /opt/local/lib/postgresql94/bin/pg_config /opt/local/bin/pg_config

Now pip install psycopg2 will be able to run pg_config without issues.I am pretty sure you've experienced the same "problem" i did, therefore I'll offer you the extremely easy solution...In your case, the actual path that you need to add to $PATH (or as a command param) is:notE.g. if you run the python setup.py script afterwards, you would run it like this:Probably too late, but still the easiest solution.LATER EDIT:Under further test I found out that if you initially add the path to pg_config in the form (without /pg_config after ...../bin) and run the pip install command it will work.However, if you then decide to follow the indication to run python setup.py, you will have to specify the path with /pg_config after ...../bin, i.e.for CentOS/RedHat make sure that /etc/alternatives/pgsql-pg_config is a non-broken symlinkThis is how I managed to install psycopg2To those on macOS Catalina using the zsh shell who have also installed the postgres app:Open your ~/.zshrc file, and add the following line:Then close all your terminals, reopen them, and you'll have resolved your problem.  If you don't want to close your terminals, simply enter source ~/.zshrc in whatever terminal you'd like to keep working on.On Gentoo You have to execute the following

How to use a decimal range() step value?

Evan Fosmark

[How to use a decimal range() step value?](https://stackoverflow.com/questions/477486/how-to-use-a-decimal-range-step-value)

Is there a way to step between 0 and 1 by 0.1? I thought I could do it like the following, but it failed:Instead, it says that the step argument cannot be zero, which I did not expect.

2009-01-25 10:20:43Z

Is there a way to step between 0 and 1 by 0.1? I thought I could do it like the following, but it failed:Instead, it says that the step argument cannot be zero, which I did not expect.Rather than using a decimal step directly, it's much safer to express this in terms of how many points you want. Otherwise, floating-point rounding error is likely to give you a wrong result.You can use the linspace function from the NumPy library (which isn't part of the standard library but is relatively easy to obtain). linspace takes a number of points to return, and also lets you specify whether or not to include the right endpoint:If you really want to use a floating-point step value, you can, with numpy.arange.Floating-point rounding error will cause problems, though. Here's a simple case where rounding error causes arange to produce a length-4 array when it should only produce 3 numbers:Python's range() can only do integers, not floating point. In your specific case, you can use a list comprehension instead:(Replace the call to range with that expression.) For the more general case, you may want to write a custom function or generator.Building on 'xrange([start], stop[, step])', you can define a generator that accepts and produces any type you choose (stick to types supporting + and <):Increase the magnitude of i for the loop and then reduce it when you need it.EDIT: I honestly cannot remember why I thought that would work syntacticallyThat should have the desired output. scipy has a built in function arange which generalizes Python's range() constructor to satisfy your requirement of float handling. from scipy import arangeNumPy is a bit overkill, I think.Generally speaking, to do a step-by-1/x up to y you would do(1/x produced less rounding noise when I tested).Similar to R's seq function, this one returns a sequence in any order given the correct step value. The last value is equal to the stop value. The range() built-in function returns a sequence of integer values, I'm afraid, so you can't use it to do a decimal step.  I'd say just use a while loop:If you're curious, Python is converting your 0.1 to 0, which is why it's telling you the argument can't be zero.Here's a solution using itertools:Usage Example:in Python 2.7x gives you the result of:but if you use:gives you the desired:And if you do this often, you might want to save the generated list rMy versions use the original range function to create multiplicative indices for the shift. This allows same syntax to the original range function.

I have made two versions, one using float, and one using Decimal, because I found that in some cases I wanted to avoid the roundoff drift introduced by the floating point arithmetic.It is consistent with empty set results as in range/xrange.Passing only a single numeric value to either function will return the standard range output to the integer ceiling value of the input parameter (so if you gave it 5.5, it would return range(6).)Edit: the code below is now available as package on pypi: FrangesThis is my solution to get ranges with float steps.

Using this function it's not necessary to import numpy, nor install it.

I'm pretty sure that it could be improved and optimized. Feel free to do it and post it here.The output is:more_itertools is a third-party library that implements a numeric_range tool:OutputThis tool also works for Decimal and Fraction.For completeness of boutique, a functional solution:You can use this function:The trick to avoid round-off problem is to use a separate number to move through the range, that starts and half the step ahead of start.Alternatively, numpy.arange can be used.It can be done using Numpy library. arange() function allows steps in float. But, it returns a numpy array which can be converted to list using tolist() for our convenience.My answer is similar to others using map(), without need of NumPy, and without using lambda (though you could).    To get a list of float values from 0.0 to t_max in steps of dt:Suprised no-one has yet mentioned the recommended solution in the Python 3 docs:Once defined, the recipe is easy to use and does not require numpy or any other external libraries, but functions like numpy.linspace(). Note that rather than a step argument, the third num argument specifies the number of desired values, for example:I quote a modified version of the full Python 3 recipe from  Andrew Barnert below:To counter the float precision issues, you could use the Decimal module.This demands an extra effort of converting to Decimal from int or float while writing the code, but you can instead pass str and modify the function if that sort of convenience is indeed necessary.Sample outputs -Add auto-correction for the possibility of an incorrect sign on step:My solution:Best Solution: no rounding error

__________________________________________________________________________________________________________________________________________________________________

Or, for a set range instead of set data points (e.g. continuous function), use:To implement a function: replace x / pow(step, -1) with f( x / pow(step, -1) ), and define f.

For example:start and stop are inclusive rather than one or the other (usually stop is excluded) and without imports, and using generatorsI know I'm late to the party here, but here's a trivial generator solution that's working in 3.6:then you can call it just like the original range()... there's no error handling, but let me know if there is an error that can be reasonably caught, and I'll update. or you can update it. this is StackOverflow.Here is my solution which works fine with float_range(-1, 0, 0.01) and works without floating point representation errors. It is not very fast, but works fine:    I am only a beginner, but I had the same problem, when simulating some calculations. Here is how I attempted to work this out, which seems to be working with decimal steps.I am also quite lazy and so I found it hard to write my own range function.Basically what I did is changed my xrange(0.0, 1.0, 0.01) to xrange(0, 100, 1) and used the division by 100.0 inside the loop.

I was also concerned, if there will be rounding mistakes. So I decided to test, whether there are any. Now I heard, that if for example 0.01 from a calculation isn't exactly the float 0.01 comparing them should return False (if I am wrong, please let me know).So I decided to test if my solution will work for my range by running a short test:And it printed True for each.Now, if I'm getting it totally wrong, please let me know.This one liner will not clutter your code. The sign of the step parameter is important.

How can I force division to be floating point? Division keeps rounding down to 0?

Nathan Fellman

[How can I force division to be floating point? Division keeps rounding down to 0?](https://stackoverflow.com/questions/1267869/how-can-i-force-division-to-be-floating-point-division-keeps-rounding-down-to-0)

I have two integer values a and b, but I need their ratio in floating point.  I know that a < b and I want to calculate a / b, so if I use integer division I'll always get 0 with a remainder of a.How can I force c to be a floating point number in Python in the following?

2009-08-12 18:25:15Z

I have two integer values a and b, but I need their ratio in floating point.  I know that a < b and I want to calculate a / b, so if I use integer division I'll always get 0 with a remainder of a.How can I force c to be a floating point number in Python in the following?In Python 2, division of two ints produces an int. In Python 3, it produces a float. We can get the new behaviour by importing from __future__.You can cast to float by doing c = a / float(b). If the numerator or denominator is a float, then the result will be also.A caveat: as commenters have pointed out, this won't work if b might be something other than an integer or floating-point number (or a string representing one). If you might be dealing with other types (such as complex numbers) you'll need to either check for those or use a different method.What is really being asked here is:"How do I force true division such that a / b will return a fraction?"In Python 3, to get true division, you simply do a / b. Floor division, the classic division behavior for integers, is now a // b:However, you may be stuck using Python 2, or you may be writing code that must work in both 2 and 3.In Python 2, it's not so simple. Some ways of dealing with classic Python 2 division are better and more robust than others.You can get Python 3 division behavior in any given module with the following import at the top:which then applies Python 3 style division to the entire module. It also works in a python shell at any given point. In Python 2:This is really the best solution as it ensures the code in your module is more forward compatible with Python 3.If you don't want to apply this to the entire module, you're limited to a few workarounds. The most popular is to coerce one of the operands to a float. One robust solution is a / (b * 1.0). In a fresh Python shell:Also robust is truediv from the operator module operator.truediv(a, b), but this is likely slower because it's a function call:Commonly seen is a / float(b). This will raise a TypeError if b is a complex number. Since division with complex numbers is defined, it makes sense to me to not have division fail when passed a complex number for the divisor.It doesn't make much sense to me to purposefully make your code more brittle.You can also run Python with the -Qnew flag, but this has the downside of executing all modules with the new Python 3 behavior, and some of your modules may expect classic division, so I don't recommend this except for testing. But to demonstrate:In Python 3.x, the single slash (/) always means true (non-truncating) division. (The // operator is used for truncating division.) In Python 2.x (2.2 and above), you can get this same behavior by putting aat the top of your module.Just making any of the parameters for division in floating-point format also produces the output in floating-point.Example:or,or, or, Add a dot (.) to indicate floating point numbersThis will also workIf you want to use "true" (floating point) division by default, there is a command line flag:There are some drawbacks (from the PEP):You can learn more about the other flags values that change / warn-about the behavior of division by looking at the python man page.For full details on division changes read: PEP 238 -- Changing the Division Operatorwhere a is dividend and b is the divisor.

This function is handy when quotient after division  of two integers is a float.

Why is [] faster than list()?

Augusta

[Why is [] faster than list()?](https://stackoverflow.com/questions/30216000/why-is-faster-than-list)

I recently compared the processing speeds of [] and list() and was surprised to discover that [] runs more than three times faster than list(). I ran the same test with {} and dict() and the results were practically identical: [] and {} both took around 0.128sec / million cycles, while list() and dict() took roughly 0.428sec / million cycles each.Why is this? Do [] and {} (and probably () and '', too) immediately pass back a copies of some empty stock literal while their explicitly-named counterparts (list(), dict(), tuple(), str()) fully go about creating an object, whether or not they actually have elements?I have no idea how these two methods differ but I'd love to find out.

I couldn't find an answer in the docs or on SO, and searching for empty brackets turned out to be more problematic than I'd expected.I got my timing results by calling timeit.timeit("[]") and timeit.timeit("list()"), and timeit.timeit("{}") and timeit.timeit("dict()"), to compare lists and dictionaries, respectively. I'm running Python 2.7.9.I recently discovered "Why is if True slower than if 1?" that compares the performance of if True to if 1 and seems to touch on a similar literal-versus-global scenario; perhaps it's worth considering as well.

2015-05-13 13:16:22Z

I recently compared the processing speeds of [] and list() and was surprised to discover that [] runs more than three times faster than list(). I ran the same test with {} and dict() and the results were practically identical: [] and {} both took around 0.128sec / million cycles, while list() and dict() took roughly 0.428sec / million cycles each.Why is this? Do [] and {} (and probably () and '', too) immediately pass back a copies of some empty stock literal while their explicitly-named counterparts (list(), dict(), tuple(), str()) fully go about creating an object, whether or not they actually have elements?I have no idea how these two methods differ but I'd love to find out.

I couldn't find an answer in the docs or on SO, and searching for empty brackets turned out to be more problematic than I'd expected.I got my timing results by calling timeit.timeit("[]") and timeit.timeit("list()"), and timeit.timeit("{}") and timeit.timeit("dict()"), to compare lists and dictionaries, respectively. I'm running Python 2.7.9.I recently discovered "Why is if True slower than if 1?" that compares the performance of if True to if 1 and seems to touch on a similar literal-versus-global scenario; perhaps it's worth considering as well.Because [] and {} are literal syntax. Python can create bytecode just to create the list or dictionary objects:list() and dict() are separate objects. Their names need to be resolved, the stack has to be involved to push the arguments, the frame has to be stored to retrieve later, and a call has to be made. That all takes more time.For the empty case, that means you have at the very least a LOAD_NAME (which has to search through the global namespace as well as the __builtin__ module) followed by a CALL_FUNCTION, which has to preserve the current frame:You can time the name lookup separately with timeit:The time discrepancy there is probably a dictionary hash collision. Subtract those times from the times for calling those objects, and compare the result against the times for using literals:So having to call the object takes an additional 1.00 - 0.31 - 0.30 == 0.39 seconds per 10 million calls.You can avoid the global lookup cost by aliasing the global names as locals (using a timeit setup, everything you bind to a name is a local):but you never can overcome that CALL_FUNCTION cost.list() requires a global lookup and a function call but [] compiles to a single instruction. See:Because list is a function to convert say a string to a list object, while [] is used to create a list off the bat. Try this (might make more sense to you):WhileGives you a actual list containing whatever you put in it.The answers here are great, to the point and fully cover this question. I'll drop a further step down from byte-code for those interested. I'm using the most recent repo of CPython; older versions behave similar in this regard but slight changes might be in place.Here's a break down of the execution for each of these, BUILD_LIST for [] and CALL_FUNCTION for list().You should just view the horror:Terribly convoluted, I know. This is how simple it is:No wonder it is fast! It's custom-made for creating new lists, nothing else :-)Here's the first thing you see when you peek at the code handling CALL_FUNCTION:Looks pretty harmless, right? Well, no, unfortunately not, call_function is not a straightforward guy that will call the function immediately, it can't. Instead, it grabs the object from the stack, grabs all arguments of the stack and then switches based on the type of the object; is it a:We're calling the list type, the argument passed in to call_function is PyList_Type. CPython now has to call a generic function to handle any callable objects named _PyObject_FastCallKeywords, yay more function calls.This function again makes some checks for certain function types (which I cannot understand why) and then, after creating a dict for kwargs if required, goes on to call _PyObject_FastCallDict._PyObject_FastCallDict finally gets us somewhere! After performing even more checks  it grabs the tp_call slot from the type of the type we've passed in, that is, it grabs type.tp_call. It then proceeds to create a tuple out of of the arguments passed in with _PyStack_AsTuple and, finally, a call can finally be made!tp_call, which matches type.__call__ takes over and finally creates the list object. It calls the lists __new__ which corresponds to PyType_GenericNew and allocates memory for it with PyType_GenericAlloc: This is actually the part where it catches up with PyList_New, finally. All the previous are necessary to handle objects in a generic fashion.In the end, type_call calls list.__init__ and initializes the list with any available arguments, then we go on a returning back the way we came. :-)Finally, remmeber the LOAD_NAME, that's another guy that contributes here.It's easy to see that, when dealing with our input, Python generally has to jump through hoops in order to actually find out the appropriate C function to do the job. It doesn't have the curtesy of immediately calling it because it's dynamic, someone might mask list (and boy do many people do) and another path must be taken. This is where list() loses much: The exploring Python needs to do to find out what the heck it should do.Literal syntax, on the other hand, means exactly one thing; it cannot be changed and always behaves in a pre-determined way.Footnote: All function names are subject to change from one release to the other. The point still stands and most likely will stand in any future versions, it's the dynamic look-up that slows things down.The biggest reason is that Python treats list() just like a user-defined function, which means you can intercept it by aliasing something else to list and do something different (like use your own subclassed list or perhaps a deque). It immediately creates a new instance of a builtin list with [].My explanation seeks to give you the intuition for this.[] is commonly known as literal syntax. In the grammar, this is referred to as a "list display". From the docs:In short, this means that a builtin object of type list is created. There is no circumventing this - which means Python can do it as quickly as it may.On the other hand, list() can be intercepted from creating a builtin list using the builtin list constructor.For example, say we want our lists to be created noisily:We could then intercept the name list on the module level global scope, and then when we create a list, we actually create our subtyped list:Similarly we could remove it from the global namespace and put it in the builtin namespace:And now:And note that the list display creates a list unconditionally:We probably only do this temporarily, so lets undo our changes - first remove the new List object from the builtins:Oh, no, we lost track of the original. Not to worry, we can still get list - it's the type of a list literal:So...As we've seen - we can overwrite list - but we can't intercept the creation of the literal type. When we use list we have to do the lookups to see if anything is there.Then we have to call whatever callable we have looked up. From the grammar:We can see that it does the same thing for any name, not just list:For [] there is no function call at the Python bytecode level:It simply goes straight to building the list without any lookups or calls at the bytecode level.We have demonstrated that list can be intercepted with user code using the scoping rules, and that list() looks for a callable and then calls it.Whereas [] is a list display, or a literal, and thus avoids the name lookup and function call.

How to overcome「datetime.datetime not JSON serializable」?

Rolando

[How to overcome「datetime.datetime not JSON serializable」?](https://stackoverflow.com/questions/11875770/how-to-overcome-datetime-datetime-not-json-serializable)

I have a basic dict as follows:When I try to do jsonify(sample) I get:TypeError: datetime.datetime(2012, 8, 8, 21, 46, 24, 862000) is not JSON serializableWhat can I do such that my dictionary sample can overcome the error above?Note: Though it may not be relevant, the dictionaries are generated from the retrieval of records out of mongodb where when I print out str(sample['somedate']), the output is 2012-08-08 21:46:24.862000.

2012-08-09 02:02:51Z

I have a basic dict as follows:When I try to do jsonify(sample) I get:TypeError: datetime.datetime(2012, 8, 8, 21, 46, 24, 862000) is not JSON serializableWhat can I do such that my dictionary sample can overcome the error above?Note: Though it may not be relevant, the dictionaries are generated from the retrieval of records out of mongodb where when I print out str(sample['somedate']), the output is 2012-08-08 21:46:24.862000.The original answer accommodated the way MongoDB "date" fields were represented as:{"$date": 1506816000000}If you want a generic Python solution for serializing datetime to json, check out @jjmontes' answer for a quick solution which requires no dependencies.As you are using mongoengine (per comments) and pymongo is a dependency, pymongo has built-in utilities to help with json serialization:

http://api.mongodb.org/python/1.10.1/api/bson/json_util.htmlExample usage (serialization):Example usage (deserialization):Django provides a native DjangoJSONEncoder serializer that deals with this kind of properly.See https://docs.djangoproject.com/en/dev/topics/serialization/#djangojsonencoderOne difference I've noticed between DjangoJSONEncoder and using a custom default like this:Is that Django strips a bit of the data:So, you may need to be careful about that in some cases.My quick & dirty JSON dump that eats dates and everything:Building on other answers, a simple solution based on a specific serializer that just converts datetime.datetime and datetime.date objects to strings.As seen, the code just checks to find out if object is of class datetime.datetime or datetime.date, and then uses .isoformat() to produce a serialized version of it, according to ISO 8601 format, YYYY-MM-DDTHH:MM:SS (which is easily decoded by JavaScript). If more complex serialized representations are sought, other code could be used instead of str() (see other answers to this question for examples). The code ends by raising an exception, to deal with the case it is called with a non-serializable type.This json_serial function can be used as follows:The details about how the default parameter to json.dumps works can be found in Section Basic Usage of the json module documentation. I have just encountered this problem and my solution is to subclass json.JSONEncoder:In your call do something like: json.dumps(yourobj, cls=DateTimeEncoder) The .isoformat() I got from one of the answers above.Convert the date to a stringFor others who do not need or want to use the pymongo library for this.. you can achieve datetime JSON conversion easily with this small snippet:Then use it like so:output: Here is my solution:Then you can use it like that:I have an application with a similar issue; my approach was to JSONize the datetime value as a 6-item list (year, month, day, hour, minutes, seconds); you could go to microseconds as a 7-item list, but I had no need to:produces:My solution (with less verbosity, I think):Then use jsondumps instead of json.dumps. It will print:I you want, later you can add other special cases to this with a simple twist of the default method. Example:  This Q repeats time and time again - a simple way to patch the json module such that serialization would support datetime.  Than use json serialization as you always do - this time with datetime being serialized as isoformat.Resulting in: '{"created": "2015-08-26T14:21:31.853855"}'See more details and some words of caution at:

StackOverflow: JSON datetime between Python and JavaScriptif you are using python3.7, then the best solution is using

datetime.isoformat() and

datetime.fromisoformat(); they work with both naive and

aware datetime objects:output:if you are using python3.6 or below, and you only care about the time value (not

the timezone), then you can use datetime.timestamp() and

datetime.fromtimestamp() instead;if you are using python3.6 or below, and you do care about the timezone, then

you can get it via datetime.tzinfo, but you have to serialize this field

by yourself; the easiest way to do this is to add another field _tzinfo in the

serialized object;finally, beware of precisions in all these examples;The json.dumps method can accept an optional parameter called default which is expected to be a function. Every time JSON tries to convert a value it does not know how to convert it will call the function we passed to it. The function will receive the object in question, and it is expected to return the JSON representation of the object.You should use .strftime() method on .datetime.now() method to making it as a serializable method.Here's an example:Output:Here is a simple solution to over come "datetime not JSON serializable"

problem. Output:-> {"date": "2015-12-16T04:48:20.024609"} You have to supply a custom encoder class with the cls parameter of json.dumps. To quote from the docs:This uses complex numbers as the example, but you can just as easily create a class to encode dates (except I think JSON is a little fuzzy about dates)The simplest way to do this is to change the part of the dict that is in datetime format to isoformat. That value will effectively be a string in isoformat which json is ok with.Actually it is quite simple.

If you need to often serialize dates, then work with them as strings. You can easily convert them back as datetime objects if needed.If you need to work mostly as datetime objects, then convert them as strings before serializing. As you can see, the output is the same in both cases. Only the type is different.If you are using the result in a view be sure to return a proper response. According to the API, jsonify does the following:To mimic this behavior with json.dumps you have to add a few extra lines of code.You should also return a dict to fully replicate jsonify's response. So, the entire file will look like thisTry this one with an example to parse it:My solution ...Ok, now some tests.Here is my full solution for converting datetime to JSON and back..OutputJSON FileThis has enabled me to import and export strings, ints, floats and datetime objects.

It shouldn't be to hard to extend for other types.Convert the date  to  stringGenerally there are several ways to serialize datetimes, like:If you're okay with the last way, the json_tricks package handles dates, times and datetimes including timezones.which gives:So all you need to do isand then import from json_tricks instead of json.The advantage of not storing it as a single string, int or float comes when decoding: if you encounter just a string or especially int or float, you need to know something about the data to know if it's a datetime. As a dict, you can store metadata so it can be decoded automatically, which is what json_tricks does for you. It's also easily editable for humans.Disclaimer: it's made by me. Because I had the same problem.I got the same error message while writing the serialize decorator inside a Class  with sqlalchemy. So instead of :I  simply borrowed jgbarah's idea of using isoformat() and appended the original value with isoformat(), so that it now looks like:A quick fix if you want your own formattingIf you are on both sides of the communication you can use repr() and eval() functions along with json.You shouldn't import datetime assince eval will complain. Or you can pass datetime as a parameter to eval. In any case this should work.I had encountered same problem when externalizing django model object to dump as JSON.

Here is how you can solve it.Usage of above utility:This library superjson can do it. And you can easily custom json serializer for your own Python Object by following this instruction https://superjson.readthedocs.io/index.html#extend.The general concept is:your code need to locate the right serialization / deserialization method based on the python object. Usually, the full classname is a good identifier.And then your ser / deser method should be able to transform your object to a regular Json serializable object, a combination of generic python type, dict, list, string, int, float. And implement your deser method reversely.I may not 100% correct but,

this is the simple way to do serialize

What is the difference between dict.items() and dict.iteritems() in Python2?

the wolf

[What is the difference between dict.items() and dict.iteritems() in Python2?](https://stackoverflow.com/questions/10458437/what-is-the-difference-between-dict-items-and-dict-iteritems-in-python2)

Are there any applicable differences between dict.items() and dict.iteritems()?From the Python docs:If I run the code below, each seems to return a reference to the same object. Are there any subtle differences that I am missing?Output:

2012-05-05 02:58:27Z

Are there any applicable differences between dict.items() and dict.iteritems()?From the Python docs:If I run the code below, each seems to return a reference to the same object. Are there any subtle differences that I am missing?Output:It's part of an evolution. Originally, Python items() built a real list of tuples and returned that. That could potentially take a lot of extra memory. Then, generators were introduced to the language in general, and that method was reimplemented as an iterator-generator method named iteritems(). The original remains for backwards compatibility. One of Python 3’s changes is that  items() now return iterators, and a list is never fully built. The iteritems() method is also gone, since items() in Python 3 works like viewitems() in Python 2.7. dict.items() returns a list of 2-tuples ([(key, value), (key, value), ...]), whereas dict.iteritems() is a generator that yields 2-tuples. The former takes more space and time initially, but accessing each element is fast, whereas the second takes less space and time initially, but a bit more time in generating each element.The commands dict.items(), dict.keys() and dict.values() return a copy of the dictionary's list of (k, v) pair, keys and values.

This could take a lot of memory if the copied list is very large.The commands dict.iteritems(), dict.iterkeys() and dict.itervalues() return an iterator over the dictionary’s (k, v) pair, keys and values.The commands dict.viewitems(), dict.viewkeys() and dict.viewvalues() return the view objects, which can reflect the dictionary's changes.

(I.e. if you del an item or add a (k,v) pair in the dictionary, the view object can automatically change at the same time.)In Py3.x, things are more clean, since there are only dict.items(), dict.keys() and dict.values() available, which return the view objects just as dict.viewitems() in Py2.x did. Just as @lvc noted, view object isn't the same as iterator, so if you want to return an iterator in Py3.x, you could use iter(dictview) :You asked: 'Are there any applicable differences between dict.items() and dict.iteritems()'This may help (for Python 2.x):You can see that d.items() returns a list of tuples of the key, value pairs and d.iteritems() returns a dictionary-itemiterator.As a list, d.items() is slice-able:But would not have an __iter__ method:As an iterator, d.iteritems() is not slice-able:But does have __iter__:So the items themselves are same -- the container delivering the items are different. One is a list, the other an iterator (depending on the Python version...) So the applicable differences between dict.items() and dict.iteritems() are the same as the applicable differences between a list and an iterator. dict.items() return list of tuples, and dict.iteritems() return iterator object of tuple in dictionary as (key,value). The tuples are the same, but container is different. dict.items() basically copies all dictionary into list. Try using following code to compare the execution times of the dict.items() and dict.iteritems(). You will see the difference.Output in my machine:This clearly shows that dictionary.iteritems() is much more efficient.If you have dict = {key1:value1, key2:value2, key3:value3,...}In Python 2, dict.items() copies each tuples and returns the list of tuples in dictionary i.e. [(key1,value1), (key2,value2), ...]. 

Implications are that the whole dictionary is copied to new list containing tuplesdict.iteritems() returns the dictionary item iterator. The value of the item returned is also the same i.e. (key1,value1), (key2,value2), ..., but this is not a list. This is only dictionary item iterator object. That means less memory usage (50% less).The tuples are the same. You compared tuples in each so you get same.In Python 3, dict.items() returns iterator object. dict.iteritems() is removed so there is no more issue.dict.iteritems is gone in Python3.x So use iter(dict.items()) to get the same output and memory alocationdict.iteritems(): gives you an iterator. You may use the iterator in other patterns outside of the loop.If you want a way to iterate the item pairs of a dictionary that works with both Python 2 and 3, try something like this:Use it like this:dict.iteritems() in python 2 is equivalent to dict.items() in python 3.

How to get an absolute file path in Python

izb

[How to get an absolute file path in Python](https://stackoverflow.com/questions/51520/how-to-get-an-absolute-file-path-in-python)

Given a path such as "mydir/myfile.txt", how do I find the file's absolute path relative to the current working directory in Python? E.g. on Windows, I might end up with:

2008-09-09 10:19:32Z

Given a path such as "mydir/myfile.txt", how do I find the file's absolute path relative to the current working directory in Python? E.g. on Windows, I might end up with:Also works if it is already an absolute path:You could use the new Python 3.4 library pathlib. (You can also get it for Python 2.6 or 2.7 using pip install pathlib.) The authors wrote: "The aim of this library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them."To get an absolute path in Windows:Or on UNIX:Docs are here: https://docs.python.org/3/library/pathlib.htmlBetter still, install the module (found on PyPI), it wraps all the os.path functions and other related functions into methods on an object that can be used wherever strings are used:Today you can also use the unipath package which was based on path.py: http://sluggo.scrapping.cc/python/unipath/I would recommend using this package as it offers a clean interface to common os.path utilities.Update for Python 3.4+ pathlib that actually answers the question:If you only need a temporary string, keep in mind that you can use Path objects with all the relevant functions in os.path, including of course abspath:Note that expanduser is necessary (on Unix) in case the given expression for the file (or directory) name and location may contain a leading ~/(the tilde refers to the user's home directory), and expandvars takes care of any other environment variables (like $HOME).This always gets the right filename of the current script, even when it is called from within another script. It is especially useful when using subprocess.from there, you can get the script's full path with:It also makes easier to navigate folders by just appending /.. as many times as you want to go 'up' in the directories' hierarchy. To get the cwd:For the parent path:By combining "/.." with other filenames, you can access any file in the system.Module os provides a way to find abs path.BUT most of the paths in Linux start with ~ (tilde), which doesn't give a satisfactory result.so you can use srblib for that.install it using python3 -m pip install srblibhttps://pypi.org/project/srblib/I prefer to use globhere is how to list all file types in your current folder:here is how to list all (for example) .txt files in your current folder:here is how to list all file types in a chose directory:hope this helped youif you are on a mac this will give you a full path:will show the following path:In case someone is using python and linux and looking for full path to file:

not None test in Python [duplicate]

prosseek

[not None test in Python [duplicate]](https://stackoverflow.com/questions/3965104/not-none-test-in-python)

Out of these not None tests.Which one is preferable, and why?

2010-10-19 03:20:18Z

Out of these not None tests.Which one is preferable, and why?is the Pythonic idiom for testing that a variable is not set to None. This idiom has particular uses in the case of declaring keyword functions with default parameters. is tests identity in Python. Because there is one and only one instance of None present in a running Python script/program, is is the optimal test for this. As Johnsyweb points out, this is discussed in PEP 8 under "Programming Recommendations".As for why this is preferred to this is simply part of the Zen of Python: "Readability counts." Good Python is often close to good pseudocode.From, Programming Recommendations, PEP 8:PEP 8 is essential reading for any Python programmer.Either of the latter two, since val could potentially be of a type that defines __eq__() to return true when passed None.The best bet with these types of questions is to see exactly what python does.  The dis module is incredibly informative:Note that the last two cases reduce to the same sequence of operations (python reads not (val is None) and uses the is not operator).  The first uses the != operator when comparing with None.As pointed out by other answers, using != when comparing with None is a bad idea

About catching ANY exception

user469652

[About catching ANY exception](https://stackoverflow.com/questions/4990718/about-catching-any-exception)

How can I write a try/except block that catches all exceptions?

2011-02-14 09:46:58Z

How can I write a try/except block that catches all exceptions?You can but you probably shouldn't:However, this will also catch exceptions like KeyboardInterrupt and you usually don't want that, do you? Unless you re-raise the exception right away - see the following example from the docs:Apart from a bare except: clause (which as others have said you shouldn't use), you can simply catch Exception:You would normally only ever consider doing this at the outermost level of your code if for example you wanted to handle any otherwise uncaught exceptions before terminating.The advantage of except Exception over the bare except is that there are a few exceptions that it wont catch, most obviously KeyboardInterrupt and SystemExit: if you caught and swallowed those then you could make it hard for anyone to exit your script.You can do this to handle general exceptionsTo catch all possible exceptions, catch BaseException. It's on top of the Exception hierarchy:Python 3:

https://docs.python.org/3.5/library/exceptions.html#exception-hierarchyPython 2.7:

https://docs.python.org/2.7/library/exceptions.html#exception-hierarchyBut as other people mentioned, you would usually not need this, only for specific cases.Very simple example, similar to the one found here:http://docs.python.org/tutorial/errors.html#defining-clean-up-actionsIf you're attempting to catch ALL exceptions, then put all your code within the "try:" statement, in place of 'print "Performing an action which may throw an exception."'.In the above example, you'd see output in this order:1) Performing an action which may throw an exception.2) Finally is called directly after executing the try statement whether an exception is thrown or not.3) "An exception was thrown!" or "Everything looks great!" depending on whether an exception was thrown.Hope this helps!There are multiple ways to do this in particular with Python 3.0 and aboveApproach 1 This is simple approach but not recommended because you would not know exactly which line of code is actually throwing the exception:Approach 2This approach is recommended because it provides more detail about each exception. It includes:The only drawback is tracback needs to be imported.I've just found out this little trick for testing if exception names in Python 2.7 . Sometimes i have handled specific exceptions in the code, so i needed a test to see if that name is within a list of handled exceptions.It is worth mentioning this is not proper Python coding. This will catch also many errors you might not want to catch.

How to print a date in a regular format?

NomadAlien

[How to print a date in a regular format?](https://stackoverflow.com/questions/311627/how-to-print-a-date-in-a-regular-format)

This is my code:This prints: 2008-11-22 which is exactly what I want.But, I have a list I'm appending this to and then suddenly everything goes "wonky". Here is the code:This prints the following: How can I get just a simple date like 2008-11-22?

2008-11-22 18:37:07Z

This is my code:This prints: 2008-11-22 which is exactly what I want.But, I have a list I'm appending this to and then suddenly everything goes "wonky". Here is the code:This prints the following: How can I get just a simple date like 2008-11-22?In Python, dates are objects. Therefore, when you manipulate them, you manipulate objects, not strings, not timestamps nor anything.Any object in Python have TWO string representations:What happened is that when you have printed the date using "print", it used str() so you could see a nice date string. But when you have printed mylist, you have printed a list of objects and Python tried to represent the set of data, using repr().Well, when you manipulate dates, keep using the date objects all long the way. They got thousand of useful methods and most of the Python API expect dates to be objects.When you want to display them, just use str(). In Python, the good practice is to explicitly cast everything. So just when it's time to print, get a string representation of your date using str(date).One last thing. When you tried to print the dates, you printed mylist. If you want to print a date, you must print the date objects, not their container (the list).E.G, you want to print all the date in a list :Note that in that specific case, you can even omit str() because print will use it for you. But it should not become a habit :-)Dates have a default representation, but you may want to print them in a specific format. In that case, you can get a custom string representation using the strftime() method.strftime() expects a string pattern explaining how you want to format your date.E.G : All the letter after a "%" represent a format for something :etcHave a look at the official documentation, or McCutchen's quick reference you can't know them all.Since PEP3101, every object can have its own format used automatically by the method format of any string. In the case of the datetime, the format is the same used in

strftime. So you can do the same as above like this:The advantage of this form is that you can also convert other objects at the same time.

With the introduction of Formatted string literals (since Python 3.6, 2016-12-23) this can be written asDates can automatically adapt to the local language and culture if you use them the right way, but it's a bit complicated. Maybe for another question on SO(Stack Overflow) ;-)Edit:After Cees suggestion, I have started using time as well:The date, datetime, and time objects all support a strftime(format) method, 

to create a string representing the time under the control of an explicit format

string.Here is a list of the format codes with their directive and meaning. This is what we can do with the datetime and time modules in PythonThat will print out something like this:Use date.strftime. The formatting arguments are described in the documentation.This one is what you wanted:This one takes Locale into account. (do this)This is shorter:Or even Out: '25.12.2013orOut: 'Today - 25.12.2013'Out: 'Wednesday'Out: '__main____2014.06.09__16-56.log'Simple answer - With type-specific datetime string formatting (see nk9's answer using str.format().) in a Formatted string literal (since Python 3.6, 2016-12-23):The date/time format directives are not documented as part of the Format String Syntax but rather in date, datetime, and time's strftime() documentation. The are based on the 1989 C Standard, but include some ISO 8601 directives since Python 3.6.You need to convert the date time object to a string.The following code worked for me:Let me know if you need any more help.You can do:I hate the idea of importing too many modules for convenience. I would rather work with available module which in this case is datetime rather than calling a new module time.You may want to append it as a string?Since the print today returns what you want this means that the today object's __str__ function returns the string you are looking for. So you can do mylist.append(today.__str__()) as well.Considering the fact you asked for something simple to do what you wanted, you could just:For those wanting locale-based date and not including time, use:You can use easy_date to make it easy:A quick disclaimer for my answer - I've only been learning Python for about 2 weeks, so I am by no means an expert; therefore, my explanation may not be the best and I may use incorrect terminology. Anyway, here it goes.I noticed in your code that when you declared your variable today = datetime.date.today() you chose to name your variable with the name of a built-in function. When your next line of code mylist.append(today) appended your list, it appended the entire string datetime.date.today(), which you had previously set as the value of your today variable, rather than just appending today(). A simple solution, albeit maybe not one most coders would use when working with the datetime module, is to change the name of your variable.Here's what I tried: and it prints yyyy-mm-dd.Here is how to display the date as (year/month/day) :this will print 6-23-2018 if that's what you want :)In this way you can get Date formatted like this example: 22-Jun-2017I don't fully understand but, can use pandas for getting times in right format:And:But it's storing strings but easy to convert:

Running unittest with typical test directory structure

Major Major

[Running unittest with typical test directory structure](https://stackoverflow.com/questions/1896918/running-unittest-with-typical-test-directory-structure)

The very common directory structure for even a simple Python module seems to be to separate the unit tests into their own test directory:for example see this Python project howto.My question is simply What's the usual way of actually running the tests? I suspect this is obvious to everyone except me, but you can't just run python test_antigravity.py from the test directory as its import antigravity will fail as the module is not on the path.I know I could modify PYTHONPATH and other search path related tricks, but I can't believe that's the simplest way - it's fine if you're the developer but not realistic to expect your users to use if they just want to check the tests are passing.The other alternative is just to copy the test file into the other directory, but it seems a bit dumb and misses the point of having them in a separate directory to start with.So, if you had just downloaded the source to my new project how would you run the unit tests? I'd prefer an answer that would let me say to my users: "To run the unit tests do X."

2009-12-13 16:10:23Z

The very common directory structure for even a simple Python module seems to be to separate the unit tests into their own test directory:for example see this Python project howto.My question is simply What's the usual way of actually running the tests? I suspect this is obvious to everyone except me, but you can't just run python test_antigravity.py from the test directory as its import antigravity will fail as the module is not on the path.I know I could modify PYTHONPATH and other search path related tricks, but I can't believe that's the simplest way - it's fine if you're the developer but not realistic to expect your users to use if they just want to check the tests are passing.The other alternative is just to copy the test file into the other directory, but it seems a bit dumb and misses the point of having them in a separate directory to start with.So, if you had just downloaded the source to my new project how would you run the unit tests? I'd prefer an answer that would let me say to my users: "To run the unit tests do X."The best solution in my opinion is to use the unittest command line interface which will add the directory to the sys.path so you don't have to (done in the TestLoader class).For example for a directory structure like this:You can just run:For a directory structure like yours:And in the test modules inside the test package, you can import the antigravity package and its modules as usual:Running a single test module:To run a single test module, in this case test_antigravity.py:Just reference the test module the same way you import it.Running a single test case or test method:Also you can run a single TestCase or a single test method:Running all tests:You can also use test discovery which will discover and run all the tests for you, they must be modules or packages named test*.py (can be changed with the -p, --pattern flag):This will run all the test*.py modules inside the test package.The simplest solution for your users is to provide an executable script (runtests.py or some such) which bootstraps the necessary test environment, including, if needed, adding your root project directory to sys.path temporarily. This doesn't require users to set environment variables, something like this works fine in a bootstrap script:Then your instructions to your users can be as simple as "python runtests.py".Of course, if the path you need really is os.path.dirname(__file__), then you don't need to add it to sys.path at all; Python always puts the directory of the currently running script at the beginning of sys.path, so depending on your directory structure, just locating your runtests.py at the right place might be all that's needed.Also, the unittest module in Python 2.7+ (which is backported as unittest2 for Python 2.6 and earlier) now has test discovery built-in, so nose is no longer necessary if you want automated test discovery: your user instructions can be as simple as python -m unittest discover.I generally create a "run tests" script in the project directory (the one that is common to both the source directory and test) that loads my "All Tests" suite. This is usually boilerplate code, so I can reuse it from project to project.run_tests.py:test/all_tests.py (from How do I run all Python unit tests in a directory?)With this setup, you can indeed just include antigravity in your test modules. The downside is you would need more support code to execute a particular test... I just run them all every time.From the article you linked to:Perhaps you should look at nose as it suggests?I had the same problem, with a separate unit tests folder. From the mentioned suggestions I add the absolute source path to sys.path.The benefit of the following solution is, that one can run the file test/test_yourmodule.py without changing at first into the test-directory:if you run "python setup.py develop" then the package will be in the path. But you may not want to do that because you could infect your system python installation, which is why tools like virtualenv and buildout exist.  Solution/Example for Python unittest moduleGiven the following project structure:You can run your project from the root directory with python project_name, which calls ProjectName/project_name/__main__.py.To run your tests with python test, effectively running ProjectName/test/__main__.py, you need to do the following:1) Turn your test/models directory into a package by adding a __init__.py file. This makes the test cases within the sub directory accessible from the parent test directory.2) Modify your system path in test/__main__.py to include the project_name directory.Now you can successfully import things from project_name in your tests.Use setup.py develop to make your working directory be part of the installed Python environment, then run the tests.If you use VS Code and your tests are located on the same level as your project then running and debug your code doesn't work out of the box. What you can do is change your launch.json file:The key line here is envFileIn the root of your project add .env fileInside of your .env file add path to the root of your project. This will temporarily add path to your project and you will be able to use debug unit tests from VS CodeI noticed that if you run the unittest command line interface from your "src" directory, then imports work correctly without modification.If you want to put that in a batch file in your project directory, you can do this:Following is my project structure:I found it better to import in the setUp() method:I use Python 3.6.2To install pytest: sudo pip install pytestI didn't set any path variable and my imports are not failing with the same "test" project structure.I commented out this stuff: if __name__ == '__main__' like this:test_antigravity.pyIt's possible to use wrapper which runs selected or all tests.For instance:or to run all tests recursively use globbing (tests/**/*.py) (enable by shopt -s globstar).The wrapper can basically use argparse to parse the arguments like:Then load all the tests:then add them into your test suite (using inspect):and run them:Check this example for more details.See also: How to run all Python unit tests in a directory?Adding to @PierreUsing unittest directory structure like this:To run the test module test_antigravity.py:Or a single TestCase Mandatory don't forget the __init__.py even if empty otherwise will not work.You can't import from the parent directory without some voodoo. Here's yet another way that works with at least Python 3.6.First, have a file test/context.py with the following content:Then have the following import in the file test/test_antigravity.py:Note that the reason for this try-except clause is that With this trickery they both work. Now you can run all the test files within test directory with:or run an individual test file with:Ok, it's not much prettier than having the content of context.py within test_antigravity.py, but maybe a little. Suggestions are welcome.This BASH script will execute the python unittest test directory from anywhere in the file system, no matter what working directory you are in.This is useful when staying in the ./src or ./example working directory and you need a quick unit test:No need for a test/__init__.py file to burden your package/memory-overhead during production.If you have multiple directories in your test directory, then you have to add to each directory an __init__.py file.Then to run every test at once, run:Source:  python -m unittest -hThis way will let you run the test scripts from wherever you want without messing around with system variables from the command line.This adds the main project folder to the python path, with the location found relative to the script itself, not relative to the current working directory. Add that to the top of all your test scripts. That will add the main project folder to the system path, so any module imports that work from there will now work. And it doesn't matter where you run the tests from. You can obviously change the project_path_hack file to match your main project folder location. If you are looking for a command line-only solution:Based on the following directory structure (generalized with a dedicated source directory):Windows: (in new_project)See this question if you want to use this in a batch for-loop.Linux: (in new_project)With this approach, it is also possible to add more directories to the PYTHONPATH if necessary.You should really use the pip tool.Use pip install -e . to install your package in development mode. This is a very good practice, recommended by pytest (see their good practices documentation, where you can also find two project layouts to follow).I've had the same problem for a long time. What I recently chose is the following directory structure:and in the __init__.py script of the test folder, I write the following:Super important for sharing the project is the Makefile, because it enforces running the scripts properly. Here is the command that I put in the Makefile:The Makefile is important not just because of the command it runs but also because of where it runs it from. If you would cd in tests and do python -m unittest discover ., it wouldn't work because the init script in unit_tests calls os.getcwd(), which would then point to the incorrect absolute path (that would be appended to sys.path and you would be missing your source folder). The scripts would run since discover finds all the tests, but they wouldn't run properly. So the Makefile is there to avoid having to remember this issue.I really like this approach because I don't have to touch my src folder, my unit tests or my environment variables and everything runs smoothly. Let me know if you guys like it. Hope that helps,

How to make a Python script standalone executable to run without ANY dependency?

Jeff

[How to make a Python script standalone executable to run without ANY dependency?](https://stackoverflow.com/questions/5458048/how-to-make-a-python-script-standalone-executable-to-run-without-any-dependency)

I'm building a Python application and don't want to force my clients to install Python and modules.So, is there a way to compile a Python script to be a standalone executable?

2011-03-28 11:04:09Z

I'm building a Python application and don't want to force my clients to install Python and modules.So, is there a way to compile a Python script to be a standalone executable?You can use py2exe as already answered and use cython to convert your key .py files in .pyc, C compiled files, like .dll in Windows and .so in linux, much harder to revert than common .pyo and .pyc files (and also gain in performance!)You can use PyInstaller to package Python programs as standalone executables. It works on Windows, Linux, and Mac.You might wish to investigate  Nuitka. It takes python source code and converts it in to C++ API calls. Then it compiles into an executable binary (ELF on Linux). It has been around for a few years now and supports a wide range of Python versions. You will probably also get a performance improvement if you use it. Recommended.I would like to compile some useful information about creating standalone files on windows using Python 2.7.I have used py2exe and it works, but I had some problems.This last reason made me try PyInstaller http://www.pyinstaller.org/ .In my opinion, it is much better because:I suggest creating a .bat file with the following lines for example (pyinstaller.exe must be in Windows Path):So, I think that, at least for python 2.7, a better and simpler option is PyInstaller.And a third option is cx_Freeze, which is cross-platform.Yes, it is possible to compile Python scripts into standalone executable.PyInstaller can be used to convert Python programs into stand-alone executables, under Windows, Linux, Mac OS X, FreeBSD, Solaris and AIX. It is one of the recommended converters.py2exe converts Python scripts into only executable in Windows platform.Cython is a static compiler for both the Python programming language and the extended Cython programming language.you may like py2exe. you'll also find in there infos for doing it on linuxUse py2exe.... use below set up files:I like pyinstaller - especially the "windowed" variant:It will create one single *.exe file in a dist/ folder.I also recommend  pyinstaller 

for better backward compatibility such as python 2.3 - 2.7.

for py2exe, you have to have python 2.6For Python 3.2 scripts the only choice is Cxfreeze. Build it from sources otherwise it won't work.For python 2.x I suggest pyinstaller as it can package a python program in a single executable, unlike CxFreeze which outputs also libraries.py2exe will make the exe file you want but you need to have the same version of MSVCR90.dll on the machine you're going to use your new exe. See http://www.py2exe.org/index.cgi/Tutorial for more info.You can find the list of distribution utilities listed @ https://wiki.python.org/moin/DistributionUtilities. I use bbfreeze and it has been working very well (yet to have python 3 support though).Not exactly a packaging of the python code, but there is now also grumpy from google, which transpiles the code to Go. It doesn't support the python C api, so it may not work for all projects.This creates a standalone EXE file in Windowsyou can install pyinstaller using pip install PyInstallerUse Cython to convert to c, compile and link with gcc. Another could be, make the core functions in c (the ones you want to make hard to reverse), compile them and use python boost to import the compiled code ( plus you get a much faster code execution). then use any tool mentioned to distribute.Using pyinstaller, I found a better method using shortcut to the .exe rather than making --onefile. Anyways there's probably some data files around and if you're running a site-based app then your program depends on html, js, css files too. No point in moving all these files somewhere.. instead what if we move the working path up.Make a shortcut to the exe, move it at top and set the target and start-in paths as specified, to have relative paths going to dist\folder:

Target: %windir%\system32\cmd.exe /c start dist\web_wrapper\web_wrapper.exe

Start in: "%windir%\system32\cmd.exe /c start dist\web_wrapper\"

Can rename shortcut to anything so renaming to "GTFS-Manager"

Now when I double-click the shortcut, it's as if I python-ran the file! I found this approach better than the --onefile one as:   Oh, remember to delete off the build folder after building, will save on size.I'm told that PyRun, https://www.egenix.com/products/python/PyRun/, is also an option.

Styling multi-line conditions in 'if' statements?

Eli Bendersky

[Styling multi-line conditions in 'if' statements?](https://stackoverflow.com/questions/181530/styling-multi-line-conditions-in-if-statements)

Sometimes I break long conditions in ifs onto several lines. The most obvious way to do this is:Isn't very very appealing visually, because the action blends with the conditions. However, it is the natural way using correct Python indentation of 4 spaces.For the moment I'm using:But this isn't very pretty. :-)Can you recommend an alternative way?

2008-10-08 06:19:07Z

Sometimes I break long conditions in ifs onto several lines. The most obvious way to do this is:Isn't very very appealing visually, because the action blends with the conditions. However, it is the natural way using correct Python indentation of 4 spaces.For the moment I'm using:But this isn't very pretty. :-)Can you recommend an alternative way?You don't need to use 4 spaces on your second conditional line.  Maybe use:Also, don't forget the whitespace is more flexible than you might think:Both of those are fairly ugly though.Maybe lose the brackets (the Style Guide discourages this though)?This at least gives you some differentiation.Or even:I think I prefer:Here's the Style Guide, which (since 2010) recommends using brackets.I've resorted to the following in the degenerate case where it's simply AND's or OR's.It shaves a few characters and makes it clear that there's no subtlety to the condition.Someone has to champion use of vertical whitespace here! :)This makes each condition clearly visible. It also allows cleaner expression of more complex conditions:Yes, we're trading off a bit of vertical real estate for clarity. Well worth it IMO.I prefer this style when I have a terribly large if-condition:Here's my very personal take: long conditions are (in my view) a code smell that suggests refactoring into a boolean-returning function/method. For example:Now, if I found a way to make multi-line conditions look good, I would probably find myself content with having them and skip the refactoring.On the other hand, having them perturb my aesthetic sense acts as an incentive for refactoring.My conclusion, therefore, is that multiple line conditions should look ugly and this is an incentive to avoid them.This doesn't improve so much but...I suggest moving the and keyword to the second line and indenting all lines containing conditions with two spaces instead of four:This is exactly how I solve this problem in my code. Having a keyword as the first word in the line makes the condition a lot more readable, and reducing the number of spaces further distinguishes condition from action.It seems worth quoting PEP 0008 (Python's official style guide), since it comments upon this issue at modest length:Note the "not limited to" in the quote above; besides the approaches suggested in the style guide, some of the ones suggested in other answers to this question are acceptable too.Here's what I do, remember that "all" and "any" accepts an iterable, so I just put a long condition in a list and let "all" do the work.I'm surprised not to see my preferred solution,Since and is a keyword, it gets highlighted by my editor, and looks sufficiently different from the do_something below it.Personally, I like to add meaning to long if-statements. I would have to search through code to find an appropriate example, but here's the first example that comes to mind: let's say I happen to run into some quirky logic where I want to display a certain page depending on many variables.English: "If the logged-in user is NOT an administrator teacher, but is just a regular teacher, and is not a student themselves..."Sure this might look fine, but reading those if statements is a lot of work. How about we assign the logic to label that makes sense. The "label" is actually the variable name:This may seem silly, but you might have yet another condition where you ONLY want to display another item if, and only if, you're displaying the teacher panel OR if the user has access to that other specific panel by default:Try writing the above condition without using variables to store and label your logic, and not only do you end up with a very messy, hard-to-read logical statement, but you also just repeated yourself. While there are reasonable exceptions, remember: Don't Repeat Yourself (DRY)."all" and "any" are nice for the many conditions of same type case. BUT they always evaluates all conditions. As shown in this example:(I've lightly modified the identifiers as fixed-width names aren't representative of real code – at least not real code that I encounter – and will belie an example's readability.)This works well for "and" and "or" (it's important that they're first on the second line), but much less so for other long conditions.  Fortunately, the former seem to be the more common case while the latter are often easily rewritten with a temporary variable.  (It's usually not hard, but it can be difficult or much less obvious/readable to preserve the short-circuiting of "and"/"or" when rewriting.)Since I found this question from your blog post about C++, I'll include that my C++ style is identical:Adding to what @krawyoti said... Long conditions smell because they are difficult to read and difficult to understand. Using a function or a variable makes the code clearer. In Python, I prefer to use vertical space, enclose parenthesis, and place the logical operators at the beginning of each line so the expressions don't look like "floating".If the conditions need to be evaluated more than once, as in a while loop, then using a local function is best.Plain and simple, also passes pep8 checks:In recent times I have been preferring the all and any functions, since I rarely mix And and Or comparisons this works well, and has the additional advantage of Failing Early with generators comprehension:Just remember to pass in a single iterable! Passing in N-arguments is not correct.Note: any is like many or comparisons, all is like many and comparisons.This combines nicely with generator comprehensions, for example:More on: generator comprehensionWhat if we only insert an additional blank line between the condition and the body and do the rest in the canonical way?p.s. I always use tabs, not spaces; I cannot fine-tune...What I usually do is:this way the closing brace and colon visually mark the end of our condition.All respondents that also provide multi-conditionals for the if statement is just as ugly as the problem presented. You don't solve this problem by doing the same thing.. Even the PEP 0008 answer is repulsive.Here is a far more readable approachWant me to eat my words? Convince me you need multi-conditionals and I'll literally print this and eat it for your amusement.I think @zkanda's solution would be good with a minor twist. If you had your conditions and values in their own respective lists, you could use a list comprehension to do the comparison, which would make things a bit more general for adding condition/value pairs.If I did want to hard-code a statement like this, I would write it like this for legibility:And just to throw another solution out there with an iand operator:Just a few other random ideas for completeness's sake.  If they work for you, use them.  Otherwise, you're probably better off trying something else.You could also do this with a dictionary:This option is more complicated, but you may also find it useful:Dunno if that works for you, but it's another option to consider.  Here's one more way:The last two I haven't tested, but the concepts should be enough to get you going if that's what you want to go with.(And for the record, if this is just a one time thing, you're probably just better off using the method you presented at first.  If you're doing the comparison in lots of places, these methods may enhance readability enough to make you not feel so bad about the fact that they are kind of hacky.)I've been struggling to find a decent way to do this as well, so I just came up with an idea (not a silver bullet, since this is mainly a matter of taste).I find a few merits in this solution compared to others I've seen, namely, you get exactly an extra 4 spaces of indentation (bool), allowing all conditions to line up vertically, and the body of the if statement can be indented in a clear(ish) way. This also keeps the benefits of short-circuit evaluation of boolean operators, but of course adds the overhead of a function call that basically does nothing. You could argue (validly) that any function returning its argument could be used here instead of bool, but like I said, it's just an idea and it's ultimately a matter of taste.Funny enough, as I was writing this and thinking about the "problem", I came up with yet another idea, which removes the overhead of a function call. Why not indicate that we're about to enter a complex condition by using extra pairs of parentheses? Say, 2 more, to give a nice 2 space indent of the sub-conditions relative to the body of the if statement. Example:I kind of like this because when you look at it, a bell immediatelly rings in your head saying "hey, there's a complex thing going on here!". Yes, I know that parentheses don't help readability, but these conditions should appear rarely enough, and when they do show up, you are going to have to stop and read them carefuly anyway (because they're complex).Anyway, just two more proposals that I haven't seen here. Hope this helps someone :)You could split it into two linesOr even add on one condition at a time. That way, at least it separates the clutter from the if.I know this thread is old, but I have some Python 2.7 code and PyCharm (4.5) still complains about this case:Even with the PEP8 warning "visually indented line with same indent as next logical line", the actual code is completely OK?  It's not "over-indenting?"...there are times I wish Python would've bit the bullet and just gone with curly braces.  I wonder how many bugs have been accidentally introduced over the years due to accidental mis-indentation...Pack your conditions into a list, then do smth. like:I find that when I have long conditions, I often have a short code body.  In that case, I just double-indent the body, thus:or if this is clearer:There is no reason indent should be a multiple of 4 in this case, e.g. see "Aligned with opening delimiter":http://google-styleguide.googlecode.com/svn/trunk/pyguide.html?showone=Indentation#IndentationHere's another approach:This also makes it easy to add another condition easily without changing the if statement by simply appending another condition to the list:I usually use: if our if & an else condition has to execute multiple statement inside of it than we can write like below.

Every when we have if else example with one statement inside of it .Thanks it work for me.Pardon my noobness, but it happens that I'm not as knowledgeable of #Python as anyone of you here, but it happens that I have found something similar when scripting my own objects in a 3D BIM modeling, so I will adapt my algorithm to that of python.The problem that I find here, is double sided:Do to bypass all these problems, your script must go like thisPros of this method:Hope it help you all

Split Strings into words with multiple word boundary delimiters

ooboo

[Split Strings into words with multiple word boundary delimiters](https://stackoverflow.com/questions/1059559/split-strings-into-words-with-multiple-word-boundary-delimiters)

I think what I want to do is a fairly common task but I've found no reference on the web. I have text with punctuation, and I want a list of the words. should beBut Python's str.split() only works with one argument, so I have all words with the punctuation after I split with whitespace. Any ideas?

2009-06-29 17:49:35Z

I think what I want to do is a fairly common task but I've found no reference on the web. I have text with punctuation, and I want a list of the words. should beBut Python's str.split() only works with one argument, so I have all words with the punctuation after I split with whitespace. Any ideas?A case where regular expressions are justified:re.split()Another quick way to do this without a regexp is to replace the characters first, as below:So many answers, yet I can't find any solution that does efficiently what the title of the questions literally asks for (splitting on multiple possible separators—instead, many answers remove anything that is not a word, which is different). So here is an answer to the question in the title, that relies on Python's standard and efficient re module:where:This re.split() precisely "splits with multiple separators", as asked for in the question title.This solution is furthermore immune to the problems with non-ASCII characters in words found in some other solutions (see the first comment to ghostdog74's answer).The re module is much more efficient (in speed and concision) than doing Python loops and tests "by hand"!Another way, without regexPro-Tip: Use string.translate for the fastest string operations Python has.Some proof...First, the slow way (sorry pprzemek):Next, we use re.findall() (as given by the suggested answer). MUCH faster:Finally, we use translate:Explanation:string.translate is implemented in C and unlike many string manipulation functions in Python, string.translate does not produce a new string. So it's about as fast as you can get for string substitution.It's a bit awkward, though, as it needs a translation table in order to do this magic. You can make a translation table with the maketrans() convenience function. The objective here is to translate all unwanted characters to spaces. A one-for-one substitute. Again, no new data is produced. So this is fast!Next, we use good old split(). split() by default will operate on all whitespace characters, grouping them together for the split. The result will be the list of words that you want. And this approach is almost 4x faster than re.findall()!I had a similar dilemma and didn't want to use 're' module.First, I want to agree with others that the regex or str.translate(...) based solutions are most performant.  For my use case the performance of this function wasn't significant, so I wanted to add ideas that I considered with that criteria.My main goal was to generalize ideas from some of the other answers into one solution that could work for strings containing more than just regex words (i.e., blacklisting the explicit subset of punctuation characters vs whitelisting word characters).Note that, in any approach, one might also consider using string.punctuation in place of a manually defined list.I was surprised to see no answer so far uses re.sub(...).  I find it a simple and natural approach to this problem.In this solution, I nested the call to re.sub(...) inside re.split(...) — but if performance is critical, compiling the regex outside could be beneficial — for my use case, the difference wasn't significant, so I prefer simplicity and readability.This is a few more lines, but it has the benefit of being expandable without having to check whether you need to escape a certain character in regex.It would have been nice to be able to map the str.replace to the string instead, but I don't think it can be done with immutable strings, and while mapping against a list of characters would work, running every replacement against every character sounds excessive. (Edit: See next option for a functional example.)(In Python 2, reduce is available in global namespace without importing it from functools.)Then this becomes a three-liner:ExplanationThis is what in Haskell is known as the List monad. The idea behind the monad is that once "in the monad" you "stay in the monad" until something takes you out. For example in Haskell, say you map the python range(n) -> [1,2,...,n] function over a List. If the result is a List, it will be append to the List in-place, so you'd get something like map(range, [3,4,1]) -> [0,1,2,0,1,2,3,0]. This is known as map-append (or mappend, or maybe something like that). The idea here is that you've got this operation you're applying (splitting on a token), and whenever you do that, you join the result into the list.You can abstract this into a function and have tokens=string.punctuation by default. Advantages of this approach:try this:this will print ['Hey', 'you', 'what', 'are', 'you', 'doing', 'here']Use replace two times:results in: I like re, but here is my solution without it:sep.__contains__ is a method used by 'in' operator. Basically it is the same asbut is more convenient here.groupby gets our string and function. It splits string in groups using that function:  whenever a value of function changes - a new group is generated. So, sep.__contains__ is exactly what we need.groupby returns a sequence of pairs, where pair[0] is a result of our function and pair[1] is a group. Using 'if not k' we filter out groups with separators (because a result of sep.__contains__ is True on separators). Well, that's all - now we have a sequence of groups where each one is a word (group is actually an iterable so we use join to convert it to string).This solution is quite general, because it uses a function to separate string (you can split by any condition you need). Also, it doesn't create intermediate strings/lists (you can remove join and the expression will become lazy, since each group is an iterator)Instead of using a re module function re.split you can achieve the same result using the series.str.split method of pandas. First, create a series with the above string and then apply the method to the series.thestring = pd.Series("Hey, you - what are you doing here!?")

thestring.str.split(pat = ',|-')parameter pat takes the delimiters and returns the split string as an array. Here the two delimiters are passed using a | (or operator).

The output is as follows:[Hey,  you ,  what are you doing here!?]

 I'm re-acquainting myself with Python and needed the same thing.

The findall solution may be better, but I came up with this:using maketrans and translate you can do it easily and neatlyIn Python 3, your can use the method from PY4E - Python for Everybody.your_string.translate(your_string.maketrans(fromstr, tostr, deletestr))Your can see the "punctuation":For your example:For more information, you can refer:Another way to achieve this is to use the Natural Language Tool Kit (nltk).This prints: ['Hey', 'you', 'what', 'are', 'you', 'doing', 'here']The biggest drawback of this method is that you need to install the nltk package.The benefits are that you can do a lot of fun stuff with the rest of the nltk package once you get your tokens.First of all, I don't think that your intention is to actually use punctuation as delimiters in the split functions.  Your description suggests that you simply want to eliminate punctuation from the resultant strings.I come across this pretty frequently, and my usual solution doesn't require re.(requires import string):As a traditional function, this is still only two lines with a list comprehension (in addition to import string):It will also naturally leave contractions and hyphenated words intact. You can always use text.replace("-", " ") to turn hyphens into spaces before the split.For a more general solution (where you can specify the characters to eliminate), and without a list comprehension, you get:Of course, you can always generalize the lambda function to any specified string of characters as well.First of all, always use re.compile() before performing any RegEx operation in a loop because it works faster than normal operation.so for your problem first compile the pattern and then perform action on it.Here is the answer with some explanation.or in one line, we can do like this:updated answerCreate a function that takes as input two strings (the source string to be split and the splitlist string of delimiters) and outputs a list of split words:I like pprzemek's solution because it does not assume that the delimiters are single characters and it doesn't try to leverage a regex (which would not work well if the number of separators got to be crazy long).Here's a more readable version of the above solution for clarity:got same problem as @ooboo and find this topic

@ghostdog74 inspired me, maybe someone finds my solution usefullinput something in space place and split using same character if you dont want to split at spaces.Here is my go at a split with multiple deliminaters:I think the following is the best answer to suite your needs :\W+ maybe suitable for this case, but may not be suitable for other cases.Heres my take on it....I like the replace() way the best. The following procedure changes all separators defined in a string splitlist to the first separator in splitlist and then splits the text on that one separator. It also accounts for if splitlist happens to be an empty string. It returns a list of words, with no empty strings in it.Here is the usage:If you want a reversible operation (preserve the delimiters), you can use this function:I recently needed to do this but wanted a function that somewhat matched the standard library str.split function, this function behaves the same as standard library when called with 0 or 1 arguments. NOTE: This function is only useful when your separators consist of a single character (as was my usecase).

How do I determine the size of an object in Python?

user46646

[How do I determine the size of an object in Python?](https://stackoverflow.com/questions/449560/how-do-i-determine-the-size-of-an-object-in-python)

I want to know how to get size of objects like a string, integer, etc. in Python.Related question: How many bytes per element are there in a Python list (tuple)?I am using an XML file which contains size fields that specify the size of value. I must parse this XML and do my coding. When I want to change the value of a particular field, I will check the size field of that value. Here I want to compare whether the new value that I'm gong to enter is of the same size as in XML. I need to check the size of new value. In case of a string I can say its the length. But in case of int, float, etc. I am confused.

2009-01-16 05:07:12Z

I want to know how to get size of objects like a string, integer, etc. in Python.Related question: How many bytes per element are there in a Python list (tuple)?I am using an XML file which contains size fields that specify the size of value. I must parse this XML and do my coding. When I want to change the value of a particular field, I will check the size field of that value. Here I want to compare whether the new value that I'm gong to enter is of the same size as in XML. I need to check the size of new value. In case of a string I can say its the length. But in case of int, float, etc. I am confused.Just use the sys.getsizeof function defined in the sys module.Usage example, in python 3.0:If you are in python < 2.6 and don't have sys.getsizeof you can use this extensive module instead. Never used it though.The answer, "Just use sys.getsizeof" is not a complete answer. That answer does work for builtin objects directly, but it does not account for what those objects may contain, specifically, what types, such as custom objects, tuples, lists, dicts, and sets contain. They can contain instances each other, as well as numbers, strings and other objects.Using 64 bit Python 3.6 from the Anaconda distribution, with sys.getsizeof, I have determined the minimum size of the following objects, and note that sets and dicts preallocate space so empty ones don't grow again until after a set amount (which may vary by implementation of the language):Python 3:How do you interpret this? Well say you have a set with 10 items in it. If each item is 100 bytes each, how big is the whole data structure? The set is 736 itself because it has sized up one time to 736 bytes. Then you add the size of the items, so that's 1736 bytes in totalSome caveats for function and class definitions:Note each class definition has a proxy __dict__ (48 bytes) structure for class attrs. Each slot has a descriptor (like a property) in the class definition.Slotted instances start out with 48 bytes on their first element, and increase by 8 each additional. Only empty slotted objects have 16 bytes, and an instance with no data makes very little sense.Also, each function definition has code objects, maybe docstrings, and other possible attributes, even a __dict__.Also note that we use sys.getsizeof() because we care about the marginal space usage, which includes the garbage collection overhead for the object, from the docs:Also note that resizing lists (e.g. repetitively appending to them) causes them to preallocate space, similarly to sets and dicts. From the listobj.c source code:Python 2.7 analysis, confirmed with guppy.hpy and sys.getsizeof:Note that dictionaries (but not sets) got a more

compact representation in Python 3.6I think 8 bytes per additional item to reference makes a lot of sense on a 64 bit machine. Those 8 bytes point to the place in memory the contained item is at. The 4 bytes are fixed width for unicode in Python 2, if I recall correctly, but in Python 3, str becomes a unicode of width equal to the max width of the characters.(And for more on slots, see this answer )We want a function that searches the elements in lists, tuples, sets, dicts, obj.__dict__'s, and obj.__slots__, as well as other things we may not have yet thought of.We want to rely on gc.get_referents to do this search because it works at the C level (making it very fast). The downside is that get_referents can return redundant members, so we need to ensure we don't double count.Classes, modules, and functions are singletons - they exist one time in memory. We're not so interested in their size, as there's not much we can do about them - they're a part of the program. So we'll avoid counting them if they happen to be referenced.We're going to use a blacklist of types so we don't include the entire program in our size count.To contrast this with the following whitelisted function, most objects know how to traverse themselves for the purposes of garbage collection (which is approximately what we're looking for when we want to know how expensive in memory certain objects are. This functionality is used by gc.get_referents.) However, this measure is going to be much more expansive in scope than we intended if we are not careful. For example, functions know quite a lot about the modules they are created in.Another point of contrast is that strings that are keys in dictionaries are usually interned so they are not duplicated. Checking for id(key) will also allow us to avoid counting duplicates, which we do in the next section. The blacklist solution skips counting keys that are strings altogether.To cover most of these types myself, instead of relying on the gc module, I wrote this recursive function to try to estimate the size of most Python objects, including most builtins, types in the collections module, and custom types (slotted and otherwise).This sort of function gives much more fine-grained control over the types we're going to count for memory usage, but has the danger of leaving types out:And I tested it rather casually (I should unittest it):This implementation breaks down on class definitions and function definitions because we don't go after all of their attributes, but since they should only exist once in memory for the process, their size really doesn't matter too much.The Pympler package's asizeof module can do this.Use as follows:Unlike sys.getsizeof, it works for your self-created objects. It even works with numpy.As mentioned,And if you need other view on live data, Pympler's For numpy arrays, getsizeof doesn't work - for me it always returns 40 for some reason:Then (in ipython):Happily, though:This can be more complicated than it looks depending on how you want to count things.  For instance, if you have a list of ints, do you want the size of the list containing the references to the ints? (ie. list only, not what is contained in it), or do you want to include the actual data pointed to, in which case you need to deal with duplicate references, and how to prevent double-counting when two objects contain references to the same object. You may want to take a look at one of the python memory profilers, such as pysizer to see if they meet your needs.Python 3.8 (Q1 2019) will change some of the results of sys.getsizeof, as announced here by Raymond Hettinger:This comes after issue 33597 and Inada Naoki (methane)'s work around Compact PyGC_Head, and PR 7043See commit d5c875b:Having run into this problem many times myself, I wrote up a small function (inspired by @aaron-hall's answer) & tests that does what I would have expected sys.getsizeof to do:https://github.com/bosswissam/pysizeIf you're interested in the backstory, here it isEDIT: Attaching the code below for easy reference. To see the most up-to-date code, please check the github link.Here is a quick script I  wrote based on the previous answers to list sizes of all variablesYou can serialize the object to derive a measure that is closely related to the size of the object:If you want to measure objects that cannot be pickled (e.g. because of lambda expressions) cloudpickle can be a solution.If you don't need the exact size of the object but roughly to know how big it is, one quick (and dirty) way is to let the program run, sleep for an extended period of time, and check the memory usage (ex: Mac's activity monitor) by this particular python process. This would be effective when you are trying to find the size of one single large object in a python process. For example, I recently wanted to check the memory usage of a new data structure and compare it with that of Python's set data structure. First I wrote the elements (words from a large public domain book) to a set, then checked the size of the process, and then did the same thing with the other data structure. I found out the Python process with a set is taking twice as much memory as the new data structure. Again, you wouldn't be able to exactly say the memory used by the process is equal to the size of the object. As the size of the object gets large, this becomes close as the memory consumed by the rest of the process becomes negligible compared to the size of the object you are trying to monitor.Use sys.getsizeof() if you DON'T want to include sizes of linked (nested) objects. However, if you want to count sub-objects nested in lists, dicts, sets, tuples - and usually THIS is what you're looking for - use the recursive deep sizeof() function as shown below:You can also find this function in the nifty toolbox, together with many other useful one-liners:https://github.com/mwojnars/nifty/blob/master/util.pyYou can make use of getSizeof() as mentioned below to determine the size of an object

Relative imports for the billionth time

BrenBarn

[Relative imports for the billionth time](https://stackoverflow.com/questions/14132789/relative-imports-for-the-billionth-time)

I've been here:and plenty of URLs that I did not copy, some on SO, some on other sites, back when I thought I'd have the solution quickly.The forever-recurring question is this: With Windows 7, 32-bit Python 2.7.3, how do I solve this "Attempted relative import in non-package" message?  I built an exact replica of the package on pep-0328:The imports were done from the console.I did make functions named spam and eggs in their appropriate modules.  Naturally, it didn't work.  The answer is apparently in the 4th URL I listed, but it's all alumni to me. There was this response on one of the URLs I visited:The above response looks promising, but it's all hieroglyphs to me.  So my question, how do I make Python not return to me "Attempted relative import in non-package"? has an answer that involves -m, supposedly.Can somebody please tell me why Python gives that error message, what it means by "non-package", why and how do you define a 'package', and the precise answer put in terms easy enough for a kindergartener to understand. 

2013-01-03 03:50:40Z

I've been here:and plenty of URLs that I did not copy, some on SO, some on other sites, back when I thought I'd have the solution quickly.The forever-recurring question is this: With Windows 7, 32-bit Python 2.7.3, how do I solve this "Attempted relative import in non-package" message?  I built an exact replica of the package on pep-0328:The imports were done from the console.I did make functions named spam and eggs in their appropriate modules.  Naturally, it didn't work.  The answer is apparently in the 4th URL I listed, but it's all alumni to me. There was this response on one of the URLs I visited:The above response looks promising, but it's all hieroglyphs to me.  So my question, how do I make Python not return to me "Attempted relative import in non-package"? has an answer that involves -m, supposedly.Can somebody please tell me why Python gives that error message, what it means by "non-package", why and how do you define a 'package', and the precise answer put in terms easy enough for a kindergartener to understand. Script vs. ModuleHere's an explanation.  The short version is that there is a big difference between directly running a Python file, and importing that file from somewhere else.  Just knowing what directory a file is in does not determine what package Python thinks it is in.  That depends, additionally, on how you load the file into Python (by running or by importing).There are two ways to load a Python file: as the top-level script, or as a 

module.  A file is loaded as the top-level script if you execute it directly, for instance by typing python myfile.py on the command line.  It is loaded as a module if you do python -m myfile, or if it is loaded when an import statement is encountered inside some other file.  There can only be one top-level script at a time; the top-level script is the Python file you ran to start things off.NamingWhen a file is loaded, it is given a name (which is stored in its __name__ attribute).  If it was loaded as the top-level script, its name is __main__.  If it was loaded as a module, its name is the filename, preceded by the names of any packages/subpackages of which it is a part, separated by dots.So for instance in your example:if you imported moduleX (note: imported, not directly executed), its name would be package.subpackage1.moduleX.  If you imported moduleA, its name would be package.moduleA.  However, if you directly run moduleX from the command line, its name will instead be __main__, and if you directly run moduleA from the command line, its name will be __main__.  When a module is run as the top-level script, it loses its normal name and its name is instead __main__.Accessing a module NOT through its containing packageThere is an additional wrinkle: the module's name depends on whether it was imported "directly" from the directory it is in, or imported via a package.  This only makes a difference if you run Python in a directory, and try to import a file in that same directory (or a subdirectory of it).  For instance, if you start the Python interpreter in the directory package/subpackage1 and then do import moduleX, the name of moduleX will just be moduleX, and not package.subpackage1.moduleX.  This is because Python adds the current directory to its search path on startup; if it finds the to-be-imported module in the current directory, it will not know that that directory is part of a package, and the package information will not become part of the module's name.A special case is if you run the interpreter interactively (e.g., just type python and start entering Python code on the fly).  In this case the name of that interactive session is __main__.Now here is the crucial thing for your error message: if a module's name has no dots, it is not considered to be part of a package.  It doesn't matter where the file actually is on disk.  All that matters is what its name is, and its name depends on how you loaded it.Now look at the quote you included in your question:Relative imports...Relative imports use the module's name to determine where it is in a package.  When you use a relative import like from .. import foo, the dots indicate to step up some number of levels in the package hierarchy.  For instance, if your current module's name is package.subpackage1.moduleX, then ..moduleA would mean package.moduleA.  For a from .. import to work, the module's name must have at least as many dots as there are in the import statement.... are only relative in a packageHowever, if your module's name is __main__, it is not considered to be in a package.  Its name has no dots, and therefore you cannot use from .. import statements inside it.  If you try to do so, you will get the "relative-import in non-package" error.Scripts can't import relativeWhat you probably did is you tried to run moduleX or the like from the command line.  When you did this, its name was set to __main__, which means that relative imports within it will fail, because its name does not reveal that it is in a package. Note that this will also happen if you run Python from the same directory where a module is, and then try to import that module, because, as described above, Python will find the module in the current directory "too early" without realizing it is part of a package.Also remember that when you run the interactive interpreter, the "name" of that interactive session is always __main__.  Thus you cannot do relative imports directly from an interactive session.  Relative imports are only for use within module files.Two solutions:NotesThis is really a problem within python. The origin of confusion is that people mistakenly takes the relative import as path relative which is not. For example when you write in faa.py:This has a meaning only if faa.py was identified and loaded by python, during execution, as a part of a package. In that case,the module's name 

 for faa.py would be for example some_packagename.faa. If the file was loaded just because it is in the current directory, when python is run, then its name would not refer to any package and eventually relative import would fail. A simple solution to refer modules in the current directory, is to use this:Here's a general recipe, modified to fit as an example, that I am using right now for dealing with Python libraries written as packages, that contain interdependent files, where I want to be able to test parts of them piecemeal.  Let's call this lib.foo and say that it needs access to lib.fileA for functions f1 and f2, and lib.fileB for class Class3.I have included a few print calls to help illustrate how this works.  In practice you would want to remove them (and maybe also the from __future__ import print_function line).This particular example is too simple to show when we really need to insert an entry into sys.path.  (See Lars' answer for a case where we do need it, when we have two or more levels of package directories, and then we use os.path.dirname(os.path.dirname(__file__))—but it doesn't really hurt here either.)  It's also safe enough to do this without the if _i in sys.path test.  However, if each imported file inserts the same path—for instance, if both fileA and fileB want to import utilities from the package—this clutters up sys.path with the same path many times, so it's nice to have the if _i not in sys.path in the boilerplate.The idea here is this (and note that these all function the same across python2.7 and python 3.x):There is still a quirk.  If you run this whole thing from outside:or:the behavior depends on the contents of lib/__init__.py.  If that exists and is empty, all is well:But if lib/__init__.py itself imports routine so that it can export routine.name directly as lib.name, you get:That is, the module gets imported twice, once via the package and then again as __main__ so that it runs your main code.  Python 3.6 and later warn about this:The warning is new, but the warned-about behavior is not.  It is part of what some call the double import trap.  (For additional details see issue 27487.)  Nick Coghlan says:Note that while we violate that rule here, we do it only when the file being loaded is not being loaded as part of a package, and our modification is specifically designed to allow us to access other files in that package.  (And, as I noted, we probably shouldn't do this at all for single level packages.)  If we wanted to be extra-clean, we might rewrite this as, e.g.:That is, we modify sys.path long enough to achieve our imports, then put it back the way it was (deleting one copy of _i if and only if we added one copy of _i).So after carping about this along with many others, I came across a note posted by Dorian B in this article that solved the specific problem I was having where I would develop modules and classes for use with a web service, but I also want to be able to test them as I'm coding, using the debugger facilities in PyCharm. To run tests in a self-contained class, I would include the following at the end of my class file:but if I wanted to import other classes or modules in the same folder, I would then have to change all my import statements from relative notation to local references (i.e. remove the dot (.))  But after reading Dorian's suggestion, I tried his 'one-liner' and it worked!  I can now test in PyCharm and leave my test code in place when I use the class in another class under test, or when I use it in my web service!The if statement checks to see if we're running this module as main or if it's being used in another module that's being tested as main.  Perhaps this is obvious, but I offer this note here in case anyone else frustrated by the relative import issues above can make use of it.Here is one solution that I would not recommend, but might be useful in some situations where modules were simply not generated:I had a similar problem where I didn't want to change the Python module search 

path and needed to load a module relatively from a script (in spite of "scripts can't import relative with all" as BrenBarn explained nicely above).So I used the following hack. Unfortunately, it relies on the imp module that 

became deprecated since version 3.4 to be dropped in favour of importlib.

(Is this possible with importlib, too? I don't know.) Still, the hack works for now.Example for accessing members of moduleX in subpackage1 from a script residing in the subpackage2 folder:A cleaner approach seems to be to modify the sys.path used for loading modules as mentioned by Federico.__name__ changes depending on whether the code in question is run in the global namespace or as part of an imported module.If the code is not running in the global space, __name__ will be the name of the module. If it is running in global namespace -- for example, if you type it into a console, or run the module as a script using python.exe yourscriptnamehere.py then __name__ becomes "__main__".  You'll see a lot of python code with  if __name__ == '__main__' is used to test whether the code is being run from the global namespace – that allows you to have a module that doubles as a script. Did you try to do these imports from the console? Wrote a little python package to PyPi that might help viewers of this question. The package acts as workaround if one wishes to be able to run python files containing imports containing upper level packages from within a package / project without being directly in the importing file's directory. https://pypi.org/project/import-anywhere/ @BrenBarn's answer says it all, but if you're like me it might take a while to understand.  Here's my case and how @BrenBarn's answer applies to it, perhaps it will help you.The caseUsing our familiar example, and add to it that moduleX.py has a relative import to ..moduleA.  Given that I tried writing a test script in the subpackage1 directory that imported moduleX, but then got the dreaded error described by the OP.SolutionMove test script to the same level as package and import package.subpackage1.moduleXExplanationAs explained, relative imports are made relative to the current name.  When my test script imports moduleX from the same directory, then module name inside moduleX is moduleX.  When it encounters a relative import the interpreter can't back up the package hierarchy because it's already at the topWhen I import moduleX from above, then name inside moduleX is package.subpackage1.moduleX and the relative import can be foundTo make Python not return to me "Attempted relative import in non-package".

package/init.py

    subpackage1/

        init.py

        moduleX.py

        moduleY.py

    subpackage2/

        init.py

        moduleZ.py

    moduleA.pyThis error occurs only if you are applying relative import to the parent file. For example parent file already returns main after you code  "print(name)" in moduleA.py .so THIS file is already main it cannot return any parent package further on.

relative imports are required in files of packages subpackage1 and subpackage2

you can use ".." to refer to the parent directory or module .But parent is if already top level package it cannot go further above that parent directory(package). Such files where you are applying relative importing to parents can only work with the application of absolute import.

If you will use ABSOLUTE IMPORT  IN PARENT PACKAGE NO ERROR will come as python knows who is at the top level of package even if your file is in subpackages  because of the concept of PYTHON PATH which defines the top level of the project 

Adding a Method to an Existing Object Instance

akdom

[Adding a Method to an Existing Object Instance](https://stackoverflow.com/questions/972/adding-a-method-to-an-existing-object-instance)

I've read that it is possible to add a method to an existing object (i.e., not in the class definition) in Python. I understand that it's not always good to do so. But how might one do this?

2008-08-04 02:17:51Z

I've read that it is possible to add a method to an existing object (i.e., not in the class definition) in Python. I understand that it's not always good to do so. But how might one do this?In Python, there is a difference between functions and bound methods.Bound methods have been "bound" (how descriptive) to an instance, and that instance will be passed as the first argument whenever the method is called.Callables that are attributes of a class (as opposed to an instance) are still unbound, though, so you can modify the class definition whenever you want:Previously defined instances are updated as well (as long as they haven't overridden the attribute themselves):The problem comes when you want to attach a method to a single instance:The function is not automatically bound when it's attached directly to an instance:To bind it, we can use the MethodType function in the types module:This time other instances of the class have not been affected:More information can be found by reading about descriptors and metaclass programming.Module new is deprecated since python 2.6 and removed in 3.0, use typessee http://docs.python.org/library/new.htmlIn the example below I've deliberately removed return value from patch_me() function.

I think that giving return value may make one believe that patch returns a new object, which is not true - it modifies the incoming one. Probably this can facilitate a more disciplined use of monkeypatching.Preface - a note on compatibility: other answers may only work in Python 2 - this answer should work perfectly well in Python 2 and 3. If writing Python 3 only, you might leave out explicitly inheriting from object, but otherwise the code should remain the same.I don't recommend this. This is a bad idea. Don't do it. Here's a couple of reasons:Thus, I suggest that you not do this unless you have a really good reason. It is far better to define the correct method in the class definition or less preferably to monkey-patch the class directly, like this:Since it's instructive, however, I'm going to show you some ways of doing this.Here's some setup code. We need a class definition. It could be imported, but it really doesn't matter.Create an instance:Create a method to add to it:Dotted lookups on functions call the __get__ method of the function with the instance, binding the object to the method and thus creating a "bound method."and now:First, import types, from which we'll get the method constructor:Now we add the method to the instance. To do this, we require the MethodType constructor from the types module (which we imported above).The argument signature for types.MethodType is (function, instance, class):and usage: First, we create a wrapper function that binds the method to the instance:usage:A partial function applies the first argument(s) to a function (and optionally keyword arguments), and can later be called with the remaining arguments (and overriding keyword arguments). Thus:This makes sense when you consider that bound methods are partial functions of the instance.If we try to add the sample_method in the same way as we might add it to the class, it is unbound from the instance, and doesn't take the implicit self as the first argument.We can make the unbound function work by explicitly passing the instance (or anything, since this method doesn't actually use the self argument variable), but it would not be consistent with the expected signature of other instances (if we're monkey-patching this instance):You now know several ways you could do this, but in all seriousness - don't do this.I think that the above answers missed the key point. Let's have a class with a method:Now, let's play with it in ipython:Ok, so m() somehow becomes an unbound method of A. But is it really like that?It turns out that m() is just a function, reference to which is added to A class dictionary - there's no magic. Then why A.m gives us an unbound method? It's because the dot is not translated to a simple dictionary lookup. It's de facto a call of A.__class__.__getattribute__(A, 'm'):Now, I'm not sure out of the top of my head why the last line is printed twice, but still it's clear what's going on there.Now, what the default __getattribute__ does is that it checks if the attribute is a so-called descriptor or not, i.e. if it implements a special __get__ method. If it implements that method, then what is returned is the result of calling that __get__ method. Going back to the first version of our A class, this is what we have:And because Python functions implement the descriptor protocol, if they are called on behalf of an object, they bind themselves to that object in their __get__ method.Ok, so how to add a method to an existing object? Assuming you don't mind patching class, it's as simple as:Then B.m "becomes" an unbound method, thanks to the descriptor magic.And if you want to add a method just to a single object, then you have to emulate the machinery yourself, by using types.MethodType:By the way:In Python monkey patching generally works by overwriting a class or functions signature with your own. Below is an example from the Zope Wiki:That code will overwrite/create a method called speak on the class. In Jeff Atwood's recent post on monkey patching. He shows an example in C# 3.0 which is the current language I use for work.You can use lambda to bind a method to an instance:Output:There are at least two ways for attach a method to an instance without types.MethodType:1:2:Useful links:

Data model - invoking descriptors

Descriptor HowTo Guide - invoking descriptorsWhat you're looking for is setattr I believe.

Use this to set an attribute on an object.Since this question asked for non-Python versions, here's JavaScript:Consolidating Jason Pratt's and the community wiki answers, with a look at the results of different methods of binding:Especially note how adding the binding function as a class method works, but the referencing scope is incorrect.Personally, I prefer the external ADDMETHOD function route, as it allows me to dynamically assign new method names within an iterator as well.Although Jasons answer works, it does only work if one wants to add a function to a class. 

It did not work for me when I tried to reload an already existing method from the .py source code file.It took me for ages to find a workaround, but the trick seems simple...

1.st import the code from the source code file

2.nd force a reload

