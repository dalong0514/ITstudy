brew install python@2

Then on my mac, it looks as below:I had the same issue. I have both Python 2.7 & 3.6 installed. Python 2.7 had virtualenv working, but after installing Python3, virtualenv kept looking for version 2.7 and couldn't find it. 

Doing pip install virtualenv installed the Python3 version of virtualenv.Then, for each command, if I want to use Python2, I would use virtualenv --python=python2.7 somecommandI had the same issue, virtualenv was pointing to an old python path. Fixing the path resolved the issue:I got same problem. If I run brew link --overwrite python2. There was still zsh: /usr/local/bin//fab: bad interpreter: /usr/local/opt/python/bin/python2.7: no such file or directory.Solved it! Now we can use python2 version fabric.=== 2018/07/25 updatedThere is convinient way to use python2 version fab when your os python linked to python3. .sh for your command.Hope this helps.In my case, I decided to remove the homebrew python installation from my mac as I already had two other versions of python installed on my mac through MacPorts. This caused the error message. Reinstalling python through brew solved my issue.this command worked out for meEditing the first line of this file worked to me:MBP-de-Jose:~ josejunior$ which python3MBP-de-Jose:~ josejunior$beforeafterAll you need to do is... close the terminal window and reopen new one to fix this issue.The issue is, new python path is not added to bashrc(Either source or new terminal window would help).

Pairwise circular Python 'for' loop

The Oddler

[Pairwise circular Python 'for' loop](https://stackoverflow.com/questions/36917042/pairwise-circular-python-for-loop)

Is there a nice Pythonic way to loop over a list, retuning a pair of elements? The last element should be paired with the first.So for instance, if I have the list [1, 2, 3], I would like to get the following pairs:

2016-04-28 14:02:56Z

Is there a nice Pythonic way to loop over a list, retuning a pair of elements? The last element should be paired with the first.So for instance, if I have the list [1, 2, 3], I would like to get the following pairs:A Pythonic way to access a list pairwise is: zip(L, L[1:]). To connect the last item to the first one:I would use a deque with zip to achieve this.I'd use a slight modification to the pairwise recipe from the itertools documentation:This will simply keep a reference to the first value and when the second iterator is exhausted, zip_longest will fill the last place with the first value.(Also note that it works with iterators like generators as well as iterables like lists/tuples.)Note that @Barry's solution is very similar to this but a bit easier to understand in my opinion and easier to extend beyond one element.I would pair itertools.cycle with zip:cycle returns an iterable that yields the values of its argument in order, looping from the last value to the first.We skip the first value, so it starts at position 1 (rather than 0).Next, we zip it with the original, unmutated list.  zip is good, because it stops when any of its argument iterables are exhausted.  Doing it this way avoids the creation of any intermediate lists: cycle holds a reference to the original, but doesn't copy it.  zip operates in the same way.It's important to note that this will break if the input is an iterator, such as a file, (or a map or zip in python-3), as advancing in one place (through next(second)) will automatically advance the iterator in all the others.  This is easily solved using itertools.tee, which produces two independently operating iterators over the original iterable:tee can use large amounts of additional storage, for example, if one of the returned iterators is used up before the other is touched, but as we only ever have one step difference, the additional storage is minimal.There are more efficient ways (that don't built temporary lists), but I think this is the most concise:I would use a list comprehension, and take advantage of the fact that l[-1] is the last element.You don't need a temporary list that way.If you like the accepted answer, you can go much more memory light with semantically the same code using itertools:And this barely materializes anything in memory beyond the original list (assuming the list is relatively large):To use, just consume (for example, with a list):This can be made extensible to any width:and usage:You can also use tee to avoid making a redundant cycle object:and now:This is incredibly efficient, an expected usage of iter with next, and elegant usage of cycle, tee, and zip.Don't pass cycle directly to list unless you have saved your work and have time for your computer to creep to a halt as you max out its memory - if you're lucky, after a while your OS will kill the process before it crashes your computer. Finally, no standard lib imports, but this only works for up to the length of original list (IndexError otherwise.)You can continue this with modulo:Amazing how many different ways there are to solve this problem.Here's one more. You can use the pairwise recipe but instead of zipping with b, chain it with the first element that you already popped off. Don't need to cycle when we just need a single extra value:I like a solution that does not modify the original list and does not copy the list to temporary storage:Output:I can imagine this being used on some very large in-memory data.This one will work even if the list l has consumed most of the system's memory. (If something guarantees this case to be impossible, then zip as posted by chepner is fine)or more generalizably (works for any offset n i.e. l[ (i+n)%len(l) ] )provided you are on a system with decently fast modulo division (i.e. not some pea-brained embedded system). There seems to be a often-held belief that indexing a list with an integer subscript is un-pythonic and best avoided. Why? This is my solution, and it looks Pythonic enough to me:prints:The generator function version:How about this?(As above with @j-f-sebastian's "zip" answer, but using itertools.)NB: EDITED given helpful nudge from @200_success. previously was:Just another tryIf you don't want to consume too much memory, you can try my solution:[(l[i], l[(i+1) % len(l)]) for i, v in enumerate(l)]It's a little slower, but consume less memory.L = [1, 2, 3]

a = zip(L, L[1:]+L[:1])

for i in a:

    b = list(i)

    print bthis seems like combinations would do the job.this would yield a generator. this can then be iterated over as suchthe results would look something like 

Proxy with urllib2

Chris Stryker

[Proxy with urllib2](https://stackoverflow.com/questions/1450132/proxy-with-urllib2)

I open urls with:site = urllib2.urlopen('http://google.com')And what I want to do is connect the same way with a proxy

I got somewhere telling me:site = urllib2.urlopen('http://google.com', proxies={'http':'127.0.0.1'})but that didn't work either.I know urllib2 has something like a proxy handler, but I can't recall that function.

2009-09-20 02:30:24Z

I open urls with:site = urllib2.urlopen('http://google.com')And what I want to do is connect the same way with a proxy

I got somewhere telling me:site = urllib2.urlopen('http://google.com', proxies={'http':'127.0.0.1'})but that didn't work either.I know urllib2 has something like a proxy handler, but I can't recall that function.You have to install a ProxyHandlerYou can set proxies using environment variables.urllib2 will add proxy handlers automatically this way. You need to set proxies for different protocols separately otherwise they will fail (in terms of not going through proxy), see below.For example:Instead To use the default system proxies (e.g. from the http_support environment variable), the following works for the current request (without installing it into urllib2 globally):In Addition to the accepted answer:

My scipt gave me an error Solution was to add http:// in front of the proxy string:One can also use requests if we would like to access a web page using proxies. Python 3 code: More than one proxies can also be added.In addition set the proxy for the command line session

Open a command line where you might want to run your scriptrun your script in that terminal.

how to split column of tuples in pandas dataframe?

Donbeo

[how to split column of tuples in pandas dataframe?](https://stackoverflow.com/questions/29550414/how-to-split-column-of-tuples-in-pandas-dataframe)

I have a pandas dataframe (this is only a little piece)I want to split all the columns that contain tuples. For example I want to replace the column LCV with the columns LCV-a and LCV-b . How can I do that?

2015-04-09 22:50:38Z

I have a pandas dataframe (this is only a little piece)I want to split all the columns that contain tuples. For example I want to replace the column LCV with the columns LCV-a and LCV-b . How can I do that?You can do this by doing pd.DataFrame(col.tolist()) on that column:Note: in an earlier version, this answer recommended to use df['b'].apply(pd.Series) instead of pd.DataFrame(df['b'].tolist(), index=df.index). That works as well (because it makes of each tuple a Series, which is then seen as a row of a dataframe), but is slower / uses more memory than the tolist version, as noted by the other answers here (thanks to @denfromufa).

I updated this answer to make sure the most visible answer has the best solution.On much larger datasets, I found that .apply() is few orders slower than pd.DataFrame(df['b'].values.tolist(), index=df.index)This performance issue was closed in GitHub, although I do not agree with this decision:https://github.com/pandas-dev/pandas/issues/11615EDIT: based on this answer: https://stackoverflow.com/a/44196843/2230844The str accessor that is available to pandas.Series objects of dtype == object is actually an iterable.Assume a pandas.DataFrame df:We can test if it is an iterableWe can then assign from it like we do other iterables:So in one line we can assign both columnsOnly slightly more complicate, we can use zip to create a similar iterableMeaning, don't mutate existing df

This works because assign takes keyword arguments where the keywords are the new(or existing) column names and the values will be the values of the new column.  You can use a dictionary and unpack it with ** and have it act as the keyword arguments.  So this is a clever way of assigning a new column named 'g' that is the first item in the df.col.str iterable and 'h' that is the second item in the df.col.str iterable.With modern list comprehension and variable unpacking.

Note: also inline using joinThe mutating version would beUse one defined above10^3 times biggerI think a simpler way is: I know this is from a while ago, but a caveat of the second solution:is that it will explicitly discard the index, and add in a default sequential index, whereas the accepted answerwill not, since the result of apply will retain the row index. While the order is initially retained from the original array, pandas will try to match the indicies from the two dataframes. This can be very important if you are trying to set the rows into an numerically indexed array, and pandas will automatically try to match the index of the new array to the old, and cause some distortion in the ordering. A better hybrid solution would be to set the index of the original dataframe onto the new, i.e.Which will retain the speed of using the second method while ensuring the order and indexing is retained on the result. 

Pycharm: run only part of my Python file

FrankTheTank

[Pycharm: run only part of my Python file](https://stackoverflow.com/questions/23441657/pycharm-run-only-part-of-my-python-file)

Is it possible to run only a part of a program in PyCharm?In other editors there is something like a cell which I can run, but I can't find such an option in PyCharm?If this function doesn't exist it would be a huge drawback for me... Because for my data analysis I very often only need to run the last few lines of my code.

2014-05-03 07:44:38Z

Is it possible to run only a part of a program in PyCharm?In other editors there is something like a cell which I can run, but I can't find such an option in PyCharm?If this function doesn't exist it would be a huge drawback for me... Because for my data analysis I very often only need to run the last few lines of my code.I found out an easier way. This is the same shortcut to the same action in Spyder and R-Studio.Pycharm shortcut for running "Selection" in the console is ALT + SHIFT + e For this to work properly, you'll have to run everything this way.You can select a code snippet and use right click menu to choose the action "Execute Selection in console".You can set a breakpoint, and then just open the debug console. So, the first thing you need to turn on your debug console:After you've enabled, set a break-point to where you want it to:After you're done setting the break-point:Once that has been completed:A cell is delimited by ##Ref

https://plugins.jetbrains.com/plugin/7858-pycharm-cell-mode

Getting TypeError: __init__() missing 1 required positional argument: 'on_delete' when trying to add parent table after child table with entries

Christian Lisangola

[Getting TypeError: __init__() missing 1 required positional argument: 'on_delete' when trying to add parent table after child table with entries](https://stackoverflow.com/questions/44026548/getting-typeerror-init-missing-1-required-positional-argument-on-delete)

I have two classes in my sqlite database, a parent table named Categorie and the child table called Article. I created first the child table class and addes entries. So first I had this:And after I have added parent table, and now my models.py looks like this:So when I run python manage.py makemigrations <my_app_name>, I get this error:I've seen some similar issues in stackoverflow, but it seems to not be the same problem: __init__() missing 1 required positional argument: 'quantity'

2017-05-17 13:39:11Z

I have two classes in my sqlite database, a parent table named Categorie and the child table called Article. I created first the child table class and addes entries. So first I had this:And after I have added parent table, and now my models.py looks like this:So when I run python manage.py makemigrations <my_app_name>, I get this error:I've seen some similar issues in stackoverflow, but it seems to not be the same problem: __init__() missing 1 required positional argument: 'quantity'You can change the property categorie of the class Article like this:and the error should disappear.Eventually you might need another option for on_delete, check the documentation for more details:https://docs.djangoproject.com/en/1.11/ref/models/fields/#django.db.models.ForeignKeyEDIT:As you stated in your comment, that you don't have any special requirements for on_delete, you could use the option DO_NOTHING:Since Django 2.x, on_delete is required. Django DocumentationFrom Django 2.0 on_delete is required:  It will delete the child table data if the User is deleted. For more details check the Django documentation.Since Django 2.0 the ForeignKey field requires two positional arguments:Here are some methods can used in on_deleteCascade deletes. Django emulates the behavior of the SQL constraint ON DELETE CASCADE and also deletes the object containing the ForeignKeyPrevent deletion of the referenced object by raising ProtectedError, a subclass of django.db.IntegrityError.Take no action. If your database backend enforces referential integrity, this will cause an IntegrityError unless you manually add an SQL ON DELETE constraint to the database field.you can find more about on_delete by reading the documentation.If you are using foreignkey then you have to use "on_delete=models.CASCADE" as it will eliminate the complexity developed after deleting the original element from the parent table. As simple as that.Here are available options if it helps anyone for on_deleteIf you don't know which option to enter the params.

Just want to keep the default value like on_delete=None before migration:This is a code snippet in the old version:This worked for me 

pip install django-csvimport --upgrade

How to get the value of a variable given its name in a string? [duplicate]

dsavi

[How to get the value of a variable given its name in a string? [duplicate]](https://stackoverflow.com/questions/9437726/how-to-get-the-value-of-a-variable-given-its-name-in-a-string)

For simplicity this is a stripped down version of what I want to do:I know how to do this in PHP:Any way to do this?

2012-02-24 20:41:22Z

For simplicity this is a stripped down version of what I want to do:I know how to do this in PHP:Any way to do this?If it's a global variable, then you can do:A note about the various "eval" solutions: you should be careful with eval, especially if the string you're evaluating comes from a potentially untrusted source -- otherwise, you might end up deleting the entire contents of your disk or something like that if you're given a malicious string.(If it's not global, then you'll need access to whatever namespace it's defined in.  If you don't have that, there's no way you'll be able to access it.)Edward Loper's answer only works if the variable is in the current module. To get a value in another module, you can use getattr:https://docs.python.org/2/library/functions.html#getattrAssuming that you know the string is safe to evaluate, then eval will give the value of the variable in the current context.tada!

Quantile-Quantile Plot using SciPy

John

[Quantile-Quantile Plot using SciPy](https://stackoverflow.com/questions/13865596/quantile-quantile-plot-using-scipy)

How would you create a qq-plot using Python?Assuming that you have a large set of measurements and are using some plotting function that takes XY-values as input. The function should plot the quantiles of the measurements against the corresponding quantiles of some distribution (normal, uniform...).The resulting plot lets us then evaluate in our measurement follows the assumed distribution or not.http://en.wikipedia.org/wiki/Quantile-quantile_plotBoth R and Matlab provide ready made functions for this, but I am wondering what the cleanest method for implementing in in Python would be.

2012-12-13 17:54:07Z

How would you create a qq-plot using Python?Assuming that you have a large set of measurements and are using some plotting function that takes XY-values as input. The function should plot the quantiles of the measurements against the corresponding quantiles of some distribution (normal, uniform...).The resulting plot lets us then evaluate in our measurement follows the assumed distribution or not.http://en.wikipedia.org/wiki/Quantile-quantile_plotBoth R and Matlab provide ready made functions for this, but I am wondering what the cleanest method for implementing in in Python would be.I think that scipy.stats.probplot will do what you want.  See the documentation for more detail.ResultUsing qqplot of statsmodels.api is another option:Very basic example:Result:Documentation and more example are hereIf you need to do a QQ plot of one sample vs. another, statsmodels includes qqplot_2samples(). Like Ricky Robinson in a comment above, this is what I think of as a QQ plot vs a probability plot which is a sample against a theoretical distribution.http://statsmodels.sourceforge.net/devel/generated/statsmodels.graphics.gofplots.qqplot_2samples.htmlI came up with this. Maybe you can improve it. Especially the method of generating the quantiles of the distribution seems cumbersome to me.You could replace np.random.normal with any other distribution from np.random to compare data against other distributions.It exists now in the statsmodels package:http://statsmodels.sourceforge.net/devel/generated/statsmodels.graphics.gofplots.qqplot.htmlYou can use bokeh Here probplot draw the graph measurements vs normal distribution which speofied in dist="norm"

How to capture botocore's NoSuchKey exception?

theist

[How to capture botocore's NoSuchKey exception?](https://stackoverflow.com/questions/42975609/how-to-capture-botocores-nosuchkey-exception)

I'm trying to write "good" python and capture a S3 no such key error with this:But NoSuchKey isn't defined and I can't trace it to the import I need to have it defined.e.__class__ is botocore.errorfactory.NoSuchKey but from botocore.errorfactory import NoSuchKey gives an error and from botocore.errorfactory import * doesn't work either and I don't want to capture a generic error. 

2017-03-23 12:09:00Z

I'm trying to write "good" python and capture a S3 no such key error with this:But NoSuchKey isn't defined and I can't trace it to the import I need to have it defined.e.__class__ is botocore.errorfactory.NoSuchKey but from botocore.errorfactory import NoSuchKey gives an error and from botocore.errorfactory import * doesn't work either and I don't want to capture a generic error. Using botocore 1.5, it looks like the client handle exposes the exception classes:In boto3, I was able to access the exception in resource's meta client.I think the most elegant way to do this is in Boto3 isThe documentation on error handling seems sparse, but the following prints the error codes this works for:

What does hash do in python?

Roman

[What does hash do in python?](https://stackoverflow.com/questions/17585730/what-does-hash-do-in-python)

I saw an example of code that where hash function is applied to tuple. As a result it returns a negative integer. I wonder what does this function does. Google does not help. I found a page that explains how hash is calculated but it does not explain why we need this function.

2013-07-11 05:32:50Z

I saw an example of code that where hash function is applied to tuple. As a result it returns a negative integer. I wonder what does this function does. Google does not help. I found a page that explains how hash is calculated but it does not explain why we need this function.A hash is an fixed sized integer that identifies a particular value. Each value needs to have its own hash, so for the same value you will get the same hash even if it's not the same object.Hash values need to be created in such a way that the resulting values are evenly distributed to reduce the number of hash collisions you get. Hash collisions are when two different values have the same hash. Therefore, relatively small changes often result in very different hashes.These numbers are very useful, as they enable quick look-up of values in a large collection of values. Two examples of their use are Python's set and dict. In a list, if you want to check if a value is in the list, with if x in values:, Python needs to go through the whole list and compare x with each value in the list values. This can take a long time for a long list. In a set, Python keeps track of each hash, and when you type if x in values:, Python will get the hash-value for x, look that up in an internal structure and then only compare x with the values that have the same hash as x.The same methodology is used for dictionary lookup. This makes lookup in set and dict very fast, while lookup in list is slow. It also means you can have non-hashable objects in a list, but not in a set or as keys in a dict. The typical example of non-hashable objects is any object that is mutable, meaning that you can change its value. If you have a mutable object it should not be hashable, as its hash then will change over its life-time, which would cause a lot of confusion, as an object could end up under the wrong hash value in a dictionary.Note that the hash of a value only needs to be the same for one run of Python. In Python 3.3 they will in fact change for every new run of Python:This is to make is harder to guess what hash value a certain string will have, which is an important security feature for web applications etc.Hash values should therefore not be stored permanently. If you need to use hash values in a permanent way you can take a look at the more "serious" types of hashes, cryptographic hash functions, that can be used for making verifiable checksums of files etc.Please refer to the glossary: hash() is used as a shortcut to comparing objects, an object is deemed hashable if it can be compared to other objects. that is why we use hash(). It's also used to access dict and set elements which are implemented as resizable hash tables in CPython.If you read about how dictionaries are implemented, they use hash tables, which means deriving a key from an object is a corner stone for retrieving objects in dictionaries in O(1). That's however very dependent on your hash function to be collision-resistant. The worst case for getting an item in a dictionary is actually O(n).On that note, mutable objects are usually not hashable. The hashable property means you can use an object as a key. If the hash value is used as a key and the contents of that same object change, then what should the hash function return? Is it the same key or a different one? It depends on how you define your hash function.Imagine we have this class:Please note: this is all based on the assumption that the SSN never changes for an individual (don't even know where to actually verify that fact from authoritative source).And we have Bob:Bob goes to see a judge to change his name:This is what we know:But these are two different objects with different memory allocated, just like two different records of the same person:Now comes the part where hash() is handy:Guess what:From two different records you are able to access the same information.

Now try this:What just happened? That's a collision. Because hash(jim) == hash(hash(jim)) which are both integers btw, we need to compare the input of __getitem__ with all items that collide. The builtin int does not have an ssn attribute so it trips.In this last example, I show that even with a collision, the comparison is performed, the objects are no longer equal, which means it successfully raises a KeyError.The Python docs for hash() state:Python dictionaries are implemented as hash tables. So any time you use a dictionary, hash() is called on the keys that you pass in for assignment, or look-up.Additionally, the docs for the dict type state:The hash is used by dictionaries and sets to quickly look up the object. A good starting point is Wikipedia's article on hash tables.I also looking for it from a very long time, Now I got my answer so shearing with you all... Please use the Dictionary data type in python, its very very simile to the hash... Its also supported nested, simile to the to nested hash.Example:-Hopes its solves the problem.. 

What is the purpose of __str__ and __repr__?

Daniel

[What is the purpose of __str__ and __repr__?](https://stackoverflow.com/questions/3691101/what-is-the-purpose-of-str-and-repr)

I really don't understand where are __str__ and __repr__ used in Python. I mean, I get that __str__ returns the string representation of an object. But why would I need that? In what use case scenario? Also, I read about the usage of __repr__But what I don't understand is, where would I use them?

2010-09-11 13:10:01Z

I really don't understand where are __str__ and __repr__ used in Python. I mean, I get that __str__ returns the string representation of an object. But why would I need that? In what use case scenario? Also, I read about the usage of __repr__But what I don't understand is, where would I use them?__repr__ __str__Use __str__ if you have a class, and you'll want an informative/informal output, whenever you use this object as part of string. E.g. you can define __str__ methods for Django models, which then gets rendered in the Django administration interface. Instead of something like <Model object> you'll get like first and last name of a person, the name and date of an event, etc.__repr__ and __str__ are similar, in fact sometimes equal (Example from BaseSet class in sets.py from the standard library):The one place where you use them both a lot is in an interactive session. If you print an object then its __str__ method will get called, whereas if you just use an object by itself then its __repr__ is shown:The __str__ is intended to be as human-readable as possible, whereas the __repr__ should aim to be something that could be used to recreate the object, although it often won't be exactly how it was created, as in this case.It's also not unusual for both __str__ and __repr__ to return the same value (certainly for built-in types).Grasshopper, when in doubt go to the mountain and read the Ancient Texts. In them you will find that __repr__() should: Building up and on the previous answers and showing some more examples. If used properly, the difference between str and repr is clear. In short repr should return a string that can be copy-pasted to rebuilt the exact state of the object, whereas str is useful for logging and observing debugging results. Here are some examples to see the different outputs for some known libraries.The str is good to print into a log file, where as repr can be re-purposed if you want to run it directly or dump it as commands into a file. in Numpy the repr is again directly consumable. In this example, repr returns again a string that can be directly consumed/executed, whereas str is more useful as a debug output. Lets have a class without __str__ function.When we print this instance of the class, emp1, this is what we get:This is not very helpful, and certainly this is not what we want printed if we are using it to display (like in html)So now, the same class, but with __str__ function:Now instead of printing that there is an object, we get what we specified with return of __str__ function:The employee John Williams earns 90000str will be informal and readable format whereas repr will give official object representation.

The modulo operation on negative numbers in Python

facha

[The modulo operation on negative numbers in Python](https://stackoverflow.com/questions/3883004/the-modulo-operation-on-negative-numbers-in-python)

I've found some strange behaviour in Python regarding negative numbers:Could anyone explain what's going on?

2010-10-07 15:00:39Z

I've found some strange behaviour in Python regarding negative numbers:Could anyone explain what's going on?Unlike C or C++, Python's modulo operator (%) always return a number having the same sign as the denominator (divisor). Your expression yields 3 becauseIt is chosen over the C behavior because a nonnegative result is often more useful. An example is to compute week days. If today is Tuesday (day #2), what is the week day N days before? In Python we can compute withbut in C, if N ≥ 3, we get a negative number which is an invalid number, and we need to manually fix it up by adding 7:(See http://en.wikipedia.org/wiki/Modulo_operator for how the sign of result is determined for different languages.)Here's an explanation from Guido van Rossum:http://python-history.blogspot.com/2010/08/why-pythons-integer-division-floors.htmlEssentially, it's so that a/b = q with remainder r preserves the relationships b*q + r = a and 0 <= r < b.There is no one best way to handle integer division and mods with negative numbers.  It would be nice if a/b was the same magnitude and opposite sign of (-a)/b.  It would be nice if a % b was indeed a modulo b.  Since we really want a == (a/b)*b + a%b, the first two are incompatible.Which one to keep is a difficult question, and there are arguments for both sides.  C and C++ round integer division towards zero (so a/b == -((-a)/b)), and apparently Python doesn't.As pointed out, Python modulo makes a well-reasoned exception to the conventions of other languages.  This gives negative numbers a seamless behavior, especially when used in combination with the // integer-divide operator, as % modulo often is (as in math.divmod):Produces:Modulo, equivalence classes for 4:Here's a link to modulo's behavior with negative numbers.  (Yes, I googled it)I also thought it was a strange behavior of Python. It turns out that I was not solving the division well (on paper); I was giving a value of 0 to the quotient and a value of -5 to the remainder. Terrible... I forgot the geometric representation of integers numbers. By recalling the geometry of integers given by the number line, one can get the correct values for the quotient and the remainder, and check that Python's behavior is fine. (Although I assume that you have already resolved your concern a long time ago).It's also worth to mention that also the division in python is different from C:

Considerin C you expect the resultwhat is x/y in python?and % is modulo - not the remainder! While x%y in C yieldspython yields.You can get both as in CThe division:And the remainder (using the division from above):This calculation is maybe not the fastest but it's working for any sign combinations of x and y to achieve the same results as in C plus it avoids conditional statements.In python, modulo operator works like this. so the result is (for your case).whereas other languages such as C, JAVA, JavaScript are used trunc instead of floor.If you need more information about rounding in python, read this

Python pickle error: UnicodeDecodeError

90abyss

[Python pickle error: UnicodeDecodeError](https://stackoverflow.com/questions/32957708/python-pickle-error-unicodedecodeerror)

I'm trying to do some text classification using Textblob. I'm first training the model and serializing it using pickle as shown below. And when I try to run this file:I get this error:Following are the content of my sample.csv:Where am I going wrong here? Please help.

2015-10-05 20:53:53Z

I'm trying to do some text classification using Textblob. I'm first training the model and serializing it using pickle as shown below. And when I try to run this file:I get this error:Following are the content of my sample.csv:Where am I going wrong here? Please help.By choosing to open the file in mode wb, you are choosing to write in raw binary.  There is no character encoding being applied.Thus to read this file, you should simply open in mode rb.I think you should open the file as You shouldn't have to decode it.  pickle.load will give you an exact copy of whatever it is you saved.  At this point you, should be able to work with cl as if you just created it.  

How do I clear all variables in the middle of a Python script?

snakile

[How do I clear all variables in the middle of a Python script?](https://stackoverflow.com/questions/3543833/how-do-i-clear-all-variables-in-the-middle-of-a-python-script)

I am looking for something similar to 'clear' in Matlab: A command/function which removes all variables from the workspace, releasing them from system memory. Is there such a thing in Python?EDIT: I want to write a script which at some point clears all the variables.

2010-08-22 23:05:06Z

I am looking for something similar to 'clear' in Matlab: A command/function which removes all variables from the workspace, releasing them from system memory. Is there such a thing in Python?EDIT: I want to write a script which at some point clears all the variables.The following sequence of commands does remove every name from the current module:I doubt you actually DO want to do this, because "every name" includes all built-ins, so there's not much you can do after such a total wipe-out.  Remember, in Python there is really no such thing as a "variable" -- there are objects, of many kinds (including modules, functions, class, numbers, strings, ...), and there are names, bound to objects; what the sequence does is remove every name from a module (the corresponding objects go away if and only if every reference to them has just been removed).Maybe you want to be more selective, but it's hard to guess exactly what you mean unless you want to be more specific.  But, just to give an example:This sequence leaves alone names that are private or magical, including the __builtins__ special name which houses all built-in names.  So, built-ins still work -- for example:As you see, name n (the control variable in that for) also happens to stick around (as it's re-bound in the for clause every time through), so it might be better to name that control variable _, for example, to clearly show "it's special" (plus, in the interactive interpreter, name _ is re-bound anyway after every complete expression entered at the prompt, to the value of that expression, so it won't stick around for long;-).Anyway, once you have determined exactly what it is you want to do, it's not hard to define a function for the purpose and put it in your start-up file (if you want it only in interactive sessions) or site-customize file (if you want it in every script).No, you are best off restarting the interpreterIPython is an excellent replacement for the bundled interpreter and has the %reset command which usually worksIf you write a function then once you leave it all names inside disappear.The concept is called namespace and it's so good, it made it into the Zen of Python:The namespace of IPython can likewise be reset with the magic command %reset -f. (The -f means "force"; in other words, "don't ask me if I really want to delete all the variables, just do it.")This is a modified version of Alex's answer.

We can save the state of a module's namespace and restore it by using the following 2 methods...You can also add a line "clear = restoreContext" before calling saveContext() and clear() will work like matlab's clear.In Spyder one can configure the IPython console for each Python file to clear all variables before each execution in the Menu Run -> Configuration -> General settings -> Remove all variables before execution.The globals() function returns a dictionary, where keys are names of objects you can name (and values, by the way, are ids of these objects)

The exec() function takes a string and executes it as if you just type it in a python console

So, the code isHere else None is added to follow python syntax. This code doesn't look beutiful, but it's short. The better way to write it isIn the idle IDE there is Shell/Restart Shell.  Cntrl-F6 will do it.Isn't the easiest way to create a class contining all the needed variables? 

Then you have one object with all curretn variables, and if you need you can overwrite this variable?

How to initialize weights in PyTorch?

Fábio Perez

[How to initialize weights in PyTorch?](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)

How to initialize the weights and biases (for example, with He or Xavier initialization) in a network in PyTorch? 

2018-03-22 16:34:42Z

How to initialize the weights and biases (for example, with He or Xavier initialization) in a network in PyTorch? To initialize the weights of a single layer, use a function from torch.nn.init. For instance:Alternatively, you can modify the parameters by writing to conv1.weight.data (which is a torch.Tensor). Example:The same applies for biases: Pass an initialization function to torch.nn.Module.apply. It will initialize the weights in the entire nn.Module recursively.Example:If you follow the principle of Occam's razor, you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.A uniform distribution has the equal probability of picking any number from a set of numbers. Let's see how well the neural network trains using a uniform weight initialization, where low=0.0 and high=1.0.Below, we'll see another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, we can:The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. below we compare performance of NN, weights initialized with uniform distribution [-0.5,0.5) versus the one whose weight is initialized using general rulebelow we show the performance of two NN one initialized using uniform-distribution and the other using normal-distribution PyTorch will do it for you. If you think about, this has lot of sense. Why should we initialize layers, when PyTorch can do that following the latest trends.Check for instance the Linear layer.In the __init__ method it will call Kaiming He init function.The similar is for other layers types. For conv2d for instance check here.To note : The gain of proper initialization is the faster training speed.

If your problem deserves special initialization you can do it afterwords.Sorry for being so late, I hope my answer will help.To initialise weights with a normal distribution use:Or to use a constant distribution write:Or to use an uniform distribution:You can check other methods to initialise tensors hereIf you cannot use apply for instance if the model does not implement Sequential directly:You can try with torch.nn.init.constant_(x, len(x.shape)) to check that they are appropriately initialized:If you see a deprecation warning (@Fábio Perez)...If you want some extra flexibility, you can also set the weights manually. Say you have input of all ones:And you want to make a dense layer with no bias (so we can visualize):Set all the weights to 0.5 (or anything else):The weights:All your weights are now 0.5. Pass the data through:Remember that each neuron receives 8 inputs, all of which have weight 0.5 and value of 1 (and no bias), so it sums up to 4 for each. Cuz I haven't had the enough reputation so far, I can't add a comment under But I wanna point out that actually we know some assumptions in the paper of Kaiming He, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, are not appropriate, though it looks like the deliberately designed initialization method makes a hit in practice.E.g., within the subsection of Backward Propagation Case, they assume that $w_l$ and $\delta y_l$ are independent of each other. But as we all known, take the score map $\delta y^L_i$ as an instance, it often is $y_i-softmax(y^L_i)=y_i-softmax(w^L_ix^L_i)$ if we use a typical cross entropy loss function objective.So I think the true underlying reason why He's Initialization works well remains to unravel. Cuz everyone has witnessed its power on boosting deep learning training.

Is there an easy way to make sessions timeout in flask?

verrochio

[Is there an easy way to make sessions timeout in flask?](https://stackoverflow.com/questions/11783025/is-there-an-easy-way-to-make-sessions-timeout-in-flask)

I'm building a website with flask where users have accounts and are able to login.

I'm using flask-principal for the loging in part and the role management.

Is there a way of making the user's session expire after say 5 minutes or 10 minutes?

I was not able to find that in flask documentation or, flask-principal's documentation.I thought of a way of doing it by hand, set a variable server-side with a time tag at the moment of login and at the next action the user takes, the server verifies the time-delta on that timestamp and deletes the session. 

2012-08-02 17:49:19Z

I'm building a website with flask where users have accounts and are able to login.

I'm using flask-principal for the loging in part and the role management.

Is there a way of making the user's session expire after say 5 minutes or 10 minutes?

I was not able to find that in flask documentation or, flask-principal's documentation.I thought of a way of doing it by hand, set a variable server-side with a time tag at the moment of login and at the next action the user takes, the server verifies the time-delta on that timestamp and deletes the session. flask sessions expire once you close the browser unless you have a permanent session. You can possibly try the following:By default in Flask, permanent_session_lifetime is set to 31 days. Yes, We should setBut I don't think it should be set at app.before_request, This will lead to set them too may times.The permanent_session_lifetime is a Basics Configuration, so it should be set at you configure the app:The session will created for each client, seperated from other clients. So, I think the best place to set session.permanent is when you login():

Default arguments with *args and **kwargs

nneonneo

[Default arguments with *args and **kwargs](https://stackoverflow.com/questions/15301999/default-arguments-with-args-and-kwargs)

In Python 2.x (I use 2.7), which is the proper way to use default arguments with *args and **kwargs?

I've found a question on SO related to this topic, but that is for Python 3:

Calling a Python function with *args,**kwargs and optional / default argumentsThere, they say this method works:In 2.7, it results in a SyntaxError. Is there any recommended way to define such a function?

I got it working this way, but I'd guess there is a nicer solution.

2013-03-08 19:43:12Z

In Python 2.x (I use 2.7), which is the proper way to use default arguments with *args and **kwargs?

I've found a question on SO related to this topic, but that is for Python 3:

Calling a Python function with *args,**kwargs and optional / default argumentsThere, they say this method works:In 2.7, it results in a SyntaxError. Is there any recommended way to define such a function?

I got it working this way, but I'd guess there is a nicer solution.Just put the default arguments before the *args:Now, b will be explicitly set if you pass it as a keyword argument or the second positional argument. Examples:Note that, in particular, foo(x, y, b=z) doesn't work because b is assigned by position in that case.This code works in Python 3 too. Putting the default arg after *args in Python 3 makes it a "keyword-only" argument that can only be specified by name, not by position. If you want a keyword-only argument in Python 2, you can use @mgilson's solution.The syntax in the other question is python3.x only and specifies keyword only arguments.  It doesn't work on python2.x.  For python2.x, I would pop it out of kwargs:You could also use a decorator like this:Then just do:For instance:Sticking quite close to your solution approach while trying to make it more generic and more compact I would suggest to consider something like this:Another way to handle with Python 2.x:This handles passing arbitrary *args to the underlying call unlike @nneonneo's answer.

Understanding inplace=True

Aran Freel

[Understanding inplace=True](https://stackoverflow.com/questions/43893457/understanding-inplace-true)

In the pandas library many times there is an option to change the object inplace such as with the following statement...I am curious what is being returned as well as how the object is handled when inplace=True is passed vs. when inplace=False.Are all operations modifying self when inplace=True? And when inplace=False is a new object created immediately such as new_df = self and then new_df is returned?

2017-05-10 13:08:21Z

In the pandas library many times there is an option to change the object inplace such as with the following statement...I am curious what is being returned as well as how the object is handled when inplace=True is passed vs. when inplace=False.Are all operations modifying self when inplace=True? And when inplace=False is a new object created immediately such as new_df = self and then new_df is returned?When inplace=True is passed, the data is renamed in place (it returns nothing), so you'd use:When inplace=False is passed (this is the default value, so isn't necessary), performs the operation and returns a copy of the object, so you'd use:The way I use it isOrCONCLUSION:TLDR;I don't advise setting this parameter as it serves little purpose. See this GitHub issue which proposes the inplace argument be deprecated api-wide. It is a common misconception that using inplace=True will lead to more efficient or optimized code. In reality, there are absolutely no performance benefits to using inplace=True. Both the in-place and out-of-place versions create a copy of the data anyway, with the in-place version automatically assigning the copy back. inplace=True also hinders method chaining. Contrast the working of As opposed to One final caveat to keep in mind is that calling inplace=True can trigger the SettingWithCopyWarning:Which can cause unexpected behavior. Use with caution!Save it to the same variabledata["column01"].where(data["column01"]< 5, inplace=True)Save it to a separate variabledata["column02"] = data["column01"].where(data["column1"]< 5)But, you can always overwrite the variabledata["column01"] = data["column01"].where(data["column1"]< 5)FYI: In default inplace = False The inplace parameter:in Pandas and in general means: 1. Pandas creates a copy of the original data2. ... does some computation on it3. ... assigns the results to the original data.4. ... deletes the copy.As you can read in the rest of my answer's further below, we still can have good reason to use this parameter i.e. the inplace operations, but we should avoid it if we can, as it generate more issues, as:1. Your code will be harder to debug (Actually SettingwithCopyWarning stands for warning you to this possible problem)2. Conflict with method chainingDefinitely yes. If we use pandas or any tool for handeling huge dataset, we can easily face the situation, where some big data can consume our entire memory.

To avoid this unwanted effect we can use some technics like method chaining:which make our code more compact (though harder to interpret and debug too) and consumes less memory as the chained methods works with the other method's returned values, thus resulting in only one copy of the input data. We can see clearly, that we will have 2 x original data memory consumption after this operations.Or we can use inplace parameter (though harder to interpret and debug too) our memory consumption will be 2 x original data, but our memory consumption after this operation remains 1 x original data, which if somebody whenever worked with huge datasets exactly knows can be a big benefit.Avoid using inplace parameter unless you don't work with huge data and be aware of its possible issues in case of still using of it.inplace=True makes the function impure. It changes the original dataframe and returns None. In that case, You breaks the DSL chain. 

Because most of dataframe functions return a new dataframe, you can use the DSL conveniently. Like Function call with inplace=True returns None and DSL chain is broken. For examplewill throw NoneType object has no attribute 'rename'Something similar with python’s build-in sort and sorted. lst.sort() returns None and sorted(lst) returns a new list. Generally, do not use inplace=True unless you have specific reason of doing so. When you have to write reassignment code like df = df.sort_values(), try attaching the function call in the DSL chain, e.g. If you don't use inplace=True or you use inplace=False you basically get back a copy.So for instance:will alter the structure with the data sorted in descending order.then:will make testdf2 a copy. the values will all be the same but the sort will be reversed and you will have an independent object.then given another column, say LongMA and you do:the LongMA column in testdf will have the original values and testdf2 will have the decrimented values.It is important to keep track of the difference as the chain of calculations grows and the copies of dataframes have their own lifecycle.inplace=True is used depending if you want to make changes to the original df or not.will only make a view of dropped values but not make any changes to df will drop values and make changes to df.Hope this helps.:)As Far my experience in pandas I would like to answer.The 'inplace=True' argument stands for the data frame has to make changes permanent

eg. changes the same dataframe (as this pandas find NaN entries in index and drops them).

If we try pandas shows the dataframe with changes we make but will not modify the original dataframe 'df'.

Extract a page from a pdf as a jpeg

vishvAs vAsuki

[Extract a page from a pdf as a jpeg](https://stackoverflow.com/questions/46184239/extract-a-page-from-a-pdf-as-a-jpeg)

In python code, how to efficiently save a certain page in a pdf as a jpeg file? (Use case: I've a python flask web server where pdf-s will be uploaded and jpeg-s corresponding to each page is stores.)This solution is close, but the problem is that it does not convert the entire page to jpeg.

2017-09-12 19:44:53Z

In python code, how to efficiently save a certain page in a pdf as a jpeg file? (Use case: I've a python flask web server where pdf-s will be uploaded and jpeg-s corresponding to each page is stores.)This solution is close, but the problem is that it does not convert the entire page to jpeg.The pdf2image library can be used.You can install it simply using, Once installed you can use following code to get images.Saving pages in jpeg formatEdit: the Github repo pdf2image also mentions that it uses pdftoppm and that it requires other installations:You can install the latest version under Windows using anaconda by doing:note: Windows versions upto 0.67 are available at http://blog.alivate.com.au/poppler-windows/ but note that 0.68 was released in Aug 2018 so you'll not be getting the latest features or bug fixes.The Python library pdf2image (used in the other answer) in fact doesn't do much more than just launching pdttoppm with subprocess.Popen, so here is a short version doing it directly:Here is the Windows installation link for pdftoppm (contained in a package named poppler): http://blog.alivate.com.au/poppler-windows/I found this simple solution, PyMuPDF, output to png fileThere is no need to install Poppler on your OS. This will work:pip install Wand@gaurwraith, install poppler for Windows and use pdftoppm.exe as follows:@vishvAs vAsuki, this code should generate the jpgs you want through the subprocess module for all pages of one or more pdfs in a given folder:Or using the pdf2image module:Their is a utility called pdftojpg which can be used to convert the pdf to imgYou can found the code here https://github.com/pankajr141/pdf2jpgGhostScript performs much faster than Poppler for a Linux based system. Following is the code for pdf to image conversion.GhostScript can be installed on macOS using brew install ghostscriptInstallation information for other platforms can be found here. If it is not already installed on your system.I use a (maybe) much simpler option of pdf2image:This is a small part of a bash script in a loop for the use of a narrow casting device.

Checks every 5 seconds on added pdf files (all) and processes them.

This is for a demo device, at the end converting will be done at a remote server. Converting to .PNG now, but .JPG is possible too.This converting, together with transitions on A4 format, displaying a video, two smooth scrolling texts and a logo (with transition in three versions) sets the Pi3 to allmost 4x 100% cpu-load ;-)

What does it mean for an object to be picklable (or pickle-able)?

Paul

[What does it mean for an object to be picklable (or pickle-able)?](https://stackoverflow.com/questions/3603581/what-does-it-mean-for-an-object-to-be-picklable-or-pickle-able)

Python docs mention this word a lot and I want to know what it means! Googling doesn't help much..

2010-08-30 19:32:50Z

Python docs mention this word a lot and I want to know what it means! Googling doesn't help much..It simply means it can be serialized by the pickle module.  For a basic explanation of this, see What can be pickled and unpickled?.  The pickle protocol provides more details, and shows how classes can customize the process.Things that are usually not pickable are, for example, sockets, file(handler)s, database connections, and so on. Everything that's build up (recursively) from basic python types (dicts, lists, primitives, objects, object references, even circular) can be pickled by default.You can implement custom pickling code that will, for example, store the configuration of a database connection and restore it afterwards, but you will need special, custom logic for this.All of this makes pickling a lot more powerful than xml, json and yaml (but definitely not as readable) 

Why does map return a map object instead of a list in Python 3?

NoIdeaHowToFixThis

[Why does map return a map object instead of a list in Python 3?](https://stackoverflow.com/questions/40015439/why-does-map-return-a-map-object-instead-of-a-list-in-python-3)

I am interested in understanding the new language design of Python 3.x.I do enjoy, in Python 2.7, the function map:However, in Python 3.x things have changed:I understand the how, but I could not find a reference to the why. Why did the language designers make this choice, which, in my opinion, introduces a great deal of pain. Was this to arm-wrestle developers in sticking to list comprehensions?IMO, list can be naturally thought as Functors; and I have been somehow been thought to think in this way:

2016-10-13 08:01:41Z

I am interested in understanding the new language design of Python 3.x.I do enjoy, in Python 2.7, the function map:However, in Python 3.x things have changed:I understand the how, but I could not find a reference to the why. Why did the language designers make this choice, which, in my opinion, introduces a great deal of pain. Was this to arm-wrestle developers in sticking to list comprehensions?IMO, list can be naturally thought as Functors; and I have been somehow been thought to think in this way:I think the reason why map still exists at all when generator expressions also exist, is that it can take multiple iterator arguments that are all looped over and passed into the function:That's slightly easier than using zip:Otherwise, it simply doesn't add anything over generator expressions.Because it returns an iterator, it omit storing the full size list in the memory. So that you can easily iterate over it in the future not making any pain to memory. Possibly you even don't need a full list, but the part of it, until your condition is reached.You can find this docs useful, iterators are awesome.Guido answers this question here: "since creating a list would just be wasteful".  He also says that the correct transformation is to use a regular for loop.Converting map() from 2 to 3 might not just be a simple case of sticking a list( ) around it.  Guido also says:"If the input sequences are not of equal length, map() will stop at the termination of the shortest of the sequences. For full compatibility with map() from Python 2.x, also wrap the sequences in itertools.zip_longest(), e.g.becomes  "In Python 3 many functions (not just map but zip, range and others) return an iterator rather than the full list. You might want an iterator (e.g. to avoid holding the whole list in memory) or you might want a list (e.g. to be able to index).However, I think the key reason for the change in Python 3 is that while it is trivial to convert an iterator to a list using list(some_iterator) the reverse equivalent iter(some_list) does not achieve the desired outcome because the full list has already been built and held in memory.For example, in Python 3 list(range(n)) works just fine as there is little cost to building the range object and then converting it to a list. However, in Python 2 iter(range(n)) does not save any memory because the full list is constructed by range() before the iterator is built.Therefore, in Python 2, separate functions are required to create an iterator rather than a list, such as imap for map (although they're not quite equivalent), xrange for range, izip for zip. By contrast Python 3 just requires a single function as a list() call creates the full list if required.

How do Python properties work?

Fred Foo

[How do Python properties work?](https://stackoverflow.com/questions/6193556/how-do-python-properties-work)

I've been successfully using Python properties, but I don't see how they could work. If I dereference a property outside of a class, I just get an object of type property:But if I put a property in a class, the behavior is very different:I've noticed that unbound Foo.hello is still the property object, so class instantiation must be doing the magic, but what magic is that?

2011-05-31 21:06:35Z

I've been successfully using Python properties, but I don't see how they could work. If I dereference a property outside of a class, I just get an object of type property:But if I put a property in a class, the behavior is very different:I've noticed that unbound Foo.hello is still the property object, so class instantiation must be doing the magic, but what magic is that?As others have noted, they use a language feature called descriptors.The reason that the actual property object is returned when you access it via a class Foo.hello lies in how the property implements the __get__(self, instance, owner) special method:Besides the Descriptors howto, see also the documentation on Implementing Descriptors and Invoking Descriptors in the Language Guide.In order for @properties to work properly the class needs to be a subclass of object. 

when the class is not a subclass of object then the first time you try access the setter it actually makes a new attribute with the shorter name instead of accessing through the setter. The following does not work correctly.The following will work correctlyProperties are descriptors, and descriptors behave specially when member of a class instance.  In short, if a is an instance of type A, and A.foo is a descriptor, then a.foo is equivalent to A.foo.__get__(a).The property object just implements the descriptor protocol: http://docs.python.org/howto/descriptor.html

Why are there no sorted containers in Python's standard libraries?

Neil G

[Why are there no sorted containers in Python's standard libraries?](https://stackoverflow.com/questions/5953205/why-are-there-no-sorted-containers-in-pythons-standard-libraries)

Is there a Python design decision (PEP) that precludes a sorted container from being added to Python?(OrderedDict is not a sorted container since it is ordered by insertion order.)

2011-05-10 16:24:24Z

Is there a Python design decision (PEP) that precludes a sorted container from being added to Python?(OrderedDict is not a sorted container since it is ordered by insertion order.)It's a conscious design decision on Guido's part (he was even somewhat reluctant regarding the addition of the collections module). His goal is to preserve "one obvious way to do it" when it comes to the selection of data types for applications.The basic concept is that if a user is sophisticated enough to realise that the builtin types aren't the right solution for their problem, then they're also up to the task of finding an appropriate third party library.Given that list+sorting, list+heapq and list+bisect cover many of the use cases that would otherwise rely on inherently sorted data structures, and packages like blist exist, there isn't a huge drive to add more complexity in this space to the standard library.In some ways, it is similar to the fact that there's no multi-dimensional array in the standard library, instead ceding that task to the NumPy folks.There's also a python sortedcontainers module that implements sorted list, dict, and set types. It's very similar to blist but implemented in pure-Python and in most cases faster.It also has functionality uncommon to other packages:Disclaimer: I am the author of the sortedcontainers module.There is also the blist module that contains a sortedset data type:Not exactly a "sorted container", but you might be interested in the standard library's bisect module, which "provides support for maintaining a list in sorted order without having to sort the list after each insertion".There is a heapq in the standard library, it is not exactly sorted, but kind of. There is also a blist package, but it is not in the standard library.Python lists are ordered. If you sort them, they stay that way. In Python 2.7 an OrderedDict type was added to maintain an explicitly ordereded dictionary.Python also has sets (a collection in which the members must be unique), but by definition they are unordered. Sorting a set just returns a list.

Python round up integer to next hundred

ofko

[Python round up integer to next hundred](https://stackoverflow.com/questions/8866046/python-round-up-integer-to-next-hundred)

Seems that should have already been asked hundreds (pun are fun =) of times but i can only find function for rounding floats. How do I round up an integer, for example: 130 -> 200 ?

2012-01-14 22:52:42Z

Seems that should have already been asked hundreds (pun are fun =) of times but i can only find function for rounding floats. How do I round up an integer, for example: 130 -> 200 ?Rounding is typically done on floating point numbers, and here there are three basic functions you should know: round (rounds to the nearest integer), math.floor (always rounds down), and math.ceil (always rounds up).You ask about integers and rounding up to hundreds, but we can still use math.ceil as long as your numbers smaller than 253. To use math.ceil, we just divide by 100 first, round up, and multiply with 100 afterwards:Dividing by 100 first and multiply with 100 afterwards "shifts" two decimal places to the right and left so that math.ceil works on the hundreds. You could use 10**n instead of 100 if you want to round to tens (n = 1), thousands (n = 3), etc.An alternative way to do this is to avoid floating point numbers (they have limited precision) and instead use integers only. Integers have arbitrary precision in Python, so this lets you round numbers of any size. The rule for rounding is simple: find the remainder after division with 100, and add 100 minus this remainder if it's non-zero:This works for numbers of any size:I did a mini-benchmark of the two solutions:The pure integer solution is faster by a factor of two compared to the math.ceil solution. Thomas proposed an integer based solution that is identical to the one I have above, except that it uses a trick by multiplying Boolean values. It is interesting to see that there is no speed advantage of writing the code this way:As a final remark, let me also note, that if you had wanted to round 101–149 to 100 and round 150–199 to 200, e.g., round to the nearest hundred, then the built-in round function can do that for you:This is a late answer, but there's a simple solution that combines the best aspects of the existing answers: the next multiple of 100 up from x is x - x % -100 (or if you prefer, x + (-x) % 100).This is fast and simple, gives correct results for any integer x (like John Machin's answer) and also gives reasonable-ish results (modulo the usual caveats about floating-point representation) if x is a float (like Martin Geisler's answer).Try this:Here's a general way of rounding up to the nearest multiple of any positive integer:Sample usage:For a non-negative, b positive, both integers:Update The currently-accepted answer falls apart with integers such that float(x) / float(y) can't be accurately represented as a float. See this code:Output:And here are some timings:If your int is x: x + 100 - x % 100However, as pointed in comments, this will  return 200 if x==100.If this is not the expected behavior, you can use x + 100*(x%100>0) - x%100Try this:Sample usage:Warning: Premature optimizations ahead...Since so many of the answers here do the timing of this I wanted to add another alternative.Taking @Martin Geisler 's (which i like best for several reasons)but factoring out the % actionYields a ~20% speed improvement over the originalIs even better and is ~36% faster then the originalfinally I was thinking that I could drop the not operator and change the order of the branches hoping that this would also increase speed but was baffled to find out that it is actually slower dropping back to be only 23% faster then the original.explanations as to why 3 is faster then 4 would be most welcome.Here is a very simple solution:How does it work?Some examplesA slightly modified approach rounds 1...100 to 100, 101...200 to 200, etc.:

Best way to randomize a list of strings in Python

Roee Adler

[Best way to randomize a list of strings in Python](https://stackoverflow.com/questions/1022141/best-way-to-randomize-a-list-of-strings-in-python)

I receive as input a list of strings and need to return a list with these same strings but in randomized order. I must allow for duplicates - same string may appear once or more in the input and must appear the same number of times in the output.I see several "brute force" ways of doing that (using loops, god forbid), one of which I'm currently using. However, knowing Python there's probably a cool one-liner do get the job done, right?

2009-06-20 18:05:44Z

I receive as input a list of strings and need to return a list with these same strings but in randomized order. I must allow for duplicates - same string may appear once or more in the input and must appear the same number of times in the output.I see several "brute force" ways of doing that (using loops, god forbid), one of which I'm currently using. However, knowing Python there's probably a cool one-liner do get the job done, right?Looks like this is the simplest way, if not the most truly random (this question more fully explains the limitations): http://docs.python.org/library/random.html#random.shuffleGiven a string item, here is a one-liner:  You'll have to read the strings into an array and then use a shuffling algorithm. I recommend Fisher-Yates shuffle

Pandas How to filter a Series

Kiem Nguyen

[Pandas How to filter a Series](https://stackoverflow.com/questions/28272137/pandas-how-to-filter-a-series)

I have a Series like this after doing groupby('name') and used mean() function on other columnCould anyone please show me how to filter out the rows with 1.000000 mean values? Thank you and I greatly appreciate your help.

2015-02-02 06:21:20Z

I have a Series like this after doing groupby('name') and used mean() function on other columnCould anyone please show me how to filter out the rows with 1.000000 mean values? Thank you and I greatly appreciate your help.From pandas version 0.18+ filtering a series can also be done as belowCheckout:

http://pandas.pydata.org/pandas-docs/version/0.18.1/whatsnew.html#method-chaininng-improvementsAs DACW pointed out, there are method-chaining improvements in pandas 0.18.1 that do what you are looking for very nicely.Rather than using .where, you can pass your function to either the .loc indexer or the Series indexer [] and avoid the call to .dropna:Similar behavior is supported on the DataFrame and NDFrame classes. A fast way of doing this is to reconstruct using numpy to slice the underlying arrays.  See timings below.naive timingAnother way is to first convert to a DataFrame and use the query method (assuming you have numexpr installed):If you like a chained operation, you can also use compress function:In my case I had a panda Series where the values are tuples of characters:Therefore I could use indexing to filter the series, but to create the index I needed apply. My condition is "find all tuples which have exactly one 'H'".I admit it is not "chainable", (i.e. notice I repeat series_of_tuples twice; you must store any temporary series into a variable so you can call apply(...) on it). There may also be other methods (besides .apply(...))  which can operate elementwise to produce a Boolean index.Many other answers (including accepted answer) using the chainable functions like:These accept callables (lambdas) which are applied to the Series, not to the individual values in those series!Therefore my Series of tuples behaved strangely when I tried to use my above condition / callable / lambda, with any of the chainable functions, like .loc[]:Produces the error:KeyError: 'Level H must be same as name (None)'I was very confused, but it seems to be using the Series.count series_of_tuples.count(...) function , which is not what I wanted.I admit that an alternative data structure may be better:This creates a series of strings (i.e. by concatenating the tuple; joining the characters in the tuple on a single string)So I can then use the chainable Series.str.count

AWS Lambda import module error in python

Nithin K Anil

[AWS Lambda import module error in python](https://stackoverflow.com/questions/35340921/aws-lambda-import-module-error-in-python)

I am creating a AWS Lambda python deployment package. I am using one external dependency requests . I installed the external dependency using the AWS documentation http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html. Below is my python code. Created the Zip the content of the project-dir directory and uploaded to the lambda(Zip the directory content, not the directory). When I am execute the function I am getting the below mentioned error. Kindly help me to debug the error.

2016-02-11 13:40:31Z

I am creating a AWS Lambda python deployment package. I am using one external dependency requests . I installed the external dependency using the AWS documentation http://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html. Below is my python code. Created the Zip the content of the project-dir directory and uploaded to the lambda(Zip the directory content, not the directory). When I am execute the function I am getting the below mentioned error. Kindly help me to debug the error.Error was due to file name of the lambda function. While creating the lambda function it will ask for Lambda function handler. You have to name it as your Python_File_Name.Method_Name. In this scenario I named it as lambda.lambda_handler (lambda.py is the file name). Please find below the snapshot. 

 If you are uploading a zip file. Make sure that you are zipping the contents of the directory and not the directory itself. Another source of this problem is the permissions on the file that is zipped. It MUST be at least world-wide readable. (min chmod 444)I ran the following on the python file before zipping it and it worked fine.I found Nithin's answer very helpful. Here is a specific walk-through:Look-up these values:With these values, you would need to rename the handler (shown in the screenshot) to "cool.lambda_handler". This is one way to get rid of the "Unable to import module 'lambda_function'" errorMessage. If you were to rename the handler in your python script to "sup" then you'd need to rename the handler in the lambda dashboard to "cool.sup"There are just so many gotchas when creating deployment packages for AWS Lambda (for Python). I have spent hours and hours on debugging sessions until I found a formula that rarely fails.I have created a script that automates the entire process and therefore makes it less error prone. I have also wrote tutorial that explains how everything works. You may want to check it out: Hassle-Free Python Lambda Deployment [Tutorial + Script]I found this hard way after trying all of the solutions above. If you're using sub-directories in the zip file, ensure you include the __init__.py file in each of the sub-directories and that worked for me.I had the error too.

Turn out that my zip file include the code parent folder. When I unzip and inspect the zip file, the lambda_function file is under parent folder ./lambda.Use the zip command, fix the error:Here's a quick step through.Assume you have a folder called deploy, with your lambda file inside call lambda_function.py. Let's assume this file looks something like this. (p1 and p2 represent third-party packages.)For every third-party dependency, you need to pip install <third-party-package> --target . from within the deploy folder. Once you've done this, here's what your structure should look like.Finally, you need to zip all the contents within the deploy folder to a compressed file. On a Mac or Linux, the command would look like zip -r ../deploy.zip * from within the deploy folder. Note that the -r flag is for recursive subfolders.The structure of the file zip file should mirror the original folder.Upload the zip file and specify the <file_name>.<function_name> for Lambda to enter into your process, such as lambda_function.lambda_handler for the example above.I ran into the same issue, this was an exercise as part of a tutorial on lynda.com if I'm not wrong. 

The mistake I made was not selecting the runtime as Python 3.6 which is an option in the lamda function console.@nithin,

AWS released layers concept inside Lambda functions. You can create your layer and there you can upload as much as libraries and then you can connect the layer with the lambda functions.

For more details: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.htmlin lambda_handler the format must be lambda_filename.lambda_functionName

suppose if you want to run lambda_handler function ans if it's in lambda_fuction.py then your handler formate is "lambda_function.lambda_handler" . another reason for getting error is module dependencies .

 your lambda_fuction.py must be in root directory of zip.A perspective from 2019:AWS Lambda now supports Python 3.7, which many people (including myself) choose to use as the runtime for inline lambdas.Then I had to import an external dependency and I followed AWS Docs as the OP referred to. (local install --> zip --> upload).I had the import module error and I realised my local PC had Python 2.7 as the default Python. When I invoked pip, it installed my dependency for Python 2.7.So I switched locally to the Python version that matches the selected runtime version in the lambda console and then re-installed the external dependencies. This solved the problem for me. E.g.:No need to do that mess.use python-lambdahttps://github.com/nficano/python-lambdawith single command pylambda deploy

it will automatically deploy your functionYou need to zip all the requirements, use this scriptuse with:Sharing my solution for the same issue, just in case it helps anyone.Issue:

I got error: "[ERROR] Runtime.ImportModuleError: Unable to import module 'lambda_function': No module named 'StringIO'" while executing aws-big-data-blog  code[1] provided in AWS article[2]. Solution: 

Changed Runtime from Python 3.7 to Python 2.7[1] — https://github.com/bsnively/aws-big-data-blog/blob/master/aws-blog-vpcflowlogs-athena-quicksight/CloudwatchLogsToFirehose/lambdacode.py

[2] — https://aws.amazon.com/blogs/big-data/analyzing-vpc-flow-logs-with-amazon-kinesis-firehose-amazon-athena-and-amazon-quicksight/You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. References:- 

How to assign a value to a TensorFlow variable?

abora

[How to assign a value to a TensorFlow variable?](https://stackoverflow.com/questions/34220532/how-to-assign-a-value-to-a-tensorflow-variable)

I am trying to assign a new value to a tensorflow variable in python.But the output I get isSo the value has not changed. What am I missing?

2015-12-11 09:51:46Z

I am trying to assign a new value to a tensorflow variable in python.But the output I get isSo the value has not changed. What am I missing?In TF1, the statement x.assign(1) does not actually assign the value 1 to x, but rather creates a tf.Operation that you have to explicitly run to update the variable.* A call to Operation.run() or Session.run() can be used to run the operation:(* In fact, it returns a tf.Tensor, corresponding to the updated value of the variable, to make it easier to chain assignments.)However, in TF2 x.assign(1) will now assign the value eagerly:You can also assign a new value to a tf.Variable without adding an operation to the graph: tf.Variable.load(value, session). This function can also save you adding placeholders when assigning a value from outside the graph and it is useful in case the graph is finalized.Update: This is depricated in TF2 as eager execution is default and graphs are no longer exposed in the user-facing API.First of all you can assign values to variables/constants just by feeding values into them the same way you do it with placeholders. So this is perfectly legal to do:Regarding your confusion with the tf.assign() operator. In TF nothing is executed before you run it inside of the session. So you always have to do something like this: op_name = tf.some_function_that_create_op(params) and then inside of the session you run sess.run(op_name). Using assign as an example you will do something like this:Also, it has to be noted that if you're using your_tensor.assign(), then the tf.global_variables_initializer need not be called explicitly since the assign operation does it for you in the background.Example:However, this will not initialize all variables, but it will only initialize the variable on which assign was executed on.I answered a similar question here.  I looked in a lot of places that always created the same problem.  Basically, I did not want to assign a value to the weights, but simply change the weights.  The short version of the above answer is:tf.keras.backend.set_value(tf_var, numpy_weights)Here is the complete working example:Note the output will be:This means at the very start the Variable was 0, as defined, then after just one step of gradient decent the variable was -0.049999997, and after 10.000 more steps we are reaching -2.499994 (based on our cost function).Note: You originally used the Interactive session. Interactive session is useful when multiple different sessions needed to be run in the same script. However, I used the non interactive session for simplicity.Use Tensorflow eager execution mode which is latest.So i had a adifferent case where i needed to assign values before running a session, So this was the easiest way to do that:here i'm creating a variable as well as assigning it values at the same timeThere is an easier approach: 

Django upgrading to 1.9 error「AppRegistryNotReady: Apps aren't loaded yet.」

Kishore K

[Django upgrading to 1.9 error「AppRegistryNotReady: Apps aren't loaded yet.」](https://stackoverflow.com/questions/34114427/django-upgrading-to-1-9-error-appregistrynotready-apps-arent-loaded-yet)

When upgraded to django 1.9 from 1.8 I got this error. I checked answers for similar questions, but I didn't think this is an issue with any 3rd party packages or apps.I'd modified the Installed apps for 'django.contrib.auth'.

2015-12-06 05:36:10Z

When upgraded to django 1.9 from 1.8 I got this error. I checked answers for similar questions, but I didn't think this is an issue with any 3rd party packages or apps.I'd modified the Installed apps for 'django.contrib.auth'.Try to add this lines to the top of your settings file:And if this will not help you try to remove third-party applications from your installed apps list one-by-one.I'd a custom function written on one of my models __init__.py file. It was causing the error. When I moved this function from __init__.py it worked.My problem was that I tried to import a Django model before calling django.setup()This worked for me:The above script is in the project root folder.In my case, the error occurred when I made python manage.py makemigrations on Django 2.0.6. The solution was to run python manage.py runserver and see the actual error (which was just a missing environment variable).This error may occur when you are adding an app in INSTALLED_APPS  in the settings.py file but you do not have that app installed in your computer. You have two solution:This error may also arise if you are not in your virtual environment which you may have created for your project.Try removing the entire settings.LOGGING dictConfig and restart the server. If that works, rewrite the setting according to the v1.9 documentation.https://docs.djangoproject.com/en/1.9/topics/logging/#examplesFor me, the problem came from the fact that I was importing an app in INSTALLED_APPS which was itself importing a model in its __init__.py fileI had :settings.pymyapp.__init__.pycommenting out import models in myapp.__init__.py made it work :django.setup() in the top will not work while you are running a script explicitly.

My problem solved when I added this in the bottom of the settings fileFor me commenting outin INSTALLED_APPS workedMy problem was:

django-reversion>=1.8.7,<1.9for django 1.9.7 you should use:

django-reversion==1.10.0I were upgraded django-cms 3.2 to 3.3, and found it by commenting apps, then uncommenting back.Correct answer here:

https://stackoverflow.com/a/34040556/2837890This issue is also observed for inconsistent settings.py for incorrectly writing INSTALLED_APPS, verify if you correctly included apps and separated with "," .When I change my django version to 1.9, it don't arise the error.

pip uninstall django

pip install django==1.9

I put the User import into the settings file for managing the rest call token like this Because at that moment, Django libs are not ready yet. Therefore, I put the import inside the function and it started to work. The function needs to be called after the server is started In my case one of my settings, 'CORS_ORIGIN_WHITELIST' was set in the settings.py file but was not available in my .env file. So I'll suggest that you check your settings, especially those linked to .envAs others have said this can be caused when you've not installed an app that is listed in INSTALLED_APPS.In my case, manage.py was attempting to log the exception, which led to an attempt to render it which failed due to the app not being initialized yet. By 

commenting out the except clause in manage.py the exception was displayed without special rendering, avoiding the confusing error.I tried tons of things, but only downgrading Django to 1.8.18 fixed this issue for me:It is one of the installed apps that is failing, but I could not find which one.Got this error while trying to access model objects in apps.py:Trying to access Question before the app has loaded the model class caused the error for me.I get that error when i try to run:i tried so many things and realized that i added some references to "settings.py" - "INSTALLED_APPS"Just be sure what you write there is correct. My mistake was ".model." instead of ".app."Corrected that mistake and it's working now.Late to the party, but grappelli was the reason for my error as well. I looked up the compatible version on pypi and that fixed it for me.Try activating the virtual env.

In my case, using the git command line tool: Solves my problem.If your setting.py files fill are correct，you can try to arrive manage.py files proceed call danjgo.setup() in main method . Then run manage.py ,finally again run project ,the issue could disappear.In the "admin" module of your app package, do register all the databases created in "models" module of the package.Suppose you have a database class defined in "models" module as:so you must register this in the admin module as:I hope this resolve the error.

Django: signal when user logs in?

ssc

[Django: signal when user logs in?](https://stackoverflow.com/questions/1990502/django-signal-when-user-logs-in)

In my Django app, I need to start running a few periodic background jobs when a user logs in and stop running them when the user logs out, so I am looking for an elegant way toFrom my perspective, the ideal solution would beDjango 1.1.1 does not have that and I am reluctant to patch the source and add it (not sure how to do that, anyway).As a temporary solution, I have added an is_logged_in boolean field to the UserProfile model which is cleared by default, is set the first time the user hits the landing page (defined by LOGIN_REDIRECT_URL = '/') and is queried in subsequent requests. I added it to UserProfile, so I don't have to derive from and customize the builtin User model for that purpose only.I don't like this solution. If the user explicitely clicks the logout button, I can clear the flag, but most of the time, users just leave the page or close the browser; clearing the flag in these cases does not seem straight forward to me. Besides (that's rather data model clarity nitpicking, though), is_logged_in does not belong in the UserProfile, but in the User model.Can anyone think of alternate approaches ?

2010-01-02 03:19:49Z

In my Django app, I need to start running a few periodic background jobs when a user logs in and stop running them when the user logs out, so I am looking for an elegant way toFrom my perspective, the ideal solution would beDjango 1.1.1 does not have that and I am reluctant to patch the source and add it (not sure how to do that, anyway).As a temporary solution, I have added an is_logged_in boolean field to the UserProfile model which is cleared by default, is set the first time the user hits the landing page (defined by LOGIN_REDIRECT_URL = '/') and is queried in subsequent requests. I added it to UserProfile, so I don't have to derive from and customize the builtin User model for that purpose only.I don't like this solution. If the user explicitely clicks the logout button, I can clear the flag, but most of the time, users just leave the page or close the browser; clearing the flag in these cases does not seem straight forward to me. Besides (that's rather data model clarity nitpicking, though), is_logged_in does not belong in the UserProfile, but in the User model.Can anyone think of alternate approaches ?You can use a signal like this (I put mine in models.py)See django docs: https://docs.djangoproject.com/en/dev/ref/contrib/auth/#module-django.contrib.auth.signals and here http://docs.djangoproject.com/en/dev/topics/signals/One option might be to wrap Django's login/logout views with your own.  For example:You then use these views in your code rather than Django's, and voila.With regards to querying login status, it's pretty simple if you have access to the request object; simply check request's user attribute to see if they're a registered user or the anonymous user, and bingo.  To quote the Django documentation:If you don't have access to the request object, then determining if the current user is logged in is going to be difficult.Edit:Unfortunately, you'll never be able to get User.is_logged_in() functionality - it's a limitation of the HTTP protocol.  If you make a few assumptions, however, you might be able to get close to what you want.First, why can't you get that functionality?  Well, you can't tell the difference between someone closing the browser, or someone spending a while on a page before fetching a new one.  There's no way to tell over HTTP when someone actually leaves the site or closes the browser.So you have two options here that aren't perfect:These solutions are messy and not ideal, but they're the best you can do, unfortunately.In addition to @PhoebeB answer: 

you can also use @receiver decorator like this:And if you put it into signals.py in your app dir, then add this to app.py:a quick solution for this would be, in the _ _ init _ _.py of your app place the following code:The only reliable way (that also detects when the user has closed the browser) is to update some last_request field every time the user loads a page.You could also have a periodic AJAX request that pings the server every x minutes if the user has a page open.Then have a single background job that gets a list of recent users, create jobs for them, and clear the jobs for users not present in that list.Inferring logout, as opposed to having them explicitly click a button (which nobody does), means picking an amount of idle time that equates to "logged out". phpMyAdmin uses a default of 15 minutes, some banking sites use as little as 5 minutes.The simplest way of implementing this would be to change the cookie-lifetime. You can do this for your entire site by specifying settings.SESSION_COOKIE_AGE. Alternatively, you could change it on a per-user basis (based on some arbitrary set of criteria) by using HttpResponse.setcookie(). You can centralize this code by creating your own version of render_to_response() and having it set the lifetime on each response.Rough idea - you could use middleware for this. This middleware could process requests and fire signal when relevant URL is requested. It could also process responses and fire signal when given action actually succeded.

Run setUp only once for a set of automated tests

Kesandal

[Run setUp only once for a set of automated tests](https://stackoverflow.com/questions/14305941/run-setup-only-once-for-a-set-of-automated-tests)

My Python version is 2.6.I would like to execute the test setUp method only once since I do things there which are needed for all tests.My idea was to create a boolean variable which will be set to 'true' after the first execution and then disable more than one call to the setup method.The output:why is this not working? Did I miss anything?

2013-01-13 17:11:02Z

My Python version is 2.6.I would like to execute the test setUp method only once since I do things there which are needed for all tests.My idea was to create a boolean variable which will be set to 'true' after the first execution and then disable more than one call to the setup method.The output:why is this not working? Did I miss anything?You can use setUpClass to define methods that only run once per testsuite.Daniel's answer is correct, but here is an example to avoid some common mistakes I found, such as not calling super() in setUpClass() when TestCase is a subclass of unittest.TestCase (like in django.test or falcon.testing).The documentation for setUpClass() doesn't mention that you need to call super() in such cases. You will get an error if you don't, as seen in this related question.Don't try to dedupe the calls to setUp, just call it once.For example:This will call _set_up() when the module's first loaded. I've defined it to be a module-level function, but you could equally make it a class method of MyClass.Place all code you want set up once outside the mySelTest.Another possibility is having a Singleton class that you instantiate in setUp(), which will only run the __new__ code once and return the object instance for the rest of the calls.

See: Is there a simple, elegant way to define singletons?Your way works too though.If you ended up here because of need to load some data for testing...

then as far as you are using Django 1.9+ please go for setUpTestData:setup_done is a class variable, not an instance variable.  You are referencing it as an instance variable:self.setup_doneBut you need to reference it as a class variable:mySelTest.setup_doneHere's the corrected code:I'm using Python 3 and found that the cls reference is also available in the setup method and so the following works:

How to make two plots side-by-side using Python?

Heißenberg93

[How to make two plots side-by-side using Python?](https://stackoverflow.com/questions/42818361/how-to-make-two-plots-side-by-side-using-python)

I found the following example on matplotlib:My question is: What do i need to change, to have the plots side-by-side?

2017-03-15 18:52:28Z

I found the following example on matplotlib:My question is: What do i need to change, to have the plots side-by-side?Change your subplot settings to:The parameters for subplot are: number of rows, number of columns, and which subplot you're currently on. So 1, 2, 1 means "a 1-row, 2-column figure: go to the first subplot." Then 1, 2, 2 means "a 1-row, 2-column figure: go to the second subplot."You currently are asking for a 2-row, 1-column (that is, one atop the other) layout. You need to ask for a 1-row, 2-column layout instead. When you do, the result will be:In order to minimize the overlap of subplots, you might want to kick in a:before the show. Yielding:Check this page out: http://matplotlib.org/examples/pylab_examples/subplots_demo.htmlplt.subplots is similar. I think it's better since it's easier to set parameters of the figure. The first two arguments define the layout (in your case 1 row, 2 columns), and other parameters change features such as figure size:

How do I iterate through the alphabet?

MillsOnWheels

[How do I iterate through the alphabet?](https://stackoverflow.com/questions/17182656/how-do-i-iterate-through-the-alphabet)

In Python, could I simply ++ a char? What is an efficient way of doing this?I want to iterate through URLs that have the www.website.com/term/#, www.website.com/term/a, www.website.com/term/b, www.website.com/term/c, www.website.com/term/d ... www.website.com/term/z format.

2013-06-19 03:59:17Z

In Python, could I simply ++ a char? What is an efficient way of doing this?I want to iterate through URLs that have the www.website.com/term/#, www.website.com/term/a, www.website.com/term/b, www.website.com/term/c, www.website.com/term/d ... www.website.com/term/z format.You can use string.ascii_lowercase which is simply a convenience string of lowercase letters,In addition to string.ascii_lowercase you should also take a look at the ord and chr built-ins. ord('a') will give you the ascii value for 'a' and chr(ord('a')) will give you back the string 'a'.Using these you can increment and decrement through character codes and convert back and forth easily enough. ASCII table is always a good bookmark to have too.

What algorithm does python's sorted() use? [duplicate]

Hartley Brody

[What algorithm does python's sorted() use? [duplicate]](https://stackoverflow.com/questions/10948920/what-algorithm-does-pythons-sorted-use)

Name says it all.I'm trying to explain to someone why they should use Python's builtin sorted() function instead of rolling their own, and I realized I have no idea what algorithm it uses.If it matters, we're talking python 2.7

2012-06-08 12:33:36Z

Name says it all.I'm trying to explain to someone why they should use Python's builtin sorted() function instead of rolling their own, and I realized I have no idea what algorithm it uses.If it matters, we're talking python 2.7Python uses an algorithm called Timsort:The sort algorithm is called Timsort.  See timsortSince 2.3 Python has used timsort.More info: http://bugs.python.org/file4451/timsort.txt

How can I remove Nan from list Python/NumPy

user3001937

[How can I remove Nan from list Python/NumPy](https://stackoverflow.com/questions/21011777/how-can-i-remove-nan-from-list-python-numpy)

I have a list that countain values, one of the values I got is 'nan'I tried to remove it, but I everytime get an error When I tried this one : 

2014-01-09 04:46:59Z

I have a list that countain values, one of the values I got is 'nan'I tried to remove it, but I everytime get an error When I tried this one : The question has changed, so to has the answer:Strings can't be tested using math.isnan as this expects a float argument. In your countries list, you have floats and strings.In your case the following should suffice:In your countries list, the literal 'nan' is a string not the Python float nan which is equivalent to:In your case the following should suffice:The problem comes from the fact that np.isnan() does not handle string values correctly. For example, if you do:However the pandas version pd.isnull() works for numeric and string values:This should remove all NaN. Of course, I assume that it is not a string here but actual NaN (np.nan).Using your example where...countries= [nan, 'USA', 'UK', 'France']Since nan is not equal to nan (nan != nan) and countries[0] = nan, you should observe the following:However, Therefore, the following should work:use numpy fancy indexing:if you check for the element typethe result will be <class float>

so you can use the following code:I like to remove missing values from a list like this:In your example 'nan' is a string so instead of using isnan() just check for the stringlike this:Another way to do it would include using filter like this:I noticed that Pandas for example will return 'nan' for blank values. Since it's not a string you need to convert it to one in order to match it. For example:

Compiling Python to WebAssembly

Robbie

[Compiling Python to WebAssembly](https://stackoverflow.com/questions/44761748/compiling-python-to-webassembly)

I have read that it is possible to convert Python 2.7 code to Web Assembly, but I cannot find a definitive guide on how to to so.So far I have compiled a C program to Web Assembly using Emscripten and all its necessary components, so I know it is working (guide used: http://webassembly.org/getting-started/developers-guide/)What are the steps I must take in order to do this on an Ubuntu machine? Do I have to convert the python code to LLVM bitcode then compile it using Emscripten? If so, how would I achieve this?

2017-06-26 14:13:31Z

I have read that it is possible to convert Python 2.7 code to Web Assembly, but I cannot find a definitive guide on how to to so.So far I have compiled a C program to Web Assembly using Emscripten and all its necessary components, so I know it is working (guide used: http://webassembly.org/getting-started/developers-guide/)What are the steps I must take in order to do this on an Ubuntu machine? Do I have to convert the python code to LLVM bitcode then compile it using Emscripten? If so, how would I achieve this?First, let's take a look how, in principle, WebAssembly is different from asm.js, and whether there's potential to reuse existing knowledge and tooling. The following gives pretty good overview:Let's recapitulate, WebAssembly (MVP, as there's more on its roadmap, roughly):Thus, currently WebAssembly is an iteration on asm.js and targets only C/C++.It doesn't look like GC is the only thing that stops Python code from targeting WebAssembly/asm.js. Both represent low-level statically typed code, in which Python code can't (realistically) be represented. As current toolchain of WebAssembly/asm.js is based on LLVM, a language that can be easily compiled to LLVM IR can be converted to WebAssembly/asm.js. But alas, Python is too dynamic to fit into it as well, as proven by Unladen Swallow and several attempts of PyPy.This asm.js presentation has slides about the state of dynamic languages. What it means is that currently it's only possible to compile whole VM (language implementation in C/C++) to WebAssembly/asm.js and interpret (with JIT where possible) original sources. For Python there're several existing projects:Another potentially interesting thing here is Nuitka, a Python to C++ compiler. Potentially it can be possible to build your Python app to C++ and then compile it along with CPython with Emscripten. But practically I've no idea how to do it.For the time being, if you're building a conventional web site or web app where download several-megabyte JS file is barely an option, take a look at Python-to-JavaScript transpilers (e.g. Transcrypt) or JavaScript Python implementations (e.g. Brython). Or try your luck with others from list of languages that compile to JavaScript.Otherwise, if download size is not an issue, and you're ready to tackle a lot of rough edges, choose between the three above.This won't be possible until web assembly implements garbage collection. You can follow progress here: https://github.com/WebAssembly/proposals/issues/16In short: you can't convert arbitrary Python to Web Assembly, and I doubt you will be able to for a long time to come.  A workaround might be Python to C to Web Assembly, but that isn't generally going to work either since Python-to-C is fragile (see below).WebAssembly is specifically targeted to C-like languages as you can see at http://webassembly.org/docs/high-level-goals/Translating from Python to C can be done with tools like PyPy, which has been under development for a long time, but which still does not work for arbitrary Python code.  There are several reasons for this:If you look more carefully into why Python-to-C (or Python to C++) has been so tricky you can see the detailed reasons behind this terse answer, but I think that's outside the scope of your question.

Python get proper line ending

Evan Fosmark

[Python get proper line ending](https://stackoverflow.com/questions/454725/python-get-proper-line-ending)

Is there an easy way to get the type of line ending that the current operating system uses?

2009-01-18 06:01:14Z

Is there an easy way to get the type of line ending that the current operating system uses?If you are operating on a file that you opened in text mode, then you are correct that line breaks all show up as '\n'. Otherwise, you are looking for os.linesep .From http://docs.python.org/library/os.html:Oh, I figured it out. Apparently, PEP-278 states the following: If specify test resp. binary properly when opening files, and use universal newlines, you shouldn't have to worry about different newlines most of the time.But if you have to, use os.linesepos.linesep is important as it depends (as the name implied:)) on os. E.g. on Windows, it is not "\n" but rather "\r\n".But if you don't care about multi-platform stuff you can just use '\n'.

What is a 'NoneType' object?

insecure-IT

[What is a 'NoneType' object?](https://stackoverflow.com/questions/21095654/what-is-a-nonetype-object)

I'm getting this error when I run my python script:I'm pretty sure the 'str' means string, but I dont know what a 'NoneType' object is. My script craps out on the second line, I know the first one works because the commands from that line are in my asa as I would expect. At first I thought it may be because I'm using variables and user input inside send_command.Everything in 'CAPS' are variables, everything in 'lower case' is input from 'parser.add_option' options.I'm using pexpect, and optparse

2014-01-13 15:58:06Z

I'm getting this error when I run my python script:I'm pretty sure the 'str' means string, but I dont know what a 'NoneType' object is. My script craps out on the second line, I know the first one works because the commands from that line are in my asa as I would expect. At first I thought it may be because I'm using variables and user input inside send_command.Everything in 'CAPS' are variables, everything in 'lower case' is input from 'parser.add_option' options.I'm using pexpect, and optparseNoneType is the type for the None object, which is an object that indicates no value. None is the return value of functions that "don't return anything". It is also a common default return value for functions that search for something and may or may not find it; for example, it's returned by re.search when the regex doesn't match, or dict.get when the key has no entry in the dict. You cannot add None to strings or other objects.One of your variables is None, not a string. Maybe you forgot to return in one of your functions, or maybe the user didn't provide a command-line option and optparse gave you None for that option's value. When you try to add None to a string, you get that exception:One of group or SNMPGROUPCMD or V3PRIVCMD has None as its value.NoneType is simply the type of the None singleton:From the latter link above:In your case, it looks like one of the items you are trying to concatenate is None, hence your error.For the sake of defensive programming, objects should be checked against nullity before using.orIt means you're trying to concatenate a string with something that is None.None is the "null" of Python, and NoneType is its type.This code will raise the same kind of error:In Python, to represent the absence of a value, you can use the None value types.NoneType.NoneIn the error message, instead of telling you that you can't concatenate two objects by showing their values (a string and None in this example), the Python interpreter tells you this by showing the types of the objects that you tried to concatenate. The type of every string is str while the type of the single None instance is called NoneType.You normally do not need to concern yourself with NoneType, but in this example it is necessary to know that type(None) == NoneType.Your error's occurring due to something like this:

>>> None + "hello world"

Traceback (most recent call last):

  File "<stdin>", line 1, in <module>

TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'

>>>  Python's None object is roughly equivalent to null, nil, etc. in other languages.One of the variables has not been given any value, thus it is a NoneType. You'll have to look into why this is, it's probably a simple logic error on your part.A nonetype is the type of a None.See the docs here:

https://docs.python.org/2/library/types.html#types.NoneType

How to remove lines in a Matplotlib plot

David Morton

[How to remove lines in a Matplotlib plot](https://stackoverflow.com/questions/4981815/how-to-remove-lines-in-a-matplotlib-plot)

How can I remove a line (or lines) of a matplotlib axes in such a way as it actually gets garbage collected and releases the memory back?  The below code appears to delete the line, but never releases the memory (even with explicit calls to gc.collect())So is there a way to just delete one line from an axes and get the memory back?

This potential solution also does not work.

2011-02-13 01:19:11Z

How can I remove a line (or lines) of a matplotlib axes in such a way as it actually gets garbage collected and releases the memory back?  The below code appears to delete the line, but never releases the memory (even with explicit calls to gc.collect())So is there a way to just delete one line from an axes and get the memory back?

This potential solution also does not work.I'm showing that a combination of lines.pop(0) l.remove() and del l does the trick.I checked your large dataset and the release of the memory is confirmed on the system monitor as well.Of course the simpler way (when not trouble-shooting) would be to pop it from the list and call remove on the line object without creating a hard reference to it: This is a very long explanation that I typed up for a coworker of mine.  I think it would be helpful here as well.  Be patient, though.  I get to the real issue that you are having toward the end.  Just as a teaser, it's an issue of having extra references to your Line2D objects hanging around.WARNING: One other note before we dive in.  If you are using IPython to test this out, IPython keeps references of its own and not all of them are weakrefs.  So, testing garbage collection in IPython does not work.  It just confuses matters.Okay, here we go.  Each matplotlib object (Figure, Axes, etc) provides access to its child artists via various attributes.  The following example is getting quite long, but should be illuminating.We start out by creating a Figure object, then add an Axes object to that figure.  Note that ax and fig.axes[0] are the same object (same id()).This also extends to lines in an axes object:If you were to call plt.show() using what was done above, you would see a figure containing a set of axes and a single line: Now, while we have seen that the contents of lines and ax.lines is the same, it is very important to note that the object referenced by the lines variable is not the same as the object reverenced by ax.lines as can be seen by the following:As a consequence, removing an element from lines does nothing to the current plot, but removing an element from ax.lines removes that line from the current plot.  So:So, if you were to run the second line of code, you would remove the Line2D object contained in ax.lines[0] from the current plot and it would be gone.  Note that this can also be done via ax.lines.remove() meaning that you can save a Line2D instance in a variable, then pass it to ax.lines.remove() to delete that line, like so:All of the above works for fig.axes just as well as it works for ax.linesNow, the real problem here.  If we store the reference contained in ax.lines[0] into a weakref.ref object, then attempt to delete it, we will notice that it doesn't get garbage collected:The reference is still live!  Why?  This is because there is still another reference to the Line2D object that the reference in wr points to.  Remember how lines didn't have the same ID as ax.lines but contained the same elements?  Well, that's the problem.So, the moral of the story is, clean up after yourself.  If you expect something to be garbage collected but it isn't, you are likely leaving a reference hanging out somewhere.I've tried lots of different answers in different forums. I guess it depends on the machine your developing. But I haved used the statementand works perfectly. I don't use cla() cause it deletes all the definitions I've made to the plotEx.but I've tried deleting the lines many times. Also using the weakref library to check the reference to that line while I was deleting but nothing worked for me.Hope this works for someone else =D(using the same example as the guy above)

Returning boolean if set is empty

C.B.

[Returning boolean if set is empty](https://stackoverflow.com/questions/21191259/returning-boolean-if-set-is-empty)

I am struggling to find a more clean way of returning a boolean value if my set is empty at the end of my functionI take the intersection of two sets, and want to return True or False based on if the resulting set is empty.My initial thought was to doHowever, in my interpreter I can easily see that statement will return true if c = set([])I've also tried all of the following:Now I've read from the documentation that I can only use and, or, and not with empty sets to deduce a boolean value. So far, the only thing I can come up with is returning not not cI have a feeling there is a much more pythonic way to do this, by I am struggling to find it. I don't want to return the actual set to an if statement because I don't need the values, I just want to know if they intersect.

2014-01-17 16:45:13Z

I am struggling to find a more clean way of returning a boolean value if my set is empty at the end of my functionI take the intersection of two sets, and want to return True or False based on if the resulting set is empty.My initial thought was to doHowever, in my interpreter I can easily see that statement will return true if c = set([])I've also tried all of the following:Now I've read from the documentation that I can only use and, or, and not with empty sets to deduce a boolean value. So far, the only thing I can come up with is returning not not cI have a feeling there is a much more pythonic way to do this, by I am struggling to find it. I don't want to return the actual set to an if statement because I don't need the values, I just want to know if they intersect.bool() will do something similar to not not, but more ideomatic and clear.not as pythonic as the other answers, but mathematics: If you want to return True for an empty set, then I think it would be clearer to do:i.e. "c is equal to an empty set". (Or, for the other way around, return c != set()).In my opinion, this is more explicit (though less idiomatic) than relying on Python's interpretation of an empty set as False in a boolean context.If c is a set then you can check whether it's empty by doing: return not c.If c is empty then not c will be True.Otherwise, if c contains any elements not c will be False.When you say:You are actually checking if c and None reference the same object. That is what the "is" operator does. In python None is a special null value conventionally meaning you don't have a value available. Sorta like null in c or java. Since python internally only assigns one None value using the "is" operator to check if something is None (think null) works, and it has become the popular style. However this does not have to do with the truth value of the set c, it is checking that c actually is a set rather than a null value.If you want to check if a set is empty in a conditional statement, it is cast as a boolean in context so you can just say:But if you want it converted to a boolean to be stored away you can simply say:Not as clean as bool(c) but it was an excuse to use ternary.Also using a bit of the same logic there is no need to assign to c unless you are using it for something else.Finally, I would assume you want a True / False value because you are going to perform some sort of boolean test with it. I would recommend skipping the overhead of a function call and definition by simply testing where you need it.Instead of:Maybe this:

Byte Array in Python

d0ctor

[Byte Array in Python](https://stackoverflow.com/questions/7380460/byte-array-in-python)

How can I represent a byte array (like in Java with byte[]) in Python? I'll need to send it over the wire with gevent.

2011-09-11 18:45:07Z

How can I represent a byte array (like in Java with byte[]) in Python? I'll need to send it over the wire with gevent.In Python 3, we use the bytes object, also known as str in Python 2.I find it more convenient to use the base64 module...You can also use literals...Just use a bytearray (Python 2.6 and later) which represents a mutable sequence of bytesIndexing get and sets the individual bytesand if you need it as a str (or bytes in Python 3), it's as simple asAn alternative that also has the added benefit of easily logging its output:allows you to do easy substitutions like so:Dietrich's answer is probably just the thing you need for what you describe, sending bytes, but a closer analogue to the code you've provided for example would be using the bytearray type.

Handling backreferences to capturing groups in re.sub replacement pattern

Richard

[Handling backreferences to capturing groups in re.sub replacement pattern](https://stackoverflow.com/questions/8157267/handling-backreferences-to-capturing-groups-in-re-sub-replacement-pattern)

I want to take the string 0.71331, 52.25378 and return 0.71331,52.25378 - i.e. just look for a digit, a comma, a space and a digit, and strip out the space. This is my current code:But this gives me 0.7133,2.25378. What am I doing wrong?

2011-11-16 19:10:35Z

I want to take the string 0.71331, 52.25378 and return 0.71331,52.25378 - i.e. just look for a digit, a comma, a space and a digit, and strip out the space. This is my current code:But this gives me 0.7133,2.25378. What am I doing wrong?You should be using raw strings for regex, try the following:With your current code, the backslashes in your replacement string are escaping the digits, so you are replacing all matches the equivalent of chr(1) + "," + chr(2):Any time you want to leave the backslash in the string, use the r prefix, or escape each backslash (\\1,\\2).Python interprets the \1 as a character with ASCII value 1, and passes that to sub.Use raw strings, in which Python doesn't interpret the \.This is covered right in the beginning of the re documentation, should you need more info.

What does 'killed' mean?

user1893354

[What does 'killed' mean?](https://stackoverflow.com/questions/19189522/what-does-killed-mean)

I have a Python script that imports a large CSV file and then counts the number of occurrences of each word in the file, then exports the counts to another CSV file.But what is happening is that once that counting part is finished and the exporting begins it says Killed in the terminial.I don't think this is a memory problem (if it was I assume I would be getting a memory error and not Killed).Could it be that the process is taking too long? If so, is there a way to extend the time-out period so I can avoid this?Here is the code:And the Killed happens after finished counting has printed, and the full message is: 

2013-10-04 19:44:44Z

I have a Python script that imports a large CSV file and then counts the number of occurrences of each word in the file, then exports the counts to another CSV file.But what is happening is that once that counting part is finished and the exporting begins it says Killed in the terminial.I don't think this is a memory problem (if it was I assume I would be getting a memory error and not Killed).Could it be that the process is taking too long? If so, is there a way to extend the time-out period so I can avoid this?Here is the code:And the Killed happens after finished counting has printed, and the full message is: Exit code 137 (128+9) indicates that your program exited due to receiving signal 9, which is SIGKILL. This also explains the killed message. The question is, why did you receive that signal?The most likely reason is probably that your process crossed some limit in the amount of system resources that you are allowed to use. Depending on your OS and configuration, this could mean you had too many open files, used too much filesytem space or something else. The most likely is that your program was using too much memory. Rather than risking things breaking when memory allocations started failing, the system sent a kill signal to the process that was using too much memory.As I commented earlier, one reason you might hit a memory limit after printing finished counting is that your call to counter.items() in your final loop allocates a list that contains all the keys and values from your dictionary. If your dictionary had a lot of data, this might be a very big list. A possible solution would be to use counter.iteritems() which is a generator. Rather than returning all the items in a list, it lets you iterate over them with much less memory usage.So, I'd suggest trying this, as your final loop:Note that in Python 3, items returns a "dictionary view" object which does not have the same overhead as Python 2's version. It replaces iteritems, so if you later upgrade Python versions, you'll end up changing the loop back to the way it was.There are two storage areas involved: the stack and the heap.The stack is where the current state of a method call is kept (ie local variables and references), and the heap is where objects are stored. recursion and memoryI gues there are too many keys in the counter dict that will consume too much memory of the heap region, so the Python runtime will raise a OutOfMemory exception.To save it, don't create a giant object, e.g. the counter.1.StackOverflowa program that create too many local variables.2.OutOfMemorya program that creats a giant dict includes too many keys.I doubt anything is killing the process just because it takes a long time.  Killed generically means something from the outside terminated the process, but probably not in this case hitting Ctrl-C since that would cause Python to exit on a KeyboardInterrupt exception.  Also, in Python you would get MemoryError exception if that was the problem.  What might be happening is you're hitting a bug in Python or standard library code that causes a crash of the process.

Which is the best way to allow configuration options be overridden at the command line in Python?

andreas-h

[Which is the best way to allow configuration options be overridden at the command line in Python?](https://stackoverflow.com/questions/3609852/which-is-the-best-way-to-allow-configuration-options-be-overridden-at-the-comman)

I have a Python application which needs quite a few (~30) configuration parameters. Up to now, I used the OptionParser class to define default values in the app itself, with the possibility to change individual parameters at the command line when invoking the application.Now I would like to use 'proper' configuration files, for example from the ConfigParser class. At the same time, users should still be able to change individual parameters at the command line.I was wondering if there is any way to combine the two steps, e.g. use optparse (or the newer argparse) to handle command line options, but reading the default values from a config file in ConfigParse syntax.Any ideas how to do this in an easy way? I don't really fancy manually invoking ConfigParse, and then manually setting all defaults of all the options to the appropriate values...

2010-08-31 14:12:58Z

I have a Python application which needs quite a few (~30) configuration parameters. Up to now, I used the OptionParser class to define default values in the app itself, with the possibility to change individual parameters at the command line when invoking the application.Now I would like to use 'proper' configuration files, for example from the ConfigParser class. At the same time, users should still be able to change individual parameters at the command line.I was wondering if there is any way to combine the two steps, e.g. use optparse (or the newer argparse) to handle command line options, but reading the default values from a config file in ConfigParse syntax.Any ideas how to do this in an easy way? I don't really fancy manually invoking ConfigParse, and then manually setting all defaults of all the options to the appropriate values...I just discovered you can do this with argparse.ArgumentParser.parse_known_args(). Start by using parse_known_args() to parse a configuration file from the commandline, then read it with ConfigParser and set the defaults, and then parse the rest of the options with parse_args(). This will allow you to have a default value, override that with a configuration file and then override that with a commandline option. E.g.:Default with no user input:Default from configuration file:Default from configuration file, overridden by commandline:argprase-partial.py follows. It is slightly complicated to handle -h for help properly. Check out ConfigArgParse - its a new PyPI package (open source) that serves as a drop in replacement for argparse with added support for config files and environment variables.I'm using ConfigParser and argparse with subcommands to handle such tasks. The important line in the code below is:This will set the defaults of the subcommand (from argparse) to the values in the section of the config file. A more complete example is below:I can't say it's the best way, but I have an OptionParser class that I made that does just that - acts like optparse.OptionParser with defaults coming from a config file section. You can have it...Feel free to browse the source. Tests are in a sibling directory.You can use ChainMapYou can combine values from command line, environment variables, configuration file, and in case if the value is not there define a default value.Try to this wayUse it:And create example config:Update: This answer still has issues; for example, it cannot handle required arguments, and requires an awkward config syntax. Instead, ConfigArgParse seems to be exactly what this question asks for, and is a transparent, drop-in replacement.One issue with the current is that it will not error if the arguments in the config file are invalid. Here's a version with a different downside: you'll need to include the -- or - prefix in the keys.Here's the python code (Gist link with MIT license):Here's an example of a config file:Now, runningHowever, if our config file has an error:Running the script will produce an error, as desired:The main downside is that this uses parser.parse_args somewhat hackily in order to obtain the error checking from ArgumentParser, but I am not aware of any alternatives to this.fromfile_prefix_charsMaybe not the perfect API, but worth knowing about. main.py:Then:Documentation: https://docs.python.org/3.6/library/argparse.html#fromfile-prefix-charsTested on Python 3.6.5, Ubuntu 18.04.

What SOAP libraries exist for Python 3.x? [closed]

gecco

[What SOAP libraries exist for Python 3.x? [closed]](https://stackoverflow.com/questions/7817303/what-soap-libraries-exist-for-python-3-x)

I searched the web for an existing and supported SOAP library for Python 3. (both client and server)Here the list of libraries I've found:Python 2:Python 3:Does this list seems complete to you? (FYI, I used this post as starting point (The purpose of that post was the same but for Python 2))ladon seems to me the only existing framework for Python 3 but can AFAIK only be used for implementing the server side.NO: I don't want to migrate one of the discontinued Python 2 projects myself. I am looking for a supported project with an active team providing help if needed.Updated on 28/09/2013

2011-10-19 06:20:36Z

I searched the web for an existing and supported SOAP library for Python 3. (both client and server)Here the list of libraries I've found:Python 2:Python 3:Does this list seems complete to you? (FYI, I used this post as starting point (The purpose of that post was the same but for Python 2))ladon seems to me the only existing framework for Python 3 but can AFAIK only be used for implementing the server side.NO: I don't want to migrate one of the discontinued Python 2 projects myself. I am looking for a supported project with an active team providing help if needed.Updated on 28/09/2013Depending on the complexity of the service, you could use ladon for the server side and mock up the client by hand until there's a better solution available.Just call the service with suds (or similar) with logging turned on and note the SOAP wrapping on the request.  Use that to wrap your request and call the service with plain http.It's not an ideal solution, but it can get you by until you have a package to replace it.I did this same search several months ago and came to the same conclusions. There really isn't much to choose from in this space. I ended up sticking with Python 2.7 and using SOAPy for my project because it was so easy to use. It may be discontinued but it still works. I figure that sometimes you just have to get your hands a little dirty and support yourself, that is why we are called programmers.rpclib: seems the only active project. In their description, they say they are looking for volunteers to test it for Python 3. So maybe you should volunteer yourself!

How do I force Django to ignore any caches and reload data?

scippy

[How do I force Django to ignore any caches and reload data?](https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data)

I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.I can't find any documentation on that update() method and have no idea what other bad things it might do.My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?

2010-07-27 17:17:43Z

I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.I can't find any documentation on that update() method and have no idea what other bad things it might do.My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?Having had this problem and found two definitive solutions for it I thought it worth posting another answer.This is a problem with MySQL's default transaction mode.  Django opens a transaction at the start, which means that by default you won't see changes made in the database.Demonstrate like thisRun a django shell in terminal 1And another in terminal 2Back to terminal 1 to demonstrate the problem  - we still read the old value from the database.Now in terminal 1 demonstrate the solutionThe new data is now readHere is that code in an easy to paste block with docstringThe alternative solution is to change my.cnf for MySQL to change the default transaction modeNote that that is a relatively new feature for Mysql and has some consequences for binary logging / slaving.  You could also put this in the django connection preamble if you wanted.Update 3 years laterNow that Django 1.6 has turned on autocommit in MySQL this is no longer a problem.  The example above now works fine without the flush_transaction() code whether your MySQL is in REPEATABLE-READ (the default) or READ-COMMITTED transaction isolation mode.What was happening in previous versions of Django which ran in non autocommit mode was that the first select statement opened a transaction.  Since MySQL's default mode is REPEATABLE-READ this means that no updates to the database will be read  by subsequent select statements - hence the need for the flush_transaction() code above which stops the transaction and starts a new one.There are still reasons why you might want to use READ-COMMITTED transaction isolation though. If you were to put terminal 1 in a transaction and you wanted to see the writes from the terminal 2 you would need READ-COMMITTED.The flush_transaction() code now produces a deprecation warning in Django 1.6 so I recommend you remove it.We've struggled a fair bit with forcing django to refresh the "cache" - which it turns out wasn't really a cache at all but an artifact due to transactions. This might not apply to your example, but certainly in django views, by default, there's an implicit call to a transaction, which mysql then isolates from any changes that happen from other processes ater you start.we used the @transaction.commit_manually decorator and calls to transaction.commit() just before every occasion where you need up-to-date info.As I say, this definitely applies to views, not sure whether it would apply to django code not being run inside a view.detailed info here: http://devblog.resolversystems.com/?p=439Seems like the count() goes to cache after the first time. This is the django source for QuerySet.count:update does seem to be doing quite a bit of extra work, besides what you need.

But I can't think of any better way to do this, short of writing your own SQL for the count.

If performance is not super important, I would just do what you're doing, calling update before count.QuerySet.update:I'm not sure I'd recommend it...but you can just kill the cache yourself:And here's a better technique that doesn't rely on fiddling with the innards of the QuerySet: Remember that the caching is happening within a QuerySet, but refreshing the data simply requires the underlying Query to be re-executed. The QuerySet is really just a high-level API wrapping a Query object, plus a container (with caching!) for Query results. Thus, given a queryset, here is a general-purpose way of forcing a refresh:Pretty easy! You can of course implement this as a helper function and use as needed.If you append .all() to a queryset, it'll force a reread from the DB. Try

MyModel.objects.all().count() instead of MyModel.objects.count().You can also use MyModel.objects._clone().count().  All of the methods in the the QuerySet call _clone() prior to doing any work - that ensures that any internal caches are invalidated.The root cause is that MyModel.objects is the same instance each time.  By cloning it you're creating a new instance without the cached value.  Of course, you can always reach in and invalidate the cache if you'd prefer to use the same instance.

How can I add a background thread to flask?

Marinus

[How can I add a background thread to flask?](https://stackoverflow.com/questions/14384739/how-can-i-add-a-background-thread-to-flask)

I'm busy writing a small game server to try out flask. The game exposes an API via REST to users. It's easy for users to perform actions and query data, however I'd like to service the "game world" outside the app.run() loop to update game entities, etc. Given that Flask is so cleanly implemented, I'd like to see if there's a Flask way to do this.

2013-01-17 17:29:40Z

I'm busy writing a small game server to try out flask. The game exposes an API via REST to users. It's easy for users to perform actions and query data, however I'd like to service the "game world" outside the app.run() loop to update game entities, etc. Given that Flask is so cleanly implemented, I'd like to see if there's a Flask way to do this.Your additional threads must be initiated from the same app that is called by the WSGI server.The example below creates a background thread that executes every 5 seconds and manipulates data structures that are also available to Flask routed functions.Call it from Gunicorn with something like this:In addition to using pure threads or the Celery queue (note that flask-celery is no longer required), you could also have a look at flask-apscheduler:https://github.com/viniciuschiele/flask-apschedulerA simple example copied from https://github.com/viniciuschiele/flask-apscheduler/blob/master/examples/jobs.py:

How to specify multiple author(s) / email(s) in setup.py

priya

[How to specify multiple author(s) / email(s) in setup.py](https://stackoverflow.com/questions/9999829/how-to-specify-multiple-authors-emails-in-setup-py)

We wrote a small wrapper to a twitter app and published this information to http://pypi.python.org. But setup.py just contained a single field for specifying email / name of the author. How do I specify multiple contributors / email list, to the following fields since we would like this package to be listed under our names, much similar to how it shows up in http://rubygems.org.

2012-04-03 19:13:34Z

We wrote a small wrapper to a twitter app and published this information to http://pypi.python.org. But setup.py just contained a single field for specifying email / name of the author. How do I specify multiple contributors / email list, to the following fields since we would like this package to be listed under our names, much similar to how it shows up in http://rubygems.org.As far as I know, setuptools doesn't support using a list of strings in order to specify multiple authors. Your best bet is to list the authors in a single string:I'm not sure if PyPI validates the author_email field, so you may run into trouble with that one. In any case, I would recommend you limit these to a single author and mention all contributors in the documentation or description.This has been registered as a bug, actually, but it seems like support for multiple authors was not implemented. Here is an alternative solution. Here is an idea for how to provide a contact email for a project with multiple authors.I'm sort of just piggybacking off of @modocache's answer, in case you want some specifics.  Throughout this answer, I'll be refering to a python3.6 version of the FOO-PYTHON-ENV\Lib\distutils\dist.py file& Here's the whole thing: 

Where should I put my own python module so that it can be imported

Cobry

[Where should I put my own python module so that it can be imported](https://stackoverflow.com/questions/16196268/where-should-i-put-my-own-python-module-so-that-it-can-be-imported)

I have my own package in python and I am using it very often. what is the most elegant or conventional directory where i should put my package so it is going to be imported without playing with PYTHONPATH or sys.path?What about site-packages for example?

/usr/lib/python2.7/site-packages.

Is it common in python to copy and paste the package there ?

2013-04-24 15:38:18Z

I have my own package in python and I am using it very often. what is the most elegant or conventional directory where i should put my package so it is going to be imported without playing with PYTHONPATH or sys.path?What about site-packages for example?

/usr/lib/python2.7/site-packages.

Is it common in python to copy and paste the package there ?I usually put the stuff i want to have ready to import in the user site directory:To show the right directory for your platform, you can use python -m site --user-siteedit: it will show up in sys.path once you create it:So if your a novice like myself and your directories are not very well organized you may want to try this method.Open your python terminal. Import a module that you know works such as numpy in my case and do the following.

 Import numpynumpy.__file__which results in'/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-                       packages/numpy/__init__.py'The result of numpy.__file__ is the location you should put the python file with your module (excluding the numpy/__init__.py) so for me that would be /Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-                            packagesTo do this just go to your terminal and typemv "location of your module" "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-                            packages"Now you should be able to import your module.This is something that works for me (I have to frequently create python packages that are uploaded to a private pip repository). elaborating on the comment by @joran on the question.you can use pip install --force-reinstall if you need to play around with the libraries more and re-create the dist packages.I've found that this method works great for me. If you do not need to package the modules for use of other systems instead of just your local, this method might be an overkillHappy hacking.On my Mac, I did a sudo find / -name "site-packages". That gave me a few paths like /Library/Python/2.6/site-packages, /Library/Python/2.7/site-packages, and /opt/X11/lib/python2.6/site-packages.So, I knew where to put my modules if I was using v2.7 or v2.6. Hope it helps. import folders could be extracted by adding following source code:automatic symlink generation example would be:instead of "em" you may use other package you've "just installed but the python can't see it"below I'll explain in more details as being requested in the comment.suppose you've installed python module em or pyserial with the following command (examples are for ubuntu):and the output is like this:the question would be following - python can't see the module pyserial, why?

because the location where the module has been installed isn't the one python is looking at for your particular user account.solution - we have to create symlink from the path where pyserial arrived to the path where your python is looking for.symlink creation command would be:instead of typing exact location we are asking pip to tell us where it stored modules by executing command:instead of typing exact location we are asking python to tell us where it looks for the modules being installed by executing command:both commands has to be escaped with "`" character (usually on the left of your 1 button for the US keyboards)in result following command will be provided for ln and the missing symlink would be created:or something similar, depending on your distro and python/pip defaults.

How to run Ansible without specifying the inventory but the host directly?

Ngoc Tran

[How to run Ansible without specifying the inventory but the host directly?](https://stackoverflow.com/questions/17188147/how-to-run-ansible-without-specifying-the-inventory-but-the-host-directly)

I want to run Ansible in Python without specifying the inventory file through (ANSIBLE_HOST) but just by:I can actually do this in fabric easily but just wonder how to do this in Python. On the other hand, documentation of the Ansible API for python is not really complete.

2013-06-19 09:50:23Z

I want to run Ansible in Python without specifying the inventory file through (ANSIBLE_HOST) but just by:I can actually do this in fabric easily but just wonder how to do this in Python. On the other hand, documentation of the Ansible API for python is not really complete.Surprisingly, the trick is to append a ,orThe host parameter preceding the , can be either a hostname or an IPv4/v6 address.I know this question is really old but think that this little trick might helpful for future users who need help for this:if you run for local host:The trick is that after ip address/dns name, put the comma inside the quotes and  requires 'hosts: all' in your playbook.Hope this will help. You can do this with:Whatever is in hosts becomes your inventory and you can search it with pattern (or do "all").In my case, I did not want to have hosts: all in my playbook, because it would be bad if someone ran the playbook and forgot to include -i 10.254.3.133,This was my solution (ansible 2.6):And then, in the playbook:This is a special use-case when I need to provision a host and I don't want/need to add it to the inventory.I also needed to drive the Ansible Python API, and would rather pass hosts as arguments rather than keep an inventory. I used a temporary file to get around Ansible's requirement, which may be helpful to others:This isn't a full answer, but there's some discussion of this topic in this discussion thread. At the end of the first post in that thread, a suggestion is made to create a wrapper bash script for ansible-playbook, which is a bit of a hack but workable.Other things that I've been considering are the use of 'ansible-pull' and the creation of an ansible inventory plugin. I'm also interested in finding the answer to this question, and I'll keep updating this answer as I find more information.There seems to be not direct way to give a pattern. This is my hack to solve it.

pandas dataframe groupby datetime month

atomh33ls

[pandas dataframe groupby datetime month](https://stackoverflow.com/questions/24082784/pandas-dataframe-groupby-datetime-month)

Consider a csv file:I can read this in, and reformat the date column into datetime format:I have been trying to group the data by month. It seems like there should be an obvious way of accessing the month and grouping by that. But I can't seem to do it. Does anyone know how?What I am currently trying is re-indexing by the date:I can access the month like so:However I can't seem to find a function to lump together by month.

2014-06-06 13:15:41Z

Consider a csv file:I can read this in, and reformat the date column into datetime format:I have been trying to group the data by month. It seems like there should be an obvious way of accessing the month and grouping by that. But I can't seem to do it. Does anyone know how?What I am currently trying is re-indexing by the date:I can access the month like so:However I can't seem to find a function to lump together by month.Managed to do it:Or (update: 2018)Note that pd.Timegrouper is depreciated and will be removed. Use instead:One solution which avoids MultiIndex is to create a new datetime column setting day = 1. Then group by this column. Trivial example below.The subtle benefit of this solution is, unlike pd.Grouper, the grouper index is normalized to the beginning of each month rather than the end, and therefore you can easily extract groups via get_group:Calculating the last day of October is slightly more cumbersome. pd.Grouper, as of v0.23, does support a convention parameter, but this is only applicable for a PeriodIndex grouper.Slightly alternative solution to @jpp's but outputting a YearMonth string:

How to remove punctuation marks from a string in Python 3.x using .translate()?

cybujan

[How to remove punctuation marks from a string in Python 3.x using .translate()?](https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate)

I want to remove all punctuation marks from a text file using .translate() method. It seems to work well under Python 2.x but under Python 3.4 it doesn't seem to do anything. My code is as follows and the output is the same as input text.

2015-12-15 16:05:03Z

I want to remove all punctuation marks from a text file using .translate() method. It seems to work well under Python 2.x but under Python 3.4 it doesn't seem to do anything. My code is as follows and the output is the same as input text.You have to create a translation table using maketrans that you pass to the str.translate method.In Python 3.1 and newer, maketrans is now a static-method on the str type, so you can use it to create a translation of each punctuation you want to None.This should output: The call signature of str.translate has changed and apparently the parameter deletechars has been removed. You could useinstead, or create a table as shown in the other answer.In python3.x ,it can be done using :I just compared the three methods by speed. translate is slower than re.sub (with precomilation) in about 10 times. And str.replace is faster than re.sub in about 3 times. By str.replace I mean:

scipy: savefig without frames, axes, only content

Jakub M.

[scipy: savefig without frames, axes, only content](https://stackoverflow.com/questions/8218608/scipy-savefig-without-frames-axes-only-content)

In numpy/scipy I have an image stored in an array. I can display it, I want to save it using savefig without any borders, axes, labels, titles,... Just pure image, nothing else.I want to avoid packages like PyPNG or scipy.misc.imsave, they are sometimes problematic (they do not always install well, only basic savefig() for me

2011-11-21 21:13:47Z

In numpy/scipy I have an image stored in an array. I can display it, I want to save it using savefig without any borders, axes, labels, titles,... Just pure image, nothing else.I want to avoid packages like PyPNG or scipy.misc.imsave, they are sometimes problematic (they do not always install well, only basic savefig() for meEDITChanged aspect='normal to aspect='auto' since that changed in more recent versions of matplotlib (thanks to @Luke19).Assuming : To make a figure without the frame :To make the content fill the whole figure Then draw your image on it :The aspect parameter changes the pixel size to make sure they fill the figure size specified in fig.set_size_inches(…). To get a feel of how to play with this sort of things, read through matplotlib's documentation, particularly on the subject of Axes, Axis and Artist.An easier solution seems to be:You can find the bbox of the image inside the axis (using get_window_extent), and use the bbox_inches parameter to save only that portion of the image:I learned this trick from Joe Kington here.I've tried several options in my case, and the best solution was this:then save your figure with savefigI will suggest heron13 answer with a slight addition borrowed from here to remove the padding left after setting the bbox to tight mode, therefore:This one work for meI had the same problem while doing some visualization using librosa where I wanted to extract content of the plot without any other information. So this my approach. unutbu answer also helps me to make to work.While the above answers address removing margins and padding, they did not work for me in removing labels. Here's what worked, for anyone who stumbles upon this question later:Assuming you want a 2x2 grid of subplots from four images stored in images:For anybody trying to do this in JupyterI tried to get rid of the border too, using tips here but nothing really worked.

Some fiddling about and I found that changing the faceolor gave me no border in jupyter labs (Any color resulted in getting rid of the white border). 

Hope this helps.

Easiest way to read/write a file's content in Python

ibz

[Easiest way to read/write a file's content in Python](https://stackoverflow.com/questions/3758147/easiest-way-to-read-write-a-files-content-in-python)

In Ruby you can read from a file using s = File.read(filename). The shortest and clearest I know in Python isIs there any other way to do it that makes it even shorter (preferably one line) and more readable?Note: initially I phrased the question as "doing this in a single line of code". As pointed by S.Lott, shorter doesn't necessary mean more readable. So I rephrased my question just to make clear what I meant. I think the Ruby code is better and more readable not necessarily because it's one line versus two (though that matters as well), but also because it's a class method as opposed to an instance method, which poses no question about who closes the file, how to make sure it gets closed even if an exception is raised, etc. As pointed in the answers below, you can rely on the GC to close your file (thus making this a one-liner), but that makes the code worse even though it's shorter. Not only by being unportable, but by making it unclear.

2010-09-21 07:29:50Z

In Ruby you can read from a file using s = File.read(filename). The shortest and clearest I know in Python isIs there any other way to do it that makes it even shorter (preferably one line) and more readable?Note: initially I phrased the question as "doing this in a single line of code". As pointed by S.Lott, shorter doesn't necessary mean more readable. So I rephrased my question just to make clear what I meant. I think the Ruby code is better and more readable not necessarily because it's one line versus two (though that matters as well), but also because it's a class method as opposed to an instance method, which poses no question about who closes the file, how to make sure it gets closed even if an exception is raised, etc. As pointed in the answers below, you can rely on the GC to close your file (thus making this a one-liner), but that makes the code worse even though it's shorter. Not only by being unportable, but by making it unclear.If you're open to using libraries, try installing forked-path (with either easy_install or pip).Then you can do:This library is fairly new, but it's a fork of a library that's been floating around Python for years and has been used quite a bit. Since I found this library years ago, I very seldom use os.path or open() any more.***grins***This is same as above but does not handle errors:Use pathlib.Python 3.5 and above:For lower versions of Python use pathlib2:ThenWriting is just as easy:This isn't Perl; you don't want to force-fit multiple lines worth of code onto a single line.  Write a function, then calling the function takes one line of code.Slow, ugly, platform-specific... but one-liner ;-)Simple like that:And do whatever you want with the content "s"This gives you generator so you must save somewhere the values though, or This does the saving to list explicit close is not then possible (at least with my knowledge of Python).

How to find tags with only certain attributes - BeautifulSoup

Snaxib

[How to find tags with only certain attributes - BeautifulSoup](https://stackoverflow.com/questions/8933863/how-to-find-tags-with-only-certain-attributes-beautifulsoup)

How would I, using BeautifulSoup, search for tags containing ONLY the attributes I search for? For example, I want to find all <td valign="top"> tags. The following code:

raw_card_data = soup.fetch('td', {'valign':re.compile('top')})gets all of the data I want, but also grabs any <td> tag that has the attribute valign:top I also tried:

raw_card_data = soup.findAll(re.compile('<td valign="top">'))

and this returns nothing (probably because of bad regex)I was wondering if there was a way in BeautifulSoup to say "Find <td> tags whose only attribute is valign:top"UPDATE

FOr example, if an HTML document contained the following <td> tags:I would want only the first <td> tag (<td width="580" valign="top">) to return

2012-01-19 21:50:05Z

How would I, using BeautifulSoup, search for tags containing ONLY the attributes I search for? For example, I want to find all <td valign="top"> tags. The following code:

raw_card_data = soup.fetch('td', {'valign':re.compile('top')})gets all of the data I want, but also grabs any <td> tag that has the attribute valign:top I also tried:

raw_card_data = soup.findAll(re.compile('<td valign="top">'))

and this returns nothing (probably because of bad regex)I was wondering if there was a way in BeautifulSoup to say "Find <td> tags whose only attribute is valign:top"UPDATE

FOr example, if an HTML document contained the following <td> tags:I would want only the first <td> tag (<td width="580" valign="top">) to returnAs explained on the BeutifulSoup documentationYou may use this :EDIT :To return tags that have only the valign="top" attribute, you can check for the length of the tag attrs property :That returns :You can use lambda functions in findAll as explained in documentation. So that in your case to search for td tag with only valign = "top" use following:if you want to only search with attribute name with any valueas per Steve Lorimer better to pass True instead of regexThe easiest way to do this is with the new CSS style select method:Just pass it as an argument of findAll:Adding a combination of Chris Redford's and Amr's answer, you can also search for an attribute name with any value with the select command:

Python: try statement in a single line

Brant

[Python: try statement in a single line](https://stackoverflow.com/questions/2524853/python-try-statement-in-a-single-line)

Is there a way in python to turn a try/except into a single line?something like...Where b is a declared variable and c is not... so c would throw an error and a would become b...

2010-03-26 16:21:19Z

Is there a way in python to turn a try/except into a single line?something like...Where b is a declared variable and c is not... so c would throw an error and a would become b...There is no way to compress a try/except block onto a single line in Python.Also, it is a bad thing not to know whether a variable exists in Python, like you would in some other dynamic languages. The safer way (and the prevailing style) is to set all variables to something. If they might not get set, set them to None first (or 0 or '' or something if it is more applicable.)If you do assign all the names you are interested in first, you do have options. This is terribly hackish, but I've used it at the prompt when I wanted to write up a sequence of actions for debugging:For the most part, I'm not at all bothered by the no-single-line-try-except restriction, but when I'm just experimenting and I want readline to recall a whole chunk of code at once in the interactive interpreter so that I can adjust it somehow, this little trick comes in handy.For the actual purpose you are trying to accomplish, you might try locals().get('c', b); ideally it would be better to use a real dictionary instead of the local context, or just assign c to None before running whatever may-or-may-not set it.In python3 you can use contextlib.suppress:Another way is to define a context manager:Then use the with statement to ignore errors in one single line:No exception will be raised in case of a runtime error. It's like a try: without the except:.There is always a solution.You can do it by accessing the namespace dict using vars(), locals(), or globals(), whichever is most appropriate for your situation.Use something like this:Where try_or is an utility function defined by you:Optionally you can restrict the accepted exception types to NameError, AttributeError, etc.Version of poke53280 answer with limited expected exceptions.and it could be used asYou mentioned that you're using django. If it makes sense for what you're doing you might want to use:created will be True or False. Maybe this will help you.if you need to actually manage exceptions:

(modified from poke53280's answer)note that if the exception is not supported, it will raise as expected:also if Exception is given, it will match anything below.

(BaseException is higher, so it will not match)Works on Python3, inspired by Walter MundtFor mulitiples lines into one linePs: Exec is unsafe to use on data you don't have control over.

How can I check if two segments intersect?

aneuryzm

[How can I check if two segments intersect?](https://stackoverflow.com/questions/3838329/how-can-i-check-if-two-segments-intersect)

How can I check if 2 segments intersect?I've the following data:I need to write a small algorithm in Python to detect if the 2 lines are intersecting.

2010-10-01 10:24:41Z

How can I check if 2 segments intersect?I've the following data:I need to write a small algorithm in Python to detect if the 2 lines are intersecting.The equation of a line is:For a segment, it is exactly the same, except that x is included on an interval I.If you have two segments, defined as follow:The abcisse Xa of the potential point of intersection (Xa,Ya) must be contained in both interval I1 and I2, defined as follow :And we could say that Xa is included into :Now, we need to check that this interval Ia exists :So, we have two line formula, and a mutual interval. Your line formulas are:As we got two points by segment, we are able to determine A1, A2, b1 and b2:If the segments are parallel, then A1 == A2 :A point (Xa,Ya) standing on both line must verify both formulas f1 and f2:The last thing to do is check that Xa is included into Ia:In addition to this, you may check at startup that two of the four provided points are not equals to avoid all that testing.User @i_4_got points to this page with a very efficent solution in Python. I reproduce it here for convenience (since it would have made me happy to have it here):You don't have to compute exactly where does the segments intersect, but only understand whether they intersect at all. This will simplify the solution.The idea is to treat one segment as the "anchor" and separate the second segment into 2 points.

Now, you will have to find the relative position of each point to the "anchored" segment (OnLeft, OnRight or Collinear).

After doing so for both points, check that one of the points is OnLeft and the other is OnRight (or perhaps include Collinear position, if you wish to include improper intersections as well).You must then repeat the process with the roles of anchor and separated segments.An intersection exists if, and only if, one of the points is OnLeft and the other is OnRight. See this link for a more detailed explanation with example images for each possible case.Implementing such method will be much easier than actually implementing a method that finds the intersection point (given the many corner cases which you will have to handle as well).UpdateThe following functions should illustrate the idea (source: Computational Geometry in C).

Remark: This sample assumes the usage of integers. If you're using some floating-point representation instead (which could obviously complicate things), then you should determine some epsilon value to indicate "equality" (mostly for the IsCollinear evaluation).Of course, when using these functions, one must remember to check that each segment lies "between" the other segment (since these are finite segments, and not infinite lines).  Also, using these functions you can understand whether you've got a proper or improper intersection.Suppose the two segments have endpoints A,B and C,D. The numerically robust way to determine intersection is to check the sign of the four determinants:For intersection, each determinant on the left must have the opposite sign of the one to the right, but there need not be any relationship between the two lines. You are basically checking each point of a segment against the other segment to make sure they lie on opposite sides of the line defined by the other segment.See here: http://www.cs.cmu.edu/~quake/robust.htmlBased on Liran's and Grumdrig's excellent answers here is a complete Python code to verify if closed segments do intersect. Works for collinear segments, segments parallel to axis Y, degenerate segments (devil is in details). Assumes integer coordinates. Floating point coordinates require a modification to points equality test.You have two line segments. Define one segment by endpoints A & B and the second segment by endpoints C & D. There is a nice trick to show that they must intersect, WITHIN the bounds of the segments. (Note that the lines themselves may intersect beyond the bounds of the segments, so you must be careful. Good code will also watch for parallel lines.)The trick is to test that points A and B must line on opposite sides of line CD, AND that points C and D must lie on opposite sides of line AB.Since this is homework, I won't give you an explicit solution. But a simple test to see which side of a line a point falls on, is to use a dot product. Thus, for a given line CD, compute the normal vector to that line (I'll call it N_C.) Now, simply test the signs of these two results:andIf those results have opposite signs, then A and B are opposite sides of line CD. Now do the same test for the other line, AB. It has normal vector N_A. Compare the signs ofandI'll leave it to you to figure out how to compute a normal vector. (In 2-d, that is trivial, but will your code worry about whether A and B are distinct points? Likewise, are C and D distinct?)You still need to worry about line segments that lie along the same infinite line, or if one point actually falls on the other line segment itself. Good code will cater to every possible problem.Here is C code to check if two points are on the opposite sides of the line segment. Using this code you can check if two segments intersect as well.}Here is another python code to check whether closed segments intersect. It is the rewritten version of the C++ code in http://www.cdn.geeksforgeeks.org/check-if-two-given-line-segments-intersect/. This implementation covers all special cases (e.g. all points colinear).Below is a test function to verify that it works.Checking if line segments intersect is very easy with Shapely library using intersects method:for segments AB and CD, find the slope of CDextend CD over A and B, and take the distance to CD going straight upcheck if they are on opposite sidesSince you do not mention that you want to find the intersection point of the line, the problem becomes simpler to solve. If you need the intersection point, then the answer by OMG_peanuts is a faster approach. However, if you just want to find whether the lines intersect or not, you can do so by using the line equation (ax + by + c = 0). The approach is as follows:We can also solve this utilizing vectors. Let's define the segments as [start, end]. Given two such segments [A, B] and [C, D] that both have non-zero length, we can choose one of the endpoints to be used as a reference point so that we get three vectors:From there, we can look for an intersection by calculating t and u in p + t*r = u*q. After playing around with the equation a little, we get:Thus, the function is:if your data define line you just have to prove that they are not parallel. To do this you can compute  If this coefficient is equal for both Line1 and Line2, it means the line are parallel. If not, it means they will intersect. If they are parallel you then have to prove that they are not the same. For that, you compute   If beta is the same for Line1 and Line2,it means you line intersect as they are equalIf they are segment, you still have to compute alpha and beta as described above for each Line. Then you have to check that (beta1 - beta2) / (alpha1 - alpha2) is greater than Min(x1_line1, x2_line1) and less than Max(x1_line1, x2_line1)Calculate the intersection point of the lines laying on your segments (it means basically to solve a linear equation system), then check whether is it between the starting and ending points of your segments.This is what I've got for AS3, don't know much about python but the concept is thereImplemented in JAVA.  However It seems that it does not work for co-linear lines (aka line segments that exist within each other L1(0,0)(10,10) L2(1,1)(2,2)Output thus far is I thought I'd contribute a nice Swift solution:This is my way of checking for line crossing and where the intersection occurs.  Lets use x1 through x4 and y1 through y4Then we need some vectors to represent themNow we look at the determinantIf the determinant is 0.0, then the line segments are parallel.  This could mean they overlap.  If they overlap just at endpoints, then there is one intersection solution.  Otherwise there will be infinite solutions.  With infinitely many solutions, what do say is your point of intersection?  So it's an interesting special case.  If you know ahead of time that the lines can't overlap then you can just check if det == 0.0 and if so just say they don't intersect and be done.  Otherwise, lets continue onNow, if det, det1 and det2 are all zero, then your lines are co-linear and could overlap.  If det is zero but either det1 or det2 are not, then they are not co-linear, but are parallel, so there is no intersection.  So what's left now if det is zero is a 1D problem instead of 2D.  We will need to check one of two ways, depending if dx1 is zero or not (so we can avoid division by zero).  If dx1 is zero then just do the same logic with y values rather than x below.This computes two scalers, such that if we scale the vector (dx1, dy1) by s we get point (x3, y3), and by t we get (x4, y4).  So if either s or t is between 0.0 and 1.0, then point 3 or 4 lies on our first line.  Negative would mean the point is behind the start of our vector, while > 1.0 means it is further ahead of the end of our vector.  0.0 means it is at (x1, y1) and 1.0 means it is at (x2, y2).  If both s and t are < 0.0 or both are > 1.0, then they don't intersect.  And that handles the parallel lines special case.Now, if det != 0.0 thenThis is similar to what we were doing above really.  Now if we pass the above test, then our line segments intersect, and we can calculate the intersection quite easily like so:If you want to dig deeper into what the math is doing, look into Cramer's Rule.One of the solutions above worked so well I decided to write a complete demonstration program using wxPython. You should be able to run this program like this: python "your file name"Resolved but still why not with python... :)This:Output:And this:Output:

What’s the best way to get an HTTP response code from a URL?

Gourneau

[What’s the best way to get an HTTP response code from a URL?](https://stackoverflow.com/questions/1140661/what-s-the-best-way-to-get-an-http-response-code-from-a-url)

I’m looking for a quick way to get an HTTP response code from a URL (i.e. 200, 404, etc).  I’m not sure which library to use.

2009-07-16 22:27:54Z

I’m looking for a quick way to get an HTTP response code from a URL (i.e. 200, 404, etc).  I’m not sure which library to use.Update using the wonderful requests library. Note we are using the HEAD request, which should happen more quickly then a full GET or POST request. Here's a solution that uses httplib instead.You should use urllib2, like this:In future, for those that use python3 and later, here's another code to find response code.The urllib2.HTTPError exception does not contain a getcode() method. Use the code attribute instead.Here's an httplib solution that behaves like urllib2.  You can just give it a URL and it just works.  No need to mess about splitting up your URLs into hostname and path.  This function already does that.Addressing @Niklas R's comment to @nickanor's answer:

Convert image from PIL to openCV format

md1hunox

[Convert image from PIL to openCV format](https://stackoverflow.com/questions/14134892/convert-image-from-pil-to-opencv-format)

I'm trying to convert image from PIL to OpenCV format. I'm using OpenCV 2.4.3.

here is what I've attempted till now.But I think the image is not getting converted to CV format. The Window shows me a large brown image.

Where am I going wrong in Converting image from PIL to CV format?Also , why do i need to type cv.cv to access functions?

2013-01-03 07:44:03Z

I'm trying to convert image from PIL to OpenCV format. I'm using OpenCV 2.4.3.

here is what I've attempted till now.But I think the image is not getting converted to CV format. The Window shows me a large brown image.

Where am I going wrong in Converting image from PIL to CV format?Also , why do i need to type cv.cv to access functions?use this: This is the shortest version I could find,saving/hiding an extra conversion:If reading a file from a URL:

How to measure time taken between lines of code in python?

alvas

[How to measure time taken between lines of code in python?](https://stackoverflow.com/questions/14452145/how-to-measure-time-taken-between-lines-of-code-in-python)

So in Java, we can do How to measure time taken by a function to executeBut how is it done in python? To measure the time start and end time between lines of codes?

Something that does this:

2013-01-22 05:31:40Z

So in Java, we can do How to measure time taken by a function to executeBut how is it done in python? To measure the time start and end time between lines of codes?

Something that does this:If you want to measure CPU time, can use time.process_time() for Python 3.3 and above:First call turns the timer on, and second call tells you how many seconds have elapsed.There is also a function time.clock(), but it is deprecated since Python 3.3 and will be removed in Python 3.8.There are better profiling tools like timeit and profile, however this one will measure the CPU time and this is what you're are asking about.If you want to measure wall clock time instead, use time.time().You can also use time library:With a help of a small convenience class, you can measure time spent in indented lines like this:Which will show the following after the indented line(s) finishes executing:UPDATE: You can now get the class with pip install linetimer and then from linetimer import CodeTimer. See this GitHub project.The code for above class:You could then name the code blocks you want to measure:And nest them:Regarding timeit.default_timer(), it uses the best timer based on OS and Python version, see this answer.I always prefer to check time in hours, minutes and seconds (%H:%M:%S) format:output: I was looking for a way how to output a formatted time with minimal code, so here is my solution. Many people use Pandas anyway, so in some cases this can save from additional library imports.Output:I would recommend using this if time precision is not the most important, otherwise use time library:%timeit pd.Timestamp.now() outputs 3.29 µs ± 214 ns per loop%timeit time.time() outputs 154 ns ± 13.3 ns per loop

「Cannot open include file: 'config-win.h': No such file or directory」while installing mysql-python

saturdayplace

[「Cannot open include file: 'config-win.h': No such file or directory」while installing mysql-python](https://stackoverflow.com/questions/1972259/cannot-open-include-file-config-win-h-no-such-file-or-directory-while-inst)

I'm trying to install mysql-python in a virtualenv using pip on windows.  At first, I was getting the same error reported here, but the answer there worked for me too.  Now I'm getting this following error:If I symlink (Win7) to my regular (not the virtualenv's) python's site-packages/MySQLdb dir I get I'm rather at a loss here.  Any pointers?

2009-12-29 00:13:07Z

I'm trying to install mysql-python in a virtualenv using pip on windows.  At first, I was getting the same error reported here, but the answer there worked for me too.  Now I'm getting this following error:If I symlink (Win7) to my regular (not the virtualenv's) python's site-packages/MySQLdb dir I get I'm rather at a loss here.  Any pointers?Update for mysql 5.5 and config-win.h not visible issueIn 5.5 config-win. has actually moved to Connector separate folder in windows. i.e. smth like:C:\Program Files\MySQL\Connector C 6.0.2\includeTo overcome the problem one need not only to download "dev bits" (which actually connects the connector) but also to modify mysqldb install scripts to add the include folder. I've done a quick dirty fix as that. site.cfg:in setup_windows.py locate the line and add:after it.Ugly but works until mysqldb authors will change the behaviour.Almost forgot to mention. In the same manner one needs to add similar additional entry for libs:i.e. your setup_windows.py looks pretty much like:All I had to do was go over to oracle, and download the MySQL Connector C 6.0.2 (newer doesn't work!) and do the typical install.https://downloads.mysql.com/archives/c-c/Be sure to include all optional extras (Extra Binaries) via the custom install, without these it did not work for the win64.msiOnce that was done, I went into pycharms, and selected the MySQL-python>=1.2.4 package to install, and it worked great.  No need to update any configuration or anything like that.  This was the simplest version for me to work through.Hope it helpsThe accepted solution no longer seems to work for newer versions of mysql-python. The installer no longer provides a site.cfg file to edit.If you are installing mysql-python it'll look for C:\Program Files (x86)\MySQL\MySQL Connector C 6.0.2\include. If you have a 64-bit installation of MySQL, you can simply invoke:The accepted answer is out of date. Some of the suggestions were already incorporated in the package, and I was still getting the error about missing config-win.h & mysqlclient.lib.P.S. Since I don't use MySQL anymore, my answer may be out of date as well. I know this post is super old, but it is still coming up as the top hit in google so I will add some more info to this issue. I was having the same problems as OP but none of the suggested answers seemed to work for me. Mainly because "config-win.h" didn't exist anywhere in the connector install folder. I was using the latest Connector C 6.1.6 as that was what was suggested by the MySQL installer. This however doesn't seem to be supported by the latest MySQL-python package (1.2.5). When trying to install it I could see that it was explicitly looking for C Connector 6.0.2. So by installing this version from https://dev.mysql.com/downloads/file/?id=378015 the python package installed without any problem.Most probably the answer is to install MySQL Developer Build and selecting "C headers\libs" option during configuration. (as reported in this entry: Building MySQLdb for Python on Windows on rationalpie.wordpress.com)Maybe even better solution is to install a precompiled build: http://www.technicalbard.com/files/MySQL-python-1.2.2.win32-py2.6.exeIf pip fails to install "MySQLdb", a workaround is to download and install it on your machine first from this linkhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-pythonthen copy all MySQL* and _mysql* files and directories from your system Python to your Virtualenv dir:c:\Python27\Lib\site-packages  (or similar path to your system Python) to

c:\my_virtenv\Lib\site-packages (path to your virtualenv)Well, if you are still having the problem, you can download the installer from http://code.google.com/p/soemin/downloads/detail?name=MySQL-python-1.2.3.win32-py2.7.exeI had a lot of headache with MySQLdb too.Why not use the official MysQL Python Connector?Or you can download it from here:

http://dev.mysql.com/downloads/connector/python/Documentation:

http://dev.mysql.com/doc/refman/5.5/en/connector-python.htmlInstalling dev bits for mysql got rid of the config-win.h error I was having, and threw another. Failed to load and parse the manifest. The system cannot find the file specified. I found the answer to my problem in this post: http://www.fuyun.org/2009/12/install-mysql-for-python-on-windows/.I copied the file 'C:\Python26\Lib\distutils\msvc9compiler.py` into my virtualenv, made the edit suggested in the above link, and things are working fine.Simplest working solution:Download the MySQL Connector C 6.0.2 from below link and Install.http://dev.mysql.com/downloads/connector/c/6.0.html#downloadsAfter installing the MySQL Connector C 6.0.2, copy the folder "MySQL Connector C 6.0.2" from "C:\Program Files\MySQL" to "C:\Program Files (x86)\MySQL".Then typeIt will definitely work.In my case, my fix was copying the folder created from mysql-connector-c-6.0.2-win32.msi (referenced from username buffer in a previous post), which is located at c:\Program Files\MySQL\MySQL Connector C 6.0.2 and creating a new path with Program Files (x86) and pasting the content there since the installation is not properly checking between 32-bit and 64-bit machines.  So, the new path is C:\Program Files (x86)\MySQL\MySQL Connector C 6.0.2.  This is the path that the installer is looking to find, so I pasted the files there to help the installer to find the files, which was causing the error about missing config-win.hTry ActivePython,I did follow the answer from Bugagotti, And it does not work in my windows (Win7 64 bit, py27 and have mysql connector 6.1 installed) for mysql-python-1.2.5, so I made some even dirty changes inside mysql-python-1.2.5:First, the site.cfg:Second, the _mysql.c :To:And with these changes ,the config_win.h issue will gone, but there is still a link issue:For this, I changed the setup_windows.py:Then it worked finally.I followed Mingcai SHEN's method.But in my case, I changed the connector toAnd the library_dirs is changed to because I don't have a vs9 directory. It works, but I don't know why.I have vs2012 installed, and the lib directory of the connector only has vs10 and vs11, in which vs11 doesn't work. The VCForPyhton27.mis I installed seems to support vs9.Anyway, this works. And if you want to risk it, you can try.Solution that worked for me on Windows: Install both the 32-bit and 64-bit versions of the  MySQL Connector/C 6.0.2. Open Command Prompt and run:For me the following approach solved the issue (Python 3.5.2; mysqlclient 1.3.9):The issue here is only for x64 bit installation owners, since build script is trying to locate C connector includes in x86 program files directory.If you see this when you try pip install mysql-python, the easiest way is to copy C:\Program Files\MySQL\MySQL Connector C 6.0.2

to C:\Program Files (x86)\MySQL\MySQL Connector C 6.0.2I tried to create the symbolic link but Windows keeps throwing me

C:\WINDOWS\system32>mklink /d "C:\Program Files (x86)\MySQL\MySQL Connector C 6.0.2\include" "C:\Program Files\MySQL\MySQL Connector C 6.0.2\include"

The system cannot find the path specified.If you are doing this in a virtual environment whether using Visual Studio or otherwise, try 

easy_install MySQL-python Download the version of Connector from https://dev.mysql.com/downloads/connector/c/6.0.html

For my case I had installed 64 bit of connector and my python was 32 bit. So I had to copy MySQL from program files to Program Files(86)Steps for Window10:

How to hide *pyc files in atom editor

andilabs

[How to hide *pyc files in atom editor](https://stackoverflow.com/questions/24079976/how-to-hide-pyc-files-in-atom-editor)

Started using Atom for Python/Django development and would like to hide all the *.pyc files from sidebar. How to configure it?

2014-06-06 10:47:35Z

Started using Atom for Python/Django development and would like to hide all the *.pyc files from sidebar. How to configure it?The method for hiding files that you do not want showing up in the Tree View (which is what most people mean when they ask this question) depends on whether or not you've added the files to your .gitignore. If you have, then all you have to do is:If you want to hide certain files in the Tree View whether you have a Git project open or not:Also note that when you add a file mask to the list of Ignored Names that files matching that mask will not show up in other parts of Atom like the fuzzy-finder:find-file (Cmd+T on OS X and Ctrl+T on Windows/Linux by default) command.I found this where it is said that you can toggle that in Preferences->Tree View->Hide Ignored Names and Hide Vcs Ignored Files.Edit: The files to hide you have to specify first in Preferences->Settings->Core Settings->Ignored Names. This was described here.Let me know if it works.In Atom 1.25.0 on Ubuntu I found this setting Hide Vcs Ignored Files under packages configuration: Preferences -> Packages -> Tree View -> Settings.Also, pressing I while Tree View activated also works for me (@Suraj Thapar's comment).Please note, you have 2 options here: Ignore file by .gitignore and ignore files by Hide Ignored Names which refers to Ignored Names core config settings.

How to read one single line of csv data in Python?

andrebruton

[How to read one single line of csv data in Python?](https://stackoverflow.com/questions/17262256/how-to-read-one-single-line-of-csv-data-in-python)

There is a lot of examples of reading csv data using python, like this one:I only want to read one line of data and enter it into various variables. How do I do that? I've looked everywhere for a working example.My code only retrieves the value for i, and none of the other values

2013-06-23 15:25:30Z

There is a lot of examples of reading csv data using python, like this one:I only want to read one line of data and enter it into various variables. How do I do that? I've looked everywhere for a working example.My code only retrieves the value for i, and none of the other valuesTo read only the first row of the csv file use next() on the reader object.or :you could get just the first row like:You can use Pandas library to read the first few lines from the huge dataset.You can mention the number of lines to be read in the nrows parameter.From the Python documentation:Just drop your string data into a singleton list.The simple way to get any row in csv fileJust for reference, a for loop can be used after getting the first row to get the rest of the file:

Django Rest Framework Token Authentication

Prometheus

[Django Rest Framework Token Authentication](https://stackoverflow.com/questions/14838128/django-rest-framework-token-authentication)

I have read the Django Rest Framework Guides and done all the tutorials. Everything seemed to make sense and work just how it should. I got basic and session authentication working as described.

http://django-rest-framework.org/api-guideHowever, I'm struggling with the Token Authentication part of the documentation, its a little lacking or does not go into as much depth as the tutorials.

http://django-rest-framework.org/api-guide/authentication/#tokenauthenticationIt says I need to create tokens for users but does state where, in models.py?My question is:Can someone explain the Token Authentication part of the documentation a little better for a first timer?

2013-02-12 17:20:14Z

I have read the Django Rest Framework Guides and done all the tutorials. Everything seemed to make sense and work just how it should. I got basic and session authentication working as described.

http://django-rest-framework.org/api-guideHowever, I'm struggling with the Token Authentication part of the documentation, its a little lacking or does not go into as much depth as the tutorials.

http://django-rest-framework.org/api-guide/authentication/#tokenauthenticationIt says I need to create tokens for users but does state where, in models.py?My question is:Can someone explain the Token Authentication part of the documentation a little better for a first timer?No, not in your models.py -- on the models side of things, all you need to do is include the appropriate app (rest_framework.authtoken) in your INSTALLED_APPS. That will provide a Token model which is foreign-keyed to User.What you need to do is decide when and how those token objects should be created. In your app, does every user automatically get a token? Or only certain authorized users? Or only when they specifically request one?If every user should always have a token, there is a snippet of code on the page you linked to that shows you how to set up a signal to create them automatically:(put this in a models.py file, anywhere, and it will be registered when a Django thread starts up)If tokens should only be created at certain times, then in your view code, you need to create and save the token at the appropriate time:Once the token is created (and saved), it will be usable for authentication.@ian-clelland has already provided the correct answer. There are just a few tiny pieces that wasn't mentioned in his post, so I am going to document the full procedures (I am using Django 1.8.5 and DRF 3.2.4):That's it!On Django 1.8.2 and rest framework 3.3.2 following all of the above was not enough to enable token based authentication.Although REST_FRAMEWORK setting is specified in django settings file, function based views required @api_view decorator:Otherwise no token authentication is performed at allJust to add my two cents to this, if you've got a custom user manager that handles user creation (and activation), you may also perform this task like so:If you already have users created, then you may drop down into the python shell in your terminal and create Tokens for all the users in your db. Hope that helps. There is a cleaner way to get the user token.simply run manage.py shelland thenthen a record should be found in table DB_Schema.authtoken_tokenIn addition to the excellent answers here, I'd like to mention a better approach to token authentication: JSON Web Token Authentication. The implementation offered by http://getblimp.github.io/django-rest-framework-jwt/ is very easy to use.The benefits are explained in more detail in this answer.JSON Web Token Authentication is a better alternative than Token Authentication. This project has implemented JWT Auth with Django (http://getblimp.github.io/django-rest-framework-jwt/) but currently the project is unmaintained.For alternatives you can follow :  https://github.com/davesque/django-rest-framework-simplejwt

Listing available com ports with Python

doom

[Listing available com ports with Python](https://stackoverflow.com/questions/12090503/listing-available-com-ports-with-python)

I am searching for a simple method to list all available com port on a PC.I have found this method but it is Windows-specific: Listing serial (COM) ports on Windows?I am using Python 3 with pySerial on a Windows 7 PC.I have found in the pySerial API (http://pyserial.sourceforge.net/pyserial_api.html) a function serial.tools.list_ports.comports() that lists com ports (exactly what I want).But it seems that it doesn't work. When my USB to COM gateway is connected to the PC (I see the COM5 in the Device Manager), this COM port isn't included in the list returned by list_ports.comports(). Instead I only get COM4 which seems to be connected to a modem (I don't see it in the COM&LPT section of Device Manager)!Do you know why it doesn't work? Have you got another solution which is not system specific?

2012-08-23 11:25:30Z

I am searching for a simple method to list all available com port on a PC.I have found this method but it is Windows-specific: Listing serial (COM) ports on Windows?I am using Python 3 with pySerial on a Windows 7 PC.I have found in the pySerial API (http://pyserial.sourceforge.net/pyserial_api.html) a function serial.tools.list_ports.comports() that lists com ports (exactly what I want).But it seems that it doesn't work. When my USB to COM gateway is connected to the PC (I see the COM5 in the Device Manager), this COM port isn't included in the list returned by list_ports.comports(). Instead I only get COM4 which seems to be connected to a modem (I don't see it in the COM&LPT section of Device Manager)!Do you know why it doesn't work? Have you got another solution which is not system specific?This is the code I use.Successfully tested on Windows 8.1 x64, Windows 10 x64, Mac OS X 10.9.x / 10.10.x / 10.11.x and Ubuntu 14.04 / 14.10 / 15.04 / 15.10 with both Python 2 and Python 3.You can use:python -c "import serial.tools.list_ports;print serial.tools.list_ports.comports()"Filter by know port:

python -c "import serial.tools.list_ports;print [port for port in serial.tools.list_ports.comports() if port[2] != 'n/a']"See more info here:

https://pyserial.readthedocs.org/en/latest/tools.html#module-serial.tools.list_portsA possible refinement to Thomas's excellent answer is to have Linux and possibly OSX also try to open ports and return only those which could be opened. This is because Linux, at least, lists a boatload of ports as files in /dev/ which aren't connected to anything. If you're running in a terminal, /dev/tty is the terminal in which you're working and opening and closing it can goof up your command line, so the glob is designed to not do that.  Code: This modification to Thomas's code has been tested on Ubuntu 14.04 only. Basically mentioned this in pyserial documentation 

https://pyserial.readthedocs.io/en/latest/tools.html#module-serial.tools.list_portsResult : COM1: Communications Port (COM1) [ACPI\PNP0501\1]COM7: MediaTek USB Port (COM7) [USB VID:PID=0E8D:0003 SER=6 LOCATION=1-2.1]refinement on moylop260's answer:This lists the ports that exist in hardware, including ones that are in use.  A whole lot more information exists in the list, per the pyserial tools documentationone line solution with pySerial package.Several options are available:Call QueryDosDevice with a NULL lpDeviceName to list all DOS devices. Then use CreateFile and GetCommConfig with each device name in turn to figure out whether it's a serial port.Call SetupDiGetClassDevs with a ClassGuid of GUID_DEVINTERFACE_COMPORT.WMI is also available to C/C++ programs.There's some conversation on the win32 newsgroup and a CodeProject, er, project.Please, try this code:first of all, you need to import package for serial port communication,

so:then you create the list of all the serial ports currently available:and then, walking along whole list, you can for example print port names:This is just an example how to get the list of ports and print their names, but there some other options you can do with this data. Just try print different variants after 

PyCharm and PYTHONPATH

Trying_hard

[PyCharm and PYTHONPATH](https://stackoverflow.com/questions/28326362/pycharm-and-pythonpath)

I am new to PyCharm. I have a directory that I use for my PYTHONPATH: c:\test\my\scripts\. In this directory I have some modules I import. It works fine in my Python shell. How do I add this directory path to PyCharm so I can import what is in that directory?

2015-02-04 16:25:49Z

I am new to PyCharm. I have a directory that I use for my PYTHONPATH: c:\test\my\scripts\. In this directory I have some modules I import. It works fine in my Python shell. How do I add this directory path to PyCharm so I can import what is in that directory?Out of data, see Duane's answer below.You need to go to the Main PyCharm Preferences, which will open up a separate window. In the left pane, choose Project:... > Project Interpreter. Now, in the main pane on the right, click the settings symbol (gear symbol) next to the field for "Project Interpreter". Choose More or Show All in the menu that pops up. Now in the final step, pick the interpreter you are using for this project and click on the tree symbol at the bottom of the window (hovering over the symbol reveals it as "Show paths for the selected interpreter"). Add your path by click in the "plus" symbol.It took me ages to find, so I hope the detailed instructions will help. Further details are available in the PyCharm docs.It is good practice to have __init__.py in each subfolder of the module you are looking to add, as well as making your project folder a 'Source Root'. Simply right-click on the folder in the path bar and choose 'Mark Directory as ...'For Pycharm Community 2019.3In PyCharm Community 2019.2/2019.3 (and probably other versions), you can simply:Modules within that folder will now be available for import. Any number of folders can be so marked.

logging.info doesn't show up on console but warn and error do

daydreamer

[logging.info doesn't show up on console but warn and error do](https://stackoverflow.com/questions/11548674/logging-info-doesnt-show-up-on-console-but-warn-and-error-do)

When I log an event with logging.info, it doesn't appear in the Python terminal.In contrast, events logged with logging.warn do appear in the terminal.Is there a environment level change I can to make logging.info print to the console? I want to avoid making changes in each Python file.

2012-07-18 19:14:52Z

When I log an event with logging.info, it doesn't appear in the Python terminal.In contrast, events logged with logging.warn do appear in the terminal.Is there a environment level change I can to make logging.info print to the console? I want to avoid making changes in each Python file.The root logger always defaults to WARNING level. Try callingand you should be fine.Like @ztyx said that default logger level is WARNING. You have to set it to a lower levelYou can do it by using logging.basicConfig and setting logger level:The above solutions didn't work for me, but the code here did:(I omitted parts of the code for the sake of readability)

Does Python have a cleaner way to express「if x contains a|b|c|d…」? [duplicate]

tom

[Does Python have a cleaner way to express「if x contains a|b|c|d…」? [duplicate]](https://stackoverflow.com/questions/19714041/does-python-have-a-cleaner-way-to-express-if-x-contains-abcd)

The Pythonic way to check if a string x is a substring of y is:Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:But checking if some string x contains either a, b, c, d, e, f or g seems clunky:Is there a more Pythonic method of checking if a string x contains an element of a list?I know it is trivial to write this myself using a loop or using a regex:but I was wondering if there was a cleaner way that does not involve regex.

2013-10-31 18:29:10Z

The Pythonic way to check if a string x is a substring of y is:Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:But checking if some string x contains either a, b, c, d, e, f or g seems clunky:Is there a more Pythonic method of checking if a string x contains an element of a list?I know it is trivial to write this myself using a loop or using a regex:but I was wondering if there was a cleaner way that does not involve regex.The Pythonic approach would be to use any():From the linked documentation: Also, notice that I've used a tuple instead of a list here. If your a-g values are pre-defined, then a tuple would indeed be preferred. See: Are tuples more efficient than lists in Python?I think that's about as short & Pythonic as you can get it.A bit late to the party, butwould work, and may be faster (algorithmically, but maybe not for smaller test cases).without using any but simply maxgives

dropping rows from dataframe based on a「not in」condition

gaurav gurnani

[dropping rows from dataframe based on a「not in」condition](https://stackoverflow.com/questions/27965295/dropping-rows-from-dataframe-based-on-a-not-in-condition)

I want to drop rows from a pandas dataframe when the value of the date column is in a list of dates. The following code doesn't work:I get the following error:

2015-01-15 14:10:17Z

I want to drop rows from a pandas dataframe when the value of the date column is in a list of dates. The following code doesn't work:I get the following error:You can use pandas.Dataframe.isin.pandas.Dateframe.isin will return boolean values depending on whether each element is inside the list a or not. You then invert this with the ~ to convert True to False and vice versa.You can use Series.isin:While the error message suggests that all() or any() can be used, they are useful only when you want to reduce the result into a single Boolean value. That is however not what you are trying to do now, which is to test the membership of every values in the Series against the external list, and keep the results intact (i.e., a Boolean Series which will then be used to slice the original DataFrame).You can read more about this in the Gotchas.

Regular expression syntax for「match nothing」?

grigoryvp

[Regular expression syntax for「match nothing」?](https://stackoverflow.com/questions/940822/regular-expression-syntax-for-match-nothing)

I have a python template engine that heavily uses regexp. It uses concatenation like:I can modify the individual substrings (regexp1, regexp2 etc).Is there any small and light expression that matches nothing, which I can use inside a template where I don't want any matches? Unfortunately, sometimes '+' or '*' is appended to the regexp atom so I can't use an empty string - that will raise a "nothing to repeat" error.

2009-06-02 17:30:50Z

I have a python template engine that heavily uses regexp. It uses concatenation like:I can modify the individual substrings (regexp1, regexp2 etc).Is there any small and light expression that matches nothing, which I can use inside a template where I don't want any matches? Unfortunately, sometimes '+' or '*' is appended to the regexp atom so I can't use an empty string - that will raise a "nothing to repeat" error.This shouldn't match anything:So if you replace regexp1, regexp2 and regexp3 with '$^' it will be impossible to find a match. Unless you are using the multi line mode.After some tests I found a better solutionIt is impossible to match and will fail earlier than the previous solution. You can replace a with any other character and it will always be impossible to match(?!) should always fail to match.  It is the zero-width negative look-ahead.  If what is in the parentheses matches then the whole match fails.  Given that it has nothing in it, it will fail the match for anything (including nothing).To match an empty string - even in multiline mode - you can use \A\Z, so:The difference is that \A and \Z are start and end of string, whilst ^ and $ these can match start/end of lines, so $^|$^*|$^+ could potentially match a string containing newlines (if the flag is enabled).And to fail to match anything (even an empty string), simply attempt to find content before the start of the string, e.g:Since no characters can come before \A (by definition), this will always fail to match.Maybe '.{0}'?You could use

\z..

This is the absolute end of string, followed by two of anythingIf + or * is tacked on the end this still works refusing to match anything  Or, use some list comprehension to remove the useless regexp entries and join to put them all together. Something like:Be sure to add some comments next to that line of code though :-)

How to use python timeit when passing variables to functions?

Lostsoul

[How to use python timeit when passing variables to functions?](https://stackoverflow.com/questions/7523767/how-to-use-python-timeit-when-passing-variables-to-functions)

I'm struggling with this using timeit and was wondering if anyone had any tipsBasically I have a function(that I pass a value to) that I want to test the speed of and created this:but when I run it, I get weird errors like coming from the timeit module.:If I run the function on its own, it works fine.  Its when I wrap it in the time it module, I get the errors(I have tried using double quotes and without..sameoutput).any suggestions would be awesome!Thanks!

2011-09-23 02:52:37Z

I'm struggling with this using timeit and was wondering if anyone had any tipsBasically I have a function(that I pass a value to) that I want to test the speed of and created this:but when I run it, I get weird errors like coming from the timeit module.:If I run the function on its own, it works fine.  Its when I wrap it in the time it module, I get the errors(I have tried using double quotes and without..sameoutput).any suggestions would be awesome!Thanks!Make it a callable:Should workTimer(superMegaIntenseFunction(10)) means "call superMegaIntenseFunction(10), then pass the result to Timer". That's clearly not what you want. Timer expects either a callable (just as it sounds: something that can be called, such as a function), or a string (so that it can interpret the contents of the string as Python code). Timer works by calling the callable-thing repeatedly and seeing how much time is taken.Timer(superMegaIntenseFunction) would pass the type check, because superMegaIntenseFunction is callable. However, Timer wouldn't know what values to pass to superMegaIntenseFunction.The simple way around this, of course, is to use a string with the code. We need to pass a 'setup' argument to the code, because the string is "interpreted as code" in a fresh context - it doesn't have access to the same globals, so you need to run another bit of code to make the definition available - see @oxtopus's answer.With lambda (as in @Pablo's answer), we can bind the parameter 10 to a call to superMegaIntenseFunction. All that we're doing is creating another function, that takes no arguments, and calls superMegaIntenseFunction with 10. It's just as if you'd used def to create another function like that, except that the new function doesn't get a name (because it doesn't need one).You should be passing a string. i.e.A note for future visitors. If you need to make it work in pdb debugger, and superMegaIntenseFunction is not in the global scope, you can make it work by adding to globals:One way to do it would be by using partial so that the function, 'superMegaIntenseFunction' is used as a callable (ie without the ()) in the timer or directly inside timeit.timeit. Using partial will pass the argument to the function when it will be call by the timer.

How can I check if code is executed in the IPython notebook?

Christoph

[How can I check if code is executed in the IPython notebook?](https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook)

I have some Python code example I'd like to share that should do something different if executed in the terminal Python / IPython or in the IPython notebook.How can I check from my Python code if it's running in the IPython notebook?

2013-03-14 14:25:44Z

I have some Python code example I'd like to share that should do something different if executed in the terminal Python / IPython or in the IPython notebook.How can I check from my Python code if it's running in the IPython notebook?The question is what do you want execute differently.We do our best in IPython prevent the kernel from knowing to which kind of frontend is connected, and actually you can even have a kernel connected to many differents frontends at the same time. Even if you can take a peek at the type of stderr/out to know wether you are in a ZMQ kernel or not, it does not guaranties you of what you have on the other side. You could even have no frontends at all.You should probably write your code in a frontend independent manner, but if you want to display different things, you can use the rich display system (link pinned to version 4.x of IPython) to display different things depending on the frontend, but the frontend will choose, not the library.The following worked for my needs:It returns 'TerminalInteractiveShell' on a terminal IPython, 'ZMQInteractiveShell' on Jupyter (notebook AND qtconsole) and fails (NameError) on a regular Python interpreter. The method get_python() seems to be available in the global namespace by default when IPython is started.Wrapping it in a simple function:The above was tested with Python 3.5.2, IPython 5.1.0 and Jupyter 4.2.1 on macOS 10.12 and Ubuntu 14.04.4 LTSTo check if you're in a notebook, which can be important e.g. when determining what sort of progressbar to use, this worked for me:You can check whether python is in interactive mode with the following snippet [1]:I have found this method very useful because I do a lot of prototyping in the notebook. For testing purposes, I use default parameters. Otherwise, I read the parameters from sys.argv.Following the implementation of autonotebook, you can tell whether you are in a notebook using the following code.Recently I encountered a bug in Jupyter notebook which needs a workaround, and I wanted to do this without loosing functionality in other shells. I realized that keflavich's solution does not work in this case, because get_ipython() is available only directly from the notebook, and not from imported modules. So I found a way to detect from my module whether it is imported and used from a Jupyter notebook or not:Comments are appreciated if this is robust enough.Similar way it is possible to get some info about the client, and IPython version as well:The following captures the cases of https://stackoverflow.com/a/50234148/1491619 without needing to parse the output of psAs far as I know, Here has 3 kinds of ipython that used ipykerneluse 'spyder' in sys.modules can distinguish spyderbut for qtipython and jn are hard to distinguish causethey have same sys.modules and same IPython config:get_ipython().configI find a different between qtipython and jn:first run os.getpid() in IPython shell get the pid numberthen run ps -ef|grep [pid number]my qtipython pid is 8699

yanglei   8699  8693  4 20:31 ?        00:00:01 /home/yanglei/miniconda2/envs/py3/bin/python -m ipykernel_launcher -f /run/user/1000/jupyter/kernel-8693.json

my jn pid is 8832

yanglei   8832  9788 13 20:32 ?        00:00:01 /home/yanglei/miniconda2/bin/python -m ipykernel_launcher -f /run/user/1000/jupyter/kernel-ccb962ec-3cd3-4008-a4b7-805a79576b1b.json

the different of qtipython and jn is the ipython's json name, jn's json name are longer than qtipython'sso, we can auto detection all Python Environment by following code:the source code are here: Detection Python Environment, Especially distinguish Spyder, Jupyter notebook, Qtconsole.pyI would recommend avoiding to detect specific frontend because there are too many of them. Instead you can just test if you are running from within iPython environment:Above will return False if you are invoking running_from_ipython from usual python command line. When you invoke it from Jupyter Notebook, JupyterHub, iPython shell, Google Colab etc then it will return True.Tested for python 3.7.3CPython implementations have the name __builtins__ available as part of their globals which btw. can be retrieved by the function globals().

If a script is running in an Ipython environment then __IPYTHON__ should be an attribute of __builtins__.

The code below therefore returns True if run under Ipython or else it gives FalseI am using Django Shell Plus to launch IPython, and I wanted to make 'running in notebook' available as a Django settings value. get_ipython() is not available when loading settings, so I use this (which is not bulletproof, but good enough for the local development environments it's used in):Assuming you have control of the Jupyter Notebook you could: This will generate a file that will not have the jupyter environment flag set allowing for code that uses it to deterministically execute.How about something like this:import sysinJupyter = sys.argv[-1].endswith('json')print(inJupyter);All you have to do is to place these two cells at the beginning of your notebook:Cell 1: (marked as "code"):Cell 2: (marked as "Raw NBConvert"):The first cell will always be executed, but the second cell will only be executed when you export the notebook as a Python script.Later, you can check:Hope this helps.

How to organize a Python Project?

André

[How to organize a Python Project?](https://stackoverflow.com/questions/5155135/how-to-organize-a-python-project)

I'm new to Python and I'm starting a mini Project, but I have some doubts on how to organize the folders in the "Python Way".I'm using PyDev in my Development Environment, and when I create a new project a folder is created called srcNow, in the PyDev, I can create Pydev Module and PyDev PackageI need to organize my Project in the following way:How can I organize this in terms of Modules and Packages? What is the meaning of Modules and Packages?Best Regards,

2011-03-01 13:28:57Z

I'm new to Python and I'm starting a mini Project, but I have some doubts on how to organize the folders in the "Python Way".I'm using PyDev in my Development Environment, and when I create a new project a folder is created called srcNow, in the PyDev, I can create Pydev Module and PyDev PackageI need to organize my Project in the following way:How can I organize this in terms of Modules and Packages? What is the meaning of Modules and Packages?Best Regards,A Package is basically a folder with __init__.py file under it and usually some Modules, where Module is a *.py file.

It has to do with import mainly. If you add __init__.py to Indicators you can use:or By the way, I would recommend to keep module/package names lowercase. It does not affect functionality but it's more "pythonic".From a file system perspective, a module is a file ending with .py and a package is a folder containing modules and (nested) packages again. Python recognizes a folder as a package if it contains a __init__.py file.A file structure like thatdefines the package some, which has a module foofoo and a nested package thing, which again has a module barbar. However, when using packages and modules, you don't really distinguish these two types:Please follow PEP8 when selecting naming your packages/modules (i.e. use lower-case names).See python-package-templateDirectory structurecat MakefileYou might want to check out the modern-package-template libary. It provides a way to setup a really nice basic layout for a project that walks you through a few questions and tries to help you get something that's able to be distributed fairly easily.http://pypi.python.org/pypi/modern-package-templateBefore deciding on a project structure, it's good to ask yourself what the purpose of the project is going to be. Is this going to be one off analysis? A toy concept you want to investigate? A full blown project you intend to distribute? The amount of effort you want to put into structuring your project will be different. The detailed reasons why I like this structure are in my blog post, but the basic gist is that the hierarchically lower level projectname directory contains your actual project. Alongside it are all the tools that help manage (git) and package (setup.py, MANIFEST.in) it.A package is a directory with a __init__.py in it. The difference from a directory is that you can import it.There isn't a "Python way" per se, but you'll find that it's a good idea to put all your modules in one package with a name related to the project.Also, to follow the Python style guide, PEP8, the package and module names should be all lowercase. So, if we assume the project is called "Botond Statistics" your structure would be something like this:You would then find the Stochastics class by doing(There are various ways to keep the structure but make imports shorter, but that's another question).You can put this structure under src/ if you want to, but it's not necessary. I never do.

Instead I have a main directory:In this directory I also typically have a virtualenv so I actually also have bin/ lib/ et al. Development is typically done by runningAs I use the Distrubute test runner to run the tests.That's how I do it. :-)Try python_boilerplate_template:https://pypi.python.org/pypi/python_boilerplate_templateThe cookiecutter project by audreyr includes several Python project templates:The package uses a single ~/.cookiecutterrc file to create custom project templates in Python, Java, JS, and other languages.For example, a Python template compatible with PyPI:

Construct pandas DataFrame from items in nested dictionary

vladimir montealegre

[Construct pandas DataFrame from items in nested dictionary](https://stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary)

Suppose I have a nested dictionary 'user_dict' with structure:For example, an entry of this dictionary would be:each item in user_dict has the same structure and user_dict contains a large number of items which I want to feed to a pandas DataFrame, constructing the series from the attributes. In this case a hierarchical index would be useful for the purpose.Specifically, my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the "level 3" in the dictionary?If I try something like:The items in "level 1" (the UserId's) are taken as columns, which is the opposite of what I want to achieve (have UserId's as index). I know I could construct the series after iterating over the dictionary entries, but if there is a more direct way this would be very useful. A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file. 

2012-11-26 23:41:31Z

Suppose I have a nested dictionary 'user_dict' with structure:For example, an entry of this dictionary would be:each item in user_dict has the same structure and user_dict contains a large number of items which I want to feed to a pandas DataFrame, constructing the series from the attributes. In this case a hierarchical index would be useful for the purpose.Specifically, my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the "level 3" in the dictionary?If I try something like:The items in "level 1" (the UserId's) are taken as columns, which is the opposite of what I want to achieve (have UserId's as index). I know I could construct the series after iterating over the dictionary entries, but if there is a more direct way this would be very useful. A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file. A pandas MultiIndex consists of a list of tuples. So the most natural approach would be to reshape your input dict so that its keys are tuples corresponding to the multi-index values you require. Then you can just construct your dataframe using pd.DataFrame.from_dict, using the option orient='index': An alternative approach would be to build your dataframe up by concatenating the component dataframes:pd.concat accepts a dictionary. With this in mind, it is possible to improve upon the currently accepted answer in terms of simplicity and performance by use a dictionary comprehension to build a dictionary mapping keys to sub-frames.Or,So I used to use a for loop for iterating through the dictionary as well, but one thing I've found that works much faster is to convert to a panel and then to a dataframe. 

Say you have a dictionary dThe commandwhere pd.Panel(d)[item] yields a dataframeYou can then hit the command to_frame() to turn it into a dataframe. I use reset_index as well to turn the major and minor axis into columns rather than have them as indices.Finally, if you don't like the way the frame looks you can use the transpose function of panel to change the appearance before calling to_frame() see documentation here 

http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Panel.transpose.htmlJust as an exampleHope this helps.

Python: Append item to list N times

Toji

[Python: Append item to list N times](https://stackoverflow.com/questions/4654414/python-append-item-to-list-n-times)

This seems like something Python would have a shortcut for. I want to append an item to a list N times, effectively doing this:It would seem to me that there should be an "optimized" method for that, something like:Is there?

2011-01-11 05:11:23Z

This seems like something Python would have a shortcut for. I want to append an item to a list N times, effectively doing this:It would seem to me that there should be an "optimized" method for that, something like:Is there?For immutable data types:For values that are stored by reference and you may wish to modify later (like sub-lists, or dicts):(The reason why the first method is only a good idea for constant values, like ints or strings, is because only a shallow copy is does when using the <list>*<number> syntax, and thus if you did something like [{}]*100, you'd end up with 100 references to the same dictionary - so changing one of them would change them all. Since ints and strings are immutable, this isn't a problem for them.)If you want to add to an existing list, you can use the extend() method of that list (in conjunction with the generation of a list of things to add via the above techniques):Use extend to add a list comprehension to the end.See the Python docs for more information.Itertools repeat combined with list extend.You could do this with a list comprehensionI had to go another route for an assignment but this is what I ended up with.

Get row-index values of Pandas DataFrame as list? [duplicate]

TravisVOX

[Get row-index values of Pandas DataFrame as list? [duplicate]](https://stackoverflow.com/questions/18358938/get-row-index-values-of-pandas-dataframe-as-list)

I'm probably using poor search terms when trying to find this answer. Right now, before indexing a DataFrame, I'm getting a list of values in a column this way......then I'll set_index on the column. This seems like a wasted step. When trying the above on an index, I get a key error.How can I grab the values in an index (both single and multi) and put them in a list or a list of tuples?

2013-08-21 13:36:05Z

I'm probably using poor search terms when trying to find this answer. Right now, before indexing a DataFrame, I'm getting a list of values in a column this way......then I'll set_index on the column. This seems like a wasted step. When trying the above on an index, I get a key error.How can I grab the values in an index (both single and multi) and put them in a list or a list of tuples?To get the index values as a list/list of tuples for Index/MultiIndex do:orIf you're only getting these to manually pass into df.set_index(), that's unnecessary. Just directly do df.set_index['your_col_name', drop=False], already.It's very rare in pandas that you need to get an index as a Python list (unless you;re doing something pretty funky, or else passing them back to NumPy), so if you're doing this a lot, it's a code smell that you're doing something wrong.

c++11 regex slower than python

locojay

[c++11 regex slower than python](https://stackoverflow.com/questions/14205096/c11-regex-slower-than-python)

hi i would like to understand why the following code which does a split string split using regexis slower then the following python codehere's Im using clang++ on osx.compiling with -O3 brings it to  down to 0.09s user 0.00s system 99% cpu 0.109 total

2013-01-07 22:20:33Z

hi i would like to understand why the following code which does a split string split using regexis slower then the following python codehere's Im using clang++ on osx.compiling with -O3 brings it to  down to 0.09s user 0.00s system 99% cpu 0.109 totalSee also this answer: https://stackoverflow.com/a/21708215 which was the base for the EDIT 2 at the bottom here.I've augmented the loop to 1000000 to get a better timing measure.This is my Python timing:Here's an equivalent of your code, just a bit prettier:Timing:This is an optimization to avoid construction/allocation of vector and string objects:Timing:This is near a 100% performance improvement.The vector is created before the loop, and can grow its memory in the first iteration. Afterwards there's no memory deallocation by clear(), the vector maintains the memory and construct strings in-place.Another performance increase would be to avoid construction/destruction std::string completely, and hence, allocation/deallocation of its objects.This is a tentative in this direction:Timing:An ultimate improvement would be to have a std::vector of const char * as return, where each char pointer would point to a substring inside the original s c string itself. The problem is that, you can't do that because each of them would not be null terminated (for this, see usage of C++1y string_ref in a later sample).This last improvement could also be achieved with this:I've built the samples with clang 3.3 (from trunk) with -O3. Maybe other regex libraries are able to perform better, but in any case, allocations/deallocations are frequently a performance hit.This is the boost::regex timing for the c string arguments sample:Same code, boost::regex and std::regex interface in this sample are identical, just needed to change the namespace and include.Best wishes for it to get better over time, C++ stdlib regex implementations are in their infancy.For sake of completion, I've tried this (the above mentioned "ultimate improvement" suggestion) and it didn't improved performance of the equivalent std::vector<std::string> &v version in anything:This has to do with the array_ref and string_ref proposal. Here's a sample code using it:It will also be cheaper to return a vector of string_ref rather than string copies for the case of split with vector return.This new solution is able to get output by return. I have used Marshall Clow's string_view (string_ref got renamed) libc++ implementation found at https://github.com/mclow/string_view.Timing:Note how faster this is compared to previous results. Of course, it's not filling a vector inside the loop (nor matching anything in advance probably too), but you get a range anyway, which you can range over with range-based for, or even use it to fill a vector.As ranging over the iterator_range creates string_views over an original string(or a null terminated string), this gets very lightweight, never generating unnecessary string allocations.Just to compare using this split implementation but actually filling a vector we could do this:This uses boost range copy algorithm to fill the vector in each iteration, the timing is:As can be seen, no much difference in comparison with the optimized string_view output param version.Note also there's a proposal for a std::split that would work like this.For optimizations, in general, you want to avoid two things:The two can be antithetic as sometimes it ends up being faster computing something than caching all of it in memory... so it's a game of balance.Let's analyze your code:Can we do better ? Well, if we could reuse existing storage instead of keeping allocating and deallocating memory, we should see a significant improvement [1]:In the test that you perform, where the number of submatches is constant across iterations, this version is unlikely to be beaten: it will only allocate memory on the first run (both for rsplit and result) and then keep reusing existing memory.[1]: Disclaimer, I have only proved this code to be correct I have not tested it (as Donald Knuth would say).How about this verion? It is no regex, but it solves the split pretty fast ...$ time splittest.exereal    0m0.132s

user    0m0.000s

sys     0m0.109sI would say C++11 regex is much slower than perl and possibly than python. To measure properly the performance it is better to do the tests

using some not trivial expression or else you are measuring everything 

but the regex itself.For example, to compare C++11 and perl C++11 test codein my computer compiling with gcc 4.9.3 I get the ouputperl test codeusing perl v5.8.8 again in my computerSo in this test the ratio C++11 / perl is in real scenarios I get ratios about 100 to 200 times slower.

So, for example parsing a big file with a million of lines takes

about a second for perl while it can take more minutes(!) for a C++11 

program using regex.

Reading tab-delimited file with Pandas - works on Windows, but not on Mac

user3062149

[Reading tab-delimited file with Pandas - works on Windows, but not on Mac](https://stackoverflow.com/questions/27896214/reading-tab-delimited-file-with-pandas-works-on-windows-but-not-on-mac)

I've been reading a tab-delimited data file in Windows with Pandas/Python without any problems. The data file contains notes in first three lines and then follows with a header. I'm now trying to read this file with my Mac. (My first time using Python on Mac.) I get the following error.If set the error_bad_lines argument for read_csv to False, I get the following information, which continues until the end of the last row.Do I need to specify a value for the encoding argument? It seems as though I shouldn't have to because reading the file works fine on Windows.

2015-01-12 06:05:53Z

I've been reading a tab-delimited data file in Windows with Pandas/Python without any problems. The data file contains notes in first three lines and then follows with a header. I'm now trying to read this file with my Mac. (My first time using Python on Mac.) I get the following error.If set the error_bad_lines argument for read_csv to False, I get the following information, which continues until the end of the last row.Do I need to specify a value for the encoding argument? It seems as though I shouldn't have to because reading the file works fine on Windows.The biggest clue is the rows are all being returned on one line. This indicates line terminators are being ignored or are not present. You can specify the line terminator for csv_reader. If you are on a mac the lines created will end with \rrather than the linux standard \n or better still the suspenders and belt approach of windows with \r\n.You could also open all your data using the codecs package. This may increase robustness at the expense of document loading speed.Another option would be to add engine='python' to the command pandas.read_csv(filename, sep='\t', engine='python')

What is %timeit in python?

xirururu

[What is %timeit in python?](https://stackoverflow.com/questions/29280470/what-is-timeit-in-python)

I always read the code to calculate the time like this way:Can you explain what means "%" here? I think, the "%" is always used to replace something in a string, like %s means replace a string, %d replace a data,  but I have no idea about this case.

2015-03-26 14:02:28Z

I always read the code to calculate the time like this way:Can you explain what means "%" here? I think, the "%" is always used to replace something in a string, like % s means replace a string, % d replace a data,  but I have no idea about this case.% timeit is an ipython magic function, which can be used to time a particular piece of code (A single execution statement, or a single method).From the docs:To use it, for example if we want to find out whether using xrange is any faster than using range, you can simply do:And you will get the timings for them.The major advantage of % timeit are:This is known as a line magic in iPython.  They are unique in that their arguments only extend to the end of the current line, and magics themselves are really structured for command line development.  timeit is used to time the execution of code. If you wanted to see all of the magics you can use, you could simply type:to get a list of both line magics and cell magics. Some further magic information from documentation here:Depending on whether you are in line or cell mode, there are two different ways to use % timeit. Your question illustrates the first way:vs.IPython intercepts those, they're called built-in magic commands, here's the list: https://ipython.org/ipython-doc/dev/interactive/magics.htmlYou can also create your own custom magics, https://ipython.org/ipython-doc/dev/config/custommagics.htmlYour timeit is here https://ipython.org/ipython-doc/dev/interactive/magics.html#magic-timeitI would just like to add another useful advantage of using % timeit to answer by mu 無 that: PS: I know this should be a comment to answer above but I currently don't have enough reputation for that, hope what I write will be helpful to someone and help me earn enough reputation to comment next time.Line magics are prefixed with the % character and work much like OS command-line calls: they get as an argument the rest of the line, where arguments are passed without parentheses or quotes. Cell magics are prefixed with a double %%, and they are functions that get as an argument not only the rest of the line, but also the lines below it in a separate argument.

Why doesn't Pylint like built-in functions?

igorgue

[Why doesn't Pylint like built-in functions?](https://stackoverflow.com/questions/3569134/why-doesnt-pylint-like-built-in-functions)

I have a line like this:Pylint is showing a warning:Why is that? is a list comprehension the recommended method?Of course I can rewrite this like this:And I get no warnings, but I was wondering if there's a PEP for this?

2010-08-25 18:37:37Z

I have a line like this:Pylint is showing a warning:Why is that? is a list comprehension the recommended method?Of course I can rewrite this like this:And I get no warnings, but I was wondering if there's a PEP for this?Pylint often chatters on about stuff it shouldn't. You can disable the warning in a .pylintrc file.This page http://pylint-messages.wikidot.com/messages:w0141 indicates the problem is that filter and map have been superseded by list comprehensions.A line like this in your pylintrc file will quiet the warning:List comprehension is recommended in the tutorial example, which statesand by most answerers on SO's Python List Comprehension Vs. Map where it isTL;DR: use list comprehension in most casesI ran into the same problem and could not figure outwhy the built-in function `input' is bad. I you intendto disable it:Once you like the settings: Verify that your settings are in the file, e.g.:After that you can use this file locallyor even use it as your default rcfile. For this I kindly refer you toI've got the same warning on my project.  I'm changing the source code to be py2/3 compatible, and pylint helps a lot.Running pylint --py3k shows only errors about compatibility.In python 2, if use filter, it returns a list:But in python 3, filter and other similar methods (map, range, zip, ..) return a iterator, that is incompatible types and perhaps cause bugs in your code.To make your code python 2/3 compatible, I use a cheat sheet from python future siteTo avoid this warning, you can use 4 approaches, that works on python 2 and 3:1 - Using a list comprehension like you said.2 - Using a list function, grant that return always is a materialized list, result is same on both python versions3 - Using lfilter, that's a future package import.  It always return a list, uses filter on py2, and list(filter(..) on py3.  So, both pythons got the same behaviour and you got a cleaner syntax.4 - The best! Use filter always on a loop, this way pylint don't give  warnings, and it have a nice performance boost on python 3.Always prefer functions that works on python 3, because python 2 will be retired soon.

Call a Python method by name

Jazz

[Call a Python method by name](https://stackoverflow.com/questions/3521715/call-a-python-method-by-name)

If I have an object and a method name in a string, how can I call the method?

2010-08-19 12:26:28Z

If I have an object and a method name in a string, how can I call the method?Use the built-in getattr() function:You can also use setattr() for setting class attributes by names.I had similar question, wanted to call instance method by reference. Here are funny things I found:Python is amazing!No need to instantiate Foo first!Here is a more generalized version using Python decorators. You can call by short or long name. I found it useful when implementing CLI with short and long sub commands. Python decorators are wonderful. Bruce Eckel (Thinking in Java) describes Python decorators beautifully here.http://www.artima.com/weblogs/viewpost.jsp?thread=240808

http://www.artima.com/weblogs/viewpost.jsp?thread=240845

How to dynamically create a derived type in the Python C-API

Sven Marnach

[How to dynamically create a derived type in the Python C-API](https://stackoverflow.com/questions/8066438/how-to-dynamically-create-a-derived-type-in-the-python-c-api)

Assume we have the type Noddy as defined in the tutorial on writing C extension modules for Python.  Now we want to create a derived type, overwriting only the __new__() method of Noddy.Currently I use the following approach (error checking stripped for readability):This works, but I'm not sure if it is The Right Way To Do It.  I would have expected that I have to set the Py_TPFLAGS_HEAPTYPE flag, too, because I dynamically allocate the type object on the heap, but doing so leads to a segfault in the interpreter.I also thought about explicitly calling type() using PyObject_Call() or similar, but I discarded the idea.  I would need to wrap the function BrownNoddy_new() in a Python function object and create a dictionary mapping __new__ to this function object, which seems silly.What is the best way to go about this?  Is my approach correct? Is there an interface function I missed?There are two threads on a related topic on the python-dev mailing list (1) (2).  From these threads and a few experiments I deduce that I shouldn't set Py_TPFLAGS_HEAPTYPE unless the type is allocated by a call to type().  There are different recommendations in these threads whether it is better to allocate the type manually or to call type().  I'd be happy with the latter if only I knew what the recommended way to wrap the C function that is supposed to go in the tp_new slot is.  For regular methods this step would be easy -- I could just use PyDescr_NewMethod() to get a suitable wrapper object. I don't know how to create such a wrapper object for my __new__() method, though -- maybe I need the undocumented function PyCFunction_New() to create such a wrapper object.

2011-11-09 14:26:33Z

Assume we have the type Noddy as defined in the tutorial on writing C extension modules for Python.  Now we want to create a derived type, overwriting only the __new__() method of Noddy.Currently I use the following approach (error checking stripped for readability):This works, but I'm not sure if it is The Right Way To Do It.  I would have expected that I have to set the Py_TPFLAGS_HEAPTYPE flag, too, because I dynamically allocate the type object on the heap, but doing so leads to a segfault in the interpreter.I also thought about explicitly calling type() using PyObject_Call() or similar, but I discarded the idea.  I would need to wrap the function BrownNoddy_new() in a Python function object and create a dictionary mapping __new__ to this function object, which seems silly.What is the best way to go about this?  Is my approach correct? Is there an interface function I missed?There are two threads on a related topic on the python-dev mailing list (1) (2).  From these threads and a few experiments I deduce that I shouldn't set Py_TPFLAGS_HEAPTYPE unless the type is allocated by a call to type().  There are different recommendations in these threads whether it is better to allocate the type manually or to call type().  I'd be happy with the latter if only I knew what the recommended way to wrap the C function that is supposed to go in the tp_new slot is.  For regular methods this step would be easy -- I could just use PyDescr_NewMethod() to get a suitable wrapper object. I don't know how to create such a wrapper object for my __new__() method, though -- maybe I need the undocumented function PyCFunction_New() to create such a wrapper object.I encountered the same problem when I was modifying an extension to be compatible with Python 3, and found this page when I was trying to solve it.I did eventually solve it by reading the source code for the Python interpreter, PEP 0384 and the documentation for the C-API.Setting the Py_TPFLAGS_HEAPTYPE flag tells the interpreter to recast your PyTypeObject as PyHeapTypeObject, which contains additional members that must also be allocated. At some point the interpreter attempts to refer to these extra members and, if you leave them unallocated, it will cause a segfault.Python 3.2 introduced the C structures PyType_Slot and PyType_Spec and the C function PyType_FromSpec that simplify the creation of dynamic types. In a nutshell, you use PyType_Slot and PyType_Spec to specify the tp_* members of the PyTypeObject and then call PyType_FromSpec to do the dirty work of allocating and initialising the memory.From PEP 0384, we have:(The above isn't a literal copy from PEP 0384, which also includes const char *doc as a member of PyType_Spec. But that member doesn't appear in the source code.)To use these in the original example, assume we have a C structure, BrownNoddy, that extends the C structure for the base class Noddy. Then we would have:This should do everything in the original code, including calling PyType_Ready, plus what is necessary for creating a dynamic type, including setting Py_TPFLAGS_HEAPTYPE, and allocating and initialising the extra memory for a PyHeapTypeObject.I hope that's helpful.I apologize up front if this answer is terrible, but you can find an implementation of this idea in PythonQt, in particular I think the following files might be useful references:This fragment from PythonQtClassWrapper_init jumps out at me as being somewhat interesting:It's worth noting that PythonQt does use a wrapper generator, so it's not exactly in line with what you're asking for, but personally I think trying to outsmart the vtable isn't the most optimal design. Basically, there are many different C++ wrapper generators for Python and people use them for a good reason - they're documented, there are examples floating around in search results and on stack overflow. If you hand roll a solution for this that nobody's seen before, it'll be that much harder for them to debug if they run into problems. Even if it's closed-source, the next guy who has to maintain it will be scratching his head and you'll have to explain it to every new person who comes along.Once you get a code generator working, all you need to do is maintain the underlying C++ code, you don't have to update or modify your extension code by hand. (Which is probably not too far away from the tempting solution you went with)The proposed solution is an example of breaking the type-safety that the newly introduced PyCapsule provides a bit more protection against (when used as directed).So, while its possible it might not be the best long term choice to implement derived/subclasses this way, but rather wrap the code and let the vtable do what it does best and when the new guy has questions you can just point him at the documentation for whatever solution fits best.This is just my opinion though. :DOne way to try and understand how to do this is to create a version of it using SWIG.  See what it produces and see if it matches or is done a different way.  From what I can tell the people who have been writing SWIG have an in depth understanding of extending Python.  Can't hurt to see how they do things at any rate. It may help you understand this problem.

Python 3 print without parenthesis

Laura

[Python 3 print without parenthesis](https://stackoverflow.com/questions/32122868/python-3-print-without-parenthesis)

The print used to be a statement in Python 2, but now it became a function that requires parenthesis in Python 3. Is there anyway to suppress these parenthesis in Python 3? Maybe by re-defining the print function? So, instead of I could type:

2015-08-20 15:53:43Z

The print used to be a statement in Python 2, but now it became a function that requires parenthesis in Python 3. Is there anyway to suppress these parenthesis in Python 3? Maybe by re-defining the print function? So, instead of I could type:Although you need a pair of parentheses to print in Python 3, you no longer need a space after print, because it's a function. So that's only a single extra character.If you still find typing a single pair of parentheses to be "unnecessarily time-consuming," you can do p = print and save a few characters that way. Because you can bind new references to functions but not to keywords, you can only do this print shortcut in Python 3.Python 2:Python 3:It'll make your code less readable, but you'll save those few characters every time you print something.The AHK script is a great idea. Just for those interested I needed to change it a little bit to work for me:Using print without parentheses in Python 3 code is not a good idea. Nor is creating aliases, etc. If that's a deal breaker, use Python 2.However, print without parentheses might be useful in the interactive shell. It's not really a matter of reducing the number of characters, but rather avoiding the need to press Shift twice every time you want to print something while you're debugging. IPython lets you call functions without using parentheses if you start the line with a slash:And if you turn on autocall, you won't even need to type the slash:Use Autohotkey to make a macro. AHK is free and dead simple to install. www.autohotkey.comYou could assign the macro to, say, alt-p:That will make alt-p put out print() and move your cursor to inside the parens.Or, even better, to directly solve your problem, you define an autoreplace and limit its scope to when the open file has the .py extension:This is a guaranteed, painless, transparent solution. No. That will always be a syntax error in Python 3. Consider using 2to3 to translate your code to Python 3You can't, because the only way you could do it without parentheses is having it be a keyword, like in Python 2. You can't manually define a keyword, so no.I finally figured out the regex to change these all in old Python2 example scripts. Otherwise use 2to3.py.Try it out on Regexr.com, doesn't work in NP++(?):for variables:for label and variable:In Python 3, print is a function, whereas it used to be a statement in previous versions. As @holdenweb suggested, use 2to3 to translate your code.

equivalent of a python dict in R

user1357015

[equivalent of a python dict in R](https://stackoverflow.com/questions/10678872/equivalent-of-a-python-dict-in-r)

I want to make the equivalent of a python dict in R. Basically, in python I have:The idea is, if I saw that specific, atom_count, I have visited[atom_count] = 1. Thus, if I see that atom_count again, i don't "Do Stuff". Atom_Count is an integer. Thanks!

2012-05-21 02:22:28Z

I want to make the equivalent of a python dict in R. Basically, in python I have:The idea is, if I saw that specific, atom_count, I have visited[atom_count] = 1. Thus, if I see that atom_count again, i don't "Do Stuff". Atom_Count is an integer. Thanks!The closest thing to a python dict in R is simply a list. Like most R data types, lists can have a names attribute that can allow lists to act like a set of name-value pairs:Now for the usual disclaimer: they are not exactly the same; there will be differences. So you will be inviting disappointment to try to literally use lists exactly the way you might use a dict in python.I believe that the use of a hash table (creating a new environment) may be the solution to your problem.  I'd type out how to do this but I just did so yesterday day at talkstats.com.If your dictionary is large and only two columns then this may be the way to go.  Here's the link to the talkstats thread with sample R code:HASH TABLE LINKIf, like in your case, you just want your "dictionary" to store values of the same type, you can simply use a vector, and name each element.If you want to access the "keys", use names.

yield in list comprehensions and generator expressions

zabolekar

[yield in list comprehensions and generator expressions](https://stackoverflow.com/questions/32139885/yield-in-list-comprehensions-and-generator-expressions)

The following behaviour seems rather counterintuitive to me (Python 3.4):The intermediate values of the last line are actually not always None, they are whatever we send into the generator, equivalent (I guess) to the following generator:It strikes me as funny that those three lines work at all. The Reference says that yield is only allowed in a function definition (though I may be reading it wrong and/or it may simply have been copied from the older version). The first two lines produce a SyntaxError in Python 2.7, but the third line doesn't.Also, it seems oddCould someone provide more information?

2015-08-21 12:05:06Z

The following behaviour seems rather counterintuitive to me (Python 3.4):The intermediate values of the last line are actually not always None, they are whatever we send into the generator, equivalent (I guess) to the following generator:It strikes me as funny that those three lines work at all. The Reference says that yield is only allowed in a function definition (though I may be reading it wrong and/or it may simply have been copied from the older version). The first two lines produce a SyntaxError in Python 2.7, but the third line doesn't.Also, it seems oddCould someone provide more information?Generator expressions, and set and dict comprehensions are compiled to (generator) function objects. In Python 3, list comprehensions get the same treatment; they are all, in essence, a new nested scope.You can see this if you try to disassemble a generator expression:The above shows that a generator expression is compiled to a code object, loaded as a function (MAKE_FUNCTION creates the function object from the code object). The .co_consts[0] reference lets us see the code object generated for the expression, and it uses YIELD_VALUE just like a generator function would.As such, the yield expression works in that context, as the compiler sees these as functions-in-disguise.This is a bug; yield has no place in these expressions. The Python grammar before Python 3.7 allows it (which is why the code is compilable), but the yield expression specification shows that using yield here should not actually work:This has been confirmed to be a bug in issue 10544. The resolution of the bug is that using yield and yield from will raise a SyntaxError in Python 3.8; in Python 3.7 it raises a DeprecationWarning to ensure code stops using this construct. You'll see the same warning in Python 2.7.15 and up if you use the -3 command line switch enabling Python 3 compatibility warnings.The 3.7.0b1 warning looks like this; turning warnings into errors gives you a SyntaxError exception, like you would in 3.8:The differences between how yield in a list comprehension and yield in a generator expression operate stem from the differences in how these two expressions are implemented. In Python 3 a list comprehension uses LIST_APPEND calls to add the top of the stack to the list being built, while a generator expression instead yields that value. Adding in (yield <expr>) just adds another YIELD_VALUE opcode to either:The YIELD_VALUE opcode at bytecode indexes 15 and 12 respectively is extra, a cuckoo in the nest. So for the list-comprehension-turned-generator you have 1 yield producing the top of the stack each time (replacing the top of the stack with the yield return value), and for the generator expression variant you yield the top of the stack (the integer) and then yield again, but now the stack contains the return value of the yield and you get None that second time.For the list comprehension then, the intended list object output is still returned, but Python 3 sees this as a generator so the return value is instead attached to the StopIteration exception as the value attribute:Those None objects are the return values from the yield expressions.And to reiterate this again; this same issue applies to dictionary and set comprehension in Python 2 and Python 3 as well; in Python 2 the yield return values are still added to the intended dictionary or set object, and the return value is 'yielded' last instead of attached to the StopIteration exception:

How to postpone/defer the evaluation of f-strings?

JDAnders

[How to postpone/defer the evaluation of f-strings?](https://stackoverflow.com/questions/42497625/how-to-postpone-defer-the-evaluation-of-f-strings)

I am using template strings to generate some files and I love the conciseness of the new f-strings for this purpose, for reducing my previous template code from something like this:Now I can do this, directly replacing variables:However, sometimes it makes sense to have the template defined elsewhere — higher up in the code, or imported from a file or something. This means the template is a static string with formatting tags in it. Something would have to happen to the string to tell the interpreter to interpret the string as a new f-string, but I don't know if there is such a thing.Is there any way to bring in a string and have it interpreted as an f-string to avoid using the .format(**locals()) call?Ideally I want to be able to code like this... (where magic_fstring_function is where the part I don't understand comes in):...with this desired output (without reading the file twice):...but the actual output I get is:

2017-02-27 23:24:02Z

I am using template strings to generate some files and I love the conciseness of the new f-strings for this purpose, for reducing my previous template code from something like this:Now I can do this, directly replacing variables:However, sometimes it makes sense to have the template defined elsewhere — higher up in the code, or imported from a file or something. This means the template is a static string with formatting tags in it. Something would have to happen to the string to tell the interpreter to interpret the string as a new f-string, but I don't know if there is such a thing.Is there any way to bring in a string and have it interpreted as an f-string to avoid using the .format(**locals()) call?Ideally I want to be able to code like this... (where magic_fstring_function is where the part I don't understand comes in):...with this desired output (without reading the file twice):...but the actual output I get is:Here's a complete "Ideal 2".It's not an f-string—it doesn't even use f-strings—but it does as requested. Syntax exactly as specified. No security headaches since we are not using eval().It uses a little class and implements __str__ which is automatically called by print. To escape the limited scope of the class we use the inspect module to hop one frame up and see the variables the caller has access to.Yes, that's exactly why we have literals with replacement fields and .format, so we can replace the fields whenever we like by calling format on it.That's the prefix f/F. You could wrap it in a function and postpone the evaluation during call time but of course that incurs extra overhead:Which prints out:but feels wrong and is limited by the fact that you can only peek at the global namespace in your replacements. Trying to use it in a situation which requires local names will fail miserably unless passed to the string as arguments (which totally beats the point).Other than a function (limitations included), nope, so might as well stick with .format.An f-string is simply a more concise way of creating a formatted string, replacing .format(**names) with f. If you don't want a string to be immediately evaluated in such a manner, don't make it an f-string. Save it as an ordinary string literal, and then call format on it later when you want to perform the interpolation, as you have been doing.Of course, there is an alternative with eval.template.txt:Code:But then all you've managed to do is replace str.format with eval, which is surely not worth it. Just keep using regular strings with a format call.A concise way to have a string evaluated as an f-string (with its full capabilities) is using following function:Then you can do:And, in contrast to many other proposed solutions, you can also do:Using .format is not a correct answer to this question.   Python f-strings are very different from str.format() templates ... they can contain code or other expensive operations - hence the need for deferral.Here's an example of a deferred logger.   This uses the normal preamble of logging.getLogger, but then adds new functions that interpret the f-string only if the log level is correct.   This has the advantage of being able to do things like:  log.fdebug("{obj.dump()}") .... without dumping the object unless debugging is enabled.   IMHO: This should have been the default operation of f-strings, however now it's too late.   F-string evaluation can have massive and unintended side-effects, and having that happen in a deferred manner will change program execution.   In order to make f-strings properly deferred, python would need some way of explicitly switching behavior.   Maybe use the letter 'g'? ;)What you want appears to be being considered as a Python enhancement.Meanwhile — from the linked discussion — the following seems like it would be a reasonable workaround that doesn't require using eval():Output:inspired by the answer by kadee, the following can be used to define a deferred-f-string class.which is exactly what the question asked forOr maybe do not use f-strings, just format:In version without names:A suggestion that uses f-strings. Do your evaluation on the

logical level where the templating is occurring and pass it as a generator.

You can unwind it at whatever point you choose, using f-strings

When will Jython support Python 3?

Adam Paynter

[When will Jython support Python 3?](https://stackoverflow.com/questions/2351008/when-will-jython-support-python-3)

According to Jython's documentation:Are there any plans to support Python 3? If so, when is it scheduled to be released?

2010-02-28 12:21:52Z

According to Jython's documentation:Are there any plans to support Python 3? If so, when is it scheduled to be released?Jython roadmap is definitely outdated.

However, on Frank Wierzbicki's Weblog (one of Jython's main developers) you can get an update, telling that Python 3 is definitely on the radar.Unfortunately, it is not yet clear when, as it is stated in a comment in that same blog from 2010:In an interview in 2017, Wierzbicki stated that Python 3 is desirable but difficult.English:Jython – Python in der Java-Welt [Pirates of the JVM], JAXenter, 2017-04-12.Update: 9 years after the question has been originally asked the answer now seems to be not in the near future. Most recent commit is now more than two years old (July 18, 2017), and developers say that the 3.x branch is "resting".My original answer:5 years after the question has been asked, the answer is still "it will come, but the time frame for an initial release is not clear yet".What we can say is that now there is a jython3 repository targetting Python 3.5. README.md, dated 28 May 2015, says:Jython FAQ page states that:I suggest looking elsewhere for a Python 3 interpreter on the JVM.The GraalPython project (https://github.com/graalvm/graalpython) looks promissing. There is good rationale behind its existence, essentially it says "Let's support scientific computing and data analysis in Python, which means e.g. numpy, including native-code modules, and let's make it fast and interoperable".Jython, not so much, at this point.

Understanding celery task prefetching

Henrik Heimbuerger

[Understanding celery task prefetching](https://stackoverflow.com/questions/16040039/understanding-celery-task-prefetching)

I just found out about the configuration option CELERYD_PREFETCH_MULTIPLIER (docs). The default is 4, but (I believe) I want the prefetching off or as low as possible. I set it to 1 now, which is close enough to what I'm looking for, but there's still some things I don't understand:

2013-04-16 14:42:30Z

I just found out about the configuration option CELERYD_PREFETCH_MULTIPLIER (docs). The default is 4, but (I believe) I want the prefetching off or as low as possible. I set it to 1 now, which is close enough to what I'm looking for, but there's still some things I don't understand:Old question, but still adding my answer in case it helps someone. My understanding from some initial testing was same as that in David Wolever's answer. I just tested this more in celery 3.1.19 and -Ofair does work. Just that it is not meant to disable prefetch at the worker node level. That will continue to happen. Using -Ofair has a different effect which is at the pool worker level. In summary, to disable prefetch completely, do this:Adding some more details:I found that the worker node will always prefetch by default. You can only control how many tasks it prefetches by using CELERYD_PREFETCH_MULTIPLIER. If set to 1, it will only prefetch as many tasks as the number of pool workers (concurrency) in the node. So if you had concurrency = n, the max tasks prefetched by the node will be n. Without the -Ofair option, what happened for me was that if one of the pool worker processes was executing a long running task, the other workers in the node would also stop processing the tasks already prefetched by the node. By using -Ofair, that changed. Even though one of the workers in the node was executing a long running tasks, others would not stop processing and would continue to process the tasks prefetched by the node. So I see two levels of prefetching. One at the worker node level. The other at the individual worker level. Using -Ofair for me seemed to disable it at the worker level.How is ACKS_LATE related? ACKS_LATE = True means that the task will be acknowledged only when the task succeeds. If not, I suppose it would happen when it is received by a worker. In case of prefetch, the task is first received by the worker (confirmed from logs) but will be executed later. I just realized that prefetched messages show up under "unacknowledged messages" in rabbitmq. So I'm not sure if setting it to True is absolutely needed. We anyway had our tasks set that way (late ack) for other reasons.Just a warning: as of my testing with the redis broker + Celery 3.1.15, all of the advice I've read pertaining to CELERYD_PREFETCH_MULTIPLIER = 1 disabling prefetching is demonstrably false.To demonstrate this:CELERYD_PREFETCH_MULTIPLIER = 1 does not prevent prefetching, it simply limits the prefetching to 1 task per queue.-Ofair, despite what the documentation says, also does not prevent prefetching.Short of modifying the source code, I haven't found any method for entirely disabling prefetching.I cannot comment on David Wolever's answers, since my stackcred isn't high enough. So, I've framed my comment as an answer since I'd like to share my experience with Celery 3.1.18 and a Mongodb broker. I managed to stop prefetching with the following:Leaving CELERY_ACKS_LATE to the default, the worker still prefetches. Just like the OP I don't fully grasp the link between prefetching and late acks. I understand what David says "CELERY_ACKS_LATE=True prevents acknowledging messages when they reach to a worker", but I fail to understand why late acks would be incompatible with prefetch. In theory a prefetch would still allow to ack late right - even if not coded as such in celery ?I experienced something a little bit different with SQS as broker. The setup was:After task fail (exception raised), the worker became unavailable since the message was not acked, both local and remote queue. The solution that made the workers continue consuming work was setting CELERYD_PREFETCH_MULTIPLIER = 0I can only speculate that acks_late was not taken in consideration when writing the SQS transport

Django Query That Get Most Recent Objects From Different Categories

Zach

[Django Query That Get Most Recent Objects From Different Categories](https://stackoverflow.com/questions/2074514/django-query-that-get-most-recent-objects-from-different-categories)

I have two models A and B. All B objects have a foreign key to an A object. Given a set of A objects, is there anyway to use the ORM to get a set of B objects containing the most recent object created for each A object.Here's an simplified example:So I'm looking for a query that returns the most recent cake baked in each bakery in Anytown, USA.

2010-01-15 20:21:01Z

I have two models A and B. All B objects have a foreign key to an A object. Given a set of A objects, is there anyway to use the ORM to get a set of B objects containing the most recent object created for each A object.Here's an simplified example:So I'm looking for a query that returns the most recent cake baked in each bakery in Anytown, USA.As far as I know, there is no one-step way of doing this in Django ORM.But you can split it in two queries:If id's of cakes are progressing along with bake_at timestamps, you can simplify and disambiguate the above code (in case two cakes arrives at the same time you can get both of them):BTW credits for this goes to Daniel Roseman, who once answered similar question of mine:http://groups.google.pl/group/django-users/browse_thread/thread/3b3cd4cbad478d34/3e4c87f336696054?hl=pl&q=If the above method is too slow, then I know also second method - you can write custom SQL producing only those Cakes, that are hottest in relevant Bakeries, define it as database VIEW, and then write unmanaged Django model for it. It's also mentioned in the above django-users thread. Direct link to the original concept is here:http://web.archive.org/web/20130203180037/http://wolfram.kriesing.de/blog/index.php/2007/django-nice-and-critical-article#comment-48425Hope this helps.Starting from Django 1.11 and thanks to Subquery and OuterRef and we can finally build a latest-per-group query using the ORM.If you happen to be using PostGreSQL, you can use Django's interface to DISTINCT ON:As the docs say, you must order by the same fields that you distinct on. As Simon pointed out below, if you want to do additional sorting, you'll have to do it in Python-space.This should do the job:I was fighting with similar problem and finally come to following solution. It does not rely on order_by and distinct so can be sorted as desired on db-side and also can be used as nested query for filtering. I also believe this implementation is db engine independent, because it's based on standard sql HAVING clause. The only drawback is that it will return multiple hottest cakes per bakery, if they are baked in that bakery at exactly same time.I haven't built out the models on my end, but in theory this should work. Broken down:Note: This answer is for Django 1.11.

This answer modified from Queries shown here in Django 1.11 Docs.

MongoKit vs MongoEngine vs Flask-MongoAlchemy for Flask [closed]

oscarmlage

[MongoKit vs MongoEngine vs Flask-MongoAlchemy for Flask [closed]](https://stackoverflow.com/questions/9447629/mongokit-vs-mongoengine-vs-flask-mongoalchemy-for-flask)

Anyone has experiences with MongoKit, MongoEngine or Flask-MongoAlchemy for Flask?Which one do you prefer? Positive or negative experiences?. Too many options for a Flask-Newbie. 

2012-02-25 20:09:37Z

Anyone has experiences with MongoKit, MongoEngine or Flask-MongoAlchemy for Flask?Which one do you prefer? Positive or negative experiences?. Too many options for a Flask-Newbie. I have invested a lot of time evaluating the popular Python ORMs for MongoDB. This was an exhaustive exercise, as I really wanted to pick one.My conclusion is that an ORM removes the fun out of MongoDB. None feels natural, they impose restrictions similar to the ones which made me move away from relational databases in the first place.Again, I really wanted to use an ORM, but now I am convinced that using pymongo directly is the way to go. Now, I follow a pattern which embraces MongoDB, pymongo, and Python. A Resource Oriented Architecture leads to very natural representations. For instance, take the following User resource:The Resource base class looks likeNotice that I use the WSGI spec directly, and leverage Werkzeug where possible (by the way, I think that Flask adds an unnecessary complication to Werkzeug).The function representation takes the request's Accept headers, and produces a suitable representation (for example, application/json, or text/html). It is not difficult to implement. It also adds the Last-Modified header.Of course, your input needs to be sanitized, and the code, as presented, will not work (I mean it as an example, but it is not difficult to understand my point).Again, I tried everything, but this architecture made my code flexible, simple, and extensible.

How to check whether optional function parameter is set

Matthias

[How to check whether optional function parameter is set](https://stackoverflow.com/questions/14749328/how-to-check-whether-optional-function-parameter-is-set)

Is there an easy way in Python to check whether the value of an optional parameter comes from its default value, or because the user has set it explicitly at the function call?

2013-02-07 10:52:05Z

Is there an easy way in Python to check whether the value of an optional parameter comes from its default value, or because the user has set it explicitly at the function call?Lot of answers have little pieces of the full info, so I'd like to bring it all together with my favourite pattern(s).If the default value is a mutable object, you are lucky: you can exploit the fact that Python’s default arguments are evaluated once when the function is defined (some more about this at the end of the answer in the last section)This means you can easily compare a default mutable value using is to see if it was passed as an argument or left by default, as in the following examples as function or method:andNow, it's a bit less elegant if your default is expected to be an immutable value (and remember that even strings are immutable!) because you can't exploit the trick as it is, but there is still something you can do, still exploiting mutable type; basically you put a mutable "fake" default in the function signature, and the desired "real" default value in the function body.It feels particularly funny if you real default is None, but None is immutable so... you still need to explicitly use a mutable as the function default parameter, and switch to None in the code.or, similar to @c-z suggestion, if python docs are not enough :-) , you can add an object in between to make the API more explicit (without reading the docs); the used_proxy_ Default class instance is mutable, and will contain the real default value you want to use.now:The above works nicely also for Default(None).Obviously the above patterns looks uglier than they should because of all the print which are there only for showing how they work. Otherwise I find them terse and repeatable enough.You could write a decorator to add the __call__ pattern suggested by @dmg in a more streamlined way, but this will still oblige to use weird tricks in the function definition itself - you would need to split out value and value_default if your code need to distinguish them, so I don't see much advantage and I won't write the example :-) A bit more about #1 python gotcha!, abused for your own pleasure above.

You can see what happens due to the evaluation at definition by doing:You can run testme() as many time as you want, you will always see a reference to the same default instance (so basically your default is immutable :-) ).Remember that in Python there are only 3 mutable built-in types: set, list, dict; everything else - even strings! - is immutable.Not really.  The standard way is to use a default value that the user would not be expected to pass, e.g. an object instance:Usually you can just use None as the default value, if it doesn't make sense as a value the user would want to pass.The alternative is to use kwargs:However this is overly verbose and makes your function more difficult to use as its documentation will not automatically include the param parameter.The following function decorator, explicit_checker, makes a set of parameter names of all the parameters given explicitly. It adds the result as an extra parameter (explicit_params) to the function. Just do 'a' in explicit_params to check if parameter a is given explicitly.I sometimes use a universally unique string (like a UUID).This way, no user could even guess the default if they tried so I can be very confident that when I see that value for arg, it was not passed in.@Ellioh's answer works in python 2. In python 3, the following code should work:This method can keep the argument names and default values (instead of **kwargs) with better readability. You can check it from foo.__defaults__ and foo.__kwdefaults__see a simple example bellowthen by using operator = and is you can compare themand for some cases code bellow is enoughtFor example, you need to avoid changing default value then you can check on equality and then copy if soAlso, it is quite convenient to use getcallargs from inspect as it returns real arguments with which function will be invoked. You pass a function and args and kwargs to it (inspect.getcallargs(func, /, *args, **kwds)), it will return real method's arguments used for invocation, taking into consideration default values and other stuff. Have a look at an example below.https://docs.python.org/3/library/inspect.htmlI agree with Volatility's comment. But you could check in the following manner:I've seen this pattern a few times (e.g. library unittest, py-flags, jinja):...or the equivalent one-liner...:Unlike DEFAULT = object(), this assists type-checking and provides information when errors occur -- frequently either the string representation ("DEFAULT") or the class name ("Default") are used in error messages.A little freakish approach would be:Which outputs:Now this, as I mentioned, is quite freakish, but it does the job. However this is quite unreadable and similarly to ecatmur's suggestion won't be automatically documented.

Is there a standard way to create Debian packages for distributing Python programs?

mac

[Is there a standard way to create Debian packages for distributing Python programs?](https://stackoverflow.com/questions/7110604/is-there-a-standard-way-to-create-debian-packages-for-distributing-python-progra)

There is a ton of information on how to do this, but since "there is more than one way to skin a cat", and all the tutorials/manuals that cover a bit of the process seem to make certain assumptions which are different from other tutorials, I still didn't manage to grasp it.So far this is what I think I understood.Now my questions:BTW: These are the best sources of information that I could find myself so far. If you have anything better than this, please share! :)

2011-08-18 16:10:36Z

There is a ton of information on how to do this, but since "there is more than one way to skin a cat", and all the tutorials/manuals that cover a bit of the process seem to make certain assumptions which are different from other tutorials, I still didn't manage to grasp it.So far this is what I think I understood.Now my questions:BTW: These are the best sources of information that I could find myself so far. If you have anything better than this, please share! :)It looks like stdeb will do what you want.Also, for installing scripts, I strongly recommend distribute's console_scripts entry point support.This article by Barry Warsaw helped me in getting quite far through the process. I still had to do a lot of searching on the side, though, and I read most of the Ubuntu packaging guide some time in the past.Having a good setup.py is a really good advice. I found these two guides quite good:The right way of building a deb package is using dpkg-buildpackage, but sometimes it is a little bit complicated. Instead you can use dpkg -b <folder>, and it will create your Debian package.These are the basics for creating a Debian package with dpkg -b <folder> with any binary or with any kind of script that runs automatically without needing manual compilation (Python, Bash, Perl, and Ruby):Here is an example of the control file. You only need to copy-paste it in to an empty file called "control" and put it in the DEBIAN folder.There are several libraries out there which abstract away all the necessary steps and let you transform your Python package into a Debian package with a single command.Assuming your python package already has the setup.py, in the directory where setup.py is located, you could use:Each of these libraries has its own caveats, so you might want to try what works best for you.This method works for me.

Comma separated lists in django templates

Alasdair

[Comma separated lists in django templates](https://stackoverflow.com/questions/1236593/comma-separated-lists-in-django-templates)

If fruits is the list ['apples', 'oranges', 'pears'],is there a quick way using django template tags to produce "apples, oranges, and pears"?I know it's not difficult to do this using a loop and {% if counter.last %} statements, but because I'm going to use this repeatedly I think I'm going to have to learn how to write custom tags filters, and I don't want to reinvent the wheel if it's already been done.As an extension, my attempts to drop the Oxford Comma (ie return "apples, oranges and pears") are even messier.

2009-08-06 01:50:18Z

If fruits is the list ['apples', 'oranges', 'pears'],is there a quick way using django template tags to produce "apples, oranges, and pears"?I know it's not difficult to do this using a loop and {% if counter.last %} statements, but because I'm going to use this repeatedly I think I'm going to have to learn how to write custom tags filters, and I don't want to reinvent the wheel if it's already been done.As an extension, my attempts to drop the Oxford Comma (ie return "apples, oranges and pears") are even messier.First choice: use the existing join template tag.http://docs.djangoproject.com/en/dev/ref/templates/builtins/#joinHere's their exampleSecond choice: do it in the view.Provide fruits_text to the template for rendering.Here's a super simple solution.  Put this code into comma.html:And now wherever you'd put the comma, include "comma.html" instead:Update: @user3748764 gives us a slightly more compact version, without the deprecated ifequal syntax:Note that it should be used before the element, not after.I would suggest a custom django templating filter rather than a custom tag -- filter is handier and simpler (where appropriate, like here). {{ fruits | joinby:", " }} looks like what I'd want to have for the purpose... with a custom joinby filter:which as you see is simplicity itself!On the Django template this all you need to do for establishing a comma after each fruit. The comma will stop once its reached the last fruit.Here's the filter I wrote to solve my problem (it doesn't include the Oxford comma)To use it in the template: {{ fruits|join_with_commas }}If you want a '.' on the end of Michael Matthew Toomim's answer, then use:I would simply use ', '.join(['apples', 'oranges', 'pears']) before sending it to the template as a context data.UPDATE:You will get apples, oranges and pears output.All of the answers here fail one or more of the following:Here's my entry into this canon. First, the tests:And now the code. Yes, it gets a bit messy, but you'll see that it doesn't use negative indexing:You can use this in a django template with:Django doesn't have support for this out-of-the-box. You can define a custom filter for this:However, if you want to deal with something more complex than just lists of strings, you'll have to use an explicit {% for x in y %} loop in your template.If you like one-liners:and then in the template:I think the simplest solution might be:

How to make python argparse mutually exclusive group arguments without prefix?

Carlo Pires

[How to make python argparse mutually exclusive group arguments without prefix?](https://stackoverflow.com/questions/7869345/how-to-make-python-argparse-mutually-exclusive-group-arguments-without-prefix)

Python2.7 argparse only accepts optional arguments (prefixed) in mutually exclusive groups:$ mydaemon -hIs there a way to make argparse arguments behaves like traditional unix daemon control:

2011-10-23 21:27:18Z

Python2.7 argparse only accepts optional arguments (prefixed) in mutually exclusive groups:$ mydaemon -hIs there a way to make argparse arguments behaves like traditional unix daemon control:For all the abilities and options in argparse I don't think you'll ever get a "canned" usage string that looks like what you want.That said, have you looked at sub-parsers since your original post?Here's a barebones implementation:Running this with the -h option yields:One of the benefits of this approach is being able to use set_defaults for each sub-parser to hook up a function directly to the argument.  I've also added a "graceful" option for stop and restart:Showing the "help" message for stop:Stopping "gracefully":from pymotw  output:version2It sounds like you want a positional argument instead of mutually exclusive options.  You can use 'choices' to restrict the possible acceptable options.This produces a usage line that looks like this:Building on Adam's answer... if you wanted to specify a default you could always do the following so they can effectively leave it blank.which will print:

how to split an iterable in constant-size chunks

mathieu

[how to split an iterable in constant-size chunks](https://stackoverflow.com/questions/8290397/how-to-split-an-iterable-in-constant-size-chunks)

I am surprised I could not find a "batch" function that would take as input an iterable and return an iterable of iterables.For example:or:Now, I wrote what I thought was a pretty simple generator:But the above does not give me what I would have expected:So, I have missed something and this probably shows my complete lack of understanding of python generators. Anyone would care to point me in the right direction ?[Edit: I eventually realized that the above behavior happens only when I run this within ipython rather than python itself]

2011-11-28 00:52:46Z

I am surprised I could not find a "batch" function that would take as input an iterable and return an iterable of iterables.For example:or:Now, I wrote what I thought was a pretty simple generator:But the above does not give me what I would have expected:So, I have missed something and this probably shows my complete lack of understanding of python generators. Anyone would care to point me in the right direction ?[Edit: I eventually realized that the above behavior happens only when I run this within ipython rather than python itself]This is probably more efficient (faster)It avoids building new lists.FWIW, the recipes in the itertools module provides this example:It works like this:As others have noted, the code you have given does exactly what you want.  For another approach using itertools.islice you could see an example of following recipe:Weird, seems to work fine for me in Python 2.xI just gave one answer. However, now I feel the best solution might be not writing any new functions. More-itertools includes plenty of additional tools, and chunked is amongst them.This is a very short code snippet I know that does not use len and works under both Python 2 and 3 (not my creation):This is what I use in my project. It handles iterables or lists as efficiently as possible.Here is an approach using reduce function.Oneliner:Or more readable version:Test:This would work for any iterable. It would work like this:PS: It would not work if iterable has None values.You can just group iterable items by their batch index.It is often the case when you want to collect inner iterables so here is more advanced version.Examples:

Python, remove all non-alphabet chars from string

KDecker

[Python, remove all non-alphabet chars from string](https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string)

I am writing a python MapReduce word count program. Problem is that there are many non-alphabet chars strewn about in the data, I have found this post Stripping everything but alphanumeric chars from a string in Python which shows a nice solution using regex, but I am not sure how to implement itI'm afraid I am not sure how to use the library re or even regex for that matter. I am not sure how to apply the regex pattern to the incoming string (line of a book) v properly to retrieve the new line without any non-alphanumeric chars.Suggestions?

2014-03-20 00:16:52Z

I am writing a python MapReduce word count program. Problem is that there are many non-alphabet chars strewn about in the data, I have found this post Stripping everything but alphanumeric chars from a string in Python which shows a nice solution using regex, but I am not sure how to implement itI'm afraid I am not sure how to use the library re or even regex for that matter. I am not sure how to apply the regex pattern to the incoming string (line of a book) v properly to retrieve the new line without any non-alphanumeric chars.Suggestions?Use re.subAlternatively, if you only want to remove a certain set of characters (as an apostrophe might be okay in your input...)If you prefer not to use regex, you might tryYou can use the re.sub() function to remove these characters:re.sub(MATCH PATTERN, REPLACE STRING, STRING TO SEARCH)Try:This will take every char from the string, keep only alphanumeric ones and build a string back from them.The fastest method is regex

Python: how to join entries in a set into one string?

Spencer

[Python: how to join entries in a set into one string?](https://stackoverflow.com/questions/7323782/python-how-to-join-entries-in-a-set-into-one-string)

Basically, I am trying to join together the entries in a set in order to output one string. I am trying to use syntax similar to the join function for lists. Here is my attempt:However I get this error: AttributeError: 'set' object has no attribute 'join'What is the equivalent call for sets? 

2011-09-06 17:30:37Z

Basically, I am trying to join together the entries in a set in order to output one string. I am trying to use syntax similar to the join function for lists. Here is my attempt:However I get this error: AttributeError: 'set' object has no attribute 'join'What is the equivalent call for sets? The join is a string method, not a set method.Sets don't have a join method but you can use str.join instead.The str.join method will work on any iterable object including lists and sets.Note: be careful about using this on sets containing integers; you will need to convert the integers to strings before the call to join.  For exampleThe join is called on the string:Nor the set nor the list has such method join, string has it:By the way you should not use name list for your variables. Give it a list_, my_list or some other name because list is very often used python function.Set's do not have an order - so you may lose your order when you convert your list into a set, i.e.:Generally the order will remain, but for large sets it almost certainly won't.Finally, just incase people are wondering, you don't need a ', ' in the join. Just:  ''.join(set):)I think you just have it backwards.You have the join statement backwards try:I wrote a method that handles the following edge-cases:

Generating permutations with repetitions

Bwmat

[Generating permutations with repetitions](https://stackoverflow.com/questions/3099987/generating-permutations-with-repetitions)

I know about itertools, but it seems it can only generate permutations without repetitions.For example, I'd like to generate all possible dice rolls for 2 dice. So I need all permutations of size 2 of [1, 2, 3, 4, 5, 6] including repetitions: (1, 1), (1, 2), (2, 1)... etcIf possible I don't want to implement this from scratch

2010-06-23 08:17:14Z

I know about itertools, but it seems it can only generate permutations without repetitions.For example, I'd like to generate all possible dice rolls for 2 dice. So I need all permutations of size 2 of [1, 2, 3, 4, 5, 6] including repetitions: (1, 1), (1, 2), (2, 1)... etcIf possible I don't want to implement this from scratchYou are looking for the Cartesian Product.In your case, this would be {1, 2, 3, 4, 5, 6} x {1, 2, 3, 4, 5, 6}.

itertools can help you there:To get a random dice roll (in a totally inefficient way):You're not looking for permutations - you want the Cartesian Product. For this use product from itertools:In python 2.7 and 3.1 there is a itertools.combinations_with_replacement function:In this case, a list comprehension is not particularly needed.GivenCodeDetailsUnobviously, Cartesian product can generate subsets of permutations. However, it follows that:Permutations with replacement, n**rPermutations without replacement, n! First, you'll want to turn the generator returned by itertools.permutations(list) into a list first. Then secondly, you can use set() to remove duplicates

Something like below:

Get date from week number

Ali SAID OMAR

[Get date from week number](https://stackoverflow.com/questions/17087314/get-date-from-week-number)

Please what's wrong with my code: Display "2013-01-01 00:00:00", Thanks.

2013-06-13 12:39:50Z

Please what's wrong with my code: Display "2013-01-01 00:00:00", Thanks.A week number is not enough to generate a date; you need a day of the week as well. Add a default:The -1 and -%w pattern tells the parser to pick the Monday in that week. This outputs:%W uses Monday as the first day of the week. While you can pick your own weekday, you may get unexpected results if you deviate from that.See the strftime() and strptime() behaviour section in the documentation, footnote 4:Note, if your week number is a ISO week date, you'll want to use %G-W%V-%u instead! Those directives require Python 3.6 or newer.To complete the other answers - if you are using ISO week numbers, this string is appropriate (to get the Monday of a given ISO week number):%G, %V, %u are ISO equivalents of %Y, %W, %w, so this outputs:Availabe in Python 3.6+; from docs.Adding of 1 as week day will yield exact current week start. Adding of timedelta(days=6) will gives you the week end.In case you have the yearly number of week, just add the number of weeks to the first day of the year.In Python 3.8 there is the handy datetime.date.fromisocalendar:In older Python versions (3.7-) the calculation can use the information from  datetime.date.isocalendar to figure out the week ISO8601 compliant weeks:Both works also with datetime.datetime.Here's a handy function including the issue with zero-week.

How do you send an HTTP Get Web Request in Python? [duplicate]

LeChosenOne

[How do you send an HTTP Get Web Request in Python? [duplicate]](https://stackoverflow.com/questions/17178483/how-do-you-send-an-http-get-web-request-in-python)

I am having trouble sending data to a website and getting a response in Python. I have seen similar questions, but none of them seem to accomplish what I am aiming for.This is my C# code I'm trying to port to Python:Here is my attempt in Python:Any help would be appreciated!

2013-06-18 20:39:55Z

I am having trouble sending data to a website and getting a response in Python. I have seen similar questions, but none of them seem to accomplish what I am aiming for.This is my C# code I'm trying to port to Python:Here is my attempt in Python:Any help would be appreciated!You can use urllib2Also you can use httplibor the requests libraryIn Python, you can use urllib2 (http://docs.python.org/2/library/urllib2.html) to do all of that work for you. Simply enough:Will print the received HTTP response.To pass GET/POST parameters the urllib.urlencode() function can be used. For more information, you can refer to the Official Urllib2 Tutorial

In what case would I use a tuple as a dictionary key?

Escualo

[In what case would I use a tuple as a dictionary key?](https://stackoverflow.com/questions/1938614/in-what-case-would-i-use-a-tuple-as-a-dictionary-key)

I was studying the difference between lists and tuples (in Python). An obvious one is that tuples are immutable (the values cannot be changed after initial assignment), while lists are mutable.A sentence in the article got me: I have a hard time thinking of a situation where I would like to use a tuple as a dictionary key. Can you provide an example problem where this would be the natural, efficient, elegant, or obvious solution?Edit:Thanks for your examples. So far I take that a very important application is the caching of function values.

2009-12-21 07:01:29Z

I was studying the difference between lists and tuples (in Python). An obvious one is that tuples are immutable (the values cannot be changed after initial assignment), while lists are mutable.A sentence in the article got me: I have a hard time thinking of a situation where I would like to use a tuple as a dictionary key. Can you provide an example problem where this would be the natural, efficient, elegant, or obvious solution?Edit:Thanks for your examples. So far I take that a very important application is the caching of function values.Classic Example: You want to store point value as tuple of  (x, y) EDIT 1

Of course you can do salaries['John Smith'] = whatever, but then you'll have to do extra work to separate the key into first and last names. What about pointColor[(x, y, z)] = "red", here the benefit of tuple key is more prominent. I must stress out that this is not the best practice. In many cases you better create special classes to handle situations like that, but Arrieta asked for examples, which I gave her (him).EDIT 0By the way, each tuple element has to be hashable too:I use tuple lots of time as dict key e.g.I used tuples as dictionary keys in application that compares network devices by geographical location.  Since the devices are named similarly for each location, it provides a natural way to know if a device matching that pairing has been seen yet while processing multiples.i.e.You use tuples as keys when you want to show multiple elements which form a key together.Eg: {(<x-coordinate>,<y-coordinate>): <indicating letter>}Here if we use x-coordinate or y-coordinate separately, we wouldn't be representing that point.In the context of Machine Learning and Deep Learning, if you're doing hyperparameter search for the best hyperparameters, then using tuples as keys is definitely super useful.Let's say you're searching for the best hyperparameter combination for learning_rate, regularization_factor, and model_complexity.Then you can have a dictionary in Python where you make the different combination that these hparams can take as keys and their corresponding weight matrices from the training algorithm as valuesThese weight matrices are further needed to make realtime prediction.I suppose in the case of sorting, there could be merit in using a tuple.  For example, suppose the dictionary key represents a sort field (obviously there would be a default sort field to prevent the key from being None).  If you needed multiple sort fields, such as the case of sorting by last name, then first name, wouldn't using a tuple as the dictionary key be a good idea?Sure, such an idea might have limited use, but that doesn't mean it is completely useless.You can use it for funnel analysis if you are building a basic analytics tool.For example, counting how many people clicked the image3 after hovering on text2.You can use it for approx constant time search of a point in search space. For example you can use it for constraint satisfaction problem, where each tuple might contain some constraints. Constraint might be of the form (v1.v2) where color(v1)!=color(v2) for coloring prob, etc. 

     Using tuples as dictionary keys, you will able to tell in constant time whether a permutation satisfies a constraint or not. 

How to check type of files without extensions in python?

emnoor

[How to check type of files without extensions in python?](https://stackoverflow.com/questions/10937350/how-to-check-type-of-files-without-extensions-in-python)

I have a folder full of files and they don't have an extension. How can I check file types? I want to check the file type and change the filename accordingly. Let's assume a function filetype(x) returns a file type like png. I want to do this:How do I do this?

2012-06-07 18:06:40Z

I have a folder full of files and they don't have an extension. How can I check file types? I want to check the file type and change the filename accordingly. Let's assume a function filetype(x) returns a file type like png. I want to do this:How do I do this?There are Python libraries that can recognize files based on their content (usually a header / magic number) and that don't rely on the file name or extension.If you're addressing many different file types, you can use python-magic.  That's just a Python binding for the well-established magic library.  This has a good reputation and (small endorsement) in the limited use I've made of it, it has been solid.There are also libraries for more specialized file types.  For example, the Python standard library has the imghdr module that does the same thing just for image file types.If you need dependency-free (pure Python) file type checking, see filetype.The Python Magic library provides the functionality you need.You can install the library with pip install python-magic and use it as follows:The Python code in this case is calling to libmagic beneath the hood, which is the same library used by the *NIX file command. Thus, this does the same thing as the subprocess/shell-based answers, but without that overhead.On unix and linux there is the file command to guess file types.  There's even a windows port.From the man page:You would need to run the file command with the subprocess module and then parse the results to figure out an extension.edit:  Ignore my answer.  Use Chris Johnson's answer instead.You can also install the official file binding for Python, a library called file-magic (it does not use ctypes, like python-magic).It's available on PyPI as file-magic and on Debian as python-magic. For me this library is the best to use since it's available on PyPI and on Debian (and probably other distributions), making the process of deploying your software easier.

I've blogged about how to use it, also.As Steven pointed out, subprocess is the way. You can get the command output by the way above as this post saidIn the case of images, you can use the imghdr module.Python 2 imghdr doc

Python 3 imghdr docWith newer subprocess library, you can now use the following code (*nix only solution):Only works for Linux but Using the "sh" python module you can simply call any shell commandhttps://pypi.org/project/sh/ pip install shOutput:

/root/file: ASCII textalso you can use this code (pure python by 3 byte of header file):

Accessing a class' member variables in Python?

Adam

[Accessing a class' member variables in Python?](https://stackoverflow.com/questions/3434581/accessing-a-class-member-variables-in-python)

How do I access a class's variable?  I've tried adding this definition:Yet, that fails also.

2010-08-08 14:02:08Z

How do I access a class's variable?  I've tried adding this definition:Yet, that fails also.The answer, in a few wordsIn your example, itsProblem is a local variable.Your must use self to set and get instance variables. You can set it in the __init__ method. Then your code would be:But if you want a true class variable, then use the class name directly:But be careful with this one, as theExample.itsProblem is automatically set to be equal to Example.itsProblem, but is not the same variable at all and can be changed independently.Some explanationsIn Python, variables can be created dynamically. Therefore, you can do the following:prints Therefore, that's exactly what you do with the previous examples.Indeed, in Python we use self as this, but it's a bit more than that. self is the the first argument to any object method because the first argument is always the object reference. This is automatic, whether you call it self or not.Which means you can do:or:It's exactly the same. The first argument of ANY object method is the current object, we only call it self as a convention. And you add just a variable to this object, the same way you would do it from outside.Now, about the class variables. When you do:You'll notice we first set a class variable, then we access an object (instance) variable. We never set this object variable but it works, how is that possible?Well, Python tries to get first the object variable, but if it can't find it, will give you the class variable. Warning: the class variable is shared among instances, and the object variable is not.As a conclusion, never use class variables to set default values to object variables. Use __init__ for that.Eventually, you will learn that Python classes are instances and therefore objects themselves, which gives new insight to understanding the above. Come back and read this again later, once you realize that.You are declaring a local variable, not a class variable. To set an instance variable (attribute), useTo set a class variable (a.k.a. static member), useIf you have an instance function (i.e. one that gets passed self) you can use self to get a reference to the class using self.__class__For example in the code below tornado creates an instance to handle get requests, but we can get hold of the get_handler class and use it to hold a riak client so we do not need to create one for every request.  Implement the return statement like the example below! You should be good. I hope it helps someone..

Annotate bars with values on Pandas bar plots

leroygr

[Annotate bars with values on Pandas bar plots](https://stackoverflow.com/questions/25447700/annotate-bars-with-values-on-pandas-bar-plots)

I was looking for a way to annotate my bars in a Pandas bar plot with the rounded numerical values from my DataFrame.I would like to get something like this:I tried with this code sample, but the annotations are all centered on the x ticks:

2014-08-22 13:01:37Z

I was looking for a way to annotate my bars in a Pandas bar plot with the rounded numerical values from my DataFrame.I would like to get something like this:I tried with this code sample, but the annotations are all centered on the x ticks:You get it directly from the axes' patches:You'll want to tweak the string formatting and the offsets to get things centered, maybe use the width from p.get_width(), but that should get you started. It may not work with stacked bar plots unless you track the offsets somewhere.Solution which also handles the negative values with sample float formatting.Still needs tweaking offsets.

Python Pandas replace NaN in one column with value from corresponding row of second column

edesz

[Python Pandas replace NaN in one column with value from corresponding row of second column](https://stackoverflow.com/questions/29177498/python-pandas-replace-nan-in-one-column-with-value-from-corresponding-row-of-sec)

I am working with this Pandas DataFrame in Python 2.7.I need to replace all NaNs in the Temp_Rating column with the value from the Farheit column.This is what I need:If I do a Boolean selection, I can pick out only one of these columns at a time. The problem is if I then try to join them, I am not able to do this while preserving the correct order.How can I only find Temp_Rating rows with the NaNs and replace them with the value in the same row of the Farheit column?

2015-03-20 23:43:01Z

I am working with this Pandas DataFrame in Python 2.7.I need to replace all NaNs in the Temp_Rating column with the value from the Farheit column.This is what I need:If I do a Boolean selection, I can pick out only one of these columns at a time. The problem is if I then try to join them, I am not able to do this while preserving the correct order.How can I only find Temp_Rating rows with the NaNs and replace them with the value in the same row of the Farheit column?Assuming your DataFrame is in df:First replace any NaN values with the corresponding value of df.Farheit. Delete the 'Farheit' column. Then rename the columns. Here's the resulting DataFrame:The above mentioned solutions did not work for me. The method I used was:An other way to solve this problem,returns:

Any tutorials for developing chatbots? [closed]

Surya

[Any tutorials for developing chatbots? [closed]](https://stackoverflow.com/questions/9706769/any-tutorials-for-developing-chatbots)

As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.

2012-03-14 17:14:39Z

As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.You can read a nice introduction to various techniques used to design chatbots here: http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.phpAlso, here are a few useful links:The Natural Language Toolkit (python) implements a few chatbots: http://nltk.github.com/api/nltk.chat.htmlSimple pipeline architecture for a spoken dialogue system from the book Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit By Steven Bird, Ewan Klein, Edward Loper:The two places I would start with are how cleverbot works [part of a podcast] and then go through the Natural Language Toolkit Book to learn about the algorithms to use. (NLTK uses python, but the book is also a python tutorial)

How to save a figure remotely with pylab? [duplicate]

gerry

[How to save a figure remotely with pylab? [duplicate]](https://stackoverflow.com/questions/4706451/how-to-save-a-figure-remotely-with-pylab)

I'm trying to generate a figure at a remote computer with the command pylab.savefig.

But I got such error:How can I save the figure properly?

2011-01-16 16:11:59Z

I'm trying to generate a figure at a remote computer with the command pylab.savefig.

But I got such error:How can I save the figure properly?By default, matplotlib will use something like the TkAgg backend.  This requires an X-server to be running.While you can just use X-forwarding, there will be a noticeable lag as matplotlib tries to connect with the remote X-server.  If you don't need to interact with the plot, it's often nicer to speed things up by avoiding an X-connection entirely.If you want to make a plot without needing an X-server at all, use the Agg backend  instead.E.g. do something like this:If you want this to be the default behavior, you can modify your matplotlibrc file to use the Agg backend by default.See this article for more information.Try setting the DISPLAY variable to the appropriate value. Graphics over the network using X11 work by the client (remote) computer having a DISPLAY environment variable that says where to draw the graphics. Typically it would be something like mydesktop.example.com:0.0 - then when an X11 program tries to draw something, it gets whizzed over the network to mydesktop.example.com, which is the machine you are sitting in front of (the X server) and up it pops.Now, if the machine in front of you is Windows, then you'll need to get an X server from somewhere - cygwin/X11 or commercial eXceed will do nicely.You also need to make sure security is handled - you cant just have anyone writing to your screen over the network.How are you connecting to the remote machine? Because if you are going from a Linux box to another Linux box with ssh then the simple solution is probably 'Use ssh -X foo.example.com' to connect - this pipes the X11 connection over a local socket.So, if ssh -X isnt the answer, can we have some more info on the operating systems involved please?

Overriding urllib2.HTTPError or urllib.error.HTTPError and reading response HTML anyway

Backus

[Overriding urllib2.HTTPError or urllib.error.HTTPError and reading response HTML anyway](https://stackoverflow.com/questions/2233687/overriding-urllib2-httperror-or-urllib-error-httperror-and-reading-response-html)

I receive a 'HTTP Error 500: Internal Server Error' response, but I still want to read the data inside the error HTML.With Python 2.6, I normally fetch a page using:When attempting to use this on the failing URL, I get the exception urllib2.HTTPError:How can I fetch such error pages (with or without urllib2), all while they are returning Internal Server Errors?Note that with Python 3, the corresponding exception is urllib.error.HTTPError.

2010-02-10 00:55:01Z

I receive a 'HTTP Error 500: Internal Server Error' response, but I still want to read the data inside the error HTML.With Python 2.6, I normally fetch a page using:When attempting to use this on the failing URL, I get the exception urllib2.HTTPError:How can I fetch such error pages (with or without urllib2), all while they are returning Internal Server Errors?Note that with Python 3, the corresponding exception is urllib.error.HTTPError.The HTTPError is a file-like object.  You can catch it and then read its contents.If you mean you want to read the body of the 500:In your case, you don't need to build up the request.  Just doso, you don't override urllib2.HTTPError, you just handle the exception.

Easiest way to replace a string using a dictionary of replacements?

meder omuraliev

[Easiest way to replace a string using a dictionary of replacements?](https://stackoverflow.com/questions/2400504/easiest-way-to-replace-a-string-using-a-dictionary-of-replacements)

Consider..I'd like to replace all dict keys with their respective dict values in s.

2010-03-08 10:08:23Z

Consider..I'd like to replace all dict keys with their respective dict values in s.Using re:This will match whole words only. If you don't need that, use the pattern:Note that in this case you should sort the words descending by length if some of your dictionary entries are substrings of others.You could use the reduce function:Solution found here (I like its simplicity):one way, without reAlmost the same as ghostdog74, though independently created. One difference,

using d.get() in stead of d[] can handle items not in the dict.I used this in a similar situation (my string was all in uppercase):hope that helps in some way... :)With the warning that it fails if key has space, this is a compressed solution similar to ghostdog74 and extaneons answers:

Understanding nested list comprehension

eugene

[Understanding nested list comprehension](https://stackoverflow.com/questions/8049798/understanding-nested-list-comprehension)

I want to understand nested list comprehension.

Below, I listed a list comprehension expression and their for loop equivalent.

I wonder if my understanding is correct on those.   For example,is equivalent to If I may generalize, I guess form can be translated to the following. (I hope I'm correct on this)For simpler case, is equal towhereas,is equal toI asked a similar question on Equivalent for loop expression for complex list comprehension

The answers given there reconstruct the form after understanding what it does internally.

I'd like to know how it works systematically so I can apply the concept to other slightly varying examples.  

2011-11-08 11:38:53Z

I want to understand nested list comprehension.

Below, I listed a list comprehension expression and their for loop equivalent.

I wonder if my understanding is correct on those.   For example,is equivalent to If I may generalize, I guess form can be translated to the following. (I hope I'm correct on this)For simpler case, is equal towhereas,is equal toI asked a similar question on Equivalent for loop expression for complex list comprehension

The answers given there reconstruct the form after understanding what it does internally.

I'd like to know how it works systematically so I can apply the concept to other slightly varying examples.  The short answer is: yes, you are correct in your understanding.There's only a catch: the way you normally use nested list comprehension in python code is to operate on multidimensional arrays.A typical example is when you operate on matrices:As you can see the "nesting" works by operating on each dimension of the matrix.In the examples you provided, it seems that ySet [unfortunate name btw, as sets are one of the types provided with python] is just a generic counter, which makes a bit harder to follow what is going on under the hood.As for your first example:You might wish to look into the zip built-in function:or for maximum brevity and elegance:HTH!Indeed, you are correct. This is described in detail in the Expressions section in the Python Language Reference.Note especially the order of nesting of several fors in a single list comprehension, which is always left-to-right:

Are python variables pointers? or else what are they?

Lior

[Are python variables pointers? or else what are they?](https://stackoverflow.com/questions/13530998/are-python-variables-pointers-or-else-what-are-they)

Variables in Python are just pointers, as far as I know.  Based on this rule, I can assume that the result for this code snippet:would be 3.

But I got an unexpected result for me, it was 5.Moreover, my Python book does cover this example:the result would be [5,2,3].What am I understanding wrong?

2012-11-23 14:23:39Z

Variables in Python are just pointers, as far as I know.  Based on this rule, I can assume that the result for this code snippet:would be 3.

But I got an unexpected result for me, it was 5.Moreover, my Python book does cover this example:the result would be [5,2,3].What am I understanding wrong?We call them references. They work like thisSmall ints are interned, but that isn't important to this explanationVariables are not pointers. When you assign to a variable you are binding the name to an object. From that point onwards you can refer to the object by using the name, until that name is rebound.In your first example the name i is bound to the value 5. Binding different values to the name j does not have any effect on i, so when you later print the value of i the value is still 5.In your second example you bind both i and j to the same list object. When you modify the contents of the list, you can see the change regardless of which name you use to refer to the list.Note that it would be incorrect if you said "both lists have changed". There is only one list but it has two names (i and j) that refer to it.Related documentationFrom the docs:When you dothat's the same as doing:j doesn't point to i, and after the assignment, j doesn't know that i exists. j is simply bound to whatever i was pointing to at the time of assignment.If you did the assignments on the same line, it would look like this:And the result would be exactly the same. Thus, later doing doesn't change what j is pointing to - and you can swap it - j = 3 would not change what i is pointing to.So when you do this:It's the same as doing this:so i and j both point to the same list. Then your example mutates the list:Python lists are mutable objects, so when you change the list from one reference, and you look at it from another reference, you'll see the same result because it's the same list.TLDR: Python names work like pointers with automatic de/referencing but do not allow explicit pointer operations. Other targets represent indirections, which behave similar to pointers.The CPython implementation uses pointers of type PyObject* under the hood. As such, it is possible to translate name semantics to pointer operations. The key is to separate names from actual objects.The example Python code includes both names (i) and objects (5).This can be roughly translated to C code with separate names and objects.The important part is that "names-as-pointers" do not store objects! We did not define *i = five, but i = &five. The names and objects exist independent from each other.Names only point to existing objects in memory.When assigning from name to name, no objects are exchanged! When we define j = i, this is equivalent to j = &five. Neither i nor j are connected to the other.As a result, changing the target of one name does not affect the other. It only updates what that specific name points to.Python also has other kinds of name-like elements: attribute references (i.j), subscriptions (i[j]) and slicing (i[:j]). Unlike names, which refer directly to objects, all three indirectly refer to elements of objects.The example code includes both names (i) and a subscription (i[0]).A CPython list uses a C array of PyObject* pointers under the hood. This can again be roughly translated to C code with separate names and objects.The important part is that we did not change any names! We did change i->elements[0], the element of an object both our names point to.Values of existing compound objects may be changed.When changing the value of an object through a name, names are not changed. Both i and j still refer to the same object, whose value we can change.The intermediate object behaves similar to a pointer in that we can directly change what it points to and reference it from multiple names.They are not quite pointers, they are references to objects. Objects can be either mutable, or immutable. An immutable object is copied when it is modified. A mutable object is altered in-place. An integer is an immutable object, that you reference by your i and j variables. A list is a mutable object.In your first exampleIn your second example:When you set j=3 the label j no longer applies (points) to i, it starts to point to the integer 3. The name i is still referring to the value you set originally, 5.what ever variable is on the left side of '=' sign is assigned with the value on the right side of '='i = 5j = i  --- j has 5 j = 3  --- j has 3 (overwrites the value of 5) but nothing has been changed regarding iprint(i)-- so this prints 5Assignment doesn't modify objects; all it does is change where the variable points.  Changing where one variable points won't change where another one points.You are probably thinking of the fact that lists and dictionaries are mutable types.  There are operators to modify the actual objects in-place, and if you use one of those, you will see the change in all variables pointing to the same object:But assignment still just moves the pointer around:Numbers, unlike dictionaries and lists, are immutable. If you do x = 3; x += 2, you aren't transforming the number 3 into the number 5; you're just making the variable x point to 5 instead. The 3 is still out there unchanged, and any variables pointing to it will still see 3 as their value.(In the actual implementation, numbers are probably not reference types at all; it's more likely that the variables actually contain a representation of the value directly rather than pointing to it. But that implementation detail doesn't change the semantics where immutable types are concerned.)In Python, everything is object including the memory pieces themselves that you are returned. That means, when new memory chunk is created (irrespective of what've you created: int, str, custom object etc.), you have a new memory object. In your case this is the assignment to 3 which creates a new (memory) object and thus has a new address. If you run the following, you see what I mean easily.IMO, memory wise, this is the key understanding/difference between C and Python. In C/C++, you're returned a memory pointer (if you use pointer syntax of course) instead of a memory object which gives you more flexibility in terms of changing the referred address.

Editing specific line in text file in Python

test

[Editing specific line in text file in Python](https://stackoverflow.com/questions/4719438/editing-specific-line-in-text-file-in-python)

Let's say I have a text file containing:Is there a way I can edit a specific line in that text file? Right now I have this:Yes, I know that myfile.writelines('Mage')[1] is incorrect. But you get my point, right? I'm trying to edit line 2 by replacing Warrior with Mage. But can I even do that?

2011-01-18 00:41:03Z

Let's say I have a text file containing:Is there a way I can edit a specific line in that text file? Right now I have this:Yes, I know that myfile.writelines('Mage')[1] is incorrect. But you get my point, right? I'm trying to edit line 2 by replacing Warrior with Mage. But can I even do that?You want to do something like this:The reason for this is that you can't do something like "change line 2" directly in a file. You can only overwrite (not delete) parts of a file - that means that the new content just covers the old content. So, if you wrote 'Mage' over line 2, the resulting line would be 'Mageior'.you can use fileinput to do in place editingAnd then:You can do it in two ways, choose what suits your requirement:Method I.) Replacing using line number. You can use built-in function enumerate() in this case:First, in read mode get all data in a variableSecond, write to the file (where enumerate comes to action) Method II.) Using the keyword you want to replace:Open file in read mode and copy the contents to a list"Warrior" has been replaced by "Mage", so write the updated data to the file:This is what the output will be in both cases:If your text contains only one individual:If your text contains several individuals:import reIf the「job「 of an individual was of a constant length in the texte, you could change only the portion of texte corresponding to the「job「 the desired individual:

that’s the same idea as senderle’s one.But according to me, better would be to put the characteristics of individuals in a dictionnary recorded in file with cPickle:I have been practising working on files this evening and realised that I can build on Jochen's answer to provide greater functionality for repeated/multiple use. Unfortunately my answer does not address issue of dealing with large files but does make life easier in smaller files.Suppose I have a file named file_name as following:We have to replace line 2 with "modification is done":

Install only available packages using「conda install --yes --file requirements.txt」without error

cdeepakroy

[Install only available packages using「conda install --yes --file requirements.txt」without error](https://stackoverflow.com/questions/35802939/install-only-available-packages-using-conda-install-yes-file-requirements-t)

While installing packages in requirements.txt using Conda through the following commandIf a package in requirements.txt is not available, then it throws a "No package error" such as the one shown below:Instead of throwing an error, is it possible to change this behavior such that it installs all the available packages in requirements.txt and throws a warning for those that are not available?I would like this because, the package nimfa which the error says is not available, can be pip installed. So if I can change the behavior of conda install --yes --file requirements.txt to just throw a warning for unavailable packages, I can follow it up with the command pip install -r requirments.txt in .travis.yml so TravisCI attempts to install it from either place where it is available.

2016-03-04 17:58:21Z

While installing packages in requirements.txt using Conda through the following commandIf a package in requirements.txt is not available, then it throws a "No package error" such as the one shown below:Instead of throwing an error, is it possible to change this behavior such that it installs all the available packages in requirements.txt and throws a warning for those that are not available?I would like this because, the package nimfa which the error says is not available, can be pip installed. So if I can change the behavior of conda install --yes --file requirements.txt to just throw a warning for unavailable packages, I can follow it up with the command pip install -r requirments.txt in .travis.yml so TravisCI attempts to install it from either place where it is available.I ended up just iterating over the lines of the file$ while read requirement; do conda install --yes $requirement; done < requirements.txtEdit: If you would like to install a package using pip if it is not available through conda, give this a go:$ while read requirement; do conda install --yes $requirement || pip install $requirement; done < requirements.txtEdit: If you are using Windows (credit goes to @Clay):$ FOR /F "delims=~" %f in (requirements.txt) DO conda install --yes "%f" || pip install "%f"You can do this as mentioned in thisExport to .yml fileTo reproduce:For those looking, I used this as @TillHoffmann 's solution for the fish shell:And Pbms's answer here is the right way to do it, assuming you have an existing environment to copy off of.  Conda is fully capable of installing both Conda packages and pip packages, as listed in environment.yml.  I wanted to document the whole process in more detail.  Note that I am using folder-based environments, which is why I added --prefix [path to environment folder] to most of the commands.Say you installed an environment for an existing project to a folder called env in the current folder, like this:You'd generate environment.yml for that project's environment like this:You'd create a new environment within some other folder by copying environment.yml to there and then running this from there:You'd get an already-existing environment to match environment.yml by once again copying environment.yml to there and then running this from there:With the environment in question active, you'd verify the state of its packages like this:This is an abridged version of what that command might print (note that the pip packages are marked pypi):Finally, this is an abridged version of what environment.yml might look like (note that the pip packages are listed in their own category):Be aware that using Conda and pip together can cause some heartburn because they can unknowingly blow away each other's dependencies.  You are supposed to install all of your Conda packages first and then all of your pip packages afterward, rather than alternating between the two.  If your environment breaks, the official recommendation is to delete and recreate it (from your environment.yml file).  For more details, see this guide:https://www.anaconda.com/using-pip-in-a-conda-environment/

Python unittest.TestCase execution order

Mike

[Python unittest.TestCase execution order](https://stackoverflow.com/questions/5387299/python-unittest-testcase-execution-order)

Is there a way in Python unittest to set the order in which test cases are run? In my current TestCase class, some testcases have side-effects that set conditions for the others to run properly. Now I realize the proper way to do this is to use setUp() to do all setup realted things, but I would like to implement a design where each successive test builds slightly more state that the next can use. I find this much more elegant.Ideally, I would like the tests to be run in the order they appear in the class. It appears that they run in alphabetical order.

2011-03-22 05:40:12Z

Is there a way in Python unittest to set the order in which test cases are run? In my current TestCase class, some testcases have side-effects that set conditions for the others to run properly. Now I realize the proper way to do this is to use setUp() to do all setup realted things, but I would like to implement a design where each successive test builds slightly more state that the next can use. I find this much more elegant.Ideally, I would like the tests to be run in the order they appear in the class. It appears that they run in alphabetical order.Don't make them independent tests - if you want a monolithic test, write a monolithic test.If the test later starts failing and you want information on all failing steps instead of halting the test case at the first failed step, you can use the subtests feature: https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests(The subtest feature is available via unittest2 for versions prior to Python 3.4: https://pypi.python.org/pypi/unittest2 )Its a good practice to always write a monolithic test for such expectations, however if yer a goofy dude like me, then you could simply write ugly looking methods in alphabetical order so that they are sorted from a to b as mentioned in the python docs http://docs.python.org/library/unittest.htmlEXAMPLE:http://docs.python.org/library/unittest.htmlSo just make sure test_setup's name has the smallest string value.Note that you should not rely on this behavior — different test functions are supposed to be independent of the order of execution. See ngcohlan's answer above for a solution if you explicitly need an order.Old question, but another way that I didn't see listed in any related questions: Use a TestSuite.Another way to accomplish ordering is to add the tests to a unitest.TestSuite. This seems to respect the order in which the tests are added to the suite using suite.addTest(...). To do this:For context, I had need for this and wasn't satisfied with the other options. I settled on the above way of doing test ordering. I didn't see this TestSuite method listed any of the several "unit-test ordering questions" (e.g., this question and others including execution order, or changing order, or tests order).I ended up with a simple solution that worked for me:And thenTests which really depend on each other should be explicitly chained into one test.Tests which require different levels of setup, could also have their corresponding setUp() running enough setup - various ways thinkable.Otherwise unittest handles the test classes and test methods inside the test classes in alphabetical order by default (even when loader.sortTestMethodsUsing is None). dir() is used internally which sorts by guarantee.The latter behavior can be exploited for practicability - e.g. for having the latest-work-tests run first to speed up the edit-testrun-cycle.

But that behavior should not be used to establish real dependencies. Consider that tests can be run individually via command-line options etc.@ncoghlan's answer was exactly what I was looking for when I came to this thread. I ended up modifying it to allow each step-test to run, even if a previous step had already thrown an error; this helps me (and maybe you!) to discover and plan for the propagation of error in multi-threaded database-centric software.

How to add a custom CA Root certificate to the CA Store used by pip in Windows?

Eric B.

[How to add a custom CA Root certificate to the CA Store used by pip in Windows?](https://stackoverflow.com/questions/39356413/how-to-add-a-custom-ca-root-certificate-to-the-ca-store-used-by-pip-in-windows)

I just installed Python3 from python.org and am having trouble installing packages with pip.  By design, there is a man-in-the-middle packet inspection appliance on the network here that inspects all packets (ssl included) by resigning all ssl connections with its own certificate.  Part of the GPO pushes the custom root certificate into the Windows Keystore.When using Java, if I need to access any external https sites, I need to manually update the cacerts in the JVM to trust the Self-Signed CA certificate.How do I accomplish that for python?  Right now, when I try to install packages using pip, understandably, I get wonderful [SSL: CERTIFICATE_VERIFY_FAILED] errors.I realize I can ignore them using the --trusted-host parameter, but I don't want to do that for every package I'm trying to install.Is there a way to update the CA Certificate store that python uses?

2016-09-06 19:24:30Z

I just installed Python3 from python.org and am having trouble installing packages with pip.  By design, there is a man-in-the-middle packet inspection appliance on the network here that inspects all packets (ssl included) by resigning all ssl connections with its own certificate.  Part of the GPO pushes the custom root certificate into the Windows Keystore.When using Java, if I need to access any external https sites, I need to manually update the cacerts in the JVM to trust the Self-Signed CA certificate.How do I accomplish that for python?  Right now, when I try to install packages using pip, understandably, I get wonderful [SSL: CERTIFICATE_VERIFY_FAILED] errors.I realize I can ignore them using the --trusted-host parameter, but I don't want to do that for every package I'm trying to install.Is there a way to update the CA Certificate store that python uses?After extensively documenting a similar problem with Git (How can I make git accept a self signed certificate?), here we are again behind a corporate firewall with a proxy giving us a MitM "attack" that we should trust and:But where do we get ca-bundle.crt?cURL publishes an extract of the Certificate Authorities bundled with Mozilla Firefoxhttps://curl.haxx.se/docs/caextract.html I recommend you open up this cacert.pem file in a text editor as we will need to add our self-signed CA to this file.Certificates are a document complying with X.509 but they can be encoded to disk a few ways. The below article is a good read but the short version is that we are dealing with the base64 encoding which is often called PEM in the file extensions. You will see it has the format:https://support.ssl.com/Knowledgebase/Article/View/19/0/der-vs-crt-vs-cer-vs-pem-certificates-and-how-to-convert-themBelow are a few options on how to get our self signed certificate:https://unix.stackexchange.com/questions/451207/how-to-trust-self-signed-certificate-in-curl-command-line/468360#468360Thanks to this answer and the linked blog, it shows steps (on Windows) how to view the certificate and then copy to file using the base64 PEM encoding option.Copy the contents of this exported file and paste it at the end of your cacerts.pem file.For consistency rename this file cacerts.pem --> ca-bundle.crt and place it somewhere easy like:Thanks to all the brilliant answers in:How to get response SSL certificate from requests in python?I have put together the following to attempt to take it a step further.https://github.com/neozenith/get-ca-pySet the configuration in pip and conda so that it knows where this CA store resides with our extra self-signed CA.ORTHENRun: python -c "import ssl; print(ssl.get_default_verify_paths())" to check the current paths which are used to verify the certificate. Add your company's root certificate to one of those.The path openssl_capath_env points to the environment variable: SSL_CERT_DIR.If SSL_CERT_DIR doesn't exist, you will need to create it and point it to a valid folder within your filesystem. You can then add your certificate to this folder to use it.Not best answer but you can reuse an already created ca bundle using --cert option of pip, for instance:On Windows, I solved it by creating a pip.ini file in %APPDATA%\pip\e.g. C:\Users\asmith\AppData\Roaming\pip\pip.iniIn the pip.ini I put the path to my certificate:https://pip.pypa.io/en/stable/user_guide/#configuration has more information about the configuration file.Open Anaconda Navigator.Go to File\Preferences. Enable SSL verification Disable (not recommended) or Enable and indicate SSL certificate path(Optional) Update a package to a specific version:Select Install on Top-Right Select package click on tickMark for updateMark for specific version installationClick Apply

matplotlib set yaxis label size

zje

[matplotlib set yaxis label size](https://stackoverflow.com/questions/10404759/matplotlib-set-yaxis-label-size)

How can I change the size of only the yaxis label?

Right now, I change the size of all labels usingbut in my case, I would like to make the y-axis label larger than the x-axis. However, I'd like to leave the tick labels alone.I've tried, for example:but I only get:So, obviously that doesn't work. I've seen lots of stuff for tick sizes, but nothing for the axis labels themselves.

2012-05-01 21:39:26Z

How can I change the size of only the yaxis label?

Right now, I change the size of all labels usingbut in my case, I would like to make the y-axis label larger than the x-axis. However, I'd like to leave the tick labels alone.I've tried, for example:but I only get:So, obviously that doesn't work. I've seen lots of stuff for tick sizes, but nothing for the axis labels themselves.If you are using the 'pylab' for interactive plotting you can set the labelsize at creation time with pylab.ylabel('Example', fontsize=40).If you use pyplot programmatically you can either set the fontsize on creation with ax.set_ylabel('Example', fontsize=40) or afterwards with ax.yaxis.label.set_size(40).

Terminate a multi-thread python program

jack

[Terminate a multi-thread python program](https://stackoverflow.com/questions/1635080/terminate-a-multi-thread-python-program)

How to make a multi-thread python program response to Ctrl+C key event?Edit: The code is like this:I tried to remove join() on all threads but it still doesn't work. Is it because the lock segment inside each thread's run() procedure?Edit: The above code is supposed to work but it always interrupted when current variable was in 5,000-6,000 range and through out the errors as below

2009-10-28 03:41:20Z

How to make a multi-thread python program response to Ctrl+C key event?Edit: The code is like this:I tried to remove join() on all threads but it still doesn't work. Is it because the lock segment inside each thread's run() procedure?Edit: The above code is supposed to work but it always interrupted when current variable was in 5,000-6,000 range and through out the errors as belowMake every thread except the main one a daemon (t.daemon = True in 2.6 or better, t.setDaemon(True) in 2.6 or less, for every thread object t before you start it).  That way, when the main thread receives the KeyboardInterrupt, if it doesn't catch it or catches it but decided to terminate anyway, the whole process will terminate.  See the docs.edit: having just seen the OP's code (not originally posted) and the claim that "it doesn't work", it appears I have to add...:Of course, if you want your main thread to stay responsive (e.g. to control-C), don't mire it into blocking calls, such as joining another thread -- especially not totally useless blocking calls, such as joining daemon threads.  For example, just change the final loop in the main thread from the current (utterless and damaging):to something more sensible like:if your main has nothing better to do than either for all threads to terminate on their own, or for a control-C (or other signal) to be received.Of course, there are many other usable patterns if you'd rather have your threads not terminate abruptly (as daemonic threads may) -- unless they, too, are mired forever in unconditionally-blocking calls, deadlocks, and the like;-).There're two main ways, one clean and one easy.The clean way is to catch KeyboardInterrupt in your main thread, and set a flag your background threads can check so they know to exit; here's a simple/slightly-messy version using a global:The messy but easy way is to catch KeyboardInterrupt and call os._exit(), which terminates all threads immediately.A Worker might be helpful for you:I would rather go with the code proposed in this blog post:What I have changed is the t.join from t.join(1) to t.join(1000). The actual number of seconds does not matter, unless you specify a timeout number, the main thread will stay responsive to Ctrl+C. The except on KeyboardInterrupt makes the signal handling more explicit.You can always set your threads to "daemon" threads like:And whenever the main thread dies all threads will die with it.http://www.regexprn.com/2010/05/killing-multithreaded-python-programs.htmlIf you spawn a Thread like so - myThread = Thread(target = function) - and then do myThread.start(); myThread.join(). When CTRL-C is initiated, the main thread doesn't exit because it is waiting on that blocking myThread.join() call. To fix this, simply put in a timeout on the .join() call. The timeout can be as long as you wish. If you want it to wait indefinitely, just put in a really long timeout, like 99999. It's also good practice to do myThread.daemon = True so all the threads exit when the main thread(non-daemon) exits.When you want to kill the thread just use:

python: get number of items from list(sequence) with certain condition

cinsk

[python: get number of items from list(sequence) with certain condition](https://stackoverflow.com/questions/15375093/python-get-number-of-items-from-listsequence-with-certain-condition)

Assuming that I have a list with huge number of items.I want to get the number of items from that list, where an item should satisfy certain condition. My first thought was:But if the my_condition() filtered list has also great number of items, I think that

creating new list for filtered result is just waste of memory.  For efficiency, IMHO, above call can't be better than:Is there any functional-style way to achieve to get the # of items that satisfy certain condition without generating temporary list?Thanks in advance. 

2013-03-13 00:51:18Z

Assuming that I have a list with huge number of items.I want to get the number of items from that list, where an item should satisfy certain condition. My first thought was:But if the my_condition() filtered list has also great number of items, I think that

creating new list for filtered result is just waste of memory.  For efficiency, IMHO, above call can't be better than:Is there any functional-style way to achieve to get the # of items that satisfy certain condition without generating temporary list?Thanks in advance. You can use a generator expression:or evenwhich uses the fact that int(True) == 1.  Alternatively, you could use itertools.imap (python 2) or simply map (python 3):You want a generator comprehension rather than a list here.For example, Or use itertools.imap (though I think the explicit list and generator expressions look somewhat more Pythonic).Note that, though it's not obvious from the sum example, you can compose generator comprehensions nicely.  For example,The cool thing about this technique is that you can specify conceptually separate steps in code without forcing evaluation and storage in memory until the final result is evaluated.This can also be done using reduce if you prefer functional programmingThis way you only do 1 pass and no intermediate list is generated. you could do something like:which just adds 1 for each element that satisfies the condition.

how to update spyder on anaconda

Diego

[how to update spyder on anaconda](https://stackoverflow.com/questions/41849718/how-to-update-spyder-on-anaconda)

I have Anaconda installed (Python 2.7.11 |Anaconda custom (64-bit)| (default, Feb 16 2016, 09:58:36) [MSC v.1500 64 bit (AMD64)] on win32)  and I am using Spyder 2.3.8 Would like to update Spyder to the latest version, so I went through the commands:They all ran without errors, but the spyder version didn't change - this is command I'm using to launch:Am I missing something?

2017-01-25 10:54:46Z

I have Anaconda installed (Python 2.7.11 |Anaconda custom (64-bit)| (default, Feb 16 2016, 09:58:36) [MSC v.1500 64 bit (AMD64)] on win32)  and I am using Spyder 2.3.8 Would like to update Spyder to the latest version, so I went through the commands:They all ran without errors, but the spyder version didn't change - this is command I'm using to launch:Am I missing something?To expand on juanpa.arrivillaga's comment:If you want to update Spyder in the root environment, then conda update spyder 

works for me.  If you want to update Spyder for a virtual environment you have created (e.g., for a different version of Python), then conda update -n $ENV_NAME spyder where $ENV_NAME is your environment name.Go to Anaconda Naviagator, find spyder,click settings in the top right corner of the spyder app.click update tabI see that you used pip to update. This is strongly discouraged (at least in Spyder 3). The Spyder update notices I receive have always included the following: "IMPORTANT NOTE: It seems that you are using Spyder with Anaconda/Minconda. Please don't use pip to update it as that will probably break your installation. Instead please wait until new conda packages are available and use conda to perform the update." Use this conda install spyder=4.0.0

This will not mess up your anaconda dependencies.

https://github.com/spyder-ide/spyder/releasesOne way to avoid errors during installing or updating packages is to run the Anaconda prompt as Administrator. Hope it helps!This worked for me: conda install --force-reinstall pyqt qt

Based on: https://github.com/spyder-ide/spyder/issues/5525You can easily install update version if you use Anaconda by closing Spyder and then running the following command in a system terminal (Anaconda Prompt on Windows, xterm on Linux or Terminal.app on macOS):(For example, Version is  3.1)Or you can use pip with this command in a system terminal (cmd.exe on Windows, xterm on Linux or Terminal.app on macOS):Note: Do not use this command if you are using Anaconda because it could break your installation.In iOS, Your computer is going to start downloading and installing the new version. After finishing, just restart Spyder and that's it.It's very easy just in 2 click That it

Happy codingmake sure you in your base directory.

then conda install spyder will work.

Do it like this: conda install spyder=new_version_number.

new_version_number should be in digits.Simply select 'Update Application' after clicking on the settings symbol(top right corner) for Spyder in the Anaconda Navigator console. In my case I just updated it so it's in disabled state. Using pip directly:WARNING: This will break your Anaconda Installation as described by Spyder maintainer in the comments below; you can try this solution only if the solution mentioned above that use Conda do not workYou might get an error once launching the new Spyder "nbconvert >= 4.0: None (NOK)", which will require you to resinstall configparser:You should now have a fresh and up to date installation of Spyder.

Concatenate Numpy arrays without copying

Fred Foo

[Concatenate Numpy arrays without copying](https://stackoverflow.com/questions/7869095/concatenate-numpy-arrays-without-copying)

In Numpy, I can concatenate two arrays end-to-end with np.append or np.concatenate:But these make copies of their input arrays:Is there a way to concatenate two arrays into a view, i.e. without copying? Would that require an np.ndarray subclass?

2011-10-23 20:50:00Z

In Numpy, I can concatenate two arrays end-to-end with np.append or np.concatenate:But these make copies of their input arrays:Is there a way to concatenate two arrays into a view, i.e. without copying? Would that require an np.ndarray subclass?The memory belonging to a Numpy array must be contiguous. If you allocated the arrays separately, they are randomly scattered in memory, and there is no way to represent them as a view Numpy array.If you know beforehand how many arrays you need, you can instead start with one big array that you allocate beforehand, and have each of the small arrays be a view to the big array (e.g. obtained by slicing).Just initialize the array before you fill it with data. If you want you can allocate more space than needed and it will not take up more RAM because of the way numpy works.The memory is used only once data is put into the array. Creating a new array from concatenating two will never finish on a dataset of any size, i.e. dataset > 1GB or so.Not really elegant at all but you can get close to what you want using a tuple to store pointers to the arrays.  Now I have no idea how I would use it in the case but I have done things like this before.I had the same problem and ended up doing it reversed, after concatenating normally (with copy) I reassigned the original arrays to become views on the concatenated one:You can test it as follows:You may create an array of arrays, like:The problem is that it creates copies on broadcast operations (sounds like a bug).The answer is based on my other answer in Reference to ndarray rows in ndarray 

Call Python function from JavaScript code

xralf

[Call Python function from JavaScript code](https://stackoverflow.com/questions/13175510/call-python-function-from-javascript-code)

I'd like to call a Python function from JavaScript code, because there isn't an alternative in JavaScript for doing what I want. Is this possible? Could you adjust the below snippet to work?JavaScript code:~/pythoncode.py contains functions using advanced libraries that don't have an easy to write equivalent in JavaScript:

2012-11-01 10:52:29Z

I'd like to call a Python function from JavaScript code, because there isn't an alternative in JavaScript for doing what I want. Is this possible? Could you adjust the below snippet to work?JavaScript code:~/pythoncode.py contains functions using advanced libraries that don't have an easy to write equivalent in JavaScript:All you need is to make an ajax request to your pythoncode.

You can do this with jquery http://api.jquery.com/jQuery.ajax/, or use just javascriptFrom the document.getElementsByTagName I guess you are running the javascript in a browser.The traditional way to expose functionality to javascript running in the browser is calling a remote URL using AJAX. The X in AJAX is for XML, but nowadays everybody uses JSON instead of XML.For example, using jQuery you can do something like:You will need to implement a python webservice on the server side. For simple webservices I like to use Flask.A typical implementation looks like:You can run IronPython (kind of Python.Net) in the browser with silverlight, but I don't know if NLTK is available for IronPython.Typically you would accomplish this using an ajax request that looks likeYou cannot run .py files from JavaScript without the Python program like you cannot open .txt files without a text editor. But the whole thing becomes a breath with a help of a Web API Server (IIS in the example below).Remember that your .py file won't run on your user's computer, but instead on the server.Communicating through processesExample: Python: This python code block should return random temperatures.Javascript (Nodejs): Here we will need to spawn a new child process to run our python code and then get the printed output.

How to append to the end of an empty list?

LostLin

[How to append to the end of an empty list?](https://stackoverflow.com/questions/6339235/how-to-append-to-the-end-of-an-empty-list)

I have a list:the length of the list is undetermined so I am trying to append objects to the end of list1 like such:But my output keeps giving this error: AttributeError: 'NoneType' object has no attribute 'append'Is this because list1 starts off as an empty list? How do I fix this error?

2011-06-14 04:56:26Z

I have a list:the length of the list is undetermined so I am trying to append objects to the end of list1 like such:But my output keeps giving this error: AttributeError: 'NoneType' object has no attribute 'append'Is this because list1 starts off as an empty list? How do I fix this error?append actually changes the list. Also, it takes an item, not a list. Hence, all you need is(By the way, note that you can use range(n), in this case.)I assume your actual use is more complicated, but you may be able to use a list comprehension, which is more pythonic for this:Or, in this case, in Python 2.x range(n) in fact creates the list that you want already, although in Python 3.x, you need list(range(n)).You don't need the assignment operator.  append returns None.append returns None, so at the second iteration you are calling method append of NoneType. Just remove the assignment:Mikola has the right answer but a little more explanation.  It will run the first time, but because append returns None, after the first iteration of the for loop, your assignment will cause list1 to equal None and therefore the error is thrown on the second iteration.I personally prefer the + operator than append:But this is creating a new list every time, so might not be the best if performance is critical.Like Mikola said, append() returns a void, so every iteration you're setting list1 to a nonetype because append is returning a nonetype.  On the next iteration, list1 is null so you're trying to call the append method of a null.  Nulls don't have methods, hence your error.Note that you also can use insert in order to put number into the required position within list:And also note that in python you can always get a list length using method len()use my_list.append(...)

and do not use and other list to append as list are mutable.

SQLAlchemy classes across files

joveha

[SQLAlchemy classes across files](https://stackoverflow.com/questions/7478403/sqlalchemy-classes-across-files)

I'm trying to figure out how to have SQLAlchemy classes spread across several files, and I can for my life not figure out how to do it. I am pretty new to SQLAlchemy so forgive me if this question is trivial..Consider these 3 classes in each their own file:A.py:B.py:C.py:And then say we have a main.py something like this:The above gives the error:How do I share the declarative base across these files?What is the "the right" way to accomplish this, considering that I might throw something like Pylons or Turbogears on top of this?edit 10-03-2011I found this description from the Pyramids framework which describes the problem and more importantly verifies that this is an actual issue and not (only) just my confused self that's the problem. Hope it can help others who dares down this dangerous road :)

2011-09-19 23:16:06Z

I'm trying to figure out how to have SQLAlchemy classes spread across several files, and I can for my life not figure out how to do it. I am pretty new to SQLAlchemy so forgive me if this question is trivial..Consider these 3 classes in each their own file:A.py:B.py:C.py:And then say we have a main.py something like this:The above gives the error:How do I share the declarative base across these files?What is the "the right" way to accomplish this, considering that I might throw something like Pylons or Turbogears on top of this?edit 10-03-2011I found this description from the Pyramids framework which describes the problem and more importantly verifies that this is an actual issue and not (only) just my confused self that's the problem. Hope it can help others who dares down this dangerous road :)The simplest solution to your problem will be to take Base out of the module that imports A, B and C;  Break the cyclic import.  Works on my machine:I'm using Python 2.7 + Flask 0.10 + SQLAlchemy 1.0.8 + Postgres 9.4.4.1This boilerplate comes configured with a User and UserDetail models stored in the same file "models.py" in the "user" module. These classes both inherit from an SQLAlchemy base class. All of the additional classes I've added to my project also derived from this base class, and as the models.py file grew larger, I decided to split the models.py file into one file per class, and ran into the problem described here.The solution I found, along the same lines as @computermacgyver's Oct 23 2013 post, was to include all my classes to the init.py file of the new module I created to hold all the newly created class files. Looks like this:If I may add my bit of sense as well since I had the same problem. You need to import the classes in the file where you create the Base = declarative_base() AFTER you created the Base and the Tables. Short example how my project is set up:model/user.pymodel/budget.pymodel/__init__.pyFor me, adding import app.tool.tool_entity inside app.py and from app.tool.tool_entity import Tool inside tool/__init__.py was enough to get the table to be created. I haven't tried adding relationship yet, though.Folder structure:

When to use Serializer's create() and ModelViewset's create() perform_create()

Roel

[When to use Serializer's create() and ModelViewset's create() perform_create()](https://stackoverflow.com/questions/41094013/when-to-use-serializers-create-and-modelviewsets-create-perform-create)

I want to clarify the given documentation of django-rest-framework regarding the creation of a model object. So far I found that there are 3 approaches on how to handle such events.These three approaches are important depending on your application environment. But WHEN do we need to use each create() / perform_create() function??. On the other hand I found some account that two create methods were called for a single post request the modelviewset's create() and serializer's create().Hopefully anyone would share some of their knowledge to explain and this will surely be very helpful in my development process.

2016-12-12 03:59:34Z

I want to clarify the given documentation of django-rest-framework regarding the creation of a model object. So far I found that there are 3 approaches on how to handle such events.These three approaches are important depending on your application environment. But WHEN do we need to use each create() / perform_create() function??. On the other hand I found some account that two create methods were called for a single post request the modelviewset's create() and serializer's create().Hopefully anyone would share some of their knowledge to explain and this will surely be very helpful in my development process.Example:As you can see, the above create function takes care of calling validation on your serializer and producing the correct response. The beauty behind this, is that you can now isolate your application logic and NOT concern yourself about the mundane and repetitive validation calls and handling response output :). This works quite well in conjuction with the create(self, validated_data) found in the serializer (where your specific application logic might reside).Hope this helps!

Is there a way to negate a boolean returned to variable?

Furbeenator

[Is there a way to negate a boolean returned to variable?](https://stackoverflow.com/questions/8335029/is-there-a-way-to-negate-a-boolean-returned-to-variable)

I have a Django site, with an Item object that has a boolean property active. I would like to do something like this to toggle the property from False to True and vice-versa:This syntax is valid in many C-based languages, but seems invalid in Python. Is there another way to do this WITHOUT using:The native python neg() method seems to return the negation of an integer, not the negation of a boolean.Thanks for the help.

2011-12-01 00:23:41Z

I have a Django site, with an Item object that has a boolean property active. I would like to do something like this to toggle the property from False to True and vice-versa:This syntax is valid in many C-based languages, but seems invalid in Python. Is there another way to do this WITHOUT using:The native python neg() method seems to return the negation of an integer, not the negation of a boolean.Thanks for the help.You can do this:That should do the trick :)I think you wantitem.active = not item.active is the pythonic wayAnother (less concise readable, more arithmetic) way to do it would be:The negation for booleans is not.Thanks guys, that was a lightning fast response!Its simple to do :So, finally you will end up with :

How to remove duplicates from Python list and keep order? [duplicate]

Josh Glover

[How to remove duplicates from Python list and keep order? [duplicate]](https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order)

Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:but I don't know how to retrieve the list members from the hash in alphabetical order.I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.

2009-01-26 14:09:15Z

Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:but I don't know how to retrieve the list members from the hash in alphabetical order.I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.A list can be sorted and deduplicated using built-in functions:If your input is already sorted, then there may be a simpler way to do it:If you want to keep order of the original list, just use OrderedDict with None as values.In Python2:In Python3 it's even simpler:If you don't like iterators (zip and repeat) you can use a generator (works both in 2 & 3):If it's clarity you're after, rather than speed, I think this is very clear:It's O(n^2) though, with the repeated use of not in for each element of the input list.> but I don't know how to retrieve the list members from the hash in alphabetical order.Not really your main question, but for future reference Rod's answer using sorted can be used for traversing a dict's keys in sorted order:and also because tuple's are ordered by the first member of the tuple, you can do the same with items:For the string data

multiple axis in matplotlib with different scales [duplicate]

Jack_of_All_Trades

[multiple axis in matplotlib with different scales [duplicate]](https://stackoverflow.com/questions/9103166/multiple-axis-in-matplotlib-with-different-scales)

How can multiple scales can be implemented in Matplotlib? I am not talking about the primary and secondary axis plotted against the same x-axis, but something like many trends which have different scales plotted in same y-axis and that can be identified by their colors. For example, if I have trend1 ([0,1,2,3,4]) and trend2 ([5000,6000,7000,8000,9000]) to be plotted against time and want the two trends to be of different colors and in Y-axis, different scales, how can I accomplish this with Matplotlib? When I looked into Matplotlib, they say that they don't have this for now though it is definitely on their wishlist, Is there a way around to make this happen?Are there any other plotting tools for python that can make this happen?

2012-02-01 20:50:14Z

How can multiple scales can be implemented in Matplotlib? I am not talking about the primary and secondary axis plotted against the same x-axis, but something like many trends which have different scales plotted in same y-axis and that can be identified by their colors. For example, if I have trend1 ([0,1,2,3,4]) and trend2 ([5000,6000,7000,8000,9000]) to be plotted against time and want the two trends to be of different colors and in Y-axis, different scales, how can I accomplish this with Matplotlib? When I looked into Matplotlib, they say that they don't have this for now though it is definitely on their wishlist, Is there a way around to make this happen?Are there any other plotting tools for python that can make this happen?If I understand the question, you may interested in this example in the Matplotlib gallery.Yann's comment above provides a similar example.Edit - Link above fixed. Corresponding code copied from the Matplotlib gallery:if you want to do very quick plots with secondary Y-Axis then there is much easier way using Pandas wrapper function and just 2 lines of code. Just plot your first column then plot the second but with parameter secondary_y=True, like this:This would look something like below:You can do few more things as well. Take a look at Pandas plotting doc.Since Steve Tjoa's answer always pops up first and mostly lonely when I search for multiple y-axes at Google, I decided to add a slightly modified version of his answer. This is the approach from this matplotlib example.Reasons:Bootstrapping something fast to chart multiple y-axes sharing an x-axis using @joe-kington's answer:

how to know if a variable is a tuple, a string or an integer?

Gabriel Ross

[how to know if a variable is a tuple, a string or an integer?](https://stackoverflow.com/questions/7086990/how-to-know-if-a-variable-is-a-tuple-a-string-or-an-integer)

I am trying to figure out a type mismatch while adding a string to another string in a concatenate operation.Basically the error returned is a TypeError (cannot concatenate string and tuple); so I would like to figure out where I assigned a value as tuple instead of string.All the values that I assign are strings, so I gotta figure out where the tuple is coming from, so I was hoping that there is a way in Python to find out what is contained inside a variable and what type is it.So far using pdb I was able to check the content of the variables, and I get correctly the values that I would expect; but I would like to know also the type of the variable (by logic, if the compiler is able to raise a type error, it means that it knows what is inside a variable and if it is compatible with the operation to perform; so there must be a way to get that value/flag out).Is there any way to print out the type of a variable in python?BTW, I tried to change all my variables to be explicitly strings, but is not feasible to force str (myvar), so I cannot just cast as string type everywhere I use strings.

2011-08-17 01:21:37Z

I am trying to figure out a type mismatch while adding a string to another string in a concatenate operation.Basically the error returned is a TypeError (cannot concatenate string and tuple); so I would like to figure out where I assigned a value as tuple instead of string.All the values that I assign are strings, so I gotta figure out where the tuple is coming from, so I was hoping that there is a way in Python to find out what is contained inside a variable and what type is it.So far using pdb I was able to check the content of the variables, and I get correctly the values that I would expect; but I would like to know also the type of the variable (by logic, if the compiler is able to raise a type error, it means that it knows what is inside a variable and if it is compatible with the operation to perform; so there must be a way to get that value/flag out).Is there any way to print out the type of a variable in python?BTW, I tried to change all my variables to be explicitly strings, but is not feasible to force str (myvar), so I cannot just cast as string type everywhere I use strings.You just use:which will output int, str, float, etc...make use of isinstance ?Please note, should you wanted to check your var type in if statement, the construct

if type(varname) == "tuple": won't work. But these will:You probably want to test (assuming Python 2.x) using isinstance(obj, basestring). You have the options of using isinstance, type, and calling the attribute __class__, but isinstance is likely to be the one you want here. Take a look at this article for a more thorough treatment of the differences between the three options.repr(object) will give a textual description of object, which should show type and value. Your can print or view this in the debugger.For simple values repr usually returns the same string as you would write the value literally in code. For custom classes it gives the class name and object id, or something else if the class'esis overridden.To complement Goujon's answer consider this list of mixed integers and tuples:Generates this output:So the secret isn't to test for "int" character string but, rather test for int keyword instead. If you are like me and migrating from the Bash scripting world, using == tests in the Python scripting world will probably be your first step.

What's a good lightweight Python MVC framework? [closed]

jon

[What's a good lightweight Python MVC framework? [closed]](https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework)

I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features.

Here are a couple of other things that I'd want (or don't want):

2008-09-16 02:44:25Z

I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features.

Here are a couple of other things that I'd want (or don't want):web2py is 265Kbytes of source code and 1.2MB all inclusive (compare with 4.6MB of Django). Yet web2py will do everything you need (manage session, cookies, request, response, cache, internationalization, errors/tickets, database abstraction for GAE, SQLite, MSSQL, MySQL, Postgres, Oracle, FireBird, etc.) It does not need installation - just unzip and click on it - and you can do development in your browser.Web2py has both routes and reverse routes.Web2py has a hierarchical template systems which means a view can extend a layout which can extend another layout, etc. views can also include other views.Since you explicitly don't want an ORM, I'd stay away from a "full stack" framework if I were you. Python's WSGI standard gives you a ton of easy-to-use options that will give you just the features you need and even let you choose your style of working.Here's an example: for URL dispatch you can use Routes, which ports over the URL dispatch style of Rails. Or you could combine Selector with WebOb if that style suits you more.For "layouts", you can use the powerful Jinja2 if you want templates that cannot run code. Or, Mako if you prefer to be able to mix a little code in with your templates. You can even use Deliverance to control the layout of pages that are composed from multiple apps and even multiple languages!A full-stack web framework is nice in that it makes a bunch of choices for you, letting you pay attention just to the app your building. But, the choices I've listed above are a good collection to get you going building your own. If you head down that path, you'll find it easy to plug in Beaker for caching and sessions if you need them, or WebError to help you with debugging.Personally, I'm a big fan of ORMs (particularly SQLAlchemy), but if you're looking to go ORM free and lightweight overall you can't beat combining the great WSGI components available in Python.Give web.py a try. It's very simple and may provide the minimalism that you are looking for.Pylons.It's much better than django, and doesn't come with a crappy ORM.People already gave many answers concerning web application frameworks, but MVC (or any other paradigm) is not tied to web only. That's just for clarity.If you are about plain MVC, Pylons conforms to paradigm in a stricter way. Django interprets MVC and they call it model-template-view, but the idea of role separation is the same. Actual choice is a matter of personal taste, although none of these two I consider lightweigth (Pylons might seem lighter, but in fact is not, and recently Django gathered some additional weight - most likely you will not fit even small application like personal blog in 20MB resident memory).Of course, nothing will stop you from writing your own framework, eg. with WebOb. You can make it as light as you want (and learn many things trying).If you want something simple, without having to make your own framework, while still not being all inclusive (django), you might want to try CherryPy. It can use almost any dispatcher (Page Handler / URL routing system). You would also have to pick your own templating engine, Genshi is my favorite.checkout https://github.com/salimane/bottle-mvc or https://github.com/salimane/flask-mvc . They are boilerplates that could get you started with controllers, models in separate folders. They are based on bottle and flask micro frameworks, no useless features, they give you the flexibility to plugin whatever modules you want.When it comes to desktop applications Dabo is a good choice. It's a cross platform framework on top of wxPython which supports MySql, Postgresql, Firebird and Sqlite.You want web2py.  Check it:Web2py doesn't have a ORM, but it does have a syntactic wrapper over SQL that makes it much easier to do the SQL....web2py has both of these.And web2py is more lightweight than django/rails/whatever on pretty much all counts.  It's VERY easy to learn.  The hardest things about learning a MVC framework are the scripts, the ORM, and (with django) the template language.  But web2py got rid of the scripts, simplified the ORM, and the template language is just python in a rad clever way.Yes, I would say Django is definitely the way to go. Its modular design ensures that you can mix and match components (ORM, templating engine, URL dispatch, ...) Instead of being stuck with a component the framework provides you, you can replace it with any 3rd party equivalent instead.Django!Google App Engine uses it.I use it too for my own pet projects.web2py! django calls a controller a view, 'nuf said.If you want simplicity use web2py or pylons. Django is good...but the learning curve is steep Django. You don't have to use the additional features, and it's well designed so you can mix-n-match 3rd-party libraries as needed.I'm really new on Python but I tried quiet a few, specially Django and web2py. 

I loved the simplicity of web2py, I was able to create a site of medium complexity in a few days.

It has an imprecessive sets of feature a DAL, code generation, HTML hlpers and for me the most important feature was the documentation in the site is quite complete.I would take a look at Pylons; it is lightweight and fast.I'm also on the Django boat. Here are a few reasons why:Django is my recommendation.You can find an introduction to it here (a Google Tech Talk by Jacob Kaplan-Moss):And you may also want to have a look at Adrian Holovaty's talk given at Snakes and Rubies, DePaul University:While not all Python frameworks explicitly support MVC, it is often trivial to create a web site which uses the MVC pattern by separating the data logic (the model) from the user interaction logic (the controller) and the templates (the view).http://docs.python.org/howto/webservers.html#model-view-controller

Python Pandas How to assign groupby operation results back to columns in parent dataframe?

ely

[Python Pandas How to assign groupby operation results back to columns in parent dataframe?](https://stackoverflow.com/questions/12200693/python-pandas-how-to-assign-groupby-operation-results-back-to-columns-in-parent)

I have the following data frame in IPython, where each row is a single stock:I want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the "yearmonth" column.This works as expected:But then I want to sort of "broadcast" these values back to the indices in the original data frame, and save them as constant columns where the dates match.I realize this naive assignment should not work. But what is the "right" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?In the end, I want a column called "MarketReturn" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.One hack to achieve this would be the following:But this is slow, bad, and unPythonic.

2012-08-30 15:45:26Z

I have the following data frame in IPython, where each row is a single stock:I want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the "yearmonth" column.This works as expected:But then I want to sort of "broadcast" these values back to the indices in the original data frame, and save them as constant columns where the dates match.I realize this naive assignment should not work. But what is the "right" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?In the end, I want a column called "MarketReturn" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.One hack to achieve this would be the following:But this is slow, bad, and unPythonic.While I'm still exploring all of the incredibly smart ways that apply concatenates the pieces it's given, here's another way to add a new column in the parent after a groupby operation.As a general rule when using groupby(), if you use the .transform() function pandas will return a table with the same length as your original. When you use other functions like .sum() or .first() then pandas will return a table where each row is a group. I'm not sure how this works with apply but implementing elaborate lambda functions with transform can be fairly tricky so the strategy that I find most helpful is to create the variables I need, place them in the original dataset and then do my operations there. If I understand what you're trying to do correctly (I apologize if I'm mistaken) first you can calculate the total market cap for each group: This will add a column called "group_MarketCap" to your original data which would contain the sum of market caps for each group. Then you can calculate the weighted values directly: And finally you would calculate the weighted average for each group using the same transform function:I tend to build my variables this way. Sometimes you can pull off putting it all in a single command but that doesn't always work with groupby() because most of the time pandas needs to instantiate the new object to operate on it at the full dataset scale (i.e. you can't add two columns together if one doesn't exist yet). Hope this helps :) May I suggest the transform method (instead of aggregate)? If you use it in your original example it should do what you want (the broadcasting).

Call to operating system to open url?

Bolster

[Call to operating system to open url?](https://stackoverflow.com/questions/4216985/call-to-operating-system-to-open-url)

What can I use to call the OS to open a URL in whatever browser the user has as default? 

Not worried about cross-OS compatibility; if it works in linux thats enough for me!

2010-11-18 16:16:01Z

What can I use to call the OS to open a URL in whatever browser the user has as default? 

Not worried about cross-OS compatibility; if it works in linux thats enough for me!Here is how to open the user's default browser with a given url:Here is the documentation about this functionality. It's part of Python's stdlibs: http://docs.python.org/library/webbrowser.htmlI have tested this successfully on Linux, Ubuntu 10.10.Personally I really wouldn't use the webbrowser module.It's a complicated mess of sniffing for particular browsers, which will won't find the user's default browser if they have more than one installed, and won't find a browser if it doesn't know the name of it (eg Chrome). Better on Windows is simply to use the os.startfile function, which also works on a URL. On OS X, you can use the open system command. On Linux there's xdg-open, a freedesktop.org standard command supported by GNOME, KDE and XFCE.This will give a better user experience on mainstream platforms. You could fall back to webbrowser on other platforms, perhaps. Though most likely if you're on an obscure/unusual/embedded OS where none of the above work, chances are webbrowser will fail too.You can use the webbrowser module.Then how about mixing codes of @kobrien and @bobince up:Have a look at the webbrowser module.

First Python list index greater than x?

c00kiemonster

[First Python list index greater than x?](https://stackoverflow.com/questions/2236906/first-python-list-index-greater-than-x)

What would be the most Pythonic way to find the first index in a list that is greater than x?For example, withThe functionwould return

2010-02-10 13:01:40Z

What would be the most Pythonic way to find the first index in a list that is greater than x?For example, withThe functionwould returnif list is sorted then bisect_left(alist, value) is faster for a large list than next(i for i, x in enumerate(alist) if x >= value).Another one:I had similar problem when my list was very long. comprehension or filter -based solutions would go thru whole list. itertools.takewhile will break the loop once condition become false first time:I know there are already plenty of answers, but I sometimes I feel that the word pythonic is translated into 'one-liner'.When I think a better definition is closer to this answer:"Exploiting the features of the Python language to produce code that is clear, concise and maintainable."While some of the above answers are concise, I do not find them to be clear and it would take a newbie programmer a while to understand, therefore not making them extremely maintainable for a team built of many skill levels.orI think the above is easily understood by a newbie and is still concise enough to be accepted by any veteran python programmer.  I think writing dumb code is a positive.Try this one:example code:output:

Convert string into datetime.time object

Zed

[Convert string into datetime.time object](https://stackoverflow.com/questions/14295673/convert-string-into-datetime-time-object)

Given the string in this format "HH:MM", for example "03:55", that represents 3 hours and 55 minutes.I want to convert it to datetime.time object for easier manipulation. What would be the easiest way to do that?

2013-01-12 17:06:45Z

Given the string in this format "HH:MM", for example "03:55", that represents 3 hours and 55 minutes.I want to convert it to datetime.time object for easier manipulation. What would be the easiest way to do that?Use datetime.datetime.strptime() and call .time() on the result:The first argument to .strptime() is the string to parse, the second is the expected format.

Are locks unnecessary in multi-threaded Python code because of the GIL?

Corey Goldberg

[Are locks unnecessary in multi-threaded Python code because of the GIL?](https://stackoverflow.com/questions/105095/are-locks-unnecessary-in-multi-threaded-python-code-because-of-the-gil)

If you are relying on an implementation of Python that has a Global Interpreter Lock (i.e. CPython) and writing multithreaded code, do you really need locks at all?If the GIL doesn't allow multiple instructions to be executed in parallel, wouldn't shared data be unnecessary to protect?sorry if this is a dumb question, but it is something I have always wondered about Python on multi-processor/core machines.  same thing would apply to any other language implementation that has a GIL.

2008-09-19 20:07:37Z

If you are relying on an implementation of Python that has a Global Interpreter Lock (i.e. CPython) and writing multithreaded code, do you really need locks at all?If the GIL doesn't allow multiple instructions to be executed in parallel, wouldn't shared data be unnecessary to protect?sorry if this is a dumb question, but it is something I have always wondered about Python on multi-processor/core machines.  same thing would apply to any other language implementation that has a GIL.You will still need locks if you share state between threads. The GIL only protects the interpreter internally. You can still have inconsistent updates in your own code.For example:Here, your code can be interrupted between reading the shared state (balance = shared_balance) and writing the changed result back (shared_balance = balance), causing a lost update. The result is a random value for the shared state.To make the updates consistent, run methods would need to lock the shared state around the read-modify-write sections (inside the loops) or have some way to detect when the shared state had changed since it was read.No - the GIL just protects python internals from multiple threads altering their state.  This is a very low-level of locking, sufficient only to keep python's own structures in a consistent state.  It doesn't cover the application level locking you'll need to do to cover thread safety in your own code.The essence of locking is to ensure that a particular block of code is only executed by one thread.  The GIL enforces this for blocks the size of a single bytecode, but usually you want the lock to span a larger block of code than this.Adding to the discussion:Because the GIL exists, some operations are atomic in Python and do not need a lock. http://www.python.org/doc/faq/library/#what-kinds-of-global-value-mutation-are-thread-safeAs stated by the other answers, however, you still need to use locks whenever the application logic requires them (such as in a Producer/Consumer problem).This post describes the GIL at a fairly high-level:Of particular interest are these quotes:and It sounds like the GIL just provides fewer possible instances for a context switch, and makes multi-core/processor systems behave as a single core, with respect to each python interpreter instance, so yes, you still need to use synchronization mechanisms.The Global Interpreter Lock prevents threads from accessing the interpreter simultaneously (thus CPython only ever uses one core). However, as I understand it, the threads are still interrupted and scheduled preemptively, which means you still need locks on shared data structures, lest your threads stomp on each other's toes.The answer I've encountered time and time again is that multithreading in Python is rarely worth the overhead, because of this. I've heard good things about the PyProcessing project, which makes running multiple processes as "simple" as multithreading, with shared data structures, queues, etc. (PyProcessing will be introduced into the standard library of the upcoming Python 2.6 as the multiprocessing module.) This gets you around the GIL, as each process has its own interpreter.Think of it this way:On a single processor computer, multithreading happens by suspending one thread and starting another fast enough to make it appear to be running at the same time. This is like Python with the GIL: only one thread is ever actually running.The problem is that the thread can be suspended anywhere, for example, if I want to compute b = (a + b) * 3, this might produce instructions something like this:Now, lets say that is running in a thread and that thread is suspended after either line 1 or 2 and then another thread kicks in and runs:Then when the other thread resumes, b is overwritten by the old computed values, which is probably not what was expected.So you can see that even though they're not ACTUALLY running at the same time, you still need locking.You still need to use locks (your code could be interrupted at any time to execute another thread and this can cause data inconsistencies). The problem with GIL is that it prevents Python code from using more cores at the same time (or multiple processors if they are available).Locks are still needed. I will try explaining why they are needed.Any operation/instruction is executed in the interpreter. GIL ensures that interpreter is held by a single thread at a particular instant of time. And your program with multiple threads works in a single interpreter. At any particular instant of time, this interpreter is held by a single thread. It means that only thread which is holding the interpreter is running at any instant of time.Suppose there are two threads,say t1 and t2, and both want to execute two instructions which is reading the value of a global variable and incrementing it.As put above, GIL only ensures that two threads can't execute an instruction simultaneously, which means both threads can't execute read_var = var at any particular instant of time. But they can execute instruction one after another and you can still have problem. Consider this situation:A little bit of update from Will Harris's example:Put a value check statement in the withdraw and I don't see negative anymore and updates seems consistent. My question is:If GIL prevents only one thread can be executed at any atomic time, then where would be the stale value? If no stale value, why we need lock? (Assuming we only talk about pure python code)If I understand correctly, the above condition check wouldn't work in a real threading environment. When more than one threads are executing concurrently, stale value can be created hence the inconsistency of the share state, then you really need a lock. But if python really only allows just one thread at any time (time slicing threading), then there shouldn't be possible for stale value to exist, right?

