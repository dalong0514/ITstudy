搜索关键词：[scrapy]

How do I make pylint recognize twisted and ephem members?

DonGar

[How do I make pylint recognize twisted and ephem members?](https://stackoverflow.com/questions/17142236/how-do-i-make-pylint-recognize-twisted-and-ephem-members)

I very much like having pylint tell me if I'm using a non-existent member. However, my new project is using both twisted and the ephem modules, which seem to confuse pylint.How can I get rid of these (incorrect) pylint warnings without either turning off E1101 in general, and without sprinkling warning removal comments around every relevant call?

2013-06-17 07:18:38Z

I very much like having pylint tell me if I'm using a non-existent member. However, my new project is using both twisted and the ephem modules, which seem to confuse pylint.How can I get rid of these (incorrect) pylint warnings without either turning off E1101 in general, and without sprinkling warning removal comments around every relevant call?The answer is to add a section like the following to your pylintrc with the problematic classes.

How to crawl in desired order or Synchronously in Scrapy?

Bustencio

[How to crawl in desired order or Synchronously in Scrapy?](https://stackoverflow.com/questions/57808974/how-to-crawl-in-desired-order-or-synchronously-in-scrapy)

I'm trying to create a spider that crawls and scrapes every product from a  store and outputs the results to a JSON file, that includes going into each category in the main page and scraping every product (just name and price), each product class page includes infinite scrolling. My problem is that each time I make a request after scraping the first page of a class of item, instead of getting the next batch of items from that same type, I get the items from the next category and the output ends up being a mess. I've already tried messing with settings and forcing concurrent requests to one and setting different priorities for each request. I've found out about asynchronous crawling but I can't figure out how to create the requests in order. What it does: Category 1 page 1 > Cat 2 page 1 > Cat 3 page 1 > ...What I want it to do: Cat 1 page 1 > Cat 1 page 2 > Cat 1 page 3 > ... > Cat 2 page 1

2019-09-05 15:46:01Z

I'm trying to create a spider that crawls and scrapes every product from a  store and outputs the results to a JSON file, that includes going into each category in the main page and scraping every product (just name and price), each product class page includes infinite scrolling. My problem is that each time I make a request after scraping the first page of a class of item, instead of getting the next batch of items from that same type, I get the items from the next category and the output ends up being a mess. I've already tried messing with settings and forcing concurrent requests to one and setting different priorities for each request. I've found out about asynchronous crawling but I can't figure out how to create the requests in order. What it does: Category 1 page 1 > Cat 2 page 1 > Cat 3 page 1 > ...What I want it to do: Cat 1 page 1 > Cat 1 page 2 > Cat 1 page 3 > ... > Cat 2 page 1This is easy, Get list of all categories in all_categories, now don't scrape all links, just scrape 1st category link, and once all pages have been scraped for that category, then send request to another category link.Here is the code, I did not run code so there maybe some syntax error, but logic is what you need

Scrapy - How to run spiders with AWS Lambda functions?

olegario

[Scrapy - How to run spiders with AWS Lambda functions?](https://stackoverflow.com/questions/51386930/scrapy-how-to-run-spiders-with-aws-lambda-functions)

Currently I have two small projects using Scrapy. One project is basically to scrape URL's, while the other is only to scrape products of the scrape URL's. The directory structure is this:When I want to run a spider using the command, I always must follow this path: ~/search/url$ scrapy crawl store1 or ~/search/product$ scrapy crawl store1. How can I deploy and run this project using AWS lambda functions?

2018-07-17 17:10:35Z

Currently I have two small projects using Scrapy. One project is basically to scrape URL's, while the other is only to scrape products of the scrape URL's. The directory structure is this:When I want to run a spider using the command, I always must follow this path: ~/search/url$ scrapy crawl store1 or ~/search/product$ scrapy crawl store1. How can I deploy and run this project using AWS lambda functions?

Trouble downloading images using scrapy

SIM

[Trouble downloading images using scrapy](https://stackoverflow.com/questions/51139170/trouble-downloading-images-using-scrapy)

I've written a script in python scrapy to download some images from a website. When i run my script, I can see the link of images (all of them are in .jpg format) in the console. However, when I open the folder in which the images are supposed to be saved when the downloading is done, I get nothing in there. Where I'm making mistakes?This is my spider (I'm running from sublime text editor):This is what I've defined in settings.py for the images to be saved:To make things clearer:It's not about running the script successfully with the help of items.py file. So, any solution to make the download happen with the use of items.py file is not what I'm looking for.

2018-07-02 15:10:33Z

I've written a script in python scrapy to download some images from a website. When i run my script, I can see the link of images (all of them are in .jpg format) in the console. However, when I open the folder in which the images are supposed to be saved when the downloading is done, I get nothing in there. Where I'm making mistakes?This is my spider (I'm running from sublime text editor):This is what I've defined in settings.py for the images to be saved:To make things clearer:It's not about running the script successfully with the help of items.py file. So, any solution to make the download happen with the use of items.py file is not what I'm looking for.The item you are yielding does not follow the documentation of Scrapy. As detailed in their media pipeline documentation the item should have a field called image_urls. You should change your parse method to something similar to this.I just tested this and it works. Additionally, as commented by Pruthvi Kumar, the IMAGES_STORE should just be likeWhat strikes me first thing scanning the code above is the PATH for IMAGES_STORE. the / means that you are going to the absolute root path of your machine, so you either put the absolute path to where you want to save or just do a relative path from where you are running your crawlerI'm on a linux machine so my absolute path will be something like IMAGES_STORE = /home/pk/myProjects/scraper/imagesOR IMAGES_STORE = 'images'Also, most importantly, if you are using default pipeline, the variable which holds the extracted image, (where you do extract_first()) must literally be image_urls.You are also missing a couple of steps. In your spider, add this: In the yield step, modify to: yield ImgData(image_urls=response.urljoin(q.css("::attr(src)").extract_first()))

Prevent showing debugging log info inside ipython shell

Arun

[Prevent showing debugging log info inside ipython shell](https://stackoverflow.com/questions/49442523/prevent-showing-debugging-log-info-inside-ipython-shell)

I'm using scrapy shell inside virtualenv. IPython is installed inside virtualenv. When I start scrapy shell usingand press tab for autocomplete suggestions, it shows a lot of debug information. How can I disable this?

2018-03-23 04:47:54Z

I'm using scrapy shell inside virtualenv. IPython is installed inside virtualenv. When I start scrapy shell usingand press tab for autocomplete suggestions, it shows a lot of debug information. How can I disable this?https://github.com/ipython/ipython/issues/10946 looks like it's reported bug here.In case you need debug logging in ipython, try to



logging.getLogger('parso.cache').disabled=True

logging.getLogger('parso.cache.pickle').disabled=True

and keep wait for parso updateTry doing this to set the logging level to WARNING:Any log messages of level INFO or DEBUG shouldn't appear anymore. You can also set the log level to logging.ERROR. Then WARNING messages won't appear as well.Good luck!You can set log level in settings.py file as described in docsLOG_LEVEL = 'INFO'This will hide DEBUG level messages from scrapy.You can use the -L command-line option to change the log level to INFO:Building on the answer add this to your pythonstartup $PYTHONSTARTUP

Unable to use proxies one by one until there is a valid response

robots.txt

[Unable to use proxies one by one until there is a valid response](https://stackoverflow.com/questions/54801031/unable-to-use-proxies-one-by-one-until-there-is-a-valid-response)

I've written a script in python's scrapy to make a proxied requests using either of the newly generated proxies by get_proxies() method. I used requests module to fetch the proxies in order to reuse them in the script. However, the problem is the proxy my script chooses to use may not be the good one always so sometimes it doesn't fetch valid response. My script so far:PS My intension is to seek any solution the way I've started here.

2019-02-21 06:59:59Z

I've written a script in python's scrapy to make a proxied requests using either of the newly generated proxies by get_proxies() method. I used requests module to fetch the proxies in order to reuse them in the script. However, the problem is the proxy my script chooses to use may not be the good one always so sometimes it doesn't fetch valid response. My script so far:PS My intension is to seek any solution the way I've started here.As we know http response needs to pass all middlewares in order to reach spider methods.It means that only requests with valid proxies can proceed to spider callback functions.

In order to use valid proxies we need to check ALL proxies first and after that choose only from valid proxies.When our previously chosen proxy doesn't work anymore - we mark this proxy as not valid and choose new one from remaining valid proxies in spider errback.you need write a downloader middleware, to install a process_exception hook, scrapy calls this hook when exception raised. in the hook, you could return a new Request object, with dont_filter=True flag, to let scrapy reschedule the request until it succeeds.in the meanwhile, you could verify response extensively in process_response hook, check the status code, response content etc., and reschedule request as necessary.in order to change proxy easily, you should use built-in HttpProxyMiddleware, instead of tinker with environ:take a look at this project as an example. 

Understanding infinite loading when using Scrapy - what's wrong?

Grajdeanu Alex.

[Understanding infinite loading when using Scrapy - what's wrong?](https://stackoverflow.com/questions/57260135/understanding-infinite-loading-when-using-scrapy-whats-wrong)

I'm trying to get all the data from this website in order to later use it in some model training project (ML). I've chosen to do it by using Scrapy + Python 3.7. So far so good. I've set up my Scrapy project structure and I started working on the scraper. In order to do this, I created some steps that need to be followed in order to accordingly get the data that I need.I've tried to reproduce the above by using the following piece of code:The issue with my code is that not all the products are being parsed, only ~3k / out of 70k. Now, Where I suppose it's the issue is between the lines 148-165. I've ran it through the debugger but I still couldn't figure out what's wrong.Can someone please explain me what's wrong in my code logic? 

2019-07-29 19:15:20Z

I'm trying to get all the data from this website in order to later use it in some model training project (ML). I've chosen to do it by using Scrapy + Python 3.7. So far so good. I've set up my Scrapy project structure and I started working on the scraper. In order to do this, I created some steps that need to be followed in order to accordingly get the data that I need.I've tried to reproduce the above by using the following piece of code:The issue with my code is that not all the products are being parsed, only ~3k / out of 70k. Now, Where I suppose it's the issue is between the lines 148-165. I've ran it through the debugger but I still couldn't figure out what's wrong.Can someone please explain me what's wrong in my code logic? Not sure if that's the only issue as I don't have time to test it further, but it seems you're only parsing the first product when you load the 8-bulk data here:The .get() method won't return all the urls. You might use the getall() method instead which returns a list with all the urls:And then just loop over the returned list and yield what you yielded before:You made the same mistake in the parse method of your BannerSolutionsSpider class, as you did in parse_plm method(highlighted by @Cajuu'). Rather using getall method to get all the hyperlinks, you used the get method which only returns the first URL of each sub-category. You may try below solution, it is giving all the sub-category urls to parse.

Scrape ASIN from Amazon's Search page

helloworld1990

[Scrape ASIN from Amazon's Search page](https://stackoverflow.com/questions/55534679/scrape-asin-from-amazons-search-page)

I try to scrape the ASIN numbers on Amazon. Please note that this is not about the product details (like this: https://www.youtube.com/watch?v=qRVRIh3GZgI), but this is when you search for a keyword (in this example "trimmer", try this: 

https://www.amazon.com/s?k=trimmer&ref=nb_sb_noss_2). The results are many products, I am able to scrape all the Titles. What is not visible is the ASIN (which is a unique Amazon number). I saw, while inspecting the HTML a link in the text (href), which is containing the ASIN number. In the example below, the ASIN = B01MSHQ5IQEnding with my question: How can I retrieve all the Product Titles AND ASIN numbers on the page? For example: So far, I am using scrapy (but also open for other Python solutions) and I am able to scrape the Titles. My code so far: First run in the command line: Then, adjust the files in the Spider (see example 1) and items.py (see example 2). Example 1 As requested by @glhr, adding the items.py code: Example 2 

2019-04-05 11:46:01Z

I try to scrape the ASIN numbers on Amazon. Please note that this is not about the product details (like this: https://www.youtube.com/watch?v=qRVRIh3GZgI), but this is when you search for a keyword (in this example "trimmer", try this: 

https://www.amazon.com/s?k=trimmer&ref=nb_sb_noss_2). The results are many products, I am able to scrape all the Titles. What is not visible is the ASIN (which is a unique Amazon number). I saw, while inspecting the HTML a link in the text (href), which is containing the ASIN number. In the example below, the ASIN = B01MSHQ5IQEnding with my question: How can I retrieve all the Product Titles AND ASIN numbers on the page? For example: So far, I am using scrapy (but also open for other Python solutions) and I am able to scrape the Titles. My code so far: First run in the command line: Then, adjust the files in the Spider (see example 1) and items.py (see example 2). Example 1 As requested by @glhr, adding the items.py code: Example 2 You can get the link to the product by extracting the href attribute of <a class="a-link-normal a-text-normal" href="...">:From a link, you can use a regular expression to extract the ASIN number from the link:The regular expression above will match 10 characters (either uppercase letters or numbers) preceded by dp/. See demo here: https://regex101.com/r/mLMv3k/1Here's a working implementation of the parse() method:This requires extending AmazonItem with new fields:Sample output:Demo: https://repl.it/@glhr/55534679-AmazonSpiderTo write the output to a JSON file, simply specify feed export settings in the spider:

scrapy - How to retrieve a variable value using regex

PyRar

[scrapy - How to retrieve a variable value using regex](https://stackoverflow.com/questions/52443739/scrapy-how-to-retrieve-a-variable-value-using-regex)

I want to retrieve the value of the var modelCode. I made a regex function like this, but it doesn't work at all. I've posted the structure of the page below.Can somebody help me, please?Structure of the page:}

2018-09-21 12:25:17Z

I want to retrieve the value of the var modelCode. I made a regex function like this, but it doesn't work at all. I've posted the structure of the page below.Can somebody help me, please?Structure of the page:}That code is inside a <script> tag, I suppose. In that case, you could use:Some tips:Try Regex: (?<=var modelCode = ")(.+)(?=";)Demowe need not do the re.sub as we get the value of modelCode as the match.

How to install Visual Studio 2015 PlatformSDK now?

Taurus Dang

[How to install Visual Studio 2015 PlatformSDK now?](https://stackoverflow.com/questions/51476784/how-to-install-visual-studio-2015-platformsdk-now)

When I ran something (such as pip install scrapy) with python 3.7, I got a trouble with 

error: [WinError 3] The system cannot find the path specified: 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\PlatformSDK\\lib'

and I found there is no PlatformSDK in my VS 14.0 at all.(Update on 24 Jul 2018: I got another error [WinError 3] The system cannot find the path specified: 'C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v8.1\\lib' when I ran pip install scrapy on another laptop, also Win 10 OS. I'm not sure whether my case is similar to this question's)Then I got a vs_community.exe source from here and installed Web Developer Tools and Visual Studio Extensibility Tools Update 3 successfully, but still no PlatformSDK. So how can I get this missing folder and its packages inside it? 

2018-07-23 10:35:43Z

When I ran something (such as pip install scrapy) with python 3.7, I got a trouble with 

error: [WinError 3] The system cannot find the path specified: 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\PlatformSDK\\lib'

and I found there is no PlatformSDK in my VS 14.0 at all.(Update on 24 Jul 2018: I got another error [WinError 3] The system cannot find the path specified: 'C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v8.1\\lib' when I ran pip install scrapy on another laptop, also Win 10 OS. I'm not sure whether my case is similar to this question's)Then I got a vs_community.exe source from here and installed Web Developer Tools and Visual Studio Extensibility Tools Update 3 successfully, but still no PlatformSDK. So how can I get this missing folder and its packages inside it? Once I had a problem installing another Python program (QARK) that required Windows Visual C++. In fact in cmd / PowerShell I was told that a required version was missing.

So I followed what was written on THIS site, so I didn't have this problem anymore (I also rebooted, for safety):

After that I didn't have any problems running it.

I hope I have been helpful! 🤞

Scrapy CrawlSpider does not perform LinkExtractor if ran via process.crawl()

Carlos

[Scrapy CrawlSpider does not perform LinkExtractor if ran via process.crawl()](https://stackoverflow.com/questions/59740252/scrapy-crawlspider-does-not-perform-linkextractor-if-ran-via-process-crawl)

I can't figure out why my spider is only crawling the start_url, and ignoring extracting any urls that match the allow parameter. I am uncertain if the issue occurs due to it being called from __name__.

2020-01-14 19:09:37Z

I can't figure out why my spider is only crawling the start_url, and ignoring extracting any urls that match the allow parameter. I am uncertain if the issue occurs due to it being called from __name__.The problem is probably that you're redefining the parse method, which should be avoided. From the crawling rules docs:So I'd try naming the function something else (I renamed it to parse_item, similar to the CrawlSpider example from the docs, but you can use any name):

Scrapy FormRequest parameter not working but showing all result instead

adrian

[Scrapy FormRequest parameter not working but showing all result instead](https://stackoverflow.com/questions/59259699/scrapy-formrequest-parameter-not-working-but-showing-all-result-instead)

I am scraping this webpage https://researchgrant.gov.sg/eservices/advanced-search/?keyword=&source=sharepoint&type=project&status=open&_pp_projectstatus=&_pp_hiname=&_pp_piname=&_pp_source=sharepoint&_pp_details=#project using Scrapy FormRequest. My code is as below. The parameter _pp_hiname with ab and _pp_piname with pua should only return 1 result in response.text but instead it returns all the result in HTML code. Parameter apparently not working but I couldn't see any wrong with it. Should be only 1 entry:

But apparently it is showing all entry:

Latest update:Console message:



2019-12-10 03:17:05Z

I am scraping this webpage https://researchgrant.gov.sg/eservices/advanced-search/?keyword=&source=sharepoint&type=project&status=open&_pp_projectstatus=&_pp_hiname=&_pp_piname=&_pp_source=sharepoint&_pp_details=#project using Scrapy FormRequest. My code is as below. The parameter _pp_hiname with ab and _pp_piname with pua should only return 1 result in response.text but instead it returns all the result in HTML code. Parameter apparently not working but I couldn't see any wrong with it. Should be only 1 entry:

But apparently it is showing all entry:

Latest update:Console message:

It sends in POST body only Name=advancesearchawardedprojectsp. Other parameters should be in url as query. So url should behttps://researchgrant.gov.sg/eservices/mvcgrid?keyword=&source=sharepoint&type=project&status=open&page=1&_pp_projectstatus=&_pp_hiname=ab&_pp_piname=pua&_pp_source=&_pp_detailsYou can use urllib.parse.urlencode(args) for this. And it gives me one result.EDIT: example which loads next pages and check button Next Page to stop.EDIT: now it can save in csv file.

Extending Scrapy Pause and Resume Extension to save requests periodically [closed]

Ahsan Roy

[Extending Scrapy Pause and Resume Extension to save requests periodically [closed]](https://stackoverflow.com/questions/58111083/extending-scrapy-pause-and-resume-extension-to-save-requests-periodically)

Scrapy pause and resume only work pressing ctrl+C. And in case of forcefully closing the app, one cannot resume. How can I periodically save the requests (say every X seconds) using/modifying the Scrapy PauseResume Extension

2019-09-26 06:53:25Z

Scrapy pause and resume only work pressing ctrl+C. And in case of forcefully closing the app, one cannot resume. How can I periodically save the requests (say every X seconds) using/modifying the Scrapy PauseResume Extension

Unable to create process using '“c:\bld\scrapy_1564674375870\_h_env\python.exe”

Mateusz Pełechaty

[Unable to create process using '“c:\bld\scrapy_1564674375870\_h_env\python.exe”](https://stackoverflow.com/questions/57339230/unable-to-create-process-using-c-bld-scrapy-1564674375870-h-env-python-exe)

I'm trying to install Scrapy with AnacondaAfter downloading scrapy by command conda install -c conda-forge scrapy

and then giving the path of Anaconda/Scripts to environmental variables, I'm getting following error:I'm getting this error in both cmd and Anaconda

2019-08-03 14:28:18Z

I'm trying to install Scrapy with AnacondaAfter downloading scrapy by command conda install -c conda-forge scrapy

and then giving the path of Anaconda/Scripts to environmental variables, I'm getting following error:I'm getting this error in both cmd and AnacondaForce reinstalling scrapy using pip solved the problem.All the comments here helped me to solve this issue. However, I'm listing here the steps that I followed.use anaconda prompt as administrator its work for me. 

run anaconda as administrator and pip install scrapyIt worksI don't know why :)I had the same error.

I fixed it by:(Note: Just re-installing Anaconda again, and selecting "All Users" didn't work. I HAD to first remove it via Add/Remove programs.)Then when I ran Anaconda Prompt (as Administrator) it worked.The path I followed:It works for me

How to fix “Fatal error in launcher: Unable to create process using *path*/scrapy.exe” in anaconda? [duplicate]

Mateusz Pełechaty

[How to fix “Fatal error in launcher: Unable to create process using *path*/scrapy.exe” in anaconda? [duplicate]](https://stackoverflow.com/questions/57307710/how-to-fix-fatal-error-in-launcher-unable-to-create-process-using-path-scrap)

I am trying to install scrapy on Windows 10, 

By following these tutorials: 

https://docs.scrapy.org/en/latest/intro/tutorial.html

https://www.accordbox.com/blog/scrapy-tutorial-4-how-install-scrapy-windows/After installing with anaconda using conda install -c conda-forge scrapy, 

I write "scrapy" in anaconda prompt

and get: also scrapy -h yields almost the same information, with -h on the endtyping scrapy startproject x in cmd gives My environmental variables do not have any spaces.Also I have tried creating virtualenv and installing scrapy inside of it. 

The error i gotI have got newest Microsoft Visual C++

2019-08-01 11:11:08Z

I am trying to install scrapy on Windows 10, 

By following these tutorials: 

https://docs.scrapy.org/en/latest/intro/tutorial.html

https://www.accordbox.com/blog/scrapy-tutorial-4-how-install-scrapy-windows/After installing with anaconda using conda install -c conda-forge scrapy, 

I write "scrapy" in anaconda prompt

and get: also scrapy -h yields almost the same information, with -h on the endtyping scrapy startproject x in cmd gives My environmental variables do not have any spaces.Also I have tried creating virtualenv and installing scrapy inside of it. 

The error i gotI have got newest Microsoft Visual C++For installing scrapy you don't need to install Anaconda.run anaconda prompt as administrator and install Scrapycheck the version as scrapy install correctlynow run anaconda prompt without administrator

How to scrape recommendations on web page

Newbie

[How to scrape recommendations on web page](https://stackoverflow.com/questions/57108551/how-to-scrape-recommendations-on-web-page)

Consider this link :

https://www.michaelkors.com/logo-tape-ribbed-stretch-viscose-sweater/_/R-US_MH86NXK5ZWOnce you scroll down you see recommendations on this page. I want to get titles of these products mentioned. I have tried using this:However, it returns nothing. Reason might be that the products are lazy loaded. How should I extract their titles.

2019-07-19 08:33:50Z

Consider this link :

https://www.michaelkors.com/logo-tape-ribbed-stretch-viscose-sweater/_/R-US_MH86NXK5ZWOnce you scroll down you see recommendations on this page. I want to get titles of these products mentioned. I have tried using this:However, it returns nothing. Reason might be that the products are lazy loaded. How should I extract their titles.You can use selenium to scroll to the bottom of the page. However, it still takes a while for the site to load the recommendations. As such, this solution waits by using a while loop until the product recommendation section appears:Output:I am not familiar with Python but your XPath cannot match. Try //div[contains(@class, "product-tile-container")]//a//img/@src instead. A single slash means that the element is a direct child of the previous one. A double slash means that you expect the mentioned element somewhere in the hierarchy of the current one.You could make the XPath more robust if you add an additional path check for any div with the class product-image-container:  //div[contains(@class, "product-tile-container")]//a/div[contains(@class, 'product-image-container')]//img/@srcI highly suggest you to use an plugin to check XPath, e.g. https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjlYou can't access that data with a simple scrapy spider cause the page is rendered through JS. You can try by disabling the JS of your browser and refreshing the page. You will see a blank page. If you inspect it, you will notice there are no data related to the products.If you want to scrape that type of JS rendered page, I would recommand you to use splash and scrapy-splash. It is well documented and easy to use. It is a rendering service that will allow you to scrape that data you want. (It is supported by scrapinghub, the smart brains behind scrapy).

Scrapy LinkExtractor with svg element as Next button

user299791

[Scrapy LinkExtractor with svg element as Next button](https://stackoverflow.com/questions/55919187/scrapy-linkextractor-with-svg-element-as-next-button)

I am using a CrawlSpider that recursively follow links calling the next page using a link extraction like:I have applied this strategy to recursively crawl different websites, and as far as there was text in the html tag, like <a href="somelink">sometext</a>, everything worked fine.I am now trying to scrape a website that has an <div class="bui-pagination__item bui-pagination__next-arrow">

    <a class="pagenext" href="/url.html" aria-label="Pagina successiva">

        <svg class="bk-icon -iconset-navarrow_right bui-pagination__icon" height="18" role="presentation" width="18" viewBox="0 0 128 128">

           <path d="M54.3 96a4 4 0 0 1-2.8-6.8L76.7 64 51.5 38.8a4 4 0 0 1 5.7-5.6L88 64 57.2 94.8a4 4 0 0 1-2.9 1.2z"></path>

        </svg>

    </a>

</div> as a 'next' button instead of simple text, and my LinkExtractor rule does not seem to apply anymore, and the spider stops after the first page.I have tried to look for the svg element, but that doesn't seem to trigger the extraction:Is there anything I am missing?

2019-04-30 10:39:24Z

I am using a CrawlSpider that recursively follow links calling the next page using a link extraction like:I have applied this strategy to recursively crawl different websites, and as far as there was text in the html tag, like <a href="somelink">sometext</a>, everything worked fine.I am now trying to scrape a website that has an <div class="bui-pagination__item bui-pagination__next-arrow">

    <a class="pagenext" href="/url.html" aria-label="Pagina successiva">

        <svg class="bk-icon -iconset-navarrow_right bui-pagination__icon" height="18" role="presentation" width="18" viewBox="0 0 128 128">

           <path d="M54.3 96a4 4 0 0 1-2.8-6.8L76.7 64 51.5 38.8a4 4 0 0 1 5.7-5.6L88 64 57.2 94.8a4 4 0 0 1-2.9 1.2z"></path>

        </svg>

    </a>

</div> as a 'next' button instead of simple text, and my LinkExtractor rule does not seem to apply anymore, and the spider stops after the first page.I have tried to look for the svg element, but that doesn't seem to trigger the extraction:Is there anything I am missing?

How can I combine the two spiders into just one?

it_is_a_literature

[How can I combine the two spiders into just one?](https://stackoverflow.com/questions/55205449/how-can-i-combine-the-two-spiders-into-just-one)

There are two spiders which use the same resource file and almost the same structure.The spiderA contains :The spiderB contains :How can I combine spiderA and spiderB, and add a switch variable to let crapy scral call different spider depending on my need?  

2019-03-17 09:03:53Z

There are two spiders which use the same resource file and almost the same structure.The spiderA contains :The spiderB contains :How can I combine spiderA and spiderB, and add a switch variable to let crapy scral call different spider depending on my need?  Try to add separate parameter for spider type. You can set it with calling scrapy crawl myspider -a spider_type=second. Check this code example:And also you can always create base main class and then inherit from it, overloading only one variable (that you add to url) and name (for separate calls).spider_type  result in errorIt is self.spider_type in spider class.To make it more strictly and accurately.

Form Request Using Scrapy + Splash

J. Dykstra

[Form Request Using Scrapy + Splash](https://stackoverflow.com/questions/53787925/form-request-using-scrapy-splash)

I am trying to login to a website using the following code (slightly modified for this post): I am getting the error: I am still not able to login, for some reason.  I have bounced around many different posts on here, and have tried many different variation of "splash:select", but I can't seem to find my issue.  When i inspect the webpage with chrome, I see this (with a similar html for the password): The above html, I believe is written in JS though. So I am not able to grab it with Scrapy, so, I viewed the source of the page and I think the relevant JS code to use with Splash is this (not sure though): Can someone nudge me in the right direction?

2018-12-14 22:56:57Z

I am trying to login to a website using the following code (slightly modified for this post): I am getting the error: I am still not able to login, for some reason.  I have bounced around many different posts on here, and have tried many different variation of "splash:select", but I can't seem to find my issue.  When i inspect the webpage with chrome, I see this (with a similar html for the password): The above html, I believe is written in JS though. So I am not able to grab it with Scrapy, so, I viewed the source of the page and I think the relevant JS code to use with Splash is this (not sure though): Can someone nudge me in the right direction?The main problem here is that the login form is inside an iframe element.

I do not know scrapy_splash, so below POC code uses selenium and beautiful soup. But the mechanism will be similar with splash, you need to switch to the iframe and then back when id disappears.For this code to work you need to have firefox and geckodriver installed and in the path, and compatible version.

Is Scrapy compatible with multiprocessing?

AngelLB

[Is Scrapy compatible with multiprocessing?](https://stackoverflow.com/questions/53733190/is-scrapy-compatible-with-multiprocessing)

So I have been using selenium to make my scraping. BUT I want to change all the code to Scrapy. The only thing I'm no sure about is that I'm using multiprocessing (python library) to speed up my process. I have researched a lot but I quite don't get it. I have found: Multiprocessing of Scrapy Spiders in Parallel Processes but it doesn't help me because it says that it can be done with Twisted but I haven't found an example yet.In other forums it says that Scrapy can work with multiprocessing.Last thing, in scrapy the option CONCURRENT_REQUESTS (settings) has some connection with multiprocessing?

2018-12-11 22:18:44Z

So I have been using selenium to make my scraping. BUT I want to change all the code to Scrapy. The only thing I'm no sure about is that I'm using multiprocessing (python library) to speed up my process. I have researched a lot but I quite don't get it. I have found: Multiprocessing of Scrapy Spiders in Parallel Processes but it doesn't help me because it says that it can be done with Twisted but I haven't found an example yet.In other forums it says that Scrapy can work with multiprocessing.Last thing, in scrapy the option CONCURRENT_REQUESTS (settings) has some connection with multiprocessing?The recommended way for working with scrapy is to NOT use multiprocessing inside the running spiders.The better alternative would be to invoke several scrapy jobs with the respective separated inputs.Scrapy jobs themselves are very fast IMO, of course, you can always go faster, special settings as you mentioned CONCURRENT_REQUESTS, CONCURRENT_REQUESTS_PER_DOMAIN, DOWNLOAD_DELAY, etc. But this is basically because scrapy is asynchronous, meaning it won't wait for the requests to be completed to schedule and continue working on the remaining tasks (scheduling more requests, parsing responses, etc.)The CONCURRENT_REQUESTS doesn't have a connection with multiprocessing. It is mostly a way to "limit" the speed of how many requests could be scheduled, because of being asynchronous.You can use: If you need more than that or you have some heavy processing, I suggest that you move this part in a separate process.Scrapy's responsibility is web parsing, you could for example, in an item pipeline, send tasks to a queue and have a separate process consume and process tasks.

Passing Cyrilics in Xpath Contains returns XML value error. Scrapy. Python 2

Billy Jhon

[Passing Cyrilics in Xpath Contains returns XML value error. Scrapy. Python 2](https://stackoverflow.com/questions/51864582/passing-cyrilics-in-xpath-contains-returns-xml-value-error-scrapy-python-2)

I am trying to get an element like this by xpath Text contains.As a result I am getting this error.Here is my xpath'Полное' is russian text I use for search. 

How do I fix the error?

2018-08-15 18:40:45Z

I am trying to get an element like this by xpath Text contains.As a result I am getting this error.Here is my xpath'Полное' is russian text I use for search. 

How do I fix the error?Prefix your expression string with a u to make a unicode string:

Export scrapy items to different files

Luis Ramon Ramirez Rodriguez

[Export scrapy items to different files](https://stackoverflow.com/questions/50083638/export-scrapy-items-to-different-files)

I'm scraping review from moocs likes this oneFrom there I'm getting all the course details,  5 items and another 6 items from each review itself. This is the code I have for the course details:Now I want to include the review details, another 5 items for each review. 

Since the course data is common for all the reviews I want to store it in a different file and use course name/id to relate the data afterward.This is the code I have for the review's items:I tried to work with a temporary solution, returning all the items for each case but is not working either:The output file for that script is corrupted, cells are displaced and the size of the fields is not correct.EDIT: 

I want to have two files at the output:The first one containing:And the second one with:

2018-04-29 05:20:43Z

I'm scraping review from moocs likes this oneFrom there I'm getting all the course details,  5 items and another 6 items from each review itself. This is the code I have for the course details:Now I want to include the review details, another 5 items for each review. 

Since the course data is common for all the reviews I want to store it in a different file and use course name/id to relate the data afterward.This is the code I have for the review's items:I tried to work with a temporary solution, returning all the items for each case but is not working either:The output file for that script is corrupted, cells are displaced and the size of the fields is not correct.EDIT: 

I want to have two files at the output:The first one containing:And the second one with:The issue is you are mixing everything up into a single item, which is not the right way to do it. You should created two items MoocsItem and MoocsReviewItemAnd then update the code like belowNow what you want is that different item type goes in different csv files. Which is what the below SO thread answersHow can scrapy export items to separate csv files per itemHave not tested the below, but the code will become something like belowYou need make sure the ITEM_PIPELINES is updated to use this MultiCSVItemPipeline class

What is a good crawling speed rate?

Nilesh Guria

[What is a good crawling speed rate?](https://stackoverflow.com/questions/49494093/what-is-a-good-crawling-speed-rate)

I'm crawling web pages to create a search engine and have been able to crawl close to 9300 pages in 1 hour using Scrapy. I'd like to know how much more can I improve and what value is considered as a 'good' crawling speed.

2018-03-26 14:38:24Z

I'm crawling web pages to create a search engine and have been able to crawl close to 9300 pages in 1 hour using Scrapy. I'd like to know how much more can I improve and what value is considered as a 'good' crawling speed.Short answer: There is no real recommended speed for creating a search engine.Long answer:Crawling speed, in general, doesn't really determine if your crawler is good or bad, or even if it will work as the program that feeds your search engine.You also cannot talk about crawling speed when talking to crawl a lot of pages, on multiple sites. Crawling speed should be determined per site only, meaning that the crawler should be configurable in a way that it can be changed how often it hits a site at any specific time, you can see that Google also offers this.If we are talking about the current rate you mentioned (9300/hour), it means you are collecting ~2.5 pages per second, which I would say it is not bad, but as explained before, it doesn't help determine your end goal (create a search engine).Also, if you really decide to implement a broad crawler for creating a search engine with Scrapy, you'll never only send 1 process with Scrapy. You'll need to setup thousands (even more) of spiders running to check to get the more information needed. Also you'll have to setup different services to help you maintain those spiders and how they behave between processes. For starters I would recommend checking Frontera and Scrapyd.I'm no expert but I would say that your speed is pretty slow. I just went to google, typed in the word "hats", pressed enter and: about 650,000,000 results (0.63 seconds). That's gonna be tough to compete with. I'd say that there's plenty of room to improve.

Crawling a specific information from a URL in Python

Roshni Amber

[Crawling a specific information from a URL in Python](https://stackoverflow.com/questions/49318015/crawling-a-specific-information-from-a-url-in-python)

The simplest way to crawl HTML tables is using pandas.read_html(url). For the following URL, I get all of its tablesFrom the above URL I just want this specific information.Considering above df as a list I have written following code to get this specific informationBut this sometimes works fine for some URL's and sometimes it also includes other class details as well as Current CPC Class and Current International Class. I have also tried BeautifulSoap using View Page Source feature but I am confused to mention class.

2018-03-16 10:07:40Z

The simplest way to crawl HTML tables is using pandas.read_html(url). For the following URL, I get all of its tablesFrom the above URL I just want this specific information.Considering above df as a list I have written following code to get this specific informationBut this sometimes works fine for some URL's and sometimes it also includes other class details as well as Current CPC Class and Current International Class. I have also tried BeautifulSoap using View Page Source feature but I am confused to mention class.Using BeautifulSoupExplanationThe table you are after, is the 5th table in the page. find_all('table') returns a list of all the tables. So, find_all('table')[4] will give the 5th table.The text you want, is located in the first row, or the first tr tag. table.find('tr') returns the first tr tag found inside the table.Finally, .text gives you the text inside the tag.

Scrapy get all links from any website

Brandon Skerritt

[Scrapy get all links from any website](https://stackoverflow.com/questions/48946320/scrapy-get-all-links-from-any-website)

I have the following code for a web crawler in Python 3:The code basically gets all the links off of my GitHub pages website, and then it gets all the links off of those links, and so on until the end of time or an error occurs.I want to recreate this code in Scrapy so it can obey robots.txt and be a better web crawler overall. I've researched online and I can only find tutorials / guides / stackoverflow / quora / blog posts about how to scrape a specific domain (allowed_domains=["google.com"], for example). I do not want to do this. I want to create code that will scrape all websites recursively.This isn't much of a problem but all the blog posts etc only show how to get the links from a specific website (for example, it might be that he links are in list tags). The code I have above works for all anchor tags, regardless of what website it's being run on.I do not want to use this in the wild, I need it for demonstration purposes so I'm not going to suddenly annoy everyone with excessive web crawling.Any help will be appreciated!

2018-02-23 11:01:22Z

I have the following code for a web crawler in Python 3:The code basically gets all the links off of my GitHub pages website, and then it gets all the links off of those links, and so on until the end of time or an error occurs.I want to recreate this code in Scrapy so it can obey robots.txt and be a better web crawler overall. I've researched online and I can only find tutorials / guides / stackoverflow / quora / blog posts about how to scrape a specific domain (allowed_domains=["google.com"], for example). I do not want to do this. I want to create code that will scrape all websites recursively.This isn't much of a problem but all the blog posts etc only show how to get the links from a specific website (for example, it might be that he links are in list tags). The code I have above works for all anchor tags, regardless of what website it's being run on.I do not want to use this in the wild, I need it for demonstration purposes so I'm not going to suddenly annoy everyone with excessive web crawling.Any help will be appreciated!If you want to allow crawling of all domains, simply don't specify allowed_domains, and use a LinkExtractor which extracts all links.A simple spider that follows all links:There is an entire section of scrapy guide dedicated to broad crawls. I suggest you to fine-grain your settings for doing this succesfully.For recreating the behaviour you need in scrapy, you mustAn untested example (that can be, of course, refined):

Split hyphen separated words with spaces in between | Python

Dan

[Split hyphen separated words with spaces in between | Python](https://stackoverflow.com/questions/59004714/split-hyphen-separated-words-with-spaces-in-between-python)

I want to split either comma, semicolon or hyphen (with preceding space) separated words.

The reason for this is the inconsistent structure of a website I am scraping with Scrapy.

So far, I am able to split either comma or semicolon separated words with follwing code:That works if the words are separated like: But sometimes the keywords are also stored as follows:keyword1 - keyword2 - keyword3I don't know how to split them properly, because the spaces in between the hyphens are giving me headache :). Help is very much appreciated!

2019-11-23 05:10:52Z

I want to split either comma, semicolon or hyphen (with preceding space) separated words.

The reason for this is the inconsistent structure of a website I am scraping with Scrapy.

So far, I am able to split either comma or semicolon separated words with follwing code:That works if the words are separated like: But sometimes the keywords are also stored as follows:keyword1 - keyword2 - keyword3I don't know how to split them properly, because the spaces in between the hyphens are giving me headache :). Help is very much appreciated!You may want to use Regular Expressions. re.split('\s*-\s*', mystring) should do the job. Have you tried:You may want to look into regular expressionsyou can first use strip() then try to splitIt appears to be a problem with my code I posted in my original question. Thus, there isn't really a problem with the spaces in between hyphens and I can simply solve the issue by using the elif statement as follows:Anyway, thank you all for your suggestions on solving this issue. You can simply replace all special characters giving you headache with wight space then split it.Output:Data.replace(' - ','; ') will replace all keywords separated by hyphens and a space on each side to keywords separated by semicolons and one space. Add that into your code to prior to the if statement and you should be good to go.Code:Output:

Script throws an error while using a particular link among several

robots.txt

[Script throws an error while using a particular link among several](https://stackoverflow.com/questions/58051328/script-throws-an-error-while-using-a-particular-link-among-several)

I've written a script using scrapy in combination with selenium to parse the name of CEO's of different companies from a webpage. You can find the name of different companies in the landing page. However, you can get the name of CEO's once you click on the name of the company links. The following script can parse the links of different companies and use those links to scrape the names of CEO'S except for the second company. When the script tries to parse the name of CEO using the link of the second company, it encounters stale element reference error. The script fetches the rest of the results in the right way even when It encountered that error along the way. Once again - it only throws error parsing the information using the second company link. How weird!!The webpage linkThis is what I've tried so far with:This is the type of results I'm getting:PS I can use their api to get all the information but I'm curious to know why this weird trouble the above script is facing.

2019-09-22 16:41:30Z

I've written a script using scrapy in combination with selenium to parse the name of CEO's of different companies from a webpage. You can find the name of different companies in the landing page. However, you can get the name of CEO's once you click on the name of the company links. The following script can parse the links of different companies and use those links to scrape the names of CEO'S except for the second company. When the script tries to parse the name of CEO using the link of the second company, it encounters stale element reference error. The script fetches the rest of the results in the right way even when It encountered that error along the way. Once again - it only throws error parsing the information using the second company link. How weird!!The webpage linkThis is what I've tried so far with:This is the type of results I'm getting:PS I can use their api to get all the information but I'm curious to know why this weird trouble the above script is facing.A slightly modified approach should get you all the desired content from that site without any issues. All you need to do is store all the target links as a list within get_links() method and use return or yield while making callback to get_inner_content() method. You can also disable the images to make the script slightly faster.The following attempt should get you all the results:Or using yield:You are getting a Stale Element Exception because on line 24 you are navigating away from the original page.Since you are looping through the links on line 19...Any subsequent access to item will be rendered as a Stale Element Exception if you try to perform an access on it since the page was navigated away from in the context of driver.Your script will work for "Walmart" since it's the first item.  You are getting this error on Exxon Mobil because the page was then navigated away from on line 24.To parse the name of CEO's of different companies from the webpage https://fortune.com/fortune500/search/ Selenium alone itself would be enough and you  need to:Here is how you can get companies details without using Selenium much faster and lighter.

See how I get company_name and change_the_world to extract other details.Result:

How to target data attribute with Scrapy

Boky

[How to target data attribute with Scrapy](https://stackoverflow.com/questions/50734845/how-to-target-data-attribute-with-scrapy)

I'm using Scrapy library to crawl a webpage.But I have a problem. I do not know how to target data attribute.I have an link with data attribute and href as follows:What I want is the value of href. If a had class I could do it as follows:But the problem is that I do not know how to target data-item-name attribute.Any advice?

2018-06-07 07:02:39Z

I'm using Scrapy library to crawl a webpage.But I have a problem. I do not know how to target data attribute.I have an link with data attribute and href as follows:What I want is the value of href. If a had class I could do it as follows:But the problem is that I do not know how to target data-item-name attribute.Any advice?Using scrapy css selector, you can do :I'm not sure, if you can do this with the css method, but with the xpath method you should be able to do:

duplicate requests post to scrapy FormRequest

hadesfv

[duplicate requests post to scrapy FormRequest](https://stackoverflow.com/questions/57452400/duplicate-requests-post-to-scrapy-formrequest)

I am try to learn how  scrapy FormRequest works on website,I have the following scrapy code:I can't seem to get to right response,I first tried using dictionary but it didn't work, then I tested with requests as following and both my tries works.FormRequest takes in key,value not string which is why json.dumps() is throwing an error. My question is How can I get FormRequest (or any scrapy methods) to work on this example i.e get the same results as requests.I believe res3 = requests.post(url, data=payload) is the same as FormRequest(url,formdata=payload) which is why it is not working.

2019-08-11 18:09:40Z

I am try to learn how  scrapy FormRequest works on website,I have the following scrapy code:I can't seem to get to right response,I first tried using dictionary but it didn't work, then I tested with requests as following and both my tries works.FormRequest takes in key,value not string which is why json.dumps() is throwing an error. My question is How can I get FormRequest (or any scrapy methods) to work on this example i.e get the same results as requests.I believe res3 = requests.post(url, data=payload) is the same as FormRequest(url,formdata=payload) which is why it is not working.According to scrapy docs - dict objects are allowed.And Your code works

Update (not actual as problem was in request body not in headers) I use fiddler debugging proxy and compare requests and responses made by different libraries. 





 As you can see Scrapy and requests library make request with different headers. If you need to receive tha valid request in scrapy code - you need to modify your headers.UPDATE_2It's a common trap for scrapy users. FormRequest is forming urlencoded payload from dict, eg:In your case you should use regular Request class with body=json.dumps(your_dict)

Getting a response body with scrapy splash

user61629

[Getting a response body with scrapy splash](https://stackoverflow.com/questions/56744120/getting-a-response-body-with-scrapy-splash)

I'm working with scrapy 1.6 and splash 3.2 I have:The problem is that when I try to open the response in the browser I get it opening in notepad instead.looking at https://splash.readthedocs.io/en/stable/scripting-response-object.html. How do I activate the response.body so I can open the response in a browser (I want to be able to then use browser dev tools to get xpaths)?

2019-06-24 21:21:08Z

I'm working with scrapy 1.6 and splash 3.2 I have:The problem is that when I try to open the response in the browser I get it opening in notepad instead.looking at https://splash.readthedocs.io/en/stable/scripting-response-object.html. How do I activate the response.body so I can open the response in a browser (I want to be able to then use browser dev tools to get xpaths)?open_in_browser() cannot detect responses from Splash as HTML responses. This is because Splash HTML response objects are subclasses of Scrapy’s TextResponse instead of HtmlResponse (for now).You could reimplement open_in_browser() in a way that works for your use case for the time being.I got it working with:

Click Button in Scrapy-Splash

Tim

[Click Button in Scrapy-Splash](https://stackoverflow.com/questions/56706272/click-button-in-scrapy-splash)

I am writing a scrapy-splash program and I need to click on the display button on the webpage, as seen in the image below, in order to display the data, for 10th edition, so I can scrape it. I have the code I tried below but it does not work. The information I need is only accessible if I click the display button.  UPDATE: Still struggling with this and I have to believe there is a way to do this.  I do not want to scrape the JSON because that could be a red flag to site owners.



2019-06-21 15:22:56Z

I am writing a scrapy-splash program and I need to click on the display button on the webpage, as seen in the image below, in order to display the data, for 10th edition, so I can scrape it. I have the code I tried below but it does not work. The information I need is only accessible if I click the display button.  UPDATE: Still struggling with this and I have to believe there is a way to do this.  I do not want to scrape the JSON because that could be a red flag to site owners.

Your code can't work because there is no anchor element and no href attribute. Clicking the button will send an XMLHttpRequest to http://www.starcitygames.com/buylist/search?search-type=category&id=5061 and the data you want is found in the JSON response.As you can see, you don't even need to log in into the website or Splash.

scrapinghub starting job too slow

Mara M

[scrapinghub starting job too slow](https://stackoverflow.com/questions/56249880/scrapinghub-starting-job-too-slow)

I am new in scraping and I am running different jobs on scrapinghub. I run them via their API. The problem is that starting the spider and initializing it takes too much time like 30 seconds. When I run it locally, it takes up to 5 seconds to finish the spider. But in scrapinghub it takes 2:30 minutes. I understand that closing a spider after all requests are finished takes a little bit more time, but this is not a problem. Anyway, my problem is that from the moment I call the API to start the job (I see that it appear in running jobs instantly, but takes too long to make the first request) and the moment the first request is done, I have to wait too much. Any idea how I can make it to last as shortly as locally? Thanks!I already tried to put AUTOTHROTTLE_ENABLED = false as I saw in some other question on stackoverflow.

2019-05-22 05:30:02Z

I am new in scraping and I am running different jobs on scrapinghub. I run them via their API. The problem is that starting the spider and initializing it takes too much time like 30 seconds. When I run it locally, it takes up to 5 seconds to finish the spider. But in scrapinghub it takes 2:30 minutes. I understand that closing a spider after all requests are finished takes a little bit more time, but this is not a problem. Anyway, my problem is that from the moment I call the API to start the job (I see that it appear in running jobs instantly, but takes too long to make the first request) and the moment the first request is done, I have to wait too much. Any idea how I can make it to last as shortly as locally? Thanks!I already tried to put AUTOTHROTTLE_ENABLED = false as I saw in some other question on stackoverflow.According to scrapy cloud docs:

Scrapy Cloud jobs run in containers. These containers can be of different sizes defined by Scrapy Cloud units.A Scrapy Cloud provides: 1 GB of RAM, 2.5GB of disk space,1x CPU and 1 concurrent crawl slot.Resources available to the job are proportional to the number of units allocated. 

It means that allocating more Scrapy Cloud units can solve your problem.

Reg Ex for a negative lookaround or negative assertion for an underscore needed

Chris

[Reg Ex for a negative lookaround or negative assertion for an underscore needed](https://stackoverflow.com/questions/54304875/reg-ex-for-a-negative-lookaround-or-negative-assertion-for-an-underscore-needed)

I've got URL patterns that always start with one of 3 words behind the toplevel url:Then there could be anything in several subdirectories (up to 4 levels deep) but ALWAYS:

lowercaseword or lowercaseword-lowercasewordNow I'm searching for a reg ex which would match exactly this but not a subdirectory starting with an underscore:

_lowercasewordSo my URLs are:and I'm searching for a reg ex that matches the first three.I can't figure out the negative lookaround (if that's what is needed).

Any ideas?These are my expressions, but they're not working:

2019-01-22 09:17:35Z

I've got URL patterns that always start with one of 3 words behind the toplevel url:Then there could be anything in several subdirectories (up to 4 levels deep) but ALWAYS:

lowercaseword or lowercaseword-lowercasewordNow I'm searching for a reg ex which would match exactly this but not a subdirectory starting with an underscore:

_lowercasewordSo my URLs are:and I'm searching for a reg ex that matches the first three.I can't figure out the negative lookaround (if that's what is needed).

Any ideas?These are my expressions, but they're not working:You may useSee the regex demo.Details

Scrapy: follow external links only

lecodesportif

[Scrapy: follow external links only](https://stackoverflow.com/questions/53547246/scrapy-follow-external-links-only)

With OffsiteMiddleware you can control how to follow external links in Scrapy.I want the spider to ignore all internal links on a site and follow external links only.Dynamic rules to add the response URL domain to deny_domains didn't work.Can you override get_host_regex in OffsiteMiddleware to filter out all onsite links? Any other way?Clarification: I want the spider to ignore the domains defined in allowed_domains and all internal links on each domain crawled. So the domain of every URL followed by the spider must be ignored when the spider is on that URL. In other words: When the crawler reaches a site like example.com, I want it to ignore any links on example.com and only follow external links to sites that are not on example.com.

2018-11-29 20:41:57Z

With OffsiteMiddleware you can control how to follow external links in Scrapy.I want the spider to ignore all internal links on a site and follow external links only.Dynamic rules to add the response URL domain to deny_domains didn't work.Can you override get_host_regex in OffsiteMiddleware to filter out all onsite links? Any other way?Clarification: I want the spider to ignore the domains defined in allowed_domains and all internal links on each domain crawled. So the domain of every URL followed by the spider must be ignored when the spider is on that URL. In other words: When the crawler reaches a site like example.com, I want it to ignore any links on example.com and only follow external links to sites that are not on example.com.You can create reverse offsite middleware by simply reversing should_follow() method: Then activate it in your settings.py:Now all domains in spider.allowed_domains will be ignored :)My answer doesn't use Scrapy.  Feel free to flag this response if you think it's too far off topic.But what I'm providing is a solution to help solve your more general problem. I ran into a similar problem when I was parsing results from google.  I didn't want any of the boilerplate urls that show up on the results page to be included in my final list of urls.  I also didn't want any of the google relate query strings to show up.  Using BeautifulSoup, re module, and the requests module I was able to do this.  For your problem I would say you only need BeautifulSoup and re.  You'll need a function that will filter domains.  The function should take two params, reference and the url under test.  Using the re module you can check to see if the test url base string is the same as the reference string; if yes then it would reasonable to conclude that it's an internal url. You'd use BeautifulSoup to parse the html for <a> tags that contain an href.

Rename scrapy project

New Guy

[Rename scrapy project](https://stackoverflow.com/questions/53498732/rename-scrapy-project)

I couldn't find much information on this. I want to rename my Scrapy project, spiders made, JSONs created but want to change the name to something meaningful without messing anything up.A solution through the terminal would also be appreciated.

2018-11-27 11:33:03Z

I couldn't find much information on this. I want to rename my Scrapy project, spiders made, JSONs created but want to change the name to something meaningful without messing anything up.A solution through the terminal would also be appreciated.As of right now there is no command that changes the project name from the command line. I would just rename it in your file explorer, it shouldn't mess with your spiders. Change project directory name, search old project name in scrapy.cfg and settings.py files and replace with new project name.Basically, you only need to change default = new_project_title under [settings] in scrapy.cfg file and SPIDER_MODULES = ['new_project_title.spiders'] in the settings.py file to make the spider run with the new name of the project. After longer inspection of documentation, I have to say that the quick change of the project name through the scrapy's command line is not that easy.

How to crawl local HTML file with Scrapy

Baka

[How to crawl local HTML file with Scrapy](https://stackoverflow.com/questions/53325150/how-to-crawl-local-html-file-with-scrapy)

I tried to crawl a local HTML file stored in my desktop with the code below, but I encounter the following errors before crawling procedure, such as "No such file or directory: '/robots.txt'".[Scrapy command][Scrapy spider][Errors]

2018-11-15 17:42:32Z

I tried to crawl a local HTML file stored in my desktop with the code below, but I encounter the following errors before crawling procedure, such as "No such file or directory: '/robots.txt'".[Scrapy command][Scrapy spider][Errors]When working with it locally, I never specify the allowed_domains.

Try to take that line of code out and see if it works. In your error its testing the 'empty' domain that you have given it.

Can't get Scrapy Stats from scrapy.CrawlerProcess

Zx3s

[Can't get Scrapy Stats from scrapy.CrawlerProcess](https://stackoverflow.com/questions/53231606/cant-get-scrapy-stats-from-scrapy-crawlerprocess)

I'm running scrapy spiders from another script and I need to retrieve and save to variable stats from Crawler. I've looked into docs and other StackOverflow questions but I haven't been able to solve this issue.This is my script from which I'm running crawling:I would like stats to contain this piece of data (scrapy.statscollectors):I've inspected CrawlerProcess which returns deferred and deletes crawlers from its 'crawlers' field once the scraping process is finished.Is there a way to solve this?Best,

Peter

2018-11-09 18:41:19Z

I'm running scrapy spiders from another script and I need to retrieve and save to variable stats from Crawler. I've looked into docs and other StackOverflow questions but I haven't been able to solve this issue.This is my script from which I'm running crawling:I would like stats to contain this piece of data (scrapy.statscollectors):I've inspected CrawlerProcess which returns deferred and deletes crawlers from its 'crawlers' field once the scraping process is finished.Is there a way to solve this?Best,

PeterAccording to the documentation, CrawlerProcess.crawl accepts either a crawler or a spider class, and you're able to create a crawler from the spider class via CrawlerProcess.create_crawler.Thus you may create the crawler instance before starting the crawl process, and retrieve the expected attributes after that.Below I've got you an example, by editing a few lines of your original code:

How to set a default value when Scrapy selector with extract() returns None?

petezurich

[How to set a default value when Scrapy selector with extract() returns None?](https://stackoverflow.com/questions/53238215/how-to-set-a-default-value-when-scrapy-selector-with-extract-returns-none)

I am trying to yield the value of a tag that isn't always present in the pages that I scrape with Scrapy. I am using the extract() function rather than extract_first(). Therefore I cannot seem to set a default value, like suggested in this SO post.This doesn't work:How can I set None as default when I want to use extract() rather than extract_first()?Thanks very much in advance! 

2018-11-10 10:53:10Z

I am trying to yield the value of a tag that isn't always present in the pages that I scrape with Scrapy. I am using the extract() function rather than extract_first(). Therefore I cannot seem to set a default value, like suggested in this SO post.This doesn't work:How can I set None as default when I want to use extract() rather than extract_first()?Thanks very much in advance! Try this syntax:If result of response.css(CSS) is empty list, then None will be assigned as value of comments key. Otherwise, actual value will be assigned

Scrapy / crawling - detecting spider traps or infinite websites

ak1652

[Scrapy / crawling - detecting spider traps or infinite websites](https://stackoverflow.com/questions/53045620/scrapy-crawling-detecting-spider-traps-or-infinite-websites)

Having read "Why Johnny Can’t Pentest: An Analysis of Black-box Web Vulnerability Scanners", it is understood that there are websites such as calendar applications which crawlers have difficulty in dealing with. They are seemingly "infinite" websites which can just contain links to the next day/month/year etc.Also, some websites set up spider traps or may inadvertently create a similar system (where the page links are never-ending).If I a) have the permission of the site owner to crawl freely through their website and b) wish to use scrapy, what sort of technique can I use to determine if I have indeed encountered an "infinite" website, not specific to any example?Note: I'm not talking about "infinite" scrolling, but rather when there are endless pages.An example of an infinite website could be (though pointless and trivial):where you just keep click next and previous to reveal more pages.

2018-10-29 12:34:35Z

Having read "Why Johnny Can’t Pentest: An Analysis of Black-box Web Vulnerability Scanners", it is understood that there are websites such as calendar applications which crawlers have difficulty in dealing with. They are seemingly "infinite" websites which can just contain links to the next day/month/year etc.Also, some websites set up spider traps or may inadvertently create a similar system (where the page links are never-ending).If I a) have the permission of the site owner to crawl freely through their website and b) wish to use scrapy, what sort of technique can I use to determine if I have indeed encountered an "infinite" website, not specific to any example?Note: I'm not talking about "infinite" scrolling, but rather when there are endless pages.An example of an infinite website could be (though pointless and trivial):where you just keep click next and previous to reveal more pages.Even when pagination is endless, content usually is not. So, when the issue is endless pagination, you can prevent endless looping by fetching the next page only if the current page has content or, if you want to be optimum, only when the current page has the known number of items per page.In other cases, such as browsing a calendar where some dates may have values where others do not, you can hardcode a limit on your spider (if the date covered by the next URL is X or older, do not parse further).One thing I can think of it that pass all items IDs to next page you are scraping

and then check if the next page has same items, that means pagination has ended, there are no new records

How to crawl multiple websites in different timing in scrapy

bhattraideb

[How to crawl multiple websites in different timing in scrapy](https://stackoverflow.com/questions/52399825/how-to-crawl-multiple-websites-in-different-timing-in-scrapy)

I have multiple websites stored in database with different crawl time like every 5/10 minutes for every websites. I have created spider to crawl and running with cron. It will take all the websites from database and run crawling parallely for all websites. How can I implement to crawl each websites with different timing which is stored in the database? Is there any way to handle this in scrapy?

2018-09-19 06:52:41Z

I have multiple websites stored in database with different crawl time like every 5/10 minutes for every websites. I have created spider to crawl and running with cron. It will take all the websites from database and run crawling parallely for all websites. How can I implement to crawl each websites with different timing which is stored in the database? Is there any way to handle this in scrapy?Have you tried playing around with adding a scheduling component in start_requests? 

How to extract the file modification time of a scraped image?

not2qubit

[How to extract the file modification time of a scraped image?](https://stackoverflow.com/questions/52365494/how-to-extract-the-file-modification-time-of-a-scraped-image)

I'm trying to scrape part of a part-website that contain images of the parts, to collect some statistics. However, there is no url or image upload or creation date, so I have to use the approximate image file modification-date to get this info. Using cURL, this is an easy task with:However, I think it would be more convenient to get this within the scrapy spider. But I have no idea if scrapy supports this at all, since I cannot find it in the documentation.Is there a way to get the last-modified date of a scraped image in scrapy?

2018-09-17 10:18:18Z

I'm trying to scrape part of a part-website that contain images of the parts, to collect some statistics. However, there is no url or image upload or creation date, so I have to use the approximate image file modification-date to get this info. Using cURL, this is an easy task with:However, I think it would be more convenient to get this within the scrapy spider. But I have no idea if scrapy supports this at all, since I cannot find it in the documentation.Is there a way to get the last-modified date of a scraped image in scrapy?From the documentation of Scrapy, the response has a headers dict field.So you can access the last-modified with response.headers.get('Last-Modified').

Unable to get results from Scrapy on AWS Lambda

The Empire Strikes Back

[Unable to get results from Scrapy on AWS Lambda](https://stackoverflow.com/questions/52291998/unable-to-get-results-from-scrapy-on-aws-lambda)

I built a crawler using the python scrapy library. It works perfectly and reliably when running locally. I have attempted to port it over to the AWS lambda (I have packaged it appropriately). However when I run it the process isn't blocked whilst the crawl runs and instead completes before the crawlers can return giving no results. These are the last lines I get out of logs before it exits: Whereas normally I would get a whole of information about the pages being crawled. I've tried sleeping after starting the crawl, installing crochet and adding its declarators and installing and using this specific framework that sounds like it addresses this problem but it also doesn't work.I'm sure this is an issue with Lambda not respecting scrapys blocking, but I have no idea on how to address it. 

2018-09-12 09:35:19Z

I built a crawler using the python scrapy library. It works perfectly and reliably when running locally. I have attempted to port it over to the AWS lambda (I have packaged it appropriately). However when I run it the process isn't blocked whilst the crawl runs and instead completes before the crawlers can return giving no results. These are the last lines I get out of logs before it exits: Whereas normally I would get a whole of information about the pages being crawled. I've tried sleeping after starting the crawl, installing crochet and adding its declarators and installing and using this specific framework that sounds like it addresses this problem but it also doesn't work.I'm sure this is an issue with Lambda not respecting scrapys blocking, but I have no idea on how to address it. I had the same problem and fixed it by creating empty modules for sqlite3, as described in this answer: https://stackoverflow.com/a/44532317/5441099. Appearently, Scrapy imports sqlite3, but doesn't necessarily use it. Python3 expects sqlite3 to be on the host machine, but the AWS Lambda machines don't have it. The error message doesn't always show up in the logs.Which means you can make it work by switching to Python2, or creating empty modules for sqlite3 like I did.My entry file for running the crawler is as follows, and it works on Lambda with Python3.6:

Instagram scraping after the new changes with scrapy

kiwibg

[Instagram scraping after the new changes with scrapy](https://stackoverflow.com/questions/52103834/instagram-scraping-after-the-new-changes-with-scrapy)

I've been trying to scrape comments from a bunch of public instagram posts by writing a Python crawler (Scrapy).

I've looked at all the available material, especially this, but so far I've had no luck. It's worth mentioning that I've also tried making the hash from as mentioned here, but also without luck.I keep getting 403 responses from the server. At first I thought It was due to my USER_AGENT settings (set to Mozzila 5....) or headers, but I've tested those (I even went ahead, analyzed X-Instagram-GIS of a request made from inside the browser and checked if the MD5 hash matches on my Scrapy request. 

The generated URL works fine inside a browser regardless if I'm logged into IG or not - however, it breaks when used inside an Incognito window, Scrapy or Scrapy shell.At first I thought it meant that scraping is just not possible, however, the rarcega scraper works just fine (except it's not really handy for scraping individual posts rather than whole user profiles).Any feedback or thoughts would be very much appreciated!

2018-08-30 19:05:30Z

I've been trying to scrape comments from a bunch of public instagram posts by writing a Python crawler (Scrapy).

I've looked at all the available material, especially this, but so far I've had no luck. It's worth mentioning that I've also tried making the hash from as mentioned here, but also without luck.I keep getting 403 responses from the server. At first I thought It was due to my USER_AGENT settings (set to Mozzila 5....) or headers, but I've tested those (I even went ahead, analyzed X-Instagram-GIS of a request made from inside the browser and checked if the MD5 hash matches on my Scrapy request. 

The generated URL works fine inside a browser regardless if I'm logged into IG or not - however, it breaks when used inside an Incognito window, Scrapy or Scrapy shell.At first I thought it meant that scraping is just not possible, however, the rarcega scraper works just fine (except it's not really handy for scraping individual posts rather than whole user profiles).Any feedback or thoughts would be very much appreciated!

Scrapy. How to yield item after spider_close call?

Billy Jhon

[Scrapy. How to yield item after spider_close call?](https://stackoverflow.com/questions/51753307/scrapy-how-to-yield-item-after-spider-close-call)

I want to yield an item only when the crawling is finished.

I am trying to do it via But it does not yield anything, though the function is called.

How do I yield an item after the scraping is over?

2018-08-08 18:21:55Z

I want to yield an item only when the crawling is finished.

I am trying to do it via But it does not yield anything, though the function is called.

How do I yield an item after the scraping is over?Oof, I'm afraid spider_closed is used for tearing down. I suppose you can do it by attaching some custom stuff to Pipeline to post-process your items.Depending on what you want to do, there might be a veeeery hacky solution for this.  Instead of spider_closed you may want to consider using spider_idle signal which is fired before spider_closed. One difference between idle and close is that spider_idle allows execution of requests which then may contain a callback or errback to yield the desired item.Inside spider class:However this comes with several side effects so i discourage anyone from using this in production, for example the final request which will raise a DNSLookupError. I just want to show what is possible.

Scrapy-Splash doesn't set custom request headers

Belle-P

[Scrapy-Splash doesn't set custom request headers](https://stackoverflow.com/questions/51772199/scrapy-splash-doesnt-set-custom-request-headers)

I am trying to scrape a website using Scrapy + Splash in Python 2.7. 

The website uses JavaScript to generate most of the HTML, which is why I need Splash. First, I make a FormRequest with Scrapy to login to a website. It is successful.I then extract "access_token" from JSON response, because it should be used in the next request as an "Authorization" header - to confirm to the website that I am logged in. Before proceeding with SplashRequest, I decided to test the session with scrapy.Request. I passed cookies and the new headers:The HTML from result.body confirmed that I was logged in. Great!Calling response.request.headers showed that 'Authorization' header was also sent. Cookie DEBUG showed that all cookies were sent without issues.After that I substituted scrapy.Request with SplashRequest:lua_script:However, the HTML that I got from Splash response showed that I was not logged in.Cookie DEBUG didn't show any issues - the same cookies were sent as before. But here is what I got from calling response.request.headers:As you can see, Splash didn't set my custom headers, instead it just combined cookies with the default ones. I tried setting my own headers both as SplashRequest function arguments and inside lua_script, but none of the approaches worked.My question is, how to set my own request headers in Splash? 

2018-08-09 16:45:21Z

I am trying to scrape a website using Scrapy + Splash in Python 2.7. 

The website uses JavaScript to generate most of the HTML, which is why I need Splash. First, I make a FormRequest with Scrapy to login to a website. It is successful.I then extract "access_token" from JSON response, because it should be used in the next request as an "Authorization" header - to confirm to the website that I am logged in. Before proceeding with SplashRequest, I decided to test the session with scrapy.Request. I passed cookies and the new headers:The HTML from result.body confirmed that I was logged in. Great!Calling response.request.headers showed that 'Authorization' header was also sent. Cookie DEBUG showed that all cookies were sent without issues.After that I substituted scrapy.Request with SplashRequest:lua_script:However, the HTML that I got from Splash response showed that I was not logged in.Cookie DEBUG didn't show any issues - the same cookies were sent as before. But here is what I got from calling response.request.headers:As you can see, Splash didn't set my custom headers, instead it just combined cookies with the default ones. I tried setting my own headers both as SplashRequest function arguments and inside lua_script, but none of the approaches worked.My question is, how to set my own request headers in Splash? 

How to install Selenium on Anaconda and How to use selenium with Scrapy on Spyder IDE of Anaconda?

Aditya Mandal

[How to install Selenium on Anaconda and How to use selenium with Scrapy on Spyder IDE of Anaconda?](https://stackoverflow.com/questions/51722339/how-to-install-selenium-on-anaconda-and-how-to-use-selenium-with-scrapy-on-spyde)

I have searched a lot on google but couldn't find any helpful post and wondering like how to install selenium on Anaconda and then afterward How to use selenium along with Scrapy on Spyder IDE of anaconda.

2018-08-07 08:35:06Z

I have searched a lot on google but couldn't find any helpful post and wondering like how to install selenium on Anaconda and then afterward How to use selenium along with Scrapy on Spyder IDE of anaconda.That's an awefully broad question. Since it is not possible to write a whole tutorial here, I can give you the first steps:Installing selenium can be done by conda install -c conda-forge seleniumAfter that you can use it in any python script. It doesn't matter which IDE you use or if you use an interactive session. Make sure though that Spider is set to use the same python version that you installed selenium for.For the basic usage of Selenium and Scrapy, you should follow the basic tutorials here and here. If you have more specific questions, you should open a new question that shows your attempt at a script, what you expect it to do, the error/output you are getting instead etc.

TELNETCONSOLE_ENABLED setting is True but required twisted modules failed to import

Deep Shah

[TELNETCONSOLE_ENABLED setting is True but required twisted modules failed to import](https://stackoverflow.com/questions/51522281/telnetconsole-enabled-setting-is-true-but-required-twisted-modules-failed-to-imp)

I am getting below error. Can anyone help me solve the issue?

2018-07-25 15:09:18Z

I am getting below error. Can anyone help me solve the issue?you can disable the telnet extension on settings.py with:Or you can remove the Extension all together (also on settings.py):the None on the settings priority disables the extension.This is a known issue when using Scrapy with Python 3.7+. The fix has been  included in Scrapy 1.6.0.In the meantime, see the issue report thread for details and workarounds.

Scrapy throws “ModuleNotFoundError” upon execution

SIM

[Scrapy throws “ModuleNotFoundError” upon execution](https://stackoverflow.com/questions/51159487/scrapy-throws-modulenotfounderror-upon-execution)

I've written a very basic spider in scrapy to scrape the title of questions from the landing page of stackoverflow. I've been trying to make it work for the last few hours but I get the same error every time I execute my script. The full traceback is given below.items.py includes:infograbber.py aka spider contains:This is the Project Hierarchy:The error I'm having:FYI: when I try like below then it works but I do not wish to go like this.Where I'm going wrong? Thanks in advance for any solution. Btw, I'm using python 3.6 and scrapy 1.5.0. I've tried with cmd and sublime text editor to execute the file but in both cases I get the same error.

2018-07-03 16:45:58Z

I've written a very basic spider in scrapy to scrape the title of questions from the landing page of stackoverflow. I've been trying to make it work for the last few hours but I get the same error every time I execute my script. The full traceback is given below.items.py includes:infograbber.py aka spider contains:This is the Project Hierarchy:The error I'm having:FYI: when I try like below then it works but I do not wish to go like this.Where I'm going wrong? Thanks in advance for any solution. Btw, I'm using python 3.6 and scrapy 1.5.0. I've tried with cmd and sublime text editor to execute the file but in both cases I get the same error.i think your problem is that you execute the spider like so:but you need to go in your folder where is scrapy.cfg and run scrapy like this:

scrapy on Macbook error: Module 'tutorial' already exists

Bryan Xue

[scrapy on Macbook error: Module 'tutorial' already exists](https://stackoverflow.com/questions/50765651/scrapy-on-macbook-error-module-tutorial-already-exists)

After installation of scrapy,

I use 'scrapy startproject tutorial' to start but it shows below:When I just type import scrapy, it shows:

2018-06-08 17:21:15Z

After installation of scrapy,

I use 'scrapy startproject tutorial' to start but it shows below:When I just type import scrapy, it shows:This should be a relatively simple fix. It seems on some platforms that when installing scrapy a symlink does not get created or the cli tool does not get added to your $PATH. The first thing that needs to happen is finding the location of your current Python and version:Take the first two paths of the which python command and prepend it to:So you end up with:Now the final step is to create an alias for the command in ~/.bash_profile:scrapy_alias.shIn terminal run the script, then source the changes to ~/.bash_profile:Now you should be able to start the tutorial:

No module named _sqlite3 for scrapyd

Dhaval

[No module named _sqlite3 for scrapyd](https://stackoverflow.com/questions/50699601/no-module-named-sqlite3-for-scrapyd)

I'm new bie to Python who is stucked in the need of recompilation of Python.I have server with Centos where I have installed Python3.6 and then scrapyd. Everything installed successfully, however when I try to run scrapyd command, it shows me below error:After search a while, I got to know that sqlite3 should be installed even before Python3.6, however I have installed sqlite3 now but now found that I need recompile the Python.So, Above is the situation, now I have a question that, is that the case where I needed to install sqlite3 and recompile the Python? If yes then how I can recompile Python? If no then how can I get rid of this issue and can start the scrapyd?Help is really appreciable.

2018-06-05 12:00:58Z

I'm new bie to Python who is stucked in the need of recompilation of Python.I have server with Centos where I have installed Python3.6 and then scrapyd. Everything installed successfully, however when I try to run scrapyd command, it shows me below error:After search a while, I got to know that sqlite3 should be installed even before Python3.6, however I have installed sqlite3 now but now found that I need recompile the Python.So, Above is the situation, now I have a question that, is that the case where I needed to install sqlite3 and recompile the Python? If yes then how I can recompile Python? If no then how can I get rid of this issue and can start the scrapyd?Help is really appreciable.I believe, you'll need to re-install the python version as per this as actually the sqlite3 is the extension which should be install before Python gets installed.Please check this - 

Integrate Scrapy with Django : How to

Vira Xeva

[Integrate Scrapy with Django : How to](https://stackoverflow.com/questions/50637920/integrate-scrapy-with-django-how-to)

I'm still very new in Django

I am following this tutorial on how to integrate scrapy and django.the problem is when i am trying to use my own spider it's just wont work.

I have tried the spider outside django and it's work just fine, some help will be very helpful.This is my spider.py filethis is my pipeline file And this is the models file in django 

2018-06-01 07:09:34Z

I'm still very new in Django

I am following this tutorial on how to integrate scrapy and django.the problem is when i am trying to use my own spider it's just wont work.

I have tried the spider outside django and it's work just fine, some help will be very helpful.This is my spider.py filethis is my pipeline file And this is the models file in django It's example how to write Scrapy pileline in Django project.By the way you need to run Scrapy in Django environment. There are several ways to do that:With code inside:It allow to run Scrapy in Django environment like this:

How to sets up multiple environment configuration in a scrapy project

jeason.wang

[How to sets up multiple environment configuration in a scrapy project](https://stackoverflow.com/questions/50363235/how-to-sets-up-multiple-environment-configuration-in-a-scrapy-project)

I don't know how to set up multiple environment in my scrapy project, just like the 'profile' in spring-framework, because i want to deploy my project on both development and production environment

2018-05-16 05:46:16Z

I don't know how to set up multiple environment in my scrapy project, just like the 'profile' in spring-framework, because i want to deploy my project on both development and production environment

Can Xpath expressions access shadow-root elements?

Necronet

[Can Xpath expressions access shadow-root elements?](https://stackoverflow.com/questions/49763626/can-xpath-expressions-access-shadow-root-elements)

Currently I am scraping article news sites, in the process of getting its main content, I ran into the issue that a lot of them have embedded tweets in them like these:I use XPath expressions with XPath helper(chrome addon) in order to test if I can get content, then add this expression to scrapy python, but with elements that are inside a #shadow-root elements seem to be outside the scope of the DOM, I am looking for a way to get content inside these types of elements, preferably with XPath.

2018-04-10 22:14:23Z

Currently I am scraping article news sites, in the process of getting its main content, I ran into the issue that a lot of them have embedded tweets in them like these:I use XPath expressions with XPath helper(chrome addon) in order to test if I can get content, then add this expression to scrapy python, but with elements that are inside a #shadow-root elements seem to be outside the scope of the DOM, I am looking for a way to get content inside these types of elements, preferably with XPath.Most web scrapers, including Scrapy, don't support the Shadow DOM, so you will not be able to access elements in shadow trees at all.And even if a web scraper did support the Shadow DOM, XPath is not supported at all. Only selectors are supported to some extent, as documented in the CSS Scoping spec.One way to scrape pages containing shadow DOMs with tools that don't work with shadow DOM API is to recursively iterate over shadow DOM elements and replace them with their HTML code:If you are scraping using a full browser (Chrome with Puppeteer, PhantomJS, etc.) then just inject this script to the page. Important is to execute this after the whole page is rendered because it possibly breaks the JS code of shadow DOM components.Check full article I wrote on this topic: https://kb.apify.com/tips-and-tricks/how-to-scrape-pages-with-shadow-dom

How to process Scrapy output for NLP?

SY9

[How to process Scrapy output for NLP?](https://stackoverflow.com/questions/49690572/how-to-process-scrapy-output-for-nlp)

I'm trying to extract text data from companies' website using python Scrapy.The code below scrapes texts with no errors, but the output seems some further processes are required for NLP.Spider code:Item codeThe output json file:For the list of dicts output, I want delete blank content such as " ", or "\n" from the list, and then aggregate them as the one sentence in order for NLP.

So after all I want to make the dict, of which key is company's name and the value is the aggregated sentence like:How can I process the Scrapy output?

Any answers / advice will be greatly appreciated. Thanks in advance.

2018-04-06 10:19:23Z

I'm trying to extract text data from companies' website using python Scrapy.The code below scrapes texts with no errors, but the output seems some further processes are required for NLP.Spider code:Item codeThe output json file:For the list of dicts output, I want delete blank content such as " ", or "\n" from the list, and then aggregate them as the one sentence in order for NLP.

So after all I want to make the dict, of which key is company's name and the value is the aggregated sentence like:How can I process the Scrapy output?

Any answers / advice will be greatly appreciated. Thanks in advance.

Scrapy - how to check if spider is running

Milano

[Scrapy - how to check if spider is running](https://stackoverflow.com/questions/48917312/scrapy-how-to-check-if-spider-is-running)

I have a Scrapy spider which I run every hour using bash script and crontab.The running time of the spider is about 50 minutes but can be more than hour. What I want is to check whether the spider is running and only if not, start new crawling.BASH SCRIPTThe only thing which comes to my mind is to  use telnet.If it can connect - telnet localhost 6023, it means that spider is still running otherwise I can run spider.

2018-02-21 23:38:21Z

I have a Scrapy spider which I run every hour using bash script and crontab.The running time of the spider is about 50 minutes but can be more than hour. What I want is to check whether the spider is running and only if not, start new crawling.BASH SCRIPTThe only thing which comes to my mind is to  use telnet.If it can connect - telnet localhost 6023, it means that spider is still running otherwise I can run spider.You need some sort of locking mechanism. The best way to achieve an atomic lock from bash is to use mkdir and check the result code to know if you acquired the lock or not.Here's a more in depth explanation: http://wiki.bash-hackers.org/howto/mutexOf course you could always go for dirtier methods like a grep on process names or stuff like that.You could also have a lock in scrapy itself, add a simple middleware check for a shared resource... Plenty of ways to do it :)

store the filtered requests from scrapy dupefilter

wishmaster

[store the filtered requests from scrapy dupefilter](https://stackoverflow.com/questions/59431069/store-the-filtered-requests-from-scrapy-dupefilter)

I am getting this from the stats after scrapy crawl test     'dupefilter/filtered': 288, how can I store the filtered requests into a .txt file (or any type) so I view later on?

2019-12-20 20:35:52Z

I am getting this from the stats after scrapy crawl test     'dupefilter/filtered': 288, how can I store the filtered requests into a .txt file (or any type) so I view later on?To achieve this you need to make 2 things:One of possible way to do it - by setting custom_settings spider attribute:....You will have log lines like this:

2019-12-21 20:34:07 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET http://quotes.toscrape.com/page/4/> (referer: http://quotes.toscrape.com/page/3/)UPDATE 

To save only dupefilter logs:....Additional info:

scraping ASPX website not working - Crawled 31 pages (at 31 pages/min), scraped 0 items (at 0 items/min)

tomas11

[scraping ASPX website not working - Crawled 31 pages (at 31 pages/min), scraped 0 items (at 0 items/min)](https://stackoverflow.com/questions/59231185/scraping-aspx-website-not-working-crawled-31-pages-at-31-pages-min-scraped)

I am trying to scrape an ASP website that contains crime statistics in Israel.

In the website, a user is supposed to choose a town from a drop-down list:

following this selection, the user's result is returned to the bottom of that same page:

For some reason, scrapy seems to be crawling the website, but not returning anything:Code:Any tips would be appreciated and thanks in advance!!

2019-12-07 23:33:04Z

I am trying to scrape an ASP website that contains crime statistics in Israel.

In the website, a user is supposed to choose a town from a drop-down list:

following this selection, the user's result is returned to the bottom of that same page:

For some reason, scrapy seems to be crawling the website, but not returning anything:Code:Any tips would be appreciated and thanks in advance!!If you look for where the data is coming from by following requests it sends you will find out it is coming from something like this link: https://www.police.gov.il/MapSkifoutService?city=7900&street=&house=&subjects=1,2,10,11&quarter=1,2,3&year=2019

So by specifying year and city code you will get your desire data.

But how can you get city codes?

City codes are exist in script part of main page source so you can get it like this:Sample output would be:

Python multi layer web scraping [closed]

Sohel Ahmad

[Python multi layer web scraping [closed]](https://stackoverflow.com/questions/59010437/python-multi-layer-web-scraping)

I want to go through every URL on this list (https://express-press-release.net/Industries/Automotive-press-releases.php) and then copy data and get back to root list for the next one.

I can scrape from the single page but can't scrape through multiple links.

2019-11-23 17:41:54Z

I want to go through every URL on this list (https://express-press-release.net/Industries/Automotive-press-releases.php) and then copy data and get back to root list for the next one.

I can scrape from the single page but can't scrape through multiple links.YOu can find all the <a> tags with href and pull those into a list. Then just iterate over that list. You may need to add some extra filters of some sort as you may only want specific links, but this will get you going:

Scrapy - Response from crawling doesn't have the same encoding as its shell equivalent

taco

[Scrapy - Response from crawling doesn't have the same encoding as its shell equivalent](https://stackoverflow.com/questions/58894952/scrapy-response-from-crawling-doesnt-have-the-same-encoding-as-its-shell-equi)

I am having an encoding issue, when making the exact same request from my spider on the one side, and from the scrapy shell on the other side, the responses I get are not in the same encoding.I.e. when scraping using my spider:Whereas when using the scrapy shell:And this is highly problematic as the page is encoded in iso-8859-1, therefore I'm getting unicode replacement characters while scraping from my spider afterwards.

Any ideas?Thank you

2019-11-16 20:34:59Z

I am having an encoding issue, when making the exact same request from my spider on the one side, and from the scrapy shell on the other side, the responses I get are not in the same encoding.I.e. when scraping using my spider:Whereas when using the scrapy shell:And this is highly problematic as the page is encoded in iso-8859-1, therefore I'm getting unicode replacement characters while scraping from my spider afterwards.

Any ideas?Thank youRegardless of the reason why you are getting a different response header in different scenarios, if the response consistenly uses an encoding (ISO-8859-1) that not always matches the Content-Type response header, read the response body as bytes from response.body and decode it with .decode('iso-8859-1').

Scrapy, custome method not called

zdd

[Scrapy, custome method not called](https://stackoverflow.com/questions/58776085/scrapy-custome-method-not-called)

I met a problem when I parse a web page by scrapy, my custome method was not called by scrapy. the url is: http://www.duilian360.com/chunjie/117.html, and the code is:On above code, method parse_paragraph was not called, since the print clause has no output, I can't step into this method even when i set a breakpoint on the print line.But if I move all code in method parse_paragraph into the calling method parse_page as below, then everything works well, why?My code has lots of custome method, and I don't want the move all code in them to the calling method. this is not a good practice.

2019-11-09 03:15:23Z

I met a problem when I parse a web page by scrapy, my custome method was not called by scrapy. the url is: http://www.duilian360.com/chunjie/117.html, and the code is:On above code, method parse_paragraph was not called, since the print clause has no output, I can't step into this method even when i set a breakpoint on the print line.But if I move all code in method parse_paragraph into the calling method parse_page as below, then everything works well, why?My code has lots of custome method, and I don't want the move all code in them to the calling method. this is not a good practice.I would use yield from instead direct calling parse_paragraph since that returns a generator rather than yielding items/requests from another parser.

Scrapy crawl shows output in terminal but not in json excel file

Kamoo

[Scrapy crawl shows output in terminal but not in json excel file](https://stackoverflow.com/questions/58724460/scrapy-crawl-shows-output-in-terminal-but-not-in-json-excel-file)

I am able to show the result of scrape in the terminal, but when i insert -o .csv, the 3rd line of coding will have output in json excel file but not the first and second line ( trying to scrape the start date and end date as shown in picture below)Coding:Empty in the excel json fileThere are output for Max ERC and MAX ERC1 in terminal:The html code:

2019-11-06 06:44:33Z

I am able to show the result of scrape in the terminal, but when i insert -o .csv, the 3rd line of coding will have output in json excel file but not the first and second line ( trying to scrape the start date and end date as shown in picture below)Coding:Empty in the excel json fileThere are output for Max ERC and MAX ERC1 in terminal:The html code:I'm not sure, but I guess it's the list messing with the csv, generally I want to avoid that. Can you try if this parser fix the issue?

Scrapy login works for certain website but does not work for other websites

abab

[Scrapy login works for certain website but does not work for other websites](https://stackoverflow.com/questions/57929715/scrapy-login-works-for-certain-website-but-does-not-work-for-other-websites)

I can use below code to login to Github. But when I try the same code to other websites, it still remains at the login page, it does not login. Did I miss anything? 

2019-09-13 20:01:58Z

I can use below code to login to Github. But when I try the same code to other websites, it still remains at the login page, it does not login. Did I miss anything? Looking at the source of the website you provided, you can find the form field here:This, in particular the onsubmit="return false" part, tells you that this form will never be submitted by the browser through the usual method to the 'href' target and (unless you tested the form without javascript enabled and it works) that this site will probably only work in javascript-enabled browsers.To then find out how the form is actually submitted, through XHR, you'd need to find and take apart the submitLogin() function here in the site's javascript code and emulate it in your code.Another option is to use a javascript engine with scrapy, which can handle the scripts for you, but has the disadvantage of being more ressource intensive and possibly hard to set up.

Scrapy - “scrapy crawl” catches exceptions internally and hides them from Jenkins' “catch” clause

displayname

[Scrapy - “scrapy crawl” catches exceptions internally and hides them from Jenkins' “catch” clause](https://stackoverflow.com/questions/57608702/scrapy-scrapy-crawl-catches-exceptions-internally-and-hides-them-from-jenkin)

I'm running scrapy through Jenkins on a daily basis, and I want exceptions to be sent to me in emails.This is an example spider:This is the .Jenkinsfile:And this is the log:And no mail is sent.

I believe Scrapy catches the exception internally, saves it to log later and then quits with no error.How can I make Jenkins get the exception?

2019-08-22 11:49:40Z

I'm running scrapy through Jenkins on a daily basis, and I want exceptions to be sent to me in emails.This is an example spider:This is the .Jenkinsfile:And this is the log:And no mail is sent.

I believe Scrapy catches the exception internally, saves it to log later and then quits with no error.How can I make Jenkins get the exception?The problem is that scrapy does not use a non-zero exit code when a scrap fails (src: https://github.com/scrapy/scrapy/issues/1231).As said by the commenters in that issue, I suggest you to add a custom command (http://doc.scrapy.org/en/master/topics/commands.html#custom-project-commands).

How to get rid of duplicate links while crawling a website using python scrapy?

Rinku Yadav

[How to get rid of duplicate links while crawling a website using python scrapy?](https://stackoverflow.com/questions/57608722/how-to-get-rid-of-duplicate-links-while-crawling-a-website-using-python-scrapy)

I have the following code which crawls the given website address but the problem is that it duplicates the URL while crawling. I need unique and complete list of URL which can be reached from home page of the website.Please help me in doing that.Just run this code as it is.

output of the above codeOutput of running the code:

2019-08-22 11:50:28Z

I have the following code which crawls the given website address but the problem is that it duplicates the URL while crawling. I need unique and complete list of URL which can be reached from home page of the website.Please help me in doing that.Just run this code as it is.

output of the above codeOutput of running the code:scrapy should automatically avoid revisiting previously visited urls (using the dupefilter class). It's not entirely clear to me what you want to do here but I think you want to crawl the website and find all links? In that case you should move your second yield (yield {'url': response.url}) to earlier in your parse function. I think the following gives you what you want:if I run this as:then the resulting json file does not contain any duplicate links.I don't know scrapy at all, but can't you use a list (or a set, that's easier)and check if there's already a record of the same link ? Edit : you seem to use a set already, that you change for a list just after : This will work because Scrapy will deal with duplicate URLs for you:

Issue with scraping href in Python using Scrapy Spider

Sam Edeus

[Issue with scraping href in Python using Scrapy Spider](https://stackoverflow.com/questions/57583159/issue-with-scraping-href-in-python-using-scrapy-spider)

I am currently trying to scrape the href from the title on a craiglist page. I am using python scrapy, and have been having trouble with it I have tried several things, I don't understand what is wrong. There arent any error messages that show up, I just get zero results.

2019-08-21 00:56:51Z

I am currently trying to scrape the href from the title on a craiglist page. I am using python scrapy, and have been having trouble with it I have tried several things, I don't understand what is wrong. There arent any error messages that show up, I just get zero results.I fixed your code a bit to dump the hrefs (removed Selector and replaced extract_first with extract):Output:Update - dumping results to json-file:Corresponding docs are here.

Accessing the body in response of callback

MUHAMMAD AHMAD L1F14BSCS0435

[Accessing the body in response of callback](https://stackoverflow.com/questions/57358389/accessing-the-body-in-response-of-callback)

I have a scrapy request with a body. I want to access this body in my callback method through response

2019-08-05 12:13:59Z

I have a scrapy request with a body. I want to access this body in my callback method through responseThe scrapy.http.Response object has request attribute.

How to capture multiple responses on one single POST request using Scrapy?

Tony Montana

[How to capture multiple responses on one single POST request using Scrapy?](https://stackoverflow.com/questions/57236421/how-to-capture-multiple-responses-on-one-single-post-request-using-scrapy)

I am trying to web scrape this website and download the pdf files available when you complete the whole lifecycle of this website. I am using Scrapy for this. I am having some trouble with capturing the captcha at the right time.This site is an ASPX webpage and uses 'Viewstates' to keep track of each POST requests. Now, if you go through this site, you'll understand that whenever you fill any dropdown fields, it sends POST request with 'Viewstate' value to a certain URL path, which you can see in the browser console. But at the same time, it sends another GET request to another URL to fetch the "CAPTCHA" image. But I am not able to get this response. I don't have any idea whether using Scrapy can we capture multiple requests multiple responses at the same time.Now, I tried to find a workaround for this issue. And I have followed almost everything mentioned in this StackOverflow post, but in response I am getting HTML code with javascript alert code mentioning "Wrong text inserted, Please enter new characters shown in image textbox". So, this solution is also not working for me. This is my scrapy spider code:If you go through the above-mentioned site and fill all the form details, monitor the browser consols network tab, you'll get an idea about this problem.Kindly guide me in how to solve this issue. Thank you.

2019-07-27 22:06:37Z

I am trying to web scrape this website and download the pdf files available when you complete the whole lifecycle of this website. I am using Scrapy for this. I am having some trouble with capturing the captcha at the right time.This site is an ASPX webpage and uses 'Viewstates' to keep track of each POST requests. Now, if you go through this site, you'll understand that whenever you fill any dropdown fields, it sends POST request with 'Viewstate' value to a certain URL path, which you can see in the browser console. But at the same time, it sends another GET request to another URL to fetch the "CAPTCHA" image. But I am not able to get this response. I don't have any idea whether using Scrapy can we capture multiple requests multiple responses at the same time.Now, I tried to find a workaround for this issue. And I have followed almost everything mentioned in this StackOverflow post, but in response I am getting HTML code with javascript alert code mentioning "Wrong text inserted, Please enter new characters shown in image textbox". So, this solution is also not working for me. This is my scrapy spider code:If you go through the above-mentioned site and fill all the form details, monitor the browser consols network tab, you'll get an idea about this problem.Kindly guide me in how to solve this issue. Thank you.That is the reason I hate ASP.NET applications, it just make you go nuts while scraping. Anyways, you had everything almost perfect, except one thingThis comes from a response where you set the part, but what you do is copy the __VIEWSTATE and __EVENTVALIDATION previous to setting the part. So you need to make sure you capture the correct statesNot an answer (yet), but a few pointers: Besides this your approach looks sound and has no obvious problems. BTW: In the end it might turn out that emulating the full sequence of requests is unnecessary, it might be ok to skip to the last two requests for getting the final captcha and sending the final form submission ... but that won't help us here, just for later refactoring and making the code simpler.

how to call output filename from scrapy

wishmaster

[how to call output filename from scrapy](https://stackoverflow.com/questions/57047023/how-to-call-output-filename-from-scrapy)

How can I call the Output filename from code i.e I would like to use the filename inputed in terminal in spider_closed function 

2019-07-15 20:50:09Z

How can I call the Output filename from code i.e I would like to use the filename inputed in terminal in spider_closed function You can use self.settings.attributes["FEED_URI"].value in your spider to get output file name.

how to know what links were extracted by scrapy rule

programmerwiz32

[how to know what links were extracted by scrapy rule](https://stackoverflow.com/questions/56959097/how-to-know-what-links-were-extracted-by-scrapy-rule)

I am trying to use Rule and LinkExtractor to extract links, this is my code in scrapy shellI tried to use dir(x) to see what methods I can apply on it best of I can find is x.__sizeof__() but this shows 32 instead of actual 10 links.

My question is how can I find out what links are actually extracted using them (a list like).

this is what dir(x) shows['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_csstranslator', '_extract_links', '_link_allowed', '_process_links', 'allow_domains', 'allow_res', 'canonicalize', 'deny_domains', 'deny_extensions', 'deny_res', 'extract_links', 'link_extractor', 'matches', 'restrict_xpaths']

2019-07-09 19:00:55Z

I am trying to use Rule and LinkExtractor to extract links, this is my code in scrapy shellI tried to use dir(x) to see what methods I can apply on it best of I can find is x.__sizeof__() but this shows 32 instead of actual 10 links.

My question is how can I find out what links are actually extracted using them (a list like).

this is what dir(x) shows['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_csstranslator', '_extract_links', '_link_allowed', '_process_links', 'allow_domains', 'allow_res', 'canonicalize', 'deny_domains', 'deny_extensions', 'deny_res', 'extract_links', 'link_extractor', 'matches', 'restrict_xpaths']You can use the following method to get exactly what is extractedfor the actual urls you can use 

Can't parse a certain information from some html elements using xpath

MITHU

[Can't parse a certain information from some html elements using xpath](https://stackoverflow.com/questions/56871695/cant-parse-a-certain-information-from-some-html-elements-using-xpath)

I've created an xpath expression to target an element so that I can extract a certain information out of some html elements using xpath within scrapy. I can't reach it anyway.Html elements:I wish to extract R 3500 out of it.I've tried with:Upon running my above script this is what I'm getting <br class="hidden-md hidden-lg"> whereas I wish to get R 3500.I could have used .tail if opted for lxml. However, when I go for scrapy I don't find anything similar. How can I extract that rate out of the html elements using xpath?

2019-07-03 13:51:31Z

I've created an xpath expression to target an element so that I can extract a certain information out of some html elements using xpath within scrapy. I can't reach it anyway.Html elements:I wish to extract R 3500 out of it.I've tried with:Upon running my above script this is what I'm getting <br class="hidden-md hidden-lg"> whereas I wish to get R 3500.I could have used .tail if opted for lxml. However, when I go for scrapy I don't find anything similar. How can I extract that rate out of the html elements using xpath?To get a text node as a following-sibling after the label node:The output:Addition: "//*[@class='rates']/label/following::text()" should also work.https://www.w3.org/TR/1999/REC-xpath-19991116#axesTo complement the accepted answer, which is entirely correct, here is an explanation whygiven the documentdoes not return the text R 3500: * only selects element nodes that follow after label elements, but not text nodes. Elements and text nodes are different concepts in the XPath document model. You can test this claim with a slightly different document:Which causes your code to return the any element.Both text() (more specific) and node() (more general) select this text node, and in this case both the following:: and following-sibling:: axes work.

Scrapy - How to define structure of the csv export (columns,etc..)

whands

[Scrapy - How to define structure of the csv export (columns,etc..)](https://stackoverflow.com/questions/56233207/scrapy-how-to-define-structure-of-the-csv-export-columns-etc)

I am quite new with scrapy, and i'm to figure out how to set the structure of a csv export. I have in the following example 2 kind of data scraped: ids and linksHere is the simple code i'm using :But my export is like this :instead, i would like to export with for each id corresponding the href and separated in rows :

2019-05-21 07:26:53Z

I am quite new with scrapy, and i'm to figure out how to set the structure of a csv export. I have in the following example 2 kind of data scraped: ids and linksHere is the simple code i'm using :But my export is like this :instead, i would like to export with for each id corresponding the href and separated in rows :Make yielding items in this way:Here you zip your arrays to ((link, dataid), (link, dataid), (link, dataid), ...) and then yielding them one by one. So it should give you desired output.

White space and selectors

Elsior Moreira Alves Junior

[White space and selectors](https://stackoverflow.com/questions/56173432/white-space-and-selectors)

Try to use a selector on scrapy shell to extract information from a web page and didn't work proprely. I believe that it happened because exist white space into class name. Any idea what's going wrong? I tried different syntaxes like:expected result: Apartamento para arrendar: Olivais, Lisboaactual result: []

2019-05-16 17:05:52Z

Try to use a selector on scrapy shell to extract information from a web page and didn't work proprely. I believe that it happened because exist white space into class name. Any idea what's going wrong? I tried different syntaxes like:expected result: Apartamento para arrendar: Olivais, Lisboaactual result: []For this cases I prefer using css selectors because of its minimalistic syntax:

response.css("p.text-nowrap.hidden-xs::text")Also google chrome developer tools displays css selectors when you observing html code This makes scraper development much easier

The whitespace in the class section means that there are multiple classes, the "text-nnowrap" class and the "hidden-xs" class. In order to select by xpath for multiple classes, you can use the following format: "//element[contains(@class, 'class1') and contains(@class, 'class2')]"(grabbed this from How to get html elements with multiple css classes)So in your example, I believe this would work.

How to login using scrapy-spider

Kanika Sahni

[How to login using scrapy-spider](https://stackoverflow.com/questions/56038127/how-to-login-using-scrapy-spider)

I am trying to crawl a website using scrapy. The website contains login forum and I am unable to login through scrapy. I want to login and extract the data from that page and for testing purpose I tried to extract the title as well as link info but from the result it seems the page isn't logged in. Output of above code is(a csv file):Log:

2019-05-08 10:02:04Z

I am trying to crawl a website using scrapy. The website contains login forum and I am unable to login through scrapy. I want to login and extract the data from that page and for testing purpose I tried to extract the title as well as link info but from the result it seems the page isn't logged in. Output of above code is(a csv file):Log:Formdata should have name of the element. Please change

username => user_name 

and 

password => user_pass 

and 

use url alone. Not url=url

as follows

ORConfirm this : 

I took html content of your login-page and searched for first <a href which is this => <a href="/"> 

So confirm that, the home page (after login) html also has the first <a href is <a href="/"> or not. Mostly it would be ! Which means login is working perfectly

If so, for testing, just change this code "link" : item[1] to something which is unique in home page (I mean which is not in login page).

Is there a way to scrape a non scrap-able website other than using BeautifulSoup and Scrapy as these aren't working?

Shay

[Is there a way to scrape a non scrap-able website other than using BeautifulSoup and Scrapy as these aren't working?](https://stackoverflow.com/questions/55926718/is-there-a-way-to-scrape-a-non-scrap-able-website-other-than-using-beautifulsoup)

I'm trying to scrape the nbn plans from Tangerine website as a scraping practice. I'm using BeautifulSoup and I'm able to scrape the data and see the scraped data in the terminal but once I save the data into a csv file, it doesn't work and I get some kind of weird typing.I used BeautifulSoup but I also know how to use scrapy and used it before. I just want to know if it's possible to scrape the data and save it into a csv file using scrapy before I try and if it's not, what else can I use?



There's also some sites that I tried to scrape using scrapy but it wasn't working. I know that there's nothing wrong with my code because I tried scraping other sites and it worked.The expected result would be this data in a csv file:But I get some kind of a weird typing in the csv file instead:



2019-04-30 18:21:10Z

I'm trying to scrape the nbn plans from Tangerine website as a scraping practice. I'm using BeautifulSoup and I'm able to scrape the data and see the scraped data in the terminal but once I save the data into a csv file, it doesn't work and I get some kind of weird typing.I used BeautifulSoup but I also know how to use scrapy and used it before. I just want to know if it's possible to scrape the data and save it into a csv file using scrapy before I try and if it's not, what else can I use?



There's also some sites that I tried to scrape using scrapy but it wasn't working. I know that there's nothing wrong with my code because I tried scraping other sites and it worked.The expected result would be this data in a csv file:But I get some kind of a weird typing in the csv file instead:

Your issue is not the code but the encoding of your libra file. Use these steps to change the encoding from UTF-16 to UTF-8:File > New > Spreadsheet, then Insert > Sheet from file. Choose your file and OK. You should get the text import window. At the top, check the "Character set" setting -- my guess is that it's not set properly. If it's not already, change it to UTF-8.

Processing images without downloading using Scrapy Spiders

G.Dantas

[Processing images without downloading using Scrapy Spiders](https://stackoverflow.com/questions/55891234/processing-images-without-downloading-using-scrapy-spiders)

I'm trying to use a Scrapy Spider to solve a problem (a programming question from HackThisSite):(1) I have to log in a website, giving a username and a password (already done)(2) After that, I have to access an image with a given URL (the image is only accessible to logged in users)(3) Then, without saving the image in the hard disk, I have to read its information in a kind of buffer(4) And the result of the function will fill a form and send the data to the website server (I already know how to do this step)So, I can resume to question to: would it be possible (using a spider) to read an image accessible only by logged-in users and process it in the spider code?I tried to research different methods, using item pipelines is not a good approach (I don't want to download the file).The code that I already have is:I expect to solve this problem using Scrapy functions, otherwise it would be necessary to log in the website (sending the form data) again.

2019-04-28 14:26:11Z

I'm trying to use a Scrapy Spider to solve a problem (a programming question from HackThisSite):(1) I have to log in a website, giving a username and a password (already done)(2) After that, I have to access an image with a given URL (the image is only accessible to logged in users)(3) Then, without saving the image in the hard disk, I have to read its information in a kind of buffer(4) And the result of the function will fill a form and send the data to the website server (I already know how to do this step)So, I can resume to question to: would it be possible (using a spider) to read an image accessible only by logged-in users and process it in the spider code?I tried to research different methods, using item pipelines is not a good approach (I don't want to download the file).The code that I already have is:I expect to solve this problem using Scrapy functions, otherwise it would be necessary to log in the website (sending the form data) again.You can make a scrapy request to crawl the image and then callback to some other endpoint:

How to fix the order problem when using scrapy?

Chi

[How to fix the order problem when using scrapy?](https://stackoverflow.com/questions/55448154/how-to-fix-the-order-problem-when-using-scrapy)

I believe this is a simple one, and I am willing to learn more. The thing is that I want to crawl the website titles via URL. The purpose of this is predicting the online news popularity and the data is from the UCI Machine Learning Repository. Here's the link.I follow the tutorial of Scrapy and change the code in "quotes spider" as following. After I run "scrapy crawl quotes" in the terminal, I used "scrapy crawl quotes -o quotes.json" to save all the title in JSON.There are 158 missing. I have 39,486 URL but 39,644 Website Titles. In addition, the order of each website does not fit each URL. For example, The final Title corresponds to the third last URL. Could you please help me identify the problems?Here's the ResultI tried to use "Beautiful soup" in Jupyter Notebook, but it was slow and cannot tell if the code is still running or not. 

2019-04-01 04:34:24Z

I believe this is a simple one, and I am willing to learn more. The thing is that I want to crawl the website titles via URL. The purpose of this is predicting the online news popularity and the data is from the UCI Machine Learning Repository. Here's the link.I follow the tutorial of Scrapy and change the code in "quotes spider" as following. After I run "scrapy crawl quotes" in the terminal, I used "scrapy crawl quotes -o quotes.json" to save all the title in JSON.There are 158 missing. I have 39,486 URL but 39,644 Website Titles. In addition, the order of each website does not fit each URL. For example, The final Title corresponds to the third last URL. Could you please help me identify the problems?Here's the ResultI tried to use "Beautiful soup" in Jupyter Notebook, but it was slow and cannot tell if the code is still running or not. If your aim is only to keep the correspondence between URL and title, you can add the URL to your scraped item:On the contrary, if you want to process URLs in order, there are various ways, a bit more complex.

The most common idea is to write a method start_request, where you request only the first URL; then, in the method parse, you request the second URL, setting the same method (parse) as callback; and so on...See Sequential scraping from multiple start_urls leading to error in parsing and Scrapy Crawl URLs in Order 

How to solve this ModuleNotFoundError: No module named '_sqlite3' in docker-debian

Roman

[How to solve this ModuleNotFoundError: No module named '_sqlite3' in docker-debian](https://stackoverflow.com/questions/55419396/how-to-solve-this-modulenotfounderror-no-module-named-sqlite3-in-docker-debi)

While trying to run scrapy spider in docker-debian every time I get an error:File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 82, in 

 crawl

     yield self.engine.open_spider(self.spider, start_requests)

 builtins.ModuleNotFoundError: No module named '_sqlite3'python version 3.6.3scrapy 1.6.0tried instruction from this post - ImportError: No module named '_sqlite3' in python3.3

still see this error.when type sqlite3 command in terminal it shows sqlite3 version and start sqlite terminal.

SQLite version 3.16.2 2017-01-06when type python and write next command in terminal:get this error:ModuleNotFoundError: No module named '_sqlite3'is there any way to solve this problem? looks like there is some misconfiguration but I can't find where it's

2019-03-29 14:16:52Z

While trying to run scrapy spider in docker-debian every time I get an error:File "/usr/local/lib/python3.6/site-packages/scrapy/crawler.py", line 82, in 

 crawl

     yield self.engine.open_spider(self.spider, start_requests)

 builtins.ModuleNotFoundError: No module named '_sqlite3'python version 3.6.3scrapy 1.6.0tried instruction from this post - ImportError: No module named '_sqlite3' in python3.3

still see this error.when type sqlite3 command in terminal it shows sqlite3 version and start sqlite terminal.

SQLite version 3.16.2 2017-01-06when type python and write next command in terminal:get this error:ModuleNotFoundError: No module named '_sqlite3'is there any way to solve this problem? looks like there is some misconfiguration but I can't find where it's

Scheduled Jobs with Custom Clock Processes in Python with APScheduler is not running

Nadeem Khan

[Scheduled Jobs with Custom Clock Processes in Python with APScheduler is not running](https://stackoverflow.com/questions/55165254/scheduled-jobs-with-custom-clock-processes-in-python-with-apscheduler-is-not-run)

I push my scrapy project on heroku. I need to automate some task. So from documentation I come to know about the custom clock with Apsheduler.

This is my directory structure:This is my requirement file And here is my clock.py fileI am not getting any message(This job is run every three minutes) to print in my heroku log. I have commit all the changes and deploy the application. Kindly help to run my script so I move further to automate the task.This is the current state of log:

2019-03-14 14:37:14Z

I push my scrapy project on heroku. I need to automate some task. So from documentation I come to know about the custom clock with Apsheduler.

This is my directory structure:This is my requirement file And here is my clock.py fileI am not getting any message(This job is run every three minutes) to print in my heroku log. I have commit all the changes and deploy the application. Kindly help to run my script so I move further to automate the task.This is the current state of log:This looks like there is a typo in your Procfile, as the command should read 'python clock.py' (note the missing colon). The proper line for your Procfile is:

clock: python clock.pyAlso, scale up the clock process if you haven't done so already. In the Heroku CLI use the command heroku ps:scale clock=1.See the Heroku page for APScheduler for more details.

Solving reCAPTCHA with scrapy

Nurullah Macun

[Solving reCAPTCHA with scrapy](https://stackoverflow.com/questions/55018702/solving-recaptcha-with-scrapy)

I am using scrapy to crawl some webpages. But at some point, Google reCAPTCHA blocks the way. Google reCAPTCHA even doesn't load if the browser(scrapy in this case) doesn't have a running javascript. It simply asks you to enable your javascript to view and solve the reCAPTCHA. So, I think that, if I can find a way to show this reCAPTCHA to user when it occurs, user can manually solve this and scrapy continues to crawl but I couldn't a way to interrupt this process with a real javascript. What can I use at this point ? Is it possible to mix selenium with scrapy ? 

2019-03-06 08:35:01Z

I am using scrapy to crawl some webpages. But at some point, Google reCAPTCHA blocks the way. Google reCAPTCHA even doesn't load if the browser(scrapy in this case) doesn't have a running javascript. It simply asks you to enable your javascript to view and solve the reCAPTCHA. So, I think that, if I can find a way to show this reCAPTCHA to user when it occurs, user can manually solve this and scrapy continues to crawl but I couldn't a way to interrupt this process with a real javascript. What can I use at this point ? Is it possible to mix selenium with scrapy ? Sounds like you want to build something semi-automatic. Scrapy is not good for that, and as you say, it cannot handle javascript.I would recommend trying selenium. It launches a full chrome browser and is scriptable. See https://selenium-python.readthedocs.io/You can stop the script and some event (e.g. reCAPTCHA) and then let the user take over.

How does Scrapy proceed with the urls given in the urls variable under start_requests?

Five9

[How does Scrapy proceed with the urls given in the urls variable under start_requests?](https://stackoverflow.com/questions/54848316/how-does-scrapy-proceed-with-the-urls-given-in-the-urls-variable-under-start-req)

Just wondering why when I have url = ['site1', 'site2'] and I run scrapy from script using .crawl() twice, in a row likethe output is:as opposed to 

2019-02-24 03:03:44Z

Just wondering why when I have url = ['site1', 'site2'] and I run scrapy from script using .crawl() twice, in a row likethe output is:as opposed to Start Request uses the yield functionality. yield queues the requests. To understand it fully read this StackOverflow answer.Here is the code example of how it works with start_urls in the start_request method.For custom request ordering this priority feature can be used.the one with the higher number of priority will be yielded first from the queue. By default, priority is 0.Because as soon as you call process.start(), requests are handled asynchronously. The order is not guaranteed.In fact, even if you only call process.crawl() once, you may sometimes get:To run spiders sequentially from Python, see this other answer.

Scrapy splash spider not following links to fetch new pages

Homunculus Reticulli

[Scrapy splash spider not following links to fetch new pages](https://stackoverflow.com/questions/54867680/scrapy-splash-spider-not-following-links-to-fetch-new-pages)

I am fetching data from a page that uses Javascript to link to new pages. I am using Scrapy + splash to fetch this data, however, for some reason, the links are not being followed.Here is the code for my spider:Only the first page is fetched, and I'm unable to get the subsequent pages by 'clicking' through the links at the bottom of the page.How do I fix this so I can click through the pages given at the bottom of the page?

2019-02-25 13:48:16Z

I am fetching data from a page that uses Javascript to link to new pages. I am using Scrapy + splash to fetch this data, however, for some reason, the links are not being followed.Here is the code for my spider:Only the first page is fetched, and I'm unable to get the subsequent pages by 'clicking' through the links at the bottom of the page.How do I fix this so I can click through the pages given at the bottom of the page?Your code looks fine, the only thing is that since the yielded requests have the same url, they are ignored by the duplicate filter. Just uncomment the DUPEFILTER_CLASS and try again.EDIT: to browse data pages without running javascript, you can do like this:I'm looking forward to a solution using javascript though.You can use the page query string variable. It starts at 0 so the first page is page=0. You can check the total pages by looking at:That way you know to call pages 0-156.

Python/Scrapy: Custom pipeline has no effect / download files with custom filename

R0byn

[Python/Scrapy: Custom pipeline has no effect / download files with custom filename](https://stackoverflow.com/questions/54803908/python-scrapy-custom-pipeline-has-no-effect-download-files-with-custom-filena)

This is a follow-up question to my initial question.

I want to download PDFs and save them on harddisk with custom filename.For the custom filename I tried this code in my pipelines.py according to this recommendation:In my settings.py I have:But the files keep being saved only with their SHA1-hash, for example: a8569143c987cdd43dd1f6d9a6f98b7aa6fbc284.PDF. So my custom file_path function seems not to be used by Scrapy.When I comment out the linenothing will be downloaded.I am confused...

2019-02-21 09:44:46Z

This is a follow-up question to my initial question.

I want to download PDFs and save them on harddisk with custom filename.For the custom filename I tried this code in my pipelines.py according to this recommendation:In my settings.py I have:But the files keep being saved only with their SHA1-hash, for example: a8569143c987cdd43dd1f6d9a6f98b7aa6fbc284.PDF. So my custom file_path function seems not to be used by Scrapy.When I comment out the linenothing will be downloaded.I am confused...Your problem is your custom pipeline is not a real file pipeline, therefore it does nothing. You need to subclass the original FilesPipeline and then use only PrangerPipeline in the settings.  For example:pipelines.py:settings.py:Refer to my examples using ImagesPipeline here: Unable to rename downloaded images through pipelines without the usage of item.pyTrouble renaming downloaded images in a customized manner through pipelines

Scrapy spider finishing scraping process without scraping anything

Manuel

[Scrapy spider finishing scraping process without scraping anything](https://stackoverflow.com/questions/54471844/scrapy-spider-finishing-scraping-process-without-scraping-anything)

I have this spider that scrapes amazon for information.The spider reads a .txt file in which I write which product it must search and then enters amazon page for that product, for example : https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=laptopI use the keyword=laptop for changing which product to search and such.The issue that I'm having is that the spider just does not work, which is weird because a week ago it did her job just fine.Also, no errors appear on the console, the spider starts, "crawls" the keyword and then just stops.Here is the full spiderOther spiders with a similar structure I made also work, any idea what's going on?Here's what I get in the console

2019-02-01 01:58:45Z

I have this spider that scrapes amazon for information.The spider reads a .txt file in which I write which product it must search and then enters amazon page for that product, for example : https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=laptopI use the keyword=laptop for changing which product to search and such.The issue that I'm having is that the spider just does not work, which is weird because a week ago it did her job just fine.Also, no errors appear on the console, the spider starts, "crawls" the keyword and then just stops.Here is the full spiderOther spiders with a similar structure I made also work, any idea what's going on?Here's what I get in the consoleInteresting! It is possibly due to website wasn't returning any data. Have you tried to debug with scrapy shell. If not, try to check with that is response.body returning intended data which you want to crawl.For more details, please read detailed info on scrapy shellAfter debugging, If you still not getting intended data that means there is more into the site which obstructing to crawling process. That could be dynamic script or cookie/local-storage/session dependency.For dynamic/JS script, you can use selenium or splash.

selenium-with-scrapy-for-dynamic-page

handling-javascript-in-scrapy-with-splash

For cookie/local-storage/session, you have to look deeper into inspect window and find out which is essential for getting the data.

Scrapy Extract method yields a Cannot mix str and non-str arguments error

Josiah Hulsey

[Scrapy Extract method yields a Cannot mix str and non-str arguments error](https://stackoverflow.com/questions/54209812/scrapy-extract-method-yields-a-cannot-mix-str-and-non-str-arguments-error)

I am in the middle of learning scrappy right now and am building a simple scraper of a real estate site. With this code I am trying to scrape all of the URLs for the real estate listing of a specific city. I have run into the following error with my code -   "Cannot mix str and non-str arguments". I believe I have isolated my problem to following part of my code If I use the extract_first() function instead of the extract function in the props xpath assignment, the code kind of works. It grabs the first link for the property on each page. However, this ultimately is not what I want. I believe I have the xpath call correct as the code runs if I use the extract_first() method.Can someone explain what I am doing wrong here? I have listed my full code belowPlease let me know if I can clarify anything.

2019-01-16 02:59:53Z

I am in the middle of learning scrappy right now and am building a simple scraper of a real estate site. With this code I am trying to scrape all of the URLs for the real estate listing of a specific city. I have run into the following error with my code -   "Cannot mix str and non-str arguments". I believe I have isolated my problem to following part of my code If I use the extract_first() function instead of the extract function in the props xpath assignment, the code kind of works. It grabs the first link for the property on each page. However, this ultimately is not what I want. I believe I have the xpath call correct as the code runs if I use the extract_first() method.Can someone explain what I am doing wrong here? I have listed my full code belowPlease let me know if I can clarify anything.You are passing props list of strings to response.urljoin() but meant prop instead:Alecxe's is right, it was a simple oversight in the spelling of iterator in your loop. You can use the following notation:It's cleaner and you're not instantiating the "absolute_url" per loop. On a larger scale, would help you save some memory.

Trying to extract value via xpath of variable, not getting correct result

joe

[Trying to extract value via xpath of variable, not getting correct result](https://stackoverflow.com/questions/54089749/trying-to-extract-value-via-xpath-of-variable-not-getting-correct-result)

I am trying to get this value via xpath.  i am trying to grab the actual integer value (165)this is the xpath I am getting via debug toolsI am trying to gather information on this property listing and i am trying to capture the area of an apartment.  I have played around with the xpath that i get from the debugger but i always get either an empty string or an error that says u cant turn a xpath selector into a string.  Not sure where to go with this to get the 165 value. Please, any input u can provide, i have looked at this for several days.via this xpath i am expecting the 165, but i get nothing.  i just get this []

2019-01-08 10:23:29Z

I am trying to get this value via xpath.  i am trying to grab the actual integer value (165)this is the xpath I am getting via debug toolsI am trying to gather information on this property listing and i am trying to capture the area of an apartment.  I have played around with the xpath that i get from the debugger but i always get either an empty string or an error that says u cant turn a xpath selector into a string.  Not sure where to go with this to get the 165 value. Please, any input u can provide, i have looked at this for several days.via this xpath i am expecting the 165, but i get nothing.  i just get this []Try to use relative XPath instead of absolute:orTry using:// relative path selectiontable.item select table with item classtr[contains(., "Area")] Checks for all trs having text 'Area'td//text() for the tr having text 'Area' select td of that tr and 

//text() extracts every text. which is probably 165.

Handshake Failure: SSL Alert number 40

FcknGioconda

[Handshake Failure: SSL Alert number 40](https://stackoverflow.com/questions/53965049/handshake-failure-ssl-alert-number-40)

I'm trying to crawl a page without success:when try a SSL connection I got:    Also when I try this page with curl happens the same:I try to specify -servername in openssl but it doesn't fix the problem. Also trying to specify -tls1_2 doesn't work. TLS Info:

2018-12-28 22:38:00Z

I'm trying to crawl a page without success:when try a SSL connection I got:    Also when I try this page with curl happens the same:I try to specify -servername in openssl but it doesn't fix the problem. Also trying to specify -tls1_2 doesn't work. TLS Info:Found your question while searching for the exact same problem (curl succeeds to connect while openssl fails with alert number 40). It might be related to a server with several virtual hosts to serve, and you need to tell which host you want to connect to, to let the TLS handshake succeed.Specify the exact host name you want with -servername parameter. E.g:

Part of HTML not visible for Scrapy

LucSpan

[Part of HTML not visible for Scrapy](https://stackoverflow.com/questions/53780385/part-of-html-not-visible-for-scrapy)

Set-upI'm using scrapy to scrape housing ads.For each ad, I'm trying to obtain info on year of construction. This info is stated in most ads. I can see the year of construction and the other info around it in the about section when I check the ad in the browser and its HTML code in developer mode.However, when I use Scrapy I get returned an empty list. I can scrape other parts of the ad page (price, rooms, etc.), but not the about section. Check this example ad.If I use response.css('#caracteristique_bien').extract_first(), I get,That's as far as I can go. Any deeper returns emptiness. How can I obtain the year of construction?

2018-12-14 13:05:32Z

Set-upI'm using scrapy to scrape housing ads.For each ad, I'm trying to obtain info on year of construction. This info is stated in most ads. I can see the year of construction and the other info around it in the about section when I check the ad in the browser and its HTML code in developer mode.However, when I use Scrapy I get returned an empty list. I can scrape other parts of the ad page (price, rooms, etc.), but not the about section. Check this example ad.If I use response.css('#caracteristique_bien').extract_first(), I get,That's as far as I can go. Any deeper returns emptiness. How can I obtain the year of construction?As I mentioned, this is rendered using javascript, which means that some parts of the html will be loaded dynamically by the browser (Scrapyis not a browser).The good thing for this case is that the javascript is inside the actual request, which means you can still parse the information that information, but differently.for example to get the description, you can find it inside:As you can see script_info contains all the information, you just need to come up with a way to parse that to get what you wantBut there is some information that isn't inside the same response. To get it you need to do a GET request to:As you can see, it only requires the idannonce, which you can get from the previous response with:Later with the second request, you can get for example the "construction year" with:Looking at your example, the add is loaded dynamically with javascript so you won't be able to get it via scrapy.You can use Selenium for (massive) scrapping (I did similar things on a famous french ads website)Just use it headless with Chrome options and this will be fine : Loaded the page, opened devtools of the browser, and did a ctrl-F with the css selector you used (caracteristique_bien), and found out this request: https://www.seloger.com/detail,json,caracteristique_bien.json?idannonce=139747359

where you can find what you are looking for

Scrapy get text spanning multiple lines and within nested elements

AJITH SHENOY

[Scrapy get text spanning multiple lines and within nested elements](https://stackoverflow.com/questions/53790206/scrapy-get-text-spanning-multiple-lines-and-within-nested-elements)

I'm trying to scrape indeed to get the information of all the job listings in Bangalore.URL : https://www.indeed.co.in/jobs?q=software+developer&l=Bengaluru,+Karnataka&start=0Xpath for the parent div that i'm interested in :I want to extract the company name which is structured like this :and some like :I'm using a common Xpath expression to scrape both kind of titles. I am having trouble with the second type as it includes multiple escape characters like  \n which reflect in my results and on stripping result in an empty string.Xpath used to extract titles:Result :what can i do to get rid of those extra '\n' characters ?

2018-12-15 06:43:02Z

I'm trying to scrape indeed to get the information of all the job listings in Bangalore.URL : https://www.indeed.co.in/jobs?q=software+developer&l=Bengaluru,+Karnataka&start=0Xpath for the parent div that i'm interested in :I want to extract the company name which is structured like this :and some like :I'm using a common Xpath expression to scrape both kind of titles. I am having trouble with the second type as it includes multiple escape characters like  \n which reflect in my results and on stripping result in an empty string.Xpath used to extract titles:Result :what can i do to get rid of those extra '\n' characters ?You can use the normalize-space XPath function to achieve this.

How to have an arbitrary value in an xpath command for web scraping with python scrapy

Gianluca

[How to have an arbitrary value in an xpath command for web scraping with python scrapy](https://stackoverflow.com/questions/53881731/how-to-have-an-arbitrary-value-in-an-xpath-command-for-web-scraping-with-python)

How can I define an xpath command in python (scrapy) to accept any number at the place indicated in the code. I have already tried to put an * or any() at the position.

2018-12-21 08:54:36Z

How can I define an xpath command in python (scrapy) to accept any number at the place indicated in the code. I have already tried to put an * or any() at the position.You can do this using regular expressions:Now assuming your any as like ANY, so you can try this.

x_path = '//*[@id="olnof_{}_altlinesodd"]/tr[1]/TD[1]/A[1]'

x_path.format("put your any here, may b from rand function or extracting the value from somewhere else.")

Then,

table = response.xpath(x_path) this will do the work.You can try below workaround:ends-with(@id, "_altlinesodd") suites better in this case, but Scrapy doesn't support ends-with syntax, so contains used instead

I got timeout error with scrapy when crawling this page

zafiron

[I got timeout error with scrapy when crawling this page](https://stackoverflow.com/questions/53822827/i-got-timeout-error-with-scrapy-when-crawling-this-page)

I can't crawl this page https://www.adidas.pe/, scrapy crawl my_spider returns:I tried to change settings.py:and doesn't works

2018-12-17 20:52:25Z

I can't crawl this page https://www.adidas.pe/, scrapy crawl my_spider returns:I tried to change settings.py:and doesn't worksYou could try changing USER_AGENT in settings.py, it works for me. My settings.py:

Encoding string in scrapy and dropping to JSON

D_rock

[Encoding string in scrapy and dropping to JSON](https://stackoverflow.com/questions/53682710/encoding-string-in-scrapy-and-dropping-to-json)

I need need to scrape text data from sites using languages other than English (mostly Eastern European langs), using Scrapy. When Scrapy finishes, it needs to convert scraped data to JSON for further use.The thing is, if I just scrape the text like this:without encoding it, Scrapy throws something like this:On the other hand, if I do encode it, and try to process that with json.dumps(), I get a TypeError, since json can't serialize bytes. I've seen this explanation (How to encode bytes in JSON? json.dumps() throwing a TypeError), but its of little use, since I need to use utf-8 or utf-16, and not ascii.Any idea how to solve this? 

2018-12-08 12:51:53Z

I need need to scrape text data from sites using languages other than English (mostly Eastern European langs), using Scrapy. When Scrapy finishes, it needs to convert scraped data to JSON for further use.The thing is, if I just scrape the text like this:without encoding it, Scrapy throws something like this:On the other hand, if I do encode it, and try to process that with json.dumps(), I get a TypeError, since json can't serialize bytes. I've seen this explanation (How to encode bytes in JSON? json.dumps() throwing a TypeError), but its of little use, since I need to use utf-8 or utf-16, and not ascii.Any idea how to solve this? have you taken a look at the response headers? What encoding does it tell you? I can imagine that it tells you another encoding than it actually is. Pythons decoding function has a parameter error ('strict', 'replace', 'ignore') which you can use to debug and find the problem'Sorry this more a comment than an answer but i cant comment yet (too less rep)

Parse info from tables with Scrapy and XPath

merlin

[Parse info from tables with Scrapy and XPath](https://stackoverflow.com/questions/53437448/parse-info-from-tables-with-scrapy-and-xpath)

I am trying to extract attributes from a website with scrapy and xpath: The attributes are nested in the following way:There are two problems associated with this:I am new to phyton and scrapy, any help is greatly appreciated.

2018-11-22 20:04:27Z

I am trying to extract attributes from a website with scrapy and xpath: The attributes are nested in the following way:There are two problems associated with this:I am new to phyton and scrapy, any help is greatly appreciated.First of all you should try to remove tbody tag from XPath as usually it's not in page source.You can update your code as below:You will get list of attribute-value pairs:or to get dictionaryTry:

scrapy pipeline to JSON with Chinese characters

Solaris_9

[scrapy pipeline to JSON with Chinese characters](https://stackoverflow.com/questions/53395450/scrapy-pipeline-to-json-with-chinese-characters)

I'm trying to scrapy some web contents with Chinese character. the content scraped like belowBut after the pipeline process, the content has been like this:The pipeline looks like:my question is: how can I keep the Chinese character printed as-is in the *.json file? I really don't want those encoded Unicode characters :)

2018-11-20 14:41:10Z

I'm trying to scrapy some web contents with Chinese character. the content scraped like belowBut after the pipeline process, the content has been like this:The pipeline looks like:my question is: how can I keep the Chinese character printed as-is in the *.json file? I really don't want those encoded Unicode characters :)It seems like the json lib escape those symbols, try to add ensure_ascii=False to json.dumps() as follow:

Extracting image url as a string from inside a div class XPath

Manuel

[Extracting image url as a string from inside a div class XPath](https://stackoverflow.com/questions/53343980/extracting-image-url-as-a-string-from-inside-a-div-class-xpath)

Im having some issue extracting some image urls from amazon with xpath.The page im trying to extract the url is this one, as an examplehttps://www.amazon.com/Touchscreen-Laptop-Tablet-Windows-Quad-Core/dp/B07FYX613Z/ref=sr_1_23/147-3050782-9544926?s=pc&ie=UTF8&qid=1542390985&sr=1-23&keywords=gaming+laptop&refinements=p_36%3A-100000I have this:My goal is to extract https://images-na.ssl-images-amazon.com/images/I/81zqMok22fL.SL1500.jpgI am currently using the xpathWhich actually gives me https://images-na.ssl-images-amazon.com/images/I/81zqMok22fL.SL1500.jpg when i check with XPath HelperProblem is that, when i extract that information withNo data appears in that variable.EDIT: Added amazon links

2018-11-16 19:08:30Z

Im having some issue extracting some image urls from amazon with xpath.The page im trying to extract the url is this one, as an examplehttps://www.amazon.com/Touchscreen-Laptop-Tablet-Windows-Quad-Core/dp/B07FYX613Z/ref=sr_1_23/147-3050782-9544926?s=pc&ie=UTF8&qid=1542390985&sr=1-23&keywords=gaming+laptop&refinements=p_36%3A-100000I have this:My goal is to extract https://images-na.ssl-images-amazon.com/images/I/81zqMok22fL.SL1500.jpgI am currently using the xpathWhich actually gives me https://images-na.ssl-images-amazon.com/images/I/81zqMok22fL.SL1500.jpg when i check with XPath HelperProblem is that, when i extract that information withNo data appears in that variable.EDIT: Added amazon linksI can get required image with below XPath:Try and let me know in case it doesn't work as expectedMaybe try extract_first() instead of extract()?extract() typically returns a selector list, not a single value.

Getting 502 Bad Gateway and then redirected to another website. Scrapy

Jackknife

[Getting 502 Bad Gateway and then redirected to another website. Scrapy](https://stackoverflow.com/questions/53329336/getting-502-bad-gateway-and-then-redirected-to-another-website-scrapy)

scrapy mega noob here. When I try to scrapy shell a website like for example:I get the following messages:Then when I try to see what the response.body has:Which is not the website HTML, I can check in a Browser that the HTML of https://shop.coles.com.au/a/a-vic-metro-oakleigh/product/gasmate-cartridge-butane is totally different, therefore I know I'm being redirected to somewhere. Question is how and why this is happening? and most importantly how to avoid it?Additional info: I'm using a proxy service that will use random proxies each time I use Scrapy shell from a pool of over 20.000. 

It's also worth noting that I've been scraping this webpage for quite a long time before this issue started.

2018-11-15 23:33:20Z

scrapy mega noob here. When I try to scrapy shell a website like for example:I get the following messages:Then when I try to see what the response.body has:Which is not the website HTML, I can check in a Browser that the HTML of https://shop.coles.com.au/a/a-vic-metro-oakleigh/product/gasmate-cartridge-butane is totally different, therefore I know I'm being redirected to somewhere. Question is how and why this is happening? and most importantly how to avoid it?Additional info: I'm using a proxy service that will use random proxies each time I use Scrapy shell from a pool of over 20.000. 

It's also worth noting that I've been scraping this webpage for quite a long time before this issue started.If you look at the Javascript code, it sets a cookie and redirects on itself.It seems that the website expects you to have a specific cookie to access the "normal" pages, but since scrapy can't execute javascript, it stops there.You may want to parse the Javascript code somehow, and set your cookie manually and re-query the same URL.

Implementing scrapy rules by overriding CrawlSpider __init__() method

T the shirt

[Implementing scrapy rules by overriding CrawlSpider __init__() method](https://stackoverflow.com/questions/53239306/implementing-scrapy-rules-by-overriding-crawlspider-init-method)

I'm trying to override the init() method of a CrawlSpider in order to be able to pass domain name and start page. However, I can't seem to pass the rules in. I have tried the approach suggested here(Scrapy: Rules set inside __init__ are ignored by CrawlSpider), and defined rules before the super() method, but it doesn't seem to work.Here is my spider:I pass these values to the terminal, but it stops at the first page: this is the log:

2018-11-10 13:14:48Z

I'm trying to override the init() method of a CrawlSpider in order to be able to pass domain name and start page. However, I can't seem to pass the rules in. I have tried the approach suggested here(Scrapy: Rules set inside __init__ are ignored by CrawlSpider), and defined rules before the super() method, but it doesn't seem to work.Here is my spider:I pass these values to the terminal, but it stops at the first page: this is the log:It's probably something wrong with your allowed_domains, make sure it's a well formed list. If I try this, it works fine:

getting while trying to parse page with scrapy

Andrew

[getting while trying to parse page with scrapy](https://stackoverflow.com/questions/53214316/getting-while-trying-to-parse-page-with-scrapy)

When i'm trying to get all page content i'm getting this error in consoleThis is how my code look like

2018-11-08 18:51:02Z

When i'm trying to get all page content i'm getting this error in consoleThis is how my code look likeThe error message in your post is related to a missing http(s):// in the start_urls. I suppose you forgot to update the error message when you updated the code.But after running your code, it seems this site is blocking clients based on the User-Agent. Consider trying a browser's user-agent string. Eg:

Scrapy Crawler not following links

Kevin Tham

[Scrapy Crawler not following links](https://stackoverflow.com/questions/53213853/scrapy-crawler-not-following-links)

I am writing a Scrapy crawler to scrape information from a property website, https://www.iproperty.com.sg/sale/?page=1, https://www.iproperty.com.sg/sale/?page=2 etc.. The idea is that for each row, obtain information from that row and make a request to a link on that row for further information. Once all rows on that page have been processed, move on to the next page and repeat: Running this crawler results in no data scraped:If I change parse_item to parse_start_url only the first page is scraped but the following links are not followed:I would like to seek enlightenment on this issue as to why I am unable to follow the link to the next pages.

2018-11-08 18:18:05Z

I am writing a Scrapy crawler to scrape information from a property website, https://www.iproperty.com.sg/sale/?page=1, https://www.iproperty.com.sg/sale/?page=2 etc.. The idea is that for each row, obtain information from that row and make a request to a link on that row for further information. Once all rows on that page have been processed, move on to the next page and repeat: Running this crawler results in no data scraped:If I change parse_item to parse_start_url only the first page is scraped but the following links are not followed:I would like to seek enlightenment on this issue as to why I am unable to follow the link to the next pages.Judging by the Scrapy documentation, it looks like your passing a reference to your parse_item method to the callback argument of the rule. However, according to the docs, this callback operates on the extracted links. That's not what you want because your function requires a Scrapy Response to run. So, what you should do is use the process_request argument. On a related note, I changed your regex because the way you have it now it'll only work for pages 1 to 9As an aside, you probably shouldn't return a Request object back to Scrapy and instead should use scrapy.Item and ItemLoader to store your data.So I discovered that there was a problem with the rule itself and had to use an xpath selector instead.

signal only works in main thread: scrappy

fat potato

[signal only works in main thread: scrappy](https://stackoverflow.com/questions/52835180/signal-only-works-in-main-thread-scrappy)

I am making an api which return the JsonResponse as my text from the scrapy. When i run the scripts individually it runs perfectly. But when i try to integrate the scrapy script with python django i am not getting the output. What i want is only return the response to the request(which in my case is POSTMAN POST request. Here is the code which i am tryingI am very new to python and django stuff.Any kind of help would be much appreciated.

2018-10-16 12:10:47Z

I am making an api which return the JsonResponse as my text from the scrapy. When i run the scripts individually it runs perfectly. But when i try to integrate the scrapy script with python django i am not getting the output. What i want is only return the response to the request(which in my case is POSTMAN POST request. Here is the code which i am tryingI am very new to python and django stuff.Any kind of help would be much appreciated.In your code, process_test is a CrawlerProcess, not the output from the crawling. You need additional configuration to make your spider store its output "somewhere". See this SO Q&A about writing a custom pipeline.If you just want to synchronously retrieve and parse a single page, you may be better off using requests to retrieve the page, and parsel to parse it.

Scrapy : Use selector (xpath) in ItemLoader 's output processor

BoobaGump

[Scrapy : Use selector (xpath) in ItemLoader 's output processor](https://stackoverflow.com/questions/52748837/scrapy-use-selector-xpath-in-itemloader-s-output-processor)

I don't know how to make my logic works in my case. I want to extract row from a table from which I don't know in advance the number of columns. 

Here is the source : To extract I hardcoded this : However when I want to implement this hard-coded solution with item loader it gets tricky because I can iterate in feeding_box_table.xpath("tr")since the value collected in the out_processor is a strand not an object where xpathis collected.I don't know if processors should be use that way actually. That's why I tried to do everything with xpat like : table=response.xpath('//*[@id="feedingrecommendation-panel"]/div/article/table/td/small/text()') but the output is not structured. The output wasn't satisfying as it was : 

Output : ["Poids du chat","Maigre","2 kg","39 g",...."10 kg", "121 g"]That's the reason why I wanted to iterate through <tr></tr>Desired Output : [["Poids du chat","Maigre"],["2 kg","39 g"],....["10 kg", "121 g"]]Any help appreciated. 

Kind regards

2018-10-10 21:10:06Z

I don't know how to make my logic works in my case. I want to extract row from a table from which I don't know in advance the number of columns. 

Here is the source : To extract I hardcoded this : However when I want to implement this hard-coded solution with item loader it gets tricky because I can iterate in feeding_box_table.xpath("tr")since the value collected in the out_processor is a strand not an object where xpathis collected.I don't know if processors should be use that way actually. That's why I tried to do everything with xpat like : table=response.xpath('//*[@id="feedingrecommendation-panel"]/div/article/table/td/small/text()') but the output is not structured. The output wasn't satisfying as it was : 

Output : ["Poids du chat","Maigre","2 kg","39 g",...."10 kg", "121 g"]That's the reason why I wanted to iterate through <tr></tr>Desired Output : [["Poids du chat","Maigre"],["2 kg","39 g"],....["10 kg", "121 g"]]Any help appreciated. 

Kind regardsI tried this : It does work. But I'm really not sure that I'm making the most out of ItemLoader here. 

Populating data with scrapy's item loader works in shell but not in spider

BoobaGump

[Populating data with scrapy's item loader works in shell but not in spider](https://stackoverflow.com/questions/52711743/populating-data-with-scrapys-item-loader-works-in-shell-but-not-in-spider)

I have the following simple spider composed of three files. 

My goal is to use item loader correctly to populate the data I'm currently scrapping. 

The pipeline.pyis a simple json file creator as explained in scrapy documentation. items.pyspider.pyIf I do it manually and copy exactly the code of the spider into the shell, I populate exactly what I want. Xpath are for sure right because it's already an hardcoded and functional spider which I want to refine using pipelines and item loader. I can't get where there is the obvious mistake. It looks pretty straightforward though. Any ideas welcome. 

2018-10-09 00:28:29Z

I have the following simple spider composed of three files. 

My goal is to use item loader correctly to populate the data I'm currently scrapping. 

The pipeline.pyis a simple json file creator as explained in scrapy documentation. items.pyspider.pyIf I do it manually and copy exactly the code of the spider into the shell, I populate exactly what I want. Xpath are for sure right because it's already an hardcoded and functional spider which I want to refine using pipelines and item loader. I can't get where there is the obvious mistake. It looks pretty straightforward though. Any ideas welcome. You are using CrawlSpider incorrectly.If you want to crawl a single product just stick to original Spider base class:* changes marked with ^Maybe you should check the ItemLoader written in the spider.py. In my opinion, maybe you can check you logfile.  @BoobaGump use  CrawlSpider, and parse_item that is different name as parse， which is right @Granitosaurus. The warning Crawling Rules  is specified:When I use ItemLoader, I can see item in the log, but I don't get item in the shell. 

Scrapy and xpath weirdness - automatic adding of tags, axis and steps?

pandita

[Scrapy and xpath weirdness - automatic adding of tags, axis and steps?](https://stackoverflow.com/questions/52678687/scrapy-and-xpath-weirdness-automatic-adding-of-tags-axis-and-steps)

I'm having trouble understanding some of the details of how to use xpaths with scrapy. E.g.:Extra html-body padding I didn't addWhere is the html-body tags padding coming from?Axis and steps??Why can I select 'body' without using '/'? I had similar behaviour with a 'div' element in a project.Also the following:Why doesn't the xpath chain return the same as the first line? The selectors seem to be same in both cases? Shouldn't the second xpath call work on a new root?  

2018-10-06 11:42:42Z

I'm having trouble understanding some of the details of how to use xpaths with scrapy. E.g.:Extra html-body padding I didn't addWhere is the html-body tags padding coming from?Axis and steps??Why can I select 'body' without using '/'? I had similar behaviour with a 'div' element in a project.Also the following:Why doesn't the xpath chain return the same as the first line? The selectors seem to be same in both cases? Shouldn't the second xpath call work on a new root?  Scrapy Selector uses lxml.html parser to parse the input text and when lxml receives non full html (html fragment) it always wraps it to be a full html document tree I believe (same way the web browsers work for example).Xpath expression work whole lot similar to the basic file system path expressions such as /home/john/Downloads/file.pdf (absolute path) or Downloads/file.pdf (relative path and is same as ./Downloads/file.pdf).Simple XPath expression such as body are also the same as ./body, which means starting from current node locate <body> element which should be a direct child of current node. Dot refers to the current node, single slash to a single level below it (and double dash means any level below).By default you are located relative to the html tree root (<html> node). Root node has no direct child element <html> so xpath('html') gives you nothing. Root node does have a direct <body> child so xpath('body') yields it. Root node has no direct <a> child so xpath('a') yields none (however you could retrieve it via xpath('.//a')).This chaining xpath('//body').xpath('/body') does not work the way you think it works. First of, starting the expression with / or // (both are absolute paths) instructs the evaluator to start looking relative to the root of the document with no regard as to where you currently are. So your expression goes as: find body element anywhere in the document and then find body element which must be located at the very top (except there's only one element at the top and that is <html>).

mailer.send(mimetype='text/html') does not work along with “attachs” - Scrapy

Umair

[mailer.send(mimetype='text/html') does not work along with “attachs” - Scrapy](https://stackoverflow.com/questions/52589704/mailer-sendmimetype-text-html-does-not-work-along-with-attachs-scrapy)

I am creating CSV files and sending as an attachment in email via Gmail SMTPHere is related code in Scrapy 1.5.1I simply get raw/unrendered HTML in mail instead of rendered.If I remove attachs param from send then I get rendered email I also triedbut still I get raw html instead of rendered one.

2018-10-01 11:01:07Z

I am creating CSV files and sending as an attachment in email via Gmail SMTPHere is related code in Scrapy 1.5.1I simply get raw/unrendered HTML in mail instead of rendered.If I remove attachs param from send then I get rendered email I also triedbut still I get raw html instead of rendered one.

Scraping ASP.net website using Scrapy login failure

arsha_ex

[Scraping ASP.net website using Scrapy login failure](https://stackoverflow.com/questions/52556853/scraping-asp-net-website-using-scrapy-login-failure)

I'm using Scrapy to scrape data from a site that uses ASP.NET. When I’m not logged in, I can scrape all data that is freely available. However, for proprietary material I need to log in. 

When I call FormRequest.from_response(), I fail to login. Any Idea what I'm missing? Here is my spider:Here is Form Data after “inspect”ing login page via “network tab”, POST method with 302 status code:

2018-09-28 13:50:28Z

I'm using Scrapy to scrape data from a site that uses ASP.NET. When I’m not logged in, I can scrape all data that is freely available. However, for proprietary material I need to log in. 

When I call FormRequest.from_response(), I fail to login. Any Idea what I'm missing? Here is my spider:Here is Form Data after “inspect”ing login page via “network tab”, POST method with 302 status code:

Scrapy: merge items from different sites

Bernd

[Scrapy: merge items from different sites](https://stackoverflow.com/questions/52590532/scrapy-merge-items-from-different-sites)

I want to merge items where I get items from site A and items from site B.A items and B items share some fields which allow me to correlate items. Otherwise, each have fields which are unique to A or B.I want to merge these items based on correlation, creating items containing Items which are unique to A or B should pass through unchanged.I cannot assume:How would I do that in Scrapy? Does it make sense to do in Scrapy or better in a postprocessing step?Thoughts on implementation:Since I cannot assume order, I would need a temp store. I am looking at <1000 items, so in-memory temp storage seems feasible.Pseudocode:Where to put this logic?

2018-10-01 11:53:53Z

I want to merge items where I get items from site A and items from site B.A items and B items share some fields which allow me to correlate items. Otherwise, each have fields which are unique to A or B.I want to merge these items based on correlation, creating items containing Items which are unique to A or B should pass through unchanged.I cannot assume:How would I do that in Scrapy? Does it make sense to do in Scrapy or better in a postprocessing step?Thoughts on implementation:Since I cannot assume order, I would need a temp store. I am looking at <1000 items, so in-memory temp storage seems feasible.Pseudocode:Where to put this logic?For this sorta use case I think post processing is the easiest, most straightforward and reliable path. Will also make things easier if you have to do any additional post processing / aggregation later on.

Xpath + Scrapy + Python : data point couldn't be scraped

Debbie

[Xpath + Scrapy + Python : data point couldn't be scraped](https://stackoverflow.com/questions/52399981/xpath-scrapy-python-data-point-couldnt-be-scraped)

This is the XML structure:I want to extract : Hiranandani Gardens, PowaiI tried with these:Both returned an empty list.Note: we must have to use the text of  tag, i.e., "Location:". Otherwise, there are many other places on the site where the same XML structure is used. So, it'll fetch many more unnecessary things apart from the desired value if the text of strong tag is not used.

2018-09-19 07:01:53Z

This is the XML structure:I want to extract : Hiranandani Gardens, PowaiI tried with these:Both returned an empty list.Note: we must have to use the text of  tag, i.e., "Location:". Otherwise, there are many other places on the site where the same XML structure is used. So, it'll fetch many more unnecessary things apart from the desired value if the text of strong tag is not used.Try below XPath to get required output

Scrape url from an image using Scrapy

charls

[Scrape url from an image using Scrapy](https://stackoverflow.com/questions/52325790/scrape-url-from-an-image-using-scrapy)

I am trying to scrape links of images from this websiteThe image that appears early in the page has its url as The image that appears later has its url as,The src in the second case contains a link to a universal image that appears before the actual image of the page loads and data_src is the actual url to be scrapped.So I tried this code to scrape url using ternary expressions (if else).The result is not of the kind that I want.Result of the query:

2018-09-14 06:03:24Z

I am trying to scrape links of images from this websiteThe image that appears early in the page has its url as The image that appears later has its url as,The src in the second case contains a link to a universal image that appears before the actual image of the page loads and data_src is the actual url to be scrapped.So I tried this code to scrape url using ternary expressions (if else).The result is not of the kind that I want.Result of the query:This is indeed the problem, you set the image url to img_url_datasrc in both cases.

You probably wanted:Try this one =) 

Scrapy custom settings

Jan

[Scrapy custom settings](https://stackoverflow.com/questions/52294394/scrapy-custom-settings)

Using scrapy, I have in one of my spiders:However, when I later try to access the settings viathey are empty. Background is that I want to control the settings (and possible pipelines) on a per-spider basis.

What am I doing wrong here?

2018-09-12 11:47:11Z

Using scrapy, I have in one of my spiders:However, when I later try to access the settings viathey are empty. Background is that I want to control the settings (and possible pipelines) on a per-spider basis.

What am I doing wrong here?custom_settings is supposed to be a class attribute:

Scrapy running consecutively not concurrently

MatthewExpungement

[Scrapy running consecutively not concurrently](https://stackoverflow.com/questions/52245908/scrapy-running-consecutively-not-concurrently)

I have a spider that takes a list of addresses and requests information from a single page sending an address and some session information as a scrapy.FormRequest. However, the scraper seems to be running them consecutively. I can tell this because if you search 10 addresses it takes 17 seconds and if you search 20 addresses it takes double the time at about 32 seconds. Since the biggest delay is just waiting for the response, and there's no complex parsing going on, if it was running concurrently, it should just take a little longer correct? I've tried making sure concurrent requests was set high enough in the settings.py. I have added dont_filter = true to the form request. But still can't seem to get it to work. It should also be noted that the scraper is working in the sense that it does pull the information requested.settings.pymain.pyOutputEnd of Output

2018-09-09 15:21:45Z

I have a spider that takes a list of addresses and requests information from a single page sending an address and some session information as a scrapy.FormRequest. However, the scraper seems to be running them consecutively. I can tell this because if you search 10 addresses it takes 17 seconds and if you search 20 addresses it takes double the time at about 32 seconds. Since the biggest delay is just waiting for the response, and there's no complex parsing going on, if it was running concurrently, it should just take a little longer correct? I've tried making sure concurrent requests was set high enough in the settings.py. I have added dont_filter = true to the form request. But still can't seem to get it to work. It should also be noted that the scraper is working in the sense that it does pull the information requested.settings.pymain.pyOutputEnd of Output

Scrapy return/pass data to another module

MrNetroful

[Scrapy return/pass data to another module](https://stackoverflow.com/questions/52080024/scrapy-return-pass-data-to-another-module)

Hi I'm wondering how could I pass scraping result which is pandas file to module which created creating spider.Spider:Printed value in main method is 0 when in parse method is 1005. Could you tell me how should I pass value between.I would like to do that cause I'm running multiple spiders. After they finish scraping I'll merge and save to file.SOLUTION

2018-08-29 14:31:46Z

Hi I'm wondering how could I pass scraping result which is pandas file to module which created creating spider.Spider:Printed value in main method is 0 when in parse method is 1005. Could you tell me how should I pass value between.I would like to do that cause I'm running multiple spiders. After they finish scraping I'll merge and save to file.SOLUTIONThe main reason for this behavior is the asynchronous nature of Scrapy itself. The print(len(spider1.result)) line would be executed before the .parse() method is called.There are multiple ways to wait for the spider to be finished. I would do the spider_closed signal:

extracting text from css node scrapy

Elio Diaz

[extracting text from css node scrapy](https://stackoverflow.com/questions/51805651/extracting-text-from-css-node-scrapy)

I'm trying to scrape a catalog id number from this page:using the css selector (which works in R with rvest::html_nodes)I would like to retrieve the catalog id, which in this case should be:I'm ok if it is done easier with the xpath

2018-08-12 04:20:51Z

I'm trying to scrape a catalog id number from this page:using the css selector (which works in R with rvest::html_nodes)I would like to retrieve the catalog id, which in this case should be:I'm ok if it is done easier with the xpathI don't have scrapy here, but tested this xpath and it will get you the href:If you're having too much trouble with scrapy and css selector syntax, I would also suggest trying out BeautifulSoup python package. With BeautifulSoup you can do things likeIf you need to parse id from href:There seems to be only one link in the h5 element. So in short: 

Images downloaded from scrapy smaller than expected (jpegs) or unreadable (tifs)

pdbutler

[Images downloaded from scrapy smaller than expected (jpegs) or unreadable (tifs)](https://stackoverflow.com/questions/51681601/images-downloaded-from-scrapy-smaller-than-expected-jpegs-or-unreadable-tifs)

I'm not sure how best to pose this question. I'm new to both python and scrapy.Essentially, the files I download using my scrapy script do not match the files I would download manually. All the files (even the smallest jpeg image) is reduced in size. When I open the images in Photoshop, the 'tif' files are in an unrecognizable format. The jpegs open fine. Further, the files I download are downloaded as grayscale files, and the ones my scrapy script pulls are RGB. As far as I can tell the documentation on the image_pipeline is pretty much all there is for processing images with scrapy, but it does mention it uses the pillow library for processing. My thinking is that it's doing something under the hood by default to adjust the images &| limit the size of the downloads. But I don't know what that could be or how to disable it. I'd like to download the images 'as is', i.e., with as little (read: none) processing as possible.If it helps, below are the relevant files. I've omitted some of the code for my spider for brevity, the omitted parts only relate to scraping metadata such as titles and reference numbers.items.pypipelines.pysettings.pyspider.py

2018-08-04 00:58:47Z

I'm not sure how best to pose this question. I'm new to both python and scrapy.Essentially, the files I download using my scrapy script do not match the files I would download manually. All the files (even the smallest jpeg image) is reduced in size. When I open the images in Photoshop, the 'tif' files are in an unrecognizable format. The jpegs open fine. Further, the files I download are downloaded as grayscale files, and the ones my scrapy script pulls are RGB. As far as I can tell the documentation on the image_pipeline is pretty much all there is for processing images with scrapy, but it does mention it uses the pillow library for processing. My thinking is that it's doing something under the hood by default to adjust the images &| limit the size of the downloads. But I don't know what that could be or how to disable it. I'd like to download the images 'as is', i.e., with as little (read: none) processing as possible.If it helps, below are the relevant files. I've omitted some of the code for my spider for brevity, the omitted parts only relate to scraping metadata such as titles and reference numbers.items.pypipelines.pysettings.pyspider.pyLooks like I figured it out and solved my own issue! I poked around in the source code for the ImagesPipeline and discovered that by default, scrapy uses a method convert_images when it calls get_images. convert_images was the issue, as it converts both the filetype and colorspace for non-jpeg and bmp images.I re-wrote get_images to handle both the tiff and jpeg formats I was interested in scraping: Hope that helps others in the future!

Pyinstaller error on scrapy?

cooler

[Pyinstaller error on scrapy?](https://stackoverflow.com/questions/51213515/pyinstaller-error-on-scrapy)

I am using scrapy importing it. I built the python file using pyinstaller. After building it I ran the file ./new.py. But the error pops:

2018-07-06 15:24:36Z

I am using scrapy importing it. I built the python file using pyinstaller. After building it I ran the file ./new.py. But the error pops:You did not use Pyinstaller properly when you had built your stand-alone program. Here is a short, layman's description of how Pyinstaller works: Pyinstaller bundles the Python interpreter, necessary DLLs (for Windows), your project's source code, and all the modules it can find into a folder or self-extracting executable. Pyinstaller does not include modules or files it cannot find in the final .exe (Windows), .app (macOS), folder, etc. that results when you run Pyinstaller.So, here is what happened:You ran your frozen/stand-alone program. As soon as you did this, your program was 'extracted' to a new, temporary folder on your computer /temp/_MEIbxALM3/. This folder contains the Python interpreter, your program's source code, and the modules Pyinstaller managed to find (plus a couple other necessary files). The Scrapy module is more than just a module. It is an entire framework. It has its own plain text files (besides Python files) that it uses. And, it imports a lot of modules itself.The Scrapy framework especially does not get along with Pyinstaller because it uses many methods to import modules that Pyinstaller cannot 'see'. Also, Pyinstaller basically makes no attempt to include files in the final build that are not .py files unless you tell it to. So, what really happened?The text file 'VERSION' that exists in the 'normal' scrapy module on your computer (that you had installed with pip or pipenv) was not included in the copycat scrapy module in the build of your program. Scrapy needs this file; Python is giving you the FileNotFoundError because it simply was never included. So, you have to include the file in the build of your program with Pyinstaller.How do you tell Pyinstaller where to find modules and files?This guy says to just copy the missing files from where they are installed on your computer into your build folder spit out from Pyinstaller. This does work. But, there is a better way and Pyinstaller can do more of the work for you (preventing further ImportErrors and FileNotFoundErrors you may get). See below:build.spec Files are Your Friendspec files are just Python files that Pyinstaller uses like a configuration file to tell it how to build your program. Read more about them here. Below is an example of a real build.spec file I used recently to build a Scrapy program with a GUI for Windows (my project's name is B.O.T. Bot):Uncomment the last region if you want to build a folder instead of a stand-alone .exe. This is a configuration file specific to my computer and project structure. So in your file, you would have to change a few things (for example pathex to tell Pyinstaller where to find DLLs on Windows 10. But, the premise is the same.My project directory looks like this:Pay special attention to the hooks/ directory. Using hooks will save you from a lot of headaches down the road. Read more about Pyinstaller's hooks feature here. In the hooks/ directory there is a hook file for Scrapy. This will tell Pyinstaller to include many modules and files it would have otherwise missed if you did not use a .spec file. This is the most important thing I have wrote here so far. If you do not do this step, you will keep getting ImportErrors every time you try to run a Scrapy program built using Pyinstaller. Scrapy imports MANY modules that Pyinstaller misses.hook-scrapy.py (Note: Your hook file must be named just like this.):After you finished writing a proper build.spec file, all you need to do is run Pyinstaller like this in your shell prompt:Pyinstaller should then spit out a proper build of your program that should work. Problem solved.Those Google'ing for an answer to this problem or really any issue with Pyinstaller or Scrapy, pray you find my answer.

Downloader Middleware, Requests, and Responses

oldboy

[Downloader Middleware, Requests, and Responses](https://stackoverflow.com/questions/51164398/downloader-middleware-requests-and-responses)

I know that Middleware passes requests to the Downloader and responses to the Spider, but nothing explains what Downloader Middleware should be used to do and I cannot find a decent explanation anywhere.What is its purpose? What are some of the ways that Downloader Middleware is used to modify requests and responses? Is the Downloader Middleware used to handle exceptions, manage proxies and user-agent strings, et cetera?

2018-07-04 00:34:10Z

I know that Middleware passes requests to the Downloader and responses to the Spider, but nothing explains what Downloader Middleware should be used to do and I cannot find a decent explanation anywhere.What is its purpose? What are some of the ways that Downloader Middleware is used to modify requests and responses? Is the Downloader Middleware used to handle exceptions, manage proxies and user-agent strings, et cetera?Lets take an example given it scrapy's docsWell it works fine, but as you already know that it is not true for all the websites, like amazon.comYou need to send the user agent like (Mozilla's or Chrome's) in the request header and that way it knows that the request is coming from browser. you can easily change it from the settings by replacing USER_AGENT="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36".Now you have changed the user agent and the amazon's sever will feel like the request is coming from chrome browser. One step further you're increasing the concurrency to some large number like 100. Now what happens is amazon can easily track your bot and you'll get ban because what does that means is a single device is accessing the server 100 times in a second, which is not possible by human.So here proxy rotation and user agent rotation comes in. The question is how we rotate them.Lets say you have bunch of USER_AGENTS = [...] in your settingmiddlewares.py

    from .settings import USER_AGENT_LIST

    import random

    from scrapy import logAnd you have to tell the scrapy project like,What actually happens when you run the spider?Explanation:Technically it will call the downloader middleware class, It calls process_request method with providing current request object and spider objectNow the request object is available to you, you can do any thing with it, like changing the headers proxy even url itself. but we are focused in user agent now.so by our algorithm we will change the user agent, Again new request gets schedule comes to this middleware it'll change the user agent and it will pass the request object forward.While registering our project we have to specify some number between 100 to 900,

what 500 it indicates? (as value of middleware namespace)There are number of different middlewares already exists in scrapy and also we can define many as we want now in such case 

how can prioritise them? 

what would be the order of execution of them?It gets executed in ascending order like, 100, 200, 230, 500, 650, 900.Same methodology applied for the proxy rotation.Request:If you have used lower level urllib or requests module, what it does?

It takes url as an argument (cookies, proxies, headers, payload optional arguments)When you run it, what it does is? 

It makes http request to the url "http://quotes.toscrape.com"Like you normally would in browser:But there is lot goes on, between step 2 and 3.

Your browser takes an url requests to the server by using lower level http request. takes response then compiles it down and displays it in the browser.The pseudo code which does the same thing,requests.get to the url "http://quotes.toscrape.com"

send the HTTP GET request to "http://quotes.toscrape.com" which returns you the response. you can execute it and check it out it'll return some html to us which is nothing but a response.What scrapy is doing to just provide their functionality it wraps you this requests module in scrapy.Request and response module to HttpResponse.HttpResponse offers xpath & css selector.the response here provided by the scrapy framework in this parse method is an instance of HttpResponse which is a wrapper of lower level urllib's response object.  

Scraping a Website without Crawling the “Search Engine” section of the page

vasco_t

[Scraping a Website without Crawling the “Search Engine” section of the page](https://stackoverflow.com/questions/51138637/scraping-a-website-without-crawling-the-search-engine-section-of-the-page)

Lately I've been trying to develop a Web Crawler using Scrapy to extract all the PDF documents from a specific domain and, although I can somehow achieve the goal, it still gets stuck in some kind of an "infinite loop" because of the "search engine" part of the said website. Is there any way that I can avoid this kind of behavior?Here's the code of the crawler that has been written so far (sorry if it's spaghetti, still trying to get better..):I tried to limit the recursion by adding a cap (n_iterations) of how deeply the crawler should dig into. It works but still, I'd like to filter the links it crawls into in a proper manner.Thank you so much for helping me out, have a good week!

2018-07-02 14:38:01Z

Lately I've been trying to develop a Web Crawler using Scrapy to extract all the PDF documents from a specific domain and, although I can somehow achieve the goal, it still gets stuck in some kind of an "infinite loop" because of the "search engine" part of the said website. Is there any way that I can avoid this kind of behavior?Here's the code of the crawler that has been written so far (sorry if it's spaghetti, still trying to get better..):I tried to limit the recursion by adding a cap (n_iterations) of how deeply the crawler should dig into. It works but still, I'd like to filter the links it crawls into in a proper manner.Thank you so much for helping me out, have a good week!

How to handle temporary errors which are not signaled by http status code?

C. Yduqoli

[How to handle temporary errors which are not signaled by http status code?](https://stackoverflow.com/questions/51074475/how-to-handle-temporary-errors-which-are-not-signaled-by-http-status-code)

I am writing a crawler using Scrapy (Python) and don't know how to handle certain errors.I have got a website which sometimes returns an empty body or a normal page with an error message. Both replies come with a standard 200 HTTP status code.What I want to do when I encounter such a situation is tell Scrapy toIs there an easy way like raising a certain exception a la raise scrapy.TemporaryError or do I have to do everything manually. In the later case, how do I delete content from the cache or talk to the autothrottle module?I know I can use dont_cache on requests to not cache them. But usually I do want to cache my requests and only decide on the response if I want to keep it. Also the documentation is not clear weather this flag avoids saving the response of the request to cache or if it also avoids reading the request from cache...Autothrottle uses the download latency to adjust the request rate. The throttling algorithm treats non-200 responses as failed responses and does not decrease the download delay. However my requests return 200 status codes. So autothrottle cannot handle the situation. There must be a way to tell autothrottle to use its throttling logic and treat these specific requests as failed.

2018-06-28 03:55:13Z

I am writing a crawler using Scrapy (Python) and don't know how to handle certain errors.I have got a website which sometimes returns an empty body or a normal page with an error message. Both replies come with a standard 200 HTTP status code.What I want to do when I encounter such a situation is tell Scrapy toIs there an easy way like raising a certain exception a la raise scrapy.TemporaryError or do I have to do everything manually. In the later case, how do I delete content from the cache or talk to the autothrottle module?I know I can use dont_cache on requests to not cache them. But usually I do want to cache my requests and only decide on the response if I want to keep it. Also the documentation is not clear weather this flag avoids saving the response of the request to cache or if it also avoids reading the request from cache...Autothrottle uses the download latency to adjust the request rate. The throttling algorithm treats non-200 responses as failed responses and does not decrease the download delay. However my requests return 200 status codes. So autothrottle cannot handle the situation. There must be a way to tell autothrottle to use its throttling logic and treat these specific requests as failed.In your response you can check for a condition and decide to re-queue the URL.requests disappear after queueing in scrapyAdjusting throttle dynamicallyIf you check self.crawler.extensions.middlewares, you will see that it has all loaded extensionsIn my case self.crawler.extensions.middlewares[5] gives <scrapy.extensions.throttle.AutoThrottle object at 0x10b75a208> (Of course you will loop through the tuple and find which one is of type AutoThrottle)Now you can use this object and adjust the values dynamically in your scraper

Parse_detail not working in scrapy

Vira Xeva

[Parse_detail not working in scrapy](https://stackoverflow.com/questions/50701035/parse-detail-not-working-in-scrapy)

Hello i am new to Scrapy. And i am trying to use scrapy with scrapydI am trying to scrap this two website, and somehow one of the website is not working eventhough i use basically same spider.py file. Only changing somePlease kindly helpthe scrapydthe Working spider codethe NOT WORKING spider codei think the problem is they are not going through the parse_detail. cause the print("SCRAP EVERY LINK KOMPAS") is not being printed in the log.

2018-06-05 13:14:46Z

Hello i am new to Scrapy. And i am trying to use scrapy with scrapydI am trying to scrap this two website, and somehow one of the website is not working eventhough i use basically same spider.py file. Only changing somePlease kindly helpthe scrapydthe Working spider codethe NOT WORKING spider codei think the problem is they are not going through the parse_detail. cause the print("SCRAP EVERY LINK KOMPAS") is not being printed in the log.

Using scrapy with scrapyd in Django not entering def(parse)

Vira Xeva

[Using scrapy with scrapyd in Django not entering def(parse)](https://stackoverflow.com/questions/50645414/using-scrapy-with-scrapyd-in-django-not-entering-defparse)

i'm still learning scrapy and i am trying to use scrapy with scrapyd inside a Django Project.But i am noticing that the spider just wont enter the def(parse)The print("Start Spider") is in the log but the print("Search Link") is not.i also have this kind of errorPlease help.

PS : When i run it outside the Django it work just fineThank you 

2018-06-01 14:15:05Z

i'm still learning scrapy and i am trying to use scrapy with scrapyd inside a Django Project.But i am noticing that the spider just wont enter the def(parse)The print("Start Spider") is in the log but the print("Search Link") is not.i also have this kind of errorPlease help.

PS : When i run it outside the Django it work just fineThank you It seems to me that you are missing the crawling rules in your spider.Try addingto your code, after the start_urls.

I don't understand how it could work outside of django though.

Scrapy: cycle results in variables not updating

Eichhorn

[Scrapy: cycle results in variables not updating](https://stackoverflow.com/questions/50655036/scrapy-cycle-results-in-variables-not-updating)

Here's the part of code that I want to run 4 times. Without counters it works as intended: link to next page is retrieved and scraped for relevant data:Here are the variants of the cycle I tried:Adding a simple global counter:Adding a sub-function:In case of counter the response value is updated either once (if placed as in this code) or not at all if count = count + 1 is placed at the end.   In case of sub function response is updated only on the last iteration, resulting in 2 scraped pages instead of 4.What is the correct way to implement the cycle so that variables are updated as intended?Here's complete code if that helps(I use 4 defs instead of a cycle right now):

2018-06-02 08:39:16Z

Here's the part of code that I want to run 4 times. Without counters it works as intended: link to next page is retrieved and scraped for relevant data:Here are the variants of the cycle I tried:Adding a simple global counter:Adding a sub-function:In case of counter the response value is updated either once (if placed as in this code) or not at all if count = count + 1 is placed at the end.   In case of sub function response is updated only on the last iteration, resulting in 2 scraped pages instead of 4.What is the correct way to implement the cycle so that variables are updated as intended?Here's complete code if that helps(I use 4 defs instead of a cycle right now):

Using Custom middleware in standalone scrapy script

Calimocho

[Using Custom middleware in standalone scrapy script](https://stackoverflow.com/questions/50499816/using-custom-middleware-in-standalone-scrapy-script)

I am writing a standalone scraping script (update.py) that implements a custom downloader middleware.The script is currently using the CrawlerProcess() API documented here and here.It looks something like this:The script returns the error: "No module named 'update'"If I replace update.CustomMiddleware with CustomMiddleware it returns 'Not a valid path'I am aware of the get_project_settings() utility but my script cannot be in a project folder and must be able to run without any additional files.Is this achievable?, if so what is the best way to achieve this?

2018-05-24 01:54:58Z

I am writing a standalone scraping script (update.py) that implements a custom downloader middleware.The script is currently using the CrawlerProcess() API documented here and here.It looks something like this:The script returns the error: "No module named 'update'"If I replace update.CustomMiddleware with CustomMiddleware it returns 'Not a valid path'I am aware of the get_project_settings() utility but my script cannot be in a project folder and must be able to run without any additional files.Is this achievable?, if so what is the best way to achieve this?you need separate middleware file and import middleware on top of the script.class CustomMiddleware(object):

.... custom middleware definitionthis class will be in middleware.py and in settings only add like thisand both middleware.py and you script both in the same directory.

Accelerate scrapy python3 script

Til Hund

[Accelerate scrapy python3 script](https://stackoverflow.com/questions/50320520/accelerate-scrapy-python3-script)

I would like to bulk download free-to-download pdfs (copies of an old newspaper from 1843 to 1900 called Gaceta) from this website of the Nicaraguan National Assembly with Python3/Scrapy (see former question here) using the below script:The script does its job fetching the direct links from a php file and downloading the PDF subsequently, however there are two things still bugging me:

2018-05-13 20:48:31Z

I would like to bulk download free-to-download pdfs (copies of an old newspaper from 1843 to 1900 called Gaceta) from this website of the Nicaraguan National Assembly with Python3/Scrapy (see former question here) using the below script:The script does its job fetching the direct links from a php file and downloading the PDF subsequently, however there are two things still bugging me:Disclaimer: I did not test the script since scrapy requires Microsoft Visual C++ 14.0 and it takes a while to download and install :(Here's an updated script, I added the date range as start and end and modified the parse_rdds method to only download files in the time frame.As for optimize it, scrapy is a non-blocking lib and as I understand it should be able to download several files in parallel as it is right now. Keep in mind that you're downloading what it seems a lot of files so, it naturally could take a while.

Scrapy follow all the links and get status

bhattraideb

[Scrapy follow all the links and get status](https://stackoverflow.com/questions/50200752/scrapy-follow-all-the-links-and-get-status)

I want to follow all the links of the website and get the status of every links like 404,200. I tried this:I can see the links without status code on the console like:but how to save in text file with status of all links?

2018-05-06 14:24:11Z

I want to follow all the links of the website and get the status of every links like 404,200. I tried this:I can see the links without status code on the console like:but how to save in text file with status of all links?I solved this as below. Hope this will help for anyone who needs.

How import module in PyCharm

dorinand

[How import module in PyCharm](https://stackoverflow.com/questions/50115672/how-import-module-in-pycharm)

I have two projects. In first one, I can import my module importme.py like:And now I can use my function hello() in importme module without any problem. In second one, I recieve:But I can import it via:Why I cant import my module the same way in both projects? Should I configure some paths ? EDIT1:Directory structure of first project:Directory structure of second project:file init.py is empty.

2018-05-01 11:26:55Z

I have two projects. In first one, I can import my module importme.py like:And now I can use my function hello() in importme module without any problem. In second one, I recieve:But I can import it via:Why I cant import my module the same way in both projects? Should I configure some paths ? EDIT1:Directory structure of first project:Directory structure of second project:file init.py is empty.My favorite method of dealing with PYTHONPATH is installing package in edit mode in virtual environment.This is a great workflow because it's clean and reliable - you are using exactly what you'd be using in production/finished package environment.

ValueError: unsupported format character in mysql scrapy pipline

meow

[ValueError: unsupported format character in mysql scrapy pipline](https://stackoverflow.com/questions/50128291/valueerror-unsupported-format-character-in-mysql-scrapy-pipline)

I am working on a scrapy crawler, and this issue really bothers me since I already been trapped by this for days.This placeholders function works fine when I using "?" instead of "%s" for SQLite db. But while using "?" as the database switching to MySQL, it shows: even I pay lots of effort modifying codes and changing the placeholder(supposedly?) it still shows :more specifically :

2018-05-02 06:22:40Z

I am working on a scrapy crawler, and this issue really bothers me since I already been trapped by this for days.This placeholders function works fine when I using "?" instead of "%s" for SQLite db. But while using "?" as the database switching to MySQL, it shows: even I pay lots of effort modifying codes and changing the placeholder(supposedly?) it still shows :more specifically :The problem is that this line:placeholders = ",".join(len(item) * "%s")isn't doing what you expect.",".join(len(item) * "%s") does two things - computes  len(item) * "%s", then joins the result with ','.The result of len(item) * '%s' is the string (or iterable) '%s%s%s'.  str.join(iterable) returns a string consisting of all the elements of iterable separated by the string that provides the method.  So the result of calling ','.join('%s%s%s')

is'%,s,%,s,%,s', not '%s,%s,%s'You want to do or so that str.join is operating on an iterable of '%s' strings rather than a single string like '%s%s%s'.

`

Duplicates in asp.net pagination when scraping?

Billy Jhon

[Duplicates in asp.net pagination when scraping?](https://stackoverflow.com/questions/50036740/duplicates-in-asp-net-pagination-when-scraping)

I am scraping this asp.net site and since request url is the same Scrapy dupefilter does not work. As a result I am getting tons of duplicated urls which puts my spider into infintite run. How can I deal with it?

My code looks as this.I tried to add a set to keep track of page numbers but have no clue how to deal with '...' which leads to the next 10 pages.The more threads I set the more duplicates I recieve.

So it looks like the only solution so far is to reduce thread_count to 3 or less. 

2018-04-26 07:02:16Z

I am scraping this asp.net site and since request url is the same Scrapy dupefilter does not work. As a result I am getting tons of duplicated urls which puts my spider into infintite run. How can I deal with it?

My code looks as this.I tried to add a set to keep track of page numbers but have no clue how to deal with '...' which leads to the next 10 pages.The more threads I set the more duplicates I recieve.

So it looks like the only solution so far is to reduce thread_count to 3 or less. I'm not certain if I understad you correctly but asp.net usually relies a lot on cookies for delivering content. So when crawling asp.net websites you want to use cookiejar feature of scrapy:Read more about cookiejars here:

https://doc.scrapy.org/en/latest/topics/downloader-middleware.html?highlight=cookiejar#multiple-cookie-sessions-per-spider

Scrapy splash: screenshot specific element

SphinX

[Scrapy splash: screenshot specific element](https://stackoverflow.com/questions/49971106/scrapy-splash-screenshot-specific-element)

Is there a way to screenshot a specific element in splash? I cannot seem to find a solution for this, The only option that I found is using "render.png" which takes a screenshot of the full page, I only need a specific element for example "//table".I found this which I am currently using (solution in selenium), Splash seems faster with less overhead.Thanks a lot.

2018-04-22 21:53:48Z

Is there a way to screenshot a specific element in splash? I cannot seem to find a solution for this, The only option that I found is using "render.png" which takes a screenshot of the full page, I only need a specific element for example "//table".I found this which I am currently using (solution in selenium), Splash seems faster with less overhead.Thanks a lot.Yes. You can do that by using scrapy-splash plugin.https://github.com/scrapy-plugins/scrapy-splash

Please read the "More complex Splash Lua Script example - get a screenshot of an HTML element by its CSS selector (it requires Splash 2.1+). Note how are arguments passed to the script:" part there.You do not use render.png endpoint, you use execute endpoint and execute a lua script to get a screenshot of a specific element.I implemented it like this in scrapy spider.

How to turn display from none to block in Scrapy?

Danyal Mughal

[How to turn display from none to block in Scrapy?](https://stackoverflow.com/questions/49955430/how-to-turn-display-from-none-to-block-in-scrapy)

I'm trying to scrape data from a drop down menu(Here is the link). During inspecting to get the xpath, I realized that the display is none. So is there any way to scrape data from that drop down manu(Fits the following cars) who's display has set to none. If yes/no, how/why?

2018-04-21 11:46:45Z

I'm trying to scrape data from a drop down menu(Here is the link). During inspecting to get the xpath, I realized that the display is none. So is there any way to scrape data from that drop down manu(Fits the following cars) who's display has set to none. If yes/no, how/why?The data you want to scrape gets populated via Ajax call. So, you need to find out the url of the Ajax call. Once, you get that ,your work is easy.Follow the steps below.In your case, it's a post request that happens over the fly.Here is the pic of the callTherefore, you need to find the url and the request parameters passed during the request.You can see that the request parameters are as follows:Now you got the url and data, it's just a matter of few lines of code.

Instantiate multiple spiders from a single class in Scrapy

NFB

[Instantiate multiple spiders from a single class in Scrapy](https://stackoverflow.com/questions/49600870/instantiate-multiple-spiders-from-a-single-class-in-scrapy)

In the interest of saving time and lines of repeated code on a very large project, I have been attempting to instantiate multiple spiders in Scrapy from a single class definition. I don't find in the docs that this is a standard practice, but I also don't find any indication that it cannot or should not be done. However, it is not working. Here is what I'm trying:The error I get is :Is it possible to use Scrapy this way, and if so, what am I doing incorrectly?

2018-04-01 17:42:10Z

In the interest of saving time and lines of repeated code on a very large project, I have been attempting to instantiate multiple spiders in Scrapy from a single class definition. I don't find in the docs that this is a standard practice, but I also don't find any indication that it cannot or should not be done. However, it is not working. Here is what I'm trying:The error I get is :Is it possible to use Scrapy this way, and if so, what am I doing incorrectly?1.scrapy looks for spider classes, not instances.Here in your code ExampleSpider is a class, while SpiderInstance is an instance of it.You may need to do something like this instead:2.It's also worth noticing that the allowed_domains attribute of a spider is expected to contain a list, tuple, or set of domains. While in your sample code it's a string.3.Instead of subclassing the ExampleSpider as shown in #1, you may also make ExampleSpider a metaclass. So that instantiating ExampleSpider would bring you a class, instead of a class instance.After reading @starrify's answer, a simple solution I hadn't gone to before:

Using list variables in xpath

Land Owner

[Using list variables in xpath](https://stackoverflow.com/questions/49573276/using-list-variables-in-xpath)

I want to grab the cover of the book on this HTML. The cover sometimes is at the first order, sometimes it is at the second order.I put the cover types in this listWhen I specifically write 'Hardcover' in the xpath, it works.However, when I use the index of the list cover[0], it brings other things as well.I want to iterate the list values to check one of them between tags.

2018-03-30 11:08:27Z

I want to grab the cover of the book on this HTML. The cover sometimes is at the first order, sometimes it is at the second order.I put the cover types in this listWhen I specifically write 'Hardcover' in the xpath, it works.However, when I use the index of the list cover[0], it brings other things as well.I want to iterate the list values to check one of them between tags.You need string concatenation :or 

Check this about xpath injection

Scrapy scraping nested text using css selectors

patlatka

[Scrapy scraping nested text using css selectors](https://stackoverflow.com/questions/49516650/scrapy-scraping-nested-text-using-css-selectors)

I have the following html code:So to get the text data as: Lorem ipsum si ammet, so I tried to use:But I only receive only lorem sie ammet. How can I get both <p> and <strong> texts using CSS selectors?

2018-03-27 15:15:58Z

I have the following html code:So to get the text data as: Lorem ipsum si ammet, so I tried to use:But I only receive only lorem sie ammet. How can I get both <p> and <strong> texts using CSS selectors?One liner solution.div.article * means to scrape everything inside the div.articleOr an easy way to write itBoth approaches are same, 

Django + Celery + Scrapy twisted reactor(ReactorNotRestartable) and database(SSL error) errors

Levi Moreira

[Django + Celery + Scrapy twisted reactor(ReactorNotRestartable) and database(SSL error) errors](https://stackoverflow.com/questions/49347506/django-celery-scrapy-twisted-reactorreactornotrestartable-and-databasessl)

I have a Django 2.0, Celery 4 and Scrapy 1.5 setup where I have a Spider inside a django custom command and I need to call this command at regular intervals, I use Celery to call these commands and they involve scraping a web page and saving some results to the database. Here are my files:get_data.pycelery.pytasks.py(Django) settings.pyI've removed some imports and reduced the code for simplicity. The problem is that when I run my celery task, my spider will only execute the first time, the second time I get ReactorNotRestartable error. I understand that the problem comes from the Twisted reactor being restarted more than once, which is not possible. I've already looked into these questions 1, 2, 3 and many others involving the same error, but none of them considered the concurrency problem when using Django to save to the database. When I tried applying their solution to my problem I receive a django.db.utils.OperationalError: SSL error: decryption failed or bad record mac. I've looked that up as well and it is caused by the multiple processes that open a database connection, which is actually hapenning because of their solution.So my question boils down to: Is there a way to run Celery+Scrapy+Django without having problems with the Twisted reactor being opened and finished multiple times? 

2018-03-18 11:48:08Z

I have a Django 2.0, Celery 4 and Scrapy 1.5 setup where I have a Spider inside a django custom command and I need to call this command at regular intervals, I use Celery to call these commands and they involve scraping a web page and saving some results to the database. Here are my files:get_data.pycelery.pytasks.py(Django) settings.pyI've removed some imports and reduced the code for simplicity. The problem is that when I run my celery task, my spider will only execute the first time, the second time I get ReactorNotRestartable error. I understand that the problem comes from the Twisted reactor being restarted more than once, which is not possible. I've already looked into these questions 1, 2, 3 and many others involving the same error, but none of them considered the concurrency problem when using Django to save to the database. When I tried applying their solution to my problem I receive a django.db.utils.OperationalError: SSL error: decryption failed or bad record mac. I've looked that up as well and it is caused by the multiple processes that open a database connection, which is actually hapenning because of their solution.So my question boils down to: Is there a way to run Celery+Scrapy+Django without having problems with the Twisted reactor being opened and finished multiple times? I've found a solution myself. I had to add the following to by celery settings file:This tells celery to start every task with a clean slate, therefore every task will be started in a new process and the ReactorNotRestartable problem won't occur.

How would I select a selector in CSS that changes?

user3656280

[How would I select a selector in CSS that changes?](https://stackoverflow.com/questions/49266391/how-would-i-select-a-selector-in-css-that-changes)

I'm trying to scrape movie titles from Tmdb but each title has a different selector. Is there a way for me to get them all in one go? For example: The css selector for Birdman is .7, Star Wars is .9, and other movies have different ones.You may ask why not just got the titles like this but it is because I need to go on each page in order to get the genre as well.

2018-03-13 21:48:32Z

I'm trying to scrape movie titles from Tmdb but each title has a different selector. Is there a way for me to get them all in one go? For example: The css selector for Birdman is .7, Star Wars is .9, and other movies have different ones.You may ask why not just got the titles like this but it is because I need to go on each page in order to get the genre as well.I think when a site has an API (and it has the information you are looking for), you should use it instead of webscraping. TheMovieDB API seems to allow 4 requests per second and took only a minute to sign up.This script below (written with Python 3.6.4) uses total_pages=100 (you could set up to a maximum of 1000 as per API) and each page has 20 movies returned as JSON. I had to make a separate API call to get the human-readable genres but everything seems to work fine. For 100 pages, this code took about 40sec to run and then all the results are saved to a file for you to work with later.The console output of this script was too long to put in this question but here is a link to it.Also, here is a link to popular_movies.json to show how much extra information you get for each movie (allowing you to expand in the future to more than just titles and genres).Not what you asked for but a method for doing what you want, I think.The usual preliminaries:Now use find_all with a Python function to identify elements whose id attributes match 'movie_'.There are 61 of them in the page you highlighted for consideration.Here's the content of the first item.You can dig out the title in this way.

Scrapy tbody tag return an empty answer but has text inside

VioGeo

[Scrapy tbody tag return an empty answer but has text inside](https://stackoverflow.com/questions/49171414/scrapy-tbody-tag-return-an-empty-answer-but-has-text-inside)

I try to scrap and crawl a website. The data is in the tbody tag (event names). 

When I check the google console, the tbody tag has text data, but when I try to scrap it it returns an empty answer (also tested in scrapy shell). I checked for an AJAX method, because it can bug the script, but it seems does not have it.Do you have any idea why the answer is empty whereas the tbody tag has text inside source code ?Here is my codeThe terminal windowAnd the scrapy shell

2018-03-08 10:56:29Z

I try to scrap and crawl a website. The data is in the tbody tag (event names). 

When I check the google console, the tbody tag has text data, but when I try to scrap it it returns an empty answer (also tested in scrapy shell). I checked for an AJAX method, because it can bug the script, but it seems does not have it.Do you have any idea why the answer is empty whereas the tbody tag has text inside source code ?Here is my codeThe terminal windowAnd the scrapy shellI assume you are trying to select all the event names, if so you can use this as your xpath //*[@class="cal2table"]/tbody/tr/td[2]/div/div[1]/div/a/text()  So I believe the issue you are having is due to your xpath definitions, without any further information on what you're trying to select this is the best answer I can give.  A tip, you can use the following command in Chrome/Firefox console to test your xpath:

$x('//*[@class="cal2table"]/tbody/tr/td[2]/div/div[1]/div/a/text()')  To use this as you currently are trying to load the items in, then try the following snippet instead. I haven't tested this so you may need to make small adjustments.for unElement in response.xpath('//*[@class="cal2table"]//tr'): 

        loader = ItemLoader(JustrunlahItem(), selector=unElement)

        loader.add_xpath('eve_nom_evenement', './/td[2]/div/div[1]/div/a/text()')

It's a common problem: sometimes there is no tbody tag in source HTML for tables (modern browsers add it to the DOM automatically). So always check HTML source code:Just remove tbody from you xpath or css expression and it will work.Modern browsers are known for adding tbody elements to tables. Scrapy, on the other hand, does not modify the original page HTML, so you won’t be able to extract any data if you use tbody in your XPath expressions.

Python Twitter scrapy used for extracting twitter following, followers count etc

JimmyLoughnan

[Python Twitter scrapy used for extracting twitter following, followers count etc](https://stackoverflow.com/questions/48930987/python-twitter-scrapy-used-for-extracting-twitter-following-followers-count-etc)

Im using scrapy to extract user info on twitter but im currently having issues extracting the following, followers count etc using python.i can succesfully extract the id, screenname and avatar etc using..twitter htmlunfortunately im having issues extracting the attributes count from the 'following' html for the user as i do not know the correct xpath to extract the data or if its possible...i can successfully extract the count using java script using the code below but having issues in python.Any help and suggestions would be brilliant. 

thanks picture of twitter without javascript

twitter without javascript

2018-02-22 15:20:27Z

Im using scrapy to extract user info on twitter but im currently having issues extracting the following, followers count etc using python.i can succesfully extract the id, screenname and avatar etc using..twitter htmlunfortunately im having issues extracting the attributes count from the 'following' html for the user as i do not know the correct xpath to extract the data or if its possible...i can successfully extract the count using java script using the code below but having issues in python.Any help and suggestions would be brilliant. 

thanks picture of twitter without javascript

twitter without javascriptYou need to check whether you have the element you are looking for, because the pages that your scraper downloads are without the elements rendered using javascript. You can check using scrapy shell(here is a link with info about scrapy shell). You can also find out the css selector using this addon or a similar one. 

Besides xpath you can use css selectors with scrapy

item.css('<selector goes here>')

how to get the comments in a html page while scraping?

Natesh bhat

[how to get the comments in a html page while scraping?](https://stackoverflow.com/questions/48849183/how-to-get-the-comments-in-a-html-page-while-scraping)

Here's the issue . im trying to scrape this facebook about page for the birthday date and when I see the page source in the browser , it shows me the birthday date as a comment in html within a div of classname class="hidden_elem" .It might that becoz of this, when I see the source code of this page in my get request using (selenium , scrapy , requests) all I get just a div with class="hidden_elem" and that comment is nowhere to be seen let alone parsing it for info.So how to get this text and if possible please show how to get the birthday dates too.There might be some javascript things which is trickily causing this by design on the facebook page. how to get around this ? Here is the URL from which im trying to get the birthday dates . 

https://www.facebook.com/profile.php?id=100004456147835&sk=aboutFrom the source page of the browser it looks like this :-When I get the page source from my script , only <div class="hidden_elem"> </div> this is coming . 

2018-02-18 06:50:51Z

Here's the issue . im trying to scrape this facebook about page for the birthday date and when I see the page source in the browser , it shows me the birthday date as a comment in html within a div of classname class="hidden_elem" .It might that becoz of this, when I see the source code of this page in my get request using (selenium , scrapy , requests) all I get just a div with class="hidden_elem" and that comment is nowhere to be seen let alone parsing it for info.So how to get this text and if possible please show how to get the birthday dates too.There might be some javascript things which is trickily causing this by design on the facebook page. how to get around this ? Here is the URL from which im trying to get the birthday dates . 

https://www.facebook.com/profile.php?id=100004456147835&sk=aboutFrom the source page of the browser it looks like this :-When I get the page source from my script , only <div class="hidden_elem"> </div> this is coming . You need to scroll down the page with:After that, you will be able to get the "hidden_elem" list of objects:With BeautifulSoup you can do thisTry this:-

Cant figure out what is wrong with this spider

vivajustice

[Cant figure out what is wrong with this spider](https://stackoverflow.com/questions/60249794/cant-figure-out-what-is-wrong-with-this-spider)

So im strugging to determine what im doing wrong here.. Looking to scrape this site and extract some details for each item - category - title - price - url. But cant figure out why the spider will not give me the results im after.If i remove the parse_details function in this spider, the spider will crawl the urls i want, yet when i call parse_details to grab some data, its breaking the crawl and not getting the full scope im looking for. (newb so please be gentle!)without my yield requestlogwith yield requestlog

2020-02-16 14:59:35Z

So im strugging to determine what im doing wrong here.. Looking to scrape this site and extract some details for each item - category - title - price - url. But cant figure out why the spider will not give me the results im after.If i remove the parse_details function in this spider, the spider will crawl the urls i want, yet when i call parse_details to grab some data, its breaking the crawl and not getting the full scope im looking for. (newb so please be gentle!)without my yield requestlogwith yield requestlogMaybe it will not resolve your problem but put it here.I put all in single file script.py as standalone script with CrawlerProcess() and run as and I get file output.csv with some data. But only on two pages it can find items. Other pages have only list of subcategories.script.pyLog on screenoutput.csvEDIT: you have to search new links also in parse_detailsAlright, so i started over..  this time i tried something different and it worked (so far anyway, still need to check for defects). But at least im getting resultsUsing LinkExtractor seems to of solved my issues. Thanks for all the help anyway.

Is there a way to get the URL that a link is scraped from?

lifeotheparty

[Is there a way to get the URL that a link is scraped from?](https://stackoverflow.com/questions/60217398/is-there-a-way-to-get-the-url-that-a-link-is-scraped-from)

I've got a spider written out that crawls my website and scrapes a bunch of tags. I'm now trying to have it return the URL that the link was discovered on.For example:www.example.com/product/123 was found on www.example.com/page/2.When scrapy scrapes information from /product/123 I want to have a field that is "Scraped From" and return /page/2. For every URL that is scraped, I'd want to find the originating page that the URL was found. I've been pouring over the docs and can't seem to figure this out. Any help would be appreciated!

2020-02-13 22:25:06Z

I've got a spider written out that crawls my website and scrapes a bunch of tags. I'm now trying to have it return the URL that the link was discovered on.For example:www.example.com/product/123 was found on www.example.com/page/2.When scrapy scrapes information from /product/123 I want to have a field that is "Scraped From" and return /page/2. For every URL that is scraped, I'd want to find the originating page that the URL was found. I've been pouring over the docs and can't seem to figure this out. Any help would be appreciated!The easiest way is to use the response.headers. There should be a referer header.You can also use meta to pass information along to the next URL.if you want the whole url i would use else use the meta if you want some part of the link

Scrapy - Extract Data from mutliple pages

chrisHG

[Scrapy - Extract Data from mutliple pages](https://stackoverflow.com/questions/60217432/scrapy-extract-data-from-mutliple-pages)

Edit-The way I got .hd-pagination__link was using a CSS selector extension for google chrome and selected the next page icon (Screenshot attached)So I've tried a few things and this is the way that made the most sense to me and I think that I'm just grabbing the wrong object for the next page. As of right now my program only grabs the data from the first page and seems like the code block to traverse pages is being ignored. I found a pattern with the URL where page numbers are denoted within increments of 24 (Maybe due to items numbers?). EX: Page 1: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=0Page 2: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=24page 3: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=48

.

.

.

.

.

.

.page n: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=[(n*24) - 24]ect.When I tried testing out code related to page numbers[incrementing the number after Na0 = x], I would just loop through the first page x amount of times. (My output would be the first page (24 items) repeated x amount of times.I've also looked into crawl spider but couldn't really understand it/implementation. Any help with my code/clarification on other methods would be appreciated!Also this is not my whole program, I'm keeping out my parseHomeDepot function because I don't think it is necessary, but if the code is needed, just let me know! 

2020-02-13 22:28:06Z

Edit-The way I got .hd-pagination__link was using a CSS selector extension for google chrome and selected the next page icon (Screenshot attached)So I've tried a few things and this is the way that made the most sense to me and I think that I'm just grabbing the wrong object for the next page. As of right now my program only grabs the data from the first page and seems like the code block to traverse pages is being ignored. I found a pattern with the URL where page numbers are denoted within increments of 24 (Maybe due to items numbers?). EX: Page 1: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=0Page 2: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=24page 3: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=48

.

.

.

.

.

.

.page n: https://www.homedepot.com/b/N-5yc1v/Ntk-ProductInfoMatch/Ntt-zline?NCNI-5&experienceName=default&Nao=[(n*24) - 24]ect.When I tried testing out code related to page numbers[incrementing the number after Na0 = x], I would just loop through the first page x amount of times. (My output would be the first page (24 items) repeated x amount of times.I've also looked into crawl spider but couldn't really understand it/implementation. Any help with my code/clarification on other methods would be appreciated!Also this is not my whole program, I'm keeping out my parseHomeDepot function because I don't think it is necessary, but if the code is needed, just let me know! Seems to me like you have a couple issues.First of all, you may be getting the whole html element that contains the link for the next page, whereas what you're looking for is the link only. So I suggest you use the css selector like so:This will get you the links instead of the whole HTML element. I suggest looking further into css selectors here.Secondly, there seems to be an issue with your code, logically.this piece of code gets you a list of all the 'next page' links on your current page, but you treat the whole list as one link. I suggest a for loop. Something like this:Now moving on, I think to better make use of Scrapy's parallel and concurrency features, you may want to return a list of 'scrapy.Requests' instead of doing a yield for every request you find. So to summarize:Good luck!Try this approach:Get current page number and using it as reference, get next page's number and then use it in url after multiplying with the counter

Python Scrapy - How to scrap from 2 different website at the same time?

Luc Semon

[Python Scrapy - How to scrap from 2 different website at the same time?](https://stackoverflow.com/questions/60152670/python-scrapy-how-to-scrap-from-2-different-website-at-the-same-time)

I need to scrap data from a list of domain given in Excel;

The problem is that I need to scrap data from the original website (let's take for example : https://www.lepetitballon.com) and data from similartech (https://www.similartech.com/websites/lepetitballon.com).I want them to scrap at the same time so I could receive them and format them once at the end, after that i'll just go to the next domain.Theoretically, I should just use 2 spiders in an asynchronous way with scrapy?

2020-02-10 14:38:56Z

I need to scrap data from a list of domain given in Excel;

The problem is that I need to scrap data from the original website (let's take for example : https://www.lepetitballon.com) and data from similartech (https://www.similartech.com/websites/lepetitballon.com).I want them to scrap at the same time so I could receive them and format them once at the end, after that i'll just go to the next domain.Theoretically, I should just use 2 spiders in an asynchronous way with scrapy?Ideally you would want to keep spiders which scrape differently structured sites separate, that way your code will be a lot easier to maintain in the long run.Theoretically, if, for some reason you MUST parse them in the same spider, you could just collect the URLs you want to scrape and based on the base path you could invoke different parser callback methods. That being said, I personally cannot think of a reason why you would have to do that. Even if you would have the same structure, you can just reuse your scrapy.Item classes.Twisted networking library is used by the scrapy framework for its internal networking tasks, and the scrapy has provided to handle the concurrent requests in settings.Explained here: https://docs.scrapy.org/en/latest/topics/settings.html#concurrent-requestsOr you could use multiple spider which are independent to each others which is already explained in scrapy docs, this might be what you are looking for.https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-processAs per the efficiency you could choose either option A or B, this really depends upon your resources and requirements whereas option A can be good for lower resources with decent speed or option B can be ideal for better speed with higher resources consumption than option A.

scrapy-splash returns no javascript version of page

Cyclehacker

[scrapy-splash returns no javascript version of page](https://stackoverflow.com/questions/60039902/scrapy-splash-returns-no-javascript-version-of-page)

I'm using scrapy-splash via docker to access this page: https://finance.yahoo.com/quote/NFLX/options?p=NFLXThe script works (sort of!), however, the page that's returned has class="NoJs chrome featurephone" specified and doesn't contain all of the fields I want to extract.These are my settings:This is my script:Initially, I'd like to extract the option expiry dates from the dropdown menu which isn't available in the none JS version of the page. I tried changing the USER_AGENT from the scrapy default to the one detailed in the settings above. I also tried switching IP addresses with a VPN in an attempt to obfuscate that the request was coming from a docker instance.I've managed to achieve what I'm aiming for using HTMLSession() from  requests_html and wanted to find out if the same was possible with Scrapy + Splash.I sense this could be due to the site identifying the request as a bot. Any suggestions on how I can get around this would be much appreciated.

2020-02-03 13:09:23Z

I'm using scrapy-splash via docker to access this page: https://finance.yahoo.com/quote/NFLX/options?p=NFLXThe script works (sort of!), however, the page that's returned has class="NoJs chrome featurephone" specified and doesn't contain all of the fields I want to extract.These are my settings:This is my script:Initially, I'd like to extract the option expiry dates from the dropdown menu which isn't available in the none JS version of the page. I tried changing the USER_AGENT from the scrapy default to the one detailed in the settings above. I also tried switching IP addresses with a VPN in an attempt to obfuscate that the request was coming from a docker instance.I've managed to achieve what I'm aiming for using HTMLSession() from  requests_html and wanted to find out if the same was possible with Scrapy + Splash.I sense this could be due to the site identifying the request as a bot. Any suggestions on how I can get around this would be much appreciated.

Scrapy: How to get address using CSS and XPath?

Sasha Podolyan

[Scrapy: How to get address using CSS and XPath?](https://stackoverflow.com/questions/59954284/scrapy-how-to-get-address-using-css-and-xpath)

How to get address using CSS and XPath? I tried to use CSS: response.css('.office-address::text').extract()

2020-01-28 17:39:40Z

How to get address using CSS and XPath? I tried to use CSS: response.css('.office-address::text').extract()Try this response.css('.office-address ::text').extract() Added space before ::textDirty solution with a one liner XPath :Output :Option with CSS selectors with scrapy:Here is future-proof idea since ids/classes can change over the period:Result:

How to collect jpeg by Scrapy

komarihatena

[How to collect jpeg by Scrapy](https://stackoverflow.com/questions/59690938/how-to-collect-jpeg-by-scrapy)

I want to collect photo of idol by Scrapy.A collecting homepage is https://news.mynavi.jp/article/20191229-947707/.I wrote spider...(save_gradol.py)I also wrote item...(items.py)I also wrote pipelines...(pipelines.py)I also wrote setteing...(settings.py)Then, I tried to spider [sudo scrapy crawl save_gradol],

but no crawling and not-collecting photos.Please help me this problem.

2020-01-11 01:36:49Z

I want to collect photo of idol by Scrapy.A collecting homepage is https://news.mynavi.jp/article/20191229-947707/.I wrote spider...(save_gradol.py)I also wrote item...(items.py)I also wrote pipelines...(pipelines.py)I also wrote setteing...(settings.py)Then, I tried to spider [sudo scrapy crawl save_gradol],

but no crawling and not-collecting photos.Please help me this problem.You can do it much easiest way:enjoy.Solutions with simplified_scrapy.You can get the examples of simplified_scrapy here

Scrapy shell does not work for https://www.target.com.au/

Edward Liu

[Scrapy shell does not work for https://www.target.com.au/](https://stackoverflow.com/questions/59639147/scrapy-shell-does-not-work-for-https-www-target-com-au)

I have tested that my shell works for other websites, but refuse to work on target.

2020-01-08 03:45:48Z

I have tested that my shell works for other websites, but refuse to work on target.It looks like the site is ignoring requests based on your user agent string.This works:

Multipart POST works with Python requests but not treq inside Crawler

R3_

[Multipart POST works with Python requests but not treq inside Crawler](https://stackoverflow.com/questions/59646434/multipart-post-works-with-python-requests-but-not-treq-inside-crawler)

I have a crawler that takes pdf files and I want to integrate with a service that generate the OCR text from those PDF files.I can make it work with requests but not with treq. All examples are from scrapy shell.This works, returning the correct OCR json:Testing, this simpler code works with treq:While this don't work, returning None:It looks like the file sent with treq is empty.

2020-01-08 13:00:53Z

I have a crawler that takes pdf files and I want to integrate with a service that generate the OCR text from those PDF files.I can make it work with requests but not with treq. All examples are from scrapy shell.This works, returning the correct OCR json:Testing, this simpler code works with treq:While this don't work, returning None:It looks like the file sent with treq is empty.

My scrapy pagination works but it just show me the first page data for all pages

Hny

[My scrapy pagination works but it just show me the first page data for all pages](https://stackoverflow.com/questions/59659798/my-scrapy-pagination-works-but-it-just-show-me-the-first-page-data-for-all-pages)

Im new to python and i mix up many example codes for my scrapy but it just show me the first page data for all pages. what is the problem?

My code is:

2020-01-09 08:25:18Z

Im new to python and i mix up many example codes for my scrapy but it just show me the first page data for all pages. what is the problem?

My code is:

Upload plugin to WordPress using scrapy

Muhammad Haseeb

[Upload plugin to WordPress using scrapy](https://stackoverflow.com/questions/59545586/upload-plugin-to-wordpress-using-scrapy)

I'm trying to upload WordPress plugin using scrapy. I'm able to log in and navigate to choose the file path but I'm unable to see file path passing in upload request. Probably it's opening and sending file content but I'm also unable to crack that too.Here's code what I have tried yet:

2019-12-31 14:53:27Z

I'm trying to upload WordPress plugin using scrapy. I'm able to log in and navigate to choose the file path but I'm unable to see file path passing in upload request. Probably it's opening and sending file content but I'm also unable to crack that too.Here's code what I have tried yet:

Spider Error Processing - Python Web Scraping Error

Kyle

[Spider Error Processing - Python Web Scraping Error](https://stackoverflow.com/questions/59546821/spider-error-processing-python-web-scraping-error)

I am new to Python and very new to web scraping, but I am trying to build a web scraper for this site: https://www.fortune.com/2019/09/23/term-sheet-monday-september-23/However, my scraper is running into issues before I am even able to get any data from the website. It kicks back:

"2019-12-31 09:37:16 [scrapy.core.scraper] ERROR: Spider error processing https://www.fortune.com/2019/09/23/term-sheet-monday-september-23/> (referer: None)". I created a scraper that is very similar that worked well. I cannot figure out why this is happening on this website.Any help or suggestions are appreciated!My spider looks like this:This is what gets outputted from running the code:

2019-12-31 17:07:37Z

I am new to Python and very new to web scraping, but I am trying to build a web scraper for this site: https://www.fortune.com/2019/09/23/term-sheet-monday-september-23/However, my scraper is running into issues before I am even able to get any data from the website. It kicks back:

"2019-12-31 09:37:16 [scrapy.core.scraper] ERROR: Spider error processing https://www.fortune.com/2019/09/23/term-sheet-monday-september-23/> (referer: None)". I created a scraper that is very similar that worked well. I cannot figure out why this is happening on this website.Any help or suggestions are appreciated!My spider looks like this:This is what gets outputted from running the code:

MY scrapy spider just show me the first two pages of site

Hny

[MY scrapy spider just show me the first two pages of site](https://stackoverflow.com/questions/59559510/my-scrapy-spider-just-show-me-the-first-two-pages-of-site)

here is my code. i'm beginner in Python and my scrapy pagination just repeat first tow pages.

2020-01-02 07:02:51Z

here is my code. i'm beginner in Python and my scrapy pagination just repeat first tow pages.

Scrapy: the execution precedence

mrbf

[Scrapy: the execution precedence](https://stackoverflow.com/questions/59475430/scrapy-the-execution-precedence)

I'm using the following spider for crawling a website that requires an authentication.The problem is that Line2 executes before Line1. So despite the captcha not yet downloaded, it prompts to get captcha value by user!

What am I doing wrong and how I can solve the problem?

2019-12-25 05:18:27Z

I'm using the following spider for crawling a website that requires an authentication.The problem is that Line2 executes before Line1. So despite the captcha not yet downloaded, it prompts to get captcha value by user!

What am I doing wrong and how I can solve the problem?

Scrapy: Batch processing before spider closes down

Joey Coder

[Scrapy: Batch processing before spider closes down](https://stackoverflow.com/questions/59486245/scrapy-batch-processing-before-spider-closes-down)

I use Scrapy to collect new contacts into my Hubspot account. Now I started to use the pipeline. However, that would lead to a lot of API calls, as each item is handled by itself. Since Hubspot also has an API call for batch processing, I wonder if there is a way to access all items at the end, once my crawler is done.

pipelines.py

2019-12-26 09:16:24Z

I use Scrapy to collect new contacts into my Hubspot account. Now I started to use the pipeline. However, that would lead to a lot of API calls, as each item is handled by itself. Since Hubspot also has an API call for batch processing, I wonder if there is a way to access all items at the end, once my crawler is done.

pipelines.pyYou could implement batching in your pipeline: have the pipeline store input items in a local variable, flush them every N items, and use close_spider to flush the last batch. 

Is there a way to scrape data that is loading using python [closed]

srinivas muralidharan

[Is there a way to scrape data that is loading using python [closed]](https://stackoverflow.com/questions/59425294/is-there-a-way-to-scrape-data-that-is-loading-using-python)

I was working on a data scraping from a website. I found that the table data is displayed as loading in the page's source code. I am wondering how to collect that data using python. It seems to be a react js web app. 

2019-12-20 12:29:52Z

I was working on a data scraping from a website. I found that the table data is displayed as loading in the page's source code. I am wondering how to collect that data using python. It seems to be a react js web app. If you Go To NetWork Tab you will find below API which returns data in json format.

You don't need selenium or beautifulsoup.Here is the code below.Snapshot Of APIResponse:Can't find it as a request under XHR, so you could use Selenium which will allow the page to render, and then grab the table with pandas:Output:

Scraping pages with different data element and all span tag have the same name

adrian

[Scraping pages with different data element and all span tag have the same name](https://stackoverflow.com/questions/59342171/scraping-pages-with-different-data-element-and-all-span-tag-have-the-same-name)

The problem I am facing is that different pages have different data elements, for ex: link 1 and link 2. As you can see, first link has applicant but not the second link, and there are many more under the project description. And they use the same span class and value. So how do I scrape them under the variable I want?

For example: I want to get applicant data here. But i couldn't use the div[index] format because different pages will have different data elements. The span class and value are the same so I couldn't get the data I want using the specific name.My attempt on the 2nd link: html code for  1st link:html code for 2nd link:I want it like this:

2019-12-15 08:04:07Z

The problem I am facing is that different pages have different data elements, for ex: link 1 and link 2. As you can see, first link has applicant but not the second link, and there are many more under the project description. And they use the same span class and value. So how do I scrape them under the variable I want?

For example: I want to get applicant data here. But i couldn't use the div[index] format because different pages will have different data elements. The span class and value are the same so I couldn't get the data I want using the specific name.My attempt on the 2nd link: html code for  1st link:html code for 2nd link:I want it like this:Let say you want a JSON output:As a output you will have a JSON object contains class="name" as names and class="value" as values.

Update: for issue you mentioned in comment parse() would be:res value at the end:

Scrapy query is returning an empty list

terminatorash2199

[Scrapy query is returning an empty list](https://stackoverflow.com/questions/59343833/scrapy-query-is-returning-an-empty-list)

I want to scrape a website to the links. 

https://www.rentomojo.com/mumbai/furniture/bedroom-furniture-on-rent

The link is the href link inside div. 

My scrapy code is but this does not work.I even tried using xpathBut this also does not work.Any help would be appreciated.

2019-12-15 12:18:45Z

I want to scrape a website to the links. 

https://www.rentomojo.com/mumbai/furniture/bedroom-furniture-on-rent

The link is the href link inside div. 

My scrapy code is but this does not work.I even tried using xpathBut this also does not work.Any help would be appreciated.I tried your xpath code in bash:and it work fine.Code with cssreturn nothing.Simply write response.css(".col-xs-6 ::attr(href)").extract()No need to write HTML tags within the selector when the class or id is present. Similarly no need to write a tag ::attr(href) is enough for extracting links. 

Getting a list of multiple elements with same class name using Scrapy

Bernardo García

[Getting a list of multiple elements with same class name using Scrapy](https://stackoverflow.com/questions/59293394/getting-a-list-of-multiple-elements-with-same-class-name-using-scrapy)

There's a website I want to scrape that has numerous divs with the same class name.For example, let's say this class name is article-container

There are 12 divs holding this class name in the site. I tried to retrieve them all like this:And then call them individually using something like:But that didn't work. What's the best approach to solve this?

2019-12-11 20:15:03Z

There's a website I want to scrape that has numerous divs with the same class name.For example, let's say this class name is article-container

There are 12 divs holding this class name in the site. I tried to retrieve them all like this:And then call them individually using something like:But that didn't work. What's the best approach to solve this?get the div first, by name, this will render a list type data structureyou can then loop through this list with which also means you should be able to use indexes on the list and also get and extract the text of the class if you so desire with the 

Refused to load the script because it violates the following Content Security Policy directive: script-src error with ChromeDriver Chrome and Selenium

Rao Tauqeer Sajid

[Refused to load the script because it violates the following Content Security Policy directive: script-src error with ChromeDriver Chrome and Selenium](https://stackoverflow.com/questions/59207838/refused-to-load-the-script-because-it-violates-the-following-content-security-po)

I am trying to scrape Phone Number from these links "https://www.practo.com/delhi/doctor/dr-meeka-gulati-dentist-3?specialization=Dentist&practice_id=722421" and "https://www.practo.com/delhi/doctor/dr-rajeev-puri-ear-nose-throat-ent-specialist?specialization=Ear-Nose-Throat%20(ENT)%20Specialist&practice_id=912154"if element present it scrapes the phone number otherwise phone number is NoneSpider Code:Output

2019-12-06 06:14:51Z

I am trying to scrape Phone Number from these links "https://www.practo.com/delhi/doctor/dr-meeka-gulati-dentist-3?specialization=Dentist&practice_id=722421" and "https://www.practo.com/delhi/doctor/dr-rajeev-puri-ear-nose-throat-ent-specialist?specialization=Ear-Nose-Throat%20(ENT)%20Specialist&practice_id=912154"if element present it scrapes the phone number otherwise phone number is NoneSpider Code:OutputThis error message......implies that the ChromeDriver was unable to initiate/spawn a new Browsing Context i.e. Chrome Browser session.To mitigate the cross-site scripting issues Chrome's extension system has implemented the concept of Content Security Policy (CSP) which introduces some strict policies that will make extensions more secure by default and provides us the ability to create and enforce rules governing the types of content that can be loaded and executed by your extensions and applications. CSP works as a block/allowlisting mechanism for resources loaded or executed by your extensions. Defining a reasonable policy for your extension enables you to consider the resources that your extension requires and to negotiate with the browser to ensure that those are the only resources your extension has access to. These policies provide security even above the host permissions your extension requests acting as an additional layer of protection. Such policies are defined via an HTTP header or meta element. Within Chrome's extension system the extension's policy is defined via the extension's manifest.json file as follows:Till Chrome 45, there was no mechanism for relaxing the restriction against executing inline JavaScript. In particular, setting a script policy that includes 'unsafe-inline' will have no effect. However, from Chrome 46 onwards, inline scripts can be allowed by specifying the base64-encoded hash of the source code in the policy. This hash must be prefixed by the used hash algorithm (sha256, sha384 or sha512). This can be achived by setting adding http://* to both style-src and/or script-src as follows:and/orHowever I was able to access the webpage https://www.practo.com/delhi/doctor/dr-rajeev-puri-ear-nose-throat-ent-specialist?specialization=Ear-Nose-Throat%20(ENT)%20Specialist&practice_id=912154 easily as follows:Ensure that:You can find a relevant discussion in Call to eval() blocked by CSP with Selenium IDEThere's actually a chrome devtools protocal command for this but it's marked experimental:

Trying to get links from web

T Tallos

[Trying to get links from web](https://stackoverflow.com/questions/59110252/trying-to-get-links-from-web)

The code only gets 1 URL listing then stops. It is supposed to get 209 items. Why does it only print one line?

2019-11-29 19:46:15Z

The code only gets 1 URL listing then stops. It is supposed to get 209 items. Why does it only print one line?You have incorrect indents (you need to move yield inside for loop):

Extract articles from its corresponding links from a webpage using scrapy

Shruti Dhruv

[Extract articles from its corresponding links from a webpage using scrapy](https://stackoverflow.com/questions/59115746/extract-articles-from-its-corresponding-links-from-a-webpage-using-scrapy)

Hi I am new to scrapy and I am Trying to extract text from links in a given webpage. Here is the code I wrote for the same and after running scrapy crawl article, it gives no module named article. Can you help me find where I am wrong? Thanks in advance.

2019-11-30 11:32:13Z

Hi I am new to scrapy and I am Trying to extract text from links in a given webpage. Here is the code I wrote for the same and after running scrapy crawl article, it gives no module named article. Can you help me find where I am wrong? Thanks in advance.If you take a look at your log you'll see 'offsite/filtered': 211, and that the cause of not getting anything. In order to dodge this you can do two things:I tested your code it does not seems to work properly if you want to get text body so i rewrote it with XPath which I am more comfortable with.

scraper not scraping field “Description”

Davey Boy

[scraper not scraping field “Description”](https://stackoverflow.com/questions/59127552/scraper-not-scraping-field-description)

I have a web scraper coded for me using scrapy.I wish to add an extra field from the website the scraper is scraping from.The column header "Description" is created in the CSV database but nothing is scraped.Thanks"It looks like your post is mostly code; please add some more details." Sorry. :(

"It looks like your post is mostly code; please add some more details." Sorry. :(

2019-12-01 16:28:44Z

I have a web scraper coded for me using scrapy.I wish to add an extra field from the website the scraper is scraping from.The column header "Description" is created in the CSV database but nothing is scraped.Thanks"It looks like your post is mostly code; please add some more details." Sorry. :(

"It looks like your post is mostly code; please add some more details." Sorry. :(Try replace your temp4:on:in <h4>Description</h4> you haven't <ul><li> tags, only <p>

SplashRequest - Cannot get data attribute

RhymeGuy

[SplashRequest - Cannot get data attribute](https://stackoverflow.com/questions/59011139/splashrequest-cannot-get-data-attribute)

I'm strugling to find out why I receive error:From documentation:Here is my sample code:Splash rendering server is running on http://0.0.0.0:8050/ and I can receive data from it when I issue curl command from console.

2019-11-23 19:03:32Z

I'm strugling to find out why I receive error:From documentation:Here is my sample code:Splash rendering server is running on http://0.0.0.0:8050/ and I can receive data from it when I issue curl command from console.

scrapy: restricting link extraction to the request domain

AdamF

[scrapy: restricting link extraction to the request domain](https://stackoverflow.com/questions/58961758/scrapy-restricting-link-extraction-to-the-request-domain)

I have a scrapy project which uses a list of URLs from different domains as the seeds, but for any given page, I only want to follow links in the same domain as that page's URL (so the usual LinkExtractor(accept='example.com') approach wouldn't work. I'm surprised I couldn't find a solution on the web, as I'd expect this to be a common task.  The best I could come up with was this in the spider file and refer to it in the Rules:But that doesn't work (the spider goes off-domain).Now I'm trying to use the process_request option in the Rule:andbut I get an exception because it's passing self to the method (the spider has no url attribute); when I add self to the method signature, I get an exception that the response positional argument is missing! If I change the callback to process_request=self.check_r_r_domains, I get an error because self isn't defined where I set the rules!

2019-11-20 19:14:39Z

I have a scrapy project which uses a list of URLs from different domains as the seeds, but for any given page, I only want to follow links in the same domain as that page's URL (so the usual LinkExtractor(accept='example.com') approach wouldn't work. I'm surprised I couldn't find a solution on the web, as I'd expect this to be a common task.  The best I could come up with was this in the spider file and refer to it in the Rules:But that doesn't work (the spider goes off-domain).Now I'm trying to use the process_request option in the Rule:andbut I get an exception because it's passing self to the method (the spider has no url attribute); when I add self to the method signature, I get an exception that the response positional argument is missing! If I change the callback to process_request=self.check_r_r_domains, I get an error because self isn't defined where I set the rules!If you are using Scrapy 1.7.0 or later, you can pass Rule a callable, process_request, to check the URLs of both the request and the response, and drop the request (return None) if the domains do not match.Oops, it turns out that conda on the server I'm using had installed a 1.6 version of scrapy.  I've forced it to install 1.8.0 from conda-forge and I think it's working now.

Using Scrapy to scrape ASP.NET pages using VIEWSTATE

Phillis Peters

[Using Scrapy to scrape ASP.NET pages using VIEWSTATE](https://stackoverflow.com/questions/58901183/using-scrapy-to-scrape-asp-net-pages-using-viewstate)

I followed this post SCRAPING WEBSITES BASED ON VIEWSTATES WITH SCRAPY to scrape a site that is almost identical. It works well but the problem is that my site has many items and thus has a lot of pagination. I am able to go to the next pages but only if they are viewable from the page I am on. Pagination is up to 10 pages, which means that the ViewState for page 1 only works for the first set when I go to the next set, say page 14, it is unable to get the data since it still uses ViewState from page 1. Here is the code: First I get to page 1 then use it to go to the last page to determine the number of pages. Then I loop through each page. In the loop, the response passed is from the last page which only works for the last 10 pages which are visible from the last page in the pagination. EDIT

how to scrape a page request using Viewstate parameter? This question explains what I am already doing. My issue is not how to fetch the ViewState from the response and pass it to the next request. I can already achieve that. My issue is that I need to update the response within the loop so that it passes the ViewState for the previous page. Right now it's only passing the last page whose view state expires after like 10 pages.The site I am scraping is https://www.mevzuat.gov.tr/Kanunlar.aspx

2019-11-17 13:56:29Z

I followed this post SCRAPING WEBSITES BASED ON VIEWSTATES WITH SCRAPY to scrape a site that is almost identical. It works well but the problem is that my site has many items and thus has a lot of pagination. I am able to go to the next pages but only if they are viewable from the page I am on. Pagination is up to 10 pages, which means that the ViewState for page 1 only works for the first set when I go to the next set, say page 14, it is unable to get the data since it still uses ViewState from page 1. Here is the code: First I get to page 1 then use it to go to the last page to determine the number of pages. Then I loop through each page. In the loop, the response passed is from the last page which only works for the last 10 pages which are visible from the last page in the pagination. EDIT

how to scrape a page request using Viewstate parameter? This question explains what I am already doing. My issue is not how to fetch the ViewState from the response and pass it to the next request. I can already achieve that. My issue is that I need to update the response within the loop so that it passes the ViewState for the previous page. Right now it's only passing the last page whose view state expires after like 10 pages.The site I am scraping is https://www.mevzuat.gov.tr/Kanunlar.aspx

Why subprocess.check_output not working when running as service

thinkmore

[Why subprocess.check_output not working when running as service](https://stackoverflow.com/questions/58902294/why-subprocess-check-output-not-working-when-running-as-service)

I'm trying to make python scrapy that use flask running in vps server.

Python scrapy runs well when use following command.That works well and I got the desired result.

But if i close the ssh(putty), the command is closed with that.

So I'm trying to run command as service.

I've done all needed to run as service.

And This command works well, but while running following error occurs.The related code is as follows.Why this happens although that works well when runs as shell?

Please help me.here is the error log(journalctl -u xxx.service)

2019-11-17 15:52:14Z

I'm trying to make python scrapy that use flask running in vps server.

Python scrapy runs well when use following command.That works well and I got the desired result.

But if i close the ssh(putty), the command is closed with that.

So I'm trying to run command as service.

I've done all needed to run as service.

And This command works well, but while running following error occurs.The related code is as follows.Why this happens although that works well when runs as shell?

Please help me.here is the error log(journalctl -u xxx.service)

Scrapy Gives None as output

Lex

[Scrapy Gives None as output](https://stackoverflow.com/questions/58855014/scrapy-gives-none-as-output)

I am hoping you could help me on a scraping script. 

From Chrome, I have confirmed the XPath is correct.                                             I am using XPATH selector for the script:`

BUT, when I try to output the Scraped Companyname I am get 'None'. I am not sure why this is the case. Could it be because of .php? Any workaropund I will appreciate 

2019-11-14 10:50:10Z

I am hoping you could help me on a scraping script. 

From Chrome, I have confirmed the XPath is correct.                                             I am using XPATH selector for the script:`

BUT, when I try to output the Scraped Companyname I am get 'None'. I am not sure why this is the case. Could it be because of .php? Any workaropund I will appreciate The range is starting from zero in your code, it's fine if intentional. Next you can use response.xpath('//table[3]/tbody/tr[1]/td[2]').extract().If my answer is wrong, please provide the URL for the page you wish to scrape for better answer.

How to make sure, that the request is made from the Rotating IPs?

yajant b

[How to make sure, that the request is made from the Rotating IPs?](https://stackoverflow.com/questions/58855702/how-to-make-sure-that-the-request-is-made-from-the-rotating-ips)

I am working on scrapy, where I am using a concept of Rotating IPs and randomly changing the UserAgents,

However, I am not sure if the request that is being made is from the Different IP, or from my local machine's IP?What I tried to do is:As every request passes through a middleware's "process_request(self, request, spider)" method,

I have made this  logic of randomly taking the Proxy IPs from the Free Proxy API, write it into a file, then

read the Random IPs from the file, append it to a list and take random IP from the list, and append it to meta with key 'http_proxy'.Below is the logic, that I am currently using for rotating IPs:I am not sure if Is it the correct way to set Random IP in meta as:should I use 'http_proxy, 'proxy'? , I have searched about it, but could not find a correct way to add IP in a request?Is there any way to know, that every request that I make is from a specific Proxy IP and not from my local IP? If Yes, what is the correct way to Rotate IPs in scrapy ?

2019-11-14 11:25:35Z

I am working on scrapy, where I am using a concept of Rotating IPs and randomly changing the UserAgents,

However, I am not sure if the request that is being made is from the Different IP, or from my local machine's IP?What I tried to do is:As every request passes through a middleware's "process_request(self, request, spider)" method,

I have made this  logic of randomly taking the Proxy IPs from the Free Proxy API, write it into a file, then

read the Random IPs from the file, append it to a list and take random IP from the list, and append it to meta with key 'http_proxy'.Below is the logic, that I am currently using for rotating IPs:I am not sure if Is it the correct way to set Random IP in meta as:should I use 'http_proxy, 'proxy'? , I have searched about it, but could not find a correct way to add IP in a request?Is there any way to know, that every request that I make is from a specific Proxy IP and not from my local IP? If Yes, what is the correct way to Rotate IPs in scrapy ?To check the IP with which servers associate your requests, send a few requests to https://httpbin.org/ip and check the IP from the response.

Scrapy select HTML elements that have specific attribute name

hydradon

[Scrapy select HTML elements that have specific attribute name](https://stackoverflow.com/questions/58698132/scrapy-select-html-elements-that-have-specific-attribute-name)

There is this HTML:I need to select the inner div that have the attribute data-id (regardless of values) only. How do I achieve that with Scrapy?

2019-11-04 17:11:03Z

There is this HTML:I need to select the inner div that have the attribute data-id (regardless of values) only. How do I achieve that with Scrapy?You can use the followingIt will give you a list of all divs with data-id attribute.Use BeautifulSoup. CodeOUTPUT:You can specify which attribute to be present in find or find_all with the value as TrueTake look and above example HTML code.

To get all div containing data-class in Scrapy v1.6+In scrapy version <1.6 you can use extract() in place of getall().

Hope this helps 

How to dynamically add jobs to a Scrapy/Selenium spider?

Maxim  Biryukov

[How to dynamically add jobs to a Scrapy/Selenium spider?](https://stackoverflow.com/questions/58440171/how-to-dynamically-add-jobs-to-a-scrapy-selenium-spider)

I'm using Scrapy + Selenium to scrape Facebook friends data. I want to write a program that would take 2 usernames as input, recursively scrape their friendlists until a chain that connects the 2 users is found. My spider can login, generate Items and send them to MongoDB (through a Pipeline). I have a working function that can make a query to Mongo to check if a chain exists and return it. My take on the algorithm:The login process is just a function that creates a Selenium instance, successfully logins and returns the webdriver instance, that is further passed into the spider.At the moment I can't understand how to organise the QUEUE process. I can pass a single username or a list into the spider, but can't figure out how to pass more jobs into the same spider depending on the results of the crawling.I start the crawl for a single user as follows:How would I go about adding new usernames to the QUEUE (right after one crawl is finished and the Item is processed) without stopping the spider? Or maybe the approach should be 'wait for spider to finish job'->'create new one'?I've tried digging through the Scrapy documentation, but for a newbie like me it was overwhelming. I tried to use CrawlerRunner, but couldn't get it to to work.

2019-10-17 20:18:36Z

I'm using Scrapy + Selenium to scrape Facebook friends data. I want to write a program that would take 2 usernames as input, recursively scrape their friendlists until a chain that connects the 2 users is found. My spider can login, generate Items and send them to MongoDB (through a Pipeline). I have a working function that can make a query to Mongo to check if a chain exists and return it. My take on the algorithm:The login process is just a function that creates a Selenium instance, successfully logins and returns the webdriver instance, that is further passed into the spider.At the moment I can't understand how to organise the QUEUE process. I can pass a single username or a list into the spider, but can't figure out how to pass more jobs into the same spider depending on the results of the crawling.I start the crawl for a single user as follows:How would I go about adding new usernames to the QUEUE (right after one crawl is finished and the Item is processed) without stopping the spider? Or maybe the approach should be 'wait for spider to finish job'->'create new one'?I've tried digging through the Scrapy documentation, but for a newbie like me it was overwhelming. I tried to use CrawlerRunner, but couldn't get it to to work.

Scrapy: How do I get text and text with <b> tag at the same time when using scrapy and xpath?

AKN

[Scrapy: How do I get text and text with <b> tag at the same time when using scrapy and xpath?](https://stackoverflow.com/questions/58433912/scrapy-how-do-i-get-text-and-text-with-b-tag-at-the-same-time-when-using-scra)

I need to get 183.7 from the html belowbut if run below code with scrapy shell mode, only '.7' is availableHow shall I write the code to get complete number?I have read Scrapy tutial at http://doc.scrapy.org/en/1.7/topics/selectors.html#topics-selectors

but it is still hard for me to understand the right xpath setting to get values I need.If I try it returnswhich is also not what exactly I need.

2019-10-17 13:30:37Z

I need to get 183.7 from the html belowbut if run below code with scrapy shell mode, only '.7' is availableHow shall I write the code to get complete number?I have read Scrapy tutial at http://doc.scrapy.org/en/1.7/topics/selectors.html#topics-selectors

but it is still hard for me to understand the right xpath setting to get values I need.If I try it returnswhich is also not what exactly I need.you can use the "//" to get all child text on the element like this:

Scrapy didn't produce output correctly for a different Base URL?

Jovan Geraldy Candra

[Scrapy didn't produce output correctly for a different Base URL?](https://stackoverflow.com/questions/58444936/scrapy-didnt-produce-output-correctly-for-a-different-base-url)

I am Still Beginner and learning ScrapySo I am making Scrapy script to scrape an amount of links in rumah123.com, exactly at https://www.rumah123.com/en/sale/surabaya/surabaya-kota/all-residential/, and it turns out a success! it produces csv of linksBut when I changed link at https://www.rumah123.com/en/rent/surabaya/surabaya-kota/all-residential/, My Scrapy Script didn't produce anythingWhen I run the script, Scrapy Log exactly says:But when I check the real csv, it contains nothing inside!This is the Scripts whole code:The Expected Results is like when in the First try of URL, here is a screenshot of the links:https://imgur.com/eynTo5WThe Current Result for the "rent" URL is empty, here is a screenshot:https://imgur.com/a/iUdRUDtExtra Note: I tested to run using scrapy shell https://www.rumah123.com/en/sale/surabaya/surabaya-kota/all-residential/, if i run the code manually, it can produces the CSV directly, but it will be very tiring by running code one on one :(Can Anyone point me why this can happen? Thankyou :)

2019-10-18 06:13:54Z

I am Still Beginner and learning ScrapySo I am making Scrapy script to scrape an amount of links in rumah123.com, exactly at https://www.rumah123.com/en/sale/surabaya/surabaya-kota/all-residential/, and it turns out a success! it produces csv of linksBut when I changed link at https://www.rumah123.com/en/rent/surabaya/surabaya-kota/all-residential/, My Scrapy Script didn't produce anythingWhen I run the script, Scrapy Log exactly says:But when I check the real csv, it contains nothing inside!This is the Scripts whole code:The Expected Results is like when in the First try of URL, here is a screenshot of the links:https://imgur.com/eynTo5WThe Current Result for the "rent" URL is empty, here is a screenshot:https://imgur.com/a/iUdRUDtExtra Note: I tested to run using scrapy shell https://www.rumah123.com/en/sale/surabaya/surabaya-kota/all-residential/, if i run the code manually, it can produces the CSV directly, but it will be very tiring by running code one on one :(Can Anyone point me why this can happen? Thankyou :)Extracting url in our spider

    import scrapyThe simplest way to store the scraped data is by using Feed exports, with the following command:The issue is your spider is not yielding anything.You can try the following parse method I have found out the issues! By changing the loops from 10 to something bigger like 30, My CSV is now filled with the URL list! although I don't really know why this works

Scrapy keeps hanging on depth limit

Hussar

[Scrapy keeps hanging on depth limit](https://stackoverflow.com/questions/58315875/scrapy-keeps-hanging-on-depth-limit)

I'm trying to crawl a list of sites and have set a depth limit of 5. Scrapy is not able to crawl for more than a few minutes without getting an endless number of these messages: DEBUG: Ignoring link (depth > 5) which seams to go on for hours on the same site. Am I misunderstanding the way that the depth middleware should be used or is this a problem with scrapy? The code prevents the link extractor from navigating outside of the site by creating a list of rules using the site URLs. I'm including the code for it below because I suspect it may be part of the problem. 

2019-10-10 05:17:59Z

I'm trying to crawl a list of sites and have set a depth limit of 5. Scrapy is not able to crawl for more than a few minutes without getting an endless number of these messages: DEBUG: Ignoring link (depth > 5) which seams to go on for hours on the same site. Am I misunderstanding the way that the depth middleware should be used or is this a problem with scrapy? The code prevents the link extractor from navigating outside of the site by creating a list of rules using the site URLs. I'm including the code for it below because I suspect it may be part of the problem. 

Retrieve value from span class XPath

saraherceg

[Retrieve value from span class XPath](https://stackoverflow.com/questions/58270432/retrieve-value-from-span-class-xpath)

I am trying to scrape some information from this website https://www.gumtree.co.za (https://www.gumtree.co.za/a-house-rentals-flat-rentals-offered/tamboerskloof/studio-flatlet-in-tamboerskloof/1005754794350910092234609 this is the link of the property I am taking information from); more specifically I am trying to take information from these span classes:I first want to check if the span class has Bathroom in it and then take the value for that. This is what I have right now:However, I do not get anything.Any suggestions? Thank you!

2019-10-07 13:20:01Z

I am trying to scrape some information from this website https://www.gumtree.co.za (https://www.gumtree.co.za/a-house-rentals-flat-rentals-offered/tamboerskloof/studio-flatlet-in-tamboerskloof/1005754794350910092234609 this is the link of the property I am taking information from); more specifically I am trying to take information from these span classes:I first want to check if the span class has Bathroom in it and then take the value for that. This is what I have right now:However, I do not get anything.Any suggestions? Thank you!This is the correct way to extract all the siblings.

Bathrooms=response.xpath("//span[contains(text(),'Bathrooms')]/following-sibling::*").extract_first()For more, you can refer to this: XPath Axes

Hope this helps.

How to Print Scrapy Depth

przucidlo

[How to Print Scrapy Depth](https://stackoverflow.com/questions/58206508/how-to-print-scrapy-depth)

So I'm scraping through a site and am looking for a specific link in the site. If I start on the home page I want to see how far from the original page the spider got before it found the linkLinkdepth is currently set to 10 as a setting, but if it finds the link before it goes 10 out, i want to know how far it had to travel to get there. Is there something like a 'link depth' variable that scrapy has inherently that i can call about where in the process it is or do i have to make a counter?

2019-10-02 17:33:17Z

So I'm scraping through a site and am looking for a specific link in the site. If I start on the home page I want to see how far from the original page the spider got before it found the linkLinkdepth is currently set to 10 as a setting, but if it finds the link before it goes 10 out, i want to know how far it had to travel to get there. Is there something like a 'link depth' variable that scrapy has inherently that i can call about where in the process it is or do i have to make a counter?I am foolish. It's just  response.meta['depth']so you can set that as a variable. 

How to scrape data from <script> element using Scrapy

Bashar Abdullah

[How to scrape data from <script> element using Scrapy](https://stackoverflow.com/questions/58154564/how-to-scrape-data-from-script-element-using-scrapy)

I'm trying to scrape image urls from magento run site. Product photo urls are listed in What I want is the full values. I'm not Python guru, but I was able to get the inside of the script element into a dict object. Not sure if this is the right step or not, and how to proceed from there.Any tips?

2019-09-29 11:01:03Z

I'm trying to scrape image urls from magento run site. Product photo urls are listed in What I want is the full values. I'm not Python guru, but I was able to get the inside of the script element into a dict object. Not sure if this is the right step or not, and how to proceed from there.Any tips?

Scrapy isnt scraping the next page

Hozayfa El Rifai

[Scrapy isnt scraping the next page](https://stackoverflow.com/questions/58162318/scrapy-isnt-scraping-the-next-page)

I am trying to scrape article news from skynewsarabia.com The basic idea here is that you first scrape a link which has 20 links. besides that, the first link has also a token for the next link which you need to add to the next URL so you can scrape the next 20 links.  However, the problem I am facing is that when you first run the script, it is taking the next token and get all the links of that token and then it stops! so I am just scraping 20 links only! when I print the first_token it's giving me something different than 1569266773000 which is provided by default in the script.

2019-09-30 06:11:49Z

I am trying to scrape article news from skynewsarabia.com The basic idea here is that you first scrape a link which has 20 links. besides that, the first link has also a token for the next link which you need to add to the next URL so you can scrape the next 20 links.  However, the problem I am facing is that when you first run the script, it is taking the next token and get all the links of that token and then it stops! so I am just scraping 20 links only! when I print the first_token it's giving me something different than 1569266773000 which is provided by default in the script.You need to change allowed_domains = ['www.skynewsarabia.com'] to allowed_domains = ['skynewsarabia.com']. Alternatively remove the allowed_domains variable completely.Since you have specified the hostname www Scrapy filters the requests to api.skynewsarabia.com as offsite and the calls are just being dropped.Additional tip: Try to use self.logger.info and self.logger.debug instead of the print commands in your code.

How to scrap 2 level web page from a website

HappyMan

[How to scrap 2 level web page from a website](https://stackoverflow.com/questions/58119118/how-to-scrap-2-level-web-page-from-a-website)

I would like to ask you how to scrap web pages using python + Beautiful soup or Scrapy that contains for example job announcements if there are 2 level pages, a short description + a LINK to a full detail of the job post ? I need to scrap data from the title of the jobs announcements then go deeper and extract full description and add that data to a database or text file ? the problem is going to the second level where the full description lives and get the full detail, including image links if exists...

Anyone have done that ? Thank you in advance.

2019-09-26 14:31:27Z

I would like to ask you how to scrap web pages using python + Beautiful soup or Scrapy that contains for example job announcements if there are 2 level pages, a short description + a LINK to a full detail of the job post ? I need to scrap data from the title of the jobs announcements then go deeper and extract full description and add that data to a database or text file ? the problem is going to the second level where the full description lives and get the full detail, including image links if exists...

Anyone have done that ? Thank you in advance.check:if I understand what you want to do, I would save what you need... companyName, description, secondlinks, etc then for each company saved, make a request to secondLinks and save/set information

Scrapy & Selenium: How to call a method in a loop

AppliedResearcher

[Scrapy & Selenium: How to call a method in a loop](https://stackoverflow.com/questions/58056234/scrapy-selenium-how-to-call-a-method-in-a-loop)

I already found similar questions here, but my crawler is still not running. I am trying to crawl several URLs which I extract from a txt.file. 

This works properly. However, scrapy/selenium opens the browser for each URL one after the other  but does not run the "crawltips"-function. Only for the last URL in my txt.file the code in def crawltips(self, response): is executed.How can I call the "crawltips"-function for each URL in my txt.file? 

2019-09-23 05:39:39Z

I already found similar questions here, but my crawler is still not running. I am trying to crawl several URLs which I extract from a txt.file. 

This works properly. However, scrapy/selenium opens the browser for each URL one after the other  but does not run the "crawltips"-function. Only for the last URL in my txt.file the code in def crawltips(self, response): is executed.How can I call the "crawltips"-function for each URL in my txt.file? From my understanding, WebDriver can focus only on one tab (window) at a time. While running the loop, the last URL is selected and executing the functions there.For the solution, you have to find the number of tabs based on the URL count and switch back to each URL after completion of the next one.Eg: 

Regex Expression wrong

AppliedResearcher

[Regex Expression wrong](https://stackoverflow.com/questions/58032289/regex-expression-wrong)

I am trying to extract the country (here Indonesia) from the following phrase:At the moment, I am just using the following command to extract the text:What is the right regex command to extract just Indonesia?

2019-09-20 16:42:33Z

I am trying to extract the country (here Indonesia) from the following phrase:At the moment, I am just using the following command to extract the text:What is the right regex command to extract just Indonesia?You can use this XPath-1.0 expression:which has the result Indonesia /.

If you want to get rid of the slash, you have several possibilities:There are other XPath expressions that would work as well. Choose the one that fits best in your situation. The leading dot before the // is only necessary if you designate a relative path to the current node. In the above expressions I did assume that the lookup is global.And, of course, these XPath expressions have to be surrounded byMaybe, from bs4 importing BeautifulSoup, we could extract the country, if that'd be OK:The question is, how much do you know about the input? You obviously don't know that it contains "Indonesia", but are all the other parts of the input completely fixed? For example, does the text you are looking for always immediately follow a span element with content Football /?If that's the case then you can do

Retrieve full url using Scrapy and Xpath

Pierre Alvarez

[Retrieve full url using Scrapy and Xpath](https://stackoverflow.com/questions/57954395/retrieve-full-url-using-scrapy-and-xpath)

I'm using Scrapy to crawl a webpage. I'm interested in recovering a "complex" URL in this source code :The xpath command I use is :However, I get only  "/searchresults.ja.html" ==> Everything after the ".html" is dumped. I'm not interested in recovering the domain name, but the complex part after the ".hmtl?"What I would like to have is Do you know what I should do ?By the way the page is this one, and I'm trying to get the "next page" of results, at the bottom

2019-09-16 09:56:13Z

I'm using Scrapy to crawl a webpage. I'm interested in recovering a "complex" URL in this source code :The xpath command I use is :However, I get only  "/searchresults.ja.html" ==> Everything after the ".html" is dumped. I'm not interested in recovering the domain name, but the complex part after the ".hmtl?"What I would like to have is Do you know what I should do ?By the way the page is this one, and I'm trying to get the "next page" of results, at the bottomThe website is using JavaScript to render the next URL. The easiest way to check whether you can scrape anything directly without using JavaScript is using scrapy shell 'website' in your terminal (navigate to the directory where your scrapy spider is using the terminal and then execute the command. Check this image for execution of scrapy shellThis will open the response of the website in your terminal. Then you can type commands and check what the response is. In your case, the command will be: Or As you can see, the links are not complete as per your reference in the question. Hence, this proves that the link you're trying to extract cannot be extracted using the straightforward method. You can use Splash (for JS rendering) or manually inspect the request and then duplicate the request using the Request module in scrapy. 

Fatal error in launcher: Unable to creat process using “c:\bld\scrapy_345323\_h_env\python.exe”

Marcelo 

[Fatal error in launcher: Unable to creat process using “c:\bld\scrapy_345323\_h_env\python.exe”](https://stackoverflow.com/questions/57923698/fatal-error-in-launcher-unable-to-creat-process-using-c-bld-scrapy-345323-h)

I installed anaconda and python on Windows 7 without errors.

I tried to run an example from scrapy.org. I got an error: How can I resolve this?

2019-09-13 12:30:30Z

I installed anaconda and python on Windows 7 without errors.

I tried to run an example from scrapy.org. I got an error: How can I resolve this?Well for anyone that would like to create his first bot from scrapy.org I can confirm the solution is as follows:first uninstall scrapy from conda and use pip instead as follows:I had an astroid 2.2.5 problem, but reinstallation succedeed

and I ran the script succesfullyAnyway you may experience other problems running the example provided quotes_spider.py, then I suggest trying locating your spider .py to the same folder as your anaconda prompt (in my case,c:\users\bla bla)running the anaconda powershell did not work anymore, so I will continue working from anaconda prompt

Having issue while downloading all pdf files on .asp website using Scrapy

Tony Montana

[Having issue while downloading all pdf files on .asp website using Scrapy](https://stackoverflow.com/questions/57929354/having-issue-while-downloading-all-pdf-files-on-asp-website-using-scrapy)

I am having an issue while downloading multiple pdf files on .asp website using Scrapy. This is the URL of the website: https://ceo.maharashtra.gov.in/searchlist/SearchRollPDF.aspx.Now, if you go through the website, it sends multiple form request to the same above URL and generated the newly updated HTML content for the same page. Now, I have gone through every step, including solving the CAPTCHA and finally, I have arrived at the final step where pdfs can be downloaded.When you fill all the form details, including CAPTCHA, you'll get to see more than one links to download the same numbers of unique pdf files. And this is where I am having the issue.Now, when you click on any links, it sends one POST request to same above URL and refreshes the page with the following javascript content.And this above code, opens the another tab with the url https://ceo.maharashtra.gov.in/searchlist/ViewRoll.aspx which shows pdf in the tab. And I want to download this pdf file. So far, I am able to download a single pdf file with no issues using Scrapy. But the issue I have is downloading more than one pdf files. Sometimes, my below code download same pdf file twice, sometimes it downloads only one pdf file. But every time, it downloads at least one pdf file if not every other pdf files.Kindly find the below scrapy log:Kindly help me in solving this problem.

2019-09-13 19:26:25Z

I am having an issue while downloading multiple pdf files on .asp website using Scrapy. This is the URL of the website: https://ceo.maharashtra.gov.in/searchlist/SearchRollPDF.aspx.Now, if you go through the website, it sends multiple form request to the same above URL and generated the newly updated HTML content for the same page. Now, I have gone through every step, including solving the CAPTCHA and finally, I have arrived at the final step where pdfs can be downloaded.When you fill all the form details, including CAPTCHA, you'll get to see more than one links to download the same numbers of unique pdf files. And this is where I am having the issue.Now, when you click on any links, it sends one POST request to same above URL and refreshes the page with the following javascript content.And this above code, opens the another tab with the url https://ceo.maharashtra.gov.in/searchlist/ViewRoll.aspx which shows pdf in the tab. And I want to download this pdf file. So far, I am able to download a single pdf file with no issues using Scrapy. But the issue I have is downloading more than one pdf files. Sometimes, my below code download same pdf file twice, sometimes it downloads only one pdf file. But every time, it downloads at least one pdf file if not every other pdf files.Kindly find the below scrapy log:Kindly help me in solving this problem.It's very likely that every request for downloading a PDF is changing the ASP session state. So in order to download all PDFs, you need to do the downloading sequentially:

Error inserting data into MS SQL DB Using Pymssql

Bamieschijf

[Error inserting data into MS SQL DB Using Pymssql](https://stackoverflow.com/questions/57682646/error-inserting-data-into-ms-sql-db-using-pymssql)

Fixed itFor an educational project I am trying to store the scraped data on a MS SQL Database. First of all I'd like every unique item to be placed in products_tb. Once the unique product is inserted, SQL must generate an unique ID for said item, being productgroupid. The products_tb table will only yield the product information which will never change, such as productid, category, name and description. In a second table, which I will create after I get this working, I will store the following data: productgroupid, price, timestamp. The reason for this is that these might change every now and then. With the productgroupid I can always group all the data at any given time and create graphs and so on.The problem is that I cannot get my pipelines.py to work.. I however did manage to insert data into my SQL database using the commented chunk of code:It seems to be working with the following codepipelines.pyitems.py

2019-08-27 21:48:26Z

Fixed itFor an educational project I am trying to store the scraped data on a MS SQL Database. First of all I'd like every unique item to be placed in products_tb. Once the unique product is inserted, SQL must generate an unique ID for said item, being productgroupid. The products_tb table will only yield the product information which will never change, such as productid, category, name and description. In a second table, which I will create after I get this working, I will store the following data: productgroupid, price, timestamp. The reason for this is that these might change every now and then. With the productgroupid I can always group all the data at any given time and create graphs and so on.The problem is that I cannot get my pipelines.py to work.. I however did manage to insert data into my SQL database using the commented chunk of code:It seems to be working with the following codepipelines.pyitems.pyEDIT:Another small error I missed. "IF NOT EXIST" needs to changed to "IF NOT EXISTS".ORIGINAL:You're not calling the values in the item dictionary correctly when defining sql_statement. Try this:

Python + Scrapy: Issues running “ImagesPipeline” when running crawler from script

tycrone

[Python + Scrapy: Issues running “ImagesPipeline” when running crawler from script](https://stackoverflow.com/questions/57616611/python-scrapy-issues-running-imagespipeline-when-running-crawler-from-scrip)

I'm brand new to Python so I apologize if there's a dumb mistake here...I've been scouring the web for days, looking at similar issues and combing through Scrapy docs and nothing seems to really resolve this for me... I have a Scrapy project which successfully scrapes the source website, returns the required items, and then uses an ImagePipeline to download (and then rename accordingly) the images from the returned image links... but only when I run from the terminal with "runspider". Whenever I use "crawl" from the terminal or CrawlProcess to run the spider from within the script, it returns the items but does not download the images and, I assume, completely misses the ImagePipeline.I read that I needed to import my settings when running this way in order to properly load the pipeline, which makes sense after looking into the differences between "crawl" and "runspider" but I still cannot get the pipeline working. There are no error messages but I notice that it does return "[scrapy.middleware] INFO: Enabled item pipelines: []" ... Which I assumed was showing that it is still missing my pipeline?Here's my spider.py: Here is my items.py:Here is my pipelines.py:Here is my settings.py:Thank you to anybody that looks at this or even attempts to help me out. It's greatly appreciated.

2019-08-22 20:26:32Z

I'm brand new to Python so I apologize if there's a dumb mistake here...I've been scouring the web for days, looking at similar issues and combing through Scrapy docs and nothing seems to really resolve this for me... I have a Scrapy project which successfully scrapes the source website, returns the required items, and then uses an ImagePipeline to download (and then rename accordingly) the images from the returned image links... but only when I run from the terminal with "runspider". Whenever I use "crawl" from the terminal or CrawlProcess to run the spider from within the script, it returns the items but does not download the images and, I assume, completely misses the ImagePipeline.I read that I needed to import my settings when running this way in order to properly load the pipeline, which makes sense after looking into the differences between "crawl" and "runspider" but I still cannot get the pipeline working. There are no error messages but I notice that it does return "[scrapy.middleware] INFO: Enabled item pipelines: []" ... Which I assumed was showing that it is still missing my pipeline?Here's my spider.py: Here is my items.py:Here is my pipelines.py:Here is my settings.py:Thank you to anybody that looks at this or even attempts to help me out. It's greatly appreciated.Since you are running your spider as a script, there is no scrapy project environment, get_project_settings won't work (aside from grabbing the default settings).

The script must be self-contained, i.e. contain everything you need to run your spider (or import it from your python search path, like any regular old python code).  I've reformatted that code for you, so that it runs, when you execute it with the plain python interpreter: python3 script.py.

How to get cookies from response of scrapy splash

jay padaliya

[How to get cookies from response of scrapy splash](https://stackoverflow.com/questions/57533828/how-to-get-cookies-from-response-of-scrapy-splash)

I want to get the cookie value from the response object of a splash. but it is not working as I expected.Here is spider codeOutput log:

2019-08-17 06:26:43Z

I want to get the cookie value from the response object of a splash. but it is not working as I expected.Here is spider codeOutput log:You can try the following approach:

- write a small Lua script that returns the html + the cookies:Change your Request to the following:Then find the cookies in your parse-method as follows:

Unable to understand XPath siblings behaviour

Jausk

[Unable to understand XPath siblings behaviour](https://stackoverflow.com/questions/57538197/unable-to-understand-xpath-siblings-behaviour)

I am trying to scrape a HTML page in an scenario where I only have consecutive tags with information.From the following code I would like to get the text for the  tags (e.g. Name1, Name2, ...), taking into consideration:"a" followed by "span" gives information about that ID being a Customer or not."a" followed by "a" means that ID is anonymous.I'm using the following XPATH to try to match "a" followed by "span"This will return Name1, Name2 and Name4, even if Name1 is not a Customer. What am I doing wrong?

2019-08-17 16:48:56Z

I am trying to scrape a HTML page in an scenario where I only have consecutive tags with information.From the following code I would like to get the text for the  tags (e.g. Name1, Name2, ...), taking into consideration:"a" followed by "span" gives information about that ID being a Customer or not."a" followed by "a" means that ID is anonymous.I'm using the following XPATH to try to match "a" followed by "span"This will return Name1, Name2 and Name4, even if Name1 is not a Customer. What am I doing wrong?It's because the first following-sibling span of that Name1 does indeed equal "(Customer)".What you should do instead is find the first following sibling (*[1]) and check to see if that sibling is a span ([self::span]) and if it is, then check to see if it's equal to "(Customer)"...

Scrapy splash connection refused on localhost:8050

jay padaliya

[Scrapy splash connection refused on localhost:8050](https://stackoverflow.com/questions/57487402/scrapy-splash-connection-refused-on-localhost8050)

I have installed scrapy-splash as per official docs in windows 8.1 but when I visit http://localhost:8050/, I am getting connection refused errorRun:

docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splashhere is a docker cmd log:

2019-08-14 02:20:36Z

I have installed scrapy-splash as per official docs in windows 8.1 but when I visit http://localhost:8050/, I am getting connection refused errorRun:

docker run -p 5023:5023 -p 8050:8050 -p 8051:8051 scrapinghub/splashhere is a docker cmd log:It is working now. in my case, it is not working with localhost because maybe I have also installed the apache server. but when I use docker default machine IP with port 8050 then it is working

How to download html table content?

MJ O

[How to download html table content?](https://stackoverflow.com/questions/57449268/how-to-download-html-table-content)

I want to download financial data ("konsernregnskap" not "morregnskap") from the following website, but I am not sure how to get all content downloaded: https://www.proff.no/regnskap/yara-international-asa/oslo/hovedkontortjenester/IGB6AV410NZ/Tried to locate the tables with xpath but I have been unsuccessful.I want to download all content into one excel sheet.

2019-08-11 10:40:16Z

I want to download financial data ("konsernregnskap" not "morregnskap") from the following website, but I am not sure how to get all content downloaded: https://www.proff.no/regnskap/yara-international-asa/oslo/hovedkontortjenester/IGB6AV410NZ/Tried to locate the tables with xpath but I have been unsuccessful.I want to download all content into one excel sheet.The answer given by @rusu_ro1 is correct. However, I think that Pandas is the right tool for job here.You can use pandas.read_html to get  all the tables in the page. Then use  pandas.DataFrame.to_excel to write only the last 4 tables to the excel workbook.The following script scrapes the data and writes each table to a different sheet.Notes:From  HTML Table Parsing GotchasIn your specific case it drops the 5th table (it returns only 7). Perhaps b'coz both 1st and 5th table has the same data.you have 8 tables within class table-wrap, first 4 tables belong to "morregnskap" tab and the next 4 tables belong to "konsernregnskap" tab, so by choosing the last 4 you are choosing your desired tables from where you can start to scrape your data

Scrapy disable retry middleware

NeDark

[Scrapy disable retry middleware](https://stackoverflow.com/questions/57426543/scrapy-disable-retry-middleware)

I commented the line in settings.py but it continues being enabled.At the start of the program it loads a lot of middlewares I didn't enableAm I missing something? Is there a way to disable it?

2019-08-09 08:50:57Z

I commented the line in settings.py but it continues being enabled.At the start of the program it loads a lot of middlewares I didn't enableAm I missing something? Is there a way to disable it?According to the documentation, the DOWNLOADER_MIDDLEWARES is merged with DOWNLOADER_MIDDLEWARES_BASE. In the latter, the option scrapy.downloadermiddlewares.httpproxy.RetryMiddleware is enabled by default.  So either writeOr look into DOWNLOADER_MIDDLEWARES_BASE. See their documentation for more details.

Scrapy request doesn't yield full HTML - but Requests library does

whiteteeth520

[Scrapy request doesn't yield full HTML - but Requests library does](https://stackoverflow.com/questions/57359726/scrapy-request-doesnt-yield-full-html-but-requests-library-does)

I am building a crawl.spider to scrape statutory law data from the following website (https://www.azleg.gov/viewdocument/?docName=https://www.azleg.gov/ars/1/00101.htm). I am aiming to extract the statute text, which is contained in the following XPath [//div[@class = 'first']/p/text()]. This path should provide the statute text.All of my scrapy requests are yielding incomplete html responses, such that when I search for the relevant xpath queries, it yields an empty list. However, when I use the requests library, the html downloads correctly.Using XPath tester online, I've verified that my xpath queries should produce the desired content. Using scrapy shell, I've viewed the response object from scrapy in my browser - and it looks just like it does when I'm browsing natively. I've tried enabling middleware for both BeautifulSoup and Selenium, but neither has appeared to work. Here's my crawl spider And here's the code that succsessfuly generated the correct response object

2019-08-05 13:35:08Z

I am building a crawl.spider to scrape statutory law data from the following website (https://www.azleg.gov/viewdocument/?docName=https://www.azleg.gov/ars/1/00101.htm). I am aiming to extract the statute text, which is contained in the following XPath [//div[@class = 'first']/p/text()]. This path should provide the statute text.All of my scrapy requests are yielding incomplete html responses, such that when I search for the relevant xpath queries, it yields an empty list. However, when I use the requests library, the html downloads correctly.Using XPath tester online, I've verified that my xpath queries should produce the desired content. Using scrapy shell, I've viewed the response object from scrapy in my browser - and it looks just like it does when I'm browsing natively. I've tried enabling middleware for both BeautifulSoup and Selenium, but neither has appeared to work. Here's my crawl spider And here's the code that succsessfuly generated the correct response object

Action based on page content

cplusplusye

[Action based on page content](https://stackoverflow.com/questions/57359989/action-based-on-page-content)

Is there any way of creating a scrapy rule based on content of page? The reason being is I am trying to create a crawler for a website which has a certain page that is displayed but the URL doesn't change, so I need the spider to recognize it is on that page from the content and then call a certain function.

2019-08-05 13:48:28Z

Is there any way of creating a scrapy rule based on content of page? The reason being is I am trying to create a crawler for a website which has a certain page that is displayed but the URL doesn't change, so I need the spider to recognize it is on that page from the content and then call a certain function.

Save image path names in the database

Biddaris

[Save image path names in the database](https://stackoverflow.com/questions/57357984/save-image-path-names-in-the-database)

I have successfully crawled image data on a website with Scrapy, and save the image in a folder. but I want to save the path name of the image in the mysql database.like the results of the spider below, I want to forward the data 'path' to the pipelines but I don't know how to select itMy pipelines.py:My items.pyI want to save the name of the image path to the database. Anyone who is familiar with this problem, please let me know. thankyou.

2019-08-05 11:47:43Z

I have successfully crawled image data on a website with Scrapy, and save the image in a folder. but I want to save the path name of the image in the mysql database.like the results of the spider below, I want to forward the data 'path' to the pipelines but I don't know how to select itMy pipelines.py:My items.pyI want to save the name of the image path to the database. Anyone who is familiar with this problem, please let me know. thankyou.If the images list is part of your item, you can just select it like this: item['images'][0]['path'].You can add it to the pipeline by changing the store_db method as follows:

Scrapy extracting specific data

Jackesio

[Scrapy extracting specific data](https://stackoverflow.com/questions/57364651/scrapy-extracting-specific-data)

I have an issue extracting a specific set of data using scrapy.Here is an example code I am trying to extract.I want to extract just 'Terry'.I know how to get to 'Terry' by searching in the div class through index, but the problem is that the index number is hard-coded and when you got to the next page the other person's 'Last Name' might not be in the same index.So instead of trying to search through index I was trying to search for the span that contains 'Last Name' but every time I do that I get a return of the whole thing.Meaning I getJason

TerryBut I want just 'Terry'

2019-08-05 19:01:23Z

I have an issue extracting a specific set of data using scrapy.Here is an example code I am trying to extract.I want to extract just 'Terry'.I know how to get to 'Terry' by searching in the div class through index, but the problem is that the index number is hard-coded and when you got to the next page the other person's 'Last Name' might not be in the same index.So instead of trying to search through index I was trying to search for the span that contains 'Last Name' but every time I do that I get a return of the whole thing.Meaning I getJason

TerryBut I want just 'Terry'Have you tried this xpath? //span[contains(.,'Last Name')]/following-sibling::text() It should return the text after span element that contains Last Name textresult: can you show your code

and also try this xpath:

Selecting a value from Dynamic List of Values using Selenium and Scrapy

SarahB

[Selecting a value from Dynamic List of Values using Selenium and Scrapy](https://stackoverflow.com/questions/57364173/selecting-a-value-from-dynamic-list-of-values-using-selenium-and-scrapy)

I am trying to scrape a website which displays a list of values. I need to select one value and scrape, followed by another selection. I am just unable to do this, after I save the page, I see the following code for the list of value:It shows that the lov has a dynamic id assigned to it. Any idea how to select values from this lov? It has values like "All", "Not Classified" etc.I have been able to resolve this by below This solves my issue, but I receive an error in python as: "KeyError: None".I am not sure why this error comes, but for time being I am ignoring it by a try-except block.

2019-08-05 18:24:35Z

I am trying to scrape a website which displays a list of values. I need to select one value and scrape, followed by another selection. I am just unable to do this, after I save the page, I see the following code for the list of value:It shows that the lov has a dynamic id assigned to it. Any idea how to select values from this lov? It has values like "All", "Not Classified" etc.I have been able to resolve this by below This solves my issue, but I receive an error in python as: "KeyError: None".I am not sure why this error comes, but for time being I am ignoring it by a try-except block.In css you would get those with:

Scrapy spider not saving html files

Meredith Abrams

[Scrapy spider not saving html files](https://stackoverflow.com/questions/57298091/scrapy-spider-not-saving-html-files)

I have a Scrapy spider that I've generated. The purpose of the spider is to return network data for the purposes of graphing the network as well as to return the html files for each page the spider reaches.  The spider is achieving the first goal but not the second. It results in a csv file with the tracking information but I cannot see that it is saving the html files.The traceback I receive is as follows: 

2019-07-31 20:02:39Z

I have a Scrapy spider that I've generated. The purpose of the spider is to return network data for the purposes of graphing the network as well as to return the html files for each page the spider reaches.  The spider is achieving the first goal but not the second. It results in a csv file with the tracking information but I cannot see that it is saving the html files.The traceback I receive is as follows: parse method:According to  scrapy docs and another stack overflow question it is not recommended to override parse method because crawlspider use it to implement it's logic.If You need to override parse method and in the same time count with Crawlspider.parse original source code - You need to add it's original source to fix parse method:csv feed:

This log line:

2019-07-23 14:16:41 [scrapy.extensions.feedexport] INFO: Stored csv feed (153 items) in: exampledomainlevel1.csv - means that csv feedexporter enabled (probably in settings.py project settings file.)UPDATE

 I observed Crawlspider source code again.

 It looks like parse method called only once at the beginning and it don't cover all web responses.

If my theory correct - after adding this function to your spider class should save all html responses:

scrapy-splash crawler starts fast but slows down (not throttled by website)

user1837332

[scrapy-splash crawler starts fast but slows down (not throttled by website)](https://stackoverflow.com/questions/57299055/scrapy-splash-crawler-starts-fast-but-slows-down-not-throttled-by-website)

I have a single crawler written in scrapy using the splash browser via the scrapy-splash python package. I am using the aquarium python package to load balance the parallel scrapy requests to a splash docker cluster.The scraper uses a long list of urls as the start_urls list. There is no "crawling" from page to page via hrefs or pagination. I am running six splash dockers with 5 slots per splash as the load balanced browser cluster. I am running scrapy at six concurrent requests.The dev machine is a macbook pro with a dual core 2.4Ghz CPU with 16Gb RAM.When the spider starts up, the aquarium stdout shows fast request/responses, the onboard fan spins up and the system is running at 90% used with 10% idle so I am not overloading the system resources. The memory/swap is not exhausted either.At this time, I get a very slow ~30 pages/minute. After a few minutes, the fans spin down, the system resources are significantly free (>60% idle) and the scrapy log shows every request having a 503 timeout.When I look at the stdout of the aquarium cluster, there are requests being processed, albeit very slowly compared to when the spider is first invoked. If I got to localhost:9050, I do get the splash page after 10 seconds or so, so the load balancer/splash is online.If I stop the spider and restart it, it starts up normally so this does not seem to be a throttle from the target site as a spider restart would also be throttled but it's not.I appreciate any insight that the community can offer.Thanks.

2019-07-31 21:25:30Z

I have a single crawler written in scrapy using the splash browser via the scrapy-splash python package. I am using the aquarium python package to load balance the parallel scrapy requests to a splash docker cluster.The scraper uses a long list of urls as the start_urls list. There is no "crawling" from page to page via hrefs or pagination. I am running six splash dockers with 5 slots per splash as the load balanced browser cluster. I am running scrapy at six concurrent requests.The dev machine is a macbook pro with a dual core 2.4Ghz CPU with 16Gb RAM.When the spider starts up, the aquarium stdout shows fast request/responses, the onboard fan spins up and the system is running at 90% used with 10% idle so I am not overloading the system resources. The memory/swap is not exhausted either.At this time, I get a very slow ~30 pages/minute. After a few minutes, the fans spin down, the system resources are significantly free (>60% idle) and the scrapy log shows every request having a 503 timeout.When I look at the stdout of the aquarium cluster, there are requests being processed, albeit very slowly compared to when the spider is first invoked. If I got to localhost:9050, I do get the splash page after 10 seconds or so, so the load balancer/splash is online.If I stop the spider and restart it, it starts up normally so this does not seem to be a throttle from the target site as a spider restart would also be throttled but it's not.I appreciate any insight that the community can offer.Thanks.

Scrapy: How to crawl the next url in start_urls when a condition is met

user11847694

[Scrapy: How to crawl the next url in start_urls when a condition is met](https://stackoverflow.com/questions/57239332/scrapy-how-to-crawl-the-next-url-in-start-urls-when-a-condition-is-met)

Is there any way to stop crawling current url and,  jump and crawl the next url in start_urls, when a given condition is met.

Here I test the dates in the page with a pre-defined date.

I want to stop crawling the url when that condition is met. Edit

My code is as follows,

2019-07-28 08:55:52Z

Is there any way to stop crawling current url and,  jump and crawl the next url in start_urls, when a given condition is met.

Here I test the dates in the page with a pre-defined date.

I want to stop crawling the url when that condition is met. Edit

My code is as follows,Simply calling return after yield worked for me:In the parse_number function, check the condition for matching the date. If the condition matches yield the data and stop crawling from the specific domain. else continue the rest crawling.

How to get text inside div

Newbie

[How to get text inside div](https://stackoverflow.com/questions/57201608/how-to-get-text-inside-div)

Take a look at this webpage:https://www.michaelkors.com/large-crossgrain-leather-dome-crossbody-bag/_/R-US_32S9SF5C3L?color=2519I want to get text under details section. When I look at the div it has class detail and text under it. This is the statement I am using:However, it is returning nothing.

2019-07-25 12:07:22Z

Take a look at this webpage:https://www.michaelkors.com/large-crossgrain-leather-dome-crossbody-bag/_/R-US_32S9SF5C3L?color=2519I want to get text under details section. When I look at the div it has class detail and text under it. This is the statement I am using:However, it is returning nothing.Looks like the div you're trying to parse does not exist when the page is loaded.

Product data is stored as json inside a script tag, and the div is generated from it using javascript.This leaves you with a couple of options:class detail element not found in the page source. Which means it is not found in the response loaded by scrapy request.

Scrapy deals with static requests, it responses all the elements present in the page source.If the request is a dynamic request, it responses elements present in the inspect element, loaded by javascript, ajax type requests). we should try some other packages along with scrapy to scrape those data.Examples: Splash, Selenium etcIn your case you should handle it as dynamic requests.

CrawlSpider fetches only a subset of the matched links in the first page, then moves to scrape links in second page

Little Rocket Guy

[CrawlSpider fetches only a subset of the matched links in the first page, then moves to scrape links in second page](https://stackoverflow.com/questions/57240663/crawlspider-fetches-only-a-subset-of-the-matched-links-in-the-first-page-then-m)

Crawlspider fetches only a subset of the matched links on the first page of the listings. Soon after, it moves to the second page where it successfully follows all matched links, exactly as intended. How to make Crawlspider follow all matched links before proceding in the second page?I have added the "process_links='link_filter''" argument in the second Rule and verified it matched all links as intended, but it follows a seemingly semi-random subset of them.I expected that Crawlspider would move to the second page only after it finishes following the links in the first.

2019-07-28 11:55:10Z

Crawlspider fetches only a subset of the matched links on the first page of the listings. Soon after, it moves to the second page where it successfully follows all matched links, exactly as intended. How to make Crawlspider follow all matched links before proceding in the second page?I have added the "process_links='link_filter''" argument in the second Rule and verified it matched all links as intended, but it follows a seemingly semi-random subset of them.I expected that Crawlspider would move to the second page only after it finishes following the links in the first.After ~10 hours of digging through the source code, I was able to spot the problem in the way the scheduler stores requests in memory. The solution was to change it to a queue(FIFO) so that the older requests get fetched first. It can easily be changed by setting in settings.py:

Getting HTTP “400” or “404” error while submitting form using Scrapy or requests module

Tony Montana

[Getting HTTP “400” or “404” error while submitting form using Scrapy or requests module](https://stackoverflow.com/questions/57206395/getting-http-400-or-404-error-while-submitting-form-using-scrapy-or-requests)

I am trying to scrape a webform and submit it, using post request with data and trying to get a response. But using Scrapy I am getting HTTP error code "400" and with requests module, I am getting "404" error.Overall, I am trying to download a PDF document which shows in the browser after submitting all form details, successfully. I am trying to automate this whole workflow.This is the URL of a web form. Now, when you select any values from the dropdown, the form makes a POST request to the same URL with some parameters and returns a response containing HTML code. This HTML code has values of the next dropdown field. So, basically, there are two dependent dropdowns. And the selection of each dropdown, that webpage makes a POST request to itself, refresh and update the web page with the response you get from the request.This is my code using requests moduleAnd this is my code using Scrapy: Now, when I am trying to make a POST request using requests and Scrapy, I get HTTP error '404' and '400' respectively. I am not able to find a solution for this issue.So, kindly help me out here. Thanks.Note: I can't use any web browser automation tools such as Selenium for this task.

2019-07-25 16:20:12Z

I am trying to scrape a webform and submit it, using post request with data and trying to get a response. But using Scrapy I am getting HTTP error code "400" and with requests module, I am getting "404" error.Overall, I am trying to download a PDF document which shows in the browser after submitting all form details, successfully. I am trying to automate this whole workflow.This is the URL of a web form. Now, when you select any values from the dropdown, the form makes a POST request to the same URL with some parameters and returns a response containing HTML code. This HTML code has values of the next dropdown field. So, basically, there are two dependent dropdowns. And the selection of each dropdown, that webpage makes a POST request to itself, refresh and update the web page with the response you get from the request.This is my code using requests moduleAnd this is my code using Scrapy: Now, when I am trying to make a POST request using requests and Scrapy, I get HTTP error '404' and '400' respectively. I am not able to find a solution for this issue.So, kindly help me out here. Thanks.Note: I can't use any web browser automation tools such as Selenium for this task.

Selecting XPATH of all Divs that contains certain attribute using Scrapy

falco97

[Selecting XPATH of all Divs that contains certain attribute using Scrapy](https://stackoverflow.com/questions/57203515/selecting-xpath-of-all-divs-that-contains-certain-attribute-using-scrapy)

Assume given HTMLIf know I can select all divs by their class name as:But I want to access divs with a specific attribute such as index (with any value).Just to clarify: We have to select div that has index as an attribute, not by a specific value of index.

Any help would highly be appreciated!

2019-07-25 13:47:43Z

Assume given HTMLIf know I can select all divs by their class name as:But I want to access divs with a specific attribute such as index (with any value).Just to clarify: We have to select div that has index as an attribute, not by a specific value of index.

Any help would highly be appreciated!You can use simple xpath.Screenshot:

How to scrape classes that only show up when logged in?

ecclark1

[How to scrape classes that only show up when logged in?](https://stackoverflow.com/questions/57072925/how-to-scrape-classes-that-only-show-up-when-logged-in)

I'm trying to write a spider with Scrapy that will ideally return a list of URLs from a site if said URL(s) contain a certain class which I would define in print response.css(".class"), but I'm not sure if this is even possible when the class will only be on the page if a user is logged in. I've gone through guides on how to write spiders with Scrapy and I've gotten it to return a list of selectors using a different class that I know is on the page whether or not a user is logged in, just as a test to know that I didnt write it wrong. I really just want to know if this is even possible and if so what steps I can take to get there.The code I have so far is obviously very basic and barely edited from the generated template as I'm still in the simple testing phase of this. Ideally I want to get a list of selectors which would, if this is possible, then give me a list of URLs for each page where the class is found. All I'm looking for is the URLs of the pages that contain the defined class.

2019-07-17 09:41:39Z

I'm trying to write a spider with Scrapy that will ideally return a list of URLs from a site if said URL(s) contain a certain class which I would define in print response.css(".class"), but I'm not sure if this is even possible when the class will only be on the page if a user is logged in. I've gone through guides on how to write spiders with Scrapy and I've gotten it to return a list of selectors using a different class that I know is on the page whether or not a user is logged in, just as a test to know that I didnt write it wrong. I really just want to know if this is even possible and if so what steps I can take to get there.The code I have so far is obviously very basic and barely edited from the generated template as I'm still in the simple testing phase of this. Ideally I want to get a list of selectors which would, if this is possible, then give me a list of URLs for each page where the class is found. All I'm looking for is the URLs of the pages that contain the defined class.I did not clearly understand your problem. I assume that you want to get URL's which have a specific class attribute. If this is what you want to do, you can change definiton of parse method of the spider:Also, the information you want to scrape is only available when you log in the target website, then you should make a form request for authentication.For more information about form requests check the link below:

https://doc.scrapy.org/en/latest/topics/request-response.html#using-formrequest-from-response-to-simulate-a-user-login

Scrape some child links and then returning to the main scraping

Perth

[Scrape some child links and then returning to the main scraping](https://stackoverflow.com/questions/57044375/scrape-some-child-links-and-then-returning-to-the-main-scraping)

I am trying to scrape a site with div elements and iteratively, for each div element I want to scrape some data from it and follow the child links it has and scrape more data from them.Here is the code of quote.pyHere is the code of items.pyI ran the command scrapy crawl quote -o data.json, but there was no error message and data.json was empty. I was expecting to get all the data in its corresponding field.Can you please help me?

2019-07-15 17:10:35Z

I am trying to scrape a site with div elements and iteratively, for each div element I want to scrape some data from it and follow the child links it has and scrape more data from them.Here is the code of quote.pyHere is the code of items.pyI ran the command scrapy crawl quote -o data.json, but there was no error message and data.json was empty. I was expecting to get all the data in its corresponding field.Can you please help me?Take a closer look at your logs, you'll be able to find messages like this:Scrapy is automatically managing duplicates and trying not to visit one URL twice(for obvious reasons).

In you case you can add dont_filter = True to your requests and will see something like this:Which is kinda strange indeed, because of page yields request to itself.Overall you could end up with something like this:

Select attribute value by attribute name of element with xpath using scrapy

dave

[Select attribute value by attribute name of element with xpath using scrapy](https://stackoverflow.com/questions/56922233/select-attribute-value-by-attribute-name-of-element-with-xpath-using-scrapy)

Tried response.css('#logsss_pageid').extract()  and got:all I need is the x-444511621

2019-07-07 12:39:14Z

Tried response.css('#logsss_pageid').extract()  and got:all I need is the x-444511621Try withMore info: selectors in scrapy using xpath

How to have scrapy iterate over archive using url?

kynik0s

[How to have scrapy iterate over archive using url?](https://stackoverflow.com/questions/56927628/how-to-have-scrapy-iterate-over-archive-using-url)

I'm trying to have a scrapy spider crawl through several pages in an archive, with the goal of opening each individual link and scraping the contents of the linked page. I'm running into some random HTTP 500 errors, which I am trying to skip by simply doing a try-except to skip over those pages returning 500 errors. The first part of the parse function iterates over the hrefs in the archive page for the pages to scrape using the parse_art function. The second part is to find the next page in the archive and follow through to that page to continue crawling.I'm trying to change the program to iterate over an initial URL, but can't seem to get it right. Any help would be appreciated. Running scrapy on Python 3.7. I'm trying to make it so that the spider crawls through the archive by utilizing the url and simply adding 1 to the current archive number, rather than relying on the (unreliable) "Next Page" Xpath.

2019-07-08 02:12:58Z

I'm trying to have a scrapy spider crawl through several pages in an archive, with the goal of opening each individual link and scraping the contents of the linked page. I'm running into some random HTTP 500 errors, which I am trying to skip by simply doing a try-except to skip over those pages returning 500 errors. The first part of the parse function iterates over the hrefs in the archive page for the pages to scrape using the parse_art function. The second part is to find the next page in the archive and follow through to that page to continue crawling.I'm trying to change the program to iterate over an initial URL, but can't seem to get it right. Any help would be appreciated. Running scrapy on Python 3.7. I'm trying to make it so that the spider crawls through the archive by utilizing the url and simply adding 1 to the current archive number, rather than relying on the (unreliable) "Next Page" Xpath.Changing value url_number can't change value in url.You have to generate full url again 

How to loop through drop down list Scrapy

Tom

[How to loop through drop down list Scrapy](https://stackoverflow.com/questions/56894086/how-to-loop-through-drop-down-list-scrapy)

I am scraping the following website, https://www.trollandtoad.com/magic-the-gathering/magic-2020-m20-/14878?Keywords=&min-price=&max-price=&items-pp=60&item-condition=&selected-cat=14878&sort-order=&page-no=1&view=list&Rarity=&Ruleset=&minMana=&maxMana=&minPower=&maxPower=&minToughness=&maxToughness=, and I need to loop through the drop down list for quantity until it reaches the end in order to determine the remaining stock.  I put a counter in there to determine how many times it runs through the loop to determine how much to stock is left, but it is only running through the loop once.  

2019-07-04 20:49:07Z

I am scraping the following website, https://www.trollandtoad.com/magic-the-gathering/magic-2020-m20-/14878?Keywords=&min-price=&max-price=&items-pp=60&item-condition=&selected-cat=14878&sort-order=&page-no=1&view=list&Rarity=&Ruleset=&minMana=&maxMana=&minPower=&maxPower=&minToughness=&maxToughness=, and I need to loop through the drop down list for quantity until it reaches the end in order to determine the remaining stock.  I put a counter in there to determine how many times it runs through the loop to determine how much to stock is left, but it is only running through the loop once.  There is a much simple way with XPath:I counted the quantity by basically selecting all <option>s, extracting their value attribute and taking their max integer value. Like so:I also refactored your code a bitoutput:Items.py    Spider CodeResult

Scrapy - How to stop meta refresh redirect?

gunesevitan

[Scrapy - How to stop meta refresh redirect?](https://stackoverflow.com/questions/56866704/scrapy-how-to-stop-meta-refresh-redirect)

This is the website I am crawling. I had no problem at first, but then I encountered this error.Website knows I am a bot and redirects me to a page with a captcha code. I think handle_httpstatus_list or dont_redirect doesn't work because redirection isn't done with http status codes. This is my crawler's code. Is there any way to stop this redirection?UPDATE: I tried those settings, but they didn't work either.

2019-07-03 09:12:43Z

This is the website I am crawling. I had no problem at first, but then I encountered this error.Website knows I am a bot and redirects me to a page with a captcha code. I think handle_httpstatus_list or dont_redirect doesn't work because redirection isn't done with http status codes. This is my crawler's code. Is there any way to stop this redirection?UPDATE: I tried those settings, but they didn't work either.This website is protected by Distil Networks. They are using JavaScript to determine you are a bot. Are they letting some requests through or none at all? You may be able to have some success with Selenium, but in my experience they will catch on eventually. The solution involves randomizing the entire browser fingerprint from screen size and everything else you can think of. If anybody else has additional info I would be interested to hear about it. I'm not sure about SoF ToS on stuff like this. If you load up a proxy like charles proxy or something so you can see everything going on you can look at all the JS they are running on you.If they are letting 0 requests through I'd advise using Selenium to see your luck.If they are letting some through and redirecting others my experience is over time they will eventually redirect them all. What I would do if they are letting some through is set http_retry_codes = []Just to expand on this some more I will link to this post about over riding your navigator object with Selenium which is what contains much of your browser fingerprint. It must be done in JS and on every page load. I can't attest to its effectiveness against Distil. See this answerThen you can retry all of them until your rotator  gives you a good IP which I suspect you will see that over a short period of time they will all be banned. To stop meta refresh disable download middleware MetaRefreshMiddleware in project settings by setting it's value to None:https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#activating-a-downloader-middleware

Not able to iterate data using scrapy in python

moreshwar pantwalavalkar

[Not able to iterate data using scrapy in python](https://stackoverflow.com/questions/56823192/not-able-to-iterate-data-using-scrapy-in-python)

I am scraping below website for Actress name ,rank and score 

http://www.timescelebex.com/top_actressesi could able to get 1 record but cannot iterate with for look for rest of the records I could able to receive data like below but for only 1 record{'Score': u'41.0', 'Name': u'Deepika Padukone', 'Rank': u'1'}i want to extract like above for all recordsif i use .extract instead of .extract_first() i could see all data but its coming in like below{'Score': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>, 'Name': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>, 'Rank': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>}

2019-06-30 07:34:39Z

I am scraping below website for Actress name ,rank and score 

http://www.timescelebex.com/top_actressesi could able to get 1 record but cannot iterate with for look for rest of the records I could able to receive data like below but for only 1 record{'Score': u'41.0', 'Name': u'Deepika Padukone', 'Rank': u'1'}i want to extract like above for all recordsif i use .extract instead of .extract_first() i could see all data but its coming in like below{'Score': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>, 'Name': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>, 'Rank': , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ]>}You need to use relative XPath inside for loop:Use this XPath to get all the wanted selectors:This will select all rows with actresses (skipping the table header) from the first table (Jan 2018). Then you can iterate over the names list of selectors.Css Version     

How to crawl drupal website by Python Scrapy?

Дядя Фея

[How to crawl drupal website by Python Scrapy?](https://stackoverflow.com/questions/56804214/how-to-crawl-drupal-website-by-python-scrapy)

I have a Drupal website(can't show), on it is a html table. I need to crowl rows from this table.By default view of table is undesirible for crawling:These two issues resolve by filter: Thereafter table is ready to crowling.I noticed, when i push on "next" button or change table options in F12 -> Network I see an ajax POST request with big body. There is data which is responsible for "rows per page" and " 'country' column". I tried to make request POST with body that consists from my data. I only get the response "Bad request". Thereafter I saw that in body is data that is responsible for salt. 

I find this salt in html and get it dynamically from html.

At the moment I make POST request, get response 200 , but this page is wrong.Question:

How i can get page by Scrapy in "ready for crawling" mode? How can i interact with filter from code? I understand that my explanation of situation is very abstract, but I will be glad to all ideas and solutions! Thanks a lot in advance!!

2019-06-28 09:19:50Z

I have a Drupal website(can't show), on it is a html table. I need to crowl rows from this table.By default view of table is undesirible for crawling:These two issues resolve by filter: Thereafter table is ready to crowling.I noticed, when i push on "next" button or change table options in F12 -> Network I see an ajax POST request with big body. There is data which is responsible for "rows per page" and " 'country' column". I tried to make request POST with body that consists from my data. I only get the response "Bad request". Thereafter I saw that in body is data that is responsible for salt. 

I find this salt in html and get it dynamically from html.

At the moment I make POST request, get response 200 , but this page is wrong.Question:

How i can get page by Scrapy in "ready for crawling" mode? How can i interact with filter from code? I understand that my explanation of situation is very abstract, but I will be glad to all ideas and solutions! Thanks a lot in advance!!

“'str' object has no attribute 'get'” when using Google Cloud Storage with ScrapingHub

Nathan Wailes

[“'str' object has no attribute 'get'” when using Google Cloud Storage with ScrapingHub](https://stackoverflow.com/questions/56746229/str-object-has-no-attribute-get-when-using-google-cloud-storage-with-scrap)

I'm trying to get Google Cloud Storage working with a Scrapy Cloud + Crawlera project so that I can save text files I'm trying to download.  I'm encountering an error when I run my script that seems to have to do with my Google permissions not working properly.Error:__init__.py where I create the credentials file:settings.py:

2019-06-25 02:54:52Z

I'm trying to get Google Cloud Storage working with a Scrapy Cloud + Crawlera project so that I can save text files I'm trying to download.  I'm encountering an error when I run my script that seems to have to do with my Google permissions not working properly.Error:__init__.py where I create the credentials file:settings.py:After looking at the code for _load_credentials_from_file it seems to me that I had not saved the JSON to a text file correctly: in __init__.py, rather than having text_file.write(json.dumps(credentials_content)), I should have had text_file.write(credentials_content) or text_file.write(json.dumps(json.loads(credentials_content))).

Scrapy crawling fails if too many spiders are used (Crawled 0 pages)

Pt Fisch

[Scrapy crawling fails if too many spiders are used (Crawled 0 pages)](https://stackoverflow.com/questions/56718800/scrapy-crawling-fails-if-too-many-spiders-are-used-crawled-0-pages)

I'm trying to crawl prices from a popular reselling platform. For every item, I instantiate the same spider with different searching tags. With less than about 300 items the process works perfectly fine, just not for more than that.

Every spider writes its results to a document before it closes, so there are no special pipelines or yielding hierarchies.I tried different approaches like Or(Those two work fine for lower numbers of spiders, another try with subprocesses issuing scrapy command line commands did not work).I tried different settings and addons in different combinations and with different values for the past two weeks without any noticeable change to the outcome. (Although I have certainly not tried every possible combination) 

settings and addons likeAnd so on...The spiders get instantiated like this:Since the crawler itself is working, when issued from command line or when collections only contain about 300 items, I don't think, there should be a problem.

The params above are put together to form the searching tags.The outcome with more than ~300 items results in some minutes of Thank you for your time in advance.Not to forget: I am open for new approaches on how to run multiple instances of one spider with different params, even if it needs much more time.

2019-06-22 20:09:04Z

I'm trying to crawl prices from a popular reselling platform. For every item, I instantiate the same spider with different searching tags. With less than about 300 items the process works perfectly fine, just not for more than that.

Every spider writes its results to a document before it closes, so there are no special pipelines or yielding hierarchies.I tried different approaches like Or(Those two work fine for lower numbers of spiders, another try with subprocesses issuing scrapy command line commands did not work).I tried different settings and addons in different combinations and with different values for the past two weeks without any noticeable change to the outcome. (Although I have certainly not tried every possible combination) 

settings and addons likeAnd so on...The spiders get instantiated like this:Since the crawler itself is working, when issued from command line or when collections only contain about 300 items, I don't think, there should be a problem.

The params above are put together to form the searching tags.The outcome with more than ~300 items results in some minutes of Thank you for your time in advance.Not to forget: I am open for new approaches on how to run multiple instances of one spider with different params, even if it needs much more time.Note that CrawlerRunner.crawl method's first argument is not an instance of Crawler, but a class.Try to pass it like this:

How to fix callback inside the spider in python scrapy?

Mohamed Kamal

[How to fix callback inside the spider in python scrapy?](https://stackoverflow.com/questions/56632515/how-to-fix-callback-inside-the-spider-in-python-scrapy)

I'm creating a web scraper and want to callback to get sub-pages, but it seems not working correctly and no result, any one can help ?Here is my code I want to callback from response to parse_detailsI expect to loop over all adds in sub_req and scrap the data from it

and here is the logfile:

2019-06-17 13:38:07Z

I'm creating a web scraper and want to callback to get sub-pages, but it seems not working correctly and no result, any one can help ?Here is my code I want to callback from response to parse_detailsI expect to loop over all adds in sub_req and scrap the data from it

and here is the logfile:

My first Scrapy Spider does not work with MySQL database

jozefPython

[My first Scrapy Spider does not work with MySQL database](https://stackoverflow.com/questions/56696249/my-first-scrapy-spider-does-not-work-with-mysql-database)

I'm new on web scraping, and my scrapy code does not work and I have no clue! I want to scrape this website (http://quotes.toscrape.com) then save the data into a MySQL database. So, I designed a basic Spider :and here is my 'pipelines.py' code :you find the error message in a reply 

and thanks ^^

2019-06-21 02:50:48Z

I'm new on web scraping, and my scrapy code does not work and I have no clue! I want to scrape this website (http://quotes.toscrape.com) then save the data into a MySQL database. So, I designed a basic Spider :and here is my 'pipelines.py' code :you find the error message in a reply 

and thanks ^^all the message error :I founded the solution x)

It's init and not int , i forget the 'i' x))Inside your process item you are calling store_db is trying to use the database object curr. And it isn't defined anywhere on your pipeline.I guess this is what you should doWe will return the cursor and connection object from create_connection and cursor object from create_table.We can use this on store_db now.

Could not find a version that satisfies the requirement geckodriver==0.24.0 (from -r /app/requirements.txt (line 4)) error with Selenium Geckodriver

Christian Read

[Could not find a version that satisfies the requirement geckodriver==0.24.0 (from -r /app/requirements.txt (line 4)) error with Selenium Geckodriver](https://stackoverflow.com/questions/56565278/could-not-find-a-version-that-satisfies-the-requirement-geckodriver-0-24-0-fro)

There's no problem on my local machine. 

But when I deployed it in cloud server specifically in Scrapinghub 

I need to add geckodriverHow can include geckodriver in my requirement.txt?here's my working codemy Requirement.txtError observed:

2019-06-12 15:04:34Z

There's no problem on my local machine. 

But when I deployed it in cloud server specifically in Scrapinghub 

I need to add geckodriverHow can include geckodriver in my requirement.txt?here's my working codemy Requirement.txtError observed:This error message......implies that the there was an error while GeckoDriver tried to initiate a browsing session through Firefox.Seems there is no incompatibility between geckodriver==0.24.0 and selenium==3.13.0 as per the documentation in Supported platforms.Presumably, it looks like an issue with the installation location of Mozilla Firefox. Either Firefox is not installed within your system or Firefox is not installed at the default (desired) location.You need to have Firefox installed at the default location. Incase Firefox is installed at a customized location you need to pass the absolute path of the firefox binary as follows:

multiple if statements with similar pattern

hadesfv

[multiple if statements with similar pattern](https://stackoverflow.com/questions/56475708/multiple-if-statements-with-similar-pattern)

I have the following code which works but I am looking for a way to write it in a more pythonic wayI get two merchants (may or may not have a price) they will sure have a link,if any of them doesn't have a price it should yield their respective parse_seller(amazon,google,ebay,...). I wrote all the parse methods in similar pattern in order to write a better looking (Pythonic) code.

I am looking for a more compact way of writing those if statements

2019-06-06 10:29:22Z

I have the following code which works but I am looking for a way to write it in a more pythonic wayI get two merchants (may or may not have a price) they will sure have a link,if any of them doesn't have a price it should yield their respective parse_seller(amazon,google,ebay,...). I wrote all the parse methods in similar pattern in order to write a better looking (Pythonic) code.

I am looking for a more compact way of writing those if statementsOne approach would be to use a simple list.I don't know if this is more pythonic, 

but you could reorganize item a little bit:

right now you have:You could change it by something like:Google and Amazon being Merchant objects which would be defined by a name and a specific parse method

and then you just have to iterate over it:Of course you have to set non-existing prize with something like 0 or None.Here is an example on how to bind your Merchant object with a specific parse_method: 

Web Scraping - I need to login LinkedIn in order to webscrape (scrapy)

DataScience_Junior

[Web Scraping - I need to login LinkedIn in order to webscrape (scrapy)](https://stackoverflow.com/questions/56331631/web-scraping-i-need-to-login-linkedin-in-order-to-webscrape-scrapy)

I am currently creating a web scraper for Linkedin but LinkedIn blocks my bot from scraping data because i am not logged in into any account. My goal is basically to scrape any job offering made by companies but my question is primarly on the logging in aspect of the code.I am on Python 3.X using scrapy and formrequest.I therefore looked online for the solution the formrequest package seemed to be the answer i needed. However, after several attempts it's still not working.I added the open_in_browser package in order to verify if i was logged in properly and the browser opens to the login screen and it's wrritten: please enter an email adressI expect my browser to open to the home page with my account logged in.Thank you

2019-05-27 19:39:48Z

I am currently creating a web scraper for Linkedin but LinkedIn blocks my bot from scraping data because i am not logged in into any account. My goal is basically to scrape any job offering made by companies but my question is primarly on the logging in aspect of the code.I am on Python 3.X using scrapy and formrequest.I therefore looked online for the solution the formrequest package seemed to be the answer i needed. However, after several attempts it's still not working.I added the open_in_browser package in order to verify if i was logged in properly and the browser opens to the login screen and it's wrritten: please enter an email adressI expect my browser to open to the home page with my account logged in.Thank youWhile I can't provide an answer to your original question, I can tell you that what you're doing is against LinkedIn's software extensions policy.Be careful here. Not only do many websites employ methods to protect against scraping data, but some (LinkedIn included) have been to known to take developers to court over this kind of thing.For a safe, legal way to do what you're trying to do, check out LinkedIn's API page.

Scraping E-Commerce sites and aggregating same products

Haq.H

[Scraping E-Commerce sites and aggregating same products](https://stackoverflow.com/questions/56349829/scraping-e-commerce-sites-and-aggregating-same-products)

I am trying to learn about web-scraping and as an application I figured I'd build an aggregator that crawls retailers for certain products and sets up a price comparison for the same product from different retailers. As I got started on this I realized exactly how large a tasks this is. First, I need to crawl sites that have various formats for not only their DOM structures but also slightly different names for the same products and formats for item's prices and prices for items on sale. Second, After I've somehow decoded the DOM for x number of sites (doing it for one or two is easy but I want to make the crawler scalable!) and fetched the data for various items. I need to be able to compare the different names of same products so I can compare the differing prices (convert them to the same currency, check if the returned price is the original/on-sale price, etc...) between retailers. I am trying to write my crawlers using Scrapy but can someone recommend an approach for how to adapt the crawler for a variety of retailers and if there are any libraries/approaches that would work well for the second problem of comparing like(unlike) items?

2019-05-28 20:46:01Z

I am trying to learn about web-scraping and as an application I figured I'd build an aggregator that crawls retailers for certain products and sets up a price comparison for the same product from different retailers. As I got started on this I realized exactly how large a tasks this is. First, I need to crawl sites that have various formats for not only their DOM structures but also slightly different names for the same products and formats for item's prices and prices for items on sale. Second, After I've somehow decoded the DOM for x number of sites (doing it for one or two is easy but I want to make the crawler scalable!) and fetched the data for various items. I need to be able to compare the different names of same products so I can compare the differing prices (convert them to the same currency, check if the returned price is the original/on-sale price, etc...) between retailers. I am trying to write my crawlers using Scrapy but can someone recommend an approach for how to adapt the crawler for a variety of retailers and if there are any libraries/approaches that would work well for the second problem of comparing like(unlike) items?For comparison you can convert strings of product names to lists, compare them and put a threshold to determine whether two products are same or not. 

Compare CSV File Records with Python Scrapy Output Data

amal

[Compare CSV File Records with Python Scrapy Output Data](https://stackoverflow.com/questions/56351206/compare-csv-file-records-with-python-scrapy-output-data)

I'm new to Python and web scraping. Pls excuse me for my ignorance. In this program, I have crawled some urls and saved product name and price to csv file. When I run the script again, I want to read the existing csv records(product name and price) and  if there's a change of the price for any product/s, I want to print a message. I'm struggling with implementing 2nd part of the program, Which is compare values against saved csv records. Please help. Any help would be highly appreciated. CSV Output

2019-05-28 23:21:57Z

I'm new to Python and web scraping. Pls excuse me for my ignorance. In this program, I have crawled some urls and saved product name and price to csv file. When I run the script again, I want to read the existing csv records(product name and price) and  if there's a change of the price for any product/s, I want to print a message. I'm struggling with implementing 2nd part of the program, Which is compare values against saved csv records. Please help. Any help would be highly appreciated. CSV OutputUse Scrapy to generate CSV files only.Write a separate script to compare CSV files.

DEBUG: Crawled (404) <GET >

Vartika Singh

[DEBUG: Crawled (404) <GET >](https://stackoverflow.com/questions/56211088/debug-crawled-404-get)

I'm trying to extract data about the various competitions offered by kaggle.I have tried fetching the data from the website through the shell as well as through the code but failed. I tried adding HTTPERROR_ALLOWED_CODES = [404] to the setting.py and made ROBOTSTXT_OBEY = False, yet the error did not go away.

2019-05-19 19:04:41Z

I'm trying to extract data about the various competitions offered by kaggle.I have tried fetching the data from the website through the shell as well as through the code but failed. I tried adding HTTPERROR_ALLOWED_CODES = [404] to the setting.py and made ROBOTSTXT_OBEY = False, yet the error did not go away.To work around the 404, setting an user-agent will do. You can do that in 'settings.py' or in the spider itself:Besides that, you won't be able to scrape the competitions using those selectors you have. Those elements are created dynamically by some javascript code, after the page load. However, you can find the data you want in a <script> tag.

To recover that, you can use a regex with .re_first(). Eg.Change the user agent in the settings as well:

Scrapy duplicating results

Timpo

[Scrapy duplicating results](https://stackoverflow.com/questions/56225394/scrapy-duplicating-results)

I am trying to parse various data items for each advert on a page such as https://www.pistonheads.com/classifieds?Category=used-cars&M=1044&ResultsPerPage=750My code catches most of the items correctly. However, I'm running into two issues:General comments about my code also appreciated. Perhaps I should be using ItemLoaders for this? (I haven't learnt how they work yet).

2019-05-20 17:09:04Z

I am trying to parse various data items for each advert on a page such as https://www.pistonheads.com/classifieds?Category=used-cars&M=1044&ResultsPerPage=750My code catches most of the items correctly. However, I'm running into two issues:General comments about my code also appreciated. Perhaps I should be using ItemLoaders for this? (I haven't learnt how they work yet).Here we iterate by item, so we never miss whether transmission appears for this item or not.

How to click next button while scraping a webpage

Tim

[How to click next button while scraping a webpage](https://stackoverflow.com/questions/56257948/how-to-click-next-button-while-scraping-a-webpage)

I am scraping a webpage using scrapy that has multiple pages of information and I need the program to click the next button and then scrape the next page and then keep doing that until all the pages have been scraped.  But I cannot figure out how to do that, I can only scrape the first page.

2019-05-22 13:25:28Z

I am scraping a webpage using scrapy that has multiple pages of information and I need the program to click the next button and then scrape the next page and then keep doing that until all the pages have been scraped.  But I cannot figure out how to do that, I can only scrape the first page.Documentation is pretty explicit about it : You could use css selector to target the next buttonwith nth-childYou can get it working like below:

Scrape different URL's with different user agents and IP Addresses

Tim

[Scrape different URL's with different user agents and IP Addresses](https://stackoverflow.com/questions/56262899/scrape-different-urls-with-different-user-agents-and-ip-addresses)

I have a program that needs to scrape several different urls using scrapy and I need it to use the same user agent and IP address for each url.  So if I am scraping like 50 urls I need each url to have one unique user agent and ip address that are only used when scraping that url.  But the IP address and user agent get changed when the program scrapes the next url.I have already got it to rotate user agents randomly but now I just need to pair user agents with different urls and use those same user agents with same urls each time.  As for the IP addresses I cannot even get it to rotate them randomly let alone pair them with one unique url.SplashSpider.pysettings.pyIn the end it should simply pair each of the urls I need to scrape with a ip address and user agent for the lists I have in my settings.py file.

2019-05-22 18:27:20Z

I have a program that needs to scrape several different urls using scrapy and I need it to use the same user agent and IP address for each url.  So if I am scraping like 50 urls I need each url to have one unique user agent and ip address that are only used when scraping that url.  But the IP address and user agent get changed when the program scrapes the next url.I have already got it to rotate user agents randomly but now I just need to pair user agents with different urls and use those same user agents with same urls each time.  As for the IP addresses I cannot even get it to rotate them randomly let alone pair them with one unique url.SplashSpider.pysettings.pyIn the end it should simply pair each of the urls I need to scrape with a ip address and user agent for the lists I have in my settings.py file.This is bit out of the scope of a simple stackoverflow question.However the general approach when dealing with customizing requests sent out by a scrapy crawler is to write a downloader middleware[1].In your example you want to write a downloader middleware that would:Briefly as code it would look like this:I haven't tested this, it's just to illustrate general ideaThen activate it, somewhere at the end of middleware chain:1 - More on scrapy's downloader middlewares: https://docs.scrapy.org/en/latest/topics/downloader-middleware.html

scrapy-selenium driver doesn't follow

madboy

[scrapy-selenium driver doesn't follow](https://stackoverflow.com/questions/56176541/scrapy-selenium-driver-doesnt-follow)

settings file:documentation on scrapy-selenium

I have followed the instructions step by step, but it the driver does not follow any links.I believe both requests are handled by scrapy. I don't want to change __init__ because I want some requests to be handled with scrapy-selenium others by scrapy(alone). I checked passing-selenium-driver-to-scrapy but it changes the entire init to make selenium as self.driver.I want some requests to be handled by SeleniumRequest others by scrapy RequestNote: I have used this site as example site that uses java to display results, if handled by scrapy (alone) data hasn't been rendered yet so empty lists will be the result

2019-05-16 20:59:10Z

settings file:documentation on scrapy-selenium

I have followed the instructions step by step, but it the driver does not follow any links.I believe both requests are handled by scrapy. I don't want to change __init__ because I want some requests to be handled with scrapy-selenium others by scrapy(alone). I checked passing-selenium-driver-to-scrapy but it changes the entire init to make selenium as self.driver.I want some requests to be handled by SeleniumRequest others by scrapy RequestNote: I have used this site as example site that uses java to display results, if handled by scrapy (alone) data hasn't been rendered yet so empty lists will be the resultI replaced firefox with chrome:

ImportError: cannot import name spider

MyNameIsCaleb

[ImportError: cannot import name spider](https://stackoverflow.com/questions/56063386/importerror-cannot-import-name-spider)

I am writing a simple web-scraping program in python and I got the program written but when I try to run it in command line(linux) by using the following command "scrapy crawl splash_spider" I get the following error message: "ImportError: cannot import name spider".Error Messageitems.pysettings.pySplashSpider.py

2019-05-09 15:58:18Z

I am writing a simple web-scraping program in python and I got the program written but when I try to run it in command line(linux) by using the following command "scrapy crawl splash_spider" I get the following error message: "ImportError: cannot import name spider".Error Messageitems.pysettings.pySplashSpider.pySpider needs a capital S.  You can see that initial class here in their code.

Split comma separated items into list in scrapy

Dan

[Split comma separated items into list in scrapy](https://stackoverflow.com/questions/56004678/split-comma-separated-items-into-list-in-scrapy)

I want to extract the keywords from following code and store them as separated list items in json.So far, I was using the following code:This will result in a json-file looking like this:Or in raw data like this:But I need them separated as follows:Or put in raw data:Any ideas how to solve this?*

2019-05-06 11:52:35Z

I want to extract the keywords from following code and store them as separated list items in json.So far, I was using the following code:This will result in a json-file looking like this:Or in raw data like this:But I need them separated as follows:Or put in raw data:Any ideas how to solve this?*Try:Or:UPD:Maybe in better to split logics on two cases, like here:Try changing code to, adding i.split(', ') within [] will generate individual arrays.

How to parse multiple child pages, merge/append and pass upwards to parent level?

delbocavista

[How to parse multiple child pages, merge/append and pass upwards to parent level?](https://stackoverflow.com/questions/55960550/how-to-parse-multiple-child-pages-merge-append-and-pass-upwards-to-parent-level)

This is my first scrapy project - and admittedly, one of my first exercises with python as well. I am looking for a way to scrape multiple child pages, merge/append the content to a single value, and pass the data BACK / UP to the originating parent page. The number of child pages per parent page is variable as well - it could be as few as 1 but would never be 0 (possibly helpful for error handling?). Additionally, child pages could repeat and re-appear since they are NOT exclusive to a single parent. I have managed to pass parent page meta data DOWN to corresponding child pages but stumped on accomplishing the inverse.Here is an example page structure:The output I am looking for (per recipe) is something like the below:I am extracting the URL for each ingredient from the respective recipe page. I need to extract the calorie count from each ingredient page, merge it with the calorie count of the other ingredients, and ideally yield a single item. Since a single ingredient is not exclusive to a single recipe, I need to be able to re-visit an ingredient page later in my crawl.(note - this is not the real example since calorie count obviously varies based on volume required by recipe)My posted code is getting me close to what I'm looking for but I have to imagine there is a more graceful way to solve the problem. The posted code is successful at passing DOWN the recipe's meta data to the ingredient level, looping through the ingredients, and appending the calorie count. Since the info is being passed down though, I am yielding at the ingredient level and creating a number of recipe duplicates (one per ingredient) until I loop through the last ingredient. At this stage, I'm looking to add in the ingredient index number so that I can somehow retain the record with the greatest Ingredient Index# per Recipe URL. Before I got to that point, I figured I would turn to the pro's here for guidance.Scraper code:As is, my less than ideal output is as follows (note the calorie list):

2019-05-02 21:34:29Z

This is my first scrapy project - and admittedly, one of my first exercises with python as well. I am looking for a way to scrape multiple child pages, merge/append the content to a single value, and pass the data BACK / UP to the originating parent page. The number of child pages per parent page is variable as well - it could be as few as 1 but would never be 0 (possibly helpful for error handling?). Additionally, child pages could repeat and re-appear since they are NOT exclusive to a single parent. I have managed to pass parent page meta data DOWN to corresponding child pages but stumped on accomplishing the inverse.Here is an example page structure:The output I am looking for (per recipe) is something like the below:I am extracting the URL for each ingredient from the respective recipe page. I need to extract the calorie count from each ingredient page, merge it with the calorie count of the other ingredients, and ideally yield a single item. Since a single ingredient is not exclusive to a single recipe, I need to be able to re-visit an ingredient page later in my crawl.(note - this is not the real example since calorie count obviously varies based on volume required by recipe)My posted code is getting me close to what I'm looking for but I have to imagine there is a more graceful way to solve the problem. The posted code is successful at passing DOWN the recipe's meta data to the ingredient level, looping through the ingredients, and appending the calorie count. Since the info is being passed down though, I am yielding at the ingredient level and creating a number of recipe duplicates (one per ingredient) until I loop through the last ingredient. At this stage, I'm looking to add in the ingredient index number so that I can somehow retain the record with the greatest Ingredient Index# per Recipe URL. Before I got to that point, I figured I would turn to the pro's here for guidance.Scraper code:As is, my less than ideal output is as follows (note the calorie list):One solution would be to scrape recipes and ingredients separately, as different items, and then do some post-processing after the crawling has finished, using regular Python for example, that merges recipe and ingredient data as needed. This is the most efficient solution.Alternatively, you could extract ingredient URLs from recipe responses and, instead of yielding requests for all of them at once, you can yield a request for the first ingredient and save the remaining ingredient URLs to the new request meta, along with the recipe item. When an ingredient response is received, you parse all needed information into meta and yield a new request for the next ingredient URL. When there are no more ingredient URLs, you yield your complete recipe item.For example:Note, however, that if two or more recipes can have the same ingredient URL, you will have to add dont_filter=True to your requests, repeating multiple requests for the same ingredients. Seriously consider the first suggestion instead, if ingredient URLs are not recipe-specific.

Converting Unicode to ASCII equivalent (SCRAPY)

carl

[Converting Unicode to ASCII equivalent (SCRAPY)](https://stackoverflow.com/questions/55967845/converting-unicode-to-ascii-equivalent-scrapy)

I am using Scrapy to crawl articles from News Website and add it to mongoDB. But while inserting i got unicode characters in MongoDb like this I have tried But it only worked when i run crawler and export data as JSON File not when storing Data in MongoDBIn spider.py file i wrote this line of code to get articleHow to replace these characters with their ASCII equivalent ?

2019-05-03 10:20:57Z

I am using Scrapy to crawl articles from News Website and add it to mongoDB. But while inserting i got unicode characters in MongoDb like this I have tried But it only worked when i run crawler and export data as JSON File not when storing Data in MongoDBIn spider.py file i wrote this line of code to get articleHow to replace these characters with their ASCII equivalent ?This solution worked for me (Character encoding in python to replace 'u2019' with ')

Use Scrapy to find duplicate images across pages

W2a

[Use Scrapy to find duplicate images across pages](https://stackoverflow.com/questions/55912253/use-scrapy-to-find-duplicate-images-across-pages)

I'm trying to use scrapy to find image URLs that are used more than once on a website across all pages.This is my spider:Is there a more (speed / memory / code length) efficient way to do this?

2019-04-30 00:03:32Z

I'm trying to use scrapy to find image URLs that are used more than once on a website across all pages.This is my spider:Is there a more (speed / memory / code length) efficient way to do this?

Request is not being proxied through middleware

MITHU

[Request is not being proxied through middleware](https://stackoverflow.com/questions/55928665/request-is-not-being-proxied-through-middleware)

I've written a script in scrapy to make a request pass through a custom middleware in order for that request to be proxied. However, the script doesn't seem to have any effect of that middleware. When I print response.meta, I get {'download_timeout': 180.0, 'download_slot': 'httpbin.org', 'download_latency': 0.9680554866790771} which clearly indicates that my request is not passing through the custom middleware. I've used CrawlerProcess to run the script.spider contains:middleware contains:Change that I've made in settings.py:The following image shows the project hierarchy:

What possible change should I bring about to make a proxied request through middleware?

2019-04-30 21:07:32Z

I've written a script in scrapy to make a request pass through a custom middleware in order for that request to be proxied. However, the script doesn't seem to have any effect of that middleware. When I print response.meta, I get {'download_timeout': 180.0, 'download_slot': 'httpbin.org', 'download_latency': 0.9680554866790771} which clearly indicates that my request is not passing through the custom middleware. I've used CrawlerProcess to run the script.spider contains:middleware contains:Change that I've made in settings.py:The following image shows the project hierarchy:

What possible change should I bring about to make a proxied request through middleware?You need to check log output of this line: [scrapy.middleware] INFO: Enabled downloader middlewares: for list of active downloader middlewares. Your middleware should be in the list if it's active.As far as I remember usage of  scrapy.contrib modules deprecated now.

Scrapy: No module named 'scrapy.contrib' Your code with custom middleware is nearly ready for usage of scrapy command line tool scrapy crawl proxiedscript.Hovewer Your crawler process needs toread_projects_settings first if need to launch scrapy application as script.

or define DOWNLOADER_MIDDLEWARES setting as argument for CrawlerProcess:   perhaps return None instead of a Request? Returning a Request prevents any other downloader middlewares from running.https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request

Amazon reviews: List index out of range

Janman

[Amazon reviews: List index out of range](https://stackoverflow.com/questions/55801839/amazon-reviews-list-index-out-of-range)

I would like to scrape the customer reviews of the kindle paperwhite of amazon. I am aware that although amazon might say the have 5900 reviews, it is only possible to access 5000 of them. (after page=500 no more reviews are displayed with 10 reviews per page).For the first few pages my spider returns 10 reviews per page, but later this shrinks to just one or two. This results in only about 1300 reviews. 

There seems to be a problem with adding the data of the variable "helpul" and "verified". Both throw the following error:Any help would be greatly appreciated!I tried implementing if statements in case the variables were empty or contained a list, but it didnt work.My Spider amazon_reviews.py:My settings.py :The extracting of the data works fine. The reviews I do get have complete and accurate information. Just the amount of reviews I get are too little.When I run the spider with the following command:The ouput on the console looks like the following:

2019-04-22 22:20:48Z

I would like to scrape the customer reviews of the kindle paperwhite of amazon. I am aware that although amazon might say the have 5900 reviews, it is only possible to access 5000 of them. (after page=500 no more reviews are displayed with 10 reviews per page).For the first few pages my spider returns 10 reviews per page, but later this shrinks to just one or two. This results in only about 1300 reviews. 

There seems to be a problem with adding the data of the variable "helpul" and "verified". Both throw the following error:Any help would be greatly appreciated!I tried implementing if statements in case the variables were empty or contained a list, but it didnt work.My Spider amazon_reviews.py:My settings.py :The extracting of the data works fine. The reviews I do get have complete and accurate information. Just the amount of reviews I get are too little.When I run the spider with the following command:The ouput on the console looks like the following:Turns out that if a review didnt't have the "verified" tag or if no one commented it, the html part scrapy was looking for isn't there and therefore no item gets added to the list which makes the "verified" and "comments" list shorter than the other ones. Because of this error all items in the list got dropped and werent added to my csv file. The simple fix below which checks if the lists are as long as the other lists worked just fine :)Edit:

When using this fix it might happen that values are assigned to the wrong review, because it is always added to the end of the list.

If you want to be on the safe side, don't scrape the verified tag or replace the whole list with "Na" or something else that indicates that the value is unclear.

Scrapy Can't Select By ID

John Rogerson

[Scrapy Can't Select By ID](https://stackoverflow.com/questions/55654109/scrapy-cant-select-by-id)

I'm trying to scrap data for a school project but for some reason I can't figure out how to gt the data from the table on this page.  I'm testing out in scrapy shell and just not getting any data back.  Here's what I'm tryingthis is returning []I've tried with the entire table class as well like suchand getting the same [] responsei'm stumped. any ideas?

2019-04-12 14:39:54Z

I'm trying to scrap data for a school project but for some reason I can't figure out how to gt the data from the table on this page.  I'm testing out in scrapy shell and just not getting any data back.  Here's what I'm tryingthis is returning []I've tried with the entire table class as well like suchand getting the same [] responsei'm stumped. any ideas?All data that you need is in another request. You can find it in developer tools in Chrome, for example. Check https://www.psacard.com/Pop/GetItemTable?headingID=51453&categoryID=20003&isPSADNA=false&pf=0&_=1555080293549. Even common GET request to this link will give you table rows.

How to know which user-agent is currently used in the scrapy spider?

AvyWam

[How to know which user-agent is currently used in the scrapy spider?](https://stackoverflow.com/questions/55658640/how-to-know-which-user-agent-is-currently-used-in-the-scrapy-spider)

I would like to know how to access which user-agent is currently used.

For instance I want to print() in the terminal during the process: the current user agent is Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1 or in the log file. How could I access to it with scrapy?Version: Scrapy 1.5.2

2019-04-12 19:57:03Z

I would like to know how to access which user-agent is currently used.

For instance I want to print() in the terminal during the process: the current user agent is Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1 or in the log file. How could I access to it with scrapy?Version: Scrapy 1.5.2If the user-agent is set like in this solutionOne can use:settings.py:middlewares.py:Using this solution or not, one can make it appear in any method of your spider class as:What changes is response added.

Scrapy/Python request priority and CONCURRENT_REQUESTS

Evren Yurtesen

[Scrapy/Python request priority and CONCURRENT_REQUESTS](https://stackoverflow.com/questions/55601375/scrapy-python-request-priority-and-concurrent-requests)

I am scraping a single domain which has a single IP. It has a list of store URLs and each store URL has a list of products (multi-page)First I go through store URLs and yield requests while assigning a priority which goes smaller for each store. Store 1 gets -1, Store 2 gets -2 etc.When loading next page of product list URLs, I assign the same priority to the request.This all works fine when CONCURRENT_REQUESTS are set. I get URLs with priority -1 and -2 loaded one after another. Scrapy does not progress to URLs with priority -3 or lower.However, I tried CONCURRENT_REQUESTS_PER_DOMAIN and CONCURRENT_REQUESTS_PER_IP and these ends up URLs with priority -1, -2, -3, -4 loaded.I would like to understand why CONCURRENT_REQUESTS work differently than ...PER_DOMAIN and ...PER_IP options as I am downloading from a single domain with single IP address.Can anybody explain this?Here is sample code to see how it fails:

2019-04-09 21:10:58Z

I am scraping a single domain which has a single IP. It has a list of store URLs and each store URL has a list of products (multi-page)First I go through store URLs and yield requests while assigning a priority which goes smaller for each store. Store 1 gets -1, Store 2 gets -2 etc.When loading next page of product list URLs, I assign the same priority to the request.This all works fine when CONCURRENT_REQUESTS are set. I get URLs with priority -1 and -2 loaded one after another. Scrapy does not progress to URLs with priority -3 or lower.However, I tried CONCURRENT_REQUESTS_PER_DOMAIN and CONCURRENT_REQUESTS_PER_IP and these ends up URLs with priority -1, -2, -3, -4 loaded.I would like to understand why CONCURRENT_REQUESTS work differently than ...PER_DOMAIN and ...PER_IP options as I am downloading from a single domain with single IP address.Can anybody explain this?Here is sample code to see how it fails:Request priority does not ensure order.If you have 16 requests, each with a different priority, and a CONCURRENT_REQUESTS_PER_DOMAIN of 16, all 16 requests are sent almost simultaneously, and they are parsed in the order they come from the server, which is unlikely to match their priority value.CONCURRENT_REQUESTS and CONCURRENT_REQUESTS_PER_DOMAIN actually work in combination. CONCURRENT_REQUESTS_PER_DOMAIN is 8 by default, and not defining CONCURRENT_REQUESTS_PER_DOMAIN is equivalent to defining it with 8 as value. If you are seeing a different behavior when setting CONCURRENT_REQUESTS_PER_DOMAIN, that must be because you used a value other than 8.CONCURRENT_REQUESTS_PER_IP, it differs from CONCURRENT_REQUESTS_PER_DOMAIN as the name suggests. If defined, the value of CONCURRENT_REQUESTS_PER_DOMAIN is ignored.

How to loop/parse through table rows in order without skipping to next instance of a variable?

Bobby Grant Jr

[How to loop/parse through table rows in order without skipping to next instance of a variable?](https://stackoverflow.com/questions/55657538/how-to-loop-parse-through-table-rows-in-order-without-skipping-to-next-instance)

I'm new to all of this so please bear with me.  I have some knowledge of Java but I decided I wanted to learn to use scrapy and python recently.As a project to help me get started I am trying to write a spider that will scrape online contact directories that I will then import to a csv.  For right now I am focusing on directories that are formatted as a .aspx table.To this point I have gotten it working exactly how I want it with one exception being the headers.  In the directory, departments are separated by th tags containing the name of each department with everyone in that department listed below.My goal is to set it up so the output is formatted like:

[Department, Name, Title, Email, Phone#]However, with my current code, when a new row is parsed, the xpath I have set to find the header skips to the next instance of that xpath. So assuming Name1 and Name2 are in Department1 rather than the output for Name2 looking like this:

[Department1, Name2, Title, Email, Phone#] 

the output looks like this because the selector for the Department header skipped to the next instance of that xpath.

[Department2, Name2, Title, Email, Phone#]

because it is the second contact entry and department is the second department entry.Below is my parse.It is set to loop through the table rows and for each row it will gather the contact information from the given xpath.I tried writing an xpath contains statement to check if there was the header's xpath first but it didn't work and ultimately only printed the headers.Here is my output (for the privacy of those in the directory I have replaced their email addresses and phone#s).As you can see, where as the second entry's header should be the same as the first, instead it has gone on to the second department header.  How can I write an if statement or some sort of rule that will return the department header as the department variable for each staff member below it until it reaches a new department header.

2019-04-12 18:20:38Z

I'm new to all of this so please bear with me.  I have some knowledge of Java but I decided I wanted to learn to use scrapy and python recently.As a project to help me get started I am trying to write a spider that will scrape online contact directories that I will then import to a csv.  For right now I am focusing on directories that are formatted as a .aspx table.To this point I have gotten it working exactly how I want it with one exception being the headers.  In the directory, departments are separated by th tags containing the name of each department with everyone in that department listed below.My goal is to set it up so the output is formatted like:

[Department, Name, Title, Email, Phone#]However, with my current code, when a new row is parsed, the xpath I have set to find the header skips to the next instance of that xpath. So assuming Name1 and Name2 are in Department1 rather than the output for Name2 looking like this:

[Department1, Name2, Title, Email, Phone#] 

the output looks like this because the selector for the Department header skipped to the next instance of that xpath.

[Department2, Name2, Title, Email, Phone#]

because it is the second contact entry and department is the second department entry.Below is my parse.It is set to loop through the table rows and for each row it will gather the contact information from the given xpath.I tried writing an xpath contains statement to check if there was the header's xpath first but it didn't work and ultimately only printed the headers.Here is my output (for the privacy of those in the directory I have replaced their email addresses and phone#s).As you can see, where as the second entry's header should be the same as the first, instead it has gone on to the second department header.  How can I write an if statement or some sort of rule that will return the department header as the department variable for each staff member below it until it reaches a new department header.use beautiful soup and try a more pythonic way

learn about requests module

i dont think xpath should be used when the html is quote organized and elegant, ie: it's in organized tables

basically in bs4, select, find, find_all functions can get the job done

no regex necessary

in requests, learn about headers, user agent, referer headerSince you are already using xpath selection you might be able to get away with using more specific selectors?:And then use class selectors to pull the category types:

Is there any way to get text inside anchor tag in Scrapy's Crawlspider?

suraj deshmukh

[Is there any way to get text inside anchor tag in Scrapy's Crawlspider?](https://stackoverflow.com/questions/55450472/is-there-any-way-to-get-text-inside-anchor-tag-in-scrapys-crawlspider)

I have a crawlspider which crawls given site upto certain dept and download the pdfs on that site. Everything works fine but along with link of pdf, i also need text inside anchor tag.for eg:consider this anchor tag, in callback i get response object and along with this object i need text inside that tag for eg 'Project Report'.

Is there any way to get this information along with the response object. i have gone through https://docs.scrapy.org/en/latest/topics/selectors.html link but it not something that i am looking for.sample code:

2019-04-01 08:02:44Z

I have a crawlspider which crawls given site upto certain dept and download the pdfs on that site. Everything works fine but along with link of pdf, i also need text inside anchor tag.for eg:consider this anchor tag, in callback i get response object and along with this object i need text inside that tag for eg 'Project Report'.

Is there any way to get this information along with the response object. i have gone through https://docs.scrapy.org/en/latest/topics/selectors.html link but it not something that i am looking for.sample code:It seems like it's not documented, but the meta attribute does contain the link text. It is updated in this line.

A minimal example would be:Which produces an output similar to:I believe the best way to achieve that is not to use crawling rules, and instead user regular crawling, with your own parse_* methods to handle all responses.Then, when you yield a request that has parse_document as callback, you can include the link text on the meta parameter of your request, and read it from response.meta on your parse_document method.

Does robots.txt prevent humans to gather data?

M. Coppée

[Does robots.txt prevent humans to gather data?](https://stackoverflow.com/questions/55284346/does-robots-txt-prevent-humans-to-gather-data)

I understand that robots.txt is a file which is intended for "robots" or should I say "automated crawler". However, does it prevent a human from typing the "forbidden" page and gather the data by hand? Maybe it's clearer with an example: I cannot crawl this page: Can I still take "manually" via the my web browser's developers tool the JSON file containing the data?

2019-03-21 15:50:23Z

I understand that robots.txt is a file which is intended for "robots" or should I say "automated crawler". However, does it prevent a human from typing the "forbidden" page and gather the data by hand? Maybe it's clearer with an example: I cannot crawl this page: Can I still take "manually" via the my web browser's developers tool the JSON file containing the data?robots.txt files are guidelines, they do not prevent anyone, human or machine, from accessing any content.The default settings.py file that is generated for a Scrapy project sets ROBOTSTXT_OBEY to True. You can set it to False if you wish.Mind that websites may employ anti-scraping measures to prevent you from scraping those pages, nonetheless. But that is a whole other topic.Based on the original robots.txt specification from 1994, the rules in a robots.txt only target robots (bold emphasis mine):So, robots are programs that automatically retrieve documents linked/referenced in other documents. If a human retrieves a document (using a browser or some other program), or if a human feeds a list of manually collected URLs to some program (and the program doesn’t add/follow references in the retrieved documents), the rules in the robots.txt do not apply.The FAQ "What is a WWW robot?" confirms this:

How to do Scrapy historical output comparison using Spidermon

Aminah Nuraini

[How to do Scrapy historical output comparison using Spidermon](https://stackoverflow.com/questions/55221375/how-to-do-scrapy-historical-output-comparison-using-spidermon)

So Scrapinghub is releasing a new feature for Scrapy quality insurance. It says it has historical comparison features where it can detect if the current scrape quantity is only below 50% of the previous scrape, which is suspicious. But, how can I apply it?

2019-03-18 12:25:47Z

So Scrapinghub is releasing a new feature for Scrapy quality insurance. It says it has historical comparison features where it can detect if the current scrape quantity is only below 50% of the previous scrape, which is suspicious. But, how can I apply it?To compare the current scraped items with a previous run you first need to store the stats of the previous run somewhere.Take the Spidermon example project on Github, specially the monitors.py file.

It has two monitors defined, ItemCountMonitor and ItemValidationMonitor, the former checks if the spider scrapped less than 1000 items, if so it will send a message on Slack. The latter checks if the item schema was validated correctly and if not it will also send a message on Slack.So now to your question.If you want to detect if the current scrape extracted 50% less items than the previous scrape you should store the scape stats in some place or even store the scraped items, let's say you store the scraped items on a directory /home/user/scraped_items/%(date)s.json, where %(date)s is the date where your spider ran (eg: 2019-01-01).

To simplify let's say you run the spider everyday and there is one file per day.Then you can write a monitor like this:Spidermon version 1.10 introduced a new stats collector, that keeps inside your .scrapy directory the stats of your last job executions (https://spidermon.readthedocs.io/en/latest/stats-collection.html). So every time you execute your spider, your will have available a stats_history property in your Spider instance containing a list of all previous stats of your jobs that were executed before. You don't need to handle the storage of your stats manually as Luiz suggested in his answer anymore (but the principle is basically the same).                                                                                                                                                     Having that information, you can create your own monitors that handles theses statistics and calculate the mean of items scraped and compare them with your latest execution for example (or you can use the stats as you want). You can see an example of a monitor like that in the docs mentioned before.

Use Items in Scrapy

lf_celine

[Use Items in Scrapy](https://stackoverflow.com/questions/55223783/use-items-in-scrapy)

I use Scrapy and try to output a json file. It works great when I'm not using item but I'd like to use them.So my spider code is:My items.py code is:And my Json output:But I want the second object inside the result's array and I don't know how to do it...

2019-03-18 14:34:10Z

I use Scrapy and try to output a json file. It works great when I'm not using item but I'd like to use them.So my spider code is:My items.py code is:And my Json output:But I want the second object inside the result's array and I don't know how to do it...Try to collect your items first, and then yield your result only once:

How to get scraped items into Pyqt5 widget?

Artsiom

[How to get scraped items into Pyqt5 widget?](https://stackoverflow.com/questions/55091095/how-to-get-scraped-items-into-pyqt5-widget)

I'm trying to make a simple GUI for Scrapy crawler, where user can push the Start button to run scraping and see the scraped results in textBrowser (or other qt widget, please advise).The app design was made in Qt Designer:And here is the code that I tried to make to handle data:With this code I could get only the stdout as text and place it in textBrowser only after finishing the scraping. And if scraping takes 20-30 minutes - I cannot see any changes in textBrowser. Is it any chance to get scraped items and display them in real time? And maybe there's a solution to stop/pause the scraping process with a second button?

2019-03-10 18:43:52Z

I'm trying to make a simple GUI for Scrapy crawler, where user can push the Start button to run scraping and see the scraped results in textBrowser (or other qt widget, please advise).The app design was made in Qt Designer:And here is the code that I tried to make to handle data:With this code I could get only the stdout as text and place it in textBrowser only after finishing the scraping. And if scraping takes 20-30 minutes - I cannot see any changes in textBrowser. Is it any chance to get scraped items and display them in real time? And maybe there's a solution to stop/pause the scraping process with a second button?Instead of using subproces.Popen() + QThread you should use QProcess since the task is easier by informing you through signals.I have created an application that scans all the spiders within a project showing them in a QComboBox where you can select which spider you want to run, then there is a button that allows you to start or stop the application by displaying the log in a QTextBrowser.Assuming that the scrapy project has the following structure (the project is an example of scrapy, you can find it here):The user has to select the .cfg file, this will show the available spiders, then press the start-stop button as desired.Output:You can listen to item_scraped event and update the UI with every new Car.

What's the easiest way to have “settings profiles” in Scrapy?

Maksim Kviatkouski

[What's the easiest way to have “settings profiles” in Scrapy?](https://stackoverflow.com/questions/55147404/whats-the-easiest-way-to-have-settings-profiles-in-scrapy)

Scrapy picks up settings from settings.py (there are default settings, project settings, per-spider settings as well). What I'm looking for is being able to have more than one file with settings and being able to switch between them as I launch my spiders quickly. If there is some inheritance between files that would be awesome too.If you know Spring Boot from Java world there is an idea of profile. You have application.settings file with your base settings. And then you can have application-dev.settings and application-prod.settings. If you run your application with option -Dspring.profiles.active=dev then it will pick up application.settings and add application-dev.settings on top of it. This way you can maintain multiple configurations in parallel and rapidly switch between them.I've found an approach for Scrapy with no supporting code required. The approach is to use SCRAPY_SETTINGS_MODULE and import base settings file in my dev and prod modules. Are there any other approaches that you use?Launch line in my case would look like:

2019-03-13 16:57:32Z

Scrapy picks up settings from settings.py (there are default settings, project settings, per-spider settings as well). What I'm looking for is being able to have more than one file with settings and being able to switch between them as I launch my spiders quickly. If there is some inheritance between files that would be awesome too.If you know Spring Boot from Java world there is an idea of profile. You have application.settings file with your base settings. And then you can have application-dev.settings and application-prod.settings. If you run your application with option -Dspring.profiles.active=dev then it will pick up application.settings and add application-dev.settings on top of it. This way you can maintain multiple configurations in parallel and rapidly switch between them.I've found an approach for Scrapy with no supporting code required. The approach is to use SCRAPY_SETTINGS_MODULE and import base settings file in my dev and prod modules. Are there any other approaches that you use?Launch line in my case would look like:Firstly, if you're only going to change one or two values, then it would be simpler to use a single dynamic settings.py (as mentioned in Gallaecio's answer).However, if you really need separate settings, there is an even shorter way by defining separate "projects" in scrapy.cfg (docs):Then to run a specific one:If you don't specify SCRAPY_PROJECT it will use default.And yes, you can inherit from settings files. Replace your settings.py file with a module instead:In base.py you can have exactly what you have in settings.py. Then at the top of each override file you add:That wildcard import is usually a bad practice, but in this case since it's just a plain Python file so the end result is just having all the variables available. This is a trick we often use in Django (example).I believe SCRAPY_SETTINGS_MODULE is the best approach.Alternatively, since a settings module is a Python script, you could change settings dynamically from within settings.py. I’ve seen this done, for example, to detect automatically whether a spider is running in a local machine or on a Scrapyd server, and adjust the settings accordingly at run time.

Web Crawler not printing pages correctly

M. Coppée

[Web Crawler not printing pages correctly](https://stackoverflow.com/questions/55163841/web-crawler-not-printing-pages-correctly)

Good morning !I've developed a very simple spider with Scrapy just to get used with FormRequest. I'm trying to send a request to this page: https://www.caramigo.eu/ which should lead me to a page like this one: https://www.caramigo.eu/be/fr/recherche?address=Belgique%2C+Li%C3%A8ge&date_debut=16-03-2019&date_fin=17-03-2019. The issue is that my spider does not prompt the page correctly (the cars images and info do not appear at all) and therefore I can't collect any data from it. Here is my spider:Sorry if the syntax is not correct, I'm pretty new to coding.Thank you in advance ! 

2019-03-14 13:30:37Z

Good morning !I've developed a very simple spider with Scrapy just to get used with FormRequest. I'm trying to send a request to this page: https://www.caramigo.eu/ which should lead me to a page like this one: https://www.caramigo.eu/be/fr/recherche?address=Belgique%2C+Li%C3%A8ge&date_debut=16-03-2019&date_fin=17-03-2019. The issue is that my spider does not prompt the page correctly (the cars images and info do not appear at all) and therefore I can't collect any data from it. Here is my spider:Sorry if the syntax is not correct, I'm pretty new to coding.Thank you in advance ! 

Online one spider is running from from scrapy import cmdline

Saiful Hasan

[Online one spider is running from from scrapy import cmdline](https://stackoverflow.com/questions/55052044/online-one-spider-is-running-from-from-scrapy-import-cmdline)

This is my python script to run my spider name ndtv ,republic , thehindu ,zee and indiatvwhen I run my script then only the first spider i.e. ndtv spider is running and script close without open/running the other spider. I need a way to run all spider.

2019-03-07 20:10:08Z

This is my python script to run my spider name ndtv ,republic , thehindu ,zee and indiatvwhen I run my script then only the first spider i.e. ndtv spider is running and script close without open/running the other spider. I need a way to run all spider.

scrapy-spash: SplashRequest response object differs between invocation by scrapy crawl vs CrawlerProcess

user2081488

[scrapy-spash: SplashRequest response object differs between invocation by scrapy crawl vs CrawlerProcess](https://stackoverflow.com/questions/55084220/scrapy-spash-splashrequest-response-object-differs-between-invocation-by-scrapy)

I'd like to use scrapy-splash to fetch both the html and screenshot png of the target page. I need to be able to invoke it programmatically. According to the spashy doc, specifying and passing argumentshould result in a response object ('scrapy_splash.response.SplashJsonResponse') with a .data attribute that contains decoded JSON data representing a png screenshot of the target page.When the spider (here named 'search') is invoked withThe result is as expected, with response.data['png'] containing the png data.However, if it is invoked via scrapy's CrawlerProcess, a different response object is returned: 'scrapy.http.response.html.HtmlResponse'. This object does not have the .data attribute.Here's the code:Restating: and invoking byresponse is typeBut settingand running the script with CrawlerProcess results in response of type(p.s. I had some trouble with ReactorNotRestartable, so adopted crochet as decribed in this post, which seems to have fixed the problem. I confess I don't understand why, but assume it is unrelated...)Any thoughts on how to debug this?

2019-03-10 03:41:20Z

I'd like to use scrapy-splash to fetch both the html and screenshot png of the target page. I need to be able to invoke it programmatically. According to the spashy doc, specifying and passing argumentshould result in a response object ('scrapy_splash.response.SplashJsonResponse') with a .data attribute that contains decoded JSON data representing a png screenshot of the target page.When the spider (here named 'search') is invoked withThe result is as expected, with response.data['png'] containing the png data.However, if it is invoked via scrapy's CrawlerProcess, a different response object is returned: 'scrapy.http.response.html.HtmlResponse'. This object does not have the .data attribute.Here's the code:Restating: and invoking byresponse is typeBut settingand running the script with CrawlerProcess results in response of type(p.s. I had some trouble with ReactorNotRestartable, so adopted crochet as decribed in this post, which seems to have fixed the problem. I confess I don't understand why, but assume it is unrelated...)Any thoughts on how to debug this?If you're running this code as a standalone script the settings module will never be loaded and your crawler will not know about the Splashy middleware (which is what's adding the .data attribute you're referencing in .parse).You can load these settings within your script by calling get_project_settings and passing the result to your Crawler:

TypeError: close_spider() missing 1 required positional argument: 'reason'

Emanuel Agnelli

[TypeError: close_spider() missing 1 required positional argument: 'reason'](https://stackoverflow.com/questions/55012786/typeerror-close-spider-missing-1-required-positional-argument-reason)

When executing the spider data is extracted from the page but when the pipeline starts something goes wrong... I get the following error:I'm sending the request through Scrapy Splash to execute some java on the page and then extracting the links information... however this is the first time I get this error.This is my spiderThis is my pipelineand my Settings:

2019-03-05 22:43:10Z

When executing the spider data is extracted from the page but when the pipeline starts something goes wrong... I get the following error:I'm sending the request through Scrapy Splash to execute some java on the page and then extracting the links information... however this is the first time I get this error.This is my spiderThis is my pipelineand my Settings:While scrapy Pipeline is expected to have close_spider(self, spider) method the actual signal callback is expected to be close_spider(self, spider, reason).Something in your code changed the pipeline's close_spider method to be a direct signal callback. You can fix that by adjusting your method signature to include reason:See signals documentation on spider_closed

And scrapy Pipeline.close_spider

GUI and user interaction for Scrapy+Splash (osx)

ddofborg

[GUI and user interaction for Scrapy+Splash (osx)](https://stackoverflow.com/questions/54990580/gui-and-user-interaction-for-scrapysplash-osx)

I'm looking for a way to be able to interact with Scrapy+Splash. Something like opening a browser window and see what is happening, but also click around if needed.Is there a good way for this?The method from https://splash.readthedocs.io/en/stable/kernel.html#live-webkit-windowdidn't work.

2019-03-04 19:52:54Z

I'm looking for a way to be able to interact with Scrapy+Splash. Something like opening a browser window and see what is happening, but also click around if needed.Is there a good way for this?The method from https://splash.readthedocs.io/en/stable/kernel.html#live-webkit-windowdidn't work.

Xpath for finding text inside of an anchor tag using class (Scrapy)

adnan

[Xpath for finding text inside of an anchor tag using class (Scrapy)](https://stackoverflow.com/questions/54942552/xpath-for-finding-text-inside-of-an-anchor-tag-using-class-scrapy)

I am trying to extract the link of Anchor Tag using Xpath URLCode     And the python functionPlease tell me what I am doing wrong

The expected output would be the text of Anchor Tag. e.g Business

2019-03-01 10:21:09Z

I am trying to extract the link of Anchor Tag using Xpath URLCode     And the python functionPlease tell me what I am doing wrong

The expected output would be the text of Anchor Tag. e.g Business/text() is meant to get the element's inner text. To extract the href attribute use /@href instead.Here is a handy xpath cheatsheet

Scrapy can't return reference because it takes up 2 lines

john mondego

[Scrapy can't return reference because it takes up 2 lines](https://stackoverflow.com/questions/54966020/scrapy-cant-return-reference-because-it-takes-up-2-lines)

The first line of the table is easy to scrape because the title "P/E Ratio (TTM)" is written neatly on one line. The other lines of the table however, I can't seem to scrape because "P/E High - Last 5 Yrs." takes up 2 lines and I have no idea how to reference it. Same goes for "P/E Low - Last 5 Yrs."I am trying to return the numbers in this table. What I have so far that works is:I have a feeling I'm missing something very obvious here. Here is the link to the site: https://www.reuters.com/finance/stocks/financial-highlights/ABAny help would be greatly appreciated. Thank you!

2019-03-03 05:59:14Z

The first line of the table is easy to scrape because the title "P/E Ratio (TTM)" is written neatly on one line. The other lines of the table however, I can't seem to scrape because "P/E High - Last 5 Yrs." takes up 2 lines and I have no idea how to reference it. Same goes for "P/E Low - Last 5 Yrs."I am trying to return the numbers in this table. What I have so far that works is:I have a feeling I'm missing something very obvious here. Here is the link to the site: https://www.reuters.com/finance/stocks/financial-highlights/ABAny help would be greatly appreciated. Thank you!You can use xpath like shown below to access the <td> under <tr> having text like as mentioned in questionnormalize-space() trims space at both ends of the text so you can match it even if it is spread in multiple lines

C:\Python37\python.exe: can't open file 'scrapy': [Errno 2] No such file or dire ctory

AvyWam

[C:\Python37\python.exe: can't open file 'scrapy': [Errno 2] No such file or dire ctory](https://stackoverflow.com/questions/54867048/c-python37-python-exe-cant-open-file-scrapy-errno-2-no-such-file-or-dire)

My paths environment in windows 7:Few days ago, without modified stuffs since, I used to run my spider like this:I run it in the project spider as C:\Users\Truc\FolderA\FolderB\FolderC\...\spiders

it worked well, no problem, but today by a magic I obtain this message:How is it possible when I did not change anything?I did try the solution of @YOU in this topic, so create a .py file in C:\Users\Truc\FolderA\FolderB\FolderC\...\spiders edited it as said, and obtain this error:I did check if there is cmdline and there is, but 'scrapy' is not a package is this weird.

2019-02-25 13:13:10Z

My paths environment in windows 7:Few days ago, without modified stuffs since, I used to run my spider like this:I run it in the project spider as C:\Users\Truc\FolderA\FolderB\FolderC\...\spiders

it worked well, no problem, but today by a magic I obtain this message:How is it possible when I did not change anything?I did try the solution of @YOU in this topic, so create a .py file in C:\Users\Truc\FolderA\FolderB\FolderC\...\spiders edited it as said, and obtain this error:I did check if there is cmdline and there is, but 'scrapy' is not a package is this weird.Test it in the python REPL if you can import scrapy.Change py scrapy to simply scrapy in your call.

Lua script failed clicking on a button

Elad Aharon

[Lua script failed clicking on a button](https://stackoverflow.com/questions/54811946/lua-script-failed-clicking-on-a-button)

I'm trying to scrape flights from link with scrapy-splash using this lua script:and from some reason I'm getting this error:Does anyone know why this happens?

Also does anyone know where I can get a toturial for lua script integration with splash? besides the offical site?Thanks in advance!

2019-02-21 16:30:00Z

I'm trying to scrape flights from link with scrapy-splash using this lua script:and from some reason I'm getting this error:Does anyone know why this happens?

Also does anyone know where I can get a toturial for lua script integration with splash? besides the offical site?Thanks in advance!This just looks like a timing issue. I ran your Lua script a couple of times and I got that error only once.Simply waiting longer before getting the button should be enough. However, if the time it takes varies a lot and you don't always want to wait the full time, then you can try a slightly smarter loop like this:

How the scrapy on python3 can get text datas working on the javascript

bluebamus

[How the scrapy on python3 can get text datas working on the javascript](https://stackoverflow.com/questions/54817969/how-the-scrapy-on-python3-can-get-text-datas-working-on-the-javascript)

https://www.reddit.com/r/gameofthrones/In this page, I try to get a time info from some time text like '14 days ago'.In the browser, when I move a mouse point to that text, it show the date-time based information. But if I view the web page source, I can't find it. So I think it work based on Javascript.I am not web-developer, and I'm not sure how can I trace the problem.My question is: how can I trace the data working based on the javascript using chrome dev-tools and scrapy shell?

2019-02-21 23:45:26Z

https://www.reddit.com/r/gameofthrones/In this page, I try to get a time info from some time text like '14 days ago'.In the browser, when I move a mouse point to that text, it show the date-time based information. But if I view the web page source, I can't find it. So I think it work based on Javascript.I am not web-developer, and I'm not sure how can I trace the problem.My question is: how can I trace the data working based on the javascript using chrome dev-tools and scrapy shell?You need to use Reddit's read-only (no participation) subdomain:https://np.reddit.com/r/gameofthrones/Note that it is np and not www. In this view you get the full date and time in the HTML in the <time> tag (in any browser) without running Javascript:

Example:...Reddit has an official API. When that is the case I would seriously consider using it instead of HTML scraping.On the other hand, if you really want to do HTML scraping then I would suggest you use something like Scrapy Splash to trigger that Javascript. Trying to find the raw data hidden away in Javascript objects probably isn't worth the effort. Especially if the Javascript does additional processing which you'll need to replicate because it won't run on plain Scrapy anyway.this picture result captured on the explorer.

raise SerializationError

joes

[raise SerializationError](https://stackoverflow.com/questions/54829632/raise-serializationerror)

i'm using elasticsearch to save scrapy Data but 

when i run my code i get this error:raise SerializationError(data, e)elasticsearch.exceptions.SerializationError: ({'real_estate_ID': [],but it function with the other items i get a problem only with the item : real_estate_ID

2019-02-22 14:49:01Z

i'm using elasticsearch to save scrapy Data but 

when i run my code i get this error:raise SerializationError(data, e)elasticsearch.exceptions.SerializationError: ({'real_estate_ID': [],but it function with the other items i get a problem only with the item : real_estate_ID

Tabled data with user's additional elements into csv using scrapy

Rickoshet

[Tabled data with user's additional elements into csv using scrapy](https://stackoverflow.com/questions/54833211/tabled-data-with-users-additional-elements-into-csv-using-scrapy)

I have a running script using scrapy which takes data from the table. But it's saving in the format because original data is in row-argument order:How can I save this dict in row format without 'name' likeI already have the list which contains the current time, so I need to rewrite this dict as a list to parse it into CSV.EDIT I replaced dictionary's key with current_time argument, but problem with output format still exists.EDIT

Target Html code with replaced values(I need the value of all 'Item'):

2019-02-22 18:37:06Z

I have a running script using scrapy which takes data from the table. But it's saving in the format because original data is in row-argument order:How can I save this dict in row format without 'name' likeI already have the list which contains the current time, so I need to rewrite this dict as a list to parse it into CSV.EDIT I replaced dictionary's key with current_time argument, but problem with output format still exists.EDIT

Target Html code with replaced values(I need the value of all 'Item'):Item / ItemLoader mechanism serves your purpose. Something like:Define an Item for data row:Then declare the matching ItemLoader:In the parse function:And then serialize the items in CSV using for example this method: Export csv file from scrapy (not via command line)

INSERT scrapy data after check no row insert before

Syaifur Rohman

[INSERT scrapy data after check no row insert before](https://stackoverflow.com/questions/54779902/insert-scrapy-data-after-check-no-row-insert-before)

After I learnt to crawling and able to insert data to mysqlInsert multiple Scrapy data into mysqlI found another problem when I insert several crawling data my data in mysql become big because there are too many redundant data (duplicate data)I tried to make INSERT with WHERE NOT EXIST condition like I did in php but not work ni pythonThis is my sqlI hope somebody help me since the error keep say

2019-02-20 06:11:55Z

After I learnt to crawling and able to insert data to mysqlInsert multiple Scrapy data into mysqlI found another problem when I insert several crawling data my data in mysql become big because there are too many redundant data (duplicate data)I tried to make INSERT with WHERE NOT EXIST condition like I did in php but not work ni pythonThis is my sqlI hope somebody help me since the error keep sayThe error is about duplicate quotes. You are getting that because you have added your own ' quotes for the last variable in the query string. Those quotes get added automatically, so remove them:WHERE NOT EXISTS (SELECT judul FROM berita WHERE judul like %s)

In Scrapy, download files nested below the to of a yielded item dict

Toby

[In Scrapy, download files nested below the to of a yielded item dict](https://stackoverflow.com/questions/54802733/in-scrapy-download-files-nested-below-the-to-of-a-yielded-item-dict)

To download files in Scrapy, one adds the key 'fileurls' to the yielded item dict with a value of the urls to download. But my files are nested somewhere below the top level of the yielded dict. An item looks like this:Ideally, I'd like each file downloaded and have scrapy add its "file" element next to the "fileurl". But this does not seem to work automatically.How can I achieve this? The current version of Scrapy is 1.6.0.

2019-02-21 08:46:53Z

To download files in Scrapy, one adds the key 'fileurls' to the yielded item dict with a value of the urls to download. But my files are nested somewhere below the top level of the yielded dict. An item looks like this:Ideally, I'd like each file downloaded and have scrapy add its "file" element next to the "fileurl". But this does not seem to work automatically.How can I achieve this? The current version of Scrapy is 1.6.0.To do something like this, you will need to make your own subclass of scrapy's FilesPipeline.To make the downloading happen, you'll need a custom get_media_requests method, which should get the URLs from your item and return an iterable of requests which will be used to download the files.After that, you'll also need to modify the item_completed and/or the file_downloaded method to store the result in the exact way you want.If you need more details than what's provided in the docs, take a look at the source and see how the existing pipeline works.

Cannot import Items to Scrapy Spider [No module named] - Python

Matija Žiberna

[Cannot import Items to Scrapy Spider [No module named] - Python](https://stackoverflow.com/questions/54770529/cannot-import-items-to-scrapy-spider-no-module-named-python)

I have troubles importing Items.py from parent folder when using Scrapy framework. I've read that this is a fairly common issue but none of the proposed solution could work for me so far.I've created a new project via scrapy startproject ime_projekta. CMD prompted me with: The error I am presented is No module named as indicated in CMD below. Any help would be great.Thank you

2019-02-19 16:10:26Z

I have troubles importing Items.py from parent folder when using Scrapy framework. I've read that this is a fairly common issue but none of the proposed solution could work for me so far.I've created a new project via scrapy startproject ime_projekta. CMD prompted me with: The error I am presented is No module named as indicated in CMD below. Any help would be great.Thank youThere is no __init__.py file in the ime_projeckta folder. Either add one, or switch the line 3 of worldmaps.py to a relative import (from ..items).Also note that you are trying to import Item from the wrong import path. It should be ime_projeckta.items.I've solved this by putting my spider in the same solder as items.py

Scrapy Spider Close

dcarlo56ave

[Scrapy Spider Close](https://stackoverflow.com/questions/54656702/scrapy-spider-close)

I have a script that I need to run after my spider closes. I see that Scrapy has a handler called spider_closed() but what I dont understand is how to incorporate this into my script. What I am looking to do is once the scraper is done crawling I want to combine all my csv files them load them to sheets. If anyone has any examples of this can be done that would be great.

2019-02-12 18:43:31Z

I have a script that I need to run after my spider closes. I see that Scrapy has a handler called spider_closed() but what I dont understand is how to incorporate this into my script. What I am looking to do is once the scraper is done crawling I want to combine all my csv files them load them to sheets. If anyone has any examples of this can be done that would be great.As per the example in the documentation, you add the following to your Spider:As per the comments on my other answer about a signal-based solution, here is a way to run some code after multiple spiders are done. This does not involve using the spider_closed signal.

scrape product specification from amazon using scrapy

sachin safale

[scrape product specification from amazon using scrapy](https://stackoverflow.com/questions/54585656/scrape-product-specification-from-amazon-using-scrapy)

Hello i want to scrape product specification table available on the product page of  the link : https://www.amazon.com/dp/B07HJ41HCF for which i have written the following spider in scrapy.In the above code everything works fine but the item['proddescription'] does yield an empty list any help with the above will be highly appreciated

2019-02-08 03:32:39Z

Hello i want to scrape product specification table available on the product page of  the link : https://www.amazon.com/dp/B07HJ41HCF for which i have written the following spider in scrapy.In the above code everything works fine but the item['proddescription'] does yield an empty list any help with the above will be highly appreciatedWorked for your variant:

How to crawl internal links via javascript?

johncsmith427

[How to crawl internal links via javascript?](https://stackoverflow.com/questions/54575632/how-to-crawl-internal-links-via-javascript)

I am trying to crawl all the internal links of a site but running into some issues getting the links to crawl via scrapy as it looks like they are being generated via javascript. One page for example of this site is here https://www.vecteezy.com/vector-art/274468-wavy-lines-pastel-backgroundI have tried running the below code and not having luck getting links/tags in the section called "This Image Appears In Searches For" on the pageThe links I am trying to get would be to the tags you see below this image appears in searches for: pastel background, pastel, background, abstract, wavy, line, diagonal, seamless..etcLooks like after some updates I could get it to start crawling more but now it won't crawl the entire site? Here's the dump after the crawl completedINFO: Dumping Scrapy stats:

2019-02-07 14:31:16Z

I am trying to crawl all the internal links of a site but running into some issues getting the links to crawl via scrapy as it looks like they are being generated via javascript. One page for example of this site is here https://www.vecteezy.com/vector-art/274468-wavy-lines-pastel-backgroundI have tried running the below code and not having luck getting links/tags in the section called "This Image Appears In Searches For" on the pageThe links I am trying to get would be to the tags you see below this image appears in searches for: pastel background, pastel, background, abstract, wavy, line, diagonal, seamless..etcLooks like after some updates I could get it to start crawling more but now it won't crawl the entire site? Here's the dump after the crawl completedINFO: Dumping Scrapy stats:

Scrapy not getting clean text using extract_first()

Sohan Das

[Scrapy not getting clean text using extract_first()](https://stackoverflow.com/questions/54597251/scrapy-not-getting-clean-text-using-extract-first)

I'm trying to scrape some text from a website under many span tags, but not getting clean text, any help would be appreciated!Here is the url: This is what i'm tryingexpected output:

2019-02-08 17:10:09Z

I'm trying to scrape some text from a website under many span tags, but not getting clean text, any help would be appreciated!Here is the url: This is what i'm tryingexpected output:You can get required text by extracting string representation of the div:You need to grab the xpath text() for everything inside your given xpath.

For example:This is going to return multiple span elements, so you have to use extract().

Then you can join and clean it however you want, maybe like:There is one useful lib for this task (from creators of Scrapy), you should try it: https://github.com/TeamHG-Memex/html-textOut[4]: 'Level 18, 25 Bligh Street, SYDNEY, NSW 2000'

Scrapy: crawler doesn't crawl

ewha

[Scrapy: crawler doesn't crawl](https://stackoverflow.com/questions/54452808/scrapy-crawler-doesnt-crawl)

I'm obviously new to python, scrapy and programming in general. I'm trying to scrape this site but my code doesn't seem to work. All the examples and tutorials I found deal with simple and plain websites. Or maybe I just can't get my head around it. There are hundreds of results I need to scrape, and I really don't want to do it manually.  So at this instance im just trying to only get the href from the div object to check if it works. It doesn't. When run on the terminal (ignoring robots) it returns:Thanks for any help you can provide.

2019-01-31 03:17:46Z

I'm obviously new to python, scrapy and programming in general. I'm trying to scrape this site but my code doesn't seem to work. All the examples and tutorials I found deal with simple and plain websites. Or maybe I just can't get my head around it. There are hundreds of results I need to scrape, and I really don't want to do it manually.  So at this instance im just trying to only get the href from the div object to check if it works. It doesn't. When run on the terminal (ignoring robots) it returns:Thanks for any help you can provide.It is for you). understand it yourselfOUTPUT:As far as I can see, there are really no such elements on the page:But if you will try:I have visited the website you are trying to scrap, but your css does not seems to be matching any attribute in HTML.There is no any tag with the class name of m-dealer_list__rowAll i see is m-dealer_citylistIf i describe your css:You css describes that you are extracting a div element with 2 classes one is row and the second one is m-dealer_list__rowif you want a div with row class then in this div any tag with m-dealer_list__row you can try this:

Stuck on extracting title and next page url from pages

johncsmith427

[Stuck on extracting title and next page url from pages](https://stackoverflow.com/questions/54544398/stuck-on-extracting-title-and-next-page-url-from-pages)

I am trying to extract the link of each rv unit detail page in these search results as well as the next page of search results so I can get the links to every rv unit they have on there siteExample url for each detail unit would be: https://rv.campingworld.com/rvdetails/new-class-c-rvs/2019-thor-freedom-elite-26he-front-living-60k-BKY1571461Next page url would be:

https://rv.campingworld.com/searchresults?condition=new_used&custompricerange=true&custompaymentrange=true&sort=featured_asc&zipsearch=true&search_mode=advanced&locations=nationwide&scpc=&make=&landingMake=0&page=2

2019-02-05 23:08:46Z

I am trying to extract the link of each rv unit detail page in these search results as well as the next page of search results so I can get the links to every rv unit they have on there siteExample url for each detail unit would be: https://rv.campingworld.com/rvdetails/new-class-c-rvs/2019-thor-freedom-elite-26he-front-living-60k-BKY1571461Next page url would be:

https://rv.campingworld.com/searchresults?condition=new_used&custompricerange=true&custompaymentrange=true&sort=featured_asc&zipsearch=true&search_mode=advanced&locations=nationwide&scpc=&make=&landingMake=0&page=2I've tried your code in scrapy shell and everything looks fine:What kind of problem did you meet?

In which file/place should Scrapy process the data?

Sergey

[In which file/place should Scrapy process the data?](https://stackoverflow.com/questions/54421455/in-which-file-place-should-scrapy-process-the-data)

Scrapy has several points/places where allowed processing scraped data: spider, items and spider middlewares. But I don't understand where I should do it right. I can process some scraped data in all these places. Could you explain to me differences between them in detail?For example: downloader middleware returns some data to the spider (number, short string, url, a lot of HTML, list and other). And what and where i should do with them? I understand what to do, but is not clear where to do it...

2019-01-29 12:47:14Z

Scrapy has several points/places where allowed processing scraped data: spider, items and spider middlewares. But I don't understand where I should do it right. I can process some scraped data in all these places. Could you explain to me differences between them in detail?For example: downloader middleware returns some data to the spider (number, short string, url, a lot of HTML, list and other). And what and where i should do with them? I understand what to do, but is not clear where to do it...Spiders are the main point where you define how to extract data, as items. When in doubt, implement your extraction logic in your spider only, and forget about the other Scrapy features.Item loaders, item pipelines, downloader middlewares, spider middlewares and extensions are used mainly for code sharing in scraping projects that have several spiders.If you ever find yourself repeating the same code in two or more spiders, and you decide to stop repeating yourself, then you should go into those components and choose which ones to use to simplify your codebase my moving existing, duplicate code into one or more components of these types.It is generally a better approach than simply using class inheritance between Spider subclasses.As to how to use each component:I will try to explain in orderSpider is the one where you decide which URLs to make requests toDownloadMiddleware has a process_request method which is called before a request to URL is made, and it has process_response method which is called once response from that URL is receivedPipeline is the thing where data is sent when you yield a dictionary from your Spider

Extract content from meta-tag in <head> with Xpath using multiple conditions

runner2018

[Extract content from meta-tag in <head> with Xpath using multiple conditions](https://stackoverflow.com/questions/54385604/extract-content-from-meta-tag-in-head-with-xpath-using-multiple-conditions)

I want to select the xpath of a meta-tag with two conditions. Usually it works like this:However, the meta-tag I want to extract looks like this:And I tried:Also:Also tried several other options, but none of them work.Does anybody know how to solve this problem?

2019-01-27 06:23:55Z

I want to select the xpath of a meta-tag with two conditions. Usually it works like this:However, the meta-tag I want to extract looks like this:And I tried:Also:Also tried several other options, but none of them work.Does anybody know how to solve this problem?You have space in tag < meta, so I also did not succeed to extract data from it.

But you can try:After Observing you website., meta Tag is actually: To extract the content use the following xpath:Moreover there are multiple meta tags under same Selector. xml:lang is the attribute that differentiate the content but xpaths or css' does not handle this kind of attribute with delimiter. you got to do this:Now to get the respective language description e.g; 'en'

How to ensure every URL is being parsed in my Scrapy spider

Devin

[How to ensure every URL is being parsed in my Scrapy spider](https://stackoverflow.com/questions/54474018/how-to-ensure-every-url-is-being-parsed-in-my-scrapy-spider)

I'm attempting to crawl every page of recipe listings on a food blog, scrape the recipe URLs on each page, and write them all to a single .txt file. As my code currently stands, it works properly, but only for the first URL listed within urls inside the start_requests method.I've added a .log() to check that urls does indeed contain all the correct URLs I'm trying to scrape from and when I execute Scrapy in command prompt, I get the following confirmation that they're there:etc.My current code:When I run the above, I get the following output written to links.txt:The links here are correct, but there should be 50+ more pages worth of them.Any suggestions? What am I missing?

2019-02-01 06:27:39Z

I'm attempting to crawl every page of recipe listings on a food blog, scrape the recipe URLs on each page, and write them all to a single .txt file. As my code currently stands, it works properly, but only for the first URL listed within urls inside the start_requests method.I've added a .log() to check that urls does indeed contain all the correct URLs I'm trying to scrape from and when I execute Scrapy in command prompt, I get the following confirmation that they're there:etc.My current code:When I run the above, I get the following output written to links.txt:The links here are correct, but there should be 50+ more pages worth of them.Any suggestions? What am I missing?What I understand is that you want to make sure every page inside urls got scraped successfully and had links in it, if yes, then see this code belowWhat it does is that it appends all urls to be scraped into self.urls and once a URL is scraped and also has links in it, it removes from self.urlsAnd notice there is another method called spider_closed, it is executed only when scraper is finished, so it will print the urls which were not scraped or it didnt had links in itAlso, why using BeautifulSoup? simply use Python Scrapy's Selector class

Cannot run 'scrapy crawl quotes'

Jaaks

[Cannot run 'scrapy crawl quotes'](https://stackoverflow.com/questions/54337540/cannot-run-scrapy-crawl-quotes)

Cannot get scrapy tutorial to work.Am trying to learn scrapy but can't get even the tutorial to run. I have tried to run this in python 3.7 & 3.5.5 with the same resultsimport scrapyclass QuotesSpider(scrapy.Spider):

    name = "quotes"This appears to run OK. At least it throws no errors.When I run "scrapy crawl quotes" in Anaconda prompt window, I get this:"The output should be similar to this:Thank in advance for any help you can give.

2019-01-24 00:07:31Z

Cannot get scrapy tutorial to work.Am trying to learn scrapy but can't get even the tutorial to run. I have tried to run this in python 3.7 & 3.5.5 with the same resultsimport scrapyclass QuotesSpider(scrapy.Spider):

    name = "quotes"This appears to run OK. At least it throws no errors.When I run "scrapy crawl quotes" in Anaconda prompt window, I get this:"The output should be similar to this:Thank in advance for any help you can give.Perhaps your source code has been placed in the wrong directory?I had a very similar, if not the same, problem. (I am not using Anaconda, but the error was also "line 69, in load return self._spiders[spider_name] KeyError: 'quotes'".What fixed it for me was moving the source code file (quotes_spider.py) from the projectname/tutorial/tutorial directory to the projectname/tutorial/tutorial/spiders directory.From the tutorial page . . . 

"This is the code for our first Spider. Save it in a file named quotes_spider.py under the tutorial/spiders directory in your project"I believe I found the answer. The tutorial doesn't mention one step that is only mentioned in your command line after you create the project viaThe output for that command, besides creating your tutorial project, is For the tutorial to work, you need to entername is required and unique for every spider you create. You can check this blog for getting started on Scrapy

https://www.inkoop.io/blog/web-scraping-using-python-and-scrapy/

Scrapinghub Getting Error caught on signal handler: <bound method ? on Yield

Bashar Abdullah

[Scrapinghub Getting Error caught on signal handler: <bound method ? on Yield](https://stackoverflow.com/questions/54419576/scrapinghub-getting-error-caught-on-signal-handler-bound-method-on-yield)

I have a scrapy script that works locally, but when I deploy it to Scrapinghub, it's giving all errors. Upon debugging, the error is coming from Yielding the item.This is the error I get.It doesn't specify the field with issues, but by process of elimination, I came to realize it's this part of the code:I cleared some parts up to make it easier to tackle, but basically this part is the issue. Something it doesn't like about the item media.I'm beginner in both Python and Scrapy. So sorry if this turns out to be silly basic Python mistake. Any idea?EDIT: So after getting the answer from ThunderMind, the solution was to simply do str(media_index) for key

2019-01-29 11:02:18Z

I have a scrapy script that works locally, but when I deploy it to Scrapinghub, it's giving all errors. Upon debugging, the error is coming from Yielding the item.This is the error I get.It doesn't specify the field with issues, but by process of elimination, I came to realize it's this part of the code:I cleared some parts up to make it easier to tackle, but basically this part is the issue. Something it doesn't like about the item media.I'm beginner in both Python and Scrapy. So sorry if this turns out to be silly basic Python mistake. Any idea?EDIT: So after getting the answer from ThunderMind, the solution was to simply do str(media_index) for keyYeah, right here:media_index is a mutable. and Keys can't be mutable.

Read Python dict, to know what should be used as keys.

how to specific TLS version or disable TLS verify in scrapy

zpoint

[how to specific TLS version or disable TLS verify in scrapy](https://stackoverflow.com/questions/54339183/how-to-specific-tls-version-or-disable-tls-verify-in-scrapy)

For the site: https://www.cnbanbao.cn/I tried this command on my MACAnd the result:I think the problem may be that the site use TLS1.2 for ServerHello but TLS1.0 for TLS Handshake, which cause the problem when I try to download the site in scrapyI tried to specific the TLS1.0 version: SSL issue when scraping website, it seems not workI also tried to Disable SSL certificate verification in Scrapy, But I don't know how to define a HttpsDownloaderIgnoreCNError to disable ssl verificationAny idea to make the follwing command work?

2019-01-24 04:01:28Z

For the site: https://www.cnbanbao.cn/I tried this command on my MACAnd the result:I think the problem may be that the site use TLS1.2 for ServerHello but TLS1.0 for TLS Handshake, which cause the problem when I try to download the site in scrapyI tried to specific the TLS1.0 version: SSL issue when scraping website, it seems not workI also tried to Disable SSL certificate verification in Scrapy, But I don't know how to define a HttpsDownloaderIgnoreCNError to disable ssl verificationAny idea to make the follwing command work?

scrapy fetching incomplete html

Clark

[scrapy fetching incomplete html](https://stackoverflow.com/questions/54338207/scrapy-fetching-incomplete-html)

I'm a retired programmer but new to scrapy.  Actually, this is my first python project so I could be doing anything wrong.I brought up scrapy under anaconda and started a shell with :Looks like everything is working fine and I can get some querys to work.Here is my problem:

   when I enter :I get:['<body><noscript>If you\'re seeing this message, that means <strong>JavaScript has been disabled on your browser</strong>, please <strong>enable JS</strong> to make this app work.</noscript><div id="app"></div><script src="//apis.google.com/js/platform.js" async></script><script>!function(e,a,n,t,g,c,i){e.GoogleAnalyticsObject="ga",e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(n),i=a.getElementsByTagName(n)[0],c.async=1,c.src="//www.google-analytics.com/analytics.js",i.parentNode.insertBefore(c,i)}(window,document,"script"),ga("create","UA-15981085-17","auto"),ga("require","linkid"),ga("set","anonymizeIp",!0),ga("send","pageview")</script><script type="application/ld+json">{\n\t\t\t"@context": "http://schema.org",\n\t\t\t"@type": "Organization",\n\t\t\t"name": "Sailing Channels"\n\t\t\t"url": "https://www.sailing-channels.com",\n\t\t\t"logo": "https://sailing-channels.com/img/banner.png",\n\t\t\t"sameAs" : [\n\t\t\t\t"https://www.facebook.com/sailingchannels",\n\t\t\t\t"https://twitter.com/sailchannels"\n\t\t\t]\n\t    }</script><script type="text/javascript" src="https://cdn.sailing-channels.com/1.15.9/main.1dad65fcb7a507930e1f.js"></script></body>']My problem is I expect a lot more.   When I do an inspect on chrome I see a lot more /div sections inside <div id="app"></div>Could someone shine some light on what I'm doing wrong?  I want to scrape the channel name, subscriber count, and viewsThanks

2019-01-24 01:38:17Z

I'm a retired programmer but new to scrapy.  Actually, this is my first python project so I could be doing anything wrong.I brought up scrapy under anaconda and started a shell with :Looks like everything is working fine and I can get some querys to work.Here is my problem:

   when I enter :I get:['<body><noscript>If you\'re seeing this message, that means <strong>JavaScript has been disabled on your browser</strong>, please <strong>enable JS</strong> to make this app work.</noscript><div id="app"></div><script src="//apis.google.com/js/platform.js" async></script><script>!function(e,a,n,t,g,c,i){e.GoogleAnalyticsObject="ga",e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(n),i=a.getElementsByTagName(n)[0],c.async=1,c.src="//www.google-analytics.com/analytics.js",i.parentNode.insertBefore(c,i)}(window,document,"script"),ga("create","UA-15981085-17","auto"),ga("require","linkid"),ga("set","anonymizeIp",!0),ga("send","pageview")</script><script type="application/ld+json">{\n\t\t\t"@context": "http://schema.org",\n\t\t\t"@type": "Organization",\n\t\t\t"name": "Sailing Channels"\n\t\t\t"url": "https://www.sailing-channels.com",\n\t\t\t"logo": "https://sailing-channels.com/img/banner.png",\n\t\t\t"sameAs" : [\n\t\t\t\t"https://www.facebook.com/sailingchannels",\n\t\t\t\t"https://twitter.com/sailchannels"\n\t\t\t]\n\t    }</script><script type="text/javascript" src="https://cdn.sailing-channels.com/1.15.9/main.1dad65fcb7a507930e1f.js"></script></body>']My problem is I expect a lot more.   When I do an inspect on chrome I see a lot more /div sections inside <div id="app"></div>Could someone shine some light on what I'm doing wrong?  I want to scrape the channel name, subscriber count, and viewsThanksUnderstandable. It is because of they rendering the data through another script during loading of the page. In normal scrapy setting, dynamic page loading content doesn't appear. For scraping that data you can use selenium.



selenium-with-scrapy-for-dynamic-pageFor an alternative way, you can use splash for handling javascript enabled content.handling-javascript-in-scrapy-with-splash



How to automatically increase scrapy's DOWNLOAD_DELAY while detecting code 500 in response's status

李宗衡

[How to automatically increase scrapy's DOWNLOAD_DELAY while detecting code 500 in response's status](https://stackoverflow.com/questions/54232560/how-to-automatically-increase-scrapys-download-delay-while-detecting-code-500-i)

I am going to write hundreds of spiders to crawl different static web pages, so I choose Scrapy to help me to finish my work.   During the work, I find most of the websites are simple and do not anti spiders. But I found it difficult to set a suit DOWNLOAD_DELAY in scrapy.setting.py file. There are too many spiders to code and find a suitable DOWNLOAD_DELAY for each spider will run me out of time. I want to know which models of scrapy load and use DOWNLOAD_DELAY parameter, and how to code a program to automatically increase DOWNLOAD_DELAY while detecting serve error (the spider requests too frequent).

2019-01-17 09:18:24Z

I am going to write hundreds of spiders to crawl different static web pages, so I choose Scrapy to help me to finish my work.   During the work, I find most of the websites are simple and do not anti spiders. But I found it difficult to set a suit DOWNLOAD_DELAY in scrapy.setting.py file. There are too many spiders to code and find a suitable DOWNLOAD_DELAY for each spider will run me out of time. I want to know which models of scrapy load and use DOWNLOAD_DELAY parameter, and how to code a program to automatically increase DOWNLOAD_DELAY while detecting serve error (the spider requests too frequent).You can extend AutoThrottle middleware that is responsible for managing delays with your own policy:And enable it instead of default one in your settings.py:

Running custom code after crawling has finished (scrapy)

Snail_nnaail

[Running custom code after crawling has finished (scrapy)](https://stackoverflow.com/questions/54224645/running-custom-code-after-crawling-has-finished-scrapy)

I need to test all of my scraped data after the crawl has finished (percentage of some fields' availability etc). The data is contained in a csv file, so for testing I decided to use Pandas. Is there any way to launch the code that tests the .csv file inside the scrapy spider after Scrapy tells me the crawling has finished? I've tried using extentions, but couldn't get it to work. Thx 

2019-01-16 20:13:00Z

I need to test all of my scraped data after the crawl has finished (percentage of some fields' availability etc). The data is contained in a csv file, so for testing I decided to use Pandas. Is there any way to launch the code that tests the .csv file inside the scrapy spider after Scrapy tells me the crawling has finished? I've tried using extentions, but couldn't get it to work. Thx Scrapy provides you the flow to control the item in PipelinesIn Pipelines you can verify or can apply any check on the item, if it does not match your criteria or you want to update data depending on some attribute values, you can do there.More info on Pipelines you can read here

Scraping elements with changing xpath under same class

scrapy_newbie

[Scraping elements with changing xpath under same class](https://stackoverflow.com/questions/54274952/scraping-elements-with-changing-xpath-under-same-class)

I am trying to scrape "li" elements that will change their xpath depending on how many "li" elements will be added. I don't know how to describe that better, so I will go right into the example to make it more clear.Let's say this is about scraping soccer data. The structure of the website is as follows:As you can see, the p and li elements are not "mapped" to each other. They are kind of independent, even though p is the heading and li the content. It's easy to scrape the Man of the Match, because the element to scrape will be always "ul/li[1]/span/text()" and there is just one man of the match. But now occurs the problem. Since goals, assists etc. don't have an own class and are not listed under "p", it can happen that there will be more players who scored, received cards etc. So in one example li[3] is a player who scored a goal. In another example (when there are no goals), li[3] could be a yellow card. Let's see another example:So in the example above, we will have a different xpath for all the li-elements. How do I write my code in order to tell scrapy which "li"-element belongs to Goals, Assists, Yellow Cards etc., since the structure of the website is not really clear? I tried:which gives me all the li elements but not the spans. Of course I could add span in the end but than I won't be able to map it to the write item (because li's are always changing). But actually I would like to have the items goals, assists, yellow cards etc.Basically I want to know how to map elements to the same items, that will change their xpath depending on the number of elements added (in this case goals, assists etc.). I hope I could make my problem clear, since English isn't my first language, I apologize for a possible bad description. Thanks in advance, help is very much appreciated.  

2019-01-20 09:01:14Z

I am trying to scrape "li" elements that will change their xpath depending on how many "li" elements will be added. I don't know how to describe that better, so I will go right into the example to make it more clear.Let's say this is about scraping soccer data. The structure of the website is as follows:As you can see, the p and li elements are not "mapped" to each other. They are kind of independent, even though p is the heading and li the content. It's easy to scrape the Man of the Match, because the element to scrape will be always "ul/li[1]/span/text()" and there is just one man of the match. But now occurs the problem. Since goals, assists etc. don't have an own class and are not listed under "p", it can happen that there will be more players who scored, received cards etc. So in one example li[3] is a player who scored a goal. In another example (when there are no goals), li[3] could be a yellow card. Let's see another example:So in the example above, we will have a different xpath for all the li-elements. How do I write my code in order to tell scrapy which "li"-element belongs to Goals, Assists, Yellow Cards etc., since the structure of the website is not really clear? I tried:which gives me all the li elements but not the spans. Of course I could add span in the end but than I won't be able to map it to the write item (because li's are always changing). But actually I would like to have the items goals, assists, yellow cards etc.Basically I want to know how to map elements to the same items, that will change their xpath depending on the number of elements added (in this case goals, assists etc.). I hope I could make my problem clear, since English isn't my first language, I apologize for a possible bad description. Thanks in advance, help is very much appreciated.  This line: response.css("ul.stats p, ul.stats li") 

 returns a list of p and ul tag selectors in the same order as in response.

After that you need to separately process each type of nodes.You can use XPath’s preceding-sibling to find the li elements that are preceded by a specific key:

Scrapy: Crawled but not scraped any data

Argha

[Scrapy: Crawled but not scraped any data](https://stackoverflow.com/questions/54248135/scrapy-crawled-but-not-scraped-any-data)

I wrote the following code to scrape Booking.com given the name of the city. Ideally, the program should find out all the hotels that are available in the city and scrape all the reviews for each hotel. Unfortunately, it will scrape only a few hotels and only the first 75 reviews of those hotels. Will you please tell me what am I doing wrong here??

2019-01-18 05:22:55Z

I wrote the following code to scrape Booking.com given the name of the city. Ideally, the program should find out all the hotels that are available in the city and scrape all the reviews for each hotel. Unfortunately, it will scrape only a few hotels and only the first 75 reviews of those hotels. Will you please tell me what am I doing wrong here??

Trouble writing to csv file with scrapy spider

Jerry

[Trouble writing to csv file with scrapy spider](https://stackoverflow.com/questions/54125585/trouble-writing-to-csv-file-with-scrapy-spider)

I wrote a simple spider and get an error when I try to write data to csv file. The error I get it ERROR: Spider must return Request, BaseItem, dict or None, got 'str'

When I try to print my results, everything seems fine, I just don't know how to write it to csv. I used scrapy crawl mmadness -o file.csv but nothing besides that error happens. Csv file is created but it's empty.

The problem is I'm not really sure what should I do with last statement or how do I return "table_rows" correctly.

Thanks

2019-01-10 09:30:22Z

I wrote a simple spider and get an error when I try to write data to csv file. The error I get it ERROR: Spider must return Request, BaseItem, dict or None, got 'str'

When I try to print my results, everything seems fine, I just don't know how to write it to csv. I used scrapy crawl mmadness -o file.csv but nothing besides that error happens. Csv file is created but it's empty.

The problem is I'm not really sure what should I do with last statement or how do I return "table_rows" correctly.

Thanksyou are printing your data while you should use :

10049: The requested address is not valid in its context.. Scrapy-Splash not reading URL correctly

Nick P

[10049: The requested address is not valid in its context.. Scrapy-Splash not reading URL correctly](https://stackoverflow.com/questions/54119019/10049-the-requested-address-is-not-valid-in-its-context-scrapy-splash-not-rea)

I am trying to get the code to read in the web page using splash for a more complicated site, but I can't even get the code to run for this simple site location.  I ran the docker and have the 8050 port mapped to 0.0.0.0 in my settings.py file. Any help would be greatly appreciated.  Please provide version you used for any package as I fear this may be an issue.  I have tried numerous error fixes along the way.  Changing the versions of Splash, Scrapy, and Twisted.  Scrapy only works on Python 3.x with a newer version of Twisted, but Splash says incomparable with Twisted > 16.2.  So I tried switching up the versioning some there with no fixes.  I should just receive the Quote Texts, ie. this is the same URL from the python documentation

2019-01-09 22:01:31Z

I am trying to get the code to read in the web page using splash for a more complicated site, but I can't even get the code to run for this simple site location.  I ran the docker and have the 8050 port mapped to 0.0.0.0 in my settings.py file. Any help would be greatly appreciated.  Please provide version you used for any package as I fear this may be an issue.  I have tried numerous error fixes along the way.  Changing the versions of Splash, Scrapy, and Twisted.  Scrapy only works on Python 3.x with a newer version of Twisted, but Splash says incomparable with Twisted > 16.2.  So I tried switching up the versioning some there with no fixes.  I should just receive the Quote Texts, ie. this is the same URL from the python documentation

Load Item fileds with ItemLoader across multiple responses

maivel

[Load Item fileds with ItemLoader across multiple responses](https://stackoverflow.com/questions/54102498/load-item-fileds-with-itemloader-across-multiple-responses)

This is a followup question to accepted answer to question Scrapy: populate items with item loaders over multiple pages. I want to use ItemLoader to collect values from multiple requests to a single Item. The accepted answer suggests that the loaded Item.load_item() should be passed to the next request via meta field in request.However, I would like to apply output_processors to all collected values of a single field when returning the loaded object at the end of the crawl.Example:

Ignore the context of the actual websites.

2019-01-09 02:29:29Z

This is a followup question to accepted answer to question Scrapy: populate items with item loaders over multiple pages. I want to use ItemLoader to collect values from multiple requests to a single Item. The accepted answer suggests that the loaded Item.load_item() should be passed to the next request via meta field in request.However, I would like to apply output_processors to all collected values of a single field when returning the loaded object at the end of the crawl.Example:

Ignore the context of the actual websites.Yes, you can just pass the ItemLoader instance.  If I recall this correctly from irc or github chat way long ago, there might be some potential issues with doing this, like increased memory usage or leaks from reference handling, because you carry around object references of ItemLoader instances (and processors?) and potentially over long times, depending on the order of your download queues, by binding these itemloader instances to those requests.

So keep that in mind and perhaps beware of using this style on large crawls, or do some memory debugging to be certain.  However, I used this method extensively in the past myself (and would still do so when using ItemLoaders), and haven't seen any problems with that approach myself.Here is how I do that:This requires using a customized ItemLoader class, which can be found in my scrapy scrapyard,

but the relevant part of the class is here:

scrapy-splash not rendering dynamic content generated by thirdparty script(plugin)

Bikash Burma

[scrapy-splash not rendering dynamic content generated by thirdparty script(plugin)](https://stackoverflow.com/questions/54102610/scrapy-splash-not-rendering-dynamic-content-generated-by-thirdparty-scriptplugi)

I am trying to scrape webpages reviews.. For example:https://www.chubbiesshorts.com/products/the-arnoldsHere the reviews are generated by plugin, running its script. I can not render those dynamic content using splash, nor the content is being render in splash UI("http://localhost:8050/info?wait=0.5&images=1&expand=1&timeout=3600.0&url=https%3A%2F%2Fwww.chubbiesshorts.com%2Fproducts%2Fthe-arnolds&lua_source=function+main%28splash%2C+args%29%0D%0A++assert%28splash%3Ago%28args.url%29%29%0D%0A++assert%28splash%3Await%28150%29%29%0D%0A++return+%7B%0D%0A++++html+%3D+splash%3Ahtml%28%29%2C%0D%0A++++png+%3D+splash%3Apng%28%29%2C%0D%0A++++har+%3D+splash%3Ahar%28%29%2C%0D%0A++%7D%0D%0Aend").Can anyone help me to get those content.

2019-01-09 02:48:39Z

I am trying to scrape webpages reviews.. For example:https://www.chubbiesshorts.com/products/the-arnoldsHere the reviews are generated by plugin, running its script. I can not render those dynamic content using splash, nor the content is being render in splash UI("http://localhost:8050/info?wait=0.5&images=1&expand=1&timeout=3600.0&url=https%3A%2F%2Fwww.chubbiesshorts.com%2Fproducts%2Fthe-arnolds&lua_source=function+main%28splash%2C+args%29%0D%0A++assert%28splash%3Ago%28args.url%29%29%0D%0A++assert%28splash%3Await%28150%29%29%0D%0A++return+%7B%0D%0A++++html+%3D+splash%3Ahtml%28%29%2C%0D%0A++++png+%3D+splash%3Apng%28%29%2C%0D%0A++++har+%3D+splash%3Ahar%28%29%2C%0D%0A++%7D%0D%0Aend").Can anyone help me to get those content.

scrapy 503 Service Unavailable on starturl

Freec0re 123

[scrapy 503 Service Unavailable on starturl](https://stackoverflow.com/questions/54069663/scrapy-503-service-unavailable-on-starturl)

I modifed this spider but it gives this errorsCrawler code:

2019-01-07 06:49:28Z

I modifed this spider but it gives this errorsCrawler code:Your crawler is trying to check robots.txt file but the website doesn't have one present. To avoid this you can set ROBOTSTXT_OBEY setting to false in your settings.py file.

By default it's False but new scrapy projects generated with scrapy startproject command has ROBOTSTXT_OBEY = True generated from the template.Further the website seems to respond as 503 on every first request. The website is using some sort of bot protection:First request is 503 then some javascript is being executed to make an AJAX request for generating __shovlshield cookie:Seems like https://shovl.io/ ddos protection is being used. To solve this you need to reverse engineer how javascript generates the cookie or employ javascript rendering techniques/services such as selenium or splash

Couldn't load initial seed urls using frontera in scrapy

ARUN ARUMUGAM

[Couldn't load initial seed urls using frontera in scrapy](https://stackoverflow.com/questions/53967695/couldnt-load-initial-seed-urls-using-frontera-in-scrapy)

I'm trying to load seed urls for scrapy using frontera but seems not working, could any one help to load initial seed urls using frontera.

2018-12-29 07:43:38Z

I'm trying to load seed urls for scrapy using frontera but seems not working, could any one help to load initial seed urls using frontera.

Set conditions on xpath statement

dcarlo56ave

[Set conditions on xpath statement](https://stackoverflow.com/questions/54083587/set-conditions-on-xpath-statement)

I am scraping a website using scrapy and would like to only grab the data from the links that are grayed out which I have done by targeting the css class for the grayed elements. The issue I am facing is the second xpath /div[1]/text() is selecting the elements from the elements that are active, so when my list returns I have the address for the companies that are not active in my list. What I would like to know is if there is a way to only select the parent div from the class font_grey. I did try using parent::text() but that returned an empty value. 

2019-01-08 00:03:26Z

I am scraping a website using scrapy and would like to only grab the data from the links that are grayed out which I have done by targeting the css class for the grayed elements. The issue I am facing is the second xpath /div[1]/text() is selecting the elements from the elements that are active, so when my list returns I have the address for the companies that are not active in my list. What I would like to know is if there is a way to only select the parent div from the class font_grey. I did try using parent::text() but that returned an empty value. This XPath,will select all div elements with a child h3 element that has a @class attribute value of "font_grey".Try this xpath //*[@class='font_grey]/..Explanation: //* - any element at any level

[@class='font_grey'] - where class attribute is equal to "font_grey"

/.. - select parent  

How to download mobile version of a website using Scrapy

Jgaldos

[How to download mobile version of a website using Scrapy](https://stackoverflow.com/questions/53900151/how-to-download-mobile-version-of-a-website-using-scrapy)

I am trying to understand how to download the mobile version of a website using Scrapy (Google for example).I don't know how to emulate those requests as I don't have a debugger in my phone. How could I do it using Scrapy or how could I debug mobile requests using my desktop

2018-12-22 23:49:22Z

I am trying to understand how to download the mobile version of a website using Scrapy (Google for example).I don't know how to emulate those requests as I don't have a debugger in my phone. How could I do it using Scrapy or how could I debug mobile requests using my desktopYou can totally use your desktop to emulate mobile behavior. Use Chrome's Developer  Tools and click the Toogle device toolbar button:That way you can check which requests are being used for mobile version.Now normally, this is specified by the User-Agent header, so just a mobile one, that could be for example changing the USER_AGENT in settings:

Scrapy not following the next parse function

sam007

[Scrapy not following the next parse function](https://stackoverflow.com/questions/53869879/scrapy-not-following-the-next-parse-function)

I am trying to write a simple scraping script to scrape off google summer of code orgs with the tech that I require. Its work in progress. My parse function is working fine but whenever I callback into org function it doesn't throw any output.

2018-12-20 13:41:07Z

I am trying to write a simple scraping script to scrape off google summer of code orgs with the tech that I require. Its work in progress. My parse function is working fine but whenever I callback into org function it doesn't throw any output.first of all, you are configuring incorrectly the allowed_domains, as it specifies in the documentation:As you can see, you need to include only the domains, and this is a filtering functionality (so other domains don't get crawled). Also this is optional, so I would actually recommend to not include it.Also your css for getting tech is incorrect, it should be:

Scrapy script running without exception,but no data collected [duplicate]

pansal

[Scrapy script running without exception,but no data collected [duplicate]](https://stackoverflow.com/questions/53786615/scrapy-script-running-without-exception-but-no-data-collected)

Here is a Python Scrapy script which I am learning from some material book. 

It is a simple web scraping sample.

I can run it without any exception.But it seems no data actually collected by running it. 

So I past the code bellow, could anyone be kind to try running it and let me know if it works for you? cuz this is learning sample I don't think it is wrong, or maybe my python lib not match. thank you.

2018-12-14 20:39:50Z

Here is a Python Scrapy script which I am learning from some material book. 

It is a simple web scraping sample.

I can run it without any exception.But it seems no data actually collected by running it. 

So I past the code bellow, could anyone be kind to try running it and let me know if it works for you? cuz this is learning sample I don't think it is wrong, or maybe my python lib not match. thank you.Scrapy spider callback methods can return (or yield) two things:On the code you shared there is a first request (against the sites defined in start_urls) and the parse method is the default callback method for all requests (if no callback argument is specified).That parse method should return an Item or a Request, and you are not doing that, so you should change it something like:Now your spider is outputting an item (a dict in this case called event_details).

How to identify a change in a websites’ structure programmatically

Carlos

[How to identify a change in a websites’ structure programmatically](https://stackoverflow.com/questions/53811691/how-to-identify-a-change-in-a-websites-structure-programmatically)

Within the implementation of a Python Scrapy crawler I would like to add a robust mechanism for monitoring/detecting potential layout changes within a website.These changes do not necessarily affect existing spider selectors - for example, a site adds a new HTML element to represent the number of visitors an item has received - an element I might now be interested in parsing. 

Having said that, detecting selector issues (Xpath/CSS) would be also beneficial in case where they are removed/relocated. Please note this is not about selector content change or a website refresh (if-modified-since or last-modified), but rather a modification in the structure / nodes / layout of a site. Therefore, how would one implement logic to monitor such circumstances?

2018-12-17 08:53:08Z

Within the implementation of a Python Scrapy crawler I would like to add a robust mechanism for monitoring/detecting potential layout changes within a website.These changes do not necessarily affect existing spider selectors - for example, a site adds a new HTML element to represent the number of visitors an item has received - an element I might now be interested in parsing. 

Having said that, detecting selector issues (Xpath/CSS) would be also beneficial in case where they are removed/relocated. Please note this is not about selector content change or a website refresh (if-modified-since or last-modified), but rather a modification in the structure / nodes / layout of a site. Therefore, how would one implement logic to monitor such circumstances?This is actually a topic for research as you can see on this paper but there are of course some implemented tools that you can check out:Basically the base for comparing (on the previous approaches) is to use the Tree Edit Distance of the html layout.

scrapy stuck in DEBUG: Telnet console listening on 127.0.0.1

Nick Chu

[scrapy stuck in DEBUG: Telnet console listening on 127.0.0.1](https://stackoverflow.com/questions/53789762/scrapy-stuck-in-debug-telnet-console-listening-on-127-0-0-1)

I scrapy the html sucessful but splash.Don't know why it fail to Crawl.The code using splash below,but it always stucked here did'nt go on:and a few minutes later, it show this behind:

2018-12-15 05:20:05Z

I scrapy the html sucessful but splash.Don't know why it fail to Crawl.The code using splash below,but it always stucked here did'nt go on:and a few minutes later, it show this behind:

How to INSERT or UPDATE a large number of rows (regarding the auto_increment value of a table)

timavo

[How to INSERT or UPDATE a large number of rows (regarding the auto_increment value of a table)](https://stackoverflow.com/questions/53801493/how-to-insert-or-update-a-large-number-of-rows-regarding-the-auto-increment-val)

I have a MySQL table with around 3 million rows (listings) at the moment. These listings are updated 24/7 (around 30 listings/sec) by a python script (Scrapy) using pymsql - so the performance of the queries is relevant!If a listing doesn't exist (i.e. the UNIQUE url), a new record will be inserted (which is around every hundredth listing). The id is set to auto_increment and I am using a INSERT INTO listings ... ON DUPLICATE KEY UPDATE last_seen_at = CURRENT_TIMESTAMP. The update on last_seen_at is necessary to check if the item is still online, as I am crawling the search results page with multiple listings on it and not checking each individual URL each time. At first, it all went fine. Then I noticed larger and larger gaps in the auto_incremented id column and found out it's due to the INSERT INTO ... statement: MySQL attempts to do the insert first. This is when the id gets auto incremented. Once incremented, it stays. Then the duplicate is detected and the update happens.Option A: Set the id column to unsigned INT or BIGINT and just ignore the gaps. Problem here is I'm afraid of hitting the maximum after a couple of years updating. I'm already at an auto_increment value of around 12,000,000 for around 3,000,000 listings after two days of updating...Option B: Switch to an INSERT IGNORE ... statement, check the affected rows and UPDATE ... if necessary.  Option C: SELECT ... the existing listings, check existence within python and INSERT ... or UPDATE ... dependingly. Any other wise options?Additonal Info: I need an id for information related to a listing stored in other tables (e.g. listings_images, listings_prices etc.). IMHO using the URL (which is unique) won't be the best option for foreign keys.

2018-12-16 10:55:48Z

I have a MySQL table with around 3 million rows (listings) at the moment. These listings are updated 24/7 (around 30 listings/sec) by a python script (Scrapy) using pymsql - so the performance of the queries is relevant!If a listing doesn't exist (i.e. the UNIQUE url), a new record will be inserted (which is around every hundredth listing). The id is set to auto_increment and I am using a INSERT INTO listings ... ON DUPLICATE KEY UPDATE last_seen_at = CURRENT_TIMESTAMP. The update on last_seen_at is necessary to check if the item is still online, as I am crawling the search results page with multiple listings on it and not checking each individual URL each time. At first, it all went fine. Then I noticed larger and larger gaps in the auto_incremented id column and found out it's due to the INSERT INTO ... statement: MySQL attempts to do the insert first. This is when the id gets auto incremented. Once incremented, it stays. Then the duplicate is detected and the update happens.Option A: Set the id column to unsigned INT or BIGINT and just ignore the gaps. Problem here is I'm afraid of hitting the maximum after a couple of years updating. I'm already at an auto_increment value of around 12,000,000 for around 3,000,000 listings after two days of updating...Option B: Switch to an INSERT IGNORE ... statement, check the affected rows and UPDATE ... if necessary.  Option C: SELECT ... the existing listings, check existence within python and INSERT ... or UPDATE ... dependingly. Any other wise options?Additonal Info: I need an id for information related to a listing stored in other tables (e.g. listings_images, listings_prices etc.). IMHO using the URL (which is unique) won't be the best option for foreign keys.I was in exact situation as yoursI have millions of records being entered by scraper into table, scraper was running every dayI tried following but failedSOLUTION WORKED FOR ME: (for table with millions of rows)Notice it is using INSERT IGNORE INTO, so only new records will be entered and if it exists, it will be ignored completelyIf you use REPLACE INTO instead of INSERT IGNORE INTO in MySQL, the new records will be entered, but if a record exists, it will be updated 

Scrapy Crawler Process Setting

dcarlo56ave

[Scrapy Crawler Process Setting](https://stackoverflow.com/questions/53747127/scrapy-crawler-process-setting)

I have built multiple crawlers and want to run them simultaneously using CrawlerProcess. However, when building the spiders I set it up so they would run a little slower and have a download delay. While running the spiders individually the settings work fine but when I run all four spiders its crawling very fast and a few of sites are kicking me off the network. What I would like to know is why doesn't CrawlerProcess follow the settings and if there is a way to make this happen how can I achieve that.Here's how I have it setup:

2018-12-12 16:15:06Z

I have built multiple crawlers and want to run them simultaneously using CrawlerProcess. However, when building the spiders I set it up so they would run a little slower and have a download delay. While running the spiders individually the settings work fine but when I run all four spiders its crawling very fast and a few of sites are kicking me off the network. What I would like to know is why doesn't CrawlerProcess follow the settings and if there is a way to make this happen how can I achieve that.Here's how I have it setup:This happens because each spider is running individually without them knowing about each other.Of course, all spiders are using the same settings, but that's the only connection.The site must be complaining about multiple requests being done, maybe by the same origin proxy/IP so I would recommend maybe to use a proxy iterator service or to slow the spiders even more.You can play with the following settings:Fixed the issue by adding custom settings to each one of my spiders. You can add this right below the start urls list. 

How do I add `-o` argument in scrapy while running it programmatically in python

Rahat Zaman

[How do I add `-o` argument in scrapy while running it programmatically in python](https://stackoverflow.com/questions/53683250/how-do-i-add-o-argument-in-scrapy-while-running-it-programmatically-in-python)

Suppose I have a code, where I run the scrapy crawler with python.As in command line, I always use scrapy crawl <proj_name> -o out.json.How do I use this -o out.json in python?

2018-12-08 13:58:10Z

Suppose I have a code, where I run the scrapy crawler with python.As in command line, I always use scrapy crawl <proj_name> -o out.json.How do I use this -o out.json in python?

Loop on scrapy FormRequest but only one item created

Ayra

[Loop on scrapy FormRequest but only one item created](https://stackoverflow.com/questions/53652805/loop-on-scrapy-formrequest-but-only-one-item-created)

So I've tried to loop on a formrequest that call my function that create, fill and yield the item, only pb : only one and only one item is done no matter how many times he looped and I can't figure out why ?To be sure everything is clear :

When I run this, the log "annonce" is printed only one time while my logging a['id'] in my request loop is printed a lot and i can't find a way to fix this

2018-12-06 13:45:18Z

So I've tried to loop on a formrequest that call my function that create, fill and yield the item, only pb : only one and only one item is done no matter how many times he looped and I can't figure out why ?To be sure everything is clear :

When I run this, the log "annonce" is printed only one time while my logging a['id'] in my request loop is printed a lot and i can't find a way to fix thisI found the way ! 

If any one has the same pb : as my url is always the same (only formdata change) the scrapy filter is taking the control and destroy the duplicates.

Activate dont_filter to true in the formrequest to make it works

Scrapy don't follow links to images

user4421975

[Scrapy don't follow links to images](https://stackoverflow.com/questions/53611683/scrapy-dont-follow-links-to-images)

Is there a way in Scrapy to not follow <a> tags pointing to images?For example:<a href="http://jamsphere.com/wp-content/uploads/2015/11/Franki-Dennull-PROFILE.jpg">My code at the moment:Obviously I can add a hard coded check but was wondering if there is a built in option?

2018-12-04 11:13:05Z

Is there a way in Scrapy to not follow <a> tags pointing to images?For example:<a href="http://jamsphere.com/wp-content/uploads/2015/11/Franki-Dennull-PROFILE.jpg">My code at the moment:Obviously I can add a hard coded check but was wondering if there is a built in option?Use a LinkExtractor, by default it filters out the common image / video / audio / file extensions.Look here to see the ignored extensions.

Scrapy Shell Response 204

Adilzada

[Scrapy Shell Response 204](https://stackoverflow.com/questions/53574994/scrapy-shell-response-204)

i try to parse a specific website : www.bina.az/items/all. And i want to test it before building fully functional spider. So i type scrapy shell bina.az/items/all in terminal and i get this :The reason of this is Cloudfare protection. I know how to bypass cloudfare in scrapy project, but i need to use scrapy shell also.How can i solve this issue ?

2018-12-01 20:59:26Z

i try to parse a specific website : www.bina.az/items/all. And i want to test it before building fully functional spider. So i type scrapy shell bina.az/items/all in terminal and i get this :The reason of this is Cloudfare protection. I know how to bypass cloudfare in scrapy project, but i need to use scrapy shell also.How can i solve this issue ?You could run scrapy shell from your project. Suppose that you have the following project:First go to your project:If you don't have virtual env, create one:Then activate the virtual env:Then, in your virtual environment you should install:Then try to run scrapy shell:If there is an error when you run scrapy shell try:As you can see there is the 'scrapy_cloudflare_middleware.middlewares.CloudFlareMiddleware' in [scrapy.middleware]I also noted that you need to set USER_AGENT to it could work, there is my settings.py file:

Empty .json file

eX a7mdooh

[Empty .json file](https://stackoverflow.com/questions/53582997/empty-json-file)

I have written this short spider code to extract titles from hacker news front page(http://news.ycombinator.com/). However when i run the code scrapy scrawl hackernewscrawler -o hntitles.json -t json  i get an empty .json file that does not have any content in it. 

2018-12-02 17:52:51Z

I have written this short spider code to extract titles from hacker news front page(http://news.ycombinator.com/). However when i run the code scrapy scrawl hackernewscrawler -o hntitles.json -t json  i get an empty .json file that does not have any content in it. You should change print statement to yield:Then run:

Scrapy / How to select data directly nested into span

SidGabriel

[Scrapy / How to select data directly nested into span](https://stackoverflow.com/questions/53543391/scrapy-how-to-select-data-directly-nested-into-span)

One website I am trying to scrap has a specific structure for prices. It is something like :It is possible to access directly the data-item-price data nested into the span ?I mean, not something like :But another way with data-item-price ?

2018-11-29 16:22:03Z

One website I am trying to scrap has a specific structure for prices. It is something like :It is possible to access directly the data-item-price data nested into the span ?I mean, not something like :But another way with data-item-price ?Try response.css("span.sale-price::attr(data-item-price)").get() for getting data from this field. Or if you want to get all span with such field use selector span[data-item-price].

How do I get the next element's text for all h2 elements using xpath?

Jane Wayne

[How do I get the next element's text for all h2 elements using xpath?](https://stackoverflow.com/questions/53548657/how-do-i-get-the-next-elements-text-for-all-h2-elements-using-xpath)

I have HTML that looks like the following.I want to be able to get the text ["News\nSports", "Entertainment\nBusiness"] using XPath. How do I do this?//div[contains(@class,"topics")]/h2/text() gives me the h2 text, but I also want the corresponding (the following) text below as well.//div[contains(@class,"topics")]/h2/following-sibling::text() does give me all the text after h2 but in a pattern like this this array ["News", "\n", "Sports", "Entertainment", "\n", "Business"]. There is now no way I can associate the array of text strings back to the header.I'm using Scrapy v1.5.1 to issue the XPath.The strange thing is that this XPath query works in Chrome (viewing the yellow highlighted text), but not via Scrapy.

2018-11-29 22:40:17Z

I have HTML that looks like the following.I want to be able to get the text ["News\nSports", "Entertainment\nBusiness"] using XPath. How do I do this?//div[contains(@class,"topics")]/h2/text() gives me the h2 text, but I also want the corresponding (the following) text below as well.//div[contains(@class,"topics")]/h2/following-sibling::text() does give me all the text after h2 but in a pattern like this this array ["News", "\n", "Sports", "Entertainment", "\n", "Business"]. There is now no way I can associate the array of text strings back to the header.I'm using Scrapy v1.5.1 to issue the XPath.The strange thing is that this XPath query works in Chrome (viewing the yellow highlighted text), but not via Scrapy.

scrapy rules do not call parsing method

merlin

[scrapy rules do not call parsing method](https://stackoverflow.com/questions/53436914/scrapy-rules-do-not-call-parsing-method)

I am new to scrapy and am trying to crawl a domain, following all internal links and scraping the title of url with the pattern /example/.*crawling works, but the scraping of the title does not since the output file is empty. Most likely I got the rules wrong. Is this the right syntax using the rules in order to achieve what I am looking for?spider.pycrawl: scrapy crawl getbid -o 012916.csv

2018-11-22 19:12:32Z

I am new to scrapy and am trying to crawl a domain, following all internal links and scraping the title of url with the pattern /example/.*crawling works, but the scraping of the title does not since the output file is empty. Most likely I got the rules wrong. Is this the right syntax using the rules in order to achieve what I am looking for?spider.pycrawl: scrapy crawl getbid -o 012916.csvFrom the CrawlSpider docs:Since your first rule will match all links, it will always be used and all other rules will be ignored.Fixing the problem is as simple as switching the order of the rules.

Scrapy/Python/MySQL: What is the best approach for saving additional item information in a separate table?

timavo

[Scrapy/Python/MySQL: What is the best approach for saving additional item information in a separate table?](https://stackoverflow.com/questions/53395411/scrapy-python-mysql-what-is-the-best-approach-for-saving-additional-item-inform)

I am crawling a website with Scrapy containing some kind of listings and store the new listings in a MySQL table. For each listing I want to add the price in a separate table (when it changes).The listings table:The listing_prices table:Using a AddListingsToDatabase() pipeline for saving new listings and afterwards using a AddPricesToDatabase() pipeline for saving new/updated prices to the database.In the AddPricesToDatabase() I am querying the database to get all listings with prices. Then I check if the price has changed and add/update the price. Therefor I need the listing.id.So far, this only works for listings which already were in the database before the current crawl.When I want to add new prices to the database, I need the listing.id (auto_increment) from the database. When I am querying the database in the AddPricesToDatabase() pipeline it doesn't yet find the listings newly added by the AddListingsToDatabase() pipeline. What is the best approach for saving additional information for a scraped Item in a separate table?

2018-11-20 14:38:50Z

I am crawling a website with Scrapy containing some kind of listings and store the new listings in a MySQL table. For each listing I want to add the price in a separate table (when it changes).The listings table:The listing_prices table:Using a AddListingsToDatabase() pipeline for saving new listings and afterwards using a AddPricesToDatabase() pipeline for saving new/updated prices to the database.In the AddPricesToDatabase() I am querying the database to get all listings with prices. Then I check if the price has changed and add/update the price. Therefor I need the listing.id.So far, this only works for listings which already were in the database before the current crawl.When I want to add new prices to the database, I need the listing.id (auto_increment) from the database. When I am querying the database in the AddPricesToDatabase() pipeline it doesn't yet find the listings newly added by the AddListingsToDatabase() pipeline. What is the best approach for saving additional information for a scraped Item in a separate table?As mentioned in the comments, instead of auto-incrementing your identifier, you could create a combined identifier, such as website_id + listing_id that would uniquely identify your record.You can store this in a single column, or in two separate columns and create a combined key.For example:websites table:listings table:listing_prices table:In the listings table, you would have to ensure that the combination of id and website_id is unique.

Scrapy for mac ModuleNotFoundError: No module named '${project_name}'

陈宝佳

[Scrapy for mac ModuleNotFoundError: No module named '${project_name}'](https://stackoverflow.com/questions/53405686/scrapy-for-mac-modulenotfounderror-no-module-named-project-name)

I want to run Scrapy on Mac, but this problem has been bothering me for a long time and has never been solved.I tried it1, reinstall Python3.x and upgrade pip.2, xcode-select --install3, use virtualenv to create a new environment.4, use anaconda3 to create a new environment.But this problem can not be solved because I updated the macOS 10.14.1?

Thank you for your help.

2018-11-21 05:20:45Z

I want to run Scrapy on Mac, but this problem has been bothering me for a long time and has never been solved.I tried it1, reinstall Python3.x and upgrade pip.2, xcode-select --install3, use virtualenv to create a new environment.4, use anaconda3 to create a new environment.But this problem can not be solved because I updated the macOS 10.14.1?

Thank you for your help.

How to get <p> that contains text which matches regex

deekay

[How to get <p> that contains text which matches regex](https://stackoverflow.com/questions/53409077/how-to-get-p-that-contains-text-which-matches-regex)

I am trying to scrape this website using scrapy, xpath and regex.

I have checked and tried the answers to this question:

xpath+ regex: matches textI want to create a 'scrapy.selector.unified.SelectorList' of <p> that contain the text "11 (sun)" or "9 (fri)" and such, and loop through the list.does not work.FYI, below does work.What am I missing here?

2018-11-21 09:37:52Z

I am trying to scrape this website using scrapy, xpath and regex.

I have checked and tried the answers to this question:

xpath+ regex: matches textI want to create a 'scrapy.selector.unified.SelectorList' of <p> that contain the text "11 (sun)" or "9 (fri)" and such, and loop through the list.does not work.FYI, below does work.What am I missing here?If you're only after text, Karan Verma's answer is sufficient.

If you're after the elements themselves, keep reading.matches is only available in XPath 2.0 and higher (as are the other regex functions), and is not available in scrapy.Scrapy uses parsel for parsing, which in turn uses lxml, which only supports XPath 1.0.

It does, however, support regular expressions in the EXSLT namespaceSince the regex namespace is enabled by default in scrapy, you can do this:You can use re() instead of extract()

Call the .re() method for each element in this list and return their results flattened, as a list of unicode strings.

.re() returns a list of unicode strings. So you can’t construct nested .re() calls.event = response.xpath('//p/text()').extract("\d+\s\(\w{3}\)")Note: re()  decode HTML entities (except < and &).For more information please refer doc here : https://doc.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.SelectorList.re

Scrapy selector outside html tag

llermaly

[Scrapy selector outside html tag](https://stackoverflow.com/questions/53347193/scrapy-selector-outside-html-tag)

I have a special case where a script tag is placed outside the html tag : both css and xpath selectors are not finding this script tag, the only way I found is using response.text , but that responds with a giant string and I can not make regex operations on it with selector re() function.Is there a way to CSS or Xpath tags outside html tag?I tried with But only consider script tags inside html tagThanks

2018-11-17 00:52:01Z

I have a special case where a script tag is placed outside the html tag : both css and xpath selectors are not finding this script tag, the only way I found is using response.text , but that responds with a giant string and I can not make regex operations on it with selector re() function.Is there a way to CSS or Xpath tags outside html tag?I tried with But only consider script tags inside html tagThanksCorrection :css selector does not consider tags outside HTML , xpath does.I used some conditions to filter the tag :

Scrapy crawl spider does not follow all the links and does not populate Item loader

GKV

[Scrapy crawl spider does not follow all the links and does not populate Item loader](https://stackoverflow.com/questions/53346991/scrapy-crawl-spider-does-not-follow-all-the-links-and-does-not-populate-item-loa)

I am trying to take the links for this website (https://minerals.usgs.gov/science/mineral-deposit-database/#products) and scrape the title from each one. However it doen't work! The spider does not seem to follow the links!CODEitemsOUTPUTI am quite new at python, so what seems to be the problem? Is it something to do with linkextraction or the parse function? 

2018-11-17 00:14:20Z

I am trying to take the links for this website (https://minerals.usgs.gov/science/mineral-deposit-database/#products) and scrape the title from each one. However it doen't work! The spider does not seem to follow the links!CODEitemsOUTPUTI am quite new at python, so what seems to be the problem? Is it something to do with linkextraction or the parse function? You have to change a couple of things.First, when you use a CrawlSpider, you can't have a callback named parse as you would override the CrawlSpider's parse: https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rulesSecondly, you want to have the correct list of allowed_domains.Try something like this:

Process new item in request error callback

wanaryytel

[Process new item in request error callback](https://stackoverflow.com/questions/53408326/process-new-item-in-request-error-callback)

I want to add an errback function to every Request to catch DNS lookup failures, timeouts and such. Upon catching them, I would like to submit a new item to be processed in the pipelines to record (in the database) that URL x failed with error y.  I can think of two theoretical ways to do it (but in practice I don't know how to do either).

1) Somehow patch into the crawler engine and add a new item to the item processing queue.

2) Manually call the respective pipelines (I only need to call one to be fair), but accessing them probably goes something like option 1, somehow need to patch to the engine and then hack-ishly find the pipelines...  Any suggestions?

2018-11-21 08:54:05Z

I want to add an errback function to every Request to catch DNS lookup failures, timeouts and such. Upon catching them, I would like to submit a new item to be processed in the pipelines to record (in the database) that URL x failed with error y.  I can think of two theoretical ways to do it (but in practice I don't know how to do either).

1) Somehow patch into the crawler engine and add a new item to the item processing queue.

2) Manually call the respective pipelines (I only need to call one to be fair), but accessing them probably goes something like option 1, somehow need to patch to the engine and then hack-ishly find the pipelines...  Any suggestions?Figured out a way of doing it, not sure if this is the best or the worst way.  All my requests receive an errback callback as suggested in the docs. The errback is this:  Don't pay any mind to what happens in the upper part, but the three bottom lines are the magic. First line increments the item processing queue counter to limit the CONCURRENT_ITEMS correctly. Second line calls the processing and third line adds the callbacks that are added by Scrapy for every item processed. The None argument is the response value that could actually be put in, too, at least for some errors. You can access it from failure.value.response but I don't care about that for now.Oh, and if it was unclear, then self is the Spider of course.PS! Since this is pretty heavily dependent on scrapy engine, then the version I'm using is 1.5.1.You could create your downloader middleware and implement the process_exception method.You have access to the request object, so you can write the details to your database, and then return a new request.

Scrapy view redirect to other page and get <400> error

Patrick Chan

[Scrapy view redirect to other page and get <400> error](https://stackoverflow.com/questions/53322247/scrapy-view-redirect-to-other-page-and-get-400-error)

I am trying to do scrapy view or fetch https://www.watsons.com.sg and the page will be redirected and return <400> error. Wonder if there is anyway to work around it. The log shows something like this:If I use request.get("https://www.watsons.com.sg") its fine. Any idea or comment much appreciated. Thanks.

2018-11-15 15:00:48Z

I am trying to do scrapy view or fetch https://www.watsons.com.sg and the page will be redirected and return <400> error. Wonder if there is anyway to work around it. The log shows something like this:If I use request.get("https://www.watsons.com.sg") its fine. Any idea or comment much appreciated. Thanks.Okay, so this is one of the weird behaviors of scrapy.If you look at the location header in the HTTP response (with Firefox developer tools for example), you can see:Note that there is no / between the .com.sg and the ?.Looking at how Firefox behaves, on the next request it adds the missing /:However, somehow scrapy does not do it!

If you look at your logs, when the HTTP 400 error is received, we can see that the / is missing.This is being discussed in this issue: https://github.com/scrapy/scrapy/issues/1133For now, the way I go around it is to have my own downloader middleware that normalizes the location header, before the response is being passed in the redirect middleware.It looks like this:

How to scrape item position number in scrapy

vezunchik

[How to scrape item position number in scrapy](https://stackoverflow.com/questions/53322999/how-to-scrape-item-position-number-in-scrapy)

How to scrape item position number from this site website:

http://books.toscrape.com/Please check this screenshothttps://prnt.sc/lim3zl

2018-11-15 15:43:17Z

How to scrape item position number from this site website:

http://books.toscrape.com/Please check this screenshothttps://prnt.sc/lim3zlTry to use enumerate in cycle, this will solve the problem. As I remember, something like this:You can simply use a class variable to track the position, like this:Then:scrapy runspider myspider.py -o out.jsonThe out.json file contains:can you check this code please How to i can apply your method in this selenium>scrapy code

scrapy how to crawl after logging in?

Gabriel Alejandro

[scrapy how to crawl after logging in?](https://stackoverflow.com/questions/53269477/scrapy-how-to-crawl-after-logging-in)

I'm currently trying to crawl a ecommerce site after login into it but it seems that im doing something wrong because scrapy its only crawling (no logging info in console)How can I proper log in into the page?This is the login html:

2018-11-12 20:18:05Z

I'm currently trying to crawl a ecommerce site after login into it but it seems that im doing something wrong because scrapy its only crawling (no logging info in console)How can I proper log in into the page?This is the login html:

Scrapy crawl and downloading particular Type files

Revathi

[Scrapy crawl and downloading particular Type files](https://stackoverflow.com/questions/53256317/scrapy-crawl-and-downloading-particular-type-files)

I'm very new to this, My Task is : Let me tell what exactly I need, I want to scrapy to search and download for some contract which TYPE is EX-10.1, EX-10.2, etc., upto EX-10.99. Contracts available in .htm format and txt format. 

1. How to scrap the links and download.

2. How to filter the file types and download.

Scrapy wants to go through this path and download, Path : url->Enter into  each CIK links -> search and Download the EX-10 Type files.My codeWhat changes I have to do? Can anyone help me with this Issue please. Thank you in Advance.

2018-11-12 05:20:57Z

I'm very new to this, My Task is : Let me tell what exactly I need, I want to scrapy to search and download for some contract which TYPE is EX-10.1, EX-10.2, etc., upto EX-10.99. Contracts available in .htm format and txt format. 

1. How to scrap the links and download.

2. How to filter the file types and download.

Scrapy wants to go through this path and download, Path : url->Enter into  each CIK links -> search and Download the EX-10 Type files.My codeWhat changes I have to do? Can anyone help me with this Issue please. Thank you in Advance.

What? No module named scrapy_splash? But I installed it

Jonas__G

[What? No module named scrapy_splash? But I installed it](https://stackoverflow.com/questions/53268045/what-no-module-named-scrapy-splash-but-i-installed-it)

Yes - another newbie question from a pure amateur here ... I'm in way over my head - but how else could I learn this stuff quickly?I'm trying to set up a spider for my work. I've got a brand new, self-hosted, Ubuntu Server 18.04 LTS install running on a virtual server. I've got Scrapy (v 1.5.1) up and running - shell working and everything. Docker installed too. Python 3.5.2 installed. Pip installed. Splash (v 3.2) installed and it appears to be working.I do encounter some problems installing scrapy-splash: scrapy-splash installAnd even though my Splash seems to be up and running ...

splash running... there are absolutely no way that my scrapy-spider will talk to it --> 

failed spider crawlI've googled, I've stackoverflowed, I've pulled hairs, I've cursed (yeah - believe it or not) - but I'm at a loss ...Any advise in any direction will be much appreciated! Best regards from Norway. 

2018-11-12 18:28:39Z

Yes - another newbie question from a pure amateur here ... I'm in way over my head - but how else could I learn this stuff quickly?I'm trying to set up a spider for my work. I've got a brand new, self-hosted, Ubuntu Server 18.04 LTS install running on a virtual server. I've got Scrapy (v 1.5.1) up and running - shell working and everything. Docker installed too. Python 3.5.2 installed. Pip installed. Splash (v 3.2) installed and it appears to be working.I do encounter some problems installing scrapy-splash: scrapy-splash installAnd even though my Splash seems to be up and running ...

splash running... there are absolutely no way that my scrapy-spider will talk to it --> 

failed spider crawlI've googled, I've stackoverflowed, I've pulled hairs, I've cursed (yeah - believe it or not) - but I'm at a loss ...Any advise in any direction will be much appreciated! Best regards from Norway. make sure you have $HOME/.local/bin in your $PATH environment variable when using the --user flag with pip. One way of doing this is appending



export PATH=$HOME/.local/bin:$PATH



to your $HOME/bash_profile file.

Scrapy: repeating request for 440 status code

HospiCZ

[Scrapy: repeating request for 440 status code](https://stackoverflow.com/questions/53239804/scrapy-repeating-request-for-440-status-code)

Is there a way in scrapy how to repeat a request, if the response status code is 440? Sometimes the server i am trying to scrape responds with 440 code and the I can´t scrape the data. Thanks a lot! 

2018-11-10 14:12:19Z

Is there a way in scrapy how to repeat a request, if the response status code is 440? Sometimes the server i am trying to scrape responds with 440 code and the I can´t scrape the data. Thanks a lot! Based on the scrapy documentation you can add and configure the RetryMiddleware to behave the way you want. The settings you will want to change are: RETRY_TIMES and RETRY_HTTP_CODES. Specially the second one where the defaults are: [500, 502, 503, 504, 408], so there you just need to add 440 to the list.Check more details about the middleware here

Scrapy: How do I select rowspan

Yasir

[Scrapy: How do I select rowspan](https://stackoverflow.com/questions/53202770/scrapy-how-do-i-select-rowspan)

Here is my code:In the 2nd <tr> where rowspan ="2" I want to apply the content of 1st <td> i.e 4892 to the next <tr> where there are two <td>.  I have tried the following, but it does not work:

2018-11-08 06:56:09Z

Here is my code:In the 2nd <tr> where rowspan ="2" I want to apply the content of 1st <td> i.e 4892 to the next <tr> where there are two <td>.  I have tried the following, but it does not work:So instead of "select rowspan" you're actually looking to "select by rowspan".There're several approaches you may try.Select it when a rowspan exists:Select it when a rowspan has a specific value ("2" here):See also:

Scrapy-Splash: Failed to run docker container with scrapinghub/splash:latest as base image

sadiqmc

[Scrapy-Splash: Failed to run docker container with scrapinghub/splash:latest as base image](https://stackoverflow.com/questions/53203995/scrapy-splash-failed-to-run-docker-container-with-scrapinghub-splashlatest-as)

Am building a python Scrapy application which uses some Azure Services and Scrapy-Splash. I tried creating a docker image for my application with scrapinghub/splash:latest as base image in my local windows machine.

Below is the Dockerfile am using,  The init_container.sh file has the statement to run the application, python /usr/src/snapshot/SiteCrawler.py.

Now when I run the docker image with the command docker run testsnapshot:0.1, the application starts and stops due to import error, ImportError: No module named azure.servicebus  I tried creating a docker image of the application with python:3.6.6 as base image, it works fine.  The application build the docker image and installs the packages from requirements.txt correctly. 

Attaching below my requirements.txt content  

2018-11-08 08:33:19Z

Am building a python Scrapy application which uses some Azure Services and Scrapy-Splash. I tried creating a docker image for my application with scrapinghub/splash:latest as base image in my local windows machine.

Below is the Dockerfile am using,  The init_container.sh file has the statement to run the application, python /usr/src/snapshot/SiteCrawler.py.

Now when I run the docker image with the command docker run testsnapshot:0.1, the application starts and stops due to import error, ImportError: No module named azure.servicebus  I tried creating a docker image of the application with python:3.6.6 as base image, it works fine.  The application build the docker image and installs the packages from requirements.txt correctly. 

Attaching below my requirements.txt content  Got it. Just had to add the WORKDIR /usr/src/snapshot to VOLUME as below.  

Python3 script stops working after scrapy update

Til Hund

[Python3 script stops working after scrapy update](https://stackoverflow.com/questions/53205762/python3-script-stops-working-after-scrapy-update)

I am on macOS 10.14.2 using "homebrewed" python3.7.2, scrapy 1.5.1 and twisted 18.9.0 as a python novice with the following script to download old newspaper archived on a website:It worked perfectly fine (although slow), however, I have two persisting issues with the script.Firstly, I get the following error since the update:Secondly, once the script runs again (as it did some days ago before updating python and packages) I ran into an issue where the script would sometimes complain that UnicodeEncodeError: ‘ascii’ codec can’t encode character u’\xb0’ in position 27: ordinal not in range(128) which I guess let sometimes to zero byte files. Do you see the encoding error in the source code? Is this related to the above problem?

2018-11-08 10:24:06Z

I am on macOS 10.14.2 using "homebrewed" python3.7.2, scrapy 1.5.1 and twisted 18.9.0 as a python novice with the following script to download old newspaper archived on a website:It worked perfectly fine (although slow), however, I have two persisting issues with the script.Firstly, I get the following error since the update:Secondly, once the script runs again (as it did some days ago before updating python and packages) I ran into an issue where the script would sometimes complain that UnicodeEncodeError: ‘ascii’ codec can’t encode character u’\xb0’ in position 27: ordinal not in range(128) which I guess let sometimes to zero byte files. Do you see the encoding error in the source code? Is this related to the above problem?

parse entire website using python beatifulsoup

Guillaume

[parse entire website using python beatifulsoup](https://stackoverflow.com/questions/53210998/parse-entire-website-using-python-beatifulsoup)

When i try to parse https://www.forbes.com/ for learning purpose. when i run the code, it only parse one page, i mean, home page.How can i parse entire website, i mean, all the page from a site.My attempted codes are given below: Can you tell me please how can i parse entire websites, not one page?

2018-11-08 15:31:24Z

When i try to parse https://www.forbes.com/ for learning purpose. when i run the code, it only parse one page, i mean, home page.How can i parse entire website, i mean, all the page from a site.My attempted codes are given below: Can you tell me please how can i parse entire websites, not one page?You have a couple of alternatives, it depends what you want to achieve.Write your own crawlerSimilarly as what you are trying to do in your code snippet, fetch a page from the website, identify all the interesting links in this page (using xpath, regular expressions, ...) and iterate until you have visited the whole domain.This is probably most suitable for learning the basics of crawling, or to get some information quickly as a one-off task.You'll have to be careful about a couple of thinks, like not to visit the same links twice, limit the domain(s) to avoid going to other websites etc.Use a web scraping frameworkIf you are looking to perform some serious scraping, for a production application or some large scale scraping, consider using a framework such as scrapy.It solves a lot of common problems for you, and it is a great way to learn advanced techniques of web scraping, by reading the documentation and diving into the code.

How to extract the value of an HTML attribute using scrapy response.xpath?

Jackknife

[How to extract the value of an HTML attribute using scrapy response.xpath?](https://stackoverflow.com/questions/53218461/how-to-extract-the-value-of-an-html-attribute-using-scrapy-response-xpath)

I'm trying to extract the value of the attribute data-asin-price inside a <div> tagWhich in the example below you can see is 22.63Is there any way to do this using response.xpath() with scrapy?Thank you

2018-11-09 01:16:13Z

I'm trying to extract the value of the attribute data-asin-price inside a <div> tagWhich in the example below you can see is 22.63Is there any way to do this using response.xpath() with scrapy?Thank youI just wanted to post the answer I found.To get the 22.63 value our of the data-asin-price attribute in scrapy shell I did the following:Cheers

How to reuse yield on spider

Bigair

[How to reuse yield on spider](https://stackoverflow.com/questions/52978989/how-to-reuse-yield-on-spider)

I'm new to Scrapy and quite new to python also.

I have multiple yield with the same body within a spider.Each yield is inside different parse methods, but I would only like to write the yield once and reuse it.I first tried a method takes response as an argument and returns yield.

Then I called that method from the parse method, but Scrapy claims that I cannot return yield.

2018-10-24 22:53:12Z

I'm new to Scrapy and quite new to python also.

I have multiple yield with the same body within a spider.Each yield is inside different parse methods, but I would only like to write the yield once and reuse it.I first tried a method takes response as an argument and returns yield.

Then I called that method from the parse method, but Scrapy claims that I cannot return yield.You can define a method for that code, eg:And call it from another method using yield from. Eg:Looks like you want something like this

How to run several versions of one single spider at one time with Scrapy?

AvyWam

[How to run several versions of one single spider at one time with Scrapy?](https://stackoverflow.com/questions/52977185/how-to-run-several-versions-of-one-single-spider-at-one-time-with-scrapy)

My problematic is the following:To win time, I would like to run several versions of one single spider.

The process (parsing definitions) is the same, the items are the same, and the collection in database is the same. What is changing is the start_url variable.

It looks like this: Considering the date is the same, for instance 2018-10-24, I would like to launch two versions in the same time:This is the first part of my problematic. And here I wonder if I must create two different classes in one single spider, like class SpiderPmu(scrapy.Spider): and class SpiderPmh(scrapy.Spider): in spider.py. But if it is the best way you think I must do, I don't know how to implement it considering settings.py, pipelines.py. I already read about CrawlerProcess from scrapy.crawler module but I don't understand well how to implement it in my project. stack subject, scrapy doc. I am not sure the part 

process = CrawlerProcess()

process.crawl(MySpider1)

process.crawl(MySpider2)

process.start() must be in the spider.py file. Above all, I am not sure it answers to my problematic.The second part, is how to launch several versions considering different date intervals.I already created some range of intervals in my spider class like: and put it in a loop. That works well.But to win time, I would like to launch several spiders, with different intervals of years.Seven versions in the same time means seven times faster.I could create 7 different projects for each range of years, but I think this is not the smartest way... and I am not sure if it will, or not, create a conflict to use the same collection database for 7 different projects running in the same time.I expect to do something like opening 7 commands:in parallel, if this is compulsory, to do:Possibly using one single spider, or one single project if needed.How can I do?PS: I already made arrangements with prolipo as proxy, Tor network to change IP, and USER_AGENT always changing. So, I avoid to be banned by crawling with multiple spiders in the same time. And my spider is "polite" with AUTOTHROTTLE_ENABLED = True. I want to keep it polite, but faster.Scrapy version: 1.5.0, Python version: 2.7.9, Mongodb version: 3.6.4, Pymongo version: 3.6.1

2018-10-24 20:07:03Z

My problematic is the following:To win time, I would like to run several versions of one single spider.

The process (parsing definitions) is the same, the items are the same, and the collection in database is the same. What is changing is the start_url variable.

It looks like this: Considering the date is the same, for instance 2018-10-24, I would like to launch two versions in the same time:This is the first part of my problematic. And here I wonder if I must create two different classes in one single spider, like class SpiderPmu(scrapy.Spider): and class SpiderPmh(scrapy.Spider): in spider.py. But if it is the best way you think I must do, I don't know how to implement it considering settings.py, pipelines.py. I already read about CrawlerProcess from scrapy.crawler module but I don't understand well how to implement it in my project. stack subject, scrapy doc. I am not sure the part 

process = CrawlerProcess()

process.crawl(MySpider1)

process.crawl(MySpider2)

process.start() must be in the spider.py file. Above all, I am not sure it answers to my problematic.The second part, is how to launch several versions considering different date intervals.I already created some range of intervals in my spider class like: and put it in a loop. That works well.But to win time, I would like to launch several spiders, with different intervals of years.Seven versions in the same time means seven times faster.I could create 7 different projects for each range of years, but I think this is not the smartest way... and I am not sure if it will, or not, create a conflict to use the same collection database for 7 different projects running in the same time.I expect to do something like opening 7 commands:in parallel, if this is compulsory, to do:Possibly using one single spider, or one single project if needed.How can I do?PS: I already made arrangements with prolipo as proxy, Tor network to change IP, and USER_AGENT always changing. So, I avoid to be banned by crawling with multiple spiders in the same time. And my spider is "polite" with AUTOTHROTTLE_ENABLED = True. I want to keep it polite, but faster.Scrapy version: 1.5.0, Python version: 2.7.9, Mongodb version: 3.6.4, Pymongo version: 3.6.1Scrapy supports spider arguments. Weirdly enough there's no straightforward documentation, but I'll try to fill in:When you run a crawl command you may provide -a NAME=VALUE arguments and these will be set as your spider class instance variables. For example:And if we run it:So, I find a solution inspired of the scrapy crawl -a variable=valueThe spider concerned, in "spiders" folder was transformed:Then, it answers to my problematic: to keep one single spider, and to run several versions of it by serveral commands at one time without trouble.Without a def __init__ it didn't work for me. I tried a lot of ways, that is this perfectible code that works for me.Scrapy version: 1.5.0, Python version: 2.7.9, Mongodb version: 3.6.4, Pymongo version: 3.6.1

Python tool to check broken links on a big urls list

roma98

[Python tool to check broken links on a big urls list](https://stackoverflow.com/questions/52984173/python-tool-to-check-broken-links-on-a-big-urls-list)

I have a search engine in production serving around 700 000 url. The crawling is done using Scrapy, and all spiders are scheduled using DeltaFetch in order to get daily new links.The difficulty I'm facing is handling broken links.I have a hard time finding a good way to periodically scan, and remove broken links. I was thinking about a few solutions : Do you have any recommendations / best practice to solve this problem?Thanks a lot.Edit : I forgot to give one precision : I'm looking to "validate" those 700k urls, not to crawl them. actually those 700k urls are the crawling result of around 2500k domains.

2018-10-25 07:57:11Z

I have a search engine in production serving around 700 000 url. The crawling is done using Scrapy, and all spiders are scheduled using DeltaFetch in order to get daily new links.The difficulty I'm facing is handling broken links.I have a hard time finding a good way to periodically scan, and remove broken links. I was thinking about a few solutions : Do you have any recommendations / best practice to solve this problem?Thanks a lot.Edit : I forgot to give one precision : I'm looking to "validate" those 700k urls, not to crawl them. actually those 700k urls are the crawling result of around 2500k domains.I would suggest using scrapy, since you're already looking up each URL with this tool and thus knows which URLs errors out. This means you don't have to check the URLs a second time.I'd go about it like this:Since your third bullet is concerned about Scrapy being shaky with URL results, the same could be said for websites in general. If a site errors out on 1 try, it might not mean a broken link.You could write a small script that just check the return http status like so:This would be the same as your first point. You could also run this async in order to optimize the time it takes to run through your 700k links.If you go for creating a script of our own check this solution 

In addition an optimization that I suggest is to make heirarchy in your URL repository. If you get 404 from one of a parent URL you can avoid checking all it children URLs 

Scrapy Scraper not scraping further than first page

bullybear17

[Scrapy Scraper not scraping further than first page](https://stackoverflow.com/questions/52917123/scrapy-scraper-not-scraping-further-than-first-page)

I'm trying to create a spider that starts on a wikipedia page called https://en.wikipedia.org/wiki/North_Korea_and_weapons_of_mass_destruction and then scrapes the text and image files I feed it to. It appears to be kind of working except I only get the first response (doesn't go to the following pages. Any help would be greatly appreciated.Here is my code:

2018-10-21 15:54:22Z

I'm trying to create a spider that starts on a wikipedia page called https://en.wikipedia.org/wiki/North_Korea_and_weapons_of_mass_destruction and then scrapes the text and image files I feed it to. It appears to be kind of working except I only get the first response (doesn't go to the following pages. Any help would be greatly appreciated.Here is my code:There are few things you can tryinstead of http in set it to 2nd thing, comment out this lineI think that is why it is not following the link

Follow parameter does not seem to work when configuring rule in crawlspider

hfldqwe

[Follow parameter does not seem to work when configuring rule in crawlspider](https://stackoverflow.com/questions/52878687/follow-parameter-does-not-seem-to-work-when-configuring-rule-in-crawlspider)

I want to extract the link I want only on the first page, and I set DEPTH_LIMIT to 1 in the crawler, and the parameter rule() in the matching rule follows=False, but I still initiated multiple requests, I I don't know why. I hope someone can answer my doubts.

Thanks in advance. output：

enter image description here

2018-10-18 16:33:56Z

I want to extract the link I want only on the first page, and I set DEPTH_LIMIT to 1 in the crawler, and the parameter rule() in the matching rule follows=False, but I still initiated multiple requests, I I don't know why. I hope someone can answer my doubts.

Thanks in advance. output：

enter image description hereFrom the docs:This means that follow=False will only stop the crawler from following links found when processing the response created by this rule, it can't affect those found when parsing the result of start_urls.There would be no point to the follow argument disabling a rule completely; if you don't want to use a rule, why would you create it at all?

Python Webscrape via Scrapy or Excel Query Search?

Shark

[Python Webscrape via Scrapy or Excel Query Search?](https://stackoverflow.com/questions/52875938/python-webscrape-via-scrapy-or-excel-query-search)

My question pertains to discovering the overall efficiency of performing a Python webscrape via Scrapy verses simply performing a Web Query search via Microsoft Excel for a particular task. What I am trying to do is automatically extract data from the NFL website. For example, 

http://www.nfl.com/stats/categorystats?tabSeq=2&offensiveStatisticCategory=GAME_STATS&conference=ALL&role=TM&season=2018&seasonType=REG&d-447263-s=TOTAL_YARDS_GAME_AVG&d-447263-o=2&d-447263-n=1For example, extracting the NFL Offense Rank each week on a weekly basis. I know that I can simply just copy and paste the information on to a Excel spreadsheet but that is not what I am trying to do. I'm trying to discover a way to automate the process so that I do not have to manually do it myself. When it comes to Python Webscrape via Scrapy, the challenge is developing the code that will obtain all the information I am seeking. Is it worth developing the code if Microsoft Query search is a better alternative?Ideally, my overall objective is to utilize either Python Webscrape via Scrapy or Microsoft Excel to pull data every week so that I don't have to manually do it myself. These include pulling data from multiple external sources and saving into one file. For example, NFL Offense, NFL Offense Passing, NLF Defense Rush, etc. I simply either run the program or open the Excel Spreadsheet, and there will be a new sheet (or a new excel file is created) with the update data so then I can go ahead and perform my data analysis. Any thoughts or opinions will be greatly appreciated!

2018-10-18 14:09:34Z

My question pertains to discovering the overall efficiency of performing a Python webscrape via Scrapy verses simply performing a Web Query search via Microsoft Excel for a particular task. What I am trying to do is automatically extract data from the NFL website. For example, 

http://www.nfl.com/stats/categorystats?tabSeq=2&offensiveStatisticCategory=GAME_STATS&conference=ALL&role=TM&season=2018&seasonType=REG&d-447263-s=TOTAL_YARDS_GAME_AVG&d-447263-o=2&d-447263-n=1For example, extracting the NFL Offense Rank each week on a weekly basis. I know that I can simply just copy and paste the information on to a Excel spreadsheet but that is not what I am trying to do. I'm trying to discover a way to automate the process so that I do not have to manually do it myself. When it comes to Python Webscrape via Scrapy, the challenge is developing the code that will obtain all the information I am seeking. Is it worth developing the code if Microsoft Query search is a better alternative?Ideally, my overall objective is to utilize either Python Webscrape via Scrapy or Microsoft Excel to pull data every week so that I don't have to manually do it myself. These include pulling data from multiple external sources and saving into one file. For example, NFL Offense, NFL Offense Passing, NLF Defense Rush, etc. I simply either run the program or open the Excel Spreadsheet, and there will be a new sheet (or a new excel file is created) with the update data so then I can go ahead and perform my data analysis. Any thoughts or opinions will be greatly appreciated!If I understand you correctly you want to scrape the website. If you are familiar with Python I would recommend using the beautiful soup package. It probably is the go-to framework for webscraping and all you have to identify the HTML tags which you want to scrape. It will also help looping through pages.There are many tutorials like this one which can help you understand how to solve your problem.Hope this helps! 

downloading full page with scrapy

mahdi jamshidian

[downloading full page with scrapy](https://stackoverflow.com/questions/52879252/downloading-full-page-with-scrapy)

i need to crawl a website.

get some of its pages and store them with all of the CSS files and images. exactly like saving the pages in browser.i have tried selenium, but with selenium i can only save the html not full page so it is not possible to do this with selenium.

2018-10-18 17:11:33Z

i need to crawl a website.

get some of its pages and store them with all of the CSS files and images. exactly like saving the pages in browser.i have tried selenium, but with selenium i can only save the html not full page so it is not possible to do this with selenium.Yes - you should be able to do this in scrapy

Inside of the <head>tag in the html you should see urls to javascript references in <script> tags and you should see <link> tags that give you the url to get the css filesOnce you get the url, it's a simple matter to do a request in scrapy. The scrapy tutorial shows this:

https://doc.scrapy.org/en/latest/intro/tutorial.html#a-shortcut-for-creating-requestsThese urls contain the raw css or javascript and you can either download that separately or construct a new single HTML documentOne thing to note is that the <script> tags may contain the full javascript and not a url reference. In this case you'll get the data when you get the html portion

Trying to scrape, getting [] back

Anna.Klee

[Trying to scrape, getting [] back](https://stackoverflow.com/questions/52842572/trying-to-scrape-getting-back)

I am using Scrapy 1.5.1 with Python 2.7.6. I am trying to scrape the usernames from the following page.I have implemented the following code:However, when trying to scrape the site I am getting [] back. I checked my xpath via the shell and everything seems to work.Any suggestions what I am doing wrong?

2018-10-16 19:16:41Z

I am using Scrapy 1.5.1 with Python 2.7.6. I am trying to scrape the usernames from the following page.I have implemented the following code:However, when trying to scrape the site I am getting [] back. I checked my xpath via the shell and everything seems to work.Any suggestions what I am doing wrong?Some of the profiles urls simply doesn't exists, so XPath expression evaluated to nothing.for ex: https://bitcointalk.org/index.php?action=profile;u=2But, also, You need to specify a start url for ex: start_urls = ['https://bitcointalk.org'] or just add start_requests function.Here is a quote from Scrapy docs regarding start_urls1...

Setting debug scrappy for pycharm

Anna.Klee

[Setting debug scrappy for pycharm](https://stackoverflow.com/questions/52861737/setting-debug-scrappy-for-pycharm)

I have installed scrapy 1.5.1 on windows 8.1 using conda 4.5.11 with:I am running Pycharm:I have set my project interpreter to anaconda's python:Furthermore, I have setup a debug configuration and pointing my working directory to my spider's project directory:I have setup a breakpoint in my spiders code. However, when I press debug I get the following error on my console output:Any suggestions what I am doing wrong?I appreciate your replies!

2018-10-17 18:51:57Z

I have installed scrapy 1.5.1 on windows 8.1 using conda 4.5.11 with:I am running Pycharm:I have set my project interpreter to anaconda's python:Furthermore, I have setup a debug configuration and pointing my working directory to my spider's project directory:I have setup a breakpoint in my spiders code. However, when I press debug I get the following error on my console output:Any suggestions what I am doing wrong?I appreciate your replies!You should use cmdline.py from scrapy as "Script path"Found response here: How to use PyCharm to debug Scrapy projects

Python + Scrapy + JSON + XPath : How to scrape JSON data with Scrapy

Debbie

[Python + Scrapy + JSON + XPath : How to scrape JSON data with Scrapy](https://stackoverflow.com/questions/52779161/python-scrapy-json-xpath-how-to-scrape-json-data-with-scrapy)

I know how to fetch the XPATHs for HTML datapoints with Scrapy. But I have to scrape all the URLs(starting URLs), of this page on this site, which are written in JSON format:https://highape.com/bangalore/all-eventsview-source:https://highape.com/bangalore/all-eventsI usually write this in this format:Please tell me what I should write in 'What To Write Here?' portion.         

2018-10-12 12:04:03Z

I know how to fetch the XPATHs for HTML datapoints with Scrapy. But I have to scrape all the URLs(starting URLs), of this page on this site, which are written in JSON format:https://highape.com/bangalore/all-eventsview-source:https://highape.com/bangalore/all-eventsI usually write this in this format:Please tell me what I should write in 'What To Write Here?' portion.         View page source of the url then copy line 76 - 9045 and save as data.json in your local drive then use this code...

Selecting id attribute using xpath/scrapy

Anna.Klee

[Selecting id attribute using xpath/scrapy](https://stackoverflow.com/questions/52754187/selecting-id-attribute-using-xpath-scrapy)

I am trying to select the user name from the following forum url.However, when I use the following in the scrapy shell:However, in Chrome the selector works fine.Any suggestions what I am doing wrong?I appreciate your replies!

2018-10-11 07:05:01Z

I am trying to select the user name from the following forum url.However, when I use the following in the scrapy shell:However, in Chrome the selector works fine.Any suggestions what I am doing wrong?I appreciate your replies!This is because of quotes inconsistent usage. Note that you're using single quotes both for XPath and string inside XPath.Use either or

How to handle connection or download error in Scrapy?

not2qubit

[How to handle connection or download error in Scrapy?](https://stackoverflow.com/questions/52757819/how-to-handle-connection-or-download-error-in-scrapy)

I am using the following to check for (internet) connection errors in my spider.py:However, when the spider runs and the connection is down, I still get a Traceback (most recent call last): messages. I would like to get rid of this by handling the error and shutting down the spider properly.The output I get is: From this you can notice the following: How can I handle [scrapy.core.scraper] ERROR: Error downloading and make sure the spider get shut down properly?(Or: How can I check internet connection on spider startup?)

2018-10-11 10:27:54Z

I am using the following to check for (internet) connection errors in my spider.py:However, when the spider runs and the connection is down, I still get a Traceback (most recent call last): messages. I would like to get rid of this by handling the error and shutting down the spider properly.The output I get is: From this you can notice the following: How can I handle [scrapy.core.scraper] ERROR: Error downloading and make sure the spider get shut down properly?(Or: How can I check internet connection on spider startup?)Ok, I have been trying to play nice with Scrapy, trying to exit 

gracefully when there is no internet connection or other error. The result? I could not get it to work properly. Instead I ended up just shutting down the entire interpreter and all it's obnoxious deferred children using os._exit(0), like this: That did it!NOTEI tried to use various internal methods to shutdown Scrapy, and handle the obnoxious:issue. This only (?) happens when you use: raise CloseSpider('Because of Connection issues!') among many other attempts. Again followed by a twisted.internet.error.DNSLookupError, even though I have handled that in my code, it seem to appear out of nowhere. Obviously raise is the manual way to always raise an exception. So instead use the CloseSpider() without it. The issue at hand also seem to be a re-occurring issue in the Scrapy framework...and in fact the source code has some FIXME's in there. Even when I tried to apply things like:and using these...PS. I would be great if the Scrapy devs could document how to best deal with such a simple case as a no internet-connection?I believe I may just have found an answer. To exit out of start_requests gracefully, return []. This is telling it there are no requests to process.To close a spider, call the close() method on the spider:

self.close('reason')Addendum: For some strange reason, on one spider self.close('reason') worked while as on another I had to change it to self.close(self, 'reason').I'm having a similar issue with twisted.defer trapping the exceptions after trying to closing the twisted connections which prevents the code shutting down gracefully.So, eject the core I guess...

Scrapy Spider gives an error in Processing

cancun1

[Scrapy Spider gives an error in Processing](https://stackoverflow.com/questions/52678981/scrapy-spider-gives-an-error-in-processing)

I'm new to programing on python and working with scrapy. I am facing an error with the web crawling. I have used similar help pages on this site and even followed a tutorial from beginning to end to no avail, any help will be appreciated.Error says: Spider error processing http://quotes.toscrape.com/> (referer: None) Traceback (most recent call last):I found some similar code and similar one is runing well

but what i write isn'tHere is my code:Here is command promp:Thanks!

2018-10-06 12:17:05Z

I'm new to programing on python and working with scrapy. I am facing an error with the web crawling. I have used similar help pages on this site and even followed a tutorial from beginning to end to no avail, any help will be appreciated.Error says: Spider error processing http://quotes.toscrape.com/> (referer: None) Traceback (most recent call last):I found some similar code and similar one is runing well

but what i write isn'tHere is my code:Here is command promp:Thanks!The problem is your command prompt uses cp437 character set, which doesn't support some characters that you have just scraped (for example, “ and ”)I don't think it is possible to print that characters to windows shell (maybe new PowerShells support it, I don't know). You may create a new file, select a rich character set for it, and write all your output there:If you really want to use the default character encoding that your operating system uses, create the file like that:With the latter option, you will see something like that: I change a little my code and its work the way expected here is updated code:Thanks for all the support!!!

'ImportError: No module named scrapy' when executing exe in wine

mrclx

['ImportError: No module named scrapy' when executing exe in wine](https://stackoverflow.com/questions/52624646/importerror-no-module-named-scrapy-when-executing-exe-in-wine)

I am trying to execute an exe-file on my Raspberry Pi 3 using Exagear and Wine. When I execute the file, the following error occurs:I installed Scrapy in Wine and built the exe with PyInstaller (in wine too).I even created a file “hooks-data.py” with the contentThe file is in the same directory as mimi.py. I used the command wine pyinstaller -F --additional-hooks-dir=. mimi.py to build the exe.Still the exe-file cannot find the module scrapy when executed. Therefore, it seems to me that this is an exagear or a wine issue.Versions used:editThe same error occurs with and without Exagear. So error cannot be caused by it.

2018-10-03 10:09:02Z

I am trying to execute an exe-file on my Raspberry Pi 3 using Exagear and Wine. When I execute the file, the following error occurs:I installed Scrapy in Wine and built the exe with PyInstaller (in wine too).I even created a file “hooks-data.py” with the contentThe file is in the same directory as mimi.py. I used the command wine pyinstaller -F --additional-hooks-dir=. mimi.py to build the exe.Still the exe-file cannot find the module scrapy when executed. Therefore, it seems to me that this is an exagear or a wine issue.Versions used:editThe same error occurs with and without Exagear. So error cannot be caused by it.

Can´t extract the pagination link with scrapy

Jonas

[Can´t extract the pagination link with scrapy](https://stackoverflow.com/questions/52616835/can%c2%b4t-extract-the-pagination-link-with-scrapy)

I want to identify the "next-page-link" with and for scrapy of a multi page website. 

I have the feeling that I cannot do it the common way as the href-content is empty (href=""). See here:I tried 

    response.css('div.page-navigation > a::attr(href)').extract_first()but it's not working.I´d appreciate if someone could help me as I´m struggeling with this problem already for a while.

2018-10-02 21:42:40Z

I want to identify the "next-page-link" with and for scrapy of a multi page website. 

I have the feeling that I cannot do it the common way as the href-content is empty (href=""). See here:I tried 

    response.css('div.page-navigation > a::attr(href)').extract_first()but it's not working.I´d appreciate if someone could help me as I´m struggeling with this problem already for a while.You can simply generate the urls, then parse.

Scrapy - Disable Selenium after first request

olegario

[Scrapy - Disable Selenium after first request](https://stackoverflow.com/questions/52631469/scrapy-disable-selenium-after-first-request)

I'm scraping urls from a site but only the first request needs of selenium and the other don't. Is it possible to turn off Selenium in the middle of scrap process? I want do that, because, as you probably now, Selenium slows down a lot the scrap process. This is the code for the Spider:

2018-10-03 16:13:46Z

I'm scraping urls from a site but only the first request needs of selenium and the other don't. Is it possible to turn off Selenium in the middle of scrap process? I want do that, because, as you probably now, Selenium slows down a lot the scrap process. This is the code for the Spider:You could modify your middleware so that it only uses Selenium when the request comes with a render_js meta key.Something like this:This works because when a downloader middleware's process_request returns None, the request will proceed to the next middlewares in the chain, eventually hitting Scrapy's downloader.More info here: https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request

Python Scrapy crawling takes too much time using xpath element selection with selenium in chrome

Marek Bernád

[Python Scrapy crawling takes too much time using xpath element selection with selenium in chrome](https://stackoverflow.com/questions/52582834/python-scrapy-crawling-takes-too-much-time-using-xpath-element-selection-with-se)

My problem is that I wrote few selenium Scrapy web spiders just for school task purposes and I wanted to crawl politely (DOWNLOAD_DELAY = 5 *per page), but I even don't have to, because it take too much time to crawl one page. For finding all elements in one page I wait even 30 seconds, where in every page I am looking for 13 elements that could or not be present on the page.The problem I found is between Pycharm IDE terminal from where I run python script and web bot spider selenium browser during selecting data elements by xpath.What my spider do at all:The terminal will make demand with POST method on selenium chrome browser to find specific one element by xpath, and if this element is not present on web page, selenium chrome browser will respond always with delay of 5 seconds per one xpath element search.The page in selenium browser will load quickly - in one second.If element is not found, an exception occurs, which I treat like this in code of spider (IDE waits 5 seconds on selenium chrome to throw exception):Loading URLs to crawl [*Updated]:During crawling I have upper described terminal output as follows:I would even like to give here some links of similar problems, but I did not find any. People were talking about problems mainly on server side [1] [2], but I think the problem is on my side.Could you someone explain me, why it takes so much time, and how to repair it - reduce that explicit time delay please?

2018-09-30 22:58:37Z

My problem is that I wrote few selenium Scrapy web spiders just for school task purposes and I wanted to crawl politely (DOWNLOAD_DELAY = 5 *per page), but I even don't have to, because it take too much time to crawl one page. For finding all elements in one page I wait even 30 seconds, where in every page I am looking for 13 elements that could or not be present on the page.The problem I found is between Pycharm IDE terminal from where I run python script and web bot spider selenium browser during selecting data elements by xpath.What my spider do at all:The terminal will make demand with POST method on selenium chrome browser to find specific one element by xpath, and if this element is not present on web page, selenium chrome browser will respond always with delay of 5 seconds per one xpath element search.The page in selenium browser will load quickly - in one second.If element is not found, an exception occurs, which I treat like this in code of spider (IDE waits 5 seconds on selenium chrome to throw exception):Loading URLs to crawl [*Updated]:During crawling I have upper described terminal output as follows:I would even like to give here some links of similar problems, but I did not find any. People were talking about problems mainly on server side [1] [2], but I think the problem is on my side.Could you someone explain me, why it takes so much time, and how to repair it - reduce that explicit time delay please?You're probably using implicit_wait of 5 seconds. Because of that, when find_element doesn't find anything it waits for 5 seconds to give it a chance to appear...

Spider for a messenger

Z.Mon

[Spider for a messenger](https://stackoverflow.com/questions/52585694/spider-for-a-messenger)

I want to extract the channels information of Bale messenger.(very similar to telegram) It's an Iranian messenger which doesn't give developers any API. So I should write a spider myself. I searched and I found out scrapy will help my to do it. However my problem is that Bale take the number and send a code to that phone number for its authentication(just like telegram). I don't know how to handle this authentication in my spider. Can you please help me how to authenticate correctly? Or what is the better solution for extracting information of its channel?Thank you! :)

2018-10-01 06:35:03Z

I want to extract the channels information of Bale messenger.(very similar to telegram) It's an Iranian messenger which doesn't give developers any API. So I should write a spider myself. I searched and I found out scrapy will help my to do it. However my problem is that Bale take the number and send a code to that phone number for its authentication(just like telegram). I don't know how to handle this authentication in my spider. Can you please help me how to authenticate correctly? Or what is the better solution for extracting information of its channel?Thank you! :)I guess we cant handle direct authentication using scrapy I guess.But with given credentials we can access it.Check here for more information on handling login staff.

Scrapy Contracts - Unhandled error in Deferred

ElToro1966

[Scrapy Contracts - Unhandled error in Deferred](https://stackoverflow.com/questions/52587552/scrapy-contracts-unhandled-error-in-deferred)

I am writing a spider using Scrapy, and I am currently adding contracts to the spider. The spider is still running fine, but I am getting weird results when running checks, after adding @returns in the contracts. I am suddenly getting "Unhandled error is Deferred" when running scrapy check:The spider code:Two things are odd here. First, that the feedback from scrapy check says 0 contracts, even though there are 3 (In fact, contracts seem to be counted only when they fail). Second, the error message, which does not make much sense (btw, the error does not interrupt the execution of the check). Scrapy bugs?Note: Running gives me:

2018-10-01 08:54:08Z

I am writing a spider using Scrapy, and I am currently adding contracts to the spider. The spider is still running fine, but I am getting weird results when running checks, after adding @returns in the contracts. I am suddenly getting "Unhandled error is Deferred" when running scrapy check:The spider code:Two things are odd here. First, that the feedback from scrapy check says 0 contracts, even though there are 3 (In fact, contracts seem to be counted only when they fail). Second, the error message, which does not make much sense (btw, the error does not interrupt the execution of the check). Scrapy bugs?Note: Running gives me:I expect poor exception reporting here is a scrapy bug. Contracts are still considered a new feature and are also pretty limited. As for what is going on: you are supposed to specify @returns requests 1 not @returns responses 1. Specifying multiple @url directives will also not work for you and only the first url will be checked, I am not sure how to address that frankly without actually extending contracts functionality.

how to download images from scrapy python and save them to a folder and their path in the variable

shahrukh ijaz

[how to download images from scrapy python and save them to a folder and their path in the variable](https://stackoverflow.com/questions/52560172/how-to-download-images-from-scrapy-python-and-save-them-to-a-folder-and-their-pa)

I want to download an image and url to look like this "/get.aspx?id=1988147"

and save the downloaded image to a file, with the save path in the item like item['image'].

2018-09-28 17:27:58Z

I want to download an image and url to look like this "/get.aspx?id=1988147"

and save the downloaded image to a file, with the save path in the item like item['image'].Scrapy has a built-in mechanism for downloading files, media pipelines.In the simplest case, you would set a field of your item (either file_urls or image_urls), activate the pipeline in your settings.py, and set the path (FILES_STORE or IMAGES_STORE) you want to download to.

If you need to make additional changes, you can create a custom subclass.This is enough to download the files you want, as well as save some additional metadata.

Pass a list of URLs to Scrapy function

maverick_

[Pass a list of URLs to Scrapy function](https://stackoverflow.com/questions/52560904/pass-a-list-of-urls-to-scrapy-function)

I have a API Python that gets two arguments (URL and a user-defined word) and provides in the JSON file how many times the specified word appears in the URL.In the meanwhile, I would like to spend more than one URL at a time, a list. I would also like to make the request with AsyncIO. Any suggestion ?Follow the code:

2018-09-28 18:28:02Z

I have a API Python that gets two arguments (URL and a user-defined word) and provides in the JSON file how many times the specified word appears in the URL.In the meanwhile, I would like to spend more than one URL at a time, a list. I would also like to make the request with AsyncIO. Any suggestion ?Follow the code:

i want to click a link of website using scrapy python

shahrukh ijaz

[i want to click a link of website using scrapy python](https://stackoverflow.com/questions/52517882/i-want-to-click-a-link-of-website-using-scrapy-python)



2018-09-26 12:33:58Z

The following script should fetch you the required items exhausting all the clicks connected to next page link. You can't use here response.follow() as there is no link to follow other than clicking on it.I used harcoded wait within the script which is not recommended at all. You should replace the same with Explicit Wait.

How do I convert javascript postData to Python list?

Billy Jhon

[How do I convert javascript postData to Python list?](https://stackoverflow.com/questions/52531068/how-do-i-convert-javascript-postdata-to-python-list)

I use Scrapy to mimic Post Request from the page. Need to get payload values from the following extract.

I need to get the values(postData) from this JS construction into python list.What kind of data type is this postData?So, what I do is as follows:Which returns a list. The problem, however, is that one of the values has , in it, So it is broken into two separate values in a list, which is no good.

Like this one is supposed to be a single list value, but appears to be 2 values.So how do convert this postData into python list saving all values as they are?

2018-09-27 06:49:24Z

I use Scrapy to mimic Post Request from the page. Need to get payload values from the following extract.

I need to get the values(postData) from this JS construction into python list.What kind of data type is this postData?So, what I do is as follows:Which returns a list. The problem, however, is that one of the values has , in it, So it is broken into two separate values in a list, which is no good.

Like this one is supposed to be a single list value, but appears to be 2 values.So how do convert this postData into python list saving all values as they are?This is no data type, it is an arbitrary javascript function defined by the page you are working with and the values here are arguments to that function which is called when the link is clicked. You could parse it a little "by hand" to be seen as json for example like so:Not very robust but does the trick, will fail if there are additional double / single quotes inside the string values. Otherwise check js2xml or slimit for parsing javascript.

Scrapy Downloading Media Files From Embedded HREF Links On A Page Using ITEM_PIPELINES

R.Zane

[Scrapy Downloading Media Files From Embedded HREF Links On A Page Using ITEM_PIPELINES](https://stackoverflow.com/questions/52524376/scrapy-downloading-media-files-from-embedded-href-links-on-a-page-using-item-pip)

I know this must be a newbie question, but I can't find how to use the actual href link pointing to an mp3 file, to go to that link and download the mp3 file (or any file for that matter). I have tried the Documentation and various stackoverflow questions, but can't seem to figure it out.Here is my code setup:settings.pyitems.pyspider.pyI am almost positive I don't even need the filename open/write implementation because the pipeline is designed to do this. But I can't figure out where in the spider to put the file_urls field and to get the pipeline to work. Any help would be very much appreciated.

2018-09-26 18:54:59Z

I know this must be a newbie question, but I can't find how to use the actual href link pointing to an mp3 file, to go to that link and download the mp3 file (or any file for that matter). I have tried the Documentation and various stackoverflow questions, but can't seem to figure it out.Here is my code setup:settings.pyitems.pyspider.pyI am almost positive I don't even need the filename open/write implementation because the pipeline is designed to do this. But I can't figure out where in the spider to put the file_urls field and to get the pipeline to work. Any help would be very much appreciated.

Get list of data in different sections on scrapy

Lucas Siqueira

[Get list of data in different sections on scrapy](https://stackoverflow.com/questions/52547535/get-list-of-data-in-different-sections-on-scrapy)

I need data scrape in different sections of a site. In first section, I get data from a customer and the id of your orders. With this id, I access a second section and get the items details from the orders. So, I need to concatenate the result of a dict "customer" with a list of "orders" with a list of "itens". Basically, my algorithm is:But I can't code this logic with Scrapy asynchronous architecture. The closer of this what I got it was print many times the same costumer dict in result. Anybody can help a to do this?

2018-09-28 02:38:44Z

I need data scrape in different sections of a site. In first section, I get data from a customer and the id of your orders. With this id, I access a second section and get the items details from the orders. So, I need to concatenate the result of a dict "customer" with a list of "orders" with a list of "itens". Basically, my algorithm is:But I can't code this logic with Scrapy asynchronous architecture. The closer of this what I got it was print many times the same costumer dict in result. Anybody can help a to do this?Since you have A and B type requests for 1 item you have two chain requests to act in order: first crawl A then crawl B N-times:So your crawl logic is:In scrapy it would look something like:

Saving images from scrapy integrated with django

jay queue

[Saving images from scrapy integrated with django](https://stackoverflow.com/questions/52473892/saving-images-from-scrapy-integrated-with-django)

I’ve got a Scrapy project hooked up to a Django project and everything is working fine (i.e. when I run my scraper, I’m able to save items to the DB). I’m trying to add an image scraper to my project and I can’t get it to work. I can get Scrapy’s image scraper to work on its own but not when hooked up to the Django projectThe error I’m getting is the following:Here’s my project:Models.pyItems.py --

Note - I'm adding the image_urls and images field to the django item  objectspider.pysettings.pyAny help here would be much appreciated

2018-09-24 06:42:24Z

I’ve got a Scrapy project hooked up to a Django project and everything is working fine (i.e. when I run my scraper, I’m able to save items to the DB). I’m trying to add an image scraper to my project and I can’t get it to work. I can get Scrapy’s image scraper to work on its own but not when hooked up to the Django projectThe error I’m getting is the following:Here’s my project:Models.pyItems.py --

Note - I'm adding the image_urls and images field to the django item  objectspider.pysettings.pyAny help here would be much appreciatedThe error you're having ValueError: Missing scheme in request url: h, comes from a request you are making in the media pipeline. You are requesting an url without a scheme, such as http(s), ftp, file, data, irc, and others... Exhaustiv list here.Check the urls you are getting from image_urls = response.xpath('//img/@data-img')[0].extract(). It needs to be a list of urls (don't forget the scheme !)

Can't send an email with Scrapy(1.5.1) in Python

Soutzikevich

[Can't send an email with Scrapy(1.5.1) in Python](https://stackoverflow.com/questions/52428670/cant-send-an-email-with-scrapy1-5-1-in-python)

I am new to Python and Scrapy.All I want to achieve, is send a simple email, using only Scrapy.

I have read the documentation from the creators' webpage and I must be missing something, but I can't quite put my finger on it.Here is the code:Update: I am running this script with pycharm instead of using scrapy.cmdline. Does this have anything to do as to why my code is not working?This is the output I get: 

2018-09-20 15:35:58Z

I am new to Python and Scrapy.All I want to achieve, is send a simple email, using only Scrapy.

I have read the documentation from the creators' webpage and I must be missing something, but I can't quite put my finger on it.Here is the code:Update: I am running this script with pycharm instead of using scrapy.cmdline. Does this have anything to do as to why my code is not working?This is the output I get: The problem was that I was running the script directly from my IDE (Pycharm).To run a spider from the IDE, without using a terminal, one could invoke a command programmatically.Scrapy has a module that allows doing this:Scrapy email library works like a charm!

Copy data from a dynamic website using scrapy

Никита Ворошилов

[Copy data from a dynamic website using scrapy](https://stackoverflow.com/questions/52406933/copy-data-from-a-dynamic-website-using-scrapy)

I started to write a scraper for the site to collect data on cars. As it turned out, the data structure can change, since the sellers do not fill all the fields, because of what there are fields that can change, and during the scraper as a result in the csv file, the values ​​are in different fields.page example:https://www.olx.ua/obyavlenie/prodam-voikswagen-touran-2011-goda-IDBzxYq.html#87fcf09cbdhttps://www.olx.ua/obyavlenie/fiat-500-1-4-IDBjdOc.html#87fcf09cbddata example:

Data exampleOne approach was to check the field name with text () = "Category name", but I'm not sure how to correctly write the result to the correct cells.Also I use the built-in Google developer tool, and with the help of the command document.getElementsByClassName('margintop5')[0].innerText

I brought out the whole contents of the table, but the results are not structured. So, if the output can be in json format then it would solve my problem?innerText resultIn addition, when I studied the page code, I came across a javascript script in which all the necessary data is already structured, but I do not know how to get them.data in json dictHow can I get the data from the pages using python and scrapy?

2018-09-19 13:23:59Z

I started to write a scraper for the site to collect data on cars. As it turned out, the data structure can change, since the sellers do not fill all the fields, because of what there are fields that can change, and during the scraper as a result in the csv file, the values ​​are in different fields.page example:https://www.olx.ua/obyavlenie/prodam-voikswagen-touran-2011-goda-IDBzxYq.html#87fcf09cbdhttps://www.olx.ua/obyavlenie/fiat-500-1-4-IDBjdOc.html#87fcf09cbddata example:

Data exampleOne approach was to check the field name with text () = "Category name", but I'm not sure how to correctly write the result to the correct cells.Also I use the built-in Google developer tool, and with the help of the command document.getElementsByClassName('margintop5')[0].innerText

I brought out the whole contents of the table, but the results are not structured. So, if the output can be in json format then it would solve my problem?innerText resultIn addition, when I studied the page code, I came across a javascript script in which all the necessary data is already structured, but I do not know how to get them.data in json dictHow can I get the data from the pages using python and scrapy?You can do it by extracting the JS code from the <script> block, using a regex to get only the JS object with the data and then loading it using the json module:This way, data is a python dict containing the data from the JS object.More about the re_first method here: https://doc.scrapy.org/en/latest/topics/selectors.html#using-selectors-with-regular-expressionsI would say that you will need to : 1) Convert the below C# class to a python class. (I created it using this post : https://stackoverflow.com/a/48023576/4180382) 2) Make a webcall from Python to the javascript file extracting the json string using regex (the text behind 'GPT.targeting')3) Convert the json string to your newly created Python class.

How to read div id

user3661384

[How to read div id](https://stackoverflow.com/questions/52338156/how-to-read-div-id)

I have this HTML code:I'm trying to read the tag id from the html. I´m using the code below, but it does not work.

2018-09-14 19:25:21Z

I have this HTML code:I'm trying to read the tag id from the html. I´m using the code below, but it does not work.You're using a CSS selector with an xpath expression.If you want to use xpath expression, you should do something like this:Also, the xpath expression for the element you want is probably this:This expression will search for any div with class="competition" and return it's id attribute. The HTML you posted is a bit messy so I didn't really test it, but it seems correct. if you have any doubts building your xpath, or want to improve it on your own here is a great guide.

how do i pass a list in scrapy's itemloader?

shovan rai

[how do i pass a list in scrapy's itemloader?](https://stackoverflow.com/questions/52273393/how-do-i-pass-a-list-in-scrapys-itemloader)

I am having difficulty getting the image url. how can i get the image_url in itemloader? It works fine if I yield a dictionary with value as image_url (withoud ItemLoader).def parse_property(self, response):   

2018-09-11 10:00:48Z

I am having difficulty getting the image url. how can i get the image_url in itemloader? It works fine if I yield a dictionary with value as image_url (withoud ItemLoader).def parse_property(self, response):   This will work fine

Scrapy Xpath: Extracting @title from img node

Anh Quoc Vo

[Scrapy Xpath: Extracting @title from img node](https://stackoverflow.com/questions/52243212/scrapy-xpath-extracting-title-from-img-node)

I want to extract the @title from the Main Notes According to Your Votes section from this page: https://www.fragrantica.com/perfume/Remy-Latour/Cigar-9351.htmlI have fetched the HTML, then tried this line of code on scrapy shell but the output was None:What am I doing wrong?

2018-09-09 09:38:46Z

I want to extract the @title from the Main Notes According to Your Votes section from this page: https://www.fragrantica.com/perfume/Remy-Latour/Cigar-9351.htmlI have fetched the HTML, then tried this line of code on scrapy shell but the output was None:What am I doing wrong?This will workDo not forget to set USER AGENT to your settings.pyCrawler data form website use Scrapy 1.5.0 - PythonIf you check source code (Ctrl+U) you'll find:that means that above <div> is rendered by Javascript that's why your code doesn't work. 

New directory or file is not getting created on running the scraper (scrapy) through cmd

Mann Baraiya

[New directory or file is not getting created on running the scraper (scrapy) through cmd](https://stackoverflow.com/questions/52171266/new-directory-or-file-is-not-getting-created-on-running-the-scraper-scrapy-thr)

this is the main filethis is the settings filepipelines.py fileitems.py fileWhenever I run the scraper, it is supposed to create a json file. The scraper works fine, but it just doesn't save the contents to the file.

2018-09-04 17:23:09Z

this is the main filethis is the settings filepipelines.py fileitems.py fileWhenever I run the scraper, it is supposed to create a json file. The scraper works fine, but it just doesn't save the contents to the file.You need to define FEED_URI like below in settings.py%(Name)s will be replaced by the name of the spider and %(time)s will be replaced by the time when the crawler was run thus creating a new file every time.

Scrapy is showing notImplementedError and I don't know why

Sandy Lee

[Scrapy is showing notImplementedError and I don't know why](https://stackoverflow.com/questions/52173365/scrapy-is-showing-notimplementederror-and-i-dont-know-why)

My Scrapy code doesn't work and I am not sure why. My spider is a test to crawl the Game of Throne subreddit on Reddit. Here is my code:And here is a log of the errors:The Redditbot2Spider.parse method is there, so I don't get why it is saying that. Any ideas?

2018-09-04 20:10:41Z

My Scrapy code doesn't work and I am not sure why. My spider is a test to crawl the Game of Throne subreddit on Reddit. Here is my code:And here is a log of the errors:The Redditbot2Spider.parse method is there, so I don't get why it is saying that. Any ideas?Your code has an indentation problem, the parse method is in the same level as the class is, so the interpreter doesn't realize that it is a member of the class. You have to make an indent in the parse method:

How to scrape information from a website that doesn't use POST

J.Paravicini

[How to scrape information from a website that doesn't use POST](https://stackoverflow.com/questions/52137368/how-to-scrape-information-from-a-website-that-doesnt-use-post)

I need to get some information from a website that uses an HTML select to filter its content. However, i'm having difficulties doing so, since when changing the value from the select, the website does not 'reload' it uses some internal function to do get the new content.The webpage in question is this and if I use the Chrome developer tools to see what happens when I change the value of the select. I get a call looking like this.Interesting is, that the uid is the id of the option of the select, so we are getting the correct id. However, when I go to this link I just get a page saying null.Taking a similar website into account, this one. When I change the select form there, I get a form data which I can use to get the information I want.I'm fairly new to scraping and honestly I don't understand how I can get this information. If it's for some use I'm using scrapy in python to parse the information from the websites.

2018-09-02 13:11:57Z

I need to get some information from a website that uses an HTML select to filter its content. However, i'm having difficulties doing so, since when changing the value from the select, the website does not 'reload' it uses some internal function to do get the new content.The webpage in question is this and if I use the Chrome developer tools to see what happens when I change the value of the select. I get a call looking like this.Interesting is, that the uid is the id of the option of the select, so we are getting the correct id. However, when I go to this link I just get a page saying null.Taking a similar website into account, this one. When I change the select form there, I get a form data which I can use to get the information I want.I'm fairly new to scraping and honestly I don't understand how I can get this information. If it's for some use I'm using scrapy in python to parse the information from the websites.One solution is to use client layer which executes both: your scraping "script" and all javascript sent by the website, simulating a real browser. I'm succesfully using PhantomJS for this together with Selenium aka Webdriver API:

  https://selenium-python.readthedocs.io/getting-started.htmlNote that historically Selenium was the first product doing that so the name of this API. In my opinion PhantomJS is better suited, headless by default (doesn't run any GUI process) and faster. Both Selenium and PhantomJS implement a protocol called Webdriver which your Python program would use.It may sound complicated but please just use Getting Started documentation cited above and check if it suits you.EDIT:

this article also contains simple example of using the described setup:

https://realpython.com/headless-selenium-testing-with-python-and-phantomjs/Note that in many articles people do the similar thing for testing, so the term "scraping" is not even mentioned. but technically it's the same - emulating the user clicking in the browser and at the end getting data from specific page elements.

Connection was refused by other side: 111: Connection refused

Omar Riaz

[Connection was refused by other side: 111: Connection refused](https://stackoverflow.com/questions/52098291/connection-was-refused-by-other-side-111-connection-refused)

I have a spider for LinkedIn. It is working fine on my local machine but when I deploy on Scrapinghub I got error:The complete log of Scrapinghub is:How can I fix this?

2018-08-30 13:27:57Z

I have a spider for LinkedIn. It is working fine on my local machine but when I deploy on Scrapinghub I got error:The complete log of Scrapinghub is:How can I fix this?LinkedIn prohibits scraping:It's reasonable to think that they may actively block connections from Scrapinghub and similar services.

How check if website support http, htts and www prefix with scrapy

dorinand

[How check if website support http, htts and www prefix with scrapy](https://stackoverflow.com/questions/52104875/how-check-if-website-support-http-htts-and-www-prefix-with-scrapy)

I am using scrapy to check, if some website works fine, when I use http://example.com, https://example.com or http://www.example.com. When I create scrapy request, it works fine. for example, on my page1.com, it is always redirected to https://. I need to get this information as return value, or is there better way how to get this information using scrapy?The output of this spider is this:This is nice, but I would like to get this information as return value to know, that e.g. on http is response code 200 and than save it to dictionary for later processing or save it as json to file(using items in scrapy). DESIRED OUTPUT:

I would like to have dictionary named a with all information:Later I would like to scrape more information, so I will need to store all information under one object/json/...

2018-08-30 20:24:41Z

I am using scrapy to check, if some website works fine, when I use http://example.com, https://example.com or http://www.example.com. When I create scrapy request, it works fine. for example, on my page1.com, it is always redirected to https://. I need to get this information as return value, or is there better way how to get this information using scrapy?The output of this spider is this:This is nice, but I would like to get this information as return value to know, that e.g. on http is response code 200 and than save it to dictionary for later processing or save it as json to file(using items in scrapy). DESIRED OUTPUT:

I would like to have dictionary named a with all information:Later I would like to scrape more information, so I will need to store all information under one object/json/...Instead of using the meta possibility which was pointed out by eLRuLL you can parse  request.url: To store the values for different runs together in one dict/json you can use an additional pipeline like mentioned in https://doc.scrapy.org/en/latest/topics/item-pipeline.html#duplicates-filter 

So you have something like: You must additionally activate the pipelineyou are doing one extra request at the beginning of the spider and you could deal with all those domains with the start_requests method:check that I am using the meta request parameter to pass the information to the next callback method on which prefix was used.

How to select all text content inside div using XPath?

Riyas PK

[How to select all text content inside div using XPath?](https://stackoverflow.com/questions/52095112/how-to-select-all-text-content-inside-div-using-xpath)

I want to select all text inside a div without considering tags inside.I need to get the result as I tried this 

2018-08-30 10:45:53Z

I want to select all text inside a div without considering tags inside.I need to get the result as I tried this You're asking for the string-value of that div:Or, if you wish whitespace to be trimmed from the ends and consolidated internally:Try to string() it with XPath:check the following code for clarification and try the following for the required output

using xpath go to next page with scrapy

Chi Pham

[using xpath go to next page with scrapy](https://stackoverflow.com/questions/52069519/using-xpath-go-to-next-page-with-scrapy)

I create a spider to scrape data from a website. It was ok until I added a crawlspider with rule to keep it continue with next pages. I guess that my xpath in Rule is wrong. Could you please help me to fix it? Ps: I'm using python3This is my spider: And this is my expect result which I have got after scaping the first page (and want to get more from next pages):

2018-08-29 04:31:13Z

I create a spider to scrape data from a website. It was ok until I added a crawlspider with rule to keep it continue with next pages. I guess that my xpath in Rule is wrong. Could you please help me to fix it? Ps: I'm using python3This is my spider: And this is my expect result which I have got after scaping the first page (and want to get more from next pages):You need to use this for restrict_xpaths (not a link's text or href but link node itself):I would use the class of the Next Page button: Which results in 2 results. One for top and one for button arrow. 

But when you open the first next page (2nd page), the previous page gets a link with class prevNext as well. 

This isn't a big issue because scrapy would filter most of the additional requests. 

But it's possible to restrict the links with a text filter: Or if you have doubts that Next is in other links as well you can combine them: 

Scrapy difficulties in parsing multiple urls and storing data

PyRar

[Scrapy difficulties in parsing multiple urls and storing data](https://stackoverflow.com/questions/52069753/scrapy-difficulties-in-parsing-multiple-urls-and-storing-data)

I with to retrieve data from the script tags of multiple urls with Regex. I have a csv file('links.csv ') that contains all the urls I'll need to scrape. I managed to read the csv and store all the urls in the variable named 'start_urls'. My problem is that I need a way to read the urls from 'start_urls' one at a time and execute the next part of my code. When I execute my code in the terminal and it returns 2 errors:Here are some examples of urls I stored in the initial csv('links.csv'):Here is my code:

2018-08-29 04:58:38Z

I with to retrieve data from the script tags of multiple urls with Regex. I have a csv file('links.csv ') that contains all the urls I'll need to scrape. I managed to read the csv and store all the urls in the variable named 'start_urls'. My problem is that I need a way to read the urls from 'start_urls' one at a time and execute the next part of my code. When I execute my code in the terminal and it returns 2 errors:Here are some examples of urls I stored in the initial csv('links.csv'):Here is my code:The site for the S9 has a different structure to S8 so there will be always an error because in S9 you don't find COUNTRY_SHOP_STATUS. Using the csv-writer directly is not scrapy like. You overwrite your result many times. Because you open a new csv-File for every product. If you realy want to do it that way. Open the csv file in start_requests and append in parse. But have a look into item pipelines. 

I remove the loop with the zip because parse is already at the lowest level.I changed input csv_file (so_52069753.csv) as well: So it's possible to configure if a url is processed or not. 

How can I collect multiple items from two pages and merge them into a single database row multiple times with one new item?

MaybeUser

[How can I collect multiple items from two pages and merge them into a single database row multiple times with one new item?](https://stackoverflow.com/questions/52046850/how-can-i-collect-multiple-items-from-two-pages-and-merge-them-into-a-single-dat)

Disclaimer: I'm fairly new to Scrapy.I already made a working version of the spider. But, it takes only one image. How to make a link to the image taken from a new page and added to the rest of the elements and add in the database? In this case, as many times as there will be images. At the end of the code I wrote what I wanted to do. I also made a visual scheme.SchemeSpider code with comments in the end:

2018-08-27 20:56:27Z

Disclaimer: I'm fairly new to Scrapy.I already made a working version of the spider. But, it takes only one image. How to make a link to the image taken from a new page and added to the rest of the elements and add in the database? In this case, as many times as there will be images. At the end of the code I wrote what I wanted to do. I also made a visual scheme.SchemeSpider code with comments in the end:

How to remove duplicates while inserting new records MongoDB using PyMongo in Scrapy project

Krish

[How to remove duplicates while inserting new records MongoDB using PyMongo in Scrapy project](https://stackoverflow.com/questions/51945112/how-to-remove-duplicates-while-inserting-new-records-mongodb-using-pymongo-in-sc)

In my Scrapy project I'm storing the scraped data in MongoDB using PyMongo. There are duplicate records while crawling the web pages in page by page manner, I just want to remove those duplicate records which are with same name at the time of inserting them in to database. Please suggest me the best solution.

Here is my code in "pipelines.py" . Please guide me how to remove duplicates in the method "process_item". I found few queries to remove duplicates from the database in the internet but want a solution in Python.

2018-08-21 08:59:16Z

In my Scrapy project I'm storing the scraped data in MongoDB using PyMongo. There are duplicate records while crawling the web pages in page by page manner, I just want to remove those duplicate records which are with same name at the time of inserting them in to database. Please suggest me the best solution.

Here is my code in "pipelines.py" . Please guide me how to remove duplicates in the method "process_item". I found few queries to remove duplicates from the database in the internet but want a solution in Python.It slightly depends on what's in the item but I would use update with upsert like thisYou could also play around with filter. Basically, you wouldn't even have to remove dupes. It works like if-else condition if applied properly. If the object doesn't exist, create one. Else, update with given properties on given keys. Like in a dictionary. Worst case scenario it updates with the same values. So it's faster than inserting, querying and deleting found duplicates.docsThere's no literal if-else in MongoDB and @tanaydin advice with automatically dropping dupes also works in Python. It could be better than my advice, depending on what you really need.If you really want to remove documents given some criteria, then there's delete_one and delete_many in pymongo.docs

scrapy fail to cooperate with python3.7

Zhe Yu He

[scrapy fail to cooperate with python3.7](https://stackoverflow.com/questions/51939334/scrapy-fail-to-cooperate-with-python3-7)

I failed to install twisted by pip command, so I manually downloaded the .whl file and got it installed( version 18.7.0).  Only after i did that, my laptop could install scrapy;  however, it seems that the twisted package is not compatible with python 3.7 and it keeps saying "syntax error"I have tried some method posted on the Github about this issue(https://github.com/scrapy/scrapy/issues/3143), but none of them solve it. I wonder whether I need to shift to python 3.6 or not? cause my python spider can only setup it's downloader and cannot parse webpagesCould anyone please give me some advise?    

2018-08-20 22:33:34Z

I failed to install twisted by pip command, so I manually downloaded the .whl file and got it installed( version 18.7.0).  Only after i did that, my laptop could install scrapy;  however, it seems that the twisted package is not compatible with python 3.7 and it keeps saying "syntax error"I have tried some method posted on the Github about this issue(https://github.com/scrapy/scrapy/issues/3143), but none of them solve it. I wonder whether I need to shift to python 3.6 or not? cause my python spider can only setup it's downloader and cannot parse webpagesCould anyone please give me some advise?    Yes.  Twisted does not yet support Python 3.7.  Try Python 3.6 or earlier.One of the comments from the issue helped me:

twisted.internet.error.TimeoutError: User timeout caused connection failure

Filipe Ferminiano

[twisted.internet.error.TimeoutError: User timeout caused connection failure](https://stackoverflow.com/questions/51913874/twisted-internet-error-timeouterror-user-timeout-caused-connection-failure)

I'm trying to run this command:But I'm getting:this my scrapy version:how can I fix this?

2018-08-19 02:05:12Z

I'm trying to run this command:But I'm getting:this my scrapy version:how can I fix this?You may have a connectivity issue. Start troubleshooting:

python scrapy best approach to not re-crawl if already content ID existed

mrWiga

[python scrapy best approach to not re-crawl if already content ID existed](https://stackoverflow.com/questions/51871259/python-scrapy-best-approach-to-not-re-crawl-if-already-content-id-existed)

below is the crawl results, how do I prevent duplication based on seller_id? At the moment I crawl through pages and pages but want to get unique results based on the ID. If it see the same seller_id within the page from the previous crawl, don't crawl again.below is what I have so far, as you can see I do have if int(clean_total_ads) > 500 which filter only ads that is over 500+ and show in the results but I also need to filter unique seller_ID as well

2018-08-16 06:57:39Z

below is the crawl results, how do I prevent duplication based on seller_id? At the moment I crawl through pages and pages but want to get unique results based on the ID. If it see the same seller_id within the page from the previous crawl, don't crawl again.below is what I have so far, as you can see I do have if int(clean_total_ads) > 500 which filter only ads that is over 500+ and show in the results but I also need to filter unique seller_ID as wellScrapy manages duplicate crawling automatically in fact.

If you wish to do it explicitly you can try this:

Scrapy yielding requests with a depth of 2

Recur

[Scrapy yielding requests with a depth of 2](https://stackoverflow.com/questions/51805642/scrapy-yielding-requests-with-a-depth-of-2)

So I am having trouble logging from a function that is 2 requests down from the starting parse method. Here is the code:Now the only logging that shows up is the Starting off in the parse function and then inside parse_hw_images about to scrapy request parse_hw_imageExpected behavior would be:Can anyone see what is off with what I'm doing?

2018-08-12 04:19:15Z

So I am having trouble logging from a function that is 2 requests down from the starting parse method. Here is the code:Now the only logging that shows up is the Starting off in the parse function and then inside parse_hw_images about to scrapy request parse_hw_imageExpected behavior would be:Can anyone see what is off with what I'm doing?yield scrapy.Request(self.start_urls[0], callback=self.parse) means you are calling same parse method with same URL so scrapy is filtering it as duplicate URL.Set DUPEFILTER_DEBUG=True to see duplicate URLs.

Using a Scrapy pipeline without using settings.py config

Juicy

[Using a Scrapy pipeline without using settings.py config](https://stackoverflow.com/questions/51804320/using-a-scrapy-pipeline-without-using-settings-py-config)

I'm avoiding using the Scrapy boilerplate generator because my code will be integrated as part of a wider project.My current project tree is like this:My pipeline.py contains a pipeline that looks like this:How can I use this class in spider.py without using a settings.py file and scrapy.conf?I've tried importing the pipeline class and setting ITEM_PIPELINES in custom_settings but that throws ValueError: Error loading object 'MongoPipeline': not a full path:

2018-08-11 22:42:07Z

I'm avoiding using the Scrapy boilerplate generator because my code will be integrated as part of a wider project.My current project tree is like this:My pipeline.py contains a pipeline that looks like this:How can I use this class in spider.py without using a settings.py file and scrapy.conf?I've tried importing the pipeline class and setting ITEM_PIPELINES in custom_settings but that throws ValueError: Error loading object 'MongoPipeline': not a full path:it should be:

how to get single script variable data from multiple script in html page

Milan Hirpara

[how to get single script variable data from multiple script in html page](https://stackoverflow.com/questions/51722913/how-to-get-single-script-variable-data-from-multiple-script-in-html-page)

I have one html page there where several script tag but i want only one varible data from within those script .

you can find html page code here https://jsfiddle.net/9Lzc5fxy/i only want data of var roomsAndRatePlans variable because but when i execute below code i get all data within that script tag can any one help me to fix thatHere i code i used to retrieve that data.But i always get all data from script tag that contain that variable and because of that i alway get convertion error injstree = js2xml.parse(snippet)Can any one suggest any solution so i can retrieve only that variable data "var roomsAndRatePlans"

2018-08-07 09:05:28Z

I have one html page there where several script tag but i want only one varible data from within those script .

you can find html page code here https://jsfiddle.net/9Lzc5fxy/i only want data of var roomsAndRatePlans variable because but when i execute below code i get all data within that script tag can any one help me to fix thatHere i code i used to retrieve that data.But i always get all data from script tag that contain that variable and because of that i alway get convertion error injstree = js2xml.parse(snippet)Can any one suggest any solution so i can retrieve only that variable data "var roomsAndRatePlans"I always use regular expressions for the cases when I need to retrieve some variable from JS code.You can achieve what you want with re_first method:The object in data will look like this:P.S. If you use scrapy only for its Selector functionality, you can use parsel instead, which is a scrapy dependency.

How to extract image/href url from div class using scrapy

trisha brara

[How to extract image/href url from div class using scrapy](https://stackoverflow.com/questions/51682677/how-to-extract-image-href-url-from-div-class-using-scrapy)

I having hard time to extract href url from given website code 

2018-08-04 05:18:25Z

I having hard time to extract href url from given website code Probably, you can use regular expressions for this. Here is example:

How to log scrapy log into the Logstash

Yusef

[How to log scrapy log into the Logstash](https://stackoverflow.com/questions/51691756/how-to-log-scrapy-log-into-the-logstash)

I've set up ELK stack on my server correctly and with using python-logstash  I could send my logs to logstash with the following snippet and everything is working correctly.**Next step ** is I want to integrate Logstash with Scrapy,this is part of my Spider codes:I could see my custom logs in Scrapyd log but nothing has been sent to logstashmy question is why it's not sending the log to logstash? How can I log the scrapy logs into the Logstash?

2018-08-05 05:54:34Z

I've set up ELK stack on my server correctly and with using python-logstash  I could send my logs to logstash with the following snippet and everything is working correctly.**Next step ** is I want to integrate Logstash with Scrapy,this is part of my Spider codes:I could see my custom logs in Scrapyd log but nothing has been sent to logstashmy question is why it's not sending the log to logstash? How can I log the scrapy logs into the Logstash?Actually, I've done 99% on it, I just needed to get the scrapy as the loggerI post the answer maybe it will be useful for another person in the same situation.

Dockerfile run scrapy crawl command in a folder

Zerontelli

[Dockerfile run scrapy crawl command in a folder](https://stackoverflow.com/questions/51651063/dockerfile-run-scrapy-crawl-command-in-a-folder)

I got a scrapy spider which can be run in terminal with scrapy crawl estate in the tutorial folder.How do I use the run command in Dockerfile to cd to the tutorial folder and run it?My Dockerfile without the RUN cd:

2018-08-02 10:15:55Z

I got a scrapy spider which can be run in terminal with scrapy crawl estate in the tutorial folder.How do I use the run command in Dockerfile to cd to the tutorial folder and run it?My Dockerfile without the RUN cd:You should set up WORKDIR, ENTRYPOINT and CMD in your docker file:Then:

Scrapy CrawlSpider doesn't quit

Luigi Tiburzi

[Scrapy CrawlSpider doesn't quit](https://stackoverflow.com/questions/51658531/scrapy-crawlspider-doesnt-quit)

I have a problem with scrapy Crawlspider: basically, it doesn't quit, as it is supposed to do, if a CloseSpider exception is raised. Below is the code:The problem is that, although I can see it enters in the conditions, and I can see the Eception raised in the log, the crawler continues crawling.

I run it with:

2018-08-02 16:40:33Z

I have a problem with scrapy Crawlspider: basically, it doesn't quit, as it is supposed to do, if a CloseSpider exception is raised. Below is the code:The problem is that, although I can see it enters in the conditions, and I can see the Eception raised in the log, the crawler continues crawling.

I run it with:I'm gonna guess this is a case of scrapy just taking too long at being shut down rather than actually ignoring the exception. The engine will not exit until it runs through all scheduled/sent requests so I suggest lowering the values of CONCURRENT_REQUESTS/CONCURRENT_REQUESTS_PER_DOMAIN settings to see if that works for you.You created a "Recursive" Spider, so it works recursively. Remove the "rules" parameter and it will stop after a complete crawl.

json file is not created with Python Scrapy Spider

OKY

[json file is not created with Python Scrapy Spider](https://stackoverflow.com/questions/51567877/json-file-is-not-created-with-python-scrapy-spider)

Thing I want to doI want to make json file using Python's Scrapy spider.

I am currently studying at "Data Visualization with Python and JavaScript". In scraping, it is unknown why the json file is not created.Directory structureWorking process/CodeEnter the following code in nwinners_list_spider.py in / nobel_winners / spiders.Enter the following code in the root directory.ErrorThe following display appears and no data is entered in the json file.What I tried1.In the text, it was a bit longer source, but I checked it out only for 'country' variables.2.I entered the scrapy shell and checked the movements of each one using IPython based shell. And It was confirmed that the value was firmly in 'country'.

2018-07-28 03:53:55Z

Thing I want to doI want to make json file using Python's Scrapy spider.

I am currently studying at "Data Visualization with Python and JavaScript". In scraping, it is unknown why the json file is not created.Directory structureWorking process/CodeEnter the following code in nwinners_list_spider.py in / nobel_winners / spiders.Enter the following code in the root directory.ErrorThe following display appears and no data is entered in the json file.What I tried1.In the text, it was a bit longer source, but I checked it out only for 'country' variables.2.I entered the scrapy shell and checked the movements of each one using IPython based shell. And It was confirmed that the value was firmly in 'country'.Try using this code:And then run

scrapy crawl nwinners_list -o nobel_winners.json -t jsonIn the callback function, you parse the response (web page) and return either dicts with extracted data, Item objects, Request objects, or an iterable of these objects. See scrapy documentationThis is the reason why there is 0 item scraped, you need to return them !Also note that .extract() return a list based on your xpath query and .extract_first() returns the first element of the list.

Data missing while scraping website

Amith

[Data missing while scraping website](https://stackoverflow.com/questions/51460020/data-missing-while-scraping-website)

I am trying to scrap a website (Please refer to urls in the code). 

From the website ,i am trying to scrap all the information and transfer the data to json file.To extract the information from websiteI am able to retrieve most of the information from the website.Concern:

Able to scrap data under "Intimation",expect'Intimation For  September 2017' not able to scrap information under this tab.Finding:For 'Intimation For  September 2017', the value is stored in the span tagFor the remaining month the values are stored in the font tagHow to extract information for "Intimation For  September 2017" ?

2018-07-21 20:52:37Z

I am trying to scrap a website (Please refer to urls in the code). 

From the website ,i am trying to scrap all the information and transfer the data to json file.To extract the information from websiteI am able to retrieve most of the information from the website.Concern:

Able to scrap data under "Intimation",expect'Intimation For  September 2017' not able to scrap information under this tab.Finding:For 'Intimation For  September 2017', the value is stored in the span tagFor the remaining month the values are stored in the font tagHow to extract information for "Intimation For  September 2017" ?You tables use different @class (MsoTableGrid and MsoNormalTable) so you need some way to process all of them:

How to find a particular string within a Source code(Xpath) and extract the proceeding text?

Vishal Sharma

[How to find a particular string within a Source code(Xpath) and extract the proceeding text?](https://stackoverflow.com/questions/51430095/how-to-find-a-particular-string-within-a-source-codexpath-and-extract-the-proc)

From the following source code :I want to extract the (bla bla) which is definetely coming after (,[null,") till the point ("])

The reason why I cannot go via script name id and div is because they are dynamic for every page I'm scraping via scrapy. Thus the code must look for (,[null,") and extract the proceeding text.

2018-07-19 19:01:48Z

From the following source code :I want to extract the (bla bla) which is definetely coming after (,[null,") till the point ("])

The reason why I cannot go via script name id and div is because they are dynamic for every page I'm scraping via scrapy. Thus the code must look for (,[null,") and extract the proceeding text.You can try to find script node and get its text with XPath:and then extract required substring:But it looks like you're trying to parse some kind of JSON string inside Javascript so it's better to parse whole JSON and next navigate to your string:

Need help in scraping list of hotel from a website with fixed urls and dynamically loading the content?

Muthu Vigneshwaran

[Need help in scraping list of hotel from a website with fixed urls and dynamically loading the content?](https://stackoverflow.com/questions/51474650/need-help-in-scraping-list-of-hotel-from-a-website-with-fixed-urls-and-dynamical)

I am trying to scrape the details from a hotel listing site this site.

Here when we click the next button for the next page the url remain the same and when looked with inspect element there site is sending a XHR request. I tried to use selenium webdriver and python and the following is my codeIn the above code the yield Request doesnot goto the parse function? Am I  missing anything?I am not getting any error But the scrape output is only of the 1st page even though the pages are being iterated

2018-07-23 08:35:30Z

I am trying to scrape the details from a hotel listing site this site.

Here when we click the next button for the next page the url remain the same and when looked with inspect element there site is sending a XHR request. I tried to use selenium webdriver and python and the following is my codeIn the above code the yield Request doesnot goto the parse function? Am I  missing anything?I am not getting any error But the scrape output is only of the 1st page even though the pages are being iterated

Scrapy request, shell Fetch() in spider

Timothy Webb

[Scrapy request, shell Fetch() in spider](https://stackoverflow.com/questions/51386843/scrapy-request-shell-fetch-in-spider)

I'm trying to reach a specific page, let's call it http://example.com/puppers. This page cannot be reached when connecting directly using scrapy shell or the standard scrapy.request module (results in  <405> HTTP).However, when I use scrapy shell 'http://example.com/kittens' first, and then use fetch('http://example.com/puppers') it works and I get a <200> OK HTTP code. I can now extract data using scrapy shell.I tried implementing this in my script, by altering the referer (using url #1), the user-agent and a few others while connecting to the puppers (url #2) page. I still get a <405> code..I appreciate all the help. Thank you.

2018-07-17 17:04:55Z

I'm trying to reach a specific page, let's call it http://example.com/puppers. This page cannot be reached when connecting directly using scrapy shell or the standard scrapy.request module (results in  <405> HTTP).However, when I use scrapy shell 'http://example.com/kittens' first, and then use fetch('http://example.com/puppers') it works and I get a <200> OK HTTP code. I can now extract data using scrapy shell.I tried implementing this in my script, by altering the referer (using url #1), the user-agent and a few others while connecting to the puppers (url #2) page. I still get a <405> code..I appreciate all the help. Thank you.

Scraping data from Ajax Form Requests using Scrapy

m0rpheu5

[Scraping data from Ajax Form Requests using Scrapy](https://stackoverflow.com/questions/51388018/scraping-data-from-ajax-form-requests-using-scrapy)

I am trying to scrape all the hospital data from this website.

https://www.german-hospital-directory.com/search/Bundesland/Baden-Wuerttemberg.html.After looking at the requests,it makes a form request. And it is not accessible through scrapy shell  And in the response payload,it gives the entire html content. How do I extract each of the hospital data such as URL,NAME, IMAGE and traverse through all the hospitals. Any help would be appreciated as I am new to scrapy. Do I need to use selenium or can I somehow achieve this using scrapy. 

2018-07-17 18:22:37Z

I am trying to scrape all the hospital data from this website.

https://www.german-hospital-directory.com/search/Bundesland/Baden-Wuerttemberg.html.After looking at the requests,it makes a form request. And it is not accessible through scrapy shell  And in the response payload,it gives the entire html content. How do I extract each of the hospital data such as URL,NAME, IMAGE and traverse through all the hospitals. Any help would be appreciated as I am new to scrapy. Do I need to use selenium or can I somehow achieve this using scrapy. You need to GET your URL first (to receive cookies): https://www.german-hospital-directory.com/search/Bundesland/Baden-Wuerttemberg.htmlBut next you need to GET this URL https://www.german-hospital-directory.com/search/_files/main-search/Suchergebnis.jsfSomething like this:

How to pass arguments between two spiders with scrapy callback

R Khalili

[How to pass arguments between two spiders with scrapy callback](https://stackoverflow.com/questions/51346410/how-to-pass-arguments-between-two-spiders-with-scrapy-callback)

I have two scrapy that the first one crawl a sitemap and extract urls and put it in a txt file and the second one reads it and crawl this urls line by line.my code like bellow :second spider :How to change the code to delete read/write to the file?How to use callback in it?Note : This code does not work in one scrapy spider ;code is :Two given scrapy + bellow code ,As an example is said in doc

2018-07-15 07:53:15Z

I have two scrapy that the first one crawl a sitemap and extract urls and put it in a txt file and the second one reads it and crawl this urls line by line.my code like bellow :second spider :How to change the code to delete read/write to the file?How to use callback in it?Note : This code does not work in one scrapy spider ;code is :Two given scrapy + bellow code ,As an example is said in docThis should work:

Scrape 2 different format tables from webpage - Beautiful Soup

Ozdanny

[Scrape 2 different format tables from webpage - Beautiful Soup](https://stackoverflow.com/questions/51284741/scrape-2-different-format-tables-from-webpage-beautiful-soup)

So I'm aiming to scrape 2 tables (in different formats) from a website - FSC Public Search after iterating this over a list of license codes. My issue is that because the two tables I want, Product Data and Certificate Data are in 2 different formats, so I have to scrape them separately. Eg the Product data is in the normal "tr" format on the webpage and the Certificate Data is in "div" form.From a previous question I asked, I have almost resolved my issue and I can retrieve the Certificate data ("div" form) completely fine over a range of license codes. However I can't get the Product Data table to output as I wish. Instead of showing the product data for the 5 license codes, it shows me 5 copies of the first license code. I've tried putting this scraping in the defined function get_data_by_code but I still couldn't get it in the format I would like, which is just a table in a CSV file.Basically I'm not sure where to include this scraping in my function/script so any input would be much appreciated, thank you.EDITSo using this code below, I get 5 copies of the last license code's Product Data.. marginally closer but I still don't see why this is the caseEDIT 2The sample codes I have been using:Format EditThis is the correct tabular format however as you can see, it is the information from the first license code repeated 5 times as opposed to the unique data. This is the format & information I want, everything working fine here:



2018-07-11 11:47:42Z

So I'm aiming to scrape 2 tables (in different formats) from a website - FSC Public Search after iterating this over a list of license codes. My issue is that because the two tables I want, Product Data and Certificate Data are in 2 different formats, so I have to scrape them separately. Eg the Product data is in the normal "tr" format on the webpage and the Certificate Data is in "div" form.From a previous question I asked, I have almost resolved my issue and I can retrieve the Certificate data ("div" form) completely fine over a range of license codes. However I can't get the Product Data table to output as I wish. Instead of showing the product data for the 5 license codes, it shows me 5 copies of the first license code. I've tried putting this scraping in the defined function get_data_by_code but I still couldn't get it in the format I would like, which is just a table in a CSV file.Basically I'm not sure where to include this scraping in my function/script so any input would be much appreciated, thank you.EDITSo using this code below, I get 5 copies of the last license code's Product Data.. marginally closer but I still don't see why this is the caseEDIT 2The sample codes I have been using:Format EditThis is the correct tabular format however as you can see, it is the information from the first license code repeated 5 times as opposed to the unique data. This is the format & information I want, everything working fine here:

For the codes you have provided, this simplified approach should suffice. It simply extracts the necessary information directly from BeautifulSoup without needing to use Pandas to try and extract it:This would produce Certificate_Data.csv containing:And produce Product_Data.csv containing:

Cloudflare Scrapy

Luis Miguel

[Cloudflare Scrapy](https://stackoverflow.com/questions/51253538/cloudflare-scrapy)

I'm trying to scrape a URL using Scrapy with Cloudflare but I can't obtain any results:As the website is protected by Cloudflare, I've installed this:

    https://github.com/clemfromspace/scrapy-cloudflare-middlewareWhen I modified my settings.py, I obtained the next error:At this point I'm stuck. I don't know if I have to change the settings.py or middlewares.py.Could you help me please? I'd like to improve my skills. ;)P.S. I've added my middlewares.py:

2018-07-09 20:28:27Z

I'm trying to scrape a URL using Scrapy with Cloudflare but I can't obtain any results:As the website is protected by Cloudflare, I've installed this:

    https://github.com/clemfromspace/scrapy-cloudflare-middlewareWhen I modified my settings.py, I obtained the next error:At this point I'm stuck. I don't know if I have to change the settings.py or middlewares.py.Could you help me please? I'd like to improve my skills. ;)P.S. I've added my middlewares.py:Use scrapy_rotaing-proxies to get away:

How to mark scrape failed because of 503 as error in Scrapy?

Aminah Nuraini

[How to mark scrape failed because of 503 as error in Scrapy?](https://stackoverflow.com/questions/51253357/how-to-mark-scrape-failed-because-of-503-as-error-in-scrapy)

So I got status 503 when I crawl. It's retried, but then it gets ignored. I want it to be marked as an error, not ignored. How to do that?I prefer to set it in settings.py so it would apply to all of my spiders. handle_httpstatus_list seems will only affect one spider.

2018-07-09 20:14:17Z

So I got status 503 when I crawl. It's retried, but then it gets ignored. I want it to be marked as an error, not ignored. How to do that?I prefer to set it in settings.py so it would apply to all of my spiders. handle_httpstatus_list seems will only affect one spider.There are two settings that you should look into:RETRY_HTTP_CODES:https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#retry-http-codesAnd HTTPERROR_ALLOWED_CODES:https://doc.scrapy.org/en/latest/topics/spider-middleware.html#std:setting-HTTPERROR_ALLOWED_CODESIn the end, I overwrite the retry middleware just for a small change. I set so every time the scraper gave up retrying on something, doesn't matter what is the status code, it will be marked as an error.It seems Scrapy somehow doesn't associate giving up retrying as an error. That's weird for me.This is the middleware if anyone wants to use it. Don't forget to activate it on the settings.py

Scraping Specific Tables using Scrapy

ndewan

[Scraping Specific Tables using Scrapy](https://stackoverflow.com/questions/51258362/scraping-specific-tables-using-scrapy)

What should i do to get only specific tables which are having a heading such as "Specifications" or "Product Details".The Url has many tables and i want to scrape only the table having the above details.the link is:(http://catalog.koffler.com/item/ieee-841-motors/ge-x-d-ultra-841-tefc-extra-severe-duty-motors/M352)

and here is my code in xpath:

2018-07-10 06:18:52Z

What should i do to get only specific tables which are having a heading such as "Specifications" or "Product Details".The Url has many tables and i want to scrape only the table having the above details.the link is:(http://catalog.koffler.com/item/ieee-841-motors/ge-x-d-ultra-841-tefc-extra-severe-duty-motors/M352)

and here is my code in xpath:The site is a bit ugly (it's use tables for presentation) but you can get any value you want using this way (you'll need to split() below values later ):

Stuck on my first scrapy project

Andrea D'Agostino

[Stuck on my first scrapy project](https://stackoverflow.com/questions/51181540/stuck-on-my-first-scrapy-project)

I am trying to get the basics of Scrapy and I've been reading some tutorials.

I am trying to scrape the data from https://www.immobiliare.it, an italian housing website. I would like to reach the homepage (start url) and from there crawl the website following the pagehompage-->province/city-->listing-->house noticeI came up with this code, but it doesn't work and I do not understand why. If I do debug on the single code snippets they work properly, but as a whole the spider won't yield any results, as it stops at the homepage. Can someone hint me where is the mistake? Thank you.

2018-07-04 22:51:31Z

I am trying to get the basics of Scrapy and I've been reading some tutorials.

I am trying to scrape the data from https://www.immobiliare.it, an italian housing website. I would like to reach the homepage (start url) and from there crawl the website following the pagehompage-->province/city-->listing-->house noticeI came up with this code, but it doesn't work and I do not understand why. If I do debug on the single code snippets they work properly, but as a whole the spider won't yield any results, as it stops at the homepage. Can someone hint me where is the mistake? Thank you.

StaleElementReferenceException

oldboy

[StaleElementReferenceException](https://stackoverflow.com/questions/51181949/staleelementreferenceexception)

I've read about the StaleElementReferenceException in the official documentation, but I still don't understand why my code is raising this exception? Does browser.get() instantiate a new spider?I've tried the following to no avail. I've also tried placing the links variable elsewhere in the script, in a different scope, also to no avail.Not sure why hrefs and especially links are erased from memory? When I extract the value of the href attribute of each item in the hrefs iterable, and then stick all of the URLs in the links variable, shouldn't the links list be independent of the DOM and page changes?Not sure what to do at this point. Any ideas?

2018-07-05 00:08:39Z

I've read about the StaleElementReferenceException in the official documentation, but I still don't understand why my code is raising this exception? Does browser.get() instantiate a new spider?I've tried the following to no avail. I've also tried placing the links variable elsewhere in the script, in a different scope, also to no avail.Not sure why hrefs and especially links are erased from memory? When I extract the value of the href attribute of each item in the hrefs iterable, and then stick all of the URLs in the links variable, shouldn't the links list be independent of the DOM and page changes?Not sure what to do at this point. Any ideas?@Anthony, your second code block with links should work, it just looks like you have a copy/paste bug:should beAs documentaion says:A stale element reference exception is thrown in one of two cases, the first being more common than the second:In your case it is:It is because of browser.get(href.get_attribute('href')). When you are redirecting to the another page, your DOM will be completely reloaded and the hrefs does not reference to the elements from previous page. That's why you are getting an error. How to deal with this error? You can do like this:

Web Scraper won't go to correct page

ThaJokerKid

[Web Scraper won't go to correct page](https://stackoverflow.com/questions/51126973/web-scraper-wont-go-to-correct-page)

So i keep trying to go to http://ofcc.ohio.gov/Opportunities2#lt-126679-construction-bids to get the list of bids. However the data read is always from the home page which is: http://ofcc.ohio.gov/Opportunities2. I've tried this with Beautiful Soup and Scrapy and nothing works. Any auggestions? When I curl the first url it also loads the home page data.

Code:

2018-07-01 20:57:33Z

So i keep trying to go to http://ofcc.ohio.gov/Opportunities2#lt-126679-construction-bids to get the list of bids. However the data read is always from the home page which is: http://ofcc.ohio.gov/Opportunities2. I've tried this with Beautiful Soup and Scrapy and nothing works. Any auggestions? When I curl the first url it also loads the home page data.

Code:Just clicking on http://ofcc.ohio.gov/Opportunities2#lt-126679-construction-bids and I'm stuck waiting for the bids to load... is this a website issue?Are you getting any error messages? your question isn't very detailed

Scrappy empty csv/json

Lucas Maraal

[Scrappy empty csv/json](https://stackoverflow.com/questions/51088647/scrappy-empty-csv-json)

I am start to use scrapy. I write my code following the tutorial from documentation. When i run to output a json or a csv, the file outputed is empty. When I test my selector in the shell i got the data. I will post my code:By testing, I discover that 'descrição' is broking the code, if i remove the json renders. When, in the shell, i put the selector, I got:One more question: this line breaks \r\n\t will render in my json? if yes, how I can get rid of them?thanks

2018-06-28 17:35:54Z

I am start to use scrapy. I write my code following the tutorial from documentation. When i run to output a json or a csv, the file outputed is empty. When I test my selector in the shell i got the data. I will post my code:By testing, I discover that 'descrição' is broking the code, if i remove the json renders. When, in the shell, i put the selector, I got:One more question: this line breaks \r\n\t will render in my json? if yes, how I can get rid of them?thanksFirst, livro.css('//*[@id="description"]/text()').extract_first(), should be livro.xpath('//*[@id="description"]/text()').extract_first(),. Using xpath inside a css selector will raise an exception that will kill the scraping process and that's probably the reason why you don't get anything on your output.Second, line breaks like \r\n\t will be keeped on your json file and they will be rendered  or not based on the software you use  to inspect the json file. If you want to remove then you can use the strip() function:Please, notice that if the xpath doesn't find any information it will return None and strip() will fail, in that case you will have to add an extra check to be sure the value is not None

Scrapy - LinkExtractor & setting DEPTH_LIMIT not working?

ocean800

[Scrapy - LinkExtractor & setting DEPTH_LIMIT not working?](https://stackoverflow.com/questions/51092507/scrapy-linkextractor-setting-depth-limit-not-working)

So I am passing in a start_url that is a page of news articles(ex. cnn.com). But, I just want to extract the news article itself, I don't want to follow any links on article page. To do that, I'm using a CrawlSpider with the following rule: I've enabled the scrapy.spidermiddlewares.depth.DepthMiddleware and set DEPTH_LIMIT = 1.However, I'm still getting links crawled from the individual article pages that happen to match the regexToMatchArticleUrls, as they are links to other parts of the same website (and I cannot make the regex more restrictive).But, why are these links getting crawled at all when the DEPTH_LIMIT=1? Is it because the DEPTH_LIMIT resets for each link extracted from LinkExtractor, ie. the article page urls? Is there a way either to make DEPTH_LIMIT work or extend the DepthMiddleware to not crawl links on the article page? Thanks! 

2018-06-28 23:44:15Z

So I am passing in a start_url that is a page of news articles(ex. cnn.com). But, I just want to extract the news article itself, I don't want to follow any links on article page. To do that, I'm using a CrawlSpider with the following rule: I've enabled the scrapy.spidermiddlewares.depth.DepthMiddleware and set DEPTH_LIMIT = 1.However, I'm still getting links crawled from the individual article pages that happen to match the regexToMatchArticleUrls, as they are links to other parts of the same website (and I cannot make the regex more restrictive).But, why are these links getting crawled at all when the DEPTH_LIMIT=1? Is it because the DEPTH_LIMIT resets for each link extracted from LinkExtractor, ie. the article page urls? Is there a way either to make DEPTH_LIMIT work or extend the DepthMiddleware to not crawl links on the article page? Thanks! For the DepthMiddleware to work correctly the meta attribute needs to be passed from one request to another, otherwise, depth will be set to 0 after each new request.Unfortunaly, by default, the CrawlSpider doesn't keep this meta attribute from one requests to the next.This can be solved by using spider middlewares (middlewares.py):Also, don't forget to include this middleware on your settings.py:

Scrapy Data Flow and Items and Item Loaders

oldboy

[Scrapy Data Flow and Items and Item Loaders](https://stackoverflow.com/questions/51092757/scrapy-data-flow-and-items-and-item-loaders)

I am looking at the Architecture Overview page in the Scrapy documentation, but I still have a few questions regarding data and or control flow.Scrapy Architecture

Default File Structure of Scrapy Projectsitem.pywhich, I'm assuming, becomesso that errors are thrown when trying to populate undeclared fields of Product instancesQuestion #1Where, when, and how does our crawler become aware of items.py if we have created class CrowdfundingItem in items.py?Is this done in...Question #2Once I have declared an item such as Product, how do I then store the data by creating instances of Product in a context similar to the one below?Lastly, say we forego naming the Product instances altogether, and instead simply create unnamed instances.If such instances are indeed stored somewhere, where are they stored? By creating instances this way, would it be impossible to access them?

2018-06-29 00:27:56Z

I am looking at the Architecture Overview page in the Scrapy documentation, but I still have a few questions regarding data and or control flow.Scrapy Architecture

Default File Structure of Scrapy Projectsitem.pywhich, I'm assuming, becomesso that errors are thrown when trying to populate undeclared fields of Product instancesQuestion #1Where, when, and how does our crawler become aware of items.py if we have created class CrowdfundingItem in items.py?Is this done in...Question #2Once I have declared an item such as Product, how do I then store the data by creating instances of Product in a context similar to the one below?Lastly, say we forego naming the Product instances altogether, and instead simply create unnamed instances.If such instances are indeed stored somewhere, where are they stored? By creating instances this way, would it be impossible to access them?Using an Item is optional; they're just a convenient way to declare your data model and apply validation. You can also use a plain dict instead.If you do choose to use Item, you will need to import it for use in the spider. It's not discovered automatically. In your case:As a spider runs the parse method on each page, you can load the extracted data into your Item or dict. Once it's loaded, yield it, which passes it back to the scrapy engine for processing downstream, in pipelines or exporters. This is how scrapy enables "storage" of the data you scrape.For example:

how to use a variable which has tag info in xpath to extract

ndewan

[how to use a variable which has tag info in xpath to extract](https://stackoverflow.com/questions/51076102/how-to-use-a-variable-which-has-tag-info-in-xpath-to-extract)

I am writing a scrapy code, which takes the url, the tags where my data to be scraped is stored from a csv file. That tag I am assigning to a temporary variable like z(having h1), av(having title), an(having td) etc. When I am using that variable in xpath , it does not extract anything out of that tag. Can anyone help me

?

2018-06-28 06:31:31Z

I am writing a scrapy code, which takes the url, the tags where my data to be scraped is stored from a csv file. That tag I am assigning to a temporary variable like z(having h1), av(having title), an(having td) etc. When I am using that variable in xpath , it does not extract anything out of that tag. Can anyone help me

?Since you're using variables, you should denote them with $ and supply their value as a keyword argument per the docs.Alternatively, if you prefer, use string formatting.

Error: pyMySQL is not working in spiders of scrapy

Ishan Verma

[Error: pyMySQL is not working in spiders of scrapy](https://stackoverflow.com/questions/50998545/error-pymysql-is-not-working-in-spiders-of-scrapy)

when I am importing pyMysql library in the scrapy project in python it is giving an error that no module found. I want to ask how can I import pyMysql library in python file of scrapy project. And when I am importing pyMySQL in simple python it is working properly.in the spider , which was generated by command "genspider spider_name (url)" i used this code which is giving error .

2018-06-23 06:23:32Z

when I am importing pyMysql library in the scrapy project in python it is giving an error that no module found. I want to ask how can I import pyMysql library in python file of scrapy project. And when I am importing pyMySQL in simple python it is working properly.in the spider , which was generated by command "genspider spider_name (url)" i used this code which is giving error .Check that the PyMySQL module is installed:This command should returns nothing if everything is ok.If it returns a ModuleNotFoundError, so install the module by using pip:

Scrapy follow previous links

Elgin Cahangirov

[Scrapy follow previous links](https://stackoverflow.com/questions/50996404/scrapy-follow-previous-links)

I'm trying to follow previous year links using scrapy starting from url 'https://umanity.jp/en/racedata/race_6.php'. In this url, current year is 2018 and there is previous button. When you click that button it goes to 2017, 2016 ... until 2000. But scrapy spider I wrote stops at year 2017. My code:Can't figure out why it stops at 2017 and doesn't go to previous years. What's the problem?

2018-06-22 22:45:42Z

I'm trying to follow previous year links using scrapy starting from url 'https://umanity.jp/en/racedata/race_6.php'. In this url, current year is 2018 and there is previous button. When you click that button it goes to 2017, 2016 ... until 2000. But scrapy spider I wrote stops at year 2017. My code:Can't figure out why it stops at 2017 and doesn't go to previous years. What's the problem?The problem is that the parse_years function doesn't look for any further links. Switch:

yield scrapy.Request(follow_link, self.parse_years) 

to

yield scrapy.Request(follow_link, self.parse) and all the years are found because the parse function continues to find links.If you do want two separate functions (perhaps, parse_years to do something with the data and parse to find the next link) it is doable. parse_years would just need this:You need to send request to self.parse; not self.parse_years to achieve the results. I tried to kick out your hardcoded index from xpaths to make it less prone to break. Try the below approach:However, keeping the second method alive:

How to call scrapy spider inside django view using spider.CrawlerRunner() or CrawlerProcess()

Chandni Bhatia

[How to call scrapy spider inside django view using spider.CrawlerRunner() or CrawlerProcess()](https://stackoverflow.com/questions/50966001/how-to-call-scrapy-spider-inside-django-view-using-spider-crawlerrunner-or-cra)



After going to all the links in Stack Overflow as well as other git etc. too, I am unable to solve my error. Basically, I want to scrap instagram by giving specific hashtag i.e. User will give a hashtag from Django UI and Django function will send that hashtag to scrapy spider for crawling usingHope for the help !Following is my code:django/models.pydjango/views.pyscrapy/settings.pyscrapy/myspiders.pyscrapy/piplines.pyscrapy/items.pyMy ErrorIn django/admin pageError after entering hashtag 

2018-06-21 10:15:05Z



After going to all the links in Stack Overflow as well as other git etc. too, I am unable to solve my error. Basically, I want to scrap instagram by giving specific hashtag i.e. User will give a hashtag from Django UI and Django function will send that hashtag to scrapy spider for crawling usingHope for the help !Following is my code:django/models.pydjango/views.pyscrapy/settings.pyscrapy/myspiders.pyscrapy/piplines.pyscrapy/items.pyMy ErrorIn django/admin pageError after entering hashtag 

Scrapy iterator stops immediately after Request finished

meme creep

[Scrapy iterator stops immediately after Request finished](https://stackoverflow.com/questions/50845189/scrapy-iterator-stops-immediately-after-request-finished)

This is my code that scans a user and outputs their SteamID and the value of their inventory:An expected output would be:However, the current output is:Despite the multithreading(the linkgen generator downloading the request while the parse function is activating it again), the function should still work(?)

2018-06-13 19:50:32Z

This is my code that scans a user and outputs their SteamID and the value of their inventory:An expected output would be:However, the current output is:Despite the multithreading(the linkgen generator downloading the request while the parse function is activating it again), the function should still work(?)I think you shouldn't just call lgen.next() but you should yield it like this yield lgen.next() because lgen is  just a generator and lgen.next() will just retrieve a scrapy Request, in order scrapy downloads it you have to yield this Request.

Splash script to retrieve site returns bad body

William Flanagan

[Splash script to retrieve site returns bad body](https://stackoverflow.com/questions/50847258/splash-script-to-retrieve-site-returns-bad-body)

I am trying to retrieve a number of different URLs using Splash. I am posting to the /execute endpoint a script that looks like this:The HAR must be returned to the the real headers received between the Splash server and the end destination.  So, the problem is that the HTML I receive is likely gzipped compressed. As I understand from Splash, when I return the hash, it automatically JSON encodes it... which JSON encodes the binary gzip, which essentially turns it all into garbage.  Sometimes the sites will return HTML. Sometimes, they will return a Gzip or Defalte or BR.  Why am I not just using "identity"? Many sites we've found do not respond well to identity, either not responding at all, or returning a Forbidden. Changing the encoding both reduces the uniqueness of our browser's footprint, and the number of random problems. Except that we can't read the page. It seems like we need to detect, and handle the binary if it's there, through some sort of treat.as_binary(x), or something.. but the documentation is getting REALLY light on how to do this.This has been stumping me for weeks.  Help! Any thoughts? P.s. Link to what the html looks like that's returned via a Splash Jupyter notebook. 

Link

2018-06-13 22:37:07Z

I am trying to retrieve a number of different URLs using Splash. I am posting to the /execute endpoint a script that looks like this:The HAR must be returned to the the real headers received between the Splash server and the end destination.  So, the problem is that the HTML I receive is likely gzipped compressed. As I understand from Splash, when I return the hash, it automatically JSON encodes it... which JSON encodes the binary gzip, which essentially turns it all into garbage.  Sometimes the sites will return HTML. Sometimes, they will return a Gzip or Defalte or BR.  Why am I not just using "identity"? Many sites we've found do not respond well to identity, either not responding at all, or returning a Forbidden. Changing the encoding both reduces the uniqueness of our browser's footprint, and the number of random problems. Except that we can't read the page. It seems like we need to detect, and handle the binary if it's there, through some sort of treat.as_binary(x), or something.. but the documentation is getting REALLY light on how to do this.This has been stumping me for weeks.  Help! Any thoughts? P.s. Link to what the html looks like that's returned via a Splash Jupyter notebook. 

Link

How to Scrape JSON Data Using Scrapy

WhiteDillPickle

[How to Scrape JSON Data Using Scrapy](https://stackoverflow.com/questions/50805355/how-to-scrape-json-data-using-scrapy)

I'm using scrapy and I'm trying to test my selector using scrapy shell but nothing is working.  I'm trying to scrape the JSON data on this website. https://web.archive.org/web/20180604230058/https://api.simon.com/v1.2/tenant?mallId=231&key=40A6F8C3-3678-410D-86A5-BAEE2804C8F2&lw=true I've tried to scrape the data using the selectorHowever, this doesn't seem to be working.  Not sure what's wrong...Ideally, I just want to get all the "Name: XXX" elements from the JSON data.  So If you know how to select those specifically, that would be very helpful as well!Currently my code looks like this

2018-06-11 20:13:53Z

I'm using scrapy and I'm trying to test my selector using scrapy shell but nothing is working.  I'm trying to scrape the JSON data on this website. https://web.archive.org/web/20180604230058/https://api.simon.com/v1.2/tenant?mallId=231&key=40A6F8C3-3678-410D-86A5-BAEE2804C8F2&lw=true I've tried to scrape the data using the selectorHowever, this doesn't seem to be working.  Not sure what's wrong...Ideally, I just want to get all the "Name: XXX" elements from the JSON data.  So If you know how to select those specifically, that would be very helpful as well!Currently my code looks like thisSince the content is inside an iframe, it is a separate page, you have to navigate to the iframe first. Like a link, something like that:then define a  new parse_iframe method where you parse the iframes response.

Classify scraped results side by side into a row

fg42

[Classify scraped results side by side into a row](https://stackoverflow.com/questions/50707253/classify-scraped-results-side-by-side-into-a-row)

So I'm using python/scrapy to scrape data from a webpage. Basically the webpage is made of 15 blocks that contain various sort of information. My spider reiterates through every block to scrape some specific content. I'm happy with the content of the results, but not with how the data is presented. I want all the scraped information belonging in one block to be presented in a single row. You will see from the screenshot below that the results of the same block are not presented side by side, which is what I want.

2018-06-05 19:04:49Z

So I'm using python/scrapy to scrape data from a webpage. Basically the webpage is made of 15 blocks that contain various sort of information. My spider reiterates through every block to scrape some specific content. I'm happy with the content of the results, but not with how the data is presented. I want all the scraped information belonging in one block to be presented in a single row. You will see from the screenshot below that the results of the same block are not presented side by side, which is what I want.If there is the same number of results on each row you can do this:You can append extracted values into a list and then yield the same, like this

Scrapy crawls but doesn't compare to file and returns null

Schneejäger

[Scrapy crawls but doesn't compare to file and returns null](https://stackoverflow.com/questions/50602414/scrapy-crawls-but-doesnt-compare-to-file-and-returns-null)

This spider is made with the idea of comparing a list of keywords from a file with text from a page and extract strings / paragraphs. Given the next spider:My question is like in the title, the spider crawls, shows the code 200 for every link (if more are added) but it doesn't give any output in a csv file.Edit:If I introduce a keyword in the search and extract, it works thus extracting and writing in a file any paragraph that contains "a la" and " a la Anglaise".

2018-05-30 10:31:01Z

This spider is made with the idea of comparing a list of keywords from a file with text from a page and extract strings / paragraphs. Given the next spider:My question is like in the title, the spider crawls, shows the code 200 for every link (if more are added) but it doesn't give any output in a csv file.Edit:If I introduce a keyword in the search and extract, it works thus extracting and writing in a file any paragraph that contains "a la" and " a la Anglaise".

has_key for item in Scrapy

Aminah Nuraini

[has_key for item in Scrapy](https://stackoverflow.com/questions/50548180/has-key-for-item-in-scrapy)

I want to check if a field is set in a Scrapy item. But I use has_key on the item, I get this error:How can I check if the field is set without using has_key?

2018-05-27 00:52:41Z

I want to check if a field is set in a Scrapy item. But I use has_key on the item, I get this error:How can I check if the field is set without using has_key?Finally found it. It turns out, even though we can use Scrapy on python 2.x, it expects us to use python 3 pattern. I should use 'field' in item instead of item.has_key('field').

Scrapy long time to initialize

Emanuel Reynolds Agnelli

[Scrapy long time to initialize](https://stackoverflow.com/questions/50546029/scrapy-long-time-to-initialize)

Recently when i launch the command "scrapy crawl my_spider" it takes betweem 10 to 15 MIN to start the extensions, enable the middlewares, pipelines, etc. and then starts crawling. Before, this was not happening. Any idea on what could be the causes for this huge low speed? screenshot1screenshot2

2018-05-26 18:48:42Z

Recently when i launch the command "scrapy crawl my_spider" it takes betweem 10 to 15 MIN to start the extensions, enable the middlewares, pipelines, etc. and then starts crawling. Before, this was not happening. Any idea on what could be the causes for this huge low speed? screenshot1screenshot2

how to check if a class exists in selector or not Python scrapy

F. Shahid

[how to check if a class exists in selector or not Python scrapy](https://stackoverflow.com/questions/50502708/how-to-check-if-a-class-exists-in-selector-or-not-python-scrapy)

This question has already been asked but I couldn't find any approved answer. So kindly don't mark it duplicate.I'm new to scrapy and scraping an ecommerce website , I've to extract sizes of a product and mark them out of stock or not , my HTML structure is as followsI've extracted all the li tags usingBut I want to set out-of-stock flag for he list items which have not-available class. Is there any way to do it as simple as possible.Thanks in advance.,

2018-05-24 06:47:23Z

This question has already been asked but I couldn't find any approved answer. So kindly don't mark it duplicate.I'm new to scrapy and scraping an ecommerce website , I've to extract sizes of a product and mark them out of stock or not , my HTML structure is as followsI've extracted all the li tags usingBut I want to set out-of-stock flag for he list items which have not-available class. Is there any way to do it as simple as possible.Thanks in advance.,Try:It will return a result like this:Finally, you will be able to pair them and mark them out of stock based on available or not-available stringsOther approach would be to extract them separately:Which would, respectively, return:

Scrapy Prevent Visiting Same URL Across Schedule

Marcus Christiansen

[Scrapy Prevent Visiting Same URL Across Schedule](https://stackoverflow.com/questions/50514455/scrapy-prevent-visiting-same-url-across-schedule)

I am planning on deploying a Scrapy spider to ScrapingHub and using the schedule feature to run the spider on a daily basis. I know that, by default, Scrapy does not visit the same URLs. However, I was wondering if this duplicate URL avoidance is persistent across scheduled starts on ScrapingHub? And whether or not I can set it so that Scrapy does not visit the same URLs across its scheduled starts. 

2018-05-24 16:50:04Z

I am planning on deploying a Scrapy spider to ScrapingHub and using the schedule feature to run the spider on a daily basis. I know that, by default, Scrapy does not visit the same URLs. However, I was wondering if this duplicate URL avoidance is persistent across scheduled starts on ScrapingHub? And whether or not I can set it so that Scrapy does not visit the same URLs across its scheduled starts. DeltaFetch is a Scrapy plugin that stores fingerprints of visited URLs across different Spider runs. You can use this plugin for incremental (delta) crawls. Its main purpose is to avoid requesting pages that have been already scraped before, even if it happened in a previous execution. It will only make requests to pages from where no items were extracted before, to URLs from the spiders' start_urls attribute or requests generated in the spiders' start_requests method.See: https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016/Plugin repository: https://github.com/scrapy-plugins/scrapy-deltafetchIn Scrapinghub's dashboard, you can activate it on the Addons Setup page, inside a Scrapy Cloud project. Though, you'll also need to activate/enable DotScrapy Persistence addon for it to work.

Click on a button using Selenium and scrapy spider

TheZelucho

[Click on a button using Selenium and scrapy spider](https://stackoverflow.com/questions/50469764/click-on-a-button-using-selenium-and-scrapy-spider)

I just started and I've been on this for a week or two. Just using the internet to help but now I reached the point where I cant understand or my problem cannot be found anywhere else. In case you didnt understand my program I want to scrape data then click on a button then scrape data until I scrape an already collected data. then go to the next page which is in the list.

I reached the point where I scrape the first 8 data but I cant find a way to click on the "see more!" button. I know I should use Selenium and the button's Xpath. Anyway here is my code :

2018-05-22 14:06:04Z

I just started and I've been on this for a week or two. Just using the internet to help but now I reached the point where I cant understand or my problem cannot be found anywhere else. In case you didnt understand my program I want to scrape data then click on a button then scrape data until I scrape an already collected data. then go to the next page which is in the list.

I reached the point where I scrape the first 8 data but I cant find a way to click on the "see more!" button. I know I should use Selenium and the button's Xpath. Anyway here is my code :Be shure web driver used in scrapy loads and interprets JS (idk... it can be a solution)

Prevent Scrapy Spider from starting if it's already running

MoreScratch

[Prevent Scrapy Spider from starting if it's already running](https://stackoverflow.com/questions/50437162/prevent-scrapy-spider-from-starting-if-its-already-running)

I am using scrapyd to schedule spiders and sometimes, depending on the volume of data, a spider will be initiated when an instance is already running. I want to prevent this. I have tried writing a lock file and checking it but I am running into issues because every spider in my project is implementing the lock. Here is what I have:I understand that the issue is because I am creating the lock in the instance of the MySpider class but I don't know where else to put it. I was thinking of a middlewares component but not sure that would make sense. Ideas?

2018-05-20 16:44:39Z

I am using scrapyd to schedule spiders and sometimes, depending on the volume of data, a spider will be initiated when an instance is already running. I want to prevent this. I have tried writing a lock file and checking it but I am running into issues because every spider in my project is implementing the lock. Here is what I have:I understand that the issue is because I am creating the lock in the instance of the MySpider class but I don't know where else to put it. I was thinking of a middlewares component but not sure that would make sense. Ideas?Use listjobs.json api.The response is something like thisHere you can check if the spider is already running in the r.json()["running"] list.

Source: http://scrapyd.readthedocs.io/en/stable/api.html#listjobs-json

Collect data into a CSV file using Scrapy

pali112

[Collect data into a CSV file using Scrapy](https://stackoverflow.com/questions/50433349/collect-data-into-a-csv-file-using-scrapy)

I'm learning how to use ScrapyI run this spider with command: scrapy crawl test_spider -o test.csvThis is working fine for //div[@class="product-name", but I don't know how to add another CSS/XPath class in the same spider fileI'm trying this but it does't workPlease help me to do this.

2018-05-20 09:25:15Z

I'm learning how to use ScrapyI run this spider with command: scrapy crawl test_spider -o test.csvThis is working fine for //div[@class="product-name", but I don't know how to add another CSS/XPath class in the same spider fileI'm trying this but it does't workPlease help me to do this.Just use two for loops:

Scraping Hub Periodic Script / IOError No such file or directory

nicolasdavid

[Scraping Hub Periodic Script / IOError No such file or directory](https://stackoverflow.com/questions/50437741/scraping-hub-periodic-script-ioerror-no-such-file-or-directory)

I am trying to run a periodic script and connect it with a json file within my project. I tried this (https://support.scrapinghub.com/support/solutions/articles/22000200416-deploying-non-code-files) but this is not working for me, structure imported from scraping hub looks very different. Script is working well until i need to call this file.The error which I got is :with this in setup.py :Thanks a lot for your help.

2018-05-20 17:47:40Z

I am trying to run a periodic script and connect it with a json file within my project. I tried this (https://support.scrapinghub.com/support/solutions/articles/22000200416-deploying-non-code-files) but this is not working for me, structure imported from scraping hub looks very different. Script is working well until i need to call this file.The error which I got is :with this in setup.py :Thanks a lot for your help.

scrapy-splash doesn't display the right links if i don't put a print

Ludoliv

[scrapy-splash doesn't display the right links if i don't put a print](https://stackoverflow.com/questions/50394797/scrapy-splash-doesnt-display-the-right-links-if-i-dont-put-a-print)

As you already saw in the title, if I don't put a print in my code somewhere I don't get all the links I should get, here is my code:Parsing_spider.pyThis is the part that gives me troubleif I don't put the print above the for loop here is my output:But here is the problem I should get more than one redirection but I don't get all the links from the page because they are generated with javascript so that's why I used scrapy-splash.But the funny part is when I let the print here is the output I get all the links but also this error:Thanks in advanceEdit :I think I found out why there is an ascii error. It could be because there is an accent on a letter in the url but I still don't know why the print make all the links appear and I don't know how to avoid this ascii error.

2018-05-17 15:07:34Z

As you already saw in the title, if I don't put a print in my code somewhere I don't get all the links I should get, here is my code:Parsing_spider.pyThis is the part that gives me troubleif I don't put the print above the for loop here is my output:But here is the problem I should get more than one redirection but I don't get all the links from the page because they are generated with javascript so that's why I used scrapy-splash.But the funny part is when I let the print here is the output I get all the links but also this error:Thanks in advanceEdit :I think I found out why there is an ascii error. It could be because there is an accent on a letter in the url but I still don't know why the print make all the links appear and I don't know how to avoid this ascii error.

How to build a stand alone Scrapy Spider?

webbeing

[How to build a stand alone Scrapy Spider?](https://stackoverflow.com/questions/50360392/how-to-build-a-stand-alone-scrapy-spider)

I'm sorry for repost, the title in my earlier post was confusing.  In the spider example (code below), how can I use "pyinstaller" (or some other installer) to build an executable (such as myspidy.exe) so the end user doesn't need to install scrapy and python in a windows environment? With Python and Scrapy installed, the spider is run by executing the command "scrapy crawl quotes".  The end user would run download and run “myspidy.exe” in a Windows pc that doesn’t have Python and Scrapy preinstalled.  Thanks so much!Thank you EVHZ.  I made change to the code as you suggested and got the below errors during run time.

2018-05-15 23:16:04Z

I'm sorry for repost, the title in my earlier post was confusing.  In the spider example (code below), how can I use "pyinstaller" (or some other installer) to build an executable (such as myspidy.exe) so the end user doesn't need to install scrapy and python in a windows environment? With Python and Scrapy installed, the spider is run by executing the command "scrapy crawl quotes".  The end user would run download and run “myspidy.exe” in a Windows pc that doesn’t have Python and Scrapy preinstalled.  Thanks so much!Thank you EVHZ.  I made change to the code as you suggested and got the below errors during run time.In order to save everything in a python file, executable just by:You can use the code you have, and add few things:Save that as script.py. Then, using pyinstaller: Will generate the bundle in a subdirectory called dist.

How to force Scrapy to show all items instead of just the last one?

Mrowkacala

[How to force Scrapy to show all items instead of just the last one?](https://stackoverflow.com/questions/50318606/how-to-force-scrapy-to-show-all-items-instead-of-just-the-last-one)

Having the following spider: I want to extract number of photos taken by the user and then export it to csv. However in my .csv I only have the last item in the table on this page (see screenshot bellow).What I want is obviously to have a member name and number of photos taken for all of the users on a page. What am I doing wrong? How to fix this?EDIT:

Possibly this is essential as well but my items.py file looks like this:FOLLOW UP QUESTION:I have introduced some improvements into my code which is now:However this created a mess in final .csv which now looks like this:Is there a simple way to fix this?Sample desired output in .csv bellow:EDIT2:My spider now:Still does not yield proper result. I have empty .csv only. Updated settings.py

2018-05-13 17:09:01Z

Having the following spider: I want to extract number of photos taken by the user and then export it to csv. However in my .csv I only have the last item in the table on this page (see screenshot bellow).What I want is obviously to have a member name and number of photos taken for all of the users on a page. What am I doing wrong? How to fix this?EDIT:

Possibly this is essential as well but my items.py file looks like this:FOLLOW UP QUESTION:I have introduced some improvements into my code which is now:However this created a mess in final .csv which now looks like this:Is there a simple way to fix this?Sample desired output in .csv bellow:EDIT2:My spider now:Still does not yield proper result. I have empty .csv only. Updated settings.pyUPDATEYou need to have this line in your settings.py (site blocks default Scrapy user-agent):and next this will work:

How to scrape multiple pages with scrapy?

Roberta Gimenez

[How to scrape multiple pages with scrapy?](https://stackoverflow.com/questions/50253623/how-to-scrape-multiple-pages-with-scrapy)

I'm trying to scrape a table with multiple pages. With the following code I print the first page data:I have written the next code to download all the pages. It is based on other posts that I have read:When I try to print all the pages I don't obtain anything. Can anyone help me to know what is the mistake?

2018-05-09 12:36:49Z

I'm trying to scrape a table with multiple pages. With the following code I print the first page data:I have written the next code to download all the pages. It is based on other posts that I have read:When I try to print all the pages I don't obtain anything. Can anyone help me to know what is the mistake?Scrapy needs parse callback first. Scrapy docor just rewrite start_request method with other callback: Here is a code to crawl all pages:

how to scrape instagram querys using scrapy?

Ron

[how to scrape instagram querys using scrapy?](https://stackoverflow.com/questions/50194819/how-to-scrape-instagram-querys-using-scrapy)

I'm trying for a while now to scrape the plain text of instagram's posts query ("https://www.instagram.com/graphql/query/query_id=17888483320059182&id=USER_ID&first=50") and get only 403 and 301 responses.

anyone knows how can i bypass these responses or scrape the data?

2018-05-05 23:10:37Z

I'm trying for a while now to scrape the plain text of instagram's posts query ("https://www.instagram.com/graphql/query/query_id=17888483320059182&id=USER_ID&first=50") and get only 403 and 301 responses.

anyone knows how can i bypass these responses or scrape the data?Instagram blocks requests with the default Scrapy User-Agent. You can change the User-Agent of Scrapy so it will use one that looks like a real browser. But you will still have problems scraping Instagram, as commented in this discussion: requests limits etc. I remind you that scraping Instagram is against their TOS, so they will eventually block your account and/or IP if the scraping is detected. It will be quite hard to get it working using Scrapy because of the way it works. If you really want to scrape data from Instaram, I would recommend you using onegram, which is a Instagram Python API-like bot powered by requests.

How to run and save scrapy state from a python script

Amit Basuri

[How to run and save scrapy state from a python script](https://stackoverflow.com/questions/50148061/how-to-run-and-save-scrapy-state-from-a-python-script)

In scrapy projects, we can get persistence support by defining a job directory through the JOBDIR setting for eg. But how to do the same when running spiders using scrapy.crawler.CrawlerProcess from a python script as answered in  How to run Scrapy from within a Python script?

2018-05-03 06:10:29Z

In scrapy projects, we can get persistence support by defining a job directory through the JOBDIR setting for eg. But how to do the same when running spiders using scrapy.crawler.CrawlerProcess from a python script as answered in  How to run Scrapy from within a Python script?As your reference question points out you can pass settings to CrawlerProcess instance.So all you need to do is pass JOBDIR setting:

Scrapy xpath syntax error when selecting more classes

dorinand

[Scrapy xpath syntax error when selecting more classes](https://stackoverflow.com/questions/50113039/scrapy-xpath-syntax-error-when-selecting-more-classes)

I would like to select all divs with classes consist of cl-list-header-title, sc-grid-col-s-12, sc-grid-col-l-7 and sc-grid-col-xl-7. When I try to do it like in scrapy selectors examples, I receive SyntaxError: invalid syntax and I have no idea why. I think I do not missing any parenthesis or quotation mark. Could anybody explain where is the problem?

2018-05-01 07:47:28Z

I would like to select all divs with classes consist of cl-list-header-title, sc-grid-col-s-12, sc-grid-col-l-7 and sc-grid-col-xl-7. When I try to do it like in scrapy selectors examples, I receive SyntaxError: invalid syntax and I have no idea why. I think I do not missing any parenthesis or quotation mark. Could anybody explain where is the problem?This is due to inconsistent quotes usage. Try to replacewith 

When scraping websites I get the error “Took longer than 180.0 seconds”. Why?

Muhammad Danial

[When scraping websites I get the error “Took longer than 180.0 seconds”. Why?](https://stackoverflow.com/questions/50118568/when-scraping-websites-i-get-the-error-took-longer-than-180-0-seconds-why)

While I run the script, the scrapy is continuously throwing this message:What are the causes for this problem and any specific solution for this?Here is the screenshot of this message.



2018-05-01 14:43:55Z

While I run the script, the scrapy is continuously throwing this message:What are the causes for this problem and any specific solution for this?Here is the screenshot of this message.

You are hitting the scrapy timeout when downloading.See  DOWNLOAD_TIMEOUTIt defaults to 360 seconds = 6 minutes. Adjust your setting to make it longer.According to the documentation your Settings You did not share your code, so its difficult to tell, but either you modify the settings for your spider or provide them as commandline option.Alternativly you can set them per-spider like so:For more information read the settings-manual

slow crawl, 6 units, 900 concurrent requests, 10k websites (scrapy cloud)

roma98

[slow crawl, 6 units, 900 concurrent requests, 10k websites (scrapy cloud)](https://stackoverflow.com/questions/50134043/slow-crawl-6-units-900-concurrent-requests-10k-websites-scrapy-cloud)

I've been running a plain 'CrawlSpider', that I'm using to gather metadata, on a list of about 10k different domains. The goal is to crawl each website entirely.Since it's a 'broad crawl', I've given it 6 units to run, and have loaded up CONCURRENT_REQUESTS to 900, while keeping AUTOTHROTTLE_TARGET_CONCURRENCY to 1.It will run pretty fast the first hour, and then will slowly speed down, after about 3h the speed is at 50 items per minute, which I think is not a lot when we have 900 concurent requests.I could speed it up to about 250 items/minutes average by increasing AUTOTHROTTLE_TARGET_CONCURRENCY to 50, but some websites would give me a lot of timeout exceptions.So my question is, Am I doing something wrong? Even when running 'top' on the spider, I can see it running between 60%-90% at the beginning, it's now at 5% max.Here is my settings.py fileHere is my spider

2018-05-02 11:52:13Z

I've been running a plain 'CrawlSpider', that I'm using to gather metadata, on a list of about 10k different domains. The goal is to crawl each website entirely.Since it's a 'broad crawl', I've given it 6 units to run, and have loaded up CONCURRENT_REQUESTS to 900, while keeping AUTOTHROTTLE_TARGET_CONCURRENCY to 1.It will run pretty fast the first hour, and then will slowly speed down, after about 3h the speed is at 50 items per minute, which I think is not a lot when we have 900 concurent requests.I could speed it up to about 250 items/minutes average by increasing AUTOTHROTTLE_TARGET_CONCURRENCY to 50, but some websites would give me a lot of timeout exceptions.So my question is, Am I doing something wrong? Even when running 'top' on the spider, I can see it running between 60%-90% at the beginning, it's now at 5% max.Here is my settings.py fileHere is my spiderI should have posted a response earlier.

so when you want to run the same spiders on thousands of websites, your best bet is to divide them into several 'buckets' and attribute each bucket to an instance of the spider (running spider_a, spider_b, spider_c...)

Set up a CONCURENT_PER_DOMAIN to 10, and you can ramp up CONCURRENT_REQUESTS to 1000 with 10 concurent spiders. This is going much much faster than my first solution.

Scrapy Crawler gets terminated at random pages

JAYESH BHATIA

[Scrapy Crawler gets terminated at random pages](https://stackoverflow.com/questions/50079894/scrapy-crawler-gets-terminated-at-random-pages)

I'm new to Scrapy. I'm crawling the r/india subreddit using a recursive parser to store the title, upvotes and URLs of each thread. It works all fine but the Scraper ends unexpectedly with a weird error that shows:And the error comes at random pages each time the spider is run, making it impossible for me to detect what's causing the problem. Here's my redditscraper.py file which contains the code(I've also used Pipeline and Items.py but that doesn't contain any problems I feel)

2018-04-28 18:41:38Z

I'm new to Scrapy. I'm crawling the r/india subreddit using a recursive parser to store the title, upvotes and URLs of each thread. It works all fine but the Scraper ends unexpectedly with a weird error that shows:And the error comes at random pages each time the spider is run, making it impossible for me to detect what's causing the problem. Here's my redditscraper.py file which contains the code(I've also used Pipeline and Items.py but that doesn't contain any problems I feel)As your exception saysthats means you try to scrap an invalid url - missing http:// or https:// in the url.I guess the problem is not in the start_urls because otherwise the parse function won't be called. The problem is in the parse function.When yield Request is called you need to check if next_page contains schema. it seems like the urls you parse are relative links, so you have two options to continue scrap those links without facing this exception:

scrapy_splash scraped 0 items

isend

[scrapy_splash scraped 0 items](https://stackoverflow.com/questions/50028213/scrapy-splash-scraped-0-items)

I'm new to scrapy, but dealing with a particularly complicated website that involves both a form and java code. I'm trying to scrape news release data from a UN website, but I don't think the website is being rendered properly, because nothing's getting scraped. Below is my scrapy code and the output.Scrapy codeOutputAny help would be very much appreciated!

2018-04-25 17:32:09Z

I'm new to scrapy, but dealing with a particularly complicated website that involves both a form and java code. I'm trying to scrape news release data from a UN website, but I don't think the website is being rendered properly, because nothing's getting scraped. Below is my scrapy code and the output.Scrapy codeOutputAny help would be very much appreciated!Your log indicates that everything is fine and scrapy is returning one item:



2018-04-25 13:25:04 [scrapy.core.scraper] DEBUG: Scraped from <200 http://www.ohchr.org/EN/NewsEvents/Pages/NewsSearch.aspx>

{'title': [], 'date': [], 'type': [], 'country': [], 'mandate': [], 'subject': []}

However, your xpaths don't seem to find anything on the page.To debug this you can use inspect_response debug function scrapy has:Then run the spider and python will open up shell once it reaches parse_table method.

There you can inspect response and see what page you are receiving with functions such as:view(response) - will open up page in your browser.

response.xpath - you can try out your xpaths.

etc.I think it's a problem related to scrapy_splash, I can get the element by the following way:

check the POST request in chrome devtools network, copy the body and use it in FormRequest. You may want to change fromdate and todate .

Set scrapy built-in loggers at different level than user code logger

scrapy_user

[Set scrapy built-in loggers at different level than user code logger](https://stackoverflow.com/questions/50029461/set-scrapy-built-in-loggers-at-different-level-than-user-code-logger)

Scrapy built-in loggers:are very verbose.I was trying to set a different log level, DEBUG, than user spider log level, INFO. This way I can reduce the 'noise'.This helper function works, some times:I call it from UserSpider init: This approach works some time, others not.What will be the correct solution?

2018-04-25 18:52:26Z

Scrapy built-in loggers:are very verbose.I was trying to set a different log level, DEBUG, than user spider log level, INFO. This way I can reduce the 'noise'.This helper function works, some times:I call it from UserSpider init: This approach works some time, others not.What will be the correct solution?You can just set LOG_LEVEL appropriately in your settings.py, read more here: https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-LOG_LEVELIf project wide settings are not focused enough, you can set them per-spider by using custom_settings:Source:

https://doc.scrapy.org/en/latest/topics/settings.html#settings-per-spiderSettings different log levels per Log handlers is not very realiable.At the end of the day the better approach will be launch scrapy cli tool from another script and filter logs output with a parser has needed.I stumbled upon the same issue. I tried various method but it looks like since Scrapy uses logging module, you have to set it at global level which result in Scrapy to print all the debug information. I've found more reliable solution to use bool flag with print statement fro DEBUG and use logger for INFO, ERROR and WARNING. 

Why selenium's click method is returning NoSuchElementException?

Muhammad Danial

[Why selenium's click method is returning NoSuchElementException?](https://stackoverflow.com/questions/50046232/why-seleniums-click-method-is-returning-nosuchelementexception)

During scraping a javascript base website(link), I've stuck with a click function which is not producing the desired result. I've pasted the code which should return the data under "Fits the following cars" drop down menu but unexpectedly it's just printing the except statement's message. I'm able to scrape all the other data from the same code. Should I add few more lines to get the data hidden under that drop down, if yes then what lines to add. 

2018-04-26 14:58:45Z

During scraping a javascript base website(link), I've stuck with a click function which is not producing the desired result. I've pasted the code which should return the data under "Fits the following cars" drop down menu but unexpectedly it's just printing the except statement's message. I'm able to scrape all the other data from the same code. Should I add few more lines to get the data hidden under that drop down, if yes then what lines to add. The reason why you keep receiving NoSuchElementException is that the dropdown is being inserted dynamically into DOM tree so you need to wait for some time until it is available. The best way to implement wait timeout is to use wait API provided by python-selenium.Here is a working code where time.sleep being used as a temporary solution.

Splash not rendering page completely

marcins

[Splash not rendering page completely](https://stackoverflow.com/questions/49986264/splash-not-rendering-page-completely)

I have been learning using scrapy + splash for scraping web pages with js. I have a problem with one site - https://aukro.cz/mobilni-telefony - it is not rendering completely. Instead I have the whole page with empty list of products.scraped pageI have already tried modifying wait time and scrolling page with no effect. Lua script belowWhat else should I do? Thanks for help in advance.

2018-04-23 16:55:57Z

I have been learning using scrapy + splash for scraping web pages with js. I have a problem with one site - https://aukro.cz/mobilni-telefony - it is not rendering completely. Instead I have the whole page with empty list of products.scraped pageI have already tried modifying wait time and scrolling page with no effect. Lua script belowWhat else should I do? Thanks for help in advance.

Avoid duplicate values on scrapy

Luis Ramon Ramirez Rodriguez

[Avoid duplicate values on scrapy](https://stackoverflow.com/questions/49958388/avoid-duplicate-values-on-scrapy)

I'm scrapying MOOCs data from course talk pages, and I'm having issues to clean some of the fields, E.G. The university name.From the above link I want to get: Massachusetts Institute of TechnologyThis is the xpath I'm using for that field:The problem here is that I'm getting duplicated values and empty strings from it:

2018-04-21 17:28:31Z

I'm scrapying MOOCs data from course talk pages, and I'm having issues to clean some of the fields, E.G. The university name.From the above link I want to get: Massachusetts Institute of TechnologyThis is the xpath I'm using for that field:The problem here is that I'm getting duplicated values and empty strings from it:You can skip inner span by using not (to exclude inner child span node) function and normalize-space function to skip white-space only text strings and clean text:In result you should get two equal strings with university name only:And you can use python set to get unique names only:If you need contents of first div only, you can get it by index 1 with just xpath:The reason lies in the fact that there are two divs with class name course-info__school__name.Therefore, to avoid duplicates, you could change the xpath so that it only select the first div element with class name of course-info__school__nameresponse.xpath('(//div[@class="course-info__school__name"])[1]//text()').extract()which will will give you the result of Hope it helps!You can try this way to get unique values always.

How can I get an element inside a class with scrapy using response.css

Rousblack

[How can I get an element inside a class with scrapy using response.css](https://stackoverflow.com/questions/49927053/how-can-i-get-an-element-inside-a-class-with-scrapy-using-response-css)

I'm trying to get value="3474636382675" from:<input class="lst" value="3474636382675" title="Zoeken" autocomplete="off" id="sbhost" maxlength="2048" name="q" type="text">I've triedThis one works but i'm getting everything back and i just need the value.

2018-04-19 17:38:17Z

I'm trying to get value="3474636382675" from:<input class="lst" value="3474636382675" title="Zoeken" autocomplete="off" id="sbhost" maxlength="2048" name="q" type="text">I've triedThis one works but i'm getting everything back and i just need the value.With CSS you select the attribute you want like this:You can reed more about the selectors in Scrapy’s documentationI use beautiful soup to parse html. Here's an example that grabs stock prices from yahoo finance.https://gist.github.com/Krewn/0e624d35c396df63262dd42d74f2beb6Not quite sure about css. But here is one from another SO answer. Alternatively try xpath:or if you need only one value:

css selector not giving the desired results

Madhur

[css selector not giving the desired results](https://stackoverflow.com/questions/49891177/css-selector-not-giving-the-desired-results)

I am at this page : https://en.wikipedia.org/wiki/2018_Commonwealth_Games_medal_tableIf I try following css selector in chrome console :I get O/P : a tag as expectedHowever when I try :I get O/P : <tr bgolor='#CCCCFF'> which is not what I am looking for. I want <a> tag in both the cases. And I can see order of tr[bgcolor],tr[style] is making all the difference. Can somebody where I am going wrong?

2018-04-18 04:26:39Z

I am at this page : https://en.wikipedia.org/wiki/2018_Commonwealth_Games_medal_tableIf I try following css selector in chrome console :I get O/P : a tag as expectedHowever when I try :I get O/P : <tr bgolor='#CCCCFF'> which is not what I am looking for. I want <a> tag in both the cases. And I can see order of tr[bgcolor],tr[style] is making all the difference. Can somebody where I am going wrong?I think you should write your code something like thisYou have to write " a" with every every query.Also can you please tell do you want to select all anchors in the table ? Because the query you are using as of now, is only selecting one of the anchor tags.To select all anchor tags use the following query Basically the query you usedcan be interpreted as 2 queries as followsThat is why you are getting <tr> tag in first and <a> tag in second

Scrapy spider cannot crawl a url but scrapy shell does it successfully

Arindam Ghosh

[Scrapy spider cannot crawl a url but scrapy shell does it successfully](https://stackoverflow.com/questions/49806884/scrapy-spider-cannot-crawl-a-url-but-scrapy-shell-does-it-successfully)

I am trying to scrape craiglist. When I try to fetch  https://tampa.craigslist.org/search/jjj?query=bookkeeper in the spider I am getting the following error: (extra newlines and white space added for readability)But, when I try to crawl it on scrapy shell, it is being crawled successfully. I don't know what I am doing wrong here. I have tried forcing TLSv1.2 but had no luck. I would really appreciate your help.

Thanks!

2018-04-12 22:41:40Z

I am trying to scrape craiglist. When I try to fetch  https://tampa.craigslist.org/search/jjj?query=bookkeeper in the spider I am getting the following error: (extra newlines and white space added for readability)But, when I try to crawl it on scrapy shell, it is being crawled successfully. I don't know what I am doing wrong here. I have tried forcing TLSv1.2 but had no luck. I would really appreciate your help.

Thanks!I've asked for an MCVE in the comments, which means you should provide a Minimal, Complete, and Verifiable example. 

To help you out, this is what it's all about:Now, this MCVE does everything you want to do in a nutshell:  This should be your starting point for debugging, removing all the unrelated boilerplate.Please test the above and verify if it's working? If it works, add more functionality in steps so you can figure out which part introduces the problem. If it doesn't work, don't add anything else until you can figure out why.UPDATE:Adding a delay between requests can be done in two ways: for example:  Documentation: https://doc.scrapy.org/en/latest/topics/settings.html#download-delay

Lua scripts select dropdown

Alex D

[Lua scripts select dropdown](https://stackoverflow.com/questions/49803253/lua-scripts-select-dropdown)

I have a page I am practicing some scraping on and having trouble with the Lua script to select from a drop down.I have been trying for hours now to figure it out. I need to select 8.4 from the elementid right_eye_156 Here is what I have right now which doesnt do anything but select the title of the page.here is the html for the dropdown from the url (https://eoptika.hu/termekek/kontaktlencse/acuvue-oasys-with-hydraclear-plus-6-db-1-2-heti-kontaktlencse.html)

2018-04-12 18:04:57Z

I have a page I am practicing some scraping on and having trouble with the Lua script to select from a drop down.I have been trying for hours now to figure it out. I need to select 8.4 from the elementid right_eye_156 Here is what I have right now which doesnt do anything but select the title of the page.here is the html for the dropdown from the url (https://eoptika.hu/termekek/kontaktlencse/acuvue-oasys-with-hydraclear-plus-6-db-1-2-heti-kontaktlencse.html)Find xpath of the option that contains text '8.4' Use splash:evaljs to find element using xpath 

How can I scrape links in all my webpages?

Pranav Barot

[How can I scrape links in all my webpages?](https://stackoverflow.com/questions/49802769/how-can-i-scrape-links-in-all-my-webpages)

I have this code so far that extracts text from the page URLs, using scrapy:How can I also extract data from the links on those pages and write them to that filename that I create? 

2018-04-12 17:34:13Z

I have this code so far that extracts text from the page URLs, using scrapy:How can I also extract data from the links on those pages and write them to that filename that I create? You could use CrawlSpider to extract each link and scrape them, your code could look like thisThough I recommend creating a different spider for each website, and use allow and deny parameters to choose which links you want to be extracted on each website.also it would be much better to use Scrapy Items

How to set IMAGES_STORE folder per Item in Scrapy 1.5

Evren Yurtesen

[How to set IMAGES_STORE folder per Item in Scrapy 1.5](https://stackoverflow.com/questions/49732290/how-to-set-images-store-folder-per-item-in-scrapy-1-5)

Scrapy 1.5 allows setting an IMAGES_STORE setting for storing all downloaded media as explained in documentationI would like to be able to specify a custom folder per Item based on some values in the Item. Not knowing much about internals of Scrapy, I am not sure exactly which methods to override to accomplish this.I thought about overriding from_settings(cls, settings) but there I do not have access to Item yet.Any ideas?

2018-04-09 11:51:49Z

Scrapy 1.5 allows setting an IMAGES_STORE setting for storing all downloaded media as explained in documentationI would like to be able to specify a custom folder per Item based on some values in the Item. Not knowing much about internals of Scrapy, I am not sure exactly which methods to override to accomplish this.I thought about overriding from_settings(cls, settings) but there I do not have access to Item yet.Any ideas?I solved the issue by overriding file_path method. So in IMAGES_STORE I have the base path then I control the variable part from file_path. Something like below. However I had a typo first and scrapy silently ignored it without printing any errors even in debug... I don't know why? So it is best to start with a simple string for testing.

Python xpath and math operators

Valter

[Python xpath and math operators](https://stackoverflow.com/questions/49735644/python-xpath-and-math-operators)

I am trying to calculate the tax of a numerical value extracted with xpath, to save it in a csv file, but I do not know what I am doing wrong or if I need to add something else.This the code:and this is the error that I get:Any help will be appreciate it.

2018-04-09 14:41:53Z

I am trying to calculate the tax of a numerical value extracted with xpath, to save it in a csv file, but I do not know what I am doing wrong or if I need to add something else.This the code:and this is the error that I get:Any help will be appreciate it.Lacking your data, this would be my suggestion:

Modules folder in Scrapinghub

Axel Eriksson

[Modules folder in Scrapinghub](https://stackoverflow.com/questions/49610533/modules-folder-in-scrapinghub)

I'm currently using Scrapinghub's Scrapy Cloud to host my 12 spiders (and 12 differnet projects).I'd like to have one folder with functions that are used by all 12 spiders but not sure what the best way to implement it without having 1 functions folder in each spider.I'm thinking about hosting all spiders under the same project, creating a private package in the cloud the spiders connect to or hosting ScrapyD myself so I can reference the modules.Has anyone stumbled upon this and what was your solution?

2018-04-02 11:35:42Z

I'm currently using Scrapinghub's Scrapy Cloud to host my 12 spiders (and 12 differnet projects).I'd like to have one folder with functions that are used by all 12 spiders but not sure what the best way to implement it without having 1 functions folder in each spider.I'm thinking about hosting all spiders under the same project, creating a private package in the cloud the spiders connect to or hosting ScrapyD myself so I can reference the modules.Has anyone stumbled upon this and what was your solution?

Steam age check circumvent by using Scrapy

Galiu

[Steam age check circumvent by using Scrapy](https://stackoverflow.com/questions/49612346/steam-age-check-circumvent-by-using-scrapy)

So I want to make a scraper for steam games with Scrapy. I didn't know why exactly some games weren't showing in the result csv/json. I figured that it was because of the age verification form.I really tried to see what was the problem... but everything seems like it should work. Can anyone help on this?I noticed that the callback would return the same agecheck link and the dupefilter would remove that link. So the game would never be scraped.Example games: Rise of the Tomb Raider, Doom.This is the code:

2018-04-02 13:34:33Z

So I want to make a scraper for steam games with Scrapy. I didn't know why exactly some games weren't showing in the result csv/json. I figured that it was because of the age verification form.I really tried to see what was the problem... but everything seems like it should work. Can anyone help on this?I noticed that the callback would return the same agecheck link and the dupefilter would remove that link. So the game would never be scraped.Example games: Rise of the Tomb Raider, Doom.This is the code:Steam stores the result of submitting this form in  lastagecheckage and birthtime, cookies, so you may need to enable cookies on your scrapy project or passing them as custom values during the creation of a Request or a FormRequest using the cookies attribute.You can read more about how to sent cookies on the scrapy documentation

Web Scraping all Urls from a website with Scrapy and Python

zeusking123

[Web Scraping all Urls from a website with Scrapy and Python](https://stackoverflow.com/questions/49560246/web-scraping-all-urls-from-a-website-with-scrapy-and-python)

I am writing a web scraper to fetch a group of links from a website and return the Title and Url of all the leafs sectioned by the leafs parent. I have two scrapers: one in python and one in Scrapy for Python. What is the purpose of callbacks in the Scrapy Request method? Should the information be in a multidimensional or single dimension list ( I believe multi-dimensional but it enhances complication)? Which of the below code is better? If the scraper code is better, how do I migrate the python code to the Scrapy code?From what I understand from callbacks is that it passes a function's arguments to another function; however, if the callback refers to itself, the data gets overwritten and therefore lost, and you're unable to go back to the root data. Is this correct? python:Scrapy:

2018-03-29 15:49:23Z

I am writing a web scraper to fetch a group of links from a website and return the Title and Url of all the leafs sectioned by the leafs parent. I have two scrapers: one in python and one in Scrapy for Python. What is the purpose of callbacks in the Scrapy Request method? Should the information be in a multidimensional or single dimension list ( I believe multi-dimensional but it enhances complication)? Which of the below code is better? If the scraper code is better, how do I migrate the python code to the Scrapy code?From what I understand from callbacks is that it passes a function's arguments to another function; however, if the callback refers to itself, the data gets overwritten and therefore lost, and you're unable to go back to the root data. Is this correct? python:Scrapy:

Scrapy Follow scraped links to get more Data

user3111115

[Scrapy Follow scraped links to get more Data](https://stackoverflow.com/questions/49571562/scrapy-follow-scraped-links-to-get-more-data)

I want to follow the links that I've scraped to get more details. For example, from here, which contains all the job titles.I wish to go to one of the links, for example, here to extract the job descriptions.Below is my working code for getting the Title Link and Date as well as getting them to insert into a CSV File.This is my attempt to go to the link but it hasn't been successful.Any idea how to do this?

Log of scraper

2018-03-30 09:09:08Z

I want to follow the links that I've scraped to get more details. For example, from here, which contains all the job titles.I wish to go to one of the links, for example, here to extract the job descriptions.Below is my working code for getting the Title Link and Date as well as getting them to insert into a CSV File.This is my attempt to go to the link but it hasn't been successful.Any idea how to do this?

Log of scraper

how do I Iterate through selector list while using Itemloaders in scrapy? details in description

mohammed_ayaz

[how do I Iterate through selector list while using Itemloaders in scrapy? details in description](https://stackoverflow.com/questions/49569875/how-do-i-iterate-through-selector-list-while-using-itemloaders-in-scrapy-detail)

I am trying to scrape a list of countries and their details that are members of UN. Here is my approach without using Item Loaders Here, I am getting a parent tag that contains the details of  all the UN members like name, date of joining, website, phone number and UN headquarters. Not all countries have a website, phone no and child details.

I am running a loop through the parent tag and extracting the details one by one and storing it in a variable then I am assigning tha variable to items. And this my progress. I get a whole list of countries in an item. I want a single country in the item. What should be my aproach in this case?

2018-03-30 06:59:05Z

I am trying to scrape a list of countries and their details that are members of UN. Here is my approach without using Item Loaders Here, I am getting a parent tag that contains the details of  all the UN members like name, date of joining, website, phone number and UN headquarters. Not all countries have a website, phone no and child details.

I am running a loop through the parent tag and extracting the details one by one and storing it in a variable then I am assigning tha variable to items. And this my progress. I get a whole list of countries in an item. I want a single country in the item. What should be my aproach in this case?After some Research, I found the solution. Instead of thisYou have will have to specify the selector parameter in the method. That means your ItemLoader will extract the Items from specified 'selector' instead of whole response (whole web page). It is like selecting a part of a page from the whole response (page) and then selecting your items from it and plus you are iterating through it. And the new code would like something like this The code is much is cleaner than very first code snippet in the question and gets the job done.

Check if item field exist in pymongo database using scrapy

JimmySmith

[Check if item field exist in pymongo database using scrapy](https://stackoverflow.com/questions/49575837/check-if-item-field-exist-in-pymongo-database-using-scrapy)

I'm trying to check if item ['email'] already exists in database, if it does not exist then insert into pymongo database. i do not want duplicate emails within the pymongo database. however i get thisthis is what i have so farPipelines.py

2018-03-30 13:59:22Z

I'm trying to check if item ['email'] already exists in database, if it does not exist then insert into pymongo database. i do not want duplicate emails within the pymongo database. however i get thisthis is what i have so farPipelines.pyYour error is when calling self.collection.find(dict(item['email'])).item is already a dictionary which contains the key email, so there's 

no need to wrap it with dict(). Using item['email'] will access the value of email just fine.You should then check if self.email and see if it contains the required result, and proceed with the rest of the function logic.EDIT The new error from your comment indicates the filter in the lookup query needs to be, among other things, a dictionary. Use {'email': item['email']} if you are looking for an email field in Mongo. dropDups = True will not save emails that has been already added to the databaseself.collection.ensure_index('email', unique=True, dropDups=True)finished code

Loop Exit: This operation would block forever (Kafka Producer Python at scrappingHub)

Jerrychayan

[Loop Exit: This operation would block forever (Kafka Producer Python at scrappingHub)](https://stackoverflow.com/questions/49376712/loop-exit-this-operation-would-block-forever-kafka-producer-python-at-scrappin)

I was trying to set up a server to server communication using kafka. 

My aim is to send data from Scrapping Hub to Python using kafka.I had Set up a Kafka server over public IP. This will work usually , But after a while (after sending 300 or 500 messages), 

sometimes the producer and consumer  fails with the error "Loop Exit: This operation would block forever" at producer end followed by "No Brokers Available" on consumer end.  Usually there will be 2 producers and 2 consumers each  sending to separate topics, there is only one Topic Partition and one Replication for each the topics. 

Currently i have given the below config.All other configurations remains the default.This is the error I am getting at Producer Side.,If I try to reconnect from consumer side hereafter i will get the following error:

2018-03-20 05:09:01Z

I was trying to set up a server to server communication using kafka. 

My aim is to send data from Scrapping Hub to Python using kafka.I had Set up a Kafka server over public IP. This will work usually , But after a while (after sending 300 or 500 messages), 

sometimes the producer and consumer  fails with the error "Loop Exit: This operation would block forever" at producer end followed by "No Brokers Available" on consumer end.  Usually there will be 2 producers and 2 consumers each  sending to separate topics, there is only one Topic Partition and one Replication for each the topics. 

Currently i have given the below config.All other configurations remains the default.This is the error I am getting at Producer Side.,If I try to reconnect from consumer side hereafter i will get the following error:

Items randomly never reach pipeline when iterating over a list

user3255061

[Items randomly never reach pipeline when iterating over a list](https://stackoverflow.com/questions/49274242/items-randomly-never-reach-pipeline-when-iterating-over-a-list)

I'm fairly new to scrapy. I'm crawling a site that offers a list of websites. I want to crawl certain pages of those websites.Example: siteX.com has the following list:All those pages have sub-sites following the same structure, I need two additional requests per site to get their data:I'm able to achieve that with a single entry with the following code:Foo is then being put into a database in the pipeline. My problem now is that when I iterate over the list on the source website, not all the items make it into the pipeline:It's totally random though - it's not always the same sites and I don't see any errors in the logs ("LOG_LEVEL": "DEBUG"). But crawling single sites always works. What could be the reason for this behaviour?

2018-03-14 09:41:08Z

I'm fairly new to scrapy. I'm crawling a site that offers a list of websites. I want to crawl certain pages of those websites.Example: siteX.com has the following list:All those pages have sub-sites following the same structure, I need two additional requests per site to get their data:I'm able to achieve that with a single entry with the following code:Foo is then being put into a database in the pipeline. My problem now is that when I iterate over the list on the source website, not all the items make it into the pipeline:It's totally random though - it's not always the same sites and I don't see any errors in the logs ("LOG_LEVEL": "DEBUG"). But crawling single sites always works. What could be the reason for this behaviour?

How to activate alias through python? [duplicate]

Om Prakash

[How to activate alias through python? [duplicate]](https://stackoverflow.com/questions/49208779/how-to-activate-alias-through-python)

I have python2( as default environment) and python3 installed on redhat server and set up different alias for them..bashrcI activate this alias by executing myconda on terminal. How can I activate it using python script as I need to run scrapy spider on python3?Update: I want to integrate scrapy spider with flask as API. When POST method hit the endpoint url, then I want to run the scrapy spider. Currently spider runs fine individually.I am trying to activate bash alias like this - This doesn't identify myconda as alias and uses default python2 environment instead of python3 to start spider and gives python2 syntax errors and /bin/sh: myconda: command not found. errorThanks in advance.

2018-03-10 11:56:22Z

I have python2( as default environment) and python3 installed on redhat server and set up different alias for them..bashrcI activate this alias by executing myconda on terminal. How can I activate it using python script as I need to run scrapy spider on python3?Update: I want to integrate scrapy spider with flask as API. When POST method hit the endpoint url, then I want to run the scrapy spider. Currently spider runs fine individually.I am trying to activate bash alias like this - This doesn't identify myconda as alias and uses default python2 environment instead of python3 to start spider and gives python2 syntax errors and /bin/sh: myconda: command not found. errorThanks in advance.If I am not misunderstanding your requirement, how about use following:You can choose your python still by environment or just use full path?

python scrapy deny rule

Omega

[python scrapy deny rule](https://stackoverflow.com/questions/49171263/python-scrapy-deny-rule)

i want to create a deny rule (dynamic)... that deny's anything in front of the domain.Example:And so on.I tried like this but it doesn't work as expected:basicaly deny anything thats in front of domain...except https://

2018-03-08 10:48:56Z

i want to create a deny rule (dynamic)... that deny's anything in front of the domain.Example:And so on.I tried like this but it doesn't work as expected:basicaly deny anything thats in front of domain...except https://In your case I think it should work as follow:For dynamic links you can use tldextract on the response.url inside parse function. For example:

Pipeline to remove None values

Casper

[Pipeline to remove None values](https://stackoverflow.com/questions/49106946/pipeline-to-remove-none-values)

My spider yields certain data but sometimes it doesn't find the data.

Instead of setting a condition such as below:I'd rather fix this in my pipeline by removing all items that have a None value. I've tried to do this by the following code:However, this results in an error:I think it has to do with the fact that a field has been yielded to the pipeline but not returned at the end but that's just a guess. Currently the pipeline has statements such as:And I'd like to prevent adding unnecessary extra statements to check if the value is None.

2018-03-05 09:18:44Z

My spider yields certain data but sometimes it doesn't find the data.

Instead of setting a condition such as below:I'd rather fix this in my pipeline by removing all items that have a None value. I've tried to do this by the following code:However, this results in an error:I think it has to do with the fact that a field has been yielded to the pipeline but not returned at the end but that's just a guess. Currently the pipeline has statements such as:And I'd like to prevent adding unnecessary extra statements to check if the value is None.Your current code would probably work if you returned the created item:That said, I would strongly recommend using item loaders in your scrapy spiders.

Fields not being created for empty data is just one of the many benefits.EDIT:Now that you've included the full pipeline code, I can see that the error is on the very last line.

Your code creates an exception object, discards it, and returns None; the DropItem exception must be raised:

Inputting the Nth Number of a sequence into Scrapy Start URL's

Adam Smith 86

[Inputting the Nth Number of a sequence into Scrapy Start URL's](https://stackoverflow.com/questions/49120260/inputting-the-nth-number-of-a-sequence-into-scrapy-start-urls)

I'm looking for a shorthand way to input the Nth term of a sequence into Scrapy start URL's. For exampe, this is the longer version for adding the 24th:I've found that URL patterns such as these are common for ecommerce websites. For every number in sequence the following works well, however I've been unable to change it for the Nth number:Any help would be greatly appreciated.

2018-03-05 22:04:05Z

I'm looking for a shorthand way to input the Nth term of a sequence into Scrapy start URL's. For exampe, this is the longer version for adding the 24th:I've found that URL patterns such as these are common for ecommerce websites. For every number in sequence the following works well, however I've been unable to change it for the Nth number:Any help would be greatly appreciated.The third argument in xrange allows you to provide a step of how much to increment it by.I think this is what you're looking for:The code above will generate the same list you gave in the example.The first argument (24) is where to start, the second argument (73) is where to stop (exclusive, so 73 to stop on 72) and the third argument (24) tells xrange how much to increment it by.

scrapy Key Error title

Torb

[scrapy Key Error title](https://stackoverflow.com/questions/49050556/scrapy-key-error-title)

i am new to python and especially to scrapy. I wanted to make a spider which gives me all the comments from a reddit page. it finds the comments but it does not save them to a .csv file. Here is my spider:And this is one example for the error i get in every step:Can anyone tell me what the problem is?

2018-03-01 13:17:46Z

i am new to python and especially to scrapy. I wanted to make a spider which gives me all the comments from a reddit page. it finds the comments but it does not save them to a .csv file. Here is my spider:And this is one example for the error i get in every step:Can anyone tell me what the problem is?

How To Keep/Export Field Items in Specific Order Per Spider Class Definition, Utilizing The Items Pipeline in Scrapy

NeilR

[How To Keep/Export Field Items in Specific Order Per Spider Class Definition, Utilizing The Items Pipeline in Scrapy](https://stackoverflow.com/questions/49058067/how-to-keep-export-field-items-in-specific-order-per-spider-class-definition-ut)

I have a spider which exports data to different CSV files (per the names of the class definitions as defined in the spider class). However, I also wanted to keep the order of the fields in a specific order as they were being processed and exported into their different CSV files.For example, this is my items.py:This is my pipelines.py:And this is my spider.py:As the spider was processing and exporting data, I was looking for a way to keep the fields in the CSV generated files "first_class_def.csv" and "second_class_def.csv", exported in the same order as in the items.py:f1,f2,f3andf1,f4,f5,f6However, whenever I would crawl the spider, the fields within the CSV files were being exported in random order:f2,f1,f3 and f5,f1,f4,f6The solution is posted below!

2018-03-01 20:21:22Z

I have a spider which exports data to different CSV files (per the names of the class definitions as defined in the spider class). However, I also wanted to keep the order of the fields in a specific order as they were being processed and exported into their different CSV files.For example, this is my items.py:This is my pipelines.py:And this is my spider.py:As the spider was processing and exporting data, I was looking for a way to keep the fields in the CSV generated files "first_class_def.csv" and "second_class_def.csv", exported in the same order as in the items.py:f1,f2,f3andf1,f4,f5,f6However, whenever I would crawl the spider, the fields within the CSV files were being exported in random order:f2,f1,f3 and f5,f1,f4,f6The solution is posted below!Unfortunately, due to the way scrapy's Item is implemented, the information about the order of field definitions is not preserved.If the order matters, the best you can do is define the order you want as a separate class variable, and use that in your pipeline. Passing the fields_to_export argument to CsvItemExporter would probably be simplest.Here's a basic idea you can play around with:Also, I just noticed you're using list comprehensions for side-effects, which is a bad idea, you should just use a normal loop instead.This is the solution to my specific problem: export fields organized per the items class definition as defined in the items.py of a scrapy spider project.So after tinkering with this problem and implementing @stranac's suggestion of getting rid of the list comprehension, I came up with the following solution, allowing to export all fields in order into their relative csv files:Now, everything works as I originally intended to.

Ignoring requests while scraping two pages

Abel Riboulot

[Ignoring requests while scraping two pages](https://stackoverflow.com/questions/49059526/ignoring-requests-while-scraping-two-pages)

I am now scraping this website on a daily basis, and am using DeltaFetch to ignore pages which have already been visited (a lot of them).The issue I am facing is that for this website, I need to first scrape page A, and then scrape page B to retrieve additional information about the item. DeltaFetch works well in ignoring requests to page B, but that also means that every time the scraping runs, it runs requests to page A regardless of whether it has visited it or not.This is how my code is structured right now:Any help would be appreciated to ignore the first request to page A when this page has already been visited, using DeltaFetch.

2018-03-01 22:13:18Z

I am now scraping this website on a daily basis, and am using DeltaFetch to ignore pages which have already been visited (a lot of them).The issue I am facing is that for this website, I need to first scrape page A, and then scrape page B to retrieve additional information about the item. DeltaFetch works well in ignoring requests to page B, but that also means that every time the scraping runs, it runs requests to page A regardless of whether it has visited it or not.This is how my code is structured right now:Any help would be appreciated to ignore the first request to page A when this page has already been visited, using DeltaFetch.DeltaFetch only keeps record of the requests that yield items in its database, which means only those will be skipped by default.However, you are able to customize the key used to store a record by using the deltafetch_key meta key. If you make this key the same for the requests that call parse_A() as for those created inside parse_A(), you should be able to achieve the effect you want.Something like this should work (untested):Note: the example above effectively replaces the filtering of requests to parse_B() urls with the filtering of requests to parse_A() urls. You might need to use a different key depending on your needs.

How to omit processing of an Item() if it has already been seen by spider in Python Scrapy

BARNOWL

[How to omit processing of an Item() if it has already been seen by spider in Python Scrapy](https://stackoverflow.com/questions/48978872/how-to-omit-processing-of-an-item-if-it-has-already-been-seen-by-spider-in-pyt)

I'm trying to remove duplicate business_names during a spider crawl. However, I still see duplicate business_names.I tried if x != item['business_name'] continue with parsing.What I want is that if a business_name doesn't already exist then parse it, if not then delete from list or skip query result.Instead, the code below ignores my if statement; Here is what I have so far.

2018-02-25 21:54:42Z

I'm trying to remove duplicate business_names during a spider crawl. However, I still see duplicate business_names.I tried if x != item['business_name'] continue with parsing.What I want is that if a business_name doesn't already exist then parse it, if not then delete from list or skip query result.Instead, the code below ignores my if statement; Here is what I have so far.The reason why you are seeing this behaviour is a problem of scope. You set item['business_name'] = to a the results of .extract() which is always a list (even if there is only one successful css.tag.Then the code iterates over item['business_name'] and checks if each element of the list is =! item['business_name']Turns out, that will always be True.It is equivalent to doing the following:Instead, initialize a list outside of the for loop and check if value is in that list. For instance, something to the effect of:I don't have access to your html file, so I can't guarantee that above code will work, but the behavior you are facing would be expected based on the code you provided in original post. Side note: The list in the solution above will only be preserved for each call to the parse step. In other words, for each start_url passed to parse.  If you want to ensure that one and only one business_name is extracted GLOBALLY for any page passed to parse during the life of the Spider class, we could maintain a list in class definition and check against it in the same way we did locally to parse. Consider:Cheers!

Speed up scrapy spiders initialisation time

fast_cen

[Speed up scrapy spiders initialisation time](https://stackoverflow.com/questions/48983803/speed-up-scrapy-spiders-initialisation-time)

I have multiple Scrapy spiders that I need to run at the same time every 5 minutes. The issue is that they take almost 30 sec to 1 minute to start.It's seem that they all start their own twisted engine, and so it take a lot of time.I've look into different ways to run multiple spiders at the same time (see Running Multiple spiders in scrapy for 1 website in parallel?), but I need to have a log for each spider and a process per spider to integrate well with Airflow.I've look into scrapyd, but it doesn't seem to share a twisted engine for multiple spiders, is that correct ?Are they different ways I could achieve my goals ?

2018-02-26 07:57:26Z

I have multiple Scrapy spiders that I need to run at the same time every 5 minutes. The issue is that they take almost 30 sec to 1 minute to start.It's seem that they all start their own twisted engine, and so it take a lot of time.I've look into different ways to run multiple spiders at the same time (see Running Multiple spiders in scrapy for 1 website in parallel?), but I need to have a log for each spider and a process per spider to integrate well with Airflow.I've look into scrapyd, but it doesn't seem to share a twisted engine for multiple spiders, is that correct ?Are they different ways I could achieve my goals ?

How to get the following siblings of a complex xpath query

maugch

[How to get the following siblings of a complex xpath query](https://stackoverflow.com/questions/48901501/how-to-get-the-following-siblings-of-a-complex-xpath-query)

I have a complex html where I need to check a child node before knowing if I can get the siblings. For example:I need to get all  from the table following the <div> that contains <a name="pickme">This is the xpath I have now:So basically I look for a div with somewhere an <a> inside that has the correct name. Once I found that <div>, I get the next sibling of it and check inside.

How do I tell xpath that "following-sibling" is referred to the <div> and not the <a>?

2018-02-21 08:48:04Z

I have a complex html where I need to check a child node before knowing if I can get the siblings. For example:I need to get all  from the table following the <div> that contains <a name="pickme">This is the xpath I have now:So basically I look for a div with somewhere an <a> inside that has the correct name. Once I found that <div>, I get the next sibling of it and check inside.

How do I tell xpath that "following-sibling" is referred to the <div> and not the <a>?Select the div instead of a and just check existence of a in XPath predicate, then you can easily add following-sibling axis in the path :

Scrapy raises ReactorNotRestartable when CrawlerProcess is ran twice

Joe Roe

[Scrapy raises ReactorNotRestartable when CrawlerProcess is ran twice](https://stackoverflow.com/questions/48913525/scrapy-raises-reactornotrestartable-when-crawlerprocess-is-ran-twice)

I have some code which looks something like this:I have two py.test tests which each call run(), when the second test executes I get the following error.I get this reactor thing is already running so I cannot runner.start() when the second test runs. But is there some way to reset its state inbetween the tests? So they are more isolated and actually can run after one another.

2018-02-21 19:02:01Z

I have some code which looks something like this:I have two py.test tests which each call run(), when the second test executes I get the following error.I get this reactor thing is already running so I cannot runner.start() when the second test runs. But is there some way to reset its state inbetween the tests? So they are more isolated and actually can run after one another.If you use CrawlerRunner instead of CrawlerProcess in conjunction with pytest-twisted, you should be able to use run your tests like this:Install Twisted integration for Pytest: pip install pytest-twistedTo put it plainly, _run_crawler() will schedule a crawl in the Twisted reactor and execute callbacks when the scrape completes. In those callbacks (_success() and _error()) is where you will do your assertions. Lastly, you have to return the Deferred object from _run_crawler() so that the test waits until the crawl is complete. This part with the Deferred, is essential and must be done for all tests.Here's an example of how to run multiple crawls and aggregate results using gatherResults.I hope this helps, if it doesn't please ask where you're struggling.According to the scrapy docs:For example:If you want to run another spider after you've called process.start then I expect you can just issue another process.crawl(SomeSpider) call at the point in your program where you determine the need to do this.Examples of other scenarios are given in the docs.

Scrapy, Scrapinghub and Google Cloud Storage: Keyerror 'gs' while running the spider on scrapinghub

Sagar Singh Verma

[Scrapy, Scrapinghub and Google Cloud Storage: Keyerror 'gs' while running the spider on scrapinghub](https://stackoverflow.com/questions/48925215/scrapy-scrapinghub-and-google-cloud-storage-keyerror-gs-while-running-the-sp)

I'm working on a scrapy project using Python 3 and the spiders are deployed to scrapinghub. I'm also using Google Cloud Storage to store the scraped files as mentioned in the official doc here. The spiders are running absolutely fine when i'm running it locally and the spiders are getting deployed to scrapinghub without any errors. I'm using scrapy:1.4-py3 as the stack for the scrapinghub. While running the spiders on it, i'm getting the following error:PS: 'gs' is used in the path to store the files likeI have researched about this error, but there aren't any solutions as such. Any help would be of immense help.

2018-02-22 10:35:47Z

I'm working on a scrapy project using Python 3 and the spiders are deployed to scrapinghub. I'm also using Google Cloud Storage to store the scraped files as mentioned in the official doc here. The spiders are running absolutely fine when i'm running it locally and the spiders are getting deployed to scrapinghub without any errors. I'm using scrapy:1.4-py3 as the stack for the scrapinghub. While running the spiders on it, i'm getting the following error:PS: 'gs' is used in the path to store the files likeI have researched about this error, but there aren't any solutions as such. Any help would be of immense help.Google Cloud Storage support is a new feature in Scrapy 1.5, so you need to use scrapy:1.5-py3 stack in Scrapy Cloud.

Invoke scrapy's custom exporter by command line

Lore

[Invoke scrapy's custom exporter by command line](https://stackoverflow.com/questions/48929331/invoke-scrapys-custom-exporter-by-command-line)

While trying to resolve my problem (output an ordered Json array by a specific item's field), I've received an answer that suggests me to create a custom exporter for the job.I'm creating one, but... all the examples that I've find suggest to call it by pipeline, but it seems a bit redundant to me (I've already defined custom behavior on personal exporter... why should I customize with pipeline too?).What I search is a way to call the custom exporter, once defined, by scrapy shell. For example, to output json I will use:Does it exists some way to specify my custom exporter for writing file in a similar manner? I've found an experimental feature COMMANDS_MODULE for custom commands, but I'm not sure how to link it to my custom exporter.

2018-02-22 14:02:31Z

While trying to resolve my problem (output an ordered Json array by a specific item's field), I've received an answer that suggests me to create a custom exporter for the job.I'm creating one, but... all the examples that I've find suggest to call it by pipeline, but it seems a bit redundant to me (I've already defined custom behavior on personal exporter... why should I customize with pipeline too?).What I search is a way to call the custom exporter, once defined, by scrapy shell. For example, to output json I will use:Does it exists some way to specify my custom exporter for writing file in a similar manner? I've found an experimental feature COMMANDS_MODULE for custom commands, but I'm not sure how to link it to my custom exporter.You can activate an exporter by using the FEED_EXPORTERS setting.

Access items yielded by my spider when running Scrapy from script

Alvaro Aguilar

[Access items yielded by my spider when running Scrapy from script](https://stackoverflow.com/questions/60289270/access-items-yielded-by-my-spider-when-running-scrapy-from-script)

I am calling a Scrapy Spider from a Python script. I would like to have access to the items that the Spider yields from within my script. But I have no idea how to do it. The script works fine, the spider is called and it yields the right items, but I don't know how to access those items from my script.This is the code for the scriptAnd this is the part of the log that shows the spider works fine and yields the items.Thanks very much for your help!!One option I can think of would be to create a pipeline that stores the item and then access the items from that storage:

2020-02-18 20:52:33Z

I am calling a Scrapy Spider from a Python script. I would like to have access to the items that the Spider yields from within my script. But I have no idea how to do it. The script works fine, the spider is called and it yields the right items, but I don't know how to access those items from my script.This is the code for the scriptAnd this is the part of the log that shows the spider works fine and yields the items.Thanks very much for your help!!One option I can think of would be to create a pipeline that stores the item and then access the items from that storage:

Problems getting next page when scraping with scrapy

Denis Dragnev

[Problems getting next page when scraping with scrapy](https://stackoverflow.com/questions/60257566/problems-getting-next-page-when-scraping-with-scrapy)

I have a scrapy code which doesn't crawl pagination links and i'm stuck.The source of the page is:https://www.levenhuk.bg/katalog/teleskopi/?page=1My code is:

2020-02-17 07:17:24Z

I have a scrapy code which doesn't crawl pagination links and i'm stuck.The source of the page is:https://www.levenhuk.bg/katalog/teleskopi/?page=1My code is:I feel like the problem is simply that you are not specifying a callback in your pagination request. Specify your parse function as callback and that should work. please comment if it still doesn't work.Edit:In this case I feel like your logic needs an overhaul. I suggest separating the pagination and item extraction login. Try the following:so now the parse function handles pagination and the extract_item function extracts items for every page.Modify allowed_domains as well as specified by Pasindu.Change this to : You also need to change:This will only work for the first page, for page 2,3,4.., this will extract a link to the first page.And also add a callback as mentioned by UzairAhmed.This is a little tricky since usually standard practice is to just check if there is a next page button on a loop until there isn't.Here's an example since there is no next page button we can figure out the total page count. There will be a duplicate request to page1 though with this method its not the most ideal situation.Another method of doing this would be to just look at how many pages there are and over ride your start_requests method as follows:

Scrapy the Vue page

Grzegorz Cieślak

[Scrapy the Vue page](https://stackoverflow.com/questions/60264008/scrapy-the-vue-page)

I'm trying to scrape data from this page https://www.fazwaz.ae/property-for-sale/united-arab-emirates but when i want to refer to the class I gets a phrase ['{{slotProps.unit.formatted_address}}'] I know it's from Vue but I don't know how to get data from this page, please Help how get this data  

2020-02-17 13:54:41Z

I'm trying to scrape data from this page https://www.fazwaz.ae/property-for-sale/united-arab-emirates but when i want to refer to the class I gets a phrase ['{{slotProps.unit.formatted_address}}'] I know it's from Vue but I don't know how to get data from this page, please Help how get this data  As long as it's not server side rendered page, it's better to use things like selenium or puppeteer to crawl the site. These tools use browser (like headless chrome), so you will have the computed vue model.

Scrapy inconsisent number of items scraped with callbacks and following links

Jaime Salazar

[Scrapy inconsisent number of items scraped with callbacks and following links](https://stackoverflow.com/questions/60267472/scrapy-inconsisent-number-of-items-scraped-with-callbacks-and-following-links)

I'm crawling yellow pages results and getting an inconsistent number of scraped items when trying to follow both the yellow pages entries as well as the pagination links. I believe I have 2 issues, but I seem to be able to work around the first. Hopefully this workaround is not causing my second issue.I have no problem getting the 121 search results I expect from here. I do this based on this example from the official tutorial:So my first issue is having to create the for loop involving the list_items as a workaround to get the next_page. This is because the button that takes you to the next page is not li > a like the shortcuts to pages 1, 2, ..., 5. The next page button is the only one with an <i> inside the <a>, that is li > a > i. Ideally I would do something like     but for some reason that div[5] is not found, although div[4] is. According to my research, a potential reason could be because that 5th div is generated by javascript and doesn't yet exist by the time the spider fetches the html. I doubt this is the real reason because I am able to see the div when I save the response into an html file. In any case, the workaround correctly results in the expected 121 items:Assuming this workaround isn't messing with my next steps, I am ok with it and would like to focus on my second issue. In order to follow each yellow pages entry to gather each entry's info, I imitate the example in the tutorial:This produces almost the exact results I want, except that, instead of 121, the number of items returned is sometimes 91, sometimes 94, etc.:My research hints at the asynchronous nature of these requests and/or yields (two different things, I know, but maybe one of them is the culprit) but slowing down (ie, adding sleep()) or simplifying the code (asking it to yield fewer keys) don't seem to improve things.As I type this, I'm noticing a scrapy stat referencing duplicate filtering, but I'm not really able to make sense of these stats. I'll dump them here in case it helps anyone help me.After the first spider and the correct 121 results: The second spider, with the inconsistent 90-94 results:Sorry for the length, but I appreciate any hints. Thanks!EDIT: The first issue (the workaround) seems to be solved thanks to @furas. Still struggling with the second issue (inconsistent results) though.

2020-02-17 17:15:03Z

I'm crawling yellow pages results and getting an inconsistent number of scraped items when trying to follow both the yellow pages entries as well as the pagination links. I believe I have 2 issues, but I seem to be able to work around the first. Hopefully this workaround is not causing my second issue.I have no problem getting the 121 search results I expect from here. I do this based on this example from the official tutorial:So my first issue is having to create the for loop involving the list_items as a workaround to get the next_page. This is because the button that takes you to the next page is not li > a like the shortcuts to pages 1, 2, ..., 5. The next page button is the only one with an <i> inside the <a>, that is li > a > i. Ideally I would do something like     but for some reason that div[5] is not found, although div[4] is. According to my research, a potential reason could be because that 5th div is generated by javascript and doesn't yet exist by the time the spider fetches the html. I doubt this is the real reason because I am able to see the div when I save the response into an html file. In any case, the workaround correctly results in the expected 121 items:Assuming this workaround isn't messing with my next steps, I am ok with it and would like to focus on my second issue. In order to follow each yellow pages entry to gather each entry's info, I imitate the example in the tutorial:This produces almost the exact results I want, except that, instead of 121, the number of items returned is sometimes 91, sometimes 94, etc.:My research hints at the asynchronous nature of these requests and/or yields (two different things, I know, but maybe one of them is the culprit) but slowing down (ie, adding sleep()) or simplifying the code (asking it to yield fewer keys) don't seem to improve things.As I type this, I'm noticing a scrapy stat referencing duplicate filtering, but I'm not really able to make sense of these stats. I'll dump them here in case it helps anyone help me.After the first spider and the correct 121 results: The second spider, with the inconsistent 90-94 results:Sorry for the length, but I appreciate any hints. Thanks!EDIT: The first issue (the workaround) seems to be solved thanks to @furas. Still struggling with the second issue (inconsistent results) though.To get link to next page you can search <a> which has <i> with class "fa icon-flecha-derecha" which displays icon >. There can be other <i> with icon < so I have to use class to recognize correct icon.Using xpath you can nest elements a[ i[@class="fa icon-flecha-derecha"] ] to get access to <a> instead of <i> and get @hrefAs for 93 and 121.I saved urls in file and used pandas to compare them. There are 121 links but only 93 links are unique - some of them are repeated on web pages. First code saves all urls (even repeated), second code saves only urls of visited pages - because Scrapy doesn't visit again the same page so it visits only 93 pages. 

Scrape a span text from multiple span elements of same name (IMDB)

Saphal Adhikari

[Scrape a span text from multiple span elements of same name (IMDB)](https://stackoverflow.com/questions/60266366/scrape-a-span-text-from-multiple-span-elements-of-same-name-imdb)

I am not able to create gross collection data from the above url

vote and gross has same attrs so i am finding difficult to extract gross data from the link mentioned below. how ever i am able to extract vote count.

the link to url = "https://www.imdb.com/search/title/?release_date=2019&sort=num_votes,desc&page=1"

2020-02-17 16:11:27Z

I am not able to create gross collection data from the above url

vote and gross has same attrs so i am finding difficult to extract gross data from the link mentioned below. how ever i am able to extract vote count.

the link to url = "https://www.imdb.com/search/title/?release_date=2019&sort=num_votes,desc&page=1"Your error means that for a given movie, there was no span found with text='Gross'. As I can see from the IMDB webpage you are scraping, it happens that some movies, while they have a Metascore, do not have their gross displayed. It is the case for movie 1917.You should first check for gross' existence, before calling it using method find_next().Replace:by:

Scrapy - pymongo not inserting items to DB

vivajustice

[Scrapy - pymongo not inserting items to DB](https://stackoverflow.com/questions/60215265/scrapy-pymongo-not-inserting-items-to-db)

So im playing about with scrapy trying to learn, and using MongoDB as my DB ive come to a dead end.

Basically the scraping works as the items im fetching are showing in the terminal log, but i cant get the data to publish on my DB. The MONGO_URI is correct as i tried it in the python shell where i can create and store data..Here are my filesitems.py spider.pysettings.pypipelines.pyAny help would be great!**Edit file structure

2020-02-13 19:34:22Z

So im playing about with scrapy trying to learn, and using MongoDB as my DB ive come to a dead end.

Basically the scraping works as the items im fetching are showing in the terminal log, but i cant get the data to publish on my DB. The MONGO_URI is correct as i tried it in the python shell where i can create and store data..Here are my filesitems.py spider.pysettings.pypipelines.pyAny help would be great!**Edit file structureShouldn't the line in the MongoPipeline class:be:since you call:I figured it out, with a fresh head i looked over everything again. Turns out in settings i had to edit the to I guess i shouldnt have changed the naming format, from ITEM_PIPELINES to MONGO_PIPELINES. what the code error and i think i need to beunder init if it posible can you upload it to git ? and i might try to have a look 

Flutter have a similar package to Scrapy (Python) to extract data from sites? Web scrapping/crawling

luke cross

[Flutter have a similar package to Scrapy (Python) to extract data from sites? Web scrapping/crawling](https://stackoverflow.com/questions/60213804/flutter-have-a-similar-package-to-scrapy-python-to-extract-data-from-sites-we)

I need make a News App and the noticies in my app must be extracted from a website of my university, on Python i know it that is possible with de Scrapy package, but, i need usage in Dart/Flutter, is possible? Or i need to use external method to scraper/crawler with Flutter?

2020-02-13 17:51:10Z

I need make a News App and the noticies in my app must be extracted from a website of my university, on Python i know it that is possible with de Scrapy package, but, i need usage in Dart/Flutter, is possible? Or i need to use external method to scraper/crawler with Flutter?

Realtime Web Crawler For Comparison Site [closed]

Jesse V

[Realtime Web Crawler For Comparison Site [closed]](https://stackoverflow.com/questions/60178641/realtime-web-crawler-for-comparison-site)

I am a novice coder and only experienced in hmtl, php and css so this I know this is probably out of my scope. Any advice is appreciated.A customer will visit my website to compare prices for a part. They will enter a part # in the search form and a list of the parts will be shown to the customer from other websites that I choose. This way the customer can see all prices for that part in 1 page.For example if the enter part #  DC93-00101N   this part will be searched on the top 10 supplier websites and results will be shown on my website. The thumbnail and price information will be shown as well. And if they want to buy the part they can click and it will take them to that website to buy.I have already looked at a few scripts such as Wrap api and Oxylabs Real time crawler but can't find any information as if they will work to produce live results. I am thinking I would have to get this special script written in Python to scrape these websites in and produce results back to my site. Is my plan feasible and if so whats the best way to look at creating this site?

2020-02-11 22:59:42Z

I am a novice coder and only experienced in hmtl, php and css so this I know this is probably out of my scope. Any advice is appreciated.A customer will visit my website to compare prices for a part. They will enter a part # in the search form and a list of the parts will be shown to the customer from other websites that I choose. This way the customer can see all prices for that part in 1 page.For example if the enter part #  DC93-00101N   this part will be searched on the top 10 supplier websites and results will be shown on my website. The thumbnail and price information will be shown as well. And if they want to buy the part they can click and it will take them to that website to buy.I have already looked at a few scripts such as Wrap api and Oxylabs Real time crawler but can't find any information as if they will work to produce live results. I am thinking I would have to get this special script written in Python to scrape these websites in and produce results back to my site. Is my plan feasible and if so whats the best way to look at creating this site?

Web Scraping Error - ERROR for site owner: Invalid domain for site key

dreambeyondit

[Web Scraping Error - ERROR for site owner: Invalid domain for site key](https://stackoverflow.com/questions/60181087/web-scraping-error-error-for-site-owner-invalid-domain-for-site-key)

I tried to get contents of this URL - https://www.zillow.com/homedetails/131-Avenida-Dr-Berkeley-CA-94708/24844204_zpid/

I used scrapy. Here is my code.I opened scraped data(test.html) and I got this content.



I tried to find solutions and I tried this - ERROR for site owner: Invalid domain for site key

But it didn't solve my issue. 

2020-02-12 04:21:46Z

I tried to get contents of this URL - https://www.zillow.com/homedetails/131-Avenida-Dr-Berkeley-CA-94708/24844204_zpid/

I used scrapy. Here is my code.I opened scraped data(test.html) and I got this content.



I tried to find solutions and I tried this - ERROR for site owner: Invalid domain for site key

But it didn't solve my issue. First of all, try this approach and see if this works:The reason why we don't see output as we see it in normal browser is that we don't use proper headers that are otherwise always sent by the browser.You need to add headers either as stated in above code or by updating them in the settings.py.A better approach would be to use 'rotating-proxies' respositories along with 'rotating-user-agent' repository.

Scrapy Bestbuy not extracting data

Hell_ Raisin 

[Scrapy Bestbuy not extracting data](https://stackoverflow.com/questions/60150202/scrapy-bestbuy-not-extracting-data)

I was wondering why scrapy is not extracting data on bestbuy website. is there anything wrong with my code?this is my results when using scrapy crawl bestbuy -o bestbuy.csv

2020-02-10 12:12:26Z

I was wondering why scrapy is not extracting data on bestbuy website. is there anything wrong with my code?this is my results when using scrapy crawl bestbuy -o bestbuy.csvThe reason it was working in the shell and not in your code is because you forgot the 's' at the end of 'start_urls'. This should work:

What is the run directory of Splash?

obsidian93

[What is the run directory of Splash?](https://stackoverflow.com/questions/60148312/what-is-the-run-directory-of-splash)

I am trying to import a helper function into my main.lua script which I am using as a Splash execute endpoint.To do this, I have disabled sandbox mode, and have require('wait_helpers') in my main.lua.wait_helpers.lua and main.lua are in the same directory, but when I try to run the code, I get this error:Some code for reference:

In wait_helpers.lua:In main.lua:

2020-02-10 10:22:28Z

I am trying to import a helper function into my main.lua script which I am using as a Splash execute endpoint.To do this, I have disabled sandbox mode, and have require('wait_helpers') in my main.lua.wait_helpers.lua and main.lua are in the same directory, but when I try to run the code, I get this error:Some code for reference:

In wait_helpers.lua:In main.lua:

Scrapy simulated login redirected several times eventually gets back to login page

David

[Scrapy simulated login redirected several times eventually gets back to login page](https://stackoverflow.com/questions/60114998/scrapy-simulated-login-redirected-several-times-eventually-gets-back-to-login-pa)

I was using Scrapy to simulate a user login process to a website (sorry, I cannot share URL here). After sending proper username and password using FromRequest, the website redirect the login page (A) to B, then B gets redirected again to C, then strangely C back to A again; While what I was expecting is C, but it's always getting back to A. I am pretty sure about the correctness of my U/P as if I intentionally made it wrong, there's no redirection from the website. With this U/P input to browser, I can successfully reach to C. My question is: how can I get to the page C with simulated user login in Scrapy?The code:Here below is the Scrapy running log:

2020-02-07 14:04:02Z

I was using Scrapy to simulate a user login process to a website (sorry, I cannot share URL here). After sending proper username and password using FromRequest, the website redirect the login page (A) to B, then B gets redirected again to C, then strangely C back to A again; While what I was expecting is C, but it's always getting back to A. I am pretty sure about the correctness of my U/P as if I intentionally made it wrong, there's no redirection from the website. With this U/P input to browser, I can successfully reach to C. My question is: how can I get to the page C with simulated user login in Scrapy?The code:Here below is the Scrapy running log:

handle start requests actions in scrapy

wishmaster

[handle start requests actions in scrapy](https://stackoverflow.com/questions/60116547/handle-start-requests-actions-in-scrapy)

I am facing weird behavior when I am yielding the requests, Ideally each request will be yielded after 6 seconds, but what is actually happening is that after 60 (6*10) seconds all requests are made at once, I was able to fix this by CONCURRENT_REQUESTS=1.If i set CONCURRENT_REQUESTS=3 it will wait 18 seconds to yield 3 requests then move to the next 3 (wait 18 sec and then yield) and so on. (I am using time.sleep(6) to replace a function I have that takes about that much time to do). How can I force it yield the request (like a regular generator)

2020-02-07 15:33:37Z

I am facing weird behavior when I am yielding the requests, Ideally each request will be yielded after 6 seconds, but what is actually happening is that after 60 (6*10) seconds all requests are made at once, I was able to fix this by CONCURRENT_REQUESTS=1.If i set CONCURRENT_REQUESTS=3 it will wait 18 seconds to yield 3 requests then move to the next 3 (wait 18 sec and then yield) and so on. (I am using time.sleep(6) to replace a function I have that takes about that much time to do). How can I force it yield the request (like a regular generator)You need to use DOWNLOAD_DELAY setting in order to make delay between requests.This code do following:

As result of start_requests method - spider will add 10 requests to scrapy scheduler queue .(It doesn't mean that scrapy immediately execute 10 requests).





Using DOWNLOAD_DELAY and RANDOMIZE_DOWNLOAD_DELAY settings:

Scrapy downloader get request from scheduler queue and send it to your website.

 When response received -> callback method called.  This process repeats each 6 seconds (As defined in DOWNLOAD_DELAY setting)

Scrapy - Selenium TypeError: 'NoneType' object is not iterable

Vraja

[Scrapy - Selenium TypeError: 'NoneType' object is not iterable](https://stackoverflow.com/questions/60117112/scrapy-selenium-typeerror-nonetype-object-is-not-iterable)

I get the following error:when I run:What am I doing wrong here?Thank you

2020-02-07 16:07:35Z

I get the following error:when I run:What am I doing wrong here?Thank you

Scraping recursively with scrapy

AaronS

[Scraping recursively with scrapy](https://stackoverflow.com/questions/60118196/scraping-recursively-with-scrapy)

I'm trying to create a scrapy script with the intent on gaining information on individual posts on the medium website. 

Now, unfortunately, it requires 3 depths of links. Each year link, and each month within that year and then each day within the months links. I've got as far as managing to get each individual link for every year, every month in that year and every day. However I just can't seem to get scrapy to deal with the individual day pages. I'm not entirely sure whether I'm confusing using rules and using functions with callbacks to get the links. There isn't much guidance on how to recursively deal with this type of pagination. I've tried using functions and response.follow by itself without being able to get it to run. The parse_item function dictionary is required because several articles on the individual day pages have several different ways of classifying the title annoyingly. So i created a function to grab the title regardless of the actual XPATH needed to grab the title.The last function get_tag is needed because on each individual article that is where the tags are to grab. I'd appreciate any insight into how to get the last step and getting the individual links to go through the parse_item function, the shell o. I should say there are no obvious errors than I can see in the shell.Any further information necessary just let me know. Thanks!CODE: OUTPUT

2020-02-07 17:18:21Z

I'm trying to create a scrapy script with the intent on gaining information on individual posts on the medium website. 

Now, unfortunately, it requires 3 depths of links. Each year link, and each month within that year and then each day within the months links. I've got as far as managing to get each individual link for every year, every month in that year and every day. However I just can't seem to get scrapy to deal with the individual day pages. I'm not entirely sure whether I'm confusing using rules and using functions with callbacks to get the links. There isn't much guidance on how to recursively deal with this type of pagination. I've tried using functions and response.follow by itself without being able to get it to run. The parse_item function dictionary is required because several articles on the individual day pages have several different ways of classifying the title annoyingly. So i created a function to grab the title regardless of the actual XPATH needed to grab the title.The last function get_tag is needed because on each individual article that is where the tags are to grab. I'd appreciate any insight into how to get the last step and getting the individual links to go through the parse_item function, the shell o. I should say there are no obvious errors than I can see in the shell.Any further information necessary just let me know. Thanks!CODE: OUTPUTremove the three functions years,months,days

Retry request in scrapy downloader middleware

Ivan Ivan

[Retry request in scrapy downloader middleware](https://stackoverflow.com/questions/60081183/retry-request-in-scrapy-downloader-middleware)

I use scrapoxy which implements IP rotation while scrapping. I have a list BLACKLIST_HTTP_STATUS_CODES of status codes that indicate that the current IP is blocked. The problem: once you got a response with status code in BLACKLIST_HTTP_STATUS_CODES scrapoxy downloader middleware raises IgnoreRequest and then changes IP. As the result my script skips the url whose response got bad status code.Example of logs:As the result my script skipped https://www.some-website.com/profile/193.The goal: I want to retry request whose response got status code that is in BLACKLIST_HTTP_STATUS_CODES until it is not in that list.My DownloaderMiddleware looks like that:

2020-02-05 17:33:52Z

I use scrapoxy which implements IP rotation while scrapping. I have a list BLACKLIST_HTTP_STATUS_CODES of status codes that indicate that the current IP is blocked. The problem: once you got a response with status code in BLACKLIST_HTTP_STATUS_CODES scrapoxy downloader middleware raises IgnoreRequest and then changes IP. As the result my script skips the url whose response got bad status code.Example of logs:As the result my script skipped https://www.some-website.com/profile/193.The goal: I want to retry request whose response got status code that is in BLACKLIST_HTTP_STATUS_CODES until it is not in that list.My DownloaderMiddleware looks like that:Instead of returning a new Request object, you should copy the original request like retry = request.copy(). You could check out how Scrapy's RetryMiddleware handles retries.For your reference:And you could call it likeThis should give you the idea.

Install scrapy ERROR - When I try to install scrapy I get the following error that I do not know how to fix it:

Aaa.

[Install scrapy ERROR - When I try to install scrapy I get the following error that I do not know how to fix it:](https://stackoverflow.com/questions/60078629/install-scrapy-error-when-i-try-to-install-scrapy-i-get-the-following-error-th)

When I try to install scrapy I get the following error that I do not know how to fix it:

2020-02-05 15:03:10Z

When I try to install scrapy I get the following error that I do not know how to fix it:Recommended way to install scrapy on windows - using Anaconda python distrubution.

how to set next page rule with CrawlSpider?

Jubayer Ahmed

[how to set next page rule with CrawlSpider?](https://stackoverflow.com/questions/60117774/how-to-set-next-page-rule-with-crawlspider)

I am tring scrap email adress from a website. in order to do that i have to extract in every single link which is  in a list then on that extract page i will extract the email address.the problem is the nextpage button conatins upto 50. but if i change the url with a slash and input 51. it also go a new page.i want use for loop for next page link. as example i will use for loop from 1 to 999 it will update the next page url. below is my code.it's working fine as long as the next_page button is available.

2020-02-07 16:49:59Z

I am tring scrap email adress from a website. in order to do that i have to extract in every single link which is  in a list then on that extract page i will extract the email address.the problem is the nextpage button conatins upto 50. but if i change the url with a slash and input 51. it also go a new page.i want use for loop for next page link. as example i will use for loop from 1 to 999 it will update the next page url. below is my code.it's working fine as long as the next_page button is available.Check your url in the start_requests function. It is not correct. I think you meant: "https://www.dastelefonbuch.de/Suche/Textilien"

Include original URL from Excel sheet in scrapy output

Jamwg

[Include original URL from Excel sheet in scrapy output](https://stackoverflow.com/questions/60040142/include-original-url-from-excel-sheet-in-scrapy-output)

I'm using Scrapy to crawl some pages. I refer to an excel sheet for the start_urls, and I want those exact start urls to appear in the results, rather than the redirected urls. I need the originals in order to process Excel lookups.The problem is that I only seem to be able to get an output that gives the destination url.My code is as follows;Pretty simple code, but I'm struggling to understand what I can do from the Scrapy docs.I have modified the code according to advice but I'm still not getting the original urlsfrom my source spreadsheet. Example urls are as follows;

2020-02-03 13:23:08Z

I'm using Scrapy to crawl some pages. I refer to an excel sheet for the start_urls, and I want those exact start urls to appear in the results, rather than the redirected urls. I need the originals in order to process Excel lookups.The problem is that I only seem to be able to get an output that gives the destination url.My code is as follows;Pretty simple code, but I'm struggling to understand what I can do from the Scrapy docs.I have modified the code according to advice but I'm still not getting the original urlsfrom my source spreadsheet. Example urls are as follows;You can use response.request.url in the parse function to get the original URL you requested.UPDATE: I either understand the documentation wrong or it's a bug. Specificallymakes me really think that the original request URL should be available under response.request.url.Anyway, as stated in the RedirectMiddleware documentation, there's an alternative way. You can use redirect_urls key of the request.meta to get a list of URLs the request goes through. So here's the modified (simplified) version of your code as a PoC:Also, note that there are some other issues with your original code you provided, specifically:Here is my final working code, with the help of Tomáš Linhart

Cant get text from span scrapy python

DeadSec

[Cant get text from span scrapy python](https://stackoverflow.com/questions/60085122/cant-get-text-from-span-scrapy-python)

So I'm making a bot to get price and name from Zara products and I managed to get the product name but the price it returning [].Here is my code:What it returns: What its supposed to return:Everything I tried:I think that's all I tried! 

I'm using scrapy version 1.8 with python 3.7

2020-02-05 22:37:27Z

So I'm making a bot to get price and name from Zara products and I managed to get the product name but the price it returning [].Here is my code:What it returns: What its supposed to return:Everything I tried:I think that's all I tried! 

I'm using scrapy version 1.8 with python 3.7The reason why you don't get price using normal 'xpath/css' approach is that, the 'price' field isn't available to your crawler directly. Your crawler see pages differently hence the xpath(s) are completely different.Try this approach:Moreover, it's better to use a different try...except for individual fields, so that you know which section exactly produced the error, for further rectification.

Scrapy run 2 spiders with outputs to 2 different files using one process (AWS Lambda)

Jamie

[Scrapy run 2 spiders with outputs to 2 different files using one process (AWS Lambda)](https://stackoverflow.com/questions/60001515/scrapy-run-2-spiders-with-outputs-to-2-different-files-using-one-process-aws-la)

I'm trying to run Scrapy on an AWS Lambda function and everything is almost working, except that I need to run 2 lambdas in the 1 function. The main catch is that I need the 2 spiders to output to 2 different JSON files.The docs look like they've got a very close solution:Except for the fact that if I were to input my settings into the CrawlerProcess like I currently have:Then both spiders would output to the one file fx_today_data.json.I've tried creating 2 CrawlerProcesses but that gives me the ReactorNotRestartable Error which I've tried solving using this thread but with no success.I've also tried running the scrapy code like so:But this results in the usual 'scrapy' command not found - because I don't have a virtualenv set up in the Lambda function (I don't know if it's worth setting one up for this?).Does anyone know how to run 2 Scrapy Spiders (they don't have to run at the same time) in one process and have them output to separate files?

2020-01-31 09:50:49Z

I'm trying to run Scrapy on an AWS Lambda function and everything is almost working, except that I need to run 2 lambdas in the 1 function. The main catch is that I need the 2 spiders to output to 2 different JSON files.The docs look like they've got a very close solution:Except for the fact that if I were to input my settings into the CrawlerProcess like I currently have:Then both spiders would output to the one file fx_today_data.json.I've tried creating 2 CrawlerProcesses but that gives me the ReactorNotRestartable Error which I've tried solving using this thread but with no success.I've also tried running the scrapy code like so:But this results in the usual 'scrapy' command not found - because I don't have a virtualenv set up in the Lambda function (I don't know if it's worth setting one up for this?).Does anyone know how to run 2 Scrapy Spiders (they don't have to run at the same time) in one process and have them output to separate files?Thanks to Corentin and this guide, I was able to get it working.By individually creating a custom_settings class attribute for the spiders I could run them off the one CrawlerProcess and not have to worry as they individually had their own file outputs.The final code looks a lot like the docs example I provided in the question.I also ended up having to use from multiprocessing.context import Process and to use a try block to terminate the process (before it's even been assigned!) in order to make sure that I would avoid the ReactorNotRestartable error.

Read data from excel file in scrapinghub cloud

user1994

[Read data from excel file in scrapinghub cloud](https://stackoverflow.com/questions/60002131/read-data-from-excel-file-in-scrapinghub-cloud)

I have an excel file that contains some data and against that, I am creating URLs to scrape data from. 

On the local machine, I am able to do this, but on the cloud, the excel file is not found due to which it does not read the data from the file. 

I am looking for a way to read data from an excel sheet within a spider that is deployed in scrapinghub cloud. 

2020-01-31 10:30:35Z

I have an excel file that contains some data and against that, I am creating URLs to scrape data from. 

On the local machine, I am able to do this, but on the cloud, the excel file is not found due to which it does not read the data from the file. 

I am looking for a way to read data from an excel sheet within a spider that is deployed in scrapinghub cloud. 

Scrapy - issues with 'dont_filter' option for Requests

M. Coppée

[Scrapy - issues with 'dont_filter' option for Requests](https://stackoverflow.com/questions/60010455/scrapy-issues-with-dont-filter-option-for-requests)

I must include the option dont_filter=True into each request of my spider, I've already used this option but I don't know why this time I get this error:Here is my Spider (sorry it is quite big):Did I miss something ? 

2020-01-31 19:59:17Z

I must include the option dont_filter=True into each request of my spider, I've already used this option but I don't know why this time I get this error:Here is my Spider (sorry it is quite big):Did I miss something ? dont_filter=True should be inside scrapy.Request meta dict: 

meta = {'dont_filter': True , 'car':...}

How to get text from inside span or outside at the same time with xpath?

Plugz

[How to get text from inside span or outside at the same time with xpath?](https://stackoverflow.com/questions/60041128/how-to-get-text-from-inside-span-or-outside-at-the-same-time-with-xpath)

I have a problem with using xpath to get inconsistent price listExampleHow to get the price inside span and Out of stock at the same time?

Because I get only $33.99 or anything that have span and text that is not inside span got skipped. And it ruined the ordering.The failed attempt that I used w/ updated from @piratefache's solution (Scrapy)It's not working because product_prices doesn't get the right text it get from all over the place. Not just inside span or outside as I intended to.Update

For the one who came later. I fixed my code Thanks to @piratefache's. Here's corrected snippet for who want to use later.

2020-02-03 14:21:52Z

I have a problem with using xpath to get inconsistent price listExampleHow to get the price inside span and Out of stock at the same time?

Because I get only $33.99 or anything that have span and text that is not inside span got skipped. And it ruined the ordering.The failed attempt that I used w/ updated from @piratefache's solution (Scrapy)It's not working because product_prices doesn't get the right text it get from all over the place. Not just inside span or outside as I intended to.Update

For the one who came later. I fixed my code Thanks to @piratefache's. Here's corrected snippet for who want to use later.See edit below with ScrapyBased on your html code, using BeautifulSoup library, you can get the information this way :output :With Scrapy:output :XPath solution (from 2.0 upwards) (same logic as @piratefache posted before):Applied onreturns BTW: <span="green"> is not valid XML. Probably an attribute @color or similar is missing (?)

Scrapy 1.8.0 returns error 500, but Python code returns success 200

User New

[Scrapy 1.8.0 returns error 500, but Python code returns success 200](https://stackoverflow.com/questions/59948466/scrapy-1-8-0-returns-error-500-but-python-code-returns-success-200)

The address is: 'https://planningapi.agileapplications.co.uk/api/application/search?reference=GDO+19%2F12'I can easily download this page by Python requests library:or I can easily download the page via CURL:They both return status 200 result:But the Scrapy returns status 500 error:Maybe it is because Scrapy capitalizes headers keys (I've tried un-capitalizing them, but then Twisted does the same - it capitalizes them again), maybe for some other reason.How can I adjust my Scrapy 1.8.0 code to successfully get the same result as Python requests do?

2020-01-28 12:09:02Z

The address is: 'https://planningapi.agileapplications.co.uk/api/application/search?reference=GDO+19%2F12'I can easily download this page by Python requests library:or I can easily download the page via CURL:They both return status 200 result:But the Scrapy returns status 500 error:Maybe it is because Scrapy capitalizes headers keys (I've tried un-capitalizing them, but then Twisted does the same - it capitalizes them again), maybe for some other reason.How can I adjust my Scrapy 1.8.0 code to successfully get the same result as Python requests do?It's really caused by the fact that Scrapy capitalizes the header fields. If you try to capitalize then in the cURL command, you'll get the same error as you get with Scrapy (you can test it in Scrapy setting handle_httpstatus_list in the spider class and printing the response.text in the parse method). As you also already stated, Twisted does the same so overriding scrapy.http.Headers is not the solution.However, you can do a trick to make Twisted not capitalize specific headers, as per this issue comment:Now you'll get the results. On the other hand, as per RFC 7230, section 3.2., the header fields should be case-insensitive.

How to increase Scrapy crawling speed when there are many duplicates links

Sergio

[How to increase Scrapy crawling speed when there are many duplicates links](https://stackoverflow.com/questions/59946072/how-to-increase-scrapy-crawling-speed-when-there-are-many-duplicates-links)

I'm using Scrapy to crawl a website with millions of pages that I need to follow and extract information from it.The crawler is currently scraping only two hundred pages per minute and I need to speed it up. For each page the crawler extracts its links and follows them. The problem is that each page has about 600 links and most of them are on all pages and I think that is the reason why it's taking so long since the crawler is calling the Request function for all links even if they are duplicated.Here is a simplified version of my code:I already tried setting the concurrent requests and concurrent requests per domain but didn't work.Does anyone know why it takes so long to crawl and how can I speed it up?

2020-01-28 09:52:18Z

I'm using Scrapy to crawl a website with millions of pages that I need to follow and extract information from it.The crawler is currently scraping only two hundred pages per minute and I need to speed it up. For each page the crawler extracts its links and follows them. The problem is that each page has about 600 links and most of them are on all pages and I think that is the reason why it's taking so long since the crawler is calling the Request function for all links even if they are duplicated.Here is a simplified version of my code:I already tried setting the concurrent requests and concurrent requests per domain but didn't work.Does anyone know why it takes so long to crawl and how can I speed it up?If i understood you correctly one of the problem you have is duplicate links. Why not simply make a set of links parsed and check all links you are going to parse against this set? Here's what I mean:This way you will be sure you will not re-visit URLs.EDIT: Maybe a better solution would be to instantiate scrapy.Request once; this way you can make use of dont_filter parameter (check https://docs.scrapy.org/en/latest/topics/request-response.html for more info; your requests will be filtered by default so you won't visit duplicate URLs).

Unsure Of Where To Load Scrapy On the Anaconda Platform

Fledgling Analyst

[Unsure Of Where To Load Scrapy On the Anaconda Platform](https://stackoverflow.com/questions/59959462/unsure-of-where-to-load-scrapy-on-the-anaconda-platform)

I am exceedingly naive when it comes to the topic of scraping websites. Up to this point, I have manually scraped or had to grab data from pre-generated datasets. There is an analytic problem I want to tackle though, a problem that due to its scale requires scraping. So I have decided to try and learn how to scrape in python. Scrapy was the tool I was recommended to use, so that is what I'll be utilizing.I am trying to follow the tutorial for loading up my first Scrapy project, but have hit a snag. When I try to input the code to start a project in Spyder to start a new project:I get the following error in the console:Is there another editor or environment I am supposed to be running Scrapy in? I have tried booting up Scrapy using the ScrapyShell command in the terminal, but that seems like an inefficient approach, seeing as I have no clue how I would save any scripts I generate when running code in the terminal. Is there some custom editor for Scrapy I'm not aware of?

2020-01-29 01:36:17Z

I am exceedingly naive when it comes to the topic of scraping websites. Up to this point, I have manually scraped or had to grab data from pre-generated datasets. There is an analytic problem I want to tackle though, a problem that due to its scale requires scraping. So I have decided to try and learn how to scrape in python. Scrapy was the tool I was recommended to use, so that is what I'll be utilizing.I am trying to follow the tutorial for loading up my first Scrapy project, but have hit a snag. When I try to input the code to start a project in Spyder to start a new project:I get the following error in the console:Is there another editor or environment I am supposed to be running Scrapy in? I have tried booting up Scrapy using the ScrapyShell command in the terminal, but that seems like an inefficient approach, seeing as I have no clue how I would save any scripts I generate when running code in the terminal. Is there some custom editor for Scrapy I'm not aware of?Assuming Scrapy is properly installed, you should go to the project directory in the terminal window.For example:(venv) C:\Users\Laptop\PycharmProjects>scrapy startproject ProjectName

Where can I access Google Play App Historical monthly downloaded data?

HungryBird

[Where can I access Google Play App Historical monthly downloaded data?](https://stackoverflow.com/questions/59959269/where-can-i-access-google-play-app-historical-monthly-downloaded-data)

I am working on my school data visualization project, which needs to use historical monthly downloaded data to track on some social media Apps, like Facebook, Twitter and Snapchat.Someone says this data is available in https://www.androidrank.org/, I checked the API they offer, it doesn't provide the daily installed data. On Youtube, I saw some people made the video related to it like Most Installed Android App from 2011 - 2019   Does anyone know how can I access this data? Many thanks

2020-01-29 01:05:57Z

I am working on my school data visualization project, which needs to use historical monthly downloaded data to track on some social media Apps, like Facebook, Twitter and Snapchat.Someone says this data is available in https://www.androidrank.org/, I checked the API they offer, it doesn't provide the daily installed data. On Youtube, I saw some people made the video related to it like Most Installed Android App from 2011 - 2019   Does anyone know how can I access this data? Many thanks

Can not run 2 spiders successfully one after another in scrapy using a script

Shadman Nashif

[Can not run 2 spiders successfully one after another in scrapy using a script](https://stackoverflow.com/questions/59948532/can-not-run-2-spiders-successfully-one-after-another-in-scrapy-using-a-script)



2020-01-28 12:13:01Z



Cannot grab value within span within h2 within div

p3nd0l0

[Cannot grab value within span within h2 within div](https://stackoverflow.com/questions/59901094/cannot-grab-value-within-span-within-h2-within-div)

I'm trying to grab a VALUE that is in a span within a h2 tag within a div that has a CLASSNAMEI'm using this lineBut I'm getting error:I can't understand what's missing?Thanks

2020-01-24 17:23:25Z

I'm trying to grab a VALUE that is in a span within a h2 tag within a div that has a CLASSNAMEI'm using this lineBut I'm getting error:I can't understand what's missing?ThanksAccording to your code, the span is within the h3 tag.

Anyway, don't hesitate to solve the problem stepwise.

How do filter out escape sequences while scraping tables using css selectors?

Isaac Ng

[How do filter out escape sequences while scraping tables using css selectors?](https://stackoverflow.com/questions/59901573/how-do-filter-out-escape-sequences-while-scraping-tables-using-css-selectors)

I am trying to scrape a table using CSS Selectors in Scrapy. The method I used is scraping row by row into a single scrapy.Field() in an item object.However, the data scraped contains a "\n\t\t" element between every other element in the table. How do I remove this in the scraping process. I can do post-processing on the data but I would like to understand what is going on.My parse method:Part of output:

2020-01-24 18:02:56Z

I am trying to scrape a table using CSS Selectors in Scrapy. The method I used is scraping row by row into a single scrapy.Field() in an item object.However, the data scraped contains a "\n\t\t" element between every other element in the table. How do I remove this in the scraping process. I can do post-processing on the data but I would like to understand what is going on.My parse method:Part of output:

Python Scrapy modify spider from extension

Taras Pinchuk

[Python Scrapy modify spider from extension](https://stackoverflow.com/questions/59908643/python-scrapy-modify-spider-from-extension)

I want to modify my spider start_requests method from custom extension. The main goal is to send url to spider when I get response from my custom api. But method that I handed to spider doesnt run. Maybe its not good practice. 

Also I read scrapy documentation about crawler engine, maybe its would be better to mange spider from extension, like spider.parse(Request(url)). But I dont see any examples of usage that attempts to my goal or something like. 

Here is code example:P.S. Scrapy doesn't return any errors.

2020-01-25 11:09:49Z

I want to modify my spider start_requests method from custom extension. The main goal is to send url to spider when I get response from my custom api. But method that I handed to spider doesnt run. Maybe its not good practice. 

Also I read scrapy documentation about crawler engine, maybe its would be better to mange spider from extension, like spider.parse(Request(url)). But I dont see any examples of usage that attempts to my goal or something like. 

Here is code example:P.S. Scrapy doesn't return any errors.

Export scrapy as csv without header every re-running

vicktor9450

[Export scrapy as csv without header every re-running](https://stackoverflow.com/questions/59907176/export-scrapy-as-csv-without-header-every-re-running)

Here is my code:So, please help me to make my output csv like below:BTW could you guys show me the best way to store data. Thank you very much.

2020-01-25 07:19:35Z

Here is my code:So, please help me to make my output csv like below:BTW could you guys show me the best way to store data. Thank you very much.

Scrapy Extracting text based on a specific pattern in the class

tlk27

[Scrapy Extracting text based on a specific pattern in the class](https://stackoverflow.com/questions/59852365/scrapy-extracting-text-based-on-a-specific-pattern-in-the-class)

I am trying to extract information based on a specific pattern in the HTML code. Ideally, I would like to extract the text for the div class that mentions "bg-deep-green" only. I am new to regular expressions so I wasn't sure if there is a way to use them or any other method via xpath. Here is a sample of the HTML:I have the xpath for the parent that scrapes all of the tags.response.xpath('//*[@class="flex flex-row w-full mb-lg"]//@class').extract()Also, the xpath that scrapes all of the text for the tags. response.xpath('//*[@class="flex flex-row w-full mb-lg"]//text()').extract()Worst case scenario I could parse it out after the scrape, I think, but it would be nice if I could do it before the scrape. I have tried using contains in various ways and by using the string notation in the documentation. Below are a few of my tries, however, I may be misunderstanding contains from the documentation.

2020-01-22 03:53:57Z

I am trying to extract information based on a specific pattern in the HTML code. Ideally, I would like to extract the text for the div class that mentions "bg-deep-green" only. I am new to regular expressions so I wasn't sure if there is a way to use them or any other method via xpath. Here is a sample of the HTML:I have the xpath for the parent that scrapes all of the tags.response.xpath('//*[@class="flex flex-row w-full mb-lg"]//@class').extract()Also, the xpath that scrapes all of the text for the tags. response.xpath('//*[@class="flex flex-row w-full mb-lg"]//text()').extract()Worst case scenario I could parse it out after the scrape, I think, but it would be nice if I could do it before the scrape. I have tried using contains in various ways and by using the string notation in the documentation. Below are a few of my tries, however, I may be misunderstanding contains from the documentation.This should be enoughOutputYour syntax for XPath Expression is wrong, it should be like this:OR You can Just Use:

How can I yield the current response URL in scrapy_splash

M Umair

[How can I yield the current response URL in scrapy_splash](https://stackoverflow.com/questions/59861687/how-can-i-yield-the-current-response-url-in-scrapy-splash)

If I try to yeild url using response.request.url in my parse() Method, It Returns:Returning URL in Lua Script works, but I don't know How can I yield it in parse() method.Tried using response.url, it returns the start url

2020-01-22 14:15:46Z

If I try to yeild url using response.request.url in my parse() Method, It Returns:Returning URL in Lua Script works, but I don't know How can I yield it in parse() method.Tried using response.url, it returns the start urlProblem Was solved by replacing link with url in return of lua script:And then adding this line in parse method:

Unable to identify the Get parameter for Python get requests

sam mathew

[Unable to identify the Get parameter for Python get requests](https://stackoverflow.com/questions/59865254/unable-to-identify-the-get-parameter-for-python-get-requests)

Am trying to get data from a website, but am not able find the following get parameter and hGet parametersRequest URLIn this above sample, am not able to find the parameter cc497bc3 and c9840ee69fffe93cd29f237a70f99f5ae62f7fe7bbfb22623c731f51c43fd11f

2020-01-22 17:32:08Z

Am trying to get data from a website, but am not able find the following get parameter and hGet parametersRequest URLIn this above sample, am not able to find the parameter cc497bc3 and c9840ee69fffe93cd29f237a70f99f5ae62f7fe7bbfb22623c731f51c43fd11f

Crawling multiple tables using Scrapy

Vraja

[Crawling multiple tables using Scrapy](https://stackoverflow.com/questions/59863290/crawling-multiple-tables-using-scrapy)

I need data from different tables. In this case tables [0:17] and table [18]. I don't need a table [17]. How to solve it in one Scrapy spider. This solution does not work. Scrapy currently fetches data only from tables [0:17], but not from the table [18].

2020-01-22 15:39:32Z

I need data from different tables. In this case tables [0:17] and table [18]. I don't need a table [17]. How to solve it in one Scrapy spider. This solution does not work. Scrapy currently fetches data only from tables [0:17], but not from the table [18].yor list slice could be done 2 times, something like this:if [18] is not the last item, you also can doo this:edited:To avoid TypeError you can do one of these solutions:Also you can make your own solution work, but it's a little bit messy solution and overdo. By the way you can callback your parse_next() in parse() and send name and price as meta like:Then do rest of the work in parse_next.

List via loop not being created

p3nd0l0

[List via loop not being created](https://stackoverflow.com/questions/59812122/list-via-loop-not-being-created)

I'm trying to build a list of urls with a loop and then grab a data point from each url, but it only seems to do it for the last item (MMM) of the list and not all of them... what am I doing wrong? Thanks!

2020-01-19 16:35:08Z

I'm trying to build a list of urls with a loop and then grab a data point from each url, but it only seems to do it for the last item (MMM) of the list and not all of them... what am I doing wrong? Thanks!When you executeyou make a cycle and in each iteration of the cycle you assign new value (an array of 1 element) to quote_page instead of appending new value to array quote_page.You can do:or you can use shorter variant as was suggested by @DarrylG in the comments:

Decoupling single spider into different spiders in scrapy

raspi

[Decoupling single spider into different spiders in scrapy](https://stackoverflow.com/questions/59822233/decoupling-single-spider-into-different-spiders-in-scrapy)

I'd like to decouple parsing into a different spiders.Currently I have:And now split it into:So that if only one thing is updated I can only use OneThingSpiderSo where and how I call the OneThingSpider inside CategoriesSpider or pipeline?In CategoriesSpider:In pipeline:Same change in the CategoriesSpider as in attempt #1. Here we simply are trying to push the URLs to the OneThingSpider after CategoriesSpider has closed.In pipeline:The error I get is that the crawler is already crawling. So what is the correct way for decoupling spider into smaller spiders?Goal is that I can run:and also

2020-01-20 11:23:03Z

I'd like to decouple parsing into a different spiders.Currently I have:And now split it into:So that if only one thing is updated I can only use OneThingSpiderSo where and how I call the OneThingSpider inside CategoriesSpider or pipeline?In CategoriesSpider:In pipeline:Same change in the CategoriesSpider as in attempt #1. Here we simply are trying to push the URLs to the OneThingSpider after CategoriesSpider has closed.In pipeline:The error I get is that the crawler is already crawling. So what is the correct way for decoupling spider into smaller spiders?Goal is that I can run:and alsoUsually you'd use class inheritence for reusing spider code in multiple spiders:As a result you'd run scrapy crawl cakes to start crawling cake category and scrapy crawl vegetables to crawl vegetable category.

Difficulty in web-scraping data using scrapy

Uchit Madhok

[Difficulty in web-scraping data using scrapy](https://stackoverflow.com/questions/59782886/difficulty-in-web-scraping-data-using-scrapy)

I am trying to scrape data using scrapy from https://www.ta.com/portfolio/business-services, however the response is NULL. I am looking to scrape href in div.tiles js-portfolio-tiles using the code response.css("div.tiles.js-portfolio-tiles a::attr(href)").extract()

I think this has something to do with ::before that appears just before this, but maybe not. How do I go about extracting this?  website HTML

2020-01-17 07:27:43Z

I am trying to scrape data using scrapy from https://www.ta.com/portfolio/business-services, however the response is NULL. I am looking to scrape href in div.tiles js-portfolio-tiles using the code response.css("div.tiles.js-portfolio-tiles a::attr(href)").extract()

I think this has something to do with ::before that appears just before this, but maybe not. How do I go about extracting this?  website HTMLThe elements that you are interested in retrieving are loaded by your browser using javascript. By default scrapy is not able to load elements using javascript as it is not a browser, it simply retrieves the raw HTML.Scrapy shell is an invaluable tool for inspecting what is available in the response that scrapy receives.This set of commands will open the response in your default web browser:As you can see the js-portfolio tiles are not visible as they have not been loaded.I have had a look at the AJAX requests in the network panel of the developer tools and it appears that the information you require may be available in an XHR request. If it is not then you will need to use additional software to load the javascript, namely scrapy splash or selenium, I would advise exploring the AJAX (XHR) request first though as this will be much faster and easier.See this question for additional details on using your browsers dev tools to inspect AJAX requests.

Retrieve items text inside dropdown list xpath

Alexander

[Retrieve items text inside dropdown list xpath](https://stackoverflow.com/questions/59824490/retrieve-items-text-inside-dropdown-list-xpath)

I have a select like thisI want to get the items text from the drop-down list (Green, Black, White) using CSS selector or XpathI tried the followingfor some reason it only returned the first value 

2020-01-20 13:37:27Z

I have a select like thisI want to get the items text from the drop-down list (Green, Black, White) using CSS selector or XpathI tried the followingfor some reason it only returned the first value Xpath :To select all of the <options>, your CSS query will be:You are not selecting option elements, of which there are several, but the select element. You only show one select element in your input document.The output is a list with one single element, but it is not what you posted in your question (at least with scrapy 1.8.0): all the child option elements are included.Using the correct XPath expression from this existing answer:

scrapy get builtins.KeyError: -2

txn

[scrapy get builtins.KeyError: -2](https://stackoverflow.com/questions/59823388/scrapy-get-builtins-keyerror-2)

In my scrapy crawl i get this error sometimes builtins.KeyError: -2But what does it mean? I dont find anything about that in scrapyHere is the full log about thatbut what is that and how to fix it?

2020-01-20 12:33:48Z

In my scrapy crawl i get this error sometimes builtins.KeyError: -2But what does it mean? I dont find anything about that in scrapyHere is the full log about thatbut what is that and how to fix it?

How to change Proxy IP in scrapy when proxy ip has a short valid time

james

[How to change Proxy IP in scrapy when proxy ip has a short valid time](https://stackoverflow.com/questions/59784322/how-to-change-proxy-ip-in-scrapy-when-proxy-ip-has-a-short-valid-time)

I use short term valid proxy IP in Middleware to crawl webpage data. When the IP expired, I got below message. I think in process_response method to set new proxy ip,But It doesn't work(prcess_resonse don't be called).

What's wrong and How to do ? Thankssettings.py

2020-01-17 09:17:32Z

I use short term valid proxy IP in Middleware to crawl webpage data. When the IP expired, I got below message. I think in process_response method to set new proxy ip,But It doesn't work(prcess_resonse don't be called).

What's wrong and How to do ? Thankssettings.py

How do I capture the response of URL every-time new tab opens

Siddharth Pilli

[How do I capture the response of URL every-time new tab opens](https://stackoverflow.com/questions/59781705/how-do-i-capture-the-response-of-url-every-time-new-tab-opens)

I'm trying to develop a web-scrapping project, in which I am scrapping a website called Startup India, which you can use for connecting with startups. Here, I have clicked based on some filters I selected , and clicked on every startup and when I click on every startup, I have to go inside that startup and scrap it.  But I can't scrap the data because I'm not able to capture the response for scraping profiles in startup India.Code will be appreciated

2020-01-17 05:30:29Z

I'm trying to develop a web-scrapping project, in which I am scrapping a website called Startup India, which you can use for connecting with startups. Here, I have clicked based on some filters I selected , and clicked on every startup and when I click on every startup, I have to go inside that startup and scrap it.  But I can't scrap the data because I'm not able to capture the response for scraping profiles in startup India.Code will be appreciatedI have setup scrapy project and run  scrapy crawl product_spider and it gives URL of new tab open after clicking on an element.

Problems about Scrapy with Splash to extract data from a webpage whose elements are created by javascript

JoeyLyu

[Problems about Scrapy with Splash to extract data from a webpage whose elements are created by javascript](https://stackoverflow.com/questions/59746757/problems-about-scrapy-with-splash-to-extract-data-from-a-webpage-whose-elements)

Recently, I'm learning to use Scrapy with splash to crawl dynamic websites.Here is the content in my spider:However, the response returned by splash still is not same as the one I inspect in the browser.The settings for splash are correct, as I have test it on localhost:8050. Here is the content in my setting.pyThe output in powershell:The output in docker:I don't know what's wrong with the code. The elements are not shown in the final html code obtained by splash. Your advises will be highly appreciated.

2020-01-15 07:25:42Z

Recently, I'm learning to use Scrapy with splash to crawl dynamic websites.Here is the content in my spider:However, the response returned by splash still is not same as the one I inspect in the browser.The settings for splash are correct, as I have test it on localhost:8050. Here is the content in my setting.pyThe output in powershell:The output in docker:I don't know what's wrong with the code. The elements are not shown in the final html code obtained by splash. Your advises will be highly appreciated.While using SplashRequest we need to explicitly mention what we need from request. In your case, you want html which is created after js is rendered so you can use this Lua script in your request

Scrapy: send post in script way without Spider

Gqqnbig

[Scrapy: send post in script way without Spider](https://stackoverflow.com/questions/59688991/scrapy-send-post-in-script-way-without-spider)

In the script above, neither errback nor callback is called.What is the simplest way to send a post request to https://example.com using scrapy in python3. I'd like to avoid using a Spider class/object if possible.

2020-01-10 21:05:06Z

In the script above, neither errback nor callback is called.What is the simplest way to send a post request to https://example.com using scrapy in python3. I'd like to avoid using a Spider class/object if possible.use requestspip3 install requests

Access request in an item pipeline

kenshin

[Access request in an item pipeline](https://stackoverflow.com/questions/59748229/access-request-in-an-item-pipeline)

We have a validation item pipeline, and we want now to extend it so that one field of the item (there is only one type of items) is sometimes not required anymore.It could be done with a custom setting of the spider, but then it would apply to all the items of the spider; however, we want to control this on a per-item basis.If this were a spider middleware, we'd add a meta key to the request, and all would be well.As it is now, however, all I can think of would be either:Is there any way at all to get the request an item comes from also in the item pipeline?The general consensus in this related question is that it's not possible, but I would assume that it should be, because in the LogFormatter's function we do have a response parameter, and when I drop an item from a pipeline I do see the right request inside that response.

2020-01-15 09:16:58Z

We have a validation item pipeline, and we want now to extend it so that one field of the item (there is only one type of items) is sometimes not required anymore.It could be done with a custom setting of the spider, but then it would apply to all the items of the spider; however, we want to control this on a per-item basis.If this were a spider middleware, we'd add a meta key to the request, and all would be well.As it is now, however, all I can think of would be either:Is there any way at all to get the request an item comes from also in the item pipeline?The general consensus in this related question is that it's not possible, but I would assume that it should be, because in the LogFormatter's function we do have a response parameter, and when I drop an item from a pipeline I do see the right request inside that response.

Scrapy: Get some information from another page

Joey Coder

[Scrapy: Get some information from another page](https://stackoverflow.com/questions/59690727/scrapy-get-some-information-from-another-page)

I am trying to add the location for my scraped data from another page. Currently, I only get the following response while expecting e.g. "Paris, France"spider.py

2020-01-11 00:46:44Z

I am trying to add the location for my scraped data from another page. Currently, I only get the following response while expecting e.g. "Paris, France"spider.pyI could find a solution here.

why my default_scrapyd.conf is missing however i installed scrapyd and all scrapyd-client

Shahrukh Ijaz

[why my default_scrapyd.conf is missing however i installed scrapyd and all scrapyd-client](https://stackoverflow.com/questions/59629092/why-my-default-scrapyd-conf-is-missing-however-i-installed-scrapyd-and-all-scrap)

I'm trying to deploy my project to scrapyd using docker (python3.6, scrapy==1.7.0)

When I run command  scrapyd-client deploy staging -p puc 

I get the following error:

Packing version 1578400467

Deploying to project "puc" in http://3.80.58.122:6800/addversion.json

Server response (200):

2020-01-07 13:09:37Z

I'm trying to deploy my project to scrapyd using docker (python3.6, scrapy==1.7.0)

When I run command  scrapyd-client deploy staging -p puc 

I get the following error:

Packing version 1578400467

Deploying to project "puc" in http://3.80.58.122:6800/addversion.json

Server response (200):

Write connection status to csv

deelite

[Write connection status to csv](https://stackoverflow.com/questions/59644435/write-connection-status-to-csv)

I use a spider to crwal many websites from a list. I works as I need but now I additionally want to get the connection status. When running the spider I see some 404, some 301 or some DNS errors.How can I get the connection status into my csv?

2020-01-08 11:03:19Z

I use a spider to crwal many websites from a list. I works as I need but now I additionally want to get the connection status. When running the spider I see some 404, some 301 or some DNS errors.How can I get the connection status into my csv?Usetaken from Checking a url for a 404 error scrapyresp.getcode() would only work if urllib2 instead of scrapy.http.Response would be used which would not be the correct way.

Scrapy Spider Stopped Crawling

Aarón González

[Scrapy Spider Stopped Crawling](https://stackoverflow.com/questions/59655065/scrapy-spider-stopped-crawling)

I tried running a Spider on a .asp site that requires login authorization and some crawling to a different page within the same site. I managed to successfully login yesterday using my spider and was working on scraping the data with a different function, and when I ran the spider again after changing the las function, the spider stopped working. I have no idea what is going on, I'm fairly new with web scraping.

Here is the code:Here is the log: The code used to be able to try to scrape some data from the page that I wanted without success, but I believe that was only because I was using the wrong css selector. Now it just opens and closes without doing anything. 

2020-01-08 23:11:03Z

I tried running a Spider on a .asp site that requires login authorization and some crawling to a different page within the same site. I managed to successfully login yesterday using my spider and was working on scraping the data with a different function, and when I ran the spider again after changing the las function, the spider stopped working. I have no idea what is going on, I'm fairly new with web scraping.

Here is the code:Here is the log: The code used to be able to try to scrape some data from the page that I wanted without success, but I believe that was only because I was using the wrong css selector. Now it just opens and closes without doing anything. I solved it. I just changed the assignation of the url at the beginning. I had to create a new method as follows:

Unable to scrap data from crawlspider class using url getting from another class

Shadman Nashif

[Unable to scrap data from crawlspider class using url getting from another class](https://stackoverflow.com/questions/59599229/unable-to-scrap-data-from-crawlspider-class-using-url-getting-from-another-class)

My crawl spider is running but the spider gets closed before scraping. I am using the URL returned from another function of a class. Could be an import error.... not sure. can anyone help me out how to scrap that information successfully? Ipython Shell Output:Comparestring.py

  

2020-01-05 11:07:29Z

My crawl spider is running but the spider gets closed before scraping. I am using the URL returned from another function of a class. Could be an import error.... not sure. can anyone help me out how to scrap that information successfully? Ipython Shell Output:Comparestring.py

  

Scrapy Import Error: No Module named Scrapy

Shreyansh Jha

[Scrapy Import Error: No Module named Scrapy](https://stackoverflow.com/questions/59596997/scrapy-import-error-no-module-named-scrapy)

I installed scrapy using the command lineonce installed, I tried importing scrapy in one of my python projects but an error arises saying:What shall I do? Can't find a correct solution on Google yet!

2020-01-05 04:18:45Z

I installed scrapy using the command lineonce installed, I tried importing scrapy in one of my python projects but an error arises saying:What shall I do? Can't find a correct solution on Google yet!Could you use:sourceoutput:when I run:I get no error

pull specific data using scrapy in python (jupyter notebook)?

Seph77

[pull specific data using scrapy in python (jupyter notebook)?](https://stackoverflow.com/questions/59547101/pull-specific-data-using-scrapy-in-python-jupyter-notebook)

I am trying to run a spider using jupyter notebook to grab specific data - in this case, the result should grab "21 Servings" from the drop-down in the link.I have the following code: It runs fine but it doesn't grab anything so I am not sure if the mistake comes from the selector path or I am doing something else wrong? (When I tried any path had the same 0 outcome)Here is part of the message after running the spider.I hope anyone can find the issue.

2019-12-31 17:46:15Z

I am trying to run a spider using jupyter notebook to grab specific data - in this case, the result should grab "21 Servings" from the drop-down in the link.I have the following code: It runs fine but it doesn't grab anything so I am not sure if the mistake comes from the selector path or I am doing something else wrong? (When I tried any path had the same 0 outcome)Here is part of the message after running the spider.I hope anyone can find the issue.There is a problem with your selector:

it should be:because #skugroup-select does not contain desired thing.With this selector I get:in the file. and if you change '/n' to '\n' you will get what you are looking for:I recommend printing links, and advancing the selector step by step for debugging.

How to yield item only after all links have been followed in Scrapy?

kakarukeys

[How to yield item only after all links have been followed in Scrapy?](https://stackoverflow.com/questions/59599163/how-to-yield-item-only-after-all-links-have-been-followed-in-scrapy)

The original code:This is the standard way to follow links on a page. However it has a flaw: if there is an http error (e.g. 503) or connection error when following the 2nd URL, parse_details is never called, and yield item is never executed. And so all data is lost.Changed code:Changed code does not work, it seems yield item is immediately executed before parse_details is run (perhaps due to Twisted framework, this behavior is different from what's expected in asynio library) and so the item is always yielded with incomplete data.How to make sure the yield item is executed after all links are followed? regardless of success or failure. Is something likepossible?

2020-01-05 10:57:56Z

The original code:This is the standard way to follow links on a page. However it has a flaw: if there is an http error (e.g. 503) or connection error when following the 2nd URL, parse_details is never called, and yield item is never executed. And so all data is lost.Changed code:Changed code does not work, it seems yield item is immediately executed before parse_details is run (perhaps due to Twisted framework, this behavior is different from what's expected in asynio library) and so the item is always yielded with incomplete data.How to make sure the yield item is executed after all links are followed? regardless of success or failure. Is something likepossible?you can send the failed requests to a function (whenever an Error happens),yield the item from there.second Edit to yield the item

How to get depth of each crawler with Scrapy

Jeong Kim

[How to get depth of each crawler with Scrapy](https://stackoverflow.com/questions/59509790/how-to-get-depth-of-each-crawler-with-scrapy)

Is there a way to keep track of each crawler's depth?I am recursively crawling some websites.My setup is similar to the below code.This approach doesn't work.For example, I would want to record each crawler's activity as suchThank you in advance.

2019-12-28 09:52:35Z

Is there a way to keep track of each crawler's depth?I am recursively crawling some websites.My setup is similar to the below code.This approach doesn't work.For example, I would want to record each crawler's activity as suchThank you in advance.I think you are almost there. Please try this code.

Null output in scrapy after entering data in form

divesh jain

[Null output in scrapy after entering data in form](https://stackoverflow.com/questions/59511348/null-output-in-scrapy-after-entering-data-in-form)

I am trying to scrape a webpage which has a form and once entered, it fetches data in the same page.

The link to the page does not change after the button is clicked.



I am trying to scrape the data once the form is filled and here is my code:

The link to the website is: https://phpans.com/free-email-validator/

The after_enter function runs that means I think that the form is successfully submitted but then the output which I want does not appear.

Can anyone help me understand what is the mistake here in my code?



Any help would be appreciated and I don't want to use selenium. So, if anyone can help me scrape this page, it would be really helpful.





Also, I have made ROBOTSTXT_OBEY = False



So, robots.txt should not be the issue.



This is the output which I receive and there is no error and no output, please see where I have gone wrong.



As in the output, I am getting the name output as none which should be some value after entering data in the form and validating. Hence, I think some changes has to be made and if anyone can help me, it would be really great

Thanks in advance:)

2019-12-28 13:31:58Z

I am trying to scrape a webpage which has a form and once entered, it fetches data in the same page.

The link to the page does not change after the button is clicked.



I am trying to scrape the data once the form is filled and here is my code:

The link to the website is: https://phpans.com/free-email-validator/

The after_enter function runs that means I think that the form is successfully submitted but then the output which I want does not appear.

Can anyone help me understand what is the mistake here in my code?



Any help would be appreciated and I don't want to use selenium. So, if anyone can help me scrape this page, it would be really helpful.





Also, I have made ROBOTSTXT_OBEY = False



So, robots.txt should not be the issue.



This is the output which I receive and there is no error and no output, please see where I have gone wrong.



As in the output, I am getting the name output as none which should be some value after entering data in the form and validating. Hence, I think some changes has to be made and if anyone can help me, it would be really great

Thanks in advance:)

TypeError: cannot use a string pattern on a bytes-like object in Python

MrWinson

[TypeError: cannot use a string pattern on a bytes-like object in Python](https://stackoverflow.com/questions/59507910/typeerror-cannot-use-a-string-pattern-on-a-bytes-like-object-in-python)

I am making an email scraper using Scrapy and I keep getting this error:

TypeError: cannot use a string pattern on a bytes-like objectHere is my Python code I am using:I have seen a lot of answers but I am very new to python so Im not sure how I would implement the code given into my code.So if you don't mind could also tell me were to put the code in.Thanks, Jude Wilson

2019-12-28 03:39:38Z

I am making an email scraper using Scrapy and I keep getting this error:

TypeError: cannot use a string pattern on a bytes-like objectHere is my Python code I am using:I have seen a lot of answers but I am very new to python so Im not sure how I would implement the code given into my code.So if you don't mind could also tell me were to put the code in.Thanks, Jude Wilsonresponse._body is not a str(string object), so you cannot use re(regex) on it. If you look for its object type you will find out it is a bytes(bytes object).By decoding it to something like UTF-8 the problem should be solved.Final re would be like this:

Python Scrapy How to insert all dictionary items in SQL DB

Omar Kamel Mostafa

[Python Scrapy How to insert all dictionary items in SQL DB](https://stackoverflow.com/questions/59509320/python-scrapy-how-to-insert-all-dictionary-items-in-sql-db)



2019-12-28 08:36:26Z



Aliexpress.com Scrapy Crawler localStorage.x5referer

tugrulv

[Aliexpress.com Scrapy Crawler localStorage.x5referer](https://stackoverflow.com/questions/59601700/aliexpress-com-scrapy-crawler-localstorage-x5referer)

I need to crawl aliexpress search result data related to my search keyword inputs.Below my Scrapy code sample. I got localStorage.x5referer script in loging file. Maybe i need to handle some cookie things.

2020-01-05 16:07:29Z

I need to crawl aliexpress search result data related to my search keyword inputs.Below my Scrapy code sample. I got localStorage.x5referer script in loging file. Maybe i need to handle some cookie things.Any findings on the topic ? I had the same problem there.

XPath to get URL and name pattern

Joey Coder

[XPath to get URL and name pattern](https://stackoverflow.com/questions/59474123/xpath-to-get-url-and-name-pattern)

My goal is to get the following result(s). Currently, I have the following XPath:Html code

2019-12-24 23:25:49Z

My goal is to get the following result(s). Currently, I have the following XPath:Html codeXPathes for URL and event_name would be:You can use . to continue from current node which is event here.

XPath: preceding-sibling

Joey Coder

[XPath: preceding-sibling](https://stackoverflow.com/questions/59477056/xpath-preceding-sibling)

I wrote the following XPath and expected the previous td to be selected. However, it skips that one and selects the previous one + 1.

2019-12-25 09:38:36Z

I wrote the following XPath and expected the previous td to be selected. However, it skips that one and selects the previous one + 1.preceding-sibling::td selects all of the preceding td sibling elements.To select just the nearest one, use:The numerical index here counts from nearest to farthest. So the nearest is [1], second-nearest is [2], and so on.

Get what's behind “Location:” with XPath

Joey Coder

[Get what's behind “Location:” with XPath](https://stackoverflow.com/questions/59480654/get-whats-behind-location-with-xpath)

I try to get France that's behind Location:I wrote this XPath: //div[@class="vevent"]/div/div/span[text()="Location: "]. That's how far I came, but how to I get France that comes after.

2019-12-25 17:42:14Z

I try to get France that's behind Location:I wrote this XPath: //div[@class="vevent"]/div/div/span[text()="Location: "]. That's how far I came, but how to I get France that comes after.This should work:It selects the div that contains a span with the string value "Location: " and then retrieves the text node directly within that div.Better give the html in script. Not picture.

And I guess you can use:

'''

following::text()

'''

Scrapy signal or similar for finishing with one domain of a broad crawl?

Chris

[Scrapy signal or similar for finishing with one domain of a broad crawl?](https://stackoverflow.com/questions/59481935/scrapy-signal-or-similar-for-finishing-with-one-domain-of-a-broad-crawl)

I've got a spider which takes an argument (batch ID) and reads 50 domains with this batch ID from a CSV file and starts crawling them all. All is working fine there.Is there any signal or something from the scheduler, to find out, when one website is completely crawled/finished?

2019-12-25 21:21:40Z

I've got a spider which takes an argument (batch ID) and reads 50 domains with this batch ID from a CSV file and starts crawling them all. All is working fine there.Is there any signal or something from the scheduler, to find out, when one website is completely crawled/finished?

Extract text from children of next nodes with XPath and Scrapy

bolino

[Extract text from children of next nodes with XPath and Scrapy](https://stackoverflow.com/questions/59486176/extract-text-from-children-of-next-nodes-with-xpath-and-scrapy)

With Python Scrapy, I am trying to get contents in a webpage whose nodes look like this:I'm a newbie with XPath and couldn't get it for now. My last try was something like:... but it seems I cannot use /following-sibling after [@id="title"].Any idea?

2019-12-26 09:09:23Z

With Python Scrapy, I am trying to get contents in a webpage whose nodes look like this:I'm a newbie with XPath and couldn't get it for now. My last try was something like:... but it seems I cannot use /following-sibling after [@id="title"].Any idea?One XPath would be:which get text from every <p> tag child or grand child of nearest <ul> tag to node with id = "title".  XPath syntaxTry this XPathIt selects both "CONTENT TO EXTRACT" text nodes.Try this using css selector.response.css('#title ::text).extract()

Different response when I fetching and crawling url(scrapy)

Deepak Aggarwal

[Different response when I fetching and crawling url(scrapy)](https://stackoverflow.com/questions/59436048/different-response-when-i-fetching-and-crawling-urlscrapy)

I am trying to scrape the website. When I use, Scrapy shell with chrome user agent, I get the response(html content) same as viewed in browser.

But when I crawl same link using Scrapy script with default user_agent, it shows different response with content I didn't need. But when I change the user_agent to same used in shell, it shows 404 error.

Please help me. I am really stuck on it.

I tried many user agents. I also changed concurrent requests, but nothing is working. 

2019-12-21 12:07:58Z

I am trying to scrape the website. When I use, Scrapy shell with chrome user agent, I get the response(html content) same as viewed in browser.

But when I crawl same link using Scrapy script with default user_agent, it shows different response with content I didn't need. But when I change the user_agent to same used in shell, it shows 404 error.

Please help me. I am really stuck on it.

I tried many user agents. I also changed concurrent requests, but nothing is working. 

Scrapy: select texts of all children elements but returns children HTML as well

hydradon

[Scrapy: select texts of all children elements but returns children HTML as well](https://stackoverflow.com/questions/59430187/scrapy-select-texts-of-all-children-elements-but-returns-children-html-as-well)

I am writing a Scrapy spider to scrape this page, I only want the text of the element with class jam_content and all its descendants. So ideally I should get This is my selector response.css(".jam_content *::text").extract() :which returns even the HTML of the childrenI attempted another one response.xpath("./*[@class='jam_content']//text()"), which returns nothingHow do I do this?

2019-12-20 19:01:53Z

I am writing a Scrapy spider to scrape this page, I only want the text of the element with class jam_content and all its descendants. So ideally I should get This is my selector response.css(".jam_content *::text").extract() :which returns even the HTML of the childrenI attempted another one response.xpath("./*[@class='jam_content']//text()"), which returns nothingHow do I do this?Update your selector to not get the contents of the style element:You could then do a list comprehension, filtering out the items that are blank text with .strip():This will return:Then you can simply join it together:

Extract tile name from SAP Firoi Apps Library with Scrapy

Radinator

[Extract tile name from SAP Firoi Apps Library with Scrapy](https://stackoverflow.com/questions/59380362/extract-tile-name-from-sap-firoi-apps-library-with-scrapy)

I recently had to create a list of all SAP tiles every department uses. So I mad this the boring way and visited https://fioriappslibrary.hana.ondemand.com/sap/fix/externalViewer/ which is the main source of where to find the tile for a app you want to use.Since this is kinda long, slow and unpleasant way to do, I tried to take use of Scrapy, which I can use to scrap data from a webpage. When visiting the page with my browser and searching for a app, I can copy and past these information for every app I want. When inspecting the site with Chrome, I can see the HTML parts that contain the data I need. Using Scrapy and let it hunt for the data by changing the URL does not work at all. Every time I scrap a site, Scrapy fetches the start page and not the page I want.Has someone had similiar problems or an idea of how to "wait" for the right site to appear?

2019-12-17 18:48:50Z

I recently had to create a list of all SAP tiles every department uses. So I mad this the boring way and visited https://fioriappslibrary.hana.ondemand.com/sap/fix/externalViewer/ which is the main source of where to find the tile for a app you want to use.Since this is kinda long, slow and unpleasant way to do, I tried to take use of Scrapy, which I can use to scrap data from a webpage. When visiting the page with my browser and searching for a app, I can copy and past these information for every app I want. When inspecting the site with Chrome, I can see the HTML parts that contain the data I need. Using Scrapy and let it hunt for the data by changing the URL does not work at all. Every time I scrap a site, Scrapy fetches the start page and not the page I want.Has someone had similiar problems or an idea of how to "wait" for the right site to appear?

How can I determine the URL address of the parent page?

zviruga

[How can I determine the URL address of the parent page?](https://stackoverflow.com/questions/59380319/how-can-i-determine-the-url-address-of-the-parent-page)

I am trying to build a model of a web resource using data obtained using the scrapy framework. How can I determine the URL address of the parent page (from which page did the crawler get to the one that is currently being used for parsing)?

2019-12-17 18:44:48Z

I am trying to build a model of a web resource using data obtained using the scrapy framework. How can I determine the URL address of the parent page (from which page did the crawler get to the one that is currently being used for parsing)?

Css selector of parent text

Ahmad Arshi

[Css selector of parent text](https://stackoverflow.com/questions/59387113/css-selector-of-parent-text)

I want to get this figure $185,000,000. Is there any way to get text from parent tag and avoiding text from child tags

2019-12-18 07:14:28Z

I want to get this figure $185,000,000. Is there any way to get text from parent tag and avoiding text from child tagsYes you can do this. Simply writeresponse.css('.txt-block::text').extract_first() 

This will return only $185,000,000. If you put space between :: and .txt-block. This extract the text of children also

Scrapy , Crawlera Filtered duplicate request:

user3725741

[Scrapy , Crawlera Filtered duplicate request:](https://stackoverflow.com/questions/59386424/scrapy-crawlera-filtered-duplicate-request)

I'm using scrapy spider to crawl google search. i'm using Crawlera as well to avoid being banned and stuffs. it was working properly but suddenly i've started to get duplicate filter error. i digged up the internet and found adding dont_filter in request constructor and also in the settings file.. 

but the spider will just keep running infinitely and the logger will print crawled 0pages/0min .any help would be highly appreciated.

2019-12-18 06:16:35Z

I'm using scrapy spider to crawl google search. i'm using Crawlera as well to avoid being banned and stuffs. it was working properly but suddenly i've started to get duplicate filter error. i digged up the internet and found adding dont_filter in request constructor and also in the settings file.. 

but the spider will just keep running infinitely and the logger will print crawled 0pages/0min .any help would be highly appreciated.

Scrapy: how to use arguments for multiple search terms

karvis6974

[Scrapy: how to use arguments for multiple search terms](https://stackoverflow.com/questions/59383208/scrapy-how-to-use-arguments-for-multiple-search-terms)

I'm playing around with scrapy and now I'm trying to search different keywords, passing arguments from the command line tool.

Basically, I would like to define a keyword and the crawler should search URLs that contains this keyword.

This is how my command line looks:And this is my crawler:Unfortunately, it doesn't work... 

Help is welcome, thanks!I have found a way! It works for me:And the command:Thank you to all of you!

2019-12-17 22:47:56Z

I'm playing around with scrapy and now I'm trying to search different keywords, passing arguments from the command line tool.

Basically, I would like to define a keyword and the crawler should search URLs that contains this keyword.

This is how my command line looks:And this is my crawler:Unfortunately, it doesn't work... 

Help is welcome, thanks!I have found a way! It works for me:And the command:Thank you to all of you!

Xpath for Next Page - Scrapy

Sanga

[Xpath for Next Page - Scrapy](https://stackoverflow.com/questions/59391554/xpath-for-next-page-scrapy)

I am scraping Walmart website using Scrapy, and I am trying it out for one category but I have trouble nailing down an xpath for next page. Here is what I have, can someone help?Walmart URL to inspect: 

https://www.walmart.com/browse/electronics/tvs/3944_1060825_447913

2019-12-18 11:58:13Z

I am scraping Walmart website using Scrapy, and I am trying it out for one category but I have trouble nailing down an xpath for next page. Here is what I have, can someone help?Walmart URL to inspect: 

https://www.walmart.com/browse/electronics/tvs/3944_1060825_447913The process is to find the <li> tag after the active page link.  The active page link is found using //*[@class="active"], and so you would have to find the index of this particular active <li> in respect to that of all the actual <li> tags.  To find all the <li> tags, use this xpath //*[@class="paginator-list"]/li. As you could see in the aria-label class, it would say 'Page x of y selected'.  The x would be the index, and so you would have to just search for the x+1 page.  To find the values in the <li> or the aria-label class, you need to use the .text() and .extract() function.    This will do the trickI tried to extract all the li page tags using paginator list://*[@class="paginator-list"]/li. I do not get any results. This container with the paginators "midas-sponsored-container-middle-1", does not show up when I try to extract all the div id tags too. Not sure what I am missing. 

Javascript Rendering Issue in Scrapy-Splash

GeneX

[Javascript Rendering Issue in Scrapy-Splash](https://stackoverflow.com/questions/59392838/javascript-rendering-issue-in-scrapy-splash)

I was exploring Scrapy+Splash and ran into issue that SplashRequest is not rendering the javascript and is giving exact same response scrapy.Request.

The webpage I want to scrape is this. I want some fields from the webpage for my course project.I am unable to get the final HTML after js is rendered even after waiting for 'wait':'30'. In fact, the result is the same as scrapy.Request. The same code works perfectly for another website that I have tried ie. this. So I believe the settings are fine.This is spider definitionThe settings.py (only relevant part) file isAnd the output file is here I do not know what to make of the output, it has embedded JavaScript in it. Opening it in a browser tells that very little has been rendered (title only). How would I get rendered HTML for the website? Any help is much appreciated.

2019-12-18 13:11:38Z

I was exploring Scrapy+Splash and ran into issue that SplashRequest is not rendering the javascript and is giving exact same response scrapy.Request.

The webpage I want to scrape is this. I want some fields from the webpage for my course project.I am unable to get the final HTML after js is rendered even after waiting for 'wait':'30'. In fact, the result is the same as scrapy.Request. The same code works perfectly for another website that I have tried ie. this. So I believe the settings are fine.This is spider definitionThe settings.py (only relevant part) file isAnd the output file is here I do not know what to make of the output, it has embedded JavaScript in it. Opening it in a browser tells that very little has been rendered (title only). How would I get rendered HTML for the website? Any help is much appreciated.

Fatal error in launcher: unable to create process - scrapy

Thendral Nilavu

[Fatal error in launcher: unable to create process - scrapy](https://stackoverflow.com/questions/59391127/fatal-error-in-launcher-unable-to-create-process-scrapy)

I am using anaconda prompt for installing scrapy. I used the command 

Conda install -C conda-forge scrapy

The scrapy is installed by this but if I use any scrapy commands I am getting error, I am using python 3.6.5 version. 

If I look into the path that the fatal error is mentioning there is no python file for scrapy just an application file. How could I clear this errorI checked environment variable and it is fine.

2019-12-18 11:31:37Z

I am using anaconda prompt for installing scrapy. I used the command 

Conda install -C conda-forge scrapy

The scrapy is installed by this but if I use any scrapy commands I am getting error, I am using python 3.6.5 version. 

If I look into the path that the fatal error is mentioning there is no python file for scrapy just an application file. How could I clear this errorI checked environment variable and it is fine.

Running Scrapy code with it's shell with python Script

Mubeen Butt

[Running Scrapy code with it's shell with python Script](https://stackoverflow.com/questions/59343361/running-scrapy-code-with-its-shell-with-python-script)

i hope all of you are fine and doing well,scrapy runspider scrappyCode.py -o savingData.csvI want to execute this command within python Script

scrapy --> Library/Framework

runspider --> it will launch scrapy's spider

scrappyCode.py is my Code that contain scrapy commands , and savingData.csv is the file where all fetched data will be saved.

So i'm looking to run this command within a python script.

Why doing like this ? Because

MainFile code contain some code that will require data from savingData.csv , MainFile.py, scrappyCode.py and savingData.csv all are in same Directory, so MainFile will import scrappyCode or will execute the above command then MainFile will read savingData.csv.

2019-12-15 11:19:09Z

i hope all of you are fine and doing well,scrapy runspider scrappyCode.py -o savingData.csvI want to execute this command within python Script

scrapy --> Library/Framework

runspider --> it will launch scrapy's spider

scrappyCode.py is my Code that contain scrapy commands , and savingData.csv is the file where all fetched data will be saved.

So i'm looking to run this command within a python script.

Why doing like this ? Because

MainFile code contain some code that will require data from savingData.csv , MainFile.py, scrappyCode.py and savingData.csv all are in same Directory, so MainFile will import scrappyCode or will execute the above command then MainFile will read savingData.csv.

Can you use a boolean in a POST request in Scrapy?

dan dan

[Can you use a boolean in a POST request in Scrapy?](https://stackoverflow.com/questions/59297086/can-you-use-a-boolean-in-a-post-request-in-scrapy)

I tried to use a boolean in the dict for the formdata parameter in scrapy.FormRequest() because the POST request requires a boolean. Scrapy wouldn't let me use a boolean in the dict. So I tried to use the string "false" instead but it doesn't seem to work and I get this error after the POST request:Is there a way to use a boolean in the POST request? And am I getting the error because I'm not using the right formdata?

2019-12-12 02:59:40Z

I tried to use a boolean in the dict for the formdata parameter in scrapy.FormRequest() because the POST request requires a boolean. Scrapy wouldn't let me use a boolean in the dict. So I tried to use the string "false" instead but it doesn't seem to work and I get this error after the POST request:Is there a way to use a boolean in the POST request? And am I getting the error because I'm not using the right formdata?

How to get Result of Scrapy code in python script instead of writing it into a File?

Mubeen Butt

[How to get Result of Scrapy code in python script instead of writing it into a File?](https://stackoverflow.com/questions/59346195/how-to-get-result-of-scrapy-code-in-python-script-instead-of-writing-it-into-a-f)

I'm using Scrapy and also able to run it within python script using os.system(), Now instead of getting output in file, i want to receive the output within python script.and is there any better way to Execute scrapy code within python ?

2019-12-15 17:09:07Z

I'm using Scrapy and also able to run it within python script using os.system(), Now instead of getting output in file, i want to receive the output within python script.and is there any better way to Execute scrapy code within python ?Here you are trying to get the stdout of scrapy, to do you must use subprocess lib, check_output is easy but subprocess is a very complete lib, you may need other function depending of your need.If you receive nothing that very probably because your program doesn't call flush, so in this case try to use a pty with the option shell=True (pty is a bit costier than without, try to avoid using it).So finaly your code may looks like :

Can't get values on Json from exported data with scrapy

H.Senoussi

[Can't get values on Json from exported data with scrapy](https://stackoverflow.com/questions/59289786/cant-get-values-on-json-from-exported-data-with-scrapy)

I am running a process with two spiders to crawl a page, but I am getting an empty json output file.The code is as follows : Spiders.pyI have also edited my settings folder

settings.pyAnd added a json pieplinepipelines.py

2019-12-11 16:05:25Z

I am running a process with two spiders to crawl a page, but I am getting an empty json output file.The code is as follows : Spiders.pyI have also edited my settings folder

settings.pyAnd added a json pieplinepipelines.py

Download file from response header Content-Disposition without a file name - Python

Stephen Paulraj

[Download file from response header Content-Disposition without a file name - Python](https://stackoverflow.com/questions/59248763/download-file-from-response-header-content-disposition-without-a-file-name-pyt)

I am trying to scrap information from youtube. Where youtube uses infinite scroll, after every pull ajax calls up for more data. I am using scrapy on python, while i request to this url(with continuation token)'https://www.youtube.com/results?search_query=tamil&ctoken=xyz&continuation=xyz' i received the status 200 with the following header.I just need to download the response json. i can view the response in Chrome and firefox inspector. here is what i tried.I am getting error on this.What i am interested is, can i download the response as JSON file for further usage, by making use of content-disposition: attachment. If i need to download the response how can i implement.

2019-12-09 12:16:00Z

I am trying to scrap information from youtube. Where youtube uses infinite scroll, after every pull ajax calls up for more data. I am using scrapy on python, while i request to this url(with continuation token)'https://www.youtube.com/results?search_query=tamil&ctoken=xyz&continuation=xyz' i received the status 200 with the following header.I just need to download the response json. i can view the response in Chrome and firefox inspector. here is what i tried.I am getting error on this.What i am interested is, can i download the response as JSON file for further usage, by making use of content-disposition: attachment. If i need to download the response how can i implement.Try: where header should the response of ajax call.

Scraping search results with Scrapy and Selenium

nolczak

[Scraping search results with Scrapy and Selenium](https://stackoverflow.com/questions/59257222/scraping-search-results-with-scrapy-and-selenium)

This might be a long shot, but people have always been really helpful with the questions I've posted in the past so I'm gonna try. If anyone could help me, that would be amazing...   I'm trying to use Scrapy to get search results (links) after searching for a keyword on a Chinese online newspaper - pages like this When I inspect the html for the page in Chrome, the links to the articles seem to be there. But then when I try to grab it using a Scrapy spider, the html is much more basic and the links I want don't show up. I think this may be because the results are being drawn to the page using JavaScript? I've tried combining Scrapy with 'scrapy-selenium' to get round this, but it is still not working. I have heard Splash might work, but this seems complicated to set up. Here is the code for my Scrapy spider: I can also post any of the other Scrapy files, if that is helpful. I have also modified settings.py - following these instructions.Any help would be really appreciated. I'm completely stuck with this! 

2019-12-09 21:53:22Z

This might be a long shot, but people have always been really helpful with the questions I've posted in the past so I'm gonna try. If anyone could help me, that would be amazing...   I'm trying to use Scrapy to get search results (links) after searching for a keyword on a Chinese online newspaper - pages like this When I inspect the html for the page in Chrome, the links to the articles seem to be there. But then when I try to grab it using a Scrapy spider, the html is much more basic and the links I want don't show up. I think this may be because the results are being drawn to the page using JavaScript? I've tried combining Scrapy with 'scrapy-selenium' to get round this, but it is still not working. I have heard Splash might work, but this seems complicated to set up. Here is the code for my Scrapy spider: I can also post any of the other Scrapy files, if that is helpful. I have also modified settings.py - following these instructions.Any help would be really appreciated. I'm completely stuck with this! In inspect tool open network tab and watch requests you will find out the data is coming from this url, so crawl this instead with normal scrapy.Request().

spider would be like this:

Cryptography Upgrade Issue on OSX

Khanjan Desai

[Cryptography Upgrade Issue on OSX](https://stackoverflow.com/questions/59258162/cryptography-upgrade-issue-on-osx)

I am trying to install Scrapy, but I am running into an openssl error. All forums seem to suggest that the problem should be solved by updating cryptography, but I am also having an error upgrading it. I am running macOS Catalina 10.15.1. I've updated my xcode, pip, and brew. How do I fix this?

2019-12-09 23:34:26Z

I am trying to install Scrapy, but I am running into an openssl error. All forums seem to suggest that the problem should be solved by updating cryptography, but I am also having an error upgrading it. I am running macOS Catalina 10.15.1. I've updated my xcode, pip, and brew. How do I fix this?This issue is a complex interaction between a bunch of dependencies that ultimately breaks pip's ability to fetch things over TLS, but the easy fix is to use a browser to download the right wheel file from https://pypi.org/project/cryptography/#files and then type pip install /path/to/wheel/wheel-file.whl. As of this writing the wheel file that should work for you is cryptography-2.8-cp27-cp27m-macosx_10_6_intel.whl.

Relative xpath not working when used with 'dot' inside the loop

Ramsha Ilyas

[Relative xpath not working when used with 'dot' inside the loop](https://stackoverflow.com/questions/59219165/relative-xpath-not-working-when-used-with-dot-inside-the-loop)

I'm pretty new to Python and Scrapy. So I created a spider and I'm having issues with relative paths. If I don't use the 'dot' inside the loop it prints the same result as long as the loop runs but if I use the 'dot' inside the loop it shows that it has scraped but the text it blank.

2019-12-06 19:25:17Z

I'm pretty new to Python and Scrapy. So I created a spider and I'm having issues with relative paths. If I don't use the 'dot' inside the loop it prints the same result as long as the loop runs but if I use the 'dot' inside the loop it shows that it has scraped but the text it blank.//div[@class='top15'] predicate is extra in your for loop. You narrowed it down to it before you get into for loop. The spider would be:The items.py would be:And this is few lines of my log:

How to forcefully execute or render a script in browser from python in scraping?

Pritish

[How to forcefully execute or render a script in browser from python in scraping?](https://stackoverflow.com/questions/59217647/how-to-forcefully-execute-or-render-a-script-in-browser-from-python-in-scraping)

I am working on data scraping and machine learning. I am new to both Python and Scraping. I am trying to scrape this particular site.https://www.space-track.org/From what I have monitored they execute several scripts in between login and next page. Hence they get those table data. I am able to successfully login and then with session get the data from next page as well, what I am missing is getting that data which they get from executing script in between. I need the data from table and achieve pagination. Following is my codeAs you can see I have used requests_html library to render the html, but I have been unsuccessful in getting the data. This is the url executed in js internally which gets my dataCan anyone help me with how to scrape that data or javascript ?Thank you :)

2019-12-06 17:18:17Z

I am working on data scraping and machine learning. I am new to both Python and Scraping. I am trying to scrape this particular site.https://www.space-track.org/From what I have monitored they execute several scripts in between login and next page. Hence they get those table data. I am able to successfully login and then with session get the data from next page as well, what I am missing is getting that data which they get from executing script in between. I need the data from table and achieve pagination. Following is my codeAs you can see I have used requests_html library to render the html, but I have been unsuccessful in getting the data. This is the url executed in js internally which gets my dataCan anyone help me with how to scrape that data or javascript ?Thank you :)You can go for selenium. It has a function browser.execute_script(). This will help you to execute script. Hope this helps :)

Paginating and getting prices from a site using Scrapy

cetiberiojr

[Paginating and getting prices from a site using Scrapy](https://stackoverflow.com/questions/59217439/paginating-and-getting-prices-from-a-site-using-scrapy)

I started to look at Scrapy and want to have one spider to get some prices of MTG Cards.First I don't know if I'm 100% correct to use the link that select all the cards available in the beginning of the function:1 - Should I use this kind of start_urls?2 - Then, if you access the site, I could not find how to get the unit and price of the card, they are blank DIV's...I got the name using:3 - I couldn't find how can I do the pagination of this site to get the next set of items unit/prices. Do I need to copy the start_urls N times?

2019-12-06 17:03:52Z

I started to look at Scrapy and want to have one spider to get some prices of MTG Cards.First I don't know if I'm 100% correct to use the link that select all the cards available in the beginning of the function:1 - Should I use this kind of start_urls?2 - Then, if you access the site, I could not find how to get the unit and price of the card, they are blank DIV's...I got the name using:3 - I couldn't find how can I do the pagination of this site to get the next set of items unit/prices. Do I need to copy the start_urls N times?

Connection to the other side was lost in a non-clean fashion: Connection lost

Олександр Сусідський

[Connection to the other side was lost in a non-clean fashion: Connection lost](https://stackoverflow.com/questions/59227967/connection-to-the-other-side-was-lost-in-a-non-clean-fashion-connection-lost)

I'm trying to create Scrapy-app, when the app execute the function 'parse' it goes well(status=200), but when it call 'parse_phone' it can't get that url and logs with the errors.

When i execute 'scrapy shell 'url that is in parse_phone' it executes without problems. 

Can anyone answer what is solution?I tried with User-agent but it doesn't change the situation.

2019-12-07 16:20:21Z

I'm trying to create Scrapy-app, when the app execute the function 'parse' it goes well(status=200), but when it call 'parse_phone' it can't get that url and logs with the errors.

When i execute 'scrapy shell 'url that is in parse_phone' it executes without problems. 

Can anyone answer what is solution?I tried with User-agent but it doesn't change the situation.

Pip Error “Import Error: No Module Named Site”

Christopher Ell

[Pip Error “Import Error: No Module Named Site”](https://stackoverflow.com/questions/59225407/pip-error-import-error-no-module-named-site)

I am trying to install Scrapy on my computer and it has been a while since I used pip to install any libraries. Whenever I type:into my command prompt on Windows 10 I get the error message:Doing a search I have found the below link:ImportError: No module named site on Windowswith a similar issue and gone to Advanced Systems Settings and Enviromental variables, adding PYTHONHOME with C:\Python38 and PYTHONPATH with C:\Python38\Lib and C:\Python38\Scripts to both user and system variables. But I am still getting the same error message. I have also uninstall Python and Anaconda and reinstalled them. Does anyone know anything else I can try or any other reason pip may not be working?

2019-12-07 11:16:41Z

I am trying to install Scrapy on my computer and it has been a while since I used pip to install any libraries. Whenever I type:into my command prompt on Windows 10 I get the error message:Doing a search I have found the below link:ImportError: No module named site on Windowswith a similar issue and gone to Advanced Systems Settings and Enviromental variables, adding PYTHONHOME with C:\Python38 and PYTHONPATH with C:\Python38\Lib and C:\Python38\Scripts to both user and system variables. But I am still getting the same error message. I have also uninstall Python and Anaconda and reinstalled them. Does anyone know anything else I can try or any other reason pip may not be working?You may use this way to install scrapy on your windows machine.From http://doc.scrapy.org/en/latest/intro/install.html Though it’s possible to install Scrapy on Windows using pip, we recommend you to install Anaconda or Miniconda and use the package from the conda-forge channel, which will avoid most installation issues.Once you’ve installed Anaconda or Miniconda, install Scrapy with:conda install -c conda-forge scrapyI finally have got scrapy installed on my computer and below I outline the things I had to fix and tried for anyone else having similar issues. Please feel free to correct me or inform me of any redundant steps I may includeThanks to yabberth for answering my question his method now works on my computer, but there were a few other things I had to fix up first to get things working (many of which seem obvious now in hindsight).Environmental VariablesMy environmental variables were not set up properly for a few reasons.to access environmental variables I just search for "View Advanced System settings" in windows 10, I then click on Environmental variables. Then I see "User Variables" and "System Variables" both of which have a field called path. path is the field I will be modifying so I double click it on both and am presented with a list of paths.But it still wasn't working so:After completing the above two steps I was able to install scrapy using windows command prompt using:or as yabberth suggested on anaconda with 

Cannot scrap AliExpress HTML element

Maciej Tułaza

[Cannot scrap AliExpress HTML element](https://stackoverflow.com/questions/59228121/cannot-scrap-aliexpress-html-element)

I would like to scrap an arbitrary offer from aliexpress. Im trying to use scrapy and selenium. The issue I face is that when I use chrome and do right click > inspect on a element I see the real HTML but when I do right click > view source I see something different - a HTML CSS and JS mess all around. As far as I understand the content is pulled asynchronously? I guess this is the reason why I cant find the element I am looking for on the page.I was trying to use selenium to load the page first and then get the content I want but failed. I'm trying to scroll down to get to reviews section and get its contentIs this some advanced anti-bot solution that they have or maybe my approach is wrong?The code that I currently have:

2019-12-07 16:38:52Z

I would like to scrap an arbitrary offer from aliexpress. Im trying to use scrapy and selenium. The issue I face is that when I use chrome and do right click > inspect on a element I see the real HTML but when I do right click > view source I see something different - a HTML CSS and JS mess all around. As far as I understand the content is pulled asynchronously? I guess this is the reason why I cant find the element I am looking for on the page.I was trying to use selenium to load the page first and then get the content I want but failed. I'm trying to scroll down to get to reviews section and get its contentIs this some advanced anti-bot solution that they have or maybe my approach is wrong?The code that I currently have:By watching requests in network tab in inspect tool of browser you will find out comments are comming from here so you can crawl this page instead.

Writing scraping stats to a database using scrapy StatsCollector

siblackburn

[Writing scraping stats to a database using scrapy StatsCollector](https://stackoverflow.com/questions/59141363/writing-scraping-stats-to-a-database-using-scrapy-statscollector)

I'm completely new to Stackoverflow and the world of coding so apologies in advance if this question is formatted poorly!I've built spiders that crawl websites for key information then write the output to a database. I'd now like to write some key stats from each scrape to a separate table in the database.I've tried using scrapy Stats Collector but so far haven't been able to get anything written to the database, and I'm struggling to find any examples of this. Can anyone help in pointing me in the right direction? Below is an example trying to write just one column (total entries that the spider found during a scrape). The crawler is successful, but my statscollector isn't writing anything. Specifically I'd love to know:1) Am I approaching this in the right way, particularly as I add more spiders and more complexity to the project2) How do I only write to the database one stat per crawl (e.g. total number of products found). Currently I believe it's trying to write to the database each time parse_main_item is called, which would be dozens of times for dozens of sub categories. Thanks in advance for your help!Finally, here's the return I'm getting in my logs

2019-12-02 14:50:13Z

I'm completely new to Stackoverflow and the world of coding so apologies in advance if this question is formatted poorly!I've built spiders that crawl websites for key information then write the output to a database. I'd now like to write some key stats from each scrape to a separate table in the database.I've tried using scrapy Stats Collector but so far haven't been able to get anything written to the database, and I'm struggling to find any examples of this. Can anyone help in pointing me in the right direction? Below is an example trying to write just one column (total entries that the spider found during a scrape). The crawler is successful, but my statscollector isn't writing anything. Specifically I'd love to know:1) Am I approaching this in the right way, particularly as I add more spiders and more complexity to the project2) How do I only write to the database one stat per crawl (e.g. total number of products found). Currently I believe it's trying to write to the database each time parse_main_item is called, which would be dozens of times for dozens of sub categories. Thanks in advance for your help!Finally, here's the return I'm getting in my logs

How do I overwrite the file_path function in scrapy 1.7.3?

Isaac Ng

[How do I overwrite the file_path function in scrapy 1.7.3?](https://stackoverflow.com/questions/59222824/how-do-i-overwrite-the-file-path-function-in-scrapy-1-7-3)

Without overwriting the file_path function, the spider download all the images with the default 'request URL hash' filenames. However when I try to overwrite the function it just doesn't work. There is nothing in the default output attribute, images.I have tried both relative and absolute paths for the IMAGES_STORE variable in settings.py as well as the file_path function to no avail. Even when I overwrite the file_path function with the exact same default file_path function, the images do not download.Any help would be much appreciated!settings.pypipelines.pyAppScrape2.pyitems.py

2019-12-07 03:51:21Z

Without overwriting the file_path function, the spider download all the images with the default 'request URL hash' filenames. However when I try to overwrite the function it just doesn't work. There is nothing in the default output attribute, images.I have tried both relative and absolute paths for the IMAGES_STORE variable in settings.py as well as the file_path function to no avail. Even when I overwrite the file_path function with the exact same default file_path function, the images do not download.Any help would be much appreciated!settings.pypipelines.pyAppScrape2.pyitems.pyAfter much trial and error, I found the solution. It was simply adding the rest of the parameters to the file_path method.ChangingtoIt seems that the my original code overrode the method incorrectly causing calls to the method to fail.

Empty lists as outputs while scrapping with scrapy the html elements

Idris Stack

[Empty lists as outputs while scrapping with scrapy the html elements](https://stackoverflow.com/questions/59096883/empty-lists-as-outputs-while-scrapping-with-scrapy-the-html-elements)

Am trying to get data from this website , but am getting empty lists with scrapy. I used selector gadget ,to get the class names of the elements. So I checked the robots.txt file of the website and the link am accessing was prohibited.Then, I used User-Agent to bypass the restrictions, but still am wondering why am getting empty lists when I extract the elements.Below is my Spider class :Then, in my settings.py I have the following :So, when , I run the application, this is the final dictionary I get :So, what am I doing wrong ?

2019-11-28 22:40:14Z

Am trying to get data from this website , but am getting empty lists with scrapy. I used selector gadget ,to get the class names of the elements. So I checked the robots.txt file of the website and the link am accessing was prohibited.Then, I used User-Agent to bypass the restrictions, but still am wondering why am getting empty lists when I extract the elements.Below is my Spider class :Then, in my settings.py I have the following :So, when , I run the application, this is the final dictionary I get :So, what am I doing wrong ?You wanna crawl this instead of this because it loads data from that one so you get empty lists because the data you want does not exists in that page.Before creating your spider it's beneficial to check website with scrapy shell by view(response) to find out what you are getting when you request.

Scrapy ignores requests

ManosL

[Scrapy ignores requests](https://stackoverflow.com/questions/59123275/scrapy-ignores-requests)

I am writing a Spider and I have this code because probably the response might not be 200(OK)and when I try the parse method individually(imagine function returns None and does not yield a request) this works but when I have it as above when one of the start_urls returns 404 the crawler does not try to redo the request.Furthermore the callback at fighter_parse method works normally. Why this happens and how can I fix it?

2019-12-01 07:00:00Z

I am writing a Spider and I have this code because probably the response might not be 200(OK)and when I try the parse method individually(imagine function returns None and does not yield a request) this works but when I have it as above when one of the start_urls returns 404 the crawler does not try to redo the request.Furthermore the callback at fighter_parse method works normally. Why this happens and how can I fix it?There are built-in Scrapy features for that: RETRY_HTTP_CODES and RETRY_TIMES

Crawler - Assign empty to missing tag in html code

Costa.Gustavo

[Crawler - Assign empty to missing tag in html code](https://stackoverflow.com/questions/59117646/crawler-assign-empty-to-missing-tag-in-html-code)

I'm developing a scrapy web crawler, based on this tutorial, where I should get the news title, subtitle and image's URL of this site. The image below depicts the part I'm crawling:The problem is that some news has no subtitle or image, that is, within the feed-post-body tag there are no feed-post-body-summary tags or picture/img/@src. Thus, the lists I create with each attribute (title, subtitle, and image) are in the wrong order, that is, in my json file I have news with the title of one article and subtitle of another. Below is my scrapy web parse function:I am iterating over feed-post-body items, but in cases where there is no subtitle or image, I can't add empty to the list. How to solve it?

2019-11-30 15:36:55Z

I'm developing a scrapy web crawler, based on this tutorial, where I should get the news title, subtitle and image's URL of this site. The image below depicts the part I'm crawling:The problem is that some news has no subtitle or image, that is, within the feed-post-body tag there are no feed-post-body-summary tags or picture/img/@src. Thus, the lists I create with each attribute (title, subtitle, and image) are in the wrong order, that is, in my json file I have news with the title of one article and subtitle of another. Below is my scrapy web parse function:I am iterating over feed-post-body items, but in cases where there is no subtitle or image, I can't add empty to the list. How to solve it?

Web scraping using Scrapy adding extra elements during scraping process

Joe Dobrow

[Web scraping using Scrapy adding extra elements during scraping process](https://stackoverflow.com/questions/59122001/web-scraping-using-scrapy-adding-extra-elements-during-scraping-process)

I'm scraping a website looking for paragraphs in a specific place over a large amount of URLs. What I would like to do is record the URL I scraped 'next' to the scraped paragraph in a csv file for each URL I am visiting.First I am making a list of all the websites I want to scrape using the search syntax for the website. I am searching for books by ISBN number. What I am currently yielding is a list of scraped paragraphs just like I wanted...However it is occasionally not working, and so I can't simply concatenate the scraped paragraphs with the list of ISBNs that I have after the fact because they don't line up perfectly.I tried putting some code inside the 'yield' brackets to no avail. Any ideas, or other scrapy suggestions?

2019-12-01 02:16:20Z

I'm scraping a website looking for paragraphs in a specific place over a large amount of URLs. What I would like to do is record the URL I scraped 'next' to the scraped paragraph in a csv file for each URL I am visiting.First I am making a list of all the websites I want to scrape using the search syntax for the website. I am searching for books by ISBN number. What I am currently yielding is a list of scraped paragraphs just like I wanted...However it is occasionally not working, and so I can't simply concatenate the scraped paragraphs with the list of ISBNs that I have after the fact because they don't line up perfectly.I tried putting some code inside the 'yield' brackets to no avail. Any ideas, or other scrapy suggestions?If you want to get an URL:

Can't parse customized results without using requests within scrapy

robots.txt

[Can't parse customized results without using requests within scrapy](https://stackoverflow.com/questions/59126853/cant-parse-customized-results-without-using-requests-within-scrapy)

I've created a script using scrapy to fetch all the links connected to the name of different actors from imdb.com and then parse the first three of their movie links and finally scrape the name of director and writer of those movies. My script does it flawlessly if I stick to the current attempt. However, I've used requests module (which I don't want to) within parse_results method to get the customized output.website addressWhat the script does (consider the first named link, as in Robert De Niro):This is I've written so far (working one):Output the above script produces (desired ones):In the above approach I used requests module within parse_results method to get the desired output as I can't use yield within any list comprehension.How can let the script produce the exact output without using requests?

2019-12-01 15:13:42Z

I've created a script using scrapy to fetch all the links connected to the name of different actors from imdb.com and then parse the first three of their movie links and finally scrape the name of director and writer of those movies. My script does it flawlessly if I stick to the current attempt. However, I've used requests module (which I don't want to) within parse_results method to get the customized output.website addressWhat the script does (consider the first named link, as in Robert De Niro):This is I've written so far (working one):Output the above script produces (desired ones):In the above approach I used requests module within parse_results method to get the desired output as I can't use yield within any list comprehension.How can let the script produce the exact output without using requests?One way you can address this is using Request.meta to keep a list of pending URLs for an item across requests, and pop URLs from it.As @pguardiario mentions, the drawback is that you are still only processing one request from that list at a time. However, if you have more items than configured concurrency, that should not be a problem.This approach would look like this:

Select the HTML element and return it as printed text on Lua_script

Givenson Mwandla

[Select the HTML element and return it as printed text on Lua_script](https://stackoverflow.com/questions/59069314/select-the-html-element-and-return-it-as-printed-text-on-lua-script)

I'm using Splash to scrape the Jsweb, I just need to select the text and return it back as the text on my lua_script. Please see what I've been doing below but its doesn't work.Below is the HTML code I working on:-The code suppose to return Page 1 of 303 as the text I'm looking for and the screenshot of the page.

2019-11-27 11:35:26Z

I'm using Splash to scrape the Jsweb, I just need to select the text and return it back as the text on my lua_script. Please see what I've been doing below but its doesn't work.Below is the HTML code I working on:-The code suppose to return Page 1 of 303 as the text I'm looking for and the screenshot of the page.

using scrapy, how to get value from inside a tag

Gaurav Marwah

[using scrapy, how to get value from inside a tag](https://stackoverflow.com/questions/59062778/using-scrapy-how-to-get-value-from-inside-a-tag)

from the following source code:Using Scrapy, how do i get the value AUD $867.64

2019-11-27 03:56:48Z

from the following source code:Using Scrapy, how do i get the value AUD $867.64If you only need to get that price you have mentioned you can do it by xpath selector like this.Otherwise you can play around with xpath to get all prices. The output would be a list object.

Can't login with Scrapy

IK KLX

[Can't login with Scrapy](https://stackoverflow.com/questions/59068101/cant-login-with-scrapy)

I am trying to scrape information about items from a website that requires me to login to my personal account. I am using the spider below. It's based on a tutorial and worked fine in tests on other websites.It seems to work fine. However, it doesn't logged into the site.I am new to scrapy and read a lot of posts on login issues but can't really make sense of what is happening. Any pointers to what is causing this would be hugely appreciated.the spider:The command line output:The most interesting result from that output is redirecting to the link.as link https://members.lapada.org/home> doesn't exist.

2019-11-27 10:31:05Z

I am trying to scrape information about items from a website that requires me to login to my personal account. I am using the spider below. It's based on a tutorial and worked fine in tests on other websites.It seems to work fine. However, it doesn't logged into the site.I am new to scrapy and read a lot of posts on login issues but can't really make sense of what is happening. Any pointers to what is causing this would be hugely appreciated.the spider:The command line output:The most interesting result from that output is redirecting to the link.as link https://members.lapada.org/home> doesn't exist.

Python Scrapy - Pass scraped data from first URL to second URL and scrape data

pthrust

[Python Scrapy - Pass scraped data from first URL to second URL and scrape data](https://stackoverflow.com/questions/59070752/python-scrapy-pass-scraped-data-from-first-url-to-second-url-and-scrape-data)

I have looked at several posts on StackOverflow but still, do not quite understand how I can do this. In Scrapy, I am scraping books from one URL. For each record of the scraped books, I want to pass that to another website's search-field and get a certain element from the website. However, Scrapy seems to be stuck on the first website, and not retrieving results from the second. (I am trying basically to replicate in Scrapy what could be very easily done with Selenium - it's just slower with Selenium& BS).Sorry if my question is to basic, but if someone could point to a link in the documentation, or a sample that would be great!

2019-11-27 12:55:25Z

I have looked at several posts on StackOverflow but still, do not quite understand how I can do this. In Scrapy, I am scraping books from one URL. For each record of the scraped books, I want to pass that to another website's search-field and get a certain element from the website. However, Scrapy seems to be stuck on the first website, and not retrieving results from the second. (I am trying basically to replicate in Scrapy what could be very easily done with Selenium - it's just slower with Selenium& BS).Sorry if my question is to basic, but if someone could point to a link in the documentation, or a sample that would be great!

When i run the scrapy its shows an error like __import__(name) ImportError: No module named home in ubuntu

shanto thomas

[When i run the scrapy its shows an error like __import__(name) ImportError: No module named home in ubuntu](https://stackoverflow.com/questions/59004967/when-i-run-the-scrapy-its-shows-an-error-like-import-name-importerror-no-m)

when i run the scrapy it shows an error like, import(name) ImportError: No module named home,how to solve this issue?

2019-11-23 05:55:47Z

when i run the scrapy it shows an error like, import(name) ImportError: No module named home,how to solve this issue?

Scrapy Request works but not SplashRequest

Dayne Jones

[Scrapy Request works but not SplashRequest](https://stackoverflow.com/questions/59019601/scrapy-request-works-but-not-splashrequest)

I'm doing a very simple GET request with splash. The splash debug page and using scrapy.Request work fine. When I try to use scrapy_splash.SplashRequest, I get an unrendered page with an empty  tag.Code that works:Code that doesn't work:settings.py has the default setup as suggested by the scrapy-splash projecthere's my SPLASH_URL setting:

2019-11-24 16:04:18Z

I'm doing a very simple GET request with splash. The splash debug page and using scrapy.Request work fine. When I try to use scrapy_splash.SplashRequest, I get an unrendered page with an empty  tag.Code that works:Code that doesn't work:settings.py has the default setup as suggested by the scrapy-splash projecthere's my SPLASH_URL setting:Using the output of Splash server, I determined that SplashRequest "magically" sets headers on your request. User-Agent included.Changing my code to the following fixes my problem:

How to solve the issue ,from scrapy_djangoitem import DjangoItem ImportError: No module named scrapy_djangoitem

shanto thomas

[How to solve the issue ,from scrapy_djangoitem import DjangoItem ImportError: No module named scrapy_djangoitem](https://stackoverflow.com/questions/58955618/how-to-solve-the-issue-from-scrapy-djangoitem-import-djangoitem-importerror-no)

Found an error in my programm ,I had installed the pip install scrapy-djangoitem and also import the from scrapy_djangoitem import DjangoItem,then issue is found like from scrapy_djangoitem import DjangoItem ImportError: No module named scrapy_djangoitem

2019-11-20 13:33:07Z

Found an error in my programm ,I had installed the pip install scrapy-djangoitem and also import the from scrapy_djangoitem import DjangoItem,then issue is found like from scrapy_djangoitem import DjangoItem ImportError: No module named scrapy_djangoitemYou must install scrapy-djangoitem first:

Why do my Scrapy rules (LinkExtractor) don't work?

bcurtain

[Why do my Scrapy rules (LinkExtractor) don't work?](https://stackoverflow.com/questions/58953476/why-do-my-scrapy-rules-linkextractor-dont-work)

This is my first question ever in Stack Overflow. I started using Python to scrap data at work and I have been using Scrapy to achieve these tasks. I tried setting up a scraper for a government website and I do not have an output. Initially I set three rules in my rules variable, but my json file would come up empty. The code is fine but I do not know what is going wrong. Thank you for any insight that you are able to share. Have a good day. 

2019-11-20 11:38:23Z

This is my first question ever in Stack Overflow. I started using Python to scrap data at work and I have been using Scrapy to achieve these tasks. I tried setting up a scraper for a government website and I do not have an output. Initially I set three rules in my rules variable, but my json file would come up empty. The code is fine but I do not know what is going wrong. Thank you for any insight that you are able to share. Have a good day. Try to rename your parse function into parse_item:

Getting DEBUG: Crawled (504) error with Lua script While loop timing out for Scrapy-Splash

gigihar

[Getting DEBUG: Crawled (504) error with Lua script While loop timing out for Scrapy-Splash](https://stackoverflow.com/questions/59027019/getting-debug-crawled-504-error-with-lua-script-while-loop-timing-out-for-scr)

I am very new to coding and am struggling with a web scrapper I am trying to build. I am using a Lua script in order for my scrapy request to wait for any web-element (don't care about which element I just need the initial page loader to finish loading so I can access the html elements) to appear after the JavaScript on the website has loaded.  The particular website I am trying to access is https://www.ladbrokes.com.au/sports/basketball/usa/nba where it has a JS initial loader page before any of the elements on the website are loadedmy current code is this:When I call my spider I end up getting these errors:It seems it is timing out on the Lua script While loop, but I am not sure if it is because I am trying to select the web-element incorrectly. I also tried putting in a long splash wait argument in the SplashRequest function, but it seemed the initial page loader never finished loading. Any help on this would be great!

2019-11-25 07:29:43Z

I am very new to coding and am struggling with a web scrapper I am trying to build. I am using a Lua script in order for my scrapy request to wait for any web-element (don't care about which element I just need the initial page loader to finish loading so I can access the html elements) to appear after the JavaScript on the website has loaded.  The particular website I am trying to access is https://www.ladbrokes.com.au/sports/basketball/usa/nba where it has a JS initial loader page before any of the elements on the website are loadedmy current code is this:When I call my spider I end up getting these errors:It seems it is timing out on the Lua script While loop, but I am not sure if it is because I am trying to select the web-element incorrectly. I also tried putting in a long splash wait argument in the SplashRequest function, but it seemed the initial page loader never finished loading. Any help on this would be great!

How to get href value from a link using its class name with CSS selector in scrapy?

Pakshadow

[How to get href value from a link using its class name with CSS selector in scrapy?](https://stackoverflow.com/questions/58980258/how-to-get-href-value-from-a-link-using-its-class-name-with-css-selector-in-scra)

<a class="a-link-normal a-text-normal" href="/Art-Dutch-Republic-1585-Everyman/dp/0297833693/ref=sr_1_1?keywords=9780297833697&amp;qid=1574351815&amp;sr=8-1">

   <span class="a-size-medium a-color-base a-text-normal">Art of the Dutch Republic 1585 - 1718 (Everyman Art Library)</span>

</a>How to get Value of href using CSS selector or Xpath?

2019-11-21 17:11:40Z

<a class="a-link-normal a-text-normal" href="/Art-Dutch-Republic-1585-Everyman/dp/0297833693/ref=sr_1_1?keywords=9780297833697&amp;qid=1574351815&amp;sr=8-1">

   <span class="a-size-medium a-color-base a-text-normal">Art of the Dutch Republic 1585 - 1718 (Everyman Art Library)</span>

</a>How to get Value of href using CSS selector or Xpath?Here is an example:CSS selectors example:Try this response.css('.a-link-normal ::attr(href)').extract()

Python web-scraping of Expedia, how to find right keyword

YingMing Wang

[Python web-scraping of Expedia, how to find right keyword](https://stackoverflow.com/questions/58959185/python-web-scraping-of-expedia-how-to-find-right-keyword)

I am learning about Python scraper. I take an simple exercise about find the cheap ticket in Expedia. Now, I meet some problems about how to find the right selector or accurate keyword. I use functions like select() and find(). I took too many tests about them but I still did it successfully. I always get empty list. How can I find the right selector or keywork in a better method?

There is a part of my code. In it, I try to find the location of the input of Place: Flying from and the button Roundtrip.  

2019-11-20 16:33:12Z

I am learning about Python scraper. I take an simple exercise about find the cheap ticket in Expedia. Now, I meet some problems about how to find the right selector or accurate keyword. I use functions like select() and find(). I took too many tests about them but I still did it successfully. I always get empty list. How can I find the right selector or keywork in a better method?

There is a part of my code. In it, I try to find the location of the input of Place: Flying from and the button Roundtrip.  Searching a part of page you need is easier with using browser developer tools, for example Chrome Ctrl + Shift + C.You can search manually in page source code in browser or in Your python request probably got 'We can't tell if you're a human or a bot.' page instead of normal expedia page. You can try to reuse cookies from your browser with your requests session to get proper pages.When you found node you needed visually you look for it's name and attributes, also for parent's name and attributes.Roundtrip exampleYour approach is fundamentally flawed. Most of today's websites, including Expedia, are heavily JavaScript-based. The data you want may not even render on the page when you fetch it this way. You probably want to use a framework similar to Puppeteer which emulates the entire browser. A simple Python-based library will not be able to execute on-page JavaScript like your browser does. If you want to stick to Python, there may be a Puppeteer wrapper, but you'd have a much easier time just using Puppeteer and JS directly.

Trying to select div tag with id not working in scrapy

Nirmal Patel

[Trying to select div tag with id not working in scrapy](https://stackoverflow.com/questions/58893945/trying-to-select-div-tag-with-id-not-working-in-scrapy)

I am new at scraping with scrapy. I was trying to scrape an item on Amazon and for that I tried to scrape div with id but I did not get any output but it returned nothing. I have even tried more divs to select with id but I guess I am doing it the wrong way it was returning an empty list.

here is the full statement:this one is also not working. Can anyone help me? I think I am making a mistake somewhere.I have tried with Beautiful Soup but not working....

2019-11-16 18:35:02Z

I am new at scraping with scrapy. I was trying to scrape an item on Amazon and for that I tried to scrape div with id but I did not get any output but it returned nothing. I have even tried more divs to select with id but I guess I am doing it the wrong way it was returning an empty list.

here is the full statement:this one is also not working. Can anyone help me? I think I am making a mistake somewhere.I have tried with Beautiful Soup but not working....You can do this using css selector in an easy way response.css("#widgetContent ::text"). 

This will also work response.xpath('//div[(@id="widgetContent")]) 

DEBUG: CRAWLED (200) (referer: None)

Thắng Cao

[DEBUG: CRAWLED (200) (referer: None)](https://stackoverflow.com/questions/58893151/debug-crawled-200-referer-none)

I'm trying to crawl a webpage using Scrapy and XPath. Here are my code and logs, can someone help me. Thanks in advance!

2019-11-16 17:06:19Z

I'm trying to crawl a webpage using Scrapy and XPath. Here are my code and logs, can someone help me. Thanks in advance!If you open the source of the start_urls using ctrl/cmd + U, you will be unable to find questions class and questions list will be empty, which results in skipping the for loop in parse method and thus you are not getting your desired results. Moreover answers is also not available in the source of the webpage as well. Thus all fields of item will empty as well.Tried scrapy shellDoes this mean that it returned nothing? (I'm also new to scrapy shell)Ran html that i followed

Crawling with scrapy - bson.errors.InvalidDocument: cannot encode object

April_Z

[Crawling with scrapy - bson.errors.InvalidDocument: cannot encode object](https://stackoverflow.com/questions/58897140/crawling-with-scrapy-bson-errors-invaliddocument-cannot-encode-object)

I can’t save data to MongoDB when I tried to scrape Google App store with scrapy. I got an error: bson.errors.InvalidDocument: cannot encode object. I searched this error on Google and the result showed there’s something wrong with the data type, but I already converted it into a dictionary data type. My Pipeline.py file is:And error information from terminal is basically like below:

2019-11-17 03:03:54Z

I can’t save data to MongoDB when I tried to scrape Google App store with scrapy. I got an error: bson.errors.InvalidDocument: cannot encode object. I searched this error on Google and the result showed there’s something wrong with the data type, but I already converted it into a dictionary data type. My Pipeline.py file is:And error information from terminal is basically like below:The error message specifies exactly what is wrong:It is because you have grabbed a "live" Selector which needs to have its .extract() or .extract_first() called in order to resolve the pointer to its actual page content, which will usually be a list or str respectivelyThankfully, the Selector's __str__ representation shows you the xpath it was trying to use, so it should be pretty easy to track down in your code and fix

Google Author Profile Scraping

Waleed Arshad Awan

[Google Author Profile Scraping](https://stackoverflow.com/questions/58860423/google-author-profile-scraping)

I am scraping the google scholar author profile page. I am facing a problem when i tried to scrape the titles of each author, every author has more than 500 titles and they are displayed using load more button, i have got the link for loadmore pagination. Problem is i want to calculate the total number of titles one author has but i am not getting right total value. When i try to scrape only 2 authors it returns correct but when i try to scrape all the authors in a page (10 authors in one page) then i got wrong total value.My code is below. where my logic is wrong?This is the result but i am getting wrong total title value.  Klaus-Robert Müller has total 837 title and Tom Mitchell has 264 titles. For Log please see attached image. i know there is a problem in my logic

2019-11-14 15:32:00Z

I am scraping the google scholar author profile page. I am facing a problem when i tried to scrape the titles of each author, every author has more than 500 titles and they are displayed using load more button, i have got the link for loadmore pagination. Problem is i want to calculate the total number of titles one author has but i am not getting right total value. When i try to scrape only 2 authors it returns correct but when i try to scrape all the authors in a page (10 authors in one page) then i got wrong total value.My code is below. where my logic is wrong?This is the result but i am getting wrong total title value.  Klaus-Robert Müller has total 837 title and Tom Mitchell has 264 titles. For Log please see attached image. i know there is a problem in my logicI think you're over complicating it. I recommend to use request.meta to save your offset and counted articles:

How can I intercept JSONs/XHR requests as opposed to scraping data using Xpaths

Avlento

[How can I intercept JSONs/XHR requests as opposed to scraping data using Xpaths](https://stackoverflow.com/questions/58855853/how-can-i-intercept-jsons-xhr-requests-as-opposed-to-scraping-data-using-xpaths)

I am trying to scrape data from a website and was in the process of using the Scrapy framework to build a spider to iterate over various pages and collect the data from relevant Xpaths but realised that all the data I want is actually being delivered in a response in the form of a JSON (see image below for JSONs) and these are being used to render the page.My understanding is that the JSON is part of an XHR request. I am wondering if there is a way to intercept or copy these JSONs/XHR requests rather than build a spider that has to navigate the fully assembled page? I am not expecting a full solution to be posted but would be quite content with a pointer to the correct framework or other relevant learning resource so I can study further. I have only been programming 4 months so far. Google Dev tools

2019-11-14 11:35:17Z

I am trying to scrape data from a website and was in the process of using the Scrapy framework to build a spider to iterate over various pages and collect the data from relevant Xpaths but realised that all the data I want is actually being delivered in a response in the form of a JSON (see image below for JSONs) and these are being used to render the page.My understanding is that the JSON is part of an XHR request. I am wondering if there is a way to intercept or copy these JSONs/XHR requests rather than build a spider that has to navigate the fully assembled page? I am not expecting a full solution to be posted but would be quite content with a pointer to the correct framework or other relevant learning resource so I can study further. I have only been programming 4 months so far. Google Dev tools

Is there any way to open a csv file that contains arabic, in excel sheet?

Ahmad Arshi

[Is there any way to open a csv file that contains arabic, in excel sheet?](https://stackoverflow.com/questions/58867211/is-there-any-way-to-open-a-csv-file-that-contains-arabic-in-excel-sheet)

I have created a csv file using python, that contains Arabic lines I scraped from a website and its showing as it is in ubuntu libreoffice. But when I try to open it in MS Excel it shows garbage valuesLibreoffice (Perfectly allright)Excel (Garbage Values)Is there any way of getting same value as of Libreoffice in excel?

2019-11-14 22:03:41Z

I have created a csv file using python, that contains Arabic lines I scraped from a website and its showing as it is in ubuntu libreoffice. But when I try to open it in MS Excel it shows garbage valuesLibreoffice (Perfectly allright)Excel (Garbage Values)Is there any way of getting same value as of Libreoffice in excel?This is from https://help.livehelpnow.net/1/article/45188/viewing-exported-csv-files-containing-arabic-characters

Unsupported URL scheme '': no handler available for that scheme

April_Z

[Unsupported URL scheme '': no handler available for that scheme](https://stackoverflow.com/questions/58873607/unsupported-url-scheme-no-handler-available-for-that-scheme)

I'm trying to scrape Google App store but while running the script, I got the following error: 'Unsupported URL scheme '': no handler available for that scheme'.

The code is like below:And error information is like:I tried to instead extract() into extract_first() and it will show another error like 'raise ValueError('Missing scheme in request url: %s' % self._url)'. I'm totally new in scrapy, please can someone help me solve this problem?

2019-11-15 09:09:12Z

I'm trying to scrape Google App store but while running the script, I got the following error: 'Unsupported URL scheme '': no handler available for that scheme'.

The code is like below:And error information is like:I tried to instead extract() into extract_first() and it will show another error like 'raise ValueError('Missing scheme in request url: %s' % self._url)'. I'm totally new in scrapy, please can someone help me solve this problem?Try to use response.urljoin() for your request URLs:

Scrape text after ::before (xpath, css)

Ofey_QED

[Scrape text after ::before (xpath, css)](https://stackoverflow.com/questions/58873052/scrape-text-after-before-xpath-css)

I have built a Scrapy Spider and would like to get the email text from:

  ::before

  "E-Mail"



"E-Mail I would like to scrape"I've tried: 'email' : response.css('#content > div.segment.morecontact.clearfix > div > div.secondary > ul > li:nth-child(1) > a > i::text').extract(), but I'm only getting "E-Mail" and NOT the actual address

2019-11-15 08:31:45Z

I have built a Scrapy Spider and would like to get the email text from:

  ::before

  "E-Mail"



"E-Mail I would like to scrape"I've tried: 'email' : response.css('#content > div.segment.morecontact.clearfix > div > div.secondary > ul > li:nth-child(1) > a > i::text').extract(), but I'm only getting "E-Mail" and NOT the actual addressYou need a simple XPath's following-sibling::*:You can use another approach and get email from href attribute:

    email = response.xpath('//a[i[contains(@class, "icon_email")]]/@href').re_first(r'mailto:(.+)')

Scrapy: Can't create object of a class having *arg and **kwarg in init

ubuntu

[Scrapy: Can't create object of a class having *arg and **kwarg in init](https://stackoverflow.com/questions/58826521/scrapy-cant-create-object-of-a-class-having-arg-and-kwarg-in-init)

In my scrapy spider I need to create object of another spider.I need to create object of SpiderA in SpiderBI tried creating spider obj using SpiderA().init(*arg, **kwargs) but it gives error,I shall be grateful for any help. Thanks

2019-11-12 20:54:53Z

In my scrapy spider I need to create object of another spider.I need to create object of SpiderA in SpiderBI tried creating spider obj using SpiderA().init(*arg, **kwargs) but it gives error,I shall be grateful for any help. Thanksspider_a_obj = SpiderA(*args, **kwargs)

worked! 

Something went wrong with scrapy

April_Z

[Something went wrong with scrapy](https://stackoverflow.com/questions/58875651/something-went-wrong-with-scrapy)

I'm trying to scrape Google App store using Scrapy, I think the script is right but actually it didn't crawl anything and I don't know why this happened.Code is below:And information showed in terminal:I checked urls and XPath selector and I thought it would be right. I don't know where the problem is. Can somebody help me solve this problem?

2019-11-15 11:01:43Z

I'm trying to scrape Google App store using Scrapy, I think the script is right but actually it didn't crawl anything and I don't know why this happened.Code is below:And information showed in terminal:I checked urls and XPath selector and I thought it would be right. I don't know where the problem is. Can somebody help me solve this problem?

Monitoring long lasting tasks in Airflow

dzieciou

[Monitoring long lasting tasks in Airflow](https://stackoverflow.com/questions/58877849/monitoring-long-lasting-tasks-in-airflow)

I've seen people using Airflow to schedule hundreds of scraping jobs through Scrapyd daemons. However, one thing they miss in Airflow is monitoring long-lasting jobs like scraping: getting number of pages and items scraped so far, number of URL that failed so far or were retried without success.What are my options to monitor current status of long lasting jobs? Is there something already available or I need to resort to external solutions like Prometheus, Grafana and instrument Scrapy spiders myself?

2019-11-15 13:15:06Z

I've seen people using Airflow to schedule hundreds of scraping jobs through Scrapyd daemons. However, one thing they miss in Airflow is monitoring long-lasting jobs like scraping: getting number of pages and items scraped so far, number of URL that failed so far or were retried without success.What are my options to monitor current status of long lasting jobs? Is there something already available or I need to resort to external solutions like Prometheus, Grafana and instrument Scrapy spiders myself?We've had better luck keeping our airflow jobs short and sweet.With long-running tasks, you risk running into queue back-ups. And we've found the parallelism limits are not quite intuitive. Check out this post for a good breakdown.In a case kind of like yours, when there's a lot of work to orchestrate and potentially retry, we reach for Kafka. The airflow dags pull messages off of a Kafka topic and then report success/failure via a Kafka wrapper. We end up with several overlapping airflow tasks running in "micro-batches" reading a tune-able number of messages off Kafka, with the goal of keeping each airflow task under a certain run time. By keeping the work small in each airflow task, we can easily scale the number of messages up or down to tune the overall task run time with the overall parallelism of the airflow workers.Seems like you could explore something similar here?Hope that helps!

Using Scrapy 1.6.0 got DEBUG: Crawled (200)

yao sy

[Using Scrapy 1.6.0 got DEBUG: Crawled (200)](https://stackoverflow.com/questions/58880435/using-scrapy-1-6-0-got-debug-crawled-200)

I'm new to scrapy and try to crawl from different pages.Following is based on Selenium. It will crush after 100 pages and the speed is 1 page per second. I think it may be the website limitation. To avoid it, disable cookies or use fake header maybe work. I didn't try it up to now. I really need some great advices.Thank you so much!

2019-11-15 15:47:14Z

I'm new to scrapy and try to crawl from different pages.Following is based on Selenium. It will crush after 100 pages and the speed is 1 page per second. I think it may be the website limitation. To avoid it, disable cookies or use fake header maybe work. I didn't try it up to now. I really need some great advices.Thank you so much!Well, that isn't an error, Scrapy is indeed working fine.The main problem I can see here is that content on that webpage needs some time to load, so your script will need to wait, and you can achieve that by using Selenium or Splash.My advice would be to use Splash, it was developed for use with Scrapy, so here are some leads that will help you.Hope this helps.

Scrapy returns no data from css path

ksbawpn

[Scrapy returns no data from css path](https://stackoverflow.com/questions/58827116/scrapy-returns-no-data-from-css-path)

I am trying to scrape the links (or rather the listing IDs) to the listings on the first page of this website.

I think the right css selector must be At least when I search for this selector (without the ::attr(href)) in the Element Inspector, it seems like it should return the correct data. However, when running the spider, I get an empty output. I am new to scrapy and css selectors but figured it out for most other websites. Where is my mistake with this one? For reference, this is the parse method of my spider. Thanks

2019-11-12 21:41:06Z

I am trying to scrape the links (or rather the listing IDs) to the listings on the first page of this website.

I think the right css selector must be At least when I search for this selector (without the ::attr(href)) in the Element Inspector, it seems like it should return the correct data. However, when running the spider, I get an empty output. I am new to scrapy and css selectors but figured it out for most other websites. Where is my mistake with this one? For reference, this is the parse method of my spider. ThanksOn your website, all the listings pages follow the same naming pattern mietangebot_ID.html. You don't really need to use CSS selectors to get the IDs.You can check it for yourself by running this Bash command:It prints the following:In your scraper, you can simply use re.findall() with the naming pattern and capture the ID as a group:Which will print your listings:

Python script for downloading data from instrument internal memory through browser

VGB

[Python script for downloading data from instrument internal memory through browser](https://stackoverflow.com/questions/58800304/python-script-for-downloading-data-from-instrument-internal-memory-through-brows)

I have to download data from an instrument by connecting LAN wire directly. After that I have to open the web browser and log into instrument Settings by entering a particular IP address.

Eventhough there are lots of tabs and options,I have to download the data by inputing  day wise by inputing start date and end date in a particular tab,How to automate this process by python?

Which libraries should I import ?

2019-11-11 11:18:06Z

I have to download data from an instrument by connecting LAN wire directly. After that I have to open the web browser and log into instrument Settings by entering a particular IP address.

Eventhough there are lots of tabs and options,I have to download the data by inputing  day wise by inputing start date and end date in a particular tab,How to automate this process by python?

Which libraries should I import ?Use your web browser dev tools to inspect the HTTP requests that your browser does during the manual process, and automate it with Requests if you don’t need multiple downloads in parallel, or Scrapy otherwise.

Downloading the HTML from another resource and not using scrapy

Amanda

[Downloading the HTML from another resource and not using scrapy](https://stackoverflow.com/questions/58796487/downloading-the-html-from-another-resource-and-not-using-scrapy)

I created the following file inside the spiders directory of the scrapy project. The problem that I am facing is that the function inside middlewares.py namely process_request and process_response are not called. What could be the reason for this?I want to have download the HTML from the webpage using another process and not scrapy. That is the reason, I want to listen in the middleware and direct to another source that could download the HTML and send back the response.

2019-11-11 06:27:00Z

I created the following file inside the spiders directory of the scrapy project. The problem that I am facing is that the function inside middlewares.py namely process_request and process_response are not called. What could be the reason for this?I want to have download the HTML from the webpage using another process and not scrapy. That is the reason, I want to listen in the middleware and direct to another source that could download the HTML and send back the response.Target website uses js/ajax for creating the beautiful layout that you see in your browser whereas your crawler see something else; your approach seems fine for regular websites however with this specific website, you will never get all the html code without using a headless browser - try selenium or splash in order to view processed html.Since you do not mention it explicitly, you might have forgotten to enable the middleware. Defining it in a middlewares.py is not enough (or necessary, middlewares can be defined in many different places).

Add a new service to scrapyd from current project

George ȚUȚUIANU

[Add a new service to scrapyd from current project](https://stackoverflow.com/questions/58803162/add-a-new-service-to-scrapyd-from-current-project)

I want to create a custom service for scrapyd, from the scrapy project but I keep getting error.I created crawler/webservice.py:and then modified scrapyd.conf and added the line backintime.json = crawler.webservice.BackInTime under [services] tag.Bu when I run the command scrapyd I keep getting the error:Tried to follow some previous solutions but without any success:How should I add the new service so scrapyd can import it?

2019-11-11 14:20:37Z

I want to create a custom service for scrapyd, from the scrapy project but I keep getting error.I created crawler/webservice.py:and then modified scrapyd.conf and added the line backintime.json = crawler.webservice.BackInTime under [services] tag.Bu when I run the command scrapyd I keep getting the error:Tried to follow some previous solutions but without any success:How should I add the new service so scrapyd can import it?

What's the best way to disable image download in scrapy?

ishandutta2007

[What's the best way to disable image download in scrapy?](https://stackoverflow.com/questions/58801190/whats-the-best-way-to-disable-image-download-in-scrapy)

It is not disabled by default.I have written a spider which consumes almost 2 GB of data per hour. Now I want to save my data consumption, images are of no use for me, so want to make sure they not being fetched.Given that this is a P0 scenario, it should be a simple flag in settings.py but surprisingly from docs I couldn't find any. I found a lot of details about ImagesPipeline, enabling those pipelines, their storage etc, but no flag for people not interested in images. Let me know if I am missing anything.

2019-11-11 12:17:41Z

It is not disabled by default.I have written a spider which consumes almost 2 GB of data per hour. Now I want to save my data consumption, images are of no use for me, so want to make sure they not being fetched.Given that this is a P0 scenario, it should be a simple flag in settings.py but surprisingly from docs I couldn't find any. I found a lot of details about ImagesPipeline, enabling those pipelines, their storage etc, but no flag for people not interested in images. Let me know if I am missing anything.Scrapy does not download images unless you explicitly tell it to do it.You can check in the run time logs the URLs that Scrapy downloads. If a image URL does not appear in the logs, it is not being downloaded, even if a webpage that contains images is downloaded.When you open a downloaded page in a web browser, images are downloaded on the fly by the web browser. They do not come from the downloaded webpage, they are not (usually) embedded in the webpage, the webpage indicates where in the Internet they are, and the web browser downloads them to display them, but Scrapy does not.The only exception would be that images are actually embedded in the HTML code, as base64. This is uncommon, and probably not your case. And when that happens, there is no way you can prevent their download, you cannot download a webpage excluding part of its content.

Save scrapy item to a Postgres table based on a condition

CSantos

[Save scrapy item to a Postgres table based on a condition](https://stackoverflow.com/questions/58805967/save-scrapy-item-to-a-postgres-table-based-on-a-condition)

I'm creating a price monitor with web scrapping from scrapy. I would like to save new items in a table called product and if the product price changed with time, save it in another table called product_change.Any tips or thoughts?Something like:table: productid url price date (when the item was scrapped for the first time)table: product_changeid url new_price date (when the item price changed)with id a unique number that identifies the product.I already create the items.py but can't figure out how to configure pipelines.py for this project.

2019-11-11 17:23:23Z

I'm creating a price monitor with web scrapping from scrapy. I would like to save new items in a table called product and if the product price changed with time, save it in another table called product_change.Any tips or thoughts?Something like:table: productid url price date (when the item was scrapped for the first time)table: product_changeid url new_price date (when the item price changed)with id a unique number that identifies the product.I already create the items.py but can't figure out how to configure pipelines.py for this project.In pipelines.py add:

Scrapy item processing blocking new requests in broad crawl

johnson23

[Scrapy item processing blocking new requests in broad crawl](https://stackoverflow.com/questions/58765545/scrapy-item-processing-blocking-new-requests-in-broad-crawl)

I'm currently building a Scrapy script that performs a broad crawl with the following settings:To perform the broad crawl I loop through a list of domains here in my spider like this:The loop works well, making more than 10 calls per second.I noticed however that my entire scrapping slows down as soon as I send my first items to be processed in my pipeline (which performs simple loops and conditional checks).The issue seem to be that the loop in start_requests() waits for currently processed items to finish, before continuing my broad crawl for new domains, and this despite being way below my 'CONCURRENT_REQUESTS' limit.Result is a script that runs 2 to 3x slower.Isn't the item processing supposed to not block the requests?

2019-11-08 11:18:28Z

I'm currently building a Scrapy script that performs a broad crawl with the following settings:To perform the broad crawl I loop through a list of domains here in my spider like this:The loop works well, making more than 10 calls per second.I noticed however that my entire scrapping slows down as soon as I send my first items to be processed in my pipeline (which performs simple loops and conditional checks).The issue seem to be that the loop in start_requests() waits for currently processed items to finish, before continuing my broad crawl for new domains, and this despite being way below my 'CONCURRENT_REQUESTS' limit.Result is a script that runs 2 to 3x slower.Isn't the item processing supposed to not block the requests?Scrapy uses concurrency, not multi-threading.Network input and output (requests sent and responses received) do not block your code, but never will two parts of your code be executed at the same time.Scrapy uses Twisted, and if you cannot make your code more efficient you may be able to make your pipeline code run in a separate thread the Twisted way.

Scrapy Crawl ValueError

VictoriaV

[Scrapy Crawl ValueError](https://stackoverflow.com/questions/58772025/scrapy-crawl-valueerror)

I am new to python and to scrapy. I followed a tutorial to have scrapy crawl quotes.toscrape.com.  I entered in the code exactly how it is in the tutorial, but I keep getting a ValueError: invalid hostname: when I run scrapy crawl quotes.  I am doing this in Pycharm on a Mac computer.I tried doing single and double quotes around the URL in start_urls = []section but that did not fix the error.This is what the code looks like:It is supposed to be scraping the site for the title.This is what the error looks like:

2019-11-08 18:31:25Z

I am new to python and to scrapy. I followed a tutorial to have scrapy crawl quotes.toscrape.com.  I entered in the code exactly how it is in the tutorial, but I keep getting a ValueError: invalid hostname: when I run scrapy crawl quotes.  I am doing this in Pycharm on a Mac computer.I tried doing single and double quotes around the URL in start_urls = []section but that did not fix the error.This is what the code looks like:It is supposed to be scraping the site for the title.This is what the error looks like:Don't use spaces for URLs!

Python - which is considered better for scrapping: selenium or beautifulsoup with selenium? [closed]

cmarios

[Python - which is considered better for scrapping: selenium or beautifulsoup with selenium? [closed]](https://stackoverflow.com/questions/47983495/python-which-is-considered-better-for-scrapping-selenium-or-beautifulsoup-wit)

This question is for Python 3.6.3, bs4 and Selenium 3.8 on Win10.I am trying to scrape pages with dynamic content. What I am trying to scrape is numbers and text (from http://www.oddsportal.com for example). From my understanding using requests+beautifulsoup will not do the job, as dynamic content will be hidden. So I have to use other tools such us selenium webdriver.Then, given that I will use selenium webdriver anyway, do you recommend ignoring beautifulsoup and stick with selenium webdriver functions, egor is it considered better practice to use selenium+beautifulsoup?Do you have any opinion as to which of the two routes will give me more convenient functions to work with?Thanks.

2017-12-26 20:40:39Z

This question is for Python 3.6.3, bs4 and Selenium 3.8 on Win10.I am trying to scrape pages with dynamic content. What I am trying to scrape is numbers and text (from http://www.oddsportal.com for example). From my understanding using requests+beautifulsoup will not do the job, as dynamic content will be hidden. So I have to use other tools such us selenium webdriver.Then, given that I will use selenium webdriver anyway, do you recommend ignoring beautifulsoup and stick with selenium webdriver functions, egor is it considered better practice to use selenium+beautifulsoup?Do you have any opinion as to which of the two routes will give me more convenient functions to work with?Thanks.Beautifulsoup is a powerful tool for Web Scrapping. It use the urllib.request Python library. urllib.request is quite powerful to extract data from static pages.Selenium is currently the most widely accepted and efficient tool for Web Automation. Selenium supports interacting with Dynamic Pages, Contents and Elements.To create a robust and efficient framework to scrape pages with dynamic content you must integrate both Selenium and Beautifulsoup in your framework. Browse and interact with dynamic elements through Selenium and scrape the contents efficiently through BeautifulsoupHere is an example using Selenium and Beautifulsoup for ScrappingSelenium has many selectorsso mostly you don't need BeautifulSoup. Especially xpath and css_selector can be useful.

Download videos in custom folder and name with scrapy

fatemeh

[Download videos in custom folder and name with scrapy](https://stackoverflow.com/questions/58777592/download-videos-in-custom-folder-and-name-with-scrapy)

I want to download videos in a custom folder and name I have overridden some methods of FilePiplines but I didn't achieve my goal .This is my piplines.py code.

2019-11-09 08:24:52Z

I want to download videos in a custom folder and name I have overridden some methods of FilePiplines but I didn't achieve my goal .This is my piplines.py code.

Scrapy text extract is not correct

txn

[Scrapy text extract is not correct](https://stackoverflow.com/questions/58770441/scrapy-text-extract-is-not-correct)

I have a Problem.I want to get a text (the Price) on a site. Every other things like title/name works, but not the price.price = product.css('.offerList-item-priceWrapper .priceRange::text')[0].extract().replace('ab','').replace('*','').replace('\xc2','').replace('\xa0','').replace('€','').strip()Thats the price call...Thats the html site You cannot that there are really much whitespaces...

If i print my price, i get only whitespaces back.I put a screenshot here, there you can see the console of chrome I think there so much whitespaces, that i dont get the 30,82 in that example... 

2019-11-08 16:29:23Z

I have a Problem.I want to get a text (the Price) on a site. Every other things like title/name works, but not the price.price = product.css('.offerList-item-priceWrapper .priceRange::text')[0].extract().replace('ab','').replace('*','').replace('\xc2','').replace('\xa0','').replace('€','').strip()Thats the price call...Thats the html site You cannot that there are really much whitespaces...

If i print my price, i get only whitespaces back.I put a screenshot here, there you can see the console of chrome I think there so much whitespaces, that i dont get the 30,82 in that example... I managed to simplify it quite a bit for you. Practice using selectors a little more. Hope this answer shows you where you could've went wrong.

How to make scrapy wait for request result before continuing to next line

obsidian93

[How to make scrapy wait for request result before continuing to next line](https://stackoverflow.com/questions/58726112/how-to-make-scrapy-wait-for-request-result-before-continuing-to-next-line)

In my spider, I have some code like this:next_page looks like this:I need next_page_url to be set before I can continue the next segment of code.

This code essentially checks if the current page is the last page for some file writing purposes

2019-11-06 08:43:20Z

In my spider, I have some code like this:next_page looks like this:I need next_page_url to be set before I can continue the next segment of code.

This code essentially checks if the current page is the last page for some file writing purposesThe answer I ended up going with:instead of checking if the next page exists and then continuing on current request, I made the request to the page, checked if I got a response, and if I didn't, I said that the previous page was the final page.I did this by using the meta keyword in scrapy's Request library (response.follow()) to pass the current page's necessary tracking data into the new request

ModuleNotFoundError: When trying to run unittest with Scrapy

ZeCarioca

[ModuleNotFoundError: When trying to run unittest with Scrapy](https://stackoverflow.com/questions/58728163/modulenotfounderror-when-trying-to-run-unittest-with-scrapy)

Let me start by saying that I've looked several answers on SE and none of them solved my issue. I have a scrapy application with the usual structurein my test_my_spyder.py I need classes from both my_spyder.py and items.py so my code is this:If I use python -m unittest test_my_spyder from new_project/ directory i getHowever, if i run the same command from inside the tests/ directory the return is:The command scrapy crawl spider-name works perfectly.--Now, all the things I tried:1: Tried this inside test_my_spyder.py, no difference.2: With or without this, no difference:3: All those variations:Also, I found a lot of places people where keep mentioning to "Add the folder's path to the environment variable (PYTHONPATH)".

2019-11-06 10:36:52Z

Let me start by saying that I've looked several answers on SE and none of them solved my issue. I have a scrapy application with the usual structurein my test_my_spyder.py I need classes from both my_spyder.py and items.py so my code is this:If I use python -m unittest test_my_spyder from new_project/ directory i getHowever, if i run the same command from inside the tests/ directory the return is:The command scrapy crawl spider-name works perfectly.--Now, all the things I tried:1: Tried this inside test_my_spyder.py, no difference.2: With or without this, no difference:3: All those variations:Also, I found a lot of places people where keep mentioning to "Add the folder's path to the environment variable (PYTHONPATH)".

Downloading images causes duplication in values and giving error

shaban

[Downloading images causes duplication in values and giving error](https://stackoverflow.com/questions/58701190/downloading-images-causes-duplication-in-values-and-giving-error)

Spider was running ok and saving data in mongodb but suddenly starts saving duplicate values in database and in json and csv also, i remove the code for downloading images and it works fine but i need images, can anyone help me out ?

thanks in advance.items.pyimport scrapypipelines.pyimport pymongoclass BucketPipeline(object):spider.py[scrapy.pipelines.files] WARNING: File (code: 400): Error downloading file from https://qne.com.pk/../product_images/14768.jpg> referred in 

2019-11-04 21:17:05Z

Spider was running ok and saving data in mongodb but suddenly starts saving duplicate values in database and in json and csv also, i remove the code for downloading images and it works fine but i need images, can anyone help me out ?

thanks in advance.items.pyimport scrapypipelines.pyimport pymongoclass BucketPipeline(object):spider.py[scrapy.pipelines.files] WARNING: File (code: 400): Error downloading file from https://qne.com.pk/../product_images/14768.jpg> referred in not sure about the source you're downloading from, but you might find a "data-src" attribute in addition to the "src" attribute...

You might wanna try having an if statement to capture "data-src" if "src" has a standard common..

Reading robots.txt file?

bover21

[Reading robots.txt file?](https://stackoverflow.com/questions/58676412/reading-robots-txt-file)

I am trying to webscrape a website and their robots.txt file says this:(where zoeksuggestie is search suggestion in english) Does this mean I can't scrape any link that is /koop/*,* ? what does the *,*mean? I really need to get data from this website for a project, but I keep getting blocked using scrapy/beautiful soup. 

2019-11-03 00:17:53Z

I am trying to webscrape a website and their robots.txt file says this:(where zoeksuggestie is search suggestion in english) Does this mean I can't scrape any link that is /koop/*,* ? what does the *,*mean? I really need to get data from this website for a project, but I keep getting blocked using scrapy/beautiful soup. The robots.txt file is part of the “Robots exclusion standard” whenever a bot visits a website, they check the robots.txt file to see what they can’t access. Google uses this to not index or at least publicly display URLs matching those in the robots.txt file. The file is however not mandatory to comply with the robots.txt.

The * is a wildcard so /koop/*,* will match anything with /koop/[wildcard],[wildcard].

Here is a great guide on wildcards in robots.txt https://geoffkenyon.com/how-to-use-wildcards-robots-txt/ You mentioned scrapy not working, that is because scrapy follows the robots.txt by default. This can be disabled in settings, that question has been answered here: getting Forbidden by robots.txt: scrapy

speed up python scrapy crawler

Maxim Zhirnov

[speed up python scrapy crawler](https://stackoverflow.com/questions/58674325/speed-up-python-scrapy-crawler)

I'm currently writing vacancies scraper with Scrapy to parse about 3M of vacancies item.

Now I'm on place when spider works and successfully scraping items and storing it tot postgreesql but the thing is it doing it pretty slow.

For 1 hr i stored only 12k vacancies so i'm really ti far from 3M of them.

Thing is that in the end i'm gonna need to scrape and update data once per day and with current performance I'm gonna need more than a day to just parse all data.I'm new in data scraping so I may do some basic thing wrong and I'll be very gratefull if anybody can hel me.Code of my spider:

2019-11-02 19:00:47Z

I'm currently writing vacancies scraper with Scrapy to parse about 3M of vacancies item.

Now I'm on place when spider works and successfully scraping items and storing it tot postgreesql but the thing is it doing it pretty slow.

For 1 hr i stored only 12k vacancies so i'm really ti far from 3M of them.

Thing is that in the end i'm gonna need to scrape and update data once per day and with current performance I'm gonna need more than a day to just parse all data.I'm new in data scraping so I may do some basic thing wrong and I'll be very gratefull if anybody can hel me.Code of my spider:

How to start a session in scrapy with login in a browser?

John M.

[How to start a session in scrapy with login in a browser?](https://stackoverflow.com/questions/58680373/how-to-start-a-session-in-scrapy-with-login-in-a-browser)

I am trying to crawl a set of webpages. However, accessing the webpages requires a somehow complicated login procedure. I am wondering whether there is a way to do the following in scrapy: 1) create a session 2) pop up the login page in a browser 3) crawl the webpages using the same session I don't think it is doable to have a formrequest for auto-login as the login has two steps. 

2019-11-03 13:00:57Z

I am trying to crawl a set of webpages. However, accessing the webpages requires a somehow complicated login procedure. I am wondering whether there is a way to do the following in scrapy: 1) create a session 2) pop up the login page in a browser 3) crawl the webpages using the same session I don't think it is doable to have a formrequest for auto-login as the login has two steps. 

Debug while webcraping with scrapy?

Umair

[Debug while webcraping with scrapy?](https://stackoverflow.com/questions/58676238/debug-while-webcraping-with-scrapy)

I am trying to webscrape a website with scrapy (with a 10s download delay + AUTOTHROTTLE_ENABLED = True + ROBOTSTXT_OBEY = True). When I run the command:I get this output for multiple "lines" until 405:and also:Why can't scrapy scrape this website? and this is the dumping stats (if useful): 

2019-11-02 23:40:19Z

I am trying to webscrape a website with scrapy (with a 10s download delay + AUTOTHROTTLE_ENABLED = True + ROBOTSTXT_OBEY = True). When I run the command:I get this output for multiple "lines" until 405:and also:Why can't scrapy scrape this website? and this is the dumping stats (if useful): You need to emulate exactly same request as a real browser does

Scrapy crawler to return only URL and Referrer when crawling

StefWill

[Scrapy crawler to return only URL and Referrer when crawling](https://stackoverflow.com/questions/58634996/scrapy-crawler-to-return-only-url-and-referrer-when-crawling)

I'm VERY new to scrapy, just found it yesterday and only basic python experience.I have a group of sub-domains (around 200) that I need to map, every internal and external link. I'm just not understanding the output side of things I think.This is what I have so far.it outputs to the terminal like so:What I'm after is a CSV or TSVAny assistance is appreciated but would prefer a referral to docs than straight solution.This is the solution I came up with.

2019-10-30 23:30:41Z

I'm VERY new to scrapy, just found it yesterday and only basic python experience.I have a group of sub-domains (around 200) that I need to map, every internal and external link. I'm just not understanding the output side of things I think.This is what I have so far.it outputs to the terminal like so:What I'm after is a CSV or TSVAny assistance is appreciated but would prefer a referral to docs than straight solution.This is the solution I came up with.You can get both urls simply in parse.referer = response.request.headers.get('Referer')

original_url = response.urlyield {'referer': referer, 'url': original_url}You can write the output to file using scrapy crawl spider_name -o bettybarclay.jsonWhile this isn't 100% correct this is a great start.

Scrapy script cannot upload to s3 when using runner = CrawlerRunner(): only works using process = CrawlerProcess()

bobparker

[Scrapy script cannot upload to s3 when using runner = CrawlerRunner(): only works using process = CrawlerProcess()](https://stackoverflow.com/questions/58636445/scrapy-script-cannot-upload-to-s3-when-using-runner-crawlerrunner-only-work)

I have a weird issue where my script does upload data to my s3 with no issues using process = CrawlerProcess() (exampled below), but when I attempt to run my script which does contain various classes (12 in total), sequentially it doesn't send to s3 when using runner = CrawlerRunner() (also shown below). Also, sometimes when I run the process = CrawlerProcess() , it will only send to the 11/12 folders but hardly ever to the top (1st) class I have a script that is running 12 classes (a portion exampled below), all going class will export to a specific on my s3. Any thoughts what could be causing issue?

2019-10-31 03:13:56Z

I have a weird issue where my script does upload data to my s3 with no issues using process = CrawlerProcess() (exampled below), but when I attempt to run my script which does contain various classes (12 in total), sequentially it doesn't send to s3 when using runner = CrawlerRunner() (also shown below). Also, sometimes when I run the process = CrawlerProcess() , it will only send to the 11/12 folders but hardly ever to the top (1st) class I have a script that is running 12 classes (a portion exampled below), all going class will export to a specific on my s3. Any thoughts what could be causing issue?

Why do I keep encountering this error: 'settings.py' is not recognized as an internal or external command, operable program or batch file?

Jordan North

[Why do I keep encountering this error: 'settings.py' is not recognized as an internal or external command, operable program or batch file?](https://stackoverflow.com/questions/58610508/why-do-i-keep-encountering-this-error-settings-py-is-not-recognized-as-an-int)

This is my first Python project and I just installed Scrapy through Anaconda Prompt. I am getting frustrated because 'settings.py' is not recognized and cant figure out where the file is. I have tried changing the PATH in Environment Variables and still cant figure out why i cant access the settings in my project. I have created a spider but get stuck when trying to change settingsCan someone please help??

2019-10-29 15:25:57Z

This is my first Python project and I just installed Scrapy through Anaconda Prompt. I am getting frustrated because 'settings.py' is not recognized and cant figure out where the file is. I have tried changing the PATH in Environment Variables and still cant figure out why i cant access the settings in my project. I have created a spider but get stuck when trying to change settingsCan someone please help??

List with several absolute urls “urljoin'ed”

Hugo

[List with several absolute urls “urljoin'ed”](https://stackoverflow.com/questions/58638738/list-with-several-absolute-urls-urljoined)

I wish to download all the files from the first post, of several forum topics of a specific forum page. I have my own file pipeline set up to take the items file_url, file_name and source(topic name), in order to save them to the folder ./source/file_name.However, the file links are relative and I need to use the absolute path. I tried response.urljoin and it gives me a string of the absolute url but of the last file of the post only. Running the spider gives me the error ValueError: Missing scheme in request url: h 

This happens because the absolute url is a string and not a listHere is my code:If it helps here's the pipeline code:So my question is.In a topic with more than 1 file to be downloaded, how can I save the several absolute urls into the file_url item? The for loop is not working as intended since it only saves the last file's url. Do I need a for loop for this problem? If so, what should it be?

2019-10-31 07:38:06Z

I wish to download all the files from the first post, of several forum topics of a specific forum page. I have my own file pipeline set up to take the items file_url, file_name and source(topic name), in order to save them to the folder ./source/file_name.However, the file links are relative and I need to use the absolute path. I tried response.urljoin and it gives me a string of the absolute url but of the last file of the post only. Running the spider gives me the error ValueError: Missing scheme in request url: h 

This happens because the absolute url is a string and not a listHere is my code:If it helps here's the pipeline code:So my question is.In a topic with more than 1 file to be downloaded, how can I save the several absolute urls into the file_url item? The for loop is not working as intended since it only saves the last file's url. Do I need a for loop for this problem? If so, what should it be?In:You are overwriting item['file_url'] every time with a new URL, and as a result the value of the last one is the value that stays.Use Python list comprehension instead of a for loop:

Scrapy can't export bodytext

sigalizer

[Scrapy can't export bodytext](https://stackoverflow.com/questions/58599416/scrapy-cant-export-bodytext)

So I'm trying to work with a Scrapy project called RISJbot for extracting the contents of news articles for my research, however I encountered a problem that I can't find the source of, or the way to fix it: the spider doesn't really return body texts (couldn't find any on the Washington Post, and few on CNN), which would be very important in the case of an article.I'm not very familiar with Python, but from what I understand, it makes several attempts to find the bodytext, and when it fails to do so, it returns a gzipped and Base 64-encoded version.What would you recommend me to do? I couldn't find a way so far to fix it.Here's the spider itself:The full error message is (it doesn't have a "Traceback" part):I also found another error, although I'm not sure if it's linked to the bodytext issue: 

2019-10-28 22:57:38Z

So I'm trying to work with a Scrapy project called RISJbot for extracting the contents of news articles for my research, however I encountered a problem that I can't find the source of, or the way to fix it: the spider doesn't really return body texts (couldn't find any on the Washington Post, and few on CNN), which would be very important in the case of an article.I'm not very familiar with Python, but from what I understand, it makes several attempts to find the bodytext, and when it fails to do so, it returns a gzipped and Base 64-encoded version.What would you recommend me to do? I couldn't find a way so far to fix it.Here's the spider itself:The full error message is (it doesn't have a "Traceback" part):I also found another error, although I'm not sure if it's linked to the bodytext issue: 

exporting data to seperate csv files in scrapy

Ibtsam Ch

[exporting data to seperate csv files in scrapy](https://stackoverflow.com/questions/58614280/exporting-data-to-seperate-csv-files-in-scrapy)

I have made a scrapy crawler that goes to this site https://www.cartoon3rbi.net/cats.htmlthen by first rule open the link to every show, get its title by parse_title method, and on third rule open every episode's link and get its name. its working fine, i just need to know how can i make a seperate csv file for each show's episodes's names with titles in parse_title method being used as name of the csv file. Any suggestions?

2019-10-29 19:37:17Z

I have made a scrapy crawler that goes to this site https://www.cartoon3rbi.net/cats.htmlthen by first rule open the link to every show, get its title by parse_title method, and on third rule open every episode's link and get its name. its working fine, i just need to know how can i make a seperate csv file for each show's episodes's names with titles in parse_title method being used as name of the csv file. Any suggestions?Assuming you have the titles stored in a list titles and the respective contents stored in a list contents, you could call the following custom function write_to_csv(title, content) each time to write the content to a file and save it by the name <title>.csv.  

Using Scrapy to get itune podcast data

Nancy Zhu

[Using Scrapy to get itune podcast data](https://stackoverflow.com/questions/58612984/using-scrapy-to-get-itune-podcast-data)

I am trying to use Scrapy to get itune podcast data, however I am having trouble scraping podcast data from itune RSS feed using response.xpath in Scrapy. I have tried using requests, which works fineHowever, I need to incorporate that into a Scrapy Crawler to get podcast rating from each individual podcast url. I don't know how to use scrapy to scrape JSON responses, so instead I tried the xml. I tried the above code in scrapy shell and got blank response. The response status was 200 and I was able to see content using view(response)Basically, I am trying to write 'Step 2' in this spider:

2019-10-29 17:54:11Z

I am trying to use Scrapy to get itune podcast data, however I am having trouble scraping podcast data from itune RSS feed using response.xpath in Scrapy. I have tried using requests, which works fineHowever, I need to incorporate that into a Scrapy Crawler to get podcast rating from each individual podcast url. I don't know how to use scrapy to scrape JSON responses, so instead I tried the xml. I tried the above code in scrapy shell and got blank response. The response status was 200 and I was able to see content using view(response)Basically, I am trying to write 'Step 2' in this spider:

Scrapy save duplicate documents in mongodb [pymongo], how to prevent it from duplicating documents

shaban

[Scrapy save duplicate documents in mongodb [pymongo], how to prevent it from duplicating documents](https://stackoverflow.com/questions/58616040/scrapy-save-duplicate-documents-in-mongodb-pymongo-how-to-prevent-it-from-dup)

spider was ok but now suddenly it is inserting duplicate documentsspider function here

2019-10-29 22:05:49Z

spider was ok but now suddenly it is inserting duplicate documentsspider function hereTo prevent duplication, you need to find a unique identifier for each item, it's often already present from the webpage or JSON data you crawled.Then, for each item, you may want to assign _id field with value of the identifier you found, since MongoDB uses it as the primary key. https://docs.mongodb.com/manual/core/document/index.html It is also viable to create a unique index in MongoDB for the key name of the identifier.https://docs.mongodb.com/manual/core/index-unique/#index-type-unique

Scrapy - Splitting selector parts between two variables

YASPLS

[Scrapy - Splitting selector parts between two variables](https://stackoverflow.com/questions/58618529/scrapy-splitting-selector-parts-between-two-variables)

I'm having issues scrapping data using my spider script even though a similar code works when I test it in the scrapy shell. The only difference is that in my script I am splitting the selector.Here is the selector that works in the shell:And here is the selector in the script:My educated guess for why this is not working is because when I'm using the selector in the shell I can put parentheses before the "//" and right before "[1]", which helps the selector work properly. However in the script I can't do this because I'm splitting the two components. Any ideas on how I can get around this?Thanks in advance for any help!

2019-10-30 03:54:44Z

I'm having issues scrapping data using my spider script even though a similar code works when I test it in the scrapy shell. The only difference is that in my script I am splitting the selector.Here is the selector that works in the shell:And here is the selector in the script:My educated guess for why this is not working is because when I'm using the selector in the shell I can put parentheses before the "//" and right before "[1]", which helps the selector work properly. However in the script I can't do this because I'm splitting the two components. Any ideas on how I can get around this?Thanks in advance for any help!First of all there is a shorter way to write td[position()=2]:Next what did you mean by this XPath:Select td at second position (position()=2) that is at the first ([1])position at the same time?UPDATE If you just want to process all rows after second and need to read td[2]:

How to pass data between sequential spiders

rodeo

[How to pass data between sequential spiders](https://stackoverflow.com/questions/58579716/how-to-pass-data-between-sequential-spiders)

I have two spiders that run in sequential order according to https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process. Now I want to pass some information from the first spider to the second (a selenium webdriver, or it's session information). I'm quite new to scrapy, but on another post it was proposed to save the data to a db and retrieve it from there. This seems a bit too much for just passing one variable, is there no other way? 

(I know in this example I could just make that into one long spider, but later I would like to run the first spider once but the second spider multiple times.)I would like to pass the variable to the constructor of the second spider, but I'm unable to get the data from the first one. If I just run the first crawler to return the variable, it apparently breaks the sequential structure. If I try to retrieve the yield, the result is None.Am I completely blind? I can't believe that this should be such a complex task.

2019-10-27 13:11:21Z

I have two spiders that run in sequential order according to https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process. Now I want to pass some information from the first spider to the second (a selenium webdriver, or it's session information). I'm quite new to scrapy, but on another post it was proposed to save the data to a db and retrieve it from there. This seems a bit too much for just passing one variable, is there no other way? 

(I know in this example I could just make that into one long spider, but later I would like to run the first spider once but the second spider multiple times.)I would like to pass the variable to the constructor of the second spider, but I'm unable to get the data from the first one. If I just run the first crawler to return the variable, it apparently breaks the sequential structure. If I try to retrieve the yield, the result is None.Am I completely blind? I can't believe that this should be such a complex task.You can pass a queue to both spiders, and let spider2 block on queue.get(), so there is no need for time.sleep(2).You can also just create the webdriver before and pass it as arguments. When I tried this initially, it didn't work because I passed the arguments incorrectly (see my comment on the post).

How to find sitemap in each domain and sub domain using python

William Johnson

[How to find sitemap in each domain and sub domain using python](https://stackoverflow.com/questions/58575046/how-to-find-sitemap-in-each-domain-and-sub-domain-using-python)

I want to know how to find sitemap in each domain and sub domain using python?

Some examples:And etc.What is the most probable sitemap names, locations and also extensions?

2019-10-26 21:58:36Z

I want to know how to find sitemap in each domain and sub domain using python?

Some examples:And etc.What is the most probable sitemap names, locations and also extensions?Please take a look at the robots.txt file first. That's what I usually do.Some domains do offer more than one sitemap and there are cases with more than 200 xml files.Please remember that according to the FAQ on sitemap.org, a sitemap file can be gzipped. Consequently, you might want to consider sitemap.xml.gz too!

How to simulate drop down selection and updated table load with scrapy?

adam

[How to simulate drop down selection and updated table load with scrapy?](https://stackoverflow.com/questions/58617336/how-to-simulate-drop-down-selection-and-updated-table-load-with-scrapy)

I am trying to load through the information about each stock page in investing.com starting from the drop-down list of "Dow Jones Industrial Average" on page investing.com/equitiesI have been thinking about using scrapy with but this does not simulate a selection action. After the selection action, I will be going through the table items one by one in #cross_rate_markets_stocks_1, and crawl those equity pages recursivelyCan you point out how to simulate a click action?

2019-10-30 00:51:46Z

I am trying to load through the information about each stock page in investing.com starting from the drop-down list of "Dow Jones Industrial Average" on page investing.com/equitiesI have been thinking about using scrapy with but this does not simulate a selection action. After the selection action, I will be going through the table items one by one in #cross_rate_markets_stocks_1, and crawl those equity pages recursivelyCan you point out how to simulate a click action?The selection action is user interaction with the browser UI, but scrapy doesn't render a webpage, we cannot simulate user interaction or run Javascript with it. However, if you're interested in crawling by simulate user interaction, selenium might be a good tool for you.Back to the question, if we are to crawl with scrapy, we should focus on requests and responses sent to/by the target website, you can log them in the Developer Tools of your browser. After you opened the Developer Tool, click the dropdown menu, you can see the corresponding request is sent to this url:https://cn.investing.com/equities/StocksFilter?noconstruct=1&smlID=0&sid=&tabletype=price&index_id=166It's a GET request, with index_id assigned with selected stock ID, you can get the stock ID and name from HTML element of https://investing.com/equities

Unable to scrape multiple pages using scrapy

Yash Sethia

[Unable to scrape multiple pages using scrapy](https://stackoverflow.com/questions/58580360/unable-to-scrape-multiple-pages-using-scrapy)

I have a project to scrape data from class-central.com. If you open this website and click on any one of the subjects (for eg. Computer Science), you will get a list of courses. At the bottom of the page, there is a next button that displays more courses and this continues till all courses have been displayed. I want to scrape all the courses.I am struggling with scraping the courses that are displayed after clicking on the 'Next' button.

Also, I am unable to scrape the 'Number of Revies' on each course.

Here is my code,Please help.(PS: I am a beginner so please try to make your answer as simple as possible)

2019-10-27 14:39:26Z

I have a project to scrape data from class-central.com. If you open this website and click on any one of the subjects (for eg. Computer Science), you will get a list of courses. At the bottom of the page, there is a next button that displays more courses and this continues till all courses have been displayed. I want to scrape all the courses.I am struggling with scraping the courses that are displayed after clicking on the 'Next' button.

Also, I am unable to scrape the 'Number of Revies' on each course.

Here is my code,Please help.(PS: I am a beginner so please try to make your answer as simple as possible)

Exporting data to csv after scraping data using scrapy

Ibtsam Ch

[Exporting data to csv after scraping data using scrapy](https://stackoverflow.com/questions/58530027/exporting-data-to-csv-after-scraping-data-using-scrapy)

Made this scrapper that scrapes data correctly but the problem is with exporting it to csv. The default - o filname.csvdoesn't paste data in the correct order. Need some guidance to do it.The item['name'] should in first column and item['link'] in second. This is the code.

2019-10-23 19:47:52Z

Made this scrapper that scrapes data correctly but the problem is with exporting it to csv. The default - o filname.csvdoesn't paste data in the correct order. Need some guidance to do it.The item['name'] should in first column and item['link'] in second. This is the code.You need to use

FEED_EXPORT_FIELDS in your settings.pyIf you want to export data to a csv you could maybe use Pandas.First you should make a Pandas-Dataframe from your and then you can export this dataframe to a csv:from https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.htmlI'm not sure if this is what you are looking for

LinkedIn Scarping Multiple Pages

aze45sq6d

[LinkedIn Scarping Multiple Pages](https://stackoverflow.com/questions/58542284/linkedin-scarping-multiple-pages)

I know there's dicussions whether scraping LinkedIn is allowed or not; but from following article:

https://www.forbes.com/sites/emmawoollacott/2019/09/10/linkedin-data-scraping-ruled-legal/#787286c31b54I think it is safe to say that scarping publicly available data from LinkedIn is legal.Now, I am trying to scrape job searches for a specific job title in a specific region.

So far so good, everything works, except for the limit of the amount of scraped jobs to be 25.I am trying to use following trick:

Inside the URL I pass a keyword &start=X

with X going from 0, to 25, 50, and so on.In browser, this allows me to go to the next page view and extract jobs from there.

However, using scrapy this method doesn't work.The code is as follows:Output:Even if I hardcode it to 25 (page 2), it sets it to 0.Any idea on how to solve this?

2019-10-24 13:18:49Z

I know there's dicussions whether scraping LinkedIn is allowed or not; but from following article:

https://www.forbes.com/sites/emmawoollacott/2019/09/10/linkedin-data-scraping-ruled-legal/#787286c31b54I think it is safe to say that scarping publicly available data from LinkedIn is legal.Now, I am trying to scrape job searches for a specific job title in a specific region.

So far so good, everything works, except for the limit of the amount of scraped jobs to be 25.I am trying to use following trick:

Inside the URL I pass a keyword &start=X

with X going from 0, to 25, 50, and so on.In browser, this allows me to go to the next page view and extract jobs from there.

However, using scrapy this method doesn't work.The code is as follows:Output:Even if I hardcode it to 25 (page 2), it sets it to 0.Any idea on how to solve this?Just disable RedirectMiddleware with a REDIRECT_ENABLED=0 setting on the scrapy shell._        For example, if you want the redirect middleware to ignore 301 and 302 responses (and pass them through to your spider) you can do this:This middleware handles redirection of requests based on response status.

https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.redirect

Is it possible to use scrapy without Anaconda?

illusiveman

[Is it possible to use scrapy without Anaconda?](https://stackoverflow.com/questions/58541489/is-it-possible-to-use-scrapy-without-anaconda)

So I am very new to scrapping and trying to learn scrapy. Most of the tutorial on web I have seen use Anaconda for scrapy project. I just wish to know is it possible to use scrapy without Anaconda.

2019-10-24 12:33:21Z

So I am very new to scrapping and trying to learn scrapy. Most of the tutorial on web I have seen use Anaconda for scrapy project. I just wish to know is it possible to use scrapy without Anaconda.it's is normally possible,

You can download it with pip

check there : http://doc.scrapy.org/en/latest/intro/install.htmlyou should probably use a dedicated virtualenv to avoid conflicting with your system packages.https://github.com/pypa/pipenv Pipenv is a good oneYes, it is possible. Following steps would lead you to that(if you're using Ubuntu): How to install pip on Windows

 Step #2 still remains the same for Windows.Cheers!

Getting the JavaScript elements on a website with scrapy_splash

Klaphat

[Getting the JavaScript elements on a website with scrapy_splash](https://stackoverflow.com/questions/58541536/getting-the-javascript-elements-on-a-website-with-scrapy-splash)

I'm trying to scrape websites (for instance https://home.dk/resultatliste/?CurrentPageNumber=0&SearchResultsPerPage=15&q=2100&Energimaerker=null&SearchType=0) for urls to loop through to get information about house listings. I have installed Scrapy 1.7.3 and Scrapy_Splash because the websites contain JavaScript, but even with the Scrapy_Splash library it doesn't load the urls in Scrapy. For instance: 

 Instead of:

My code is really simple (first real project)I would like to have a list of 15 urls loaded from each site.

2019-10-24 12:35:36Z

I'm trying to scrape websites (for instance https://home.dk/resultatliste/?CurrentPageNumber=0&SearchResultsPerPage=15&q=2100&Energimaerker=null&SearchType=0) for urls to loop through to get information about house listings. I have installed Scrapy 1.7.3 and Scrapy_Splash because the websites contain JavaScript, but even with the Scrapy_Splash library it doesn't load the urls in Scrapy. For instance: 

 Instead of:

My code is really simple (first real project)I would like to have a list of 15 urls loaded from each site.

Is there a way to bypass PHPSESSID and __cfduid cookies while using proxies and fake_useragent in scrapy?

shovan rai

[Is there a way to bypass PHPSESSID and __cfduid cookies while using proxies and fake_useragent in scrapy?](https://stackoverflow.com/questions/58472282/is-there-a-way-to-bypass-phpsessid-and-cfduid-cookies-while-using-proxies-and)

So far I have tried the above commented section of the code.I can crawl few pages with the default settings, and increase crawls to over a 100 with delays set to 30 seconds.I think the problem is with PHPSESSID being set "only once" at the very begining for every combination of proxy and user-agent, while the __cfduid is set for lifetime of the crawl for that combination.

2019-10-20 10:57:36Z

So far I have tried the above commented section of the code.I can crawl few pages with the default settings, and increase crawls to over a 100 with delays set to 30 seconds.I think the problem is with PHPSESSID being set "only once" at the very begining for every combination of proxy and user-agent, while the __cfduid is set for lifetime of the crawl for that combination.I solved the problem using 

scrapy cookiejar.Here is the code to set new cookies for every new request

return response from scrapy in python

pilogo

[return response from scrapy in python](https://stackoverflow.com/questions/58487474/return-response-from-scrapy-in-python)

I'm using scrapy to scrape certain websites in python. When the response is downloaded, I want to pass this response to the main program  I need to get the returned response after the reactor has run in the thread for post processing.

2019-10-21 13:34:08Z

I'm using scrapy to scrape certain websites in python. When the response is downloaded, I want to pass this response to the main program  I need to get the returned response after the reactor has run in the thread for post processing.

how to scrape websites that have loaders?

professional debugger

[how to scrape websites that have loaders?](https://stackoverflow.com/questions/58474049/how-to-scrape-websites-that-have-loaders)

i'm trying to scrape the website that contains loading screens. when i browse the website it shows loading.. for a sec and then it loads up. But the problem is when i try to scrape it using scrapy it gives me nothing (probably because of that loading). can i solve the problem using scrapy or should i use some other tools?

here's the link to the website if you wanna see https://www.graana.com/project/601/lotus-lake-towers

2019-10-20 14:33:42Z

i'm trying to scrape the website that contains loading screens. when i browse the website it shows loading.. for a sec and then it loads up. But the problem is when i try to scrape it using scrapy it gives me nothing (probably because of that loading). can i solve the problem using scrapy or should i use some other tools?

here's the link to the website if you wanna see https://www.graana.com/project/601/lotus-lake-towersAs it is sending a GET request to get information about the property , you should mimic the same in your code. (You can observe the GET call under console -> Network -> XHR )Output is in json format, convert it to your convenience. 

Scrapy - Scrape both text and images in the same spider

Iulia Mihet

[Scrapy - Scrape both text and images in the same spider](https://stackoverflow.com/questions/58488571/scrapy-scrape-both-text-and-images-in-the-same-spider)

Scrapy super noob here. Problem: I have an html page that contains both information that I want to scrape and an url that I want to follow to get images urls for images that I want to download and save via the scrapy image pipeline.My approach to achieve this:

1. Scrape all the details as usual with a parse method

2. Find the url in the initial page, create a request that has a second parse method as callback where I build the image_urls list.So, I have the following setup:This is not working, although with my little knowledge of both Scrapy and Python, the second parse method should return a list of image_urls. So I have 2 questions: 1. is there a better approach for my case? Maybe the whole issue is in trying to do too much with one spider? 2. If the approach is ok, what am I doing wrong?

2019-10-21 14:34:14Z

Scrapy super noob here. Problem: I have an html page that contains both information that I want to scrape and an url that I want to follow to get images urls for images that I want to download and save via the scrapy image pipeline.My approach to achieve this:

1. Scrape all the details as usual with a parse method

2. Find the url in the initial page, create a request that has a second parse method as callback where I build the image_urls list.So, I have the following setup:This is not working, although with my little knowledge of both Scrapy and Python, the second parse method should return a list of image_urls. So I have 2 questions: 1. is there a better approach for my case? Maybe the whole issue is in trying to do too much with one spider? 2. If the approach is ok, what am I doing wrong?

Why I get these strange characters with scrapy instead of html?

coderisimo

[Why I get these strange characters with scrapy instead of html?](https://stackoverflow.com/questions/58492377/why-i-get-these-strange-characters-with-scrapy-instead-of-html)

It's just a hobby task for me. I try to get booking.com start page html via scappy. and I get something strange in response.text. This is the part of response :This looks like fake data. Every time a response is different. When i try to do a similar request via a posman all working good. I get code - 302 response with a link that opens the site without errors.

I think booking detects that my code is a scraper, but I don't understand how. IP address the same with the postman, postman also ignored javascript, so I don't know what happens. Help me, please. Thank you!

2019-10-21 18:43:43Z

It's just a hobby task for me. I try to get booking.com start page html via scappy. and I get something strange in response.text. This is the part of response :This looks like fake data. Every time a response is different. When i try to do a similar request via a posman all working good. I get code - 302 response with a link that opens the site without errors.

I think booking detects that my code is a scraper, but I don't understand how. IP address the same with the postman, postman also ignored javascript, so I don't know what happens. Help me, please. Thank you!You are getting raw compressed response, which is quite strange, normally scrapy would handle HTTP session and compressed data all by itself thanks to CookiesMiddleware and HttpCompressionMiddleware, they are both enabled by default. Did you hard-coded Accept-Encoding in your code?

How to return nested JSON value to a dictionary?

Retarded Satan

[How to return nested JSON value to a dictionary?](https://stackoverflow.com/questions/58492600/how-to-return-nested-json-value-to-a-dictionary)

I'm using scrapy to scrape reviews from seek.com.au. I have found this link https://company-profiles-api.cloud.seek.com.au/v1/companies/432306/reviews?page=1 which has data I need encoded in JSON. The data looks like this :I have created a dictionary and then tried returning data but only 1 value gets returnedI expect the output to have all data but only this is returned :

2019-10-21 18:58:51Z

I'm using scrapy to scrape reviews from seek.com.au. I have found this link https://company-profiles-api.cloud.seek.com.au/v1/companies/432306/reviews?page=1 which has data I need encoded in JSON. The data looks like this :I have created a dictionary and then tried returning data but only 1 value gets returnedI expect the output to have all data but only this is returned :The dictionary I was creating was erasing my previous data. I created a list before looping and the problem was solved. 

How to use the proxy automatically when a 403 status code is encountered by the scrapy?

L.S

[How to use the proxy automatically when a 403 status code is encountered by the scrapy?](https://stackoverflow.com/questions/58496005/how-to-use-the-proxy-automatically-when-a-403-status-code-is-encountered-by-the)

In the process of grabbing, 403 status codes are encountered.

The requirement is: automatically use the agent when 403 status code is encountered.The check and response to the status code is configured in downloadermiddleware, but seems to work only for the first link

2019-10-22 01:27:14Z

In the process of grabbing, 403 status codes are encountered.

The requirement is: automatically use the agent when 403 status code is encountered.The check and response to the status code is configured in downloadermiddleware, but seems to work only for the first linkI recommend you create a new class inheriting scrapy's RetryMiddleware, and override the process_response function.

use scrapy import items

sh d

[use scrapy import items](https://stackoverflow.com/questions/58429163/use-scrapy-import-items)

When I try to import items, I have a problem. I don't understand why I say there is no such module? How can I solve this problem? Thank youitems.pyspider.pyerror:

2019-10-17 09:22:24Z

When I try to import items, I have a problem. I don't understand why I say there is no such module? How can I solve this problem? Thank youitems.pyspider.pyerror:Is your project named "tutorial"? The line in your spider from tutorial.items should be the project_name.items and case sensitive. 

Can't scrap few field using css selector and selector gadget

Yash Sethia

[Can't scrap few field using css selector and selector gadget](https://stackoverflow.com/questions/58361489/cant-scrap-few-field-using-css-selector-and-selector-gadget)

So, I have a project to scrape data from class-central.com.  I used selector gadget to get the .css tags for a few fields, but when I run my program, all I get is an empty field. I am unable to identify error in my code so please help!

(PS:I am new to scrapy and this is my first project. So please make sure that your answer is understandable for someone like me)open the following link to see the fields:https://www.classcentral.com/subject/csI am unable to scrape the following fields:

2019-10-13 07:37:39Z

So, I have a project to scrape data from class-central.com.  I used selector gadget to get the .css tags for a few fields, but when I run my program, all I get is an empty field. I am unable to identify error in my code so please help!

(PS:I am new to scrapy and this is my first project. So please make sure that your answer is understandable for someone like me)open the following link to see the fields:https://www.classcentral.com/subject/csI am unable to scrape the following fields:Actually your all_tr is just a list of all Course Name columns (instead of all table rows). That's why you can't get start_date from x (it's in another column).UPDATE For rating I grab data-timestamp attribute of the 4th column (Rating)If you look at the page source you'll find that some rows don't have course details (ads rows). That's why you get an error after 5 results. To get all course you need to modify all_tr selector:

Accessing attributes / data of Selenium “Selector” object, Python / Scrapy

NVUrbanNinja

[Accessing attributes / data of Selenium “Selector” object, Python / Scrapy](https://stackoverflow.com/questions/58361144/accessing-attributes-data-of-selenium-selector-object-python-scrapy)

I have created a Selector object to store the element selected by driver.page_source. I had issues iterating and accessing the data directly so I created a "Selector" object to allow for loop to iterate. If this is avoidable please let me know. The issue is I need to access some of the data inside that Selector object, specifically the id attribute of the element. Whenever I attempt to use a function on the object such as get_attribute, it states: "Selector" object has no attribute 'data'I attempted a variety of different accessing methods such as sub-scripting the value directly "['id']". That wasn't applicable. Does anyone have any idea how to access this data or perhaps rework my code to make it accessible? Also, you may notice I have set the value of my item['email'] equal to person.data('id'). I just wanted to try to get the id. When set to 'person' the output is the following in XML format:That is an XML version of the "Selector" object "person". 

2019-10-13 06:34:51Z

I have created a Selector object to store the element selected by driver.page_source. I had issues iterating and accessing the data directly so I created a "Selector" object to allow for loop to iterate. If this is avoidable please let me know. The issue is I need to access some of the data inside that Selector object, specifically the id attribute of the element. Whenever I attempt to use a function on the object such as get_attribute, it states: "Selector" object has no attribute 'data'I attempted a variety of different accessing methods such as sub-scripting the value directly "['id']". That wasn't applicable. Does anyone have any idea how to access this data or perhaps rework my code to make it accessible? Also, you may notice I have set the value of my item['email'] equal to person.data('id'). I just wanted to try to get the id. When set to 'person' the output is the following in XML format:That is an XML version of the "Selector" object "person". If you want an id attribute of the person Selector:

Exporting scrapped content to google sheets

Ibtsam Ch

[Exporting scrapped content to google sheets](https://stackoverflow.com/questions/58362177/exporting-scrapped-content-to-google-sheets)

I am willing to scrap a website for some information. It would be 3 to 4 columns. The difficult part is, i want to export all the data in to the google sheets and make the crawler run after some specific intervals. I 'll be using scrapy for this purpose. Any suggestions on how can i do this (by making custom pipeline or any other way as i don't have much experience in writing custom pipelines)

2019-10-13 09:22:27Z

I am willing to scrap a website for some information. It would be 3 to 4 columns. The difficult part is, i want to export all the data in to the google sheets and make the crawler run after some specific intervals. I 'll be using scrapy for this purpose. Any suggestions on how can i do this (by making custom pipeline or any other way as i don't have much experience in writing custom pipelines)You can use the Google API and  python pygsheets module. 

Refer this link for more details Click HerePlease see the sample code and this might help you.

How to end recursion when following a set of links for web scraping using Scrapy

user6235442

[How to end recursion when following a set of links for web scraping using Scrapy](https://stackoverflow.com/questions/58316075/how-to-end-recursion-when-following-a-set-of-links-for-web-scraping-using-scrapy)

My aim is to extract from as many articles as I can on a news website by following the links. However, after running this code, it doesn't seem like it will ever end unless I force stop it. Is there a good strategy to end this recursion when following links on a website? I also start on a specific article's page rather than the home page of the site. Secondly, what does Scrapy's response.urljoin do exactly? Any kind of example would be appreciated!

2019-10-10 05:37:49Z

My aim is to extract from as many articles as I can on a news website by following the links. However, after running this code, it doesn't seem like it will ever end unless I force stop it. Is there a good strategy to end this recursion when following links on a website? I also start on a specific article's page rather than the home page of the site. Secondly, what does Scrapy's response.urljoin do exactly? Any kind of example would be appreciated!Scrapy has a CLOSESPIDER_PAGECOUNT setting to close the spider after visiting a certain number of pages. You can set this in the normal ways e.g. response.urljoin just wraps the urllib.parse.urljoin function (for python 3!) in the standard library passing response.url as the first argument e.g.is equivalent toConstructs an absolute url by combining the Response’s url with a possible relative url.

Using Scrapy to scrape a webpage

Michael 

[Using Scrapy to scrape a webpage](https://stackoverflow.com/questions/58316345/using-scrapy-to-scrape-a-webpage)

I am trying to scrape the largest table from this page. I am working in conda prompthttp://www.tennisabstract.com/cgi-bin/player-classic.cgi?p=RafaelNadal&f=ACareerqqHere is my attempt:This seems to grab the correct table, but the rows I'm getting are not correct; they are coming from the top of the webpage.My other try was this:But this just returns an empty list.Any help is appreciatedThanks

2019-10-10 06:02:11Z

I am trying to scrape the largest table from this page. I am working in conda prompthttp://www.tennisabstract.com/cgi-bin/player-classic.cgi?p=RafaelNadal&f=ACareerqqHere is my attempt:This seems to grab the correct table, but the rows I'm getting are not correct; they are coming from the top of the webpage.My other try was this:But this just returns an empty list.Any help is appreciatedThankstable.extract() is returning empty table ['<table id="matches"></table>'] here. 

It seem data getting loaded dynamically in the table.

You would require to use Selenium or ScrapyJS to get data in such cases.

See the link for more info

Unaccounted time consumed by scrapy crawl

MrPandav

[Unaccounted time consumed by scrapy crawl](https://stackoverflow.com/questions/58234797/unaccounted-time-consumed-by-scrapy-crawl)

I have been trying to run a scrapy crawl in a time constrained envrionment where we want to make sure that scrapy stops after fixed run time limit. We have set the following CLOSESPIDER_TIMEOUT as 60 seconds and DOWNLOAD_TIMEOUT as 30 seconds.We have set the forceful termination (kill the subprocess) at 90 seconds to cover the edge case scenario if page request is made at 59 seconds and twisted fails to download page and download timeout signal is triggred. ( 59 + 30 = 89 < 90 seconds)but still scripts runs for random amount of time. range is between 102-115 seconds.I am not sure where this variable 15-30 seconds are being used and why scrapy is failing to close gracelully after 90 seconds as expectedI am looking forward to answer explaining scrapy/twisted reactor architecture where this lag is being introducedThanks

2019-10-04 10:40:55Z

I have been trying to run a scrapy crawl in a time constrained envrionment where we want to make sure that scrapy stops after fixed run time limit. We have set the following CLOSESPIDER_TIMEOUT as 60 seconds and DOWNLOAD_TIMEOUT as 30 seconds.We have set the forceful termination (kill the subprocess) at 90 seconds to cover the edge case scenario if page request is made at 59 seconds and twisted fails to download page and download timeout signal is triggred. ( 59 + 30 = 89 < 90 seconds)but still scripts runs for random amount of time. range is between 102-115 seconds.I am not sure where this variable 15-30 seconds are being used and why scrapy is failing to close gracelully after 90 seconds as expectedI am looking forward to answer explaining scrapy/twisted reactor architecture where this lag is being introducedThanks

Scrape urls from dynamic webpage using Scrapy

Hira 

[Scrape urls from dynamic webpage using Scrapy](https://stackoverflow.com/questions/58269081/scrape-urls-from-dynamic-webpage-using-scrapy)

I want to make a web scraper in Scrapy that extracts 10000 links of news from this website https://hamariweb.com/news/newscategory.aspx?cat=7

This webpage is dynamic when I scroll down more links load.I tried it with selenium but it's not working.The above mentioned code opens a browser in incognito mode and continues to scroll down.  I also want to extract 10000 news links and want to stop the browser when limit reached.

2019-10-07 11:55:39Z

I want to make a web scraper in Scrapy that extracts 10000 links of news from this website https://hamariweb.com/news/newscategory.aspx?cat=7

This webpage is dynamic when I scroll down more links load.I tried it with selenium but it's not working.The above mentioned code opens a browser in incognito mode and continues to scroll down.  I also want to extract 10000 news links and want to stop the browser when limit reached.You can add logic for gathering URLs to your parse() method by gathering the css hrefs:There's a lot of information regarding how to handle links in the scrapy tutorial  following links section. You can use the information there to learn what else you can do with links in scrapy.I haven't tested this with the infinite scroll, so you may need to make some changes, but this should get you going in the right direction.

how to make a POST request in Scrapy that requires Request payload

Sitanshu.K

[how to make a POST request in Scrapy that requires Request payload](https://stackoverflow.com/questions/58272009/how-to-make-a-post-request-in-scrapy-that-requires-request-payload)

I am trying to parse data from this website.

In Network section of inspect element i found this link https://busfor.pl/api/v1/searches that is used for a POST request that returns JSON i am interested in.

But for making this POST request there is request Payload with some dictionary. 

I assumed it like normal formdata that we use to make FormRequest in scrapy but it returns 403 error.

I have already tried the following.This returns 403 Error

I also tried this by referring to one of the StackOverflow post.But even this returns the same error.

Can someone help me. to figure out how to parse the required data using Scrapy.

2019-10-07 14:48:37Z

I am trying to parse data from this website.

In Network section of inspect element i found this link https://busfor.pl/api/v1/searches that is used for a POST request that returns JSON i am interested in.

But for making this POST request there is request Payload with some dictionary. 

I assumed it like normal formdata that we use to make FormRequest in scrapy but it returns 403 error.

I have already tried the following.This returns 403 Error

I also tried this by referring to one of the StackOverflow post.But even this returns the same error.

Can someone help me. to figure out how to parse the required data using Scrapy.The way to send POST requests with json data is the later, but you are passing a wrong json to the site, it expects a dictionary, not a list of dictionaries.

So instead of:You should use:Another thing you didn't notice are the headers passed to the POST request, sometimes the site uses IDs and hashes to control access to their API, in this case I found two values that appear to be needed, X-CSRF-Token and X-NewRelic-ID. Luckily for us these two values are available on the search page.Here is a working spider, the search result is available at the method self.parse_search.

Getting “Crawled 324 pages (at 133 pages/min), scraped 304 items (at 130 items/min)” after paginating 18 pages while there are 42 pages to scrap?

Danyal Mughal

[Getting “Crawled 324 pages (at 133 pages/min), scraped 304 items (at 130 items/min)” after paginating 18 pages while there are 42 pages to scrap?](https://stackoverflow.com/questions/58245976/getting-crawled-324-pages-at-133-pages-min-scraped-304-items-at-130-items-m)

While I have written a script to scrape data from the site and it is working ideally but after scraping about 18 pages' data(as there are about 42 pages), the scrapy get stuck by giving a log info after and after.I visited the similar questions answered on stackoverflow but in all of them the scripts were not working from the beginning while in my case the script scraped data from about 18 pages and then get stuck.Here is the scriptThis is the Logger infoAll the other files left default.

2019-10-05 06:37:37Z

While I have written a script to scrape data from the site and it is working ideally but after scraping about 18 pages' data(as there are about 42 pages), the scrapy get stuck by giving a log info after and after.I visited the similar questions answered on stackoverflow but in all of them the scripts were not working from the beginning while in my case the script scraped data from about 18 pages and then get stuck.Here is the scriptThis is the Logger infoAll the other files left default.Your page advance logic is correct, but it appears the server you're scraping may have some anti-scraping defense mechanisms in place.When running your code as-is I got similar results, scraping basically stops after a while. I suspect the server detects it's being scraped and either slows down or completely stops responding to the scraping requests.Just for test purposes I tweaked the code a bit to not hammer the server as bad, hoping to remain below the scraping detection radar:With these changes in place I can see the scraping (slowly) reaching past the page where it was stopping earlier and still going:Even with these tweaks scraping was detected, so I further tweaked it to skip more pages, eventually getting to the last page and displaying the But scrapy doesn't shutdown, you'll need some more tweaking for that.

Scrapy stops after running it, why?

Ahmed Maher

[Scrapy stops after running it, why?](https://stackoverflow.com/questions/58245453/scrapy-stops-after-running-it-why)

I got that error after running a python scrapy.

It shows that that scrapy got everything in place and started to crawl, but it stops immediately before crawling the first page. I tried to it several times with diffrent settings but I got the same below result.The scrapy code: This is the most part of code. 

2019-10-05 04:55:22Z

I got that error after running a python scrapy.

It shows that that scrapy got everything in place and started to crawl, but it stops immediately before crawling the first page. I tried to it several times with diffrent settings but I got the same below result.The scrapy code: This is the most part of code. Tried with your code, it goes to the dest_country callback, but there it doesn't find any link so it just exits.Maybe an issue with an xpath expression somewhere?

Does it make sense to run scrapy/selenium script on AWS?

AppliedResearcher

[Does it make sense to run scrapy/selenium script on AWS?](https://stackoverflow.com/questions/58251094/does-it-make-sense-to-run-scrapy-selenium-script-on-aws)

I am currently scraping a super slow website. Therefore, I already bought a windows2000 virtual machine. However, this machine is using a static IP. As a result, I got banned. Now, I am wondering if it would make sense to move my script (Scrapy, Selenium and Chrome) to AWS. Has anybody experiences in using AWS for crawling? What is the typical price for it?

2019-10-05 18:19:41Z

I am currently scraping a super slow website. Therefore, I already bought a windows2000 virtual machine. However, this machine is using a static IP. As a result, I got banned. Now, I am wondering if it would make sense to move my script (Scrapy, Selenium and Chrome) to AWS. Has anybody experiences in using AWS for crawling? What is the typical price for it?You don’t need a different machine, you need a proxy. Or even better, a smart proxy.

Crawling not successful with Scrapy

user11743010

[Crawling not successful with Scrapy](https://stackoverflow.com/questions/58251033/crawling-not-successful-with-scrapy)

I try to catch all product names of site https://www.kalkhoff-bikes.com/ with scrapy. But the result is lower as expected. What I do wrong? 

My first attempt was the following:After that I read, if there a dynamic content then the solution should be splash. So I tried this:Unfortunately, I didn't get all product names. Am I on the right track? 

2019-10-05 18:10:53Z

I try to catch all product names of site https://www.kalkhoff-bikes.com/ with scrapy. But the result is lower as expected. What I do wrong? 

My first attempt was the following:After that I read, if there a dynamic content then the solution should be splash. So I tried this:Unfortunately, I didn't get all product names. Am I on the right track? 

Scrapy Item pipelines not enabling

Elvijs

[Scrapy Item pipelines not enabling](https://stackoverflow.com/questions/58249560/scrapy-item-pipelines-not-enabling)

I am trying to set up a Scrapy spider inside Django app, that reads info from a page and posts it in Django's SQLite database using DjangoItems.

Right now it seems that scraper itself is working, however, it is not adding anything to database. My guess is that it happens because of scrapy not enabling any item pipelines. Here is the log:As I  can see, the scraper returns expected value as "{'product_title': ['Biezpiena sieriņš KĀRUMS vaniļas 45g']}", but it seems like it is not passed into pipeline because no pipelines are loaded.I have spent several hours looking at different tutorials and trying to fix the issue, but had no luck so far. Is there anything else I might have forgotten regarding setting up the scraper? Maybe it has something to do with file structure in the project.Here are relevant files.items.pypipelines.pysettings.pyspider product_info.py:Project file structure:

2019-10-05 15:07:12Z

I am trying to set up a Scrapy spider inside Django app, that reads info from a page and posts it in Django's SQLite database using DjangoItems.

Right now it seems that scraper itself is working, however, it is not adding anything to database. My guess is that it happens because of scrapy not enabling any item pipelines. Here is the log:As I  can see, the scraper returns expected value as "{'product_title': ['Biezpiena sieriņš KĀRUMS vaniļas 45g']}", but it seems like it is not passed into pipeline because no pipelines are loaded.I have spent several hours looking at different tutorials and trying to fix the issue, but had no luck so far. Is there anything else I might have forgotten regarding setting up the scraper? Maybe it has something to do with file structure in the project.Here are relevant files.items.pypipelines.pysettings.pyspider product_info.py:Project file structure:After further tinkering and research I found out that my settings file was not properly configured (however, it was only part of the problem). I added these lines to code based on other resources (they link Django project settings with scrapers settings):After that, the spider did not run, but this time it at least gave an error message about not finding the Django settings module. I don't remember the exact syntax but it was something like that: "broccoli.settings MODULE NOT FOUND"After some experiments I found out that moving scraper directory "scraper" from inside the app "Product_Scraper" to same level with other apps helped deal with this issue and everything worked.

twisted CRITICAL: Unhandled error in Deferred:

Ibtsam Ch

[twisted CRITICAL: Unhandled error in Deferred:](https://stackoverflow.com/questions/58191457/twisted-critical-unhandled-error-in-deferred)

I am using scrapy-splash to crawl this website and the spider is giving  "[twisted] CRITICAL: Unhandled error in Deferred:"Tried everything on the stack overflow and other websitesCode of my spider

2019-10-01 19:54:54Z

I am using scrapy-splash to crawl this website and the spider is giving  "[twisted] CRITICAL: Unhandled error in Deferred:"Tried everything on the stack overflow and other websitesCode of my spiderYou are yielding a set, not a dictionary. Can you try to yield a dictionary instead?Your set creation will fail because you can't add lists into a set.Try something like this instead:You probably also need to yeild your splash request:

scrapy html response after clicking no response in networks

hammad rauf

[scrapy html response after clicking no response in networks](https://stackoverflow.com/questions/58152581/scrapy-html-response-after-clicking-no-response-in-networks)

Site: https://www.wedding-spot.com/venue/13813/DoubleTree-Grand-Biscayne-Bay-Hotel/There is links tag which shows website link after clickingSource code has no link but when we click on links div is updated and link tag is shownhow to crawl that link using python3 lbrary scrapy

2019-09-29 05:40:29Z

Site: https://www.wedding-spot.com/venue/13813/DoubleTree-Grand-Biscayne-Bay-Hotel/There is links tag which shows website link after clickingSource code has no link but when we click on links div is updated and link tag is shownhow to crawl that link using python3 lbrary scrapy

Scraping with Scrapy and Django Integration

shahzad ch

[Scraping with Scrapy and Django Integration](https://stackoverflow.com/questions/58154325/scraping-with-scrapy-and-django-integration)

i am new in django . i am following this link . but when i run scrapy crawl example command in terminal it give error . i not unstand what problem in my code .setting.pyitem.pymodels.pywhen i run command scrapy crawl example it give error plz help me how to run and solve this  problem 

  ****File "C:\Users\Muhammad Shahzad\PycharmProjects\example_project\example_bot\example_bot\spiders\example.py", line 2, in 

    from example_bot.example_bot.items import ExampleDotcdComItem

ModuleNotFoundError: No module named 'example_bot.example_bot'

(venv) C:\Users\Muhammad Shahzad\PycharmProjects\example_project\example_bot**>**

2019-09-29 10:23:48Z

i am new in django . i am following this link . but when i run scrapy crawl example command in terminal it give error . i not unstand what problem in my code .setting.pyitem.pymodels.pywhen i run command scrapy crawl example it give error plz help me how to run and solve this  problem 

  ****File "C:\Users\Muhammad Shahzad\PycharmProjects\example_project\example_bot\example_bot\spiders\example.py", line 2, in 

    from example_bot.example_bot.items import ExampleDotcdComItem

ModuleNotFoundError: No module named 'example_bot.example_bot'

(venv) C:\Users\Muhammad Shahzad\PycharmProjects\example_project\example_bot**>**In your scrapper, you are importing your item i.e. from example_bot.example_bot.items import ExampleDotcdComItem and, it looks like there is an issue with the path of item which you are trying to load. Update your path and hopefully, it will workFirst bug i see is  The file name is item.py

but you are importing ExampleDotcdComItem from items.py

Bug:  modification:  

Why is scrapy splash returning me the original url?

gotye

[Why is scrapy splash returning me the original url?](https://stackoverflow.com/questions/58155784/why-is-scrapy-splash-returning-me-the-original-url)

It took a while but I finally understand where the discrepancies are coming from!scrapy crawl MeetupGetParticipants with url https://www.meetup.com/Google-Cloud_Meetup_Singapore_by_Cloud-Ace/events/264513425/attendees/Why is Splash returning the original url? Isn't the purpose of Splash to return the one rendered by render.html? What I want is the result of http://localhost:8050/render.html?url=https://www.meetup.com/Google-Cloud_Meetup_Singapore_by_Cloud-Ace/events/264513425/attendees/ (which gives me a rendered webpage). Bascically i could make it work by myself just tricking the url ... There is something I don't understand here.

2019-09-29 13:39:54Z

It took a while but I finally understand where the discrepancies are coming from!scrapy crawl MeetupGetParticipants with url https://www.meetup.com/Google-Cloud_Meetup_Singapore_by_Cloud-Ace/events/264513425/attendees/Why is Splash returning the original url? Isn't the purpose of Splash to return the one rendered by render.html? What I want is the result of http://localhost:8050/render.html?url=https://www.meetup.com/Google-Cloud_Meetup_Singapore_by_Cloud-Ace/events/264513425/attendees/ (which gives me a rendered webpage). Bascically i could make it work by myself just tricking the url ... There is something I don't understand here.Looks like I could make it work with a nice lua script :) It returns a rendered json response with everything I need in it.

How to use scrapy to get to the next chapter on fanfiction.net?

Cat_Gamer23

[How to use scrapy to get to the next chapter on fanfiction.net?](https://stackoverflow.com/questions/58159613/how-to-use-scrapy-to-get-to-the-next-chapter-on-fanfiction-net)

On fanfiction.net, this is the HTML code to get the chapters of a story:What I want is to use this to go to the next chapter and keep downloading the text content, but the normal way of doing it with it calling self.fanfiction() recursively which would not work because of the self.storyNum += 1 line.

2019-09-29 22:14:43Z

On fanfiction.net, this is the HTML code to get the chapters of a story:What I want is to use this to go to the next chapter and keep downloading the text content, but the normal way of doing it with it calling self.fanfiction() recursively which would not work because of the self.storyNum += 1 line.Do you really need to keep a counter for the story num?I think you can just yield the next page as long as you find one, something like:As mentioned in the comments, you should use an item pipeline to care about "storing" your items in your documents.Here is something to give you an idea, which works for me, and that you have to adapt to your use case:

Unable to scrape stats code 301 and 302 using scrapy

bobparker

[Unable to scrape stats code 301 and 302 using scrapy](https://stackoverflow.com/questions/58158766/unable-to-scrape-stats-code-301-and-302-using-scrapy)

Good Afternoon allI have run into a small issue trying to scrape data from job posting site, I am also somewhat newer to python and scrapy as a whole. I have a script that I am running to extract data from various indeed postings. The crawler seems to complete with no errors, though will not extract data from sites that respond with either a 301 or 302 error code.I have pasted the script and log at bottomAny help would be appreciated

2019-09-29 20:02:58Z

Good Afternoon allI have run into a small issue trying to scrape data from job posting site, I am also somewhat newer to python and scrapy as a whole. I have a script that I am running to extract data from various indeed postings. The crawler seems to complete with no errors, though will not extract data from sites that respond with either a 301 or 302 error code.I have pasted the script and log at bottomAny help would be appreciatedI just ran a quick test of your scraper and it seems to me that it's actually working as it's supposed to.EDIT: In an attempt to make my explanation more clear; you cannot scrape 301 or 302 redirects, because they are just that; redirects. If you request a URL that gets redirected, Scrapy automatically handles that for you and scrapes the data from the page that you get redirected to. It is the final destination from the redirect that will give you the 200 response.If you follow the logic I have presented below, you will see that Scrapy requests the URL http://www.indeed.com/rc/clk?jk=69995bf12d9f2f9a&fccid=b87e01ade6c824ee&vjs=3, but gets redirected to https://www.indeed.com/viewjob?jk=69995bf12d9f2f9a&from=serp&vjs=3. It is this final page that you will be able to scrape. (You can try it yourself by clicking the initial URL and comparing that to the final URL you end up on)Just to repeat myself, you will not be able to scrape anything from the 301 and 302 redirects (there is nothing there to scrape), only the final page that gets 200 response.I have attached a suggested version of your scraper that saves both the requested URL and the scraped URL. Everything looks fine to me, your scraper works as it is supposed to. (However, note that indeed.com will only server you up to 19 pages of search results, which limits you to 190 scraped items)I hope this makes better sense now.Here is one example from the output, starting with the original request:This gets redirected with a 301 redirect to the next link:Which again gets redirected with a 302 redirect to the next link:And finally, we can scrape the data:So the data is scraped from the final page that received a 200 response. Note that in the scraped item, the posting_url is the one passed in with the meta attribute, and not the actual scraped url. This may be what you want, but if you want to save the actual url that was scraped, then you should use posting_url = response.url instead. EDIT: See suggested update belowSuggested code update:According to the documentation of RedirectMiddleware, you have a couple of different ways of getting out of that situation:

Scrapy does not have command 'crawl'

taga

[Scrapy does not have command 'crawl'](https://stackoverflow.com/questions/58097126/scrapy-does-not-have-command-crawl)

I started to learn Scrapy but right away I get an error Unknown command: crawl.

I do not know why im getting this, but in py Scrapy commands I do not have that command.

Im using python 3.6 and pycharm as editor.This is the basic code that I wrote:

2019-09-25 11:15:43Z

I started to learn Scrapy but right away I get an error Unknown command: crawl.

I do not know why im getting this, but in py Scrapy commands I do not have that command.

Im using python 3.6 and pycharm as editor.This is the basic code that I wrote:You need to be inside the project folder within the Scrapy folder.You are currently trying to run the command from C:\Users\Pc\PycharmProjects\web skreper\venv\Scriptsbut it should be something like C:\Users\Pc\PycharmProjects\web skreper\venv\Scripts\Scrapy\My_ScraperYou must get out from the script folder and move to the project directory where scrapy.cfg file placed. Run the scrapy crawl from there it will work.

Stuck Scraping Multiple Domains sequentially - Python Scrapy

bobparker

[Stuck Scraping Multiple Domains sequentially - Python Scrapy](https://stackoverflow.com/questions/58109358/stuck-scraping-multiple-domains-sequentially-python-scrapy)

I am fairly new to python as well as web scraping. My first project is web scraping random Craiglist cities (5 cities total) under the transportation sub-domain (i.e. https://dallas.craigslist.org), though I am stuck on having to manually run the script per city after manually updating each cities respective domain under the constants >>>> (start_urls = and   absolute_next_url = ) in the script . Is there anyway that I can adjust the script to sequentially run through the cities I have defined (i.e. miami, new york, houston, chicago, etc), and auto-populate the constants for its respective city (start_urls = and  absolute_next_url = ) ?Also, is there a way to adjust the script to output each city into its own .csv >> (i.e. miami.csv, houston.csv, chicago.csv, etc) ?   Thank you in advance

2019-09-26 03:57:04Z

I am fairly new to python as well as web scraping. My first project is web scraping random Craiglist cities (5 cities total) under the transportation sub-domain (i.e. https://dallas.craigslist.org), though I am stuck on having to manually run the script per city after manually updating each cities respective domain under the constants >>>> (start_urls = and   absolute_next_url = ) in the script . Is there anyway that I can adjust the script to sequentially run through the cities I have defined (i.e. miami, new york, houston, chicago, etc), and auto-populate the constants for its respective city (start_urls = and  absolute_next_url = ) ?Also, is there a way to adjust the script to output each city into its own .csv >> (i.e. miami.csv, houston.csv, chicago.csv, etc) ?   Thank you in advanceThere might be a cleaner way but check out https://docs.scrapy.org/en/latest/topics/practices.html?highlight=multiple%20spiders and you can basically combine multiple instances of your spider together, so you can have a separate 'class' for each city. There are probably some ways to consolidate some code so it's not all repeated.As for writing to csv, are you doing that via the command line right now? I'd add the code to the spider itself https://realpython.com/python-csv/

How do I create a custom ResponseType in Scrapy?

Donny Hayward

[How do I create a custom ResponseType in Scrapy?](https://stackoverflow.com/questions/58100973/how-do-i-create-a-custom-responsetype-in-scrapy)

I'm interested in extending Scrapy in my project by adding a custom ResponseType.  For now, I want to add a PDF type that will use PDFMiner to return the plain text and structure of the document (including links).  I may want to add other document types later.It looks like the mapping from mime types to response classes happens in scrapy.responsetypes -- specifically the CLASSES dictionary in ResponseTypes.  However, it's not clear if/how that is intended to be overridden with custom response classes.  The responsetypes variable at the end of that file gets referenced directly in a few other places, and I don't see any reference to this class in the Scrapy project settings.  Additionally, I wasn't able to find anything about how to do this in the Scrapy documentation.I could of course fork Scrapy and use my own version with custom classes defined, but that would introduce a maintenance headache.  What's the best way to define custom response types in Scrapy?

2019-09-25 14:40:06Z

I'm interested in extending Scrapy in my project by adding a custom ResponseType.  For now, I want to add a PDF type that will use PDFMiner to return the plain text and structure of the document (including links).  I may want to add other document types later.It looks like the mapping from mime types to response classes happens in scrapy.responsetypes -- specifically the CLASSES dictionary in ResponseTypes.  However, it's not clear if/how that is intended to be overridden with custom response classes.  The responsetypes variable at the end of that file gets referenced directly in a few other places, and I don't see any reference to this class in the Scrapy project settings.  Additionally, I wasn't able to find anything about how to do this in the Scrapy documentation.I could of course fork Scrapy and use my own version with custom classes defined, but that would introduce a maintenance headache.  What's the best way to define custom response types in Scrapy?There is no simple way to do that. Scrapy expects you to be satisfied with its default response types.

You could replace the download handlers and middlewares which use this class, but that would probably be too much work...The simplest way to get a similar result would probably be to just add an additional downloader middleware which will:This does do additional work (creates an additional response object after the normal processing is done), but it's much simpler to implement than the alternative.

Selenium & Scrapy: Last URL overwrites other URLs

AppliedResearcher

[Selenium & Scrapy: Last URL overwrites other URLs](https://stackoverflow.com/questions/58047980/selenium-scrapy-last-url-overwrites-other-urls)

I am currently trying to crawl data from three websites (three different URLs). Therefore, I am using a text-file to load the different URLs into the start_url. 

At the moment, there are three URLs in my file. However, the script just saves/overwrites the data of the two URLs before.This is my code:

2019-09-22 09:42:00Z

I am currently trying to crawl data from three websites (three different URLs). Therefore, I am using a text-file to load the different URLs into the start_url. 

At the moment, there are three URLs in my file. However, the script just saves/overwrites the data of the two URLs before.This is my code:

permanent fix to scrapy not found

hadesfv

[permanent fix to scrapy not found](https://stackoverflow.com/questions/58051146/permanent-fix-to-scrapy-not-found)

I have a problem with scrapy on my server,each time I open I have to do the answer on [scrapy not found][1][1]: Scrapy installed, but won't run from the command line export PATH="${PATH}:${HOME}/.local/bin" it does work but I am looking for solution that fixes this permanently (I have sudo access)Error: bash: scrapy: command not found

2019-09-22 16:21:49Z

I have a problem with scrapy on my server,each time I open I have to do the answer on [scrapy not found][1][1]: Scrapy installed, but won't run from the command line export PATH="${PATH}:${HOME}/.local/bin" it does work but I am looking for solution that fixes this permanently (I have sudo access)Error: bash: scrapy: command not foundYou may want to append this command to your ~/.bashrc file so that it gets executed each time you SSH onto the server.Log in to your server and run the following command like it is suggested in Scrapy installed, but won't run from the command line:

Want to understand Robots.txt

TheGr8Destructo

[Want to understand Robots.txt](https://stackoverflow.com/questions/58050092/want-to-understand-robots-txt)

I would like to scrape a website. However I want to make sense of the robots.txt before I do.

The  lines that I don't understand are Does the User Agent Line mean access is ok anywhere? But then I have the Disallow line which is the main one I am concerned about. Does it mean don't access 8 layers deep, or don't access at all? 

2019-09-22 14:21:01Z

I would like to scrape a website. However I want to make sense of the robots.txt before I do.

The  lines that I don't understand are Does the User Agent Line mean access is ok anywhere? But then I have the Disallow line which is the main one I am concerned about. Does it mean don't access 8 layers deep, or don't access at all? I believe one simply interpret the robot.txt file with regex. The star can usually be interpreted as anything/everything.The User-Agent line User-agent: * does not mean you are allowed to scrap everything, it simply means the following rules apply to all user-agents. Here are examples of User-Agentswhich must comply with the same rules, that is:Finally, here are insightful examples and more on the topic.

Scrapy cannot find inside <div> tags

XRemixX

[Scrapy cannot find inside <div> tags](https://stackoverflow.com/questions/58006714/scrapy-cannot-find-inside-div-tags)

good day. I'm currently writing a Scrapy program to scrap a news website. I'm a beginner in Scrapy, and I've come into a bump that unable me to progress in my code. The website that I'm currrently trying to scrap is https://www.thestar.com.my/news/nationInside the page's html tags, there's a div tag with class="row list-listing". I'm trying to get the paragraph tag inside the div tag, but somehow Scrapy can't seem to find the tag. I've checked for the any not closed tags, but all of them seem to be closed. So why did Scrapy unable to fetch this tag? The most inner tag that Scrapy can fetch is div class="sub-section-list" which is outside the div class="row list-listing"Also, when I fetch the div class="sub-section-list" tag, it only extract these html tags:When inspecting the website, there are these tags that I needWebsite TagI will include my basic code. I've only started the project so I haven't made any progress since this problem. If I forgot to add any other necessary things please tell. Any help would be very appreciated.

2019-09-19 08:22:30Z

good day. I'm currently writing a Scrapy program to scrap a news website. I'm a beginner in Scrapy, and I've come into a bump that unable me to progress in my code. The website that I'm currrently trying to scrap is https://www.thestar.com.my/news/nationInside the page's html tags, there's a div tag with class="row list-listing". I'm trying to get the paragraph tag inside the div tag, but somehow Scrapy can't seem to find the tag. I've checked for the any not closed tags, but all of them seem to be closed. So why did Scrapy unable to fetch this tag? The most inner tag that Scrapy can fetch is div class="sub-section-list" which is outside the div class="row list-listing"Also, when I fetch the div class="sub-section-list" tag, it only extract these html tags:When inspecting the website, there are these tags that I needWebsite TagI will include my basic code. I've only started the project so I haven't made any progress since this problem. If I forgot to add any other necessary things please tell. Any help would be very appreciated.As Wim says the page is being loaded dynamically so there are a few options.

Using Firefox developer tools it looks like the content is being retrieved from: So you could directly load the json and get what you want from there. Something like:Of course, this probably isn't exactly what you want, but perhaps it is a good start?(Note that the above code is overkill for what it does, but if you are going to start some scraping you can work from that)The content is loaded dynamically so you won't be able to use xpath like this without rendering the page. It seems the article bodies are present in the html, and you can get it as follows:

Get all link text and href in a page using scrapy

Hjin

[Get all link text and href in a page using scrapy](https://stackoverflow.com/questions/58021794/get-all-link-text-and-href-in-a-page-using-scrapy)

this is my code but title_to_save and href_to_save return NoneI want to get all text inside tag "a" and its href 

2019-09-20 04:30:59Z

this is my code but title_to_save and href_to_save return NoneI want to get all text inside tag "a" and its href You want Note the dot before the path

(I use get instead of extract_first due to this).On the output csv, perhaps you are aware but you should probably yield the information you want to write out and then run your spider using the -o data_information/link.csv option which is a bit more flexible than opening a file for appending in your parse method. So your code would look something like

How to properly use Xpath to scrape AJAX data with scrapy?

Simon LE FOURN

[How to properly use Xpath to scrape AJAX data with scrapy?](https://stackoverflow.com/questions/57952784/how-to-properly-use-xpath-to-scrape-ajax-data-with-scrapy)

I am scraping this website, most of the data I need is rendered with Ajax. I have been at first trying to scrape it with Ruby (as it is the language I know the best) but it did not workout.

Then I was advised to do it with Python and Scrapy which I tried but I do not understand why can't I get the data.And when I run this on my terminal, I get the attempted result for codebut for tarifs I get "None".Do you please have any idea of what is wrong in my code ? I have tried differents way to scrape but none has worked. 

Maybe the xpath is not correct ? Or maybe my Python syntax is bad, I have only be using Python since I am trying to scrape this webpage.

2019-09-16 08:12:10Z

I am scraping this website, most of the data I need is rendered with Ajax. I have been at first trying to scrape it with Ruby (as it is the language I know the best) but it did not workout.

Then I was advised to do it with Python and Scrapy which I tried but I do not understand why can't I get the data.And when I run this on my terminal, I get the attempted result for codebut for tarifs I get "None".Do you please have any idea of what is wrong in my code ? I have tried differents way to scrape but none has worked. 

Maybe the xpath is not correct ? Or maybe my Python syntax is bad, I have only be using Python since I am trying to scrape this webpage.The reason why your XPath does not works - because of this data is adding from AJAX requests. If you open dev console in browser and move to Network->XHR - you will see AJAX request. Then there is 2 possible solution:

 1. Make this request manually in your script

 2. Use Js render like Splash

In this case, using the Splash will be easiest because of response from AJAX are Js files and not all data are presented there.

Also, I would recommend looking at the Aquarium, a tool that has Splash, HAProxy, and docker-compose

Using CrawlSpider rules in Scrapy

Hussar

[Using CrawlSpider rules in Scrapy](https://stackoverflow.com/questions/58018033/using-crawlspider-rules-in-scrapy)

I'm trying to build a crawler that will crawl a list of sites by following all links in their first page, then repeating this for the new pages. I think I might be incorrectly using the rules attribute. The spider never calls the processor method. It seams that no links are ever followed and there are no error messages. I've omitted some of the functions to show the changes I made to add crawling. I'm using Scrapy 1.5

2019-09-19 19:45:11Z

I'm trying to build a crawler that will crawl a list of sites by following all links in their first page, then repeating this for the new pages. I think I might be incorrectly using the rules attribute. The spider never calls the processor method. It seams that no links are ever followed and there are no error messages. I've omitted some of the functions to show the changes I made to add crawling. I'm using Scrapy 1.5Try add after your code and change your callback to parse:

stopping scrapy crawl and make it through close spider

Piggydog

[stopping scrapy crawl and make it through close spider](https://stackoverflow.com/questions/57959387/stopping-scrapy-crawl-and-make-it-through-close-spider)

Hello, I used the above command to run a spider on VM on GCP,Is there a way to stop the crawling and make it go through the close_spider function, Ideally I want to make like "ctrl+c" hence it goes through the final functions, I am not sure if kill PID_NUMBER will make go through it?Any help is appreciated

2019-09-16 14:50:58Z

Hello, I used the above command to run a spider on VM on GCP,Is there a way to stop the crawling and make it go through the close_spider function, Ideally I want to make like "ctrl+c" hence it goes through the final functions, I am not sure if kill PID_NUMBER will make go through it?Any help is appreciated

Sending data to input form with scrapy

Ibtsam Ch

[Sending data to input form with scrapy](https://stackoverflow.com/questions/57957748/sending-data-to-input-form-with-scrapy)

I want to send data into the input text-field on this site https://www.vtinfo.com/PF/product_finder.asp. I am not able to figure out how to do it.I have tried using the FormRequest method but it didn't work

2019-09-16 13:17:55Z

I want to send data into the input text-field on this site https://www.vtinfo.com/PF/product_finder.asp. I am not able to figure out how to do it.I have tried using the FormRequest method but it didn't work

Unable to get the pagination crawler to work Python3

Ickhartz

[Unable to get the pagination crawler to work Python3](https://stackoverflow.com/questions/57966928/unable-to-get-the-pagination-crawler-to-work-python3)

I m trying to use the scrapy module in python to scrape the details, but I am currently stuck on trying to get the pagination crawler to work. I'm getting the output partially right, but as I said previously, it is not scraping from the following pages on the website

2019-09-17 03:24:03Z

I m trying to use the scrapy module in python to scrape the details, but I am currently stuck on trying to get the pagination crawler to work. I'm getting the output partially right, but as I said previously, it is not scraping from the following pages on the websiteYou are not creating the structure of pagination properly. It is not advised to implement pagination and the yielding of items in a single method. Take a look at the sample code below:See that parse_pagination is implemented and how rules are implemented to call the method. If you are novice and don't know much about rules, I prefer you give them a look. They will help you alot in your journey ahead. Also, try to implement a modular approach. 

The rules above call only two things; if they see a product, they call parse_item and if they see the next page, they call parse_pagination. I hope you understand my point. Best of luck!

New to Scrapy - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)

AppliedResearcher

[New to Scrapy - INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)](https://stackoverflow.com/questions/57924547/new-to-scrapy-info-crawled-0-pages-at-0-pages-min-scraped-0-items-at-0-it)

I am new to Scrapy but spending massive effort on it the last days. However, I still fail at the basics. I am trying to crawl the following website: https://blogabet.com/tipsters

My objective is to download all Links to the userprofiles. For instance, https://sabobic.blogabet.com/When I am using scrapy shell, I can extract the specific xpath. But when I try to use a script and start it with "scrapy crawl ....". I am always getting no results. INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)What is wrong in my code? 

2019-09-13 13:28:03Z

I am new to Scrapy but spending massive effort on it the last days. However, I still fail at the basics. I am trying to crawl the following website: https://blogabet.com/tipsters

My objective is to download all Links to the userprofiles. For instance, https://sabobic.blogabet.com/When I am using scrapy shell, I can extract the specific xpath. But when I try to use a script and start it with "scrapy crawl ....". I am always getting no results. INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)What is wrong in my code? 

xpath selector - website is using same class for different datapoints

AppliedResearcher

[xpath selector - website is using same class for different datapoints](https://stackoverflow.com/questions/57929919/xpath-selector-website-is-using-same-class-for-different-datapoints)

I am trying to scrape the following website: https://blogabet.com/tipstersOn this homepage, I would like to scrape all information related to one username. Therefore, I am using the

class="tipster-info col-lg-10 no-padding".However, unfortunately the number of picks, followers as well as the since registered date are using the same class number xh-highlight.My question is: How can I scrape each datafield (picks, followers, since) individually when they are all using the same class?Thanks :)This is how my code looks like atm. 

2019-09-13 20:20:54Z

I am trying to scrape the following website: https://blogabet.com/tipstersOn this homepage, I would like to scrape all information related to one username. Therefore, I am using the

class="tipster-info col-lg-10 no-padding".However, unfortunately the number of picks, followers as well as the since registered date are using the same class number xh-highlight.My question is: How can I scrape each datafield (picks, followers, since) individually when they are all using the same class?Thanks :)This is how my code looks like atm. 

How to configure WebSphere portal to be crawled by Scrapy and ingested to elasticsearch

Espresso

[How to configure WebSphere portal to be crawled by Scrapy and ingested to elasticsearch](https://stackoverflow.com/questions/57932470/how-to-configure-websphere-portal-to-be-crawled-by-scrapy-and-ingested-to-elasti)

I want to crawl Websphere Portal8 using Scrapy, and ingest into elasticsearch. Do I need to configure anything special in WP to be scroll/index friendly? e.g avoiding javascript based pages generation.At present the crawling is getting only handful of pages, not 50k urls as expected.

2019-09-14 04:34:11Z

I want to crawl Websphere Portal8 using Scrapy, and ingest into elasticsearch. Do I need to configure anything special in WP to be scroll/index friendly? e.g avoiding javascript based pages generation.At present the crawling is getting only handful of pages, not 50k urls as expected.

Best way to crawl/extract data from multiple websites

Farukh Khan

[Best way to crawl/extract data from multiple websites](https://stackoverflow.com/questions/57855657/best-way-to-crawl-extract-data-from-multiple-websites)

I am new to the mentioned problem and just started learning about scraping and crawling the websites. I came to know about python BeautifulSoup4 that works really great for the extraction of data from a website. My main is problem is to handle a large number of sites. For example more than 10000 different websites. 

As far as I know and studied till now, that the crawling is done when we tell the program about the classes of different tags such as so for extracting it from a page. 

The python scrapy program will look like this.From the above example, it is clear that we have a site in which there is h3 tag having a class "this". Now for more than 10000 different websites, having different classes and structures. What is the best way to do that? I am trying to develop a type of search engine just like "Google" but with some specific amount of websites (i.e. 10000 or may be more in future).

2019-09-09 14:05:40Z

I am new to the mentioned problem and just started learning about scraping and crawling the websites. I came to know about python BeautifulSoup4 that works really great for the extraction of data from a website. My main is problem is to handle a large number of sites. For example more than 10000 different websites. 

As far as I know and studied till now, that the crawling is done when we tell the program about the classes of different tags such as so for extracting it from a page. 

The python scrapy program will look like this.From the above example, it is clear that we have a site in which there is h3 tag having a class "this". Now for more than 10000 different websites, having different classes and structures. What is the best way to do that? I am trying to develop a type of search engine just like "Google" but with some specific amount of websites (i.e. 10000 or may be more in future).

I'm having trouble finding a string within a scraped item

reg202

[I'm having trouble finding a string within a scraped item](https://stackoverflow.com/questions/57930348/im-having-trouble-finding-a-string-within-a-scraped-item)

I've got a very simple spider but when I try to search the contents of the item I'm scraping, it's not being foundThe url being scraped:

https://www.filmlinc.org/nyff2019/films/the-irishman/the spiderBut the spider doesn't find the word "Standby" in the scraped field. However, if I manually get it to print(item['title'])I get the following, which clearly has the word in it. I've removed some empty spacesI'm not sure why I'm having trouble? Is the title item a list? Is there a way to search it properly? Eventually the spider will be looking for a scenario when "Standby" is NOT found, but it's obviously I can't do that just yet, if it makes a difference in any possible suggestions.Thanks!

2019-09-13 21:06:39Z

I've got a very simple spider but when I try to search the contents of the item I'm scraping, it's not being foundThe url being scraped:

https://www.filmlinc.org/nyff2019/films/the-irishman/the spiderBut the spider doesn't find the word "Standby" in the scraped field. However, if I manually get it to print(item['title'])I get the following, which clearly has the word in it. I've removed some empty spacesI'm not sure why I'm having trouble? Is the title item a list? Is there a way to search it properly? Eventually the spider will be looking for a scenario when "Standby" is NOT found, but it's obviously I can't do that just yet, if it makes a difference in any possible suggestions.Thanks!You are trying to find a string in the list which looks for entire item while all you need a string to cast the list into str

Crawlera, cookies, sessions, rate limiting

kenshin

[Crawlera, cookies, sessions, rate limiting](https://stackoverflow.com/questions/57854366/crawlera-cookies-sessions-rate-limiting)

I'm trying to use scrapinghub to crawl a website that heavily limits request rate.If I run the spider as-is, I get 429 pretty soon.If I enable crawlera as per standard instructions, the spider doesn't work anymore.If I set headers = {"X-Crawlera-Cookies": "disable"} the spider works again, but I get 429s -- so I assume the limiter works (also) on the cookie.So what would an approach be here?

2019-09-09 12:47:34Z

I'm trying to use scrapinghub to crawl a website that heavily limits request rate.If I run the spider as-is, I get 429 pretty soon.If I enable crawlera as per standard instructions, the spider doesn't work anymore.If I set headers = {"X-Crawlera-Cookies": "disable"} the spider works again, but I get 429s -- so I assume the limiter works (also) on the cookie.So what would an approach be here?You can try RandomUserAgent, If you don't want to write your own implementation, you can try use this:https://github.com/cnu/scrapy-random-useragent

How to fix Crawled (403)

MD Palash Babu

[How to fix Crawled (403)](https://stackoverflow.com/questions/57853713/how-to-fix-crawled-403)

I am using python 3 and scrapy. I am fetching in scrapy shell with this code:and it's showing DEBUG: Crawled (403) Please share any idea with return 200 response in scrapy shell.

2019-09-09 12:06:08Z

I am using python 3 and scrapy. I am fetching in scrapy shell with this code:and it's showing DEBUG: Crawled (403) Please share any idea with return 200 response in scrapy shell.If you open it in your browser it's showing fill captcha to proceed. So for high traffic from a computer, it will ask for extra authentication. Hence you're seeing 403403 error - because website showing a captcha.

If resolve the captcha and extract cookie it will be work.

Example with requests for debug:You need to mimic exact same header as a real browser does

Special characters in scrapy rules

raul

[Special characters in scrapy rules](https://stackoverflow.com/questions/57812518/special-characters-in-scrapy-rules)

I'm trying to scrape a news site: https://www.larazon.es/etiquetas/noticias/meta/politica#.p:3;

I tested first the response with the following script and I see it works:However, when adding my selectors and rules I'm not getting any response. I'm new with scrapy but I have 2 assumptions of what might be happening:any ideas of what I'm missing?

Many thanks!

2019-09-05 20:22:37Z

I'm trying to scrape a news site: https://www.larazon.es/etiquetas/noticias/meta/politica#.p:3;

I tested first the response with the following script and I see it works:However, when adding my selectors and rules I'm not getting any response. I'm new with scrapy but I have 2 assumptions of what might be happening:any ideas of what I'm missing?

Many thanks!

Could not find environment ScrapyEnvironment

Anwesa Roy

[Could not find environment ScrapyEnvironment](https://stackoverflow.com/questions/57817469/could-not-find-environment-scrapyenvironment)

I am trying to scrape a website using scrapy for which I am trying to run Spyder in Anaconda Navigator. For that, I have created an environment called ScrapyEnvironment and I am trying to activate the ScrapyEnvironment from command prompt. However the messege that is being shown here is thatHowever, when I run the command conda info --envs in command prompt, it is displaying the conda environment as follows:

2019-09-06 07:21:51Z

I am trying to scrape a website using scrapy for which I am trying to run Spyder in Anaconda Navigator. For that, I have created an environment called ScrapyEnvironment and I am trying to activate the ScrapyEnvironment from command prompt. However the messege that is being shown here is thatHowever, when I run the command conda info --envs in command prompt, it is displaying the conda environment as follows:

How should I be formatting my yield requests?

Conrad Dubois

[How should I be formatting my yield requests?](https://stackoverflow.com/questions/57824376/how-should-i-be-formatting-my-yield-requests)

My scrapy spider is very confused, or I am, but one of us is not working as intended. My spider pulls start url's from a file and is supposed to: Start on an Amazon search page, crawl the page and grab the url's of each search result, follow the link to the items page, crawl the items page for information on the item, once all items have been crawled on the first page follow pagination up to page X, rinse and repeat.I am using ScraperAPI and Scrapy-user-agent to randomize my middlewares. I have formatted my start_requests with a priority based on their index in the file, so they should be crawled in order. I have checked and ensured that I AM receiving a successful 200 html response with the actual html from the Amazon page. Here is the code for the spider:For the FIRST start url, it will crawl three or four items and then it will jump to the SECOND start url. It will skip processing the remaining items and pagination pages, going directly to the second start url. For the second url, it will crawl three or four items, then it again will skip to the THIRD start url. It continues in this way, grabbing three or four items, then skipping to the next URL until it reaches the final start url. It will completely gather all information on this URL. Sometimes the spider COMPLETELY SKIPS the first or second starting url. This happens infrequently, but I have no idea as to what could cause this.My code for following result item URL's works fine, but I never get the print statement for "starting pagination" so it is not correctly following pages. Also, there is something odd with middlewares. It begins parsing before it has assigned a middleware

2019-09-06 14:56:25Z

My scrapy spider is very confused, or I am, but one of us is not working as intended. My spider pulls start url's from a file and is supposed to: Start on an Amazon search page, crawl the page and grab the url's of each search result, follow the link to the items page, crawl the items page for information on the item, once all items have been crawled on the first page follow pagination up to page X, rinse and repeat.I am using ScraperAPI and Scrapy-user-agent to randomize my middlewares. I have formatted my start_requests with a priority based on their index in the file, so they should be crawled in order. I have checked and ensured that I AM receiving a successful 200 html response with the actual html from the Amazon page. Here is the code for the spider:For the FIRST start url, it will crawl three or four items and then it will jump to the SECOND start url. It will skip processing the remaining items and pagination pages, going directly to the second start url. For the second url, it will crawl three or four items, then it again will skip to the THIRD start url. It continues in this way, grabbing three or four items, then skipping to the next URL until it reaches the final start url. It will completely gather all information on this URL. Sometimes the spider COMPLETELY SKIPS the first or second starting url. This happens infrequently, but I have no idea as to what could cause this.My code for following result item URL's works fine, but I never get the print statement for "starting pagination" so it is not correctly following pages. Also, there is something odd with middlewares. It begins parsing before it has assigned a middleware

selecting distinct values of an html attribute with xpath

L_S_P

[selecting distinct values of an html attribute with xpath](https://stackoverflow.com/questions/57817275/selecting-distinct-values-of-an-html-attribute-with-xpath)

I need to select the reference value, and some other values (e.g. delivery_out_of_stock) from this code with xpath (or css if it's better).Any suggestion on how to approach it?

2019-09-06 07:07:46Z

I need to select the reference value, and some other values (e.g. delivery_out_of_stock) from this code with xpath (or css if it's better).Any suggestion on how to approach it?I would use xpath to get the data in data-product, and then load the json as a dictionary to find the values you need:In my experience, css is better. I recommend downloading selector gadget extension for chrome for getting the values you want from a page.

Run Multiple Query on Scrapy Pipeline

FH337

[Run Multiple Query on Scrapy Pipeline](https://stackoverflow.com/questions/57826576/run-multiple-query-on-scrapy-pipeline)

I need to run multiple query to sort all of data after being crawled. I do some code in the Pipeline as follows :Multiple Queries goes here :But they rise error mysql.connector.errors.DatabaseError: 2014 (HY000): Commands out of sync; you can't run this command nowI already tried using open/closing connection as follows :But it keeps closing the connection and yield this errormysql.connector.errors.ProgrammingError: Cursor is not connectedThe expected result is that : All query can be run, although some of them takes time, so there will be some delay in the crawling process

2019-09-06 17:51:38Z

I need to run multiple query to sort all of data after being crawled. I do some code in the Pipeline as follows :Multiple Queries goes here :But they rise error mysql.connector.errors.DatabaseError: 2014 (HY000): Commands out of sync; you can't run this command nowI already tried using open/closing connection as follows :But it keeps closing the connection and yield this errormysql.connector.errors.ProgrammingError: Cursor is not connectedThe expected result is that : All query can be run, although some of them takes time, so there will be some delay in the crawling process

scrapy spider is not starting start_requests function

Jaideep Sai

[scrapy spider is not starting start_requests function](https://stackoverflow.com/questions/57788276/scrapy-spider-is-not-starting-start-requests-function)

Scrapy script stops abruptly after the spider is inititialised before the call of start_requests function.

I'm using Crawlrunner to initialise spider from a python script.python script :and the spider looks like: the script stops abruptly after this log message:

2019-09-04 12:19:01Z

Scrapy script stops abruptly after the spider is inititialised before the call of start_requests function.

I'm using Crawlrunner to initialise spider from a python script.python script :and the spider looks like: the script stops abruptly after this log message:

How can I grab all the results of an amazon search using Scrapy?

Conrad Dubois

[How can I grab all the results of an amazon search using Scrapy?](https://stackoverflow.com/questions/57793770/how-can-i-grab-all-the-results-of-an-amazon-search-using-scrapy)

I am trying to scrape info on shirts from Amazon. My spider currently accepts a list of keywords and uses them to perform a search on Amazon. For each search page I call the parse function. I want to grab each of the resulting items and further inspect them using scrapy's "reponse.follow(...)" method.I am currently trying to do this using "response.css('.s-result-item')" to get all the results. I have also tried using "response.css('.sg-col-inner'). Either way, it gets some of the results but not all of them, and sometimes it will only get two or three per page. If I add .extract() to the statement it completely fails. Here is my parse method:I am new to Python and Scrapy, I do not know if I should be using reponse.follow or scrapy.Request, or if that would even make a difference. Any ideas?

2019-09-04 18:10:25Z

I am trying to scrape info on shirts from Amazon. My spider currently accepts a list of keywords and uses them to perform a search on Amazon. For each search page I call the parse function. I want to grab each of the resulting items and further inspect them using scrapy's "reponse.follow(...)" method.I am currently trying to do this using "response.css('.s-result-item')" to get all the results. I have also tried using "response.css('.sg-col-inner'). Either way, it gets some of the results but not all of them, and sometimes it will only get two or three per page. If I add .extract() to the statement it completely fails. Here is my parse method:I am new to Python and Scrapy, I do not know if I should be using reponse.follow or scrapy.Request, or if that would even make a difference. Any ideas?I have accomplished this using: for next_page in response.css("h2.a-size-mini a").xpath("@href").extract():

File “<frozen importlib._bootstrap>”, line 1006, in _gcd_import error

Jason Brown

[File “<frozen importlib._bootstrap>”, line 1006, in _gcd_import error](https://stackoverflow.com/questions/57796382/file-frozen-importlib-bootstrap-line-1006-in-gcd-import-error)

I made a simple crawler that's supposed to get and download the images from an instagram profile but I keep getting this error message. I tried following this tutorial https://docs.scrapy.org/en/latest/topics/media-pipeline.htmlHere are all the error messages I recieved.

2019-09-04 22:37:46Z

I made a simple crawler that's supposed to get and download the images from an instagram profile but I keep getting this error message. I tried following this tutorial https://docs.scrapy.org/en/latest/topics/media-pipeline.htmlHere are all the error messages I recieved.You can try to set your IMAGES_STORE like this:This way the '\U' will be treated as literal characters, which is what you need here.

i arranged my code to ignore empty rows o the scraped data, now it is cloning the scraped data to refill the ignored data

Gabriel.F

[i arranged my code to ignore empty rows o the scraped data, now it is cloning the scraped data to refill the ignored data](https://stackoverflow.com/questions/57795113/i-arranged-my-code-to-ignore-empty-rows-o-the-scraped-data-now-it-is-cloning-th)

i am making a web aplication that scrapes data from news sites and saves it in a csv file, i am having a big problem in cleaning my dataand it is now cloning itself my idea was to compare the saved data with the new data, after saving it on a variable and comparing my new ml_items to the same array saved in a different variablethis is the spider:these are my items.pythis is my pipelinei do not know how to export the csv file to the post, so im going to leave all of my modulesi ask for your pardon if i had some gramatical error, english is not my first language

2019-09-04 20:06:47Z

i am making a web aplication that scrapes data from news sites and saves it in a csv file, i am having a big problem in cleaning my dataand it is now cloning itself my idea was to compare the saved data with the new data, after saving it on a variable and comparing my new ml_items to the same array saved in a different variablethis is the spider:these are my items.pythis is my pipelinei do not know how to export the csv file to the post, so im going to leave all of my modulesi ask for your pardon if i had some gramatical error, english is not my first language

unable to open scrapy response when True

Piggydog

[unable to open scrapy response when True](https://stackoverflow.com/questions/57796007/unable-to-open-scrapy-response-when-true)

This is my first time facing this problem, It usually opens Firefox but now I have to copy the temporary file and paste (manually) for it to open,how can I fix?I run Ubuntu and Python3 if it matters.

2019-09-04 21:48:39Z

This is my first time facing this problem, It usually opens Firefox but now I have to copy the temporary file and paste (manually) for it to open,how can I fix?I run Ubuntu and Python3 if it matters.

Scrapy extremely slow: probably bottleneck

Andrea Casettari

[Scrapy extremely slow: probably bottleneck](https://stackoverflow.com/questions/57795692/scrapy-extremely-slow-probably-bottleneck)

I need help to find the bottleneck with my scrapy/python based scraper.We are scraping products from Amazon (Italy at the moment) but we are struggling with overall requests throughput.

We are using backconnect rotating proxies: StormProxies (50 threads plan) + Proxyrotator (100 threads) + TOR but even 250+ available threads we can scrape only 1/2 URLs per second...

We are running it on OVH dedicated server, 8 core x 16GB ram, redis celery and docker as additional toolsI am an IT technician, the software is developed by my indian dev guy, if you need additional info or code just ask!Thanks in advance

2019-09-04 21:11:51Z

I need help to find the bottleneck with my scrapy/python based scraper.We are scraping products from Amazon (Italy at the moment) but we are struggling with overall requests throughput.

We are using backconnect rotating proxies: StormProxies (50 threads plan) + Proxyrotator (100 threads) + TOR but even 250+ available threads we can scrape only 1/2 URLs per second...

We are running it on OVH dedicated server, 8 core x 16GB ram, redis celery and docker as additional toolsI am an IT technician, the software is developed by my indian dev guy, if you need additional info or code just ask!Thanks in advanceAmazon, like other big websites, is using IA to detect the incoming requests. It means that if you are requesting a lot of traffic to the same "logical" order on the same website, they will detect you and try to ban you (even if you are using other proxies).Try to separate and do it on different servers. Instead of using only one big server, use several small servers.My recommendations:I hope it helps!

How to find element iTunes Connect website for scrapy(crawling)?

jerryp

[How to find element iTunes Connect website for scrapy(crawling)?](https://stackoverflow.com/questions/57752823/how-to-find-element-itunes-connect-website-for-scrapycrawling)

I need CFBundleversion of my apps so i've tried scrapy iTunes Connect website but i can't get element.I tried:I am receiving the following error:i think the site is different structure...

what should i do?

2019-09-02 07:19:47Z

I need CFBundleversion of my apps so i've tried scrapy iTunes Connect website but i can't get element.I tried:I am receiving the following error:i think the site is different structure...

what should i do?Nothing wrong with you locator :It inside iframe, you need switch first, use frame_to_be_available_and_switch_to_it and visibility_of_element_located, like this:Following import:

Another Scrapy Question: Output to Console but not to .json

Thornhale

[Another Scrapy Question: Output to Console but not to .json](https://stackoverflow.com/questions/57759763/another-scrapy-question-output-to-console-but-not-to-json)

This is another newbie scrapy question:When I first started with the scrapy tutorial linked here:https://docs.scrapy.org/en/latest/intro/tutorial.htmlI can crawl a webpage and then output the scraped content to a json file. But when I modify the tutorial to add a few rules like:The output to the json stops although I can still see the output on the console. Can someone give me pointers on what I am doing wrong? The modifications can be seen below:

2019-09-02 15:47:33Z

This is another newbie scrapy question:When I first started with the scrapy tutorial linked here:https://docs.scrapy.org/en/latest/intro/tutorial.htmlI can crawl a webpage and then output the scraped content to a json file. But when I modify the tutorial to add a few rules like:The output to the json stops although I can still see the output on the console. Can someone give me pointers on what I am doing wrong? The modifications can be seen below:

Error while scrapying: “ValueError: Missing scheme in request url”

Octaviotastico

[Error while scrapying: “ValueError: Missing scheme in request url”](https://stackoverflow.com/questions/57758399/error-while-scrapying-valueerror-missing-scheme-in-request-url)

I'm trying to scrape glassdoor with the Scrapy library. I've got all the links to extract the info in a mongo database.The error I'm getting is:The code I have is:I've tried with self.start_urls = [] and with self.start_urls = db_links (because db_links is a list I get from mongo). And of course, I putted that in a method called __init__.

None of that works.

I don't know what else to try.    .    EDIT:    I'm trying to change the code, to see if I can find a solution but it still fails.

I've checked the "db_link" variable and it's fine, is a list with all the links. I've alto putted the connection and db_close, etc inside the __init__ method.    .    EDIT 2:    If you would like to see the implementation of "read_crawled_urls", here it is:When I run this spider from a main.py file doing:

os.system('scrapy runspider gs_scraper.py')

The code throws the error. But if I run it from terminal, it aparently works fine.

2019-09-02 14:09:39Z

I'm trying to scrape glassdoor with the Scrapy library. I've got all the links to extract the info in a mongo database.The error I'm getting is:The code I have is:I've tried with self.start_urls = [] and with self.start_urls = db_links (because db_links is a list I get from mongo). And of course, I putted that in a method called __init__.

None of that works.

I don't know what else to try.    .    EDIT:    I'm trying to change the code, to see if I can find a solution but it still fails.

I've checked the "db_link" variable and it's fine, is a list with all the links. I've alto putted the connection and db_close, etc inside the __init__ method.    .    EDIT 2:    If you would like to see the implementation of "read_crawled_urls", here it is:When I run this spider from a main.py file doing:

os.system('scrapy runspider gs_scraper.py')

The code throws the error. But if I run it from terminal, it aparently works fine.

Scrapy cant scrape linked .css files

C.Acarbay

[Scrapy cant scrape linked .css files](https://stackoverflow.com/questions/57753862/scrapy-cant-scrape-linked-css-files)

I have a broad crawler that goes through all the pages, extracts links with the link extractor and continues. However, I'd also like to scrape all the linked css and js documents so I wrote separate rules to handle the js, css and normal links like below:The process_css function just prints whatever is passing through. With this setup, I can crawl and access all the js files and their content but not the css files. To be specific, these rules find the css and js links without an issue but the css links I think are not followed.Edit: the  Response content isn't text error was due to something else.

2019-09-02 08:42:10Z

I have a broad crawler that goes through all the pages, extracts links with the link extractor and continues. However, I'd also like to scrape all the linked css and js documents so I wrote separate rules to handle the js, css and normal links like below:The process_css function just prints whatever is passing through. With this setup, I can crawl and access all the js files and their content but not the css files. To be specific, these rules find the css and js links without an issue but the css links I think are not followed.Edit: the  Response content isn't text error was due to something else.The problem was due to the deny extensions parameter in Link extractor defaulting to the values given here:

https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/init.pyThey have css listed as a no-go extension.

This scrapy code enters data into the wrong form (search form). How do i get to the actual login form

Ibtsam Ch

[This scrapy code enters data into the wrong form (search form). How do i get to the actual login form](https://stackoverflow.com/questions/57730024/this-scrapy-code-enters-data-into-the-wrong-form-search-form-how-do-i-get-to)

I was trying to log into a website using scrapy and it always inserts the data in the first form i.e the search bar.

I have tried to do every possible thing that i could understandIt should log into the login form but it puts the value in the search bar

2019-08-30 16:04:44Z

I was trying to log into a website using scrapy and it always inserts the data in the first form i.e the search bar.

I have tried to do every possible thing that i could understandIt should log into the login form but it puts the value in the search barUse formid:

Why is Scrapy skipping some URL's but not others?

Conrad Dubois

[Why is Scrapy skipping some URL's but not others?](https://stackoverflow.com/questions/57760431/why-is-scrapy-skipping-some-urls-but-not-others)

I am writing a scrapy crawler to grab info on shirts from Amazon. The crawler starts on an amazon page for some search, "funny shirts" for example, and collects all the result item containers. It then parses through each result item collecting data on the shirts. I use ScraperAPI and Scrapy-user-agents to dodge amazon. The code for my spider is:Crawling looks like this:https://i.stack.imgur.com/UbVUt.pngI'm getting a 200 returned so I know I'm getting the data from the webpage, but sometimes it does not go into parse_dir_contents, or it only grabs info on a few shirts and then moves on to the next keyword without following pagination.Working with two keywords: the first keyword in my file (keywords.txt) is loaded, it may find 1-3 shirts, then it moves on to the next keyword. The second keyword is then completely successful, finding all shirts and following pagination. In a keyword file with 5+ keywords, the first 2-3 keywords are skipped, then the next keyword is loaded and only 2-3 shirts are found before it moves onto the next word which is again completely successful. In a file with 10+ keywords I get very sporadic behavior. I have no idea why this is happening can anyone explain?

2019-09-02 16:49:02Z

I am writing a scrapy crawler to grab info on shirts from Amazon. The crawler starts on an amazon page for some search, "funny shirts" for example, and collects all the result item containers. It then parses through each result item collecting data on the shirts. I use ScraperAPI and Scrapy-user-agents to dodge amazon. The code for my spider is:Crawling looks like this:https://i.stack.imgur.com/UbVUt.pngI'm getting a 200 returned so I know I'm getting the data from the webpage, but sometimes it does not go into parse_dir_contents, or it only grabs info on a few shirts and then moves on to the next keyword without following pagination.Working with two keywords: the first keyword in my file (keywords.txt) is loaded, it may find 1-3 shirts, then it moves on to the next keyword. The second keyword is then completely successful, finding all shirts and following pagination. In a keyword file with 5+ keywords, the first 2-3 keywords are skipped, then the next keyword is loaded and only 2-3 shirts are found before it moves onto the next word which is again completely successful. In a file with 10+ keywords I get very sporadic behavior. I have no idea why this is happening can anyone explain?first check if robots.txt is being ignored, from what you have said I suppose you already have that.Sometimes the html code returned from the response is not the same as the one you are seeing when you look at the product. I dont really know what exactly is going on in your case but you can check what the spider is actually "reading" with.After thatThere you can check out the code that the spider is actually seeing if the requests succeeds.Sometimes the request does not succeed (Maybe Amazon is redirecting you to a CAPTCHA or something).You can check the response while scraping with (Please check the code below, Im doing this from memory)If I remember correctly, you can get the URL from scrapy itself (something along the lines of response.url.Try to make use of dont_filter=True in your scrapy Requests. I had the same problem, seemed like the scrapy crawler was ignoring some URLs because it thought they were duplicate. This makes sure that scrapy doesn't filter any URLS with its dupefilter. 

Can't make my first spider run,any advice?

SSD93

[Can't make my first spider run,any advice?](https://stackoverflow.com/questions/57732234/cant-make-my-first-spider-run-any-advice)

This is my first time using scrapy and maybe the third in python, so i'm a noob.

The problem with this code is that it doesn't even enter the page.I have tried to use:

scrapy shell 'https://www.zooplus.es/shop/tienda_perros/pienso_perros/pienso_hipoalergenico'This works and then using...... I can retrieve information.

2019-08-30 19:23:00Z

This is my first time using scrapy and maybe the third in python, so i'm a noob.

The problem with this code is that it doesn't even enter the page.I have tried to use:

scrapy shell 'https://www.zooplus.es/shop/tienda_perros/pienso_perros/pienso_hipoalergenico'This works and then using...... I can retrieve information.I think you haven't defined the fields for your items.py

the error is coming from item['nombre']Either you should define the field in items.py or simply replace

item= scrapy.Item()

with item = dict()

Scrapy CrawlerRunner: Output missing

Aerodynamic

[Scrapy CrawlerRunner: Output missing](https://stackoverflow.com/questions/57727260/scrapy-crawlerrunner-output-missing)

I have been using the method described on stackoverflow (https://stackoverflow.com/a/43661172/5037146) , to make scrapy run from script using Crawler Runner to allow to restart the process.However, I don't get any console logs when running the process through CrawlerRunner, whereas when I using CrawlerProcess, it outputs the status and progress.Code is available online: https://colab.research.google.com/drive/14hKTjvWWrP--h_yRqUrtxy6aa4jG18nJ

2019-08-30 13:00:51Z

I have been using the method described on stackoverflow (https://stackoverflow.com/a/43661172/5037146) , to make scrapy run from script using Crawler Runner to allow to restart the process.However, I don't get any console logs when running the process through CrawlerRunner, whereas when I using CrawlerProcess, it outputs the status and progress.Code is available online: https://colab.research.google.com/drive/14hKTjvWWrP--h_yRqUrtxy6aa4jG18nJWith CrawlerRunner you need to manually setup logging, which you can do with configure_logging(). See https://docs.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script

Scrapy-Splash Waiting for Page to Load

RoboBear

[Scrapy-Splash Waiting for Page to Load](https://stackoverflow.com/questions/57733234/scrapy-splash-waiting-for-page-to-load)

I'm new to scrapy and splash, and I need to scrape data from single page and regular web apps.A caveat, though, is I'm mostly scraping data from internal tools and applications, so some require authentication and all of them require at least a couple of seconds loading time before the page fully loads.I naively tried a Python time.sleep(seconds) and it didn't work. It seems like SplashRequest and scrapy.Request both run and yield results, basically. I then learned about LUA scripts as arguments to these requests, and attempted a LUA script with various forms of wait(), but it looks like the requests never actually run the LUA scripts. It finishes right away and my HTMl selectors don't find anything I'm looking for.I'm following directions from here https://github.com/scrapy-plugins/scrapy-splash, and have their docker instance running on localhost:8050 and created a settings.py.Anyone with experience here know what I might be missing? Thanks!spider.py

2019-08-30 21:10:56Z

I'm new to scrapy and splash, and I need to scrape data from single page and regular web apps.A caveat, though, is I'm mostly scraping data from internal tools and applications, so some require authentication and all of them require at least a couple of seconds loading time before the page fully loads.I naively tried a Python time.sleep(seconds) and it didn't work. It seems like SplashRequest and scrapy.Request both run and yield results, basically. I then learned about LUA scripts as arguments to these requests, and attempted a LUA script with various forms of wait(), but it looks like the requests never actually run the LUA scripts. It finishes right away and my HTMl selectors don't find anything I'm looking for.I'm following directions from here https://github.com/scrapy-plugins/scrapy-splash, and have their docker instance running on localhost:8050 and created a settings.py.Anyone with experience here know what I might be missing? Thanks!spider.pyI figured it out!Short AnswerMy Spider class was configured incorrectly for using splash with scrapy.Long AnswerPart of running splash with scrap is, in my case, running a local Docker instance that it uses to load my requests into for it to run the Lua scripts. An important caveat to note is the settings for splash as described in the github page must be a property of the spider class itself, so I added this code to my Spider:Then I noticed my Lua code running, and the Docker container logs indicating the interactions. After fixing errors with the splash:select() my login script worked, as did my waits:Lastly, I created a Lua script to handle logging in, redirecting, and gathering links and text from pages. My application is an AngularJS app, so I can't gather links or visit them except clicking. This script let me run through every link, click it, and gather content. I suppose an alternative solution would have been to use end-to-end testing tools such as Selenium/WebDriver or Cypress, but I prefer to use scrapy to scrape and testing tools to test. To each their own (Python or NodeJS tools), I suppose.Neat TrickAnother thing to mention that's really helpful for debugging, is when you're running the Docker instance for Scrapy-Splash, you can visit that URL in your browser and there's an interactive "request tester" that lets you test out Lua scripts and see rendered HTML results (for example, verifying login or page visits). For me, this url was http://0.0.0.0:8050, and this URL is set in your settings and should be configured to match with your Docker container.Cheers!

Scrapy. I can't make a request within my parse method to scrap another page and I can't run the scraper from a script

AHAMES

[Scrapy. I can't make a request within my parse method to scrap another page and I can't run the scraper from a script](https://stackoverflow.com/questions/57678364/scrapy-i-cant-make-a-request-within-my-parse-method-to-scrap-another-page-and)

I am trying to crawl videos from alibaba, I am trying to make it with a script, and maybe make a simple app with tkinter later.Two problems:

1-I am getting this error.it could run the first time, but then I can't rerun it to test changes, if it works like this I may not be able to run it with tkinter later. Here is the spider, which I am sure it works as currently required (it is unfinished, I haven't got to the videos download yet, help on that maybe?), because I have tested it individually on scrapy shell. Which gets us to the second problem, after testing the spider on the with scrapy crawl, it stops working at parse and it doesn't seem to go into parseItem, am I doing something wrong in scrapy.Request?

2019-08-27 15:54:25Z

I am trying to crawl videos from alibaba, I am trying to make it with a script, and maybe make a simple app with tkinter later.Two problems:

1-I am getting this error.it could run the first time, but then I can't rerun it to test changes, if it works like this I may not be able to run it with tkinter later. Here is the spider, which I am sure it works as currently required (it is unfinished, I haven't got to the videos download yet, help on that maybe?), because I have tested it individually on scrapy shell. Which gets us to the second problem, after testing the spider on the with scrapy crawl, it stops working at parse and it doesn't seem to go into parseItem, am I doing something wrong in scrapy.Request?

Unable to do pagination with LinkExractor

Ahmad Amin Farooq

[Unable to do pagination with LinkExractor](https://stackoverflow.com/questions/57680539/unable-to-do-pagination-with-linkexractor)

I'm trying to pagination, I've used my method which is working fine but I want to use LinkExtractor function to do the task.The only problem is problem is that link in a tag is not complete.As you can see the code, how can I add https://www.amazon.co.uk/ in the start of that rule.

2019-08-27 18:42:00Z

I'm trying to pagination, I've used my method which is working fine but I want to use LinkExtractor function to do the task.The only problem is problem is that link in a tag is not complete.As you can see the code, how can I add https://www.amazon.co.uk/ in the start of that rule.The problem is allow='Next', it doesn’t do what you think it does.I think you are looking for restrict_text='Next' instead.Check the LinkExtractor documentation for a full list of supported parameters.

Why does my spider doesn't crawl all the elements?

CIC3RO

[Why does my spider doesn't crawl all the elements?](https://stackoverflow.com/questions/57694702/why-does-my-spider-doesnt-crawl-all-the-elements)

I'm new to python and I build a Crawler for amazon in the last time. My problem is, that I never get all Items. I have a list with links for the products. There are about 1300 links in it. But when I let the crawler run, I get a different amount of "crawled" Items. It fluctuates between 700-1100 Items. What did I wrong, so I don't get the information from all 1300 Items? [scrapy.core.engine] DEBUG: Crawled (200) https://www.amazon.de/...link..> (referer: None)does it have something to do with the (referer: None)? If yes, what does it mean?

2019-08-28 14:27:46Z

I'm new to python and I build a Crawler for amazon in the last time. My problem is, that I never get all Items. I have a list with links for the products. There are about 1300 links in it. But when I let the crawler run, I get a different amount of "crawled" Items. It fluctuates between 700-1100 Items. What did I wrong, so I don't get the information from all 1300 Items? [scrapy.core.engine] DEBUG: Crawled (200) https://www.amazon.de/...link..> (referer: None)does it have something to do with the (referer: None)? If yes, what does it mean?

Scraping raw javascript and css files with Scrapy

C.Acarbay

[Scraping raw javascript and css files with Scrapy](https://stackoverflow.com/questions/57687235/scraping-raw-javascript-and-css-files-with-scrapy)

I'd like to scrape all the linked javascript and css files on a give domain with Scrapy. The issue is that I don't quite understand how to extract the links from the link elements. Assume I'm scraping example.com. There are links to js and css of the form:These links start from the root domain, so no problem. But if the links are like the ones below, it starts to get confusing:These relative URLs are supposed to work such that if I'm on example.com/some_page/ the link paths are appended to that like: example.com/some_page/path_to_js/example.js. That's not how it always works in actual web pages however. On some web sites with language selection eg.example.com/en/some_page, the relative paths start from example.com/en instead of the full path of that page. So, while expecting to find the files at example.com/en/some_page/path_to_js/example.js, you find them at example.com/en/path_to_js/example.jsIs there any way to understand from where the relative paths start from?

2019-08-28 07:38:54Z

I'd like to scrape all the linked javascript and css files on a give domain with Scrapy. The issue is that I don't quite understand how to extract the links from the link elements. Assume I'm scraping example.com. There are links to js and css of the form:These links start from the root domain, so no problem. But if the links are like the ones below, it starts to get confusing:These relative URLs are supposed to work such that if I'm on example.com/some_page/ the link paths are appended to that like: example.com/some_page/path_to_js/example.js. That's not how it always works in actual web pages however. On some web sites with language selection eg.example.com/en/some_page, the relative paths start from example.com/en instead of the full path of that page. So, while expecting to find the files at example.com/en/some_page/path_to_js/example.js, you find them at example.com/en/path_to_js/example.jsIs there any way to understand from where the relative paths start from?While scraping, Scrapy allows you to create an absolute URL from a Relative URLYou could do something like this

Bizarre, randomly timed scraping errors

tycrone

[Bizarre, randomly timed scraping errors](https://stackoverflow.com/questions/57684030/bizarre-randomly-timed-scraping-errors)

so I've got a functioning scrapy web crawler that will search a given url ("amazon.ca" + "sku(from a csv)") and then return some info from this page and then follow a link into a 2nd level page and extract an image... but it stops working after 300 or so URL crawls. This is the traceback for an item that gives me issue: I know I'm doing some pretty random stuff to narrow in on my desired content  before saving as items (below), but everything seems to work and it just stops randomly at certain items and shows the error despite scraping and showing the URLs correctly in the terminal output. Here's my crawler:and then the items.pyand then the pipelines.py:

2019-08-28 01:38:27Z

so I've got a functioning scrapy web crawler that will search a given url ("amazon.ca" + "sku(from a csv)") and then return some info from this page and then follow a link into a 2nd level page and extract an image... but it stops working after 300 or so URL crawls. This is the traceback for an item that gives me issue: I know I'm doing some pretty random stuff to narrow in on my desired content  before saving as items (below), but everything seems to work and it just stops randomly at certain items and shows the error despite scraping and showing the URLs correctly in the terminal output. Here's my crawler:and then the items.pyand then the pipelines.py:Aah, I'm a bit of a dummy. Thanks for taking the time to help me out @AnuvratParashar and @Umair. I believe I was able to figure out the problem. I've added my full traceback above.The issue was within this section of my scraper, which follows a URL it scrapes and then tries to extract an image URL within this page:It looks like the pages that my crawler stopped extracting info from had different image ids than the rest of my pages and it was basically saying, "hey, these image URLs you're trying to extract don't exist!", and then just not outputting any of the results for these pages (even if there were results from the first scraped page). I was just focusing on the page URLs and getting frustrated because it was showing them without issue in my terminal output... I completely disregarded my image URLs.

How to fix “ImportError: cannot import name '_win32stdio'”? Typical solutions at a roadblock

Roly Poly

[How to fix “ImportError: cannot import name '_win32stdio'”? Typical solutions at a roadblock](https://stackoverflow.com/questions/57685494/how-to-fix-importerror-cannot-import-name-win32stdio-typical-solutions-at)

I am trying to overcome a seemingly answered question, but none of the provided solutions are enough. Specifically, I am trying to create a ScraPy crawler, but when I use "scrapy crawl myProjectsName", I get:ImportError: cannot import name '_win32stdio'Now, that would be fine, given that the question already has a solution posted here on stackoverflow:ImportError : cannot import name '_win32stdio' (Link to previous solution)But it's not working out for me:When I write "pip install twisted" or the alternative solution (seen elsewhere on the web), "pip install pywin32", both result in the following error msg from my terminal:Here is a link to some (but not all -- it was very long...) of the traceback shown in terminalhttps://imgur.com/LfhXBkKMy expected result is a functional output from my scrapy web crawler. But the output is an ImportError.I believe the solution is probably as simple as copy and pasting something from here to there, since the contents appear to already be on my computer. I just don't know where to put them.Note/P.S.: I presume I could sort this out on my own if I knew how to read the traceback info, is that right? If you've figured out a solution to my issue, how did you know what it was?

2019-08-28 05:18:24Z

I am trying to overcome a seemingly answered question, but none of the provided solutions are enough. Specifically, I am trying to create a ScraPy crawler, but when I use "scrapy crawl myProjectsName", I get:ImportError: cannot import name '_win32stdio'Now, that would be fine, given that the question already has a solution posted here on stackoverflow:ImportError : cannot import name '_win32stdio' (Link to previous solution)But it's not working out for me:When I write "pip install twisted" or the alternative solution (seen elsewhere on the web), "pip install pywin32", both result in the following error msg from my terminal:Here is a link to some (but not all -- it was very long...) of the traceback shown in terminalhttps://imgur.com/LfhXBkKMy expected result is a functional output from my scrapy web crawler. But the output is an ImportError.I believe the solution is probably as simple as copy and pasting something from here to there, since the contents appear to already be on my computer. I just don't know where to put them.Note/P.S.: I presume I could sort this out on my own if I knew how to read the traceback info, is that right? If you've figured out a solution to my issue, how did you know what it was?The solution was to download twisted-win from github: https://github.com/twisted/twistedThen copy _win32stdio and _pollingfile to the directory where it was missing.Now it works!

Not getting results using python scrapy

Srinath Neela

[Not getting results using python scrapy](https://stackoverflow.com/questions/57636596/not-getting-results-using-python-scrapy)

This is my first Scrapy program, i couldn't see the results even executed without errors.

2019-08-24 09:14:48Z

This is my first Scrapy program, i couldn't see the results even executed without errors.You have a typo in start_urls:Also you'll need (in settings.py):

How to run Scrapy Crawler Process parallel in separate processes? (Multiprocessing)

Dinesh Gupta

[How to run Scrapy Crawler Process parallel in separate processes? (Multiprocessing)](https://stackoverflow.com/questions/57638131/how-to-run-scrapy-crawler-process-parallel-in-separate-processes-multiprocessi)

I am trying to do Multiprocessing of my spider. I know CrawlerProcess runs the spider in a single process.I want to run multiple times the same spider with different arguments.I tried this but doesn't work.How do I do multiprocessing?Please do help. Thanks.

2019-08-24 12:52:14Z

I am trying to do Multiprocessing of my spider. I know CrawlerProcess runs the spider in a single process.I want to run multiple times the same spider with different arguments.I tried this but doesn't work.How do I do multiprocessing?Please do help. Thanks.You need to run each scrapy crawler instance inside a separate process. This is because scrapy uses twisted, and you can't use it multiple times in the same process.Also, you need to disable the telenet extension, because scrapy will try to bind to the same port on multiple processes.Test code:

How to pass a variable from Lua script to Javascript using splash and scrapy?

coolsaint

[How to pass a variable from Lua script to Javascript using splash and scrapy?](https://stackoverflow.com/questions/57632845/how-to-pass-a-variable-from-lua-script-to-javascript-using-splash-and-scrapy)

I was working on a scraping project made of scrapy and splash. I am a newbie in Lua and Javascript. I am in a situation where I need to send a variable from Lua to Javascript. but I am not being able to find out how.Here is my Lua scriptThe error I am getting is Looks like it's a Javascript error and it is not being able to find the variable pn which is present in Lua script. How can I pass the value of pn to the javascript?

2019-08-23 20:48:17Z

I was working on a scraping project made of scrapy and splash. I am a newbie in Lua and Javascript. I am in a situation where I need to send a variable from Lua to Javascript. but I am not being able to find out how.Here is my Lua scriptThe error I am getting is Looks like it's a Javascript error and it is not being able to find the variable pn which is present in Lua script. How can I pass the value of pn to the javascript?By my count, line 7 of the Lua script is assert(splash:wait(10)), but pn isn't used there.If the numbering is off by one, perhaps the error is actually in line 6, assert(splash:runjs('document.querySelector("#profile-listing-uploads > div:nth-child(2) > ul > li:nth-child("+ pn +") > a").click()')). I do not know if there is some Lua function that defines variables in the JavaScript runtime, but you should be able to concatenate the value of the variable pn into the JavaScript, like this:(This will crash if pn is nil, if args.pn wasn't a valid numerical literal in pn = tonumber(args.pn).)Try accessing your argument like this:

Using Scrapy and Splash to Follow javascript pagination

m1k1

[Using Scrapy and Splash to Follow javascript pagination](https://stackoverflow.com/questions/57641727/using-scrapy-and-splash-to-follow-javascript-pagination)

I am using Scrapy and splash to extract the data. I am looking to find a way to follow pagination that was powered with javascript. The URL is not changing it is always the same no matter on what page you are.I have tried with lua script and splash to click on the element but this does not work:Is it even possible to do it in this way? Appreciate help.

2019-08-24 21:40:58Z

I am using Scrapy and splash to extract the data. I am looking to find a way to follow pagination that was powered with javascript. The URL is not changing it is always the same no matter on what page you are.I have tried with lua script and splash to click on the element but this does not work:Is it even possible to do it in this way? Appreciate help.

Use scrapy to get list of urls, and then sent request only not in the given list

karolisb

[Use scrapy to get list of urls, and then sent request only not in the given list](https://stackoverflow.com/questions/57645911/use-scrapy-to-get-list-of-urls-and-then-sent-request-only-not-in-the-given-list)

I want to scrape a website once a day, but only new content. So far I've set a Rule that already scraped urls would not be scraped, but what I want is to mark those urls as arleady scraped. In other words, I do not want to sent a request, but to put that url in list variable.can I check if url is in the list, if not then sent a request, if it is - put in a list variable.EDIT:I'm able to collect those urls, but is it possible to output them in the same output as parse_item?if url goes to that already scraped list, I need to yield {..., 'new': 0}

2019-08-25 12:13:04Z

I want to scrape a website once a day, but only new content. So far I've set a Rule that already scraped urls would not be scraped, but what I want is to mark those urls as arleady scraped. In other words, I do not want to sent a request, but to put that url in list variable.can I check if url is in the list, if not then sent a request, if it is - put in a list variable.EDIT:I'm able to collect those urls, but is it possible to output them in the same output as parse_item?if url goes to that already scraped list, I need to yield {..., 'new': 0}

Scrapy concatenate array elements inside div in python

Ale

[Scrapy concatenate array elements inside div in python](https://stackoverflow.com/questions/57611599/scrapy-concatenate-array-elements-inside-div-in-python)

I need to concatenate some text inside a <div> with xpath in Scrapy. The div has the next structure:I've created a ScrapyItem in my Spider:If I do this, item['description'] = response.xpath('//div[@itemprop="description"]/text()').extract()everything gets mixed and separated by commas, like this:- Text1

,- Text2

,- Text3

I think that's because response.xpath('//div[@itemprop="description"]/text()').extract() returns an array so it adds commas to separate the array items.I'm trying to loop over the array and join each item inside the "description" ScrapyItem property.This is what I'm trying:I know it would work if I could do something like this:but the div that contains the text has no more tags inside.Any help would be appreciated, it's my first Scrapy project.

2019-08-22 14:26:39Z

I need to concatenate some text inside a <div> with xpath in Scrapy. The div has the next structure:I've created a ScrapyItem in my Spider:If I do this, item['description'] = response.xpath('//div[@itemprop="description"]/text()').extract()everything gets mixed and separated by commas, like this:- Text1

,- Text2

,- Text3

I think that's because response.xpath('//div[@itemprop="description"]/text()').extract() returns an array so it adds commas to separate the array items.I'm trying to loop over the array and join each item inside the "description" ScrapyItem property.This is what I'm trying:I know it would work if I could do something like this:but the div that contains the text has no more tags inside.Any help would be appreciated, it's my first Scrapy project.it is the other way around,

you have useditem['description']  = response.xpath('//div[@itemprop="description"]/text()').extract()

that will return a list

join the list directlyitem['description'] = " ".join(response.xpath('//div[@itemprop="description"]/text()').extract())

Why did the crawler get 307 status code when to deploy in scrapinghub?

it_is_a_literature

[Why did the crawler get 307 status code when to deploy in scrapinghub?](https://stackoverflow.com/questions/57611603/why-did-the-crawler-get-307-status-code-when-to-deploy-in-scrapinghub)

The target url:A scrapy project can work locally,extract info in the target url successfully.Now i deploy it into scrapinghub,start the job,get wrong info.

For example(abstract from scrapinghub's Job Requests list):The http status is 307,compared to status code---200 in my local pc;the log file in scrapinghub write such error message:When you click https://consent.yahoo.com/collectConsent?sessionId=3_cc-session_99e6deeb-eab0-4bd8-86c8-29f1700579d1&lang=&inline=false,it jump into https://finance.yahoo.com/quote/0002.HK/balance-sheet?p=0002.HK,i am puzzled  .

why the crawler can get url--https://finance.yahoo.com/quote/0002.HK/balance-sheet?p=0002.HK successfully in my local pc?

Why the crawler redirect into https://consent.yahoo.com/collectConsent?sessionId=3_cc-session_99e6deeb-eab0-4bd8-86c8-29f1700579d1&lang=&inline=false and get no response when same crawler deploy  on scrapinghub?

2019-08-22 14:26:42Z

The target url:A scrapy project can work locally,extract info in the target url successfully.Now i deploy it into scrapinghub,start the job,get wrong info.

For example(abstract from scrapinghub's Job Requests list):The http status is 307,compared to status code---200 in my local pc;the log file in scrapinghub write such error message:When you click https://consent.yahoo.com/collectConsent?sessionId=3_cc-session_99e6deeb-eab0-4bd8-86c8-29f1700579d1&lang=&inline=false,it jump into https://finance.yahoo.com/quote/0002.HK/balance-sheet?p=0002.HK,i am puzzled  .

why the crawler can get url--https://finance.yahoo.com/quote/0002.HK/balance-sheet?p=0002.HK successfully in my local pc?

Why the crawler redirect into https://consent.yahoo.com/collectConsent?sessionId=3_cc-session_99e6deeb-eab0-4bd8-86c8-29f1700579d1&lang=&inline=false and get no response when same crawler deploy  on scrapinghub?

scrapy shell unable to open response in firefox

madboy

[scrapy shell unable to open response in firefox](https://stackoverflow.com/questions/57617667/scrapy-shell-unable-to-open-response-in-firefox)

I had some trouble using firefox today, so I removed it then reinstalled it and it worked fine, however, I use as default browser but I can't open any scrapy shell view(response) in it, although the temp file is created and I can open it using Chrome.Any help is much appreciated

2019-08-22 22:19:23Z

I had some trouble using firefox today, so I removed it then reinstalled it and it worked fine, however, I use as default browser but I can't open any scrapy shell view(response) in it, although the temp file is created and I can open it using Chrome.Any help is much appreciatedThis problem may occur because of the compatibility issue with the 64-bit machine when one installs 32-bit software that requires canberra-gtk-module. You can install it like this sudo apt-get install libcanberra-gtk-module libcanberra-gtk3-module.

How to fix HttpErrorMiddleware in scrapy spider

leoro

[How to fix HttpErrorMiddleware in scrapy spider](https://stackoverflow.com/questions/57582294/how-to-fix-httperrormiddleware-in-scrapy-spider)

I am trying to so horizontal crawling using Scrapy. Using xpath, I am getting the link of each listing in a real-estate site, as well as the next-page link. However, when I run my spider I keep getting this error:"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"Thanks for the helpOutcome:

2019-08-20 22:30:42Z

I am trying to so horizontal crawling using Scrapy. Using xpath, I am getting the link of each listing in a real-estate site, as well as the next-page link. However, when I run my spider I keep getting this error:"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"Thanks for the helpOutcome:

How to get callback function result from FormRequest object

Nevega

[How to get callback function result from FormRequest object](https://stackoverflow.com/questions/57546853/how-to-get-callback-function-result-from-formrequest-object)

Use scrapy to parse site, and  need parse dropdown menu's for final request.  FormRequest class use for post request to dropdown menu to get list of options next dropdown menu using  callback function Need to get datac list, for next parcing iteration

2019-08-18 17:29:15Z

Use scrapy to parse site, and  need parse dropdown menu's for final request.  FormRequest class use for post request to dropdown menu to get list of options next dropdown menu using  callback function Need to get datac list, for next parcing iteration

scrapy keeps redirecting (meta refresh)

spacegoing

[scrapy keeps redirecting (meta refresh)](https://stackoverflow.com/questions/57586364/scrapy-keeps-redirecting-meta-refresh)

I am trying to scrape a website. However, the host keeps redirect the spider until it hits max redirections reached. Logs are as follows:Redirecting seems necessary till this point, however after that it still keeps refreshing itself.Any idea how do I look into redirected responses and how to stop redirect at an appropriate point? Many thanks!Update: I checked url in browser is http://zjip.patsev.com/. And if I use requests it does not have the same problem

2019-08-21 07:16:21Z

I am trying to scrape a website. However, the host keeps redirect the spider until it hits max redirections reached. Logs are as follows:Redirecting seems necessary till this point, however after that it still keeps refreshing itself.Any idea how do I look into redirected responses and how to stop redirect at an appropriate point? Many thanks!Update: I checked url in browser is http://zjip.patsev.com/. And if I use requests it does not have the same problem

Scrapy response returning [] but prints in terminal

Michael G

[Scrapy response returning [] but prints in terminal](https://stackoverflow.com/questions/57585810/scrapy-response-returning-but-prints-in-terminal)

I'm attempting to scrape Indeed.com and want to get information pertaining to each job in their respective div.  The response will print out in the terminal, but when I write to a file or run the spider I get a blank file and no items returned. How do I fix this issue?I've tried changing my xpaths to relative to the container its pulling from and it still runs blank.  container3 output:I expect each 'jobsearch-SerpJobCard unifiedRow row result clickcard' to be extracted into a list, then getting titles, locations, companies, and summarys from that list using relative xpaths.However, what I'm getting is a blank container3, and no items returned.  Here is the response.text info from the finished spider.

2019-08-21 06:42:31Z

I'm attempting to scrape Indeed.com and want to get information pertaining to each job in their respective div.  The response will print out in the terminal, but when I write to a file or run the spider I get a blank file and no items returned. How do I fix this issue?I've tried changing my xpaths to relative to the container its pulling from and it still runs blank.  container3 output:I expect each 'jobsearch-SerpJobCard unifiedRow row result clickcard' to be extracted into a list, then getting titles, locations, companies, and summarys from that list using relative xpaths.However, what I'm getting is a blank container3, and no items returned.  Here is the response.text info from the finished spider.Check this out, it worksOutput

Scrapy does not extract the text in certain selectors

Peretz

[Scrapy does not extract the text in certain selectors](https://stackoverflow.com/questions/57549105/scrapy-does-not-extract-the-text-in-certain-selectors)

I am crawling a website using Scrapy but when I select certain selectors, it does not extract the text in them.The website is https://www.chopo.com.mx/estudios/super-quimica-de-35-elementos/#and the selectors that are not extracting the text are:Both of them show the respective html lines of code but I can not see the text that appears in the DOM (not in the html line nor using the ::text attribute). Any ideas what's happening?Thanks 

2019-08-18 23:37:08Z

I am crawling a website using Scrapy but when I select certain selectors, it does not extract the text in them.The website is https://www.chopo.com.mx/estudios/super-quimica-de-35-elementos/#and the selectors that are not extracting the text are:Both of them show the respective html lines of code but I can not see the text that appears in the DOM (not in the html line nor using the ::text attribute). Any ideas what's happening?Thanks Scrapy doesn't work with rendered DOM but with plain source HTML instead. You can get it using Ctrl+U in most of browsers. You'll find this fragment:As you can see the information you need is loaded dynamically using JavaScript. You can try to emulate this JavaScript request or use Scrapy-Splash or similar.To emulate Javascript you need to send a POST request:



How to add multiple variables in one string (URL)

Bamieschijf

[How to add multiple variables in one string (URL)](https://stackoverflow.com/questions/57547608/how-to-add-multiple-variables-in-one-string-url)

My spider starts off with the start_urls, being:

https://www.kaercher.com/api/v1/products/search/shoppableproducts/partial/20035386?page=1&size=8&isocode=nl-NLBased on a keywords.csv file, located in my resource folder, the keywordsID (number 20035386) will change. Once the number changed, the spider will fetch the data from another product.I also have a chunk of code which constantly checks the page if isTruncated = true, if that's the case, it will change the page number in the URL to +1. The only problem I am having right now, is that I don't know how to set a second variable in one string (URL). When isTruncated = true the code need to adjust the URL's page number AND keywordsID accordingly. Currently, I only managed to add a variable for the page number.Currently the chunk of code is:However, it should become something like:When I run the spider, it will crawl all the pages of the product with keywordsID 20035386, but it will only crawl the first page of all the other products listed in the keywords.csv file.FULL CODE./krc/spiders/krc_spider.py./krc/resources/keywords.csvCurrent OutputWhen I run the spider it fetches the data from all the page's of the product with keywordsID 20035386. From all the other products with a different keywordsID, only the data from the first page will be fetched.

2019-08-18 19:12:41Z

My spider starts off with the start_urls, being:

https://www.kaercher.com/api/v1/products/search/shoppableproducts/partial/20035386?page=1&size=8&isocode=nl-NLBased on a keywords.csv file, located in my resource folder, the keywordsID (number 20035386) will change. Once the number changed, the spider will fetch the data from another product.I also have a chunk of code which constantly checks the page if isTruncated = true, if that's the case, it will change the page number in the URL to +1. The only problem I am having right now, is that I don't know how to set a second variable in one string (URL). When isTruncated = true the code need to adjust the URL's page number AND keywordsID accordingly. Currently, I only managed to add a variable for the page number.Currently the chunk of code is:However, it should become something like:When I run the spider, it will crawl all the pages of the product with keywordsID 20035386, but it will only crawl the first page of all the other products listed in the keywords.csv file.FULL CODE./krc/spiders/krc_spider.py./krc/resources/keywords.csvCurrent OutputWhen I run the spider it fetches the data from all the page's of the product with keywordsID 20035386. From all the other products with a different keywordsID, only the data from the first page will be fetched.Use response.meta for this:I believe you need a nested for when your search_text change.Checks this out, it might help you.For LoopsI think adding the keyword to the url would be the following. It may or may not need + signs before and after search_text, my knowledge is limited.though I'm not really following what this line is doing, at least the format(search_text) portion of it.

How to response data when scrapy spider's working was done?

MyungHun

[How to response data when scrapy spider's working was done?](https://stackoverflow.com/questions/57559796/how-to-response-data-when-scrapy-spiders-working-was-done)

I crawled data from web site and I used django for server.

When spider's working was done, I want to notice to my server that crawling was done or not.I found signals module that can be used for the spider_closed method.But still I don't know how to response data to the server from pipeline.

2019-08-19 15:30:25Z

I crawled data from web site and I used django for server.

When spider's working was done, I want to notice to my server that crawling was done or not.I found signals module that can be used for the spider_closed method.But still I don't know how to response data to the server from pipeline.

If statement to only write new values to PostgreSQL db in Scrapy

Brandon Boykin

[If statement to only write new values to PostgreSQL db in Scrapy](https://stackoverflow.com/questions/57562308/if-statement-to-only-write-new-values-to-postgresql-db-in-scrapy)

I have a Scrapy spider that writes the scraped data to a PostgreSQL database using psycopg2.  I have Scrapyd running and item exporters and everything is setup fine.  I'm scraping the labor section of craigslist for post url, title, and date created.  I want to create notifications on new posts so to achieve this I made the url field in my PostgreSQL database the Primary Key.    The first thing I tried was a try block (which worked in my scraper that I had running using requests and BeautifulSoup.It seems that the Scrapy engine never throws an exception because it always tries to INSERT INTO the database.  The second pass of the spider will throw a bunch of errors from psycopg2 for the url field not being unique.Next I triedAnd it still tries to write every record and I get an error for the url field not being unique.I don't understand why neither of these work, especially the try block that worked outside of the Scrapy environment.I've even seen the process_spider_exception in the middlewares.py file and it already has pass in it.Could someone point me in the right direction to why this isn't working?

2019-08-19 18:41:52Z

I have a Scrapy spider that writes the scraped data to a PostgreSQL database using psycopg2.  I have Scrapyd running and item exporters and everything is setup fine.  I'm scraping the labor section of craigslist for post url, title, and date created.  I want to create notifications on new posts so to achieve this I made the url field in my PostgreSQL database the Primary Key.    The first thing I tried was a try block (which worked in my scraper that I had running using requests and BeautifulSoup.It seems that the Scrapy engine never throws an exception because it always tries to INSERT INTO the database.  The second pass of the spider will throw a bunch of errors from psycopg2 for the url field not being unique.Next I triedAnd it still tries to write every record and I get an error for the url field not being unique.I don't understand why neither of these work, especially the try block that worked outside of the Scrapy environment.I've even seen the process_spider_exception in the middlewares.py file and it already has pass in it.Could someone point me in the right direction to why this isn't working?Your code says "Do a lookup of the number of times this url exists in the database. If that number is not zero, then insert the url, otherwise do nothing". The logic is inverted. You only want to do the insert if the count = 0. On a side note, you should look into ON DUPLICATE KEY UPDATE. So I figured out my issue and while the logic of my if statement was backwards as I posted it, I had it the other way as well and it still wasn't working.  As I said in my comment above I need to raise DropItem otherwise the item pipeline will continue to process it.  Once I figured this out I still had issues because I committed what I feel like is a noob mistake and maybe why only one person commented.While this may seem obvious to most (it does to me now that I figured it out) when dealing with Scrapy and you have your spiders running on Scrapyd.  If you change code for your spiders, pipelines, middleware, etc.  You must re-deploy your spiders for them to utilize the new code.I figured this out because I decided to completely change the logic of the pipeline to process the items into a csv file then when the spider is closed open the .csv file in a pandas database and drop_duplicates().  Then use pandas to_csv() to save the .csv file.  I was going to read the .csv into the database in my main program that would be calling the spiders.Once I put all this code in and saved I ran my spider.  Quickly pulled up the log and saw that it was still giving the sql error despite no sql code being in my current project (at least saved locally).  It didn't take long for me to put 2 and 2 together.TL;DRWhen editing code for spiders deployed onto Scrapyd you must deploy the spiders again after saving your new code.  I hope this helps save someone from the countless hours of frustration I've experienced over this.edit: adding my if statement that worked:

Scrapy: Duplicate item fields due to multiple for loops

user11949195

[Scrapy: Duplicate item fields due to multiple for loops](https://stackoverflow.com/questions/57566087/scrapy-duplicate-item-fields-due-to-multiple-for-loops)

My question is almost identical to: Scrapy - Why Item Inside For Loop Has The Same Value While Accessed in Another ParserExcept I have two For loops so creating a new item will cause me to lose the data from the first scraped page. The basic structure is:There are 10-40 pieces of data per page but I have a simplified code that looks for 1-2.Very simplified results are:Does Scrapy have a functionality that I'm missing to be able to keep one item through the whole scrape? Results:Should be:       Is:              

2019-08-20 02:47:16Z

My question is almost identical to: Scrapy - Why Item Inside For Loop Has The Same Value While Accessed in Another ParserExcept I have two For loops so creating a new item will cause me to lose the data from the first scraped page. The basic structure is:There are 10-40 pieces of data per page but I have a simplified code that looks for 1-2.Very simplified results are:Does Scrapy have a functionality that I'm missing to be able to keep one item through the whole scrape? Results:Should be:       Is:              The item you are using to store the scraped data is mutable. You should copy it doing item.copy() before passing it to the next request, for example on parse (probably on tourney_info too). Since each method produces a en entry on the json file, but you are holding multiple references to the same item, its expected that this happens.Check too https://docs.scrapy.org/en/latest/topics/items.html#copying-items if you want.

Can't find value of some requests .Aspx website

Nevega

[Can't find value of some requests .Aspx website](https://stackoverflow.com/questions/57569344/cant-find-value-of-some-requests-aspx-website)

In the webpage http://www.wiseco.com/ProductSearch.aspx,

I'm trying to call the dropdown menu selection result, 

and I can't find the value of two headers in the post request:What are these? I can't find these values in the HTML code.Update:doesn't get data, just a page with an error:What am I doing wrong?

2019-08-20 08:14:28Z

In the webpage http://www.wiseco.com/ProductSearch.aspx,

I'm trying to call the dropdown menu selection result, 

and I can't find the value of two headers in the post request:What are these? I can't find these values in the HTML code.Update:doesn't get data, just a page with an error:What am I doing wrong?This is cursor coordinates on a Search button. From my experience you can use any values you want.UPDATE

You need to send separate POST request for each dropdown change: first select Year, send request, parse it and get values for ctl00$ContentPlaceHolder$ddlModelYear etc.

Xpath can't find element and return none

Ubik Bic

[Xpath can't find element and return none](https://stackoverflow.com/questions/57573134/xpath-cant-find-element-and-return-none)

im facing a problem with this page :https://www.ouedkniss.com/op%C3%A9rateur-sur-machine-bejaia-boudjellil-algerie-offres-d-emploi-d19820393i want to scrap this elements:Employeur : SARL UFMATP AZIEZ ET ASSOCIESPoste : Opérateur sur machineBut when i use xpath to find element it cant see them and jump to other element this script return a value for 'Titre'  but none for Boss, i cheked if there are an iframe but not 

any help would be appreciateditems.pysettings.py

2019-08-20 11:56:56Z

im facing a problem with this page :https://www.ouedkniss.com/op%C3%A9rateur-sur-machine-bejaia-boudjellil-algerie-offres-d-emploi-d19820393i want to scrap this elements:Employeur : SARL UFMATP AZIEZ ET ASSOCIESPoste : Opérateur sur machineBut when i use xpath to find element it cant see them and jump to other element this script return a value for 'Titre'  but none for Boss, i cheked if there are an iframe but not 

any help would be appreciateditems.pysettings.py

Trying to log into site with scrapy and response shows login page

user2328273

[Trying to log into site with scrapy and response shows login page](https://stackoverflow.com/questions/57564056/trying-to-log-into-site-with-scrapy-and-response-shows-login-page)

I'm new to Scrapy and I'm trying to get a log in working, starting in the shell. This is the site I'm trying to log into:https://www.acdd.com/customer/account/login/First I did from scrapy.http import FormRequest and then I did token = response.xpath('//*[@name="form_key"]/@value').extract_first() to get the token and the output looks correct. I then didFormRequest.from_response(response,formdata={'form_key': token,'login[customerid]': '12345','login[username]': 'myaddress@email.com','login[password]': 'mysecret'})It outputs<GET https://www.acdd.com/catalogsearch/result/?q=&login%5Bcustomerid%5D=12345&login%5Busername%5D=myaddress%40email.com&login%5Bpassword%5D=mysecret&form_key=abcdef12345>If I do view(response) it just shows the login page and not the user page like it should. I've been following tutorials and examples but I think maybe there is just something different about this site than the simple examples I've used. I logged in with Firefox and looked in the developer tools to see what form data it POST and I have all the elements. It also looks like while the form is on https://www.acdd.com/customer/account/login/, it actually posts to https://www.acdd.com/customer/account/login/Post. I've tried to just post to that page in the shell but there are no form elements. This is outside the basic examples I've worked with. Any help is appreciated.

2019-08-19 21:16:18Z

I'm new to Scrapy and I'm trying to get a log in working, starting in the shell. This is the site I'm trying to log into:https://www.acdd.com/customer/account/login/First I did from scrapy.http import FormRequest and then I did token = response.xpath('//*[@name="form_key"]/@value').extract_first() to get the token and the output looks correct. I then didFormRequest.from_response(response,formdata={'form_key': token,'login[customerid]': '12345','login[username]': 'myaddress@email.com','login[password]': 'mysecret'})It outputs<GET https://www.acdd.com/catalogsearch/result/?q=&login%5Bcustomerid%5D=12345&login%5Busername%5D=myaddress%40email.com&login%5Bpassword%5D=mysecret&form_key=abcdef12345>If I do view(response) it just shows the login page and not the user page like it should. I've been following tutorials and examples but I think maybe there is just something different about this site than the simple examples I've used. I logged in with Firefox and looked in the developer tools to see what form data it POST and I have all the elements. It also looks like while the form is on https://www.acdd.com/customer/account/login/, it actually posts to https://www.acdd.com/customer/account/login/Post. I've tried to just post to that page in the shell but there are no form elements. This is outside the basic examples I've worked with. Any help is appreciated.You didn't select target form and Scrapy uses the first one on the page (search form):Also you don't need form_key here because Scrapy will get it from a form for you.

UPDATE Try to add send key.

Unable to get all the links within a page

Newbie

[Unable to get all the links within a page](https://stackoverflow.com/questions/57533896/unable-to-get-all-the-links-within-a-page)

I am trying to scrape this page:

https://www.jny.com/collections/bottomsIt has a total of 55 products listed with only 24 listed once the page is loaded. However, the div contains list of all the 55 products. I am trying to scrape that using scrappy like this :It only gives me a list of length 25. How do I get the rest?

2019-08-17 06:37:38Z

I am trying to scrape this page:

https://www.jny.com/collections/bottomsIt has a total of 55 products listed with only 24 listed once the page is loaded. However, the div contains list of all the 55 products. I am trying to scrape that using scrappy like this :It only gives me a list of length 25. How do I get the rest?I would suggest scraping it through the API directly - the other option would be rendering Javascript using something like Splash/Selenium, which is really not ideal.If you open up the Network panel in the Developer Tools on Chrome/Firefox, filter down to only the XHR Requests and reload the page, you should be able to see all of the requests being sent out. Some of those requests can help us figure out how the data is being loaded into the HTML. Here's a screenshot of what's going on there behind the scenes.Clicking on those requests can give us more details on how the requests are being made and the request structure. At the end of the day, for your use case, you would probably want to send out a request to https://www.jny.com/collections/bottoms/products.json?limit=250&page=1 and parse the body_html attribute for each Product in the response (perhaps using scrapy.selector.Selector) and use that however you want. Good luck!

How to define a rule in scrapy which crawls the website link recursively?

Rink16

[How to define a rule in scrapy which crawls the website link recursively?](https://stackoverflow.com/questions/57573599/how-to-define-a-rule-in-scrapy-which-crawls-the-website-link-recursively)

I am trying to build an application which uses scrapy to crawls a website to get all the links which are on homepage plus the links which can be reached using the homepage link.But the problem is that i am not able to figure out how to set the rules in scrapy to get all the direct+indirect url from start_urls. I think the rules can be set to get through the recursive call to parse function which can parse the each page.Please help. Code is attached below which is working fine but when i run it on one website it showed me only 10 urls (of homepage) while that website contain interlinked 100 URLs.Output of above code:Expected Urls in Output which i done using BeautifulSoup Library:

2019-08-20 12:23:38Z

I am trying to build an application which uses scrapy to crawls a website to get all the links which are on homepage plus the links which can be reached using the homepage link.But the problem is that i am not able to figure out how to set the rules in scrapy to get all the direct+indirect url from start_urls. I think the rules can be set to get through the recursive call to parse function which can parse the each page.Please help. Code is attached below which is working fine but when i run it on one website it showed me only 10 urls (of homepage) while that website contain interlinked 100 URLs.Output of above code:Expected Urls in Output which i done using BeautifulSoup Library:Example Answer using BeautifulSoup - I have also tried some way using BeautifulSoup

code as belowoutput of above answer

How to incorporate multiprocessing in scrapy

Newbie

[How to incorporate multiprocessing in scrapy](https://stackoverflow.com/questions/57535022/how-to-incorporate-multiprocessing-in-scrapy)

I am trying to scrape:

https://www.jny.com/collections/bottomsIn order to crawl and scrape multiple pages at once, I am using multiprocessingIt gives the following error:Other answers to this question state that pool should be stated at the start of module. However, that is not possible in this case as startRequest is the first method that is called.

2019-08-17 09:26:37Z

I am trying to scrape:

https://www.jny.com/collections/bottomsIn order to crawl and scrape multiple pages at once, I am using multiprocessingIt gives the following error:Other answers to this question state that pool should be stated at the start of module. However, that is not possible in this case as startRequest is the first method that is called.Scrapy already scrapes multiple pages at once using asynchronous programming.You can tune CONCURRENT_ settings to fit your concurrency requirements.

Fetch number of results from Google News for a given time range

Mayank Lal

[Fetch number of results from Google News for a given time range](https://stackoverflow.com/questions/57535339/fetch-number-of-results-from-google-news-for-a-given-time-range)

I want to get the total number of news articles published for a list of keywords between a specified time period (Like last six months).I've tried to use scrapy library to scrap google news but I'm unable to get results for the specified time period using the below code:I want to create a data frame as final output with the entity and number of results found on google news.

2019-08-17 10:13:26Z

I want to get the total number of news articles published for a list of keywords between a specified time period (Like last six months).I've tried to use scrapy library to scrap google news but I'm unable to get results for the specified time period using the below code:I want to create a data frame as final output with the entity and number of results found on google news.The selectors seem to work, but accessing the page is forbidden by robotstxt.

You can overcome this by setting the following in your settings.py:

ROBOTSTXT_OBEY=False. Alternatively, you can put this in your class:Caution this can cause that Google bans your IP address, this can work for few pages but after will ask for captcha challenge

Why can't replace placeholder with format function in pymysql?

it_is_a_literature

[Why can't replace placeholder with format function in pymysql?](https://stackoverflow.com/questions/57534466/why-cant-replace-placeholder-with-format-function-in-pymysql)

How i create the table mingyan.It's said that string format function with {} is more pythonic way than  placeholder %.

In my scrapy to write some fields into a table mingyan.It works fine in my scrapy,now i replace the placeholder way with string format function.The scrapy  got error info Why can't replace placeholder with format function in pymysql?The item in scrapy doucment.

item meaning in scrapy

2019-08-17 08:08:38Z

How i create the table mingyan.It's said that string format function with {} is more pythonic way than  placeholder %.

In my scrapy to write some fields into a table mingyan.It works fine in my scrapy,now i replace the placeholder way with string format function.The scrapy  got error info Why can't replace placeholder with format function in pymysql?The item in scrapy doucment.

item meaning in scrapyIn short: the parameter substitution implemented in cursor.execute is not the same as Python string formatting, despite the use of "%s" as a placeholder; that's why you get different results.Databases expect query parameters to be quoted - surrounded by single or double quotes, even backticks - in specific ways.  Python's DBAPI standard provides parameter substitution functionality to automate the tedious and error-prone process of parameter quoting.Database driver packages that implement the DBAPI standard automatically apply the correct quoting rules to query parameters.  So for example, given this codeThe driver will generate sql with this VALUES clause:Observe thatUsing string formatting rather than DBAPI parameter substitution means you need to know and apply these rules yourself, for example like this:Note the quotation marks surrounding the format placeholder.MySQL's quoting rules are discussed in detail in the answers to this question.I always do the following

Unable to perform login for scraping with Scrapy

ziliang_chong

[Unable to perform login for scraping with Scrapy](https://stackoverflow.com/questions/57496956/unable-to-perform-login-for-scraping-with-scrapy)

my first time asking a question here so please bear with me if I'm not providing everything that is needed.I'm trying to build a spider that goes to this website (https://newslink.sg/user/Login.action), logs in (I have a valid set of username and password) and then scrape some pages. I'm unable to get past the login stage. I suspect it has to do with the formdata and what I enter inside, as there are "login.x" and "login.y" fields when I check the form data. The login.x and login.y fields seem to change whenever I log in again. This question and answer seems to provide a hint of how I can fix things but I don't know how to go about extracting the correct values.Python scrapy - Login Authenication IssueBelow is my code with some modification. If I run the code without the login.x and login.y lines, I get blank scrapes. Thanks for your help!

2019-08-14 14:41:35Z

my first time asking a question here so please bear with me if I'm not providing everything that is needed.I'm trying to build a spider that goes to this website (https://newslink.sg/user/Login.action), logs in (I have a valid set of username and password) and then scrape some pages. I'm unable to get past the login stage. I suspect it has to do with the formdata and what I enter inside, as there are "login.x" and "login.y" fields when I check the form data. The login.x and login.y fields seem to change whenever I log in again. This question and answer seems to provide a hint of how I can fix things but I don't know how to go about extracting the correct values.Python scrapy - Login Authenication IssueBelow is my code with some modification. If I run the code without the login.x and login.y lines, I get blank scrapes. Thanks for your help!Two possible reasons:So I recommend you to rewrite it this way:Scrapy will send goto automatically for you. login.x and login.y is just cursor coordinates when you click on a Login button.

Scrapy distributed crawler There are several child crawlers who have not gone to redis to fetch requests, may be idling

hewm

[Scrapy distributed crawler There are several child crawlers who have not gone to redis to fetch requests, may be idling](https://stackoverflow.com/questions/57497677/scrapy-distributed-crawler-there-are-several-child-crawlers-who-have-not-gone-to)

I am using scrapy with scrapy-redis to write a distributed crawler that is run by a single crawler. After deploying to several machines separately, there are several from the crawler that just took the proxy and may not request it. I added it inside the crawler resolution. The log did not see the print log just left the proxy middleware in middlewares.pyOnly in the log

2019-08-14 18:39:56 [scrapy.extensions.logstats] INFO: Crawled 14575 pages (at 0 pages/min), scraped 13242 items (at 0 items/min)I hope it will show a crawling url that actually has nothing

2019-08-14 15:22:38Z

I am using scrapy with scrapy-redis to write a distributed crawler that is run by a single crawler. After deploying to several machines separately, there are several from the crawler that just took the proxy and may not request it. I added it inside the crawler resolution. The log did not see the print log just left the proxy middleware in middlewares.pyOnly in the log

2019-08-14 18:39:56 [scrapy.extensions.logstats] INFO: Crawled 14575 pages (at 0 pages/min), scraped 13242 items (at 0 items/min)I hope it will show a crawling url that actually has nothing

Why is Splash Request not Rendering

TheGr8Destructo

[Why is Splash Request not Rendering](https://stackoverflow.com/questions/57498590/why-is-splash-request-not-rendering)

I am doing the tutorial exercise for scrapy-splash in VS Code. I cannot parse the Javascript on http://quotes.toscrape.com/js/. I have Splash running on local host 8050, I pulled it from docker with:scrapy-splash is installed in the root directory of the scrapy project.The Settings.py is:}I have stopped and restarted the docker image with and without private browsing enabled.I have added in a wait in the splash request and nothing. I have built the spider up from scratch and just copied in the example.Code works fine on HTML pages but there is nothing when I use the JS version using SplashRequest. This is the "hello world" of scraping and I am really struggling to see where/ what I am doing wrong. I suspect its so dumb its obvious, but I cant see where or what I am doing wrong. I am using VS Code so perhaps there is something in my set up that is causing this but I am using a venv.pythonThis is the logfile:

2019-08-14 16:23:52Z

I am doing the tutorial exercise for scrapy-splash in VS Code. I cannot parse the Javascript on http://quotes.toscrape.com/js/. I have Splash running on local host 8050, I pulled it from docker with:scrapy-splash is installed in the root directory of the scrapy project.The Settings.py is:}I have stopped and restarted the docker image with and without private browsing enabled.I have added in a wait in the splash request and nothing. I have built the spider up from scratch and just copied in the example.Code works fine on HTML pages but there is nothing when I use the JS version using SplashRequest. This is the "hello world" of scraping and I am really struggling to see where/ what I am doing wrong. I suspect its so dumb its obvious, but I cant see where or what I am doing wrong. I am using VS Code so perhaps there is something in my set up that is causing this but I am using a venv.pythonThis is the logfile:

Piplines.py export data from spider to postgresql database

Максим Щедров

[Piplines.py export data from spider to postgresql database](https://stackoverflow.com/questions/57479541/piplines-py-export-data-from-spider-to-postgresql-database)

I wrote a crawler for DomRia and I want to save all data in a database.

I'm using python 3.7 and psycopg2. My database works in a docker-compose container. I can see all info about the db in pgadmin.   I thought the problem was in my query, but it seems ok.But when I run my spider I have a problem:How to fix it?

2019-08-13 14:08:20Z

I wrote a crawler for DomRia and I want to save all data in a database.

I'm using python 3.7 and psycopg2. My database works in a docker-compose container. I can see all info about the db in pgadmin.   I thought the problem was in my query, but it seems ok.But when I run my spider I have a problem:How to fix it?Can you check the type of column domria.price and domria.url. You may note that you are trying to insert a Python list into them.Can you also try using this:

Can scrapyRT run in parallel on Heroku using gunicorn?

Arnau

[Can scrapyRT run in parallel on Heroku using gunicorn?](https://stackoverflow.com/questions/57483408/can-scrapyrt-run-in-parallel-on-heroku-using-gunicorn)

So far, I have been able to run my scrapyRT project on Heroku (cloud platform). Next step is running scrapyRT in parallel on a Heroku's single node (1 dyno), as I want to use my computational resources effectively. One option is using gunicorn, as it is able to run various WSGI HTTP servers in parallel. Unfortunately, scrapyRT is based on Twisted and it does not support WSGI natively. As far as I know, twisted can serve WSGI applications (please check

https://twistedmatrix.com/documents/10.0.0/web/howto/web-in-60/wsgi.html), but I would like to know if following this website is a good idea for adapting scrapyRT in a WSGI application or I can find some design restriction.

2019-08-13 18:22:14Z

So far, I have been able to run my scrapyRT project on Heroku (cloud platform). Next step is running scrapyRT in parallel on a Heroku's single node (1 dyno), as I want to use my computational resources effectively. One option is using gunicorn, as it is able to run various WSGI HTTP servers in parallel. Unfortunately, scrapyRT is based on Twisted and it does not support WSGI natively. As far as I know, twisted can serve WSGI applications (please check

https://twistedmatrix.com/documents/10.0.0/web/howto/web-in-60/wsgi.html), but I would like to know if following this website is a good idea for adapting scrapyRT in a WSGI application or I can find some design restriction.

ModuleNotFoundError: No module named 'scrapy_user_agents'

Cff

[ModuleNotFoundError: No module named 'scrapy_user_agents'](https://stackoverflow.com/questions/57479953/modulenotfounderror-no-module-named-scrapy-user-agents)

I tried to use scrapy_user_agents with scrapy-proxy-pool.I added these lines in my settings.py:when I run my spider, I get this error message:I removed the lines of proxy in the middleware, but I get same issue, same error message.You will find below the complete log errors:

2019-08-13 14:31:06Z

I tried to use scrapy_user_agents with scrapy-proxy-pool.I added these lines in my settings.py:when I run my spider, I get this error message:I removed the lines of proxy in the middleware, but I get same issue, same error message.You will find below the complete log errors:Try uninstalling and installing the module again to make sure its installed for your version of python. 

Multiple Scripts/Spiders writing to different CSV files. Will this code cause any problems?

reg202

[Multiple Scripts/Spiders writing to different CSV files. Will this code cause any problems?](https://stackoverflow.com/questions/57486471/multiple-scripts-spiders-writing-to-different-csv-files-will-this-code-cause-an)

I'm building some spiders to do some web scraping and am trying to figure out if my code is ok as written before I start building them out. The spiders will run via crontab at the same time, though they each write to a separate file.I'm not sure how the 'open as price_list2' or 'savepriceurl2 = csv.writer' parts of the code work, and will the spiders get mixed up if they all use the same names, even for a different csv file, if they are all running at the same time?

2019-08-13 23:29:44Z

I'm building some spiders to do some web scraping and am trying to figure out if my code is ok as written before I start building them out. The spiders will run via crontab at the same time, though they each write to a separate file.I'm not sure how the 'open as price_list2' or 'savepriceurl2 = csv.writer' parts of the code work, and will the spiders get mixed up if they all use the same names, even for a different csv file, if they are all running at the same time?From the minimal code posted it is difficult to say if there will be an issue with two. Assuming that the code you posted will run in each instance of the object (I assume) they will be writing to whatever store they are scraping (defined by your item['store_name'].Regarding your questions about the code, the open(...) as price_list2 returns an io.TextIOWrapper object (details here) which is stored as the variable price_list. You could achieve the same by writing: price_list2 = open(...)however then you must close the file in order to not leak memory/data. However by writing it as with open(...) as file: means you do not have to call file.close() and thus ensures the file is always closed after usage.The other line you asked about, savepriceurl2 = csv.writer(...) creates an object that simplifies writing to the actual file. Thus, you can simply use the object function writerow() to easily write a row to the desired file. More information on that can be found here.So basically what your code is doing is this:For your last question, given there is no information on your actual design and setup, I am assuming that each spider is an instance of the class that holds this file. As long as each spider is going to different sites (thus meaning that one spider will not have item['store_name'] be the same as the other spider, you should be writing to different files. As long as this is the case it should be fine (I'm not aware of issues of writing two to files 'at the same time' in python). If this is not the case you will run into issues if your spiders try to write to the same file at the same time.As a tip, googling the functions will often get you the description and clarification on functions quicker than a post here and will have a lot more information. I hope this helps and clarifies things for you.

How to handle emty dictionary?

Cff

[How to handle emty dictionary?](https://stackoverflow.com/questions/57483682/how-to-handle-emty-dictionary)

I am making a scraper with scrapy python 3. My script suppose to scrap a directory of companies.Sometimes, scrapy don't find a field of item (the email or the website) because directory didn't publish these fields.I am trying to handle this kind of exception, but as a newbie, it is complicated for me.I tried to fix that with some "if". But my problem is to handle it in my pipeline.py file where I add the values in my MYSQL database.This is my spider file:This is my pipelines.py:And this is my items.py:So I get several issues:

Or I get some Keyerror

or I get some List error

or it is mysql which can't add dictionary in a text field.Also my scraper is stopping after hundreds of requests whereas there are thousands of urls to scrap. But this is another topic (just in case you see the reason why).Thanks in advance for trying to help me. 

2019-08-13 18:43:10Z

I am making a scraper with scrapy python 3. My script suppose to scrap a directory of companies.Sometimes, scrapy don't find a field of item (the email or the website) because directory didn't publish these fields.I am trying to handle this kind of exception, but as a newbie, it is complicated for me.I tried to fix that with some "if". But my problem is to handle it in my pipeline.py file where I add the values in my MYSQL database.This is my spider file:This is my pipelines.py:And this is my items.py:So I get several issues:

Or I get some Keyerror

or I get some List error

or it is mysql which can't add dictionary in a text field.Also my scraper is stopping after hundreds of requests whereas there are thousands of urls to scrap. But this is another topic (just in case you see the reason why).Thanks in advance for trying to help me. There are several ways in scrapy to use default values.

The most simple one is just using a structure like this:Overall this code will look better:You can also use Scrapy item loaders, it's a little more complicated, but will allow you to add more flexibility to your pipeline..

Checking source code in a scrapy response

Manuel

[Checking source code in a scrapy response](https://stackoverflow.com/questions/57486244/checking-source-code-in-a-scrapy-response)

I have made a pretty big spider that basically extracts the data from an amazon product page.The problem is that sometimes, no data comes back when I extract. After that happens I check the URL that was processed and, following the xpath with a chrome tool, the data is in fact there.I know that what me and the Chrome tool sees is not the same as what the spider processes so, is it there any way to actually see the source code the spider is trying to extract from? and will the XPath I make with the chrome tool's help be trustworthy? 

2019-08-13 22:56:21Z

I have made a pretty big spider that basically extracts the data from an amazon product page.The problem is that sometimes, no data comes back when I extract. After that happens I check the URL that was processed and, following the xpath with a chrome tool, the data is in fact there.I know that what me and the Chrome tool sees is not the same as what the spider processes so, is it there any way to actually see the source code the spider is trying to extract from? and will the XPath I make with the chrome tool's help be trustworthy? You can save a "bad" response from Scrapy and investigate it:Check the view-source with (Ctrl-U in Chrome). Chrome tools will not always line up with the html source. Probably due to the JavaScript on the page.

Scrapy (python) responses alternating between bytes and utf8

Anthony Sanchez

[Scrapy (python) responses alternating between bytes and utf8](https://stackoverflow.com/questions/57487256/scrapy-python-responses-alternating-between-bytes-and-utf8)

I am using scrapy web crawler, when scraping a site, the responses keep alternating between html and bytes, which are encoded utf8 but I receive and error when trying to decode them.I have tried multiple different headers for encoding, accepting gzip, deflate, text/html;charset=utf-8, br but they keep giving me the same issue.To receive html instead of bytes, (here is a snippet of bytes received compared to html).

expected responseActual response

2019-08-14 01:55:22Z

I am using scrapy web crawler, when scraping a site, the responses keep alternating between html and bytes, which are encoded utf8 but I receive and error when trying to decode them.I have tried multiple different headers for encoding, accepting gzip, deflate, text/html;charset=utf-8, br but they keep giving me the same issue.To receive html instead of bytes, (here is a snippet of bytes received compared to html).

expected responseActual response"Accept-Encoding": "text/html;charset=utf-8" looks wrong. Try "Accept-Encoding": "gzip", or removing it altogether.

Scrapy request not getting an actual response

Manuel

[Scrapy request not getting an actual response](https://stackoverflow.com/questions/57486875/scrapy-request-not-getting-an-actual-response)

I am having some issues with my scrapy spider.  Most of the time, when I send a request, a response is sent back and data can be scraped just fine.The problem is that, sometimes, no response comes at all. It seemed weird, so I checked the scrapy shell followed by the URL that got no response, then viewed the response and it actually existed. So some responses are not being scraped and I have no idea why, mainly because sometimes the problem occurs but most of the time it does not.In regards of my spider settings, Im running as a mozilla user with Robots.txt not obeyed.Any idea on what could be going on?

2019-08-14 00:44:38Z

I am having some issues with my scrapy spider.  Most of the time, when I send a request, a response is sent back and data can be scraped just fine.The problem is that, sometimes, no response comes at all. It seemed weird, so I checked the scrapy shell followed by the URL that got no response, then viewed the response and it actually existed. So some responses are not being scraped and I have no idea why, mainly because sometimes the problem occurs but most of the time it does not.In regards of my spider settings, Im running as a mozilla user with Robots.txt not obeyed.Any idea on what could be going on?

How to crawl a website to get all the links in a website using Scrapy in python?

Rink16

[How to crawl a website to get all the links in a website using Scrapy in python?](https://stackoverflow.com/questions/57490072/how-to-crawl-a-website-to-get-all-the-links-in-a-website-using-scrapy-in-python)

I am beginner in python and using scrapy to crawl all the link recursively and wanted to map each link to text found in that link.For this, I need to define my own spider class which can take arguments of the name and list of type of website to crawl and i want to build a dictionary of the link to text present in website, but i am lacking concept objects in python class. I tried some in below code to run the scrapy by creating objects but it is giving me error. Please help me to make the objects of the class (which pass the arguments having the name of webpage/website to crawl) and form the dictionary of {'URL':'all text found in that URL'}Output i am getting with not link being printed

2019-08-14 07:32:52Z

I am beginner in python and using scrapy to crawl all the link recursively and wanted to map each link to text found in that link.For this, I need to define my own spider class which can take arguments of the name and list of type of website to crawl and i want to build a dictionary of the link to text present in website, but i am lacking concept objects in python class. I tried some in below code to run the scrapy by creating objects but it is giving me error. Please help me to make the objects of the class (which pass the arguments having the name of webpage/website to crawl) and form the dictionary of {'URL':'all text found in that URL'}Output i am getting with not link being printedTo run with arguments you have to use __init__When you will run (without scrapy.Spider)then Python will execute something likeIf you generated project to run crawler then you don't create instance but you run scrapy which use spider automatically and have to send data in command line using but it will send it as strings so you may have to convert them into lists - ie. using split() in __init__EDIT: Working code after usingto convert relative url to absolute urland adding self. in callback=self.print_this_linkThere is no need to create hxs = scrapy.Selector(response) because response.xpath works too.It is standalone script which works without creating project. It yield url and page's title which is saved in output.csv

Trouble writing Scrapy selector

Ben

[Trouble writing Scrapy selector](https://stackoverflow.com/questions/57487450/trouble-writing-scrapy-selector)

Very new to python, trying to explore the possibility of importing a long developed project from another language and a buddy swears that Python is my answer. I have the IDE up and running, scrapy working properly and properly kicking the 'name' and 'rank' listed on the website conveniently to a .csv.Problem arises in that I have spent the last hour trying to figure out how to extract the 'team player' field on the website. It is a span, it is the first instance I have encountered with scrapy that has a space in the namespace, which seems ill advised.Below is my code, everything works fine aside from pulling the "team position" last line. The code presented is but a representation of the many iterations I have been through trying to get this. Any help would be greatly appreciated.

2019-08-14 02:26:50Z

Very new to python, trying to explore the possibility of importing a long developed project from another language and a buddy swears that Python is my answer. I have the IDE up and running, scrapy working properly and properly kicking the 'name' and 'rank' listed on the website conveniently to a .csv.Problem arises in that I have spent the last hour trying to figure out how to extract the 'team player' field on the website. It is a span, it is the first instance I have encountered with scrapy that has a space in the namespace, which seems ill advised.Below is my code, everything works fine aside from pulling the "team position" last line. The code presented is but a representation of the many iterations I have been through trying to get this. Any help would be greatly appreciated.For CSS team and position are two classes and you have to use dot two times - without space.BTW: xpath treats "team position" as one name.

ItemLoader add_value method does not work on some fields

sky

[ItemLoader add_value method does not work on some fields](https://stackoverflow.com/questions/57448745/itemloader-add-value-method-does-not-work-on-some-fields)

I am using scrapy's ItemLoader in my project. The problem is that it works fine for some fields while it does nothing for other fields. I am working with Scrapy 1.7.2 / Python 3.7 on ubuntu 16.04 server.So there are several fields which share the same processors.

While debugging, I can see that only supplementary_charges is processed and added to the item while other price fields is missing in the item. It looks like item.add_value(label, value) does nothing for 4 price fields out of 5. 

Here is my code:Labels are taken from hardcoded dictionary:I'm pretty sure that all price values are being scraped properly:

But at the end all prices are missing in the item while supplementary_charges and other fields are in place:

And if I debug it step by step,

I can clearly see that nothing is changes after item.add_value call.What am I missing or doing wrong?

2019-08-11 09:17:26Z

I am using scrapy's ItemLoader in my project. The problem is that it works fine for some fields while it does nothing for other fields. I am working with Scrapy 1.7.2 / Python 3.7 on ubuntu 16.04 server.So there are several fields which share the same processors.

While debugging, I can see that only supplementary_charges is processed and added to the item while other price fields is missing in the item. It looks like item.add_value(label, value) does nothing for 4 price fields out of 5. 

Here is my code:Labels are taken from hardcoded dictionary:I'm pretty sure that all price values are being scraped properly:

But at the end all prices are missing in the item while supplementary_charges and other fields are in place:

And if I debug it step by step,

I can clearly see that nothing is changes after item.add_value call.What am I missing or doing wrong?

Trying to scrape a website with scrapy - Not receiving any data

Bamieschijf

[Trying to scrape a website with scrapy - Not receiving any data](https://stackoverflow.com/questions/57449933/trying-to-scrape-a-website-with-scrapy-not-receiving-any-data)

For an assignment I have to fetch from data from a Kaercher webshop. The data I need to fetch is the product Title, description and price.Additionally I need to be able to fetch multiple products (high pressure cleaners, vacuum cleaners, ...) with the same script. So I probably need to make a .csv keyword file or something to adjust the URL accordingly.However, I can't seem to be able to fetch the data with my current script..Info: I will add my entire file structure and current code. I only adjusted the actual spider file (karcher_crawler.py), the other files are mostly default.My folder structure:My "karcher_crawler.py" codeMy "items.py" code:My "pipelines.py" code:my "scrapy.cfg" code:

2019-08-11 12:28:03Z

For an assignment I have to fetch from data from a Kaercher webshop. The data I need to fetch is the product Title, description and price.Additionally I need to be able to fetch multiple products (high pressure cleaners, vacuum cleaners, ...) with the same script. So I probably need to make a .csv keyword file or something to adjust the URL accordingly.However, I can't seem to be able to fetch the data with my current script..Info: I will add my entire file structure and current code. I only adjusted the actual spider file (karcher_crawler.py), the other files are mostly default.My folder structure:My "karcher_crawler.py" codeMy "items.py" code:My "pipelines.py" code:my "scrapy.cfg" code:I managed to request the required data using the following code:Spider file (.py)items file (.py.Thanks to @gangabass I managed to locate the URL's which contain the data I needed to extract. (you can find them in the "Network" tab when you are inspecting a webpage (press F12 or right click anywhere to inspect).

How to put a link in item with ItemLoader?

audiotec

[How to put a link in item with ItemLoader?](https://stackoverflow.com/questions/57453009/how-to-put-a-link-in-item-with-itemloader)

I would like to save a link into item with ItemLoader.Basically I need to transform this code:Into:I appreciate your answer!

2019-08-11 19:35:57Z

I would like to save a link into item with ItemLoader.Basically I need to transform this code:Into:I appreciate your answer!Hi again (: you could continue in your previous question.There are two ways:It's pretty straightforward.

Throttle Requests in Scrapy

myans ans

[Throttle Requests in Scrapy](https://stackoverflow.com/questions/57422076/throttle-requests-in-scrapy)

I am developing a spider with Scrapy that iterates through a keyed url. For example, it will use a url as a template (eg https:\google.com{key}). I am having a problem where I can not get it to stop iterating through those urls at the right time. For example, if I begin to receive enough failed requests such as 404s I would like to terminate so I am not sending more requests than needed. I attempted to raise CloseSpider(). This partially works. It will stop the spider, but not before some requests finish going through. I then attempted to just continually yield the requests while keeping track of how many requests have executed/ failed. The problem is I don't think Scrapy can run asynchronously from start_requests. I really need one of two solutions:1) A way to dynamically yield results from start_requests from Scrapy (from another article this doesn't seem possible).That way I can keep track of current errors and only finish yielding the results when I know i haven't hit a certain error threshold. 2) How to allow the already downloaded pages to finish processing through their callbacks and pipelines when a CloseSpider exception is thrown. This way any non 404 hits actaully finish.

2019-08-09 00:13:18Z

I am developing a spider with Scrapy that iterates through a keyed url. For example, it will use a url as a template (eg https:\google.com{key}). I am having a problem where I can not get it to stop iterating through those urls at the right time. For example, if I begin to receive enough failed requests such as 404s I would like to terminate so I am not sending more requests than needed. I attempted to raise CloseSpider(). This partially works. It will stop the spider, but not before some requests finish going through. I then attempted to just continually yield the requests while keeping track of how many requests have executed/ failed. The problem is I don't think Scrapy can run asynchronously from start_requests. I really need one of two solutions:1) A way to dynamically yield results from start_requests from Scrapy (from another article this doesn't seem possible).That way I can keep track of current errors and only finish yielding the results when I know i haven't hit a certain error threshold. 2) How to allow the already downloaded pages to finish processing through their callbacks and pipelines when a CloseSpider exception is thrown. This way any non 404 hits actaully finish.I figured this out.Since I am traversing in keyed order expecting a key to eventually not exist, I need to configure scrapy to work in FIFO order instead of the default LIFO order in settings.py:I also ensured that the 2 and three depth requests had a higher priority than the start requests. Then, by keeping track of 404s I was able to raise the CloseSpider exception with all expected results completed. 

Scrapyd - URL parsing problem when passed as a spider argument

FalconOnRails

[Scrapyd - URL parsing problem when passed as a spider argument](https://stackoverflow.com/questions/57421765/scrapyd-url-parsing-problem-when-passed-as-a-spider-argument)

I added the following code in my Spider class to be able to pass the URL as an argument:(The replace function is to remove the backslashes introduced by terminal escaping).The spider recognizes the url, starts parsing and closes perfectly locally when I run : However, when I do the same thing through scrapyd, and I run:I get an error because the url isn't parsed the same way as when using scrapy crawl. LOG:After some experimentation, I discovered that for some reason, when passing the URL as a spider argument through scrapyd, it stops parsing whenever it reaches a & character. Any insights as to how to remediate this behavior?

2019-08-08 23:21:06Z

I added the following code in my Spider class to be able to pass the URL as an argument:(The replace function is to remove the backslashes introduced by terminal escaping).The spider recognizes the url, starts parsing and closes perfectly locally when I run : However, when I do the same thing through scrapyd, and I run:I get an error because the url isn't parsed the same way as when using scrapy crawl. LOG:After some experimentation, I discovered that for some reason, when passing the URL as a spider argument through scrapyd, it stops parsing whenever it reaches a & character. Any insights as to how to remediate this behavior?I managed to solve my problem. It was with the way the POST request was being sent through cURL, not with Scrapyd.After inspection of this request: I got:Apparently, since the POST request is sent like this:Whenever there is a &, it is considered as a new argument. So the URL part that gets taken into the target_url argument is only https://www.example.com/list.htm?tri=initial and the rest is considered another argument of the POST request.After using Postman and trying the following POST request:It worked, and the job started successfully on Scrapyd!Through cURL, using -F instead of -d worked perfectly:

How to get the next page of reviews on goodreads.com using scrapy splash?

Abd Elfatah Hezema

[How to get the next page of reviews on goodreads.com using scrapy splash?](https://stackoverflow.com/questions/57396703/how-to-get-the-next-page-of-reviews-on-goodreads-com-using-scrapy-splash)

I am working on a project that need to get the reviews of some people on a book. I am using scrapy and scrapy-splash, the problem is that i can't get the next page of the reviews using the CSS selector of the next page icon. here is the scrapy-splash script: here is the link of the web page that i am trying to get its next page: https://www.goodreads.com/book/show/3243517I tried copying the selector from dev tools on chrome and on mozzila firefox but they both failedthe expected is to get the next page but it displays the same page.

2019-08-07 14:27:02Z

I am working on a project that need to get the reviews of some people on a book. I am using scrapy and scrapy-splash, the problem is that i can't get the next page of the reviews using the CSS selector of the next page icon. here is the scrapy-splash script: here is the link of the web page that i am trying to get its next page: https://www.goodreads.com/book/show/3243517I tried copying the selector from dev tools on chrome and on mozzila firefox but they both failedthe expected is to get the next page but it displays the same page.

Request again with a different proxy when one request fail

NeDark

[Request again with a different proxy when one request fail](https://stackoverflow.com/questions/57418174/request-again-with-a-different-proxy-when-one-request-fail)

I use a pool of proxies, so each request is assigned a different proxy, with a code similar to this:The problem is when a bad proxy appears and connection fails, the implied webpage will be missing. I want to restart the failed requests with a different proxy.Can it be done with a middleware?

2019-08-08 17:47:55Z

I use a pool of proxies, so each request is assigned a different proxy, with a code similar to this:The problem is when a bad proxy appears and connection fails, the implied webpage will be missing. I want to restart the failed requests with a different proxy.Can it be done with a middleware?

Scrapy CrawlSpider - Yielding multiple items from a single spider

Schlagito

[Scrapy CrawlSpider - Yielding multiple items from a single spider](https://stackoverflow.com/questions/57396755/scrapy-crawlspider-yielding-multiple-items-from-a-single-spider)

i'm very new either with Scrapy or Python so my vocabulary might be inacurateI'm trying to get two different items with my CrawlSpider, but i cannot find out how to do it. Currently only the first item "CourseItem" is crawled, the other one is not executed at all.I believe it's probably the way im calling everything but can't figure exactly what. (callback=parse_course) ?Each function is working properly if i make one spider per item.

I've tried to put everything in the same function but it doesn't help either.spider.pyitems.pyIm expecting to get both CourseItem() and PronosticItem() to get populated and sent to pipeplines and not just only CourseItem()Thanks for your help !

2019-08-07 14:29:05Z

i'm very new either with Scrapy or Python so my vocabulary might be inacurateI'm trying to get two different items with my CrawlSpider, but i cannot find out how to do it. Currently only the first item "CourseItem" is crawled, the other one is not executed at all.I believe it's probably the way im calling everything but can't figure exactly what. (callback=parse_course) ?Each function is working properly if i make one spider per item.

I've tried to put everything in the same function but it doesn't help either.spider.pyitems.pyIm expecting to get both CourseItem() and PronosticItem() to get populated and sent to pipeplines and not just only CourseItem()Thanks for your help !

How request multiple links at once and parse them later with scrapy?

Mr.SpyCat

[How request multiple links at once and parse them later with scrapy?](https://stackoverflow.com/questions/57389599/how-request-multiple-links-at-once-and-parse-them-later-with-scrapy)

I use Scrapy to get data from an API call but the server is laggy.

First I scrape one page to get some IDs, and I add them to a list.

After that, I check how many IDs I have, and I start scraping.The max IDs I can add is 10: event_id=1,2,3,4,5,6,7,8,9,10.

The problem is, because there are many IDs like 150, I have to make many requests, and the server responds after 3-5 seconds. I want to request all links at once and parse them later if this is possible.

2019-08-07 07:59:41Z

I use Scrapy to get data from an API call but the server is laggy.

First I scrape one page to get some IDs, and I add them to a list.

After that, I check how many IDs I have, and I start scraping.The max IDs I can add is 10: event_id=1,2,3,4,5,6,7,8,9,10.

The problem is, because there are many IDs like 150, I have to make many requests, and the server responds after 3-5 seconds. I want to request all links at once and parse them later if this is possible.try changing CONCURRENT_REQUESTS which is by default 16 to a higher number.as per scrapy docs:Note that in some cases this results in hardware bottlenecks, so try not to increase them by a lot. I'd recommend gradually increasing this value and observing system stats (CPU/Network).

How to integrate tests into Scrapy project structure

Melvin

[How to integrate tests into Scrapy project structure](https://stackoverflow.com/questions/57355911/how-to-integrate-tests-into-scrapy-project-structure)

I started a new scrapy project and I would like to test my spiders using pytest. The following command creates the standard scrap project structure.What would be the optimal place to put the test folder? My first thought would have been this:Would it make sense to test the python modules created by scrapy? (e.g. items.py)

2019-08-05 09:40:39Z

I started a new scrapy project and I would like to test my spiders using pytest. The following command creates the standard scrap project structure.What would be the optimal place to put the test folder? My first thought would have been this:Would it make sense to test the python modules created by scrapy? (e.g. items.py)

python scrapy check if the class exists else check for other

ds_user

[python scrapy check if the class exists else check for other](https://stackoverflow.com/questions/57356352/python-scrapy-check-if-the-class-exists-else-check-for-other)

I am new to using Scrapy. My response will have this structure.I can use this for finding sold results, but the classes differ based on the property auction results. There are three different class types padb-ribbon-not-sold, padb-ribbon-sold-post, padb-ribbon-sold. How do i capture this in order?

2019-08-05 10:05:47Z

I am new to using Scrapy. My response will have this structure.I can use this for finding sold results, but the classes differ based on the property auction results. There are three different class types padb-ribbon-not-sold, padb-ribbon-sold-post, padb-ribbon-sold. How do i capture this in order?You could try the following xpath to get the sold status for each listing:Not sure what the page structure looks like, but if you need to loop through the property-cards you can do something like this:

How do I control the flow of a scrapy crawler to target one page at a time?

runningmanjoe

[How do I control the flow of a scrapy crawler to target one page at a time?](https://stackoverflow.com/questions/57340495/how-do-i-control-the-flow-of-a-scrapy-crawler-to-target-one-page-at-a-time)

I'm trying to extract information from a simple online gallery. The gallery structure is like: (many)[pages] >containing many> [albums] >contaning many> [photos].I set up the following spider to get the title and photos for each album on all pages (just the URLs for now):This gives me the data I want but not in any order. I want to get the title and all photos of a particular album before moving to the next one, then, after every album on that page is done, move to the next page. How can this be done?

2019-08-03 17:08:23Z

I'm trying to extract information from a simple online gallery. The gallery structure is like: (many)[pages] >containing many> [albums] >contaning many> [photos].I set up the following spider to get the title and photos for each album on all pages (just the URLs for now):This gives me the data I want but not in any order. I want to get the title and all photos of a particular album before moving to the next one, then, after every album on that page is done, move to the next page. How can this be done?

How can i use requests.Session.get() in Scrapy. I tried it by myself but Got Error

Rao Tauqeer Sajid

[How can i use requests.Session.get() in Scrapy. I tried it by myself but Got Error](https://stackoverflow.com/questions/57339253/how-can-i-use-requests-session-get-in-scrapy-i-tried-it-by-myself-but-got-err)

i am trying to use requests.Session.get() to crawl comments from YouTube but i got error i don't know i am writing correct code or not.I think mainly problem is here: Also can anyone have an better example so i can understand request.Session.get() using scrapy.i can not understand why scrapy give that error.

2019-08-03 14:31:02Z

i am trying to use requests.Session.get() to crawl comments from YouTube but i got error i don't know i am writing correct code or not.I think mainly problem is here: Also can anyone have an better example so i can understand request.Session.get() using scrapy.i can not understand why scrapy give that error.This is what I have tried:And this is my whole output:

How to set input value with scrapy-splash?

Abdeldjallil Hadji

[How to set input value with scrapy-splash?](https://stackoverflow.com/questions/57301885/how-to-set-input-value-with-scrapy-splash)

I'm need to set input value in a form,the inputs remain empty after getting the response.I have tried to set assert(splash:wait(1)) to assert(splash:wait(10)) in order to wait the page loading but it didn't work :The inputs remain empty:

2019-08-01 04:34:17Z

I'm need to set input value in a form,the inputs remain empty after getting the response.I have tried to set assert(splash:wait(1)) to assert(splash:wait(10)) in order to wait the page loading but it didn't work :The inputs remain empty:

Why does scrapy crawl quotes result in a GET error?

user129916

[Why does scrapy crawl quotes result in a GET error?](https://stackoverflow.com/questions/57297408/why-does-scrapy-crawl-quotes-result-in-a-get-error)

I have generated the quotes spider in the tutorial, and I have added a yield option to parse. However, the spider is not working because it is having an issue downloading quotes.toscrape.com. 2019-07-31 12:04:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying http://www.quotes.toscrape.com/> (failed 1 times): []

2019-07-31 19:07:09Z

I have generated the quotes spider in the tutorial, and I have added a yield option to parse. However, the spider is not working because it is having an issue downloading quotes.toscrape.com. 2019-07-31 12:04:08 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying http://www.quotes.toscrape.com/> (failed 1 times): []This should actually yield the correct result. The error was a syntax error. 

Is there a way to extract data from calendar js(maybe)? python beautifulsoup selenium post calendar scraping

K evin

[Is there a way to extract data from calendar js(maybe)? python beautifulsoup selenium post calendar scraping](https://stackoverflow.com/questions/57263307/is-there-a-way-to-extract-data-from-calendar-jsmaybe-python-beautifulsoup-sel)

I want to extract datas from the calendar in this website.

https://www.dreamplus.asia/event/listIf I click the  tags which are evnets or the day of the events in the calendar. Detail information of tag pops up on the right side of the calendar. As you can see this website is made of js (probably) (if you see the detailed page source)Even though I've used selenium to click the tag which are days or the events, I couldn't find how to click those things. Any helps?I used find_elements_by_class_name to get 'fc-event-container' to get items But 'Controllers' is empty. probably because  it is js..

2019-07-30 01:46:23Z

I want to extract datas from the calendar in this website.

https://www.dreamplus.asia/event/listIf I click the  tags which are evnets or the day of the events in the calendar. Detail information of tag pops up on the right side of the calendar. As you can see this website is made of js (probably) (if you see the detailed page source)Even though I've used selenium to click the tag which are days or the events, I couldn't find how to click those things. Any helps?I used find_elements_by_class_name to get 'fc-event-container' to get items But 'Controllers' is empty. probably because  it is js..I observed that if you attempt to go direct to event you get re-directed to homepage. So, you can either go to home package and click through to events, or simply do two .gets in a row. Note: you want the child a tags within the containers for clicking to update sidebar info.Clicking through (slower):

Only scrape new items in Scrapy without a particular URL for each

lf_celine

[Only scrape new items in Scrapy without a particular URL for each](https://stackoverflow.com/questions/57267567/only-scrape-new-items-in-scrapy-without-a-particular-url-for-each)

I wrote a spider in Scrapy and I like to scrape only new items. I know DeltaFetch but I don't think it's possible in my case.

I have urls. For each url, I have items with different values. I'd like to upgrade only these items.My spider:I have a pipeline for images:I'd like to upgrade only the new items in results including images

Output sample:How can I do that ?

2019-07-30 08:39:26Z

I wrote a spider in Scrapy and I like to scrape only new items. I know DeltaFetch but I don't think it's possible in my case.

I have urls. For each url, I have items with different values. I'd like to upgrade only these items.My spider:I have a pipeline for images:I'd like to upgrade only the new items in results including images

Output sample:How can I do that ?

Cannot scrape data from second page with scrapy

Andrew

[Cannot scrape data from second page with scrapy](https://stackoverflow.com/questions/57260219/cannot-scrape-data-from-second-page-with-scrapy)

I am having problems to scrape more than one page of data. In splash console, I managed to get 2-3 pages of HTML content. When in the Lua script in the first loop I define to iterate one time to extract one page I get 50 urls. If 2 or more iterations, no data is being returned. 

In console I get: Ignoring response <504 https://shopee.sg/search?keyword=hdmi>: HTTP status code is not handled or not allowed

orHere is my code

2019-07-29 19:21:56Z

I am having problems to scrape more than one page of data. In splash console, I managed to get 2-3 pages of HTML content. When in the Lua script in the first loop I define to iterate one time to extract one page I get 50 urls. If 2 or more iterations, no data is being returned. 

In console I get: Ignoring response <504 https://shopee.sg/search?keyword=hdmi>: HTTP status code is not handled or not allowed

orHere is my codeI think that you get a timeout error because of your lua script. When you make a request from the spider, the time for receive response begins. In your lua script you have the following:

Run js twice for scrolling, its take some time

Twice call function splash:wait(5.0) for download and render some data

Then you call assert(splash:wait(8.0))Final minimum time:

(3 * 8) + (2 * 5) + time to run splash:runjs and some other thingBut in your case Splash isn't required. You can make a request for the next page directly from your spider. Chrome->Dev Tools->Network->XHR, there you will find request url https://shopee.sg/api/v2/search_items/?by=relevancy&keyword=hdmi&limit=50&newest=250&order=desc&page_type=searchThen you can use it to getting all the info that you need. In your case, it's URL to the product, but there is no direct url, you must slugify the name. For example [Spot is sold very well]Micro USB para HDMI Adaptador MHL para HDMI 1080 P to -Spot-is-sold-very-well-Micro-USB-para-HDMI-Adaptador-MHL-para-HDMI-1080-P-HD-TV- and add 2 ids: shopid, itemid as you can see there is a difference between names - but it works

Adding search parameterers to URL Python Scrapy

Karolis Pakalnis

[Adding search parameterers to URL Python Scrapy](https://stackoverflow.com/questions/57241773/adding-search-parameterers-to-url-python-scrapy)

I am trying to make spider to extract data from booking.com with Python Scrapy.The problem is that my parameters after question mark inside the url gets truncated.The URL I want to scrape is:But response URL I get is only:My question is how do I pass these checkin and checkout parameters, so I can get required data?I know that it's possible to use init function, but I could not make it work.My code is as follows and I refenced in the command which part disapears.EDIT:running the script I get this:Is there a way to disable the middleware?

2019-07-28 14:18:45Z

I am trying to make spider to extract data from booking.com with Python Scrapy.The problem is that my parameters after question mark inside the url gets truncated.The URL I want to scrape is:But response URL I get is only:My question is how do I pass these checkin and checkout parameters, so I can get required data?I know that it's possible to use init function, but I could not make it work.My code is as follows and I refenced in the command which part disapears.EDIT:running the script I get this:Is there a way to disable the middleware?Make the request like this for avoiding the redirect

Python Scrapy returns different url

Karolis Pakalnis

[Python Scrapy returns different url](https://stackoverflow.com/questions/57199480/python-scrapy-returns-different-url)

I am trying to scrape booking.com with scrapy. The problem occurs when I try to implement pagination. I'm trying to get URL to the next page, but scrapy retrieves me different URL (I get it through shell), which resulst in "page not found" when I try to paste into Chrome. And when I try to put it into JSON, it doesn't retrieve any URL for pagination. Anyone has any suggestions? Maybe I should shorten the first URL.I tried to set a canonicalize=False rule, but it didn't do anything.URL received through shell and which doesn't take me to next page against expected: 

2019-07-25 10:10:25Z

I am trying to scrape booking.com with scrapy. The problem occurs when I try to implement pagination. I'm trying to get URL to the next page, but scrapy retrieves me different URL (I get it through shell), which resulst in "page not found" when I try to paste into Chrome. And when I try to put it into JSON, it doesn't retrieve any URL for pagination. Anyone has any suggestions? Maybe I should shorten the first URL.I tried to set a canonicalize=False rule, but it didn't do anything.URL received through shell and which doesn't take me to next page against expected: I think this is because you need to specify that you want to use the href from the next button. Your code finds the button but it needs to get the link from the button in order to be used. Here is an example:You may have to fiddle around with the path but that is generally what is wrong. I had to change my user agent name to Chrome in settings. So the web page I am scraping would not block me. Joseph, thank you for your solution.

scrapy twisted.internet.error.ReactorNotRestartable

nusibrains

[scrapy twisted.internet.error.ReactorNotRestartable](https://stackoverflow.com/questions/57172662/scrapy-twisted-internet-error-reactornotrestartable)

Trying to execute a scrapy spider inside a python script.

It's exactly what is defined by the scrapy doc, but doesn't work.  errorunwrapped, spider works well

What's wrong?

2019-07-23 21:50:00Z

Trying to execute a scrapy spider inside a python script.

It's exactly what is defined by the scrapy doc, but doesn't work.  errorunwrapped, spider works well

What's wrong?

I have scrapy script, but I can not scrape data, don't knew why

Aleksandar Ciric

[I have scrapy script, but I can not scrape data, don't knew why](https://stackoverflow.com/questions/57176776/i-have-scrapy-script-but-i-can-not-scrape-data-dont-knew-why)

I run the script, but I got none, but there are data on the url I expect 52YR, but i got None

2019-07-24 06:44:06Z

I run the script, but I got none, but there are data on the url I expect 52YR, but i got NoneThe easiest way to go about this is probably to load the json in the script as a python dictionary and navigate through it to get to the codes.

The below code should get you started:

Getting 504 Gateway Time-out while running SplashRequest through ScrapySpider

Abeer

[Getting 504 Gateway Time-out while running SplashRequest through ScrapySpider](https://stackoverflow.com/questions/57178858/getting-504-gateway-time-out-while-running-splashrequest-through-scrapyspider)

Running ubuntu on VM VirtualBox.Running ifconfig command:Scrapy Terminal: Retrying GET http://www.gari.pk/used-cars-search/via

http://172.17.0.1:8050/execute (failed 1 times): 504 Gateway Time-outSplash Terminal:Tried this: docker run -p 8050:8050 scrapinghub/splash --max-timeout 240garispider.pyI expect body of the html after the lua script. But getting 

Retrying http://www.gari.pk/used-cars-search/ via http://172.17.0.1:8050/execute> (failed 1 times): 504 Gateway Time-out

2019-07-24 08:48:31Z

Running ubuntu on VM VirtualBox.Running ifconfig command:Scrapy Terminal: Retrying GET http://www.gari.pk/used-cars-search/via

http://172.17.0.1:8050/execute (failed 1 times): 504 Gateway Time-outSplash Terminal:Tried this: docker run -p 8050:8050 scrapinghub/splash --max-timeout 240garispider.pyI expect body of the html after the lua script. But getting 

Retrying http://www.gari.pk/used-cars-search/ via http://172.17.0.1:8050/execute> (failed 1 times): 504 Gateway Time-outHave no prepared project with Scrapy. I will show you an example with requests but it can be easily converted into Scrapy requestIn r.content you will have a response with data that you are looking for. The for paginations, the site makes the same request but add some offset, everything that you should do is to add this offset into data. Here is an exampleAs you can see it add 10 {'search_param': 'cars_mini/,/c_date desc/bmw/10'}

Maybe you can take more result per request.

I would recommend you check Developerconcole->network->Xhr

https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequestThis is because the url that you want to scrapy returns transfer-encoding header. I opened a issue about this on Github.Here the script to proof that (url httpbin.org/headers) returns the same headers i sent on request.

Why does `scrapy` stop working after parsing the first URL?

Zhao Yi

[Why does `scrapy` stop working after parsing the first URL?](https://stackoverflow.com/questions/57138478/why-does-scrapy-stop-working-after-parsing-the-first-url)

Below is my code in python to crawl a website with multiple pages. It starts to crawl the website https://www.reddit.com/r/movies/top.json?sort=top&limit=25/ then if there is a after field in the response, it will yield another request. But what is happening is that the scrapy stops working after the first request.The last few lines of the code is as below. It prints next page but before that you can see Spider closed (finished) in the log. It seems that the scrapy stops the second request. How can I make it work with multiple requests?

2019-07-22 02:21:53Z

Below is my code in python to crawl a website with multiple pages. It starts to crawl the website https://www.reddit.com/r/movies/top.json?sort=top&limit=25/ then if there is a after field in the response, it will yield another request. But what is happening is that the scrapy stops working after the first request.The last few lines of the code is as below. It prints next page but before that you can see Spider closed (finished) in the log. It seems that the scrapy stops the second request. How can I make it work with multiple requests?After some debugging and I found this is caused by a wrong value in allowed_domains field. It can be fixed by changing:to 

Checking to see if record exists in MongoDB before Scrapy inserts

7edubs7

[Checking to see if record exists in MongoDB before Scrapy inserts](https://stackoverflow.com/questions/57138349/checking-to-see-if-record-exists-in-mongodb-before-scrapy-inserts)

As the title implies, I'm running a Scrapy spider and storing results in MongoDB. Everything is running smoothly, except when I re-run the spider, it adds everything again, and I don't want the duplicates. My pipelines.py file looks like this:My spider looks like this:The current syntax is obviously wrong, but is there a slight fix to the for loop to make to fix it? Should I be running this loop in the spider instead? I was also looking at upsert but was having trouble understanding how to use that effectively. Any help would be great.

2019-07-22 01:54:31Z

As the title implies, I'm running a Scrapy spider and storing results in MongoDB. Everything is running smoothly, except when I re-run the spider, it adds everything again, and I don't want the duplicates. My pipelines.py file looks like this:My spider looks like this:The current syntax is obviously wrong, but is there a slight fix to the for loop to make to fix it? Should I be running this loop in the spider instead? I was also looking at upsert but was having trouble understanding how to use that effectively. Any help would be great.

How to run a `scrapy` project as a regular `python` app in order to run it from lambda?

Zhao Yi

[How to run a `scrapy` project as a regular `python` app in order to run it from lambda?](https://stackoverflow.com/questions/57138523/how-to-run-a-scrapy-project-as-a-regular-python-app-in-order-to-run-it-from)

I am creating a scrapy project and the structure looks like:I can run the app via scrapy command line scrapy crawl SPIDER_NAME but how can I package the app as a regular python program which can be run in AWS lambda?

From the command line scrapy crawl SPIDER_NAME, I don't know the entry point for the program. Lambda requries handler method as its entry point, so how can I trigger the scrapy task programmatically? 

2019-07-22 02:32:32Z

I am creating a scrapy project and the structure looks like:I can run the app via scrapy command line scrapy crawl SPIDER_NAME but how can I package the app as a regular python program which can be run in AWS lambda?

From the command line scrapy crawl SPIDER_NAME, I don't know the entry point for the program. Lambda requries handler method as its entry point, so how can I trigger the scrapy task programmatically? You should include scrapy in your Lambda package e.g.:If you will have multiple Lambdas using scrapy it is recommended to install it as a Lambda Layer to simplify deployment and maintenance. Make sure that scrapy and all the dependencies (especially binary) are available from your lambda package.In order to use scrapy as a Lambda one of the approaches is to implement scrapy.crawler.Crawler in your lambda_function and call the crawl() method from the lambda_handler. https://docs.scrapy.org/en/latest/topics/api.htmlPlease note that you may face Lambda execution time limits and you will probably need to chunk your data to multiple invocations. sosw package could be useful to simplify that.import scrapy in your AWS Python lambda function.

How to get same types of content from different website?

jason

[How to get same types of content from different website?](https://stackoverflow.com/questions/57139315/how-to-get-same-types-of-content-from-different-website)

We know that most of websites have sitemaps which contain all major categories of the site. Now I have a list of different sitemaps' urls(More than 100K) and I wish to extract a specific category's url from all different sitemaps that I have. For example, suppose I am visiting microsoft's sitemap and there's a place called news, so I can simply using xpath to get that url, but this is only for one site, what if I have huge numbers of sites and I want to extract all 'news' urls from them as long as they exist . My first thought would be training a model to recognize news. However, I am very new to machine learning, if this is the way to solve it, can someone explain to me how to approach this? What steps will need to be taken. Or if this is not the best way, any other suggestions? Thank you.

2019-07-22 04:46:07Z

We know that most of websites have sitemaps which contain all major categories of the site. Now I have a list of different sitemaps' urls(More than 100K) and I wish to extract a specific category's url from all different sitemaps that I have. For example, suppose I am visiting microsoft's sitemap and there's a place called news, so I can simply using xpath to get that url, but this is only for one site, what if I have huge numbers of sites and I want to extract all 'news' urls from them as long as they exist . My first thought would be training a model to recognize news. However, I am very new to machine learning, if this is the way to solve it, can someone explain to me how to approach this? What steps will need to be taken. Or if this is not the best way, any other suggestions? Thank you.If you are actually using news sites there is an application for this called newspaper3k. https://github.com/codelucas/newspaper/You can extract all news links with something like this.You can use xpath to make the above call a little better and ignore case if necessary.I imagine there are many other links and you want it to extract from all sitemaps. You can use CrawlSpider and linkextractor rules to crawl these sitemaps....See this answer Scrapy - Understanding CrawlSpider and LinkExtractor

I have one problem, when I put more than one url, data are overwritten

Aleksandar Ciric

[I have one problem, when I put more than one url, data are overwritten](https://stackoverflow.com/questions/57108258/i-have-one-problem-when-i-put-more-than-one-url-data-are-overwritten)

I made a simple scrapy script for scraping data from https://www.jobs2careers.com with items and exporting data to csv file. But I have one problem, when I put more than one url, data are overwritten.I tryed with some other python libraryes, like openpyxl. Maybe there is a problem with running multiple spiders

2019-07-19 08:14:40Z

I made a simple scrapy script for scraping data from https://www.jobs2careers.com with items and exporting data to csv file. But I have one problem, when I put more than one url, data are overwritten.I tryed with some other python libraryes, like openpyxl. Maybe there is a problem with running multiple spidersData are not being overwritten here.You are getting 1000 items total because you're using self.n to limit pagination.There are callbacks populated from each start_url, and they both increment the spider's self.n attribute asynchronously. The first url moves self.n from 1 to 2, then the second moves it from 2 to 3, then the first from 3 to 4, and so on. Because it's async, this isn't guaranteed to be the case exactly, but something like this is happening every time.As pwinz said, it's due to the logic used to go to the next page.

The below code builds the url of the next page based on the current page, and it stops if it reaches 1000 jobs or if the total number of jobs has been reached.

Scrapy Scraped from <303 https://m.youtube.com/view_comment?v=xHkL9PU7o9k> - Getting No Result

Rao Tauqeer Sajid

[Scrapy Scraped from <303 https://m.youtube.com/view_comment?v=xHkL9PU7o9k> - Getting No Result](https://stackoverflow.com/questions/57108789/scrapy-scraped-from-303-https-m-youtube-com-view-commentv-xhkl9pu7o9k-get)

I am trying to  Scrape Comments from " https://m.youtube.com/view_comment?v=xHkL9PU7o9k&client=mv-google&hl=en&gl=CA " but I am getting a response 303 after handling that response I am not getting any data.I've programmed my spider to not obey the robot.txt, disabled cookies, tried meta=dont_redirect.

2019-07-19 08:47:42Z

I am trying to  Scrape Comments from " https://m.youtube.com/view_comment?v=xHkL9PU7o9k&client=mv-google&hl=en&gl=CA " but I am getting a response 303 after handling that response I am not getting any data.I've programmed my spider to not obey the robot.txt, disabled cookies, tried meta=dont_redirect.

Parsing different pages with urlparse

user8496595

[Parsing different pages with urlparse](https://stackoverflow.com/questions/57065906/parsing-different-pages-with-urlparse)

I am trying to parse multiple pages of a website but I can't understand how to change the query of the url (if this makes sense?)I tried to create a next_page that took the first page and added +1 everytime it found the next page element, but I think I can't because I'll have multiple start urls (all similar). When i try to get the information of the next page element it returns this:Using url.parse(response.url).query I get:All I need to do is create a new link that uses the same scheme, path and then changes the query.If you need more info please tell me, I don't really know what is more relevant to you as I am still a beginner.

2019-07-16 21:50:22Z

I am trying to parse multiple pages of a website but I can't understand how to change the query of the url (if this makes sense?)I tried to create a next_page that took the first page and added +1 everytime it found the next page element, but I think I can't because I'll have multiple start urls (all similar). When i try to get the information of the next page element it returns this:Using url.parse(response.url).query I get:All I need to do is create a new link that uses the same scheme, path and then changes the query.If you need more info please tell me, I don't really know what is more relevant to you as I am still a beginner.You need to get the base url of that <a> element, which is the part of a url before query string starts https://example.com/a/path/?query=param so here the base url would be https://example.com/a/path/. Save that into a variable. Then use urllib.parse.parse_qsl to parse the query string, then update the page number and join it with base url.output:

Scrapy - Select item in form and extract the table that appears

C.Lee

[Scrapy - Select item in form and extract the table that appears](https://stackoverflow.com/questions/57079913/scrapy-select-item-in-form-and-extract-the-table-that-appears)

I am trying to extract information from a webpage where it requires me to select from a drop down list and based on the selection a table appears with various information. I have a list of selection values for the form/list on the page I would like to iterate through and extract the table information.Web Page: https://www.mcafee.com/enterprise/en-us/support/product-eol.htmlI am stuck on how to form the xpaths for extraction. The examples I researched all have classes that I can access but this does not. Also, this site requires me to click from a form/list before a table appears based on the selection, I am using the "FormRequest.from_response" method but I am not sure if I set this up the correct way. The information I want to extract is Product Name, Version Model, End of Support Notification, and End of Life/End of Support information. I would like to store the results in a dataframe first as I need to join information from other sources and then export into excel/csv.Expected Result for the first product in the list "Host Intrusion Prevention" from https://www.mcafee.com/enterprise/en-us/support/product-eol.html

2019-07-17 15:53:42Z

I am trying to extract information from a webpage where it requires me to select from a drop down list and based on the selection a table appears with various information. I have a list of selection values for the form/list on the page I would like to iterate through and extract the table information.Web Page: https://www.mcafee.com/enterprise/en-us/support/product-eol.htmlI am stuck on how to form the xpaths for extraction. The examples I researched all have classes that I can access but this does not. Also, this site requires me to click from a form/list before a table appears based on the selection, I am using the "FormRequest.from_response" method but I am not sure if I set this up the correct way. The information I want to extract is Product Name, Version Model, End of Support Notification, and End of Life/End of Support information. I would like to store the results in a dataframe first as I need to join information from other sources and then export into excel/csv.Expected Result for the first product in the list "Host Intrusion Prevention" from https://www.mcafee.com/enterprise/en-us/support/product-eol.htmlYou're searching in a wrong place. Above website doesn't send any FormRequest after you select anything in a list. Instead it loads everything from a https://www.mcafee.com/enterprise/admin/support/eol.xml and just show piece of data:

How to select option in webpage using scrapy

Newbie

[How to select option in webpage using scrapy](https://stackoverflow.com/questions/57072355/how-to-select-option-in-webpage-using-scrapy)

consider this websiteThere is a select option for year on the website:I want to select options from 2010-2020 and then scrape the info. How do I begin?

2019-07-17 09:10:10Z

consider this websiteThere is a select option for year on the website:I want to select options from 2010-2020 and then scrape the info. How do I begin?When you select an year, the page redirects to https://www.timeanddate.com/holidays/us/<year>. So you're better off just using that url in your scraper.

How to solve scrapy.downloadermiddlewares.redirect error in scrapy?

Mohamed Kamal

[How to solve scrapy.downloadermiddlewares.redirect error in scrapy?](https://stackoverflow.com/questions/57040747/how-to-solve-scrapy-downloadermiddlewares-redirect-error-in-scrapy)

I'm trying to crawl website and after some pages of successfully crawling I fail into scrapy.downloadermiddlewares.redirect debug (301) loop any one can help please ?This is my spider :and this is the error :

2019-07-15 13:24:40Z

I'm trying to crawl website and after some pages of successfully crawling I fail into scrapy.downloadermiddlewares.redirect debug (301) loop any one can help please ?This is my spider :and this is the error :

Error in download web page using fetch in python for scrapy

Suresh Resh

[Error in download web page using fetch in python for scrapy](https://stackoverflow.com/questions/57050517/error-in-download-web-page-using-fetch-in-python-for-scrapy)

Figure1

Figure2

 I am trying to code scrapy to crawl information from reddit webpage. As i use fetch("https://www.reddit.com/r/gameofthrones/new/") and try the inspect element to extract information using css selectors but the selector turn out to be alphabet mix with number as circled in Figure1. The css selector should appear as in the picture Figure 2.

2019-07-16 05:23:38Z

Figure1

Figure2

 I am trying to code scrapy to crawl information from reddit webpage. As i use fetch("https://www.reddit.com/r/gameofthrones/new/") and try the inspect element to extract information using css selectors but the selector turn out to be alphabet mix with number as circled in Figure1. The css selector should appear as in the picture Figure 2.

Scrapy only returning 400 http code errors

superdee

[Scrapy only returning 400 http code errors](https://stackoverflow.com/questions/57049738/scrapy-only-returning-400-http-code-errors)

I am running scrapy on many different websites. No matter what site I choose, scrapy returns one 200 code, and the rest 400, making my crawler useless. Any idea why?The downloader/response_status_count/400 is always 99.99% of the response. I've tried this on over 100 different sites, always the same issue.Please note, I am using proxies. Also, on multiple (separate) sites now, I have gotten this error:Why would I be getting an nginx error?

2019-07-16 03:47:53Z

I am running scrapy on many different websites. No matter what site I choose, scrapy returns one 200 code, and the rest 400, making my crawler useless. Any idea why?The downloader/response_status_count/400 is always 99.99% of the response. I've tried this on over 100 different sites, always the same issue.Please note, I am using proxies. Also, on multiple (separate) sites now, I have gotten this error:Why would I be getting an nginx error?

scrapy returns Ignoring non-200 response

Felix

[scrapy returns Ignoring non-200 response](https://stackoverflow.com/questions/57052866/scrapy-returns-ignoring-non-200-response)

When I crawl a website with scrapy I got this error messageBut when I call the website in browser I got 200 OKmy code looks like this:What could be the issue in this case?

2019-07-16 08:14:45Z

When I crawl a website with scrapy I got this error messageBut when I call the website in browser I got 200 OKmy code looks like this:What could be the issue in this case?Solution:Site was blocking scrapy. Adding a header solved the issue

How to get image src cascaded inside div

Newbie

[How to get image src cascaded inside div](https://stackoverflow.com/questions/57052394/how-to-get-image-src-cascaded-inside-div)

Here is my xpath:Here is the link to page where I am checking this from :

https://www.michaelkors.com/allie-mixed-media-trainer/_/R-US_43T9ALFS3LI have tried every combination but it is still returning None.

2019-07-16 07:45:31Z

Here is my xpath:Here is the link to page where I am checking this from :

https://www.michaelkors.com/allie-mixed-media-trainer/_/R-US_43T9ALFS3LI have tried every combination but it is still returning None.This website store all products details inside JSON structure (see window.__INITIAL_STATE__ in the source) that's why you can't find it in HTML code. Also there are separate images for each available SKU (usually they are all the same), in my solution I use first SKU:

caret and dollar sign doesn't work in scrapy s

user2196600

[caret and dollar sign doesn't work in scrapy s](https://stackoverflow.com/questions/57011588/caret-and-dollar-sign-doesnt-work-in-scrapy-s)

I am trying to use regular expression to extract data from a selector. But I find caret and dollar sign doesn't work as I expected. I was using .* to test the ^ and $ sign as below. I thought two lines below should return the same thing. But the first one just returns an empty list. And the second one returns the entire block as I expected.

2019-07-12 17:27:26Z

I am trying to use regular expression to extract data from a selector. But I find caret and dollar sign doesn't work as I expected. I was using .* to test the ^ and $ sign as below. I thought two lines below should return the same thing. But the first one just returns an empty list. And the second one returns the entire block as I expected..* is not including new lines and line breaks.^ Matches the beginning of the string, or the beginning of a line if the multiline flag is set.Same for $ - end of string or end of line in multiline flag is set.For better testing try ^[\s\S]*$ expression. This will include any symbols in between of string start and string end.

404 response in scrapy shell, different results in browser [duplicate]

frantic oreo

[404 response in scrapy shell, different results in browser [duplicate]](https://stackoverflow.com/questions/57015676/404-response-in-scrapy-shell-different-results-in-browser)

I am scraping the site oddsportal, just a simple query for the title text returns ['OddsPortal: Page not found'] however in the browser console this ['OddsPortal: Page not found'] does not appear. I have noticed when the shell loads the response is:In my terminalI would be expecting from the above selector:European Championship Results & Historical Odds, Darts Europe Archive

2019-07-13 02:17:02Z

I am scraping the site oddsportal, just a simple query for the title text returns ['OddsPortal: Page not found'] however in the browser console this ['OddsPortal: Page not found'] does not appear. I have noticed when the shell loads the response is:In my terminalI would be expecting from the above selector:European Championship Results & Historical Odds, Darts Europe ArchiveI also get this error when running my own request. As shown here this site does not allow scraping. My guess is they have some guards in place to prevent you from trying. I am having success using a non headless version with selenium. I would recommend doing your scraping that way. It also looks like most of the site is dynamic javascript, so thats another +1 for selenium. I am using Beautiful soup to parse in this example, and I highly recommend it. 

CSS selector equivalent of this one

audiotec

[CSS selector equivalent of this one](https://stackoverflow.com/questions/56980467/css-selector-equivalent-of-this-one)

I don't know how to put this class name in css selector because of the space. Could you help me please. Thank you!'//div[@class="posting-card super-highlighted "]' <==='div.posting-card super-highlighted' - doesn't work

'div."posting-card super-highlighted"' - neither

2019-07-11 01:17:02Z

I don't know how to put this class name in css selector because of the space. Could you help me please. Thank you!'//div[@class="posting-card super-highlighted "]' <==='div.posting-card super-highlighted' - doesn't work

'div."posting-card super-highlighted"' - neitherAlways remember to replace space with . between the classes in css.

Can scrapy download pdf completely

S.wen

[Can scrapy download pdf completely](https://stackoverflow.com/questions/56982280/can-scrapy-download-pdf-completely)

I want to download many pdf files from the pdf web links crawled by scrapy spiders, but when I use scrapy.Request(pdf_url) in filespipeline to crawl the pdf web, eventually it downloaded incomplete pdf files. all the pdf files was 1 KB Other than the first few files( which are complete). I had to use requests.get(pdf_url, stream=True) to download all the pdf files completely, but it is too slow. I want to know if scrapy filespipelines has similar method like this ?

2019-07-11 05:42:11Z

I want to download many pdf files from the pdf web links crawled by scrapy spiders, but when I use scrapy.Request(pdf_url) in filespipeline to crawl the pdf web, eventually it downloaded incomplete pdf files. all the pdf files was 1 KB Other than the first few files( which are complete). I had to use requests.get(pdf_url, stream=True) to download all the pdf files completely, but it is too slow. I want to know if scrapy filespipelines has similar method like this ?Not really sure if scrapy can do that job. You can use wget library to download it.

About rename downloaded images in scrapy

Orlando

[About rename downloaded images in scrapy](https://stackoverflow.com/questions/57015747/about-rename-downloaded-images-in-scrapy)

I am very new in scrapy so for me it is difficult to do very basic things in scrapy. My problem is that I can't rename my downloaded images. I copied part of my code from this website:"http://scrapingauthority.com/scrapy-download-images/" but it doesn't work. So my spider's code is this:The code of items:Code of Pipelines:My settings:

2019-07-13 02:42:32Z

I am very new in scrapy so for me it is difficult to do very basic things in scrapy. My problem is that I can't rename my downloaded images. I copied part of my code from this website:"http://scrapingauthority.com/scrapy-download-images/" but it doesn't work. So my spider's code is this:The code of items:Code of Pipelines:My settings:First you need to edit your settings.py:Next in your pipelines.py:and finally in your spider:You have to add your CustomImageNamePipeline instead of ImagesPipeline to settingsIf you have class in file pipelines.py then add to settings.pyor maybe with project nameIf you have all code in one file (without creating project) then add it the same file

Cannot import items.py in scrapy spider

user3479107

[Cannot import items.py in scrapy spider](https://stackoverflow.com/questions/56958639/cannot-import-items-py-in-scrapy-spider)

I cannot run my spider, using the shell command "scrapy crawl kbb," due to an error in finding my items module.My folder path follows the standard scrapy orientation.items.py:When running this via the shell command, "scrapy crawl kbb," I get the following error: "ModuleNotFoundError: No module named kbb"

2019-07-09 18:27:35Z

I cannot run my spider, using the shell command "scrapy crawl kbb," due to an error in finding my items module.My folder path follows the standard scrapy orientation.items.py:When running this via the shell command, "scrapy crawl kbb," I get the following error: "ModuleNotFoundError: No module named kbb"If your project use the standard scrapy folder structure, you can use this:See relative imports in PythonTry this one.

Scrapy loading not loading all info into postgres. Why?

Brandon

[Scrapy loading not loading all info into postgres. Why?](https://stackoverflow.com/questions/56961673/scrapy-loading-not-loading-all-info-into-postgres-why)

I'm trying to take the information I collected using scrapy and display it in postgres. Everything works except it only loads the first item in the first row in my data table. I think there's a loop that is needed in the def parse but I am not sure what to do. I've tried a few different loops.quotes_spider.pypipelines.pyI'm expecting to see my database have a list of all the information separated by rows. Not just the first info collected and one row.

2019-07-09 23:35:37Z

I'm trying to take the information I collected using scrapy and display it in postgres. Everything works except it only loads the first item in the first row in my data table. I think there's a loop that is needed in the def parse but I am not sure what to do. I've tried a few different loops.quotes_spider.pypipelines.pyI'm expecting to see my database have a list of all the information separated by rows. Not just the first info collected and one row.You put only one row because you put  item["title"][0], (etc.) but there can be also item["title"][1] and item["title"][2], etc.You can  use for-loop with zip() to group elements and insert every row separatellyor first create all rows and later use only one query with execute_values()

Not getting all the a elements from div class using xpath and Scrapy

saraherceg

[Not getting all the a elements from div class using xpath and Scrapy](https://stackoverflow.com/questions/56920784/not-getting-all-the-a-elements-from-div-class-using-xpath-and-scrapy)

I have been trying to get all the properties from this website.

When I access all of them on the main search page I can retrieve all the information from all the properties, however when I need the information from actual property link, it only seems to go through one property link.The main issue is in the link part, so when I actually try to access the link of the property. I only get the link and information from the first property but not from all the others.What I would like to get is all the other links to properties. I am not sure what I am doing wrong and how to fix this.Thank you very much for help!

2019-07-07 09:11:54Z

I have been trying to get all the properties from this website.

When I access all of them on the main search page I can retrieve all the information from all the properties, however when I need the information from actual property link, it only seems to go through one property link.The main issue is in the link part, so when I actually try to access the link of the property. I only get the link and information from the first property but not from all the others.What I would like to get is all the other links to properties. I am not sure what I am doing wrong and how to fix this.Thank you very much for help!You need couple of changes:

How to crawl dynamically generated data on google's webstore search results

Mohi_k

[How to crawl dynamically generated data on google's webstore search results](https://stackoverflow.com/questions/56921272/how-to-crawl-dynamically-generated-data-on-googles-webstore-search-results)

I want to crawl a web page which shows the results of a search in google's webstore and the link is static for that particular keyword. I want to find the ranking of an extension periodically.

Here is the URL Problem is that I can't render the dynamic data generated by Javascript code in response from server.I tried using Scrapy and Scrapy-Splash to render the desired page but I was still getting the same response. I used Docker to run an instance of scrapinghub/splash container on port 8050. I even visited the webpage http://localhost:8050 and entered my URL manually but it couldn't render the data although the message showed success.Here's the code I wrote for the crawler. It actually does nothing and its only job is to fetch the HTML contents of the desired page.and the contents of the settings.py of my Scrapy project:And for the result I always get nothing.Any help is appreciated.

2019-07-07 10:21:44Z

I want to crawl a web page which shows the results of a search in google's webstore and the link is static for that particular keyword. I want to find the ranking of an extension periodically.

Here is the URL Problem is that I can't render the dynamic data generated by Javascript code in response from server.I tried using Scrapy and Scrapy-Splash to render the desired page but I was still getting the same response. I used Docker to run an instance of scrapinghub/splash container on port 8050. I even visited the webpage http://localhost:8050 and entered my URL manually but it couldn't render the data although the message showed success.Here's the code I wrote for the crawler. It actually does nothing and its only job is to fetch the HTML contents of the desired page.and the contents of the settings.py of my Scrapy project:And for the result I always get nothing.Any help is appreciated.Works for me with a small custom lua script:You can then change your start_requests as follows:

Is it good to access spider attributes in scrapy pipeline?

Sam Min Wong

[Is it good to access spider attributes in scrapy pipeline?](https://stackoverflow.com/questions/56897368/is-it-good-to-access-spider-attributes-in-scrapy-pipeline)

In the scrapy pipeline doc saids one of the parameter of the function 'process_item' is the spiderprocess_item(self, item, spider)

Parameters: 

item (Item object or a dict) – the item scraped

spider (Spider object) – the spider which scraped the itemI want to send a list of one type of 'item' to the pipeline, but after many digging through the internet everyone is either yielding or returning the item to the pipeline once at a time. SamplerSpider.pypipeline.pyIs this a good way to do it? If not then why?Scraping information from a document will always result in more than one item. Why scrapy pipeline is designed to process the item once at a time?thanks in advance.

2019-07-05 06:08:24Z

In the scrapy pipeline doc saids one of the parameter of the function 'process_item' is the spiderprocess_item(self, item, spider)

Parameters: 

item (Item object or a dict) – the item scraped

spider (Spider object) – the spider which scraped the itemI want to send a list of one type of 'item' to the pipeline, but after many digging through the internet everyone is either yielding or returning the item to the pipeline once at a time. SamplerSpider.pypipeline.pyIs this a good way to do it? If not then why?Scraping information from a document will always result in more than one item. Why scrapy pipeline is designed to process the item once at a time?thanks in advance.

scrapy error: /usr/local/bin/scrapy: No such file or directory

Luis Ramon Ramirez Rodriguez

[scrapy error: /usr/local/bin/scrapy: No such file or directory](https://stackoverflow.com/questions/56897000/scrapy-error-usr-local-bin-scrapy-no-such-file-or-directory)

I just installed scrapy on a ubuntu instance, and I'm trying to trigger it from a bash comman like, I created a .sh file:But I get this error:/usr/local/bin/scrapy: No such file or directoryI tried this: But I'm still getting the same errorThe result forIs: But if I try:then it says that it is a directory

2019-07-05 05:27:44Z

I just installed scrapy on a ubuntu instance, and I'm trying to trigger it from a bash comman like, I created a .sh file:But I get this error:/usr/local/bin/scrapy: No such file or directoryI tried this: But I'm still getting the same errorThe result forIs: But if I try:then it says that it is a directory

Websites getting crawled but not scraped Scrapy

saraherceg

[Websites getting crawled but not scraped Scrapy](https://stackoverflow.com/questions/56868212/websites-getting-crawled-but-not-scraped-scrapy)

I have been scraping this website and trying to store properties and while some properties do get scraped some just get crawled and not scraped:Any suggestions how to make this work?

Thank you very much in advance

2019-07-03 10:36:54Z

I have been scraping this website and trying to store properties and while some properties do get scraped some just get crawled and not scraped:Any suggestions how to make this work?

Thank you very much in advanceThe way you have defined selectors are error prone. Moreover, there are few faulty selectors which are not working at all. The link to the next page is not working as well. It only goes to the page 1 and then quits. Lastly, I don't know any usage of next_sibling in css selector so I had to dig out that next sibling thing in some awkward manner.If you wanted to pursue a cleaner approach to get the three items, I think xpath is what you wanna stick to:I've kicked out two or three fields for brevity and I suppose you can manage them.

Scrapy: pagination with post request doesn't work

Tobi

[Scrapy: pagination with post request doesn't work](https://stackoverflow.com/questions/56898861/scrapy-pagination-with-post-request-doesnt-work)

I am trying to extract from this website: https://www.mrlodge.de/wohnungen/ 

The spider works without any errors but it doesn't do the pagination that I am passing with the payload. I only get back the same page. 

I have tried to use the json library to configure my payload, however the payload itself is not in json.

Please help.

2019-07-05 07:55:18Z

I am trying to extract from this website: https://www.mrlodge.de/wohnungen/ 

The spider works without any errors but it doesn't do the pagination that I am passing with the payload. I only get back the same page. 

I have tried to use the json library to configure my payload, however the payload itself is not in json.

Please help.The following approach worked for me.You'll have to set some of the values in the data dynamically of course - such as the mrl_ft[page] - to go through all pages.I see that the headers are wrong and incomplete. I see that the headers are the following:Content-Type is also different.

Xpath returning same result

Tom

[Xpath returning same result](https://stackoverflow.com/questions/56870944/xpath-returning-same-result)

Trying to use Xpath to scrape card name off the following website, https://www2.trollandtoad.com/buylist/?_ga=2.123753418.115346513.1562026676-1813285172.1559913561#!/M/10591, but it keeps returning the same result each time.  I need it to output all the card names from that link, but it just gives me the same one over and over again.I tried using getall() but it would not work correctly either.  It would give back all the card names but it would not be paired with the other data I scraped correctly. Instead of outputting one card name for one price and so on it would give me all the card names in one line along with the price for the first card and so on.

2019-07-03 13:11:09Z

Trying to use Xpath to scrape card name off the following website, https://www2.trollandtoad.com/buylist/?_ga=2.123753418.115346513.1562026676-1813285172.1559913561#!/M/10591, but it keeps returning the same result each time.  I need it to output all the card names from that link, but it just gives me the same one over and over again.I tried using getall() but it would not work correctly either.  It would give back all the card names but it would not be paired with the other data I scraped correctly. Instead of outputting one card name for one price and so on it would give me all the card names in one line along with the price for the first card and so on.You need relative XPath:UPDATE Fixed your XPath

grouping text based on tags in HTML text

Raady

[grouping text based on tags in HTML text](https://stackoverflow.com/questions/56872067/grouping-text-based-on-tags-in-html-text)

I have a text which is in format of (keeping tags and removing the text for understanding)I am trying to use scrapy to separate/group the text based on the header. So as a first step I need to get 3 groups of data from the above.I am getting an empty array. Any help appreciated. 

2019-07-03 14:12:52Z

I have a text which is in format of (keeping tags and removing the text for understanding)I am trying to use scrapy to separate/group the text based on the header. So as a first step I need to get 3 groups of data from the above.I am getting an empty array. Any help appreciated. I have this solution, made with scrapyWith which I get the headers and all the content inside them

Scrapy x path: only get first item in for loop

Tobi

[Scrapy x path: only get first item in for loop](https://stackoverflow.com/questions/56873444/scrapy-x-path-only-get-first-item-in-for-loop)

I am trying to get the details of every element of this page: https://www.mrlodge.de/wohnungen/I did this quite often with a for loop. However this time it only returns the first element. There has to be a problem in the loop because when I am using getall() instead of get(), I get all the details I need but not ordered.Please help

2019-07-03 15:25:34Z

I am trying to get the details of every element of this page: https://www.mrlodge.de/wohnungen/I did this quite often with a for loop. However this time it only returns the first element. There has to be a problem in the loop because when I am using getall() instead of get(), I get all the details I need but not ordered.Please helpTry usinginstead of to get the desired results.You need to change the first xpath query

How does Scrapy save crawl state?

Mukund Srinath

[How does Scrapy save crawl state?](https://stackoverflow.com/questions/56878360/how-does-scrapy-save-crawl-state)

I am able to save my crawl state and Scrapy successfully continues from where I cut it off. I have kept the start_urls constant each time I restart the spider i.e. the order and the list of start_urls fed each time that the spider is restarted is constant. But I need to do a random shuffle of my start_urls as I have URLs from different domains and well as in from same domain but as they are in order, the crawl delay is significantly slowing down my crawl speed. My list is 10s of millions and I have already crawled a million URLs. So I wouldn't want to jeopardize anything or restart the crawl.I have seen that requests.seen holds what looks like hashed values of the URLs that have been visited. And from Scrapy code I am certain that it's used to filter duplicates. But I am not sure what either spider.state or requests.queue does to help with saving state or restarting the crawl.

2019-07-03 21:52:14Z

I am able to save my crawl state and Scrapy successfully continues from where I cut it off. I have kept the start_urls constant each time I restart the spider i.e. the order and the list of start_urls fed each time that the spider is restarted is constant. But I need to do a random shuffle of my start_urls as I have URLs from different domains and well as in from same domain but as they are in order, the crawl delay is significantly slowing down my crawl speed. My list is 10s of millions and I have already crawled a million URLs. So I wouldn't want to jeopardize anything or restart the crawl.I have seen that requests.seen holds what looks like hashed values of the URLs that have been visited. And from Scrapy code I am certain that it's used to filter duplicates. But I am not sure what either spider.state or requests.queue does to help with saving state or restarting the crawl.You can write these requests to a txt file while seperating them when request is called by callback or errback. To reach statea of requests, just read these txt files.

Scrapy auto scheduled scraping data with fault-tolerant mechanism

Sam Min Wong

[Scrapy auto scheduled scraping data with fault-tolerant mechanism](https://stackoverflow.com/questions/56879206/scrapy-auto-scheduled-scraping-data-with-fault-tolerant-mechanism)

I am using scrapy to scraping some info from certain websites, and I am planning to schedule it to run once every 24 hours. The data is being stored in Dynamo DB. And I have a spring boot application sending request to aws for the data it scrapes and feed it to my react native front end. Now the problem is I am afraid of the spider will crawl something wrong in case of the website has changed slightly. I am looking for a mechanism which will notify me (like sending me an email) when the spider encounters a problem while scraping the website or stops itself from scraping and putting it into db and mess everything up. Since I am still learning web scraping and I am not sure if my idea is resonable or not of if there is already a mature solution/mechanism to solve similar problem.Thanks in advance.

2019-07-04 00:00:54Z

I am using scrapy to scraping some info from certain websites, and I am planning to schedule it to run once every 24 hours. The data is being stored in Dynamo DB. And I have a spring boot application sending request to aws for the data it scrapes and feed it to my react native front end. Now the problem is I am afraid of the spider will crawl something wrong in case of the website has changed slightly. I am looking for a mechanism which will notify me (like sending me an email) when the spider encounters a problem while scraping the website or stops itself from scraping and putting it into db and mess everything up. Since I am still learning web scraping and I am not sure if my idea is resonable or not of if there is already a mature solution/mechanism to solve similar problem.Thanks in advance.

How to minimize server load when parsing using scrapy?/How to ignore <body> and parse info only from <head>

Mark

[How to minimize server load when parsing using scrapy?/How to ignore <body> and parse info only from <head>](https://stackoverflow.com/questions/56825022/how-to-minimize-server-load-when-parsing-using-scrapy-how-to-ignore-body-and)

I collect statistics and all the information I need is in the <head> (script tag) of site.It have massive <body>(about 5-10 kb per page) so can I dont parse it for less server load?I would be glad if you recommend alternative optimizations to reduce the server loadCONCURRENT_REQUESTS = 32 DOWNLOAD_DELAY = 0.33 now speed 180/per min(sometimes 200)

2019-06-30 12:58:54Z

I collect statistics and all the information I need is in the <head> (script tag) of site.It have massive <body>(about 5-10 kb per page) so can I dont parse it for less server load?I would be glad if you recommend alternative optimizations to reduce the server loadCONCURRENT_REQUESTS = 32 DOWNLOAD_DELAY = 0.33 now speed 180/per min(sometimes 200)Scrapy operates with entire response body only. 

This behaviour coded in scrapy core.Scrapy don't have CONCURRENCY_REQUEST setting. Did you mean CONCURRENT_REQUESTS ?If you didn't specify RANDOMIZE_DOWNLOAD_DELAY as False (default valueTrue).

download delay will be a random number between 0.5x to 1.5x of DOWNLOAD_DELAY setting.

scrapy + splash response incomplete page

freedy

[scrapy + splash response incomplete page](https://stackoverflow.com/questions/56828059/scrapy-splash-response-incomplete-page)

I have scraped this url, https://www.disco.com.ar/prod/409496/at%C3%BAn-en-aceite-jumbo-120-gr for months in order to get the price for example. But last week I couldn't. I don't understand what change. Because the response only return the icon but not the HTML.I use scrapy + splash.Here the example of the response in splashI changed the setting in the scrapy, also the LUA in splash but nothing working.

2019-06-30 21:01:55Z

I have scraped this url, https://www.disco.com.ar/prod/409496/at%C3%BAn-en-aceite-jumbo-120-gr for months in order to get the price for example. But last week I couldn't. I don't understand what change. Because the response only return the icon but not the HTML.I use scrapy + splash.Here the example of the response in splashI changed the setting in the scrapy, also the LUA in splash but nothing working.

How to skip over child element with Scrapy

7edubs7

[How to skip over child element with Scrapy](https://stackoverflow.com/questions/56829636/how-to-skip-over-child-element-with-scrapy)

I'm looking to scrape just the job description from this page: https://www.aha.io/company/careers/current-openings/customer_success_specialist_project_management_usI'd like to get all of the text and HTML inside the div with the class of "container py2 content job", EXCEPT the button. It's in an <a> tag with the class of "btn btn-large btn-secondary".I've got two different xpath selectors that I thought should work, but don't. The first doesn't exclude the button and the second gets rid of all of the other HTML, which I'd like to keep.Neither is scraping all of the HTML in the div minus what's inside the a tag. I'm hoping there's something simple that I'm missing, but I can't find what I'm looking for in the documentation.

2019-07-01 03:13:26Z

I'm looking to scrape just the job description from this page: https://www.aha.io/company/careers/current-openings/customer_success_specialist_project_management_usI'd like to get all of the text and HTML inside the div with the class of "container py2 content job", EXCEPT the button. It's in an <a> tag with the class of "btn btn-large btn-secondary".I've got two different xpath selectors that I thought should work, but don't. The first doesn't exclude the button and the second gets rid of all of the other HTML, which I'd like to keep.Neither is scraping all of the HTML in the div minus what's inside the a tag. I'm hoping there's something simple that I'm missing, but I can't find what I'm looking for in the documentation.

how can be scrap and parse a page after post and click?

msklc

[how can be scrap and parse a page after post and click?](https://stackoverflow.com/questions/56786655/how-can-be-scrap-and-parse-a-page-after-post-and-click)

I want to write a small code in python to check my parcels daily. While making a query in the web page (https://www.internationalparceltracking.com/#/search) URL doesn't change, so it is impossible to send the query by URL. I tried to use beautifulsoup and urllib but couldn't get the return.Here, what I tried:Lastly, I don't want to use selenium (like opening an extra browser) I want, the query is made in underground... 

I am waiting for your advice

2019-06-27 08:08:16Z

I want to write a small code in python to check my parcels daily. While making a query in the web page (https://www.internationalparceltracking.com/#/search) URL doesn't change, so it is impossible to send the query by URL. I tried to use beautifulsoup and urllib but couldn't get the return.Here, what I tried:Lastly, I don't want to use selenium (like opening an extra browser) I want, the query is made in underground... 

I am waiting for your adviceI went to the url you posted. I filled out some dummy data in the form, opened up the Chrome Developer Console -> Network Tab, and was able to spot an outgoing GET request:Given that, I think the following code below will do (no need to scrape):If you look at web inspector of your browser (Ctrl+shift+i on chromium for example) You can see that the webpage is making a simple ajax request to page like this:https://www.internationalparceltracking.com/api/shipment?barcode=3SABC1234567890&checkIfValid=true&country=BS&language=enNow just replace keyword argument values with those of your own and voila!What @FelizNaveedad provided is excellent. Just to put it more clearly with this pictureYou can follow this path Chrome Developer Console -> Network Tab -> XHR. and post information by hand on the webpage, and then you will see some files revealed on XHR. What you are looking for are files starting with shipment.... Click that file and you will find Query String Parameters. That is what you post to the browser and copy them into your code as parameters.If you would like to know it more specifically, you can watch this video, I find it helpful.

scraper got killed automatically on server after few hours, while other spiders keep running on same server

Nauman Sharif

[scraper got killed automatically on server after few hours, while other spiders keep running on same server](https://stackoverflow.com/questions/56792787/scraper-got-killed-automatically-on-server-after-few-hours-while-other-spiders)

I have many scrapers running on  server, one of them got killed after few hours I couldn't understand the reason. while other scrapers keep running at that time.I have tried options to decrease concurrent requests, but it didn't workhere are my logs

2019-06-27 14:02:50Z

I have many scrapers running on  server, one of them got killed after few hours I couldn't understand the reason. while other scrapers keep running at that time.I have tried options to decrease concurrent requests, but it didn't workhere are my logs

How to simulate mouse click in Scrapy-Splash

Tom

[How to simulate mouse click in Scrapy-Splash](https://stackoverflow.com/questions/56799131/how-to-simulate-mouse-click-in-scrapy-splash)

I am scraping a webpage,http://www.starcitygames.com/buylist/, and I need to click a button in order to access some data and so I am trying to simulate a mouse click but I am confused about exactly how to do that. I have had suggestions to just scrape the JSON instead because it would be a lot easier but I really do not want to scrape it.  I would rather scrape the regular website.  Here is what I have so far, I do not know exactly what to do to get it to click that display button, but this was my best try so far. HTML Code

2019-06-27 22:31:08Z

I am scraping a webpage,http://www.starcitygames.com/buylist/, and I need to click a button in order to access some data and so I am trying to simulate a mouse click but I am confused about exactly how to do that. I have had suggestions to just scrape the JSON instead because it would be a lot easier but I really do not want to scrape it.  I would rather scrape the regular website.  Here is what I have so far, I do not know exactly what to do to get it to click that display button, but this was my best try so far. HTML CodeSplash is a light weight option for rendering JS. If you have extensive clicking and navigation to do in menus that can't be reverse engineered then you probably don't want Splash unless you don't mind trying to write a LUA script. You may want to see this answer in regards to that. You will write a LUA script and pass it to the execute Splash endpoint. Depending how complex your task Selenium may be a better choice for your project. However, first thoroughly examine the target site and be SURE that you need to render JavaScript as rendering the JS is always the worst thing you can do if you don't have to for speed and resources.PS: We can't access this site without the login credentials. I would suspect that you don't need to render the JavaScript. That is the case 90%+ of the time. 

Scrapy - xpath with a regular expression

Hana Iku

[Scrapy - xpath with a regular expression](https://stackoverflow.com/questions/56764508/scrapy-xpath-with-a-regular-expression)

I scrapping a page with some divs with the id = Content_Main_ some random number but can't get their content using the following xpath because the result is always empty, what I'm doing wrong?//div[re:test(@id, '([Content_Main_]+\d{5}[0-9])')]

2019-06-26 02:50:54Z

I scrapping a page with some divs with the id = Content_Main_ some random number but can't get their content using the following xpath because the result is always empty, what I'm doing wrong?//div[re:test(@id, '([Content_Main_]+\d{5}[0-9])')]I think you need something like (starting with Content_Main_ and next a digit):UPDATE

To select divs ending with a number you need:You are searching exactly five digts after text(Content_Main_)

\d{5} - Search 5 digits example : 12345; 76543....make it like:

\d -- if it is always one digit

or

\d+ - if it can have one or more digit

or

\d* - if it may/may not have digit

How to save 'data in CSV format using scrapy crawl -o items.csv'

Usama Tahir

[How to save 'data in CSV format using scrapy crawl -o items.csv'](https://stackoverflow.com/questions/56764251/how-to-save-data-in-csv-format-using-scrapy-crawl-o-items-csv)

I am extracting data for a testing purpose from a website indeed.com.

Information i need i am able to scrape it e.g. title, location, date, description etc on scrapy shell but when i made a python file and try to save it .csv file its not working for me .csv file is empty.I am attaching the code of that file and the scrapy crawl -o items.csv result from cmd.Here is my Code

2019-06-26 02:14:47Z

I am extracting data for a testing purpose from a website indeed.com.

Information i need i am able to scrape it e.g. title, location, date, description etc on scrapy shell but when i made a python file and try to save it .csv file its not working for me .csv file is empty.I am attaching the code of that file and the scrapy crawl -o items.csv result from cmd.Here is my Code

How to scrape <td> from <tr> which has varying <td> in a table?

Kumar

[How to scrape <td> from <tr> which has varying <td> in a table?](https://stackoverflow.com/questions/56766274/how-to-scrape-td-from-tr-which-has-varying-td-in-a-table)

I am using Python Scrapy to scrape some data from a web site. The site has a number of tables. For example, it has 50 states and each state has tables between 3 to 5, and I scrape table 3 alone.and table 3 has rows between 3 to 10.Indices used to find if table 3 exists, if not it won't get appended to rows To get the <td> value I remove all the unwanted data from the rows list.This is not getting me the output in a correct format, and this method is not working efficiently.How can I achieve this?

2019-06-26 06:12:07Z

I am using Python Scrapy to scrape some data from a web site. The site has a number of tables. For example, it has 50 states and each state has tables between 3 to 5, and I scrape table 3 alone.and table 3 has rows between 3 to 10.Indices used to find if table 3 exists, if not it won't get appended to rows To get the <td> value I remove all the unwanted data from the rows list.This is not getting me the output in a correct format, and this method is not working efficiently.How can I achieve this?

Scrapy extracts the data in any order

emanuel lemos

[Scrapy extracts the data in any order](https://stackoverflow.com/questions/56765810/scrapy-extracts-the-data-in-any-order)

someone could tell me why scrapy extracts the data in any order. I'm trying to scrapy on this page. The idea is to extract all the data houses from top to bottom, then go to the next page and do the same. (until page 20)I do not know why I take some houses only and in any order. Thank you

2019-06-26 05:29:47Z

someone could tell me why scrapy extracts the data in any order. I'm trying to scrapy on this page. The idea is to extract all the data houses from top to bottom, then go to the next page and do the same. (until page 20)I do not know why I take some houses only and in any order. Thank youI understand that you are trying to search for items by html code.If I had to guess, I would say it's doing it in order of the items it finds in the html code.It is possible that some items have the html code that you are searching for, while other items are labelled differently.I would suggest first checking the html code of the missing items. If this does not work, I suggest using Selenium.

Syntaxe Error with variable on Databse (MySQL / Python)

jozefPython

[Syntaxe Error with variable on Databse (MySQL / Python)](https://stackoverflow.com/questions/56739698/syntaxe-error-with-variable-on-databse-mysql-python)

at first I declared a variable "nom_of_game_db", but after that I can't find the correct syntax at the level of the MySQL to call the variable back:I use Python and MySQL

2019-06-24 15:24:16Z

at first I declared a variable "nom_of_game_db", but after that I can't find the correct syntax at the level of the MySQL to call the variable back:I use Python and MySQLAfter doing the execute to create table statement use self.conn.commit() to commit the db changes.and also rename the drop table exist sql command.

Scrapy X Path: can't concatenate expressions with “and”

Tobi

[Scrapy X Path: can't concatenate expressions with “and”](https://stackoverflow.com/questions/56751036/scrapy-x-path-cant-concatenate-expressions-with-and)

I am trying to extract all p nodes whose ancestor contains the word "content" in their id.I have tried this:Individually the tags work but I can't concatenate them.

Please helpThe website is this one: https://www.accenture.com/us-en/success-airbus-wearable-technology

2019-06-25 09:39:05Z

I am trying to extract all p nodes whose ancestor contains the word "content" in their id.I have tried this:Individually the tags work but I can't concatenate them.

Please helpThe website is this one: https://www.accenture.com/us-en/success-airbus-wearable-technologyThis does the job:

Why is xpath's extract() returning an empty list for the href attribute of an anchor element?

ahmedelsahooly

[Why is xpath's extract() returning an empty list for the href attribute of an anchor element?](https://stackoverflow.com/questions/56743922/why-is-xpaths-extract-returning-an-empty-list-for-the-href-attribute-of-an-an)

Why do I get an empty list when trying to extract the href attribute of the anchor tag located on the following url: https://www.udemy.com/courses/search/?src=ukw&q=accounting using scrapy?This is my code to extract the <a></a> element located inside the list-view-course-card--course-card-wrapper--TJ6ET class:

2019-06-24 21:04:15Z

Why do I get an empty list when trying to extract the href attribute of the anchor tag located on the following url: https://www.udemy.com/courses/search/?src=ukw&q=accounting using scrapy?This is my code to extract the <a></a> element located inside the list-view-course-card--course-card-wrapper--TJ6ET class:This site makes API calls to retrieve all the data. 

You can use the scrapy shell to see the response that the site is returning.

scrapy shell 'https://www.udemy.com/courses/search/?src=ukw&q=accounting' and then view(response). The data you are looking for is available at the following api call :

'https://www.udemy.com/api-2.0/search-courses/?fields[locale]=simple_english_title&src=ukw&q=accounting' . However, if you try to access this link directly, you will get a json object saying that you do not have permission to perform this action. How did I find this link ? Load the url on your browser, and go to the network tab on your developer tools and look for XHR objects. The following spider will first make a request to the primary link and then make a request to the api call. 

You will have to parse the json object that was returned to obtain your data. If you want to scale this spider for more products, you might want to look for a pattern in the structure of the api call. 

How to fix 403 response in Scrapy

palash babu

[How to fix 403 response in Scrapy](https://stackoverflow.com/questions/56714076/how-to-fix-403-response-in-scrapy)

http://prntscr.com/o56670Please check the screenshotI am using python 3 and using scrapy in my terminal. fetch("https://angel.co/adil-wali")When the link is requested, it responses with 403. so i have changed and rotated user agent and robots obey false but still showing 403 response so this time i buy crawlera plan but crawlera still saying 523 responseDo you have any idea about why the request returns 403 instead of 200 response in scrapy shell

2019-06-22 08:57:57Z

http://prntscr.com/o56670Please check the screenshotI am using python 3 and using scrapy in my terminal. fetch("https://angel.co/adil-wali")When the link is requested, it responses with 403. so i have changed and rotated user agent and robots obey false but still showing 403 response so this time i buy crawlera plan but crawlera still saying 523 responseDo you have any idea about why the request returns 403 instead of 200 response in scrapy shellTry adding headers to your request:With this approach, I was able to get Response 200 from the mentioned URL.

multiple urls using a for loop in scrapy

Pedro Barata Fernandes

[multiple urls using a for loop in scrapy](https://stackoverflow.com/questions/56718672/multiple-urls-using-a-for-loop-in-scrapy)

i want to scrape information from multiple urls. I use the following code but it doesn't work. May someone please points me to where I have gone wrong? 

2019-06-22 19:51:56Z

i want to scrape information from multiple urls. I use the following code but it doesn't work. May someone please points me to where I have gone wrong? You have a typo in method's name: it should be start_requests instead of start_request.

Scrapy program is not scraping all data

Tim

[Scrapy program is not scraping all data](https://stackoverflow.com/questions/56709737/scrapy-program-is-not-scraping-all-data)

I am writing a program in scrapy to scrape following page, https://www.trollandtoad.com/magic-the-gathering/aether-revolt/10066, and it is only scraping the first line of data and not the rest.  I think it has something to do with my for loop but when I change the loop to be broader it outputs too much data, as in it output each line of data multiple times.  UPDATE #1

2019-06-21 20:00:54Z

I am writing a program in scrapy to scrape following page, https://www.trollandtoad.com/magic-the-gathering/aether-revolt/10066, and it is only scraping the first line of data and not the rest.  I think it has something to do with my for loop but when I change the loop to be broader it outputs too much data, as in it output each line of data multiple times.  UPDATE #1I think you need below CSS (later you can use it as a base to process buying-options container):As you can see I moved item = GameItem() inside a loop. Also there is no need in saved_game here.response.css("div.row.mt-1.list-view") returns only 1 selector, so the code in your loop runs only once. Try this: for game in response.css(".mt-1.list-view .card-text"): and you will get a list of selectors to loop over.You're code -- it does not work because you are creating the GameItem() outside of your list loop. I must have missed a postcard about this .get() and .getall() methods. Maybe someone can comment how its different from extract?Your failing codeFixed code to solve your problem:

Targeting id's with scrapy css selector

runningmanjoe

[Targeting id's with scrapy css selector](https://stackoverflow.com/questions/56692345/targeting-ids-with-scrapy-css-selector)

--stuff happens here--*how do I search for divs without a class? No matter what suggestion I find online I keep getting the error "expected something got delim..."

2019-06-20 19:01:31Z

--stuff happens here--*how do I search for divs without a class? No matter what suggestion I find online I keep getting the error "expected something got delim..."You need to use # for an id attribute:Or you can select it by attribute notation:You could try something like this. The logic is it will find None and fail the first if check if there is no class and then append it to the no_class_divs list. If you want to find based on ID then accept @gangabass answer. If you want to find div elements that have no class attribute then my answer should work.If you want to select the div with a certain id, you can do what the other answers suggest.However, if you want to select any div without a class attribute, use:

route results from yield to a file

JamesMatson

[route results from yield to a file](https://stackoverflow.com/questions/56695893/route-results-from-yield-to-a-file)

I have the following Python script using Scrapy: My solution will run multiple instances of this script, each scraping a different subset of information from different 'categories'. I know you can run scrapy from the command line to output to a json file, but i want do to the output to a file from within the function, so each instance writes to a different file. Being a beginner with Python, I'm not sure where to go with my script. I need to get the output of the yield into a file while the script is executing. How do i achieve this? There will be hundreds of rows scraped, and I'm not familiar enough with how yield works to understand how to 'return' from it a set of data (or a list) that can then be written to the file. 

2019-06-21 01:48:56Z

I have the following Python script using Scrapy: My solution will run multiple instances of this script, each scraping a different subset of information from different 'categories'. I know you can run scrapy from the command line to output to a json file, but i want do to the output to a file from within the function, so each instance writes to a different file. Being a beginner with Python, I'm not sure where to go with my script. I need to get the output of the yield into a file while the script is executing. How do i achieve this? There will be hundreds of rows scraped, and I'm not familiar enough with how yield works to understand how to 'return' from it a set of data (or a list) that can then be written to the file. You are looking to append a file. But being file writing an I/O operation, you need to lock the file from being written by other processes while a process is writing. Easiest way to achieve is to write in different random files (files with random names) in a directory and concatenating them all using another process. First let me suggest you some changes to your code. If you want to remove duplicates i you could use a set like this:note that i'm also changing the [ to ( to make a generator instead of a list and save some memory. Search more about generators: https://www.python-course.eu/python3_generators.phpOK then the solution for your problem is using an Item Pipeline (https://docs.scrapy.org/en/latest/topics/item-pipeline.html), what this does perfom some action on every item yielded from your function parse_subcategories. What you do is add a class in your pipelines.py file and enable this pipeline in settings.py. This is:and then you start your crawl with this command: scrapy crawl site -a start_url=https://www.site.com.au/category-name and you could optionally add -a filename=somename

scrapy sitemap spider does not provide expected results

Billy Jhon

[scrapy sitemap spider does not provide expected results](https://stackoverflow.com/questions/56666697/scrapy-sitemap-spider-does-not-provide-expected-results)

I have this nested sitemap. Scrapy docs say it is supposed to work with nested sitemaps without any problem.

My target links are like this one https://flatinfo.ru/arenda_kvartir.asp?id=867039



So in my understanding my sitemap_rules , conaining the keyword from the link ('/arenda_kvartir/') should make the spider behave in the following logic:

all links, found in the sitemap.xml and containing keyword from sitemap_rules should be scraped into parsed function. But this never happens according to the logs. The spider just goes through all major categories in the sitemap and quits. 

Where am I wrong?

Below is my code.

2019-06-19 11:31:06Z

I have this nested sitemap. Scrapy docs say it is supposed to work with nested sitemaps without any problem.

My target links are like this one https://flatinfo.ru/arenda_kvartir.asp?id=867039



So in my understanding my sitemap_rules , conaining the keyword from the link ('/arenda_kvartir/') should make the spider behave in the following logic:

all links, found in the sitemap.xml and containing keyword from sitemap_rules should be scraped into parsed function. But this never happens according to the logs. The spider just goes through all major categories in the sitemap and quits. 

Where am I wrong?

Below is my code.You need to match two type of URLs in your sitemap_rules: https://flatinfo.ru/arenda.asp?house=43182 and https://flatinfo.ru/prodaja_kvartir.asp?id=17488515. The correct sitemap_rules will be:(I added sitemap_follow to skip other sitemap entries).One more thing: you need to wait (almost 20 minutes till the first processed URL for me!) because each XML file takes a lot of time to process.

Scrapy - Shell crawls the page without any problem, but selectors fail

gunesevitan

[Scrapy - Shell crawls the page without any problem, but selectors fail](https://stackoverflow.com/questions/56666048/scrapy-shell-crawls-the-page-without-any-problem-but-selectors-fail)

I have this start url for crawling.https://autocarro.com.br/auto-busca/carros?AutoBusca=1&qc=&qt=&q=&ai=&af=&pi=&pf=&com=&cam=&cor=&por=&est=&cid=#1When I send a request from scrapy shell, it is crawled without any problems. I can see the full page is rendered when I use view(response). This is the HTML code and the rendered website.However, when I try to use selectors to get a tags, they don't work. It's like the whole HTML table body is not there.response.css('tbody').getall() gets an empty table body or the a tags I'm looking for are not there.I also checked the whether there is an AJAX request which I'm missing, but there is not. What's the problem here?

2019-06-19 10:56:25Z

I have this start url for crawling.https://autocarro.com.br/auto-busca/carros?AutoBusca=1&qc=&qt=&q=&ai=&af=&pi=&pf=&com=&cam=&cor=&por=&est=&cid=#1When I send a request from scrapy shell, it is crawled without any problems. I can see the full page is rendered when I use view(response). This is the HTML code and the rendered website.However, when I try to use selectors to get a tags, they don't work. It's like the whole HTML table body is not there.response.css('tbody').getall() gets an empty table body or the a tags I'm looking for are not there.I also checked the whether there is an AJAX request which I'm missing, but there is not. What's the problem here?You need to check source HTML code (usually Ctrl+U in a browser) for the source data. For your URL you'll find that target table is loaded from JavaScript code starting with var COLLECTION = [. You can parse that part with below code:

I am trying to extract some data from espn as a table and getting it as list

vishnuvid

[I am trying to extract some data from espn as a table and getting it as list](https://stackoverflow.com/questions/56608807/i-am-trying-to-extract-some-data-from-espn-as-a-table-and-getting-it-as-list)

start_urls = http://www.espncricinfo.com/series/18679/scorecard/1144998/australia-vs-india-2nd-odi-india-in-aus-2018-19I scraped this site and extracted the match result(winning team) and then I yielded the player URL and I want to print the player name and the batting style. My first problem is 

1. I cant abstract the player batting sytle. it is under <pclass="ciPlayerinformationtxt"><b>Batting style</b> <span>Right-hand bat</span>. I was only able to extract the text 'Batting style'.How to extract 'Right-hand bat'

2.I was unable to yield the whole extracted data as a table. The result I got was like p

link of all the player

http://www.espncricinfo.com/ci/content/player/326434.htmlPlayer_name Country 

Alex Carey  Australia

Kuldeep Yadav   India

Mohammed Siraj  India

Winning_Team:Indiawhat I want is the extracted data as a single table and I wanted to avoid repetition.Thanks in advance

2019-06-15 08:38:36Z

start_urls = http://www.espncricinfo.com/series/18679/scorecard/1144998/australia-vs-india-2nd-odi-india-in-aus-2018-19I scraped this site and extracted the match result(winning team) and then I yielded the player URL and I want to print the player name and the batting style. My first problem is 

1. I cant abstract the player batting sytle. it is under <pclass="ciPlayerinformationtxt"><b>Batting style</b> <span>Right-hand bat</span>. I was only able to extract the text 'Batting style'.How to extract 'Right-hand bat'

2.I was unable to yield the whole extracted data as a table. The result I got was like p

link of all the player

http://www.espncricinfo.com/ci/content/player/326434.htmlPlayer_name Country 

Alex Carey  Australia

Kuldeep Yadav   India

Mohammed Siraj  India

Winning_Team:Indiawhat I want is the extracted data as a single table and I wanted to avoid repetition.Thanks in advanceYou need to adjust your XPath:    UPDATEand later:

Scrapy multiple filed insertions across multiple requests

Josmoor98

[Scrapy multiple filed insertions across multiple requests](https://stackoverflow.com/questions/56666135/scrapy-multiple-filed-insertions-across-multiple-requests)

Using Scrapy, I'm struggling with how to achieve a specific output. I'm attempting to scrape financial documents from the Securities and Exchange Commission. The task is summarised below.Using this procedure, the format I'm attempting to achieve is shown below. If scraping an individual page, achieving multiple field insertions like this hasn't been a problem, but when scraping an arbitrary number of items across multiple pages, I'm encountering issues. I'm currently getting the following output (shortened for convenience but it demonstrates the issue). How should I go about appending the documents to a single item instead of creating multiple items for the same company? I'm not fully sure how to go about dealing with multiple insertions into the same field.The code used to generate this output is given below. For testing purposes only, I'm just adding the response.url value to documents to simplify the output.The items.py file is as follows.UpdateI managed to get a piece of code that achieves the desired output. The spider.py file is as follows.I would be curious to know how this code could be improved and if ItemLoader() can be implemented to perform this task.

2019-06-19 11:01:24Z

Using Scrapy, I'm struggling with how to achieve a specific output. I'm attempting to scrape financial documents from the Securities and Exchange Commission. The task is summarised below.Using this procedure, the format I'm attempting to achieve is shown below. If scraping an individual page, achieving multiple field insertions like this hasn't been a problem, but when scraping an arbitrary number of items across multiple pages, I'm encountering issues. I'm currently getting the following output (shortened for convenience but it demonstrates the issue). How should I go about appending the documents to a single item instead of creating multiple items for the same company? I'm not fully sure how to go about dealing with multiple insertions into the same field.The code used to generate this output is given below. For testing purposes only, I'm just adding the response.url value to documents to simplify the output.The items.py file is as follows.UpdateI managed to get a piece of code that achieves the desired output. The spider.py file is as follows.I would be curious to know how this code could be improved and if ItemLoader() can be implemented to perform this task.

No module named 'scrapy.contrib' when deploying to Scrapy Cloud

lf_celine

[No module named 'scrapy.contrib' when deploying to Scrapy Cloud](https://stackoverflow.com/questions/56667778/no-module-named-scrapy-contrib-when-deploying-to-scrapy-cloud)

I had developped a spider in anaconda3 and I try to deploy it in Scrapy Cloud. But I have an error when I start the scraping.What's the problem ? (I use Scrapy 1.6. I disabled the User agent middleware cause I read that scrapy.contrib used it. But it doesn't change antything.

2019-06-19 12:35:12Z

I had developped a spider in anaconda3 and I try to deploy it in Scrapy Cloud. But I have an error when I start the scraping.What's the problem ? (I use Scrapy 1.6. I disabled the User agent middleware cause I read that scrapy.contrib used it. But it doesn't change antything.

Why Does My Code Return Blanks? (scraping with Scrapy)

Jun Ahn

[Why Does My Code Return Blanks? (scraping with Scrapy)](https://stackoverflow.com/questions/56552505/why-does-my-code-return-blanks-scraping-with-scrapy)

My goal is to scrape the comics in order of day of the week and save it to an excel datasheet. My source is https://comic.naver.com/webtoon/weekday.nhn.I have had success scraping the data directly through the terminal and would like to write a proper script for the entire process, but have had not had much success.directly scraping the data through the terminal with response.xpath("//div[@class='list_area daily_all']/div[1]/div/h4/span/text()").extract() will properly yield the data. The weekdays are ordered from div[1~7], and this code returns "Monday."The following code returns a list of Monday comics.

response.xpath("//div[@class='list_area daily_all']/div[1]/div//ul/li/a[@class='title']/text()").extract()However, the following code does not return the desired results.The expected result would be 7 lines of the following code, in order of day of the week

{'Day': [day], 'Title': [title1, title2, title3]}However, my code is returning

{'Day': [], 'Title': []}I hope this all makes sense.

2019-06-11 22:40:04Z

My goal is to scrape the comics in order of day of the week and save it to an excel datasheet. My source is https://comic.naver.com/webtoon/weekday.nhn.I have had success scraping the data directly through the terminal and would like to write a proper script for the entire process, but have had not had much success.directly scraping the data through the terminal with response.xpath("//div[@class='list_area daily_all']/div[1]/div/h4/span/text()").extract() will properly yield the data. The weekdays are ordered from div[1~7], and this code returns "Monday."The following code returns a list of Monday comics.

response.xpath("//div[@class='list_area daily_all']/div[1]/div//ul/li/a[@class='title']/text()").extract()However, the following code does not return the desired results.The expected result would be 7 lines of the following code, in order of day of the week

{'Day': [day], 'Title': [title1, title2, title3]}However, my code is returning

{'Day': [], 'Title': []}I hope this all makes sense.You need to start your "Day" and "Title" regex with a . (dot).When you do this, doesn't matter that you are not using response.xpath you are still trying to get a h4 element at the root of the XML, not a h4 tag after the list_area daily_all div.The correct way to do this is adding a . before the /h4, this dot references the current position of your previous xpath selector.

Changing the Scrapy Download Image Name

JAIvY

[Changing the Scrapy Download Image Name](https://stackoverflow.com/questions/56552886/changing-the-scrapy-download-image-name)

For my scrapy project I have been using ImagesPipeline to download images. The images are stored with filenames that correspond to the SHA1 hash of their url names.  My Question is how can i change the names to contain the name of another scrapy field stored in item['image_name']I have been looking at multiple previous questions including, 

 How can I change the scrapy download image name in pipelines?.

  Scrapy image download how to use custom filename.  However, I have not been able to make any of these methods work. Especially the 2017 answer since that was the closest answer to Scrapy 1.6 I could find. 

 From my understanding, looking at the scrapy.pipelines.images.py file is that the idea of renaming the file stems from overriding the file_path function which returns the 'full/%s.jpg' % (image_guid)

To do this I presume that the specific item container must be requested and stored in the meta data in the get_media_request function.

I am confused though as I am unclear on how this is accessing the images item field which seems to be where the path occurs in the running of the spider.

I am not sure of this process though and would really appreciate some help with the matter.My Current Code for Pipelines.pyThe 'image_name' field is updated correctly however in 'images' field the 'path' is still a SHA1 hash of the Url

------------------------------Solution----------------------------------

The solution to this problem has been discovered. The main problem was me not understanding that to overwrite the pipeline I have to actively call it into the program. The following is the code that fixed the problem.

pipelines.pysettings.pyWhere basicimage is my personal project name. Following this I was able to slightly adapt the code to also be able to change the directory folder name as follows.

2019-06-11 23:34:52Z

For my scrapy project I have been using ImagesPipeline to download images. The images are stored with filenames that correspond to the SHA1 hash of their url names.  My Question is how can i change the names to contain the name of another scrapy field stored in item['image_name']I have been looking at multiple previous questions including, 

 How can I change the scrapy download image name in pipelines?.

  Scrapy image download how to use custom filename.  However, I have not been able to make any of these methods work. Especially the 2017 answer since that was the closest answer to Scrapy 1.6 I could find. 

 From my understanding, looking at the scrapy.pipelines.images.py file is that the idea of renaming the file stems from overriding the file_path function which returns the 'full/%s.jpg' % (image_guid)

To do this I presume that the specific item container must be requested and stored in the meta data in the get_media_request function.

I am confused though as I am unclear on how this is accessing the images item field which seems to be where the path occurs in the running of the spider.

I am not sure of this process though and would really appreciate some help with the matter.My Current Code for Pipelines.pyThe 'image_name' field is updated correctly however in 'images' field the 'path' is still a SHA1 hash of the Url

------------------------------Solution----------------------------------

The solution to this problem has been discovered. The main problem was me not understanding that to overwrite the pipeline I have to actively call it into the program. The following is the code that fixed the problem.

pipelines.pysettings.pyWhere basicimage is my personal project name. Following this I was able to slightly adapt the code to also be able to change the directory folder name as follows.

Scrapy/BigQuery fails when closing spider and sends this error: OSError: [Errno 5] Input/Output error

Umur Togay Yazar

[Scrapy/BigQuery fails when closing spider and sends this error: OSError: [Errno 5] Input/Output error](https://stackoverflow.com/questions/56557563/scrapy-bigquery-fails-when-closing-spider-and-sends-this-error-oserror-errno)

I started a CrawlSpider to crawling a category from an online shopping web page. There was about 760k items. After 11 hours, I looked at logs and I realized that the spider was somehow closed. It failed when close_spider() function, from pipeline, was called. Basically, my own implementation of close_spider() function builds connection between spider and bigquery and transfers locally saved jsonlines file to bigquery database. However, as I mentioned, it fails in this step.I manually tried the close_spider() function and it successfully transferred the  same saved jsonlines file to bigquery. By the way, there are about 466k lines in jsonlines file. Also I've tried the same spider on a different category that has 8k items and it succesfully transferred feed file to bigquery and no error message received. I came across this error twice. When I first received this error message the spider scraped 700k items.Here is the log file:And close_spider() function :Any help will be appreciated.

2019-06-12 08:15:06Z

I started a CrawlSpider to crawling a category from an online shopping web page. There was about 760k items. After 11 hours, I looked at logs and I realized that the spider was somehow closed. It failed when close_spider() function, from pipeline, was called. Basically, my own implementation of close_spider() function builds connection between spider and bigquery and transfers locally saved jsonlines file to bigquery database. However, as I mentioned, it fails in this step.I manually tried the close_spider() function and it successfully transferred the  same saved jsonlines file to bigquery. By the way, there are about 466k lines in jsonlines file. Also I've tried the same spider on a different category that has 8k items and it succesfully transferred feed file to bigquery and no error message received. I came across this error twice. When I first received this error message the spider scraped 700k items.Here is the log file:And close_spider() function :Any help will be appreciated.If you look into the error trace you will see you got an exception in the print() function.Check this thread to understand the problem.I suggest you to simply remove the print or replace it with the logging module, the spider has an attribute logger if you want to use, but if you want to have a logger with the name of your Pipeline you can do this:

scrapy spider empty output using Link Extractor

Tobi

[scrapy spider empty output using Link Extractor](https://stackoverflow.com/questions/56577023/scrapy-spider-empty-output-using-link-extractor)

I am implementing a spider which is supposed to extract all content links from this website: https://www.accenture.com/us-en/internet-of-things-index

and follow them. The x path expression are fine, however I get no output. 

Please help.My spider:Items:

2019-06-13 09:04:50Z

I am implementing a spider which is supposed to extract all content links from this website: https://www.accenture.com/us-en/internet-of-things-index

and follow them. The x path expression are fine, however I get no output. 

Please help.My spider:Items:

Scrapy: Following links of Json Objects returned by Rest API

Tobi

[Scrapy: Following links of Json Objects returned by Rest API](https://stackoverflow.com/questions/56557735/scrapy-following-links-of-json-objects-returned-by-rest-api)

I am implementing a spider which is supposed to get all url links from

this page (and all other by pagination): https://www.ibm.com/search?lang=de&cc=de&q=iot. I am able to do that by using the api.Here is my problem:

I don't know how I can follow the links that I extracted since the Link Extractor from Scrapy only works with Selectors not Json Objects.When trying to scrape the url with a second request like this:I only get something like this for the content variable: Request GET http://www-01.ibm.com/support/docview.wss?uid=ibm10884852Please help.

Here is my full code:

2019-06-12 08:27:18Z

I am implementing a spider which is supposed to get all url links from

this page (and all other by pagination): https://www.ibm.com/search?lang=de&cc=de&q=iot. I am able to do that by using the api.Here is my problem:

I don't know how I can follow the links that I extracted since the Link Extractor from Scrapy only works with Selectors not Json Objects.When trying to scrape the url with a second request like this:I only get something like this for the content variable: Request GET http://www-01.ibm.com/support/docview.wss?uid=ibm10884852Please help.

Here is my full code:In your parse function you should yield not dict, but content request. Check this example:So, in parse_content you can get title, url and content of request: 

how to make one yield in multiple yield function

programmerwiz32

[how to make one yield in multiple yield function](https://stackoverflow.com/questions/56608774/how-to-make-one-yield-in-multiple-yield-function)

I have the following code in my scrapy spider, the parse method has two yields apparently both are happening,how can I make it so if scrapy.Request... happens it doesn't also yield item 

2019-06-15 08:33:46Z

I have the following code in my scrapy spider, the parse method has two yields apparently both are happening,how can I make it so if scrapy.Request... happens it doesn't also yield item Make additional flag, for example:So, your item will be yielded only in case there was no request done in cycle.This should work, set companyFound at class level, not inside parse

Am getting empty arrays from crawled data from certain website what could be problem?

Job Kilonzo

[Am getting empty arrays from crawled data from certain website what could be problem?](https://stackoverflow.com/questions/56564646/am-getting-empty-arrays-from-crawled-data-from-certain-website-what-could-be-pro)

I am getting empty arrays from crawled data from certain website what could be problem?

2019-06-12 14:31:52Z

I am getting empty arrays from crawled data from certain website what could be problem?I checked in scrapy shell and it seems there are some blocks without needed info. Check these results:So there are 48 blocks, but only 40 of them are valid.

So I offer to make small check on needed data in your for loop (for example check name or brand) and if it is absent, just continue.

How to set priorities in scrapy

Tim

[How to set priorities in scrapy](https://stackoverflow.com/questions/56565889/how-to-set-priorities-in-scrapy)

Trying to scrape webpages and I need to set priorities in order to scrape them in order.  Right now it wants to scrape all the page 1s of each url then all the page 2s and so on.  But I need it to scrape all the pages of url 1 and all the pages of url 2 and so on.  I have been trying to use priorities to do that by setting the first url equal to the highest priority, which would be the number of urls in the csv file.  But it is not working, mainly because I cannot decrement the priority value, because it is in a for loop so every time it enter the loop it resets priorities to the original number, so its the same each time so they all have the same priority.  How can I get the priorities working correctly in order to scrape the urls in the order I want.SplashSpider.pyUPDATE #1UPDATE 2

2019-06-12 15:38:14Z

Trying to scrape webpages and I need to set priorities in order to scrape them in order.  Right now it wants to scrape all the page 1s of each url then all the page 2s and so on.  But I need it to scrape all the pages of url 1 and all the pages of url 2 and so on.  I have been trying to use priorities to do that by setting the first url equal to the highest priority, which would be the number of urls in the csv file.  But it is not working, mainly because I cannot decrement the priority value, because it is in a for loop so every time it enter the loop it resets priorities to the original number, so its the same each time so they all have the same priority.  How can I get the priorities working correctly in order to scrape the urls in the order I want.SplashSpider.pyUPDATE #1UPDATE 2To change them through the array, it is better to do something like this:And don't forget to use, for example, meta to pass current priority (I don't remember is it possible to get it from response or not) to pass it to each children request.UPDATE:

Login problems with scrapy in python

Josue Alan

[Login problems with scrapy in python](https://stackoverflow.com/questions/56566330/login-problems-with-scrapy-in-python)

I've just started with python and scrapy and I'm making some test with a website I've created. The thing is that I can't login or show any results after form validation.I've read lots of tutorials and I've tried many different ways to do the same thing with no results at all.I don't have any output and the console is not showing errors, but I've a message that says: 

Filtered offsite request to 'dental-excellence.com.mx': <POST http://dental-excellence.com.mx/admin/secure/login.php>Can you help me please?Thank you,

Josue

2019-06-12 16:04:42Z

I've just started with python and scrapy and I'm making some test with a website I've created. The thing is that I can't login or show any results after form validation.I've read lots of tutorials and I've tried many different ways to do the same thing with no results at all.I don't have any output and the console is not showing errors, but I've a message that says: 

Filtered offsite request to 'dental-excellence.com.mx': <POST http://dental-excellence.com.mx/admin/secure/login.php>Can you help me please?Thank you,

Josue

Empty variable within instance of a class, despite specifically setting it

Gibson

[Empty variable within instance of a class, despite specifically setting it](https://stackoverflow.com/questions/56526926/empty-variable-within-instance-of-a-class-despite-specifically-setting-it)

When I run the following code:I get the following error:It seems that within the function start_requests, self.search_url seems to be an empty variable, even though I have explicitly set its value to something before calling the function. I cannot seem to figure out why that is.

2019-06-10 13:00:30Z

When I run the following code:I get the following error:It seems that within the function start_requests, self.search_url seems to be an empty variable, even though I have explicitly set its value to something before calling the function. I cannot seem to figure out why that is.The neatest way to do this, would be to use the constructor __init__(), but an easier(maybe just faster for what you want) is to move the definition of start_url inside the class. For example:

Using Python Scrapy to Scrape Coupon Website

userxxxso

[Using Python Scrapy to Scrape Coupon Website](https://stackoverflow.com/questions/56505155/using-python-scrapy-to-scrape-coupon-website)

I am trying to scrape the coupon codes from this website: https://www.cuponation.com.sg/zalora-couponI just want the codes (that are usually displayed after user clicks "view code", and the coupon code will open up in a new tab while the original tab will re-direct to the merchant's website) Take note that I only want the codes, and therefore do not want to click on the buttons which are "view discounts". I've attached some summary code, and have also tried to view the network requests after clicking "view code" so that I can get python to send the correct network requests to scrape the coupon code, but seems to be unable to do so. The only alternative seems to be using Selenium to simulate a user click, but this is too tedious and resource-heavy. Instead, is there a way for Python to fire the right network request and output the codes all to a json file for viewing. Tried adapting summary code below, but cannot seem to identify the right network requests being fired to obtain the coupon code. Expected Output: 

2019-06-08 09:34:43Z

I am trying to scrape the coupon codes from this website: https://www.cuponation.com.sg/zalora-couponI just want the codes (that are usually displayed after user clicks "view code", and the coupon code will open up in a new tab while the original tab will re-direct to the merchant's website) Take note that I only want the codes, and therefore do not want to click on the buttons which are "view discounts". I've attached some summary code, and have also tried to view the network requests after clicking "view code" so that I can get python to send the correct network requests to scrape the coupon code, but seems to be unable to do so. The only alternative seems to be using Selenium to simulate a user click, but this is too tedious and resource-heavy. Instead, is there a way for Python to fire the right network request and output the codes all to a json file for viewing. Tried adapting summary code below, but cannot seem to identify the right network requests being fired to obtain the coupon code. Expected Output: 

Duplicate items saved when using nested parsers in Scrapy

rmercier

[Duplicate items saved when using nested parsers in Scrapy](https://stackoverflow.com/questions/56558103/duplicate-items-saved-when-using-nested-parsers-in-scrapy)

I'm having an issue with Scrapy and the way it outputs items.Here is my items.py:Here is my only spider:And here is my JSON output:Instead, this is what I would like my output to be:As you can see, instead of appending the "player" dicts to the same item and then yielding it once, I end up with item duplicates for each iteration in my .json output file.How would I go about getting this kind of nested structure in my item without the duplicates in my output?

2019-06-12 08:47:52Z

I'm having an issue with Scrapy and the way it outputs items.Here is my items.py:Here is my only spider:And here is my JSON output:Instead, this is what I would like my output to be:As you can see, instead of appending the "player" dicts to the same item and then yielding it once, I end up with item duplicates for each iteration in my .json output file.How would I go about getting this kind of nested structure in my item without the duplicates in my output?You yield item with player for each player, so yes, you will have problem with desired output.I can recommend to use inline_requests library. Documentation is here: https://pypi.org/project/scrapy-inline-requests/ It allows you to make requests to players pages from parent function and return result in parent function.Check this working solution:

Scraping the question and answer in separate rows like 1st row of A1 column contains que 1 1st row of A2 col catains answer for qes 1 and so on

praveen

[Scraping the question and answer in separate rows like 1st row of A1 column contains que 1 1st row of A2 col catains answer for qes 1 and so on](https://stackoverflow.com/questions/56476075/scraping-the-question-and-answer-in-separate-rows-like-1st-row-of-a1-column-cont)

enter image description hereI want to scrap the questionnaire from the below website https://www.sanfoundry.com/python-mcqs-basic-operators/The format should be so that each question and answer are displayed in different columns. If the question contains only text without any html tag, that would be better.I used scrapy to scrap this information, but the problem that I'm facing is that all the questions are appearing in a single row, and all the answers are appearing in another row.

I need the display to be: question 1 in one row and the corresponding  answer for this question in another row.I need each question and corresponding answer in 2 different rows without html tag by using scrapy code. Please correct the code that I provided.

2019-06-06 10:50:32Z

enter image description hereI want to scrap the questionnaire from the below website https://www.sanfoundry.com/python-mcqs-basic-operators/The format should be so that each question and answer are displayed in different columns. If the question contains only text without any html tag, that would be better.I used scrapy to scrap this information, but the problem that I'm facing is that all the questions are appearing in a single row, and all the answers are appearing in another row.

I need the display to be: question 1 in one row and the corresponding  answer for this question in another row.I need each question and corresponding answer in 2 different rows without html tag by using scrapy code. Please correct the code that I provided.

How to make the website believe that the request is coming from a browser using Scrapy?

Christian Read

[How to make the website believe that the request is coming from a browser using Scrapy?](https://stackoverflow.com/questions/56452848/how-to-make-the-website-believe-that-the-request-is-coming-from-a-browser-using)

I am trying to scrape this url:I just wanted to scrape title and posted date only but bloomberg always banned man and think that I am robotSample Response that I've received:Any idea how can I make the website believe that the request is coming from a browser using Scrapy?This is what I've done so farI also use crawlera as well and I added it on settings.pyPlease help me thank you

2019-06-05 00:22:15Z

I am trying to scrape this url:I just wanted to scrape title and posted date only but bloomberg always banned man and think that I am robotSample Response that I've received:Any idea how can I make the website believe that the request is coming from a browser using Scrapy?This is what I've done so farI also use crawlera as well and I added it on settings.pyPlease help me thank youYou need to use headers, mainly to specify a User-Agent which tells the website general information about the browser and device. There is a massive User-Agent List on GitHub if you need help finding one.You can specify headers for a specific request like this:

How to extract all text if their is a html tag in b/w the text segment of a webpage using scrapy in python

Ayush Gupta

[How to extract all text if their is a html tag in b/w the text segment of a webpage using scrapy in python](https://stackoverflow.com/questions/56567173/how-to-extract-all-text-if-their-is-a-html-tag-in-b-w-the-text-segment-of-a-webp)

I've tried this and but can't get [34] and some_text3 and if there are more tags like <p>, <div> and <span> which contains text inside. Is there a way to extract all the text inside <li> no matter how many tags are there?The output should be Some_text1  some text_link Again some_text2[34] and some_text3 again.

The output my program getting is Some_text1 some text_link

2019-06-12 17:02:12Z

I've tried this and but can't get [34] and some_text3 and if there are more tags like <p>, <div> and <span> which contains text inside. Is there a way to extract all the text inside <li> no matter how many tags are there?The output should be Some_text1  some text_link Again some_text2[34] and some_text3 again.

The output my program getting is Some_text1 some text_link

How to identify blank `td` seperating each column in a table

Shijith

[How to identify blank `td` seperating each column in a table](https://stackoverflow.com/questions/56405331/how-to-identify-blank-td-seperating-each-column-in-a-table)

I am scraping data from sec uoip_10k, Consolidated Balance Sheets table. Each column is separated by 1 or twotd with blank data. Is there a way to identify those blanks tds. currently , what i am doing is a below.But the issue here is scraping data from rows where cell width is not specified (e.g.: heading rows).  Is there any way i can identify and delete td which are there only to separate columns.  in the above example tds 2,4,5 are blanks tds only for separating columns.please help.

2019-06-01 09:09:41Z

I am scraping data from sec uoip_10k, Consolidated Balance Sheets table. Each column is separated by 1 or twotd with blank data. Is there a way to identify those blanks tds. currently , what i am doing is a below.But the issue here is scraping data from rows where cell width is not specified (e.g.: heading rows).  Is there any way i can identify and delete td which are there only to separate columns.  in the above example tds 2,4,5 are blanks tds only for separating columns.please help.The code below (tested under python 3.6) skips empty cells and empty rows. It also skips table headers.You can remove the debug print if the code work for you.results:

How do I extract a list of URLs off a page with selenium?

Lawrence_matei

[How do I extract a list of URLs off a page with selenium?](https://stackoverflow.com/questions/56400988/how-do-i-extract-a-list-of-urls-off-a-page-with-selenium)

I am attempting to extract all urls that https://shop.freedommobile.ca/devices has when you click the 'see options' button under each phone and place them into a list of strings.I am using python with Selenium and wait libraries.

Ive already tried using .text in my parameters. However, I keep running into an error that states:typeError: 'str' object is not callable

line 17 is the issue.should return a list of urls in string format:[https://shop.freedommobile.ca/devices/Apple/iPhone_XS_Max?sku=190198786135&planSku=Freedom%20Big%20Gig%2015GB,https://shop.freedommobile.ca/devices/Apple/iPhone_XS?sku=190198790569&planSku=Freedom%20Big%20Gig%,https://shop.freedommobile.ca/devices/Apple/iPhone_XR?sku=190198776631&planSku=Freedom%20Big%20Gig%2015GB]Any help is appreciated

Thank you

2019-05-31 19:50:45Z

I am attempting to extract all urls that https://shop.freedommobile.ca/devices has when you click the 'see options' button under each phone and place them into a list of strings.I am using python with Selenium and wait libraries.

Ive already tried using .text in my parameters. However, I keep running into an error that states:typeError: 'str' object is not callable

line 17 is the issue.should return a list of urls in string format:[https://shop.freedommobile.ca/devices/Apple/iPhone_XS_Max?sku=190198786135&planSku=Freedom%20Big%20Gig%2015GB,https://shop.freedommobile.ca/devices/Apple/iPhone_XS?sku=190198790569&planSku=Freedom%20Big%20Gig%,https://shop.freedommobile.ca/devices/Apple/iPhone_XR?sku=190198776631&planSku=Freedom%20Big%20Gig%2015GB]Any help is appreciated

Thank youHere is the logic that you should use.output:['https://shop.freedommobile.ca/devices/Apple/iPhone_XS_Max?sku=190198786135&planSku=Freedom%20Big%20Gig%2015GB', 'https://shop.freedommobile.ca/devices/Apple/iPhone_XS?sku=190198790569&planSku=Freedom%20Big%20Gig%2015GB', 'https://shop.freedommobile.ca/devices/Apple/iPhone_XR?sku=190198776631&planSku=Freedom%20Big%20Gig%2015GB', 'https://shop.freedommobile.ca/devices/Apple/iPhone_8_Plus?sku=190198454249&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Apple/iPhone_8?sku=190198450944&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_S10+?sku=887276301570&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_S10?sku=887276312163&planSku=Freedom%20Big%20Gig%20%2B%20Talk%2015GB', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_S10e?sku=887276313870&planSku=Freedom%20Big%20Gig%2015GB', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_Tab_A_8_LTE?sku=887276299440&planSku=Promo%20Tablet%2015', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_Note9?sku=887276279916&planSku=Freedom%20Big%20Gig%2015GB', 'https://shop.freedommobile.ca/devices/Samsung/Galaxy_S9?sku=887276250861&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Motorola/G7_Power?sku=723755134249&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Motorola/Moto_E5_Play?sku=723755125940&planSku=Freedom%20LTE%2B3G%209.5GB%20Promo', 'https://shop.freedommobile.ca/devices/Google/Pixel_3a?sku=842776111326&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Google/Pixel_3?sku=842776109798&planSku=Freedom%20Big%20Gig%20%2B%20Talk%2010GB', 'https://shop.freedommobile.ca/devices/Google/Pixel_3_XL?sku=842776109828&planSku=Freedom%20Big%20Gig%20%2B%20Talk%2010GB', 'https://shop.freedommobile.ca/devices/ZTE/Z557?sku=885913107448&planSku=Freedom%20500MB', 'https://shop.freedommobile.ca/devices/LG/G7_ThinQ?sku=652810830737&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Huawei/P30_lite?sku=886598061131&planSku=Freedom%20Big%20Gig%20%2B%20Talk%205GB', 'https://shop.freedommobile.ca/devices/Huawei/Mate_20_Pro?sku=886598058964&planSku=Freedom%20Big%20Gig%20%2B%20Talk%2010GB', 'https://shop.freedommobile.ca/devices/LG/X_Power_3?sku=652810831130&planSku=Freedom%20LTE%2B3G%209.5GB%20Promo', 'https://shop.freedommobile.ca/devices/LG/G8_ThinQ?sku=652810832434&planSku=Freedom%20Big%20Gig%20%2B%20Talk%2010GB', 'https://shop.freedommobile.ca/devices/LG/Q_Stylo_+?sku=652810831222&planSku=Freedom%202GB', 'https://shop.freedommobile.ca/devices/Alcatel/GoFLIP?sku=889063504010&planSku=Freedom%20500MB', 'https://shop.freedommobile.ca/devices/Bring_Your/Own_Device?sku=byod']Try using list comprehension to achieve the reults. Just take a look at this portion (By.XPATH(XPathLocation))) that you used which should be wait.until(EC.visibility_of_all_elements_located((By.XPATH, "some_xpath"))).Rectified one is more like:To extract all urls that https://shop.freedommobile.ca/devices has using Selenium you have to induce WebDriverWait for the visibility_of_all_elements_located() and you can use the following Locator Strategy:

Connection refused error while trying to scrape a website

Tim

[Connection refused error while trying to scrape a website](https://stackoverflow.com/questions/56364444/connection-refused-error-while-trying-to-scrape-a-website)

Trying to run a web scraping program but I keep getting the error "Connection refused by other side: 111 : Connection refused".  I think it might have something to do with my Splash_url, even though I verified that localhost8050 is up and running.  Because if its not that then I do not know what the problem could be.  setting.pySplashSpider.pyError Message:

2019-05-29 15:55:19Z

Trying to run a web scraping program but I keep getting the error "Connection refused by other side: 111 : Connection refused".  I think it might have something to do with my Splash_url, even though I verified that localhost8050 is up and running.  Because if its not that then I do not know what the problem could be.  setting.pySplashSpider.pyError Message:In order to fix problem I needed to set splash_url equal to the current proxy in order to prevent program from crashing. 

Different webpages opening using next button and href attribute of css selector

Aviral Agarwal

[Different webpages opening using next button and href attribute of css selector](https://stackoverflow.com/questions/56332541/different-webpages-opening-using-next-button-and-href-attribute-of-css-selector)

If I am using the next button given on the bottom of the webpage it opens a different webpage than opening it by copying the URL in href attribute of its CSS selector on a new tabI am making a scraper using scrapy. This is the start URL:https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off.Spider is working ok till page 18. From page 18 it is going to a different web page than it should go using the next button. TO check the problem i manually tried to open the webpage using the link in href but landed on a different webpage. Following is the code to open crawl the next webpage-

2019-05-27 21:17:36Z

If I am using the next button given on the bottom of the webpage it opens a different webpage than opening it by copying the URL in href attribute of its CSS selector on a new tabI am making a scraper using scrapy. This is the start URL:https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off.Spider is working ok till page 18. From page 18 it is going to a different web page than it should go using the next button. TO check the problem i manually tried to open the webpage using the link in href but landed on a different webpage. Following is the code to open crawl the next webpage-

Can scrapy skip the error of empty data and keeping scraping?

Cong Luo

[Can scrapy skip the error of empty data and keeping scraping?](https://stackoverflow.com/questions/56362024/can-scrapy-skip-the-error-of-empty-data-and-keeping-scraping)

I want to scrape product pages from its sitemap, the products page are similar, but not all of them are the same.for exampleProduct A

https://www.vitalsource.com/products/environment-the-science-behind-the-stories-jay-h-withgott-matthew-v9780134446400Product B

https://www.vitalsource.com/products/abnormal-psychology-susan-nolen-hoeksema-v9781259765667we can see the product A has the subtitle but another one doesn't have.So I get errors when I trying to scrape all the product pages.My question is, is there a way to let the spider skip the error for returning no data?There is a simple way to bypass it. that is not using strip()

But I am wondering if there is a better way to do the job.error message

2019-05-29 13:41:51Z

I want to scrape product pages from its sitemap, the products page are similar, but not all of them are the same.for exampleProduct A

https://www.vitalsource.com/products/environment-the-science-behind-the-stories-jay-h-withgott-matthew-v9780134446400Product B

https://www.vitalsource.com/products/abnormal-psychology-susan-nolen-hoeksema-v9781259765667we can see the product A has the subtitle but another one doesn't have.So I get errors when I trying to scrape all the product pages.My question is, is there a way to let the spider skip the error for returning no data?There is a simple way to bypass it. that is not using strip()

But I am wondering if there is a better way to do the job.error messageSince you need only one subtitle you can use get() with setting default value to empty string. This will save you from errors about applying strip() function to empty element.You could check if a value is returned before extracting:That way the subTitle code line would only run if a value was to be returned... In general scrapy will not stop crawling if callbacks raise an exception. e.g.:In this example 10 requests will be made and 9 items will be scraped but 1 will fail and got o errbackIn your case you have nothing to fear - any request that does not raise an exception will scrape as it should, for the ones that do you'll just see an exception traceback in your terminal/logs.

Scrapy, Twisted Critical Error : ImportError: DLL load failed

MapHacks1979

[Scrapy, Twisted Critical Error : ImportError: DLL load failed](https://stackoverflow.com/questions/56333557/scrapy-twisted-critical-error-importerror-dll-load-failed)

I was running my scrapy code just fine until I got an error about not finding my module when calling "scrapy crawl file". I did not recall changing anything important and this error came out of nowhere.I reinstalled scrapy and now I have a new error:I tried to look into the file directories and crawler.py is still there. Some other posts told me to instal pywin32 but I had that already so I reinstalled to no avail. I even copied the base constructor to mine and it still doesnt work. Any help is appreciated.My simplified code:

2019-05-27 23:54:25Z

I was running my scrapy code just fine until I got an error about not finding my module when calling "scrapy crawl file". I did not recall changing anything important and this error came out of nowhere.I reinstalled scrapy and now I have a new error:I tried to look into the file directories and crawler.py is still there. Some other posts told me to instal pywin32 but I had that already so I reinstalled to no avail. I even copied the base constructor to mine and it still doesnt work. Any help is appreciated.My simplified code:

Scrapy Returns Different Results between Shell and Spider

David Scott

[Scrapy Returns Different Results between Shell and Spider](https://stackoverflow.com/questions/56336730/scrapy-returns-different-results-between-shell-and-spider)

I am attempting to get the mini-bio from the top of the following page:https://en.m.wikipedia.org/wiki/C%C3%A9sar_MilsteinWith scrapy shell I'm able to perform the following:However, the following parser code in my spider is returning an empty list when passed that URL:Output:Note the commented line in the parser, that xpath will return a set of paragraphs that include the desired paragraph (the paragraph inside of the 'mf-section-0' div), so the paragraph does appear to be being rendered. However, it will also include all the other paragraphs in the text section as well without enough information to differentiate across other similar pages. Can anyone tell me why I'm getting different results between the shell and the spider, and how I can get the same results in the spider as I am getting in the shell?

2019-05-28 07:06:04Z

I am attempting to get the mini-bio from the top of the following page:https://en.m.wikipedia.org/wiki/C%C3%A9sar_MilsteinWith scrapy shell I'm able to perform the following:However, the following parser code in my spider is returning an empty list when passed that URL:Output:Note the commented line in the parser, that xpath will return a set of paragraphs that include the desired paragraph (the paragraph inside of the 'mf-section-0' div), so the paragraph does appear to be being rendered. However, it will also include all the other paragraphs in the text section as well without enough information to differentiate across other similar pages. Can anyone tell me why I'm getting different results between the shell and the spider, and how I can get the same results in the spider as I am getting in the shell?

Getting to the next page using scrapy

Mayank

[Getting to the next page using scrapy](https://stackoverflow.com/questions/56336893/getting-to-the-next-page-using-scrapy)

I am trying to make a web scraper but I'm unable to get the link of the next page. I have tried some combinations but none of them work. The tutorial on scrapy.org has a simpler format so it doesn't solve my problemThe site I'm scraping has the following layout:I want to get the 3.html link using css selectorsIt only scrapes the first page and then stops

2019-05-28 07:18:08Z

I am trying to make a web scraper but I'm unable to get the link of the next page. I have tried some combinations but none of them work. The tutorial on scrapy.org has a simpler format so it doesn't solve my problemThe site I'm scraping has the following layout:I want to get the 3.html link using css selectorsIt only scrapes the first page and then stopsChange you code to this and it will work

Running Python Scrapy Spider on a Schedule

amal

[Running Python Scrapy Spider on a Schedule](https://stackoverflow.com/questions/56363085/running-python-scrapy-spider-on-a-schedule)

I'm new to Python and web scraping. Pls excuse me for my ignorance. In this program I want to run Scrapy Spider on a schedule. I already tried with "schedule" package. But it was not a success since do() method need an method call as an argument. I tried passing class name "MySpider", But it didn't produce the desired output.

I also tried cron jobs. But couldn't figure out how to run Spider as a cron job. Please help me to understand how to run this on a schedule. Any help would be appreciated. I use Python 3.7 on Mac OS

2019-05-29 14:36:18Z

I'm new to Python and web scraping. Pls excuse me for my ignorance. In this program I want to run Scrapy Spider on a schedule. I already tried with "schedule" package. But it was not a success since do() method need an method call as an argument. I tried passing class name "MySpider", But it didn't produce the desired output.

I also tried cron jobs. But couldn't figure out how to run Spider as a cron job. Please help me to understand how to run this on a schedule. Any help would be appreciated. I use Python 3.7 on Mac OS

Cleaning unwanted rows of a scrapy result

Sand_Hill

[Cleaning unwanted rows of a scrapy result](https://stackoverflow.com/questions/56340716/cleaning-unwanted-rows-of-a-scrapy-result)

I am trying to scrape data using Scrapy and and clean the result of price from any unwanted characters with mapcompose. The result I have so far gives me a clean price, without any unwanted characters like currency sign or any . or , 

But, it gives me "extra" empty rows. The output I got so far is:

2019-05-28 10:59:21Z

I am trying to scrape data using Scrapy and and clean the result of price from any unwanted characters with mapcompose. The result I have so far gives me a clean price, without any unwanted characters like currency sign or any . or , 

But, it gives me "extra" empty rows. The output I got so far is:Since you only need to clean the prices list before exporting the results to a CSV, you can do the following: 

(output is assumed to be the dict that contains what you've shown in the question)Your updated output['price'] will be exactly 11 in length, as below:

POST request URL not working when used directly

Shawn Parker

[POST request URL not working when used directly](https://stackoverflow.com/questions/56334315/post-request-url-not-working-when-used-directly)

I am trying to scrap a cinema site showtimes. When I observe the POST requests the site is using to retrieve the showtimes (https://www.majorcineplex.com/booking2/search_showtime/cinema=1), it is working correctly.However when I use the POST request (https://www.majorcineplex.com/ajaxbooking/ajax_showtime) directly in the browser. It is showing me "There is no information for this show".I find this weird as both were fired from the same Chrome browser but I am getting different results.I offer my appreciations in advance for any help/advice provided.Update 29-May-2019Here is my code for the Scrapy spider.Basically from the response, I am trying to retrieve a div element with the class=book_st_contain.I am sure this div element is in the HTML as I have checked using the Chrome Dev Tools. However it is just not there when I run the spider.

2019-05-28 02:33:47Z

I am trying to scrap a cinema site showtimes. When I observe the POST requests the site is using to retrieve the showtimes (https://www.majorcineplex.com/booking2/search_showtime/cinema=1), it is working correctly.However when I use the POST request (https://www.majorcineplex.com/ajaxbooking/ajax_showtime) directly in the browser. It is showing me "There is no information for this show".I find this weird as both were fired from the same Chrome browser but I am getting different results.I offer my appreciations in advance for any help/advice provided.Update 29-May-2019Here is my code for the Scrapy spider.Basically from the response, I am trying to retrieve a div element with the class=book_st_contain.I am sure this div element is in the HTML as I have checked using the Chrome Dev Tools. However it is just not there when I run the spider.You need to ensure that the headers and posted body matches the one you are seeing in your browsers devtools:A scrapy spider to replicate this would look something like this:Primarily you have to ensure Content-Type and X-Requested-With headers are present and match the values you see in your inspector.

Using scrapy to loop through discovered a-href url links to scrape the corresponding pages

JamesMatson

[Using scrapy to loop through discovered a-href url links to scrape the corresponding pages](https://stackoverflow.com/questions/56351064/using-scrapy-to-loop-through-discovered-a-href-url-links-to-scrape-the-correspon)

I'm just starting out with Scrapy and Python, and have been following the tutorial, but am stuck. I have been able to use the shell to get a list of links from a page, as per the below: Gives me: What I want to be able to do, at least using the shell for now (and then scripting it) is to be able to parse out any links that don't contain shop-online, and then scrape the corresponding URLs, which will be www..website/the link I've scrapedBut I'm not sure how to do this. I am aware there are regex expressions you can use, but I'm not sure how to apply them, and even if I could, I'm not sure how to then tell scrapy to iterate through what I've found and scrape THOSE pages? 

2019-05-28 23:01:08Z

I'm just starting out with Scrapy and Python, and have been following the tutorial, but am stuck. I have been able to use the shell to get a list of links from a page, as per the below: Gives me: What I want to be able to do, at least using the shell for now (and then scripting it) is to be able to parse out any links that don't contain shop-online, and then scrape the corresponding URLs, which will be www..website/the link I've scrapedBut I'm not sure how to do this. I am aware there are regex expressions you can use, but I'm not sure how to apply them, and even if I could, I'm not sure how to then tell scrapy to iterate through what I've found and scrape THOSE pages? In a spider callback, that would be:In the shell, you only can handle one request at a time, because it is only meant for debugging purposes, so you would just pick one of the links and fetch it:

Scrapy can't manage to request text with neither CSS or xPath

Bulbuzor

[Scrapy can't manage to request text with neither CSS or xPath](https://stackoverflow.com/questions/56352427/scrapy-cant-manage-to-request-text-with-neither-css-or-xpath)

I've been trying to extract some text for a while now, and while everything works fine, there is something I can't manage to get.Take this website : https://duproprio.com/fr/montreal/pierrefonds-roxboro/condo-a-vendre/hab-305-5221-rue-riviera-854000I want to get the texts from the class=listing-main-characteristics__number nodes (below the picture, the box with "2 chambres 1 salle de bain Aire habitable (s-sol exclu) 1,030 pi2 (95,69m2)", there are 3 elements with that class in the page ( "2", "1" and "1,030 pi² (95,69 m²)"). I've tried a bunch of options in XPath and CSS, but none has worked, some gave back strange answers.For example, with :I get :For example, something else that works just fine on the same webpage :If I get all the spans with this query :I can find my texts mentioned in the beginning in the. But from this :I only get this Could someone clue me in with what kind of selection I would need? Thank you so much!

2019-05-29 02:49:00Z

I've been trying to extract some text for a while now, and while everything works fine, there is something I can't manage to get.Take this website : https://duproprio.com/fr/montreal/pierrefonds-roxboro/condo-a-vendre/hab-305-5221-rue-riviera-854000I want to get the texts from the class=listing-main-characteristics__number nodes (below the picture, the box with "2 chambres 1 salle de bain Aire habitable (s-sol exclu) 1,030 pi2 (95,69m2)", there are 3 elements with that class in the page ( "2", "1" and "1,030 pi² (95,69 m²)"). I've tried a bunch of options in XPath and CSS, but none has worked, some gave back strange answers.For example, with :I get :For example, something else that works just fine on the same webpage :If I get all the spans with this query :I can find my texts mentioned in the beginning in the. But from this :I only get this Could someone clue me in with what kind of selection I would need? Thank you so much!Here is the xpath that you have to use.you might have to use the above xpath. (Add /text() is you want the associated text.)Below is the python sample codeOutput:2 chambres1 salle de bain1,030 pi² (95,69 m²)Screenshot:



How to remove HTML Tags and '\n' from Scrapy Python Output

amal

[How to remove HTML Tags and '\n' from Scrapy Python Output](https://stackoverflow.com/questions/56306575/how-to-remove-html-tags-and-n-from-scrapy-python-output)

I'm new to Python and Web Scraping. I wrote below 2 lines to extract title and price from website. However it gives output with html tags and '\n' characters. 

How can I remove them and get only text output?Output

2019-05-25 16:11:37Z

I'm new to Python and Web Scraping. I wrote below 2 lines to extract title and price from website. However it gives output with html tags and '\n' characters. 

How can I remove them and get only text output?OutputRemoving extra spaces an \n:Second selector also should have ::text in selector: 

product_price = response.css('#priceblock_ourprice::text').extract_first()

Trying to response Amazon's Captcha with scrapy, strange behavior on spider generator

Bruno Aquino

[Trying to response Amazon's Captcha with scrapy, strange behavior on spider generator](https://stackoverflow.com/questions/56349371/trying-to-response-amazons-captcha-with-scrapy-strange-behavior-on-spider-gene)

I'm creating a crawler to Amazon for study reason, but it is being caught by their captcha.

So I have made a captcha solver, but I'm having trouble to response the captcha form.

The problem is that if I put a yeild FormRequest in the method, it seems to not be called.below the Amazon's captcha formI put two logs into the code.The first on inside the verify_if_captcha:This one is printedThe second inside the solve_captcha:This one is never printedCan some one help me, please?

2019-05-28 20:05:03Z

I'm creating a crawler to Amazon for study reason, but it is being caught by their captcha.

So I have made a captcha solver, but I'm having trouble to response the captcha form.

The problem is that if I put a yeild FormRequest in the method, it seems to not be called.below the Amazon's captcha formI put two logs into the code.The first on inside the verify_if_captcha:This one is printedThe second inside the solve_captcha:This one is never printedCan some one help me, please?Currently, your form request object is never returned to Scrapy for handling.Replace self.solve_captcha(response, self.parse) by yield from self.solve_captcha(response, self.parse).

Unable to crawl multiple pages

Raza Ul Haq

[Unable to crawl multiple pages](https://stackoverflow.com/questions/56354096/unable-to-crawl-multiple-pages)

I am trying to crawl quotes, author names and tags from goodreads. I am able to crawl single page with following codebut when I try to crawl same spider with addition of following code I get the following error.I am not sure that the problem is with xpath because in first attempt I get but in here it is 1, which means spider isn't crawling even first page.

2019-05-29 06:09:26Z

I am trying to crawl quotes, author names and tags from goodreads. I am able to crawl single page with following codebut when I try to crawl same spider with addition of following code I get the following error.I am not sure that the problem is with xpath because in first attempt I get but in here it is 1, which means spider isn't crawling even first page.You have to fix two issues to get your next page link working. Other than what @pako pointed out, you could have used .extract_first() or .get() to get the first item of an array. Rectified one should be more like .xpath('//a[@class="next_page"]/@href').get(). I've rewritten some of your xpaths to kick out whitespaces from the output.

extract an element from an html declaration

mosorio

[extract an element from an html declaration](https://stackoverflow.com/questions/56351078/extract-an-element-from-an-html-declaration)

I am using scrapy selectors and I am trying to extract the element "1" in from the HTML declaration below:I have two equal declarations in the whole HTML source content.I have tried the command:but it returned an empty string.

2019-05-28 23:02:30Z

I am using scrapy selectors and I am trying to extract the element "1" in from the HTML declaration below:I have two equal declarations in the whole HTML source content.I have tried the command:but it returned an empty string.It works.

scrapy Request url and Response url are different

x11025

[scrapy Request url and Response url are different](https://stackoverflow.com/questions/56247153/scrapy-request-url-and-response-url-are-different)

this spider is supposed to get data from a link likebut in the lister value i getif decoded the rsargs[1] it becomes a total different name from listurl and rsargs[0] is a different number and if repeated it the responseurl is always random and more random if increased the range(1, 3)what is problem with spider?what i want is to get the same results in listurl and responseurl because i sent the listurl in a call for function def parseresponse but the responseurl which is supposed ot be listurl is totally different.

2019-05-21 22:35:50Z

this spider is supposed to get data from a link likebut in the lister value i getif decoded the rsargs[1] it becomes a total different name from listurl and rsargs[0] is a different number and if repeated it the responseurl is always random and more random if increased the range(1, 3)what is problem with spider?what i want is to get the same results in listurl and responseurl because i sent the listurl in a call for function def parseresponse but the responseurl which is supposed ot be listurl is totally different.EDITI'm sorry, I thought you were complaining about the encoding, but the real question was about the numbers, right?The problem is that lister = {} is outside the inner for loop. Thus, you're passing the same object reference to the callbacks but you keep updating it. A quick fix would be:

How can I efficiently extract text from bunch for web pages without extra information

Samresh

[How can I efficiently extract text from bunch for web pages without extra information](https://stackoverflow.com/questions/56252434/how-can-i-efficiently-extract-text-from-bunch-for-web-pages-without-extra-inform)

I have list of webpages around 1 million, I want to efficiently just extract text from those pages. Currently I am using BeautifulSoup library in python to get text from HTML and using request command to get html of a webpage. This approach extract some extra information in addition to the text like if any javascript is listed in body.Could you please suggest me any suitable and efficient way to do the task. I looked at scrapy but it looks like it crawls specific website. Can we pass it list of specific webpages to get information from ?Thank you in advance.

2019-05-22 08:28:16Z

I have list of webpages around 1 million, I want to efficiently just extract text from those pages. Currently I am using BeautifulSoup library in python to get text from HTML and using request command to get html of a webpage. This approach extract some extra information in addition to the text like if any javascript is listed in body.Could you please suggest me any suitable and efficient way to do the task. I looked at scrapy but it looks like it crawls specific website. Can we pass it list of specific webpages to get information from ?Thank you in advance.Yes, you can use Scrapy to crawl a set of URLs in a generic fashion.You simply need to set them on the start_urls list attribute of your spider, or reimplement the start_requests spider method to yield requests from any data source, and then implement your parse callback to perform the generic content extraction you want.You can use html-text to extract text from them, and regular Scrapy selectors to extract additional data like the one you mention.In scrapy you can set up your own parser. E.g. Beautiful soup. This parser you can call from your parse method.To extract text from generic pages I traverse the body only, exclude comments etc and some tags like script, style, etc:with

response.body is returning an empty file using scrapy in python

Jose

[response.body is returning an empty file using scrapy in python](https://stackoverflow.com/questions/56345516/response-body-is-returning-an-empty-file-using-scrapy-in-python)

I am trying to make a web crawler, using scrapy from python, that extracts the information that google shows in the right side when you make a searchThe URL I am using is: https://www.google.com/search?q=la%20cuartaIn this other question I asked the same (question), someone suggested me to write response.body to a file, but I am getting an empty file, when I tried a different URL this does not happendThis is my code:It does not even write the file from the google search, but in the scrapy shell response.body is not empty

2019-05-28 15:27:30Z

I am trying to make a web crawler, using scrapy from python, that extracts the information that google shows in the right side when you make a searchThe URL I am using is: https://www.google.com/search?q=la%20cuartaIn this other question I asked the same (question), someone suggested me to write response.body to a file, but I am getting an empty file, when I tried a different URL this does not happendThis is my code:It does not even write the file from the google search, but in the scrapy shell response.body is not emptyOk, I tested your code and it works, that is the spiders downloads all the pages, including the googel page.

Your problem might be in the settings, add these to your settings:

how to pass an argument into a scrapy spider and init it from within python

Kay

[how to pass an argument into a scrapy spider and init it from within python](https://stackoverflow.com/questions/56208117/how-to-pass-an-argument-into-a-scrapy-spider-and-init-it-from-within-python)

i am trying to pass a variable screen_name to my spider because this screen_name will change everytime. ( the end goal is to have multiple spiders running with different screen_names)i initialise like thisHowever i get the following error.This is not a question about how to run it from cmd line but only from within python

2019-05-19 13:06:29Z

i am trying to pass a variable screen_name to my spider because this screen_name will change everytime. ( the end goal is to have multiple spiders running with different screen_names)i initialise like thisHowever i get the following error.This is not a question about how to run it from cmd line but only from within pythonYou can pass the spider class and its arguments to the crawl method. Eg:

scrapy 307 redirects to same page

max will

[scrapy 307 redirects to same page](https://stackoverflow.com/questions/56212503/scrapy-307-redirects-to-same-page)

I am trying to scrap product details from jabong and the script is running fine when I scrap one URL but it redirects to 307 when I use more than one urls in yield requesti tried private proxies, user agents, disabling cookies, enabling dont_merge cookies this is what i m getting:

2019-05-19 22:27:54Z

I am trying to scrap product details from jabong and the script is running fine when I scrap one URL but it redirects to 307 when I use more than one urls in yield requesti tried private proxies, user agents, disabling cookies, enabling dont_merge cookies this is what i m getting:Yes, they do the same thing to Chrome and it is very likely because you have cookies disabled.You'll need to spend the time to find out which cookie they care about, and whether using that same cookie value compromises the anonymity of your crawl. That's too time consuming for a SO answer but it should be straightforward to do.

scrapy running the same spider multiple times with a different parameter (start url)

Kay

[scrapy running the same spider multiple times with a different parameter (start url)](https://stackoverflow.com/questions/56208876/scrapy-running-the-same-spider-multiple-times-with-a-different-parameter-start)

i have created a spider that fetches all the followings of a twitter screen_name. i have 1000s of these screen_names and i need to run the SAME spider but with a different screen_name. The screen_name is used to alter the starting url.Right now this is failing with 

2019-05-19 14:41:47Z

i have created a spider that fetches all the followings of a twitter screen_name. i have 1000s of these screen_names and i need to run the SAME spider but with a different screen_name. The screen_name is used to alter the starting url.Right now this is failing with 

How I can select multiple nodes from a page by the order that appears in the page using scrapy

Mohamed Oubenma

[How I can select multiple nodes from a page by the order that appears in the page using scrapy](https://stackoverflow.com/questions/56219185/how-i-can-select-multiple-nodes-from-a-page-by-the-order-that-appears-in-the-pag)

I want to extract multiple HTML nodes from a page in the same order that they appear in the page. because I want later to get just the 10 first nodes.The problem is scrapy selector selects nodes in a random order, so when I  show the first 10 results, those results are not the 10 first nodes in the HTML page. I expect to get 10 first products as in the HTML page, but what happened is that I get 10 random products from all the page.

is anyone has an idea on how I can select just the first nodes that I want?

2019-05-20 10:37:38Z

I want to extract multiple HTML nodes from a page in the same order that they appear in the page. because I want later to get just the 10 first nodes.The problem is scrapy selector selects nodes in a random order, so when I  show the first 10 results, those results are not the 10 first nodes in the HTML page. I expect to get 10 first products as in the HTML page, but what happened is that I get 10 random products from all the page.

is anyone has an idea on how I can select just the first nodes that I want?

How to use python requests with scrapy?

max will

[How to use python requests with scrapy?](https://stackoverflow.com/questions/56230826/how-to-use-python-requests-with-scrapy)

I am trying to use requests to fetch a page then pass the response object to a parser, but I ran into a problem:

2019-05-21 03:50:22Z

I am trying to use requests to fetch a page then pass the response object to a parser, but I ran into a problem:You first need to download the page's resopnse and then convert that string to HtmlResponse object

Scrapy - Javascript rendering

M. Coppée

[Scrapy - Javascript rendering](https://stackoverflow.com/questions/56224643/scrapy-javascript-rendering)

I would like to get some data from here:https://www.drivy.com/location-voiture/liege/mitsubishi-colt-359699?address=Gare+de+Li%C3%A8ge-Guillemins&city_display_name=&country_scope=BE&distance=200&end_date=2019-05-27&end_time=06%3A00&latitude=50.6251&longitude=5.5659&start_date=2019-05-26&start_time=06%3A00I'm searching for the ID of the owner of the car. This ID is within the aattribute of class car_owner_section. For the page above it is the numbers in the hrefattribute like this "/users/1228276". The issue is that this link is apparently rendered by javascript and I absolutely want to avoid scrapy-splash. Does anyone has an idea on how to find this ID ? It should be somewhere on a JSON I guess but I've searched for days now and found nothing. 

2019-05-20 16:14:06Z

I would like to get some data from here:https://www.drivy.com/location-voiture/liege/mitsubishi-colt-359699?address=Gare+de+Li%C3%A8ge-Guillemins&city_display_name=&country_scope=BE&distance=200&end_date=2019-05-27&end_time=06%3A00&latitude=50.6251&longitude=5.5659&start_date=2019-05-26&start_time=06%3A00I'm searching for the ID of the owner of the car. This ID is within the aattribute of class car_owner_section. For the page above it is the numbers in the hrefattribute like this "/users/1228276". The issue is that this link is apparently rendered by javascript and I absolutely want to avoid scrapy-splash. Does anyone has an idea on how to find this ID ? It should be somewhere on a JSON I guess but I've searched for days now and found nothing. I tested it on scrapy shell, and the response returns the link you are looking for, without using splash. You might want to check your settings. 

JSONDecodeError with Scrapy: Expecting value: line 1 column 1 (char 0)

8-Bit Borges

[JSONDecodeError with Scrapy: Expecting value: line 1 column 1 (char 0)](https://stackoverflow.com/questions/56205374/jsondecodeerror-with-scrapy-expecting-value-line-1-column-1-char-0)

I am using requests in order to fetch and parse some data scraped using Scrapy with Scrapyrt (real time scraping).This is how I do it:As per Scrapy documentation, with 'start_requests' parameter set as True, the spider automatically requests urls and passes the response to the parse method which is the default method used for parsing requests.But the setup is not working. Log:

2019-05-19 06:40:30Z

I am using requests in order to fetch and parse some data scraped using Scrapy with Scrapyrt (real time scraping).This is how I do it:As per Scrapy documentation, with 'start_requests' parameter set as True, the spider automatically requests urls and passes the response to the parse method which is the default method used for parsing requests.But the setup is not working. Log:The error was due to a bug with Twisted 19.2.0, a scrapyrt dependency, which assumed response to be of wrong type.Once I installed Twisted==18.9.0, it worked.

Scrape the Category of Products using Python

james joyce

[Scrape the Category of Products using Python](https://stackoverflow.com/questions/56237482/scrape-the-category-of-products-using-python)

I am trying to scrape this page, it has around 21000 productsMy question is how do i get all the products name,image and complete category hierarchy of 21000 products.

Image and name is on the same page but category is inside the actual product page. because of pagination i am only able to get the 32 products title and image that is present on first page CODE for getting title from first page

2019-05-21 11:39:08Z

I am trying to scrape this page, it has around 21000 productsMy question is how do i get all the products name,image and complete category hierarchy of 21000 products.

Image and name is on the same page but category is inside the actual product page. because of pagination i am only able to get the 32 products title and image that is present on first page CODE for getting title from first pageYou can access the json response of each page. But keep in mind, there are only 32 products per page which means you'll be requesting 659 times.Output:Edit:If you want the hierarchy, you're going to need to go to each products' link and pull that out. I provided to code to do that, but keep in mind this will take FORVER. Assuming it takes about 2-3 seconds per request it will take you nearly 18 hours.Output:ORIf all the products are under the same categories, then you only really need to get the first product's categories, and then apply to all the others as you iterate through the pages:The page makes a request for page one for content as follows (which returns json). See if you can alter parameters to get all resultsIt looks like you can alter the referer header and the current page in the body by altering the url to include page e.g.You can extract the total results count from your first requestYou know you are requesting in batches of 32 (though try increasing this to the largest value possible). You can then calculate the number of pages/requests, then issue in a loop.Python (page 1 request)Here's how to tackle pagination. 

Pagination is nothing but it just send request on demand instead of fetching it at once. It means every times you click on any page number you will see some change according to the websites design. 

In your case url query is changing every time you click on any pagelink. The produced url is If you will keep changing the page=2 to whatever page you want to scrape you will be able to scrape the website. 

Facing scrapy selenium issues while using SeleniumRequest

MITHU

[Facing scrapy selenium issues while using SeleniumRequest](https://stackoverflow.com/questions/56246545/facing-scrapy-selenium-issues-while-using-seleniumrequest)

I've written a very tiny script to parse the name of different restaurants from a webpage using scrapy in combination with selenium making use of scrapy-selenium library. My settings.py file contains:My spider contains: (used middleware reference within crawlerprocess)However, when I run my script and go through the downloader middlewares list, I can see that scrapy_selenium.SeleniumMiddleware reference is not being activated.How can I make it a go successfully?Traceback I'm getting:Full traceback available here

2019-05-21 21:23:31Z

I've written a very tiny script to parse the name of different restaurants from a webpage using scrapy in combination with selenium making use of scrapy-selenium library. My settings.py file contains:My spider contains: (used middleware reference within crawlerprocess)However, when I run my script and go through the downloader middlewares list, I can see that scrapy_selenium.SeleniumMiddleware reference is not being activated.How can I make it a go successfully?Traceback I'm getting:Full traceback available hereAccording to github source of that line 43 your application tried to read data from 'SELENIUM_DRIVER_ARGUMENTS' setting which is required for selenium middleware and is not presented in your code .

How to scrape infinite scroll page in Reddit using scrapy?

Christian Read

[How to scrape infinite scroll page in Reddit using scrapy?](https://stackoverflow.com/questions/56213676/how-to-scrape-infinite-scroll-page-in-reddit-using-scrapy)

How to scrape infinite scroll page in Reddit and add limit scroll?What I am aiming is to get the items in the firstpage but I am just getting some few items.What I have tried is to go the developers tool and check the XHR but I am getting different approach and can't find page there. This is what I am using to fetch the item.Please help thank you so much

2019-05-20 02:32:32Z

How to scrape infinite scroll page in Reddit and add limit scroll?What I am aiming is to get the items in the firstpage but I am just getting some few items.What I have tried is to go the developers tool and check the XHR but I am getting different approach and can't find page there. This is what I am using to fetch the item.Please help thank you so much

how can I access a variable parameter at spider class from pipelines.py

bluebamus

[how can I access a variable parameter at spider class from pipelines.py](https://stackoverflow.com/questions/56239351/how-can-i-access-a-variable-parameter-at-spider-class-from-pipelines-py)

I have 3 number of spider files and classes. And I want to save item informations at csv file which has different filename defendant the variable parameter of searching condition. For that, I need to access the spider class parameter.So, my questions are three.Bellow is my log code styleI had been searching the ways in google so many times. But I couldn't find the solution.I did try to use from_crawler function, but I couldn't find the adapt caseCode:

2019-05-21 13:26:11Z

I have 3 number of spider files and classes. And I want to save item informations at csv file which has different filename defendant the variable parameter of searching condition. For that, I need to access the spider class parameter.So, my questions are three.Bellow is my log code styleI had been searching the ways in google so many times. But I couldn't find the solution.I did try to use from_crawler function, but I couldn't find the adapt caseCode:For example, :You can see more about item pipelines here https://docs.scrapy.org/en/latest/topics/item-pipeline.htmlscrapy crawl yourspider -o output.csvBut if you really need it to be set from the spider, you can use a custom setting per spider, for example:

Convert email in image type to text with Python

Matija Žiberna

[Convert email in image type to text with Python](https://stackoverflow.com/questions/56242324/convert-email-in-image-type-to-text-with-python)

I've stumbled upon a website that in order to protect its content converts normal text to an image so it cannot be easily scraped with spiders.This is the code:The URL that contains business email looks like:https://www.bizi.si/ImageGenerator.aspx?JXwFUy4U5m5jKwuO3IgV3ASgH0Id5ve7uMFqS922Ezc6IUi0sEN3kHSxb0hVFQZUGP73%2bADQ6cwFmaVlY5EkzN0wTftd%2bET2KzDb1TxL434%3dWhich when rendered displays as the following image:Does anyone has any idead how could the above url be decoded in normal text?Thanks!

2019-05-21 16:06:16Z

I've stumbled upon a website that in order to protect its content converts normal text to an image so it cannot be easily scraped with spiders.This is the code:The URL that contains business email looks like:https://www.bizi.si/ImageGenerator.aspx?JXwFUy4U5m5jKwuO3IgV3ASgH0Id5ve7uMFqS922Ezc6IUi0sEN3kHSxb0hVFQZUGP73%2bADQ6cwFmaVlY5EkzN0wTftd%2bET2KzDb1TxL434%3dWhich when rendered displays as the following image:Does anyone has any idead how could the above url be decoded in normal text?Thanks!You must download the image (yield a Request for the image URL, the image bytes will be available at response.body) and use an OCR solution such as https://github.com/madmaze/pytesseract.

How to get the scraped data shown in the MySQL database in the terminal?

Shay

[How to get the scraped data shown in the MySQL database in the terminal?](https://stackoverflow.com/questions/56258532/how-to-get-the-scraped-data-shown-in-the-mysql-database-in-the-terminal)

I'm trying to get the scraped data to show up in the MySQL database.I'm following a course but it's not working.I made sure that the data (title, rating, upc, product_type) are in the same order as it is in the csv file.Here's my code, In the code editor: In the terminal:The expected result is a table of the scraped data in the terminal.I run the code and then run the second (SELECT * FROM books_table;) line, the table is supposed to show up but it's still an Empty set. Any help is much appreciated, thank you!

2019-05-22 13:55:47Z

I'm trying to get the scraped data to show up in the MySQL database.I'm following a course but it's not working.I made sure that the data (title, rating, upc, product_type) are in the same order as it is in the csv file.Here's my code, In the code editor: In the terminal:The expected result is a table of the scraped data in the terminal.I run the code and then run the second (SELECT * FROM books_table;) line, the table is supposed to show up but it's still an Empty set. Any help is much appreciated, thank you!

Scrapy splash - Refused connection

M. Coppée

[Scrapy splash - Refused connection](https://stackoverflow.com/questions/56172386/scrapy-splash-refused-connection)

I have an issue with Splash when running my web crawler. First when I run docker run -p 8050:8050 scrapinghub/splash in docker I get this (I use docker toolbox as I'm working on Windows 10 home):I've correctly set the middlewares and others in my settings.py:However when I run my crawler I get this log:Sorry some part is in french but basically says that the targeted computer has explicitly refused the connection. I guess something went wrong with docker because when I search for http://localhost:8050in my web browser I get nothing (connection failed).Does someone can help me with this issue ?

2019-05-16 15:51:51Z

I have an issue with Splash when running my web crawler. First when I run docker run -p 8050:8050 scrapinghub/splash in docker I get this (I use docker toolbox as I'm working on Windows 10 home):I've correctly set the middlewares and others in my settings.py:However when I run my crawler I get this log:Sorry some part is in french but basically says that the targeted computer has explicitly refused the connection. I guess something went wrong with docker because when I search for http://localhost:8050in my web browser I get nothing (connection failed).Does someone can help me with this issue ?

Am i doing something wrong with LinkExtractor attributes (restrict_xpath)? not doing callback

user3303019

[Am i doing something wrong with LinkExtractor attributes (restrict_xpath)? not doing callback](https://stackoverflow.com/questions/56174044/am-i-doing-something-wrong-with-linkextractor-attributes-restrict-xpath-not-d)

I'm testing rules to set up a spider on a website in order to enter to the url of each item, get information from each and then follow the pagination, in this case is infinite scrolling.

But first I wanted to create a rule for Callback to get called  every time it finds an item, but the problem is, that it's not going for callback.I've tried allow() with a bare minimum expression also and still don't  get nothing. The only time I'm getting anything is if I leave allow() and restric_xpaths() both empty.I expect "lol" to be printed.

2019-05-16 17:48:59Z

I'm testing rules to set up a spider on a website in order to enter to the url of each item, get information from each and then follow the pagination, in this case is infinite scrolling.

But first I wanted to create a rule for Callback to get called  every time it finds an item, but the problem is, that it's not going for callback.I've tried allow() with a bare minimum expression also and still don't  get nothing. The only time I'm getting anything is if I leave allow() and restric_xpaths() both empty.I expect "lol" to be printed.The main issue here is that the selector you're using (//div[@class="row"]) only matches div's with a single class row. 

Using XPath expressions, to match elements containing a class is a bit tricky:or you could use css selectors instead:EDIT:Some links:

Using Extension to send email with scrapped data

Stefanpt

[Using Extension to send email with scrapped data](https://stackoverflow.com/questions/56135481/using-extension-to-send-email-with-scrapped-data)

Need help please!After crawling a site and returning processing the data through pipelines, I need to send the scrapped data via email. I've tried and read everything but can't seem to connect the dots. 

In Pipelines I've tried the following:Should I be sending the email from a pipeline or an extension or is it preference? How would I implement it??Thanks all!

2019-05-14 17:16:16Z

Need help please!After crawling a site and returning processing the data through pipelines, I need to send the scrapped data via email. I've tried and read everything but can't seem to connect the dots. 

In Pipelines I've tried the following:Should I be sending the email from a pipeline or an extension or is it preference? How would I implement it??Thanks all!Scrapy provides MailSender module (which is based on smtplib):Here is a file you can use and import this send_mail function. You will need to change some things in order to make it work for your situation. You are including it in the correct way through a pipe line.Another piece to find the last modified file in your output directory

How to import my own modules into a scrapy project?

Rusco

[How to import my own modules into a scrapy project?](https://stackoverflow.com/questions/56102040/how-to-import-my-own-modules-into-a-scrapy-project)

I am trying to write a scrapy spider with multiple pipelines. I select which pipeline to use with an attribute of the spider. The attribute is of an enum type I wrote myself. The problem now is importing that enum in the pipeline classes. Every time I try to import it I get the following error:I already tried different variations to place the enum class and switched between relative and absolute imports. If I place the enum class in a own package independent of the scrapy package I can import and use the enum if I run the pipeline files directly but I still get an error if I want to run the spider over the shell.My current project structure is:And my current import is:

from data.file_types import FileTypesIf it helps I uploaded my code to GitHub:

https://github.com/JustACodingFox/NovelDownloader

2019-05-12 18:06:19Z

I am trying to write a scrapy spider with multiple pipelines. I select which pipeline to use with an attribute of the spider. The attribute is of an enum type I wrote myself. The problem now is importing that enum in the pipeline classes. Every time I try to import it I get the following error:I already tried different variations to place the enum class and switched between relative and absolute imports. If I place the enum class in a own package independent of the scrapy package I can import and use the enum if I run the pipeline files directly but I still get an error if I want to run the spider over the shell.My current project structure is:And my current import is:

from data.file_types import FileTypesIf it helps I uploaded my code to GitHub:

https://github.com/JustACodingFox/NovelDownloader

following the information using scrapy in nested div and span tags

Jose

[following the information using scrapy in nested div and span tags](https://stackoverflow.com/questions/56172524/following-the-information-using-scrapy-in-nested-div-and-span-tags)

I am trying to make web crawler, using scrapy from python, that extracts the information that google shows in the right side when you make a search, for example:I want to extract the information in the box in the rigth sideThe link is: search in googleThe source code: source codePart of the HTML code is:I saw that the information i want is nested in a lot of div tags and finally is the description of a span tag, so I tried the following:I just get emprty, I even tried like following each of the tags like this:But still can't get to the info I want

2019-05-16 16:01:26Z

I am trying to make web crawler, using scrapy from python, that extracts the information that google shows in the right side when you make a search, for example:I want to extract the information in the box in the rigth sideThe link is: search in googleThe source code: source codePart of the HTML code is:I saw that the information i want is nested in a lot of div tags and finally is the description of a span tag, so I tried the following:I just get emprty, I even tried like following each of the tags like this:But still can't get to the info I want

Optimal way to commit to DB via the Item Pipeline?

TheTurp

[Optimal way to commit to DB via the Item Pipeline?](https://stackoverflow.com/questions/56061761/optimal-way-to-commit-to-db-via-the-item-pipeline)

All right, I'm using Scrapy & SQLAlchemy to scrape data and store it into a SQLite DB. Now, everything is working fine, but something bugs me and I can't find the answer.So, in the process_item function inside Pipelines, what is the optimal way to commit to the DB. Most people seem to do something like this:And I've also seen this:My main interrogation revolves around method 1. It seems inefficient to get the session every time, commit a single item to the DB, then close the session, for every single item. Now the second method bugs me too because it waits until the scraping process is over to commit to the DB, which could cause problems if the spider crashes and you lose everything scraped.Is there an in between, could I commit every n items ? Or is this irrelevant since modern DB are so powerful they don't care about multiple commits in a short time. It's mostly about optimizing the commit process, I might be overthinking this.

2019-05-09 14:32:14Z

All right, I'm using Scrapy & SQLAlchemy to scrape data and store it into a SQLite DB. Now, everything is working fine, but something bugs me and I can't find the answer.So, in the process_item function inside Pipelines, what is the optimal way to commit to the DB. Most people seem to do something like this:And I've also seen this:My main interrogation revolves around method 1. It seems inefficient to get the session every time, commit a single item to the DB, then close the session, for every single item. Now the second method bugs me too because it waits until the scraping process is over to commit to the DB, which could cause problems if the spider crashes and you lose everything scraped.Is there an in between, could I commit every n items ? Or is this irrelevant since modern DB are so powerful they don't care about multiple commits in a short time. It's mostly about optimizing the commit process, I might be overthinking this.Method 1 is the way to go. Sessions should not be that expensive.Alternatively, do not use SQL Alchemy, or use only SQL Alchemy Core instead of the ORM.

Scrapy - Trouble with <TD> parsing alignment

Cookie Monster

[Scrapy - Trouble with <TD> parsing alignment](https://stackoverflow.com/questions/56068945/scrapy-trouble-with-td-parsing-alignment)

I'm attempting to parse data only from the item & Skill Cap columns in the html table here: http://ffxi.allakhazam.com/dyn/guilds/Alchemy.htmlWhen parsing I run into alignment issues where my script is parsing from other columns.Results: scrapy runspider cap.py  -t json

When it reaches the 3rd row data from an unintended column is being parsed. I'm not sure whats going on with  selection.  

2019-05-09 23:45:56Z

I'm attempting to parse data only from the item & Skill Cap columns in the html table here: http://ffxi.allakhazam.com/dyn/guilds/Alchemy.htmlWhen parsing I run into alignment issues where my script is parsing from other columns.Results: scrapy runspider cap.py  -t json

When it reaches the 3rd row data from an unintended column is being parsed. I'm not sure whats going on with  selection.  What about explicitly set source column with XPath:

Scrapy crawler's cookie issue: website language shifted

ju.

[Scrapy crawler's cookie issue: website language shifted](https://stackoverflow.com/questions/56066720/scrapy-crawlers-cookie-issue-website-language-shifted)

I am trying to craw a website, which has a language option drop down button on the top, and the default language is English. As shown below.

While I thought I managed to scrawl the data, then I found almost randomly the output JSON file started to have unicode Arabic characters (I only wanted English version of data).It seems the spider was lead to Arabic version pages, and the url changed and added a string '/ar/' in between. I then though I can apply some operation to the url to go to English version, but my experiment of the website also shows that as long as I choose any language, the cookie remembers it, and take any page to its translated language version. Following is the snapshot of the output json file.

My question is, why does this language shift happen, and how do I fix this issue? 

2019-05-09 19:58:40Z

I am trying to craw a website, which has a language option drop down button on the top, and the default language is English. As shown below.

While I thought I managed to scrawl the data, then I found almost randomly the output JSON file started to have unicode Arabic characters (I only wanted English version of data).It seems the spider was lead to Arabic version pages, and the url changed and added a string '/ar/' in between. I then though I can apply some operation to the url to go to English version, but my experiment of the website also shows that as long as I choose any language, the cookie remembers it, and take any page to its translated language version. Following is the snapshot of the output json file.

My question is, why does this language shift happen, and how do I fix this issue? 

scrapy output function directly to google drive

madboy

[scrapy output function directly to google drive](https://stackoverflow.com/questions/55996513/scrapy-output-function-directly-to-google-drive)

I have a scrapy code which I want its output directly to my Google drive,I have found pydrive easy to use and upload files (I tested it and it works)How can I use it with scrapy runspider test1.py -o test.csv to directly upload to drive?If it doesn't work, is there any suggestion for doing it ?

2019-05-05 21:18:12Z

I have a scrapy code which I want its output directly to my Google drive,I have found pydrive easy to use and upload files (I tested it and it works)How can I use it with scrapy runspider test1.py -o test.csv to directly upload to drive?If it doesn't work, is there any suggestion for doing it ?You need to write custom pipeline or a feed exporter.For example if you crawler is small and results can fit into your memory than a simple pipeline like this would do:Then activate it in your settings:It doesn't work like that, here is how to run scrapy spider without

scrapy runspider test1.py,

You should create a list of your dictionaries then write them to a CSV file, then use your Google function to upload then if you want you can delete the file you created. 

Scrapy close after 2 minutes running

M. T.

[Scrapy close after 2 minutes running](https://stackoverflow.com/questions/56004095/scrapy-close-after-2-minutes-running)

I need to monitor webpage to find available products and I use scrapy framework.

If I found a product I'll notify it. 

This webpage has in main page the list of product with some information about them and other information in product page.why the program go through this line?

2019-05-06 11:16:12Z

I need to monitor webpage to find available products and I use scrapy framework.

If I found a product I'll notify it. 

This webpage has in main page the list of product with some information about them and other information in product page.why the program go through this line?

Scrapy use item and save data in a json file

M. T.

[Scrapy use item and save data in a json file](https://stackoverflow.com/questions/56005678/scrapy-use-item-and-save-data-in-a-json-file)

I want to use scrapy item and manipulate data and saving all in json file (using json file like a db).Spider class search for products in main page of start_url, then it parse product page to catch also sizes.

Finally it search if there are updates on self.storage.update(product.__dict__) and if it's true send a notification.How can I implement Item in my code? I thought I could insert it in Product Class, but I can't include send method...

2019-05-06 12:53:16Z

I want to use scrapy item and manipulate data and saving all in json file (using json file like a db).Spider class search for products in main page of start_url, then it parse product page to catch also sizes.

Finally it search if there are updates on self.storage.update(product.__dict__) and if it's true send a notification.How can I implement Item in my code? I thought I could insert it in Product Class, but I can't include send method...You should define the item you want. And yield it after parsed.Last, run the command:

scrapy crawl [spider] -o xx.jsonPS:

Default scrapy had support export json file.@Jadian's answer will get you a file with JSON in it, but not quite db like access to it. In order to do this properly from a design stand point I would follow the below instructions. You don't have to use mongo either there are plenty of other nosql dbs available that use JSON.What I would recommend in this situation is that you build out the items properly using scrapy.Item() classes. Then you can use json.dumps into mongoDB. You will need to assign a PK to each item, but mongo is basically made to be a non relational json store. So what you would do is then create an item pipeline which checks for the PK of the item and if its found and no details are changed then raise DropItem() else update/store new data into the mongodb. You could even pipe into the json exporter if you wanted to probably, but I think just dumping the python object to json into mongo is the way to go and then mongo will present you with json to work with on the front end.I hope that you understand this answer, but I think from a design point this will be a much easier solution since mongo is basically a non relational data store based on JSON, and you will be dividing your item pipeline logic into its own area instead of cluttering your spider with it.I would provide a code sample, but most of mine are using ORM for SQL db. Mongo is actually easier to use than this...

Scraping a website that requires multiple requests to access a specific table (connected drop down menus)

Glenn G.

[Scraping a website that requires multiple requests to access a specific table (connected drop down menus)](https://stackoverflow.com/questions/55997329/scraping-a-website-that-requires-multiple-requests-to-access-a-specific-table-c)

I'm trying to scrape data from this website https://pigeon-ndb.com/races/. At first, I thought the problem would be easy to solve if I figured out how to select elements from a drop-down menu, but it's ending up being more complicated than anticipated. I ideally want to iterate through all the years & seasons (2010-2019) and then through all the records of all the organizations and the races. In summary, scrape the data from all the tables located in the website using scrapy (no selenium). I know that the problem involves utilizing the GET requests for the drop-down menus (3 total) like so:The following code is a basic outline of the scrapy spider I plan to use, subject to change:Since it's a GET request, I'm expecting to use this multiple times (or some variation):I think I'll need to incorporate json for the dynamic parts of the scraping process, but not sure if it's the best approachIn scrapy shell:This is the json output for the first request ( https://pigeon-ndb.com/api/?request=get_databases):{'data': [{'year': '2010', 'season': 'OB'}, {'year': '2010', 'season': 'YB'}, {'year': '2011', 'season': 'OB'}, {'year': '2011', 'season': 'YB'}, {'year': '2012', 'season': 'OB'}, {'year': '2012', 'season': 'YB'}, {'year': '2013', 'season': 'OB'}, {'year': '2013', 'season': 'YB'}, {'year': '2014', 'season': 'OB'}, {'year': '2014', 'season': 'YB'}, {'year': '2015', 'season': 'OB'}, {'year': '2015', 'season': 'YB'}, {'year': '2016', 'season': 'OB'}, {'year': '2016', 'season': 'YB'}, {'year': '2017', 'season': 'OB'}, {'year': '2017', 'season': 'YB'}, {'year': '2018', 'season': 'OB'}, {'year': '2018', 'season': 'YB'}, {'year': '2019', 'season': 'OB'}], 'jsonapi': {'version': 2.2, 'db': 'pigeon-ndb'}, 'meta': {'copyright': 'Copyright 2019 Craig Vander Galien', 'authors': ['Craig Vander Galien']}}

I'm still learning scrapy so would appreciate example code on how to approach this problem. Thanks!Edit:So I tried implementing the following code but I'm running into errors:When running spider (error):

2019-05-05 23:42:15Z

I'm trying to scrape data from this website https://pigeon-ndb.com/races/. At first, I thought the problem would be easy to solve if I figured out how to select elements from a drop-down menu, but it's ending up being more complicated than anticipated. I ideally want to iterate through all the years & seasons (2010-2019) and then through all the records of all the organizations and the races. In summary, scrape the data from all the tables located in the website using scrapy (no selenium). I know that the problem involves utilizing the GET requests for the drop-down menus (3 total) like so:The following code is a basic outline of the scrapy spider I plan to use, subject to change:Since it's a GET request, I'm expecting to use this multiple times (or some variation):I think I'll need to incorporate json for the dynamic parts of the scraping process, but not sure if it's the best approachIn scrapy shell:This is the json output for the first request ( https://pigeon-ndb.com/api/?request=get_databases):{'data': [{'year': '2010', 'season': 'OB'}, {'year': '2010', 'season': 'YB'}, {'year': '2011', 'season': 'OB'}, {'year': '2011', 'season': 'YB'}, {'year': '2012', 'season': 'OB'}, {'year': '2012', 'season': 'YB'}, {'year': '2013', 'season': 'OB'}, {'year': '2013', 'season': 'YB'}, {'year': '2014', 'season': 'OB'}, {'year': '2014', 'season': 'YB'}, {'year': '2015', 'season': 'OB'}, {'year': '2015', 'season': 'YB'}, {'year': '2016', 'season': 'OB'}, {'year': '2016', 'season': 'YB'}, {'year': '2017', 'season': 'OB'}, {'year': '2017', 'season': 'YB'}, {'year': '2018', 'season': 'OB'}, {'year': '2018', 'season': 'YB'}, {'year': '2019', 'season': 'OB'}], 'jsonapi': {'version': 2.2, 'db': 'pigeon-ndb'}, 'meta': {'copyright': 'Copyright 2019 Craig Vander Galien', 'authors': ['Craig Vander Galien']}}

I'm still learning scrapy so would appreciate example code on how to approach this problem. Thanks!Edit:So I tried implementing the following code but I'm running into errors:When running spider (error):First of all you need to set two params (database and season) using cookies. After that you can iterate over JSON results:Update You're completely ignoring my comments. parse_race_details was not implemented in your code at all!

Is there a way to delay request on recursive link crawling in Scrapy?

maksimKorzh

[Is there a way to delay request on recursive link crawling in Scrapy?](https://stackoverflow.com/questions/56006640/is-there-a-way-to-delay-request-on-recursive-link-crawling-in-scrapy)

I'm scraping news.crunchbase.com with Scrapy. The callback function on following recursive links doesn't fire in case if I follow the actual link encountered, but it works fine if I crawl some test link instead. I assume that the problem is in timing, hence want to delay the recursive request.EDIT: answer from here does set the global delay, but it doesn't adjust the recursive delay. Recursive link crawl is done instantly - just as soon as data has been scraped.

2019-05-06 13:53:05Z

I'm scraping news.crunchbase.com with Scrapy. The callback function on following recursive links doesn't fire in case if I follow the actual link encountered, but it works fine if I crawl some test link instead. I assume that the problem is in timing, hence want to delay the recursive request.EDIT: answer from here does set the global delay, but it doesn't adjust the recursive delay. Recursive link crawl is done instantly - just as soon as data has been scraped.This was actually enough.Child requests are put into request queue and processed after the parent requests. In case if requests are not related to the current domain the DOWNLOAD_DELAY is ignored and request is done instantly.P.S. I just didn't wait until start_requests(self) processes the entire list of url, hence thought I was banned. Hope this helps somebody.

How to remove the repetition of new lines?

Zaryab

[How to remove the repetition of new lines?](https://stackoverflow.com/questions/56067674/how-to-remove-the-repetition-of-new-lines)

When I run this code, it gives me one word and then leave the whole line and this process repeats till the end. I want it to be in a row not in columns.

2019-05-09 21:14:41Z

When I run this code, it gives me one word and then leave the whole line and this process repeats till the end. I want it to be in a row not in columns.Wouldn't str.strip() work? Something like:It may have several prices, in this case you could use:You could also use str.lstrip() or str.rstrip() to remove new line chars only from the beginning or ending of a string

specific treatments through conditions with xpath

how to change

[specific treatments through conditions with xpath](https://stackoverflow.com/questions/56027916/specific-treatments-through-conditions-with-xpath)

I wanna extract an xpath field and manipulate it depending on what is contained in the field, in this case i want my xpath to detect whether or not the field contains a word and if it does to do X treatment otherwise to do an Y treatement and then insert it in my scrapy itemI will try and explain it in a pseudo code alongside my actual xpath codei tried using .extract(), manipulating the field as a string and then inserting it in my item, had a lot of different problems and it was such a messid expect the extracted field to be either ./XXX.XXX.?.XXX if it has profile.php in it or ./XXX.XXX if it doesnt

2019-05-07 17:42:04Z

I wanna extract an xpath field and manipulate it depending on what is contained in the field, in this case i want my xpath to detect whether or not the field contains a word and if it does to do X treatment otherwise to do an Y treatement and then insert it in my scrapy itemI will try and explain it in a pseudo code alongside my actual xpath codei tried using .extract(), manipulating the field as a string and then inserting it in my item, had a lot of different problems and it was such a messid expect the extracted field to be either ./XXX.XXX.?.XXX if it has profile.php in it or ./XXX.XXX if it doesntI asume your are ussing XPath 1.0 because of python. Then, you can output directly the string you want with this expression:Test in http://www.xpathtester.com/xpath/cca4e5a85df20137b923d0b6f06bf6ccDo note: like in C, boolean values are cast to numbers 0 (false()) and 1 (true()), then you might use the extended reals representation (NaN, Inf, -Inf) as argument for the substring() function like the examples given by the spec

How can I parse a scrapy response immediately after yielding a request?

Vinícius Silva

[How can I parse a scrapy response immediately after yielding a request?](https://stackoverflow.com/questions/56028536/how-can-i-parse-a-scrapy-response-immediately-after-yielding-a-request)

Let's say that I am trying to scrape a website that is designed so that every request that it receives must contain a valid third party string key. Imagine that if you send a request that doesn't contain a valid key, the website will reply with an empty string.

So far, this is what I have: The problem I'm having is rather obvious: parseItem is not called after each Request is yielded, but after ALL the requests were yielded. This is why the first n items are being generated successfully and all the rest are not. After my key starts being rejected by the naughty website, it is never updated and keeps being rejected.What I would like to do is to call parseItem immediately after yielding each Request so that it would be possible to know whether the response was empty and if so, update my credentials. With updated credentials I would have no problems with subsequent requests.

Can someone please help me to accomplish this?

Thanks.

2019-05-07 18:27:47Z

Let's say that I am trying to scrape a website that is designed so that every request that it receives must contain a valid third party string key. Imagine that if you send a request that doesn't contain a valid key, the website will reply with an empty string.

So far, this is what I have: The problem I'm having is rather obvious: parseItem is not called after each Request is yielded, but after ALL the requests were yielded. This is why the first n items are being generated successfully and all the rest are not. After my key starts being rejected by the naughty website, it is never updated and keeps being rejected.What I would like to do is to call parseItem immediately after yielding each Request so that it would be possible to know whether the response was empty and if so, update my credentials. With updated credentials I would have no problems with subsequent requests.

Can someone please help me to accomplish this?

Thanks.Between generating a request and the request being actually sent a long time can pass. Requests are stored on a scheduler and consumed according to settings like CONCURRENT_REQUESTS and CONCURRENT_REQUESTS_PER_DOMAIN.You should not include the keys on your spider. Instead, write a custom downloader middleware that adds a key to your requests right before they are sent out.

Scrapy links queue

Juliano

[Scrapy links queue](https://stackoverflow.com/questions/56012349/scrapy-links-queue)

Is there a way to personalize scrapy links queue? I was thinking about creating a ranking system, so that the links that have higher probability to meet my parsing criteria have priority. I would like to know if this is possible within the scrapy module., and if so where can I find documentation about it (I havent been able to find so far).

2019-05-06 20:57:30Z

Is there a way to personalize scrapy links queue? I was thinking about creating a ranking system, so that the links that have higher probability to meet my parsing criteria have priority. I would like to know if this is possible within the scrapy module., and if so where can I find documentation about it (I havent been able to find so far).

How to do third level scraping using scrapy?

sam rahmn

[How to do third level scraping using scrapy?](https://stackoverflow.com/questions/55998795/how-to-do-third-level-scraping-using-scrapy)

I have built a spider which do second level scraping using scrapy. First it scrape profile urls from search results. Then it scrape some info like web url, phone, contact name from profile url. 

Now, I want to extend this to a third level where it scrape the weburl from profile url and gets the email ids. One challenge here is email id is not located at predictable location on websites. It can be in any pages or section. So the spider should figure out itself the page where email id is located and get the data.How to go ahead with this?

2019-05-06 04:24:56Z

I have built a spider which do second level scraping using scrapy. First it scrape profile urls from search results. Then it scrape some info like web url, phone, contact name from profile url. 

Now, I want to extend this to a third level where it scrape the weburl from profile url and gets the email ids. One challenge here is email id is not located at predictable location on websites. It can be in any pages or section. So the spider should figure out itself the page where email id is located and get the data.How to go ahead with this?

How to run both items in scrapy function?

Zaryab

[How to run both items in scrapy function?](https://stackoverflow.com/questions/56030966/how-to-run-both-items-in-scrapy-function)

Whenever I  use the link of captions and transcription in start_urls variable, it gives me the price of caption in both captions and transcription variable and again give me the price of transcription in both variables. Why and how to solve this issue? 

2019-05-07 21:57:07Z

Whenever I  use the link of captions and transcription in start_urls variable, it gives me the price of caption in both captions and transcription variable and again give me the price of transcription in both variables. Why and how to solve this issue? I suspect that you need another structure of class, sequential:

Can't figure out cause of service_identity module error

Nikhil Jindia

[Can't figure out cause of service_identity module error](https://stackoverflow.com/questions/55958519/cant-figure-out-cause-of-service-identity-module-error)

Error is:UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\Users\techn\Anaconda3\lib\site-packages\service_identity\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.I've tried all other possible answers from several sites but none of them fix my problem. Any insight?I've tried:

2019-05-02 18:49:09Z

Error is:UserWarning: You do not have a working installation of the service_identity module: 'cannot import name 'verify_ip_address' from 'service_identity.pyopenssl' (C:\Users\techn\Anaconda3\lib\site-packages\service_identity\pyopenssl.py)'.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.I've tried all other possible answers from several sites but none of them fix my problem. Any insight?I've tried:Figured it out, I needed to do conda install service_identity

when i save data in the data base i get the id and not the attribute

joes

[when i save data in the data base i get the id and not the attribute](https://stackoverflow.com/questions/56018310/when-i-save-data-in-the-data-base-i-get-the-id-and-not-the-attribute)

I have parent class and some childs class and when i save the data in The data base i receive the Id of the attribute not the content of it .Here's the classes that i have:and here i'm making an instance for each class :and here i'm saving it as json :but when i run it i get in the address something like this :and i want it to be like this :

2019-05-07 08:17:46Z

I have parent class and some childs class and when i save the data in The data base i receive the Id of the attribute not the content of it .Here's the classes that i have:and here i'm making an instance for each class :and here i'm saving it as json :but when i run it i get in the address something like this :and i want it to be like this :You could use dictionaries instead of classes:

Scrapy spider only extracts first table element

Tobi

[Scrapy spider only extracts first table element](https://stackoverflow.com/questions/55966072/scrapy-spider-only-extracts-first-table-element)

I am trying to scrape this URL: 'search.siemens.com/en/?q=iot'. For the begining I am just interested in the titel and the category which is illustrated in the screenshot below. However when I run my spider I get back only the first element: Here is my spider:And here my screenshot:



2019-05-03 08:33:44Z

I am trying to scrape this URL: 'search.siemens.com/en/?q=iot'. For the begining I am just interested in the titel and the category which is illustrated in the screenshot below. However when I run my spider I get back only the first element: Here is my spider:And here my screenshot:

ReplaceWith:Pay attention to dots in front of xpath selector, they mean relative path.UPD: just small tip, also check your allowed_domains value. It should be written in this way: allowed_domains = ['search.siemens.com']UPD2: also problem in main selector in for loop, better be more presice and point on concrete blocks list.

Extracting properties table from the product dt dd with scrapy - getting 'list index out of range'

romankk

[Extracting properties table from the product dt dd with scrapy - getting 'list index out of range'](https://stackoverflow.com/questions/55969217/extracting-properties-table-from-the-product-dt-dd-with-scrapy-getting-list-i)

Here is page I'm trying to scrape

https://www.termex.ru/catalog/protochnye_vodonagrevateli/18445/

I'm having problems with extracting properties of the product.I tested flowing code using Scapy Shell and its working.First value for dt/dd is empty that why I'm forced to use second value-  getall()[1].For some reasons when I run complete spider I get error list index out of range.

Here is my log:Here is my complete spider:

2019-05-03 11:48:25Z

Here is page I'm trying to scrape

https://www.termex.ru/catalog/protochnye_vodonagrevateli/18445/

I'm having problems with extracting properties of the product.I tested flowing code using Scapy Shell and its working.First value for dt/dd is empty that why I'm forced to use second value-  getall()[1].For some reasons when I run complete spider I get error list index out of range.

Here is my log:Here is my complete spider:You have two elements //dl[@class="product-item-detail-properties"] on page. One is empty and second contains list of dt/dd. And now you iterate not by couples, as you wanted, but by this parent list. I'd propose you this solution for dt/dd:

Possible to replace Scrapy's default lxml parser with Beautiful Soup's html5lib parser?

grahama

[Possible to replace Scrapy's default lxml parser with Beautiful Soup's html5lib parser?](https://stackoverflow.com/questions/56030689/possible-to-replace-scrapys-default-lxml-parser-with-beautiful-soups-html5lib)

Question: Is there a way to integrate BeautifulSoup's html5lib parser into a scrapy project--instead of the scrapy's default lxml parser?

Scrapy's parser fails (for some elements) of my scrape pages.

This only happens every 2 out of 20 pages.

As a fix, I've added BeautifulSoup's parser to the project (which works).

That said, I feel like I'm doubling the work with conditionals and multiple parsers...at a certain point, what's the reason for using Scrapy's parser? The code does work....it feels like a hack.

I'm no expert--is there a more elegant way to do this?

Much appreciation in advanceUpdate: Adding a middleware class to scrapy (from the python package scrapy-beautifulsoup) works like a charm. Apparently, lxml from Scrapy is not as robust as BeautifulSoup's lxml. I didn't have to resort to the html5lib parser--which is 30X+ slower.Original:

2019-05-07 21:27:50Z

Question: Is there a way to integrate BeautifulSoup's html5lib parser into a scrapy project--instead of the scrapy's default lxml parser?

Scrapy's parser fails (for some elements) of my scrape pages.

This only happens every 2 out of 20 pages.

As a fix, I've added BeautifulSoup's parser to the project (which works).

That said, I feel like I'm doubling the work with conditionals and multiple parsers...at a certain point, what's the reason for using Scrapy's parser? The code does work....it feels like a hack.

I'm no expert--is there a more elegant way to do this?

Much appreciation in advanceUpdate: Adding a middleware class to scrapy (from the python package scrapy-beautifulsoup) works like a charm. Apparently, lxml from Scrapy is not as robust as BeautifulSoup's lxml. I didn't have to resort to the html5lib parser--which is 30X+ slower.Original:It’s a common feature request for Parsel, Scrapy’s library for XML/HTML scraping.However, you don’t need to wait for such a feature to be implemented. You can fix the HTML code using BeautifulSoup, and use Parsel on the fixed HTML:

Proper FormRequest to AJAX scrolling page

gongarek

[Proper FormRequest to AJAX scrolling page](https://stackoverflow.com/questions/55972088/proper-formrequest-to-ajax-scrolling-page)

I want to scrape all 'belts' from https://www.thingiverse.com/thing:3270948/remixes in Scrapy.First of all I want write proper request. 

I tryied: Response contain only first page(24 belts). How write proper request to get next/whole belts?

2019-05-03 14:40:42Z

I want to scrape all 'belts' from https://www.thingiverse.com/thing:3270948/remixes in Scrapy.First of all I want write proper request. 

I tryied: Response contain only first page(24 belts). How write proper request to get next/whole belts?You have more parameters in request payload, I've copied them all from Network tab:

Scrapy 204 no data received

chaodi tang

[Scrapy 204 no data received](https://stackoverflow.com/questions/55933117/scrapy-204-no-data-received)

When I use Scrapy to crawl web data, it response debug 204 and debug 'no data received!' I have used this code to crawl many data, but it doesn't work this time. I don't know why.The code and feedback are below.The feedback is below:The setting header is as below

2019-05-01 07:49:24Z

When I use Scrapy to crawl web data, it response debug 204 and debug 'no data received!' I have used this code to crawl many data, but it doesn't work this time. I don't know why.The code and feedback are below.The feedback is below:The setting header is as below

How to fix my problem with missing modules when converting .py to .exe with PyInstaller?

Luka Stajkovic

[How to fix my problem with missing modules when converting .py to .exe with PyInstaller?](https://stackoverflow.com/questions/55934798/how-to-fix-my-problem-with-missing-modules-when-converting-py-to-exe-with-pyin)

I started scraping in Python using Scrapy in PyCharm few weeks ago. I found out about PyInstaller, which I used to make executable. The problem is that whenever I run .exe file, it opens and closes instantly.I found out that I get a lot of warnings when creating .exe like this ones bellow:Running .exe file through Command Prompt says that the Scrapy module is missing, tried to fix it by adding --paths parameter to PyInstaller command when creating .exe but didn't work. I already tried reinstalling pyinstaller but no effects.I'd like to be able to run scraper from my PC and on other PC-s also.

2019-05-01 10:25:49Z

I started scraping in Python using Scrapy in PyCharm few weeks ago. I found out about PyInstaller, which I used to make executable. The problem is that whenever I run .exe file, it opens and closes instantly.I found out that I get a lot of warnings when creating .exe like this ones bellow:Running .exe file through Command Prompt says that the Scrapy module is missing, tried to fix it by adding --paths parameter to PyInstaller command when creating .exe but didn't work. I already tried reinstalling pyinstaller but no effects.I'd like to be able to run scraper from my PC and on other PC-s also.if it works on your IDE then try to pack script to single package using below commandIt will include all the required libraries in the package. Also update the libraries which available on machine, that will help you to include missing files/dll's

How to capture “finish_reason” after each crawl

King110

[How to capture “finish_reason” after each crawl](https://stackoverflow.com/questions/55936821/how-to-capture-finish-reason-after-each-crawl)

I'm trying to capture "finish_reason" in scrapy after each crawl and insert this info into a database. The crawl instance is created in a pipeline before first item is collected. It seems like I have to use the "engine_stopped" signal but couldn't find an example on how or where should I put my code to do this?

2019-05-01 13:12:52Z

I'm trying to capture "finish_reason" in scrapy after each crawl and insert this info into a database. The crawl instance is created in a pipeline before first item is collected. It seems like I have to use the "engine_stopped" signal but couldn't find an example on how or where should I put my code to do this?One of possible options is to override scrapy.statscollectors.MemoryStatsCollector (docs,code) and it's close_spider method:middleware.py:Add newly created stats collector class to settings.py:STATS_CLASS = 'project.middlewares.MemoryStatsCollectorSender'

#STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'

Authenticated spider pagination. 302 redirect. reqvalidation.asps - page not found

Gerson Cifuentes

[Authenticated spider pagination. 302 redirect. reqvalidation.asps - page not found](https://stackoverflow.com/questions/55940238/authenticated-spider-pagination-302-redirect-reqvalidation-asps-page-not-fou)

I have a scrapy sider that can log into ancestry.com successfully. I then use that authenticated session to return a new link and can scrape the first page of the new link successfully. The issue happens when I try to go to the second page. I get a 302 redirect debug message, and this url:  https://secure.ancestry.com/error/reqvalidation.aspx?aspxerrorpath=http%3a%2f%2fsearch.ancestry.com%2ferror%2fPageNotFound&msg=&ti=0>.I followed the documentation and have follow some recommendations here to get me this far. Do I need a session token for each page? if so how do I got about doing that?I tired adding some request header information. I tried adding the cookie information to the request header, but that did not work. I've tried using only the USER agents that are listed in the POST packages.Right now I only get 50 results. I should be getting hundreds after crawling all the pages.

2019-05-01 17:34:59Z

I have a scrapy sider that can log into ancestry.com successfully. I then use that authenticated session to return a new link and can scrape the first page of the new link successfully. The issue happens when I try to go to the second page. I get a 302 redirect debug message, and this url:  https://secure.ancestry.com/error/reqvalidation.aspx?aspxerrorpath=http%3a%2f%2fsearch.ancestry.com%2ferror%2fPageNotFound&msg=&ti=0>.I followed the documentation and have follow some recommendations here to get me this far. Do I need a session token for each page? if so how do I got about doing that?I tired adding some request header information. I tried adding the cookie information to the request header, but that did not work. I've tried using only the USER agents that are listed in the POST packages.Right now I only get 50 results. I should be getting hundreds after crawling all the pages.Found the solution. It had nothing to do with the authentication to the website. I needed to find a different way to approach pagination. I resorted to using the page url for pagination instead of following the "next page" button link.

Running a Scrapy Crawler from a Notebook (IPY or Jupiter)

Andres

[Running a Scrapy Crawler from a Notebook (IPY or Jupiter)](https://stackoverflow.com/questions/55940754/running-a-scrapy-crawler-from-a-notebook-ipy-or-jupiter)

I am trying to create a class to take some information from a website, to do that I am using Scrapy. Most of the documents that I have found so far are about running it from a .py file. I would like to initially run the class from the notebook (IPY). There is a similar question in Stackoverflow, but the answer is not right, at least for me.The next code is just a basic application to execute the class inside the notebook:If anyone knows the way to run it from the notebook or maybe knows a source, link, book or whatever I can use to know it.Thks

2019-05-01 18:18:46Z

I am trying to create a class to take some information from a website, to do that I am using Scrapy. Most of the documents that I have found so far are about running it from a .py file. I would like to initially run the class from the notebook (IPY). There is a similar question in Stackoverflow, but the answer is not right, at least for me.The next code is just a basic application to execute the class inside the notebook:If anyone knows the way to run it from the notebook or maybe knows a source, link, book or whatever I can use to know it.Thks

Python BeautifulSoup program initialization

Chris Kotsiopoulos

[Python BeautifulSoup program initialization](https://stackoverflow.com/questions/55953514/python-beautifulsoup-program-initialization)

I'm trying to adjust a Python Scrapy project and run it locally in my PC. The purpose is to study and understand it. I have included start_requests() function in "main" but it is not called. Any help or reference to relevant resources, greatly appreciated.The program compiles without errors, but it just opens a blank browser window. The expected result isto browse through a list of ASIN codes in a .csv and scrape some data from the relevant pages.

2019-05-02 13:24:58Z

I'm trying to adjust a Python Scrapy project and run it locally in my PC. The purpose is to study and understand it. I have included start_requests() function in "main" but it is not called. Any help or reference to relevant resources, greatly appreciated.The program compiles without errors, but it just opens a blank browser window. The expected result isto browse through a list of ASIN codes in a .csv and scrape some data from the relevant pages.The return statement in get_title in poorly indented.A linter should help you out writing good python, try out pylint for example.You should indent start_requests() more - it's currently a function rather than a method within the AmazonbotSpider class.I quit trying to adjust the above code. Instead I used this script as a basis and I just added the methods I needed. Using Selenium is the main difference:

Scrapy Output File is Blank

user6680

[Scrapy Output File is Blank](https://stackoverflow.com/questions/55944102/scrapy-output-file-is-blank)

I created a scrapy spider, but when I run the command it runs, but the output report is blank. I know the xpath is correct so I'm not really sure. Still really new to scrapy. Any help is appreciated

2019-05-01 23:59:48Z

I created a scrapy spider, but when I run the command it runs, but the output report is blank. I know the xpath is correct so I'm not really sure. Still really new to scrapy. Any help is appreciatedBing knows that you aren't using a regular browser therefore do experiment with headers.Try these settings in settings.py {scrapy only}:Your code is not yielding any data.You need to yield dictionaries or instances of a subclass of Scrapy’s Item class containing the extracted data for that data to reach the output file.See the corresponding section from the Scrapy tutorial.

Python Scrapy Splash doesn't render website, stuck at loading screen

Chris1309

[Python Scrapy Splash doesn't render website, stuck at loading screen](https://stackoverflow.com/questions/55977056/python-scrapy-splash-doesnt-render-website-stuck-at-loading-screen)

I would like to render the following website with Scrapy Splash.https://m.mobilebet.com/en/sports/football/england-premier-league/Unfortunately, Splash always gets stuck at the loading screen:I have already tried using a long waiting time (up to 60 seconds) with no results. My Splash version is 3.3.1 and obey robots.txt has been set to false.Thanks!

2019-05-03 21:01:04Z

I would like to render the following website with Scrapy Splash.https://m.mobilebet.com/en/sports/football/england-premier-league/Unfortunately, Splash always gets stuck at the loading screen:I have already tried using a long waiting time (up to 60 seconds) with no results. My Splash version is 3.3.1 and obey robots.txt has been set to false.Thanks!I don't think it'll be possible - this website needs JS to be rendered. So you'll need to use something like Selenium to scrape information from it.

Also, perhaps what you are looking for is an API for that information - since scraping it from a website can be very inefficient. Try googling "sports REST API" - look for one with Python SDK.Ok, so Splash is supposed to render the JS for you it seems. But I wouldn't rely on it too much - those websites constantly change and they are developed against latest browsers, your best bet is to use Selenium with Chromium driver (though using API is much more preferable).

Want to use Scrapy to scrape a website but not sure if there is a way around javascript

drizzle123

[Want to use Scrapy to scrape a website but not sure if there is a way around javascript](https://stackoverflow.com/questions/55911584/want-to-use-scrapy-to-scrape-a-website-but-not-sure-if-there-is-a-way-around-jav)

I have a list of schools IDs for schools in NYC. I want to collect publicly available budget data for each of those schools. The budget data is available from this website:https://www.nycenet.edu/offices/d_chanc_oper/budget/dbor/galaxy/galaxybudgetsummaryto/default.aspxI am using Python for this task and for browser automation I know scrapy is much faster than selenium. The issue is, though, I need to interact with the page. Namely, I have to enter the school ID as well as the year of interest (eventually I want to collect the budget data for each year). This interaction (as far I can tell) then invokes Javascript to get the new data.I'm hoping there is some way around this so I can use scrapy and not selenium since selenium is slow. Some progress I have I made is that I found URLs of the following type (I don't remember how I found this out sadly):https://www.nycenet.edu/offices/d_chanc_oper/budget/dbor/galaxy/galaxybudgetsummaryto/default.aspx?DDBSSS_INPUT=M015can bring you directly to the data for the school (without having to interact with forms or Javascript I think). Unfortunately, I have only been able to get this method to work for the year 2019.Would somebody be able to find a way to structure the URL so as to be able to specify the year? Or perhaps somebody can let me know if this isn't possible?

2019-04-29 22:23:43Z

I have a list of schools IDs for schools in NYC. I want to collect publicly available budget data for each of those schools. The budget data is available from this website:https://www.nycenet.edu/offices/d_chanc_oper/budget/dbor/galaxy/galaxybudgetsummaryto/default.aspxI am using Python for this task and for browser automation I know scrapy is much faster than selenium. The issue is, though, I need to interact with the page. Namely, I have to enter the school ID as well as the year of interest (eventually I want to collect the budget data for each year). This interaction (as far I can tell) then invokes Javascript to get the new data.I'm hoping there is some way around this so I can use scrapy and not selenium since selenium is slow. Some progress I have I made is that I found URLs of the following type (I don't remember how I found this out sadly):https://www.nycenet.edu/offices/d_chanc_oper/budget/dbor/galaxy/galaxybudgetsummaryto/default.aspx?DDBSSS_INPUT=M015can bring you directly to the data for the school (without having to interact with forms or Javascript I think). Unfortunately, I have only been able to get this method to work for the year 2019.Would somebody be able to find a way to structure the URL so as to be able to specify the year? Or perhaps somebody can let me know if this isn't possible?If you inspect the network tab of your browser's dev tools you will notice that it is all about pretty standard post request. You can reproduce it with Scrapy's FormRequest.

XPath : How to get text between 2 html tags with same level?

Deyesta

[XPath : How to get text between 2 html tags with same level?](https://stackoverflow.com/questions/55920834/xpath-how-to-get-text-between-2-html-tags-with-same-level)

I'm new to xpath and I'm working with scrapy to get text from different html pages that are generated.

I get the {id} of a header tag from the user (<h1|2|.. id="title-{id}">text</h1|2|3..>). I need to get text from all html tags between this header and the next header of same level. So if the header is h1 I need to get all text of all tags until next h1 header.

All headers ids have same pattern "title-{id}" where {id} is generated.

To make it more clear here is an example :NOTE : I don't know what header it might be. It could be any of the html header tags from <h1> to <h6> UPDATE :

While trying few things around I noticed that I'm not sure if the next header is of same level or even exists. Since the headers are used as titles and sub-titles. The given id may be of last sub-title hence I'll have a header of higher level after or even be the last of the page. So basicaly I only have the id of the header and I need to get all text of the "paragraph".  Work Around : 

I found a kindof workaround solution : 

I do it in 3 steps :

First, I use //*[@id='title-{id}] which allows me to get the full line with the tag so now I know which tag header it is.

Second, I use //*[id='title-{id}]/following-sibling::* this allows to look for next header of same or higher level {myHeader}.

Last, I use //*[id='title-{id}]/following-sibling::* and //{myHeader}//preceding-sibling::* to get what's between or go 'till the end of page if no header found.

2019-04-30 12:18:28Z

I'm new to xpath and I'm working with scrapy to get text from different html pages that are generated.

I get the {id} of a header tag from the user (<h1|2|.. id="title-{id}">text</h1|2|3..>). I need to get text from all html tags between this header and the next header of same level. So if the header is h1 I need to get all text of all tags until next h1 header.

All headers ids have same pattern "title-{id}" where {id} is generated.

To make it more clear here is an example :NOTE : I don't know what header it might be. It could be any of the html header tags from <h1> to <h6> UPDATE :

While trying few things around I noticed that I'm not sure if the next header is of same level or even exists. Since the headers are used as titles and sub-titles. The given id may be of last sub-title hence I'll have a header of higher level after or even be the last of the page. So basicaly I only have the id of the header and I need to get all text of the "paragraph".  Work Around : 

I found a kindof workaround solution : 

I do it in 3 steps :

First, I use //*[@id='title-{id}] which allows me to get the full line with the tag so now I know which tag header it is.

Second, I use //*[id='title-{id}]/following-sibling::* this allows to look for next header of same or higher level {myHeader}.

Last, I use //*[id='title-{id}]/following-sibling::* and //{myHeader}//preceding-sibling::* to get what's between or go 'till the end of page if no header found.Here is the xpath to get all the elements between h2 tags.Here is the sample html I used to simulate the scenario. (update the id to check different options shown in the below).output if User input: {id1}

output if user input: {id4}

output if user input: {id3}

Note: This xpath is designed to suite the original post scenario.Because predicates in XPath filter the context node list you can't perform a join selection unless you are able to reintroduce target values from a relative context of your source values. Example selecting all the elements with the same name as that having specific id attribute:Now, for the in "between marks problem" use as usually the Kaysian method for intersection:Test in http://www.xpathtester.com/xpath/0dcfdf59dccb8faf3705c22167ae45f1 This is what worked for me :  

For this keep in mind that I'm using scrapy with python-2.7 :   This should work in general I tested it against multiple headers wih different levels it works fine for me.

cannot import name 'opentype' when to run spider

it_is_a_literature

[cannot import name 'opentype' when to run spider](https://stackoverflow.com/questions/55944068/cannot-import-name-opentype-when-to-run-spider)

Everytime i run a spider ,the following info pop up in the first ,before all other info.Now i download service_identity-18.1.0-py2.py3-none-any.whl and install it.Rerun the spider again,i still get the warning info  'cannot import name 'opentype',what is the matter with my scrapy?

2019-05-01 23:54:21Z

Everytime i run a spider ,the following info pop up in the first ,before all other info.Now i download service_identity-18.1.0-py2.py3-none-any.whl and install it.Rerun the spider again,i still get the warning info  'cannot import name 'opentype',what is the matter with my scrapy?

How to strip Scrapy results before outputted to CSV

Timpo

[How to strip Scrapy results before outputted to CSV](https://stackoverflow.com/questions/55922619/how-to-strip-scrapy-results-before-outputted-to-csv)

I am trying to isolate the integer from some HTML, e.g. "

                            5,500 miles

                        ".My code successfully removes the comma and "miles" but in my CSV output I get unwanted blank rows in this column which I presume are due to the carriage returns in the original source. My CSV looks like this:my CSVSo the title and price columns are fine. But the Mileage column is where the error is.is there something wrong with my Strip command?

2019-04-30 14:00:31Z

I am trying to isolate the integer from some HTML, e.g. "

                            5,500 miles

                        ".My code successfully removes the comma and "miles" but in my CSV output I get unwanted blank rows in this column which I presume are due to the carriage returns in the original source. My CSV looks like this:my CSVSo the title and price columns are fine. But the Mileage column is where the error is.is there something wrong with my Strip command?Just change the XPath for mileagefromto You will get output correct output:

Why does this code generate multiple files? I want 1 file with all entries in it

CubanGT

[Why does this code generate multiple files? I want 1 file with all entries in it](https://stackoverflow.com/questions/55925202/why-does-this-code-generate-multiple-files-i-want-1-file-with-all-entries-in-it)

Im trying to work with both beautifulsoup and xpath and was trying to using the following code, but now im getting 1 file per URL instead of before where i was getting 1 file for all the URLSI just moved over the reading from CSV to get the list of urls and also just added the parsing of the url and response.. but when i run this now i get alot of individual files and in some cases 1 file may actually contain 2 scraped pages data.. so do i need to move my file saving out (indent)Id like to fix this code as it works right now to save all the scraped data into 1 file as it was originally.

2019-04-30 16:27:59Z

Im trying to work with both beautifulsoup and xpath and was trying to using the following code, but now im getting 1 file per URL instead of before where i was getting 1 file for all the URLSI just moved over the reading from CSV to get the list of urls and also just added the parsing of the url and response.. but when i run this now i get alot of individual files and in some cases 1 file may actually contain 2 scraped pages data.. so do i need to move my file saving out (indent)Id like to fix this code as it works right now to save all the scraped data into 1 file as it was originally.You create a new filename with timestamp for each run:filename = 'dsg-%s.csv' % str(int(now))Just replace it with:filename = 'dsg.csv'

concatenate text from nested siblings if available with the text in the parent node

Priya 

[concatenate text from nested siblings if available with the text in the parent node](https://stackoverflow.com/questions/55944456/concatenate-text-from-nested-siblings-if-available-with-the-text-in-the-parent-n)

I am trying to extract the text from siblings if available and concatenate with the text in parent node. How to do this in xpath?

My HTML shown below has few instances of <sup> and <sub>.my expected output: Should concatenate like this ['<sup>'+'/'+ '<sub>']I tried with the below commands and referred multiple Scrapy documentations. but couldn't able to extract the required info. 

2019-05-02 01:04:51Z

I am trying to extract the text from siblings if available and concatenate with the text in parent node. How to do this in xpath?

My HTML shown below has few instances of <sup> and <sub>.my expected output: Should concatenate like this ['<sup>'+'/'+ '<sub>']I tried with the below commands and referred multiple Scrapy documentations. but couldn't able to extract the required info. My idea is to iterate by span.qty, extract text from there and concatenate it. Like here:Try Bs4 for such tasks:

Selecting elements for scraping data from two connected drop down menus, <option disabled>

Glenn G.

[Selecting elements for scraping data from two connected drop down menus, <option disabled>](https://stackoverflow.com/questions/55926662/selecting-elements-for-scraping-data-from-two-connected-drop-down-menus-option)

I'm trying to get to the data from this website (https://pigeon-ndb.com/races/). The data is accessed by clicking one option value in one drop-down menu ("Choose an Organization") and then clicking another value from a subsequent drop-down menu ("Choose a Race") that fills with options according to the value clicked in the previous drop-down menu. The goal is to get to the table of data values after going through the two drop-down menus and scrape them with scrapy. I've already tried to grab the option values in the first drop-down menu ("Choose an Organization") using this xpath. I expected values from all the options in the drop down menu (more than 1) but only got 1 option value that is not useful. I'd like to avoid using Selenium to click through the options (too slow). Would appreciate a scrapy solution. Thanks!

2019-04-30 18:17:09Z

I'm trying to get to the data from this website (https://pigeon-ndb.com/races/). The data is accessed by clicking one option value in one drop-down menu ("Choose an Organization") and then clicking another value from a subsequent drop-down menu ("Choose a Race") that fills with options according to the value clicked in the previous drop-down menu. The goal is to get to the table of data values after going through the two drop-down menus and scrape them with scrapy. I've already tried to grab the option values in the first drop-down menu ("Choose an Organization") using this xpath. I expected values from all the options in the drop down menu (more than 1) but only got 1 option value that is not useful. I'd like to avoid using Selenium to click through the options (too slow). Would appreciate a scrapy solution. Thanks!If you check carefully the requests sent you will notice two GET requests being sent among othershttps://pigeon-ndb.com/api/?request=get_organizations&database=2019%20OB&_=1556648619801and https://pigeon-ndb.com/api/?request=get_races&organization=AMARILLO%20RACING%20PIGEON%20CLUB&orgNum=null&_=1556648619803they will return the organisations and races as json. it's up to you to construct the second one using every organisation from the first oneEDIT: Note you need to send the database in Cookies headerEDIT2:this will print the races for first two organizations.

Further more you can supply _ as param - that is timestamp from Epochalso for race details look athttps://pigeon-ndb.com/api/?request=get_race_details&racename=BIG%20SPRING&date=03%2F23%2F2019&time=1556501306here time is mandatory to supply

scrapy python CrawlSpider not crawling

AlbertWolfgang

[scrapy python CrawlSpider not crawling](https://stackoverflow.com/questions/55825722/scrapy-python-crawlspider-not-crawling)

I'm attempting to crawl a website. For an example of my code, I'm just extracting all links and printing them out to the terminal.This process works great for the urls in the start_urls, but it doesn't seem that the spider will crawl the extracted urls.This is the point of the CrawlSpider, correct? visit a page, collect its links and visit all those links until it runs out of them?I've been stuck for a few days, any help would be great. 

2019-04-24 08:35:36Z

I'm attempting to crawl a website. For an example of my code, I'm just extracting all links and printing them out to the terminal.This process works great for the urls in the start_urls, but it doesn't seem that the spider will crawl the extracted urls.This is the point of the CrawlSpider, correct? visit a page, collect its links and visit all those links until it runs out of them?I've been stuck for a few days, any help would be great. The problem is that you name your method parse. As per the documentation, this name should be avoided in case of using CrawlSpider as it leads to problems. Just rename the method to e.g. parse_link (and adjust the callback argument in Rule) and it will work.Also, remember that allowed_domains attribute must match with URLs you intend to crawl.

Is there any way to loop Scrapy over different formdata?

Amar Srivastava

[Is there any way to loop Scrapy over different formdata?](https://stackoverflow.com/questions/55802166/is-there-any-way-to-loop-scrapy-over-different-formdata)

I work for a logistics company and part of what we're trying to automate is staying on top of tracking statuses for various shipments.I have developed a web scraper that works beautifully for taking an excel sheet that contains tracking numbers, and using those numbers to scrape tracking statuses from a carrier's website AS LONG AS THE LIST OF NUMBERS IS LESS THAN 10. This is because the carrier's website only allows for 10 tracking numbers at a time.However, in our plan to automate our entire tracking division, we will need to deal with lists much longer than 10 numbers. The primary problem I'm running into is that Scrapy requires me to restart the kernel in Jupyter notebook each time I want to run it, and this prevents me from chunking the list of tracking numbers into individual lists of ten. Is there a way to loop Scrapy from within? Up to this point I've managed to chunk the list of tracking numbers into lists of ten, and I've managed to run Scrapy with individual lists of ten or less tracking numbers and it works beautifully. I expect the output of scraped_data to be a list of thirteen statuses, but instead I get [].

2019-04-22 23:04:34Z

I work for a logistics company and part of what we're trying to automate is staying on top of tracking statuses for various shipments.I have developed a web scraper that works beautifully for taking an excel sheet that contains tracking numbers, and using those numbers to scrape tracking statuses from a carrier's website AS LONG AS THE LIST OF NUMBERS IS LESS THAN 10. This is because the carrier's website only allows for 10 tracking numbers at a time.However, in our plan to automate our entire tracking division, we will need to deal with lists much longer than 10 numbers. The primary problem I'm running into is that Scrapy requires me to restart the kernel in Jupyter notebook each time I want to run it, and this prevents me from chunking the list of tracking numbers into individual lists of ten. Is there a way to loop Scrapy from within? Up to this point I've managed to chunk the list of tracking numbers into lists of ten, and I've managed to run Scrapy with individual lists of ten or less tracking numbers and it works beautifully. I expect the output of scraped_data to be a list of thirteen statuses, but instead I get [].You can make use of the def closed(self, reason): hook. In there you can restart: 

How to select all nodes beetwen certain headers?

thiagoheron

[How to select all nodes beetwen certain headers?](https://stackoverflow.com/questions/55928020/how-to-select-all-nodes-beetwen-certain-headers)

Each <header> tag contains a Title of Conference.

Each <ul> tag contains the links of this conference.When I'll to try to crawl the website, I'm try to associating the <header> tag with yours links in <ul> tags. But I don't know how I can only select the <ul> tags of are sibling two certain <headers>.HTML:Example: Some cases:My code:Problem of My CodeI got it testing with JQuery on Console of Google Chrome, example: But How I can select this using xPath or CSS Selector? Thank you!

2019-04-30 20:07:34Z

Each <header> tag contains a Title of Conference.

Each <ul> tag contains the links of this conference.When I'll to try to crawl the website, I'm try to associating the <header> tag with yours links in <ul> tags. But I don't know how I can only select the <ul> tags of are sibling two certain <headers>.HTML:Example: Some cases:My code:Problem of My CodeI got it testing with JQuery on Console of Google Chrome, example: But How I can select this using xPath or CSS Selector? Thank you!Following combination of css selectors and python for loop can solve this task.Output is:

{'h1': ['p2'], 'h2': ['p3'], 'h3': ['p4', 'p5'], 'h4': ['p6', 'p7'], 'h5': ['p8']}This css selector: tags = response.css("header, ul") returns list of <header> and <ul> tags in the same order as in the html code.After that we can iterate through received tags using for loop and select required data.Try to use following-sibling like here:So with //header/following-sibling::*[not(self::header)] we choose all header siblings, but not header.This may be what you're looking for. Note I added a <ul>before the first and after the last <header>..</header> sets. This expressionshould select all the <ul> tags, except those I added before and after, and none of the <p> tags which may be in the way.

Privoxy 'client-header-tags' not working with HTTPS sites through Scrapy

joel

[Privoxy 'client-header-tags' not working with HTTPS sites through Scrapy](https://stackoverflow.com/questions/55923500/privoxy-client-header-tags-not-working-with-https-sites-through-scrapy)

Privoxy seems to be unable to tag HTTPS headers (i.e. by using a 'CLIENT-HEADER-TAGGER') when requesting HTTPS pages with Scrapy. However, there is no issue with header-taggers when:My reasons for wanting to do this: I ultimately want to be able to use multiple instances of Tor with Scrapy (by making use of the Python STEM and/or 'torrequest' modules). However I don't want to also launch one instance of Privoxy per Tor instance (if I can help it). I believe a more efficient way is to include an HTTP header in a Scrapy spider that identifies the Tor port to be used for that particular request. Privoxy would employ CLIENT-HEADER-TAGGERs to identify the specified port number and forward to the appropriate Tor SOCKS port. This way only one instance of Privoxy should ever be required.This works fine if using Firefox as a client. For now I'm not worrying about multiple Tor instances and ports, I just need to know if I'm correctly using header-tagging. So, I'm using Privoxy's pre-existing user-agent tag - e.g.:Firefox -> Privoxy filters for user-agent "Mozilla" -> forwards to Tor -> site 'https://check.torproject.org/' displays 'Congratulations' with tor IP.So, I know that I have setup tagging correctly in Privoxy. To be sure of this I intentionally misspell the user-agent in Privoxy's action file as "MMozilla". The forward-override no longer engages and Privoxy instead falls back to forwarding through Burp Suite on ':8080' and the Tor check site shows my regular IP.Now, switching to Scrapy (and correcting the user-agent tag to match "Mozilla" again), I request 'https://check.torproject.org/'. The page is returned correctly (so obviously no general problem fetching HTTPS pages), however it shows my regular IP (not Tor IP). Interestingly, if I ask Scrapy to request 'http://ip-check.info/' I do get a Tor IP (along with a caveat "An unknown IP address, possibly your own, was uncovered by HTTPS).I've scanned the Scrapy documentation for any seemingly relevant details with respect to downloader middleware and the like, but didn't find anything (and I'm not sure if that aspect even applies here since I'm running a spider from a script outside of any Scrapy project). Privoxy's 'default.filter' file:Privoxy's 'user.action' file:Privoxy config file does not forward to Tor. The only change I made is for fallback forwarding to my Burp Suite proxy (trailing period omitted):Scrapy spider (run as a script, not from within a Scrapy project):macOS 10.14.4, Scrapy 1.5.2 via Anaconda 3, Privoxy and Tor installed/updated via HomebrewI'm expecting Privoxy to be able to filter HTTPS client-header-tags from Scrapy, just as it can from Firefox. Although, I can see that it's correctly filtering them if I get Scrapy to request a page over HTTP. Does Scrapy fetch pages over HTTPS in a different manner to browsers, making its headers unreadable in transit? If so, is there any feasible workaround to achieve what I want?

2019-04-30 14:47:50Z

Privoxy seems to be unable to tag HTTPS headers (i.e. by using a 'CLIENT-HEADER-TAGGER') when requesting HTTPS pages with Scrapy. However, there is no issue with header-taggers when:My reasons for wanting to do this: I ultimately want to be able to use multiple instances of Tor with Scrapy (by making use of the Python STEM and/or 'torrequest' modules). However I don't want to also launch one instance of Privoxy per Tor instance (if I can help it). I believe a more efficient way is to include an HTTP header in a Scrapy spider that identifies the Tor port to be used for that particular request. Privoxy would employ CLIENT-HEADER-TAGGERs to identify the specified port number and forward to the appropriate Tor SOCKS port. This way only one instance of Privoxy should ever be required.This works fine if using Firefox as a client. For now I'm not worrying about multiple Tor instances and ports, I just need to know if I'm correctly using header-tagging. So, I'm using Privoxy's pre-existing user-agent tag - e.g.:Firefox -> Privoxy filters for user-agent "Mozilla" -> forwards to Tor -> site 'https://check.torproject.org/' displays 'Congratulations' with tor IP.So, I know that I have setup tagging correctly in Privoxy. To be sure of this I intentionally misspell the user-agent in Privoxy's action file as "MMozilla". The forward-override no longer engages and Privoxy instead falls back to forwarding through Burp Suite on ':8080' and the Tor check site shows my regular IP.Now, switching to Scrapy (and correcting the user-agent tag to match "Mozilla" again), I request 'https://check.torproject.org/'. The page is returned correctly (so obviously no general problem fetching HTTPS pages), however it shows my regular IP (not Tor IP). Interestingly, if I ask Scrapy to request 'http://ip-check.info/' I do get a Tor IP (along with a caveat "An unknown IP address, possibly your own, was uncovered by HTTPS).I've scanned the Scrapy documentation for any seemingly relevant details with respect to downloader middleware and the like, but didn't find anything (and I'm not sure if that aspect even applies here since I'm running a spider from a script outside of any Scrapy project). Privoxy's 'default.filter' file:Privoxy's 'user.action' file:Privoxy config file does not forward to Tor. The only change I made is for fallback forwarding to my Burp Suite proxy (trailing period omitted):Scrapy spider (run as a script, not from within a Scrapy project):macOS 10.14.4, Scrapy 1.5.2 via Anaconda 3, Privoxy and Tor installed/updated via HomebrewI'm expecting Privoxy to be able to filter HTTPS client-header-tags from Scrapy, just as it can from Firefox. Although, I can see that it's correctly filtering them if I get Scrapy to request a page over HTTP. Does Scrapy fetch pages over HTTPS in a different manner to browsers, making its headers unreadable in transit? If so, is there any feasible workaround to achieve what I want?

Scrapy: How to use scraped item as variable for dynamic URL

Christian Read

[Scrapy: How to use scraped item as variable for dynamic URL](https://stackoverflow.com/questions/55825235/scrapy-how-to-use-scraped-item-as-variable-for-dynamic-url)

I would like to start the scraping on the last number of pagination. From Highest page to lowesthttps://teslamotorsclub.com/tmc/threads/tesla-tsla-the-investment-world-the-2019-investors-roundtable.139047/page-page-2267 is dynamic so I need to scrape the item first before I determine the last-page number, and then the url pagination should go like this page-2267 , page-2266...here's what I have done I need to scrape items from highest page to lowest.

Please help me thank you

2019-04-24 08:09:16Z

I would like to start the scraping on the last number of pagination. From Highest page to lowesthttps://teslamotorsclub.com/tmc/threads/tesla-tsla-the-investment-world-the-2019-investors-roundtable.139047/page-page-2267 is dynamic so I need to scrape the item first before I determine the last-page number, and then the url pagination should go like this page-2267 , page-2266...here's what I have done I need to scrape items from highest page to lowest.

Please help me thank youIn case of last page to first page, try the following:You have very good element on your page link[rel=next]. So you can refactor your code in this way: parse page, call next, parse page, call next, etc.UPD: here is code that scrapes data from last page to first:I use the next algorithm to resolve it:Start on First page. Load first page, Do your stuff, at the end check if that XPATH is present on HTML and page+=1.

Insert no. of scraped item using Scrapy

Christian Read

[Insert no. of scraped item using Scrapy](https://stackoverflow.com/questions/55744027/insert-no-of-scraped-item-using-scrapy)

I want to get the total number of the scraped item and the date when script run and insert it inside Mysql, I put the code inside Pipeline and it seems that the insertion of data is in the loop, where can I properly put those data I wanted it to put when scraping is done.Any idea please? here's my codeError:

mysql.connector.errors.IntegrityError: 1062 (23000): Duplicate entry '' for key 'PRIMARY'So probably I am puting my code on wrong place. 

Please help thank you

2019-04-18 10:23:41Z

I want to get the total number of the scraped item and the date when script run and insert it inside Mysql, I put the code inside Pipeline and it seems that the insertion of data is in the loop, where can I properly put those data I wanted it to put when scraping is done.Any idea please? here's my codeError:

mysql.connector.errors.IntegrityError: 1062 (23000): Duplicate entry '' for key 'PRIMARY'So probably I am puting my code on wrong place. 

Please help thank youScrapy pipeline's purpose is to process single item at a time. However, you can achieve what you want by putting the logic in the close_spider method. You can get the total number of items scraped from Scrapy stats under the key item_scraped_count. See the example:To provide complete info, you can achieve your goal also by connecting to the signal spider_closed from pipeline, extension or from the spider itself. See this complete example connecting to the signal from the spider:

Multiple conditions in CSS-Selector returning empty value

Dan

[Multiple conditions in CSS-Selector returning empty value](https://stackoverflow.com/questions/55770128/multiple-conditions-in-css-selector-returning-empty-value)

I am trying to the select the content of a meta tag where a specific language is set like this:If I use the following css-selector, I am able to extract only meta tags in English:However, since the website has other meta tags like <meta name="DC.Description" xml:lang="en" content="Description English"> and <meta name="DC.Description" xml:lang="fr" content="Description French">, I need a second condition in order to separate description from subject. Usually it would work like this:But unfortunately this will return an empty value. I have no idea why multiple conditions don't work in this case. Any ideas on how to solve this issue?

2019-04-20 03:59:19Z

I am trying to the select the content of a meta tag where a specific language is set like this:If I use the following css-selector, I am able to extract only meta tags in English:However, since the website has other meta tags like <meta name="DC.Description" xml:lang="en" content="Description English"> and <meta name="DC.Description" xml:lang="fr" content="Description French">, I need a second condition in order to separate description from subject. Usually it would work like this:But unfortunately this will return an empty value. I have no idea why multiple conditions don't work in this case. Any ideas on how to solve this issue?

Getting spider on Scrapy Cloud to store files on Google Cloud Storage using GCSFilesStore and getting ImportError

markkazanski

[Getting spider on Scrapy Cloud to store files on Google Cloud Storage using GCSFilesStore and getting ImportError](https://stackoverflow.com/questions/55768694/getting-spider-on-scrapy-cloud-to-store-files-on-google-cloud-storage-using-gcsf)

Deploying a spider to Scraping Cloud. It gathers download links for files and should save those files in a Google Cloud bucket. It works when running locally. But when deploying to Scraping Hub it return the following errors: It runs locally and is able to upload files to the GCloud bucket. There's a service account created just for the spider. I exported the JSON credentials file and pasted it directly into the credentials object. In the YML file I specified the requirements file. In the requirements.txt file I specified all the packages that are required.At this point I'm not sure why it can't import GCSFilesStore on Scrapy Cloud when it imports FilesPipeline fine.Here is my code: pipelines.pysettings.pyscrapinghub.ymlrequirements.txt

2019-04-19 22:57:09Z

Deploying a spider to Scraping Cloud. It gathers download links for files and should save those files in a Google Cloud bucket. It works when running locally. But when deploying to Scraping Hub it return the following errors: It runs locally and is able to upload files to the GCloud bucket. There's a service account created just for the spider. I exported the JSON credentials file and pasted it directly into the credentials object. In the YML file I specified the requirements file. In the requirements.txt file I specified all the packages that are required.At this point I'm not sure why it can't import GCSFilesStore on Scrapy Cloud when it imports FilesPipeline fine.Here is my code: pipelines.pysettings.pyscrapinghub.ymlrequirements.txtYou need to add the scrapy dependency to your requirements.txt:

How do I grab the headline titles from the Google News webpage with Scrapy?

user5451844

[How do I grab the headline titles from the Google News webpage with Scrapy?](https://stackoverflow.com/questions/55742903/how-do-i-grab-the-headline-titles-from-the-google-news-webpage-with-scrapy)

I saved an offline file of https://news.google.com/search?q=amazon&hl=en-US&gl=US&ceid=US%3AenHaving trouble determining how to grab the titles of the listed articles.  

2019-04-18 09:19:52Z

I saved an offline file of https://news.google.com/search?q=amazon&hl=en-US&gl=US&ceid=US%3AenHaving trouble determining how to grab the titles of the listed articles.  The problem seems to lay in the fact that the page content is rendered dynamically using JavaScript and thus can't be extracted from the HTML using css or xpath methods. However, it's present in the response body, so you can extract it using regular expressions. Here's the Scrapy shell session to show how:

Refactor the rule in scrapy

Per5eu5

[Refactor the rule in scrapy](https://stackoverflow.com/questions/55710877/refactor-the-rule-in-scrapy)

I have the rule where I scrape main page:Section 'deny' changes manually every time a second level page is added. Is there a  simpler way in rule to block parse second level pages for parse only main page?

2019-04-16 14:43:40Z

I have the rule where I scrape main page:Section 'deny' changes manually every time a second level page is added. Is there a  simpler way in rule to block parse second level pages for parse only main page?

How to resolve “ModuleNotFoundError” No module named 'XXXX'

CubanGT

[How to resolve “ModuleNotFoundError” No module named 'XXXX'](https://stackoverflow.com/questions/55748105/how-to-resolve-modulenotfounderror-no-module-named-xxxx)

Im working with scrapy, within Spyder/Anaconda environment. I have a project im trying to use the items.py with and no matter what i do, i cant get it to recognize it, keep getting "ModuleNotFoundError"Here is what i have in my dsg_spider.py and items.py file.

As long as i dont try to use the items.py file within the spider, it runs and works as expected.. Im just trying to get the data in a structured format and want to set it up right before i proceed.File "F:/Anaconda/DSG2/DSG2/spiders/dsg_spider.py", line 4, in 

    from DSG2.items import Dsg2ItemModuleNotFoundError: No module named 'DSG2'This is the code within the items.pyThe folder structure where i have all my files

2019-04-18 14:24:35Z

Im working with scrapy, within Spyder/Anaconda environment. I have a project im trying to use the items.py with and no matter what i do, i cant get it to recognize it, keep getting "ModuleNotFoundError"Here is what i have in my dsg_spider.py and items.py file.

As long as i dont try to use the items.py file within the spider, it runs and works as expected.. Im just trying to get the data in a structured format and want to set it up right before i proceed.File "F:/Anaconda/DSG2/DSG2/spiders/dsg_spider.py", line 4, in 

    from DSG2.items import Dsg2ItemModuleNotFoundError: No module named 'DSG2'This is the code within the items.pyThe folder structure where i have all my filesThe solution as it seems for me was to within Spyder, go to Projects --- New Project --- Select Existing directory then navigate to the project folder and select create, that created a project and when i run it, it no longer complains.. I think your forgot to add your package to the pythonpath.

Scrapy Splash recursive crawl not working

Anton1589

[Scrapy Splash recursive crawl not working](https://stackoverflow.com/questions/55709525/scrapy-splash-recursive-crawl-not-working)

I tried to use tips from similar questions but did not come to success.

In the end, I returned to the starting point and I want to ask your help.I cant execute a recursive crawl process with scrapy splash, but do it without problems on a single page. I see issue in bad urls to scrape:But link must be https://www.someurl.com/***************

2019-04-16 13:36:35Z

I tried to use tips from similar questions but did not come to success.

In the end, I returned to the starting point and I want to ask your help.I cant execute a recursive crawl process with scrapy splash, but do it without problems on a single page. I see issue in bad urls to scrape:But link must be https://www.someurl.com/***************I have found a solution:Just remove a urlparse.urljoin(response.url, url) module and change it to simple string like a "someurl.com" + urlNow all links are correct and a crawl process works fine.But now I have a some troubles with crawl loops, but its another question :)

I'm trying to scrape from a website but its giving me an error

Siddhant Singh

[I'm trying to scrape from a website but its giving me an error](https://stackoverflow.com/questions/55687619/im-trying-to-scrape-from-a-website-but-its-giving-me-an-error)

I'm not able to figure out why I'm getting this error. Is this related to file structure or is this some kind of internal error. Do I have to make a different folder and try again?Here's what I have done so far:quotes_spider.py fileitems.pyplease look into this and tell me what is it I'm doing wrong 

2019-04-15 10:53:13Z

I'm not able to figure out why I'm getting this error. Is this related to file structure or is this some kind of internal error. Do I have to make a different folder and try again?Here's what I have done so far:quotes_spider.py fileitems.pyplease look into this and tell me what is it I'm doing wrong 

I'm not able to import items.py file in my main spider file[scrapy]

Siddhant Singh

[I'm not able to import items.py file in my main spider file[scrapy]](https://stackoverflow.com/questions/55652537/im-not-able-to-import-items-py-file-in-my-main-spider-filescrapy)

project structureI'm trying to import my items file into shop.py file. I also tried [from ..items import ShopcluesItem] but it doesn't work. Please tell me what is it I'm doing wrong here?

2019-04-12 13:18:16Z

project structureI'm trying to import my items file into shop.py file. I also tried [from ..items import ShopcluesItem] but it doesn't work. Please tell me what is it I'm doing wrong here?

How stop on specific tag?

gongarek

[How stop on specific tag?](https://stackoverflow.com/questions/55747021/how-stop-on-specific-tag)

How get whole text under h1 tag to the next h1 tag? I have class name of starting h1 tag  I tried: //*[@class='something']//text()I want to scrapy text from all childs and siblings. I don't need text of h1 tags. I don't know how to stop scraping to next h1 tag.

2019-04-18 13:25:04Z

How get whole text under h1 tag to the next h1 tag? I have class name of starting h1 tag  I tried: //*[@class='something']//text()I want to scrapy text from all childs and siblings. I don't need text of h1 tags. I don't know how to stop scraping to next h1 tag.With a proper example:This XPath 1.0 expression:Meaning: "descendants text nodes of root element having the first preceding h1 element with @class attribute equal to 'something´ and not having an ancestor h1 element"And it selectsTest in http://www.xpathtester.com/xpath/ecd4f379b13558572ffd62d0db3a3f98

How to scrape dynamic content from a website?

Siddhant Singh

[How to scrape dynamic content from a website?](https://stackoverflow.com/questions/55709463/how-to-scrape-dynamic-content-from-a-website)

So I'm using scrapy to scrape a data from Amazon books section. But somehow I got to know that it has some dynamic data. I want to know how dynamic data can be extracted from the website. Here's something I've tried so far:Now I was using SelectorGadget to select a class which I have to scrape but in case of a dynamic website, it doesn't work.

2019-04-16 13:33:18Z

So I'm using scrapy to scrape a data from Amazon books section. But somehow I got to know that it has some dynamic data. I want to know how dynamic data can be extracted from the website. Here's something I've tried so far:Now I was using SelectorGadget to select a class which I have to scrape but in case of a dynamic website, it doesn't work.So how do I scrape a website which has dynamic content?there are a few options:what exactly is the difference between dynamic and static content?Dynamic means the data is generated from a request after the initial page request. Static means all the data is there at the original call to the siteHow do I extract other information like price and image from the website? and how to get particular classes for example like a price?Refer to your first questionhow would I know that data is dynamically created?You'll know it's dynamically created if you see it in the dev tools page source, but not in the html page source you first request. You can also see if the data is generated by additional requests in the dev tool and looking at Network -> XHRLastlyAmazon does offer an API to access the data. Try looking into that as wellIf you want to load dynamic content, you will need to simulate a web browser. When you make an HTTP request, you will only get the text returned by that request, and nothing more. To simulate a web browser, and interact with data on the browser, use the selenium package for Python:https://selenium-python.readthedocs.io/For scraping dynamic content (like JScript) you can use Srapy Splash. Look at this: https://www.google.com/amp/s/blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash%3fhs_amp=true

ScrapingHub: No Module named Mysql.connector

Christian Read

[ScrapingHub: No Module named Mysql.connector](https://stackoverflow.com/questions/55686663/scrapinghub-no-module-named-mysql-connector)

In my local machine everything works fine, but when I deployed it on ScrapingHub I've got an error saying all "ImportError: No module named mysql.connector".All I need is to, whenever I run my spider or run through job schedule it will automatically add all the scraped items through my database.Also I am trying to use item API if I don't have a choice to solve this issuePlease help thank you!

2019-04-15 09:57:13Z

In my local machine everything works fine, but when I deployed it on ScrapingHub I've got an error saying all "ImportError: No module named mysql.connector".All I need is to, whenever I run my spider or run through job schedule it will automatically add all the scraped items through my database.Also I am trying to use item API if I don't have a choice to solve this issuePlease help thank you!You need to install the MySQL client lib to allow your code to communicate with a MySQL server. I'd suggest you to create a requirements.txt file, in order to install required dependencies on ScrapyCloud.You should create your requirements.txt on the base dir of your project with the following content:And then your scrapinghub.yml file:Note: If this file doesnt exist, you might need to run shub deploy once to generate it.You can get more information about how to install dependencies on SC on this link

https://support.scrapinghub.com/support/solutions/articles/22000200400-deploying-python-dependencies-for-your-projects-in-scrapy-cloudIt's important to note that you must point your configuration to a running MySQL server, since there is no MySQL server running on SC.The error message is very clear. You are trying to scrape some data and to store it into a MySQL database. In order to do it, you need to be able to connect to MySQL, so, on your server you will need to install MySQL if you have not done so already and the connector to MySQL.This command installs the connector for Ubuntu:If you have some other brand of Linux as your server, then the command might be different.

Scrapy: next button uses WebForm_DoPostBackWithOptions()

Everton Reis

[Scrapy: next button uses WebForm_DoPostBackWithOptions()](https://stackoverflow.com/questions/55657989/scrapy-next-button-uses-webform-dopostbackwithoptions)

I am trying to scrap some information from https://seminovos.localiza.com/Paginas/resultado-busca.aspx?&yr=2014_2019&pc=25000_500000In this webpage, next_page button has a href with the following: 'javascript:WebForm_DoPostBackWithOptions(new WebForm_PostBackOptions("ctl00$ctl42$g_f221d036_75d3_4ee2_893d_0d7b40180245$ProximaPaginaSuperior", "", true, "", "", false, true))I could do that easily with Selenium, but using scrapy, how can I go to the next page?I tried something like:What is the proper way to navigate to next page on this situation?

2019-04-12 19:00:03Z

I am trying to scrap some information from https://seminovos.localiza.com/Paginas/resultado-busca.aspx?&yr=2014_2019&pc=25000_500000In this webpage, next_page button has a href with the following: 'javascript:WebForm_DoPostBackWithOptions(new WebForm_PostBackOptions("ctl00$ctl42$g_f221d036_75d3_4ee2_893d_0d7b40180245$ProximaPaginaSuperior", "", true, "", "", false, true))I could do that easily with Selenium, but using scrapy, how can I go to the next page?I tried something like:What is the proper way to navigate to next page on this situation?Find out the details of the request that your web browser performs when you click that, and try to reproduce it based on the available data.The answers to Can scrapy be used to scrape dynamic content from websites that are using AJAX? should give you an idea of ways to approach this. There is also a pull request for the Scrapy documentation that covers dealing with this type of scenarios, which you might find useful.It uses ASP.NET, so searching a lot more and analyzing the page I found what I was looking for:the final code has this format:It worked now. Thanks.

Get the contents of a specific attribute in span tag

Artemis

[Get the contents of a specific attribute in span tag](https://stackoverflow.com/questions/55655782/get-the-contents-of-a-specific-attribute-in-span-tag)

I have the following html:I need to get the contents of the content attribute ("2349.00"), is this possible with scrapy?I have tried the following:but obviously it didn't work.... is it possible to reference to other attributes similat as the class name and get their contents??

2019-04-12 16:15:13Z

I have the following html:I need to get the contents of the content attribute ("2349.00"), is this possible with scrapy?I have tried the following:but obviously it didn't work.... is it possible to reference to other attributes similat as the class name and get their contents??Try:

How to scrape headings in about page?

Bilal Khan

[How to scrape headings in about page?](https://stackoverflow.com/questions/55663748/how-to-scrape-headings-in-about-page)

I'm trying to scrape the headings in about page but I tried so much and failed due to not the proper understanding of what to do? I'm a beginner. So I require help.

2019-04-13 08:58:07Z

I'm trying to scrape the headings in about page but I tried so much and failed due to not the proper understanding of what to do? I'm a beginner. So I require help.Can you tell what kind of output do you need? It is very unclear from your post.

Check this example, where you can:Hope it will help you.Or make separate logins for separate pages:To parse a different HTML page, you need to yield a Request object with the target URL as the first argument for its constructor, and do the parsing in the method of your spider that you pass to the constructor of that Request object as the callback parameter.I strongly encourage you to complete the Scrapy tutorial. What you are trying to achieve is covered in the Following links section.

Getting empty response from scrapy shell while crawling monsterindia.com

Pratyush Behera

[Getting empty response from scrapy shell while crawling monsterindia.com](https://stackoverflow.com/questions/55656120/getting-empty-response-from-scrapy-shell-while-crawling-monsterindia-com)

I am trying to crawl few pages from monsterindia.com. But whenever I write any xpath on scrapy shell, it gives me empty result. However, there should be some way because view(response) command gives me the same html page. I ran this command : on my terminal and then tried several ways formulating different xpaths like -  response.xpath('//*[@class="job-tittle"]/text()').extract() . But no luck .. always got empty result. on terminal:then, response.xpath('//div[@class="job-tittle"]/text()').extract()

 got empty result.then, response.xpath('//*[@class="card-apply-content"]/text()').extract()

got empty result.I expect it to give some results, I mean the text from the website after crawling. Please help me with it.

2019-04-12 16:35:54Z

I am trying to crawl few pages from monsterindia.com. But whenever I write any xpath on scrapy shell, it gives me empty result. However, there should be some way because view(response) command gives me the same html page. I ran this command : on my terminal and then tried several ways formulating different xpaths like -  response.xpath('//*[@class="job-tittle"]/text()').extract() . But no luck .. always got empty result. on terminal:then, response.xpath('//div[@class="job-tittle"]/text()').extract()

 got empty result.then, response.xpath('//*[@class="card-apply-content"]/text()').extract()

got empty result.I expect it to give some results, I mean the text from the website after crawling. Please help me with it.So what Thiago I think was getting at is that the page updates with xhr requests which include a results count query string parameter. This returns json you can parse. So you change your url to that and handle json accordingly.Using requests to demonstrateJSON of first itemhttps://jsoneditoronline.org/?id=fe49c53efe10423a8d49f9b5bdf4eb36With scrapy:The data you're looking for isn't on the home page, but in the responses retrieved after the page load. If you check the "View Page Source" in your browser, you will see what actually came in the first request.And by inspecting the network tab in dev tools, you will see the further requests, like the one to this URL: https://www.monsterindia.com/middleware/jobsearch?query=computer&sort=1&limit=25

scraping the html file saved in local system

Jacob Nelson

[scraping the html file saved in local system](https://stackoverflow.com/questions/55568381/scraping-the-html-file-saved-in-local-system)

I'm trying to scrape an HTML file saved in my local file system (windows 10 os).when I give the file path in the formatI get the error when I give the file path in the formatI get the errorHow can I read the local HTML file and scrap it in windows os?

2019-04-08 07:38:44Z

I'm trying to scrape an HTML file saved in my local file system (windows 10 os).when I give the file path in the formatI get the error when I give the file path in the formatI get the errorHow can I read the local HTML file and scrap it in windows os?I think this is wrong to use start_urls in this case. Maybe you can try to read data in the file and then apply Selector to it?

Check this example:If you need, you can put block with file reading inside function start_requests.You can write the code in this way to scrap your own file saved in local systemInside open function first parameter is your file_path(though i have given here my own path) and the second parameter is the mode that you wants.

Scrapy: How to pass an item between methods using meta

elton

[Scrapy: How to pass an item between methods using meta](https://stackoverflow.com/questions/55529949/scrapy-how-to-pass-an-item-between-methods-using-meta)

I'm new  to scrapy and python and I'm trying to pass the item item['author'] in parse_quotes to the next parse method parse_bio I tried the request.meta and response.meta approach as shown in the scrapy documentation but without succes. see code as per below.Thanks in advaced for your inputI expect to get item['author'] from parse_quotes passed to parse_bio

2019-04-05 07:05:54Z

I'm new  to scrapy and python and I'm trying to pass the item item['author'] in parse_quotes to the next parse method parse_bio I tried the request.meta and response.meta approach as shown in the scrapy documentation but without succes. see code as per below.Thanks in advaced for your inputI expect to get item['author'] from parse_quotes passed to parse_bioI suggest you to use meta in this way:

Scrapy - scrape of all of the item instead of 1 item

Christian Read

[Scrapy - scrape of all of the item instead of 1 item](https://stackoverflow.com/questions/55531931/scrapy-scrape-of-all-of-the-item-instead-of-1-item)

I need to scrape all of the items but only 1 item is scrape.

My code is working fine before but when I transfer it to other project which is same code this happens I don't know whyI need to get all of the items according to the page size in start_urlhere's my working codePlease Help. Thank you

2019-04-05 09:11:21Z

I need to scrape all of the items but only 1 item is scrape.

My code is working fine before but when I transfer it to other project which is same code this happens I don't know whyI need to get all of the items according to the page size in start_urlhere's my working codePlease Help. Thank youIt seems you have problem with indents. Move yielding request to for loop:Or this is a bit cleared version:

Scrapy Error: Ignoring response <404 …> : HTTP status code is not handled or not allowed

Tobi

[Scrapy Error: Ignoring response <404 …> : HTTP status code is not handled or not allowed](https://stackoverflow.com/questions/55597894/scrapy-error-ignoring-response-404-http-status-code-is-not-handled-or-n)

I am new to scrapy and this is probably quite trivial. Anyway

I get the following error:I have tried to change the user agent in the settings.py file without success. Does anyone have another idea?

Thank youmy code:

2019-04-09 17:05:26Z

I am new to scrapy and this is probably quite trivial. Anyway

I get the following error:I have tried to change the user agent in the settings.py file without success. Does anyone have another idea?

Thank youmy code:You have slash in the end of url in start_urls. Without it everything should work fine.

Returning one dictionary scraping multiple pages with Scrapy

Finger twist

[Returning one dictionary scraping multiple pages with Scrapy](https://stackoverflow.com/questions/55567528/returning-one-dictionary-scraping-multiple-pages-with-scrapy)

I am scraping multiple pages with Scrapy, it is working fine but I am getting 2 dictionaries in my output, instead I would like to get the result of both pages into one output row . In this particular case return the output of the get_image function from the second page back with the rest of the data : artist and album , but I don't know how to feed that information back to the main dictionary .Thanks !

2019-04-08 06:38:54Z

I am scraping multiple pages with Scrapy, it is working fine but I am getting 2 dictionaries in my output, instead I would like to get the result of both pages into one output row . In this particular case return the output of the get_image function from the second page back with the rest of the data : artist and album , but I don't know how to feed that information back to the main dictionary .Thanks !I'd use passing arguments in meta. Check this example:

How to Use ScrapingHub.com API to post data on Heroku database?

Christian Read

[How to Use ScrapingHub.com API to post data on Heroku database?](https://stackoverflow.com/questions/55566494/how-to-use-scrapinghub-com-api-to-post-data-on-heroku-database)

How to Use ScrapingHub.com API to post data on Heroku database? I am thinking that is it possible to connect my API from scrapinghub to Heroku databe?Any idea please? Please help me thank you!

2019-04-08 04:55:57Z

How to Use ScrapingHub.com API to post data on Heroku database? I am thinking that is it possible to connect my API from scrapinghub to Heroku databe?Any idea please? Please help me thank you!Thank you for helping me I already solve my problem by using requirement dependency on scrapinghub :)

Scrapy unable to connect to HTTPS site that only supports older TLSv1. Connection Lost

debugme

[Scrapy unable to connect to HTTPS site that only supports older TLSv1. Connection Lost](https://stackoverflow.com/questions/55543251/scrapy-unable-to-connect-to-https-site-that-only-supports-older-tlsv1-connectio)

Using scrapy 1.6.0 (twisted 18.9.0, pyopenssl 19.0.0, openssl 1.0.2r, osx 10.14.3). I've ruled out user agent and robots.txt. Seems to be a certificate negotiation issue. There is no web proxy involved.Destination is https://www.labor.ny.gov/To reproduce:Trying to connect and negotiating via OpenSSL directly on the command line seems to fail as well:However if I force openssl to TLSv1 it seems to work. I just don't know how to force that from scrapy -> twisted -> pyopenssl -> OpenSSL or if it's possible.Postman can't fetch the page either. It seems like anything relying on OpenSSL sort of silently dies. 

2019-04-05 20:56:31Z

Using scrapy 1.6.0 (twisted 18.9.0, pyopenssl 19.0.0, openssl 1.0.2r, osx 10.14.3). I've ruled out user agent and robots.txt. Seems to be a certificate negotiation issue. There is no web proxy involved.Destination is https://www.labor.ny.gov/To reproduce:Trying to connect and negotiating via OpenSSL directly on the command line seems to fail as well:However if I force openssl to TLSv1 it seems to work. I just don't know how to force that from scrapy -> twisted -> pyopenssl -> OpenSSL or if it's possible.Postman can't fetch the page either. It seems like anything relying on OpenSSL sort of silently dies. Not a full answer; CW in case anyone can add the scrapy (or related) part.Man that server is bad!

It supports only SSL2 SSL3 and TLS1.0 where the first two are completely broken and the first was completely broken last century. It identifies as IIS/6.0 which dates to Windows Server 2003 -- which was end-of-life long ago. FWLIW it's not actually version-intolerant, or broken for hello over 256 bytes, as some defective implementations were discovered to be years ago; if I use OpenSSL 1.0.2 to send it ClientHello offering TLS1.2 with ciphers restricted to kRSA, it does negotiate down to TLS1.0 correctly. It only fails for the OpenSSL>=1.0.2 default ClientHello, which uses a significantly larger cipherlist than previous versions because TLS1.2 added a whole bunch of new ciphersuites for the new AEAD format and new PRF scheme. Forcing TLS1.0 has the same effect, because it causes OpenSSL to offer only the smaller list of ciphersuites which were valid in TLS1.0. I vaguely recall an XP-era bug triggered by 'large' cipherlists, and that might be the problem here. It's not the certificate. The certificate is the only thing they have right.

Problem running Scrapoxy and Digital Ocean

Yassine Akermi

[Problem running Scrapoxy and Digital Ocean](https://stackoverflow.com/questions/55539571/problem-running-scrapoxy-and-digital-ocean)

I'm trying to run Scrapoxy with Digital Ocean. I successfully created a droplet image and configured Scrapoxy.When I start Scrapoxy, it keeps on creating a new instance and bypassing max limit. It stops only when it reaches 10 droplets. What annoys me is that No instance found in the GUI version. Also, when I test the proxy server I get this message: "Error: No running instance found". It seems that Scrapoxy only creates droplets and can't connect to them.I installed Scrapoxy manually. Here is my config file:

2019-04-05 16:14:50Z

I'm trying to run Scrapoxy with Digital Ocean. I successfully created a droplet image and configured Scrapoxy.When I start Scrapoxy, it keeps on creating a new instance and bypassing max limit. It stops only when it reaches 10 droplets. What annoys me is that No instance found in the GUI version. Also, when I test the proxy server I get this message: "Error: No running instance found". It seems that Scrapoxy only creates droplets and can't connect to them.I installed Scrapoxy manually. Here is my config file:Did you try to put your region in lowercase in the config fileExample:

like thisinstead ofThere are other troubleshooting steps you could take on the following github pages Issue 84 & Issue 62

Scrapy Deny query/parameter URLs

Lucas Abraham

[Scrapy Deny query/parameter URLs](https://stackoverflow.com/questions/55475829/scrapy-deny-query-parameter-urls)

I am trying to avoid crawling all URLs that contain parameters as the site i am trying to crawl has some serious issues of almost infinite parameter URLsI have tried to use the below rules to exclude all parameters but it doesn't seem to be doing anything at the moment.I am still trying to work this out but any help would be amazing. Thanks in advance.

2019-04-02 13:18:53Z

I am trying to avoid crawling all URLs that contain parameters as the site i am trying to crawl has some serious issues of almost infinite parameter URLsI have tried to use the below rules to exclude all parameters but it doesn't seem to be doing anything at the moment.I am still trying to work this out but any help would be amazing. Thanks in advance.

Post request/Form submission with scrapy leads to Error 404

Tobi

[Post request/Form submission with scrapy leads to Error 404](https://stackoverflow.com/questions/55536542/post-request-form-submission-with-scrapy-leads-to-error-404)

I am learning how to build a spider using scrapy to scrape this webpage: https://www.beesmart.city. To have access, it is necessary to do a form submission here: https://www.beesmart.city/loginInspecting the page, I didn't find a CSRF Token so I guess it is not needed in this case.I used the following code:When I run it I get a 404 however. Does anyone have an idea why? I am very thankful for any help.

Tobi :)The full response is:

2019-04-05 13:29:38Z

I am learning how to build a spider using scrapy to scrape this webpage: https://www.beesmart.city. To have access, it is necessary to do a form submission here: https://www.beesmart.city/loginInspecting the page, I didn't find a CSRF Token so I guess it is not needed in this case.I used the following code:When I run it I get a 404 however. Does anyone have an idea why? I am very thankful for any help.

Tobi :)The full response is:If you are looking to understand CSFR Tokens - they have been explained on the following SO questionAs furas explained it's not possible to use pure scrapy to login over javascript.I recommend that you read the following article about Web scraping javascript using python

Why do I get code 400 when POST "multipart/form-datain Scrapy. Python 3

Billy Jhon

[Why do I get code 400 when POST "multipart/form-datain Scrapy. Python 3](https://stackoverflow.com/questions/55442111/why-do-i-get-code-400-when-post-multipart-form-datain-scrapy-python-3)

Trying hard to submit the form to no success. 

This form is supposed to redirect and return new url with PDF.

Here is how to access the page in question:I need to mimic multipart formdata which looks like this:Here is part of my Scrapy code responsible for this request:Whenever I run the code Im getting Response 400.

How do I mimic this form correctly?UPDATE:

2019-03-31 14:49:05Z

Trying hard to submit the form to no success. 

This form is supposed to redirect and return new url with PDF.

Here is how to access the page in question:I need to mimic multipart formdata which looks like this:Here is part of my Scrapy code responsible for this request:Whenever I run the code Im getting Response 400.

How do I mimic this form correctly?UPDATE:I had to automate the entire process of filling the form and now it seems to work just fine.

Scraping complex comments in Scrapy

gongarek

[Scraping complex comments in Scrapy](https://stackoverflow.com/questions/55535645/scraping-complex-comments-in-scrapy)

I am using Scrapy. I want to scrape comments for example on page: https://www.thingiverse.com/thing:2/comments I will scrape more sites, so I want have flexible code.I have no idea how scrape comments without loosing informations about in which 'container' comment is, and comment's 'depth'.Let's say that I will have 3 Fields. Id_container, content and depth. These informations will be enough to get know about relations between comments. How to code that every comment will have this informations?    The question is general, so any tips will be useful

2019-04-05 12:40:57Z

I am using Scrapy. I want to scrape comments for example on page: https://www.thingiverse.com/thing:2/comments I will scrape more sites, so I want have flexible code.I have no idea how scrape comments without loosing informations about in which 'container' comment is, and comment's 'depth'.Let's say that I will have 3 Fields. Id_container, content and depth. These informations will be enough to get know about relations between comments. How to code that every comment will have this informations?    The question is general, so any tips will be usefulTo not lose the hierarchy information, you could start by getting all depth 1 comments and getting deeper, e.g:Output:Then, with comment id, you can have all information you want for that particular comment.

Scrapy Splash Lua incomplete login form input

Alex16237

[Scrapy Splash Lua incomplete login form input](https://stackoverflow.com/questions/55446626/scrapy-splash-lua-incomplete-login-form-input)

I am trying to login to website using scrapy splash with lua script. Login form is in format of JavaScript overlay where I first need to click button to make it visible. Splash renders login form correctly upon clicking button with Lua script. My problem is that script does not fill form correctly, it only fills first letter of a email in e-mail input type and I assume same happens for password type because login fails. Even if I try to enter non-email value, it still sends only first character of my string.Code I am using:I also tried to specify input by type in splash:select ie. splash:select('input[type=email]')This also fails.

2019-04-01 00:12:37Z

I am trying to login to website using scrapy splash with lua script. Login form is in format of JavaScript overlay where I first need to click button to make it visible. Splash renders login form correctly upon clicking button with Lua script. My problem is that script does not fill form correctly, it only fills first letter of a email in e-mail input type and I assume same happens for password type because login fails. Even if I try to enter non-email value, it still sends only first character of my string.Code I am using:I also tried to specify input by type in splash:select ie. splash:select('input[type=email]')This also fails.

My script is prevented from crawling that website (http:403) but not my browsers

Freddy

[My script is prevented from crawling that website (http:403) but not my browsers](https://stackoverflow.com/questions/55451148/my-script-is-prevented-from-crawling-that-website-http403-but-not-my-browsers)

I was crawling a website when they blocked me with a 403. I can still access the website from any of my browser but my Scrapy script is unable to crawl.I crawl from home using 5 proxies and each has 10 IPs randomly selected. I have about 40 user_agents selected randomly (see code below). A new set of proxy and the user_agent is selected on each request. Each yield request is about 1MB heavy. When about 100 requests are collected the script (using S3pipelines from Github) sends a package to my S3 on AWS. I have a download delay of 10 and auto throttles activated in the settings of my Scrapy project. All that is aligned with the website robots.txt request -crawling with delay 10.I read instructions manuals from Scrapy, from Python, and some others but couldn't find any way to become more 'human' in crawling.My settings (extract):

2019-04-01 08:45:52Z

I was crawling a website when they blocked me with a 403. I can still access the website from any of my browser but my Scrapy script is unable to crawl.I crawl from home using 5 proxies and each has 10 IPs randomly selected. I have about 40 user_agents selected randomly (see code below). A new set of proxy and the user_agent is selected on each request. Each yield request is about 1MB heavy. When about 100 requests are collected the script (using S3pipelines from Github) sends a package to my S3 on AWS. I have a download delay of 10 and auto throttles activated in the settings of my Scrapy project. All that is aligned with the website robots.txt request -crawling with delay 10.I read instructions manuals from Scrapy, from Python, and some others but couldn't find any way to become more 'human' in crawling.My settings (extract):

Scrapinghub exporting multiple items

Jithin

[Scrapinghub exporting multiple items](https://stackoverflow.com/questions/55543981/scrapinghub-exporting-multiple-items)

In scrapinghub how can we achieve multiple items exporting? I have MainItem() and a SubItem() item classes and I would like to get two separate items in scrapinghub item's page. sample code snippet given below, Here in scrapinghub I'm able to view only MainItems() fields 

2019-04-05 22:10:34Z

In scrapinghub how can we achieve multiple items exporting? I have MainItem() and a SubItem() item classes and I would like to get two separate items in scrapinghub item's page. sample code snippet given below, Here in scrapinghub I'm able to view only MainItems() fields Can you provide more information? The spider code and logs, I can't see any problem with your example. Scrapy Cloud does allow a spider to yield different items. These items can be filtered later using the SC interface.

Scrapy Custom Function Cannot Fire scrapy.Requests

Ken

[Scrapy Custom Function Cannot Fire scrapy.Requests](https://stackoverflow.com/questions/55417294/scrapy-custom-function-cannot-fire-scrapy-requests)

Seems the yield scrapy.Requests cannot be fired in a function like the following code.

Anyone could give me a help to clear me or help me fire?

Really appreciate for your help.

2019-03-29 12:17:40Z

Seems the yield scrapy.Requests cannot be fired in a function like the following code.

Anyone could give me a help to clear me or help me fire?

Really appreciate for your help.To avoid code duplication you can call your __fire_here function in this way:Your code seems to make endless calls from one function to another. Can you check your logics?

What's the correct Scrapy XPath for <p> elements incorrectly placed within <h> tags?

Eric Johnson

[What's the correct Scrapy XPath for <p> elements incorrectly placed within <h> tags?](https://stackoverflow.com/questions/55337928/whats-the-correct-scrapy-xpath-for-p-elements-incorrectly-placed-within-h-t)

I am setting up my first Scrapy Spider, and I'm having some difficulty using xpath to extract certain elements. My target is http://www.cbooo.cn/m/641515  (a Chinese website similar to Box Office Mojo). I can extract the Chinese name of the film 阿龙浴血记 with no problem , but I can't figure out how to get the information below it. I believe this is because the HTML is not standard, as discussed here. There are several paragraph elements nested beneath the  header.I have tried the solution in the link above, and also here, to no avail. When I run the spider in the Scrapy shell, the spider finds the Chinese title as expected. However, the remaining items return either a [], or a weird mish-mash of text on the page.Any advice? This is my first amature programming project, so I appreciate your patience with my ignorance and your help. Thank you!EDITTried implement the text cleaning method in the comments. The example in the comments worked, but when I tried to reimplement it I got an "Attribute Error: 'list' object has no attribute 'split'“ (please see the China Box Office, country of origin, and genre examples below)

2019-03-25 12:35:20Z

I am setting up my first Scrapy Spider, and I'm having some difficulty using xpath to extract certain elements. My target is http://www.cbooo.cn/m/641515  (a Chinese website similar to Box Office Mojo). I can extract the Chinese name of the film 阿龙浴血记 with no problem , but I can't figure out how to get the information below it. I believe this is because the HTML is not standard, as discussed here. There are several paragraph elements nested beneath the  header.I have tried the solution in the link above, and also here, to no avail. When I run the spider in the Scrapy shell, the spider finds the Chinese title as expected. However, the remaining items return either a [], or a weird mish-mash of text on the page.Any advice? This is my first amature programming project, so I appreciate your patience with my ignorance and your help. Thank you!EDITTried implement the text cleaning method in the comments. The example in the comments worked, but when I tried to reimplement it I got an "Attribute Error: 'list' object has no attribute 'split'“ (please see the China Box Office, country of origin, and genre examples below)Here are some examples from which you can infer the last one. Remember to always use a class or id attribute to identify the html element. /div[3]/div[2]/div/div[1]/.. is not a good practise.To find chinesereleasedate I took the p element whose text contains '上映时间'. You have to parse this to get the exact value.To find productionregions I took the 7th selector from the list response.xpath('//div[@class="ziliaofr"]/div/p')[6] selected the text. A better method would be to check if the text contains '国家及地区' just like above.Edit : To answer the question in the comments,returns a string like '\r\n                        上映时间：2017-7-27（中国）\r\n                 ' which is not what you are looking for. You can clean it up like:chinesereleasedate = chinesereleasedate.split('：')[1].split('（')[0]This gives us the correct date.You don't have to torture yourself with xpath by the way, you can use css:

Scrapy not parsing items

Sohan Das

[Scrapy not parsing items](https://stackoverflow.com/questions/55416873/scrapy-not-parsing-items)

I'm trying to scrape a web page with pegination but call back not parsing the items, any help would be appreciated....here is the code

2019-03-29 11:52:47Z

I'm trying to scrape a web page with pegination but call back not parsing the items, any help would be appreciated....here is the codeYour code calls urls from start_urls and goes to parse function. Since there is no any div.advertiser-profile elements, it really should close without any results. So your parse_links function is not called at all.Change your functions names:My logs from scrapy shell:

Scrapy spider only returning last item in list

Haytorade

[Scrapy spider only returning last item in list](https://stackoverflow.com/questions/55321163/scrapy-spider-only-returning-last-item-in-list)

I'm building a scraper to crawl a page and return multiple items (h3 & p tags) from within a div. For some reason, the scraper will print all 'name' fields when called, but is only saving info for the last item on the page.Here's my code:Anyone have an idea on what I'm doing wrong?

2019-03-24 06:04:39Z

I'm building a scraper to crawl a page and return multiple items (h3 & p tags) from within a div. For some reason, the scraper will print all 'name' fields when called, but is only saving info for the last item on the page.Here's my code:Anyone have an idea on what I'm doing wrong?If you run //div[@class='fsb v4'] in DevTools it will only return a single elementSo you have to find a Selector that gets all those profile DIVs

How can a scrape a page that literally contains “\x2d”, but save that character as “-” in my item?

Chris

[How can a scrape a page that literally contains “\x2d”, but save that character as “-” in my item?](https://stackoverflow.com/questions/55424802/how-can-a-scrape-a-page-that-literally-contains-x2d-but-save-that-character)

I need to scrape some text from within a script on a page, and save that text within a scrapy item, presumably as a UTF-8 string. However the actual literal text I'm scraping from has special characters written out as what I believe to be UTF hex. e.g. "-" is written as "\x2f". How can I scrape characters represented as "\x2f" but save them as "-" in my scrapy item? Excerpt of contents on scraped page:My scrapy script goes like this:For this item, scrapy's output will return: 'author': u'Kurt\x20Vonnegut', 'title': u'Slaughterhouse\x2DFive'Ideally, I would like:'author': 'Kurt Vonnegut', 'title': 'Slaughterhouse Five'Things I've tried with no change to the output:Finally, in case it needs to be explicitly stated, I do not have control over how this information is being displayed on the site I'm scraping.

2019-03-29 20:04:53Z

I need to scrape some text from within a script on a page, and save that text within a scrapy item, presumably as a UTF-8 string. However the actual literal text I'm scraping from has special characters written out as what I believe to be UTF hex. e.g. "-" is written as "\x2f". How can I scrape characters represented as "\x2f" but save them as "-" in my scrapy item? Excerpt of contents on scraped page:My scrapy script goes like this:For this item, scrapy's output will return: 'author': u'Kurt\x20Vonnegut', 'title': u'Slaughterhouse\x2DFive'Ideally, I would like:'author': 'Kurt Vonnegut', 'title': 'Slaughterhouse Five'Things I've tried with no change to the output:Finally, in case it needs to be explicitly stated, I do not have control over how this information is being displayed on the site I'm scraping.Inspired by Converting \x escaped string to UTF-8, I solved this by using .decode('string-escape'), as follows:You can use urllib's unquote function.On Python 3.x:On Python 2.7:Take a look on Item Loaders and Input Processors so you can do this for all scraped fields.

Using XPath To Extract Data

CS Student

[Using XPath To Extract Data](https://stackoverflow.com/questions/55423485/using-xpath-to-extract-data)

I want to use XPath to extract data in a webpage but get nothing, how do I extract the data?I tried using the below codes but they return nothing.I tried using but I get nothing. Here is the code that I want to extract:I want to extract " San Jorge " but I get nothing.

2019-03-29 18:22:11Z

I want to use XPath to extract data in a webpage but get nothing, how do I extract the data?I tried using the below codes but they return nothing.I tried using but I get nothing. Here is the code that I want to extract:I want to extract " San Jorge " but I get nothing.You should select internal span and then take following text, so the expression will be like house.xpath('.//span[@class="icon icon-pin"]/following-sibling::text()').get()In shell I could get data in this way:Try the following:It looks like you dropped a '-' in your second xpath query.  Additionally the amn-info-item classed span has two text nodes. Using the [2] will get the second one.

Scrapy Same item but multiple URLs

 GhostKU 

[Scrapy Same item but multiple URLs](https://stackoverflow.com/questions/55244554/scrapy-same-item-but-multiple-urls)

The problem is that sometimes I got a page with different url but same id_ and in such situation I need to add this new url to the old item with old id_/ Something like:

2019-03-19 15:27:15Z

The problem is that sometimes I got a page with different url but same id_ and in such situation I need to add this new url to the old item with old id_/ Something like:Interesting case. There are few ways you can do it but major barier in this case would be memory. When scrapy yields an item it exports it to whatever output you have (stdout,json,csv etc.) and then you're done. 

If you wish to combine your items this way you either need to post-process your output or save everything in to memory.If your spider is small go with #2 otherwise #1 is a much more memory efficient and cleaner approach.You can't do this in a simple way due to how Scrapy works. It processes the requests asynchronously yielding items one by one, it doesn't maintain any history by itself. What you could do is to use some items buffer in the spider and use signals to dump all items at the end of a crawl.See this dummy example:

Python - iterate through nested json and save values

Alex16237

[Python - iterate through nested json and save values](https://stackoverflow.com/questions/55282822/python-iterate-through-nested-json-and-save-values)

I have a nested JSON (API) webstie which i want to parse and save items to file (using Scrapy framework).I want to access each subelement of given elements, those are in following formatIf I expand element 0 i get following values, where {...} exapnds furtherHow does it look like in realityHow do I access, consecutively, each element, first 0, then 1, then 2 ... up to total of 350 and grab value of, for example and save it to item.What I have:This fails withTypeError: list indices must be integers, not strI know that i can access it with item['guid'] = results[0]["guid"]But this only gives me [0] index of the whole list and I want to iterate through all of indexes. How do I pass index number inside of the list?

2019-03-21 14:32:35Z

I have a nested JSON (API) webstie which i want to parse and save items to file (using Scrapy framework).I want to access each subelement of given elements, those are in following formatIf I expand element 0 i get following values, where {...} exapnds furtherHow does it look like in realityHow do I access, consecutively, each element, first 0, then 1, then 2 ... up to total of 350 and grab value of, for example and save it to item.What I have:This fails withTypeError: list indices must be integers, not strI know that i can access it with item['guid'] = results[0]["guid"]But this only gives me [0] index of the whole list and I want to iterate through all of indexes. How do I pass index number inside of the list?Replace results["guid"] in your for loop to var["guid"]:when you can access guid like results[0]["guid"] it means that you have list of dictionaries that every dictionary contains key named guid. in your for loop you use results (that is list) instead of var (that contain every dictionary in each iteration) that throws TypeError because list indices must be integers not strings (like "guid").  UPDATE: if you want to save each var["guid"] you can save them in a dictionary like this:now guid_holder holds all elements.

How can i neatly format my spider CSV export?

Hi tE

[How can i neatly format my spider CSV export?](https://stackoverflow.com/questions/55317613/how-can-i-neatly-format-my-spider-csv-export)

I am trying to export my scraped results into a CSV file but the export is off.. The export shows one company name, followed with 20 (all the other addresses). Then the next company name, followed with he same addresses.. I dont see whats wrong, i hope anyone sees why..

2019-03-23 19:36:03Z

I am trying to export my scraped results into a CSV file but the export is off.. The export shows one company name, followed with 20 (all the other addresses). Then the next company name, followed with he same addresses.. I dont see whats wrong, i hope anyone sees why..What format do you need? You have blocks with information, you can make dictionary with all the data and then yield it.What exactly do you want?

How to scrape a certain text regardless of which tags it is contained in using scrapy

I. K.

[How to scrape a certain text regardless of which tags it is contained in using scrapy](https://stackoverflow.com/questions/55221593/how-to-scrape-a-certain-text-regardless-of-which-tags-it-is-contained-in-using-s)

I am trying to scrape a number of sites to find if a certain code snippet is present. Most of the time the scraper works perfectly as intended.I am using the following method to find the bit of code I am looking for:However, my issue is the following: sometimes the thing I want to find is not in the script itself but as the source for the script (I know how to scrape this as well), and sometimes when JQuery is used, I can't get the correct scrape results.So my question is, is there an easier way to look through the raw HTML/JS text to find a match for what I am looking for? Trying to look through all alternatives to scrapes will quickly bloat up the code, and I only need to see if this certain text is present. I have not found a suitable method from the official scrapy documentation (though I am still somewhat inexperienced with the tool, so I might have missed it), so if anyone has a solution for this it would be greatly appreciated.

2019-03-18 12:39:07Z

I am trying to scrape a number of sites to find if a certain code snippet is present. Most of the time the scraper works perfectly as intended.I am using the following method to find the bit of code I am looking for:However, my issue is the following: sometimes the thing I want to find is not in the script itself but as the source for the script (I know how to scrape this as well), and sometimes when JQuery is used, I can't get the correct scrape results.So my question is, is there an easier way to look through the raw HTML/JS text to find a match for what I am looking for? Trying to look through all alternatives to scrapes will quickly bloat up the code, and I only need to see if this certain text is present. I have not found a suitable method from the official scrapy documentation (though I am still somewhat inexperienced with the tool, so I might have missed it), so if anyone has a solution for this it would be greatly appreciated.Maybe simple regex search through the HTML source is what you are looking for? Something likeOr, if you just know it's wrapped inside some element and just don't know which, you can doAlso, you don't need to use len to check the result, simplyis enough.

Py2app is not running scrapy spider in GUI

randal

[Py2app is not running scrapy spider in GUI](https://stackoverflow.com/questions/55225936/py2app-is-not-running-scrapy-spider-in-gui)

I'm trying to package my application as an executable, however its not fetching the scrapy log data.i'm referencing this Py2app fails with scrapygetting the errorWhen doing $ ./dist/main.app/Contents/MacOS/mainhowever the solutions are not relevant. My gui works well by it self for example, This is working when python3 main.pybut doesn't work when running python3 setup.py py2app -A not even loading resultsSetup file looks like setup.pymain.py

2019-03-18 16:29:51Z

I'm trying to package my application as an executable, however its not fetching the scrapy log data.i'm referencing this Py2app fails with scrapygetting the errorWhen doing $ ./dist/main.app/Contents/MacOS/mainhowever the solutions are not relevant. My gui works well by it self for example, This is working when python3 main.pybut doesn't work when running python3 setup.py py2app -A not even loading resultsSetup file looks like setup.pymain.py

Running scraping jobs in parallel on cluster

WoofDoggy

[Running scraping jobs in parallel on cluster](https://stackoverflow.com/questions/55226106/running-scraping-jobs-in-parallel-on-cluster)

I would like to split scraping urls among many crawling processes and run them on separate google cloud instances. I could do this by hand (the same spider with just different input data) but it is very annoying to manage 10-20 instances. Is there a possibility to run instance group and specify which process should be executed on which instance? I am using scrapy spider and right now I will split input data manually. Next step will be to use redis queue.I have past experience with MPI and cluster computing. I remember there was option to specify maximal number of processes per node. I would like to do similar thing in this case.

2019-03-18 16:40:13Z

I would like to split scraping urls among many crawling processes and run them on separate google cloud instances. I could do this by hand (the same spider with just different input data) but it is very annoying to manage 10-20 instances. Is there a possibility to run instance group and specify which process should be executed on which instance? I am using scrapy spider and right now I will split input data manually. Next step will be to use redis queue.I have past experience with MPI and cluster computing. I remember there was option to specify maximal number of processes per node. I would like to do similar thing in this case.

“moduleNotFoundError” when sets scrapy as an app in django

Zheyuuu

[“moduleNotFoundError” when sets scrapy as an app in django](https://stackoverflow.com/questions/55236051/modulenotfounderror-when-sets-scrapy-as-an-app-in-django)

When I tried to start my scrapy demo with scrapy crawl getCommodityInfo, the error below occurred.It seems that the spider cannot be found, but I don't know why it happened. My whole project hierarchy is here. GraduationProject is the django project. main and spiders are the applications of django. The bin directory stores two demo scrapy projects. When I entered the JDSpider trying to run it, the error occurred. Could you help me fix it?PS. My spider name:  name = "getCommodityInfo"With the solution applied by PS1212, the scrapy demo could run. However the pycharm warns like this. What happened?

2019-03-19 08:02:20Z

When I tried to start my scrapy demo with scrapy crawl getCommodityInfo, the error below occurred.It seems that the spider cannot be found, but I don't know why it happened. My whole project hierarchy is here. GraduationProject is the django project. main and spiders are the applications of django. The bin directory stores two demo scrapy projects. When I entered the JDSpider trying to run it, the error occurred. Could you help me fix it?PS. My spider name:  name = "getCommodityInfo"With the solution applied by PS1212, the scrapy demo could run. However the pycharm warns like this. What happened?Because it it not recognizing you project module.Try this:

how to Call again scrapy start_request in EXTENSIONS

somnus

[how to Call again scrapy start_request in EXTENSIONS](https://stackoverflow.com/questions/55237349/how-to-call-again-scrapy-start-request-in-extensions)



2019-03-19 09:19:13Z



Sequential scraping from multiple start_urls leading to error in parsing

bebissig

[Sequential scraping from multiple start_urls leading to error in parsing](https://stackoverflow.com/questions/55224532/sequential-scraping-from-multiple-start-urls-leading-to-error-in-parsing)

First, highest appreciation for all of your work answering noob questions like this one.Second, as it seems to be a quite common problem I was finding (IMO) related questions such as:

Scrapy: Wait for a specific url to be parsed before parsing othersHowever, at my current state of understanding it is not straightforward to adapt the suggestions in my specific case and I would really appreciate your help.Problem Outline: running on (Python 3.7.1, Scrapy 1.5.1)I want to scrape data from every link collected on pages like this

https://www.gipfelbuch.ch/gipfelbuch/touren/seite/1then from all links on another collectionhttps://www.gipfelbuch.ch/gipfelbuch/touren/seite/650 I manage to get the desired information (only two elements shown here) if I run the spider for one (e.g. page 1 or 650) at a time. (Note that I restircted the length of links that is crawled per page to 2.) However, once I have multiple start start_urls (setting two elements in the list [1,650] in the code below) the parsed data is no more consistent. Apparently at least one element is not found by xpath. I am suspecting some (or a lot of) incorrect logic how I handle/pass the requests that leads not to the intendet order for parsing.Code: Command promptQuestion:

How do I have to structure the first (for links) and second (for content) parsing commands correctly? Why is the "PARSE OUTPUT" not in the order i would expect (first for page 1, links top to bottom, then page 2, links top to bottom)?I already tried to reduce the number of CONCURRENT_REQUESTS = 1 and DOWNLOAD_DELAY = 2.   I hope the question is clear enough... big thanks in advance.

2019-03-18 15:14:28Z

First, highest appreciation for all of your work answering noob questions like this one.Second, as it seems to be a quite common problem I was finding (IMO) related questions such as:

Scrapy: Wait for a specific url to be parsed before parsing othersHowever, at my current state of understanding it is not straightforward to adapt the suggestions in my specific case and I would really appreciate your help.Problem Outline: running on (Python 3.7.1, Scrapy 1.5.1)I want to scrape data from every link collected on pages like this

https://www.gipfelbuch.ch/gipfelbuch/touren/seite/1then from all links on another collectionhttps://www.gipfelbuch.ch/gipfelbuch/touren/seite/650 I manage to get the desired information (only two elements shown here) if I run the spider for one (e.g. page 1 or 650) at a time. (Note that I restircted the length of links that is crawled per page to 2.) However, once I have multiple start start_urls (setting two elements in the list [1,650] in the code below) the parsed data is no more consistent. Apparently at least one element is not found by xpath. I am suspecting some (or a lot of) incorrect logic how I handle/pass the requests that leads not to the intendet order for parsing.Code: Command promptQuestion:

How do I have to structure the first (for links) and second (for content) parsing commands correctly? Why is the "PARSE OUTPUT" not in the order i would expect (first for page 1, links top to bottom, then page 2, links top to bottom)?I already tried to reduce the number of CONCURRENT_REQUESTS = 1 and DOWNLOAD_DELAY = 2.   I hope the question is clear enough... big thanks in advance.If the problem is to visit more URLs at the same time, you can visit one by one, using the signal spider_idle (https://docs.scrapy.org/en/latest/topics/signals.html).The idea is the following:1.start_requests only visits the first URL2.when the spider gets idle, the method spider_idle is called3.the method spider_idle deletes the first URL and visits the second URL4.so on...The code would be something like this (I didn't try it):

python scrapy response statis

吴洲洋

[python scrapy response statis](https://stackoverflow.com/questions/55225062/python-scrapy-response-statis)

Why have I disabled the status code to 404 and no longer access it, but the crawler still accesses the 404 page? This is a crawler written by scrapy. The main problem is response.status != 404 does not work.

2019-03-18 15:42:58Z

Why have I disabled the status code to 404 and no longer access it, but the crawler still accesses the 404 page? This is a crawler written by scrapy. The main problem is response.status != 404 does not work.This function is never going to do anything, because the first if conditional:will always be False, and so it will never go inside the if code block.  Maybe you want an OR there instead of an AND?...or maybe your indentation is wrong, or at least coming to me wrong.  In any case, the if makes no sense because it is obviously always going to be False.

Scrapy iterate by POST method

Budi Mulyo

[Scrapy iterate by POST method](https://stackoverflow.com/questions/55233379/scrapy-iterate-by-post-method)

My code is running, but idk exactly how it's works, and then i need to expand this function..I want to loop by POST method with same url after login..This code is works, I'am login, and scrap next url with login.. It's success to login, and success get result item inside url, and then after first(idle method) this scrap next url, and lastly parsing the result..But idk, is this best method to scrap after login?, and any more mature code to handling this purpose ?, is there good technical explanation(my explanation is too simple)?, and lastly how this code could scrap more with iterate target_url with different POST request,, i want to add in idle method, but still fail,,some fail try :another fail try :I am remove function class method, and after login i am add another scrap, it's fail because it didn't get session in login.. :(Thanks for help,,

2019-03-19 03:51:23Z

My code is running, but idk exactly how it's works, and then i need to expand this function..I want to loop by POST method with same url after login..This code is works, I'am login, and scrap next url with login.. It's success to login, and success get result item inside url, and then after first(idle method) this scrap next url, and lastly parsing the result..But idk, is this best method to scrap after login?, and any more mature code to handling this purpose ?, is there good technical explanation(my explanation is too simple)?, and lastly how this code could scrap more with iterate target_url with different POST request,, i want to add in idle method, but still fail,,some fail try :another fail try :I am remove function class method, and after login i am add another scrap, it's fail because it didn't get session in login.. :(Thanks for help,,I think you need to add some code in spider_idle that's is when second request sent,,something like :This code will iterate your request POST,, hope this help..

Scraping an API

M. Coppée

[Scraping an API](https://stackoverflow.com/questions/55237647/scraping-an-api)

Good morning everyone,I try to collect data about cars on this site: https://www.caramigo.eu/To do so I need to launch a request on the search bar of the home page for a specified location and date. This provides me a page like this: https://www.caramigo.eu/be/fr/recherche?address=Belgique%2C+Wallonie%2C+Li%C3%A8ge%2C+4000%2C+Li%C3%A8ge&date_debut=22-03-2019&date_fin=23-03-2019Then I can recover the data on a JSON file thanks to the developer tool of my web browser and scrap it. The issue is that the JSON file changes each time I launch a request for a new location and is located at the same URL (https://www.caramigo.eu/services/car).Does anybody has an idea on how I can create a spider which will launch a request, get the JSON file, and scrap it ? Or maybe on how I can change the data on the API directly to get other locations ? Thanks in advance !

2019-03-19 09:33:32Z

Good morning everyone,I try to collect data about cars on this site: https://www.caramigo.eu/To do so I need to launch a request on the search bar of the home page for a specified location and date. This provides me a page like this: https://www.caramigo.eu/be/fr/recherche?address=Belgique%2C+Wallonie%2C+Li%C3%A8ge%2C+4000%2C+Li%C3%A8ge&date_debut=22-03-2019&date_fin=23-03-2019Then I can recover the data on a JSON file thanks to the developer tool of my web browser and scrap it. The issue is that the JSON file changes each time I launch a request for a new location and is located at the same URL (https://www.caramigo.eu/services/car).Does anybody has an idea on how I can create a spider which will launch a request, get the JSON file, and scrap it ? Or maybe on how I can change the data on the API directly to get other locations ? Thanks in advance !Scrapy filters requests to URLs already visited, to avoid loops. Since the resource you need uses the same URL always, Scrapy will filter that.But you can disable it by using dont_filter=True in the request. Eg.:

Scrapy redirecting most urls to elb02.activatemymodem

superdee

[Scrapy redirecting most urls to elb02.activatemymodem](https://stackoverflow.com/questions/55171072/scrapy-redirecting-most-urls-to-elb02-activatemymodem)

I am running multiple Scrapy spiders on different websites on my Amazon EC2 Centos6 machine. Since last night, most of my spider requests have been getting redirected to this strange url: "https://elb02.activatemymodem.com". Sometimes it'll redirect to a weird combination of urls looking like this: "https://elb02.activatemymodem.comspa-accessories.html"I use proxies with my spiders, but my proxies come from multiple different unrelated sources, so I doubt it is an issue with a bad actor proxy.Does anyone have any idea what this could be? When I Google "elb02.activatemymodem.com" it shows it a time warner thing, which is the internet I use here. But I SSH into my Amazon EC2 instance, so why would it be redirecting there?I'm very lost, any pointers in the right direction would be amazing. It's almost as if my Centos6 machine has a virus?

2019-03-14 19:59:25Z

I am running multiple Scrapy spiders on different websites on my Amazon EC2 Centos6 machine. Since last night, most of my spider requests have been getting redirected to this strange url: "https://elb02.activatemymodem.com". Sometimes it'll redirect to a weird combination of urls looking like this: "https://elb02.activatemymodem.comspa-accessories.html"I use proxies with my spiders, but my proxies come from multiple different unrelated sources, so I doubt it is an issue with a bad actor proxy.Does anyone have any idea what this could be? When I Google "elb02.activatemymodem.com" it shows it a time warner thing, which is the internet I use here. But I SSH into my Amazon EC2 instance, so why would it be redirecting there?I'm very lost, any pointers in the right direction would be amazing. It's almost as if my Centos6 machine has a virus?

How my web crawler(python, Scrapy, Scrapy-splash) can crawl faster?

Hayden Jung

[How my web crawler(python, Scrapy, Scrapy-splash) can crawl faster?](https://stackoverflow.com/questions/55176278/how-my-web-crawlerpython-scrapy-scrapy-splash-can-crawl-faster)

Develop Environment:Server Specs:Hello.I'm a php developer, and this is my first python project. I'm trying to use python because I heard that python has many benefits for web crawling.I'm crawling one dynamic web site, and I need to crawl around 3,500 pages in every 5-15 seconds. For now, mine is too slow. It is crawl only 200 pages per minute.My source is like this:When execute these code, I'm using this command: python main.pyAfter seeing my code, please help me. I'll happily listen any saying.1.How my spider can faster? I've tried to use threading, but it's seem not working right.2.What is the best performance line up for web crawling?3.Is that possible to crawl 3500 dynamic pages in every 5-15 seconds?Thank you.

2019-03-15 05:46:34Z

Develop Environment:Server Specs:Hello.I'm a php developer, and this is my first python project. I'm trying to use python because I heard that python has many benefits for web crawling.I'm crawling one dynamic web site, and I need to crawl around 3,500 pages in every 5-15 seconds. For now, mine is too slow. It is crawl only 200 pages per minute.My source is like this:When execute these code, I'm using this command: python main.pyAfter seeing my code, please help me. I'll happily listen any saying.1.How my spider can faster? I've tried to use threading, but it's seem not working right.2.What is the best performance line up for web crawling?3.Is that possible to crawl 3500 dynamic pages in every 5-15 seconds?Thank you.

Using Scrapy and the HTML file returned is blacked out in the directory and opens to a 404 error. Any idea why?

Jashanjot Dhilon

[Using Scrapy and the HTML file returned is blacked out in the directory and opens to a 404 error. Any idea why?](https://stackoverflow.com/questions/55171484/using-scrapy-and-the-html-file-returned-is-blacked-out-in-the-directory-and-open)

The code I'm using is pretty much cut and paste from the tutorial on their site. The code runs ok but the HTML file it saves has a blacked out icon, and opens to a 404 page. EDIT: Looking into it more, it seems that this is due to some anti-scraping procedures. Is getting around this feasible/possible?

2019-03-14 20:31:19Z

The code I'm using is pretty much cut and paste from the tutorial on their site. The code runs ok but the HTML file it saves has a blacked out icon, and opens to a 404 page. EDIT: Looking into it more, it seems that this is due to some anti-scraping procedures. Is getting around this feasible/possible?

ReactorNotRestartable error, how to use scrapy CrawlerProcess in for loop

Kai

[ReactorNotRestartable error, how to use scrapy CrawlerProcess in for loop](https://stackoverflow.com/questions/55144884/reactornotrestartable-error-how-to-use-scrapy-crawlerprocess-in-for-loop)

I want use a sub-class to run my spider with different url and save info via loop. Here is my approach: after this, I encounter reactor issue: how to overcome this reactor error, thanks. 

2019-03-13 14:55:25Z

I want use a sub-class to run my spider with different url and save info via loop. Here is my approach: after this, I encounter reactor issue: how to overcome this reactor error, thanks. I'm not sure what exactly you plan on doing in save_info, but here is a minimal example of running the same spider multiple times sequentially. It is based on your class and the example in the documentation:

Scrapy request url comes from which url response

Ken

[Scrapy request url comes from which url response](https://stackoverflow.com/questions/55158511/scrapy-request-url-comes-from-which-url-response)

For Scrapy, we could get the response.url, response.request.url, but how do we know the response.url, response.request.url is extracted from which parent url?Thank you,

Ken

2019-03-14 09:03:30Z

For Scrapy, we could get the response.url, response.request.url, but how do we know the response.url, response.request.url is extracted from which parent url?Thank you,

KenYou can use Request.meta to keep track of such information.When you yield your request, include response.url in the meta:Then read it on your parsing method:That is the most straightforward way to do this, and you can use this method to keep track of original URLs even across different parsing methods, if you wish.Otherwise, you might want to look into taking advantage of the redirect_urls meta key, which keeps track of redirect jumps.

Scrapy - only scraping domain namess

David Strejc

[Scrapy - only scraping domain namess](https://stackoverflow.com/questions/55161589/scrapy-only-scraping-domain-namess)

How can I implement scraping only domain names with Scrapy.I am not interested in deep search of any domain.tld. My idea was only to use depth of 1 jump from index page of every domain - so direct links from homepage would be sufficient for links buffer. I need as fast crawler as only can be. I want to limit domains realm to .czThank you.

2019-03-14 11:40:05Z

How can I implement scraping only domain names with Scrapy.I am not interested in deep search of any domain.tld. My idea was only to use depth of 1 jump from index page of every domain - so direct links from homepage would be sufficient for links buffer. I need as fast crawler as only can be. I want to limit domains realm to .czThank you.You can use DEPTH_LIMIT parameter on SETTINGS to restrict the crawl to the depth you want.https://docs.scrapy.org/en/latest/topics/settings.html?highlight=depth_limitIf you want only to go 1 jump deep, you should set DEPTH_LIMIT=2 and select links using a selector or link_extractor.Ex: 

response.xpath('//a/@href').re(r'.*.example.com.*')https://docs.scrapy.org/en/latest/topics/selectors.html

https://docs.scrapy.org/en/latest/topics/spiders.html?highlight=link_extractor

Where does Scrapy actually do the html request?

superdee

[Where does Scrapy actually do the html request?](https://stackoverflow.com/questions/55148001/where-does-scrapy-actually-do-the-html-request)

I am using the Scrapy (Scrapy==1.6.0) library with Python3. I am wondering, where in the code does Scrapy actually do the HTML request? I want to set a breakpoint there so I can see exactly what headers / cookies / urls / and user agent is actually being passed.Also, where exactly is the response received as well? Right now my spider is failing to find any pages, so I imagine I'm getting either a blank HTML document or a 403 error, however I have no idea where to look to confirm this.Can anyone familiar with the scrapy library point me to exactly where in code I can check these parameters?

2019-03-13 17:31:44Z

I am using the Scrapy (Scrapy==1.6.0) library with Python3. I am wondering, where in the code does Scrapy actually do the HTML request? I want to set a breakpoint there so I can see exactly what headers / cookies / urls / and user agent is actually being passed.Also, where exactly is the response received as well? Right now my spider is failing to find any pages, so I imagine I'm getting either a blank HTML document or a 403 error, however I have no idea where to look to confirm this.Can anyone familiar with the scrapy library point me to exactly where in code I can check these parameters?I believe you can check out scrapy/core/engine.py method _download.

Though I'd suggest you make use of scrapy shell. It will let you execute particular request, inspect response, open response in browser to see what was received by Scrapy. Also with a bit more of tuning you can import your spider in your shell and call a particular method of your spider and put a breakpoint there.If your spider fails to find any pages then the problem is likely to be with your spider, not the framework.

Fail to telnet to the running Scrapy job with telnetlib since telnet console now requires username and password after Scrapy 1.5.2

my8100

[Fail to telnet to the running Scrapy job with telnetlib since telnet console now requires username and password after Scrapy 1.5.2](https://stackoverflow.com/questions/55163990/fail-to-telnet-to-the-running-scrapy-job-with-telnetlib-since-telnet-console-now)

I was able to telnet to the running Scrapy job with telnetlib when using Scrapy v1.5.1:But start from v1.5.2, the telnet console of Scrapy now requires username and password. And I am able to telnet to the running job with the telnet client on Win7, but fail with telnetlib:In the meanwhile, I got some unhandled error related to telnet in the log of Scrapyd:

2019-03-14 13:38:39Z

I was able to telnet to the running Scrapy job with telnetlib when using Scrapy v1.5.1:But start from v1.5.2, the telnet console of Scrapy now requires username and password. And I am able to telnet to the running job with the telnet client on Win7, but fail with telnetlib:In the meanwhile, I got some unhandled error related to telnet in the log of Scrapyd:

Distributed communication between Scrapy spiders

Bociek

[Distributed communication between Scrapy spiders](https://stackoverflow.com/questions/55158899/distributed-communication-between-scrapy-spiders)

I want to run two spiders in a coordinated fashion. First spider will scrape some website and produce URLs and the second one will consume these addresses. I can't wait for the first spider to finish and then launch the second one since the website is changing very fast and URLs produced by the first spider need to scraped right away. A very simple architecture is shown below. Currently, I am using Scrapy separately for each scraping job. Any idea how can I do it? Each spider behaves in different way (has different settings) and does different job. It would be nice to have them on different machines (distributed).

2019-03-14 09:24:06Z

I want to run two spiders in a coordinated fashion. First spider will scrape some website and produce URLs and the second one will consume these addresses. I can't wait for the first spider to finish and then launch the second one since the website is changing very fast and URLs produced by the first spider need to scraped right away. A very simple architecture is shown below. Currently, I am using Scrapy separately for each scraping job. Any idea how can I do it? Each spider behaves in different way (has different settings) and does different job. It would be nice to have them on different machines (distributed).One idea, maybe its a bad ideaRun 1st spider that saves scrapd URLs into DBRun 2nd Spider separately like thisit will keep getting URLs from table and scraping them immediately Your two spiders can be still be independent. They do not need to be coordinated and they do not need to communicate with each other. Both just need access to a central database.Spider1 is only responsible for populating a database table with URLs. And Spider2 is just responsible for reading from it (and maybe updating the rows if you want to keep track). Both spiders can start/stop independently. If Spider1 stops, then Spider2 can still keep going as long as there are URLs.In the case where there are currently no more URLs for Spider2, you can keep it alive by configuring a spider_idle signal that raises a DontCloseSpider exception (documentation). At this point you can also fetch a new batch of URLs from the database and crawl them (example of crawling in signal).Alternatively, you could just use something like cron to schedule an execution of Spider2 every few minutes. Then you don't have to worry about keeping it alive.

Scrapy xpath iterate (shell works)

Hi tE

[Scrapy xpath iterate (shell works)](https://stackoverflow.com/questions/55150055/scrapy-xpath-iterate-shell-works)

I am trying to scrape some info from the companieshouse of the UK using scrapy.

I made a connection with the website through the shell and throught he commandand withI managed to get the results back.I tried to put this into a program so i could export it to a csv or json. But I am having trouble getting it to work.. This is what i got;Very simple but tried a lot. Any insight would be appreciated!!

2019-03-13 19:42:26Z

I am trying to scrape some info from the companieshouse of the UK using scrapy.

I made a connection with the website through the shell and throught he commandand withI managed to get the results back.I tried to put this into a program so i could export it to a csv or json. But I am having trouble getting it to work.. This is what i got;Very simple but tried a lot. Any insight would be appreciated!!These lines of code are the problem:The start_requests method should return an iterable of Requests; yours returns None.The default start_requests creates this iterable from urls specified in start_urls, so simply defining that as a class variable (outside of any function) and not overriding start_requests will work as you want.Try to do:

Scrapy forloop node children

Shikuan Xu

[Scrapy forloop node children](https://stackoverflow.com/questions/55164249/scrapy-forloop-node-children)

If I know all the tags, I can get all the information.Buy if I don't know what tag is in <div class="root-div">??????</div>.How can i get every text.For example

2019-03-14 13:50:15Z

If I know all the tags, I can get all the information.Buy if I don't know what tag is in <div class="root-div">??????</div>.How can i get every text.For exampleIf you need to know the tag for each child element, do this:However, if you really just wanted the text for the child elements, do this:

Share web session between Scrapy and Selenium

Lyncoln Sousa

[Share web session between Scrapy and Selenium](https://stackoverflow.com/questions/55080952/share-web-session-between-scrapy-and-selenium)

I want to create a process that uses selenium and scrapy. I want to click a button with selenium and after pass the html page to scrapy, but I needed to login a site with selenium. If I just pass the url, scrapy will do nothing.

Can I 'share' the web session?

2019-03-09 19:07:13Z

I want to create a process that uses selenium and scrapy. I want to click a button with selenium and after pass the html page to scrapy, but I needed to login a site with selenium. If I just pass the url, scrapy will do nothing.

Can I 'share' the web session?

Check if URL exist in file before crawling with Scrapy

Mostafa Safarian

[Check if URL exist in file before crawling with Scrapy](https://stackoverflow.com/questions/55085996/check-if-url-exist-in-file-before-crawling-with-scrapy)

I want to use Scrapy to crawl a website's data. There's an element in every page content which is a URL.As the website have too many pages, I want to crawl only pages which contain a URL of specified in a TXT file by me.So crawler checks the website, extracts the response elements, checks if extracted URL from the page's content exist in that file, saves the response data into a JSON file.Here is what I've so far :Second problem:

It seems that the crawler doesn't stop crawling. The website doesn't have that many pages which this crawler save as results. It doesn't stop at all, more than 150K result files generated then I stopped command myself. I think it's recrawling results. Am I right? 

I know that scrapy will not crawl already crawled urls.But I think maybe something is wrong here which prevents this to happen.

2019-03-10 08:51:46Z

I want to use Scrapy to crawl a website's data. There's an element in every page content which is a URL.As the website have too many pages, I want to crawl only pages which contain a URL of specified in a TXT file by me.So crawler checks the website, extracts the response elements, checks if extracted URL from the page's content exist in that file, saves the response data into a JSON file.Here is what I've so far :Second problem:

It seems that the crawler doesn't stop crawling. The website doesn't have that many pages which this crawler save as results. It doesn't stop at all, more than 150K result files generated then I stopped command myself. I think it's recrawling results. Am I right? 

I know that scrapy will not crawl already crawled urls.But I think maybe something is wrong here which prevents this to happen.Have you considered storing the URLs you want to scrape in a database and passing them through as the start urls?

Scrapy, Splash and Connection was refused by other side: 10061

Peter K.

[Scrapy, Splash and Connection was refused by other side: 10061](https://stackoverflow.com/questions/55082855/scrapy-splash-and-connection-was-refused-by-other-side-10061)

I am using scrapy with splash on a Javascript driven site. However, I can't get passed a Connection was refused by other side: 10061 error.I get logs like this: and a traceback pointing to twisted:I have checked all the entries in settings, did try various USER_AGENTS and ROBOT entries, but no luck. Also tried to use --disable-private-mode to start splash, but no effect.Strangely, just copy-pasting the same url into the browser works perfectly. I used normal command line scrapy, as well as via the API. Interestingly, when using the API, of course, clicking the url of the target in the error message within PyCharm, the hashtag # is replaced by its escape-code. So I am confused whether under the hud this is another issue or whether the two are related together. Even tried to look at the packages sent via both Wireshark and Fiddler, but was not able to understand the results well enough, as I never used these tools before.Any suggestions would be greatly appreciated.

2019-03-09 23:06:16Z

I am using scrapy with splash on a Javascript driven site. However, I can't get passed a Connection was refused by other side: 10061 error.I get logs like this: and a traceback pointing to twisted:I have checked all the entries in settings, did try various USER_AGENTS and ROBOT entries, but no luck. Also tried to use --disable-private-mode to start splash, but no effect.Strangely, just copy-pasting the same url into the browser works perfectly. I used normal command line scrapy, as well as via the API. Interestingly, when using the API, of course, clicking the url of the target in the error message within PyCharm, the hashtag # is replaced by its escape-code. So I am confused whether under the hud this is another issue or whether the two are related together. Even tried to look at the packages sent via both Wireshark and Fiddler, but was not able to understand the results well enough, as I never used these tools before.Any suggestions would be greatly appreciated.Finally, managed to identify the culprit. It was indeed the connection to the docker container. First, I had to retrieve the docker container IP usingin the docker terminal. Next, I had to adjust SPLASH_URL in the scrapy settings.py file to point to the docker-machine ip instead of localhost:8050, and voila ... it works.Unfortunately, the sources I have seen so far have been rather unclear about this, so I hope this will be of some use for other poor souls setting splash up for the first time.

Scrapy request don't get parsed

Aminah Nuraini

[Scrapy request don't get parsed](https://stackoverflow.com/questions/55089271/scrapy-request-dont-get-parsed)

All of my requests don't get parsed. It's successfully parsed though. Here is my code:

2019-03-10 15:30:39Z

All of my requests don't get parsed. It's successfully parsed though. Here is my code:Found it! It happened because I had two parse function with the exact same name! I forgot to update one of the parse name to parse_bg

Error while installing openSSL in anaconda prompt

Thendral Nilavu

[Error while installing openSSL in anaconda prompt](https://stackoverflow.com/questions/55015883/error-while-installing-openssl-in-anaconda-prompt)

I got an error while creating an scrapy project to install openSSL.. Hence I installed it through the command conda install -c anaconda openSSL.... Now I get an new error saying How do I resolve this error

2019-03-06 05:02:31Z

I got an error while creating an scrapy project to install openSSL.. Hence I installed it through the command conda install -c anaconda openSSL.... Now I get an new error saying How do I resolve this error

How to assign an ID for each start_url in scrapy from dataframe

Anton

[How to assign an ID for each start_url in scrapy from dataframe](https://stackoverflow.com/questions/55022228/how-to-assign-an-id-for-each-start-url-in-scrapy-from-dataframe)

Lets say I have a dataframe as such:If I want to iterate each url in the dataframe. So what I'll do is:and that'll iterate over my urls, and yield an item for each of them. The problem is that with the yielded file I have no way of telling different id apart. The urls arent unique and I can't assign the URL as an "id", I need the "id" column from my dataframe combined with the URL to generate a unique id. How can i access the id column when iterating over my urls? Or alternatively what other approaches could I take to be able to achieve what I want?EDIT: I have tried to save url as an "id" but that doesn't work due to the urls not being unique, scrapy also works asynchronously so the order of the items will not remain constant.

2019-03-06 11:38:33Z

Lets say I have a dataframe as such:If I want to iterate each url in the dataframe. So what I'll do is:and that'll iterate over my urls, and yield an item for each of them. The problem is that with the yielded file I have no way of telling different id apart. The urls arent unique and I can't assign the URL as an "id", I need the "id" column from my dataframe combined with the URL to generate a unique id. How can i access the id column when iterating over my urls? Or alternatively what other approaches could I take to be able to achieve what I want?EDIT: I have tried to save url as an "id" but that doesn't work due to the urls not being unique, scrapy also works asynchronously so the order of the items will not remain constant.You could try iterrows:https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.htmlDespite duplicate urls, I could still use "response.url" as an ID despite duplicate records. Duplicate records would regardless return the same response and therefore I can go back to my dataframe and attach the same information to every place where I have that ID.

Scrapy Spider Returns Only White Space Characters

Jade Cowan

[Scrapy Spider Returns Only White Space Characters](https://stackoverflow.com/questions/54975841/scrapy-spider-returns-only-white-space-characters)

I'm trying to scrape data from the following URL:https://www.cheyennecity.org/Jobs.aspx?UniqueId=86&From=Professional-86&CommunityJobs=False&JobID=Senior-Planning-Technician-MPO-933I've been using the scrapy shell command, so I could debug the responses I was getting back from crawling the site. When I'm using the response.css('#divSideBar div h3').get(default='') in the terminal, I get an empty response. I ended up going up a level with the following selector... response.css('#divSideBar').get(default='') and I get a bunch of white space characters \r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tI can select the elements just fine with the developer tools in Chrome. I checked the network tab in Chrome as well and the content is coming from the URL I'm scraping: Is there a way to access the contents of the element with the #divSideBar id?

2019-03-04 02:26:20Z

I'm trying to scrape data from the following URL:https://www.cheyennecity.org/Jobs.aspx?UniqueId=86&From=Professional-86&CommunityJobs=False&JobID=Senior-Planning-Technician-MPO-933I've been using the scrapy shell command, so I could debug the responses I was getting back from crawling the site. When I'm using the response.css('#divSideBar div h3').get(default='') in the terminal, I get an empty response. I ended up going up a level with the following selector... response.css('#divSideBar').get(default='') and I get a bunch of white space characters \r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tI can select the elements just fine with the developer tools in Chrome. I checked the network tab in Chrome as well and the content is coming from the URL I'm scraping: Is there a way to access the contents of the element with the #divSideBar id?Actually all the data is coming from an dynamic post request. What you need to do is send the another FormRequest with some essential parameter as per the request which you can see in the inspect Network header tab. 

Python + web scraping + scrapy : How to get the links to all movies from an IMDb page?

Debbie

[Python + web scraping + scrapy : How to get the links to all movies from an IMDb page?](https://stackoverflow.com/questions/54982251/python-web-scraping-scrapy-how-to-get-the-links-to-all-movies-from-an-imdb)

I have to scrape all movies from this IMDb page : https://www.imdb.com/list/ls055386972/.My approach is first to scrape all the values of <a href="/title/tt0068646/?ref_=ttls_li_tt" , i.e., to extract /title/tt0068646/?ref_=ttls_li_tt portions and then add 'https://www.imdb.com' to prepare the complete URL to the movie, i.e., https://www.imdb.com/title/tt0068646/?ref_=ttls_li_tt . But whenever I am giving response.xpath('//h3[@class]/a[@href]').extract() it is extracting the desired portion along with the movie title: [u'<a href="/title/tt0068646/?ref_=ttls_li_tt">The Godfather</a>', u'<a href="/title/tt0108052/?ref_=ttls_li_tt">Schindler\'s List</a>......]'I want only the "/title/tt0068646/?ref_=ttls_li_tt" portion. How to proceed?

2019-03-04 11:23:43Z

I have to scrape all movies from this IMDb page : https://www.imdb.com/list/ls055386972/.My approach is first to scrape all the values of <a href="/title/tt0068646/?ref_=ttls_li_tt" , i.e., to extract /title/tt0068646/?ref_=ttls_li_tt portions and then add 'https://www.imdb.com' to prepare the complete URL to the movie, i.e., https://www.imdb.com/title/tt0068646/?ref_=ttls_li_tt . But whenever I am giving response.xpath('//h3[@class]/a[@href]').extract() it is extracting the desired portion along with the movie title: [u'<a href="/title/tt0068646/?ref_=ttls_li_tt">The Godfather</a>', u'<a href="/title/tt0108052/?ref_=ttls_li_tt">Schindler\'s List</a>......]'I want only the "/title/tt0068646/?ref_=ttls_li_tt" portion. How to proceed?OUTPUT:I would suggest you to use requests-html to get all the hyperlinks and remove the ones that doesn't match your criteria. You can even get the absolute url using r.html.absolute_linksit is the working code please try:

Python + scrapy + web scraping : page is not being scraped

Debbie

[Python + scrapy + web scraping : page is not being scraped](https://stackoverflow.com/questions/54983649/python-scrapy-web-scraping-page-is-not-being-scraped)

I want to scrape movie titles from this page: https://www.imdb.com/list/ls055386972/ . I wrote the following code:But it's not scraping anything. I am getting this error message:print('final_links') produces the correct links to individual movie pages:

2019-03-04 12:49:14Z

I want to scrape movie titles from this page: https://www.imdb.com/list/ls055386972/ . I wrote the following code:But it's not scraping anything. I am getting this error message:print('final_links') produces the correct links to individual movie pages:You are getting a 404 because your starting URL is incorrect. You need to remove the trailing forward slash in start_urls:Also, your allowed_domains is incorrect. It should contain only domains, not partial URLs:Refer to the documentation.Not sure about Scrapy, but if you use below code you get desired output.You haven't parsed start url to the parse function, below is working code.

Scrapy hanging with any more than five spiders running

superdee

[Scrapy hanging with any more than five spiders running](https://stackoverflow.com/questions/54974507/scrapy-hanging-with-any-more-than-five-spiders-running)

I am trying to run multiple scrapy spiders at once, each from the command line. For example: command However, once I already have 5 spiders running (all on different domains), the next one I try to run hangs with the message: I don't think its an issue with the 6th site I am trying to run (it works fine when less than 5 spiders are running), but instead the fact that 5 spiders are already running.Here is my very log output long incase it helps, any ideas how I can get more than 5 spiders running at once?

2019-03-03 22:41:20Z

I am trying to run multiple scrapy spiders at once, each from the command line. For example: command However, once I already have 5 spiders running (all on different domains), the next one I try to run hangs with the message: I don't think its an issue with the 6th site I am trying to run (it works fine when less than 5 spiders are running), but instead the fact that 5 spiders are already running.Here is my very log output long incase it helps, any ideas how I can get more than 5 spiders running at once?

How to imitate the browser to find & count text

xuke

[How to imitate the browser to find & count text](https://stackoverflow.com/questions/54994471/how-to-imitate-the-browser-to-find-count-text)

I'm trying to count words which their visibility are visible in browser.

I'm using Scrapy to get link and parse theme with Selector.Problem is I can only count all texts in spite of their visibility (hidden, in menu, in blockquote...) and the searching sites is a list of url (not the same structure)Do you have any suggestion?

2019-03-05 02:15:59Z

I'm trying to count words which their visibility are visible in browser.

I'm using Scrapy to get link and parse theme with Selector.Problem is I can only count all texts in spite of their visibility (hidden, in menu, in blockquote...) and the searching sites is a list of url (not the same structure)Do you have any suggestion?Scrapy just give you the page source(ctrl+u) to get the rendered page you have to use the Selenium  or splash my splash is little speed compare to the Selenium but Selenium give you full control

Scrapy yielding items and requesting link consecutively

jayjey

[Scrapy yielding items and requesting link consecutively](https://stackoverflow.com/questions/54996698/scrapy-yielding-items-and-requesting-link-consecutively)

my spider starts on this page https://finviz.com/screener.ashx and visits every link in the table to yield some items on the other side. This worked perfectly fine. I then wanted to add another layer of depth by having my spider visit a link on the page it initially visits like so:The spider is supposed to visit "url", yield some items along the way, then visit "url_2" and yield a few more items and then move on to the next url from the start_url. Here is my spider code: All of the xpaths and links are right, I just can't seem to yield anything at all now. I have a feeling there is an obvious mistake here. My first try at a more elaborate spider. Any help would be greatly appreciated! Thank you!***EDIT 2*** EDIT 3Managed to actually have the spider travel to url2 and yield the items there. The problem is it only does it rarely. Most of the time it redirects to the correct link and gets nothing, or doesn't seem to redirect at all and continues on. Not really sure why there is such inconsistency here. The other thing is, I know I've managed to yield a few values on url2 succesfully though they don't appear in my CSV output. I realize this could be a export issue. I updated my code to how it is currently.

2019-03-05 06:31:43Z

my spider starts on this page https://finviz.com/screener.ashx and visits every link in the table to yield some items on the other side. This worked perfectly fine. I then wanted to add another layer of depth by having my spider visit a link on the page it initially visits like so:The spider is supposed to visit "url", yield some items along the way, then visit "url_2" and yield a few more items and then move on to the next url from the start_url. Here is my spider code: All of the xpaths and links are right, I just can't seem to yield anything at all now. I have a feeling there is an obvious mistake here. My first try at a more elaborate spider. Any help would be greatly appreciated! Thank you!***EDIT 2*** EDIT 3Managed to actually have the spider travel to url2 and yield the items there. The problem is it only does it rarely. Most of the time it redirects to the correct link and gets nothing, or doesn't seem to redirect at all and continues on. Not really sure why there is such inconsistency here. The other thing is, I know I've managed to yield a few values on url2 succesfully though they don't appear in my CSV output. I realize this could be a export issue. I updated my code to how it is currently.url2 is a relative path, but scrapy.Request expects a full URL.Try this:Or even simpler:

builtins.ModuleNotFoundError: No module named '_sqlite3' windows 10

Arun Augustine

[builtins.ModuleNotFoundError: No module named '_sqlite3' windows 10](https://stackoverflow.com/questions/54979824/builtins-modulenotfounderror-no-module-named-sqlite3-windows-10)

Got error from scrapy while converting scrapy project in to windows executable using pyinstaller.Below shows my system scrapy version detailsscrapy version -vAlso sqlite3 in working via python import.

Can anyone found similar issues in windows ?

Thanks in advacce

2019-03-04 09:02:53Z

Got error from scrapy while converting scrapy project in to windows executable using pyinstaller.Below shows my system scrapy version detailsscrapy version -vAlso sqlite3 in working via python import.

Can anyone found similar issues in windows ?

Thanks in advacce

How can I navigate between links with Scrapy?

malberts

[How can I navigate between links with Scrapy?](https://stackoverflow.com/questions/54980716/how-can-i-navigate-between-links-with-scrapy)

I want to get data from a website but to get the data I have to access a link from the home page then get data after that I need to go back to the home page and then repeat the cycle where I access a link, get the data and then go back.I know how to access the link and get the data but I'd like to know how can I access to the other links and go back to where I was after accessing the first link.Here is what I currently code :

2019-03-04 09:56:47Z

I want to get data from a website but to get the data I have to access a link from the home page then get data after that I need to go back to the home page and then repeat the cycle where I access a link, get the data and then go back.I know how to access the link and get the data but I'd like to know how can I access to the other links and go back to where I was after accessing the first link.Here is what I currently code :You don't need to go back and forth between pages to follow every link on the home page. Instead, select and yield all the home page links in the first place. When selecting multiple links you need to use getall() to get all of the matches. get() returns only the first match. Then you need to loop over the results:

how can i get the text clean with scrapy shell

Aurelio Mesquita

[how can i get the text clean with scrapy shell](https://stackoverflow.com/questions/55006903/how-can-i-get-the-text-clean-with-scrapy-shell)

I'm trying the following command on scrapy shell wich returns this result:The thing is, I want to extract only the word "Ajax" thats is between <strong> tags.

im new at this and would love some help.

2019-03-05 16:02:21Z

I'm trying the following command on scrapy shell wich returns this result:The thing is, I want to extract only the word "Ajax" thats is between <strong> tags.

im new at this and would love some help.You need to add <strong> tag to your selector 

Anaconda Scrapy Installation Error OSError(22, 'The parameter is incorrect', None, 87, None)

Scott Tiers

[Anaconda Scrapy Installation Error OSError(22, 'The parameter is incorrect', None, 87, None)](https://stackoverflow.com/questions/54992573/anaconda-scrapy-installation-error-oserror22-the-parameter-is-incorrect-non)

I have been trying to install Scrapy but every time I get this error:I'm using the code line in the Scrapy manual to the Anaconda PromptHelp me out of this 

Thanks

2019-03-04 22:28:54Z

I have been trying to install Scrapy but every time I get this error:I'm using the code line in the Scrapy manual to the Anaconda PromptHelp me out of this 

Thanks

Python Trim Help Variable both in single and double bracket

user263504

[Python Trim Help Variable both in single and double bracket](https://stackoverflow.com/questions/54941404/python-trim-help-variable-both-in-single-and-double-bracket)

My Scrapy codes returning me a text --How to trim it from src="// to ".I am very new to Python.

2019-03-01 09:17:04Z

My Scrapy codes returning me a text --How to trim it from src="// to ".I am very new to Python.Check this snippet. You can apply regexps to selectors:So, and then you can do [response.urljoin(url) for url in sel.re("src\s?=\s?['\"]([^'\"]+)['\"]")]You should use python regexp.

Here is an exemple from the python interpreter. Try it, and accept if it fits your need.

How to skip duplicate in scrapy python

Rayly Esta

[How to skip duplicate in scrapy python](https://stackoverflow.com/questions/54997878/how-to-skip-duplicate-in-scrapy-python)

I am new to Scrapy. I wrote this script :When I run the script again I want to skip duplicates before the #Response.

I try a few things but did not work.

2019-03-05 07:55:24Z

I am new to Scrapy. I wrote this script :When I run the script again I want to skip duplicates before the #Response.

I try a few things but did not work.To continue crawling from the point where you stopped in a previous run, you have to make sure all required information is persisted after a run. As the scrapy documentation states:You can either add this setting via a settings.py file:or by starting your spider with the following command:You should prefer the first option as it allows you to easily add more settings to your spider.

How to make scrapy requests synchronous

S. Souza

[How to make scrapy requests synchronous](https://stackoverflow.com/questions/54945560/how-to-make-scrapy-requests-synchronous)

I recently started to use Scrapy and Python, so please bear with me.

I based my code from this 

tutorial.

I need to get some information from all cities of my country (Brazil) in different years from this website. 

The options of the dropdown are generated dynamically with an AJAX request. Thus, first I get all years and states, then I make an request to get the cities from each state.I learnt that if I use a return inside my loop, as in the code, it will finish my function, the problem is that, if I use an YIELD the request follow no order (probably because the requests are asynchronous? Let me know the reason as well), i.e it makes a request to a city with the wrong state. Therefore, I receive the wrong response.

By the way, although using the return it finishes the function, it make the correct request.My requests are created inside the loop, what I'd expect to happen is: 

  First: print the year and the state and then make request.

  Second: the callback would grab all cities and make a request for that 

    city, in that state in that year. 

  Third: the parse_result would fetch the information that I need.

Instead what happens is that it print all year and state, which means it's not executing synchronouslyHow do I make this synchronous? How can I make sure that my requests will follow the correct order of my array?Thanks a lot in advanceTying to be more clear:

2019-03-01 13:21:43Z

I recently started to use Scrapy and Python, so please bear with me.

I based my code from this 

tutorial.

I need to get some information from all cities of my country (Brazil) in different years from this website. 

The options of the dropdown are generated dynamically with an AJAX request. Thus, first I get all years and states, then I make an request to get the cities from each state.I learnt that if I use a return inside my loop, as in the code, it will finish my function, the problem is that, if I use an YIELD the request follow no order (probably because the requests are asynchronous? Let me know the reason as well), i.e it makes a request to a city with the wrong state. Therefore, I receive the wrong response.

By the way, although using the return it finishes the function, it make the correct request.My requests are created inside the loop, what I'd expect to happen is: 

  First: print the year and the state and then make request.

  Second: the callback would grab all cities and make a request for that 

    city, in that state in that year. 

  Third: the parse_result would fetch the information that I need.

Instead what happens is that it print all year and state, which means it's not executing synchronouslyHow do I make this synchronous? How can I make sure that my requests will follow the correct order of my array?Thanks a lot in advanceTying to be more clear:If I understood correctly, the problem here is because the session information is stored in a stateful server. Right?A way to handle this would be having one session for each state, managing it through cookiejars. E.g:More info about cookiejars.And yes, the requests in scrapy are scheduled to run asynchronously. That why we should provide callback functions to it.

Sort Scrapy results by url

lf_celine

[Sort Scrapy results by url](https://stackoverflow.com/questions/54947082/sort-scrapy-results-by-url)

I just started using Scrapy for webscraping a website.I have more than 9000 urls to scrape.I already tried and it worked, except that I'd like to output results in json files according to the url (if I scraped ten items from url1, I'd like these items in a json object with the url1, same for url2 etc.)Is it possible to do this or is it better to scrape the entire site and then sort it after the work ?My code right now:

2019-03-01 14:54:32Z

I just started using Scrapy for webscraping a website.I have more than 9000 urls to scrape.I already tried and it worked, except that I'd like to output results in json files according to the url (if I scraped ten items from url1, I'd like these items in a json object with the url1, same for url2 etc.)Is it possible to do this or is it better to scrape the entire site and then sort it after the work ?My code right now:Simply using response.url wouldn't do the job?

How do we pass parameters to responsive table filters from python?

James Reade

[How do we pass parameters to responsive table filters from python?](https://stackoverflow.com/questions/54950526/how-do-we-pass-parameters-to-responsive-table-filters-from-python)

I want to scrape a responsive table that has two filters, one for a simple text search and the other for number of rows displayed per page. Is it possible to pass parameters to the filters through my request? 

2019-03-01 18:43:12Z

I want to scrape a responsive table that has two filters, one for a simple text search and the other for number of rows displayed per page. Is it possible to pass parameters to the filters through my request? That table is filtered client side only. So that means all the data is already there, but just hidden. To get all the rows you can simply use one of:If you want to interact with the form you will need to use Scrapy Splash in order to execute the Javascript necessary to do the filtering.

Scrapy to follow links from HTML a data-link (not href)

Chris

[Scrapy to follow links from HTML a data-link (not href)](https://stackoverflow.com/questions/54955889/scrapy-to-follow-links-from-html-a-data-link-not-href)

I've got the following HTML:If I run my spider, it seems to not use this link (only a href).

Is it anyhow possible for Scrapy to follow data-link links as well?

2019-03-02 06:36:23Z

I've got the following HTML:If I run my spider, it seems to not use this link (only a href).

Is it anyhow possible for Scrapy to follow data-link links as well?You can get that with a selector like this: 

Scrapy only captures one question per page, but there are 10

Antonio Oliveira

[Scrapy only captures one question per page, but there are 10](https://stackoverflow.com/questions/54961121/scrapy-only-captures-one-question-per-page-but-there-are-10)

I've been trying to solve this problem for 5 days.

If anyone can help me, thank you:Scrapy only captures one question per page. Each page has 10 questions.I have already used CSS, xpath + regex, relative address, absolute address, LinkExtractor.I already disabled obey robots.txt, I already used proxy.In the scrapy shell, with get() also only captures a question, with get_all() captures all in one.My scrapy.py:My items.py

2019-03-02 17:27:56Z

I've been trying to solve this problem for 5 days.

If anyone can help me, thank you:Scrapy only captures one question per page. Each page has 10 questions.I have already used CSS, xpath + regex, relative address, absolute address, LinkExtractor.I already disabled obey robots.txt, I already used proxy.In the scrapy shell, with get() also only captures a question, with get_all() captures all in one.My scrapy.py:My items.pyYou need to loop over each of the question containers and then select the fields relative to that. Here is an example in scrapy shell:

How should I choose input_processor and output_processor in scrapy, i don't see any different between them since they are going to pipeline eventually

racle o

[How should I choose input_processor and output_processor in scrapy, i don't see any different between them since they are going to pipeline eventually](https://stackoverflow.com/questions/54924697/how-should-i-choose-input-processor-and-output-processor-in-scrapy-i-dont-see)

In the documentation, it is said that the input processor processes the extracted data as soon as it’s received, when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the item.How should I choose input_processor and output_processor in scrapy, I'm really confused now.Also, is there any different between define processor in itemloader class or in field?

2019-02-28 11:29:53Z

In the documentation, it is said that the input processor processes the extracted data as soon as it’s received, when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the item.How should I choose input_processor and output_processor in scrapy, I'm really confused now.Also, is there any different between define processor in itemloader class or in field?The key difference is the input processor runs on each list of selected values separately, whereas the output processor runs on a list of all those values returned by the input processors. That distinction is not apparent when you're attaching only a single selector to a field. However, if you add multiple selectors (like in their example) you'll notice it. In other words, in a scenario like that you can only make a final decision on which value(s) to select when you have access to all the values.Generally you would use input processors to do text preprocessing on the values (like changing case, stripping spaces, etc.), whereas the output processor is for selecting the final value(s).Of course, you 're not required to define either if you don't have a specific need. A typical scenario would be to have no input processors and just a single TakeFirst output processor for when you're just selecting single values.Also, while it is possible to perform that same text preprocessing in the output processor, it is better to keep things separate in case you plan on reusing processors.Regarding where you define the processors: it affects the precedence order (as mentioned here) But most of that only really comes into play when you start reusing processors and loaders for different items and you want certain ones to be overridden. For a single item and a single loader there's no real practical difference.

Scrapy gets TimeoutError despite download_timeout>download_latency

Borun Chowdhury

[Scrapy gets TimeoutError despite download_timeout>download_latency](https://stackoverflow.com/questions/54964214/scrapy-gets-timeouterror-despite-download-timeoutdownload-latency)

I have several instance of failure with the error twisted.internet.error.TimeoutError with the messageThe meta field of the request object showsI have removed the actual urls and download slots as they are not relevant but the rest of the numbers are real. I will not pretend to understand the meaning of them all but in particular I understand download_latency to mean the actual time the downloader spent on the request. If this is correct then I do not undestand how despite it spending only 75 seconds it gave a TimeoutError when the download_timeout is 180 seconds. Infact the failure message explicitely state that it took more than 180s. How is this 180 seconds calculated? From which step to which step and what is the download_latency?Like I said I have tons of these errors.

2019-03-02 23:48:04Z

I have several instance of failure with the error twisted.internet.error.TimeoutError with the messageThe meta field of the request object showsI have removed the actual urls and download slots as they are not relevant but the rest of the numbers are real. I will not pretend to understand the meaning of them all but in particular I understand download_latency to mean the actual time the downloader spent on the request. If this is correct then I do not undestand how despite it spending only 75 seconds it gave a TimeoutError when the download_timeout is 180 seconds. Infact the failure message explicitely state that it took more than 180s. How is this 180 seconds calculated? From which step to which step and what is the download_latency?Like I said I have tons of these errors.

SQL connection issues with background Python script

Steve M

[SQL connection issues with background Python script](https://stackoverflow.com/questions/54967932/sql-connection-issues-with-background-python-script)

I have a python script that's processing and writing data to an MS SQL database. I don't think there is anything special about this script, with below example showing the stock-standard way to get a connection. The problems I am experiencing:It's almost like there is contention for the database, but I can't see a reason why. The script is doing a lot of SQL Inserts, but all will be complete within a couple of milliseconds. So I would have imagined that python and SQL would have turns at the database, and both would get at the data -- apparently not the case.Sample code:

2019-03-03 10:35:30Z

I have a python script that's processing and writing data to an MS SQL database. I don't think there is anything special about this script, with below example showing the stock-standard way to get a connection. The problems I am experiencing:It's almost like there is contention for the database, but I can't see a reason why. The script is doing a lot of SQL Inserts, but all will be complete within a couple of milliseconds. So I would have imagined that python and SQL would have turns at the database, and both would get at the data -- apparently not the case.Sample code:

Lxml import issues when using Scrapy

Anton

[Lxml import issues when using Scrapy](https://stackoverflow.com/questions/54946842/lxml-import-issues-when-using-scrapy)

I am trying to use Scrapy with Anaconda/Miniconda on Windows 10. Installation goes fine, but trying to actually run Scrapy gives the following error:I have tried reinstalling Scrapy, lxml, and Anaconda itself (this time, I'm using a clean install of Miniconda), as well as downloading unofficial lxml build from https://www.lfd.uci.edu/~gohlke/pythonlibs/, as suggested in one of the answers on Stack Overflow, but the problem persists. I have also done this on an Amazon AWS EC2 instance started from scratch, but I'm getting the same issue.It seems to be something relatively common, but I couldn't find an answer that would work for me. What's an appropriate way to address this? Is it just about lxml, or is there something else causing this problem? 

2019-03-01 14:40:09Z

I am trying to use Scrapy with Anaconda/Miniconda on Windows 10. Installation goes fine, but trying to actually run Scrapy gives the following error:I have tried reinstalling Scrapy, lxml, and Anaconda itself (this time, I'm using a clean install of Miniconda), as well as downloading unofficial lxml build from https://www.lfd.uci.edu/~gohlke/pythonlibs/, as suggested in one of the answers on Stack Overflow, but the problem persists. I have also done this on an Amazon AWS EC2 instance started from scratch, but I'm getting the same issue.It seems to be something relatively common, but I couldn't find an answer that would work for me. What's an appropriate way to address this? Is it just about lxml, or is there something else causing this problem? 

Scrapy: How can I identify if the LinkExtractor found a relative URL

kaws

[Scrapy: How can I identify if the LinkExtractor found a relative URL](https://stackoverflow.com/questions/54891210/scrapy-how-can-i-identify-if-the-linkextractor-found-a-relative-url)

Right now Scrapy cretes an aboslute URL out of relative URLs with links that are parsed from LinkExtractor.  I would like to identify if the parsed link is a relative URL, how would I go aobut doing that?I see that in htmlparser.py a relative URL is joined with the base_url to make an aboslute URL, but I'm not sure how to pass link.url back to the bot.pyI can identify if the path(link.url) is a relative link with:

2019-02-26 17:38:38Z

Right now Scrapy cretes an aboslute URL out of relative URLs with links that are parsed from LinkExtractor.  I would like to identify if the parsed link is a relative URL, how would I go aobut doing that?I see that in htmlparser.py a relative URL is joined with the base_url to make an aboslute URL, but I'm not sure how to pass link.url back to the bot.pyI can identify if the path(link.url) is a relative link with:

Scrapy - avoid duplicate items when crawling multiple pages recursively

Alex16237

[Scrapy - avoid duplicate items when crawling multiple pages recursively](https://stackoverflow.com/questions/54842640/scrapy-avoid-duplicate-items-when-crawling-multiple-pages-recursively)

What should i change in my code to avoid Scrapy retrieving same items during deep crawl into multiple pages?Right now, Scrapy performs crawling and scraping like thisVisit Page-A >> ScrapeItem1 & Extract_link_to_Page-B >> Visit Page-B >> ScrapeItem2 & Extract_links_to_Pages-C-D-E >> ScrapeItems2-3-4-5 from Pages-C-D-ECode looks like thisProblem is that items are repeated, because Scrapy performs PARTIAL scraping on each function/meta item. So, i get entries like this:Step1:Step2:Step3:Each STEP repeats values from previous one. I know that this is caused by Scrapy visiting URLS multiple times. There is some error in my logic which i cannot full grasp. 

2019-02-23 14:30:03Z

What should i change in my code to avoid Scrapy retrieving same items during deep crawl into multiple pages?Right now, Scrapy performs crawling and scraping like thisVisit Page-A >> ScrapeItem1 & Extract_link_to_Page-B >> Visit Page-B >> ScrapeItem2 & Extract_links_to_Pages-C-D-E >> ScrapeItems2-3-4-5 from Pages-C-D-ECode looks like thisProblem is that items are repeated, because Scrapy performs PARTIAL scraping on each function/meta item. So, i get entries like this:Step1:Step2:Step3:Each STEP repeats values from previous one. I know that this is caused by Scrapy visiting URLS multiple times. There is some error in my logic which i cannot full grasp. Problem solved.Corresponding answer:https://stackoverflow.com/a/16177544/11008259Code is corrected for my case.We establish chain between items by not yielding items on each step, but yielding it at last step. Each function returns request to next one, therefore items are printed only when all of the functions complete their run.

XHR request pulls a lot of HTML content, how can I scrap it/crawl it?

user3303019

[XHR request pulls a lot of HTML content, how can I scrap it/crawl it?](https://stackoverflow.com/questions/54846781/xhr-request-pulls-a-lot-of-html-content-how-can-i-scrap-it-crawl-it)

So, I'm trying to scrape a website with infinite scrolling. I'm following this tutorial on scrapping infinite scrolling web pages: https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016But the example given looks pretty easy, it's an orderly JSON object with the data you want. I want to scrape this https://www.bahiablancapropiedades.com/buscar#/terrenos/venta/bahia-blanca/todos-los-barrios/rango-min=50.000,rango-max=350.000The XHR response for each page is weird, looks like corrupted html code

This is how the Network tab looksI'm not sure how to navigate the items inside "view". I want the spider to enter each item and crawl some information for every one. In the past I've succesfully done this with normal pagination and rules guided by xpaths.

2019-02-23 22:16:57Z

So, I'm trying to scrape a website with infinite scrolling. I'm following this tutorial on scrapping infinite scrolling web pages: https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016But the example given looks pretty easy, it's an orderly JSON object with the data you want. I want to scrape this https://www.bahiablancapropiedades.com/buscar#/terrenos/venta/bahia-blanca/todos-los-barrios/rango-min=50.000,rango-max=350.000The XHR response for each page is weird, looks like corrupted html code

This is how the Network tab looksI'm not sure how to navigate the items inside "view". I want the spider to enter each item and crawl some information for every one. In the past I've succesfully done this with normal pagination and rules guided by xpaths.It is not corrupted HTML, it is escaped to prevent it from breaking the JSON. Some websites will return simple JSON data and others, like this one, will return the actual HTML to be added.To get the elements you need to get the HTML out of the JSON response and create your own parsel Selector (this is the same as when you use response.css(...)).You can try the following in scrapy shell to get all the links in one of the "next" pages:scrapy shell https://www.bahiablancapropiedades.com/buscar/resultados/3https://www.bahiablancapropiedades.com/buscar/resultados/0 This is XHR url. 

While scrolling the page it will appear the 8 records per request.

So do one thing get all records XPath. these records divide by 8. it will appear the count of XHR requests.

do below process. your issue will solve. I get the same issue as me. I applied below logic. it will resolve.pass this url to your scrapy funciton. 

evaljs returns nil even though it shouldn't?

so stack

[evaljs returns nil even though it shouldn't?](https://stackoverflow.com/questions/54856837/evaljs-returns-nil-even-though-it-shouldnt)

The following code ...splash:evaljs('document.querySelectorAll("iframe.iframe-container.js-oddset-game-iframe")[0].contentDocument.querySelectorAll("td.leftText a.eventLink").length') ... returns 8 - i.e. there are 8 nodes in the array. However, when I then try to return the nodeList (array) directly, the result is nil? Obviously a table should be returned since an array is returned from the javascript code.Is this a bug in Splash? Can't Splash handle access to elements in iframes? I have the --js-cross-domain-access option on too.

2019-02-24 21:39:23Z

The following code ...splash:evaljs('document.querySelectorAll("iframe.iframe-container.js-oddset-game-iframe")[0].contentDocument.querySelectorAll("td.leftText a.eventLink").length') ... returns 8 - i.e. there are 8 nodes in the array. However, when I then try to return the nodeList (array) directly, the result is nil? Obviously a table should be returned since an array is returned from the javascript code.Is this a bug in Splash? Can't Splash handle access to elements in iframes? I have the --js-cross-domain-access option on too.It is not a bug. iframes are only available when you use the render.json endpoint with the iframes=1 parameter. When you use that you cannot run a custom Lua script.Refer to the documentation: https://splash.readthedocs.io/en/stable/api.html#render-jsonAnd this answer: https://stackoverflow.com/a/44682917/4082726

Scraping french site and getting the UnicodeEncodeError

Minjia Zhu

[Scraping french site and getting the UnicodeEncodeError](https://stackoverflow.com/questions/54836345/scraping-french-site-and-getting-the-unicodeencodeerror)

I am scraping a site and getting info from links on that site, however, many of the links contain accents/french characters. I am unable to get the links for these pages therefore not able to scrape them.This is the part of the code that gets URLs from start pagesAnd This is the Error that I am getting in the logAny help is appreciated! Thank you!

2019-02-22 22:57:45Z

I am scraping a site and getting info from links on that site, however, many of the links contain accents/french characters. I am unable to get the links for these pages therefore not able to scrape them.This is the part of the code that gets URLs from start pagesAnd This is the Error that I am getting in the logAny help is appreciated! Thank you!Don't use str() to convert that value. Read more about that here: UnicodeEncodeError: 'ascii' codec can't encode character u'\xa0' in position 20: ordinal not in range(128)However, there is a better way to create URLs like that using Scrapy's built-in urljoin:This will automatically create the full URL based on the current URL plus the relative path.

Scrapy TabError: inconsistent use of tabs and spaces in indentation

Adminha

[Scrapy TabError: inconsistent use of tabs and spaces in indentation](https://stackoverflow.com/questions/54967833/scrapy-taberror-inconsistent-use-of-tabs-and-spaces-in-indentation)

I want to use scrapy to crawl a website, just within the website, not external links.

here is what I've tried :But scraper doesn't work, What's wrong?!

It says :

2019-03-03 10:24:02Z

I want to use scrapy to crawl a website, just within the website, not external links.

here is what I've tried :But scraper doesn't work, What's wrong?!

It says :You need to add the indentation in this part:

AWS lambda, scrapy and catching exceptions

terreb

[AWS lambda, scrapy and catching exceptions](https://stackoverflow.com/questions/54868289/aws-lambda-scrapy-and-catching-exceptions)

I'm running scrapy as a AWS lambda function. Inside my function I need to have a timer to see whether it's running longer than 1 minute and if so, I need to run some logic. Here is my code:Watchdog timer class I took from this answer. The problem is the code never hits that except Watchdog block, but rather throws an exception outside:I need to catch exception in the function. How would I go about that. 

PS: I'm very new to Python.

2019-02-25 14:22:55Z

I'm running scrapy as a AWS lambda function. Inside my function I need to have a timer to see whether it's running longer than 1 minute and if so, I need to run some logic. Here is my code:Watchdog timer class I took from this answer. The problem is the code never hits that except Watchdog block, but rather throws an exception outside:I need to catch exception in the function. How would I go about that. 

PS: I'm very new to Python.Alright this question had me going a little crazy, here is why that doesn't work:What the Watchdog object does is create another thread where the exception is raised but not handled (the exception is only handled in the main process). Luckily, twisted has some neat features.You can do it running the reactor in another thread:I'm using python 3.7.0Twisted has scheduling primitives.  For example, this program runs for about 60 seconds:

Scrapy / XPATH : how to extract ONLY text from descendants and self

Peter K.

[Scrapy / XPATH : how to extract ONLY text from descendants and self](https://stackoverflow.com/questions/54865855/scrapy-xpath-how-to-extract-only-text-from-descendants-and-self)

I have the following simple, nested structure: I would like to extract now all text from all  nodes, but struggle with the nested node ( etc.).The expected output should be:Trying something like:gives Instead of using straight XPATH, I also tried using several steps in Scrapy, like:Usingdoes not work, since it gives all  nodes. Using div.extract() here and looking at the returned string, I could of course find  using string search, but this is rather a hack and does not look like the proper Scrapy solution.Any suggestions how to solve this either directly with Xpath or with Scrapy in general would be grealy appreciated.

2019-02-25 12:06:22Z

I have the following simple, nested structure: I would like to extract now all text from all  nodes, but struggle with the nested node ( etc.).The expected output should be:Trying something like:gives Instead of using straight XPATH, I also tried using several steps in Scrapy, like:Usingdoes not work, since it gives all  nodes. Using div.extract() here and looking at the returned string, I could of course find  using string search, but this is rather a hack and does not look like the proper Scrapy solution.Any suggestions how to solve this either directly with Xpath or with Scrapy in general would be grealy appreciated.What do you think about [i.strip() for i in response.xpath('//div[contains(@class, "2")]//text()').extract() if i.strip()]?Without stripping it gives some empty cases also: So I filter them with strip:

What would be a good pipeline to create a scalable, distributed web crawler and scraper?

Diego Rodriguez

[What would be a good pipeline to create a scalable, distributed web crawler and scraper?](https://stackoverflow.com/questions/54805379/what-would-be-a-good-pipeline-to-create-a-scalable-distributed-web-crawler-and)

I would like to build a semi-general crawler and scraper for pharmacy product webpages. I know that most of the webs are not equal, but most of the URLs I have in a list follow one specific type of logic:In essence, I am worried about building a good pipeline to address issues related with monitoring (to handle webpages that suddenly change their structure), scalability and performance.I have thought of the following pipeline (not taking into account storage):Create 2 main spiders. One that crawls the websites given their domains. It gets all the URLs inside a webpage (obeying robots.txt of course) and puts it into a queue system that stores the URLs that are scrape-ready. Then, the second spider picks up the last URL in the Queue and extracts it using either metadata, XPath or any other method. Then, this is put again into another queue system that will be eventually be handled by a module that puts all the data in the queue into a database (which I still do not know if it should be SQL or NoSQL).The advantages of this system is that by putting queues in between the main processes of extraction and storage, parallelization and scalability becomes feasible.Is there anything flawed in my logic? What are the things that I am missing?Thank you so much.

2019-02-21 10:54:29Z

I would like to build a semi-general crawler and scraper for pharmacy product webpages. I know that most of the webs are not equal, but most of the URLs I have in a list follow one specific type of logic:In essence, I am worried about building a good pipeline to address issues related with monitoring (to handle webpages that suddenly change their structure), scalability and performance.I have thought of the following pipeline (not taking into account storage):Create 2 main spiders. One that crawls the websites given their domains. It gets all the URLs inside a webpage (obeying robots.txt of course) and puts it into a queue system that stores the URLs that are scrape-ready. Then, the second spider picks up the last URL in the Queue and extracts it using either metadata, XPath or any other method. Then, this is put again into another queue system that will be eventually be handled by a module that puts all the data in the queue into a database (which I still do not know if it should be SQL or NoSQL).The advantages of this system is that by putting queues in between the main processes of extraction and storage, parallelization and scalability becomes feasible.Is there anything flawed in my logic? What are the things that I am missing?Thank you so much.First off, that approach will work; my team and I have built numerous crawlers based on that structure, and they are efficient.That said, if you're looking to scale, I would recommend a slightly different approach. For my own large-scale crawler, I have a 3-program approach.The other major recommendation is that if you're using cURL at all, you'll want to use the cURL multi interface and a FIFO queue to handle sending the data from the scheduler to the downloader.The advantage of this approach is that it separates out the processing from the downloading. This allows you to scale up your crawler by adding new servers and operating in parallel.At Potent Pages, this is the architecture we use for our site spider that handles downloading hundreds of sites simultaneously. We use MySQL for the data saving (links, etc), but as you scale up, you'll need to do a lot of optimization. Plus phpmyadmin starts to break down if you have a lot of databases, but having one database per site really speeds up the parsing process so you don't have to go through millions of rows of data.

How to resume Scrapy spider where it left off?

superdee

[How to resume Scrapy spider where it left off?](https://stackoverflow.com/questions/54811411/how-to-resume-scrapy-spider-where-it-left-off)

I have a very large website with lots of URLs I would like to spider. Is there a way to tell Scrapy to ignore a list of URLs?Right now I store all the URLs in a DB column, I would like to be able to restart the spider but pass the long list (24k rows) to Scrapy so it knows to skip the ones it has already seen.Is there anyway to do this?

2019-02-21 16:06:15Z

I have a very large website with lots of URLs I would like to spider. Is there a way to tell Scrapy to ignore a list of URLs?Right now I store all the URLs in a DB column, I would like to be able to restart the spider but pass the long list (24k rows) to Scrapy so it knows to skip the ones it has already seen.Is there anyway to do this?You will have to store the scraped URLs somewhere, I usually do it in MySQL, then when I restart scraper, I ignore them like thisCheck the information in your database:

Scrapy_splash can not get data?

张一一

[Scrapy_splash can not get data?](https://stackoverflow.com/questions/54813447/scrapy-splash-can-not-get-data)

When I want to scrapy the 'https://book.douban.com/annual/2016', I can not get the data. Code is in the bottom.It show "Hello IT, have you tried turning it off and on again?".But I run the script in "http://localhost:8050", it show me what I want.

2019-02-21 18:00:17Z

When I want to scrapy the 'https://book.douban.com/annual/2016', I can not get the data. Code is in the bottom.It show "Hello IT, have you tried turning it off and on again?".But I run the script in "http://localhost:8050", it show me what I want.

Scrapy How to scrap HTTPS site through SSL proxy

Nawashy

[Scrapy How to scrap HTTPS site through SSL proxy](https://stackoverflow.com/questions/54819546/scrapy-how-to-scrap-https-site-through-ssl-proxy)

I've SSL proxy server and I want to scrap https site. I mean the connection between scrapy and the proxy is encrypted then the proxy will open a connection to the website.

after some debugging I found the following:-

currently scrapy handle the situation as follows:- if the site is http it use ScrapyProxyAgent which send client hello then send a connect request for the website to the proxybut if the site is https it use a TunnelingAgent which does not send client hello to the proxy and hence the connection is terminated.What I need is to tell scrapy to first establish a connection via ScrapyProxyAgent then use a TunnelingAgent not sure how to do that.I tried to create a https DOWNLOAD_HANDLERS but I'm not that expert I need to establish a tunnel after the proxy agent is connected.

is that even possible?thanks in advance    

2019-02-22 03:15:18Z

I've SSL proxy server and I want to scrap https site. I mean the connection between scrapy and the proxy is encrypted then the proxy will open a connection to the website.

after some debugging I found the following:-

currently scrapy handle the situation as follows:- if the site is http it use ScrapyProxyAgent which send client hello then send a connect request for the website to the proxybut if the site is https it use a TunnelingAgent which does not send client hello to the proxy and hence the connection is terminated.What I need is to tell scrapy to first establish a connection via ScrapyProxyAgent then use a TunnelingAgent not sure how to do that.I tried to create a https DOWNLOAD_HANDLERS but I'm not that expert I need to establish a tunnel after the proxy agent is connected.

is that even possible?thanks in advance    

Scrapy Request GET url, How can I add keyword in url?

Praveen

[Scrapy Request GET url, How can I add keyword in url?](https://stackoverflow.com/questions/54826661/scrapy-request-get-url-how-can-i-add-keyword-in-url)

I have this url www.example.com. I need to send this url in scrapy Request method,How can I attain that?

url_final = https://www.example.com/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=MYSEARCHKEY&_sacat=0

2019-02-22 12:01:01Z

I have this url www.example.com. I need to send this url in scrapy Request method,How can I attain that?

url_final = https://www.example.com/sch/i.html?_from=R40&_trksid=m570.l1313&_nkw=MYSEARCHKEY&_sacat=0You can use python library for urlencoding to create final URL.

In Python3.6.5 Requests getting SSL Certificate Error

Captureca

[In Python3.6.5 Requests getting SSL Certificate Error](https://stackoverflow.com/questions/54829759/in-python3-6-5-requests-getting-ssl-certificate-error)

I had try to get the following URL using requests, but am getting an SSL certificate Error. I had tried all earlier Stack overflow Queries but nothing seems working

Code:I had given verify=False, still not worksError:

2019-02-22 14:56:05Z

I had try to get the following URL using requests, but am getting an SSL certificate Error. I had tried all earlier Stack overflow Queries but nothing seems working

Code:I had given verify=False, still not worksError:TL;DR - The server is misconfigured. Either fix the server, pass verify=ssl.CERT_NONE, or download and pass www.magidglove.com's certificate explicitly.The problem here is on the server, not the client. The server is only configured to return it's own certificate, which isn't enough for the client to trust it. Servers generally need to be configured to return the full certificate chain.In order to diagnose this, you can use openssl to view some raw information about the certificate chain returned:You can see that 3 certificates were returned by the server, and they were verified in reverse order. The GlobalSign certificate is trusted by the certifi library, the cert at depth=1 was created by the cert at depth=2, and the last cert, CN=www.google.com, was created by the cert at depth=1. Now let's compare that to the server you were trying to connect to:You can see a few things from this output:

- The server only returned a single certificate

- The client tried to verify the certificate and couldn'tIt requires some knowledge of ssl to know that the reason why it couldn't verify was that it doesn't trust the certificate, but now that we know that, we can see that having the server return the full certificate chain will fix that. I suspect that the reason why chrome and other browsers don't report an error is that the browser itself knows about DigiCert, so it doesn't require a full chain.This problem can easily be solved by adding importing the ssl to your python code and adding verify=ssl.CERT_NONE so your code should look something like this: That being said when running this code you might come across this error:which you could disable by adding the following lines to your code:Hope this helps!

2 functions in scrapy spider and the second one not running

Minjia Zhu

[2 functions in scrapy spider and the second one not running](https://stackoverflow.com/questions/54816311/2-functions-in-scrapy-spider-and-the-second-one-not-running)

I am using scrapy to get the content inside some urls on a page, similar to this question here: 

Use scrapy to get list of urls, and then scrape content inside those urlsI am able to get the subURLs from my start urls(first def), However, my second def doesn't seem to be passing through. And the result file is empty. I have tested the content inside the function in scrapy shell and it is getting the info I want, but not when I am running the spider. 

2019-02-21 21:14:21Z

I am using scrapy to get the content inside some urls on a page, similar to this question here: 

Use scrapy to get list of urls, and then scrape content inside those urlsI am able to get the subURLs from my start urls(first def), However, my second def doesn't seem to be passing through. And the result file is empty. I have tested the content inside the function in scrapy shell and it is getting the info I want, but not when I am running the spider. Your allowed_domains list contains the protocol (https). It should have only the domain name as per the documentation:Also, you should've received a message in your log:

Scrapy output not matching up with what I see when I click the link that Scrapy says it is crawling

Justin Perez

[Scrapy output not matching up with what I see when I click the link that Scrapy says it is crawling](https://stackoverflow.com/questions/54809949/scrapy-output-not-matching-up-with-what-i-see-when-i-click-the-link-that-scrapy)

this is my first questionI'm working on a Scrapy spider right now and it wasn't giving the output I expected. I am trying to get all the links from a page, but it is not getting any of them. Specifically, from this page I want all the product links.

To select them I am usingbut this gives me nothing. So just to see what would come out, I usedand it gives me which doesn't seem to match up with the site at all. Yet the console output certainly tells me this is the link that is being crawled. I am especially confused because earlier my code worked as intended, but only one single time, and it hasn't changed in any meaningful way from then. Any help would be greatly appreciated. Thanks in advance.

2019-02-21 14:54:47Z

this is my first questionI'm working on a Scrapy spider right now and it wasn't giving the output I expected. I am trying to get all the links from a page, but it is not getting any of them. Specifically, from this page I want all the product links.

To select them I am usingbut this gives me nothing. So just to see what would come out, I usedand it gives me which doesn't seem to match up with the site at all. Yet the console output certainly tells me this is the link that is being crawled. I am especially confused because earlier my code worked as intended, but only one single time, and it hasn't changed in any meaningful way from then. Any help would be greatly appreciated. Thanks in advance.Thanks for the help everyone.The output was not what was expected based on what I saw when I clicked the link provided by the console output because the spider was getting the prove you're not a robot page from amazon as its response.I discovered this by using self.logger.info(response.body) to see the response my spider was receiving 

How can I force scrapy to use postgres instead of a local sqllite database?

superdee

[How can I force scrapy to use postgres instead of a local sqllite database?](https://stackoverflow.com/questions/54869252/how-can-i-force-scrapy-to-use-postgres-instead-of-a-local-sqllite-database)

I set this lineBut now I have ran out of room on my EC2 instance, I get the error: “[Errno 28] No space left on device”The JOBDIR seems to be keeping track of what what scrapy spider has seen / not seen yet. Ideally I would use a postgres database to store this instead of JOBDIR, does anyone know how I can transfer this?Is there a setting I can set that points the JOBDIR storage to a table in a postgres DB, instead of this local sqlite file?Alternatively, is there a way I can just clear out some of the data without the spider having to start over from scratch?EDIT: Upon further investigation I found that the file in  'jobs/scrapy/requests.queue' called p0 is MASSIVE, 4GB. What is this file used for and why is it so large?I have a similar sized site to crawl, with the same number of URLs found, and the p0 file is only 5MB. Can I delete this p0 file or will that make the spider start from the beginning?

2019-02-25 15:13:02Z

I set this lineBut now I have ran out of room on my EC2 instance, I get the error: “[Errno 28] No space left on device”The JOBDIR seems to be keeping track of what what scrapy spider has seen / not seen yet. Ideally I would use a postgres database to store this instead of JOBDIR, does anyone know how I can transfer this?Is there a setting I can set that points the JOBDIR storage to a table in a postgres DB, instead of this local sqlite file?Alternatively, is there a way I can just clear out some of the data without the spider having to start over from scratch?EDIT: Upon further investigation I found that the file in  'jobs/scrapy/requests.queue' called p0 is MASSIVE, 4GB. What is this file used for and why is it so large?I have a similar sized site to crawl, with the same number of URLs found, and the p0 file is only 5MB. Can I delete this p0 file or will that make the spider start from the beginning?

How to set several socks port in torcc file, polipo config file, and implement it in scrapy project too?

AvyWam

[How to set several socks port in torcc file, polipo config file, and implement it in scrapy project too?](https://stackoverflow.com/questions/54832729/how-to-set-several-socks-port-in-torcc-file-polipo-config-file-and-implement-i)

The goal I target is to use several commands of one single spider as I did here, and to get different IP for each command launched. So I have some ideas I introduce below.As far as I understand I can write in one single torrc file several ports as it is introduced in the answer of adrelanos in this topic.

That is usefull if true because I use Vidalia to launch it and I can set only one single torrc file. Note I did not test it for the momentBut I have several interrogations.In this page (present in the previous topic), torrc file needs a Socksport and a ControlPort too. First is really necessary to have both? Second in polipo config file can I set several different socksParentProxy in it as in torrc (I do not find it in polipo manual)? Third how to implement Socksport in the scrapy middlewares.py with the controller below in the code?In my scrapy project I have the intention to set different control port according to different commands of the spider like this:In spider.pyIn middlewares.pySo how to set the socksport in it? I looked at Controller documentation of stem library I use in my middlewares.py file and I do not see which method can help me to do it.And a last question, a proxy is optional, what is the risk if I do not use a proxy and only Tor?

2019-02-22 18:03:06Z

The goal I target is to use several commands of one single spider as I did here, and to get different IP for each command launched. So I have some ideas I introduce below.As far as I understand I can write in one single torrc file several ports as it is introduced in the answer of adrelanos in this topic.

That is usefull if true because I use Vidalia to launch it and I can set only one single torrc file. Note I did not test it for the momentBut I have several interrogations.In this page (present in the previous topic), torrc file needs a Socksport and a ControlPort too. First is really necessary to have both? Second in polipo config file can I set several different socksParentProxy in it as in torrc (I do not find it in polipo manual)? Third how to implement Socksport in the scrapy middlewares.py with the controller below in the code?In my scrapy project I have the intention to set different control port according to different commands of the spider like this:In spider.pyIn middlewares.pySo how to set the socksport in it? I looked at Controller documentation of stem library I use in my middlewares.py file and I do not see which method can help me to do it.And a last question, a proxy is optional, what is the risk if I do not use a proxy and only Tor?

Scrapy finishing early, not getting all links

superdee

[Scrapy finishing early, not getting all links](https://stackoverflow.com/questions/54776577/scrapy-finishing-early-not-getting-all-links)

I am trying to run a webspider that gets all URLs for a specific url. Right now it is returning about 64 urls when I know there are hundred and thousands more. Anyone know why it is finishing early?Here are the results, I noted is says request_depth_max:1 but I have my DEPTH_LIMIT=0 in the settings

2019-02-19 23:37:02Z

I am trying to run a webspider that gets all URLs for a specific url. Right now it is returning about 64 urls when I know there are hundred and thousands more. Anyone know why it is finishing early?Here are the results, I noted is says request_depth_max:1 but I have my DEPTH_LIMIT=0 in the settingsAs per our comments under the question, you need to extract links in parse_item() too. If you only extract in parse() then subsequent links will not be followed.

Custom output in xml in Scrapy with lxml. Next item overwrites previous

Antonio Oliveira

[Custom output in xml in Scrapy with lxml. Next item overwrites previous](https://stackoverflow.com/questions/54785320/custom-output-in-xml-in-scrapy-with-lxml-next-item-overwrites-previous)

In Scrapy, with lxml, I created a custom pipeline to generate xml according to my needs. The xml is being generated, but there is a bug: the next group (items) in the list overwrites the previous one. That is, regardless of the len() of the list, only a group (items) is saved. The code is below. Someone help me?Even referring to the same theme, it is not a duplicate of this question:

P.S.: How do you append to a file in Python?

For there are quirks like preserving xml headers and footers.

2019-02-20 11:31:48Z

In Scrapy, with lxml, I created a custom pipeline to generate xml according to my needs. The xml is being generated, but there is a bug: the next group (items) in the list overwrites the previous one. That is, regardless of the len() of the list, only a group (items) is saved. The code is below. Someone help me?Even referring to the same theme, it is not a duplicate of this question:

P.S.: How do you append to a file in Python?

For there are quirks like preserving xml headers and footers.

Connexion errors with scrapy spider using toripchanger module

AvyWam

[Connexion errors with scrapy spider using toripchanger module](https://stackoverflow.com/questions/54786349/connexion-errors-with-scrapy-spider-using-toripchanger-module)

First of all, most of the items are scraped (I miss 3 items), so there is an internet connexion but the behavior is not the one I expect.

Besides I am not trained on internet protocol, I just know the principles so vaguely how it works in details.So I use scrapy to crawl a website.I try to do it anonymously just to do not be banned even if my spiders are polite.So, my middlewares.py and settings.py are configured like this about internet connection in my scrapy project:settings.pymiddlewares.pySo here we see I want to change my IP every ten requests.I use Vidalia to launch Tor and Polipo proxy.

It is configured like this.Note I translated it from French, so it is normal if you do not see the exactly same titledThe config file of polipo is configured as:nothing else is uncommented in this file above.The torrc configuration file of tor is configured as:py -m scrapy crawl spider -a arg1=0 -a arg2=30So my spider scrap 30 different adresses, so 30 different requests here at least without counting the login page.In my spider file I wrote a request to http://checkip.dyndns.org/ to check if my IP change.Contrary to what I expected, the IP doesn't change. In the command it printed IP: 195.176.3.20 25 times (because of exceptions there is not 30), while I expected a change every ten requests.

Yes this is weird to obtain 27 items and to return IP only 25 times, that's because I obtain exactly the same exceptions as you gonna see below when it requests the http://checkip.dyndns.org/ page.In the log file, I have got, there are these following lines:Well, I am sorry if it is very long, but here that is kinda complete and as I said I am not trained with these skills.

Note the execptions written above repeated, I mean it is the same pattern of exceptions that occures, so I shorted it.What must I do?Tor version: 0.3.4.8, Vidalia: 0.2.21, Scrapy 1.6.0, polipo: I don't know, python: Python 3.7.2

2019-02-20 12:24:10Z

First of all, most of the items are scraped (I miss 3 items), so there is an internet connexion but the behavior is not the one I expect.

Besides I am not trained on internet protocol, I just know the principles so vaguely how it works in details.So I use scrapy to crawl a website.I try to do it anonymously just to do not be banned even if my spiders are polite.So, my middlewares.py and settings.py are configured like this about internet connection in my scrapy project:settings.pymiddlewares.pySo here we see I want to change my IP every ten requests.I use Vidalia to launch Tor and Polipo proxy.

It is configured like this.Note I translated it from French, so it is normal if you do not see the exactly same titledThe config file of polipo is configured as:nothing else is uncommented in this file above.The torrc configuration file of tor is configured as:py -m scrapy crawl spider -a arg1=0 -a arg2=30So my spider scrap 30 different adresses, so 30 different requests here at least without counting the login page.In my spider file I wrote a request to http://checkip.dyndns.org/ to check if my IP change.Contrary to what I expected, the IP doesn't change. In the command it printed IP: 195.176.3.20 25 times (because of exceptions there is not 30), while I expected a change every ten requests.

Yes this is weird to obtain 27 items and to return IP only 25 times, that's because I obtain exactly the same exceptions as you gonna see below when it requests the http://checkip.dyndns.org/ page.In the log file, I have got, there are these following lines:Well, I am sorry if it is very long, but here that is kinda complete and as I said I am not trained with these skills.

Note the execptions written above repeated, I mean it is the same pattern of exceptions that occures, so I shorted it.What must I do?Tor version: 0.3.4.8, Vidalia: 0.2.21, Scrapy 1.6.0, polipo: I don't know, python: Python 3.7.2

Scrapy - list returns None - index out of range

Alex16237

[Scrapy - list returns None - index out of range](https://stackoverflow.com/questions/54869618/scrapy-list-returns-none-index-out-of-range)

I have two items in my list which do or do not exist. How do I write a check for list?Items look like thisSometimes list member [3] or [4] does not exists, therefore Scrapy fails withI tried few different approaches, but each fails. I do not understand why. Specifying response.xpath as local variable and checking withFails. Scrapy complains about list still being out of range.How do i make a correct checking in list extraction process?EDIT: Full function below. This is last function in 'chain'. It visits 3 pages in previous steps and passes items using meta.Another thing,Lenght == 4, list is completeLenght == 1, list is incomplete (It doesnt have tags i want to include in my items)But conditionif len(texts) == 1 is always satisfied, and whatever i want to do next will be completed for all of my items. Example:This fills both item with "None" in all possible cases.

2019-02-25 15:31:20Z

I have two items in my list which do or do not exist. How do I write a check for list?Items look like thisSometimes list member [3] or [4] does not exists, therefore Scrapy fails withI tried few different approaches, but each fails. I do not understand why. Specifying response.xpath as local variable and checking withFails. Scrapy complains about list still being out of range.How do i make a correct checking in list extraction process?EDIT: Full function below. This is last function in 'chain'. It visits 3 pages in previous steps and passes items using meta.Another thing,Lenght == 4, list is completeLenght == 1, list is incomplete (It doesnt have tags i want to include in my items)But conditionif len(texts) == 1 is always satisfied, and whatever i want to do next will be completed for all of my items. Example:This fills both item with "None" in all possible cases.Before you access an index, make sure the corresponding list is long enough:

Issue indicates wrong HashedControlPassword in scrapy while it is exactly the one presents in torrc file

AvyWam

[Issue indicates wrong HashedControlPassword in scrapy while it is exactly the one presents in torrc file](https://stackoverflow.com/questions/54790255/issue-indicates-wrong-hashedcontrolpassword-in-scrapy-while-it-is-exactly-the-on)

I have got these messages in my log file:The problem is I copied and pasted the exactly the same one present in torrc file, I even tried with 16: in it and without, but I obtain the same error.So what is wrong?

2019-02-20 15:48:39Z

I have got these messages in my log file:The problem is I copied and pasted the exactly the same one present in torrc file, I even tried with 16: in it and without, but I obtain the same error.So what is wrong?

Steam Scraper form issue using Scrapy (how to send data if there is no form)

Galiu

[Steam Scraper form issue using Scrapy (how to send data if there is no form)](https://stackoverflow.com/questions/54869632/steam-scraper-form-issue-using-scrapy-how-to-send-data-if-there-is-no-form)

A while ago I made a scraper that and it was working just fine until some months ago. It seems that steam has changed the HTML on certain pages. Mainly the ones with the agecheck or mature content.My question would be how can I send the data steam needs in order to get over the agecheck if there is no form anymore? I wanted to use selenium, but I want to also use scrapinghub and those don't seem to work since I get an error that there is no selenium module when I try to deploy my code.Any clues about what to do?Here is the code before the steam HTML change:

2019-02-25 15:32:09Z

A while ago I made a scraper that and it was working just fine until some months ago. It seems that steam has changed the HTML on certain pages. Mainly the ones with the agecheck or mature content.My question would be how can I send the data steam needs in order to get over the agecheck if there is no form anymore? I wanted to use selenium, but I want to also use scrapinghub and those don't seem to work since I get an error that there is no selenium module when I try to deploy my code.Any clues about what to do?Here is the code before the steam HTML change:

Unable to rename downloaded images through pipelines without the usage of item.py

robots.txt

[Unable to rename downloaded images through pipelines without the usage of item.py](https://stackoverflow.com/questions/54737465/unable-to-rename-downloaded-images-through-pipelines-without-the-usage-of-item-p)

I've created a script using python's scrapy module to download and rename movie images from multiple pages out of a torrent site and store them in a desktop folder. When it is about downloading and storing those images in a desktop folder, my script is the same errorlessly. However, what I'm struggling to do now is rename those files on the fly. As I didn't make use of item.py file and I do not wish to either, I hardly understand how the logic of  pipelines.py file would be to handle the renaming process.My spider (It downloads the images flawlessly):pipelines.py contains: (the following lines are the placeholders to let you know I at least tried):How can I rename the images through pipelines.py without the usage of item.py?

2019-02-17 20:37:53Z

I've created a script using python's scrapy module to download and rename movie images from multiple pages out of a torrent site and store them in a desktop folder. When it is about downloading and storing those images in a desktop folder, my script is the same errorlessly. However, what I'm struggling to do now is rename those files on the fly. As I didn't make use of item.py file and I do not wish to either, I hardly understand how the logic of  pipelines.py file would be to handle the renaming process.My spider (It downloads the images flawlessly):pipelines.py contains: (the following lines are the placeholders to let you know I at least tried):How can I rename the images through pipelines.py without the usage of item.py?You need to subclass the original ImagesPipeline:And then refer to it in your settings:But be aware that a simple "use the exact filename" idea will cause issues when different files have the same name, unless you add a unique folder structure or an additional component to the filename. That's one reason checksums-based filenames are used by default. Refer to the original file_path, in case you want to include some of the original logic to prevent that.

I can't fetch pages from https://vesti-ukr.com with scrapy

Денис Иванов

[I can't fetch pages from https://vesti-ukr.com with scrapy](https://stackoverflow.com/questions/54747325/i-cant-fetch-pages-from-https-vesti-ukr-com-with-scrapy)

I can't get pages from https://vesti-ukr.com with scrapy. 

It says:I've tried to use any other DOWNLOADER_CLIENT_TLS_METHOD like "SSLv3"

but it doesn't work. With any other sites I don't have issues using scrapy. And this I can scrapy this site with requests(it works good) or windows BitsTransfer, but I wanted to make it using scrapy and now I want to know why is it happens and how to fix it in future.

2019-02-18 12:28:22Z

I can't get pages from https://vesti-ukr.com with scrapy. 

It says:I've tried to use any other DOWNLOADER_CLIENT_TLS_METHOD like "SSLv3"

but it doesn't work. With any other sites I don't have issues using scrapy. And this I can scrapy this site with requests(it works good) or windows BitsTransfer, but I wanted to make it using scrapy and now I want to know why is it happens and how to fix it in future.Can you check your cryptography lib version? With pip freeze for example. Downgrading version to pip install cryptography==1.7.2 helped me to call scrapy shell https://vesti-ukr.com/feed/1-vse-novosti without ssl errors.

Initialise Scrapy setting with values accepted from user as argument

Jigar Chavada

[Initialise Scrapy setting with values accepted from user as argument](https://stackoverflow.com/questions/54747431/initialise-scrapy-setting-with-values-accepted-from-user-as-argument)

I want to set the HTTPCACHE_DIR setting to the value which the user provides through the custom arguments. 

2019-02-18 12:35:08Z

I want to set the HTTPCACHE_DIR setting to the value which the user provides through the custom arguments. By defalut Scrapy uses HTTPCACHE_DIR setting in  FileSystemCacheStorage which is a part HttpCacheMiddleware:As You can see Scrapy reads HTTPCACHE_DIR setting parameter only one time when Scrapy create FilesystemCacheStorage. Even if You somehow change HTTPCACHE_DIR setting later It will not change cachedir.

There is the only way to change cachedir during scraping process - is to change cachedir property of FilesystemCacheStorage object.

You can implement this in your spider code:

(for scrapy crawl myspider -a HTTPCACHE_DIR="cache_dir")

how to crawl multiple URLs?

germpark

[how to crawl multiple URLs?](https://stackoverflow.com/questions/54754507/how-to-crawl-multiple-urls)

I want to enter multiple links and scrap their data but i don't know how to code..This is one match's data. I want to increase number xpath('//*[@id="201802090{here}"]') and xpath('//*[@id="content-wrap"]/div/div[2]/div/section/div[2]/div/section/div/ul/li[{here}]/section/ul/li[3]/a') (here spot) at the same time because of next match.

Simply, 2018020904,2018020905,2018020906 and li[2],li[3],li[4] in pairs.

2019-02-18 19:59:06Z

I want to enter multiple links and scrap their data but i don't know how to code..This is one match's data. I want to increase number xpath('//*[@id="201802090{here}"]') and xpath('//*[@id="content-wrap"]/div/div[2]/div/section/div[2]/div/section/div/ul/li[{here}]/section/ul/li[3]/a') (here spot) at the same time because of next match.

Simply, 2018020904,2018020905,2018020906 and li[2],li[3],li[4] in pairs.

xpath not fetching all links

Bhawan

[xpath not fetching all links](https://stackoverflow.com/questions/54801994/xpath-not-fetching-all-links)

I am writing a scraper to fetch all the links from my local website.I am using this selector to fetch all the href values of all anchor tags: The result I am getting is:The problem is there is another anchor tag on the page with href = 'admin/pageHome.php' which is not coming in results.Any help would be highly appreciated. I am attaching the page source code screenshot for reference. 

2019-02-21 08:00:08Z

I am writing a scraper to fetch all the links from my local website.I am using this selector to fetch all the href values of all anchor tags: The result I am getting is:The problem is there is another anchor tag on the page with href = 'admin/pageHome.php' which is not coming in results.Any help would be highly appreciated. I am attaching the page source code screenshot for reference. 

scrapy crawl no response

Bingqi

[scrapy crawl no response](https://stackoverflow.com/questions/54755804/scrapy-crawl-no-response)

I just learn python and web scraping. I follow a tutorial to do web scraping. However, the output is different from the tutorial, that lead to error when I run the code. the tutorial said The ideal results should be:

But my result is:

I think I do not change it successfully. I do not know how to get the same result with the tutorial. Thank you.

2019-02-18 21:42:37Z

I just learn python and web scraping. I follow a tutorial to do web scraping. However, the output is different from the tutorial, that lead to error when I run the code. the tutorial said The ideal results should be:

But my result is:

I think I do not change it successfully. I do not know how to get the same result with the tutorial. Thank you.Remove the [...] and (...) characters. That looks like a mistake in the tutorial, although their screenshot does show the correct command without those characters.Your command should be:

Scrapy how to extract the text of the field that you are not selecting on

noel

[Scrapy how to extract the text of the field that you are not selecting on](https://stackoverflow.com/questions/54756587/scrapy-how-to-extract-the-text-of-the-field-that-you-are-not-selecting-on)

I'm new to python/scrapy.  My question is similar to this question, but I can't quite craft an answer that works:How Scrapy extract text inside class that inside attribute?Here is my code:That code yields:How do I get the text from the href contained within this response?  Thanks!

2019-02-18 22:57:21Z

I'm new to python/scrapy.  My question is similar to this question, but I can't quite craft an answer that works:How Scrapy extract text inside class that inside attribute?Here is my code:That code yields:How do I get the text from the href contained within this response?  Thanks!I can answer my own question.  The answer was:

Scrapy generic response from VM

Yohan Obadia

[Scrapy generic response from VM](https://stackoverflow.com/questions/54762765/scrapy-generic-response-from-vm)

I am trying to crawl booking from a VM and I don't get the same response like the one from my local machine. The query is the following:When I run the query from my VM, I get a response with the same URL than the one in the query while from the VM I get the generic response:I must mention that before adding the USER_AGENT part I was getting the same answer even on my local machine. Also, if I use Links, a command-line browser from the VM, I get the correct response. Hence it does not seem to come from the public IP of the VM I use.I suspect that there is another information that booking.com might be using to prevent the crawling of certain pages on top of the USER_AGENT and the robot.txt file but I don't know which one.Local Request HeadersVM Request HeadersVM Request without cookiesVM Request Headers without cookies

2019-02-19 09:27:10Z

I am trying to crawl booking from a VM and I don't get the same response like the one from my local machine. The query is the following:When I run the query from my VM, I get a response with the same URL than the one in the query while from the VM I get the generic response:I must mention that before adding the USER_AGENT part I was getting the same answer even on my local machine. Also, if I use Links, a command-line browser from the VM, I get the correct response. Hence it does not seem to come from the public IP of the VM I use.I suspect that there is another information that booking.com might be using to prevent the crawling of certain pages on top of the USER_AGENT and the robot.txt file but I don't know which one.Local Request HeadersVM Request HeadersVM Request without cookiesVM Request Headers without cookies

How to use Scrapy Crawler with Splash to crawl Javascript pages

Matija Žiberna

[How to use Scrapy Crawler with Splash to crawl Javascript pages](https://stackoverflow.com/questions/54765711/how-to-use-scrapy-crawler-with-splash-to-crawl-javascript-pages)

I am having troubles using Scrapy Crawler to crawl javascript websites. It seems like Scrapy ignores Rules and just continues normal scraping.Would it be possible to instruct Spider to use Splash in order to crawl?Thank you.

2019-02-19 11:52:13Z

I am having troubles using Scrapy Crawler to crawl javascript websites. It seems like Scrapy ignores Rules and just continues normal scraping.Would it be possible to instruct Spider to use Splash in order to crawl?Thank you.The Rules will only trigger if you actually get to a matching page after the start_requests. You also need to define callback functions for your Rules, otherwise they will try to use the default parse (in case it appears as if your Rules are doing nothing).To change a Rule's request to SplashRequest you have to return it in the process_request callback. For example:

Xpath - multiple nested divs with tables holding text value

Alex16237

[Xpath - multiple nested divs with tables holding text value](https://stackoverflow.com/questions/54769079/xpath-multiple-nested-divs-with-tables-holding-text-value)

I encountered complicated html structure on website from which i want to extract text information.Website has following structure:Each id="list_*" unfolds intoIts nightmarish structure! And its repeated for eatch list_*Relative Xpath for following is/div[9]/div[2]/div[3]/div[2]/form/div/div[2]/ul/li[1]/div[2]/table/tbody/tr/td[1]/table/tbody/tr/td/table[2]/tbody/tr/td/h2/aWhich fails.Few things i tried, with limited success are,response.xpath('//*[@id="one"]//table//tr//h2//a[position()]//text()').extract()This extracts all /h2/a from page, not from single list_*This extracts text correctly but only from first list_1 div. I can increment it with extract()[++i], but that is not optimal solution and i think there are definitely better ways to do it.What i want to accomplish is:Extract text (PRODUCT_NAME) from each list_* in order.

2019-02-19 14:55:42Z

I encountered complicated html structure on website from which i want to extract text information.Website has following structure:Each id="list_*" unfolds intoIts nightmarish structure! And its repeated for eatch list_*Relative Xpath for following is/div[9]/div[2]/div[3]/div[2]/form/div/div[2]/ul/li[1]/div[2]/table/tbody/tr/td[1]/table/tbody/tr/td/table[2]/tbody/tr/td/h2/aWhich fails.Few things i tried, with limited success are,response.xpath('//*[@id="one"]//table//tr//h2//a[position()]//text()').extract()This extracts all /h2/a from page, not from single list_*This extracts text correctly but only from first list_1 div. I can increment it with extract()[++i], but that is not optimal solution and i think there are definitely better ways to do it.What i want to accomplish is:Extract text (PRODUCT_NAME) from each list_* in order.Maybe try css selector like: response.css('li[id*="list_"] a.product_title::text').extract()?

Or xpath response.xpath('//li[contains(@id, "list_")]//a[contains(@class, "product_title")]/text()')?UPD: For iteration:or Why not to find all links and extract text from the

Trouble renaming downloaded images in a customized manner through pipelines

robots.txt

[Trouble renaming downloaded images in a customized manner through pipelines](https://stackoverflow.com/questions/54773331/trouble-renaming-downloaded-images-in-a-customized-manner-through-pipelines)

I've created a script using python's scrapy module to download and rename movie images from a torrent site and store them in a folder within scrapy project. When I run my script as it is, I find it downloading images in that folder folder errorlessly. At this moment the script is renaming those images using a convenient portion from request.url through pipelines.py. How can I rename those downloaded images through pipelines.py using their movie names from the variable movie defined within get_images() method?spider contains:pipelines.py contains:One of such downloaded images should look like Obsession.jpg when the renaming is done.

2019-02-19 19:08:40Z

I've created a script using python's scrapy module to download and rename movie images from a torrent site and store them in a folder within scrapy project. When I run my script as it is, I find it downloading images in that folder folder errorlessly. At this moment the script is renaming those images using a convenient portion from request.url through pipelines.py. How can I rename those downloaded images through pipelines.py using their movie names from the variable movie defined within get_images() method?spider contains:pipelines.py contains:One of such downloaded images should look like Obsession.jpg when the renaming is done.Override get_media_requests() and add the data you need to the request. Then grab that data from the request in file_path().For example:

Scrapy unable to connect to HTTPS site through HTTPS proxy that use basic authentication

Nawashy

[Scrapy unable to connect to HTTPS site through HTTPS proxy that use basic authentication](https://stackoverflow.com/questions/54773338/scrapy-unable-to-connect-to-https-site-through-https-proxy-that-use-basic-authen)

I'm trying to scrap HTTPS site through HTTPS proxy that requires Basic authentication, the connection is established but scrapy does not send client hello, and then does not get any data from the web site.I can do the same using curl for examplecurl -U user:password -x https://proxy:8443 -v https://icanhazip.comit works without any problem.Also if I executehttp_proxy=https://user:password@proxy:8443 scrapy shell http://icanhazip.comit works buthttp_proxy=https://user:password@proxy:8443 scrapy shell https://icanhazip.comit bypass the proxyI'm using Scrapy : 1.6.0Thanks in advance

2019-02-19 19:09:28Z

I'm trying to scrap HTTPS site through HTTPS proxy that requires Basic authentication, the connection is established but scrapy does not send client hello, and then does not get any data from the web site.I can do the same using curl for examplecurl -U user:password -x https://proxy:8443 -v https://icanhazip.comit works without any problem.Also if I executehttp_proxy=https://user:password@proxy:8443 scrapy shell http://icanhazip.comit works buthttp_proxy=https://user:password@proxy:8443 scrapy shell https://icanhazip.comit bypass the proxyI'm using Scrapy : 1.6.0Thanks in advance

How can I get the original request url in errback using scrapy

I. K.

[How can I get the original request url in errback using scrapy](https://stackoverflow.com/questions/54802529/how-can-i-get-the-original-request-url-in-errback-using-scrapy)

I have a scrapy script for crawling a list of websites from a database, and my aim is to find if a certain element is present on the website and write the data back to the database.I order the database by the urls, so I need to get the original request url in order to write the data to the correct database entry. When everything goes well, there is no problem, I can just use response.request.url with no issues in the callback function. However in case of an error (timeouts mostly) my script calls the errback function.My question is, how can I access the original url in the errback function, since I do not pass the response to it, only error? I don't need any data from the website, just to update the database with the knowledge that there was some type of error, which is easy enough, but without the original request url I cannot do that.Is there an easy way to access the original request url in errback?This is how I call the request:Thank you in advance!

2019-02-21 08:34:09Z

I have a scrapy script for crawling a list of websites from a database, and my aim is to find if a certain element is present on the website and write the data back to the database.I order the database by the urls, so I need to get the original request url in order to write the data to the correct database entry. When everything goes well, there is no problem, I can just use response.request.url with no issues in the callback function. However in case of an error (timeouts mostly) my script calls the errback function.My question is, how can I access the original url in the errback function, since I do not pass the response to it, only error? I don't need any data from the website, just to update the database with the knowledge that there was some type of error, which is easy enough, but without the original request url I cannot do that.Is there an easy way to access the original request url in errback?This is how I call the request:Thank you in advance!From the documentation, you can access it like this:response object contains original scrapy.Request objectUPDATE 

Checked documentarion link.

And checked again in debugger.

errback operates with Failure object. However As Failure object contains original request object as well as HttpResponse objects from regular callbacks - my approach also will work.

Scrapy not scraping all results

germpark

[Scrapy not scraping all results](https://stackoverflow.com/questions/54756580/scrapy-not-scraping-all-results)

I want to scrap the NHL match data. Then, two rows must be scraped per a match(a url). When I scrap one match(one url), result is not empty. But when I scrap ten matches, result is empty, ex)scraped 16 of 20.DEBUG: Crawled (200) https://www.nhl.com/gamecenter/wsh-vs-pit/2018/10/04/2018020007> (referer: None)This is cmd result.csv ex) wsh,30,15%

        pit,42,40%   <-----this is my wish

2019-02-18 22:56:43Z

I want to scrap the NHL match data. Then, two rows must be scraped per a match(a url). When I scrap one match(one url), result is not empty. But when I scrap ten matches, result is empty, ex)scraped 16 of 20.DEBUG: Crawled (200) https://www.nhl.com/gamecenter/wsh-vs-pit/2018/10/04/2018020007> (referer: None)This is cmd result.csv ex) wsh,30,15%

        pit,42,40%   <-----this is my wish

How to download (PDF) files with Python/Scrapy using the Files Pipeline?

R0byn

[How to download (PDF) files with Python/Scrapy using the Files Pipeline?](https://stackoverflow.com/questions/54790699/how-to-download-pdf-files-with-python-scrapy-using-the-files-pipeline)

Using Python 3.7.2 on Windows 10 I'm struggling with the task to let Scrapy v1.5.1 download some PDF files. I followed the docs but I seem to miss something. Scrapy gets me the desired PDF URLs but downloads nothing. Also no errors are thrown (at least).The relevant code is:scrapy.cfg:settings.py:pranger_spider.py:items.py:All other files are as they were created by the scrapy startproject command.

The output of scrapy crawl index is:Oh BTW I published the code, just in case: https://github.com/R0byn/pranger/tree/5bfa0df92f21cecee18cc618e9a8e7ceea192403

2019-02-20 16:09:47Z

Using Python 3.7.2 on Windows 10 I'm struggling with the task to let Scrapy v1.5.1 download some PDF files. I followed the docs but I seem to miss something. Scrapy gets me the desired PDF URLs but downloads nothing. Also no errors are thrown (at least).The relevant code is:scrapy.cfg:settings.py:pranger_spider.py:items.py:All other files are as they were created by the scrapy startproject command.

The output of scrapy crawl index is:Oh BTW I published the code, just in case: https://github.com/R0byn/pranger/tree/5bfa0df92f21cecee18cc618e9a8e7ceea192403The FILES_URLS_FIELD setting tells the pipeline what field of the item contains the urls you want to download.By default, this is file_urls, but if you change the setting, you also need to change the field name (key) you're storing the urls in.So you have two options -  either use the default setting, or rename your item's field to PDF_urls as well.

scrapy can not follow full link

B B

[scrapy can not follow full link](https://stackoverflow.com/questions/54773379/scrapy-can-not-follow-full-link)

however I get2019-02-19 14:16:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023

2019-02-19 14:16:35 [scrapy.core.engine] INFO: Spider opened

2019-02-19 14:16:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.winemag.com/robots.txt> (referer: None)

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.winemag.com/wine-ratings> from <GET https://www.winemag.com/wine-ratings/2/>

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.winemag.com/wine-ratings> from <GET http://www.winemag.com/wine-ratings>

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.winemag.com/wine-ratings/> from <GET https://www.winemag.com/wine-ratings>

2019-02-19 14:16:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.winemag.com/wine-ratings/> (referer: None)<200 https://www.winemag.com/wine-ratings/>I can't figure out why is it not obtaining the full link, could someone provide me a suggestion please.

2019-02-19 19:12:00Z

however I get2019-02-19 14:16:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023

2019-02-19 14:16:35 [scrapy.core.engine] INFO: Spider opened

2019-02-19 14:16:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.winemag.com/robots.txt> (referer: None)

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET http://www.winemag.com/wine-ratings> from <GET https://www.winemag.com/wine-ratings/2/>

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.winemag.com/wine-ratings> from <GET http://www.winemag.com/wine-ratings>

2019-02-19 14:16:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.winemag.com/wine-ratings/> from <GET https://www.winemag.com/wine-ratings>

2019-02-19 14:16:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.winemag.com/wine-ratings/> (referer: None)<200 https://www.winemag.com/wine-ratings/>I can't figure out why is it not obtaining the full link, could someone provide me a suggestion please.It seems winemag redirects crawlers to its homepage:so it seems this would be the expected behavior from scrapy, which is following the redirects returned to it by the website you're accessing?I've found the answer. I had to specify the USER_AGENT in the settings file.

Scrapy send stats to a URL passed as argument as a POST request every 5 minutes

Jigar Chavada

[Scrapy send stats to a URL passed as argument as a POST request every 5 minutes](https://stackoverflow.com/questions/54707698/scrapy-send-stats-to-a-url-passed-as-argument-as-a-post-request-every-5-minutes)

I need to send the crawler stats to a URL which is passed on as a spider argument. I need to make a POST request at regular intervals of 5 minutes. How can I do that?

2019-02-15 10:49:47Z

I need to send the crawler stats to a URL which is passed on as a spider argument. I need to make a POST request at regular intervals of 5 minutes. How can I do that?You will probably want to write an extension that simply makes a post request every 5 minutes.

You can make these requests either using scrapy's own mechanisms (e.g. engine.download()), or you can use a different async http client (e.g. treq)If you're not sure how to structure your extension, you can take a look at logstats.py which does a similar thing, except not logging over http.Since you're writing an extension anyway, I'd recommend making the url and interval settings, but that choice is up to you.

Scrapy. Crawl whole site and return a single value: total number of links

brainsid

[Scrapy. Crawl whole site and return a single value: total number of links](https://stackoverflow.com/questions/54871563/scrapy-crawl-whole-site-and-return-a-single-value-total-number-of-links)

It's easy to crawl the whole websiteBut how can I return a single value? Total number of links.

2019-02-25 17:24:36Z

It's easy to crawl the whole websiteBut how can I return a single value? Total number of links.For stats about the crawl, use Scrapy Stats.The stats will be available as spider.stats.The stats can be recovered from a ScrapyCloud project using the metadata() API:.

scrapy not entering parse(response.url)

germpark

[scrapy not entering parse(response.url)](https://stackoverflow.com/questions/54675575/scrapy-not-entering-parseresponse-url)

I'm a beginner. When I crawl, there is no error code, but scrapy does not enter response.url in parse. That is, the page is empty page titled "data;"

how to enter the repsonse.url?I want to enter https://www.premierleague.com/match/38567 but result did not exist.

2019-02-13 16:56:55Z

I'm a beginner. When I crawl, there is no error code, but scrapy does not enter response.url in parse. That is, the page is empty page titled "data;"

how to enter the repsonse.url?I want to enter https://www.premierleague.com/match/38567 but result did not exist.The correct property name is start_urls not starts_urls. Because of the incorrect property name it does not detect any start pages.

Using scrapy to extract raw html content of a large(r) number of landing pages

Largo Terranova

[Using scrapy to extract raw html content of a large(r) number of landing pages](https://stackoverflow.com/questions/54646723/using-scrapy-to-extract-raw-html-content-of-a-larger-number-of-landing-pages)

For a classification project I need the raw html content of roughly 1000 websites. I only need the landing page and not more, so the crawler does not have to follow links! I want to use scrapy for it but I can't get the code together. Because I read in the documentation that JSON Files are first stored in memory and then saved (which can cause Problems when crawling a large number of pages) I want to save the file in the '.js' format. I use the Anaconda promt to execute my code.I want the resulting file to have two columns, one with the domainname, and the second with the raw_html content on every siteI found many Spider Examples but I cant figure out how to put everything together. This is how far I got :(Start the Project:The actuall Spider (which might be completely wrong):I navigate to the dragonball Folder and execute the file with:Every help would be apreciated :)

2019-02-12 09:28:06Z

For a classification project I need the raw html content of roughly 1000 websites. I only need the landing page and not more, so the crawler does not have to follow links! I want to use scrapy for it but I can't get the code together. Because I read in the documentation that JSON Files are first stored in memory and then saved (which can cause Problems when crawling a large number of pages) I want to save the file in the '.js' format. I use the Anaconda promt to execute my code.I want the resulting file to have two columns, one with the domainname, and the second with the raw_html content on every siteI found many Spider Examples but I cant figure out how to put everything together. This is how far I got :(Start the Project:The actuall Spider (which might be completely wrong):I navigate to the dragonball Folder and execute the file with:Every help would be apreciated :)If you really want to store everything in a single file, then you can use the following (including part of vezunchik's answer):As mentioned, this isn't a good idea in the long run as you'll end up with a huge file. I advice you to store html to files and write name of files to csv. It will be easier to keep data in format domain, html_raw.You can download files with common with open('%s.html' % domain, 'w') as f: f.write(response.body) or download them with FILES pipeline, check documentation here: https://docs.scrapy.org/en/latest/topics/media-pipeline.htmlDomain you can get with:

Calling Scrapy spider from a Python script

joes

[Calling Scrapy spider from a Python script](https://stackoverflow.com/questions/54647322/calling-scrapy-spider-from-a-python-script)

i want to run  my scrapy spider in ActiveMQ consumer (in onMessage)heres my scrapy spider codeimport jsonclass selogerSpider(scrapy.Spider):my listener activemq code

    import time

import sys

import stompexcept IOError as e:

    print

    "I/O error({0}): {1}".format(e.errno, e.strerror)

2019-02-12 09:58:34Z

i want to run  my scrapy spider in ActiveMQ consumer (in onMessage)heres my scrapy spider codeimport jsonclass selogerSpider(scrapy.Spider):my listener activemq code

    import time

import sys

import stompexcept IOError as e:

    print

    "I/O error({0}): {1}".format(e.errno, e.strerror)

How can I check the IP of each spider launched in scrapy?

AvyWam

[How can I check the IP of each spider launched in scrapy?](https://stackoverflow.com/questions/54650485/how-can-i-check-the-ip-of-each-spider-launched-in-scrapy)

Related to a previous question I would like to know when I launch spiders like this:scrapy crawl spider -a username=Bidule -a password=TMTC #cmd1scrapy crawl spider -a username=Truc -a password=TMTC #cmd2How to get the current IP for each one?Notes: in another forum a membre told me the proxy would manage it, but I would like to check if it is true, in order to know if I must change something in my code.

2019-02-12 12:48:27Z

Related to a previous question I would like to know when I launch spiders like this:scrapy crawl spider -a username=Bidule -a password=TMTC #cmd1scrapy crawl spider -a username=Truc -a password=TMTC #cmd2How to get the current IP for each one?Notes: in another forum a membre told me the proxy would manage it, but I would like to check if it is true, in order to know if I must change something in my code.You can make a request to a site that returns your IP address, and parse the IP out of the response.For example, https://httpbin.org/ip

Can't assign value to self variable python

billy

[Can't assign value to self variable python](https://stackoverflow.com/questions/54885669/cant-assign-value-to-self-variable-python)

I'm trying to append string to indexes array inside parse function but when i try to save it to .json became empty.

2019-02-26 12:32:14Z

I'm trying to append string to indexes array inside parse function but when i try to save it to .json became empty.Variable self.indexes will not be filled after cycle with Requests. The requests even not being done there.If you don't want to use common export to file, you can put writing to file to function on spider close. Check details here: scrapy: Call a function when a spider quitsYou need to bind signal to function and write code there.

Scrapy crawl page of urls, extract value from each new page

jedens

[Scrapy crawl page of urls, extract value from each new page](https://stackoverflow.com/questions/54653645/scrapy-crawl-page-of-urls-extract-value-from-each-new-page)

First post here and I am pretty new to scrapy. I am trying to crawl a page of .htm urls:

https://www.bls.gov/bls/news-release/empsit.htm#2008and extract two datapoints from each crawled page, saving each iteration to a csv file using Scrapy. Currently, I have:then runI have hit a bottleneck with scrapy. I cannot loop through the html pages. I can extract the two datapoints from each page using their xpaths and lxml:I cannot get scrapy to loop through the urls. Any thoughts on how to proceed? Thank you for your help.

2019-02-12 15:35:21Z

First post here and I am pretty new to scrapy. I am trying to crawl a page of .htm urls:

https://www.bls.gov/bls/news-release/empsit.htm#2008and extract two datapoints from each crawled page, saving each iteration to a csv file using Scrapy. Currently, I have:then runI have hit a bottleneck with scrapy. I cannot loop through the html pages. I can extract the two datapoints from each page using their xpaths and lxml:I cannot get scrapy to loop through the urls. Any thoughts on how to proceed? Thank you for your help.

How do I scrape data using ItemLoaders from a table using Scrapy?

Dinesh Gupta

[How do I scrape data using ItemLoaders from a table using Scrapy?](https://stackoverflow.com/questions/54898597/how-do-i-scrape-data-using-itemloaders-from-a-table-using-scrapy)

I am trying to extract data from the website "https://www.brickworkratings.com/CreditRatings.aspx". There is a table through which I can easily extract data through Scrapy Shell.I wanted to use ItemLoaders as it is really powerful and gives a cleaner experience.Here is my code below.I am getting the error, I believe my XPath is right but I don't think this is the way to use it. How do I use it correctly? I need to extract data from the table and wanted to use the more powerful ItemLoaders.Any help will be appreciated, been stuck on it for a long time.

2019-02-27 05:29:08Z

I am trying to extract data from the website "https://www.brickworkratings.com/CreditRatings.aspx". There is a table through which I can easily extract data through Scrapy Shell.I wanted to use ItemLoaders as it is really powerful and gives a cleaner experience.Here is my code below.I am getting the error, I believe my XPath is right but I don't think this is the way to use it. How do I use it correctly? I need to extract data from the table and wanted to use the more powerful ItemLoaders.Any help will be appreciated, been stuck on it for a long time.You need to specify the initial/parent selector when constructing the loader. It is then unnecessary to provide the response as well. And then you need to pass an XPath string to add_xpath instead of using get_xpath. Refer to the documentation.Assuming your XPath is correct, here is an example:If you need to do additional processing, look at input/output processors.

How to execute Python code from an AJAX request?

Shay Moshe

[How to execute Python code from an AJAX request?](https://stackoverflow.com/questions/54658966/how-to-execute-python-code-from-an-ajax-request)

I've gotten a task at University to create a website for teaching other students how to use python libraries (Scrapy, BS) for web scraping. I've already set up the frontend part with React and React Ace editor and now I'm trying to figure out how pass code that user entered to the script. Do I need to make my own server (api) or I can call the python script directly and pass all the code using ajax request?What is the best approach?What I want is to pass all the code that user entered in the online editor to the python script and then return the output of that script as a response to the initial request, so that I could check whether the result is correct.

2019-02-12 21:32:17Z

I've gotten a task at University to create a website for teaching other students how to use python libraries (Scrapy, BS) for web scraping. I've already set up the frontend part with React and React Ace editor and now I'm trying to figure out how pass code that user entered to the script. Do I need to make my own server (api) or I can call the python script directly and pass all the code using ajax request?What is the best approach?What I want is to pass all the code that user entered in the online editor to the python script and then return the output of that script as a response to the initial request, so that I could check whether the result is correct.my suggestion is to create a server with API (it's actually AJAX)

and then to run the Python code,

one good option us to create express app (Node.js HTTP server) and use python-shell see the first example with runString option.you will need a server with node.js and python installed.of course this only for a university task, in this option you run the code on your server, and it's a pretty big security issue. 

Scrapy callback function in another file

Robert T

[Scrapy callback function in another file](https://stackoverflow.com/questions/54619754/scrapy-callback-function-in-another-file)

I am using Scrapy with Python to scrape several websites.I got many Spiders with a structure like this:I would like to know if there's any way I can put the last 2 functions, which are called in the callback, in another module that is common to all my Spiders (so that if I modify it then all of them change). I know they are class functions but is there anyway I could put them in another file?I have tried declaring the functions in my library.py file but my problem is how can I pass the 2 parameters needed (self, response) to them.

2019-02-10 18:44:07Z

I am using Scrapy with Python to scrape several websites.I got many Spiders with a structure like this:I would like to know if there's any way I can put the last 2 functions, which are called in the callback, in another module that is common to all my Spiders (so that if I modify it then all of them change). I know they are class functions but is there anyway I could put them in another file?I have tried declaring the functions in my library.py file but my problem is how can I pass the 2 parameters needed (self, response) to them.Create a base class to contain those common functions. Then your real spiders can inherit from that. For example, if all your spiders extend Spider then you can do the following:spiders/basespider.py:spiders/realspider.py:If you have different types of spiders you can create different base classes. Or your base class can be a plain object (not Spider) and then you can use it as a mixin.

Is there a way to start a scrpy crwal with clean jobdir?

Edik Mkoyan

[Is there a way to start a scrpy crwal with clean jobdir?](https://stackoverflow.com/questions/54655260/is-there-a-way-to-start-a-scrpy-crwal-with-clean-jobdir)

After a paused crawl I want to restart the crawl with a JOBDIR option enabled to persist the state of the new crawl, but I do want to erase the state of the previous crawl as the data is obsolete.There is no start/stop/resume for scrapy, the only command to manage the crawl is 

2019-02-12 17:08:26Z

After a paused crawl I want to restart the crawl with a JOBDIR option enabled to persist the state of the new crawl, but I do want to erase the state of the previous crawl as the data is obsolete.There is no start/stop/resume for scrapy, the only command to manage the crawl is If your intention is to start a new crawl, as opposed to resuming a previously paused crawl, you have to either delete the old directory, or specify a new directory:Starting and resuming happens with the same command, but based on the directory name. So if you specify a new directory it is considered a "start". If you specify an existing directory it is considered a "resume".

You can stop/pause a crawl by pressing Ctrl+C. Refer to the documentation: https://docs.scrapy.org/en/latest/topics/jobs.html#how-to-use-it

Python Scrapy scrape all the div in a particular div and get links from those each div

aakash singh

[Python Scrapy scrape all the div in a particular div and get links from those each div](https://stackoverflow.com/questions/54572519/python-scrapy-scrape-all-the-div-in-a-particular-div-and-get-links-from-those-ea)

I am scrapping a website which has a list or collection of div in a particular div. So I am trying to get the links from each of this div. Do I need to iterate over it?So how can I extract second  href or link from each div.

2019-02-07 11:38:41Z

I am scrapping a website which has a list or collection of div in a particular div. So I am trying to get the links from each of this div. Do I need to iterate over it?So how can I extract second  href or link from each div.Try response.xpath('//div[@class="a"]/div/div[2]/a'). It will give you second div of div.b and then extract link a from it.You can add some text to required a, so we would understand you more clear.A more compact option is just to use .css, check it out: .a is a compact way to specify a tag-class.

How to attach proxy in AWS Lambda function for avoid '403 Forbidden error'?

Slinser

[How to attach proxy in AWS Lambda function for avoid '403 Forbidden error'?](https://stackoverflow.com/questions/54577511/how-to-attach-proxy-in-aws-lambda-function-for-avoid-403-forbidden-error)

I'm making a scrapy crawler that parses site data and inserting it to dynamoDB through AWS Lambda FunctionSometimes it works fine - returning all data that i wanted with 200 status. And sometimes it wont crawling because of 403 (Forbidden) status.I checked that on AWS Cloud9 the chance that an 403 Forbidden error will be less that on AWS Lambda: 100 function runs and only 3 Forbidden on AWS Cloud9, 100 function run and 78 Forbidden on AWS Lambda.

So i think i should try to do something with AWS Lambda network settings, like attach proxy or something else, but, how can i do it?i'm already tried settings for crawler like: previously settings was: but it makes no difference: still works good on cloud9, sometimes(rarely) good on AWS LambdaUPD: i found out that i getting 403 forbidden status in the daytime (from around 8:00 to 22:00) and in the nighttime i mostly get 200 ok status, so i think its because of site permissions to allowing and dissalowing robots in different periods of the day, but why running same script via AWS cloud9 mostly works good even in the daytime, while AWS lambda mostly not?pre-apologize for grammar

2019-02-07 16:08:42Z

I'm making a scrapy crawler that parses site data and inserting it to dynamoDB through AWS Lambda FunctionSometimes it works fine - returning all data that i wanted with 200 status. And sometimes it wont crawling because of 403 (Forbidden) status.I checked that on AWS Cloud9 the chance that an 403 Forbidden error will be less that on AWS Lambda: 100 function runs and only 3 Forbidden on AWS Cloud9, 100 function run and 78 Forbidden on AWS Lambda.

So i think i should try to do something with AWS Lambda network settings, like attach proxy or something else, but, how can i do it?i'm already tried settings for crawler like: previously settings was: but it makes no difference: still works good on cloud9, sometimes(rarely) good on AWS LambdaUPD: i found out that i getting 403 forbidden status in the daytime (from around 8:00 to 22:00) and in the nighttime i mostly get 200 ok status, so i think its because of site permissions to allowing and dissalowing robots in different periods of the day, but why running same script via AWS cloud9 mostly works good even in the daytime, while AWS lambda mostly not?pre-apologize for grammar

Capture Google Search Term and ResultStats with Scrapy

M. S.

[Capture Google Search Term and ResultStats with Scrapy](https://stackoverflow.com/questions/54618966/capture-google-search-term-and-resultstats-with-scrapy)

I have built a very simple scraper using Scrapy. For the output table, I would like to show the Google News search term as well as the Google resultstats value.The information I would like to capture is showing in the source of the Google page as andI have already tried to include both through ('input.value::text') and ('id.resultstats::text'), which did not work, however. Does anyone have an idea how to solve this situation?

2019-02-10 17:20:37Z

I have built a very simple scraper using Scrapy. For the output table, I would like to show the Google News search term as well as the Google resultstats value.The information I would like to capture is showing in the source of the Google page as andI have already tried to include both through ('input.value::text') and ('id.resultstats::text'), which did not work, however. Does anyone have an idea how to solve this situation?The pages renders differently when you access it with Scrapy.The search field becomes:response.css('input#sbhost::attr(value)').get()The results count is:response.css('#resultStats::text').get()Also, there is no quote class on that page.You can test this in the scrapy shell:scrapy shell -s ROBOTSTXT_OBEY=False "https://www.google.com/search?q=elon+musk&source=lnt&tbs=cdr%3A1%2Ccd_min%3A1%2F1%2F2015%2Ccd_max%3A12%2F31%2F2015&tbm=nws"And then run those 2 commands.[EDIT]

If your goal is to get one item for each URL, then you can do this:If your goal is to extract every result on the page, then you need something different.

How to make scrapy output to stdout to be read from Python

FLOWMEEN

[How to make scrapy output to stdout to be read from Python](https://stackoverflow.com/questions/54584910/how-to-make-scrapy-output-to-stdout-to-be-read-from-python)

I have a spider that I want to output its results to standard output so that it can be read by subprocess.check_output. I don't want to output to a file as an intermediary.I've tried adding the flag '-o', 'stdout' but it doesn't work.

2019-02-08 01:31:28Z

I have a spider that I want to output its results to standard output so that it can be read by subprocess.check_output. I don't want to output to a file as an intermediary.I've tried adding the flag '-o', 'stdout' but it doesn't work.Try this:

Main .pySub yourspider.py

What is causing this error? Missing scheme in request url: h

MONINGI KAVYA

[What is causing this error? Missing scheme in request url: h](https://stackoverflow.com/questions/54571712/what-is-causing-this-error-missing-scheme-in-request-url-h)

When I am trying to crawl my webpage it is giving me the output but some error is showing up:books2.pyExpected result:The actual result is:

2019-02-07 10:55:39Z

When I am trying to crawl my webpage it is giving me the output but some error is showing up:books2.pyExpected result:The actual result is:You are extracting image_urls as u'…'. The value of image_urls must be a list: [u'…'].In your code, switch:toLooks Like you are missing or passing some invalid details in Request call.Go to your all Request and find out that the url you are passing is in correct form.Try using Request(response.urljoin(url), ...) So that it will cover any bad url schema.

How to login with a scrapy spider when the login page is as GET method instead of a POST method?

AvyWam

[How to login with a scrapy spider when the login page is as GET method instead of a POST method?](https://stackoverflow.com/questions/54596450/how-to-login-with-a-scrapy-spider-when-the-login-page-is-as-get-method-instead-o)

I plan to login this website with a scrapy spider.

The login page is a GET request methodAs you can see, this is a GET method and there is not any attributes shown for the id and passeword.

but in the HTML code, there is a link(it does not work when a write it in the url bar of my browser), it indicates a POST method method="post". But when I login, there is no POST file appearing in the networks of the browser console.What to do then?

2019-02-08 16:21:22Z

I plan to login this website with a scrapy spider.

The login page is a GET request methodAs you can see, this is a GET method and there is not any attributes shown for the id and passeword.

but in the HTML code, there is a link(it does not work when a write it in the url bar of my browser), it indicates a POST method method="post". But when I login, there is no POST file appearing in the networks of the browser console.What to do then?There actually is a POST request in the network log, but you have to enable "Persist Logs" ("Conserver les journaux"):



This POST request issues a redirect which causes the log to clear without that setting.One thing to keep in mind here is that you need to grab the CSRF token. Here is a quick example for how you can login using scrapy shell:scrapy shell https://www.paris-turf.com/Then run the following:Looks like it is POST and the post variables _username and _password are being sent:

(screen cap from Firefox when I clicked "Edit and resend" in the right had side panel)

CrawlSpider not executing callback

slash007

[CrawlSpider not executing callback](https://stackoverflow.com/questions/54528041/crawlspider-not-executing-callback)

I have created a spider by extending CrawlSpider.When spider runs and finds article page I want to get a link to authors profile and make a request to the profile page and parse it with parse_author but for some reason, this parse_author callback is never executed.My code:

2019-02-05 05:16:05Z

I have created a spider by extending CrawlSpider.When spider runs and finds article page I want to get a link to authors profile and make a request to the profile page and parse it with parse_author but for some reason, this parse_author callback is never executed.My code:The problem is that your link extraction rules also match author links, and Scrapy drops duplicate requests by default, so your parse_item method is the one receiving the responses that you expect in parse_author.Possible solutions include:You are using the wrong port on the site pages:Your line yield Request(url=author_url, callback=self.parse_author) refers to (for example) http://www.cityam.com/profile/joseph.ray (on port 80). You must use the request via HTTPS: https://www.cityam.com/profile/joseph.ray

And then program execution will proceed to the method parse_author.Change your request URL and everyone will be happy.



And by the way, I consider it a bad practice to write code like this:print 'Article url : ' + response.url (Some compilers will not understand this and there will be an error.)Must be: print("Article url : " + response.url)

Data-Structure conversion of Fields for csv output

Tribic

[Data-Structure conversion of Fields for csv output](https://stackoverflow.com/questions/54533135/data-structure-conversion-of-fields-for-csv-output)

I´m still new to this and im wondering, if there is an easier way, to separate text. Right now i`m working in excel and have multiple Data in one Cell. Separating them is no fun 😊 

Actually my data, a class of three fields(),  looks like this (Each A can have mupltiple B; Each B has 7x C):A,  “B1,B2”,    “C1,C2,C3,…, C14”And I´d like to fill/save it like this:A,  B1, C1 A,  B1, C2 … A,  B1, C7 A,  B2, …This is my code:andThanks for your help and any ideas!

2019-02-05 11:11:50Z

I´m still new to this and im wondering, if there is an easier way, to separate text. Right now i`m working in excel and have multiple Data in one Cell. Separating them is no fun 😊 

Actually my data, a class of three fields(),  looks like this (Each A can have mupltiple B; Each B has 7x C):A,  “B1,B2”,    “C1,C2,C3,…, C14”And I´d like to fill/save it like this:A,  B1, C1 A,  B1, C2 … A,  B1, C7 A,  B2, …This is my code:andThanks for your help and any ideas!You can use a function to split build skills in chunks (like chunks() here) and do something in the lines of:I think the problem lies in this part: zip(hero_names, hero_buildss, hero_buildskillss). If I understand correctly you want to make the carthesian product of the 3 lists, which you can do like this:If there is a dependency between hero-builss & herobuildskillss, the below might work better:

How to check and write to Postgres with Python and Sqlalchemy

Bart

[How to check and write to Postgres with Python and Sqlalchemy](https://stackoverflow.com/questions/54542225/how-to-check-and-write-to-postgres-with-python-and-sqlalchemy)

I'm writing a scraper with scrapy, sqlalchemy and postgres. I'd like the script to check for new items and, if there are any, send an email and write to database.

I thought of two tables- one permanent, and one temporary, dropped after its data is processed. I'd like to check if items in the temporary exist in the permanent list, and if not- write them to pernament list.

How do I construct the expression to check if the results exist in the other table, with sqlalchemy?

I can successfuly write to both of the tables, but I struggle with the next stage is to check for a change and write new items to the permanent table.Here's the model for table:Here's the pipeline 

2019-02-05 20:06:25Z

I'm writing a scraper with scrapy, sqlalchemy and postgres. I'd like the script to check for new items and, if there are any, send an email and write to database.

I thought of two tables- one permanent, and one temporary, dropped after its data is processed. I'd like to check if items in the temporary exist in the permanent list, and if not- write them to pernament list.

How do I construct the expression to check if the results exist in the other table, with sqlalchemy?

I can successfuly write to both of the tables, but I struggle with the next stage is to check for a change and write new items to the permanent table.Here's the model for table:Here's the pipeline I think rather than handling with two different tables, I will declare an unique index (constraint) spanning multiple columns, in you case if you think two items are equal if tuple (title, url) are equal, then declare a uniqueness constrain on both (title, url). You can simply insert values to your main table, and when you try to save a duplicate item, postgres will throw an exception, which is IntegrityException in SqlAlchemy. Catch that exception and ignore it. Something along the line of[3]. Please do remember that IntegrityException is kind of catch all. Please see:[1] https://www.postgresql.org/docs/9.0/indexes-unique.html[2] https://docs.sqlalchemy.org/en/latest/core/constraints.html#unique-constraint[3] What is the good approach to check an existance of unique values in database table by SqlAlchemy in python2.7

How to stop Scrapy Selector wrap an xml with html?

user3435407

[How to stop Scrapy Selector wrap an xml with html?](https://stackoverflow.com/questions/54534516/how-to-stop-scrapy-selector-wrap-an-xml-with-html)

I do this:The output is:How can I stop Selector wrapping the xml with html and body? Thanks

2019-02-05 12:31:34Z

I do this:The output is:How can I stop Selector wrapping the xml with html and body? Thanksscrapy.Selector assumes html, but takes a type argument to change that.So, to make an xml selector, simply use Selector(text=xmlstr, type='xml')

Scrapy next page, no sense results

Luis Miguel

[Scrapy next page, no sense results](https://stackoverflow.com/questions/54493670/scrapy-next-page-no-sense-results)

Trust you are doing well!I´m scraping some web pages and when I try to go to the next page I´m not able to, because the next page results, it doesn´t matter with what I´d look for at the first one.An example:

Fist page look for: https://www.mister-auto.es/buscar/?q=corteco

Second page: https://www.mister-auto.es/buscar/?page=2The problem that I´ve is that the results at the second doesn´t has no sense with what I´d look for.I´m using crawlspider with linkextractor to go to the next.

Could you give me a hand?

Thank you very much for your support.

2019-02-02 13:48:01Z

Trust you are doing well!I´m scraping some web pages and when I try to go to the next page I´m not able to, because the next page results, it doesn´t matter with what I´d look for at the first one.An example:

Fist page look for: https://www.mister-auto.es/buscar/?q=corteco

Second page: https://www.mister-auto.es/buscar/?page=2The problem that I´ve is that the results at the second doesn´t has no sense with what I´d look for.I´m using crawlspider with linkextractor to go to the next.

Could you give me a hand?

Thank you very much for your support.The website you're scraping is dynamic and when you're changing pages it does not reflect in the URL.What you want is a tool like Puppeteer or Selenium to render the page dynamically, click buttons and extract the content you want. While it is a great tool for certain jobs, Scrapy has its limitations.

Create a static column item

Ryan Evans

[Create a static column item](https://stackoverflow.com/questions/54451606/create-a-static-column-item)

I have a simple spider that crawls local obituaries. The code works perfectly until I try to add two static columns. All I want to do is add the date I pulled the information (pull item) and the state in which it was pulled (state item). It's a self loading page so when I add the pull date, I only get the first 10 results (or only the first page). If I add just the state, I only get two results. When I remove both, I get all 40+ results.I did # lines that aren't working properly:Item.py file:spider file:

2019-01-31 00:22:52Z

I have a simple spider that crawls local obituaries. The code works perfectly until I try to add two static columns. All I want to do is add the date I pulled the information (pull item) and the state in which it was pulled (state item). It's a self loading page so when I add the pull date, I only get the first 10 results (or only the first page). If I add just the state, I only get two results. When I remove both, I get all 40+ results.I did # lines that aren't working properly:Item.py file:spider file:I explain why:WILL WORKING:class AlabamaSpider(scrapy.Spider):

      name = 'alabama'

      allowed_domains = ['legacy.com']

      start_urls = ['http://www.legacy.com/obituaries/annistonstar/browsetype=paid&page=20']

Scraping an URL i got from scraping a page

Manuel

[Scraping an URL i got from scraping a page](https://stackoverflow.com/questions/54463949/scraping-an-url-i-got-from-scraping-a-page)

I'm running into some issues while trying to scrape a page. I asked this previously but probably my question was quite confusing.Link to previous question

Scraping an URL that I scraped while parsingBasically what I'm trying to do is parsing an URL I got from another page, while keeping the same Item.I get some data from a page, which is assigned to certain Item fields. What I want to do is to add more data to that item but from a URL I get while parsing the previous one.Something like this:I tried using something along the lines ofBut it did not work.How can scrape a new URL while 'saving' the item information i got from the 'parse' method?

2019-01-31 15:28:52Z

I'm running into some issues while trying to scrape a page. I asked this previously but probably my question was quite confusing.Link to previous question

Scraping an URL that I scraped while parsingBasically what I'm trying to do is parsing an URL I got from another page, while keeping the same Item.I get some data from a page, which is assigned to certain Item fields. What I want to do is to add more data to that item but from a URL I get while parsing the previous one.Something like this:I tried using something along the lines ofBut it did not work.How can scrape a new URL while 'saving' the item information i got from the 'parse' method?You used the right way. It is the most convenient way to pass data from one function to another.

How did you get data in your parseNext function? It should be like this:

i want to crawl a website with python,but i meet a trouble . requests library is ok but 400 with Scrapy,the code below

xiao ming

[i want to crawl a website with python,but i meet a trouble . requests library is ok but 400 with Scrapy,the code below](https://stackoverflow.com/questions/54464670/i-want-to-crawl-a-website-with-python-but-i-meet-a-trouble-requests-library-is)

i want to crawl a website with python,but i meet a trouble . requests library is ok but 400 with Scrapy,the code belowHow do I fix this question

2019-01-31 16:07:21Z

i want to crawl a website with python,but i meet a trouble . requests library is ok but 400 with Scrapy,the code belowHow do I fix this questionI'm sorry, but you won't succeed, because the page loads dynamically.It is necessary to compile javascript on the fly - Selenium, Splash

Extend a Scrapy ItemLoader with custom methods

NFB

[Extend a Scrapy ItemLoader with custom methods](https://stackoverflow.com/questions/54493641/extend-a-scrapy-itemloader-with-custom-methods)

The Scrapy documentation lists all the built-in methods of ItemLoader instances and explains how to declare your own Item Loaders. However, any ItemLoaders you declare will apply to all processed items. You can modify their behavior a little with Item Loader Contexts, but this is frequently not granular enough.Suppose I have a Scrapy project where the spiders and items all inherit the same base spider and item loaders, but the spiders all contain site-specific logic with a handful of common functions. Nowhere in the Scrapy documentation do I find mention of adding class methods to ItemLoaders so that instead of:You could write:Even though this seems like an obvious way to extend ItemLoaders like you would for any other class, it's not documented and I don't see examples of how to do this in Scrapy anywhere I've checked (Google, StackOverflow). Is it possible/supported, and how would you declare them?

2019-02-02 13:44:51Z

The Scrapy documentation lists all the built-in methods of ItemLoader instances and explains how to declare your own Item Loaders. However, any ItemLoaders you declare will apply to all processed items. You can modify their behavior a little with Item Loader Contexts, but this is frequently not granular enough.Suppose I have a Scrapy project where the spiders and items all inherit the same base spider and item loaders, but the spiders all contain site-specific logic with a handful of common functions. Nowhere in the Scrapy documentation do I find mention of adding class methods to ItemLoaders so that instead of:You could write:Even though this seems like an obvious way to extend ItemLoaders like you would for any other class, it's not documented and I don't see examples of how to do this in Scrapy anywhere I've checked (Google, StackOverflow). Is it possible/supported, and how would you declare them?It is possible. Which way to do it depends on the type of logic you are sharing.You can declare your methods in a Scrapy-agnostic way, i.e. as you would do with any other Python class: subclass your CustomItemLoader class and define the method in that subclass:Alternatively, depending on the actual logic that you have in that function shared by some spiders, a simple processor that you pass to your add_* methods might be the way to go.

Scrapy multiple regular expressions in LinkExtractor seem to be not working

Chris

[Scrapy multiple regular expressions in LinkExtractor seem to be not working](https://stackoverflow.com/questions/54471814/scrapy-multiple-regular-expressions-in-linkextractor-seem-to-be-not-working)

I've got my regular expressions inside a JSON file. This file gets loaded as a configuration for my spider. The spider creates one LinkExtractor with allow and deny regular expression rules.

I'd like to:It all works well on some shops, but not on others and I believe it's a problem of my Regular Expressions.URL patterns:Loading the rules from JSON inside my spider __init__If I do a pprint(vars(onerule.link_extractor)) I can see the Python regex correctly:Testing the regex in https://regex101.com/ seems to be fine as well (despite: I'm using \\/ in my JSON file and \/ in regex101.com)In my spider logfile, I can see that the produce pages are being crawled, but not parsed:Why does the spider not parse the product pages?

(same code, different JSON works on different shops)

2019-02-01 01:53:17Z

I've got my regular expressions inside a JSON file. This file gets loaded as a configuration for my spider. The spider creates one LinkExtractor with allow and deny regular expression rules.

I'd like to:It all works well on some shops, but not on others and I believe it's a problem of my Regular Expressions.URL patterns:Loading the rules from JSON inside my spider __init__If I do a pprint(vars(onerule.link_extractor)) I can see the Python regex correctly:Testing the regex in https://regex101.com/ seems to be fine as well (despite: I'm using \\/ in my JSON file and \/ in regex101.com)In my spider logfile, I can see that the produce pages are being crawled, but not parsed:Why does the spider not parse the product pages?

(same code, different JSON works on different shops)After hours of debugging and testing, I figured that I had to change the order of the rules.Now it is working.

Running spiders from a separate python script

Manuel

[Running spiders from a separate python script](https://stackoverflow.com/questions/54535507/running-spiders-from-a-separate-python-script)

I was wondering what is the best way to run spiders from another python script. My scrapy project consist of 4 different spiders, all of them create files that help the other spiders work and some of them have to read some files to work. That part is already done but individually (running the spiders separate from the console).How can I, for example, do something like thisMy final plan is to upload the full program to the cloud and make it run automatically, can this be done?I found some answers to this question but they were pretty old, probably for another version of scrapy.

2019-02-05 13:26:53Z

I was wondering what is the best way to run spiders from another python script. My scrapy project consist of 4 different spiders, all of them create files that help the other spiders work and some of them have to read some files to work. That part is already done but individually (running the spiders separate from the console).How can I, for example, do something like thisMy final plan is to upload the full program to the cloud and make it run automatically, can this be done?I found some answers to this question but they were pretty old, probably for another version of scrapy.Assuming you have everything else set up correctly, here is a trivial example as per the documentation.You can then run this on a cloud server. But I cannot answer whether this is the optimal solution for the problem you are trying to solve.

Why is my spider_idle / on-demand / URL-feeding like gradually shutting down?

Dharma Guy

[Why is my spider_idle / on-demand / URL-feeding like gradually shutting down?](https://stackoverflow.com/questions/54469412/why-is-my-spider-idle-on-demand-url-feeding-like-gradually-shutting-down)

I have an spider_idle signal set up to feed another batch of urls to the spider. However, this seems to work fine at the beginning, but then the Crawled (200)... messages appear more and more rarely to finally stop appearing. I've got 115 test URLs to distribute, and as Scrapy says it Crawled 38 pages... out of it. Below is the code of the spider and the scrapy log.In general, I'm implementing 2-stage crawling, first pass only downloads urls to the urls.jl file, second pass is to perform scraping on those URls. I'm now approaching coding of the second spider.The logs:I've expected that the spider will crawl all 115 URLs, not only 38. Also, if it doesn't want to crawl anymore, and the singal-handler function doesn't raise DontCloseSpider, then shouldn't it at least shut-down then?

2019-01-31 21:21:09Z

I have an spider_idle signal set up to feed another batch of urls to the spider. However, this seems to work fine at the beginning, but then the Crawled (200)... messages appear more and more rarely to finally stop appearing. I've got 115 test URLs to distribute, and as Scrapy says it Crawled 38 pages... out of it. Below is the code of the spider and the scrapy log.In general, I'm implementing 2-stage crawling, first pass only downloads urls to the urls.jl file, second pass is to perform scraping on those URls. I'm now approaching coding of the second spider.The logs:I've expected that the spider will crawl all 115 URLs, not only 38. Also, if it doesn't want to crawl anymore, and the singal-handler function doesn't raise DontCloseSpider, then shouldn't it at least shut-down then?The missing requests are not failing, otherwise you would see information about it in the logs as well. They are not being sent at all.If you watch your log closely, you will notice this message:The missing requests are being skipped because they are considered duplicates. See the documentation of the DUPEFILTER_CLASS setting for more information.

Scrapy spider returns no items data

Daniel Borysowski

[Scrapy spider returns no items data](https://stackoverflow.com/questions/54423375/scrapy-spider-returns-no-items-data)

My scrapy script seems not to follow links, which ends up not extracting data from each of them (to pass some content as scrapy items).I am trying to scrape a lot of data from a news website. I managed to copy/write a spider that, as I assumed, should read links from a file (I've generated it with another script), put them in start_urls list and start following these links to extract some data, and then pass it as items, and also -- write each item's data in a separate file (last part is actually for another question).After running scrapy crawl PNS, script goes through all the links from start_urls but does nothing more -- it follows links read from start_urls list (I'm getting "GET link" message in bash), but seems not to enter them and read some more links to follow and extract data from.Expected result:Actual result:

2019-01-29 14:35:57Z

My scrapy script seems not to follow links, which ends up not extracting data from each of them (to pass some content as scrapy items).I am trying to scrape a lot of data from a news website. I managed to copy/write a spider that, as I assumed, should read links from a file (I've generated it with another script), put them in start_urls list and start following these links to extract some data, and then pass it as items, and also -- write each item's data in a separate file (last part is actually for another question).After running scrapy crawl PNS, script goes through all the links from start_urls but does nothing more -- it follows links read from start_urls list (I'm getting "GET link" message in bash), but seems not to enter them and read some more links to follow and extract data from.Expected result:Actual result:Dude, I have been coding in Python Scrapy for long time and I hate using start_urlsYou can simply use start_requests which is very easy to read, and also very easy to learn for beginners I also have never used Item class and find it useless tooYou can simply have data_dic = {} instead of data_dic = ProjectNameArticle()

php call scrapy by custom command

許家瑄

[php call scrapy by custom command](https://stackoverflow.com/questions/54338381/php-call-scrapy-by-custom-command)

I have to run my five different scrapy spiders at the same time so I define a custom command in scrapy liketo call these spiders. It works successfully when I run in command line in independent.However it fails when I use php shell_exec to call the same command and the code in php file is likeThe web page will echo nothing immediately and not waiting the scrapy function(or even it wasn't be called at the beginning I don't know). So how can I call this command or other way to run these 5 spiders at the same time in php file? Thanks!

2019-01-24 02:03:35Z

I have to run my five different scrapy spiders at the same time so I define a custom command in scrapy liketo call these spiders. It works successfully when I run in command line in independent.However it fails when I use php shell_exec to call the same command and the code in php file is likeThe web page will echo nothing immediately and not waiting the scrapy function(or even it wasn't be called at the beginning I don't know). So how can I call this command or other way to run these 5 spiders at the same time in php file? Thanks!I have solved the problem. If you get the same situation, try to remove all comment in your command python file. I don't know the reason but after I do that it works successful.

Scrapy: How to get a list of urls and loop over them afterwards

Tribic

[Scrapy: How to get a list of urls and loop over them afterwards](https://stackoverflow.com/questions/54312430/scrapy-how-to-get-a-list-of-urls-and-loop-over-them-afterwards)

i'm new to python and scrapy, watched a few udemy and youtube tutorials and now trying my first own example. I know howto loop, if there's a next-button. But in my case, there is none.Here's my code, working on one of the urls, but the start url needs to be changed later:But this is only one Hero, and i want about 90 of them. Each url depends on the hero name.

I can get the list of urls by this command:But i don´t know howto store this list in order to get the parse function to loop over them.thanks in advance!

2019-01-22 16:21:58Z

i'm new to python and scrapy, watched a few udemy and youtube tutorials and now trying my first own example. I know howto loop, if there's a next-button. But in my case, there is none.Here's my code, working on one of the urls, but the start url needs to be changed later:But this is only one Hero, and i want about 90 of them. Each url depends on the hero name.

I can get the list of urls by this command:But i don´t know howto store this list in order to get the parse function to loop over them.thanks in advance!Is it critical to parse them in parse function? You can parse your hero list in one function and then iterate this list to scrape hero data it in this way:

Scrapy export dynamic items to csv

geeone

[Scrapy export dynamic items to csv](https://stackoverflow.com/questions/54271750/scrapy-export-dynamic-items-to-csv)

I have spider that scrapes some dinamic fields like this:But I do not know in advance how many items I will have, besides on each page there are a different number of items. When I export to CSV, it looks like a Scrapy creates a file with keys from first item. I mean, it writes all values, but if for example the first item has 1 key, then we will have only one key in CSV, and the remaining items will be arranged in the wrong order if they have more keys. How to make Scrapy create a CSV file from an Item with the largest number of keys?

2019-01-19 21:57:48Z

I have spider that scrapes some dinamic fields like this:But I do not know in advance how many items I will have, besides on each page there are a different number of items. When I export to CSV, it looks like a Scrapy creates a file with keys from first item. I mean, it writes all values, but if for example the first item has 1 key, then we will have only one key in CSV, and the remaining items will be arranged in the wrong order if they have more keys. How to make Scrapy create a CSV file from an Item with the largest number of keys?

Scrapy: Using CSS Selectors to exclude a node/tag

James

[Scrapy: Using CSS Selectors to exclude a node/tag](https://stackoverflow.com/questions/54342343/scrapy-using-css-selectors-to-exclude-a-node-tag)

In the documentation and SO articles, there are only references on how to exclude CSS classes using this nomenclature:What I want to achieve however is to exclude a node, or even, multiple nodes, such as <span> and <div> elements which are inside an <li> element.Let me give you an example. Let's say I am scraping this HTML:,and I am only interested in scraping the text "This is the string I want to scrape", thus I want to skip both <div> and <span> nodes. I tried to use the following, inside the scrapy shell, to no avail:,but I am still getting the excluded nodes.

2019-01-24 08:33:36Z

In the documentation and SO articles, there are only references on how to exclude CSS classes using this nomenclature:What I want to achieve however is to exclude a node, or even, multiple nodes, such as <span> and <div> elements which are inside an <li> element.Let me give you an example. Let's say I am scraping this HTML:,and I am only interested in scraping the text "This is the string I want to scrape", thus I want to skip both <div> and <span> nodes. I tried to use the following, inside the scrapy shell, to no avail:,but I am still getting the excluded nodes.Easy:

Scrapy Result not being written

extensionhelp

[Scrapy Result not being written](https://stackoverflow.com/questions/54273807/scrapy-result-not-being-written)

I'm scraping the following site: https://graphics.stltoday.com/apps/payrolls/salaries/teachers/Hoping to scrape all the data for each individual. This means following the link to each district, then to each job category within the district, and finally to each employee. I think the issue may be with my regular expressions for the URLs, but I'm not sure. Once on each employee's page, I think I've correctly identified the XPaths:The spider runs (for a few minutes before I pause it), but nothing is written to the .csv file.     

2019-01-20 05:19:24Z

I'm scraping the following site: https://graphics.stltoday.com/apps/payrolls/salaries/teachers/Hoping to scrape all the data for each individual. This means following the link to each district, then to each job category within the district, and finally to each employee. I think the issue may be with my regular expressions for the URLs, but I'm not sure. Once on each employee's page, I think I've correctly identified the XPaths:The spider runs (for a few minutes before I pause it), but nothing is written to the .csv file.     So I went down a rabbit-hole and reconstructed the spider to a basic one, rather the crawl. I couldn't understand why in the LinkEctract rule set wasn't calling back to the parser.In any case, I created a cvs_exporter function to better manage the output. Added it and its arguments to the settings and voila.

How to keep Scrapy Crawler running

buklaou

[How to keep Scrapy Crawler running](https://stackoverflow.com/questions/54274767/how-to-keep-scrapy-crawler-running)

I currently have a Scrapy crawler that runs once. I've been searching for a solution to have it continuously repeat its crawling cycle until it's stopped. In other words, once the first iteration of the crawl completes, automatically start a second iteration without stopping the entire crawler, after that a third iteration, and so on. Also, perhaps running again after x seconds, although I'm unsure how the system would react in the case of the previous crawl process not finishing while also trying to launch another iteration.Solutions I've found online thus far only refer to cron or scrapyd which I'm not interested in. I'm more interested in implementing a custom scheduler within the crawler project using processes such as CrawlerRunner or reactors. Does anyone have a couple pointers?The following code from another stackoverflow question is the closest information I found in regard to my questions, but am looking for some advice on how to implement a more continuous approach. Error:

"message": "Module 'twisted.internet.reactor' has no 'run' member",

"source": "pylint",UPDATE

How to schedule Scrapy crawl execution programmaticallyTried to implement this but am unable to import my spider, I get module not found error. Also the reactor variables are red with error and say Module 'twisted.internet.reactor' has no 'callLater' member//////or has no 'run' member.

2019-01-20 08:29:53Z

I currently have a Scrapy crawler that runs once. I've been searching for a solution to have it continuously repeat its crawling cycle until it's stopped. In other words, once the first iteration of the crawl completes, automatically start a second iteration without stopping the entire crawler, after that a third iteration, and so on. Also, perhaps running again after x seconds, although I'm unsure how the system would react in the case of the previous crawl process not finishing while also trying to launch another iteration.Solutions I've found online thus far only refer to cron or scrapyd which I'm not interested in. I'm more interested in implementing a custom scheduler within the crawler project using processes such as CrawlerRunner or reactors. Does anyone have a couple pointers?The following code from another stackoverflow question is the closest information I found in regard to my questions, but am looking for some advice on how to implement a more continuous approach. Error:

"message": "Module 'twisted.internet.reactor' has no 'run' member",

"source": "pylint",UPDATE

How to schedule Scrapy crawl execution programmaticallyTried to implement this but am unable to import my spider, I get module not found error. Also the reactor variables are red with error and say Module 'twisted.internet.reactor' has no 'callLater' member//////or has no 'run' member.Unless you ellaborate on what you mean by “more continuous”, the only way I can think of to make the code of the quoted response more continuous is to replace 5 with 0 in the deferred.Use apscheduler

JSONDecodeError: Expecting value in scrapy

vezunchik

[JSONDecodeError: Expecting value in scrapy](https://stackoverflow.com/questions/54214795/jsondecodeerror-expecting-value-in-scrapy)

I am using scrapy

json.load(response.body)and we found JSONDecodeError: Expecting value: line 1 column 1 (char 0)Here is the link 

https://www.magellanprovider.com/ProviderSearchGateway/sessions/52229928/providers.jsonp?start=1&end=100&callback=jQuery112404923709263392255_1547626291787&_=1547626291795

2019-01-16 10:14:50Z

I am using scrapy

json.load(response.body)and we found JSONDecodeError: Expecting value: line 1 column 1 (char 0)Here is the link 

https://www.magellanprovider.com/ProviderSearchGateway/sessions/52229928/providers.jsonp?start=1&end=100&callback=jQuery112404923709263392255_1547626291787&_=1547626291795You have JS callback in your response. Try to remove extra variables from your url like https://www.magellanprovider.com/ProviderSearchGateway/sessions/52229928/providers.jsonp?start=1&end=100 or use regex to cut json from response text.if you haven't notice your text starts from jQuery112404923709263392255_1547626291787 due to this line, string can't be converted to jsonTry using this simple regex to get the json string and then parse it to json.loads()

IMDB Movie Scraping gives blank csv using scrapy

Dharmik Mehta

[IMDB Movie Scraping gives blank csv using scrapy](https://stackoverflow.com/questions/54237605/imdb-movie-scraping-gives-blank-csv-using-scrapy)

I am getting Blank csv, though its not showing any error in code.

It is unable to crawl through web page.This is the code which I have written referring youtube:-This is output I am getting while running executing code withscrapy crawl imdbtestspider -o example.csv -t csv

2019-01-17 14:03:14Z

I am getting Blank csv, though its not showing any error in code.

It is unable to crawl through web page.This is the code which I have written referring youtube:-This is output I am getting while running executing code withscrapy crawl imdbtestspider -o example.csv -t csvThis is another way you might give a try with. I used css selector instead of xpath to make the script less verbose.I have tested you given xpaths i don't know they are mistakenly wrong or are actually wrong.e.g;Plus your xpaths are not yielding any results.As to why you are getting the error that says 0/pages crawled, despite not recreating your case, I have to assume that your method of page iteration is not building the page URLs correctly.I'm having trouble understanding the use for creating the variable array of all the "follow links" and then using len to send them to the parse_indetail() but a couple things to note.Should be something like this...Notice two things:

Id the parse() function. All I'm doing here is using a for loop through the links, each instance in loop referred to href, and pass the urljoined href to the parser function.  Give your use case, this is more than enough. In a situation where you have the next page, it's just creating a variable for the "next" page somehow and callback to parse, it will keep doing that till it cant fint a "next" page.Secondly, Use xpath only when in the HTML items have the same tagwith different content. This is more of a personal opinion but I tell people that xpath selectors is like scalpel and css selectors is like a butcher knife. You can get damn accurate with scalpel but it takes more time and in many cases may be just easier to go with CSS selector to get the same result.

I want to add item class within an item class

helpdoc

[I want to add item class within an item class](https://stackoverflow.com/questions/54248547/i-want-to-add-item-class-within-an-item-class)

Final JSON will be :I want to create my items.py like (just example):How to implement this , already checked this https://doc.scrapy.org/en/latest/topics/items.html#extending-itemshow to implement nested item in scrapy?but there are no clue about this concept ... any suggestion? 

2019-01-18 06:07:47Z

Final JSON will be :I want to create my items.py like (just example):How to implement this , already checked this https://doc.scrapy.org/en/latest/topics/items.html#extending-itemshow to implement nested item in scrapy?but there are no clue about this concept ... any suggestion? This way you can do this:Let me give you example,

Unable to execute /usr/local/bin/scrapyd-deploy: No such file or directory

affemann2

[Unable to execute /usr/local/bin/scrapyd-deploy: No such file or directory](https://stackoverflow.com/questions/54244399/unable-to-execute-usr-local-bin-scrapyd-deploy-no-such-file-or-directory)

I am trying to get scrapyd to deploy but everytime I run the commandI get the following errorI did the following to try and trouble shootI checked usr/local/bin and found that the following files existI'm not sure why the scrapy files exist in the folder but when I try to run scrapyd-deploy local it cannot find them. 

2019-01-17 21:14:42Z

I am trying to get scrapyd to deploy but everytime I run the commandI get the following errorI did the following to try and trouble shootI checked usr/local/bin and found that the following files existI'm not sure why the scrapy files exist in the folder but when I try to run scrapyd-deploy local it cannot find them. I had upgraded to os mojave after which all of the errors started. When I first tired to run scrapyd, brew installed python.I was able to resolve issue by starting over. I did the following:

brew uninstall python

pip uninstall scrapy

pip uninstall scrapyd

pip uninstall scrapyd-client

I deleted dockerI then reinstalled scrapyd-client after which the error resolved and I was able to deploy scrapyd.

Scrapy spider with IMAGES_STORE on DigitalOcean Spaces

Lennybee

[Scrapy spider with IMAGES_STORE on DigitalOcean Spaces](https://stackoverflow.com/questions/54234649/scrapy-spider-with-images-store-on-digitalocean-spaces)

I need help with the IMAGES_STORE setting for my scrapy spider.I am hosting with DigitalOcean and would like to download the images to their Spaces.My scrapy app is intergrated with a django website. I am already using Spaces to store the images for the django website using the instructions here https://www.digitalocean.com/community/tutorials/how-to-set-up-object-storage-with-djangoIt is very similar to the settings for AWS S3.The API endpoint for my space is https://ams3.digitaloceanspaces.comAccording to the scrapy documentation the AWS settings are as follows...I've tried adapting this for DigitalOcean Spaces but have been unsuccessful.I've already triedandAny help on this would be great.

2019-01-17 11:10:36Z

I need help with the IMAGES_STORE setting for my scrapy spider.I am hosting with DigitalOcean and would like to download the images to their Spaces.My scrapy app is intergrated with a django website. I am already using Spaces to store the images for the django website using the instructions here https://www.digitalocean.com/community/tutorials/how-to-set-up-object-storage-with-djangoIt is very similar to the settings for AWS S3.The API endpoint for my space is https://ams3.digitaloceanspaces.comAccording to the scrapy documentation the AWS settings are as follows...I've tried adapting this for DigitalOcean Spaces but have been unsuccessful.I've already triedandAny help on this would be great.I don't have access to DigitalOcean Spaces nor Amazon S3 so take this just as a starting point. Looking into API documentation for DigitalOcean Spaces (where is an example for Python) and Scrapy code for the FilesPipeline (which ImagesPipeline is based on), I would say you also need to adjust at least AWS_ENDPOINT_URL in settings.py, maybe also other AWS_* options.

finding right selector for pagination with scrapy

Ramosta

[finding right selector for pagination with scrapy](https://stackoverflow.com/questions/54250214/finding-right-selector-for-pagination-with-scrapy)

I'm trying to extract data from this forum:https://schwangerschaft.gofeminin.de/forum/allI get the data from the first page. I use the css selector 'li.selected > a::attr(href)' Unfortunately I can not get all other data from other pages.What is the right path for xpath or css selectors for pagination?Python:HTML:link of next page:

https://schwangerschaft.gofeminin.de/forum/all/p2

2019-01-18 08:38:04Z

I'm trying to extract data from this forum:https://schwangerschaft.gofeminin.de/forum/allI get the data from the first page. I use the css selector 'li.selected > a::attr(href)' Unfortunately I can not get all other data from other pages.What is the right path for xpath or css selectors for pagination?Python:HTML:link of next page:

https://schwangerschaft.gofeminin.de/forum/all/p2Try response.css('link[rel=next]::attr(href)').get(), this should work.Given the way the way this particular websites navbar is built, what I like to do in these situations is use xpath. Given that the current page will have a class of "selected", I would select the "selected" class and then use "following-sibling" syntax with the index of 1 to obtain the very following tags.In your case:So no matter which page you are in, you are dynamically choosing the "next" page.

is there any way to translate web page language, or translating scraped data while scraping using scrapy?

Yagnesh

[is there any way to translate web page language, or translating scraped data while scraping using scrapy?](https://stackoverflow.com/questions/54158991/is-there-any-way-to-translate-web-page-language-or-translating-scraped-data-whi)

i am going to scrape dintex.net website in english laguage, but can't find any way to convert scraped data in English language. I also used googletans but it also shows error, so is there any other way to convert that page or data to English?

2019-01-12 11:07:39Z

i am going to scrape dintex.net website in english laguage, but can't find any way to convert scraped data in English language. I also used googletans but it also shows error, so is there any other way to convert that page or data to English?I think you should send this cookie with your requests As the page allows for localisation depending on selection of the available langauage/region. You would need to do something like this 

see cookie part from the scrapy request from scrapy docs The request you are yielding might need to change something like this(not tested)

Scrapy. How to resolve 520?

sgr

[Scrapy. How to resolve 520?](https://stackoverflow.com/questions/54129054/scrapy-how-to-resolve-520)

This website response How can i resolve this ?I post my code for explainimport json

2019-01-10 12:46:08Z

This website response How can i resolve this ?I post my code for explainimport jsonyes because the web require valid browser's headers. while scrapy send headers as a bot.Try to use these headers:You can see crawled status over your websiteI suggest that you monitor what your web browser does when you send the form from the web browser (Network tab of the developer tools), and try to reproduce the request with Scrapy.In Firefox, for example, you can copy the successful request from the Network tab as a curl command, which is a clear representation of the request.

Scrapy stops early, no errors

Kron

[Scrapy stops early, no errors](https://stackoverflow.com/questions/54098624/scrapy-stops-early-no-errors)

so I am trying to scrape a blog with around 2000 pages of posts but scrapy seems to be finishing early and not getting to the last page. I am not too sure what would be causing this. It seems to stop at random points. Any idea why this is happening? I assume it has something to do with the pagination, but I can't figure out what's happening.

2019-01-08 19:38:25Z

so I am trying to scrape a blog with around 2000 pages of posts but scrapy seems to be finishing early and not getting to the last page. I am not too sure what would be causing this. It seems to stop at random points. Any idea why this is happening? I assume it has something to do with the pagination, but I can't figure out what's happening.Most likely your spider just fails to find next page:try adding:This will open up scrapy debug shell and you can interact with objects real time:Seems like your css for next page might be a bit over-engineered here and some html inconsistency might break it. Try just a.next.page-numbers::attr(href)

redis set can't save more data after 3w value

Carl

[redis set can't save more data after 3w value](https://stackoverflow.com/questions/54124960/redis-set-cant-save-more-data-after-3w-value)

I was using scrapy-redis crawl data and using redis to dupefilter request,after crawl about 3000k request, suddenly the spider can't get or put request into the dupefilter key, whatever I restart the spider(not clean the key) or redis, it just stop crawl. I want to know is the redis has some limit of it?I tried many times and always the request num is about 3000k, it must have some association, please help me!the redis-server show:and when the bug appear, it suddenly have two more clients connect, I don't know why they occur 

and them I close the spider and client closed connection

2019-01-10 08:55:04Z

I was using scrapy-redis crawl data and using redis to dupefilter request,after crawl about 3000k request, suddenly the spider can't get or put request into the dupefilter key, whatever I restart the spider(not clean the key) or redis, it just stop crawl. I want to know is the redis has some limit of it?I tried many times and always the request num is about 3000k, it must have some association, please help me!the redis-server show:and when the bug appear, it suddenly have two more clients connect, I don't know why they occur 

and them I close the spider and client closed connection

Scrapy: Can't restart start_requests() properly

Nikolay Shindarov

[Scrapy: Can't restart start_requests() properly](https://stackoverflow.com/questions/54125590/scrapy-cant-restart-start-requests-properly)

I have a scraper that initiates two pages - one of them is the main page, and the other is a .js file which containt long and lat coordinates I need to extract, because I need them later in the parsing process. I want first to process the .js file, extract the coordinates, and then parse the main page and start crawling its links/parsing its items. 

For this purpose I am using the priority parameter in the Request method and I am saying that I want my .js page to be processed first. This works, but only around 70% of the time (must be due to the Scrapy's asynchronous requests). The rest 30% of the time I end up in my parse method trying to parse the .js long/lat coordinates, but having passed the main website page, so it's impossible to parse them.For this reason, I tried to fix it this way:

when in parse() method, check which n-th url is that, if it is the first one and is not the .js one, restart the spider. However, when I restart the spider the next time it passes correctly the .js first, but after its processing the spider finished work and exits the script without an error as if it were completed.

Why is that happening, what is the difference with the processing of the pages when I restart the spider compared to when I just start it, and how can I fix this problem?This is the code with sample outputs in both scenarios when I was trying to debug what is being executed and why it stops when being restarted.Output when the spider starts correctly, gets first the .js page and second starts parsing the cats:And the script goes on and parses everything fine.

Output when the spider starts incorrectly, gets first the main page and restarts start_requests():And the script stops its execution without and error as if it were completed.P.S. If this matters, I did mention that the processing URL in the start_requests() is processed reversed order, but I find this natural due the the loop sequence and I expect the priority param to do its job (as it does it most of the time and it should do it as per Scrapy's docs).

2019-01-10 09:30:35Z

I have a scraper that initiates two pages - one of them is the main page, and the other is a .js file which containt long and lat coordinates I need to extract, because I need them later in the parsing process. I want first to process the .js file, extract the coordinates, and then parse the main page and start crawling its links/parsing its items. 

For this purpose I am using the priority parameter in the Request method and I am saying that I want my .js page to be processed first. This works, but only around 70% of the time (must be due to the Scrapy's asynchronous requests). The rest 30% of the time I end up in my parse method trying to parse the .js long/lat coordinates, but having passed the main website page, so it's impossible to parse them.For this reason, I tried to fix it this way:

when in parse() method, check which n-th url is that, if it is the first one and is not the .js one, restart the spider. However, when I restart the spider the next time it passes correctly the .js first, but after its processing the spider finished work and exits the script without an error as if it were completed.

Why is that happening, what is the difference with the processing of the pages when I restart the spider compared to when I just start it, and how can I fix this problem?This is the code with sample outputs in both scenarios when I was trying to debug what is being executed and why it stops when being restarted.Output when the spider starts correctly, gets first the .js page and second starts parsing the cats:And the script goes on and parses everything fine.

Output when the spider starts incorrectly, gets first the main page and restarts start_requests():And the script stops its execution without and error as if it were completed.P.S. If this matters, I did mention that the processing URL in the start_requests() is processed reversed order, but I find this natural due the the loop sequence and I expect the priority param to do its job (as it does it most of the time and it should do it as per Scrapy's docs).As to why your Spider doesn't continue in the "restarting" case; you probably run afoul of duplicate requests being filtered/dropped. Since the page has already been visited, Scrapy thinks it's done.

So you would have to re-send these requests with a dont_filter=True argument:As to a better solution instead of this hacky approach, consider using InitSpider (for example, other methods exist). This guarantees your "initial" work got done and can be depended on.

(For some reason the class was never documented in the Scrapy docs, but it's a relatively simple Spider subclass: do some initial work, before starting the actual run.)  And here is a code-example for that:which would result in output like this:So I finally handled it with a workaround:

I check what is the response.url received in parse() and based on that I send the further parsing to a corresponding method:

How to retrieve item list from html table with xpath?

merlin

[How to retrieve item list from html table with xpath?](https://stackoverflow.com/questions/54060846/how-to-retrieve-item-list-from-html-table-with-xpath)

I am trying to extract table information into a dictionary within python 3.7.The html from the table looks like this:My best guess is:I am getting a Keyerror, with a key from another part of the page. So something within the xpath selector must be wrong.More info:How can I pull the table into the dict?

2019-01-06 10:59:28Z

I am trying to extract table information into a dictionary within python 3.7.The html from the table looks like this:My best guess is:I am getting a Keyerror, with a key from another part of the page. So something within the xpath selector must be wrong.More info:How can I pull the table into the dict?Try to check whether there is a title and description values or not and if there is no value - set default value:

How to run multiple the same spider in Scrapy?

Chaii

[How to run multiple the same spider in Scrapy?](https://stackoverflow.com/questions/54063061/how-to-run-multiple-the-same-spider-in-scrapy)

I have a url list, for example [' http://example.com/page1 ', http://example.com/page2',...].These urls are in a domain name, and I've written a crawler with Scrapy, and I need to run those urls together, using the same crawler. If I have 10 urls, I want to create 10 of the same process to run the crawler to improve efficiency. Is there a solution?I tried to use CrawlerProcess to run the crawler, but if the urls were too many, it would remind me of an error with too many TCP connections. Although the crawler is always running, this method is not conducive to maintenance.

2019-01-06 15:32:49Z

I have a url list, for example [' http://example.com/page1 ', http://example.com/page2',...].These urls are in a domain name, and I've written a crawler with Scrapy, and I need to run those urls together, using the same crawler. If I have 10 urls, I want to create 10 of the same process to run the crawler to improve efficiency. Is there a solution?I tried to use CrawlerProcess to run the crawler, but if the urls were too many, it would remind me of an error with too many TCP connections. Although the crawler is always running, this method is not conducive to maintenance.If the goal is to improve the number of concurrent requests by a single spider, and not specifically to spawn multiple spiders in parallel, I suggest that you simply play around with the settings like DOWNLOAD_DELAY, CONCURRENT_REQUEST or CONCURRENT_REQUESTS_PER_DOMAIN.

Can I safely move Scrapy's HttpCacheMiddleware closer to the engine?

Jabb

[Can I safely move Scrapy's HttpCacheMiddleware closer to the engine?](https://stackoverflow.com/questions/54063606/can-i-safely-move-scrapys-httpcachemiddleware-closer-to-the-engine)

The site that I am scraping occasionally returns fake responses with error code 200. So I need to use a custom http caching policy that prevents code 200 responses with a certain substring in the response body to end up in the httpcache.Scrapy's default middleware order is below. With this order, the httpcache middleware does not have access to the uncompressed document body yet (gzip compressed).Can I safely move the HttpCacheMiddleware closer to the engine like in the below example? Or will this cause unwanted side effects on other middlewares?

2019-01-06 16:31:51Z

The site that I am scraping occasionally returns fake responses with error code 200. So I need to use a custom http caching policy that prevents code 200 responses with a certain substring in the response body to end up in the httpcache.Scrapy's default middleware order is below. With this order, the httpcache middleware does not have access to the uncompressed document body yet (gzip compressed).Can I safely move the HttpCacheMiddleware closer to the engine like in the below example? Or will this cause unwanted side effects on other middlewares?

Scrapy linkextractor ignores parameters behind the sign # and thus will not follow the link

merlin

[Scrapy linkextractor ignores parameters behind the sign # and thus will not follow the link](https://stackoverflow.com/questions/54061112/scrapy-linkextractor-ignores-parameters-behind-the-sign-and-thus-will-not-foll)

I am trying to crawl a website with scrapy where the pagination is behind the sign "#". This somehow makes scrapy ignore everything behind that character and it will always only see the first page.e.g.: If you enter a question mark manually, the site will load page 1The stats from scrapy tell me it fetched the first page:My crawler looks like this:How can I make scrapy ignore the # inside the url and visit the given URL?

2019-01-06 11:36:39Z

I am trying to crawl a website with scrapy where the pagination is behind the sign "#". This somehow makes scrapy ignore everything behind that character and it will always only see the first page.e.g.: If you enter a question mark manually, the site will load page 1The stats from scrapy tell me it fetched the first page:My crawler looks like this:How can I make scrapy ignore the # inside the url and visit the given URL?Scrapy performs HTTP requests. The data after '#' in a URL is not part of an HTTP request, it is used by JavaScript.As suggested in the comments, the site loads the data using AJAX.Moreover, it does not use pagination in AJAX: the site downloads the whole list of watches as JSON in a single request, and then the pagination is done using JavaScript.So, you can just use the Network tab of the developer tools of your web browser to see the request that obtains the JSON data, and perform a similar request instead of requesting the HTML page.Note, however, that you cannot use LinkExtractor for JSON data. You should simply parse the response with Python’s json and iterate the URLs there.

Spider not scraping a list or urls

Manuel

[Spider not scraping a list or urls](https://stackoverflow.com/questions/54061898/spider-not-scraping-a-list-or-urls)

I have a spider that gets the URLs to scrape from a list. My problem is that, when I run the spider, no data is being scraped and, what is weird to me and I can't seem to be able to solve is that the spider is indeed entering each site, but not data comes back out.My spider looks like thisWhat I see in my console is thisConsoleLightshotPicture

2019-01-06 13:19:34Z

I have a spider that gets the URLs to scrape from a list. My problem is that, when I run the spider, no data is being scraped and, what is weird to me and I can't seem to be able to solve is that the spider is indeed entering each site, but not data comes back out.My spider looks like thisWhat I see in my console is thisConsoleLightshotPictureBy default when spider crawl through start_urls then its default callback function is:You can try changing your function parse_item to parse. 

unhashable type list issue while scraping a web page

Manuel

[unhashable type list issue while scraping a web page](https://stackoverflow.com/questions/54076700/unhashable-type-list-issue-while-scraping-a-web-page)

This question is kind of a follow up to a previous question I asked.Scraping data from a http & javaScript siteNew errors have appeared so, I'm stuck on those right now.The code is the same as the previous question, something along the lines ofI have tried printing the script and it works, it's a giant amount of text but, it prints something.The problem I'm having now is the regular expression one. When the code reachesWhich should take within 'data' everything that's between brackets, I get the unhashable type 'list' error.With this, I want to, after going throughGet a sort of dictionary in which I can be able to extract data from the pageIs this error coming from a bad use of re.findall or is it somewhere else in the code? (maybe there is an easier way to achieve this)Thanks for the help!

2019-01-07 14:54:46Z

This question is kind of a follow up to a previous question I asked.Scraping data from a http & javaScript siteNew errors have appeared so, I'm stuck on those right now.The code is the same as the previous question, something along the lines ofI have tried printing the script and it works, it's a giant amount of text but, it prints something.The problem I'm having now is the regular expression one. When the code reachesWhich should take within 'data' everything that's between brackets, I get the unhashable type 'list' error.With this, I want to, after going throughGet a sort of dictionary in which I can be able to extract data from the pageIs this error coming from a bad use of re.findall or is it somewhere else in the code? (maybe there is an easier way to achieve this)Thanks for the help!re.findall function takes both arguments as string butreturns a list.if script list is more than one element use:if its a single element in the list then:

Scrapy following links, extracting new ones and following them

Snail_nnaail

[Scrapy following links, extracting new ones and following them](https://stackoverflow.com/questions/54025952/scrapy-following-links-extracting-new-ones-and-following-them)

I'm trying to create a scraper that scrapes a website for its products. I decided to extract all the categories links from the navigation menu, then follow them and extract all the products links, which I later parse in the parse_product function. But I don't actually what's the best way to do that. I'm struggling with following parse_menu links and futher extractiong product links. Criticize my code pls. 

2019-01-03 16:10:13Z

I'm trying to create a scraper that scrapes a website for its products. I decided to extract all the categories links from the navigation menu, then follow them and extract all the products links, which I later parse in the parse_product function. But I don't actually what's the best way to do that. I'm struggling with following parse_menu links and futher extractiong product links. Criticize my code pls. 

Scrapy Pagination Infinite

Nicols Esteban Morales Morales

[Scrapy Pagination Infinite](https://stackoverflow.com/questions/53991748/scrapy-pagination-infinite)

i have a scrap web project with Scrapy. And i have URL with a infinite Pagination. My start URL is : https://nuevo.jumbo.cl/lacteos-y-bebidas-vegetales/leches-blancas?sl=3a356ef2-a2d4-4f1b-865f-c79b6fcf0f2a&PS=50&cc=18&sm=0&PageNumber=1&fq=C:/1/3/ , where the "PageNumber" is 1 to N Pages. 

I put this in a for loop, for testing, in the parse function:But i need to do it without the for loop, because in "start_urls" i have many more with differnt "PageNumber".So what i need is to go from PageNumber 1 to N, and when there are no more pages, stop that request.I try with meta dont_redirect, because  when the spider not find more pages, i was redirecting to other page.

Y try with Middlewares, in process_response() i put:With that it does not redirect to another page, but it keeps trying to scan.I need to do something like:

2018-12-31 22:02:45Z

i have a scrap web project with Scrapy. And i have URL with a infinite Pagination. My start URL is : https://nuevo.jumbo.cl/lacteos-y-bebidas-vegetales/leches-blancas?sl=3a356ef2-a2d4-4f1b-865f-c79b6fcf0f2a&PS=50&cc=18&sm=0&PageNumber=1&fq=C:/1/3/ , where the "PageNumber" is 1 to N Pages. 

I put this in a for loop, for testing, in the parse function:But i need to do it without the for loop, because in "start_urls" i have many more with differnt "PageNumber".So what i need is to go from PageNumber 1 to N, and when there are no more pages, stop that request.I try with meta dont_redirect, because  when the spider not find more pages, i was redirecting to other page.

Y try with Middlewares, in process_response() i put:With that it does not redirect to another page, but it keeps trying to scan.I need to do something like:what i observe from the web behavior, Products apiThis is the api from where it gets the products, and Here PageNumber controls the pagination and fq controls the product categrory etc. but when the pageNumber exceed, it never give you redirection {I have tested it in postman} but a empty response, so check if the products are received in the api call, increment the page number otherwise stop. And change the fq [category]           

Cannot import module with CrawlerProcess

chris

[Cannot import module with CrawlerProcess](https://stackoverflow.com/questions/53965299/cannot-import-module-with-crawlerprocess)

I am using a CrawlSpider with scrapy command line. Everything is fine:Now I want to use CrawlerProcess, and the item module crashes when I import it.Same thing, in the setting.py, I enable the pipeline with a similar module, and it is not loaded.startUp.pyNewproductcrawlerSpider.pythe folder structure is (cannot paste img anymore! ;o(( )+productsupervision++spiders+++newproductcrawler.py (the crawler)+++startUp.py++middlewares.py++pipelines.py++responseitem.py++settings.py+scrapy.cfgI looking for how the way to import correctly the iteml module with CrawlerProcess

2018-12-28 23:14:52Z

I am using a CrawlSpider with scrapy command line. Everything is fine:Now I want to use CrawlerProcess, and the item module crashes when I import it.Same thing, in the setting.py, I enable the pipeline with a similar module, and it is not loaded.startUp.pyNewproductcrawlerSpider.pythe folder structure is (cannot paste img anymore! ;o(( )+productsupervision++spiders+++newproductcrawler.py (the crawler)+++startUp.py++middlewares.py++pipelines.py++responseitem.py++settings.py+scrapy.cfgI looking for how the way to import correctly the iteml module with CrawlerProcessOk, found: 

startUp.py MUST be at the project root level. Same folder than scrapy.cfg

Scraping table provides no results

Jonas__G

[Scraping table provides no results](https://stackoverflow.com/questions/54080374/scraping-table-provides-no-results)

I thought I had a decent grasp of this now, but I've run into a problem again. I get no results like this - but if I move the yield one tab back - I get the suspected first two items.Here's a little snippet of the log: Here's the function: And here's a link to [the page][1]btw: I've quadrouple checked my tab's (five times)... What am I missing? Anyone? I'm sure it's a blind spot. 

2019-01-07 19:10:19Z

I thought I had a decent grasp of this now, but I've run into a problem again. I get no results like this - but if I move the yield one tab back - I get the suspected first two items.Here's a little snippet of the log: Here's the function: And here's a link to [the page][1]btw: I've quadrouple checked my tab's (five times)... What am I missing? Anyone? I'm sure it's a blind spot. This one gives you 4 rows, while first one is redundant and other Three have data.You don't need to use splash to get this data. it is statically available on the webpage. try consuming the generator:

Scrapy vs Javascript Pagination

terrain inconnu

[Scrapy vs Javascript Pagination](https://stackoverflow.com/questions/53989816/scrapy-vs-javascript-pagination)

Edit:I managed to make some progress by using Scrapy's FormRequest and are now looking to understand how to iterate through all pages. Unfortunately, the following code only returns the contents for page 209:It looks like the loop for i runs it course before the 'def parse(self, response)' part takes place (i.e. i runs up to 209 before moving on). Any idea on how to structure the iteration and get this working would be highly appreciated. Thank you.Original post:I am trying to crawl a website with Scrapy but got stuck as the page displays the data inside a form and uses javascript for pagination.This is the html part of the pagination:And here is the corresponding javascript:It would be great if someone could point me in the right direction on how to approach this. Thank you.

2018-12-31 17:11:02Z

Edit:I managed to make some progress by using Scrapy's FormRequest and are now looking to understand how to iterate through all pages. Unfortunately, the following code only returns the contents for page 209:It looks like the loop for i runs it course before the 'def parse(self, response)' part takes place (i.e. i runs up to 209 before moving on). Any idea on how to structure the iteration and get this working would be highly appreciated. Thank you.Original post:I am trying to crawl a website with Scrapy but got stuck as the page displays the data inside a form and uses javascript for pagination.This is the html part of the pagination:And here is the corresponding javascript:It would be great if someone could point me in the right direction on how to approach this. Thank you.There are normally two approaches to solve this problem, First, Use splash to render the javascript.Second,Find the network call when you press next, and then follow that call to extract desired data.Through trial & error I figured out that the iteration has to be inside the 'def parse' part. Here is the solution:

call several callback function in scrapy

houssem gharsalli

[call several callback function in scrapy](https://stackoverflow.com/questions/53936943/call-several-callback-function-in-scrapy)

I am using scrapy and I have several problems: first problem: I put start_requests in a loop but the function is not started from each iterationsecond problem: I need to call different callback related to the start_urls given by the loop, but I can't give a dynamic name for the callback. I would like to put callback=parse_i and i come from the loop above.thanks

2018-12-26 20:40:53Z

I am using scrapy and I have several problems: first problem: I put start_requests in a loop but the function is not started from each iterationsecond problem: I need to call different callback related to the start_urls given by the loop, but I can't give a dynamic name for the callback. I would like to put callback=parse_i and i come from the loop above.thanksThe callback attribute just needs to be a callable, so you can use getattr just like normal:Separately, while you didn't ask this, it is highly unusual to make a URL call from within start_requests since (a) that's why one would use Scrapy to begin with, to deal with all that non-200 stuff and (b) doing so with requests will not honor any of the throttling, proxy, user-agent, resumption, or host of other knobs over which one would wish to influence a scraping job.

X path is not able to extract element required in spider

sachin safale

[X path is not able to extract element required in spider](https://stackoverflow.com/questions/53909924/x-path-is-not-able-to-extract-element-required-in-spider)

In the above code i am trying to fetch price,title,reviews and the description if present,It extracts everything for a link where description is present but does not write anything for a link where thereis no description can any one help. follwing are the links :

https://www.amazon.com/Angelkiss-Leather-shoulder-backpack-K15631/dp/B01NCX988Q --- with description

https://www.amazon.com/dp/B06W9HL2L1 --- without description

2018-12-24 06:28:05Z

In the above code i am trying to fetch price,title,reviews and the description if present,It extracts everything for a link where description is present but does not write anything for a link where thereis no description can any one help. follwing are the links :

https://www.amazon.com/Angelkiss-Leather-shoulder-backpack-K15631/dp/B01NCX988Q --- with description

https://www.amazon.com/dp/B06W9HL2L1 --- without descriptionMake sure to avoid using compound classes. I've tried to show how they should be defined. All you need to do is replace the xapths used below with the one you have used in your scrapy project.Don't start with "." for xpath expressions. It is for realative xpath expressions.The item you gave without description happens to be out of stock and its totally different with the scenario of not having a description.

And following current examples you give when an item is out of stock these attributes never comes. :) so first check for the availability of the product then go for its attributes.

How to recrawl pages with 4xx error code in scrapy while using cache?

merlin

[How to recrawl pages with 4xx error code in scrapy while using cache?](https://stackoverflow.com/questions/53907503/how-to-recrawl-pages-with-4xx-error-code-in-scrapy-while-using-cache)

Is there a way to ignore 4xx error codes while recrawling a domain that is partialy within cache?I have crawled a huge part of the page before running into issues, then I realigned the settings to not cache 4xx codes, because the crawler stopedCrawled (403) <GET https:/... ['cached']:Changed cache setting to: HTTPCACHE_IGNORE_HTTP_CODES = [401, 403, 404]This unfortunatelly seems to force me to recrawl the page without cache, as I am getting now this info from logs:Either way, the crawler stops at the same position as it is retrieving the cached 403 response codes, while they are now 200 from non cache.How can I adapt the settings in order to continue crawling the page?Or as an alterantive, how can the cache be emptied/saved? Because otherwise I would need to override without the cache setting as far as I understand the docs.

2018-12-23 22:07:26Z

Is there a way to ignore 4xx error codes while recrawling a domain that is partialy within cache?I have crawled a huge part of the page before running into issues, then I realigned the settings to not cache 4xx codes, because the crawler stopedCrawled (403) <GET https:/... ['cached']:Changed cache setting to: HTTPCACHE_IGNORE_HTTP_CODES = [401, 403, 404]This unfortunatelly seems to force me to recrawl the page without cache, as I am getting now this info from logs:Either way, the crawler stops at the same position as it is retrieving the cached 403 response codes, while they are now 200 from non cache.How can I adapt the settings in order to continue crawling the page?Or as an alterantive, how can the cache be emptied/saved? Because otherwise I would need to override without the cache setting as far as I understand the docs.The best solution I could find is to change the name of the crawler and start crawling fresh. This worked as it is using a new cache folder, but my original question was not answered by this and I had to recrawl pages I already had downloaded to cache. When you cache a page, then on each same request, scrapy goes to that cached data and if that page happened to be cached as 403 or any other, scrapy does not offer to crawl again. so, either you remove that page from the cached data or turn off cache to fetch the web page again.Use the HTTPCACHE_IGNORE_HTTP_CODES setting.

How to write python scrapy code for extracting url's present in sitemap of a site and export it to csv

KwodKewe

[How to write python scrapy code for extracting url's present in sitemap of a site and export it to csv](https://stackoverflow.com/questions/53936631/how-to-write-python-scrapy-code-for-extracting-urls-present-in-sitemap-of-a-sit)

I've found the working solution to write python scrapy code for extracting url's present in sitemap of a site from here but don't know how to export the data to CSV file!

When I try to run scrapy crawl myspider -o mydata.csv it returns an empty csv file, but list of urls are getting printed on screen!

2018-12-26 20:07:29Z

I've found the working solution to write python scrapy code for extracting url's present in sitemap of a site from here but don't know how to export the data to CSV file!

When I try to run scrapy crawl myspider -o mydata.csv it returns an empty csv file, but list of urls are getting printed on screen!First, you aren't make any request with scrapy, also you're combining scrapy with requests, that i think it's not the best idea. Try to change __init__ to:Also, your self._parse_sitemap SHOULD return dict-like or Request(not only your self._parse_sitemap, every function in your scrapy spider, see docs):The whole file(probably doesn't work, but explains the idea):

How can I scrape any website using scrapy in such a way that the website doesn't considers my bot as a registered or logged in user?

Avnish Kumar

[How can I scrape any website using scrapy in such a way that the website doesn't considers my bot as a registered or logged in user?](https://stackoverflow.com/questions/53910180/how-can-i-scrape-any-website-using-scrapy-in-such-a-way-that-the-website-doesnt)

The website is considering my bot as a logged in user but i want to make the website identify my bot as an unregistered user.

2018-12-24 06:57:33Z

The website is considering my bot as a logged in user but i want to make the website identify my bot as an unregistered user.

Does scrapy-crawlera handle a 429 status code?

Kevin Glasson

[Does scrapy-crawlera handle a 429 status code?](https://stackoverflow.com/questions/53910309/does-scrapy-crawlera-handle-a-429-status-code)

Wondering if anyone knows if scrapy-crawlera middleware handles the 429 status code when using scrapy, or if I need to implement my own retry logic?I can't seem to find it documented anywhere

2018-12-24 07:11:49Z

Wondering if anyone knows if scrapy-crawlera middleware handles the 429 status code when using scrapy, or if I need to implement my own retry logic?I can't seem to find it documented anywhereTo answer your question: NO, The scrapy-crawlera Middleware doesn't handle 429 status, it actually doesn't "handle" any status, it just handles communication between Crawlera and Scrapy.Now, about Crawlera, they do handle status 429 by default, meaning that when they get a 429 response status, they will mark it as a ban and retry the same request.If Crawlera didn't succeed after several retries, it will return a 503 status to the client (Scrapy in this case).You can extend list of retry codes with:(check documentation here: https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:setting-RETRY_HTTP_CODES)And then in you spider:

Is it possible to do (kind of) Polymorphism with scrapy

Manuel

[Is it possible to do (kind of) Polymorphism with scrapy](https://stackoverflow.com/questions/53873751/is-it-possible-to-do-kind-of-polymorphism-with-scrapy)

Im currently having some issues trying to adapt my scrapy program. The thing im trying do is make a different parser work depending on the "site" im in.Currently i have this start requestI want to find a way to, depending on which keyword i get from the txt file, call different parsers for extracting the data from the page.Is it there a way to accomplish this?

2018-12-20 17:50:21Z

Im currently having some issues trying to adapt my scrapy program. The thing im trying do is make a different parser work depending on the "site" im in.Currently i have this start requestI want to find a way to, depending on which keyword i get from the txt file, call different parsers for extracting the data from the page.Is it there a way to accomplish this?what about matching the callback dependant on the keyword you got inside start_requests? Something like:

Scrapy: using multiple image fields, store results

DigFor

[Scrapy: using multiple image fields, store results](https://stackoverflow.com/questions/53874596/scrapy-using-multiple-image-fields-store-results)

I am using scrapy to capture several images. I save them in separate fields. Because of dependencies with other systems, I do not want all the images results (url, path, checksum) stored in one field.The results (url, path, checksum), are stored in;I finally have it working where it downloads 2 pictures.

The results for the image_url1 is stored in images1. Only it doesn't store the results for image_url2 in images2. I don't know how I make clear that the results for image_url2 should be stored in images2. If I now run the following code, it tries to put the 2 results (url, path, checksum) together (results of image_url1 and image_url2 behind each other separated by a space). It cannot insert that field into MySQL so that fails.I already made the following edit in settings;I can't find anything how to work with multiple image fields. *** Edit/solution after feedback

After the suggestion to do this in item_completed, I come up with the following;Don't know if this is the best way to do it, but it works. Feedback is appreciated. Thanks!

2018-12-20 19:05:39Z

I am using scrapy to capture several images. I save them in separate fields. Because of dependencies with other systems, I do not want all the images results (url, path, checksum) stored in one field.The results (url, path, checksum), are stored in;I finally have it working where it downloads 2 pictures.

The results for the image_url1 is stored in images1. Only it doesn't store the results for image_url2 in images2. I don't know how I make clear that the results for image_url2 should be stored in images2. If I now run the following code, it tries to put the 2 results (url, path, checksum) together (results of image_url1 and image_url2 behind each other separated by a space). It cannot insert that field into MySQL so that fails.I already made the following edit in settings;I can't find anything how to work with multiple image fields. *** Edit/solution after feedback

After the suggestion to do this in item_completed, I come up with the following;Don't know if this is the best way to do it, but it works. Feedback is appreciated. Thanks!The item_completed() method is what does this storing, so that's what you'll need to override.You will need to know where to store the data though, so you'll probably want to add that information to the meta dict of the request you create in get_media_requests().

Need to get a specific class exists in HTML body

Salman Sakib

[Need to get a specific class exists in HTML body](https://stackoverflow.com/questions/53832836/need-to-get-a-specific-class-exists-in-html-body)

I am trying to check if class = "special-price" exists in below code. 

 Here is html code :I am using Scrapy with python. After checking if the class found I need to collect text of class="price". 

2018-12-18 12:18:20Z

I am trying to check if class = "special-price" exists in below code. 

 Here is html code :I am using Scrapy with python. After checking if the class found I need to collect text of class="price". Did you try something like:or for short:it will give you None in case there is no element with special-price class.

Cannot import my own module in scrapy crawler

chris

[Cannot import my own module in scrapy crawler](https://stackoverflow.com/questions/53871052/cannot-import-my-own-module-in-scrapy-crawler)

I'm writing a crawler using Scrapy. I've built a crawler and it works very well.Now I want to create my own modules, but I always receive this error:The code is:

I am running:  scrapy crawl --nolog  samplecrawler. I'm on Windows 10.My project structure is:

2018-12-20 14:55:01Z

I'm writing a crawler using Scrapy. I've built a crawler and it works very well.Now I want to create my own modules, but I always receive this error:The code is:

I am running:  scrapy crawl --nolog  samplecrawler. I'm on Windows 10.My project structure is:You can do several things:FirstAs suggested by @elRuLLSecondThis generally a bad and brittle solution but possible.ThirdYou can package it as package and do.init.py:samplecrawler.pyyou need to include the full module path:you have to include the name of the folder as the module nameFound, after some hours:import crawl1.spiders.moduletest

Scrapy shell works fine but 404 on crawler

LucSpan

[Scrapy shell works fine but 404 on crawler](https://stackoverflow.com/questions/53813754/scrapy-shell-works-fine-but-404-on-crawler)

Set-upI'm trying to scrape this page with Scrapy.In the scrapy shell, I get the correct 200 on the page using a USER_AGENT, i.e.ProblemIn my actual script, using the same USER_AGENT, Scrapy gets a 404,I've already tried ROBOTSTXT_OBEY = False in the settings.py. Can I solve this, and if so, how do I do this?

2018-12-17 10:56:48Z

Set-upI'm trying to scrape this page with Scrapy.In the scrapy shell, I get the correct 200 on the page using a USER_AGENT, i.e.ProblemIn my actual script, using the same USER_AGENT, Scrapy gets a 404,I've already tried ROBOTSTXT_OBEY = False in the settings.py. Can I solve this, and if so, how do I do this?

Capturing HTTP Errors using scrapy

chrisbunney

[Capturing HTTP Errors using scrapy](https://stackoverflow.com/questions/53821617/capturing-http-errors-using-scrapy)

I'm trying to scrape a website for broken links, so far I have this code which is successfully logging in and crawling the site, but it's only recording HTTP status 200 codes:The docs and these answers lead me to believe that handle_httpstatus_all = True should cause scrapy to pass errored requests to my parse method, but so far I've not been able to capture any.I've also experimented with handle_httpstatus_list and a custom errback handler in a different iteration of the code.What do I need to change to capture the HTTP error codes scrapy is encountering?

2018-12-17 19:10:30Z

I'm trying to scrape a website for broken links, so far I have this code which is successfully logging in and crawling the site, but it's only recording HTTP status 200 codes:The docs and these answers lead me to believe that handle_httpstatus_all = True should cause scrapy to pass errored requests to my parse method, but so far I've not been able to capture any.I've also experimented with handle_httpstatus_list and a custom errback handler in a different iteration of the code.What do I need to change to capture the HTTP error codes scrapy is encountering?handle_httpstatus_list can be defined on the spider level, but handle_httpstatus_all can only be defined on the Request level, including it on the meta argument.I would still recommend using an errback for these cases, but if everything is controlled, it shouldn't create new problems.So, I don't know if this is the proper scrapy way, but it does allow me to handle all HTTP status codes (including 5xx).I disabled the HttpErrorMiddleware by adding this snippet to my scrapy project's settings.py:

unable to scrape by traversing <br> in <td> using scrapy with css

vinit jha

[unable to scrape by traversing <br> in <td> using scrapy with css](https://stackoverflow.com/questions/53813743/unable-to-scrape-by-traversing-br-in-td-using-scrapy-with-css)

html code is following:url: https://azure.microsoft.com/en-in/pricing/details/search/How can I traverse <br> and scrape the data? I want to split td tags into count(br) times and then scrape. I don't want to use xpath. I want to get the result through css.

2018-12-17 10:55:46Z

html code is following:url: https://azure.microsoft.com/en-in/pricing/details/search/How can I traverse <br> and scrape the data? I want to split td tags into count(br) times and then scrape. I don't want to use xpath. I want to get the result through css.{u'(price per 1,000 images)': {u'5M+ images': {u'regional': {u'united-kingdom-south': 0.325, u'europe-north': 0.325, u'brazil-south': 0.325, u'us-west-2': 0.325, u'us-south-central': 0.325, u'central-india': 0.325, u'us-east': 0.325, u'canada-central': 0.325, u'europe-west': 0.325, u'us-east-2': 0.325, u'us-west-central': 0.325, u'asia-pacific-southeast': 0.325, u'australia-east': 0.325}}, u'0-1M images': {u'regional': {u'united-kingdom-south': 0.5, u'europe-north': 0.5, u'brazil-south': 0.5, u'us-west-2': 0.5, u'us-south-central': 0.5, u'central-india': 0.5, u'us-east': 0.5, u'canada-central': 0.5, u'europe-west': 0.5, u'us-east-2': 0.5, u'us-west-central': 0.5, u'asia-pacific-southeast': 0.5, u'australia-east': 0.5}}, u'1M-5M images': {u'regional': {u'united-kingdom-south': 0.4, u'europe-north': 0.4, u'brazil-south': 0.4, u'us-west-2': 0.4, u'us-south-central': 0.4, u'central-india': 0.4, u'us-east': 0.4, u'canada-central': 0.4, u'europe-west': 0.4, u'us-east-2': 0.4, u'us-west-central': 0.4, u'asia-pacific-southeast': 0.4, u'australia-east': 0.4}}}}

How to extract duplicates while using scrapy link extractor?

VishalQuery

[How to extract duplicates while using scrapy link extractor?](https://stackoverflow.com/questions/53820797/how-to-extract-duplicates-while-using-scrapy-link-extractor)

I'm trying to extract multiple product urls present on category pages. For the same i want to get multiple product urls against the category page url but as one product can be present on various category page, the scrapy get it filtered out. How to prevent that as dont filter = True don't work here? Also, is there a way that scrapy do not checks for response.status of a extracted url and just yields it out.

2018-12-17 18:04:31Z

I'm trying to extract multiple product urls present on category pages. For the same i want to get multiple product urls against the category page url but as one product can be present on various category page, the scrapy get it filtered out. How to prevent that as dont filter = True don't work here? Also, is there a way that scrapy do not checks for response.status of a extracted url and just yields it out.Pass unique=False to LinkExtractor.You can extract duplicates by converting your final link list to a set().Something like: set([0,1,1,2,1,3,5]) will result in {0,1,2,3,5}, removing duplicate values.Hope that helps!

Run Python Scrapy script via HTTP request

Akila

[Run Python Scrapy script via HTTP request](https://stackoverflow.com/questions/53741198/run-python-scrapy-script-via-http-request)

I'm looking for an example to run scrapy script via HTTP request. I'm planing to send url as a parameter that i need to crawl, via GET or POST method. How can i do that. 

2018-12-12 10:41:01Z

I'm looking for an example to run scrapy script via HTTP request. I'm planing to send url as a parameter that i need to crawl, via GET or POST method. How can i do that. You should use scrapyd.Link to the GitHub project page.Once you are using scrapyd you can use this api to scedule a crawl. Try something like that.

Scrapy closes suddenly on big list of urls

Sekai

[Scrapy closes suddenly on big list of urls](https://stackoverflow.com/questions/53833501/scrapy-closes-suddenly-on-big-list-of-urls)

Let me describe the flow of my spider: 

First I provide around 300 urls. 

Scrapy starts crawling the first 10 urls (is 10 configurable?) 

Then for each url, there are 2 actions: The Depth for my Scrapy is 20, so if we do some calculations, the total number of pages crawled should be: 300*20*48*50 = 14 400 000 pages in one crawl. 

Is this something Scrapy is capable of? 

My server is 8Gb of RAM Now what happens is that Scrapy gets lost with the first 10 urls and never goes beyond that. Do you guys know why would that happen? 

2018-12-18 12:55:10Z

Let me describe the flow of my spider: 

First I provide around 300 urls. 

Scrapy starts crawling the first 10 urls (is 10 configurable?) 

Then for each url, there are 2 actions: The Depth for my Scrapy is 20, so if we do some calculations, the total number of pages crawled should be: 300*20*48*50 = 14 400 000 pages in one crawl. 

Is this something Scrapy is capable of? 

My server is 8Gb of RAM Now what happens is that Scrapy gets lost with the first 10 urls and never goes beyond that. Do you guys know why would that happen? 

Organizing csv export with scrapy

Manuel

[Organizing csv export with scrapy](https://stackoverflow.com/questions/53682525/organizing-csv-export-with-scrapy)

For exporting my data to a CSV file, I'm currently using (mainly because I never understood pipelines that well):This custom settings are inside my spider.Right now, I'm scraping different categories of items, for example, laptops and cell phones.Problem is that, when I go check out my data, things are not organized, maybe a laptop appears, then a cell phone, then 2 laptops, cellphone and so on.I'm currently going into different categories this wayIs it there a way for the data to be more organized (2 files would be even better), or an easy pipeline solution.

2018-12-08 12:27:44Z

For exporting my data to a CSV file, I'm currently using (mainly because I never understood pipelines that well):This custom settings are inside my spider.Right now, I'm scraping different categories of items, for example, laptops and cell phones.Problem is that, when I go check out my data, things are not organized, maybe a laptop appears, then a cell phone, then 2 laptops, cellphone and so on.I'm currently going into different categories this wayIs it there a way for the data to be more organized (2 files would be even better), or an easy pipeline solution.There is no settings-only way to achieve what you want.That said, exporting to multiple files from a custom pipeline is pretty straight-forward:Don't forget to activate your pipeline :)

Downloading files with ItemLoaders() in Scrapy

GKV

[Downloading files with ItemLoaders() in Scrapy](https://stackoverflow.com/questions/53683748/downloading-files-with-itemloaders-in-scrapy)

I created a crawl spider to download files. However the spider downloaded only the urls of the files and not the files themselves. I uploaded a question here Scrapy crawl spider does not download files? . While the the basic yield spider kindly suggested in the answers works perfectly, when I attempt to download files with items or item loaders the spider does not work! The original question does not include the items.py. So there it is:ITEMS EDIT: added original code

EDIT: further correctionsSPIDERSETTINGSPIPELINES

2018-12-08 15:00:14Z

I created a crawl spider to download files. However the spider downloaded only the urls of the files and not the files themselves. I uploaded a question here Scrapy crawl spider does not download files? . While the the basic yield spider kindly suggested in the answers works perfectly, when I attempt to download files with items or item loaders the spider does not work! The original question does not include the items.py. So there it is:ITEMS EDIT: added original code

EDIT: further correctionsSPIDERSETTINGSPIPELINESIt seems to me that using items and/or item loaders has nothing to do with your problem.The only problems I see are in your settings file:If I correct all of those issues, the file download works as expected.

scrapy accessing inner URLs

mos

[scrapy accessing inner URLs](https://stackoverflow.com/questions/53646859/scrapy-accessing-inner-urls)

I have a url in start_urls array as below:now in shopParser() inside for loop I have different link and I need to have different response than the original response from start_urls, how I can achive that ?

2018-12-06 07:53:39Z

I have a url in start_urls array as below:now in shopParser() inside for loop I have different link and I need to have different response than the original response from start_urls, how I can achive that ?You need to call requests to new pages, otherwise you will not get any new html. Try something like:These new requests will also be parsed by parse function. Or you can set another callback, if needed.

how to keep redirection history of duplicates

Raphael

[how to keep redirection history of duplicates](https://stackoverflow.com/questions/53692473/how-to-keep-redirection-history-of-duplicates)

scrapys duplication filter ignores already seen urls/requests. So far, so good.The ProblemEven if a request is dropped I still want to keep the redirection history. Example:In this case request 2 is dropped without letting me know that it is a 'hidden' duplicate of request 1.AttemptsI already tried to catch the signal request_dropped. This works but I don't see a possibility to sent an item to the pipeline from the handler.Best regards and thanks for your help :)Raphael

2018-12-09 12:49:28Z

scrapys duplication filter ignores already seen urls/requests. So far, so good.The ProblemEven if a request is dropped I still want to keep the redirection history. Example:In this case request 2 is dropped without letting me know that it is a 'hidden' duplicate of request 1.AttemptsI already tried to catch the signal request_dropped. This works but I don't see a possibility to sent an item to the pipeline from the handler.Best regards and thanks for your help :)RaphaelYou are probably looking for DUPEFILTER_DEBUGSet it to True in settings.py file and you will see all URLs that were ignored because of being duplicateI figured out a way to handle those 'hidden' redirects:I hope this might help you if you stumble upon the same problem.

Scrapy: ItemLoader, can someone explain to me this error?

Stefan

[Scrapy: ItemLoader, can someone explain to me this error?](https://stackoverflow.com/questions/53686631/scrapy-itemloader-can-someone-explain-to-me-this-error)

I have a question, I've started using Scrapy over the BeautifulSoup  and I new to Scrapy so this error threw me away, I've tried to fix it bit with no success.I wanted to use Item Loader to yield scrapped items, but I get error, I do not understand what is the problem.Can someone explain to me what am I doing wrong.This is my spider code, quotes.py :This is my items.py file:This the error that I get when I try to run it:

2018-12-08 20:24:17Z

I have a question, I've started using Scrapy over the BeautifulSoup  and I new to Scrapy so this error threw me away, I've tried to fix it bit with no success.I wanted to use Item Loader to yield scrapped items, but I get error, I do not understand what is the problem.Can someone explain to me what am I doing wrong.This is my spider code, quotes.py :This is my items.py file:This the error that I get when I try to run it:You have no fields Text, Author and Tags in QuotesSpiderItem, their names are text, author and tags. Letter case matters.

Passing scrapy resutls to mysql databasee

user1991713

[Passing scrapy resutls to mysql databasee](https://stackoverflow.com/questions/53649704/passing-scrapy-resutls-to-mysql-databasee)

I'm trying to build a small scraper to sort some news subject as a hobby project (i'm not a professional developer or tech guy and i'm a beginner with OOP and Python, i have some experience with php and arduino programming language). I managed to understand scrapy and partialy mysql pipe. If i replace the item['titlu'] and item['articol'] with a simple string, database will be populated with that. I had search and read a lot of info but i'm totally unable to solve my problem.

I suppose that  item['titlu'] and item['articol'] are some type of array or something that mysql does not like. i will post the code  and errors for help.

The commented lines of code are some of my trials to solve the problem

mysql database table is:I have also tryed to change titlu and articol text type to varchar. In purpose i have let the table example like this(with one filde text and other varchar) for you to know what settings i have tryed.  Thanks:spider:items.py:pipelines.py:The erors are as follows:This one is when i use self.cursor.executemany(query)and this is when I use self.cursor.execute(query)

I get this:

2018-12-06 10:48:15Z

I'm trying to build a small scraper to sort some news subject as a hobby project (i'm not a professional developer or tech guy and i'm a beginner with OOP and Python, i have some experience with php and arduino programming language). I managed to understand scrapy and partialy mysql pipe. If i replace the item['titlu'] and item['articol'] with a simple string, database will be populated with that. I had search and read a lot of info but i'm totally unable to solve my problem.

I suppose that  item['titlu'] and item['articol'] are some type of array or something that mysql does not like. i will post the code  and errors for help.

The commented lines of code are some of my trials to solve the problem

mysql database table is:I have also tryed to change titlu and articol text type to varchar. In purpose i have let the table example like this(with one filde text and other varchar) for you to know what settings i have tryed.  Thanks:spider:items.py:pipelines.py:The erors are as follows:This one is when i use self.cursor.executemany(query)and this is when I use self.cursor.execute(query)

I get this:Correct code in process_item is You just have to write %s for values and then pass them as a list (array) in execute methodMoreover, you should learn about string formattingYou were doing That entire is a string, you are not passing values to your string at allhere is corrected statement

NameError: name is not defined Scrapy

dcarlo56ave

[NameError: name is not defined Scrapy](https://stackoverflow.com/questions/53605667/nameerror-name-is-not-defined-scrapy)

I am working on a web crawler using scrapy 1.5 and am getting an error that row_list is not defined when it is. I believe this is a global issue but am not sure what would be the proper way to correct this. The reason I am keeping the code like this is because I need sublists with all the details in a row. So I can use the first element in the list as a key and join everything else after the first element into 1 element stored in the second position. I have to yield a dictionary and need it in this format. any help would be great.

2018-12-04 04:24:43Z

I am working on a web crawler using scrapy 1.5 and am getting an error that row_list is not defined when it is. I believe this is a global issue but am not sure what would be the proper way to correct this. The reason I am keeping the code like this is because I need sublists with all the details in a row. So I can use the first element in the list as a key and join everything else after the first element into 1 element stored in the second position. I have to yield a dictionary and need it in this format. any help would be great.It seems that the issue is that you're calling del row_list and, after that, you try to update final_dict with that name.Check out this example:When you call del row_list, the row_list name is gonna be removed from the namespace, thus you can't access it.More about the del statement: https://docs.python.org/3/reference/simple_stmts.html#del

How to close writing of json file after crawling with scrapy?

merlin

[How to close writing of json file after crawling with scrapy?](https://stackoverflow.com/questions/53606152/how-to-close-writing-of-json-file-after-crawling-with-scrapy)

I am crawling with scrapy 1.5.1 by calling it from the CLI: My pipeline is pretty simple where I process the item and after processing I want to pull it into a zip archive within the close_spider method:cleanup method:The traceback after using self.file.close():Withou file.close there is no traceback error and it apears OK at first, but the json file gets truncated.End of decompressed file from zip archive with json file output from scrapy:json file output by scrapy:How do I close the writing of the file in order to zip it?

2018-12-04 05:21:49Z

I am crawling with scrapy 1.5.1 by calling it from the CLI: My pipeline is pretty simple where I process the item and after processing I want to pull it into a zip archive within the close_spider method:cleanup method:The traceback after using self.file.close():Withou file.close there is no traceback error and it apears OK at first, but the json file gets truncated.End of decompressed file from zip archive with json file output from scrapy:json file output by scrapy:How do I close the writing of the file in order to zip it?Try removing this line self.exporter.finish_exporting().Your object does not have a exporter attribute.

Scrapy change / update public IP via Proxy

Nikhil Redij

[Scrapy change / update public IP via Proxy](https://stackoverflow.com/questions/53610617/scrapy-change-update-public-ip-via-proxy)

I am using Scrapy for crawling Google and I want to change my IP from code. I am getting same public IP as my local from output even though proxy in meta of response is getting changed. If I go to that VM and get a response from that site then it shows me VM's IP which is I am using in request.meta['proxy'] = ip but from code it only shows Local Public IPThis is my code.middleware.pysettings.pyspider1.pyOutput:

2018-12-04 10:18:05Z

I am using Scrapy for crawling Google and I want to change my IP from code. I am getting same public IP as my local from output even though proxy in meta of response is getting changed. If I go to that VM and get a response from that site then it shows me VM's IP which is I am using in request.meta['proxy'] = ip but from code it only shows Local Public IPThis is my code.middleware.pysettings.pyspider1.pyOutput:As per my understanding, the proxy ip needs to be ip of a proxy server as in a proxy server should be reachable at the ip provided by you. You can't simply assign any random ip to any request. If you want to rotate IP that is a different thing altogether. Also just in case mention scheme(http, https) and port. Not sure whether scrapy falls back to any default if scheme and port is not mentioned. Also, please see the documentation.

How to zip and clean up downloaded files after crawling with scrapy

merlin

[How to zip and clean up downloaded files after crawling with scrapy](https://stackoverflow.com/questions/53579684/how-to-zip-and-clean-up-downloaded-files-after-crawling-with-scrapy)

I have successfully created a crawler with scrapy which downloads to CSV and pulls images into images/full folder.Now I want to clean it up after the crawl by pulling the files into a zip archive and removing the "full" folder and the CSV as well.This is how I do it:parser_attributes.py:test.py:Traceback:

2018-12-02 11:16:25Z

I have successfully created a crawler with scrapy which downloads to CSV and pulls images into images/full folder.Now I want to clean it up after the crawl by pulling the files into a zip archive and removing the "full" folder and the CSV as well.This is how I do it:parser_attributes.py:test.py:Traceback:There are two main ways of executing a scrapy spider:Your code tries to mix the two ways, which is not going to work.There are two ways I can think of to do what you want:The former is probably simpler, but the latter avoids the need for zipping and deleting your files after the scraping process completes.You just need to use the correct absolute import path.

This would probably achieve the same, but in a much less horrible way:

Is there a way to disallow domains in scrapy?

Scrape Junkie

[Is there a way to disallow domains in scrapy?](https://stackoverflow.com/questions/53577215/is-there-a-way-to-disallow-domains-in-scrapy)

Is there a way to pass a list to scrapy to tell it what domains it can't visit?Kind of like the opposite of allowed_domains = ['google.com']I am trying to do broader crawls but am getting hung up in big domains with thousands of pages containing irrelevant information.My target is too broad to list everything in 'allowed_domains', I just want to exclude a list of sites that I choose.Thanks

2018-12-02 03:50:38Z

Is there a way to pass a list to scrapy to tell it what domains it can't visit?Kind of like the opposite of allowed_domains = ['google.com']I am trying to do broader crawls but am getting hung up in big domains with thousands of pages containing irrelevant information.My target is too broad to list everything in 'allowed_domains', I just want to exclude a list of sites that I choose.ThanksThere is no built-in way to do what you want.The easiest way to achieve this would probably be replacing scrapy's OffsiteMiddleware with a custom one.

Something as simple as overwriting the should_follow() method might be all you need.

scrapy spider scraping data from link randomly why?

lauevrar77

[scrapy spider scraping data from link randomly why?](https://stackoverflow.com/questions/53580168/scrapy-spider-scraping-data-from-link-randomly-why)

First i have grabed all the coin link from the website and requested to those link.

But scrapy do'nt requesting serially from the link list.after requesting to thos link scraping data successfully but when saving to csv file it making a blank row every time after one succesfull scraped item.Result Screenshot

I am expecting that it will request serially from the  link list and it will not make any blank row.how can i do that?I am using python 3.6 and scrapy version 1.5.1My code:

2018-12-02 12:20:13Z

First i have grabed all the coin link from the website and requested to those link.

But scrapy do'nt requesting serially from the link list.after requesting to thos link scraping data successfully but when saving to csv file it making a blank row every time after one succesfull scraped item.Result Screenshot

I am expecting that it will request serially from the  link list and it will not make any blank row.how can i do that?I am using python 3.6 and scrapy version 1.5.1My code:I think scrapy is visiting pages in a multi-threaded (producer/consumer) fashion. This can explain the non-sequential aspect of your result.

To verify this hypothesis, you could change your config to use a single thread. For the blank link, are you sure any of your name or link variable contains a \n ? Scrapy is an asynchronous framework - multiple requests are executed concurrently and the responses are parsed as they are received.The only way to reliably control which responses are parsed first is to turn this feature off, e.g. by setting CONCURRENT_REQUESTS to 1.

This would make your spider less efficient though, and this kind of control of the parse order is rarely necessary, so I would avoid it if possible.The extra newlines in csv exports on windows are a known issue, and will be fixed in the next scrapy release.

How to i can generate list from p tag ?

Gallaecio

[How to i can generate list from p tag ?](https://stackoverflow.com/questions/53540662/how-to-i-can-generate-list-from-p-tag)

Please check the site :there are all in p tagi wanna separating with each element but i can found effective way

2018-11-29 13:56:29Z

Please check the site :there are all in p tagi wanna separating with each element but i can found effective wayMaybe if you use xpath 2.0, you can use regex in selector like //p[matches(text(),'[\w\s]+\([\w+]\)','i')]. 

Or try to iterate like this (not exact code, just example):

When scrapy finishes I want to create a dataframe from all data crawled

PyRar

[When scrapy finishes I want to create a dataframe from all data crawled](https://stackoverflow.com/questions/53542201/when-scrapy-finishes-i-want-to-create-a-dataframe-from-all-data-crawled)

I'm trying to scrape some websites. I would want to store all data scraped in a final dataframe named Tabel_Final. I stored each attribute in a different list and then I'm trying to concatenate the lists in the final dataframe and then output it as csv to verify the results. I have a different method in the code, where I append all data scraped directly in a csv, but I would need that dataframe so much :( Any help, please?This is my code:

2018-11-29 15:19:43Z

I'm trying to scrape some websites. I would want to store all data scraped in a final dataframe named Tabel_Final. I stored each attribute in a different list and then I'm trying to concatenate the lists in the final dataframe and then output it as csv to verify the results. I have a different method in the code, where I append all data scraped directly in a csv, but I would need that dataframe so much :( Any help, please?This is my code:I recommend that you split your codebase:Otherwise, you should learn how to run Scrapy from a script and refactor your code accordingly. I have a pet project that follows this approach:

'unicode' object has no att 'xpath' while trying to iterate over a xpath response in scrapy

Bernardo Gomes de Melo

['unicode' object has no att 'xpath' while trying to iterate over a xpath response in scrapy](https://stackoverflow.com/questions/53484513/unicode-object-has-no-att-xpath-while-trying-to-iterate-over-a-xpath-respons)

I'm trying to run my scrapy generated code to scrape a page in the web + store data in the item scrapy function, but every time I run the code, I end up with:I've tried erasing the extract() after each item xpath, but that's not working.Here's the code:

2018-11-26 15:38:16Z

I'm trying to run my scrapy generated code to scrape a page in the web + store data in the item scrapy function, but every time I run the code, I end up with:I've tried erasing the extract() after each item xpath, but that's not working.Here's the code:

I upgraded pyOpenSSL but it is still 0.13.1 version. Unable to upgrade to 18.0.0

V. Foy

[I upgraded pyOpenSSL but it is still 0.13.1 version. Unable to upgrade to 18.0.0](https://stackoverflow.com/questions/53492827/i-upgraded-pyopenssl-but-it-is-still-0-13-1-version-unable-to-upgrade-to-18-0-0)

I am trying to get pyOpenSSL 18.0.0 but it doesn't work. Here are my commands:I get this:Looks like pyOpenSSL is already installed. But then I check:And I get:

2018-11-27 04:34:06Z

I am trying to get pyOpenSSL 18.0.0 but it doesn't work. Here are my commands:I get this:Looks like pyOpenSSL is already installed. But then I check:And I get:The sudo command might be making pip install that version of pyopenssl to the site-packages of the root user. Simply running pip install pyOpenSSL==18.0.0 (outside of a virtual environment) should install it in the site-packages of the current user.As an aside, you should consider using virtual environments for better dependency management of your python project. For that, please reference {this answer}.

how to stop scrapy 301 redirects and stop parsing the redirected page

merlin

[how to stop scrapy 301 redirects and stop parsing the redirected page](https://stackoverflow.com/questions/53488198/how-to-stop-scrapy-301-redirects-and-stop-parsing-the-redirected-page)

I am trying to crawl a page that redirects scrapy for whatever reason via 301 to the english version and then the site gets parsed which it should not as the rules clearly exclude the URL.While searching for a solution on how to stop any redirect I came upon the following code:Unfortunatelly this does not have any effect. My spider class looks like this:Is this the right place to configure the redirect and why is scrapy parsing the input while the first URL rule will not execute?

2018-11-26 20:03:24Z

I am trying to crawl a page that redirects scrapy for whatever reason via 301 to the english version and then the site gets parsed which it should not as the rules clearly exclude the URL.While searching for a solution on how to stop any redirect I came upon the following code:Unfortunatelly this does not have any effect. My spider class looks like this:Is this the right place to configure the redirect and why is scrapy parsing the input while the first URL rule will not execute?Why what you tried doesn't work:How to disable redirects:The easiest way to disable redirects globally is to set the REDIRECT_ENABLED setting to False.

scrapy insert into database

user3381431

[scrapy insert into database](https://stackoverflow.com/questions/53492263/scrapy-insert-into-database)

I have a working script using scrapy that inserts scraped items into a database using a pipelines class. However this seems to slow down the scrape considerably. I'm using the process item method to insert each scraped item into the database as it is scraped. Would it be faster to output the scraped items into a csv file and then use a stored procedure to insert the data into the database?

2018-11-27 03:22:54Z

I have a working script using scrapy that inserts scraped items into a database using a pipelines class. However this seems to slow down the scrape considerably. I'm using the process item method to insert each scraped item into the database as it is scraped. Would it be faster to output the scraped items into a csv file and then use a stored procedure to insert the data into the database?It looks like you're trying to make an insert per data point. This is indeed very slow!! You should consider bulk insertions after you've collected all of your data, or at least insert in chunks.Use something like thisInstead of thisSee this answer for quite a good breakdown of performance in SQL server

How to select all href attribute in a html tag contain a common class. in Scrapy

Hossain Misho

[How to select all href attribute in a html tag contain a common class. in Scrapy](https://stackoverflow.com/questions/53491517/how-to-select-all-href-attribute-in-a-html-tag-contain-a-common-class-in-scrapy)

I want to select all the href contains in  tag ...

here is my html codeI used response.css('a.aok-block::attr(href)').extract()



but the result is: [ ]

2018-11-27 01:34:09Z

I want to select all the href contains in  tag ...

here is my html codeI used response.css('a.aok-block::attr(href)').extract()



but the result is: [ ]It is recommended that you use xpath expressions. For your example response.xpath("//a[class='aok-block aok-nowrap']").get_attribute('href')Add to the answer johnnydoeWill be:If you want get only href then you must find upper tag...like this:Will be:

Scrapy runs but doesnt output anything

Khrystyna Kosenko

[Scrapy runs but doesnt output anything](https://stackoverflow.com/questions/53445707/scrapy-runs-but-doesnt-output-anything)

I am new to scrapy, 

I am trying to get a product information from sephora.

Ideally, I want to get through all skincare, though I am having difficulties on the first stage.

I am trying to figure why this does not yiled any information?

Separately the response arguments in scrapy shell get me the data I need.The response I get is the following. But the output file is still empty.

2018-11-23 11:18:10Z

I am new to scrapy, 

I am trying to get a product information from sephora.

Ideally, I want to get through all skincare, though I am having difficulties on the first stage.

I am trying to figure why this does not yiled any information?

Separately the response arguments in scrapy shell get me the data I need.The response I get is the following. But the output file is still empty.You're missing a dot in your css selector for title.This causes title to be an empty list, and zip() with an empty list will give you nothing.zip() works on given iterables, if any iterable is empty list it will return empty. You haven't added . in title selector css-r4ddnb::text which cause empty list.

How to deal with redirects to a bookmark within a page in Scrapy (911 error)

theresearchant

[How to deal with redirects to a bookmark within a page in Scrapy (911 error)](https://stackoverflow.com/questions/53450765/how-to-deal-with-redirects-to-a-bookmark-within-a-page-in-scrapy-911-error)

I am very new to programming, so apologies if this is a rookie issue. I am a researcher, and I've been building spiders to allow me to crawl specific search results of IGN, the gaming forum. The first spider collects each entry in the search results, along with URLs, and then the second spider crawls each of those URLs for the content.The problem is that IGN redirects URLs associated with a specific post to a new URL that incorporates a #bookmark at the end of the address. This allows the visitor to the page to jump directly down to the post in question, but I want my spider to crawl over the entire thread. In addition, my spider ends up with a (911) error after the redirect and returns no data. The only data retrieved is from any search results that linked directly to a thread rather than a post.I am absolutely stumped and confused, so any help would amazing! Both spiders are attached below.Spider 1:Spider 2:

2018-11-23 17:21:57Z

I am very new to programming, so apologies if this is a rookie issue. I am a researcher, and I've been building spiders to allow me to crawl specific search results of IGN, the gaming forum. The first spider collects each entry in the search results, along with URLs, and then the second spider crawls each of those URLs for the content.The problem is that IGN redirects URLs associated with a specific post to a new URL that incorporates a #bookmark at the end of the address. This allows the visitor to the page to jump directly down to the post in question, but I want my spider to crawl over the entire thread. In addition, my spider ends up with a (911) error after the redirect and returns no data. The only data retrieved is from any search results that linked directly to a thread rather than a post.I am absolutely stumped and confused, so any help would amazing! Both spiders are attached below.Spider 1:Spider 2:

Scrapy Debug crawled 200 and nothing return

Serena

[Scrapy Debug crawled 200 and nothing return](https://stackoverflow.com/questions/53400090/scrapy-debug-crawled-200-and-nothing-return)

I am working on a crawling project and try to get each endorsement link of a band.My code is as follows:It returned nothing. However, if I put each URL of a band in the start_url, it works well. But it will be hard for me to put all the URLs I want manually in the start_url field since I am even not sure how many there are...The log is shown:Anyone can help? Thanks in advance!

2018-11-20 19:20:41Z

I am working on a crawling project and try to get each endorsement link of a band.My code is as follows:It returned nothing. However, if I put each URL of a band in the start_url, it works well. But it will be hard for me to put all the URLs I want manually in the start_url field since I am even not sure how many there are...The log is shown:Anyone can help? Thanks in advance!Your restrict xpath expression looks wrong.You could use the allow parameter instead, this is much easier:This is the output log:If you really want to use xpath, then try removing [*].The xpath that you commented looks correct, but the callback is wrong, you cannot use the parse callback with a CrawlSpider.

How I can clear scrapy jobs list?

kolas

[How I can clear scrapy jobs list?](https://stackoverflow.com/questions/53398871/how-i-can-clear-scrapy-jobs-list)

How I can clear scrapy jobs list? When I start any spider I have a lot jobs with specific spider and I know how can I kill all them ? After reading documentation I have done next code, which I run in a loop:but looks like that it doesn't works. How can I kill all jobs because even if I have finished manually scrapy's process in the next start all jobs come back.

Find this https://github.com/DormyMo/SpiderKeeper for project management. Does anybody know how to include existing project ?

2018-11-20 17:59:11Z

How I can clear scrapy jobs list? When I start any spider I have a lot jobs with specific spider and I know how can I kill all them ? After reading documentation I have done next code, which I run in a loop:but looks like that it doesn't works. How can I kill all jobs because even if I have finished manually scrapy's process in the next start all jobs come back.

Find this https://github.com/DormyMo/SpiderKeeper for project management. Does anybody know how to include existing project ?So, I do not know what is wrong with my first example, but I fixed problem with this:

How to retry other request in specific pipeline when connection false

pakachu

[How to retry other request in specific pipeline when connection false](https://stackoverflow.com/questions/53293857/how-to-retry-other-request-in-specific-pipeline-when-connection-false)

Like This . I have several pipelines and I want to request other image url when request timeout failure.I saw scrapy But It seems like for all requests. I want to Specify Only My ImagePipelines.

2018-11-14 05:45:11Z

Like This . I have several pipelines and I want to request other image url when request timeout failure.I saw scrapy But It seems like for all requests. I want to Specify Only My ImagePipelines.Look at the documentation for the RetryMiddleware.The setting RETRY_ENABLED is True by default, you can change it to False to disable the middleware.

scrapy ERROR: Spider error processing issue

Vinod kumar

[scrapy ERROR: Spider error processing issue](https://stackoverflow.com/questions/53298735/scrapy-error-spider-error-processing-issue)

I'm very new to scrapy, while am running my code, am getting this error. My CodeOnce I tried to run this code : scrapy crawl sec_gov am getting this error.Can anyone help me with this ? Thanks in advance

2018-11-14 11:05:00Z

I'm very new to scrapy, while am running my code, am getting this error. My CodeOnce I tried to run this code : scrapy crawl sec_gov am getting this error.Can anyone help me with this ? Thanks in advanceYour code should not run at all. There are several things to fix in order for your script to run. Where have you found this self.parse_page and what is it doing within your script? Your script is badly indented. I've fixed the script which is now able to track each url from it's landing page connected to concerning links to the documentation in it's inner page. Try this to get the content.

How to use scrapy to scrape SO question and store in MongoDB?

MikiBelavista

[How to use scrapy to scrape SO question and store in MongoDB?](https://stackoverflow.com/questions/53260904/how-to-use-scrapy-to-scrape-so-question-and-store-in-mongodb)

This web scraping with Scrapy is a little bit outdated link

It seems that Selector XPath has been changed.When I copy it,I haveBut code from above linkHow to constructor generator with new Selector?This isSpring data @transactional not rolling back with SQL Server and after runtimeexceptionthe SO question we are scraping as an example.Matthew Daniels suggestions

2018-11-12 11:09:24Z

This web scraping with Scrapy is a little bit outdated link

It seems that Selector XPath has been changed.When I copy it,I haveBut code from above linkHow to constructor generator with new Selector?This isSpring data @transactional not rolling back with SQL Server and after runtimeexceptionthe SO question we are scraping as an example.Matthew Daniels suggestions

python issue to join relative url to absolute url for img

Gabriel Alejandro

[python issue to join relative url to absolute url for img](https://stackoverflow.com/questions/53233318/python-issue-to-join-relative-url-to-absolute-url-for-img)

I'm facing the following issues with my current code to make it work. I just concatenate the URL but its not working:Current relative path (this is what I get with normal response.xpath crawl):This is my current code:

2018-11-09 21:11:15Z

I'm facing the following issues with my current code to make it work. I just concatenate the URL but its not working:Current relative path (this is what I get with normal response.xpath crawl):This is my current code:try scrapy documentation

IOError on Scrapy Images Pipeline

notGeek

[IOError on Scrapy Images Pipeline](https://stackoverflow.com/questions/53235252/ioerror-on-scrapy-images-pipeline)

I'm using Images Pipeline from Scrapy and for some images I'm getting this error:The images are available on the server (without redirects) and I don't find any difference between the images that work and the ones which doesn't. Any idea of what I'm missing?

2018-11-10 01:28:11Z

I'm using Images Pipeline from Scrapy and for some images I'm getting this error:The images are available on the server (without redirects) and I don't find any difference between the images that work and the ones which doesn't. Any idea of what I'm missing?This seems to be a known issue. Upgrading Pillow dependency (pip install Pillow --upgrade) fixed it.

Scrapy: Project can't be created

Helquin

[Scrapy: Project can't be created](https://stackoverflow.com/questions/53232086/scrapy-project-cant-be-created)

I'm currently following the Scrapy tutorial and am at the step where there's a creation of a project, however this error is produced everytime I run the command scrapy startproject tutorial.Error:I suspected that the issue is in my installation of Scrapy, so I tried reinstalling Scrapy(first via pip then conda) but it still doesn't fix the problem. I then tried to install the packages that Scrapy needed(one by one) but it's still not fixing anything.How do I go about fixing this?

2018-11-09 19:22:38Z

I'm currently following the Scrapy tutorial and am at the step where there's a creation of a project, however this error is produced everytime I run the command scrapy startproject tutorial.Error:I suspected that the issue is in my installation of Scrapy, so I tried reinstalling Scrapy(first via pip then conda) but it still doesn't fix the problem. I then tried to install the packages that Scrapy needed(one by one) but it's still not fixing anything.How do I go about fixing this?Can you try to install Win64OpenSSL_Light-1_0_2h and reinstall cryptography?See this existing answer.

How to extract text which lies after <strong> tag in element

Harald

[How to extract text which lies after <strong> tag in element](https://stackoverflow.com/questions/53171113/how-to-extract-text-which-lies-after-strong-tag-in-element)

Trying to extract text from a element which looks like this:When I try to extract "Rest_of_text" using Scrapy shell withIt gives me nothing. Do I have to use some special command to get to text that lies after a <strong> tag inside an element?

2018-11-06 11:36:13Z

Trying to extract text from a element which looks like this:When I try to extract "Rest_of_text" using Scrapy shell withIt gives me nothing. Do I have to use some special command to get to text that lies after a <strong> tag inside an element?Given the text you provided, the command you've mentioned should've returned the following:The problem may occur if there is whitespace before strong tag, e.g.:In this case, if you execute the same command, you'll get this:But in case if there's nothing after the strong tag, you'll get this:The best way to handle all these cases I know is to do the following:So in the text you've provided, before_strong will be equal to '' and after_strong will be equal to '"Rest_of_text"', which seems to be what you want to get.Only for "Rest_of_text" you can use response.xpath('//div/strong/following-sibling::text()').get() 

Organize items through Scrapy Pipeline

stasdeep

[Organize items through Scrapy Pipeline](https://stackoverflow.com/questions/53171757/organize-items-through-scrapy-pipeline)

I had single spiders which worked fine. I could get simple export CSV with command line, and organize output like that :Output example :Now however I'm trying to run several spiders at the same time through a simple file (this part is ok), and parsing result through items and pipeline. So here is the code for parse function :And... The result with CSV item exporter :So how do organize CSV output with the pipeline ? I would like to have all fields for one item on a single line... For instance :I searched but didn't find simple examples about how to reorganize item output ! And sorry if this is a silly question :) i'm learning python at the same time that i'm learning scrapy !

2018-11-06 12:18:12Z

I had single spiders which worked fine. I could get simple export CSV with command line, and organize output like that :Output example :Now however I'm trying to run several spiders at the same time through a simple file (this part is ok), and parsing result through items and pipeline. So here is the code for parse function :And... The result with CSV item exporter :So how do organize CSV output with the pipeline ? I would like to have all fields for one item on a single line... For instance :I searched but didn't find simple examples about how to reorganize item output ! And sorry if this is a silly question :) i'm learning python at the same time that i'm learning scrapy !I think you can achieve what you want with a simple loop:

Scrapy get result in shell but not in script

Valdir Stumm Junior

[Scrapy get result in shell but not in script](https://stackoverflow.com/questions/53173482/scrapy-get-result-in-shell-but-not-in-script)

one topic again ^^ Based on recommendations here, I've implemented my bot the following and tested it all in shell :Problem is with price field.For price in shell, I have for instance :Output from script for same product :Do you know why don't I get result for prices with script ?

2018-11-06 14:03:15Z

one topic again ^^ Based on recommendations here, I've implemented my bot the following and tested it all in shell :Problem is with price field.For price in shell, I have for instance :Output from script for same product :Do you know why don't I get result for prices with script ?product_price is a string, given that you are joining the results of the selector in:Then, when you use zip, you'll be splitting that string in parts, thus you'll have the \n for the first item, as it's probably the first character in product_price.Check this example:Output:

Running scrapy splash with proxies

陈飞宇

[Running scrapy splash with proxies](https://stackoverflow.com/questions/53132597/running-scrapy-splash-with-proxies)

I am using proxy in scrapy splash, but I get 502 proxy all the time, it troubles me several days.my downloadmiddleware:my requests:my settings:}I am sure that all the setting about proxy is right,and the proxy is valid,for it will be successful with out splash

2018-11-03 15:10:10Z

I am using proxy in scrapy splash, but I get 502 proxy all the time, it troubles me several days.my downloadmiddleware:my requests:my settings:}I am sure that all the setting about proxy is right,and the proxy is valid,for it will be successful with out splashAccording to your code, you're sending the proxy authentication headers to the Splash server:The Splash server would simply ignore the proxy authentication header you send, and thus the proxy server would reject your request due to unsuccessful authentication.The right thing to do is to have Splash send the proxy authentication header:So you'll need to remove this line:and properly configure the proxy info:See also: API reference of Splash (look for the proxy argument)

How to run faster several version of one single spider at one time with Scrapy?

AvyWam

[How to run faster several version of one single spider at one time with Scrapy?](https://stackoverflow.com/questions/53143186/how-to-run-faster-several-version-of-one-single-spider-at-one-time-with-scrapy)

I found a solution to run several version of one single spider in this post.So I run this several versions at one time with several different windows commands:I used 7 versions in once.My issue is when I run many spiders like this, I see with my own eyes the process became so slow. And this is visible too in the log with 'finish_time' and 'start_time'. It takes about 1 hour for each command if I use two versions at one time. It takes more than seven hours, very more, for each command if I use seven versions at one time.In the settings.py I wrote I am sure it is not enough because I see the results, but am I wrong to think these configurations are the ways to increase the velocity? And how could I do to make my crawling to be faster?PS: I use Tor with Vidalia and Polipo as proxy. So I know my connexion can't be very fast, that is the cons of the anonymity. But the real issue is when I run several at one time. And my CPU is far away of its max capacity.Scrapy version: 1.5.0, Python version: 2.7.9

2018-11-04 17:01:49Z

I found a solution to run several version of one single spider in this post.So I run this several versions at one time with several different windows commands:I used 7 versions in once.My issue is when I run many spiders like this, I see with my own eyes the process became so slow. And this is visible too in the log with 'finish_time' and 'start_time'. It takes about 1 hour for each command if I use two versions at one time. It takes more than seven hours, very more, for each command if I use seven versions at one time.In the settings.py I wrote I am sure it is not enough because I see the results, but am I wrong to think these configurations are the ways to increase the velocity? And how could I do to make my crawling to be faster?PS: I use Tor with Vidalia and Polipo as proxy. So I know my connexion can't be very fast, that is the cons of the anonymity. But the real issue is when I run several at one time. And my CPU is far away of its max capacity.Scrapy version: 1.5.0, Python version: 2.7.9

scrapy get element content if another element already exists

houssem gharsalli

[scrapy get element content if another element already exists](https://stackoverflow.com/questions/53133501/scrapy-get-element-content-if-another-element-already-exists)

I have this html script: I would like to get all span's value but sometimes I have 3 li and sometimes only 2, so I get wrong values.Is there a way to catch the span value based on the precedent h4 tag name? so for exemple for the seconde tag:

   if h4 == "Nombre de portes: " then give me span value below.

2018-11-03 16:54:15Z

I have this html script: I would like to get all span's value but sometimes I have 3 li and sometimes only 2, so I get wrong values.Is there a way to catch the span value based on the precedent h4 tag name? so for exemple for the seconde tag:

   if h4 == "Nombre de portes: " then give me span value below.Try to use below XPath:to get span child of li if li also contains header with text 'Nombre de portes :'You can also tryto select exactly the span node that has preceding sibling h4 with required text content, but IMO first option should be enoughYes.x.select('//li[h4/text()=="Nombre de portes :"]/span')

Scrapy spider won't jump to next page

Harald

[Scrapy spider won't jump to next page](https://stackoverflow.com/questions/53132759/scrapy-spider-wont-jump-to-next-page)

I'm building a scaper with Scrapy for swedish ecommerce site Blocket.se.

It's scraping the first page as it should, but it won't jump the next.The command for next urloutputs an "incomplete" link when I try it in Scrapy shell:Does it have to be a "full" link to work?:Starting-url: https://www.blocket.se/stockholm?q=cykel&cg=0&w=1&st=s&c=&ca=11&is=1&l=0&md=thFull code:

2018-11-03 15:28:41Z

I'm building a scaper with Scrapy for swedish ecommerce site Blocket.se.

It's scraping the first page as it should, but it won't jump the next.The command for next urloutputs an "incomplete" link when I try it in Scrapy shell:Does it have to be a "full" link to work?:Starting-url: https://www.blocket.se/stockholm?q=cykel&cg=0&w=1&st=s&c=&ca=11&is=1&l=0&md=thFull code:Yes, scrapy needs the full URL, usually. But you can keep using urljoin() or using the response.follow() method:More about this in Scrapy Tutorial.

get the latitude and longitude from url

Elizabeth79

[get the latitude and longitude from url](https://stackoverflow.com/questions/53084130/get-the-latitude-and-longitude-from-url)

With xpath I was able to obtain the url that contains the latitude and longitude, but I would need these values to be shown separately in the following way:latitude = -34.552654847695510

longitude= -58.457549057672110

2018-10-31 13:09:10Z

With xpath I was able to obtain the url that contains the latitude and longitude, but I would need these values to be shown separately in the following way:latitude = -34.552654847695510

longitude= -58.457549057672110Try this, for example: response.css('#article-map img::attr(src)').re(r'markers=([-\d\.]+),([-\d\.]+)')Or 

a. get url like response.css('#article-map img::attr(src)').get()

b. extract markers or center param via from w3lib.url import url_query_parameter and then apply regexp.But first variant looks much shorter and easier.use the url parse module is handy and accuracy:

How to extract data that loads differently with scrapy

Martin Boudreaux

[How to extract data that loads differently with scrapy](https://stackoverflow.com/questions/53095230/how-to-extract-data-that-loads-differently-with-scrapy)

I’m trying to extract product reviews on URLs like this onehttps://www.namastevaporizers.com/products/mighty-vaporizerthe spider I have extracts anything on the page but nothing from the comments, I think it is because the comments load differently but unfortunately this is where my knowledge of scrappy ends. Can anyone help me with this?here is my spider

2018-11-01 04:29:09Z

I’m trying to extract product reviews on URLs like this onehttps://www.namastevaporizers.com/products/mighty-vaporizerthe spider I have extracts anything on the page but nothing from the comments, I think it is because the comments load differently but unfortunately this is where my knowledge of scrappy ends. Can anyone help me with this?here is my spiderthe reviews in this site are loaded by JS, so you need to forge the request as your chrome doFollow these steps you will get the result

403 when copying request from chrome to curl

Elvisjames

[403 when copying request from chrome to curl](https://stackoverflow.com/questions/53097391/403-when-copying-request-from-chrome-to-curl)

Trying to scrape a website. It was working fine about a month ago but now it gives 403 response even though the curl request was copied from chrome dev tools. The requests still work in browser and through postman.

The website is kohls.com

2018-11-01 08:14:07Z

Trying to scrape a website. It was working fine about a month ago but now it gives 403 response even though the curl request was copied from chrome dev tools. The requests still work in browser and through postman.

The website is kohls.comMaybe try to use some proxy, looks like they really answer with 403 to some IP pools.You are being blocked by User Agent. Try setting it with the -A flag and the -L flag to follow redirects like this:

Scrapy - Reactor not Restartable in Django

zyznull

[Scrapy - Reactor not Restartable in Django](https://stackoverflow.com/questions/53040073/scrapy-reactor-not-restartable-in-django)

I have meet a porblem when I want to run my spiders in Djanjo.Some months ago.This method work for me:But it doesn't work now.Get these errors:I have read all answers about it. But doesn't work for me.The spider can run successfully when I run it locally without djanjo. But meet the Reactor not Restartable in Djanjo.

I have tried a method like thatIt solves the reactor problem。But the spider seems not run and crawl nothin.

2018-10-29 06:43:45Z

I have meet a porblem when I want to run my spiders in Djanjo.Some months ago.This method work for me:But it doesn't work now.Get these errors:I have read all answers about it. But doesn't work for me.The spider can run successfully when I run it locally without djanjo. But meet the Reactor not Restartable in Djanjo.

I have tried a method like thatIt solves the reactor problem。But the spider seems not run and crawl nothin.

Scrapy Next page link is giving nothing

Abhijeet Pal

[Scrapy Next page link is giving nothing](https://stackoverflow.com/questions/53044990/scrapy-next-page-link-is-giving-nothing)

Hello I want to go to every page of this site by clicking on the next page https://www.dehatilyrics.top/But the Xpath of Next page is not giving anything not even an empty dictionary.How to get the next page links of such site?

2018-10-29 11:55:51Z

Hello I want to go to every page of this site by clicking on the next page https://www.dehatilyrics.top/But the Xpath of Next page is not giving anything not even an empty dictionary.How to get the next page links of such site?

Python Scrapy get article body, extract_first() get None

Tester

[Python Scrapy get article body, extract_first() get None](https://stackoverflow.com/questions/53010943/python-scrapy-get-article-body-extract-first-get-none)

I tried to use Scrapy to get article body from news site.I try to get text in div StandardArticleBody_body but always get None value.The output is 

2018-10-26 14:30:28Z

I tried to use Scrapy to get article body from news site.I try to get text in div StandardArticleBody_body but always get None value.The output is There isn't any text belonging directly to the div you're selecting, but to it's descendants. A space between the selector path and the :: will get text of all descendants, not just the text of the node you've selected.Try thisSo that you're getting all the text of the div's descendants.

InvalidSchema(“No connection adapters were found for '%s'” % url) for skype url

daniel ding

[InvalidSchema(“No connection adapters were found for '%s'” % url) for skype url](https://stackoverflow.com/questions/53097317/invalidschemano-connection-adapters-were-found-for-s-url-for-skype-url)

I was able to gather data from a web page using thiswhen I try this to gather more data on the results of the above code I was presented with this errorI think this error is caused by the presence of ‘skype:+8615050520029?chat’ in the url. After testing this, I would like to ask how to solve this problem.strat_url = 'https:\www.advich.com'

please help me, thanks

2018-11-01 08:08:55Z

I was able to gather data from a web page using thiswhen I try this to gather more data on the results of the above code I was presented with this errorI think this error is caused by the presence of ‘skype:+8615050520029?chat’ in the url. After testing this, I would like to ask how to solve this problem.strat_url = 'https:\www.advich.com'

please help me, thanksTry to exclude such urls in your for cycle or earlier in urls = response.css('a:not([href*=skype]):not([href*=mailto])::attr(href)').extract().Because you will have problem with making request to this "url".

Scrapy Spider not returning anything

bullybear17

[Scrapy Spider not returning anything](https://stackoverflow.com/questions/52979732/scrapy-spider-not-returning-anything)

I'm very new to the Scrapy library and i'm struggling with my spider. I'm trying to scrape data from this website https://murderpedia.org/male.A/index.A.htmWhat i'm trying to do is for every link on the page, I'd like to follow the link and scrape the image as well as the text [rows 3 - 11]. Any help here would be immensely appreciated. Here is my code:Here is the crawl log:

2018-10-25 00:37:19Z

I'm very new to the Scrapy library and i'm struggling with my spider. I'm trying to scrape data from this website https://murderpedia.org/male.A/index.A.htmWhat i'm trying to do is for every link on the page, I'd like to follow the link and scrape the image as well as the text [rows 3 - 11]. Any help here would be immensely appreciated. Here is my code:Here is the crawl log:It seems like you crawler is not chained correctly.Your wanted crawl logic is:Right now your code is missing step #2 Lets try this:

Scrapy - unexpected return when there is Chinese character in xpath

Jiteng Ma

[Scrapy - unexpected return when there is Chinese character in xpath](https://stackoverflow.com/questions/52941499/scrapy-unexpected-return-when-there-is-chinese-character-in-xpath)

I am new and I know there is a similar question to this. However, I don't think that problem is solved.The version of scrapy I am using is 1.0.3 and the environment is in a virtualbox. What I am trying to do is to scrap all information from which has "西二旗" in the @title. My script is like this:and the output is like this:Which returns all the elements not mater containing the keywords or not.So I really want to know what's happening. I also tried this on my chrome with 

    $x('//h2/a[contains(@title,"领秀")]')

and it works fine (only one element returned).

2018-10-23 05:02:47Z

I am new and I know there is a similar question to this. However, I don't think that problem is solved.The version of scrapy I am using is 1.0.3 and the environment is in a virtualbox. What I am trying to do is to scrap all information from which has "西二旗" in the @title. My script is like this:and the output is like this:Which returns all the elements not mater containing the keywords or not.So I really want to know what's happening. I also tried this on my chrome with 

    $x('//h2/a[contains(@title,"领秀")]')

and it works fine (only one element returned).Your code doesn't really expand the variable on xpath implicitly like that:It should be:

How to get stats value after CrawlerProcess finished, i.e. at line after process.start()

Hellohowdododo

[How to get stats value after CrawlerProcess finished, i.e. at line after process.start()](https://stackoverflow.com/questions/52940700/how-to-get-stats-value-after-crawlerprocess-finished-i-e-at-line-after-process)

I am using this code somewhere inside spider:So, when this exceptions raised, eventually my spider closing working and I get in console stats with this string:But - how I can get it from code? Cause I want to run spider again in loop, based on info from this stats, something like this:I found in docs this thing, but can't get it right, where is that "The stats can be accessed through the spider_stats attribute, which is a dict keyed by spider domain name.": https://doc.scrapy.org/en/latest/topics/stats.html#scrapy.statscollectors.MemoryStatsCollector.spider_statsP.S.: I'm also getting error twisted.internet.error.ReactorNotRestartable when using process.start(), and recommendations to use process.start(stop_after_crawl=False) - and then spider just stops and do nothing, but this is another problem...

2018-10-23 03:23:47Z

I am using this code somewhere inside spider:So, when this exceptions raised, eventually my spider closing working and I get in console stats with this string:But - how I can get it from code? Cause I want to run spider again in loop, based on info from this stats, something like this:I found in docs this thing, but can't get it right, where is that "The stats can be accessed through the spider_stats attribute, which is a dict keyed by spider domain name.": https://doc.scrapy.org/en/latest/topics/stats.html#scrapy.statscollectors.MemoryStatsCollector.spider_statsP.S.: I'm also getting error twisted.internet.error.ReactorNotRestartable when using process.start(), and recommendations to use process.start(stop_after_crawl=False) - and then spider just stops and do nothing, but this is another problem...You need to access stats object via Crawler object:

How to get data from clicking with Scrapy

Luis Eduardo Delfin Ares De Pa

[How to get data from clicking with Scrapy](https://stackoverflow.com/questions/52940310/how-to-get-data-from-clicking-with-scrapy)

So, I'm trying to get more data from a page dependening on what I selected on a dropdown List using Scrapy. Visit: http://grace.itam.mx/EDSUP/BWZKSENP.P_Horarios1?s=1416In the Code below, first I get all the elements of the dropdown List, and it is working fine. But I would like also to get the info after selecting an Element and then clicking submbit to go to a new page with a data table. Is there anyway to do that ? Edit:

Here I used the FormRequest to parse the new page. 

2018-10-23 02:28:50Z

So, I'm trying to get more data from a page dependening on what I selected on a dropdown List using Scrapy. Visit: http://grace.itam.mx/EDSUP/BWZKSENP.P_Horarios1?s=1416In the Code below, first I get all the elements of the dropdown List, and it is working fine. But I would like also to get the info after selecting an Element and then clicking submbit to go to a new page with a data table. Is there anyway to do that ? Edit:

Here I used the FormRequest to parse the new page. Use Scrapy FormRequest.from_response using as parameter the values from the Element you want to select. This way, you can parse the other page's content on another method in your spider.EDIT 1:In your FormRequest there is a mistake. Change it to:I don't think adding the s : 1416 parameter is a requirement, but txt_materia should be lowercase. When I used it as the code above the responses were corrrect.In your after_button method, I've found two minor mistakes:This is the code I wrote for after_button:

Can't get email from html in scrapy

Softdev

[Can't get email from html in scrapy](https://stackoverflow.com/questions/52914423/cant-get-email-from-html-in-scrapy)

I can see there is an email in chrome inspect.But in page source, it looks likeI need to scrape email with python scrapy.

How can I get email?

2018-10-21 10:40:06Z

I can see there is an email in chrome inspect.But in page source, it looks likeI need to scrape email with python scrapy.

How can I get email?Here is some implelementation of the deobfuscation function in python.The functions used to deobfuscate emails are available in the page's source:It might look like a lot of code, but it's really simple, and can be broken down into a few steps:Implementing this in python should only take a few lines of code, and once yyou do that you'll be able to deobfuscate the emails yourself.

Getting error {KeyError}<Classname spidername at 0x7fb3f6b9c790> from self.files.pop(spider) in scrapy pipeline

Softdev

[Getting error {KeyError}<Classname spidername at 0x7fb3f6b9c790> from self.files.pop(spider) in scrapy pipeline](https://stackoverflow.com/questions/52906765/getting-error-keyerrorclassname-spidername-at-0x7fb3f6b9c790-from-self-files)

When I run scrapy to export multiple csv from one spider, I am getting error {KeyError} from self.files.pop(spider).Here is my pipeline.Also I added this line in settings.pyWhat is the issue in this code?

Thanks.

2018-10-20 14:35:34Z

When I run scrapy to export multiple csv from one spider, I am getting error {KeyError} from self.files.pop(spider).Here is my pipeline.Also I added this line in settings.pyWhat is the issue in this code?

Thanks.I don't see any value being added into self.filesTalking about error, it means that the key spider does not exists in self.filesI guess you are looking for Edit:

python install scrapy - failed with error code 1

tzvia

[python install scrapy - failed with error code 1](https://stackoverflow.com/questions/52915686/python-install-scrapy-failed-with-error-code-1)

I tried to install scrapy by "pip install scrapy", and I got:I tried "conda install -c conda-forge scrapy" and it seems like the scrapy already installed:so I tried using scrapy and go an error: I tried to install lxml, and got again "failed with error code 1".can someone help me?

I use python 3.6.1 and windows 10

2018-10-21 13:15:04Z

I tried to install scrapy by "pip install scrapy", and I got:I tried "conda install -c conda-forge scrapy" and it seems like the scrapy already installed:so I tried using scrapy and go an error: I tried to install lxml, and got again "failed with error code 1".can someone help me?

I use python 3.6.1 and windows 10Installing lxml on Windows can be a bit of pain. You might want to check out the info they have on their site here. They suggest that if building from source fails you might want to use the unofficial pre-built binaries by downloading from here and then running Make sure to download the version that you need. If you're using python 3.6 you should get the cp36 one. I think it is because your Twisted install failed.

Download the Twisted .whl file from https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted .Then run

Scrapy FormRequest

mylha

[Scrapy FormRequest](https://stackoverflow.com/questions/52904932/scrapy-formrequest)

I'm having trouble with scrapy FormRequest. I am trying to get all reviews from this page (infinite scrolling) : https://www.temporel-voyance.com/voyance/planning/consultations-voyants-en-privee/angele/1041When I scroll, I can see a post request with data form : {xyz":"3"}But when I try it with scrapy command line, I am not able to get the correct response. fetch("https://www.temporel-voyance.com/voyance/planning/consultations-voyants-en-privee/angele/1041")req = FormRequest(response.url, formdata={"xyz":"3"})fetch(req)I had a look to this page : https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016 but I am still stuck.Can someone help me ?Thank you very much !

Mylha

2018-10-20 11:01:07Z

I'm having trouble with scrapy FormRequest. I am trying to get all reviews from this page (infinite scrolling) : https://www.temporel-voyance.com/voyance/planning/consultations-voyants-en-privee/angele/1041When I scroll, I can see a post request with data form : {xyz":"3"}But when I try it with scrapy command line, I am not able to get the correct response. fetch("https://www.temporel-voyance.com/voyance/planning/consultations-voyants-en-privee/angele/1041")req = FormRequest(response.url, formdata={"xyz":"3"})fetch(req)I had a look to this page : https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016 but I am still stuck.Can someone help me ?Thank you very much !

MylhaIf you take a better look at the request being made, you'll see that it's actually to a different url, https://www.temporel-voyance.com/voyants/temoignages?voyant_id=1041If you make your form request to that url, you get the updated data:

XPath: Matching a text between two similar tags

Mohamed Elmahdi

[XPath: Matching a text between two similar tags](https://stackoverflow.com/questions/52920137/xpath-matching-a-text-between-two-similar-tags)

I'm trying to scrape a website with messy structure, the text I'm requiring is laying between the first 5 consecutive br tags (No more and no less, exactly 5) and the following 2 consecutive br tags.

It looks like this:

2018-10-21 21:37:12Z

I'm trying to scrape a website with messy structure, the text I'm requiring is laying between the first 5 consecutive br tags (No more and no less, exactly 5) and the following 2 consecutive br tags.

It looks like this:Scrapy converts <br> tags to newline characters, so you can just extract the whole text and split it at 5 newline characters:

Is it possible for scrapy to navigate links before actually scraping data?

Jeff

[Is it possible for scrapy to navigate links before actually scraping data?](https://stackoverflow.com/questions/52880697/is-it-possible-for-scrapy-to-navigate-links-before-actually-scraping-data)

I've been working through a few scrapy tutorials and I have a question (I'm very new to this, so I apologize if this is a dumb question). Most of what I've seen so far involved:1) feeding a starting url to scrapy2) telling scrapy what parts of the page to grab3) telling scrapy how to find the "next" page to scrape fromWhat I'm wondering is - am I able to scrape data using scrapy when the data itself isn't on the start page? For example, I have a link that goes to a forum. The forum contains links to several subforums. Each subforum has links to several threads. Each thread contains several messages (possibly over multiple pages). The messages are what I ultimately want to scrape. Is it possible to do this and use only the initial link to the forum? Is it possible for scrapy to navigate through every subforum, and every thread and then start scraping?

2018-10-18 18:51:21Z

I've been working through a few scrapy tutorials and I have a question (I'm very new to this, so I apologize if this is a dumb question). Most of what I've seen so far involved:1) feeding a starting url to scrapy2) telling scrapy what parts of the page to grab3) telling scrapy how to find the "next" page to scrape fromWhat I'm wondering is - am I able to scrape data using scrapy when the data itself isn't on the start page? For example, I have a link that goes to a forum. The forum contains links to several subforums. Each subforum has links to several threads. Each thread contains several messages (possibly over multiple pages). The messages are what I ultimately want to scrape. Is it possible to do this and use only the initial link to the forum? Is it possible for scrapy to navigate through every subforum, and every thread and then start scraping?Yes, you can navigate without scraping data, though you will need to extract the links for navigation with either xpath or css or CrawlSpider rules. These links can be used just for navigation and don't need to be loaded into items.There's no requirement that you load something into an item from every page you visit. Consider a scenario where you need to authenticate past login to get to data that you want to scrape. No need to scrape/pipeline/write any data from the login page.For your purposes:

Getting the current url page ref scrapy

Yall

[Getting the current url page ref scrapy](https://stackoverflow.com/questions/52849274/getting-the-current-url-page-ref-scrapy)

I'm trying to add the current url of the scraped page on my script. But for some reason I can't select this :It is nested into the head.I tried response.xpath("//head/link[@rel='canonical']@href").extract() What am I doing wrong ?

2018-10-17 07:18:28Z

I'm trying to add the current url of the scraped page on my script. But for some reason I can't select this :It is nested into the head.I tried response.xpath("//head/link[@rel='canonical']@href").extract() What am I doing wrong ?If you just want the url of the current response. You can just use response.urlIf you really need the canonical URL, this should work:Your expression was missing / before @href.You could also use CSS:If you don't care about the canonical URL, then you can follow @Yall's suggestion above.

Scrapy - Python - Scraping all text with p in a div

Yuri Matos

[Scrapy - Python - Scraping all text with p in a div](https://stackoverflow.com/questions/52856293/scrapy-python-scraping-all-text-with-p-in-a-div)

I am trying to scrapy a text from this divBut now the site implemented the tag  and I cant scrapy all the text

I m using this commandand it returnsAll the text in the last p tagSo how can I scrapy all text in the div with the p tag

2018-10-17 13:38:29Z

I am trying to scrapy a text from this divBut now the site implemented the tag  and I cant scrapy all the text

I m using this commandand it returnsAll the text in the last p tagSo how can I scrapy all text in the div with the p tagYou want to scrape all text of ps seprately? loop through themI do it in this wayYour way is simpler I thinkBut thanks all

Scraping a web page and need to pick right selector

noor hashem

[Scraping a web page and need to pick right selector](https://stackoverflow.com/questions/52859360/scraping-a-web-page-and-need-to-pick-right-selector)

It's my first time using Scrapy after watching a couple of tutroials, i'm trying to scrape this url https://www.hackster.io/arduino/membersI want to get the links to every user profile. I ran my scrapy shell as followsbut i get only [] as an output​I want to get the link as specified in the photo attached, can anyone please have a look and tell me if there is something wrong with my command?​url to be scrapedwhen i used google's chorme inspect option and copied the selector right away i got the same output

2018-10-17 16:16:04Z

It's my first time using Scrapy after watching a couple of tutroials, i'm trying to scrape this url https://www.hackster.io/arduino/membersI want to get the links to every user profile. I ran my scrapy shell as followsbut i get only [] as an output​I want to get the link as specified in the photo attached, can anyone please have a look and tell me if there is something wrong with my command?​url to be scrapedwhen i used google's chorme inspect option and copied the selector right away i got the same outputThat's because the html you see in the Chrome Console is built client-side in javascript. Scrapy by default does not interpret javascript and read the page source as it is sent by the server. See my answer here to find solutions for your problem.To check what response the scrapy crawler is getting :-The response as seen to the crawler will be shown in your default web browser.From this response you can check whether your crawler is getting the content you want to scrape!As I can see from the response that you are not getting Arduino_Genuino in the response, this is definitely a case of client side javascript rendering. Screenshot of the webpage as visible to the crawler.To Scrape data from such pages, you need to use a javascript rendering engine such as scrapy-splash which runs on your localhost:8050You have to pass the url to scrape to the splash rendering engine and after some timeout when the javascript is fully loaded into the splash at localhost:8050, you have to scrape the data from there. Refer splash docs: https://splash.readthedocs.io/en/stable/api.html

pass different url in start_urls

Elizabeth79

[pass different url in start_urls](https://stackoverflow.com/questions/52864568/pass-different-url-in-start-urls)

I'm new with scrapy and little by little I'm building my first spider, I'm trying to pass different url to start_urls, then I thought of adding them to a list, and then go through that list to start_urls, the problem that when I execute it only takes a url of the list and it stops.The data returns them correctly but only of one of the url, it does not make the complete loop.

What am I doing wrong?.

Thank you

2018-10-17 22:40:22Z

I'm new with scrapy and little by little I'm building my first spider, I'm trying to pass different url to start_urls, then I thought of adding them to a list, and then go through that list to start_urls, the problem that when I execute it only takes a url of the list and it stops.The data returns them correctly but only of one of the url, it does not make the complete loop.

What am I doing wrong?.

Thank youYour code is overriding variable every loop:it should be:

Python + Scrapy + JSON : 'NoneType' object has no attribute 'body_as_unicode'

Debbie

[Python + Scrapy + JSON : 'NoneType' object has no attribute 'body_as_unicode'](https://stackoverflow.com/questions/52784129/python-scrapy-json-nonetype-object-has-no-attribute-body-as-unicode)

I am trying to scrape all the urls from this JSON page with Scrapy in Python:view-source:https://highape.com/bangalore/all-eventsBut whenever I write this code on my Scrapy shell:I get the following error:This is a portion of that big json file:I need to scrape URLs  like this (it would be quite helpful if you suggest a solution without involving Beautiful Soup) : 

2018-10-12 17:07:46Z

I am trying to scrape all the urls from this JSON page with Scrapy in Python:view-source:https://highape.com/bangalore/all-eventsBut whenever I write this code on my Scrapy shell:I get the following error:This is a portion of that big json file:I need to scrape URLs  like this (it would be quite helpful if you suggest a solution without involving Beautiful Soup) : 

Web-scrape for link with Scrapy

Tim1234

[Web-scrape for link with Scrapy](https://stackoverflow.com/questions/52778040/web-scrape-for-link-with-scrapy)

I again have a problem extracting a specific link with Scrapy. Here the HTML excerpt:I need the Link in the href Tag. Among other things, I tried the following:I would be very grateful if someone could help me here.

2018-10-12 10:55:35Z

I again have a problem extracting a specific link with Scrapy. Here the HTML excerpt:I need the Link in the href Tag. Among other things, I tried the following:I would be very grateful if someone could help me here.Here's how to do in scrapy shell:Have you tried using BeautifulSoup along with requests?Heres a example using this module:something like this I from what Ive used works, I may have to edit this post later on but using bs4 and requests is still a valid method.You may not want to do findAll if there is more then one <a>, if they have the same class (for example youtube has this for each video, and using findall will get each video url for the search) it will retrieve the link Ok, when I try it with: response.css("section > div > form > a::attr(href)").extract_first() I get the Link. I don't know if this is the best solution now, but it works,url = response.css('div.testclass > form > a::attr(href)').extract_first()Your original selector is looking for as that exist as direct descendants of div.testclass. You want to look inside forms that are direct descendants of div.testclass, based on the structure of your snippet.You could also try this:url = response.css('div.testclass a::attr(href)').extract_first()but it won't be as specific.

How to Disable celery tasks logs

pakachu

[How to Disable celery tasks logs](https://stackoverflow.com/questions/52751711/how-to-disable-celery-tasks-logs)

i'm using celery to send task.i want to see only 'received task ..' and 'Task .. succeeded' 



but spider logs on celery server how to disable task logs?

2018-10-11 03:26:46Z

i'm using celery to send task.i want to see only 'received task ..' and 'Task .. succeeded' 



but spider logs on celery server how to disable task logs?From your picture i can see that this is debug logs. 

So, configure your logger properly or disable DEBUG mode. Your screenshot doesn't give enough info about the frameworks/libs your are using, so i cannot advice anything else. You can write a shell script. grep logs you need to annother file per second.

How to crawl all url in AJAX pages?

Dai Ngo

[How to crawl all url in AJAX pages?](https://stackoverflow.com/questions/52752177/how-to-crawl-all-url-in-ajax-pages)

I use scrapy and splash to crawl all url in website. In some website with static html, It works very good! But when I crawled some website has AJAX page, and html5 I cannot get any url (Example: http://testphp.vulnweb.com/AJAX/, http://testhtml5.vulnweb.com). Anyone has solution for this problem? 

Thanks so much!

2018-10-11 04:22:44Z

I use scrapy and splash to crawl all url in website. In some website with static html, It works very good! But when I crawled some website has AJAX page, and html5 I cannot get any url (Example: http://testphp.vulnweb.com/AJAX/, http://testhtml5.vulnweb.com). Anyone has solution for this problem? 

Thanks so much!Use Request package-python You can generate the request for the content and get the response.Example code here 

Scrapy redirect is always 200

Bociek

[Scrapy redirect is always 200](https://stackoverflow.com/questions/52756739/scrapy-redirect-is-always-200)

I am experiencing strange behavior in Scrapy. I collect status codes by calling response.status, by not all of them are present (Seems to be 3xx). I see in the log the following thing:whereas my csv file has only 200, 404, 403, 406, 502, 400, 405, 410, 500, 503. I set HTTPERROR_ALLOW_ALL=True in the settings.py. Can I force Scrapy to provide information about redirects? Right know I am taking it from response.meta['redirect_times'] and response.meta['redirect_urls'], but status code is still 200, instead of 3xx.

2018-10-11 09:31:42Z

I am experiencing strange behavior in Scrapy. I collect status codes by calling response.status, by not all of them are present (Seems to be 3xx). I see in the log the following thing:whereas my csv file has only 200, 404, 403, 406, 502, 400, 405, 410, 500, 503. I set HTTPERROR_ALLOW_ALL=True in the settings.py. Can I force Scrapy to provide information about redirects? Right know I am taking it from response.meta['redirect_times'] and response.meta['redirect_urls'], but status code is still 200, instead of 3xx.30X responses will never reach your callback (parse method) because they are being handles by a redirect middleware before that.However all of the response statuses are already stored in scrapy stats as you have pointed out yourself which means you can easily pull them in your crawler at any point:Anywhere where you have access to crawler object really!

How to execute requests postrequest in scrapy

Niki

[How to execute requests postrequest in scrapy](https://stackoverflow.com/questions/52757288/how-to-execute-requests-postrequest-in-scrapy)

I want to start my scrapy spider with a post requestAs long I execute the request directly from python using the requests package like above everything works fine. However as soon as I incorporate it in my spider using the scrapy's 'Formrequest'I get the following errorCould anyone please tell me what the error message means and why my request is not working in scrapy while everything is totally fine using 'requests.post'?Many thanks in advance

2018-10-11 09:59:25Z

I want to start my scrapy spider with a post requestAs long I execute the request directly from python using the requests package like above everything works fine. However as soon as I incorporate it in my spider using the scrapy's 'Formrequest'I get the following errorCould anyone please tell me what the error message means and why my request is not working in scrapy while everything is totally fine using 'requests.post'?Many thanks in advanceChange your start_requests method to this:You yield a request object to scrapy and it makes the request and parse it on the callback method defined on request.

How to exclude Javascript from raw html in scrapy

Abhijeet Pal

[How to exclude Javascript from raw html in scrapy](https://stackoverflow.com/questions/52705038/how-to-exclude-javascript-from-raw-html-in-scrapy)

I am making a scraper which scrapes the site with HTML tags but the problem is the site which I am scraping has ads between content so XPath selectors are scraping the scripts too, Which I don't want.So How to exclude javascript from HTML?

2018-10-08 14:59:58Z

I am making a scraper which scrapes the site with HTML tags but the problem is the site which I am scraping has ads between content so XPath selectors are scraping the scripts too, Which I don't want.So How to exclude javascript from HTML?

AttributeError: 'module' object has no attribute 'AsyncoreConnection'

Jebaseelan Ravi

[AttributeError: 'module' object has no attribute 'AsyncoreConnection'](https://stackoverflow.com/questions/52763064/attributeerror-module-object-has-no-attribute-asyncoreconnection)

I am using scrapy-rabbitmq to fetch the url from RabbitMQ into my scrapy.I am using the following in my settings.py file But I am unable  to connect to the rabbitmq .I am getting the foloowing errorI have asked a another question that how to consume the data from RabbitmQ using scrapy here

2018-10-11 14:50:30Z

I am using scrapy-rabbitmq to fetch the url from RabbitMQ into my scrapy.I am using the following in my settings.py file But I am unable  to connect to the rabbitmq .I am getting the foloowing errorI have asked a another question that how to consume the data from RabbitmQ using scrapy hereI had to comment this line on its source code (connection.py).

error when run scrapy crawl my_scraper -o ehadith.csv

Mohamad Ammar

[error when run scrapy crawl my_scraper -o ehadith.csv](https://stackoverflow.com/questions/52703128/error-when-run-scrapy-crawl-my-scraper-o-ehadith-csv)

I really need help to solve my problem.I have an error:When I try running scrapy crawl my_scraper -o ehadith.csv

2018-10-08 13:12:09Z

I really need help to solve my problem.I have an error:When I try running scrapy crawl my_scraper -o ehadith.csvThat's not an error. That's a debug level log telling you that you're spider successfully downloaded the domain's robots.txt file.The other issue you are having is 403 responses. Try using the AutoThrottle extension to reduce requests concurrency.

Python Scrapy get select

Tim1234

[Python Scrapy get select](https://stackoverflow.com/questions/52758834/python-scrapy-get-select)

I hope someone can help me here. I want to use Scrappy to extract different links from a select list. Unfortunately I can't find the right command.This is the HTML Code:For example, I have tried it with:But unfortunately I don't get the links.Many thanks for your help.

2018-10-11 11:23:15Z

I hope someone can help me here. I want to use Scrappy to extract different links from a select list. Unfortunately I can't find the right command.This is the HTML Code:For example, I have tried it with:But unfortunately I don't get the links.Many thanks for your help.

Extract text from div class with scrapy

oliinykmd

[Extract text from div class with scrapy](https://stackoverflow.com/questions/52677769/extract-text-from-div-class-with-scrapy)

I am using python along with scrapy. I want to extract the text from the div tag which is inside a div class. For example:I've extracted text from h1 tagbut I can't extract Price. I've tried 

2018-10-06 09:48:18Z

I am using python along with scrapy. I want to extract the text from the div tag which is inside a div class. For example:I've extracted text from h1 tagbut I can't extract Price. I've tried As you have an id, you do not need to use the complete path to the element. Ids are unique per Webpage:This Xpath:used on the give XML will return:For debugging Xpath and CSS Selectors, I always find it helpful to use an online checker (just use Google to find some suggestions).

Scrapy is throwing TWISTED Unhandled error in Deferred when the spider is using proxymiddleware

Vikram Nimbalkar

[Scrapy is throwing TWISTED Unhandled error in Deferred when the spider is using proxymiddleware](https://stackoverflow.com/questions/52677481/scrapy-is-throwing-twisted-unhandled-error-in-deferred-when-the-spider-is-using)

I am using ProxyMiddleware in scrapy , and its throwing non traceable error.

Here is Traceback :Here are files : settings.pymiddelwares.py

2018-10-06 09:12:26Z

I am using ProxyMiddleware in scrapy , and its throwing non traceable error.

Here is Traceback :Here are files : settings.pymiddelwares.pyIn your settings.py, enable scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware with a higher priority value than your custom (401 or greater), so it's further from the engine than your custom middleware.In your middlewares.py, you can access the settings with spider.settings[], just settings[] won't work. I also think you can take the __init__() out of your ProxyMiddleware altogether.See this example from scrapinghub to get an idea of how close you are.

SCRAPY SPIDER - Send Post Request

roberto swiss

[SCRAPY SPIDER - Send Post Request](https://stackoverflow.com/questions/52609093/scrapy-spider-send-post-request)

I am trying to scrap the table of this webpage (https://www.ftse.com/pr oducts/indices/uk). When I inspect the page in the Network tab, I see this page fetches its data to an API with AJAX requests (type POST), which are done by the browser after the layout is loaded. So I am trying to build a spider which send POST requests to the webpage using the form_data given in the request. I have tested quickly with the following shell command and I get the data.However when I try to code it on the spider using FormRequest classes the spider fails.

2018-10-02 13:13:09Z

I am trying to scrap the table of this webpage (https://www.ftse.com/pr oducts/indices/uk). When I inspect the page in the Network tab, I see this page fetches its data to an API with AJAX requests (type POST), which are done by the browser after the layout is loaded. So I am trying to build a spider which send POST requests to the webpage using the form_data given in the request. I have tested quickly with the following shell command and I get the data.However when I try to code it on the spider using FormRequest classes the spider fails.since the data has nested dictionaries it can not be represented as formdata in scrapy, we must pass the json dump in the body of the request which is equal to the initial representation of the "data". Also use yield from when yielding an iterator or use a single object or Request to yield instead.

simple scrapy based on https://github.com/scrapy/quotesbot/blob/master/quotesbot/spiders/toscrape-xpath.py not passing data using yield Request

jayuya

[simple scrapy based on https://github.com/scrapy/quotesbot/blob/master/quotesbot/spiders/toscrape-xpath.py not passing data using yield Request](https://stackoverflow.com/questions/52684338/simple-scrapy-based-on-https-github-com-scrapy-quotesbot-blob-master-quotesbot)

strong textMy code based on examples that I searched did not seem to function as intended so I decided to use a working model found on github: https://github.com/scrapy/quotesbot/blob/master/quotesbot/spiders/toscrape-xpath.py

I then modified it slightly to showcase what I am running into. The code below works great as intended but my ultimate goal is to pass the scraped data from first "parse"  to a second "parse2" function so that I can combine data from 2 different pages. But for now I wanted to start very simple so I can follow what is happening, hence the heavily stripped code below.I only have one item scraped and it says the rest are duplicates. It also looks like "parse2" is not even read at all. I have played with the indentation and the brackets thinking I am missing something simple, but without much success. I have looked at many examples to see if I can make sense of what could be the issue but I still am not able to make it work. I am sure its a very simple issue for those gurus out there, so I yelp "Help!" somebody!also my items.py file looks like below and I think those two files items.py and toscrape-xpath.py are the only ones in action as far as I can tell since I am quite new to all this.Thank you very much to any and all help you can provide

2018-10-06 23:59:39Z

strong textMy code based on examples that I searched did not seem to function as intended so I decided to use a working model found on github: https://github.com/scrapy/quotesbot/blob/master/quotesbot/spiders/toscrape-xpath.py

I then modified it slightly to showcase what I am running into. The code below works great as intended but my ultimate goal is to pass the scraped data from first "parse"  to a second "parse2" function so that I can combine data from 2 different pages. But for now I wanted to start very simple so I can follow what is happening, hence the heavily stripped code below.I only have one item scraped and it says the rest are duplicates. It also looks like "parse2" is not even read at all. I have played with the indentation and the brackets thinking I am missing something simple, but without much success. I have looked at many examples to see if I can make sense of what could be the issue but I still am not able to make it work. I am sure its a very simple issue for those gurus out there, so I yelp "Help!" somebody!also my items.py file looks like below and I think those two files items.py and toscrape-xpath.py are the only ones in action as far as I can tell since I am quite new to all this.Thank you very much to any and all help you can provideYour spider logic is very confusing:For every quote you find on quotes.toscrape.com you schedule another request with to the same webpage? 

What happens is that these new scheduled requests get filtered out by scrapys duplicate request filter.Maybe you should just yield the item right there:To illustrate why your current crawler does nothing see this drawing:



Scrapy Installation Error: [CondaEnvironmentNotFoundError] : could not find environment: base

noobida

[Scrapy Installation Error: [CondaEnvironmentNotFoundError] : could not find environment: base](https://stackoverflow.com/questions/52619430/scrapy-installation-error-condaenvironmentnotfounderror-could-not-find-envi)

I'm currently trying to install scrapy when I encountered my first error: I researched this error and followed the advice on this link:My issues were largely similar to his until I reached the comment which advised me to run conda update -n base conda.When I ran this code, I encountered my next error:Kindly advice if my steps taken were appropriate and how can I fix this issue.The weird thing is I installed scrapy before, and these errors occurred after I recently re-installed Anaconda.I'm not sure what other info you might require to better understand the situation. Do let me know and I will assist promptly.Thank You

2018-10-03 03:49:02Z

I'm currently trying to install scrapy when I encountered my first error: I researched this error and followed the advice on this link:My issues were largely similar to his until I reached the comment which advised me to run conda update -n base conda.When I ran this code, I encountered my next error:Kindly advice if my steps taken were appropriate and how can I fix this issue.The weird thing is I installed scrapy before, and these errors occurred after I recently re-installed Anaconda.I'm not sure what other info you might require to better understand the situation. Do let me know and I will assist promptly.Thank YouTry the conda install scrapy channel instead of the conda-forge channel.

To understand the difference between these two channels please read the answer of the following question Should conda, or conda-forge be used for Python environments?

Getting first image url using scrapy

joy

[Getting first image url using scrapy](https://stackoverflow.com/questions/52625127/getting-first-image-url-using-scrapy)

I'm trying to get first url "https://example.com/example.jpg" using xpath @src but the results always /Content/images/defaultThumb.jpg 

2018-10-03 10:33:35Z

I'm trying to get first url "https://example.com/example.jpg" using xpath @src but the results always /Content/images/defaultThumb.jpg Your xpath was wrong, try this...

scrapy encoding data text python

Abdelmoula Nami

[scrapy encoding data text python](https://stackoverflow.com/questions/52581908/scrapy-encoding-data-text-python)

I need you help folks, to scrapy a text element which is encypted

here is my spider My problem is that the returned phone string is encoded and need you help to get the the text 

Thank you in advance!

2018-09-30 20:32:50Z

I need you help folks, to scrapy a text element which is encypted

here is my spider My problem is that the returned phone string is encoded and need you help to get the the text 

Thank you in advance!Seems like the website is using their own internal AJAX calls to decrypt phone number strings; if you look at your web browser inspector:

You can replicate this request in scrapy: 

The class in Item can`t be recognized after building a python scrapy program

thomson zeng

[The class in Item can`t be recognized after building a python scrapy program](https://stackoverflow.com/questions/52587150/the-class-in-item-cant-be-recognized-after-building-a-python-scrapy-program)

I try to build a scrapy project according to a book.

After using 'scrapy startproject tutorial/cd tutorial/scrapy genspider quotes 

quotes.toscrape.coom' comands and adding the parse function & changing items, the detail code as fellow:quotes.py:items.py:The class QuoteItem cann`t be recognized in quotes.py 

error prompt pictureAnd after I changed to 'from tutorial.tutorial.items import QuoteItem'

and run 'scrapy crawl quotes', there is another error as fellow:

error againAnd this caused the results can`t be saved. Someone can help, thanks in advance.

2018-10-01 08:23:50Z

I try to build a scrapy project according to a book.

After using 'scrapy startproject tutorial/cd tutorial/scrapy genspider quotes 

quotes.toscrape.coom' comands and adding the parse function & changing items, the detail code as fellow:quotes.py:items.py:The class QuoteItem cann`t be recognized in quotes.py 

error prompt pictureAnd after I changed to 'from tutorial.tutorial.items import QuoteItem'

and run 'scrapy crawl quotes', there is another error as fellow:

error againAnd this caused the results can`t be saved. Someone can help, thanks in advance.It's working fine with that code!! try using scrapy runspider yourspiderfile.py instead of scrapy crawl quotes.There is no error in the code. 

How to use Tkinter python GUI to RUN my webscraper?

Ben P.

[How to use Tkinter python GUI to RUN my webscraper?](https://stackoverflow.com/questions/52584273/how-to-use-tkinter-python-gui-to-run-my-webscraper)

So I have a python webscraper right now that, when ran, gives the user a prompt to pick 1, 2, 3, or all. These options scrape the website/s according to the number. I want to make a python gui that, when the buttons are pressed (or checkboxes) they run the functions associated with 1, 2, 3, or all!

Here is some of my code so you know where I am started off with:and here is some of my scraper code so you see what I mean with the options 1, 2, 3, or all: Please help me understand how to link the buttons to running actual scraper code!

2018-10-01 03:36:52Z

So I have a python webscraper right now that, when ran, gives the user a prompt to pick 1, 2, 3, or all. These options scrape the website/s according to the number. I want to make a python gui that, when the buttons are pressed (or checkboxes) they run the functions associated with 1, 2, 3, or all!

Here is some of my code so you know where I am started off with:and here is some of my scraper code so you see what I mean with the options 1, 2, 3, or all: Please help me understand how to link the buttons to running actual scraper code!Try using PySimpleGUI as a GUI framework. It sounds like you simply need to show some buttons, get those button presses and then call some functions.  That's likely 10 lines of code with PySimpleGUI.  Copy some code from the Cookbook and run it and you'll immediately get an idea of how to use it.This code will get you started perhaps.  It produces this GUI:

https://user-images.githubusercontent.com/13696193/46325549-c3902380-c5c6-11e8-9e8a-75ba89c99bc5.jpg

How to use Scrapy to parse PDF pages online?

Code Monkey

[How to use Scrapy to parse PDF pages online?](https://stackoverflow.com/questions/52515592/how-to-use-scrapy-to-parse-pdf-pages-online)

I tried using Scrapy with PyPDF2 library to crawl PDfs online unsuccessfully. So far I'm able to navigate all links and able to grab the PDf files, but feeding them through PyPDF2 seems to be a problem. Note: my goal is not to grab/save PDF files, I intend to parse them by first converting PDF to text and then manipulating this text using other methods.For brevity, I did not include the entire code here. Here's part of my code:Each time I run the code, the spider attempts reader = PyPDF2.PdfFileReader(response.body) and gives the following error: AttributeError: 'bytes' object has no attribute 'seek'What am I doing wrong?

2018-09-26 10:28:57Z

I tried using Scrapy with PyPDF2 library to crawl PDfs online unsuccessfully. So far I'm able to navigate all links and able to grab the PDf files, but feeding them through PyPDF2 seems to be a problem. Note: my goal is not to grab/save PDF files, I intend to parse them by first converting PDF to text and then manipulating this text using other methods.For brevity, I did not include the entire code here. Here's part of my code:Each time I run the code, the spider attempts reader = PyPDF2.PdfFileReader(response.body) and gives the following error: AttributeError: 'bytes' object has no attribute 'seek'What am I doing wrong?That does not seem to be a problem with scrapy. PyPDF2 is expecting a stream of binary data.Hope this helps.

Scrapy handshake failure using TLSv1.0 through a proxy

Tylones

[Scrapy handshake failure using TLSv1.0 through a proxy](https://stackoverflow.com/questions/52518372/scrapy-handshake-failure-using-tlsv1-0-through-a-proxy)

I am currently trying to develop a web crawler using Scrapy to scrape a website which is not accessible outside of my company. The problem is that I have to go through a proxy, which I succeeded and I was able to run my spider on "http://quotes.toscrape.com". The problem is that the website where I should run it is using TLS 1.0 and I tried several solution which do not work :First Solution :The Output :After discovering that the website uses TLS 1.0, I tried to add a custom setting like this : Unfortunately, after doing this I get the same error and I don't know what I could do to unstuck myself. If you have an idea, I'll gladly take it !Thanks in advance 

2018-09-26 13:01:07Z

I am currently trying to develop a web crawler using Scrapy to scrape a website which is not accessible outside of my company. The problem is that I have to go through a proxy, which I succeeded and I was able to run my spider on "http://quotes.toscrape.com". The problem is that the website where I should run it is using TLS 1.0 and I tried several solution which do not work :First Solution :The Output :After discovering that the website uses TLS 1.0, I tried to add a custom setting like this : Unfortunately, after doing this I get the same error and I don't know what I could do to unstuck myself. If you have an idea, I'll gladly take it !Thanks in advance I believe this is a bug and it has already been solved on scrapy version 1.5.1.Ok, after doing some more research on the web,I found on that git issue someone having a similar problem. Updating Scrapy to 1.5.1 and downgrading Twisted to 16.6.0 did the trick. I have now another issue but this one seems fixed. 

Blank spaces between scraped values in Scrapy

Gabriel Naslaniec

[Blank spaces between scraped values in Scrapy](https://stackoverflow.com/questions/52522923/blank-spaces-between-scraped-values-in-scrapy)

I'm trying to use Scrapy to scrape some objects from the following page: https://www.reclameaqui.com.br/indices/lista_reclamacoes/?id=9980&page=1&size=10&status=ALLUsing the following code:When i run the spider, the 'status' and 'business' variables returns like this:But 'title' and 'city_date' returns like this:I don't know why it returns those blank spaces between the scraped values, how can i scrape the results without the blank spaces or do i need to remove then after scraping? (I'm also using splash to render the page, because it's a javascript-heavy page, but i don't think that this should affect the scraping)

2018-09-26 17:09:05Z

I'm trying to use Scrapy to scrape some objects from the following page: https://www.reclameaqui.com.br/indices/lista_reclamacoes/?id=9980&page=1&size=10&status=ALLUsing the following code:When i run the spider, the 'status' and 'business' variables returns like this:But 'title' and 'city_date' returns like this:I don't know why it returns those blank spaces between the scraped values, how can i scrape the results without the blank spaces or do i need to remove then after scraping? (I'm also using splash to render the page, because it's a javascript-heavy page, but i don't think that this should affect the scraping)The blank spaces usually comes because of <br> tags from HTML. This is very common in sites, unfortunately. What you can do to solve this, and this is why I use to, is join the list. Credtis to @Sven H. for the solution

loop inside “THIS” selector with xpath, scrapy

Alberto Siurob

[loop inside “THIS” selector with xpath, scrapy](https://stackoverflow.com/questions/52561725/loop-inside-this-selector-with-xpath-scrapy)

I´m using scrapy to crawl some information, but im stunned with the handle of the data. At for loop, first step is locate a "root" xpath called selectors, then is looped. My mind says, if is the first selector, is the first selector of ul, then I can crawl all the data of the first selector! Instead, scrapy returns an array of all elements. I´m using selector varible as 'THIS' 

For example, on the first loop I expectInstead, I recived in the first loopAll the loops the same data, is not respecting current selector?WHY?I can´t make a relation between arrays because sometimes the selector has not name and/or phone and Scrapy doesnt return None or null or empty.How can I solve it?

2018-09-28 19:38:05Z

I´m using scrapy to crawl some information, but im stunned with the handle of the data. At for loop, first step is locate a "root" xpath called selectors, then is looped. My mind says, if is the first selector, is the first selector of ul, then I can crawl all the data of the first selector! Instead, scrapy returns an array of all elements. I´m using selector varible as 'THIS' 

For example, on the first loop I expectInstead, I recived in the first loopAll the loops the same data, is not respecting current selector?WHY?I can´t make a relation between arrays because sometimes the selector has not name and/or phone and Scrapy doesnt return None or null or empty.How can I solve it?There are some issues in your code:So, your code should be something along these lines:

python scrapy can't see scraped data on console

Hackaholic

[python scrapy can't see scraped data on console](https://stackoverflow.com/questions/52536839/python-scrapy-cant-see-scraped-data-on-console)

Hi When i run my spider i am not able to see any scrapped data in my console.

here is my spider.I am trying to extract article title and date published.When I run my spider:

2018-09-27 12:17:16Z

Hi When i run my spider i am not able to see any scrapped data in my console.

here is my spider.I am trying to extract article title and date published.When I run my spider:You have to rename the start_request method to start_requests, which is the method that you have to override from the base Spider class. More info here.What's happening is that your spider is not issuing any requests because of the missing start_requests method.This is how it is supposed to be:

Scrapy: Extracting both text AND hyperlink text using xpath

Sean

[Scrapy: Extracting both text AND hyperlink text using xpath](https://stackoverflow.com/questions/52545278/scrapy-extracting-both-text-and-hyperlink-text-using-xpath)

I am trying to scrape all of the paragraph text, including the hyperlink text, within a specific div class. If I use the following - this results in all of the paragraph text to be extracted, but not the hyperlinks inside it. The results look like: However, if I use //a instead of //p as follows -this results in all of the hyperlinks being extracted but none of the paragraph text. I understand why this is happening, but am not sure on how to properly extract both the paragraph text AND the hyperlinked text. Thank you very much. 

2018-09-27 21:17:37Z

I am trying to scrape all of the paragraph text, including the hyperlink text, within a specific div class. If I use the following - this results in all of the paragraph text to be extracted, but not the hyperlinks inside it. The results look like: However, if I use //a instead of //p as follows -this results in all of the hyperlinks being extracted but none of the paragraph text. I understand why this is happening, but am not sure on how to properly extract both the paragraph text AND the hyperlinked text. Thank you very much. 

scrapy pass argu to selenium

Kenneth Yeung

[scrapy pass argu to selenium](https://stackoverflow.com/questions/52467045/scrapy-pass-argu-to-selenium)

In the website, have javascript need to select a date and click to get the result.

how can I pass args to selenium and select?I have already have middleware

2018-09-23 14:21:01Z

In the website, have javascript need to select a date and click to get the result.

how can I pass args to selenium and select?I have already have middleware

Python/Scrapy: How to determine if a page is html or not?

Code Monkey

[Python/Scrapy: How to determine if a page is html or not?](https://stackoverflow.com/questions/52466206/python-scrapy-how-to-determine-if-a-page-is-html-or-not)

I need to determine if a page downloaded by Scrapy spider is html or not. The site I wish the spider to crawl has a combination of pdf and html links. Hence, if it comes across a pdf file, it'll put the response through a PDFReader, else it'll read the html file as is. This is part of my code, but it's not working:I output the results of the spider to a .csv file, but it's always empty. Just having ct = response.headers outputs the entire header information, which is useless. What do I do?EDIT:

I have finally managed to return a dictionary, but still can't extract the relevant information:Outputting the above to a .csv file still returns a blank file, though output ct returns a .csv file with two lines: content-type and text/html. How do I extract the 'html' text part of the answer only?

2018-09-23 12:42:58Z

I need to determine if a page downloaded by Scrapy spider is html or not. The site I wish the spider to crawl has a combination of pdf and html links. Hence, if it comes across a pdf file, it'll put the response through a PDFReader, else it'll read the html file as is. This is part of my code, but it's not working:I output the results of the spider to a .csv file, but it's always empty. Just having ct = response.headers outputs the entire header information, which is useless. What do I do?EDIT:

I have finally managed to return a dictionary, but still can't extract the relevant information:Outputting the above to a .csv file still returns a blank file, though output ct returns a .csv file with two lines: content-type and text/html. How do I extract the 'html' text part of the answer only?Scrapy expects you to return an item from your parse method. It can be a dict or an Item object.If you are interested in the Content-Type:Scrapy has a really nice tutorial. It is worth to take a moment to follow it: https://doc.scrapy.org/en/latest/intro/tutorial.htmlEDIT:You can find the HTML code in the response.text property. But usually, you will want just a piece of this code. So, the better approach is using selectors. E.g, to get just the text inside the snippet <h1>Hello world</h1>, you can use:Take some time to read the documentation page about selectors. It worths the investment.You could use the lxml module and import the text as html.  If it parses it successfully then it's HTML.  I'm on a phone so I can't give you a full example.   etree.parse is the method you want.Not sure if it is still around. But it sounds like builtwith module may be useful to you?It shows you the various javascript frameworks, web frameworks, and web servers  being implemented. You can Google the web frameworks and determine if they are used to dynamically load content. You can: pip install builtwithhttps://pypi.org/project/builtwith/1.3.3/

Scrapy Selector attribute

Jonathan Cheng

[Scrapy Selector attribute](https://stackoverflow.com/questions/52481762/scrapy-selector-attribute)

I use the following website to test:scrapy shell http://example.webscraping.com/places/default/user/login#And do some test:Input 1: response.xpath('//div//[@style]/input')Output 1:Input 2:response.xpath('//div//@style/input')Output 2:[]Input 3:response.xpath('//div//@style/input') == response.xpath('//div[style]/input')Output 3: TrueI want to know how different 1 and 2 is,thanks.

2018-09-24 14:39:08Z

I use the following website to test:scrapy shell http://example.webscraping.com/places/default/user/login#And do some test:Input 1: response.xpath('//div//[@style]/input')Output 1:Input 2:response.xpath('//div//@style/input')Output 2:[]Input 3:response.xpath('//div//@style/input') == response.xpath('//div[style]/input')Output 3: TrueI want to know how different 1 and 2 is,thanks.I think that you're looking for this selector:This is how it works:Your 2nd selector (//div//@style/input) wouldn't work well because it does:

Scrapy + Python + Xpath : datapoint couldn't be scraped always

Debbie

[Scrapy + Python + Xpath : datapoint couldn't be scraped always](https://stackoverflow.com/questions/52439718/scrapy-python-xpath-datapoint-couldnt-be-scraped-always)

I want to scrape the address from this page:http://calendar.youtoocanrun.com/events/chennai-1/kanchipuram-half-marathon-2018-3rd-edition/When I am writing this xpath:I am getting desired result,i.e., a list containing the address elements:But when I am writing this xpath:I am getting an empty list.Why?

2018-09-21 08:32:44Z

I want to scrape the address from this page:http://calendar.youtoocanrun.com/events/chennai-1/kanchipuram-half-marathon-2018-3rd-edition/When I am writing this xpath:I am getting desired result,i.e., a list containing the address elements:But when I am writing this xpath:I am getting an empty list.Why?Because there are two spaces between the div classes (geodir_more_info and post_address).Input:Output:

Scrapy + Python + Xpath : Xpath returns an empty list

Debbie

[Scrapy + Python + Xpath : Xpath returns an empty list](https://stackoverflow.com/questions/52443176/scrapy-python-xpath-xpath-returns-an-empty-list)

I need to scrape the links to the images from this page:

http://calendar.youtoocanrun.com/events/new-delhi-1/beat-that-run/I wrote this xpath:It returned empty list. It should have returned the links to both gif and jpg files. Why?

2018-09-21 11:51:42Z

I need to scrape the links to the images from this page:

http://calendar.youtoocanrun.com/events/new-delhi-1/beat-that-run/I wrote this xpath:It returned empty list. It should have returned the links to both gif and jpg files. Why?The problem is not in your XPath expression, but in the assumption that the element you are looking for is in the page raw HTML file downloaded by Scrapy.Scrapy doesn't run any JavaScript files so that in many cases the response you get in Scrapy is different than what you see in the developer tools.If you open the same website using the "view page source" option from your browser, you'll see that the element you're looking for is not there. This means that such element is generated dynamically using JavaScript.There are some ways to solve this and I'd approach it in this order:

Scrapy+Xpath+Python: Datapoint cannot be scraped

Debbie

[Scrapy+Xpath+Python: Datapoint cannot be scraped](https://stackoverflow.com/questions/52420801/scrapyxpathpython-datapoint-cannot-be-scraped)

I want to scrape the URLs here:

I tried these:All returned an empty list.

2018-09-20 08:30:54Z

I want to scrape the URLs here:

I tried these:All returned an empty list.Does response.xpath('//h3[@class="geodir-entry-title"]/a/@href').extract()

or

response.xpath('//header[@class="geodir-entry-header"]/h3/a/@href').extract()Work for you?Looks like you just missed the h3 tag which contains the a tags you need.All you need is just add h3 tag which you accidentally missed. & if you want to get only the first url then addor 

Scraping and modifying an outout

Tux Skywalker

[Scraping and modifying an outout](https://stackoverflow.com/questions/52395894/scraping-and-modifying-an-outout)

I am trying to retrieve data from this website with scraping:https://dolar.wilkinsonpc.com.co/dolar-historico/dolar-historico-2018.htmlMy parser right now looks like this:The output looks like:['<div class="dh_col_fecha">16 Septiembre 2018</div>'] ['<div class="dh_col_precio"><b>$ 3,026.05</b></div>']And I need like this:16 Septiembre 2018;3026.05I was trying to replace with w3lib and others without success. Can anyone help me?

2018-09-18 23:30:17Z

I am trying to retrieve data from this website with scraping:https://dolar.wilkinsonpc.com.co/dolar-historico/dolar-historico-2018.htmlMy parser right now looks like this:The output looks like:['<div class="dh_col_fecha">16 Septiembre 2018</div>'] ['<div class="dh_col_precio"><b>$ 3,026.05</b></div>']And I need like this:16 Septiembre 2018;3026.05I was trying to replace with w3lib and others without success. Can anyone help me?use/modify this code:If you run this code with:You will generate a report in JSON format with the structure as in the example below:Total 262 items.

Table extraction with Scrapy failing

FourZeroFive

[Table extraction with Scrapy failing](https://stackoverflow.com/questions/52395837/table-extraction-with-scrapy-failing)

I need to extract links in a table from a Website (classname internal), but I always get Crawled 0 pages, but downloader get a bunch of bytes. Already tried a lot of things, but never worked, the code above was my last try, sorry for anything, not a specialist on scrapy/xpath.

2018-09-18 23:21:41Z

I need to extract links in a table from a Website (classname internal), but I always get Crawled 0 pages, but downloader get a bunch of bytes. Already tried a lot of things, but never worked, the code above was my last try, sorry for anything, not a specialist on scrapy/xpath.In your code you don't open the specific part for "GO" To get this data you need to loadYou can find the urlpart in Parsing the result table would be the next step.

Scrapy crawler what tells me this output?

evev

[Scrapy crawler what tells me this output?](https://stackoverflow.com/questions/52362938/scrapy-crawler-what-tells-me-this-output)

My scrapy test code is the following. I tested it via scrapy shell and it worked. But now if I finally started to write a script no output appears. Is there something wrong? Thanks.my code so far:Output:



Sorry I couldn't input the shell, so I started a screenshot.So I'm not sure if it's the error or not. If it should be an error, how show I handle it?

How to get the full number out of this?

It's always rounded by 2.

Output: 0,07 instead of 0.0688852511227967

2018-09-17 07:41:02Z

My scrapy test code is the following. I tested it via scrapy shell and it worked. But now if I finally started to write a script no output appears. Is there something wrong? Thanks.my code so far:Output:



Sorry I couldn't input the shell, so I started a screenshot.So I'm not sure if it's the error or not. If it should be an error, how show I handle it?

How to get the full number out of this?

It's always rounded by 2.

Output: 0,07 instead of 0.0688852511227967Indentation in Python is very important (more info about indentation).Incorrect indentation can cause errors or wrong program execution. In your case parse() method does not belong to your class. So during execution scrapy tries to find parse() method in CryptohunterSpider and fails with error: You need to properly indent your code so parse() belongs to the class

Why can't my video crawler detect a simple video src?

Jimmy

[Why can't my video crawler detect a simple video src?](https://stackoverflow.com/questions/52326162/why-cant-my-video-crawler-detect-a-simple-video-src)

I am building a web crawler using scrapy, my spider code for detecting and retrieving videos url is:I am not getting the simple video with id=video1 and src mov_bbb.mp4 . My target is to get any and every single link inside src when a video tag exists might be on the same level or as a child and not sure where my code fails at it. Plus it is my first spider so feel free to be code critical also kindly note that the website isn't the only one i want to scrap i'm using it for testing but my code should get all srcs when video tag exists.ALso note this is not the full code __init_subclass__ , process_page and get_rules functions exist too but the shared code should be enough.I did go to the shell and type this :then:response :response.xpath("//@src").extract()my wanted src is there but do this code get me every single src? and do i want to use that method ? Example where even the last shell test fails:this is a different websitesame shell command and it failed to get the src in this block:ideally it should return blob:http://www.aljazeera.net/9765540b-1e35-4e8c-a304-9e2f9492a07e Edit:Another example of the failing case after a comment mentioned the issue was difficult to replicate:kindly check this link http://www.aljazeera.net/programs/scenarios/2018/9/13/%D8%A7%D9%84%D8%B5%D9%8A%D9%86-%D9%88%D8%A3%D9%81%D8%B1%D9%8A%D9%82%D9%8A%D8%A7-%D8%B4%D8%B1%D8%A7%D9%83%D8%A9-%D9%85%D8%AA%D8%B9%D8%AB%D8%B1%D8%A9-%D8%A3%D9%85-%D9%85%D8%AA%D8%B7%D9%88%D8%B1%D8%A9 and inspect the video in itYou will see 2 great examples of similar issues :again from this section i'd want blob:http://www.aljazeera.net/a071437f-421f-40ac-9dc7-43afb98d96a0The second example just under that bloc:I'd also want http://bc04.ajmn.me/665001584001/201809/2309/665001584001_5834579342001_5834567349001-vs.jpg?pubId=66500158;4001 from here .

2018-09-14 06:33:37Z

I am building a web crawler using scrapy, my spider code for detecting and retrieving videos url is:I am not getting the simple video with id=video1 and src mov_bbb.mp4 . My target is to get any and every single link inside src when a video tag exists might be on the same level or as a child and not sure where my code fails at it. Plus it is my first spider so feel free to be code critical also kindly note that the website isn't the only one i want to scrap i'm using it for testing but my code should get all srcs when video tag exists.ALso note this is not the full code __init_subclass__ , process_page and get_rules functions exist too but the shared code should be enough.I did go to the shell and type this :then:response :response.xpath("//@src").extract()my wanted src is there but do this code get me every single src? and do i want to use that method ? Example where even the last shell test fails:this is a different websitesame shell command and it failed to get the src in this block:ideally it should return blob:http://www.aljazeera.net/9765540b-1e35-4e8c-a304-9e2f9492a07e Edit:Another example of the failing case after a comment mentioned the issue was difficult to replicate:kindly check this link http://www.aljazeera.net/programs/scenarios/2018/9/13/%D8%A7%D9%84%D8%B5%D9%8A%D9%86-%D9%88%D8%A3%D9%81%D8%B1%D9%8A%D9%82%D9%8A%D8%A7-%D8%B4%D8%B1%D8%A7%D9%83%D8%A9-%D9%85%D8%AA%D8%B9%D8%AB%D8%B1%D8%A9-%D8%A3%D9%85-%D9%85%D8%AA%D8%B7%D9%88%D8%B1%D8%A9 and inspect the video in itYou will see 2 great examples of similar issues :again from this section i'd want blob:http://www.aljazeera.net/a071437f-421f-40ac-9dc7-43afb98d96a0The second example just under that bloc:I'd also want http://bc04.ajmn.me/665001584001/201809/2309/665001584001_5834579342001_5834567349001-vs.jpg?pubId=66500158;4001 from here .The videos loaded are heavily dependant on javascript and it seems like brightcove.com 3rd party service is used for that. If you open up network inspector when loading: http://www.aljazeera.net/programs/scenarios/2018/9/13/%D8%A7%D9%84%D8%B5%D9%8A%D9%86-%D9%88%D8%A3%D9%81%D8%B1%D9%8A%D9%82%D9%8A%D8%A7-%D8%B4%D8%B1%D8%A7%D9%83%D8%A9-%D9%85%D8%AA%D8%B9%D8%AB%D8%B1%D8%A9-%D8%A3%D9%85-%D9%85%D8%AA%D8%B7%D9%88%D8%B1%D8%A9You can see XHR request being made to retrieve some video data in json format. 

It contains image source urls, like:

To replicate this you need to take a look at the page html source. There's an id used for video:Finally you can replicate this easily in scrapy:

Scrapy: Difference between simple spider and the one with ItemLoader

F. Shahid

[Scrapy: Difference between simple spider and the one with ItemLoader](https://stackoverflow.com/questions/52330140/scrapy-difference-between-simple-spider-and-the-one-with-itemloader)

I've been working on scrapy for 3 months. for extracting selectors I use simple response.css or response.xpath..I'm asked to switch to ItemLoaders and use add_xpath add_css etc.I know how ItemLoaders work and ho convinient they are but can anyone compare these 2 w.r.t efficiency? which way is efficient and why ??

2018-09-14 10:30:59Z

I've been working on scrapy for 3 months. for extracting selectors I use simple response.css or response.xpath..I'm asked to switch to ItemLoaders and use add_xpath add_css etc.I know how ItemLoaders work and ho convinient they are but can anyone compare these 2 w.r.t efficiency? which way is efficient and why ??Item Loaders are a convenient abstraction that allows you to reuse extraction code among multiple spiders inside a given Scrapy project.Let's say you have a Scrapy project to scrape data from several e-commerces. You'll have multiple spiders (one for each website, most likely), but they will all share the same schema for the data you're extracting. Let's say your ProductItem class looks like this:Now, let's say that in some of the websites the price field contains commas as separators in the price, such as 1,459.99. If you want to get rid of that comma to standardize the output for that field, you'd have to mix formatting logic inside your spider, and that can easily lead to a mess.If you create an ItemLoader for your ProductItem class, you can define processors for each field, so that you don't have to add formatting code into your spiders. Something like this:Now, whenever you use this in your spiders:The input processor for the price field is gonna be called to format the field for you.I recommend you to read the documentation on item loaders to have a better understanding on the example I provided above.TL;DR: item loaders are a convenience that Scrapy offers to help you better organize your spiders, avoiding mixing formatting rules (for example) in your spider code.Item loaders do exactly the same thing underneath that you do when you don't use them. So for every loader.add_css/add_xpath call there will be responce.css/xpath executed. It won't be any faster and the little amount of additional work they do won't really make things any slower (especially in comparison to xml parsing and network/io load).

Trouble installing scrapy (“Segmentation fault (core dumped)”)

Sean Dezoysa

[Trouble installing scrapy (“Segmentation fault (core dumped)”)](https://stackoverflow.com/questions/52333133/trouble-installing-scrapy-segmentation-fault-core-dumped)

I'm having issues installing scrapy with and without the sudo command. scrapy -V continues to return "command not found." Does anyone recognize this error? I'm using Lubuntu 18.04, Python 2.7.15rc1, pip 9.0.1When I try to install Scrapy here are the outputs:And with sudo:I haven't had this problem with any other libraries, although I'm fairly new to Python. I also have 3.6 installed and wonder if there might be a conflict?

2018-09-14 13:33:12Z

I'm having issues installing scrapy with and without the sudo command. scrapy -V continues to return "command not found." Does anyone recognize this error? I'm using Lubuntu 18.04, Python 2.7.15rc1, pip 9.0.1When I try to install Scrapy here are the outputs:And with sudo:I haven't had this problem with any other libraries, although I'm fairly new to Python. I also have 3.6 installed and wonder if there might be a conflict?Can you try updating your pip version.latest one is pip 18.0use this command to update your pipHad this exact same problem today, although mine was with the Twisted package of scrapy. This is how i resolved it. First thing is to make sure that pip3 is installed on your deviceNext thing i did is created a virtual environment using pipenv. If you don't already have this just doNext cd into the directory you want to work in and create a virtual environmentwhen you run that it will default to creating your environment with your default available python version 3.6. Activate your virtual environmentAnd finally you do not have to add the 3 to pip this time because it is already within a virtual environment where only the python version you used to create it is available. With this you should be good to go.

How to get all xpaths that are matching given regex?

Satyaaditya

[How to get all xpaths that are matching given regex?](https://stackoverflow.com/questions/52292384/how-to-get-all-xpaths-that-are-matching-given-regex)

Is there any python library which facilitates in getting xpaths of dom nodes which matches the given regex? I am trying to fetch question and answer pair from a faq pagethese are three different xpaths of questions from this sitenow let the regex be something like this :is it possible to get all xpaths that satisfy the regex we build through some library in python?I tried using scrapy selectors to fetch all questions but it is failing while fetching the answers, so i want to go through all questions and then fetch their answers, for this I want question Xpaths 

2018-09-12 09:55:45Z

Is there any python library which facilitates in getting xpaths of dom nodes which matches the given regex? I am trying to fetch question and answer pair from a faq pagethese are three different xpaths of questions from this sitenow let the regex be something like this :is it possible to get all xpaths that satisfy the regex we build through some library in python?I tried using scrapy selectors to fetch all questions but it is failing while fetching the answers, so i want to go through all questions and then fetch their answers, for this I want question Xpaths You don't need a tool or regex (as well as absolute XPath expressions). Try to use below XPath to match all questions on page:If you don't know how to write your own selectors, check this cheatsheet Finally, I found the solution for this, with the combination of lxml and scrapy.

used @Andersson answer to find all the text content using the selector and then for each text, iterated over the tree and used tree.getpath() from lxmlThe solution is not regex based but solved my use-case, so posting it

Scrapy save complete html file [duplicate]

woopus123

[Scrapy save complete html file [duplicate]](https://stackoverflow.com/questions/52291557/scrapy-save-complete-html-file)

I am using scrapy for my crawler and it is working perfectly but i need to save the whole html file seperate from the csv file which i am writing. I don't know how to save the whole html file which I am parsing. 

2018-09-12 09:12:25Z

I am using scrapy for my crawler and it is working perfectly but i need to save the whole html file seperate from the csv file which i am writing. I don't know how to save the whole html file which I am parsing. You can do it inside the parse method of your spiderThis is the easiest way but you should read some documentation about middlewares in scrapy. Then you can create your own middleware which will save your html before parsing it. It can be a good option as you can activate/deactivate your middleware using the settings file. Have a look to the Cache middleware, it may help you to chose the right option.

differences CrawlerProcess and scrapy crawl somespider in commandline in scrapy?

pakachu

[differences CrawlerProcess and scrapy crawl somespider in commandline in scrapy?](https://stackoverflow.com/questions/52260491/differences-crawlerprocess-and-scrapy-crawl-somespider-in-commandline-in-scrapy)

case 1 : scrapy crawl somespider type several times (same time, using nohup background)case 2 : using CrawlerProcess and configure multispider in python script and runwhat is diffrences cases? i already tried case2 using 5 spiders but not so fast.

2018-09-10 14:54:09Z

case 1 : scrapy crawl somespider type several times (same time, using nohup background)case 2 : using CrawlerProcess and configure multispider in python script and runwhat is diffrences cases? i already tried case2 using 5 spiders but not so fast.scrapy crawl uses one process for each spider, while CrawlerProcess uses a single Twisted Reactor on one process (while also doing some things under the hood which I'm not so sure) to run multiple spiders at once.So, basically: 

Scrapy:Using proxy middleware settings error

M.Mark

[Scrapy:Using proxy middleware settings error](https://stackoverflow.com/questions/52259173/scrapyusing-proxy-middleware-settings-error)

I'm a scrapy newbie.I want to use proxy middleware.But my DEBUG messages show It will always crawl 0 pages and retry.My proxies are free and no authorization required.But I try to delete proxy middleware and useyield scrapy.Request(url='https://www.example.com/', callback=self.parse_first, meta=my_proxy)It's ok.It seems to be a problem with my settings.SettingsCustomProxyMiddlewareSpider

2018-09-10 13:44:26Z

I'm a scrapy newbie.I want to use proxy middleware.But my DEBUG messages show It will always crawl 0 pages and retry.My proxies are free and no authorization required.But I try to delete proxy middleware and useyield scrapy.Request(url='https://www.example.com/', callback=self.parse_first, meta=my_proxy)It's ok.It seems to be a problem with my settings.SettingsCustomProxyMiddlewareSpiderIt sounds like you haven't changed the default ROBOTXT_OBEY setting

Set ROBOTXT_OBEY = False and then try. I'll work.

Scrapy spider converts float / int to string

mrvnklm

[Scrapy spider converts float / int to string](https://stackoverflow.com/questions/52259779/scrapy-spider-converts-float-int-to-string)

I always receive a string in my result, even in exported JSON.Using double translate to replace everything. The decimal_serializer was just for testing purposes. I called print(value) inside and it returned a valid float value. In my result it's always unicode string. add_value('offerCountNew', 1.3) returns valid float value in my result.I also tried removing any processor or serializer. Any ideas on what I am doing wrong?ItemSpiderResultJSON

2018-09-10 14:14:55Z

I always receive a string in my result, even in exported JSON.Using double translate to replace everything. The decimal_serializer was just for testing purposes. I called print(value) inside and it returned a valid float value. In my result it's always unicode string. add_value('offerCountNew', 1.3) returns valid float value in my result.I also tried removing any processor or serializer. Any ideas on what I am doing wrong?ItemSpiderResultJSON

Scrapy extracting rows from table without headers

Kvothe

[Scrapy extracting rows from table without headers](https://stackoverflow.com/questions/52276179/scrapy-extracting-rows-from-table-without-headers)

So I'm trying to extract a table from a website. It's a two column table as follows:The table doesn't have headers and the "Address" row contains blank cells in the first column for second, city, postcode, etc.I've managed to get the table, just fine. table = response.xpath('//table[@id="MemberDetails"]/tr/td//text()')This is the output:However, I'm stumped as to how I can parse the table into a proper structure.1st Question: Not sure how I can deal with the address field.

2nd Question: This is a two column table. When saving this, I'd like to transpose such that, the "Name, Number, Address, Region" are column headings.There are 1000's of pages like this that contain similar data.Appreciate if someone can point me in the right direction.

2018-09-11 12:40:15Z

So I'm trying to extract a table from a website. It's a two column table as follows:The table doesn't have headers and the "Address" row contains blank cells in the first column for second, city, postcode, etc.I've managed to get the table, just fine. table = response.xpath('//table[@id="MemberDetails"]/tr/td//text()')This is the output:However, I'm stumped as to how I can parse the table into a proper structure.1st Question: Not sure how I can deal with the address field.

2nd Question: This is a two column table. When saving this, I'd like to transpose such that, the "Name, Number, Address, Region" are column headings.There are 1000's of pages like this that contain similar data.Appreciate if someone can point me in the right direction.You can generate a dictionary for all rows in your table:You can do something like this:It does not work on every situation (for example, in your link Herd Completeness of Performance Rating: label is in a <a> tag and the value is an image), but you have a beginning of solution :)Other solution:Let's work with the sample you have provided us with. (Working with this link )Let's look at how a table line is formatted.This is great for us, we have different attributes for headers and values: headers are within the strong tags within tds with bgcolor values are directly inside their tds.Let's check out how the empty lines for the address info look:Great, same structure.This means we can loop through all trs and grab their data depending on the attributes if we need to.

Here's a minimal example that does not deal with links:You next step is to collect the data (make a dictionary? separate lists? write directly to files? It's your choice.).For dealing with the address: you could make an address variable in the loop for trs that is concatenated with the value if the header is empty (assuming only the address headers are empty). Something like this (again< minimal example):You will also have to work out how to deal with headers/values that are stored within a tags.

Scrapy - use website's search engine to scrape results

Tylones

[Scrapy - use website's search engine to scrape results](https://stackoverflow.com/questions/52272406/scrapy-use-websites-search-engine-to-scrape-results)

I have to go through the results of a search on a website. The thing is that the URL doesn't change when you search for something on that site, meaning that I can't use the URL to have the results that I want to have.My question is, can Scrapy set the filters that I need, search for the results, and then go through all the results of the search ?If yes, how ? And if not do you know something that could do it using Python or something else ? Thanks

2018-09-11 09:04:46Z

I have to go through the results of a search on a website. The thing is that the URL doesn't change when you search for something on that site, meaning that I can't use the URL to have the results that I want to have.My question is, can Scrapy set the filters that I need, search for the results, and then go through all the results of the search ?If yes, how ? And if not do you know something that could do it using Python or something else ? ThanksIf the search term is not reflected in the URL, it means that it is transmitted to the server as a POST reqest. This means that your Scrapy code also needs to make a POST request in order to submit the desired search term.The Scrapy request documentation has examples for making a POST request, simulating a form submission:

How to create JOBDIR settings in Scrpay Spider dynamically?

Lax

[How to create JOBDIR settings in Scrpay Spider dynamically?](https://stackoverflow.com/questions/52219321/how-to-create-jobdir-settings-in-scrpay-spider-dynamically)

I want to create JOBDIR setting from Spider __init__ or dynamically when I call that spider .

I want to create  different JOBDIR for different spiders , like FEED_URI in the below example and we are calling that script from this way How to achieve that dynamic creation of JOBDIR for different Spider like FEED_URI ?? Help will be appreciated. 

2018-09-07 09:16:59Z

I want to create JOBDIR setting from Spider __init__ or dynamically when I call that spider .

I want to create  different JOBDIR for different spiders , like FEED_URI in the below example and we are calling that script from this way How to achieve that dynamic creation of JOBDIR for different Spider like FEED_URI ?? Help will be appreciated. Exactly how you have set the site_name, you can pass another argument,will be available as a spiders attribute so you can write

Scrapy:meta['proxies'] or meta['proxy']?

M.Mark

[Scrapy:meta['proxies'] or meta['proxy']?](https://stackoverflow.com/questions/52218418/scrapymetaproxies-or-metaproxy)

I'm newbie at scrapy.I use a custom proxy in a scrapy spider，but I find if I use 

 request.meta["proxies"] , spider will work well,rather than using request.meta['proxy'].This is different from this answerThis is part of my DEBUG messages, if I use request.meta['proxy'].My scrapy versionUpdate:I have solved the previous problem.But I don't konw why my meta['proxy'] is wrong,and my free proxy uses requests.get('https://www.example.com/', proxies={"http": "http://{}".format(proxy)}),it works well and returns <Response [200]>,so  what's wrong with my codes?My settings:My spider:My CustomProxyMiddleware

2018-09-07 08:26:01Z

I'm newbie at scrapy.I use a custom proxy in a scrapy spider，but I find if I use 

 request.meta["proxies"] , spider will work well,rather than using request.meta['proxy'].This is different from this answerThis is part of my DEBUG messages, if I use request.meta['proxy'].My scrapy versionUpdate:I have solved the previous problem.But I don't konw why my meta['proxy'] is wrong,and my free proxy uses requests.get('https://www.example.com/', proxies={"http": "http://{}".format(proxy)}),it works well and returns <Response [200]>,so  what's wrong with my codes?My settings:My spider:My CustomProxyMiddlewareTo send request with a proxy server, you should use meta['proxy']. Looks like you have some issue with your proxy server, that's why it fails to scrape a page, causing timeout error. This may also be because you use a free proxy.The reason your spider works with meta['proxies'] is that setting this element does not affect anything and requests are send from your local IP.

How to make scrapy follow invalid links?

pawelty

[How to make scrapy follow invalid links?](https://stackoverflow.com/questions/52225956/how-to-make-scrapy-follow-invalid-links)

I use scrapy very often to check long lists of links whether they're available or dead. My problem is when the link is incorrectly formatted for example doesn't start with http:// or https:// the crawler crashes. I read the list of links from pandas Series and check each of them. When the response is reachable I log it as "ok" otherwise as "dead".I am still interested in spotting those incorrectly formatted urls. How can I validate them and yield "dead" for those as well?

2018-09-07 15:52:06Z

I use scrapy very often to check long lists of links whether they're available or dead. My problem is when the link is incorrectly formatted for example doesn't start with http:// or https:// the crawler crashes. I read the list of links from pandas Series and check each of them. When the response is reachable I log it as "ok" otherwise as "dead".I am still interested in spotting those incorrectly formatted urls. How can I validate them and yield "dead" for those as well?You just check if it starts with https and http If not, then prepend http manually.

Scrapy Piplines deal with data

Peter

[Scrapy Piplines deal with data](https://stackoverflow.com/questions/52182855/scrapy-piplines-deal-with-data)

I want to write a crawler with scrapy, after I have written the crawler file, item file, piplines file, etc. I want to save the data into the database, but I find that the data type of item['url'] is actually string type, the string contains the list, I cannot get the data inside, and I don't find where the problem is.Why is item['url'] of type str? I couldn't get list data.

2018-09-05 10:25:10Z

I want to write a crawler with scrapy, after I have written the crawler file, item file, piplines file, etc. I want to save the data into the database, but I find that the data type of item['url'] is actually string type, the string contains the list, I cannot get the data inside, and I don't find where the problem is.Why is item['url'] of type str? I couldn't get list data.You should assign the url in your item parsing function as following.Use a simpler Pipeline like SQLAlchemy or dataset which is based on SQLAlchemy. example with dataset: 

SQLAlchemy/dataset how to specify created table

Scrapy: how scrape second HTML page requested through AJAX call

vferraz

[Scrapy: how scrape second HTML page requested through AJAX call](https://stackoverflow.com/questions/52128922/scrapy-how-scrape-second-html-page-requested-through-ajax-call)

I am new to scrapy and html and I'm trying to create a simple spider to scrape the https://www.mobiel.nl website. I have managed to access the mobile phones pages, e.g. https://www.mobiel.nl/smartphone/apple/iphone-6-32gbI am trying to take information on the plans, such as operator names (taken from the image names), plan names and rates, which are stored in the following containers: I have tried dozens of different ways of writhing the selectors, such as:   I couldnt find a way to get any information, except for the device name, which is : response.xpath('//*[@class="phone-info__phone"]/text()').extract_first()In the end I would like to have something likeDoes anyone know how to correctly extract (if possible) such information from this page?Thank you in advance.I tried to fetch the inner urls using scrapy_splash but still no success.Edit 2:

I have realized that: The items data-token and data-phone feed these numbers to the URL where the data points I need are requested from, so it would be the way to go trying to fetch this info and replace them in the url or is there another more adequate way of doing something like this?

2018-09-01 14:14:03Z

I am new to scrapy and html and I'm trying to create a simple spider to scrape the https://www.mobiel.nl website. I have managed to access the mobile phones pages, e.g. https://www.mobiel.nl/smartphone/apple/iphone-6-32gbI am trying to take information on the plans, such as operator names (taken from the image names), plan names and rates, which are stored in the following containers: I have tried dozens of different ways of writhing the selectors, such as:   I couldnt find a way to get any information, except for the device name, which is : response.xpath('//*[@class="phone-info__phone"]/text()').extract_first()In the end I would like to have something likeDoes anyone know how to correctly extract (if possible) such information from this page?Thank you in advance.I tried to fetch the inner urls using scrapy_splash but still no success.Edit 2:

I have realized that: The items data-token and data-phone feed these numbers to the URL where the data points I need are requested from, so it would be the way to go trying to fetch this info and replace them in the url or is there another more adequate way of doing something like this?If you check above URL with Chrome DevTools you'll find that this information is requested throught separate AJAX call to this URLThat's why your XPath expressions don't work.

scrapy xpath selector returning empty array

BigBoyProgrammer

[scrapy xpath selector returning empty array](https://stackoverflow.com/questions/52133947/scrapy-xpath-selector-returning-empty-array)

This response.xpath statement is returning an empty array and I'm not too sure why. I'm trying to retrieve @alt result. This is what I have tried...Oddly enough this returns all elements within that divthis is the website I am trying to scrape https://baseball-reference.com/teams/BOS/2018.shtml 

2018-09-02 04:21:59Z

This response.xpath statement is returning an empty array and I'm not too sure why. I'm trying to retrieve @alt result. This is what I have tried...Oddly enough this returns all elements within that divthis is the website I am trying to scrape https://baseball-reference.com/teams/BOS/2018.shtml The content you're trying to parse is hided in HTML comment section. See this answer for details

Scrap inside link of homepage by logging into the homepage

Uchiha AJ

[Scrap inside link of homepage by logging into the homepage](https://stackoverflow.com/questions/52133734/scrap-inside-link-of-homepage-by-logging-into-the-homepage)

I will make this simple. I have a login page. I login. I see the homepage. The homepage has two links. I want to open these two links. Each link has two data. I just want four data from two links which are on the homepage that also comes after logging in. I can scrap upto the link step. I can scrap link but not the data inside the link. How do I do that? Thanks My scrapy code: P.S i just did it on my own intuition, i don't know if it's possible.UPDATE  The html element is :All it scrapped was the last p element. 

2018-09-02 03:25:36Z

I will make this simple. I have a login page. I login. I see the homepage. The homepage has two links. I want to open these two links. Each link has two data. I just want four data from two links which are on the homepage that also comes after logging in. I can scrap upto the link step. I can scrap link but not the data inside the link. How do I do that? Thanks My scrapy code: P.S i just did it on my own intuition, i don't know if it's possible.UPDATE  The html element is :All it scrapped was the last p element. If you want combined output for your two links you need to use request.meta (not tested):

Scrapy - response body contains javascript and unable to xpath select

Murali Kumar

[Scrapy - response body contains javascript and unable to xpath select](https://stackoverflow.com/questions/52102076/scrapy-response-body-contains-javascript-and-unable-to-xpath-select)

So I'm having an issue with scraping this page https://www.tribeofdumo.com/nefertiti-infinity-dress. There is some javascript in the response body and I am unable to xpath select the individual products listed on that page. I'm basically trying to select the li.gallery-item which I can only see when doing an inspect element and not view source. If you do a view source on that page that is pretty much what the response.body is. Also, I guessed maybe scrapy isn't able to render the javascript, so I installed Splash and tried to render the page but even with splash I get the same output and the screenshot does not even show the products.

2018-08-30 16:54:00Z

So I'm having an issue with scraping this page https://www.tribeofdumo.com/nefertiti-infinity-dress. There is some javascript in the response body and I am unable to xpath select the individual products listed on that page. I'm basically trying to select the li.gallery-item which I can only see when doing an inspect element and not view source. If you do a view source on that page that is pretty much what the response.body is. Also, I guessed maybe scrapy isn't able to render the javascript, so I installed Splash and tried to render the page but even with splash I get the same output and the screenshot does not even show the products.

Scrapy Error 302 and Proxy issues

T the shirt

[Scrapy Error 302 and Proxy issues](https://stackoverflow.com/questions/52080940/scrapy-error-302-and-proxy-issues)

I've been trying to scrape info about articles from https://academic.oup.com/ilarjournal, with the following code:The settings for proxies are the following:However, when I try running the code, it gets all the urls, but seems not to produce any items. The shell just keeps printing these errors:2018-08-29 16:53:38 [scrapy.proxies] DEBUG: Using proxy http://103.203.133.170:8080, 8 proxies left2018-08-29 16:53:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to https://academic.oup.com/ilarjournal/article/53/1/E99/656113> from https://academic.oup.com/ilarjournal/article-abstract/53/1/E99/656113>2018-08-29 16:53:38 [scrapy.proxies] DEBUG: Proxy user pass not foundAnother probably important thing is that I have tried using the spider without proxies, and it still returns 302 errors for all the articles. Would appreciate any ideas what might be wrong or if there is already a good solution on another topic.

2018-08-29 15:16:02Z

I've been trying to scrape info about articles from https://academic.oup.com/ilarjournal, with the following code:The settings for proxies are the following:However, when I try running the code, it gets all the urls, but seems not to produce any items. The shell just keeps printing these errors:2018-08-29 16:53:38 [scrapy.proxies] DEBUG: Using proxy http://103.203.133.170:8080, 8 proxies left2018-08-29 16:53:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to https://academic.oup.com/ilarjournal/article/53/1/E99/656113> from https://academic.oup.com/ilarjournal/article-abstract/53/1/E99/656113>2018-08-29 16:53:38 [scrapy.proxies] DEBUG: Proxy user pass not foundAnother probably important thing is that I have tried using the spider without proxies, and it still returns 302 errors for all the articles. Would appreciate any ideas what might be wrong or if there is already a good solution on another topic.30x codes are normal redirects and you should allow them to happen.Seems like your parse_item method is returning value instead of yielding, try replacing return item with yield item.

Beginner Help - Scrapping Ecommerce websites

mark0018

[Beginner Help - Scrapping Ecommerce websites](https://stackoverflow.com/questions/52034425/beginner-help-scrapping-ecommerce-websites)

I am new to scrapy and web crawling. I am trying to scrap data from ecommerce sites in India and am unable to do so. I am trying to pull the text out of the following hyperlink:Xpath/Css Selectors are not working for me.Appreciate any help.

2018-08-27 07:21:35Z

I am new to scrapy and web crawling. I am trying to scrap data from ecommerce sites in India and am unable to do so. I am trying to pull the text out of the following hyperlink:Xpath/Css Selectors are not working for me.Appreciate any help.To get the text from the a tag I would use the following css selector Response: 

Scrapy Error : HTTP status code is not handled or not allowed

kevin suryanto

[Scrapy Error : HTTP status code is not handled or not allowed](https://stackoverflow.com/questions/52034814/scrapy-error-http-status-code-is-not-handled-or-not-allowed)

i have a problem when run the spider . when i crawl it , it's shown eror like this : "HTTP status code is not handled".i have follow another instruction for edit setting.py and add code : but its still not working .this is my code : 

2018-08-27 07:47:13Z

i have a problem when run the spider . when i crawl it , it's shown eror like this : "HTTP status code is not handled".i have follow another instruction for edit setting.py and add code : but its still not working .this is my code : Your selector gets to much from the url: So the a href and the name is included in the link. 

Slicing out only the link part: and using this selector in your code and you get: 

How to make scrapy continue downloading after losing connection fo a while

rikotakson

[How to make scrapy continue downloading after losing connection fo a while](https://stackoverflow.com/questions/52035928/how-to-make-scrapy-continue-downloading-after-losing-connection-fo-a-while)

I am trying to scrape a website using scrapy, but the network in the office is unstable. If we lose network connection for even a few seconds, scrapy gets stuck and stops downloading. we can see that the last log is:I had tried to change the timeout settings, but nothing happened.

thank you!

2018-08-27 09:06:15Z

I am trying to scrape a website using scrapy, but the network in the office is unstable. If we lose network connection for even a few seconds, scrapy gets stuck and stops downloading. we can see that the last log is:I had tried to change the timeout settings, but nothing happened.

thank you!You can try to set RETRY_TIMES setting (in settings.py):

Scrapy post request getting redirected to a wrong page

Billy Jhon

[Scrapy post request getting redirected to a wrong page](https://stackoverflow.com/questions/51984464/scrapy-post-request-getting-redirected-to-a-wrong-page)

I am trying to get to details page of  this siteTo get there from the web one should click 1. Consula Titlulo 2. Select ORO from Minerals dropdown and 3. click Buscar. 4. Then click the very first item in the list.Dev tools and Fiddler show that I should make POST request with item id as a payload and this POST request is then redirected to details page. In my case Im being redirected to homepage. What do I miss?Here is my Scrapy spider.Here is the log with redirect.From what I see scrapy also assigns correct Cookie before sending request.What do I miss?Moreover if I use Postman code with GET for details page it works fine and returns the page.

Same code in Scrapy redirects.

2018-08-23 11:12:27Z

I am trying to get to details page of  this siteTo get there from the web one should click 1. Consula Titlulo 2. Select ORO from Minerals dropdown and 3. click Buscar. 4. Then click the very first item in the list.Dev tools and Fiddler show that I should make POST request with item id as a payload and this POST request is then redirected to details page. In my case Im being redirected to homepage. What do I miss?Here is my Scrapy spider.Here is the log with redirect.From what I see scrapy also assigns correct Cookie before sending request.What do I miss?Moreover if I use Postman code with GET for details page it works fine and returns the page.

Same code in Scrapy redirects.It appears that I missed POST request in the very beggining. This post request generates correct session ID which is to be new for every other search.

How to scrap a container in website with varying content?

Uchiha AJ

[How to scrap a container in website with varying content?](https://stackoverflow.com/questions/52031397/how-to-scrap-a-container-in-website-with-varying-content)

I want to scrap this website. 

https://www.dhgate.com/wholesale/electronics-robots/c103032.htmlI have built a scrapy code:The problem is that not every container has the customer review or feedback item. So, it only scraps those which has the complete Product, price, customer_review, seller and feedback items. I want to scrap all the container and where ever there is no customer_review, i want to add a null value. How do i do that? Thanks.

2018-08-27 00:59:26Z

I want to scrap this website. 

https://www.dhgate.com/wholesale/electronics-robots/c103032.htmlI have built a scrapy code:The problem is that not every container has the customer review or feedback item. So, it only scraps those which has the complete Product, price, customer_review, seller and feedback items. I want to scrap all the container and where ever there is no customer_review, i want to add a null value. How do i do that? Thanks.Don't use zip:

Twisted ResponseFailed error when using scrapy shell

Aleksander bury

[Twisted ResponseFailed error when using scrapy shell](https://stackoverflow.com/questions/51989991/twisted-responsefailed-error-when-using-scrapy-shell)

I'm hitting the error when running scrapy shell https://www.macupdate.com while other websites are being scraped fine. I'm trying to debug it but I have no idea what exactly is causing this error.Really appreciate the help!

2018-08-23 16:04:04Z

I'm hitting the error when running scrapy shell https://www.macupdate.com while other websites are being scraped fine. I'm trying to debug it but I have no idea what exactly is causing this error.Really appreciate the help!

Scrapy - Grab Child Element - Grabing Wrong part of CSS Selector

0004

[Scrapy - Grab Child Element - Grabing Wrong part of CSS Selector](https://stackoverflow.com/questions/51941013/scrapy-grab-child-element-grabing-wrong-part-of-css-selector)

I am trying to grab " "Brick Bank" from the tag belowwith my Scrapy Spider object below:

        import scrapybut I am getting(as you can see below)- 'Name: 10251', not Brick-Bank? Very new to this, so not sure why - I am following a tutorial that returned the correct name

2018-08-21 03:05:41Z

I am trying to grab " "Brick Bank" from the tag belowwith my Scrapy Spider object below:

        import scrapybut I am getting(as you can see below)- 'Name: 10251', not Brick-Bank? Very new to this, so not sure why - I am following a tutorial that returned the correct namethis is a common mistake for css selectors where the spaces are very important for the actual result of the selector.That says to get text from all inner elements below a, but what you need is:That says to only get the text element from the a tag

Scrapy + Ubuntu VPS return wrong HTML

User Name

[Scrapy + Ubuntu VPS return wrong HTML](https://stackoverflow.com/questions/51991170/scrapy-ubuntu-vps-return-wrong-html)

Made a spider on Scrapy 1.5.1, it works correctly on a personal computer. The spider is transferred to the VPS: Versions:The spider works through a proxy - https://github.com/aivarsk/scrapy-proxiesThe problem is the following: If you try to crawl with VPS, the pages of the site, like example.com/catalog/*** scrapy, return an incorrect html.While requests from a personal computer are correct and the correct HTML comes. Requests from VPS to the main page of the site or other pages outside the catalog "/catalog/" also return the correct html.How to solve the problem?my_spider.pysettings.pyI can give a link to the site

2018-08-23 17:20:26Z

Made a spider on Scrapy 1.5.1, it works correctly on a personal computer. The spider is transferred to the VPS: Versions:The spider works through a proxy - https://github.com/aivarsk/scrapy-proxiesThe problem is the following: If you try to crawl with VPS, the pages of the site, like example.com/catalog/*** scrapy, return an incorrect html.While requests from a personal computer are correct and the correct HTML comes. Requests from VPS to the main page of the site or other pages outside the catalog "/catalog/" also return the correct html.How to solve the problem?my_spider.pysettings.pyI can give a link to the site

How to scrap next page link in Alibaba

Uchiha AJ

[How to scrap next page link in Alibaba](https://stackoverflow.com/questions/51913904/how-to-scrap-next-page-link-in-alibaba)

My code for scraping data from single page of Alibaba is this one:I want to scrap data from all the pages how do i do that cause it is javascript action while clicking the next page. I have multiple links not just this one. So i want some way that it can go through next page until last page and scrap data from it. The html snippet is this one:

2018-08-19 02:12:53Z

My code for scraping data from single page of Alibaba is this one:I want to scrap data from all the pages how do i do that cause it is javascript action while clicking the next page. I have multiple links not just this one. So i want some way that it can go through next page until last page and scrap data from it. The html snippet is this one:All addresses have a pattern https://www.alibaba.com/showroom/acrylic-wine-box_(page).html, you can modify the page field to scrape different pages.

Python scrapy start_urls

mrWiga

[Python scrapy start_urls](https://stackoverflow.com/questions/51869266/python-scrapy-start-urls)

is it possible to do something like below but with multiple url like below?

Each link will have about 50 pages to crawl and loop. The current solution is working but only working if I use 1 URL instead of multiple urls.

2018-08-16 03:37:23Z

is it possible to do something like below but with multiple url like below?

Each link will have about 50 pages to crawl and loop. The current solution is working but only working if I use 1 URL instead of multiple urls.I recommend to use start_requests for this:We can perform the operation by using another list. I've shared the code for it below. Hope this is what you're looking for.About your latest enquiry, have you tried this?

Python/Scrapy How to go into a deeper link and go back

itsBlooDy

[Python/Scrapy How to go into a deeper link and go back](https://stackoverflow.com/questions/51867348/python-scrapy-how-to-go-into-a-deeper-link-and-go-back)

I am trying to scrap information about every firm in from this website : www.canadianlawlist.comI have finished most of it, but I am running into a small problem.I am trying to get the results to display in the following order :But instead I am getting very random results.It will scrape information about 2 firms and then scrap the information of employees. Like that :It goes something like that . I am not sure what i am missing in my code :

2018-08-15 22:33:05Z

I am trying to scrap information about every firm in from this website : www.canadianlawlist.comI have finished most of it, but I am running into a small problem.I am trying to get the results to display in the following order :But instead I am getting very random results.It will scrape information about 2 firms and then scrap the information of employees. Like that :It goes something like that . I am not sure what i am missing in my code :You cannot rely on the order Python's print function when it comes to concurrent programming. If you care about standard output order you need to use logging module.Scrapy has shortcut function for that in Spider class:Scrapy run multiple requests at the same time, so the content displayed on the console can be corresponding to any the multiple requests running at same time. 

You can go to settings.py and set Now only one request will be launched at a time so your console will show meaningful data but this will make the scraping slower. 

Scrapy POST request for load more button

chiff

[Scrapy POST request for load more button](https://stackoverflow.com/questions/51838422/scrapy-post-request-for-load-more-button)

I'm trying to scrape this for product names and prices.There's a load more button at the bottom of the page, I've tried using postman to modify the form data and 'productBeginIndex': and 'resultsPerPage': seem to modify the number of products shown. However, I'm unsure what's wrong with my code - it still returns the 24 products no matter how I tweak the values. I've tried using FormRequest.from_response() but it still just returns 24 products.Could someone please tell me what I'm missing? And how could I implement a loop to extract all the objects in the category?Thank you so much!

2018-08-14 09:41:04Z

I'm trying to scrape this for product names and prices.There's a load more button at the bottom of the page, I've tried using postman to modify the form data and 'productBeginIndex': and 'resultsPerPage': seem to modify the number of products shown. However, I'm unsure what's wrong with my code - it still returns the 24 products no matter how I tweak the values. I've tried using FormRequest.from_response() but it still just returns 24 products.Could someone please tell me what I'm missing? And how could I implement a loop to extract all the objects in the category?Thank you so much!You should post to (get method will work, too) /ProductListingView instead of /baby-child.To scrape all objects, modify parameters beginIndex in the loop and yield a new request.(By the way, modify productBeginIndex will not work)We don't know the total number of products, so a safe way is to crawl a group of products every time. By modifying custom_settings, you can easily control where to begin and how many products to scrape.As for how to output to CSV format file, refer to Scrapy pipeline to export csv file in the right formatFor convenience, I add the PriceItem class below, you may add it to items.py. Using command scrapy crawl PriceSpider -t csv -o test.csv, you will get a test.cvs file. Or, you can try CSVItemExporter

How do I get sentences including links?

古谷賢士

[How do I get sentences including links?](https://stackoverflow.com/questions/51841033/how-do-i-get-sentences-including-links)

I would like to collect the Japanese articles searched by google. I try to extract Japanese sentences, then I run the following code in order to get the tag including the most Japanese words.But, this code has a problem in such cases as the article has a link between sentences as below.In this case, my program get AC but what I want is ABC. I appreciate it if anyone tell me how to get the sentence as 'ABC'.

2018-08-14 12:06:08Z

I would like to collect the Japanese articles searched by google. I try to extract Japanese sentences, then I run the following code in order to get the tag including the most Japanese words.But, this code has a problem in such cases as the article has a link between sentences as below.In this case, my program get AC but what I want is ABC. I appreciate it if anyone tell me how to get the sentence as 'ABC'.You can try to use string():Or use html2text

Python - Scrapy crawling the web

Baggelis Kotsifakhs

[Python - Scrapy crawling the web](https://stackoverflow.com/questions/51839299/python-scrapy-crawling-the-web)

well here is my project/spider , works fine....check out the url inside of image because i cannot use tiny url inside this post

URL Image

but i wanted to scrape daten,cartext,spacerimg aswell im gonna get different/bad results 

    btw in settings.py i did like that to export into csv file:so, my question is why i cannot scrape like my image when im adding "daten,cartext,spacerimg"?

if i did scrape all of them together im gonna get in csv just 1 row with all of the informations in 1 cell and if ill remove the "daten,cartext,spacerimg from the loop", ill get the perfect results....hope this make sense...

2018-08-14 10:28:24Z

well here is my project/spider , works fine....check out the url inside of image because i cannot use tiny url inside this post

URL Image

but i wanted to scrape daten,cartext,spacerimg aswell im gonna get different/bad results 

    btw in settings.py i did like that to export into csv file:so, my question is why i cannot scrape like my image when im adding "daten,cartext,spacerimg"?

if i did scrape all of them together im gonna get in csv just 1 row with all of the informations in 1 cell and if ill remove the "daten,cartext,spacerimg from the loop", ill get the perfect results....hope this make sense...You're trying to zip differently sized lists: prodname, artnr, avaible, price have 41 elements but daten and cartext have only 1 element and spacerimg is 9 elements.

How do I eliminate the spaces?

古谷賢士

[How do I eliminate the spaces?](https://stackoverflow.com/questions/51844627/how-do-i-eliminate-the-spaces)

I would like to collect the Japanese articles searched by google. I try to extract Japanese sentences, then I run the following code in order to get the tag including the most Japanese words.but when I run this code, extracted sentence has spaces in their head.

For example, If html is as below and path is '//p',I got the sentences as below.I tried to eliminate this spaces by method 'text.strip()', but the spaces remained.How do I get the 'AB' from this html? Or how do I eliminate the spaces? I appreciate it if anyone tell me how to get 'AB'.

2018-08-14 15:10:20Z

I would like to collect the Japanese articles searched by google. I try to extract Japanese sentences, then I run the following code in order to get the tag including the most Japanese words.but when I run this code, extracted sentence has spaces in their head.

For example, If html is as below and path is '//p',I got the sentences as below.I tried to eliminate this spaces by method 'text.strip()', but the spaces remained.How do I get the 'AB' from this html? Or how do I eliminate the spaces? I appreciate it if anyone tell me how to get 'AB'.This can be done with a regular expression:

Scrapy returns garbage data such as spaces and newlines. How shall I filter these?

darrahts

[Scrapy returns garbage data such as spaces and newlines. How shall I filter these?](https://stackoverflow.com/questions/51846013/scrapy-returns-garbage-data-such-as-spaces-and-newlines-how-shall-i-filter-thes)

I wrote a spider and it returns me data which is littered with spaces and newline characters. The newline characters also caused extract() method to return as a list. How do I filter these before it touch the selector? Filtering these after extract() is called breaks the DRY principle as there are a lot of data from a page I need to extract that is attributeless which makes the only way to parse it is through indexing.How do I filter these?Sourceit returns bad data like thisEdit: The link to source code is different from the time of posting, to see the code back then take a look at commit faae4aff1f998f5589fab1616d21c7afc69e03eb

2018-08-14 16:32:16Z

I wrote a spider and it returns me data which is littered with spaces and newline characters. The newline characters also caused extract() method to return as a list. How do I filter these before it touch the selector? Filtering these after extract() is called breaks the DRY principle as there are a lot of data from a page I need to extract that is attributeless which makes the only way to parse it is through indexing.How do I filter these?Sourceit returns bad data like thisEdit: The link to source code is different from the time of posting, to see the code back then take a look at commit faae4aff1f998f5589fab1616d21c7afc69e03ebLooking at your code, you could try using xpaths normalize-spacemal_item['aired'] = border_class.xpath('normalize-space(.//div[11]/text())').extract()*untested, but seems legit. For a more general answer, yourString.strip('someChar') or yourString.replace('this','withThis') works well (but in the case of operating with json objects it might not be as efficient as other approaches). If those characters are present in the original data, you need to manually remove them or skip them. It is not the line breaks that is a cause of such behavior but the way nodes appear in the document tree. Text nodes which are separated by element nodes such as for example <a>, <br>, <hr> are seen as separate entities and scrappy will yield them as such (in fact extract() is supposed to always return a list, even when only a single node was selected). XPath has several basic value processing / filtering functions but it's very limited.You seem convinced that the only correct way to filter these outputs is by doing it within the selector expression. But it's no use to be so stringent about the principles, you are selecting text nodes from inside your target nodes, and these are bound to have excessive whitespace or be scattered all around their containers. XPath filtering by content is very sluggish and as such it should be done outside of it. Post processing scraped fields is a common practice. You might want to read about scrapy loaders and processors.Otherwise the simplest way is:

How to give command line args to scrapy?

Arpan Shah

[How to give command line args to scrapy?](https://stackoverflow.com/questions/51852787/how-to-give-command-line-args-to-scrapy)

I want to give command line args to scrapy and use that sys.argv[] in spider to check which urls have that argument. How can I do like this for spider named urls?$scrapy crawl urls "August 01,2018"?

2018-08-15 04:04:25Z

I want to give command line args to scrapy and use that sys.argv[] in spider to check which urls have that argument. How can I do like this for spider named urls?$scrapy crawl urls "August 01,2018"?You can pass arguments to a spider's __init__() by using -a, as specified in the docs: https://doc.scrapy.org/en/latest/topics/spiders.html#spider-argumentsThe default method will make all of the arguments into spider attributes, but you can also create a custom one if you need to do something with them.

Python/Scrapy Go to a different URL

itsBlooDy

[Python/Scrapy Go to a different URL](https://stackoverflow.com/questions/51848854/python-scrapy-go-to-a-different-url)

So I am working on a small project with scrapy and I am kinda new to python and scrapy.I am trying to scrap information about every firm on this URL.https://www.canadianlawlist.com I create a form request and get results : https://www.canadianlawlist.com/searchresult/?searchtype=firms&city=MontrealNow I want for each results to scrap the basic data like Name/Phone etc.But the link changes to something completly else.Something like https://www.canadianlawlist.com/listingdetail/company/zsa-legal-recruitment-673544/ . I tried having a basic url : canadianlawlist.com and then appending the href like that : but I am using python 2 and I can use urljoin(I think)?Any ideas how I can proceed ?

2018-08-14 19:52:53Z

So I am working on a small project with scrapy and I am kinda new to python and scrapy.I am trying to scrap information about every firm on this URL.https://www.canadianlawlist.com I create a form request and get results : https://www.canadianlawlist.com/searchresult/?searchtype=firms&city=MontrealNow I want for each results to scrap the basic data like Name/Phone etc.But the link changes to something completly else.Something like https://www.canadianlawlist.com/listingdetail/company/zsa-legal-recruitment-673544/ . I tried having a basic url : canadianlawlist.com and then appending the href like that : but I am using python 2 and I can use urljoin(I think)?Any ideas how I can proceed ?Scrapy have response.urljoin() for this task:If you use Scrapy version >= 1.4, there is pretty handy response.follow method described here:https://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.TextResponse.followMoreover Scrapy has more effective mechanism to extract links called LinkExtractor https://doc.scrapy.org/en/latest/topics/link-extractors.html It accepts not only xpaths but css selectors and extracts all the nested links for them (and have many more handy features).Combining both of above your Spider could look like:Or using CrawlSpider https://doc.scrapy.org/en/latest/topics/spiders.html#crawlspider it could be even more elegant:

How to extract an item from a javascript event in scrapy?

Ibraheem Nadeem

[How to extract an item from a javascript event in scrapy?](https://stackoverflow.com/questions/51766848/how-to-extract-an-item-from-a-javascript-event-in-scrapy)

This is the entire code. I cannot retrieve the color of the products as they are dynamic. Take a look at this URl for example: https://www.woolrich.com/mens-wool-stag-shirt-jac-6138/   It has multiple colors. Need the name of the colors only. 

2018-08-09 12:18:44Z

This is the entire code. I cannot retrieve the color of the products as they are dynamic. Take a look at this URl for example: https://www.woolrich.com/mens-wool-stag-shirt-jac-6138/   It has multiple colors. Need the name of the colors only. This solved the problem. I overlooked this line in the HTML

print blank in Web scraping

Dnet Vaggos

[print blank in Web scraping](https://stackoverflow.com/questions/51759075/print-blank-in-web-scraping)

Lets say i have the following script:and I wanted to say in looping system if does not exist item[1] then print blank, actually i don't know how to do that...and my script just skiping if all the product does not have price

2018-08-09 04:53:12Z

Lets say i have the following script:and I wanted to say in looping system if does not exist item[1] then print blank, actually i don't know how to do that...and my script just skiping if all the product does not have priceIf you want to send item with empty value instead of skipping, you can do it like thisIt is not usual to take extract_first() and do only single page requests. may be you better post more of spider code? Ok, more important thing 

could you show debug print of _scraped_info_ ? The output do not make big sense {'description': 'f', 'img': 'r', 'price': '0', 'prodname': '0'}rather you've stop truncate items with [indexes] and you'v got 

{'description': 'ref: 081097B\r\nMaterial: POLYURETHANE (PUR/PU)\r\nHardness 80ShA\r\nPcs/prod: 1\r\nRequired/car: 2\r\nTo every product we add grease!',

 'img': 'acura-integra-93-01_files/front-anti-roll-bar-bush.jpg',

 'price': '10,01 €',

 'prodname': '081097B: Front anti roll bar bush'}so, this way  scraped_info = {

 'prodname' : prodname,

 'price' : price,

 'description' : description,

 'img' : img,

 }NotaBene: the problem  seems to be a test task from employer, isn't it?  If it is the case sorry for late answer. good luck!

Is there a pipeline concept from within ScrapyD?

Apol

[Is there a pipeline concept from within ScrapyD?](https://stackoverflow.com/questions/51768719/is-there-a-pipeline-concept-from-within-scrapyd)

Looking at the documentation for scrapy and scrapyD it appears the only way you can write the result of a scrape is to write the code in the pipeline of the spider itself. I am being told by my colleagues that there is an additional way whereby I can intercept the result of the scrape from within scrapyD!!Has anyone heard of this and if so can someone shed some light on this for me please?Thanksitem exportersfeed exportsscrapyd config

2018-08-09 13:44:47Z

Looking at the documentation for scrapy and scrapyD it appears the only way you can write the result of a scrape is to write the code in the pipeline of the spider itself. I am being told by my colleagues that there is an additional way whereby I can intercept the result of the scrape from within scrapyD!!Has anyone heard of this and if so can someone shed some light on this for me please?Thanksitem exportersfeed exportsscrapyd configScrapyd is indeed a service that can be used to schedule crawling processes of your Scrapy application through JSON API. It also permits the integration of Scrapy with different frameworks such as Django, see this guide in case you are interested.Here is the documentation of Scrapyd.However if your doubt is about saving the result of your scraping, the standard way is to do so in the pipelines.py file of your Scrapy applicaiton.An example:Remember to define which pipeline are you using in your Scrapy application settings.py:Source: https://doc.scrapy.org/en/latest/topics/item-pipeline.html 

Scrapy for trying to comple a login form at freelancer

Franco Gil

[Scrapy for trying to comple a login form at freelancer](https://stackoverflow.com/questions/51718745/scrapy-for-trying-to-comple-a-login-form-at-freelancer)

I'm trying to log in to a website login using Scrapy and then become inside at main. I have successfully done this for other websites but I'm not sure why this time does not working.I'm using this spider.After that, if I want to add some code lines at my 'after_login' function, doesn't write that I expected. This page haven't a csrf token then, the formdata obtain just user and password.This is a partial image of my command prompt.

2018-08-07 04:16:50Z

I'm trying to log in to a website login using Scrapy and then become inside at main. I have successfully done this for other websites but I'm not sure why this time does not working.I'm using this spider.After that, if I want to add some code lines at my 'after_login' function, doesn't write that I expected. This page haven't a csrf token then, the formdata obtain just user and password.This is a partial image of my command prompt.

Scrapy Xpath getting the correct pagination

Bas Mienis

[Scrapy Xpath getting the correct pagination](https://stackoverflow.com/questions/51721553/scrapy-xpath-getting-the-correct-pagination)

First of all thank you if you are reading this.I have been scraping away for some time to collect minor data, however I want to pull in some additional information but I got stuck on a pagination.I would like to get the data-href of the link, however it needs to consist the i have been using [contains()] when however how do you get data-href when i needs to contain an object with a specific classI have been using the following:which works but not for the correct data-hrefMany thanks for the helpFull source code:

2018-08-07 07:49:37Z

First of all thank you if you are reading this.I have been scraping away for some time to collect minor data, however I want to pull in some additional information but I got stuck on a pagination.I would like to get the data-href of the link, however it needs to consist the i have been using [contains()] when however how do you get data-href when i needs to contain an object with a specific classI have been using the following:which works but not for the correct data-hrefMany thanks for the helpFull source code:Huh... Turned out to be such a simple case (:Your mistake is .extract_first() while you should extract last item to get next page.This will do the trick. But I'd recommend you to extract all the links from pagination list, since scrapy is managing duplication crawling. This will do a better job and having less chances for mistake:And so on..try with that :I'd suggest you to make sure that your element exists in initial html first:just Ctlr+U in Chrome and then Ctrl+F to find element..If element can be found there - something's wrong with your xpath selector.

Else element is generated by javascript and you have to use another way to get the data.PS. You shouldn't use Chrome Devtools "Elements" tab to check if element exists or not, because this tab contains elements with JS code already applied. So check source only(ctrl+U)

Scrapy not able to go to next page

Niteya Shah

[Scrapy not able to go to next page](https://stackoverflow.com/questions/51677662/scrapy-not-able-to-go-to-next-page)

I am learning how to use Scrappy and am trying to make a crawler that scrapes website link and text from it. My crawler works for http://quotes.toscrape.com/ and http://books.toscrape.com/ but not for real life examples like https://pypi.org/project/wikipedia/ or Wikipedia. I am not able to understand what is causing this. Please help meMyCodeI am running scrappy from Atom Hydrogen.EditI changed the dupe filter class and tried to make some changes to my link gatherer from https://blog.siliconstraits.vn/building-web-crawler-scrapy/ but it still isnt working .

2018-08-03 17:42:47Z

I am learning how to use Scrappy and am trying to make a crawler that scrapes website link and text from it. My crawler works for http://quotes.toscrape.com/ and http://books.toscrape.com/ but not for real life examples like https://pypi.org/project/wikipedia/ or Wikipedia. I am not able to understand what is causing this. Please help meMyCodeI am running scrappy from Atom Hydrogen.EditI changed the dupe filter class and tried to make some changes to my link gatherer from https://blog.siliconstraits.vn/building-web-crawler-scrapy/ but it still isnt working .It's crawling, but stops because you are sending requests for the same page (#content). Scrapy has DupeFilter enabled by default. 

Global ItemLoader - Share among multiplespiders

gnark

[Global ItemLoader - Share among multiplespiders](https://stackoverflow.com/questions/51680112/global-itemloader-share-among-multiplespiders)

I'm fairly new to Scrapy/Python.I want to crawl several websites but I'll get only three items from each website "date" "cota" and "name" which are updated daily and have always the same xpathAfter scraping all that, I want export to a csv file, but with my code I get the 

following formattingBut I would like to have something like thisI asked specifically about sharing the same ItemLoader among multiple spiders because that's what came to mind, but I'm open to other alternatives.That's the script that I have so far for two websites, I'll add more spiders later on:By the way, with the code like that, is there any chance of getting the values mixed up given that scrapy is asynchronous?Another way I was trying was with the following code, but add_value is substituing the old value in the ItemLoader, couldn't figure out why.

So it's only returning the values from the last website.

I'd rather use the first code because it allowes me to use different kinds of spiders and for one of the websites I'll probably need to use Selenium.

2018-08-03 21:10:51Z

I'm fairly new to Scrapy/Python.I want to crawl several websites but I'll get only three items from each website "date" "cota" and "name" which are updated daily and have always the same xpathAfter scraping all that, I want export to a csv file, but with my code I get the 

following formattingBut I would like to have something like thisI asked specifically about sharing the same ItemLoader among multiple spiders because that's what came to mind, but I'm open to other alternatives.That's the script that I have so far for two websites, I'll add more spiders later on:By the way, with the code like that, is there any chance of getting the values mixed up given that scrapy is asynchronous?Another way I was trying was with the following code, but add_value is substituing the old value in the ItemLoader, couldn't figure out why.

So it's only returning the values from the last website.

I'd rather use the first code because it allowes me to use different kinds of spiders and for one of the websites I'll probably need to use Selenium.Looks like it was a problem due to how windows is interpreting the newline in the csv file.

I solved by using the following code instead of FEED_EXPORT:the newline = '\n' solves the issues.The end code became:

Scrapy do not get setting fetch from curl scrapyd settings

Muoi Nguyen

[Scrapy do not get setting fetch from curl scrapyd settings](https://stackoverflow.com/questions/51688326/scrapy-do-not-get-setting-fetch-from-curl-scrapyd-settings)

I want to send settings to the crawler by:Or using scrapyd python api:And then from within spider:However, I always get request_user==None, which mean that crawler never receive setting from scrapyd schedule. What is wrong?Thanks in advance

2018-08-04 18:06:23Z

I want to send settings to the crawler by:Or using scrapyd python api:And then from within spider:However, I always get request_user==None, which mean that crawler never receive setting from scrapyd schedule. What is wrong?Thanks in advancerequest_user is a spider variable, not a setting. So you can pass it to spider like this:Scrapyd JSON API:Scrapyd python API:More information:Schedule in Scrapyd JSON API: https://scrapyd.readthedocs.io/en/stable/api.html#schedule-jsonSchedule in Scrapyd python API: https://python-scrapyd-api.readthedocs.io/en/latest/usage.html#schedule-a-job-to-run

How to use allowed_domains with relative paths?

math

[How to use allowed_domains with relative paths?](https://stackoverflow.com/questions/51695897/how-to-use-allowed-domains-with-relative-paths)

I'm new to scrapy and want to build a simple web crawler. Unfortunately, if I'm using a allowed_domain, scrapy filters out all subpages as the domain is using relative path. How can this be fixed?If I remove the allowed_domains all subpages are crawled. However, if I'm using a allowed domain all subpages are getting filtered because of the relative path issue. Can this be solved?

2018-08-05 15:56:54Z

I'm new to scrapy and want to build a simple web crawler. Unfortunately, if I'm using a allowed_domain, scrapy filters out all subpages as the domain is using relative path. How can this be fixed?If I remove the allowed_domains all subpages are crawled. However, if I'm using a allowed domain all subpages are getting filtered because of the relative path issue. Can this be solved?Allowed domains should not contain www. and such.If you take a look at OffsiteMiddleware it renders all values in allowed_domains to regex and then matches every page you try to crawl to this regegular expression:Regular expression allows subdomains so you can easily have allowed_domains=['example.com', 'foo.example.com']. If you leave in www. scrapy thinks it's a subdomain thus it will fail on urls that do not have it.

Using Django's app model in scrapy pipeline gives 'Apps aren't loaded yet' exception

Vidyashree Boragalli

[Using Django's app model in scrapy pipeline gives 'Apps aren't loaded yet' exception](https://stackoverflow.com/questions/51654546/using-djangos-app-model-in-scrapy-pipeline-gives-apps-arent-loaded-yet-excep)

I want to use Django's(version-2.2) Model in scrapy's pipeline.py. I followed these links,and added line 'django.setup()' in settings.py of scrpay project.

It gives me following error,Please help to resolve this.

2018-08-02 13:11:16Z

I want to use Django's(version-2.2) Model in scrapy's pipeline.py. I followed these links,and added line 'django.setup()' in settings.py of scrpay project.

It gives me following error,Please help to resolve this.The error says what you have to do.Since your Django project is configured to use MySQL as a backing database, you'll need to install the mysqlclient package (with e.g. pip install mysqlclient) in the environment you're running your Scrapy project in.

(Python) Interacting with Ajax webpages via Scrapy

Justin Simpson

[(Python) Interacting with Ajax webpages via Scrapy](https://stackoverflow.com/questions/51658982/python-interacting-with-ajax-webpages-via-scrapy)

System: Windows 10, Python 2.7.15, Scrapy 1.5.1Goal: Retrieve text from within html markup for each of the link items on the target website, including those revealed (6 at a time) via the '+ SEE MORE ARCHIVES' button.Target Website: https://magic.wizards.com/en/content/deck-lists-magic-online-products-game-infoInitial Progress: Python and Scrapy successfully installed. The following code......successfully produces the following results (when -o to .csv)...However, the spider will not touch any of the the info buried by the Ajax button. I've done a fair amount of Googling and digesting of documentation, example articles, and 'help me' posts. I am under the impression that to get the spider to actually see the ajax-buried info, that I need to simulate some sort of request. Variously, the correct type of request might be something to do with XHR, a scrapy FormRequest, or other. I am simply too new to web archetecture in general to be able to surmise the answer.I hacked together a version of the initial code that calls a FormRequest, which seems to be able to still reach the initial page just fine, yet incrementing the only parameter that appears to change (when inspecting the xhr calls sent out when physically clicking the button on the page) does not appear to have an effect. That code is here......and the results are the same as before, except the 6 output lines are repeated, as a block, 9 extra times.Can anyone help point me to what I am missing? Thank you in advance.Postscript: I always seem to get heckled out of my chair whenever I seek help for coding problems. If I am doing something wrong, please have mercy on me, I will do whatever I can to correct it.

2018-08-02 17:12:48Z

System: Windows 10, Python 2.7.15, Scrapy 1.5.1Goal: Retrieve text from within html markup for each of the link items on the target website, including those revealed (6 at a time) via the '+ SEE MORE ARCHIVES' button.Target Website: https://magic.wizards.com/en/content/deck-lists-magic-online-products-game-infoInitial Progress: Python and Scrapy successfully installed. The following code......successfully produces the following results (when -o to .csv)...However, the spider will not touch any of the the info buried by the Ajax button. I've done a fair amount of Googling and digesting of documentation, example articles, and 'help me' posts. I am under the impression that to get the spider to actually see the ajax-buried info, that I need to simulate some sort of request. Variously, the correct type of request might be something to do with XHR, a scrapy FormRequest, or other. I am simply too new to web archetecture in general to be able to surmise the answer.I hacked together a version of the initial code that calls a FormRequest, which seems to be able to still reach the initial page just fine, yet incrementing the only parameter that appears to change (when inspecting the xhr calls sent out when physically clicking the button on the page) does not appear to have an effect. That code is here......and the results are the same as before, except the 6 output lines are repeated, as a block, 9 extra times.Can anyone help point me to what I am missing? Thank you in advance.Postscript: I always seem to get heckled out of my chair whenever I seek help for coding problems. If I am doing something wrong, please have mercy on me, I will do whatever I can to correct it.Scrapy don't render dynamic content very well, you need something else to deal with Javascript. Try these:This blog post about scrapy + splash has a good introduction on the topic.

How to get XPath value from nested div

Deep Shah

[How to get XPath value from nested div](https://stackoverflow.com/questions/51592114/how-to-get-xpath-value-from-nested-div)

How can I get the value 5000+ by providing key "Installs" to xpath.I tried this response.xpath('//span/div/span/text()').extract() but it's giving all text. Can anyone help me with it?

2018-07-30 10:39:04Z

How can I get the value 5000+ by providing key "Installs" to xpath.I tried this response.xpath('//span/div/span/text()').extract() but it's giving all text. Can anyone help me with it?I tried this and it worked:It searches for a div with a div child that has the text()="Installs", then searches for the span containing the value you requested.Try this:

Regex html dynamic table

adr

[Regex html dynamic table](https://stackoverflow.com/questions/51590811/regex-html-dynamic-table)

I have stuck with regex syntax. I am trying to create a regex for html code, that looks for a specific string, which is located in a table and gives you back the next column value next to our search string. So I would like to create a regex to look for "Ingatlan \xe1llapota" and return "fel\xfaj\xedtott": 

Ingatlan \xe1llapota fel\xfaj\xedtottMy current regex expression is the following: \bIngatlan állapota\s+(.*)

I would need to incorporate the td tags and to limit how long string would it return after the search string(Ingatlan állapota)Any help is much appreciated. Thanks!

2018-07-30 09:25:27Z

I have stuck with regex syntax. I am trying to create a regex for html code, that looks for a specific string, which is located in a table and gives you back the next column value next to our search string. So I would like to create a regex to look for "Ingatlan \xe1llapota" and return "fel\xfaj\xedtott": 

Ingatlan \xe1llapota fel\xfaj\xedtottMy current regex expression is the following: \bIngatlan állapota\s+(.*)

I would need to incorporate the td tags and to limit how long string would it return after the search string(Ingatlan állapota)Any help is much appreciated. Thanks!As pointed out before use xpath or css instead: Result: 

Split and save text string on scrapy

dionisiac

[Split and save text string on scrapy](https://stackoverflow.com/questions/51597072/split-and-save-text-string-on-scrapy)

I need split a substring from a string, exactly this source text:I want delete "Article published on:" And leave only , so i can save this 

i try with:and withand this don't works, what im missing?error with this code:

2018-07-30 15:09:10Z

I need split a substring from a string, exactly this source text:I want delete "Article published on:" And leave only , so i can save this 

i try with:and withand this don't works, what im missing?error with this code:You probably just forgot to assign the result:category = category.replace('Article published on:', '')

Also it seems that you meant to use replace instead of split. The latter also works though:category = category.split(':')[1]



xpath selector return null in scrapy shell

darbulix

[xpath selector return null in scrapy shell](https://stackoverflow.com/questions/51600805/xpath-selector-return-null-in-scrapy-shell)

I am trying to scrape some data from this url https://www.farfetch.com/shopping/men/gucci-white-rhyton-web-print-leather-sneakers-item-12889013.aspx?storeid=9359The html looks something like this:I ran response.xpath('//div[@id="bannerComponents-Container"]/@class') in the scrapy shell, but I all get is :Why? I am running into similar issues on Amazon, Ebay, etc. where my xpath selector doesnt seem to work

2018-07-30 19:13:38Z

I am trying to scrape some data from this url https://www.farfetch.com/shopping/men/gucci-white-rhyton-web-print-leather-sneakers-item-12889013.aspx?storeid=9359The html looks something like this:I ran response.xpath('//div[@id="bannerComponents-Container"]/@class') in the scrapy shell, but I all get is :Why? I am running into similar issues on Amazon, Ebay, etc. where my xpath selector doesnt seem to workIt's because of the headers. Define one and get what you are after. Try the following: If you kick out the headers the result becomes none.Output:

Cycle in Scrapy ItemLoader

Sewake

[Cycle in Scrapy ItemLoader](https://stackoverflow.com/questions/51563449/cycle-in-scrapy-itemloader)

https://doc.scrapy.org/en/latest/topics/loaders.html#using-item-loaders-to-populate-items    But if I get from webpage 2 names, prices, etc, how to add it to l.load_item() ?Because I added the cycle but if in the end, I write return cycle will work only once.

How to right to do it?

2018-07-27 18:16:06Z

https://doc.scrapy.org/en/latest/topics/loaders.html#using-item-loaders-to-populate-items    But if I get from webpage 2 names, prices, etc, how to add it to l.load_item() ?Because I added the cycle but if in the end, I write return cycle will work only once.

How to right to do it?Just replace return l.load_item() by yield l.load_item()example:

If you use the ItemLoader, you have to reload your variable for each iteration

"In the callback function, you parse the response (web page) and return either dicts with extracted data, Item objects, Request objects, or an iterable of these objects." See scrapy documentationyield is used to generate an iterable of your Product Item objects

how to download image with scrapy when i got a base64

Andy e

[how to download image with scrapy when i got a base64](https://stackoverflow.com/questions/51607051/how-to-download-image-with-scrapy-when-i-got-a-base64)

I want donwload a image with this selectorwith this, on the shell i have the resultMaybe is because the website is protected with hotlinking (cloudflare protection) and i need use other method? or simply im selecting bad the image for download

2018-07-31 06:49:33Z

I want donwload a image with this selectorwith this, on the shell i have the resultMaybe is because the website is protected with hotlinking (cloudflare protection) and i need use other method? or simply im selecting bad the image for downloadAfter you get the base64 string:

Persistently Store visited link so as not to visit tomorrow

TheRigthGift

[Persistently Store visited link so as not to visit tomorrow](https://stackoverflow.com/questions/51557577/persistently-store-visited-link-so-as-not-to-visit-tomorrow)

I am new to Scrapy. I would like to know how I can save visited links persistently. I mean save links visited today so as not to visit them tomorrow.I am toying with the thought of saving each visited link to a CSV file and check it before crawling any link.

2018-07-27 12:04:17Z

I am new to Scrapy. I would like to know how I can save visited links persistently. I mean save links visited today so as not to visit them tomorrow.I am toying with the thought of saving each visited link to a CSV file and check it before crawling any link.You could do it out of the box by setting up a job directory in which crawl state will be persisted (scheduled requests, already visited requests...). See https://doc.scrapy.org/en/latest/topics/jobs.html.If I may suggest, another great option for this is to use permanent static cache which can be configured on scrapy using these settings:By doing it this way it allows you to debug spiders easily and to re-scrape items without re-downloading the pages in case you modify your page parsers or item schemas in any way.Here is if you end up maintaining a csv file anyway, official python doc have good examples:

https://docs.python.org/3/library/csv.html#examples

By what library and how can I scrape texts on an HTML by its heading and paragraph tags?

James Chou

[By what library and how can I scrape texts on an HTML by its heading and paragraph tags?](https://stackoverflow.com/questions/51571609/by-what-library-and-how-can-i-scrape-texts-on-an-html-by-its-heading-and-paragra)

My input will be any web documents that has no fixed HTML structure.

What I want to do is to extract the texts in the heading (might be nested) and its following paragraph tags (might be multiple), and output them as pairs.A simple HTML example can be:For this example, I would like to have a output of pairs:.....and so on.I am a beginner of Python, and I know the web scraping is widely done by Scrapy and BeautifulSoup, and it might require something to do with the XPath or code to identify sibling tags in this case. As how to extract the output pairs of the heading and its below paragraphs are obviously based on relative sequence of the tags. 

I am not sure which library will be better to use in this case, and it will be really helpful if you can show me how to achieve it. Thanks!

2018-07-28 12:53:47Z

My input will be any web documents that has no fixed HTML structure.

What I want to do is to extract the texts in the heading (might be nested) and its following paragraph tags (might be multiple), and output them as pairs.A simple HTML example can be:For this example, I would like to have a output of pairs:.....and so on.I am a beginner of Python, and I know the web scraping is widely done by Scrapy and BeautifulSoup, and it might require something to do with the XPath or code to identify sibling tags in this case. As how to extract the output pairs of the heading and its below paragraphs are obviously based on relative sequence of the tags. 

I am not sure which library will be better to use in this case, and it will be really helpful if you can show me how to achieve it. Thanks!Traversing the tree and collecting all the <p> tags that are under increasing levels of <h> tags can be done with BeautifulSoup:...

Scrapy-splash renders different HTML

Vimal Raj

[Scrapy-splash renders different HTML](https://stackoverflow.com/questions/51520256/scrapy-splash-renders-different-html)

I've been trying to scrape an e-commerce website recently. At first, I kept getting redirected to a "Are you a robot?" page. I then started using a browser user-agent, scrapy-splash for Javascript and a 5 second download delay. Now, there are no errors but the right page is not rendered.spider.pythe HTML renderedoutput:I realize it's somehow detecting that it's a webscraper, but I can't seem to solve that issue.Note: this doesn't happen on the classifieds page, which I can scrape easily and get the urls of each ad. The problem occurs when I request each ad's url. 

2018-07-25 13:33:58Z

I've been trying to scrape an e-commerce website recently. At first, I kept getting redirected to a "Are you a robot?" page. I then started using a browser user-agent, scrapy-splash for Javascript and a 5 second download delay. Now, there are no errors but the right page is not rendered.spider.pythe HTML renderedoutput:I realize it's somehow detecting that it's a webscraper, but I can't seem to solve that issue.Note: this doesn't happen on the classifieds page, which I can scrape easily and get the urls of each ad. The problem occurs when I request each ad's url. Most of the websites now have Robot.txt file to prevent bot scraping so to overcome that you need to visit your settings file and turn of the ROBOTSTXT_OBEY flag to False. or if you want to check this in shell you can specify the flag as such:

Async HTTP server with scrapy and mongodb in python

Bernat Felip

[Async HTTP server with scrapy and mongodb in python](https://stackoverflow.com/questions/51525645/async-http-server-with-scrapy-and-mongodb-in-python)

I am basically trying to start an HTTP server which will respond with content from a website which I can crawl using Scrapy. In order to start crawling the website I need to login to it and to do so I need to access a DB with credentials and such. The main issue here is that I need everything to be fully asynchronous and so far I am struggling to find a combination that will make everything work properly without many sloppy implementations.I already got Klein + Scrapy working but when I get to implementing DB accesses I get all messed up in my head. Is there any way to make PyMongo asynchronous with twisted or something (yes, I have seen TxMongo but the documentation is quite bad and I would like to avoid it. I have also found an implementation with adbapi but I would like something more similar to PyMongo).Trying to think things through the other way around I'm sure aiohttp has many more options to implement async db accesses and stuff but then I find myself at an impasse with Scrapy integration.I have seen things like scrapa, scrapyd and ScrapyRT but those don't really work for me. Are there any other options?Finally, if nothing works, I'll just use aiohttp and instead of Scrapy I'll do the requests to the websito to scrap manually and use beautifulsoup or something like that to get the info I need from the response. Any advice on how to proceed down that road?Thanks for your attention, I'm quite a noob in this area so I don't know if I'm making complete sense. Regardless, any help will be appreciated :)

2018-07-25 18:37:23Z

I am basically trying to start an HTTP server which will respond with content from a website which I can crawl using Scrapy. In order to start crawling the website I need to login to it and to do so I need to access a DB with credentials and such. The main issue here is that I need everything to be fully asynchronous and so far I am struggling to find a combination that will make everything work properly without many sloppy implementations.I already got Klein + Scrapy working but when I get to implementing DB accesses I get all messed up in my head. Is there any way to make PyMongo asynchronous with twisted or something (yes, I have seen TxMongo but the documentation is quite bad and I would like to avoid it. I have also found an implementation with adbapi but I would like something more similar to PyMongo).Trying to think things through the other way around I'm sure aiohttp has many more options to implement async db accesses and stuff but then I find myself at an impasse with Scrapy integration.I have seen things like scrapa, scrapyd and ScrapyRT but those don't really work for me. Are there any other options?Finally, if nothing works, I'll just use aiohttp and instead of Scrapy I'll do the requests to the websito to scrap manually and use beautifulsoup or something like that to get the info I need from the response. Any advice on how to proceed down that road?Thanks for your attention, I'm quite a noob in this area so I don't know if I'm making complete sense. Regardless, any help will be appreciated :)No. pymongo is designed as a synchronous library, and there is no way you can make it asynchronous without basically rewriting it (you could use threads or processes, but that is not what you asked, also you can run into issues with thread-safeness of the code).It doesn't. aiohttp is a http library - it can do http asynchronously and that is all, it has nothing to help you access databases. You'd have to basically rewrite pymongo on top of it.That means lots of work for not using scrapy, and it won't help you with the pymongo issue - you still have to rewrite pymongo!My suggestion is - learn txmongo! If you can't and want to rewrite it, use twisted.web to write it instead of aiohttp since then you can continue using scrapy! 

Indefinite Number of Fields

oldboy

[Indefinite Number of Fields](https://stackoverflow.com/questions/51527402/indefinite-number-of-fields)

I'm scraping a web-page, and there will be an indefinite number of related elements on each page that I plan to stick in a table of their own. However, how do you dynamically create scrapy.Field() elements to deal with this? Or do you not need to or?ExampleFor the sake of simplicity, let's say I'm scraping profile pages of random people. Some of those people have pets, some don't. Some have tons of pets, others just have one. How do I deal with this? How do I dynamically create as many pet fields as are needed?

2018-07-25 20:46:30Z

I'm scraping a web-page, and there will be an indefinite number of related elements on each page that I plan to stick in a table of their own. However, how do you dynamically create scrapy.Field() elements to deal with this? Or do you not need to or?ExampleFor the sake of simplicity, let's say I'm scraping profile pages of random people. Some of those people have pets, some don't. Some have tons of pets, others just have one. How do I deal with this? How do I dynamically create as many pet fields as are needed?create two item types:Then for each Person you can create multiple Pets

Scrapy CrawlSpider not joining

Pepe Sospechas

[Scrapy CrawlSpider not joining](https://stackoverflow.com/questions/51507882/scrapy-crawlspider-not-joining)

I've been reading a lot here and other webs about scrapy and I can't fix this problem so I ask you :P Hope someone could help me.I want to authenticate a login in the main client page and then parse all the categories and then all the products and save the title of the product, its category, its quantity and its price. My code: When I run the scrapy crawl spider on terminal I get this:The spider seems not to be working, any idea of why could it be? 

Thank you very much mates :D 

2018-07-24 21:36:46Z

I've been reading a lot here and other webs about scrapy and I can't fix this problem so I ask you :P Hope someone could help me.I want to authenticate a login in the main client page and then parse all the categories and then all the products and save the title of the product, its category, its quantity and its price. My code: When I run the scrapy crawl spider on terminal I get this:The spider seems not to be working, any idea of why could it be? 

Thank you very much mates :D There are 2 problems:As for login, I tried to make your code work but I failed. I usually override start_requests to login before crawling.Here is the code:

Scrapy: How to use different settings.py for different project

C.shayv

[Scrapy: How to use different settings.py for different project](https://stackoverflow.com/questions/51530870/scrapy-how-to-use-different-settings-py-for-different-project)

I'm looking to use different settings for different project in scrapy, my directory structure is:My scrapy.cfg is:The result shows that both project1 and project2 use project1.settings.So, how do I use project1.settings for project1 and use project2.settings for project2?

Could I set it in scrapy.cfg?

2018-07-26 04:10:09Z

I'm looking to use different settings for different project in scrapy, my directory structure is:My scrapy.cfg is:The result shows that both project1 and project2 use project1.settings.So, how do I use project1.settings for project1 and use project2.settings for project2?

Could I set it in scrapy.cfg?Each scrapy project should contain it's own scrapy.cfg file. So in your case you want to have scrapy.cfg under project1 and project2 See dccumentation on scrapy configuration settings:and documentation on scrapy's default project structure

Scrapy error loop xpath

MagicHat

[Scrapy error loop xpath](https://stackoverflow.com/questions/51527167/scrapy-error-loop-xpath)

I have the follow html structure:This is a product result page, so is 7 blocks for page with that mod_imoveis_result id. I need get image src from all blocks. Each page have 7 blocks like above.I try:I can't understand why text target is ok, but img_url get first result for all blocks for page. Example: each page have 7 blocks, so 7 texts and 7 img_urls, but, img_urls is the same for all other 6 blocks, and text is right, why?If i change extract_first to extract i get others urls, but the result come in the same brackts. Example:text: 1aaaaimg_url : a,b,c,d,e,f,gbut i need text: 1aaaaimg_url: atext: 2aaaaimg_url: bWhat is wrong with that loop?

2018-07-25 20:28:44Z

I have the follow html structure:This is a product result page, so is 7 blocks for page with that mod_imoveis_result id. I need get image src from all blocks. Each page have 7 blocks like above.I try:I can't understand why text target is ok, but img_url get first result for all blocks for page. Example: each page have 7 blocks, so 7 texts and 7 img_urls, but, img_urls is the same for all other 6 blocks, and text is right, why?If i change extract_first to extract i get others urls, but the result come in the same brackts. Example:text: 1aaaaimg_url : a,b,c,d,e,f,gbut i need text: 1aaaaimg_url: atext: 2aaaaimg_url: bWhat is wrong with that loop?// selects the root node i.e. <div id="mod_imoveis_result"> of for node you're trying to get which is div[@id="g-img-imo"] so the two tage that were missed it the reason of NO DATA**. **selects the current node which is mentioned in your xpath irrespective of how deep it is.In your case xpath('./div[@id="g-img-imo"]/div[@class="img_p_results"]/img/@src') denotes selection from root node i.e. from arrowI hope you i made it clear.If all your classes have separate div names, in your case different class tag, then you can directly call image div and extract image URL.

How do I use chained `css()` calls so the selector in the second call uses the first call as the context?

millimoose

[How do I use chained `css()` calls so the selector in the second call uses the first call as the context?](https://stackoverflow.com/questions/51460889/how-do-i-use-chained-css-calls-so-the-selector-in-the-second-call-uses-the-f)

I'm processing a table row-by-row and need to sniff the ids of the rows:So my code looks something like:However, this way, the second call to .css() gives me IDs of all the descendants of row, not just its direct children. I.e. for the above example HTML, it returns "cell_1" as well as "row_1". How do I scope the chained css() call so it only acts on direct children of the given row?I've tried using the :scope pseudo-class but that doesn't seem to be supported by Scrapy, and :root gives me no results.Alternately, can I just get the value of the id attribute without going through CSS?

2018-07-21 23:35:32Z

I'm processing a table row-by-row and need to sniff the ids of the rows:So my code looks something like:However, this way, the second call to .css() gives me IDs of all the descendants of row, not just its direct children. I.e. for the above example HTML, it returns "cell_1" as well as "row_1". How do I scope the chained css() call so it only acts on direct children of the given row?I've tried using the :scope pseudo-class but that doesn't seem to be supported by Scrapy, and :root gives me no results.Alternately, can I just get the value of the id attribute without going through CSS?I can show you how to use XPath for the same task:

Scrapy / Python Modifying extracted data before saving?

Matt Helden

[Scrapy / Python Modifying extracted data before saving?](https://stackoverflow.com/questions/51426631/scrapy-python-modifying-extracted-data-before-saving)

I'm trying to append a url to an extracted piece of data but cannot for the life of me find out how.The selector i'm using is as follow:'15_urlmod': response.url.split('=')[-1] + "_l_a1.jpg",this line of code returns something like:12306116_l_a1.jpgwhat i then want to to is append http:exampleurl.com/images/12306116_l_a1.jpgso the final url to be extracted and saved by scrapy would be:http:exampleurl.com/images/12306116_l_a1.jpgI'm new to Python and have searched for days trying to figure this out. the spider code i'm using is below in full:

2018-07-19 15:31:23Z

I'm trying to append a url to an extracted piece of data but cannot for the life of me find out how.The selector i'm using is as follow:'15_urlmod': response.url.split('=')[-1] + "_l_a1.jpg",this line of code returns something like:12306116_l_a1.jpgwhat i then want to to is append http:exampleurl.com/images/12306116_l_a1.jpgso the final url to be extracted and saved by scrapy would be:http:exampleurl.com/images/12306116_l_a1.jpgI'm new to Python and have searched for days trying to figure this out. the spider code i'm using is below in full:Problem solved, after having a little play around i came up with:"https://sampleurl" + response.url.split('=')[-1] + "_l_a8.jpg"never knew you could do that.hope this helps someone else!

Scrapy Spider following urls, but wont export the data

Timothy Webb

[Scrapy Spider following urls, but wont export the data](https://stackoverflow.com/questions/51421122/scrapy-spider-following-urls-but-wont-export-the-data)

I am trying to grab details from a real estate listing page. I can grab all the data, I just can't seem to export it.. Perhaps a problem with the way I use the yield keyword. The code work for the most part:FEED_EXPORT_FIELDS is adjusted in my SETTINGS.py. The 4 items from parse_puppers() get exported correctly, parse_inside_puppers() data is correct in the console, but wont export.I use scrapy crawl f_app -o raw_data.csv to run me spider. Thanks in advance, appreciate all the help. p.s. im fairly new to python and practising, i bet you noticed.

2018-07-19 11:08:11Z

I am trying to grab details from a real estate listing page. I can grab all the data, I just can't seem to export it.. Perhaps a problem with the way I use the yield keyword. The code work for the most part:FEED_EXPORT_FIELDS is adjusted in my SETTINGS.py. The 4 items from parse_puppers() get exported correctly, parse_inside_puppers() data is correct in the console, but wont export.I use scrapy crawl f_app -o raw_data.csv to run me spider. Thanks in advance, appreciate all the help. p.s. im fairly new to python and practising, i bet you noticed.You need to send you current item to the parse_inside_pupper using meta param:After that you can use it inside parse_inside_pupper (and yield it from here):

Scraping product title off amazon.ca

Phil

[Scraping product title off amazon.ca](https://stackoverflow.com/questions/51335673/scraping-product-title-off-amazon-ca)

I am working with this link: https://www.amazon.ca/s/ref=nb_sb_noss_1?url=search-alias%3Daps&field-keywords=ssd

I would like help retrieving product titles off amazon. I have tried a million ways xpath and css and cannot retrieve the product titles of these items. I have looked online to see how others do it but the html is different compared to theirs on this link.(Sorry if indenting off, tried to do it manually as it pasted without proper indentation).

2018-07-14 05:50:11Z

I am working with this link: https://www.amazon.ca/s/ref=nb_sb_noss_1?url=search-alias%3Daps&field-keywords=ssd

I would like help retrieving product titles off amazon. I have tried a million ways xpath and css and cannot retrieve the product titles of these items. I have looked online to see how others do it but the html is different compared to theirs on this link.(Sorry if indenting off, tried to do it manually as it pasted without proper indentation).Alrighty, first off, Amazon has API endpoints and I suggest anyone reading this to use them over scraping:

https://docs.aws.amazon.com/AWSECommerceService/latest/DG/Welcome.htmlWhy use API endpoints over html scraping?XPath query that appears to work as of 07/14/2018 (No promises it will work tomorrow):



//*/div/div/div/div[2]/div[1]/div[1]/a/@title

Modified code that appears to work.Result (07/14/2018):Many sites (such as Amazon) go through a very heavy handed HTML generation process. Because of this, creating nice clean XPath queries based on intelligently named classes and ids usually isn't possible. So I usually let another software do the heavy lifting. In this case, chrome has the ability to copy the XPath of an element.On any chrome web page, right click what you think has the data you want and select "Inspect", then on the highlighted element node in the inspector tool, right click, select "Copy", and finally select "Copy XPath".I copied the XPath of several elements that had the data I think you are after:Then removed the query section related to grabbing a single results id and had the following:

//*/div/div/div/div[2]/div[1]/div[1]/a/h2This returns the entire header, and I am assuming you only want the name of the product. It looks like the "title" attribute reliably had a name, so I added that to the XPath query.

//*/div/div/div/div[2]/div[1]/div[1]/a/h2/@title

Scrapy: CrawlSpider doesn't parse the response

Saw

[Scrapy: CrawlSpider doesn't parse the response](https://stackoverflow.com/questions/51337218/scrapy-crawlspider-doesnt-parse-the-response)

I've used the CrawlSpider successfully before. But when I changed the code in order to integrate with Redis and add my own middlewares to set UserAgent and cookies, the spider doesn't parse the responses anymore, and thus the spider doesn't generate new requests, the spider closed soon after beginning.Here's the running statsEven if I code this:

def parse_start_url(self, response):

       return self.parse_item(response)

It only parses the response from first urlHere's my code:

Spider:Settings I think crucial:Middleware:

UserAgentmiddleware changes the user agent randomly to avoid being noticed by the serverCookieMiddleware adds the cookies to request for pages that ask for log-in to scan  

2018-07-14 09:41:28Z

I've used the CrawlSpider successfully before. But when I changed the code in order to integrate with Redis and add my own middlewares to set UserAgent and cookies, the spider doesn't parse the responses anymore, and thus the spider doesn't generate new requests, the spider closed soon after beginning.Here's the running statsEven if I code this:

def parse_start_url(self, response):

       return self.parse_item(response)

It only parses the response from first urlHere's my code:

Spider:Settings I think crucial:Middleware:

UserAgentmiddleware changes the user agent randomly to avoid being noticed by the serverCookieMiddleware adds the cookies to request for pages that ask for log-in to scan  Find the problem: All the urls have been filtered by Redis server before previous requests, and restart it can solve the problem. 

Can't disable logging messages in scrapy scripts

Bob

[Can't disable logging messages in scrapy scripts](https://stackoverflow.com/questions/51284987/cant-disable-logging-messages-in-scrapy-scripts)

I am using scrapy (1.5.0) which apparently uses Pillow (5.2.0). When I run my script with scrapy runspider my_scrapy_script.py the stdout gets flooded with useless logging messages, e.g.:I tried disabling them by settings the logger level like this:etc, it didn't help, I tried to set the root logger level like this:with no effect too, higher levels also don't helpsetting LOG_LEVEL = logging.WARNING and even LOG_ENABLED = False in scrapy settings has no effect too.if I set LOG_LEVEL to 'INFO' it prints so it looks like the above mentioned flood is produced before the script is loaded

2018-07-11 12:01:39Z

I am using scrapy (1.5.0) which apparently uses Pillow (5.2.0). When I run my script with scrapy runspider my_scrapy_script.py the stdout gets flooded with useless logging messages, e.g.:I tried disabling them by settings the logger level like this:etc, it didn't help, I tried to set the root logger level like this:with no effect too, higher levels also don't helpsetting LOG_LEVEL = logging.WARNING and even LOG_ENABLED = False in scrapy settings has no effect too.if I set LOG_LEVEL to 'INFO' it prints so it looks like the above mentioned flood is produced before the script is loadedAccording to the Documentation start with an additonal parameter: https://doc.scrapy.org/en/latest/topics/logging.htmlSo it could beAnother wayfor logging levels = Python Logging Levelsmore information => Scrapy LoggingAnother way in spider:As @Lodi suggested in a question's comment, I could only solve the issue of Scrapy filling the logs with debug messages on production (including all the HTML of the scraped pages) in a Django project using celery, disabling the propagation of the scrapy logger. So, what I did is:settings.py:Also, I made my Spider use a logger that derives from 'scrapy' logger, as the CrawlSpider.logger isn't a descendant from 'scrapy' logger. So, in this case I used the scrapy.spiders logger to log messages from my Spider that inherits from CrawlSpider:And then use it with logger.debug(), logger.info(), etc.Keep in mind that messages logged with a severity higher than debug and info, that is: warning, error, critical and exception will be propagated altough propagation is disabled at the scrapy logger. So, you'll still see the DropItem exceptions logged.You could disable it completely with LOG_ENABLED=False. You could also pass settings during scrapy invocation - scrapy runspider my_scrapy_script.py -s LOG_ENABLED=FalseScrapy log documentation

scrape specific table with div that hold the text with scrapy

abduljamac

[scrape specific table with div that hold the text with scrapy](https://stackoverflow.com/questions/51283881/scrape-specific-table-with-div-that-hold-the-text-with-scrapy)

I am using scrapy to scrape the content from a website in a table. example of the code:But as you can see the text is inside a div in each "tr" tag, how would I get the text using xpath or css selector?Ive tried this is the website:http://emaps.elmbridge.gov.uk/ebc_planning.aspx?requesttype=parsetemplate&template=WeeklyListAVDetailTab.tmplt&basepage=ebc_planning.aspx&Filter=^id^=%271%27&history=8a016b5504894a589b75179582da69ca&todatetext:PARAM=06%20July%202018&count:PARAM=63&id:PARAM=1&pagerecs=500&maxrecs=500Thanks in advance!

2018-07-11 11:03:43Z

I am using scrapy to scrape the content from a website in a table. example of the code:But as you can see the text is inside a div in each "tr" tag, how would I get the text using xpath or css selector?Ive tried this is the website:http://emaps.elmbridge.gov.uk/ebc_planning.aspx?requesttype=parsetemplate&template=WeeklyListAVDetailTab.tmplt&basepage=ebc_planning.aspx&Filter=^id^=%271%27&history=8a016b5504894a589b75179582da69ca&todatetext:PARAM=06%20July%202018&count:PARAM=63&id:PARAM=1&pagerecs=500&maxrecs=500Thanks in advance!UPDATEUsing the xpath from gangabass: Only removed the [1] from the td to get all lines.You can do it easily using pandas.Now the table is a dataframe that contains the full table 

How to scrape data from a website using admin-ajax.php with Scrapy

Siktime

[How to scrape data from a website using admin-ajax.php with Scrapy](https://stackoverflow.com/questions/51286037/how-to-scrape-data-from-a-website-using-admin-ajax-php-with-scrapy)

I am trying to scrape the reviews about unibet casino on that website : https://casinoplacard.com/unibet-casino-reviews-and-bonuses/As I did for other sources of reviews I used Scrapy on Python to scrape the reviews with the code below :But for that website it does not work. To understand better I have introduced the counter (self.count) and discover that it do only 1 iteration which is not normal... Then I have spent some tiem studying the DevTools of that website and I have discover that when the page is loaded, a XHR POST request method is done automatically with the URL : https://casinoplacard.com/wp-admin/admin-ajax.phpAnd by looking into that request I have found the 182 reviews data in : So could you guys please help me understand how it works to catch those data ? Thank you very much !

2018-07-11 12:56:01Z

I am trying to scrape the reviews about unibet casino on that website : https://casinoplacard.com/unibet-casino-reviews-and-bonuses/As I did for other sources of reviews I used Scrapy on Python to scrape the reviews with the code below :But for that website it does not work. To understand better I have introduced the counter (self.count) and discover that it do only 1 iteration which is not normal... Then I have spent some tiem studying the DevTools of that website and I have discover that when the page is loaded, a XHR POST request method is done automatically with the URL : https://casinoplacard.com/wp-admin/admin-ajax.phpAnd by looking into that request I have found the 182 reviews data in : So could you guys please help me understand how it works to catch those data ? Thank you very much !I finally found how to do so, I am sure this is not the best way but at least I did what I wanted to do.So as I said in my question in the preview tab there were all the data I needed. So what I had to do was getting those data. To do so I understood that when the URL is loaded that XHR POST request were made automatically so I just tried to force python to request that URL.The parameters you just get them from the headers tab of the DevTool, then the form data is your parameters. For the header you get it also in the header tab, you search for 

User-Agent and just paste all that in the headers. The ajax URL is the one I wrote in my question. Hope that will help someone. 

Scrapy script doesn't get all the products on an ecommerce site page

MEL

[Scrapy script doesn't get all the products on an ecommerce site page](https://stackoverflow.com/questions/51287623/scrapy-script-doesnt-get-all-the-products-on-an-ecommerce-site-page)

I am still new to scrapy, and I am trying to scrape a product list page (from: nordstromrack.com). I used almost the same script on other sites without issues, but on this site, it seems like it only gets me the first 6 items of the page that I want to scrape. I used different pages on the same site with the same results (Ex: https://www.nordstromrack.com/shop/Women/Clothing/Activewear). I used scrapy shell to see if I get different results but I only get the first 6 links. The page source only shows 6 links as well. So I am a little confused on what the problem is. I researched everywhere, and apparently it could be a problem with the site using a script to load 6 products at a time. However, most of the answers I found says to look for the next page and scrape the next page (But that's only for pages with infinite scrolling). Other solutions mention to use Selenium but I guess it will have the same issue because the links that we want to follow are not on the page source. Does anyone know how to solve this problem. Greatly appreciated. Here is my script for this page: https://www.nordstromrack.com/clearance/Men/Accessories?priceRanges%5B%5D=100-200&sort=most_popular

2018-07-11 14:06:38Z

I am still new to scrapy, and I am trying to scrape a product list page (from: nordstromrack.com). I used almost the same script on other sites without issues, but on this site, it seems like it only gets me the first 6 items of the page that I want to scrape. I used different pages on the same site with the same results (Ex: https://www.nordstromrack.com/shop/Women/Clothing/Activewear). I used scrapy shell to see if I get different results but I only get the first 6 links. The page source only shows 6 links as well. So I am a little confused on what the problem is. I researched everywhere, and apparently it could be a problem with the site using a script to load 6 products at a time. However, most of the answers I found says to look for the next page and scrape the next page (But that's only for pages with infinite scrolling). Other solutions mention to use Selenium but I guess it will have the same issue because the links that we want to follow are not on the page source. Does anyone know how to solve this problem. Greatly appreciated. Here is my script for this page: https://www.nordstromrack.com/clearance/Men/Accessories?priceRanges%5B%5D=100-200&sort=most_popularIt's mostly because the data you are looking for is rendered with javascript and AJAX requests.If you open up web inspector when clicking on 2nd page on your url you can see an XHR request is being made to get all of the product data in json (later javascript unpacks it to what you see on the web).Gives this AJAX:

https://www.nordstromrack.com/api/search2/catalog/search?context=clearance&department=Accessories&division=Men&includeFlash=false&includePersistent=true&limit=99&page=2&sort=most_popular&experiment=controlWhich has all of the clothes data in json format:

All you need to do in scrapy is to scrape the AJAX url from above instead of url you are scraping initially and then just load it with json module and parse it as a normal dictionary:

scrapy how to select and pass radio button in formdata

Akshay Zankar

[scrapy how to select and pass radio button in formdata](https://stackoverflow.com/questions/51251662/scrapy-how-to-select-and-pass-radio-button-in-formdata)

I am new to scrapy and using scrapy with python 2.7 for submitting a form which contains different form elemens like dropdowns, checkboxes and radio buttons. I have done with dropdowns, checkboxes but I don't have any idea about passing radio buttons selected using FormRequest.from_response.I have tried the same way like checkbox with syntax as name : value but it is not working. And also can't find any material regarding radio buttons on internet. Below is my code.And html code is :How can I select radio button and pass it using formadata. Any idea is really helpfull.Edit:Also I can select radio button like here. But how to pass it in formdata.Thanks! 

2018-07-09 18:11:48Z

I am new to scrapy and using scrapy with python 2.7 for submitting a form which contains different form elemens like dropdowns, checkboxes and radio buttons. I have done with dropdowns, checkboxes but I don't have any idea about passing radio buttons selected using FormRequest.from_response.I have tried the same way like checkbox with syntax as name : value but it is not working. And also can't find any material regarding radio buttons on internet. Below is my code.And html code is :How can I select radio button and pass it using formadata. Any idea is really helpfull.Edit:Also I can select radio button like here. But how to pass it in formdata.Thanks! 

Scrapy cannot run crawl

Erica L

[Scrapy cannot run crawl](https://stackoverflow.com/questions/51256030/scrapy-cannot-run-crawl)

I was running Scrapy on a very simple example code(shown below, from a tutorial website ) to test if it is set up correctly.The code I copied:When I ran the following commandthe following errors happenedI am not sure what's happening and seeking for help on setting up scrapy correctly.

2018-07-10 01:21:27Z

I was running Scrapy on a very simple example code(shown below, from a tutorial website ) to test if it is set up correctly.The code I copied:When I ran the following commandthe following errors happenedI am not sure what's happening and seeking for help on setting up scrapy correctly.See:https://github.com/scrapy/scrapy/issues/3143And:https://github.com/scrapy/scrapy/issues/3325And, finally:https://github.com/twisted/twisted/pull/966Long story short, this is a bug with Twisted for Python 3.7--not Scrapy at all. Scrapy is more of a framework than a stand-alone Python module. Scrapy itself imports a ton of modules that it uses--one of those being Twisted. So, bugs can happen with any number of modules it uses.You are running Python 3.7 which currently is the newest of the new. Not all modules will support Python 3.7 perfectly right away; hence this is the case with Twisted trying to use the new async keyword that is brand new to Python 3.7. See:https://docs.python.org/3/whatsnew/3.7.htmlI would recommend installing Python 3.6 for now; then pip or pipenv will only install the versions of modules that are more stable with Python 3.6.Right now scrapy doesn't support Python 3.7 temporarily because async keyword is reserved in python 3.7 and scrapy's dependancy twisted has not yet resolved this issue.See scrapy issue regarding this: https://github.com/scrapy/scrapy/issues/3143Current fix for this is to install Twisted branch that has this fix:another solution from the scrapy side would be to disable the extension causing the error which is the scrapy.extensions.telnet.TelnetConsole extension.Just update the following lines in settings.py:It is very likely that you are not going to use telnet for your spider, so this shouldn't complicate anything on your project.

Scrape table from webpage when in <div> format - using Beautiful Soup

Ozdanny

[Scrape table from webpage when in <div> format - using Beautiful Soup](https://stackoverflow.com/questions/51211625/scrape-table-from-webpage-when-in-div-format-using-beautiful-soup)

So I'm aiming to scrape 2 tables (in different formats) from a website - https://info.fsc.org/details.php?id=a0240000005sQjGAAU&type=certificate after using the search bar to iterate this over a list of license codes. I haven't included the loop fully yet but I added it at the top for completeness.My issue is that because the two tables I want, Product Data and Certificate Data are in 2 different formats, so I have to scrape them separately. As the Product data is in the normal "tr" format on the webpage, this bit is easy and I've managed to extract a CSV file of this. The harder bit is extracting Certificate Data, as it is in "div" form.I've managed to print the Certificate Data as a list of text, using the class function, however I need to have it in a tabular form saved in a CSV file. As you can see, I've tried several unsuccessful ways of converting it to a CSV but  If you have any suggestions, it would be much appreciated, thank you!! Also any other general tips to improve my code would be great too, as I am new to web-scraping.Output:What I want but over hundreds/thousands of license codes (I just manually created this one sample in Excel):Desired outputEDITSo whilst this is now working for Certificate Data, I also want to scrape the Product Data and output that into another .csv file. However currently it is only printing 5 copies of the product data for the final license code which is not what I want.New Code: 

2018-07-06 13:35:07Z

So I'm aiming to scrape 2 tables (in different formats) from a website - https://info.fsc.org/details.php?id=a0240000005sQjGAAU&type=certificate after using the search bar to iterate this over a list of license codes. I haven't included the loop fully yet but I added it at the top for completeness.My issue is that because the two tables I want, Product Data and Certificate Data are in 2 different formats, so I have to scrape them separately. As the Product data is in the normal "tr" format on the webpage, this bit is easy and I've managed to extract a CSV file of this. The harder bit is extracting Certificate Data, as it is in "div" form.I've managed to print the Certificate Data as a list of text, using the class function, however I need to have it in a tabular form saved in a CSV file. As you can see, I've tried several unsuccessful ways of converting it to a CSV but  If you have any suggestions, it would be much appreciated, thank you!! Also any other general tips to improve my code would be great too, as I am new to web-scraping.Output:What I want but over hundreds/thousands of license codes (I just manually created this one sample in Excel):Desired outputEDITSo whilst this is now working for Certificate Data, I also want to scrape the Product Data and output that into another .csv file. However currently it is only printing 5 copies of the product data for the final license code which is not what I want.New Code: Here's all you need.

No chromedriver. No pandas. Forget about it in context of scraping.Everything is really straightforward here. I'd suggest you spend some time in Chrome dev tools "network" tab to have a better understanding of request forging, which is a must for scraping tasks. In general, you don't need to run chrome to click the "search" button, you need to forge request generated by this click. Same for any form and ajax.well... you should sharpen your skills (:As per example you can return dictionary of values from "get_data_by_code" function:

Issues installing Scrapy on windows

alberto

[Issues installing Scrapy on windows](https://stackoverflow.com/questions/51221836/issues-installing-scrapy-on-windows)

I already have anaconda installed version 4.5.4, i went to the cmd to install scrapy referring to the docs pip install Scrapy and got the following error:I tried going to that link but they just give me error 404 page not found.

2018-07-07 09:35:49Z

I already have anaconda installed version 4.5.4, i went to the cmd to install scrapy referring to the docs pip install Scrapy and got the following error:I tried going to that link but they just give me error 404 page not found.Go to http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted to download crossponding whl which may needed when install scrapy.Then install it: e.g. pip install C:\Twisted-17.5.0-cp36-cp36m-win_amd64.whl, depends on your python version and platform, you may need to download different whl. BTW,cp36 mean cpython3.6.Finally pip install Scrapy again.You did not give the full error, and I forgot what others needed. If need others, you can also find it on that web. With this, you can no need to install VC++ and quickly make your environment ready.You need to install the visual c++ build tools. The link in the error message seems like it's dead, but you can download them here: https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2017Note that while the error is calling for visual c++ 14.0 - everything will work with newer versions of visual c++.

How do I run multiple Scrapy spiders that each scrape a different URL?

Username

[How do I run multiple Scrapy spiders that each scrape a different URL?](https://stackoverflow.com/questions/51181348/how-do-i-run-multiple-scrapy-spiders-that-each-scrape-a-different-url)

I have a spiders.py in a Scrapy project with the following spiders...How do I run spiders s1 and s2, and write their scraped results to s1.json and s2.json?

2018-07-04 22:19:50Z

I have a spiders.py in a Scrapy project with the following spiders...How do I run spiders s1 and s2, and write their scraped results to s1.json and s2.json?Scrapy doesn't support running multiple spiders as a single process, so you'd simply run two processes:if you want to do it in the same terminal window you'd have to either: Personally I prefer nohup or screen options as they are clean and do not mess up your terminal with logging and whatnot.

Scrapy is not downloading images

Salva Carrión

[Scrapy is not downloading images](https://stackoverflow.com/questions/51215981/scrapy-is-not-downloading-images)

I'm trying to download a few images using Scrapy. I've followed official documentation, copy&paste some examples and read many similar questions but it's still now working at all. What am I missing?I noticed that items pipeline looks empty, but I couldn't figure it out.Also, I've tried different sites, played around with the headers,... but nothing. It looks as if it was working, but then no file has been saved.Here I post the code I'm using to test this functionality.myspider.py:items.py:settings.py:Console:

2018-07-06 18:30:30Z

I'm trying to download a few images using Scrapy. I've followed official documentation, copy&paste some examples and read many similar questions but it's still now working at all. What am I missing?I noticed that items pipeline looks empty, but I couldn't figure it out.Also, I've tried different sites, played around with the headers,... but nothing. It looks as if it was working, but then no file has been saved.Here I post the code I'm using to test this functionality.myspider.py:items.py:settings.py:Console:It is scraping from the main link, as instructed but you are not concatenating the source and the main link.. Try something like this (haven't tested it):It works when I run the spider from the terminal (using scrapy crawl myspider), but not when I run it from a script (CrawlerProcess).See https://github.com/scrapy/scrapy/issues/1904

Webscraping using selenium grid docker cluster

Vignesh 

[Webscraping using selenium grid docker cluster](https://stackoverflow.com/questions/51156826/webscraping-using-selenium-grid-docker-cluster)

I am working on selenium grid docker to scrape website. If I use only one chrome node means the selenium grid is working if I scale more than one node of chrome selenium grid and the scrapy again it stops  working. It just blinks after some time with big error message. Then I opened python shell and type the code individualAs you see  it stopped in webdriver. Remote .cursor is just blinking for long time then big error message is shown. I think problem is in webdriver.Remote(command_executor='http://localhost:5000/wd/hub',

...             desired_capabilities=DesiredCapabilities.CHROME) line.Can anyone give a solution for this problem

Note it's working  if selenium grid has one node (chrome) if I scale more than one node (chrome).This is the error message after long time:I also attached the selenium grid console screenshot when  multiple node is used.

link here to see the picture

2018-07-03 14:14:05Z

I am working on selenium grid docker to scrape website. If I use only one chrome node means the selenium grid is working if I scale more than one node of chrome selenium grid and the scrapy again it stops  working. It just blinks after some time with big error message. Then I opened python shell and type the code individualAs you see  it stopped in webdriver. Remote .cursor is just blinking for long time then big error message is shown. I think problem is in webdriver.Remote(command_executor='http://localhost:5000/wd/hub',

...             desired_capabilities=DesiredCapabilities.CHROME) line.Can anyone give a solution for this problem

Note it's working  if selenium grid has one node (chrome) if I scale more than one node (chrome).This is the error message after long time:I also attached the selenium grid console screenshot when  multiple node is used.

link here to see the pictureIt looks like you're starting up new Selenium nodes with Firefox but your tests specifically look for Chrome.I'd recommend using Zalenium to set up your Selenium Grid:

https://github.com/zalando/zalenium

My first Python Scrapy cannot work

Eric SO

[My first Python Scrapy cannot work](https://stackoverflow.com/questions/51183719/my-first-python-scrapy-cannot-work)

I am new to Python and Scrapy. and now i am working on crawling and needs to use both of them. below is my code and no output can be retrieved. May i know how can i solve this issue? Thanks in advance. 

2018-07-05 05:06:16Z

I am new to Python and Scrapy. and now i am working on crawling and needs to use both of them. below is my code and no output can be retrieved. May i know how can i solve this issue? Thanks in advance. I suggest using scrapy shell to debug your program.

try accessing the response from the shell and see if you able to fetch the elements.

Running modules on Python (Scrapy in this case)

Phil

[Running modules on Python (Scrapy in this case)](https://stackoverflow.com/questions/51161705/running-modules-on-python-scrapy-in-this-case)

I tried to install scrapy using pip, and miniconda and it would say it was installed but would give me an error when trying to import the module on my IDLE (Python 3.7). Later I got it to work when running my IDLE through CMD but I would rather just be able to open my IDLE and use it straight on there. It is most likely an issue with file locations or paths but I cannot figure it out. Any suggestions?Thanks in advance.

2018-07-03 19:28:46Z

I tried to install scrapy using pip, and miniconda and it would say it was installed but would give me an error when trying to import the module on my IDLE (Python 3.7). Later I got it to work when running my IDLE through CMD but I would rather just be able to open my IDLE and use it straight on there. It is most likely an issue with file locations or paths but I cannot figure it out. Any suggestions?Thanks in advance.This usually happens when you have multiple versions of python are installed with different paths in your computer. You try on both CMD and IDLE and see they are matching.(They probably aren't) And you can choose which version you like to keep and remove the other. Or you can remove both versions and perform a clean installation and all the modules using pip. 

using scrapy to build a spider

D. Barnes

[using scrapy to build a spider](https://stackoverflow.com/questions/51162386/using-scrapy-to-build-a-spider)

I am building a simple scraper to get data from a crowdfunding site. I run the scraper then try to output the data to a csv file. Unfortunately, my csv file keeps turning up blank. I have tried numerous suggestions but nothing seems to be working. Here is an example of what I have written:Would be grateful for some help so that I can understand what I am missing. 

2018-07-03 20:26:57Z

I am building a simple scraper to get data from a crowdfunding site. I run the scraper then try to output the data to a csv file. Unfortunately, my csv file keeps turning up blank. I have tried numerous suggestions but nothing seems to be working. Here is an example of what I have written:Would be grateful for some help so that I can understand what I am missing. 

The call to callback function failed in yield request in Scrapy

yao jian

[The call to callback function failed in yield request in Scrapy](https://stackoverflow.com/questions/51164815/the-call-to-callback-function-failed-in-yield-request-in-scrapy)

I find that when the amount of data in the request is too large (116,589), the program does not execute the code for the yield request.

I am sure that the cause of the problem is that the callback function cannot be called because of a large amount of requests. But I didn't find a solution to the problem, please help me answer it.Thank you !

2018-07-04 01:56:15Z

I find that when the amount of data in the request is too large (116,589), the program does not execute the code for the yield request.

I am sure that the cause of the problem is that the callback function cannot be called because of a large amount of requests. But I didn't find a solution to the problem, please help me answer it.Thank you !

How to output two responses.xpath in the same json field?

Lucas Maraal

[How to output two responses.xpath in the same json field?](https://stackoverflow.com/questions/51128698/how-to-output-two-responses-xpath-in-the-same-json-field)

It's possible concatenate two response.xpath in one? I have, for example i have one response.xpath to movie name and other for the director. But instead output this in two lines and the json i do like to have it in one.Like this in the json :The HTML is like :

2018-07-02 03:12:06Z

It's possible concatenate two response.xpath in one? I have, for example i have one response.xpath to movie name and other for the director. But instead output this in two lines and the json i do like to have it in one.Like this in the json :The HTML is like :

Why am I getting a “DLL load failed: %1 is not a valid Win32 application” error?

Shane Kruse

[Why am I getting a “DLL load failed: %1 is not a valid Win32 application” error?](https://stackoverflow.com/questions/51142027/why-am-i-getting-a-dll-load-failed-1-is-not-a-valid-win32-application-error)

I am trying to experiment with the python library scrapy and have run into an issue. I am following a small tutorial on Python Application Development Using Scrapy and using this code I run it using the command scrapy runspider quotes_spider.py -o quotes.json and I am presented with this output Why am I receiving these errors?I am using a 64-Bit version of Python 2.7 and I think scrapy is 64-bit as well but I am not entirely sure. Any help with this would be appreciated!Edit: Deleting the lxml libraries and reinstalling them was my fix for this particular issue

2018-07-02 18:39:21Z

I am trying to experiment with the python library scrapy and have run into an issue. I am following a small tutorial on Python Application Development Using Scrapy and using this code I run it using the command scrapy runspider quotes_spider.py -o quotes.json and I am presented with this output Why am I receiving these errors?I am using a 64-Bit version of Python 2.7 and I think scrapy is 64-bit as well but I am not entirely sure. Any help with this would be appreciated!Edit: Deleting the lxml libraries and reinstalling them was my fix for this particular issue

Scrapy parse redirection first

Pixelknight1398

[Scrapy parse redirection first](https://stackoverflow.com/questions/51140887/scrapy-parse-redirection-first)

I have some code here that is meant to detect a redirection while scraping and then return a request for the page that it was being redirected to so that I can parse the redirected page.  However I let the scraper run for a long while and nothing ever came up in regards to parsing the redirected page.  Keep in mind the fact that my start_urls list is dynamically generated and can be terribly long at times.  What I am trying to accomplish is for the scraper toScraping the original page shouldnt be too complicated and I already have that section of the code handled in the else statement.  My main issue is the first 3 tasks.  I want to move the redirection parsing to the top of the scrapers priority list.  How can I do this?

2018-07-02 17:15:28Z

I have some code here that is meant to detect a redirection while scraping and then return a request for the page that it was being redirected to so that I can parse the redirected page.  However I let the scraper run for a long while and nothing ever came up in regards to parsing the redirected page.  Keep in mind the fact that my start_urls list is dynamically generated and can be terribly long at times.  What I am trying to accomplish is for the scraper toScraping the original page shouldnt be too complicated and I already have that section of the code handled in the else statement.  My main issue is the first 3 tasks.  I want to move the redirection parsing to the top of the scrapers priority list.  How can I do this?

distinguish missing tags from empty tags

Rémi T.

[distinguish missing tags from empty tags](https://stackoverflow.com/questions/51102920/distinguish-missing-tags-from-empty-tags)

While scraping, I need to detect when a tag is missing, to know the page structure has changed. However, I get None whether the tag is missing or empty. How can I achieve that?Here's a minimal example :Current output :Desired output :

2018-06-29 13:29:16Z

While scraping, I need to detect when a tag is missing, to know the page structure has changed. However, I get None whether the tag is missing or empty. How can I achieve that?Here's a minimal example :Current output :Desired output :Querying for div element and then for its text() content relative to the previous query you can write a logic to get what you want. If brand == None do something, if(len(brand_txt) >=1)do something else, etc.From Luis Muñoz answer, I made this usefull wrapper that returns the expected values.

How to extract text data from multiple tags using response.XPath in Scrapy?

Pmsheth

[How to extract text data from multiple tags using response.XPath in Scrapy?](https://stackoverflow.com/questions/51055694/how-to-extract-text-data-from-multiple-tags-using-response-xpath-in-scrapy)

I stuck with one problem. I want to extract text from following HTML using XPath in scrapy.How to extract text from above HTML.

I tried following XPath for extract textBut Didn't get Output can anyone please tell me how to scrape this information because it has multiple {p} tags, (ul} tags inside class. so now I am confuse how to get information.Thanks in Advance

2018-06-27 06:12:54Z

I stuck with one problem. I want to extract text from following HTML using XPath in scrapy.How to extract text from above HTML.

I tried following XPath for extract textBut Didn't get Output can anyone please tell me how to scrape this information because it has multiple {p} tags, (ul} tags inside class. so now I am confuse how to get information.Thanks in AdvanceIt's not really very clear what it is you want, but it sounds like you want an XPath query that gives you all the text nodes. That you can do like this:I resolved this problem from following Answer:I just put following xpath:

 //*[contains(@class,"job-description")]/descendant::text()Thanks for you comment @Lars Marius Garshol.

Python: Sqlalchemy.exc.OperationalError: <unprintable OperationalError object>

Joana

[Python: Sqlalchemy.exc.OperationalError: <unprintable OperationalError object>](https://stackoverflow.com/questions/51062223/python-sqlalchemy-exc-operationalerror-unprintable-operationalerror-object)

I made the following code that I will present below to create a web crawler (elaborated in scrapy) and I want to put this data in a database, the one being mysql. For this I used the pipeline file and made the following configurations:Can someone tell me where the error is? I'm in it for a long time and I can not find it. Any idea ? I will appreciate it if someone can help me!Backtrace:The above exception was the direct cause of the following exception:

2018-06-27 11:57:51Z

I made the following code that I will present below to create a web crawler (elaborated in scrapy) and I want to put this data in a database, the one being mysql. For this I used the pipeline file and made the following configurations:Can someone tell me where the error is? I'm in it for a long time and I can not find it. Any idea ? I will appreciate it if someone can help me!Backtrace:The above exception was the direct cause of the following exception:

scrapy error: Error reading file '': failed to load external entity “”

Magnus Vivadeepa

[scrapy error: Error reading file '': failed to load external entity “”](https://stackoverflow.com/questions/51080936/scrapy-error-error-reading-file-failed-to-load-external-entity)

I'm currently writing a scraper with scrapy. For some websites it works just fine, but for other i get the errorHere is the code I wrote for my scraper, don't blame me but I'm still a beginner in python.When I run the code with scrapy, the error occurs only on certain websites. Does it has anything to do with the ' ' and the " " ?? Thankful for any help.EDIT1:

This is the whole error:

2018-06-28 10:46:31Z

I'm currently writing a scraper with scrapy. For some websites it works just fine, but for other i get the errorHere is the code I wrote for my scraper, don't blame me but I'm still a beginner in python.When I run the code with scrapy, the error occurs only on certain websites. Does it has anything to do with the ' ' and the " " ?? Thankful for any help.EDIT1:

This is the whole error:

Spider not found in scrapyd.schedule

Vira Xeva

[Spider not found in scrapyd.schedule](https://stackoverflow.com/questions/51066352/spider-not-found-in-scrapyd-schedule)

i'm trying to start scrapyd from django

The scrapyd code is like thisHowever, i'm getting My folder structure is something like thisNote that i'm using scrapyd.conf because i have two scrapy project. The scrapy.confThank you

2018-06-27 15:18:20Z

i'm trying to start scrapyd from django

The scrapyd code is like thisHowever, i'm getting My folder structure is something like thisNote that i'm using scrapyd.conf because i have two scrapy project. The scrapy.confThank youI have found that you must add : And after restarting scrapyd it's working 

like charm. Read more the documentation here

Scraping multiple Pages in a Loop gives Duplicated results in 3rd Level

Toleo

[Scraping multiple Pages in a Loop gives Duplicated results in 3rd Level](https://stackoverflow.com/questions/51030786/scraping-multiple-pages-in-a-loop-gives-duplicated-results-in-3rd-level)

In my following HTML Templates:Level 1 (Template_1)

Level 2 (Template_2)Level 3 (Template_3 & Template_4)What I'm trying to do is to enter Level 1 HTML Page, Then pull the Text of each a element then enter it to pull each h1 element Text using the following Spider:My problem is like the following, Firstly the Result I expect is thisBut what I get is the following result:Where I get temp_text duplicated of the last value of the a elements in Level 2I thought the problem is where I placed my yield data so I put it under the parse_lv2's yield like thisBut didn't get the data from parse_lv3, Tried to check which part is the problem So I removed the parse_lv3 and the yield scrapy.Request(url=a.css('a::attr(href)').extract_first(), callback=self.parse_lv3, dont_filter=True, meta={"data": data}) from parse_lv2, replacing it with yield data, And the problem solved (wihtout parse_lv3 data), So I'm sure the problem is either in the Loop or the parse_lv3 yield, But can't figure out How to solve this.

2018-06-25 19:41:10Z

In my following HTML Templates:Level 1 (Template_1)

Level 2 (Template_2)Level 3 (Template_3 & Template_4)What I'm trying to do is to enter Level 1 HTML Page, Then pull the Text of each a element then enter it to pull each h1 element Text using the following Spider:My problem is like the following, Firstly the Result I expect is thisBut what I get is the following result:Where I get temp_text duplicated of the last value of the a elements in Level 2I thought the problem is where I placed my yield data so I put it under the parse_lv2's yield like thisBut didn't get the data from parse_lv3, Tried to check which part is the problem So I removed the parse_lv3 and the yield scrapy.Request(url=a.css('a::attr(href)').extract_first(), callback=self.parse_lv3, dont_filter=True, meta={"data": data}) from parse_lv2, replacing it with yield data, And the problem solved (wihtout parse_lv3 data), So I'm sure the problem is either in the Loop or the parse_lv3 yield, But can't figure out How to solve this.The problem is probably that you only define data once in parse so every loops in parse_lv2 will share the same dict data and every loop in parse_lv3 will share data too, that's why at the end you have the result of the last loop of parse_lv2in data['temp_text'].You'd better initialize data in the loop of parse_lv2 like that

Suddenly, scrapy stops working

Deepak Aggarwal

[Suddenly, scrapy stops working](https://stackoverflow.com/questions/51027761/suddenly-scrapy-stops-working)

I was using scrapy framework which is running fine. But suddenly today when i crawl my spider, i experience this errorI have tried to uninstall and install scrapy again. But it did not work. Please help. I am stuck with this error.

2018-06-25 16:10:10Z

I was using scrapy framework which is running fine. But suddenly today when i crawl my spider, i experience this errorI have tried to uninstall and install scrapy again. But it did not work. Please help. I am stuck with this error.Could you please download , 

 https://github.com/python/cpython-bin-deps/tree/openssl-bin-1.0.2k 

unzip it and copy it to In your sys path : C:\Windows\SysWOW64?This seems like openssl error on windowsMy Problem is solved by installing the previous version of openssl.

Using Scrapy to parse table page and extract data from underlying links

Owais Arshad

[Using Scrapy to parse table page and extract data from underlying links](https://stackoverflow.com/questions/51030767/using-scrapy-to-parse-table-page-and-extract-data-from-underlying-links)

I am trying to scrape the underlying data on the table in the following pages: https://www.un.org/sc/suborg/en/sanctions/1267/aq_sanctions_list/summariesWhat I want to do is access the underlying link for each row, and capture:This is what I have, but it does not seems to be working, I keep getting a "NotImplementedError('{}.parse callback is notdefined'.format(self.class.name)).I believe that the Xpaths I have defined are OK, not sure what I am missing.

2018-06-25 19:40:03Z

I am trying to scrape the underlying data on the table in the following pages: https://www.un.org/sc/suborg/en/sanctions/1267/aq_sanctions_list/summariesWhat I want to do is access the underlying link for each row, and capture:This is what I have, but it does not seems to be working, I keep getting a "NotImplementedError('{}.parse callback is notdefined'.format(self.class.name)).I believe that the Xpaths I have defined are OK, not sure what I am missing.Try the below approach. It should fetch you all the ids and corresponding names from all the six pages. I suppose, the rest of the fields you can manage yourself.Just run it as it is:

Scrapy CrawlSpider allow and deny rule

personalt

[Scrapy CrawlSpider allow and deny rule](https://stackoverflow.com/questions/51029029/scrapy-crawlspider-allow-and-deny-rule)

I am trying to crawl a site, I want it to crawl all pages that have 'category' in them except if they have 'ecatalog.' If ecatalog is in the url dont crawl it include it in the output.  I just threw 'catagory' in the allow and it seems like that might work as all the urls it processed seemed to have 'catagory' in them.  It may have worked without crafting a full regular expression(but since most urls have catagory I cant be certain.   However any time I put anything in the deny value it ends up only scraping the sitemap page and then stopping.  I tried these three and a few others.  what is confusing me is that I would assume if my value for deny is bad it wouldnt deny anything and all the ursl would return.  Instead any value in deny makes it die after first url. 

2018-06-25 17:36:07Z

I am trying to crawl a site, I want it to crawl all pages that have 'category' in them except if they have 'ecatalog.' If ecatalog is in the url dont crawl it include it in the output.  I just threw 'catagory' in the allow and it seems like that might work as all the urls it processed seemed to have 'catagory' in them.  It may have worked without crafting a full regular expression(but since most urls have catagory I cant be certain.   However any time I put anything in the deny value it ends up only scraping the sitemap page and then stopping.  I tried these three and a few others.  what is confusing me is that I would assume if my value for deny is bad it wouldnt deny anything and all the ursl would return.  Instead any value in deny makes it die after first url. 

Difference Between Public and Private Selector Methods

oldboy

[Difference Between Public and Private Selector Methods](https://stackoverflow.com/questions/51035651/difference-between-public-and-private-selector-methods)

I'm just reading this documentation here and was curious: What is the difference between public and private methods in this context?I don't understand why some of those are public methods, whereas others are private methods, and it isn't explained anywhere.From doing a test, I've noticed that there are differences in the FirefoxWebElement for public and private selectors.session="234a0c66-870f-4fee-92b5-8d10541f2d2d"element="3275635e-614d-42da-95ac-306b02743bec"session="c792073f-08b3-4519-a563-0f1e272a17e7"

element="b61a6d2d-2c35-4872-a8e0-2649c189829b"

2018-06-26 05:47:39Z

I'm just reading this documentation here and was curious: What is the difference between public and private methods in this context?I don't understand why some of those are public methods, whereas others are private methods, and it isn't explained anywhere.From doing a test, I've noticed that there are differences in the FirefoxWebElement for public and private selectors.session="234a0c66-870f-4fee-92b5-8d10541f2d2d"element="3275635e-614d-42da-95ac-306b02743bec"session="c792073f-08b3-4519-a563-0f1e272a17e7"

element="b61a6d2d-2c35-4872-a8e0-2649c189829b"If you look at the definition of find_element_by_xpathIt uses the find_element method. Now why is find_element documented as private? Well few thingsSo those are my thoughts on the same, the original contributor to those line could probably tell his intentions were will making the method privateThere are two ways to get web element :  and :  More or less they both are returning a web element.  So, find_element() which is a private method have some advantages such as :  If in case you want to locate the same element by CSS_SELECTOR or XPATH for new builds :  You would do something like :  but in case of find_element() private method :  As the official docs say : private methods which might be useful with locators in page objects.  Page object means for maintainability of your project and you will have your locators in either an INI file or in Page factory. So using Private method if you have to change something while doing regression or something , it would be easy to change the value at only one place.

Python 3.6.5 installed but the file taking python 2.7 as a default

smviswa

[Python 3.6.5 installed but the file taking python 2.7 as a default](https://stackoverflow.com/questions/50989052/python-3-6-5-installed-but-the-file-taking-python-2-7-as-a-default)

when i try to get default python version its showing 3.6 but when i try to run my code its running in python 2.7 ...i have used scrapy crawl cablo to make the file run

ABC abstract method is also installedImportError: cannot import name ABC

2018-06-22 13:28:29Z

when i try to get default python version its showing 3.6 but when i try to run my code its running in python 2.7 ...i have used scrapy crawl cablo to make the file run

ABC abstract method is also installedImportError: cannot import name ABCABC python 3: here a snippet of code I use for both 2.7 and 3.5. You can skip the six stuf if you don't care about 2.7

scrapy 429 error and 503 error

snalla

[scrapy 429 error and 503 error](https://stackoverflow.com/questions/50990002/scrapy-429-error-and-503-error)

I am using the scrappy framework for the data that I want. Some of the requests are getting served but after few requests I receive status code  503 and sometimes 429.I used the following settings: I changed the DOWNLOAD_DELAY from 1.0 to 5.0. If I increase more, it will affect my need. What are the correct settings?My scrappy version IS Scrapy == 1.1.1, fake-useragent == 0.1.7, Twisted == 16.3.0.

2018-06-22 14:19:54Z

I am using the scrappy framework for the data that I want. Some of the requests are getting served but after few requests I receive status code  503 and sometimes 429.I used the following settings: I changed the DOWNLOAD_DELAY from 1.0 to 5.0. If I increase more, it will affect my need. What are the correct settings?My scrappy version IS Scrapy == 1.1.1, fake-useragent == 0.1.7, Twisted == 16.3.0.

Line magic function `%store` not found

reaction hashs

[Line magic function `%store` not found](https://stackoverflow.com/questions/51002646/line-magic-function-store-not-found)

I am using iPython with Scrapy shell inside a virtualenv, but I can't save my project's variables (they get deleted everytime I leave the project) and %store won't work, returning "Line magic function %store not found". Currently, I have Python 2.7.12, iPython 5.7.0 and Scrapy 1.5.0, all working inside a virtualenv.There are some questions here about similar errors, such as this one, but I could not find a solution regarding storemagic/persistence specifically. Maybe this issue I am facing has something to do with running it from a virtual environment? I would like to add persistence to those variables, so they can later be called from a Scrapy spider. Reading the iPython documentation, I figured out storemagic was the way to do it, but it won't work for me inside Scrapy shell. Is there another way to do that?I can't figure out what's going on, would appreciate some help. Thanks

2018-06-23 15:37:04Z

I am using iPython with Scrapy shell inside a virtualenv, but I can't save my project's variables (they get deleted everytime I leave the project) and %store won't work, returning "Line magic function %store not found". Currently, I have Python 2.7.12, iPython 5.7.0 and Scrapy 1.5.0, all working inside a virtualenv.There are some questions here about similar errors, such as this one, but I could not find a solution regarding storemagic/persistence specifically. Maybe this issue I am facing has something to do with running it from a virtual environment? I would like to add persistence to those variables, so they can later be called from a Scrapy spider. Reading the iPython documentation, I figured out storemagic was the way to do it, but it won't work for me inside Scrapy shell. Is there another way to do that?I can't figure out what's going on, would appreciate some help. Thanks

Remove special characters ( \r, \n, \t ) using Xpath/Scrapy

user_1330

[Remove special characters ( \r, \n, \t ) using Xpath/Scrapy](https://stackoverflow.com/questions/51018483/remove-special-characters-r-n-t-using-xpath-scrapy)

I am trying to remove special characters from xpath.

I used the normalize-space()here is my code before :result : After using normalize-space(), the result gives me a one line and the for it does not work on this item.Here is my code after :result : 

2018-06-25 07:46:18Z

I am trying to remove special characters from xpath.

I used the normalize-space()here is my code before :result : After using normalize-space(), the result gives me a one line and the for it does not work on this item.Here is my code after :result : 

items scrapy in sqlite3 : Error binding parameter 0 - probably unsupported type

user_1330

[items scrapy in sqlite3 : Error binding parameter 0 - probably unsupported type](https://stackoverflow.com/questions/51020783/items-scrapy-in-sqlite3-error-binding-parameter-0-probably-unsupported-type)

I'm trying to store the items in a sqlite3 database.

I have generated all the items..Here is my storage code in my pipelines:But during storage, I get this error : 

2018-06-25 10:00:45Z

I'm trying to store the items in a sqlite3 database.

I have generated all the items..Here is my storage code in my pipelines:But during storage, I get this error : the solution is to make a 'for' in each dictionary :

Scrapy to elasticsearch bulk import?

jschnurr

[Scrapy to elasticsearch bulk import?](https://stackoverflow.com/questions/51011099/scrapy-to-elasticsearch-bulk-import)

I'm trying to output some data to the elasticsearch bulk import format. This requires two lines of JL, like so:Is there a way to do this reliably in Scrapy? I had the following, but race conditions happened and it messed up the order of the lines in some cases, which caused the Elasticsearch bulk API to barf:What's the right way to ensure that the two lines of jl together while still following the generator / yield pattern?

2018-06-24 14:58:45Z

I'm trying to output some data to the elasticsearch bulk import format. This requires two lines of JL, like so:Is there a way to do this reliably in Scrapy? I had the following, but race conditions happened and it messed up the order of the lines in some cases, which caused the Elasticsearch bulk API to barf:What's the right way to ensure that the two lines of jl together while still following the generator / yield pattern?Have the spider yield a single object with all relevant data, and write a custom item exporter to format it properly for elasticsearch.

Multi spiders on scrapy

Трусов Илья

[Multi spiders on scrapy](https://stackoverflow.com/questions/50954431/multi-spiders-on-scrapy)

I use django, celery, scrapy.My settings for celery:There are two tasks on celery:The first task is triggered by a signal and maps the links.The second task is started when a new link is added to the database.My signals in django:My spiders:map_links.pyarticles.pyBut I get the error: twisted.internet.error.ReactorNotRestartableI understand that the error is caused by the launch of a new process for the spider. But I'm using threads. And I do not understand why this does not solve my problem.

2018-06-20 17:59:22Z

I use django, celery, scrapy.My settings for celery:There are two tasks on celery:The first task is triggered by a signal and maps the links.The second task is started when a new link is added to the database.My signals in django:My spiders:map_links.pyarticles.pyBut I get the error: twisted.internet.error.ReactorNotRestartableI understand that the error is caused by the launch of a new process for the spider. But I'm using threads. And I do not understand why this does not solve my problem.I think every beginning scrapper meets this question :) 

Try this:

0) pip install crochet

1) import from crochet import setup

2) setup() - at the top of the file

3) remove 2 lines:

a) d.addBoth(lambda _: reactor.stop())

b) reactor.run()



The only meaningful lines from Scrapy docs left are 2 last lines in this my code:This code allows to select what spider to run just with its name passed to run_spider function and after scrapping finishes - select another spider and run it again.

Next you simply run run_spider from Celery task.

INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)

Joana

[INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)](https://stackoverflow.com/questions/50957522/info-crawled-0-pages-at-0-pages-min-scraped-0-items-at-0-items-min)

I just began to learn Python and Scrapy.

My first project is to crawl information on a website containing web security information. But when I run that using cmd, it says that "Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)" and nothing seems to come out. I'd be grateful if someone kind could solve my problem.My code:

2018-06-20 21:45:53Z

I just began to learn Python and Scrapy.

My first project is to crawl information on a website containing web security information. But when I run that using cmd, it says that "Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)" and nothing seems to come out. I'd be grateful if someone kind could solve my problem.My code:There are two things to correct to make it work:This should work:

How to crawl the whole website and scrap data of every web page in python 3 with scrappy

Tehseen

[How to crawl the whole website and scrap data of every web page in python 3 with scrappy](https://stackoverflow.com/questions/50962379/how-to-crawl-the-whole-website-and-scrap-data-of-every-web-page-in-python-3-with)

I am trying to crawl a website and scrap some data from each web page in python 3 using scrappy. i have already scrapped data for one singe page by providing it's url but now i want to scrap data for every page.i think i am missing something because my code just not crawling because it's unable to extract the data. i have tried below code but no success. 

i am stuck here so kindly help me

i am using anaconda3 with pycharm compiler 

2018-06-21 07:06:17Z

I am trying to crawl a website and scrap some data from each web page in python 3 using scrappy. i have already scrapped data for one singe page by providing it's url but now i want to scrap data for every page.i think i am missing something because my code just not crawling because it's unable to extract the data. i have tried below code but no success. 

i am stuck here so kindly help me

i am using anaconda3 with pycharm compiler I have done it. actually I was not calling the parse method in the callback attribute. I have changed this line of code parse_web is the name of parsing definition

Error response ''NoneType' object is not iterable' while scraping website using scrapy

Amith

[Error response ''NoneType' object is not iterable' while scraping website using scrapy](https://stackoverflow.com/questions/50920152/error-response-nonetype-object-is-not-iterable-while-scraping-website-using)

I am new to web-scraping using scrapy. I am trying to scrape a website (Please refer to urls in the code).

From the website ,i am trying to scrap the information's under the 'Intimation For%Month%%Year% ' table and transfer the data to json file.Code:Problem:

In the website,all the intimation data is stored under table with same name table@class="MsoTableGrid".Option's i tried to extract the dataQuestion:

2018-06-19 03:18:46Z

I am new to web-scraping using scrapy. I am trying to scrape a website (Please refer to urls in the code).

From the website ,i am trying to scrap the information's under the 'Intimation For%Month%%Year% ' table and transfer the data to json file.Code:Problem:

In the website,all the intimation data is stored under table with same name table@class="MsoTableGrid".Option's i tried to extract the dataQuestion:To add to that start_requests is expected to be a generator of scrapy.Request objects. Your start_requests does not yield anything:To fix that either yield urls one by one in your start_requests method:Or use default start_requests method that is inherited from scrapy.Spider just by setting start_urls class attribute:

Start scraping only after and before certain element [closed]

tushar747

[Start scraping only after and before certain element [closed]](https://stackoverflow.com/questions/50919287/start-scraping-only-after-and-before-certain-element)

Here's what the HTML looks like: I'm trying to extract only the brands using Scrapy. There are no distinguishing features between the category vs. brands section except that the H4 begins the new section. Also, there are many categories and brands so it's hard to hardcode it.

2018-06-19 01:00:02Z

Here's what the HTML looks like: I'm trying to extract only the brands using Scrapy. There are no distinguishing features between the category vs. brands section except that the H4 begins the new section. Also, there are many categories and brands so it's hard to hardcode it.You can use the following or following-sibling axis. For instance, in order to get the brands you can get to the desired h4 element by text and then get to the next ul sibling via following-sibling:Demo from the Scrapy shell:

How to pass arguments to process.crawl in python's Scrapy

Boky

[How to pass arguments to process.crawl in python's Scrapy](https://stackoverflow.com/questions/50966243/how-to-pass-arguments-to-process-crawl-in-pythons-scrapy)

I'm trying to use python's Scrapy library with IBM cloud functions. I want to pass some arguments with process.crawl. How can I do that?My code is as follows:ExplanationI found here that it can be done as follows:But when I try to do that I get an error in my editor:What am I doing wrong?UPDATEI use the scrapy spider for IBM cloud functions, thus my code is as follows:And when I run main({}) from the console I get following error:

2018-06-21 10:27:51Z

I'm trying to use python's Scrapy library with IBM cloud functions. I want to pass some arguments with process.crawl. How can I do that?My code is as follows:ExplanationI found here that it can be done as follows:But when I try to do that I get an error in my editor:What am I doing wrong?UPDATEI use the scrapy spider for IBM cloud functions, thus my code is as follows:And when I run main({}) from the console I get following error:

How to combine Scrapy output

Danny

[How to combine Scrapy output](https://stackoverflow.com/questions/50919246/how-to-combine-scrapy-output)

I'm very new to Scrapy and this is my most complex spider so far. This is getting all the data I want, but the problem is that the venue_website data is in its own dictionaries. Examples:How would I manage to get the venue_website data into my primary parse_concert dictionaries? I've tried playing around with a follow statement in the parse_concert function and with having the parse_venue data return rather than yield, but I'm just not putting it together quite right. 

2018-06-19 00:53:37Z

I'm very new to Scrapy and this is my most complex spider so far. This is getting all the data I want, but the problem is that the venue_website data is in its own dictionaries. Examples:How would I manage to get the venue_website data into my primary parse_concert dictionaries? I've tried playing around with a follow statement in the parse_concert function and with having the parse_venue data return rather than yield, but I'm just not putting it together quite right. There are two ways of generating items that require multiple pages in scrapy:Since you need multiple requests to generate one item you need to chain them to go in order and carry your data with you:Alternative solution is to yield your two types of items asynchroniously and later combine them by shared id:Then combine in with alternative script:Here's what I got working with the help of the answer above:

How to collect site information having it different links?

Joana

[How to collect site information having it different links?](https://stackoverflow.com/questions/50933128/how-to-collect-site-information-having-it-different-links)

I'm developing a web crawler in python and I'm using Scrapy. For this web crawler i need to fo a site and this contains the main page of the ad and a sub page that I need to access to get the information. How do I do that ?

So far I have developed the code that I will post below. What can I implement more so that this one goes to the main page, takes the information, goes to "sub page" to take information and returns to the main page again to take information of other announcements?

Thanks.Code:

2018-06-19 16:35:27Z

I'm developing a web crawler in python and I'm using Scrapy. For this web crawler i need to fo a site and this contains the main page of the ad and a sub page that I need to access to get the information. How do I do that ?

So far I have developed the code that I will post below. What can I implement more so that this one goes to the main page, takes the information, goes to "sub page" to take information and returns to the main page again to take information of other announcements?

Thanks.Code:You can do something like that (pseudo code)

scrapy - not able to upload data to s3

Baig

[scrapy - not able to upload data to s3](https://stackoverflow.com/questions/50928693/scrapy-not-able-to-upload-data-to-s3)

I am using scrapy to scrap the data from one website which is working fine but i am not able to upload the scrapped data onto amazon s3Looking at the scrapy documentation this is what I have in my settings.pyhere is my environment details in case of this is helpfulWhen I run the scrapper it runs fine, there are no crashes however I did notice in the log following line, I am not sure why this is and how to enable it.Any help will be appreciated

2018-06-19 12:39:15Z

I am using scrapy to scrap the data from one website which is working fine but i am not able to upload the scrapped data onto amazon s3Looking at the scrapy documentation this is what I have in my settings.pyhere is my environment details in case of this is helpfulWhen I run the scrapper it runs fine, there are no crashes however I did notice in the log following line, I am not sure why this is and how to enable it.Any help will be appreciatedThe error is thrown from the _storage_support function present in There seems to be some problem with your URI Parameter.Refer this document 

After exporting scraped data to csv via Scrapy (Python), I'm getting characters like â€ in the file

Vishal Sharma

[After exporting scraped data to csv via Scrapy (Python), I'm getting characters like â€ in the file](https://stackoverflow.com/questions/50933170/after-exporting-scraped-data-to-csv-via-scrapy-python-im-getting-characters)

I wrote a spider in Scrapy to extract data from quotes.toscrape.com, but when I exported the extracted data to csv, the " (quote symbol) is converting itself to characters like â€Here is the code written under spider as can be seen on sublime text3 on a windows machine.Also, here is the command I used to export the data which is defined as in class dictionary to a csv file.(This was run via scrapy shell under windows command prompt)And this is how, I have received the data in csv file.

Please help me resolve that treating me like a beginner.

2018-06-19 16:38:15Z

I wrote a spider in Scrapy to extract data from quotes.toscrape.com, but when I exported the extracted data to csv, the " (quote symbol) is converting itself to characters like â€Here is the code written under spider as can be seen on sublime text3 on a windows machine.Also, here is the command I used to export the data which is defined as in class dictionary to a csv file.(This was run via scrapy shell under windows command prompt)And this is how, I have received the data in csv file.

Please help me resolve that treating me like a beginner.That sequence of mojibake looks like what you get if you encode smart quotes (like '“`, U+201C) as UTF-8 and then try to decode them as ISO Latin 9, Windows-1252, or something else that's similar to Latin-1 but has a Euro symbol. For example:There are two likely places things could be going wrong. Since you haven't shown us the raw bytes at any step in the process, or any of your code, it's impossible to know which of the two is going wrong, but I can explain how to deal with both of them.First, you could be decoding the HTML response that contains these quotes as Latin-9 or whatever, even though it's encoded in UTF-8.If you're doing this explicitly, just stop doing that. But more likely, you're getting, e.g, a TextResponse from Scrapy and just accessing resp.text, and the page had an incorrect header or meta tag or the like, causing Scrapy to mis-decode it. To fix this, you want to access the raw bytes and decode them explicitly. So, if you were using resp.text, you'd do resp.body.decode('utf8') instead.Alternatively, you could be decoding the HTML fine, and encoding the CSV fine, and you're just opening that CSV as Latin-9 instead of UTF-8. In which case there's nothing to change in your code; you just need to look at the settings of your spreadsheet program.However, if you're on Windows, a lot of Windows software (especially from Microsoft) makes some weird assumptions. By default, a text file is assumed to be encoded in the OEM codepage, which is usually something like Windows-1252. To override this and force UTF-8, you're expected to include a "byte order mark". This isn't really a byte order mark (because that makes no sense for 8-bit encodings), and it's strongly discouraged by the standards for UTF-8, but Microsoft does it anyway).So, if you're using Excel on Windows, and you don't want to change the settings, you can work around Microsoft's problem by writing the file with the utf-8-sig encoding instead of utf-8, which will force this "BOM" to be written:Since you appear to be creating your export pipeline just by passing -o csv to the scrapy crawl command, I believe you need to set FEED_EXPORT_ENCODING either in your config file (by editing settings.py or using the scrapy settings command), on the crawl command line (-set FEED_EXPORT_ENDCODING=utf-8-sig), or in an environment variable (SET FEED_EXPORT_ENDCODING=utf-8-sig in the cmd console window before you scrapy crawl).

use scrapy custom settings in scraper.py

venkatesh

[use scrapy custom settings in scraper.py](https://stackoverflow.com/questions/50940445/use-scrapy-custom-settings-in-scraper-py)

i want to run the scrapy file using scrapy runspider scrapy.py

it doesnot have the any settings file how to use custom settings like 'CONCURRENT_REQUESTS': 1,

but when the start_requests is called concurrently loading the all urls requestsscrapy is maintaining the concurrent requests, i want only if one url is processed then it needs to load the next url

2018-06-20 04:53:46Z

i want to run the scrapy file using scrapy runspider scrapy.py

it doesnot have the any settings file how to use custom settings like 'CONCURRENT_REQUESTS': 1,

but when the start_requests is called concurrently loading the all urls requestsscrapy is maintaining the concurrent requests, i want only if one url is processed then it needs to load the next urlYou can set scrapy settings per spider by setting custom_settings spider's attribut like that More about how to deal with scrapy settings here : https://doc.scrapy.org/en/latest/topics/settings.html?highlight=custom_settings

How to extract data from a JavaScript function call like syntax in Python

Avinash Clinton

[How to extract data from a JavaScript function call like syntax in Python](https://stackoverflow.com/questions/50944529/how-to-extract-data-from-a-javascript-function-call-like-syntax-in-python)

I'm scraping data from a website using scrapy in Python.Required data lies in a script tag as follows:I can get this content using xpath as follows:ThenBut how can I parse these contents so thatCan I get any suggestions to solve this.

2018-06-20 09:14:23Z

I'm scraping data from a website using scrapy in Python.Required data lies in a script tag as follows:I can get this content using xpath as follows:ThenBut how can I parse these contents so thatCan I get any suggestions to solve this.Since this call is also valid Python syntax, we can use the ast module. Plus the arguments are all string literals, which makes things simpler.Output:Explanation:Try this with reShould produce the following output:Picked this up from the answer here : https://stackoverflow.com/a/23720594/5907969You can use a simple regex to extract just the comma-separated quoted strings part:Output:Then there are various ways to parse a list of strings from this kind of data:Was about to write an answer which contains both methods with ast and re+json - but @Alex Hall was faster with the ast method, which imho is to prefer - but another method would involve a simple regular expression and the json module, which also gives you a list and can scan multiple function calls in the same string:Online demo hereThis would save you some time when converting JSON objects to Python structures and catching multiple method calls within the same value - but it will certainly not be able to handle anotherMethod(args) or ...value contained in JS method calls.

How to call a spider class from a normal class in Python?

João Victor Valentim

[How to call a spider class from a normal class in Python?](https://stackoverflow.com/questions/50899443/how-to-call-a-spider-class-from-a-normal-class-in-python)

I have the following problem: I wanna call a spider in a different class, like Jobs in Ruby on Rails. For instance: And then, call Foo() to execute my spider. I don't know if my point was clear enough, but I'll appreciate any help :)

2018-06-17 18:37:53Z

I have the following problem: I wanna call a spider in a different class, like Jobs in Ruby on Rails. For instance: And then, call Foo() to execute my spider. I don't know if my point was clear enough, but I'll appreciate any help :)You can do this several ways (even with functional programming), but I'm just going to show you a few:1. ConstructorYou can instantiate (I think that's what you mean by "call") MySpider in the constructor of Foo:But just note that a new instance of Foo will need to be created in order to instantiate and hold a reference to MySpider. If you need to keep the reference, just put self.my_spider = MySpider() in the __init__.2. Adding __call__Along with the __init__ approach, you can add the __call__ method. What this essentially does is allow the instantiated object to be called directly. If you have it return self, you allow a call chain to occur:

scrapy - [twisted] NameError: name 'connect' is not defined

Li Tang

[scrapy - [twisted] NameError: name 'connect' is not defined](https://stackoverflow.com/questions/50893684/scrapy-twisted-nameerror-name-connect-is-not-defined)

I am trying to use scrapy on Ubuntu 16 with python 3.6. When I run "scrapy crawl mySpiderName", it shows an error as:I searched this situation on stack overflow but most questions are not suitable for my problem.Settings.py content is shown below:Please is there anyone can help me to fix this problem?Thank you!

2018-06-17 04:27:09Z

I am trying to use scrapy on Ubuntu 16 with python 3.6. When I run "scrapy crawl mySpiderName", it shows an error as:I searched this situation on stack overflow but most questions are not suitable for my problem.Settings.py content is shown below:Please is there anyone can help me to fix this problem?Thank you!

How can I get this Spider to export a JSON file for each Items List?

Toleo

[How can I get this Spider to export a JSON file for each Items List?](https://stackoverflow.com/questions/50890686/how-can-i-get-this-spider-to-export-a-json-file-for-each-items-list)

In my following file Reddit.py, it has this Spider:What it does that it gets all the URLs from the main page of old.reddit, Then scrape each URL's title, author and score.What I've added to it is a second part, Where it checks if the score is higher than 10000, If it is, Then the Spider goes to the user's page and scrape his karma from it.I do understand that I can scrape the karma from the topic's page, But I would like to do it this way, Since there is other part of the user's page I scrape That doesn't exist in the topic's page.What I want to do is to export the topics list which contains title, author, score into a JSON file named topics.json, Then if the topic's score is higher than 10000 to export the users list which contains name, karma into a JSON file named users.json.I only know how to use the command-line ofWhich exports all the lists into a single JSON file named Reddit but in a bad structure like thisI have no-knowledge at all about Scrapy's Item Pipeline nor Item Exporters & Feed Exporters on how to implement them on my Spider, or how to use them overall, Tried to understand it from the Documentation, But it doesn't seem I get how to use it in my Spider.The final result I want is two files:topics.jsonusers.jsonwhile getting rid of duplicates in the list.

2018-06-16 18:36:55Z

In my following file Reddit.py, it has this Spider:What it does that it gets all the URLs from the main page of old.reddit, Then scrape each URL's title, author and score.What I've added to it is a second part, Where it checks if the score is higher than 10000, If it is, Then the Spider goes to the user's page and scrape his karma from it.I do understand that I can scrape the karma from the topic's page, But I would like to do it this way, Since there is other part of the user's page I scrape That doesn't exist in the topic's page.What I want to do is to export the topics list which contains title, author, score into a JSON file named topics.json, Then if the topic's score is higher than 10000 to export the users list which contains name, karma into a JSON file named users.json.I only know how to use the command-line ofWhich exports all the lists into a single JSON file named Reddit but in a bad structure like thisI have no-knowledge at all about Scrapy's Item Pipeline nor Item Exporters & Feed Exporters on how to implement them on my Spider, or how to use them overall, Tried to understand it from the Documentation, But it doesn't seem I get how to use it in my Spider.The final result I want is two files:topics.jsonusers.jsonwhile getting rid of duplicates in the list.Applying approach from below SO threadExport scrapy items to different filesI created a sample scraperAnd then in exporters.pyAdd below to the settings.pyRunning the scraper I get 3 files generatedexample.jsontopics.jsonusers.jsonThe spider is yielding two items when it crawls a user page. Perhaps it would work if:You can the post-process the JSON as you need.BTW, I don't understand why you use the plural ("topics") when dealing with single elements (a single "topic").

How to use Python Scrapy if CSS or XPATH selectors don't work?

WhiteDillPickle

[How to use Python Scrapy if CSS or XPATH selectors don't work?](https://stackoverflow.com/questions/50845799/how-to-use-python-scrapy-if-css-or-xpath-selectors-dont-work)

I'm trying to scrape from this site using scrapy shell.My goal is to extract all the tenant information. However, I'm having difficulties as it seems that both my CSS and XML selectors aren't working, or at least they're not returning any information.  The website says that it's an XML file without any style information associated with it.  I'm not sure what that means.  I've narrowed down the problem and am now just trying to return information on a single tenant/store but I still can't seem to figure it out.  Here are some of the selectors that I've triedThanks in advance

2018-06-13 20:29:34Z

I'm trying to scrape from this site using scrapy shell.My goal is to extract all the tenant information. However, I'm having difficulties as it seems that both my CSS and XML selectors aren't working, or at least they're not returning any information.  The website says that it's an XML file without any style information associated with it.  I'm not sure what that means.  I've narrowed down the problem and am now just trying to return information on a single tenant/store but I still can't seem to figure it out.  Here are some of the selectors that I've triedThanks in advance

Scrapy Installation error exit status 2

JattCode113

[Scrapy Installation error exit status 2](https://stackoverflow.com/questions/50849317/scrapy-installation-error-exit-status-2)

I'm trying to install Scrapy on my windows device. I originally got the "windows VS build tools 2015 not installed error" that has been posted on here multiple times. I tried numerous solutions that were posted on here but they didn't work. After downloading the build tools, I'm now getting this error:I've searched and tried most of the solutions on here. If someone can redirect me to the solution or point out my error that would be great.

2018-06-14 03:47:37Z

I'm trying to install Scrapy on my windows device. I originally got the "windows VS build tools 2015 not installed error" that has been posted on here multiple times. I tried numerous solutions that were posted on here but they didn't work. After downloading the build tools, I'm now getting this error:I've searched and tried most of the solutions on here. If someone can redirect me to the solution or point out my error that would be great.

How to hide the continuous hit rates(Refresh) to a website

sam mathew

[How to hide the continuous hit rates(Refresh) to a website](https://stackoverflow.com/questions/50807280/how-to-hide-the-continuous-hit-ratesrefresh-to-a-website)

I have developed a Python (Requests) and Java code to scrap data from a Website. And it will work by continuously refresh the website for new data.

But the Website recently identified my scraper as an Automated Service and my account had been Locked out. Is there any way to hide this refreshes to get new data without account lock?

2018-06-11 23:16:22Z

I have developed a Python (Requests) and Java code to scrap data from a Website. And it will work by continuously refresh the website for new data.

But the Website recently identified my scraper as an Automated Service and my account had been Locked out. Is there any way to hide this refreshes to get new data without account lock?It depends on which website it is, in any case, the scraper simulates an user behavior, which would still be blocked.

If the website detects timed tasks a solution might be to randomize a refresh time of your application.

If the website will presents a captcha code, you have no easy solution

If the website just counts the visit from a particular IP address, you might set up a dynamic proxy server to simulate requests from other IPs

Name 'MyItemName' is not defined - Scrapy Item name

Mokjee

[Name 'MyItemName' is not defined - Scrapy Item name](https://stackoverflow.com/questions/50797349/name-myitemname-is-not-defined-scrapy-item-name)

Hello guys,I'm trying to get the data from a website, I already did some projects using scrapy but I don't know how to fix this NameError...My spider : crawlingVacature.pyMy item file : items.pyMy pipeline file : pipelines.pyAnd of course, I had the following into my settings.py file : And i run my project with this command : And about the error I have is :orThanks in advance for your help :-)

2018-06-11 12:06:34Z

Hello guys,I'm trying to get the data from a website, I already did some projects using scrapy but I don't know how to fix this NameError...My spider : crawlingVacature.pyMy item file : items.pyMy pipeline file : pipelines.pyAnd of course, I had the following into my settings.py file : And i run my project with this command : And about the error I have is :orThanks in advance for your help :-)You should use strings as keys, instead of variables

Issue with Splash returning multiple html snapshots

hitro

[Issue with Splash returning multiple html snapshots](https://stackoverflow.com/questions/50778320/issue-with-splash-returning-multiple-html-snapshots)

I'm trying to return multiple html pages using splash scripts (in a single response, as in the documentation) and extracting the links from them. But I found that in the response.text and response.body, the html content is altered whenever there's more than one page returned. This is not the case with response.data, which works fine. Why is this happening?I tried this on the same code (and website) as in the documentation- http://splash.readthedocs.io/en/stable/scripting-ref.html#splash-html (from the later part, from the example for multiple html snapshots)This is my splash request-->The lua script looks like this -->The resulting response:response.data -->response.text and response.body -->Notice the extra \ \ in the second case. These might be escape characters or something, but they're messing with the LinkExtractor which uses the response.text and is resulting in broken links. Again, this happens only when I'm returning an array of html responses.

What am I missing here?

2018-06-09 20:35:58Z

I'm trying to return multiple html pages using splash scripts (in a single response, as in the documentation) and extracting the links from them. But I found that in the response.text and response.body, the html content is altered whenever there's more than one page returned. This is not the case with response.data, which works fine. Why is this happening?I tried this on the same code (and website) as in the documentation- http://splash.readthedocs.io/en/stable/scripting-ref.html#splash-html (from the later part, from the example for multiple html snapshots)This is my splash request-->The lua script looks like this -->The resulting response:response.data -->response.text and response.body -->Notice the extra \ \ in the second case. These might be escape characters or something, but they're messing with the LinkExtractor which uses the response.text and is resulting in broken links. Again, this happens only when I'm returning an array of html responses.

What am I missing here?The Lua script is returning an array of results, which will be translated to a JSON-compatible structure in Python. The value in response.text seems correct.The HTML you want to pass to LinkExtractor is at json.loads(response.text)[i]['html'], for i in 0..2.

How to convert response.text to json in Scrapy

Nicholas Kan

[How to convert response.text to json in Scrapy](https://stackoverflow.com/questions/50779567/how-to-convert-response-text-to-json-in-scrapy)

I am scraping the financial data from below link using Scrapy:Financial Data from TencentThe reponse.body is like below:I have tried to split the response using regular regression then convert it to json but it shows no json object, here is my code:It throws an error saying that no json object can be decoded:My goal is to convert it to json so that I can easily iterate the content.

Is it necessary to convert it to json and how to convert in this case? The response is in unicode format so that I need to convert it to utf-8 as well? Is there any other good way to do iteration?

2018-06-10 00:14:04Z

I am scraping the financial data from below link using Scrapy:Financial Data from TencentThe reponse.body is like below:I have tried to split the response using regular regression then convert it to json but it shows no json object, here is my code:It throws an error saying that no json object can be decoded:My goal is to convert it to json so that I can easily iterate the content.

Is it necessary to convert it to json and how to convert in this case? The response is in unicode format so that I need to convert it to utf-8 as well? Is there any other good way to do iteration?The problem seems to be that the actual data is inside jQuery1124033955090772971586_1528569153921(). I was able to get rid of it by removing a parameter in the request url. If you absolutely needs it, this may do the trick:If you prefer to remove the _callback parameter from the url, simply:As bla said without &_callback=jQuery1124033955090772971586_1528569153921 the data is vaild json, callback is not required also is not static, for example http://web.ifzq.gtimg.cn/appstock/hk/HkInfo/getFinReport?type=3&reporttime_type=1&code=00001&startyear=1990&endyear=2016&_callback=test is gives the same results instead of converting in json use eval because at the end you're going to use it as a dict of lists and etcmay be like,or may be you can use json load instead eval it is also fine

Python Spider ConnectionError: ('Connection aborted.', BadStatusLine(“''”,))

d.pami

[Python Spider ConnectionError: ('Connection aborted.', BadStatusLine(“''”,))](https://stackoverflow.com/questions/50763286/python-spider-connectionerror-connection-aborted-badstatusline)

I am performing a crawling process with python-scrapy. After a random number of iterations that crawls to next page (usually between 400-500), the spider fails, raising ConnectionError: ('Connection aborted.', BadStatusLine("''",)).I've tried several solutions like using headers with random user-agent and none referers or proxy as recommended in some other posts but the spider keeps failing.The raised error is:The iteration code part looks like:Any idea of why this is happening? Solutions could be oriented towards avoiding the error or maybe restarting the spider somehow after the error is raised and resuming the crawling process.

2018-06-08 14:52:15Z

I am performing a crawling process with python-scrapy. After a random number of iterations that crawls to next page (usually between 400-500), the spider fails, raising ConnectionError: ('Connection aborted.', BadStatusLine("''",)).I've tried several solutions like using headers with random user-agent and none referers or proxy as recommended in some other posts but the spider keeps failing.The raised error is:The iteration code part looks like:Any idea of why this is happening? Solutions could be oriented towards avoiding the error or maybe restarting the spider somehow after the error is raised and resuming the crawling process.BadStatusLine is raised because the host is detecting the spider as a bot, and disconnecting it on purpose. Since you are already specifying the UserAgent, maybe you can use a headless browser like PhantomJS to simulate a browser.There is a also a middleware that does this: scrapy-webdriver

For Loops with Scrapy

Jordan Freundlich

[For Loops with Scrapy](https://stackoverflow.com/questions/50777358/for-loops-with-scrapy)

Hey y'all I have been trying to learn scrapy, and am working on my first project right now. I have written this code to try to scrape NFL player news from http://www.rotoworld.com/playernews/nfl/football/?rw=1. I tried to set up a loop to get each container from the site, but when I run the code it isn't scraping anything. The code runs fine, even pus out a csv file when I ask it too. It just isn't scraping what I think I am telling it to scrape. Any help would be great! Thanks

2018-06-09 18:30:18Z

Hey y'all I have been trying to learn scrapy, and am working on my first project right now. I have written this code to try to scrape NFL player news from http://www.rotoworld.com/playernews/nfl/football/?rw=1. I tried to set up a loop to get each container from the site, but when I run the code it isn't scraping anything. The code runs fine, even pus out a csv file when I ask it too. It just isn't scraping what I think I am telling it to scrape. Any help would be great! ThanksYour defined xpaths do not look good. Try this instead. It should fetch you the content you wish to scrape. Just do the copy and paste.

How to get text of selected elements in XPath?

Lukas Achimsson

[How to get text of selected elements in XPath?](https://stackoverflow.com/questions/50763588/how-to-get-text-of-selected-elements-in-xpath)

I try to extract several forum posts by using the standard XPath method:response.xpath('.//div[contains(@class, "Message userContent")]')That one returns a complete list of comments as wished.But once I include //text() or string(...) the length of the list jumps up to 100 or 150 items, which makes it impossible to grasp or to iterate over the list and join it with other data like author or the date... normalize-space(...) only returns the first comment. It has to do something with all the new lines and breaks in the html code but at this stage I have no idea how to handle these. Would string-join(...[normalize-space()]) be an option here?

2018-06-08 15:09:57Z

I try to extract several forum posts by using the standard XPath method:response.xpath('.//div[contains(@class, "Message userContent")]')That one returns a complete list of comments as wished.But once I include //text() or string(...) the length of the list jumps up to 100 or 150 items, which makes it impossible to grasp or to iterate over the list and join it with other data like author or the date... normalize-space(...) only returns the first comment. It has to do something with all the new lines and breaks in the html code but at this stage I have no idea how to handle these. Would string-join(...[normalize-space()]) be an option here?Realize what each XPath is selecting:If you want to get the string values of each such div:

How do you add an attribute to your CSS selector to specify specific pagination link?

Martin Boudreaux

[How do you add an attribute to your CSS selector to specify specific pagination link?](https://stackoverflow.com/questions/50766971/how-do-you-add-an-attribute-to-your-css-selector-to-specify-specific-pagination)

I just got into Scrapy & I’m aware this is a Noob question but How do I add an attribute to specify specific pagination link?here is the html with the element I’m targeting.

I Need to follow the link in this lineHere is my scrapy codeWhat’s happening is its following this link instead of the other one because it is the first one in the class “pagination”I can see 2 differences between the attributes of the 2 links, both in the class “pagination”I’m pretty sure I can get the correct link by specifying one of the 2 attributes listed above in my css selector. I tried using the following CSS selectors but none worked.How can I achieve my goal? Why do none of the CSS selectors I tried work?

2018-06-08 19:04:02Z

I just got into Scrapy & I’m aware this is a Noob question but How do I add an attribute to specify specific pagination link?here is the html with the element I’m targeting.

I Need to follow the link in this lineHere is my scrapy codeWhat’s happening is its following this link instead of the other one because it is the first one in the class “pagination”I can see 2 differences between the attributes of the 2 links, both in the class “pagination”I’m pretty sure I can get the correct link by specifying one of the 2 attributes listed above in my css selector. I tried using the following CSS selectors but none worked.How can I achieve my goal? Why do none of the CSS selectors I tried work?You can't select multiple classes with a single dot. Either combine each of them with dots or go for this syntax "[class='fa fa-chevron-right next pagination-icon']". However, if any class out of them is generated dynamically then the selector will break.Then try with this to see what happens.

Code Error with Scrapy Tutorial

Jordan Freundlich

[Code Error with Scrapy Tutorial](https://stackoverflow.com/questions/50725551/code-error-with-scrapy-tutorial)

I am trying to learn Scrapy and going through the basic tutorial.I am using Anaconda Navigator. I am working in an environment with scrapy installed. I have inputted the code, but keep getting an error. Here is the code: The code runs for a bit. Says it crawled 0 pages. Then DEBUGS: Telnet Console, and then puts out this error,"[scrapy.core.engine] ERROR: Error while obtaining start requests."The code then runs some more, and puts out another error after, "yield scrapy.Requests(utl=url, callback = self.parse)" that says "AttributeError: Module 'scrapy' has no attribute 'Requests'. I have re-written the code, and looked for answers. Please help. Thanks! 

2018-06-06 16:44:10Z

I am trying to learn Scrapy and going through the basic tutorial.I am using Anaconda Navigator. I am working in an environment with scrapy installed. I have inputted the code, but keep getting an error. Here is the code: The code runs for a bit. Says it crawled 0 pages. Then DEBUGS: Telnet Console, and then puts out this error,"[scrapy.core.engine] ERROR: Error while obtaining start requests."The code then runs some more, and puts out another error after, "yield scrapy.Requests(utl=url, callback = self.parse)" that says "AttributeError: Module 'scrapy' has no attribute 'Requests'. I have re-written the code, and looked for answers. Please help. Thanks! You have a typo here:It's Request and not Requests. 

Extracting data from HTML table using scrapy: response.xpath() yields None

Brendan

[Extracting data from HTML table using scrapy: response.xpath() yields None](https://stackoverflow.com/questions/50729897/extracting-data-from-html-table-using-scrapy-response-xpath-yields-none)

I've been building a web scraper in python 3 using the scrapy library and I'm running into a problem I don't understand. I've successfully scraped other tables using inspect element on the table to get the xpath variables. However, with this table, I am unable to figure out how to extract the data from the table. I am new to HTML but not new to programming, so please help me if I'm way off here.An example of this web page would be: http://land.elpasoco.com/ResidentialBuilding.aspx?schd=5317443025&bldg=1Inspecting the page and getting the xpath for the target table yields //*[@id="aspnetForm"]/table/tbody/tr[3]/td[1]/table/tbody/tr[1]/td/table/tbody/tr[3]/td/tableHowever, using this in a scrapy shell response.xpath(target).extract() returns []. Trying to target any individual cells also appears to provide the same null result. My intended result would be a dataframe or dictionary correlating something like {'Dwelling Units': 1, 'Year Built': 2010 ... } Any help identifying where I'm going wrong would or how to get the data formatted as such would be appreciated. Thanks!

2018-06-06 21:30:12Z

I've been building a web scraper in python 3 using the scrapy library and I'm running into a problem I don't understand. I've successfully scraped other tables using inspect element on the table to get the xpath variables. However, with this table, I am unable to figure out how to extract the data from the table. I am new to HTML but not new to programming, so please help me if I'm way off here.An example of this web page would be: http://land.elpasoco.com/ResidentialBuilding.aspx?schd=5317443025&bldg=1Inspecting the page and getting the xpath for the target table yields //*[@id="aspnetForm"]/table/tbody/tr[3]/td[1]/table/tbody/tr[1]/td/table/tbody/tr[3]/td/tableHowever, using this in a scrapy shell response.xpath(target).extract() returns []. Trying to target any individual cells also appears to provide the same null result. My intended result would be a dataframe or dictionary correlating something like {'Dwelling Units': 1, 'Year Built': 2010 ... } Any help identifying where I'm going wrong would or how to get the data formatted as such would be appreciated. Thanks!Here you need to perform some data cleaning only

Can't login using Scrapy

Rishabh Gupta

[Can't login using Scrapy](https://stackoverflow.com/questions/50678389/cant-login-using-scrapy)

tried using scrapy.FormRequest.from_response()

but doesnt seem to work.I need to login to get full access to product details

Login page: https://cosmetics.specialchem.com/login

2018-06-04 10:23:31Z

tried using scrapy.FormRequest.from_response()

but doesnt seem to work.I need to login to get full access to product details

Login page: https://cosmetics.specialchem.com/loginIt's example how to login to target site. To do it you need to open browser and learn all data sending to server. When you understand how it work you can write your own code.It will work when you replace DOMAIN, SECRET and EMAIL to correct one.

XPATH - /a/text(), cant extract email address (text)

Tadeusz R

[XPATH - /a/text(), cant extract email address (text)](https://stackoverflow.com/questions/50675869/xpath-a-text-cant-extract-email-address-text)

I have simple HTML file with usernames and links to their sub-pages:I useto extract user name in plain text.I have a problem when user specifies username in form of email (see first example) - empty object in returned in such case.Edit: I have just noticed html has changed recently and I haven't rechecked:

<td><a href="/user/someUserName@domain.com"><span class="__cf_email__" data-cfemail="3f4d565c544c5e514bwer4rwre58525e5653115c5052">[email&#160;protected]</span></a></td>I'll extract from @href.

2018-06-04 07:58:27Z

I have simple HTML file with usernames and links to their sub-pages:I useto extract user name in plain text.I have a problem when user specifies username in form of email (see first example) - empty object in returned in such case.Edit: I have just noticed html has changed recently and I haven't rechecked:

<td><a href="/user/someUserName@domain.com"><span class="__cf_email__" data-cfemail="3f4d565c544c5e514bwer4rwre58525e5653115c5052">[email&#160;protected]</span></a></td>I'll extract from @href.I have used the following code:-Output:-Can you paste full python code? Because, xpath code seems working fine as:-Getting the text node children of an element (using text()) is generally discouraged, for exactly the reasons demonstrated here. With <a>content</a> you will get "content", with <a><span>content</span><a> you will get nothing, with <a>h<sub>2</sub>o</a> you will get two text nodes, "h" and "o".Use string() to get the string value instead. The string value contains the concatenated content of all the descendant text nodes at any depth. ("content", "content", and "h2o" in these three examples).Only reservation is that I don't know the Scrapy API so I don't know how it handles XPath expressions that return strings rather than nodes.

Execute a javascript line in Splash (Scrapy)

Himanshu Arora

[Execute a javascript line in Splash (Scrapy)](https://stackoverflow.com/questions/50697834/execute-a-javascript-line-in-splash-scrapy)

I want to execute a single line of javascript in Splash for pagination. Is there a way to execute the JS multiple times.

2018-06-05 10:27:04Z

I want to execute a single line of javascript in Splash for pagination. Is there a way to execute the JS multiple times.You can not run JavaScript exactly but there is a scripting language called lua which is similar to JavaScript, no major syntactical difference is there between them.You can refer to https://splash.readthedocs.io/en/stable/scripting-tutorial.htmlOr if i have misinterpreted then please be more specific. 

CSS Selectors for Scrapy Web Scraping

WhiteDillPickle

[CSS Selectors for Scrapy Web Scraping](https://stackoverflow.com/questions/50731368/css-selectors-for-scrapy-web-scraping)

I'm currently trying to scrape all the malls listed on the websitehttps://web.archive.org/web/20151112172204/http://www.simon.com/mallusing Python and Scrapy. I can't figure out how to extract the text "Anchorage 5th Avenue Mall".  I've tried a number of differnet attempts includingBut doesn't give me what I'm looking for.  Note that Anchorage is just the name of the first mall so I can't call that directly because there are 200 or so different malls

2018-06-07 00:22:55Z

I'm currently trying to scrape all the malls listed on the websitehttps://web.archive.org/web/20151112172204/http://www.simon.com/mallusing Python and Scrapy. I can't figure out how to extract the text "Anchorage 5th Avenue Mall".  I've tried a number of differnet attempts includingBut doesn't give me what I'm looking for.  Note that Anchorage is just the name of the first mall so I can't call that directly because there are 200 or so different malls::attr(title) gives you the value of the title attribute. What you want is the text, so you need to use ::text instead.Also, there doesn't appear to be a good way to identify the a element you want since it doesn't have anything that distinguishes it from the others, so a bit of pathing is necessary. Let me know if this works for you:

Each start_url we have to create one output CSV file

dataextract develope

[Each start_url we have to create one output CSV file](https://stackoverflow.com/questions/50700540/each-start-url-we-have-to-create-one-output-csv-file)

I am very new to spider-scrapy. I am extracting data from www.goodsearch.com.If i pass single start_url it's working fine like above code. I want to take links from csv file has input file.Each link we have to generate one csv output file meaning we have to generate three ouptput files. Can you please help me with that.

2018-06-05 12:51:17Z

I am very new to spider-scrapy. I am extracting data from www.goodsearch.com.If i pass single start_url it's working fine like above code. I want to take links from csv file has input file.Each link we have to generate one csv output file meaning we have to generate three ouptput files. Can you please help me with that.Why don't you load the csv file to a numpy narray instead of using split and regular files. you should take advantage of the csv file's organized structure. 

xpath could not recognize predicate for a tag

Howard

[xpath could not recognize predicate for a tag](https://stackoverflow.com/questions/50651458/xpath-could-not-recognize-predicate-for-a-tag)

I try to use scrapy xpath to scrape a page, but it seems it cannot capture the tag with predicates when I use a for loop,

    # This package will contain the spiders of your Scrapy projectI am looking for a tags with name as "MTG_CLASSNAME$" + str(n), with n being 0,1,2..., and I am getting empty output from my xpath query. Not sure why... PS.

I am basically trying to scrape course and their info from https://hrsa.cunyfirst.cuny.edu/psc/cnyhcprd/GUEST/HRMS/c/COMMUNITY_ACCESS.CLASS_SEARCH.GBL?FolderPath=PORTAL_ROOT_OBJECT.HC_CLASS_SEARCH_GBL&IsFolder=false&IgnoreParamTempl=FolderPath%252cIsFolder&PortalActualURL=https%3a%2f%2fhrsa.cunyfirst.cuny.edu%2fpsc%2fcnyhcprd%2fGUEST%2fHRMS%2fc%2fCOMMUNITY_ACCESS.CLASS_SEARCH.GBL&PortalContentURL=https%3a%2f%2fhrsa.cunyfirst.cuny.edu%2fpsc%2fcnyhcprd%2fGUEST%2fHRMS%2fc%2fCOMMUNITY_ACCESS.CLASS_SEARCH.GBL&PortalContentProvider=HRMS&PortalCRefLabel=Class%20Search&PortalRegistryName=GUEST&PortalServletURI=https%3a%2f%2fhome.cunyfirst.cuny.edu%2fpsp%2fcnyepprd%2f&PortalURI=https%3a%2f%2fhome.cunyfirst.cuny.edu%2fpsc%2fcnyepprd%2f&PortalHostNode=ENTP&NoCrumbs=yes

with filter applied: Kingsborough CC, fall 18, BIOThanks!

2018-06-01 22:00:38Z

I try to use scrapy xpath to scrape a page, but it seems it cannot capture the tag with predicates when I use a for loop,

    # This package will contain the spiders of your Scrapy projectI am looking for a tags with name as "MTG_CLASSNAME$" + str(n), with n being 0,1,2..., and I am getting empty output from my xpath query. Not sure why... PS.

I am basically trying to scrape course and their info from https://hrsa.cunyfirst.cuny.edu/psc/cnyhcprd/GUEST/HRMS/c/COMMUNITY_ACCESS.CLASS_SEARCH.GBL?FolderPath=PORTAL_ROOT_OBJECT.HC_CLASS_SEARCH_GBL&IsFolder=false&IgnoreParamTempl=FolderPath%252cIsFolder&PortalActualURL=https%3a%2f%2fhrsa.cunyfirst.cuny.edu%2fpsc%2fcnyhcprd%2fGUEST%2fHRMS%2fc%2fCOMMUNITY_ACCESS.CLASS_SEARCH.GBL&PortalContentURL=https%3a%2f%2fhrsa.cunyfirst.cuny.edu%2fpsc%2fcnyhcprd%2fGUEST%2fHRMS%2fc%2fCOMMUNITY_ACCESS.CLASS_SEARCH.GBL&PortalContentProvider=HRMS&PortalCRefLabel=Class%20Search&PortalRegistryName=GUEST&PortalServletURI=https%3a%2f%2fhome.cunyfirst.cuny.edu%2fpsp%2fcnyepprd%2f&PortalURI=https%3a%2f%2fhome.cunyfirst.cuny.edu%2fpsc%2fcnyepprd%2f&PortalHostNode=ENTP&NoCrumbs=yes

with filter applied: Kingsborough CC, fall 18, BIOThanks!Well... I've visited the website you put in the question description, I used element inspection and searched for "MTG_CLASSNAME" and I got 0 matches...So I will give you some tools:In addition, change for section in response.xpath("//a[contains(@name,'MTG_CLASS_NBR')]"):

by for section in response.xpath("//a[contains(@name,'MTG_CLASS_NBR')]").extract():, this will raise an error when you get the data that you are looking for.

Is there any quick way to check whether scrapy login a website successfully?

user8314628

[Is there any quick way to check whether scrapy login a website successfully?](https://stackoverflow.com/questions/50600074/is-there-any-quick-way-to-check-whether-scrapy-login-a-website-successfully)

I am trying to use Scrapy to login Github.When I logged in Github manually, I checked the box like "remember username and password". So if I don't log out, it should be automatically login when I visit Github again. I ran the script in terminal and it didn't come up with any error. However, when I visit Github, it requires me to log in. I'm not sure if my code works. I didn't touch Scrapy for a while. Is there any quick way to check if I am logged in successfully? Thank you!

2018-05-30 08:38:26Z

I am trying to use Scrapy to login Github.When I logged in Github manually, I checked the box like "remember username and password". So if I don't log out, it should be automatically login when I visit Github again. I ran the script in terminal and it didn't come up with any error. However, when I visit Github, it requires me to log in. I'm not sure if my code works. I didn't touch Scrapy for a while. Is there any quick way to check if I am logged in successfully? Thank you!Code is incorrect. Often forms have hidden  fields. Server will check thus fields when you send credential data to server. I add loop to collect all input tag fields. When form part is correct it's possible to find account name in response page. If it exists you can go ahead. 

How to use Scrapy for URL crawling

R Khalili

[How to use Scrapy for URL crawling](https://stackoverflow.com/questions/50602900/how-to-use-scrapy-for-url-crawling)

I want to crawl the link https://www.aparat.com/.I crawl it correctly and get all the video links with header tag;like this :now my question is this:How can I get all the links from the داغ ترین ها tag, more than 1000? Currently, I get only 60, more or less.

2018-05-30 10:56:13Z

I want to crawl the link https://www.aparat.com/.I crawl it correctly and get all the video links with header tag;like this :now my question is this:How can I get all the links from the داغ ترین ها tag, more than 1000? Currently, I get only 60, more or less.I did this with the following code :

Scrapy splash can't find element

Tinyik

[Scrapy splash can't find element](https://stackoverflow.com/questions/50547810/scrapy-splash-cant-find-element)

Problem:I am using scrapy splash to scrape a web page. However it seems the css path for imageURL does not return any element but the ones for name and category worked fine. (xpath and selector are all copied directly from Chrome.)Things I've Tried:At first I thought it's because the page has not been fully loaded when parse gets called so I changed the wait argument for SplashRequest to 5 but it did not help. I also downloaded a copy of the html response from splash GUI (http://localhost:8050) and verified that the xpath/selectors all work well on the downloaded copy. Here I assumed that this html would be exactly what scrapy sees in parse so I couldn't make sense of why it wouldn't work inside scrapy script.Code:Here is my code:

2018-05-26 23:29:39Z

Problem:I am using scrapy splash to scrape a web page. However it seems the css path for imageURL does not return any element but the ones for name and category worked fine. (xpath and selector are all copied directly from Chrome.)Things I've Tried:At first I thought it's because the page has not been fully loaded when parse gets called so I changed the wait argument for SplashRequest to 5 but it did not help. I also downloaded a copy of the html response from splash GUI (http://localhost:8050) and verified that the xpath/selectors all work well on the downloaded copy. Here I assumed that this html would be exactly what scrapy sees in parse so I couldn't make sense of why it wouldn't work inside scrapy script.Code:Here is my code:May they use different formatting but for me it's (source::attr(srcset) at the end):

How to make web crawler for all e-commerce websites

Prad

[How to make web crawler for all e-commerce websites](https://stackoverflow.com/questions/50504818/how-to-make-web-crawler-for-all-e-commerce-websites)

I am new to scrapy. I would like to make my web crawler for my personal experiment,that would crawl the entire Internet and store the URL of e-commerce websites to my db.I have searched all over the Google ,and found this this one and many more are almost same.But there is start_urls = ['http://brickset.com/sets/year-2016']

that I want to modify and want to add whole Internet.Is this possible ? if yes ,please guide me the right approach.Thanks in advance.

2018-05-24 08:43:31Z

I am new to scrapy. I would like to make my web crawler for my personal experiment,that would crawl the entire Internet and store the URL of e-commerce websites to my db.I have searched all over the Google ,and found this this one and many more are almost same.But there is start_urls = ['http://brickset.com/sets/year-2016']

that I want to modify and want to add whole Internet.Is this possible ? if yes ,please guide me the right approach.Thanks in advance.So let's approach this problem a bit differently. It is actually impossible to build a crawler that can actually crawl all e-commerce websites and bring you the results.This leaves us with our best option Search Engines. What you can rather do is crawl any of the search engines with your product query and gather links that have a product listed for selling. The second challenge that you'll face is how to tell the difference between e-commerce sites and other sites. Tools like DiffBot would really help in that.This needs to be done real-time cause obviously you won't be planning on making a humongous database of all the products out there on the indexed sites on internet.

assign properties to wrong items in Scrapy

gongarek

[assign properties to wrong items in Scrapy](https://stackoverflow.com/questions/50505573/assign-properties-to-wrong-items-in-scrapy)

CODEProblem is that when spider crawl to next_page. Some self.property is assign to wrong items. I don't know how to repair it.

2018-05-24 09:21:41Z

CODEProblem is that when spider crawl to next_page. Some self.property is assign to wrong items. I don't know how to repair it.self.property is a class attribute that is shared among all calls to parse2 and you can't control the order of each call to parse2 . To solve that you need to pass the property list inside the meta or as a item attribute:

Scrapy save html element to html file

Ds Klur

[Scrapy save html element to html file](https://stackoverflow.com/questions/50521609/scrapy-save-html-element-to-html-file)

I want to save div element class="col-md-12 blog-data" (contains images)  to html file.   Where should I put response.css ? I'm new to python and scrapy .Is it possible to concat custom string like this and save to html file?Please Give me some examples. Thank you. 

2018-05-25 04:32:50Z

I want to save div element class="col-md-12 blog-data" (contains images)  to html file.   Where should I put response.css ? I'm new to python and scrapy .Is it possible to concat custom string like this and save to html file?Please Give me some examples. Thank you. you cannot use response.css to give styling. response object will not have a method called .css. f want to concat a css to a div, you gotta use regex and concat Or a cleaner approach would be just append mystyle.css file in the head, And write down all your styles in mystle.css.you can do this using BeautifulSoup.

Why is the code not able to scrape any content in the HTML class?

Kristada673

[Why is the code not able to scrape any content in the HTML class?](https://stackoverflow.com/questions/50482770/why-is-the-code-not-able-to-scrape-any-content-in-the-html-class)

I can see during inspect that the content lies in the class article-wrap, as highlighted in the screenshot:But when I try to scrape the text content in it, I get nothing:Why is that? Am I specifying the class wrongly? If so, which class do I need to specify? What's the simplest way to know which class (or tag, or div, etc) I should specify?This is the code:

2018-05-23 07:51:48Z

I can see during inspect that the content lies in the class article-wrap, as highlighted in the screenshot:But when I try to scrape the text content in it, I get nothing:Why is that? Am I specifying the class wrongly? If so, which class do I need to specify? What's the simplest way to know which class (or tag, or div, etc) I should specify?This is the code:It's much easier to use BeautifulSoup's .select() method, similar to css selectors, such as below:I tested on my computer. Works great.Actually, it turned out to be 2 things: Firstly, in article links.txt, each link was in a new line (i.e., ended with a \n). So, I had to do a=a.rstrip() before page = requests.get(a) to get the correct soup. Without the strip, if I print out the soup, it looks like this:As can be seen, without stripping the \n at the end of each link, it can't extract the info in the page (in particular, there's no tag artcle-wrap in the soup it extracts).Secondly, I added both html classes, article-wrap and mag-article-wrap to accommodate for the 2 different kinds of articles:Now it works fine.

Scrapy and flask on ec2

Dude

[Scrapy and flask on ec2](https://stackoverflow.com/questions/50469867/scrapy-and-flask-on-ec2)

So basically im new into this, so bear with me, please.I have  3 python spiders that uses: scrappy,scrappy-user-agent,pandas,MongoDB.

they scrape around 150-200 pages every 12 hours and store the data locally into MongoDB collections.

And I have a flask app that connects the API endpoints with the collections and returns the data as response.Would it possible to deploy both to same ec2 instance, or would flask and response be slowed down for users while the scrapping is done in parallel in same machine?

2018-05-22 14:10:56Z

So basically im new into this, so bear with me, please.I have  3 python spiders that uses: scrappy,scrappy-user-agent,pandas,MongoDB.

they scrape around 150-200 pages every 12 hours and store the data locally into MongoDB collections.

And I have a flask app that connects the API endpoints with the collections and returns the data as response.Would it possible to deploy both to same ec2 instance, or would flask and response be slowed down for users while the scrapping is done in parallel in same machine?It is possible to deploy them both in the same instance. However, you need to know how much memory and CPU your both applications use, and choose your instance type accordingly. Given the low frequency of your web scraping, it is very possible that it does not take much memory and CPU, but it may be the case if you are doing some heavy processing of the scrapped data. To know about the memory and CPU configurations of each instance type: https://aws.amazon.com/ec2/instance-types/

How do I recurse in scrapy?

ZeeBm Chan

[How do I recurse in scrapy?](https://stackoverflow.com/questions/50471676/how-do-i-recurse-in-scrapy)

Here's my codeWhen i debug my code in 'parse_item' function,the 'response.meta["names"]' only include the first page datas(12 titles in this case), how could i get the 6 pages datas list.

2018-05-22 15:41:52Z

Here's my codeWhen i debug my code in 'parse_item' function,the 'response.meta["names"]' only include the first page datas(12 titles in this case), how could i get the 6 pages datas list.Its because you have URL http://www.piaov.com and scrapy ignores the duplicate URLs unless dont_filter=True is specified in Request like Request(url_here, dont_filter=True)Also I don't like your logic of scraper, why are you calling parse_item at all? it is not necessary. Please see the code below and do it like that.

Scrapy works on local, but does not on production

Chiefir

[Scrapy works on local, but does not on production](https://stackoverflow.com/questions/50470658/scrapy-works-on-local-but-does-not-on-production)

I have deployed my Django+Scrapy project, have running scrapyd. But when I try to run the spider, it finishes without actually scrapping info with those stats:  Does that mean that my first request got rejected with 403 error? Why that might work on local, but does not on production?

 As I have read that might be caused by wrong USER_AGENT setting, but i have already set this like:  As I said - it works on local, not on production.

P.S. Here are stats from the same spider running on local:

2018-05-22 14:50:40Z

I have deployed my Django+Scrapy project, have running scrapyd. But when I try to run the spider, it finishes without actually scrapping info with those stats:  Does that mean that my first request got rejected with 403 error? Why that might work on local, but does not on production?

 As I have read that might be caused by wrong USER_AGENT setting, but i have already set this like:  As I said - it works on local, not on production.

P.S. Here are stats from the same spider running on local:

scrapy recursive scrape website

Magnus Vivadeepa

[scrapy recursive scrape website](https://stackoverflow.com/questions/50483836/scrapy-recursive-scrape-website)

I want to write a scraper that visits all subpages of the initial page.The example website is: pydro.com

So it should for example also extract pydro.com/impressum and save it as an html file on my hard drive.The code I wrote:When I run my spider the output is only pydro.html.I think I need to adjust my filename, that i get the subpage. Or do I need a for loop?EDIT1:

I edited the code to get all html pages. But when I want to run the script on another website I get an error called:Thats the script I run:

2018-05-23 08:48:24Z

I want to write a scraper that visits all subpages of the initial page.The example website is: pydro.com

So it should for example also extract pydro.com/impressum and save it as an html file on my hard drive.The code I wrote:When I run my spider the output is only pydro.html.I think I need to adjust my filename, that i get the subpage. Or do I need a for loop?EDIT1:

I edited the code to get all html pages. But when I want to run the script on another website I get an error called:Thats the script I run:You need to create a recursive scraping.

A 'sub-page' is simply another page whose url is obtained from a 'previous' page. You must make a second request to the sub-page (its url should be in the variable sel) and use xpath on the (2nd) response.How to recursively crawl subpages with Scrapy

python scrapping error AttributeError: 'NoneType' object has no attribute 'text'

jackson

[python scrapping error AttributeError: 'NoneType' object has no attribute 'text'](https://stackoverflow.com/questions/50433117/python-scrapping-error-attributeerror-nonetype-object-has-no-attribute-text)

I am doing python scrapping with beautiful soup, the website i am crawling has a 28 container with title, link and text, the text is in <p>tag, my problem is I can crawl all the data but some <p> tags has no text, so I receive an error 

"AttributeError: 'NoneType' object has no attribute 'text'"

my code is: it print 5 times <p> tag text, because not all of the <p> tag has text, but it gives me error, how to resolve this?

2018-05-20 08:46:10Z

I am doing python scrapping with beautiful soup, the website i am crawling has a 28 container with title, link and text, the text is in <p>tag, my problem is I can crawl all the data but some <p> tags has no text, so I receive an error 

"AttributeError: 'NoneType' object has no attribute 'text'"

my code is: it print 5 times <p> tag text, because not all of the <p> tag has text, but it gives me error, how to resolve this?You can try like below to accomplish the task:

Scrapy _ How to append / delete text to listing URL

Matt Helden

[Scrapy _ How to append / delete text to listing URL](https://stackoverflow.com/questions/50438903/scrapy-how-to-append-delete-text-to-listing-url)

I'm new to Python and Scrapy. I'm trying to create a spider to scrape: https://www.festicket.com/festivals/I've managed to get the spider working, the problem is that some URLs are like so: 

https://www.festicket.com/festivals/electric-daisy-carnival-edc-las-vegas/2018/and some URLS have:    /shop/#ticket       appended to them which is stoping the spider from crawling the listing page.My question is, is there some way that if the spider finds a URL with /shop/#ticket   it simple deletes the /shop/#ticket     but keeps the rest of the URL???My code so far is below:

2018-05-20 20:03:45Z

I'm new to Python and Scrapy. I'm trying to create a spider to scrape: https://www.festicket.com/festivals/I've managed to get the spider working, the problem is that some URLs are like so: 

https://www.festicket.com/festivals/electric-daisy-carnival-edc-las-vegas/2018/and some URLS have:    /shop/#ticket       appended to them which is stoping the spider from crawling the listing page.My question is, is there some way that if the spider finds a URL with /shop/#ticket   it simple deletes the /shop/#ticket     but keeps the rest of the URL???My code so far is below:You need to change this part:to:UPDATE

If you want to replace "/shop/#ticket" at the end of an URL:

ImportError: No module named wx in scrapy python 2.7.13?

rajeshbojja

[ImportError: No module named wx in scrapy python 2.7.13?](https://stackoverflow.com/questions/50443381/importerror-no-module-named-wx-in-scrapy-python-2-7-13)

i tried to give import wx in scrapy python 2.7.13 but i'm getting "ImportError: No module named wx"? can anyone help please? Thanx in advance!! 

2018-05-21 06:46:10Z

i tried to give import wx in scrapy python 2.7.13 but i'm getting "ImportError: No module named wx"? can anyone help please? Thanx in advance!! 

Scrapyd project does not persist between Docker container runs

rstuppi

[Scrapyd project does not persist between Docker container runs](https://stackoverflow.com/questions/50400138/scrapyd-project-does-not-persist-between-docker-container-runs)

I have a Docker container running Ubuntu hosted on Windows 10 Pro. The Scrapy and Scrapyd packages have been installed in the container and all is well.  I added a project, Project A, to Scrapyd using scrapyd-deploy and all is still well. I can schedule Project A's spider using curl and the schedule.json API, and crawl till the cows come home.The problem is that Project A does not persist between container runs. After the cows have come home, and stopping and running the Ubuntu container, Scrapyd contains 0 projects as evidenced by listprojects.json. I have tried commiting the container but the project does not persist.Any help is appreciated!

2018-05-17 20:36:13Z

I have a Docker container running Ubuntu hosted on Windows 10 Pro. The Scrapy and Scrapyd packages have been installed in the container and all is well.  I added a project, Project A, to Scrapyd using scrapyd-deploy and all is still well. I can schedule Project A's spider using curl and the schedule.json API, and crawl till the cows come home.The problem is that Project A does not persist between container runs. After the cows have come home, and stopping and running the Ubuntu container, Scrapyd contains 0 projects as evidenced by listprojects.json. I have tried commiting the container but the project does not persist.Any help is appreciated!Summing up a discussion from comments.You have to mount the two paths from container into your host system in order to persist your data.You define the mount like this:docs ref

how to extract list of hrefs from javascript onclick attributes and to parse through looping in python using scrapy?

rajeshbojja

[how to extract list of hrefs from javascript onclick attributes and to parse through looping in python using scrapy?](https://stackoverflow.com/questions/50370669/how-to-extract-list-of-hrefs-from-javascript-onclick-attributes-and-to-parse-thr)

how to extract all list of hrefs from above javascript onclick attributes and to parse through looping in python scrapy?

2018-05-16 12:13:50Z

how to extract all list of hrefs from above javascript onclick attributes and to parse through looping in python scrapy?that will get you your hrefs as an array of strings.const hrefArray = Array.from(document.querySelectorAll('a')).map(el => el.href);

What order is followed when Scrapy CrawlSpider scraping pages?

TomLeung

[What order is followed when Scrapy CrawlSpider scraping pages?](https://stackoverflow.com/questions/50363321/what-order-is-followed-when-scrapy-crawlspider-scraping-pages)

I am new to Scrapy and reading Learing Scrapy to study, and I have a question about the scrape order.The book provide a piece of code:And it said that Scrapy using a LIFO strategy to crawl. So I suppose that the first item should be the item on the last page, but it turns out the first item is on the first page.Why? According to the code, I think Scrapy will keep following the first rule until it find the last page, and then it will start to parse items on the last page. I am confused.And if a website has millions of pages, Scrapy won't parse any items until it reaches the last page?

2018-05-16 05:53:10Z

I am new to Scrapy and reading Learing Scrapy to study, and I have a question about the scrape order.The book provide a piece of code:And it said that Scrapy using a LIFO strategy to crawl. So I suppose that the first item should be the item on the last page, but it turns out the first item is on the first page.Why? According to the code, I think Scrapy will keep following the first rule until it find the last page, and then it will start to parse items on the last page. I am confused.And if a website has millions of pages, Scrapy won't parse any items until it reaches the last page?All of the rules are being followed on every page in order of the tuple. For example you have two rules: If you run this spider on the 1st page it will find other pagination urls and schedule them then find products and schedule them with parse_product callback or whatever you have set. Afterwards for any scheduled url that has default callback (where you haven't specificied callback argument) it will repeat this untill nothing is found anymore.

scrapy - scraping multiple fields

paperelephant

[scrapy - scraping multiple fields](https://stackoverflow.com/questions/50370707/scrapy-scraping-multiple-fields)

very new and inexperienced programmer here!Am building a scrapy project that can scrape this website for company names and locations and output a JSON file.https://www.f6s.com/programs?type[]=accelerator&page=93&page_alt=1&sort=openCurrently, my scraper is pulling the company names, but is also pulling dates. In addition, the JSON output is coming in sections, first a list of company names then a list of locations (with additional info that I don't want).How do I just pull the company names / locations and format it so I can associate each company name with the specific location?I think my issue is that the locations are not defined as a specific class.In addition, advice on how to set the JSON output format would be greatly appreciated!!My Project Directory:My Spider File:My TerminalMy JSON output:Thanks!

2018-05-16 12:16:03Z

very new and inexperienced programmer here!Am building a scrapy project that can scrape this website for company names and locations and output a JSON file.https://www.f6s.com/programs?type[]=accelerator&page=93&page_alt=1&sort=openCurrently, my scraper is pulling the company names, but is also pulling dates. In addition, the JSON output is coming in sections, first a list of company names then a list of locations (with additional info that I don't want).How do I just pull the company names / locations and format it so I can associate each company name with the specific location?I think my issue is that the locations are not defined as a specific class.In addition, advice on how to set the JSON output format would be greatly appreciated!!My Project Directory:My Spider File:My TerminalMy JSON output:Thanks!Looks like you don't need two for loops for that, you can do with one:You need to format location in certain cases where date is coming, like thisOutput:

How to set global settings in Scrapy in Ubuntu 16.04?

Nitin Gandhi

[How to set global settings in Scrapy in Ubuntu 16.04?](https://stackoverflow.com/questions/50317933/how-to-set-global-settings-in-scrapy-in-ubuntu-16-04)

It is given that setting is in "scrapy.settings.default_settings" Module, but I am unable to find it. Is it a text file or something? How to access it?

2018-05-13 15:51:54Z

It is given that setting is in "scrapy.settings.default_settings" Module, but I am unable to find it. Is it a text file or something? How to access it?scrapy.settings.default_settings is a inbuilt python module with default setting constants.You cannot change that outside from forking scrapy itself, however there are plenty of other ways to set settings:https://doc.scrapy.org/en/latest/topics/settings.html

python scrape a div with more than one class name using response.css

niloofar

[python scrape a div with more than one class name using response.css](https://stackoverflow.com/questions/50319579/python-scrape-a-div-with-more-than-one-class-name-using-response-css)

I'm new to Scrapy and working on a scrapy project.I want to scrape a div with more than one class name as below:Here is my scripy (the def parse) :And the output is empty.So what should I do?

2018-05-13 18:53:41Z

I'm new to Scrapy and working on a scrapy project.I want to scrape a div with more than one class name as below:Here is my scripy (the def parse) :And the output is empty.So what should I do?Here you goIn case of multiple classes, you can separate them by dot(.)

Scraped elements not showing up in .jl file

fg42

[Scraped elements not showing up in .jl file](https://stackoverflow.com/questions/50258604/scraped-elements-not-showing-up-in-jl-file)

Quick background I'm learning to code with Python and at the same time learning to scrape. My problem here is that when I run this code, the console shows that indeed the wanted content is scraped, but when I open the .jl document it is empty. As I'm a beginner there might have been many things that I have missed. Thanks for your help and sorry in advance if I missed a similar post.

2018-05-09 17:03:59Z

Quick background I'm learning to code with Python and at the same time learning to scrape. My problem here is that when I run this code, the console shows that indeed the wanted content is scraped, but when I open the .jl document it is empty. As I'm a beginner there might have been many things that I have missed. Thanks for your help and sorry in advance if I missed a similar post.You need to use yield instead of just printing locations for the output to be present in .jl file

Scrapy - different page content when download response.body

dorinand

[Scrapy - different page content when download response.body](https://stackoverflow.com/questions/50194806/scrapy-different-page-content-when-download-response-body)

I am trying to parse page, e.g. www.page.com/results?sort=price. I am parsing it with this code:The output file is different than file generate by this code:When I download page via scrapy shell 'www.page.com/results?sort=price&type=12' The output is similar like file2.txt. The problem is, that in the file1.txt, there is no tags with data I need to crawl. What is the difference between this two way to crawl page, why is the downloaded file different?

2018-05-05 23:08:27Z

I am trying to parse page, e.g. www.page.com/results?sort=price. I am parsing it with this code:The output file is different than file generate by this code:When I download page via scrapy shell 'www.page.com/results?sort=price&type=12' The output is similar like file2.txt. The problem is, that in the file1.txt, there is no tags with data I need to crawl. What is the difference between this two way to crawl page, why is the downloaded file different?I think in the second case you going to the wrong url. Check your logs to make sure. I'm not sure how response.follow works. I don't see any reason to use it here because you are using the full URL (not only path). 

Try to change it to simple Request 

How to use Selenium with Scrapy for crawling ajax pages

stacker

[How to use Selenium with Scrapy for crawling ajax pages](https://stackoverflow.com/questions/50259652/how-to-use-selenium-with-scrapy-for-crawling-ajax-pages)

I'm new to Scrapy and I need to scrape a page and I'm having trouble crawling the page to be scraped.Without filling any of the fields on the page, and clicking the "PESQUISAR" (translate: search) button directly, I need to scrape all the pages shown below.It looks like my problem is in the page javascript .. and I've never worked with javascript.My main difficulty is tracking this page. So if anyone can help me with this, I appreciate it.Sorry for the bad English, I hope you have understood

2018-05-09 18:11:07Z

I'm new to Scrapy and I need to scrape a page and I'm having trouble crawling the page to be scraped.Without filling any of the fields on the page, and clicking the "PESQUISAR" (translate: search) button directly, I need to scrape all the pages shown below.It looks like my problem is in the page javascript .. and I've never worked with javascript.My main difficulty is tracking this page. So if anyone can help me with this, I appreciate it.Sorry for the bad English, I hope you have understoodYou have to use driver to click on button Pesquisar, call WebDriverWait to wait until the table element with id tblJurisprudencia is present indicating that page is fully loaded to get source-code, them parse Acordão values from the page.

Scrapy: why endless log: Crawled 14091 pages (at 0 pages/min)?

Tony Kyriakidis

[Scrapy: why endless log: Crawled 14091 pages (at 0 pages/min)?](https://stackoverflow.com/questions/50198730/scrapy-why-endless-log-crawled-14091-pages-at-0-pages-min)

when the program runs for a time, the CPU usage is 100%, and the program seems doesn't crawl pages.When i kill the program, I get the following log:

2018-05-06 10:36:18Z

when the program runs for a time, the CPU usage is 100%, and the program seems doesn't crawl pages.When i kill the program, I get the following log:Can you check your file (or include it here) for this: dont_filter=TrueIf thats the case scrapy doesn't filter out the duplicate requests.Please include your scrapy file. 

Scrapy: working correctly in debug mode with break point, and not correctly without a break point

TJ1

[Scrapy: working correctly in debug mode with break point, and not correctly without a break point](https://stackoverflow.com/questions/50207340/scrapy-working-correctly-in-debug-mode-with-break-point-and-not-correctly-with)

I have a Python3 code that uses Scrapy.

When I put a breakpoint inside the def parse(self, response): then the code works as expected. But if I run it without a break-point in normal mode it is not doing what it is supposed to do.

Is there any timing issue that should be aware of?Here is part of my code:When I put a breakpoint at image_link = self.get_image_link(response) then when I run the code step by step, I see that item has some values. But if I put a breakpoint at tmp=1 I see that item is None. Also if I run the code without a breakpoint I don't get the expected result which shows me item is None.

The image_link = self.get_image_link(response) is just finding the URL of an image on the website.What could be the issue and how to get the expected result without running the code in debug mode? 

2018-05-07 05:01:44Z

I have a Python3 code that uses Scrapy.

When I put a breakpoint inside the def parse(self, response): then the code works as expected. But if I run it without a break-point in normal mode it is not doing what it is supposed to do.

Is there any timing issue that should be aware of?Here is part of my code:When I put a breakpoint at image_link = self.get_image_link(response) then when I run the code step by step, I see that item has some values. But if I put a breakpoint at tmp=1 I see that item is None. Also if I run the code without a breakpoint I don't get the expected result which shows me item is None.

The image_link = self.get_image_link(response) is just finding the URL of an image on the website.What could be the issue and how to get the expected result without running the code in debug mode? As pennyliangliping just commented and didn't put his comment in an answer, I will put his comment here as an answer so it helps others who have similar issue:first of all will not raise an exception even the element not found, it just return None, so //*[@id='theid2'] never runs. Since the order of response from start_urls depends on download time, I suspect when you add breakpoint, 1.html back first, item will not be None, otherwise get None, please try replace try except with if else and see.

how to store a table with scrapy?

Roberta Gimenez

[how to store a table with scrapy?](https://stackoverflow.com/questions/50152546/how-to-store-a-table-with-scrapy)

I'm new in Scrapy and I have some troubles to get a table data.

I'm trying to store in a file the table with id = grdTableView_DXMainTable from:

view-source:http://databank.worldbank.org/data/reports.aspx?source=2&series=SE.PRM.NENR&country=I'm using the following code:The resulting html file is empty. Can anyone help me?

2018-05-03 10:17:19Z

I'm new in Scrapy and I have some troubles to get a table data.

I'm trying to store in a file the table with id = grdTableView_DXMainTable from:

view-source:http://databank.worldbank.org/data/reports.aspx?source=2&series=SE.PRM.NENR&country=I'm using the following code:The resulting html file is empty. Can anyone help me?There are some points on your code that aren't correct:1) You are using hxs.select which hasn't be defined in any part of your code.2) The value grdTableView_DXMainTable is not the class name, is the ID. You can extract all the table info by using: 

  response.xpath('//table[@id="grdTableView_DXMainTable"]//td//text()').extract()3) If you want to keep all the HTML code it would be better to do this instead:

    response.xpath('//table[@id="grdTableView_DXMainTable"]').extract_first()

why my scrapy always tell me “TCP connection timed out” in Scrapinghub but working fine in my local Machine

Krishna joshi

[why my scrapy always tell me “TCP connection timed out” in Scrapinghub but working fine in my local Machine](https://stackoverflow.com/questions/50149940/why-my-scrapy-always-tell-me-tcp-connection-timed-out-in-scrapinghub-but-worki)

I am getting following error in app.scrapinghub.com but working fine in my local machine.Note: I am using requests module in python(Scrapy framework) to send request and using BeautifulSoup to parse responseExample Code:

2018-05-03 08:05:32Z

I am getting following error in app.scrapinghub.com but working fine in my local machine.Note: I am using requests module in python(Scrapy framework) to send request and using BeautifulSoup to parse responseExample Code:

How to store the scraped data properly in 2 servers with Scrapy ?

parik

[How to store the scraped data properly in 2 servers with Scrapy ?](https://stackoverflow.com/questions/50158045/how-to-store-the-scraped-data-properly-in-2-servers-with-scrapy)

I use Scrapy and i want to export the items in 2 servers.I use it works for one FEED_URI, but I need to store the results in 2 different servers, I tried :but It stored just in the second one. How can I export the results in 2 servers ?Thanks, 

2018-05-03 14:54:00Z

I use Scrapy and i want to export the items in 2 servers.I use it works for one FEED_URI, but I need to store the results in 2 different servers, I tried :but It stored just in the second one. How can I export the results in 2 servers ?Thanks, That is not supported by default, but it can be done by extending the defaultFeedExporter extension:You may use something like settings['FEED_URI'].split(';') to extract a list of FEED_URIS from that configuration field. And then replace every use of self.urifmt with a loop. For example:Would become:Finally, don't forget to disable the default exporter extension and enable the new one:

Scrappy - Spiders gets stuck after yielding request and callback function is not working

Malik Faiq

[Scrappy - Spiders gets stuck after yielding request and callback function is not working](https://stackoverflow.com/questions/50133885/scrappy-spiders-gets-stuck-after-yielding-request-and-callback-function-is-not)

I have been working on Scrapy from past 2 to 3 weeks with the following dependencies:Now, I have upgraded my dependencies toAfter updating my dependencies, my spiders gets stuck.

It gets stuck after yielding requests, also callback function is not initiated and spider moves to an infinite waiting state.ScrapperRequestCode:Parse Function Code:Scrapper gets stuck after this request:Further no callback function gets initiated and it stuck forever after this error occurs. Any help is highly appreciated, thanks in advance.

2018-05-02 11:43:45Z

I have been working on Scrapy from past 2 to 3 weeks with the following dependencies:Now, I have upgraded my dependencies toAfter updating my dependencies, my spiders gets stuck.

It gets stuck after yielding requests, also callback function is not initiated and spider moves to an infinite waiting state.ScrapperRequestCode:Parse Function Code:Scrapper gets stuck after this request:Further no callback function gets initiated and it stuck forever after this error occurs. Any help is highly appreciated, thanks in advance.you're doing driver.close() in your start_requests() method. However, I think you should not close selenium driver too early. Remove this line and try. Don't close your driver in start_requests.

you can close and quit it like this And your yield dummay_news should be in except

Tor + Mongodb +localhost27017

John

[Tor + Mongodb +localhost27017](https://stackoverflow.com/questions/50069001/tor-mongodb-localhost27017)

When I run my crawler, I get the error message:I am not sure how to resolve this.

2018-04-27 19:19:33Z

When I run my crawler, I get the error message:I am not sure how to resolve this.

Splash doesn't get entire page

P. Daimaou

[Splash doesn't get entire page](https://stackoverflow.com/questions/50087492/splash-doesnt-get-entire-page)

I first run splash on a docker using:When I go to port 8050 and try to render:

 http://warframe.market/ It doesn't render the body. More specifically between <section id="warframe_react"><section>I tried using verbose mode, longer wait times, but it doesn't seem to get it. Any ideas why?

2018-04-29 14:12:31Z

I first run splash on a docker using:When I go to port 8050 and try to render:

 http://warframe.market/ It doesn't render the body. More specifically between <section id="warframe_react"><section>I tried using verbose mode, longer wait times, but it doesn't seem to get it. Any ideas why?Ok I solved it. You need to disable private mode before you render the page in splash (-_-')

Scrapy xpath returning 0 instead of the actual price

NaD

[Scrapy xpath returning 0 instead of the actual price](https://stackoverflow.com/questions/50083325/scrapy-xpath-returning-0-instead-of-the-actual-price)

The site I'm trying to scrape is https://coins.ph. Whenever I try to extract from scrapy shell it returned 0 instead of the actual price which I can see in the Chrome Dev console.Here's the code I'm running:And here's what it looks like in the Chrome Dev Console

2018-04-29 04:15:14Z

The site I'm trying to scrape is https://coins.ph. Whenever I try to extract from scrapy shell it returned 0 instead of the actual price which I can see in the Chrome Dev console.Here's the code I'm running:And here's what it looks like in the Chrome Dev ConsoleThere's an AJAX request being made when you connect to the page.

If you check "Network" tab in your "Web Inspector" (ctrl+shift+i in chromium for example) you can see:So a GET request is being made to this address to retrieve the pricing:https://quote.coins.ph/v1/markets/BTC-PHPAll you need to do is connect to that address and you can load the response with json.loads(response.body) function and parse it as a dict.

Scrapy not scraping all intended urls when ran using default settings

Madhur

[Scrapy not scraping all intended urls when ran using default settings](https://stackoverflow.com/questions/50037745/scrapy-not-scraping-all-intended-urls-when-ran-using-default-settings)

I am trying to scrape urls of the form :https://in.bookmyshow.com/XXXXX/cinemas : where 'XXXXX' is any city name. I have around 880 different city names in a file. And I want to scrap data from each of the url.My Sample Code is as follows : https://www.jdoodle.com/a/u1EFile from which data is read is as follows : https://www.jdoodle.com/a/u1GThe problem that I am facing is that whenever I try to run scrapy using default settings, it runs asynchronously and concurrently. However, in doing so, it misses on half the urls that are to be scraped.Also, if I run scrapy using settings mentioned in option 2 : (see below). It scraps all the urls but this time it takes insane amount of time for completion. Isn't there any way in which I can still run my script concurrently without losing any data to be scraped.Option 1:Settings : DefaultStats :Option 2:Settings : {'AUTOTHROTTLE_ENABLED': True, 'CONCURRENT_REQUESTS': 1, 'DOWNLOAD_DELAY': 3}Stats :

2018-04-26 07:58:39Z

I am trying to scrape urls of the form :https://in.bookmyshow.com/XXXXX/cinemas : where 'XXXXX' is any city name. I have around 880 different city names in a file. And I want to scrap data from each of the url.My Sample Code is as follows : https://www.jdoodle.com/a/u1EFile from which data is read is as follows : https://www.jdoodle.com/a/u1GThe problem that I am facing is that whenever I try to run scrapy using default settings, it runs asynchronously and concurrently. However, in doing so, it misses on half the urls that are to be scraped.Also, if I run scrapy using settings mentioned in option 2 : (see below). It scraps all the urls but this time it takes insane amount of time for completion. Isn't there any way in which I can still run my script concurrently without losing any data to be scraped.Option 1:Settings : DefaultStats :Option 2:Settings : {'AUTOTHROTTLE_ENABLED': True, 'CONCURRENT_REQUESTS': 1, 'DOWNLOAD_DELAY': 3}Stats :Seems like your issue is not delay but concurrency. When you have 1 concurrent request per minute it works, when you have more it doesn't. Most likely reason of this is that the website is serving you content based on your cookies.Try disabling cookies via COOKIES_ENABLED settings in your settings.py file:If you see that content is not being served without cookies at all you need to use cookiejars to start multiple cookie instances that are working in parallel.See: https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#multiple-cookie-sessions-per-spider

store long string in to MySQL using python

itsmnthn

[store long string in to MySQL using python](https://stackoverflow.com/questions/50046129/store-long-string-in-to-mysql-using-python)

It is part of my scrapy project. and it is big so can not able to put whole script here but I can try to make thing clear as possible.

python version is 3.6.2I am trying to store string that have emoji into MySQL database here is my database schemaCreate database:Alter database:Table replyes, threads, user:Insert Query:I am inserting data that have various characters like emoji, special char($ ,\xc2\xa0 ~ \xc2\xa0 \xc2\xa0 ~ \xc2\xa0 \xc2\xa0 \xc2\xa0 etc.).

I have tried many CHARACTER SET for database and also table.And the whole thing is working accept some data is not stored into data baseWorking data:above are working that actually in the databaseNot working datathis is how I creates the insert query from scrapy items all code to store and create insert statement is written in piplines.py under project's pipline

code to store:All Not working data gives same error it looks like belowI have tried many times I have tried to enter data manually by using python shell, i searched on the internet for whole day I have been trying to solve this about four days but nothings actually work

2018-04-26 14:53:41Z

It is part of my scrapy project. and it is big so can not able to put whole script here but I can try to make thing clear as possible.

python version is 3.6.2I am trying to store string that have emoji into MySQL database here is my database schemaCreate database:Alter database:Table replyes, threads, user:Insert Query:I am inserting data that have various characters like emoji, special char($ ,\xc2\xa0 ~ \xc2\xa0 \xc2\xa0 ~ \xc2\xa0 \xc2\xa0 \xc2\xa0 etc.).

I have tried many CHARACTER SET for database and also table.And the whole thing is working accept some data is not stored into data baseWorking data:above are working that actually in the databaseNot working datathis is how I creates the insert query from scrapy items all code to store and create insert statement is written in piplines.py under project's pipline

code to store:All Not working data gives same error it looks like belowI have tried many times I have tried to enter data manually by using python shell, i searched on the internet for whole day I have been trying to solve this about four days but nothings actually work

Scrapy, scraping text in HTML tag when there are no quotation marks?

WebOrCode

[Scrapy, scraping text in HTML tag when there are no quotation marks?](https://stackoverflow.com/questions/50049920/scrapy-scraping-text-in-html-tag-when-there-are-no-quotation-marks)

UPDATE: this number 48 is showed in "Inspect" in Chrome, but not in "View Page Source". Now understand that it is generated by JavaScript and that is why I can not extract it. This is part of HTML that I am trying to scrape   Problem is that I can not get this 48 number.

I think that problem is because there are no "" around 48.

Because I can get "times" text with no problems, and the only difference that I can see is that there are no "" around 48.This is code that is working for "times":For 48:As you can see, 48 is missing.Does anybody have some solution or idea? 

2018-04-26 18:38:51Z

UPDATE: this number 48 is showed in "Inspect" in Chrome, but not in "View Page Source". Now understand that it is generated by JavaScript and that is why I can not extract it. This is part of HTML that I am trying to scrape   Problem is that I can not get this 48 number.

I think that problem is because there are no "" around 48.

Because I can get "times" text with no problems, and the only difference that I can see is that there are no "" around 48.This is code that is working for "times":For 48:As you can see, 48 is missing.Does anybody have some solution or idea? If you look at the body of the page and search for your number you can see that there's some embeded json.

To solve this you can:

When should I close the webdriver?

parik

[When should I close the webdriver?](https://stackoverflow.com/questions/50048640/when-should-i-close-the-webdriver)

I use Scrapy and Selenium for scraping.when i run my spider, with htop i see the instance of my webdriver.I would like to know when should I close the webdriver in my code ? I add this in my code for the end of the script, but I don't know should I close webdriver after having taken each link?  Thanks, 

2018-04-26 17:14:22Z

I use Scrapy and Selenium for scraping.when i run my spider, with htop i see the instance of my webdriver.I would like to know when should I close the webdriver in my code ? I add this in my code for the end of the script, but I don't know should I close webdriver after having taken each link?  Thanks, The question is. Where do you open it?If your webdriver is in the context of spider then ideally you'd want to open it when spider opens and close it when spider closes. You can do that by connectin open_spider and close_spider signals:

use scrapy ltem_loader add_css extract data, but the value is None, how do i set default value

何明扬

[use scrapy ltem_loader add_css extract data, but the value is None, how do i set default value](https://stackoverflow.com/questions/49973026/use-scrapy-ltem-loader-add-css-extract-data-but-the-value-is-none-how-do-i-set)

when I use scrapy item_loader to fill item, the selector extract data is None. Then, I use mysql to save data, but it gives me a mistake: key Error, the reason is this instance no this key. after i researched, there is no solution. Could you help me? Thank you very much!

2018-04-23 03:33:00Z

when I use scrapy item_loader to fill item, the selector extract data is None. Then, I use mysql to save data, but it gives me a mistake: key Error, the reason is this instance no this key. after i researched, there is no solution. Could you help me? Thank you very much!By default scrapy's ItemLoader discards any fields that are NoneTo fix this you need to make sure the loader fallsback to some other value like empty string: ""Now you can see this in action:

Scrapy callback function doesn't work “sometimes”

C. Pave

[Scrapy callback function doesn't work “sometimes”](https://stackoverflow.com/questions/49957880/scrapy-callback-function-doesnt-work-sometimes)

I'm trying to parse a job finding web.

The process is like this;The problem is, the callback doesn't work sometimes.

The job finding website contains 64 jobs, but I get only 49 jobs, so I look at my log.

The [ JobPageRequest ] has been logged exactly 64 times, the same of amount of jobs in the website, but the  [ JobPageParsing ] has been logged only 49 times.  I have tried this several times, and result is exactly the same, 49 pages out of 64 times. The urls that has not been called are exactly the same too, however there is no specific pattern/difference to the pages that has been called successfully in anyway I can see.So, it seems to me that those specific pages are not called for some reasons.Here is the relevant parts of the code.the start_requeststhe parse_listThe parse detail part is not really important, the only relevant part is I start logging the first thing inside the functionhere is my full code in case the error is somewhere else.

2018-04-21 16:29:47Z

I'm trying to parse a job finding web.

The process is like this;The problem is, the callback doesn't work sometimes.

The job finding website contains 64 jobs, but I get only 49 jobs, so I look at my log.

The [ JobPageRequest ] has been logged exactly 64 times, the same of amount of jobs in the website, but the  [ JobPageParsing ] has been logged only 49 times.  I have tried this several times, and result is exactly the same, 49 pages out of 64 times. The urls that has not been called are exactly the same too, however there is no specific pattern/difference to the pages that has been called successfully in anyway I can see.So, it seems to me that those specific pages are not called for some reasons.Here is the relevant parts of the code.the start_requeststhe parse_listThe parse detail part is not really important, the only relevant part is I start logging the first thing inside the functionhere is my full code in case the error is somewhere else.So during I was typing the question, I recognized my error, which is rather silly but might be useful to anyone else.In parse_list function, I have this part of code that detect last job-list pageHere is the my mistake,It was not obvious to be since I experimented and found out that the CloseSpider raise doesn't kill the spider immediately, so I assumed wrongly that if any has been requested before SpiderClose, it will be finished eventually.

datapoint can't be scraped with scrapy and python

Deba

[datapoint can't be scraped with scrapy and python](https://stackoverflow.com/questions/49957978/datapoint-cant-be-scraped-with-scrapy-and-python)

I want to scrape the title of an event. For that I have written the following xpath commands, none of which worked:All the commands above returned an empty list.

2018-04-21 16:40:54Z

I want to scrape the title of an event. For that I have written the following xpath commands, none of which worked:All the commands above returned an empty list.These ones work. It might be due to 503 error. In scrapy shell, use view(response) to check whether you get the page or not. After that, you can go with one of these selectors.Note: If you did not enable USER AGENT in your setting file, that might help you. Or you can change your IP address.Try this xpath to get the title:Here is how you can give a go from any IDE:

Ubuntu 16.04 pyvirtualdisplay ValueError

sretko

[Ubuntu 16.04 pyvirtualdisplay ValueError](https://stackoverflow.com/questions/49924876/ubuntu-16-04-pyvirtualdisplay-valueerror)

I'm getting the following error when trying to start pyvirtualdisplay:I installed Xvfb:I can't figure out why is this happening. I'm using Python3.6, Ubuntu 16.04. I need to run some Scrapy scripts with Selenium and --headless mode isn't appropriate so this seems to be the only option:Any alternatives suggestion or any other help with this?

2018-04-19 15:29:47Z

I'm getting the following error when trying to start pyvirtualdisplay:I installed Xvfb:I can't figure out why is this happening. I'm using Python3.6, Ubuntu 16.04. I need to run some Scrapy scripts with Selenium and --headless mode isn't appropriate so this seems to be the only option:Any alternatives suggestion or any other help with this?

Scrapy / Python: getting the unknown [index] of TR item

Edgar E. 

[Scrapy / Python: getting the unknown [index] of TR item](https://stackoverflow.com/questions/49961214/scrapy-python-getting-the-unknown-index-of-tr-item)

I got this selector:This will return selector list of TRs located in different positions of the page.is there any way I can know the absolute index number of each TR (starting from (//TABLE/TR) so I can address them sequentially later on in another function?

2018-04-21 23:51:58Z

I got this selector:This will return selector list of TRs located in different positions of the page.is there any way I can know the absolute index number of each TR (starting from (//TABLE/TR) so I can address them sequentially later on in another function?Not really sure what you are trying to do here, but following on your comment: I would say you need the following-sibling magic which does what it says:That will get you the following tr after the one you pinpointed.

Scrapy scraping content that is visible sometimes but not others

jkatt

[Scrapy scraping content that is visible sometimes but not others](https://stackoverflow.com/questions/49932855/scrapy-scraping-content-that-is-visible-sometimes-but-not-others)

I am scraping some info off of zappos.com, specifically a part of the details page that displays what customers that view the current item have also viewed. This is one such item listing:

https://www.zappos.com/p/chaco-marshall-tartan-rust/product/8982802/color/725500The thing is that I discovered that the section that I am scraping appears right away on some items, but on others it will only appear after I have refreshed the page 2 or three times.I am using scrapy to scrape and splash to render.The alsoviewimg is one of the elements that I am pulling from the "Customers Who Viewed this Item Also Viewed" section. I have tested pulling this and other elements, all in the scrapy shell with splash rendering to get the dynamic content, and it pulled the content fine, however in the spider it rarely, if ever, gets any hits.Is there something I can set so that it loads the page a couple times to get the content? Or something else that I am missing?

2018-04-20 02:10:30Z

I am scraping some info off of zappos.com, specifically a part of the details page that displays what customers that view the current item have also viewed. This is one such item listing:

https://www.zappos.com/p/chaco-marshall-tartan-rust/product/8982802/color/725500The thing is that I discovered that the section that I am scraping appears right away on some items, but on others it will only appear after I have refreshed the page 2 or three times.I am using scrapy to scrape and splash to render.The alsoviewimg is one of the elements that I am pulling from the "Customers Who Viewed this Item Also Viewed" section. I have tested pulling this and other elements, all in the scrapy shell with splash rendering to get the dynamic content, and it pulled the content fine, however in the spider it rarely, if ever, gets any hits.Is there something I can set so that it loads the page a couple times to get the content? Or something else that I am missing?You should check if the element you're looking for exists. If it doesn't, load the page again.I'd look into why refreshing the page requires multiple attempts, you might be able to solve the problem without this ad-hoc multiple refresh solution.Scrapy How to check if certain class exists in a given elementThis link explains how to see if a class exists.

Scrapy - order of crawled urls

ann

[Scrapy - order of crawled urls](https://stackoverflow.com/questions/49896079/scrapy-order-of-crawled-urls)

I've got an issue with scrapy and python.

I have several links. I crawl data from each of them in one script with the use of loop. But the order of crawled data is random or at least doesn't match to the link.So I can't match url of each subpage with the outputed data.Like: crawled url, data1, data2, data3.

Data 1, data2, data3 => It's ok, because it comes from one loop, but how can I add to the loop current url or can I set the order of link's list? Like first from the list is crawled as first, second is crawled as second...

2018-04-18 09:29:24Z

I've got an issue with scrapy and python.

I have several links. I crawl data from each of them in one script with the use of loop. But the order of crawled data is random or at least doesn't match to the link.So I can't match url of each subpage with the outputed data.Like: crawled url, data1, data2, data3.

Data 1, data2, data3 => It's ok, because it comes from one loop, but how can I add to the loop current url or can I set the order of link's list? Like first from the list is crawled as first, second is crawled as second...time.sleep() - would it be a solution?Ok, It seems that the solution is in settings.py file in scrapy.DOWNLOAD_DELAY = 3   Between requests.It should be uncommented. Defaultly it's commented.

(scrapy) HTTP GET response different from what's being displayed on browser

hsy

[(scrapy) HTTP GET response different from what's being displayed on browser](https://stackoverflow.com/questions/49850523/scrapy-http-get-response-different-from-whats-being-displayed-on-browser)

I am new to web scraping and scrapy.I am trying to scrape items from a website by parsing the GET response which is in json. However, I am noticing that instead of having just the 90 or so elements that are shown on the website, the raw json response contains 140 + elements. Just by inspecting the json array, there doesn't seem to be any difference between items that do end up getting displayed in the browser vs those that don't.Is it possible for me to capture with scrapy the filtered array of items instead of the raw information?So I've realized that when the website loads, it makes 1 request for product details and 1 for stock availability. By cross-checking those responses, I realized that only those products with items available are displayed. Now my questions is, can these 2 requests be handled in one scrapy spider class? 

2018-04-16 06:04:12Z

I am new to web scraping and scrapy.I am trying to scrape items from a website by parsing the GET response which is in json. However, I am noticing that instead of having just the 90 or so elements that are shown on the website, the raw json response contains 140 + elements. Just by inspecting the json array, there doesn't seem to be any difference between items that do end up getting displayed in the browser vs those that don't.Is it possible for me to capture with scrapy the filtered array of items instead of the raw information?So I've realized that when the website loads, it makes 1 request for product details and 1 for stock availability. By cross-checking those responses, I realized that only those products with items available are displayed. Now my questions is, can these 2 requests be handled in one scrapy spider class? I'd recommend scraping all the items, and then filtering them in a custom pipeline.You would simply get the stock data in open_spider(), and filter out the items you don't need in process_item().

Insert statement does not work for datatype mismatch

Deba

[Insert statement does not work for datatype mismatch](https://stackoverflow.com/questions/49847153/insert-statement-does-not-work-for-datatype-mismatch)

I am scraping data from a site using Scrapy and Python and storing the data in a csv file. Then I am trying to fetch values from the csv file and trying to store the values in a mysql database table. The insert statement is neither triggering error nor inserting any data to the database. I checked the data types of fields whose values are coming from the csv. all are strings. All the values stored in csv are in string format. That's why while storing the values in db, it's creating problem for all the datatypes except string/varchar. What should I do now? Apart from varchar, I have columns of int(6) and timestamp datatypes in my database table.import csv

    import re

    import pymysql

    import sysconnection.commit() 

2018-04-15 21:51:17Z

I am scraping data from a site using Scrapy and Python and storing the data in a csv file. Then I am trying to fetch values from the csv file and trying to store the values in a mysql database table. The insert statement is neither triggering error nor inserting any data to the database. I checked the data types of fields whose values are coming from the csv. all are strings. All the values stored in csv are in string format. That's why while storing the values in db, it's creating problem for all the datatypes except string/varchar. What should I do now? Apart from varchar, I have columns of int(6) and timestamp datatypes in my database table.import csv

    import re

    import pymysql

    import sysconnection.commit() Regardless of this particular SQL related error, that is very likely depending on some data mismatch, I can strongly suggest to avoid the step of exporting to CSV and instead adding the scrapy-mysql-pipeline , this will export your scraped items directly into a MySQL table and from there you can move the date easily to other tables or process it ...If you can't use the pipeline and/or you want something more customizable then you can have a look at this answer here on stackoverflow and you'll find useful information on how to write your own custom mysql pipeline...

Scrapy - unable to hit the URL for webscrapping

Soumya Ranjan Sahoo

[Scrapy - unable to hit the URL for webscrapping](https://stackoverflow.com/questions/49853247/scrapy-unable-to-hit-the-url-for-webscrapping)

I was trying to replicate the following scrapy tutorial - http://blog.florian-hopf.de/2014/07/scrapy-and-elasticsearch.html.I get the following error trace while running the spider-Can someone kindly help me fix/understand the issue. 

2018-04-16 08:54:29Z

I was trying to replicate the following scrapy tutorial - http://blog.florian-hopf.de/2014/07/scrapy-and-elasticsearch.html.I get the following error trace while running the spider-Can someone kindly help me fix/understand the issue. It seems like you cannot reach the destination address, what is the output of : wget http://www.meetup.com/robots.txt? Anyway you are trying to reach the http endpoint while meetup has been upgraded to just https, try to change start_urls to https, like :

how to reload response content in scrapy after in each timer?

elidje aka emmanuel

[how to reload response content in scrapy after in each timer?](https://stackoverflow.com/questions/49800714/how-to-reload-response-content-in-scrapy-after-in-each-timer)

I would like to retrieve the contents of a dynamic HTML table after each timer, but the response is always the same after each scrapping;

But I do not know how to do it then I'm new with scrapy; here is my code content 

2018-04-12 15:41:12Z

I would like to retrieve the contents of a dynamic HTML table after each timer, but the response is always the same after each scrapping;

But I do not know how to do it then I'm new with scrapy; here is my code content 

Scrapy Python xpath for <mark>

pradhox

[Scrapy Python xpath for <mark>](https://stackoverflow.com/questions/49758944/scrapy-python-xpath-for-mark)

I would like to extract with scrapy some information contained on  tags with .xpath or .css on Scrapy.But I want to put some condition and I have no idea how to that.

For example, let's see this HTML code of the webpage that I want to scrap.My problem is for each page of the website, values inside <mark> tag are different and I want to extract, for example,  for <mark>2</mark> the value 2 if the previous mark tag contains "Nombre de chambre(s)".For example, I wanted to do something like this:and the result must be 2For the moment I can just scrap with this way:but I don't want to use [int] because for each page the values are different.Is it possible with mark tag?

2018-04-10 16:45:02Z

I would like to extract with scrapy some information contained on  tags with .xpath or .css on Scrapy.But I want to put some condition and I have no idea how to that.

For example, let's see this HTML code of the webpage that I want to scrap.My problem is for each page of the website, values inside <mark> tag are different and I want to extract, for example,  for <mark>2</mark> the value 2 if the previous mark tag contains "Nombre de chambre(s)".For example, I wanted to do something like this:and the result must be 2For the moment I can just scrap with this way:but I don't want to use [int] because for each page the values are different.Is it possible with mark tag?You can create condition directly in your xpath query :Return :The xpath query is looking for an element containing "Nombre de chambre(s)". If it find it, it will get the following "mark" element. From this element, the text will be taken.Cheers :)If you wanna do the same using selector then this is what you can try as well:Output:

scrapy crawl urls retrieved

Samuel M.

[scrapy crawl urls retrieved](https://stackoverflow.com/questions/49811054/scrapy-crawl-urls-retrieved)

I have this snippet of a spider in scrapysince the content I want to query are listed on pages, first I've queried all the pages and retrieved the list of urls and stored them in self.urls what I intend to do once this process is complete is start querying the urls to now retrieve the useful info.Not sure if yield would be the suitable command to use.

2018-04-13 06:58:50Z

I have this snippet of a spider in scrapysince the content I want to query are listed on pages, first I've queried all the pages and retrieved the list of urls and stored them in self.urls what I intend to do once this process is complete is start querying the urls to now retrieve the useful info.Not sure if yield would be the suitable command to use.What you need to do is callback. To call another functions which will take care of the url's which you are trying to scrape.Here, like this

Scrapy pipeline how to save multiple items in multiple tables in a scrapy pipeline

En Ware

[Scrapy pipeline how to save multiple items in multiple tables in a scrapy pipeline](https://stackoverflow.com/questions/49759785/scrapy-pipeline-how-to-save-multiple-items-in-multiple-tables-in-a-scrapy-pipeli)

I am trying to run three spiders at the same time and have the scraped items dumped to three different tables using sqlalchemy connected to postgresql database. 

All three tables are created in the database, but only one table actually gets populated when running all three spiders. I know I'm doing something wrong but can't see it.

2018-04-10 17:33:54Z

I am trying to run three spiders at the same time and have the scraped items dumped to three different tables using sqlalchemy connected to postgresql database. 

All three tables are created in the database, but only one table actually gets populated when running all three spiders. I know I'm doing something wrong but can't see it.

Unable to use “regex” within scrapy

SIM

[Unable to use “regex” within scrapy](https://stackoverflow.com/questions/49763597/unable-to-use-regex-within-scrapy)

How can I use regex within scrapy? I've searched a lot but could not find any good instruction to go with. However, I've tried like following but it throws an exception which I'm gonna paste below.The exception it throws upon execution:The email within my scraper above is just a placeholder. All I wanna know is how can I use regex within scrapy. Thanks for any help.

2018-04-10 22:11:32Z

How can I use regex within scrapy? I've searched a lot but could not find any good instruction to go with. However, I've tried like following but it throws an exception which I'm gonna paste below.The exception it throws upon execution:The email within my scraper above is just a placeholder. All I wanna know is how can I use regex within scrapy. Thanks for any help.A Selector isn't a string, it's an object that knows how to run queries on an HTML string or response object to find sub-elements.Once you've found the element or elements you want (it will find a list of elements if there are any non-singular queries), the extract method will let you get the text of the found element or elements.For example:It's only the last one you can do anything useful to with a regex:

Selecting elements with scrapy-splash and screenshot after

Alex D

[Selecting elements with scrapy-splash and screenshot after](https://stackoverflow.com/questions/49762675/selecting-elements-with-scrapy-splash-and-screenshot-after)

When using scrapy-splash I am trying to go to a dropdown and select "Vanilla" in this case. After selecting from the dropdown I need to take a screenshot which I currently have working. Here is what I have so far.

2018-04-10 20:52:14Z

When using scrapy-splash I am trying to go to a dropdown and select "Vanilla" in this case. After selecting from the dropdown I need to take a screenshot which I currently have working. Here is what I have so far.You can pass a js script to run in your splash_args through the js_source argument. In your case you may want to do something like this:

right codec for json file with 2 language

mohsen74persian

[right codec for json file with 2 language](https://stackoverflow.com/questions/49764405/right-codec-for-json-file-with-2-language)

i am crowling a page with scrapy and i saving the data in a json file.but the data have diffrent codec(persian and english).for persian data encoding with uft-8 works fine but when there is english words between persian words i get an error: utf-8' codec can't encode characters in position 6315-6316: surrogates not allowed.

i have to encode my file because persian words show  as numbers whit out encoding

my json file:i tried uft-16 but is did not work.

2018-04-10 23:52:27Z

i am crowling a page with scrapy and i saving the data in a json file.but the data have diffrent codec(persian and english).for persian data encoding with uft-8 works fine but when there is english words between persian words i get an error: utf-8' codec can't encode characters in position 6315-6316: surrogates not allowed.

i have to encode my file because persian words show  as numbers whit out encoding

my json file:i tried uft-16 but is did not work.If you are crawling the page with scrapy you can simply ask scrapy to generate the json file for you with:in that case scrapy will do the export and you won't have to worry about encoding.Since Scrapy 1.2.0, a new setting FEED_EXPORT_ENCODING is introduced. By specifying it as utf-16, JSON output will not be escaped.That is to add in your settings.py:Take a look at this page for more information. In addition I highly suggest you to take a look at the official intro tutorial here.

Different scenarios of scraping - scrapy

chodi

[Different scenarios of scraping - scrapy](https://stackoverflow.com/questions/49731961/different-scenarios-of-scraping-scrapy)

I am currently working on my scrapy project, where I scrap content from internet forums. I want my application to offer different scenarios of scraping, so I wrote a method that should handle these scenarios based on some input flagsParse topics method should read categories from mongodb and send request to each category and parse posts.

However the parse_topics method is never executed although flag is set.Work executor is called from parse method of my spiderEarlier I did it this way, which workedThis solution is hovewer not generic enough for my project so that I am looking for alternatives of how to achieve this efect in the way I tried (and described in the first part of my post)Any help would be greatly appreciated.

2018-04-09 11:33:32Z

I am currently working on my scrapy project, where I scrap content from internet forums. I want my application to offer different scenarios of scraping, so I wrote a method that should handle these scenarios based on some input flagsParse topics method should read categories from mongodb and send request to each category and parse posts.

However the parse_topics method is never executed although flag is set.Work executor is called from parse method of my spiderEarlier I did it this way, which workedThis solution is hovewer not generic enough for my project so that I am looking for alternatives of how to achieve this efect in the way I tried (and described in the first part of my post)Any help would be greatly appreciated.

Python Scrapy is not getting all html elements from a webpage

SPFort

[Python Scrapy is not getting all html elements from a webpage](https://stackoverflow.com/questions/49738811/python-scrapy-is-not-getting-all-html-elements-from-a-webpage)

I am trying to use Scrapy to get the names of all current WWE superstars from the following url: http://www.wwe.com/superstars

However, when I run my scraper, it does not return any names. I believe (through attempting the problem with other modules) that the problem is that Scrapy is not finding all of the html elements from the page. I attempted the problem with requests and Beautiful Soup, and when I looked at the html that requests got, it was missing important aspects of the html that I was seeing in my browsers inspector. The html containing the names looks like this:My code is posted below. Is there something that I am doing wrong that is causing this not to work?

2018-04-09 17:50:38Z

I am trying to use Scrapy to get the names of all current WWE superstars from the following url: http://www.wwe.com/superstars

However, when I run my scraper, it does not return any names. I believe (through attempting the problem with other modules) that the problem is that Scrapy is not finding all of the html elements from the page. I attempted the problem with requests and Beautiful Soup, and when I looked at the html that requests got, it was missing important aspects of the html that I was seeing in my browsers inspector. The html containing the names looks like this:My code is posted below. Is there something that I am doing wrong that is causing this not to work?Since the content is javascript generated, you have two options: use something like selenium to mimic a browser and parse the html content, or if you can, query an API directly.In this case, this simple solution works:Output (first 10 records):Sounds like the site has dynamic content which maybe loaded using javascript and/or xhr calls. Look into splash it's a javascript render engine that behaves a lot like phantomjs. If you know how to use docker, splash is super simple to setup. After you have splash setup, you'll have to integrate it with scrapy by using the scrapy-splash plugin.

Unable to insert data to MySQL using Python and scrapy pipelines

Hammad

[Unable to insert data to MySQL using Python and scrapy pipelines](https://stackoverflow.com/questions/49738576/unable-to-insert-data-to-mysql-using-python-and-scrapy-pipelines)

I have tried hours to get around this but still cannot make it to work properly. I am using scrapy to scrape data from a website and then trying to insert that into MySQL database. Here is my database code: Here is my pipeline code where I am making insert query and passing to the above class' insert method:This totally works fine on my local machine. But on server I get following error: When I print the params and query from exception block I have this: query variable: params variable: I feel the the tuple data is the culprit of MySQL string breaking somewhere due to quotes. But  I am very new to Python not sure I checked in another question on SO to follow this syntax to insert into MySQL database i.e: The above code works fine on my local machine but fails on server. Please guide me in right direction. 

Thank you very much!

2018-04-09 17:35:07Z

I have tried hours to get around this but still cannot make it to work properly. I am using scrapy to scrape data from a website and then trying to insert that into MySQL database. Here is my database code: Here is my pipeline code where I am making insert query and passing to the above class' insert method:This totally works fine on my local machine. But on server I get following error: When I print the params and query from exception block I have this: query variable: params variable: I feel the the tuple data is the culprit of MySQL string breaking somewhere due to quotes. But  I am very new to Python not sure I checked in another question on SO to follow this syntax to insert into MySQL database i.e: The above code works fine on my local machine but fails on server. Please guide me in right direction. 

Thank you very much!It very much looks like the tuple encapsulation is your issue.  What is the output of:That's "print the (coder's) representation of item['location']" (rather than trying to be smart about printing.If it's the first, then your passed data structure inside of item apparently has an extra layer of encapsulation for which your code does not account.  The quick and dirty approach to get you up and running:Note that this is hardly a robust solution, API-wise: what happens if one of those embedded tuples is zero length?  (Hint: Exception).  I've also not filled out the rest, because it looks like you have some elements in item that are not encapsulated at all, and others which are doubly encapsulated.Additionally, you may have some encoding errors with your data after this as some of your elements are unicode and others are not.  For example:You may want to check exactly what your schema requires.

Scraping links with Scrapy

ghostfkrcb

[Scraping links with Scrapy](https://stackoverflow.com/questions/49694649/scraping-links-with-scrapy)

I am trying to scrape a swedish real estate website www.booli.se . However, i can't figure out how to follow links for each house and extract for example price, rooms, age etc. I only know how to scrape one page and i can't seem to wrap my head around this. I am looking to do something like:So that i can scrape the data and output it to an excel-file. My code for scraping a simple page without following links is as follows:Thankful for any help. Really having trouble getting it all together and outputting specific link contents to a cohesive output-file (excel).

2018-04-06 14:04:29Z

I am trying to scrape a swedish real estate website www.booli.se . However, i can't figure out how to follow links for each house and extract for example price, rooms, age etc. I only know how to scrape one page and i can't seem to wrap my head around this. I am looking to do something like:So that i can scrape the data and output it to an excel-file. My code for scraping a simple page without following links is as follows:Thankful for any help. Really having trouble getting it all together and outputting specific link contents to a cohesive output-file (excel).You could use scrapy's CrawlSpider for following and scraping linksYour code should look like this:And you can run your code via:and import your csv to Excel.

How can I group prices and sellers on Scrapy Python 3.6.4

Rousblack

[How can I group prices and sellers on Scrapy Python 3.6.4](https://stackoverflow.com/questions/49645043/how-can-i-group-prices-and-sellers-on-scrapy-python-3-6-4)

I'm new to Scrapy Python and I would like to know the best way to group prices and sellers.My code:

2018-04-04 07:21:07Z

I'm new to Scrapy Python and I would like to know the best way to group prices and sellers.My code:Solution:

Scrapy code error - inconsistent use of tabs

mageshwaran

[Scrapy code error - inconsistent use of tabs](https://stackoverflow.com/questions/49690100/scrapy-code-error-inconsistent-use-of-tabs)

when I try to run this code, I get this error 

help me with this

2018-04-06 09:54:48Z

when I try to run this code, I get this error 

help me with this

Scrapy unable to get xpath

Valter

[Scrapy unable to get xpath](https://stackoverflow.com/questions/49649014/scrapy-unable-to-get-xpath)

I got this code in this website NTPAnd I am trying to extract the element "Mazda - 3" and I am unable to get it, it return blank.

In the code, the "Mazda - 3" part is in brand value. I get the name and the version value.This is how I implemented:This post is related to this one, it work there, but I realized that I get only part of the data. See here: Scrapy add.xpath or join xpathCan anybody help me please.Thank you in advance.

2018-04-04 10:45:10Z

I got this code in this website NTPAnd I am trying to extract the element "Mazda - 3" and I am unable to get it, it return blank.

In the code, the "Mazda - 3" part is in brand value. I get the name and the version value.This is how I implemented:This post is related to this one, it work there, but I realized that I get only part of the data. See here: Scrapy add.xpath or join xpathCan anybody help me please.Thank you in advance.I'm not sure what ntp is in your code but this should work:

Scrapy item result inside an html code

Valter

[Scrapy item result inside an html code](https://stackoverflow.com/questions/49645306/scrapy-item-result-inside-an-html-code)

it is possible to insert into html code the result of a scraped item? How can I implement it?Something like this:Thank you in advance.

2018-04-04 07:36:51Z

it is possible to insert into html code the result of a scraped item? How can I implement it?Something like this:Thank you in advance.You can use simple string manipulation .

1. '%d' % (description)you will need to have your description variable in string format .

str(description) will work i guess.as suggested in comment you can use python RE https://docs.python.org/3/library/re.html

Scrapy pagination

Javier Jaramillo

[Scrapy pagination](https://stackoverflow.com/questions/49642105/scrapy-pagination)

I am facing a issue with scrapy pagination. Here is the html:Scrapy python approach:I need some help to solve this, when I click next button it should go to the next page. However, I see the next href is on onclick="return false;" I don't know how to do solve this issue. Could you please provide me with some hints how to solve the issue above. Thanks.

2018-04-04 03:08:51Z

I am facing a issue with scrapy pagination. Here is the html:Scrapy python approach:I need some help to solve this, when I click next button it should go to the next page. However, I see the next href is on onclick="return false;" I don't know how to do solve this issue. Could you please provide me with some hints how to solve the issue above. Thanks.Learn how to use Inspect in Chrome or Firebug if you have Mozilla.Click on Preserve Logs and then click on next page button, you will see this AJAX POST being fired.

Error downloading PDF files

Eric G

[Error downloading PDF files](https://stackoverflow.com/questions/49497626/error-downloading-pdf-files)

I have the following (simplified) code:When i run it, I get this error: Do I need to introduce a middleware or something to handle this?  This looks like it should be valid, at least by other examples.Note: at the moment I'm not using a pipeline because there in my real spider I have a lot of checks on whether the related item has been scraped, validating if this pdf belongs to the item, and checking a custom name of a pdf to see if it was downloaded.  And as mentioned, many samples did what I'm doing here so I thought it would be easier and work.

2018-03-26 17:51:13Z

I have the following (simplified) code:When i run it, I get this error: Do I need to introduce a middleware or something to handle this?  This looks like it should be valid, at least by other examples.Note: at the moment I'm not using a pipeline because there in my real spider I have a lot of checks on whether the related item has been scraped, validating if this pdf belongs to the item, and checking a custom name of a pdf to see if it was downloaded.  And as mentioned, many samples did what I'm doing here so I thought it would be easier and work.The issue because of your own scrapy_beautifulsoup\middleware.py which is trying to replace the return response.replace(body=str(BeautifulSoup(response.body, self.parser))). You need to correct that and that should fix the issue

scrapy pipeline returns 'NoneType' object has no attribute '__getitem__'

user2521546

[scrapy pipeline returns 'NoneType' object has no attribute '__getitem__'](https://stackoverflow.com/questions/49511545/scrapy-pipeline-returns-nonetype-object-has-no-attribute-getitem)

In my spider i have two pipelines, One for images one for files,in the spider i check for a result of images like so and also do the same for files.my settings are like so for the PipelinesMy process item in the image pipeline that comes first is like soThis is the same for the files pipelineI do this because i am getting multiple images and can only yield the item once in the spider.What i dont understand is why i am getting an error in the second pipelineif there is no item['file_urls'] the pipeline should not even execute.Im totaly lost as to what is happening. Thanks for any help and i hope my question is not to badly formated..This is where i check for a file that exits and return item if not. maybe i have it wrong where i am in a loop and trying to return the item.

but it comes back to the next pipeline as none

2018-03-27 11:19:36Z

In my spider i have two pipelines, One for images one for files,in the spider i check for a result of images like so and also do the same for files.my settings are like so for the PipelinesMy process item in the image pipeline that comes first is like soThis is the same for the files pipelineI do this because i am getting multiple images and can only yield the item once in the spider.What i dont understand is why i am getting an error in the second pipelineif there is no item['file_urls'] the pipeline should not even execute.Im totaly lost as to what is happening. Thanks for any help and i hope my question is not to badly formated..This is where i check for a file that exits and return item if not. maybe i have it wrong where i am in a loop and trying to return the item.

but it comes back to the next pipeline as noneYour try/except clause isn't right. You must list error types after except instead of saying it what to log. Try this:

Scrapy can't check duplicate item

Octorber12

[Scrapy can't check duplicate item](https://stackoverflow.com/questions/49503883/scrapy-cant-check-duplicate-item)

I'm new to python and I try to write a crawler by scrapy. I've followed the doc scrapy to remove duplicate crawled item but it didn't work. selft.ids_seen return []. Here my pipeline:My setting.pyCan anyone explain me what set() method will return? Thank you so much.

2018-03-27 03:27:09Z

I'm new to python and I try to write a crawler by scrapy. I've followed the doc scrapy to remove duplicate crawled item but it didn't work. selft.ids_seen return []. Here my pipeline:My setting.pyCan anyone explain me what set() method will return? Thank you so much.

Scrapy redirects to logout before data is scrapped

Matt Turner

[Scrapy redirects to logout before data is scrapped](https://stackoverflow.com/questions/49506025/scrapy-redirects-to-logout-before-data-is-scrapped)

The title might be a bit confusing but let me explain some more. I'm trying to build a simple scraper using scrapy to scrape a banking website for some automated budgeting. So far it seems that I can get logged in but then immediately after I get logged out without getting the data I need. Here's some text from my terminal:Line 4 is where it starts to redirect me. Here's my code:I'm so far just trying to get a single number from the site to test whether or not I can actually retrieve data. My login_test returns that I am logging in correctly, but instead of continuing on to the main page it redirects me to logout. I've omitted some info such as my username and password for obvious reasons, and also I changed the websites name. Some help would be much appreciated.

2018-03-27 06:45:27Z

The title might be a bit confusing but let me explain some more. I'm trying to build a simple scraper using scrapy to scrape a banking website for some automated budgeting. So far it seems that I can get logged in but then immediately after I get logged out without getting the data I need. Here's some text from my terminal:Line 4 is where it starts to redirect me. Here's my code:I'm so far just trying to get a single number from the site to test whether or not I can actually retrieve data. My login_test returns that I am logging in correctly, but instead of continuing on to the main page it redirects me to logout. I've omitted some info such as my username and password for obvious reasons, and also I changed the websites name. Some help would be much appreciated.You are redirected to logout because it detects that you are a robot.You can try to set ROBOTSTXT_OBEY to False  For more information see the Doc 

Scrapy Link Extractor Rules

Matthew Morrissey

[Scrapy Link Extractor Rules](https://stackoverflow.com/questions/49436971/scrapy-link-extractor-rules)

I have a spider setup using link extractor rules.  The spider crawls and scrapes the items that I expect, although it will only follow the 'Next' pagination button to the 3rd page, where the spider then finishes without any errors, there are a total of 50 pages to crawl via the 'Next' pagination.  Here is my code:It feels like I may be missing a setting or something as the code functions as expected for the first 3 iterations.  My settings file does not override the DEPTH_LIMIT default of 0.  Any help is greatly appreciated, thank you.EDIT 1

It appears it may not have anything to do with my code as if I start with a different product page I can get up to 8 pages scraped before the spider exits.  Not sure if its the site I am crawling or how to troubleshoot?EDIT 2

Troubleshooting it some more it appears that my 'Next' link disappears from the web page.  When I start on the first page the pagination element is present to go to the next page.  When I view the response for the next page there are no products and no next link element so the spider thinks it it done.  I have tried enabling cookies to see if the site is requiring a cookie in order to paginate.  That doesnt have any affect.  Could it be a timing thing?EDIT 3

I have adjusted the download delay and concurrent request values to see if that makes a difference.  I get the same results whether I pull the page of data in 1 second or 30 minutes.  I am assuming 30 minutes is slow enough as I can manually do it faster than that.EDIT 4

Tried to enable cookies along with the cookie middleware in debug mode to see if that would make a difference.  The cookies are fetched and sent with the requests but I get the same behavior when trying to navigate pages.

2018-03-22 19:29:29Z

I have a spider setup using link extractor rules.  The spider crawls and scrapes the items that I expect, although it will only follow the 'Next' pagination button to the 3rd page, where the spider then finishes without any errors, there are a total of 50 pages to crawl via the 'Next' pagination.  Here is my code:It feels like I may be missing a setting or something as the code functions as expected for the first 3 iterations.  My settings file does not override the DEPTH_LIMIT default of 0.  Any help is greatly appreciated, thank you.EDIT 1

It appears it may not have anything to do with my code as if I start with a different product page I can get up to 8 pages scraped before the spider exits.  Not sure if its the site I am crawling or how to troubleshoot?EDIT 2

Troubleshooting it some more it appears that my 'Next' link disappears from the web page.  When I start on the first page the pagination element is present to go to the next page.  When I view the response for the next page there are no products and no next link element so the spider thinks it it done.  I have tried enabling cookies to see if the site is requiring a cookie in order to paginate.  That doesnt have any affect.  Could it be a timing thing?EDIT 3

I have adjusted the download delay and concurrent request values to see if that makes a difference.  I get the same results whether I pull the page of data in 1 second or 30 minutes.  I am assuming 30 minutes is slow enough as I can manually do it faster than that.EDIT 4

Tried to enable cookies along with the cookie middleware in debug mode to see if that would make a difference.  The cookies are fetched and sent with the requests but I get the same behavior when trying to navigate pages.To check if the site denies too many requests in short time you can add the following code to your spider (e.g. before your rulesstatement) and play around with the value. See also the excellent documentation.

Scrapy spider logger doesn't recognize extra argument

amarynets

[Scrapy spider logger doesn't recognize extra argument](https://stackoverflow.com/questions/49439487/scrapy-spider-logger-doesnt-recognize-extra-argument)

I tried to add some context information to Scrapy spider log message and send it to Logstash, like this:But it didn't work because I have a value from extra into args variable of LogstashFormaters(args({'status': response.status}).

But if I use directly module like:It works well. What do I do wrong?

2018-03-22 22:30:57Z

I tried to add some context information to Scrapy spider log message and send it to Logstash, like this:But it didn't work because I have a value from extra into args variable of LogstashFormaters(args({'status': response.status}).

But if I use directly module like:It works well. What do I do wrong?

Starting out with Python and Scrapy Nothing returned on scrape

Fiona Stone

[Starting out with Python and Scrapy Nothing returned on scrape](https://stackoverflow.com/questions/49440084/starting-out-with-python-and-scrapy-nothing-returned-on-scrape)

Sorry for the newbie question i'm just trying to get to grips and everything was going well until i got stuck on this as it's not giving me any errors just giving no result.Been working from the https://doc.scrapy.org/ and just a little stuck with this one, any advice would be amazing and possibly if there is any error handling that could help along the way.Thanks!

2018-03-22 23:28:15Z

Sorry for the newbie question i'm just trying to get to grips and everything was going well until i got stuck on this as it's not giving me any errors just giving no result.Been working from the https://doc.scrapy.org/ and just a little stuck with this one, any advice would be amazing and possibly if there is any error handling that could help along the way.Thanks!Your parse method should be indented as a member of the QuotesSpider class:

scrapy crash on i am try to scrape large file list urls [closed]

Julian Medic

[scrapy crash on i am try to scrape large file list urls [closed]](https://stackoverflow.com/questions/49342937/scrapy-crash-on-i-am-try-to-scrape-large-file-list-urls)

I need to scrape large list urls, this is my actual variable for start_urls:when i execute the crowl the system kill scrapy for missing memory, is possible do stream file on scrapy ? without load all urls on memory ?

2018-03-17 23:26:17Z

I need to scrape large list urls, this is my actual variable for start_urls:when i execute the crowl the system kill scrapy for missing memory, is possible do stream file on scrapy ? without load all urls on memory ?Mistake #1: .readlines(). File can be arbitrary large and this operation will read it entirely into memory. Since you are iterating it anyway, removing readlines() is absolutely safe:start_urls = [url.strip() for url in open('urls.txt','r')]Mistake #2: using list instead of an iterator. Lists are still stored completely in memory; iterators are used on demand (only one element is stored). If you don't iterate this list several times, it should be safe to change to an iterator:start_urls = (url.strip() for url in open('urls.txt','r'))

Scrapy for dynamic data website using Angular or VueJs

Sreejith Sasidharan

[Scrapy for dynamic data website using Angular or VueJs](https://stackoverflow.com/questions/49383023/scrapy-for-dynamic-data-website-using-angular-or-vuejs)

How do I scrape data using Scrapy Framework from websites which loads data using javascript frameworks? Scrapy download the html from each page requests but some website uses js frameworks like Angular or VueJs which will load data separately. I have tried using a combination of Scrapy,Selenium and chrome driver to retrieve the htmls which gives the rendered html with content. But when using this method I am not able to retain the session cookies set for selecting country and currency as each request is handled by a separate instance of selenium or chrome. Please suggest if there is any alternative options to scrape the dynamic content while retaining the session. Adding the code which i used to set the country and currency 

2018-03-20 11:27:13Z

How do I scrape data using Scrapy Framework from websites which loads data using javascript frameworks? Scrapy download the html from each page requests but some website uses js frameworks like Angular or VueJs which will load data separately. I have tried using a combination of Scrapy,Selenium and chrome driver to retrieve the htmls which gives the rendered html with content. But when using this method I am not able to retain the session cookies set for selecting country and currency as each request is handled by a separate instance of selenium or chrome. Please suggest if there is any alternative options to scrape the dynamic content while retaining the session. Adding the code which i used to set the country and currency what you said is not correct, You can continue to use Selenium and i suggest you to use phantomJS instead of chrome. 

i can't help more because you didn't put your code.one example for phantomJS:and if you don't want to use Selenium, you can use Splash as @Granitosaurus said in this question

Scrapy - Data Extraction (as json)

M.liza

[Scrapy - Data Extraction (as json)](https://stackoverflow.com/questions/49344639/scrapy-data-extraction-as-json)

I am trying to scrap two web pages by using scrapy but i am not getting the expected output. Also trying to extract the data as json file but output file is blank. So far i have tried below mentioned code:In order to get the output as json i have used below command:But i am not getting any output except some log detail which is mentioned below:I am new in python, so it would be really great if someone help me to solve the problem.

2018-03-18 05:07:02Z

I am trying to scrap two web pages by using scrapy but i am not getting the expected output. Also trying to extract the data as json file but output file is blank. So far i have tried below mentioned code:In order to get the output as json i have used below command:But i am not getting any output except some log detail which is mentioned below:I am new in python, so it would be really great if someone help me to solve the problem.Take a look at the following lines in the log:To use scrapy you must implement a parse() callback which you did, but in Python, indentation matters. The way that you indented puts the parse() function outside of the PlantSpider class definition and explains why you are not getting the desired output.

Keeping proper JSON structure when using JSONlines to scrape large amounts of data

Daniel Johnson Maia

[Keeping proper JSON structure when using JSONlines to scrape large amounts of data](https://stackoverflow.com/questions/49391892/keeping-proper-json-structure-when-using-jsonlines-to-scrape-large-amounts-of-da)

Recently I've been having to scrape significantly larger amounts of data and changed from using the feed format 'json' to 'jsonlines' to avoid having it all scrambled and duplicated. The issue is that now none of my programs recognize the exported files as JSON since it removes the beginning and end square brackets and the comma after each item. The first example shows what the data looks like, the second what I would like to achieve. Is there a way of manually adding the commas and make it an array while still using the JsonLinesItemExporter? The only piece of code from my crawler i'd imagine is relevant is my yield keyword but i'm happy to show the full code. I'm not using PHP or MySQL.Thank you very much in advance.

2018-03-20 18:39:20Z

Recently I've been having to scrape significantly larger amounts of data and changed from using the feed format 'json' to 'jsonlines' to avoid having it all scrambled and duplicated. The issue is that now none of my programs recognize the exported files as JSON since it removes the beginning and end square brackets and the comma after each item. The first example shows what the data looks like, the second what I would like to achieve. Is there a way of manually adding the commas and make it an array while still using the JsonLinesItemExporter? The only piece of code from my crawler i'd imagine is relevant is my yield keyword but i'm happy to show the full code. I'm not using PHP or MySQL.Thank you very much in advance.First, the commas.The nicest solution would be to wrap JsonLinesItemExporter so that it adds a comma at the end of each item. If the appropriate method isn't exposed in a way that you can override it, super it, and add the comma, you may have to reimplement the method in your subclass, or even monkeypatch the exporter class. Less nice.Alternatively, you can hook the file you pass into the exporter to make writes do a replace('\n', ',\n'). This is hacky, so I wouldn't do it if you can hook the exporter instead, but it does have the virtue of being simple.Now, the brackets at start and end of file. Without knowing the library you're using or the way you're using it, this will be pretty vague.If you're using a single "session" of the exporter per file—that is, you open it at startup, write a bunch of items to it, then close it, and never re-open it and append to it, this is pretty easy. Let's assume you solved the first problem by subclassing the exporter class to hook its writes, something like this:I'm guessing at what the implementation looks like, but you've already discovered the right thing to do, so you should be able to translate from my guess to reality. Now, you need to add two methods like this: You may need a flush somewhere before the _writebytes if the exporter class has its own buffer inside it, but that's the only extra complexity I'd expect to see.If you're reopening files and appending to them in each session, this obviously won't work. You could do something like this pseudocode in __init__:That has the advantage of being transparent to your client code, but it's a bit hacky. If your client code knows when it's opening a new file rather than appending to an old one, and knows when it's finishing off a file for good, it's probably cleaner to add addStartMarker and addEndMarker methods and call those, or just have the client manually write the brackets to the file before initializing/after closing the exporter.

log_count/ERROR while scraping site with Scrapy

Deba

[log_count/ERROR while scraping site with Scrapy](https://stackoverflow.com/questions/49351946/log-count-error-while-scraping-site-with-scrapy)

I am getting the following log_count/ERROR while scraping a site with Scrapy. I can see that it has made 43 requests and got 43 responses. Everything looks fine. Then what the error for?:and this is my code for spider:I am absolutely new to scraping world. How to overcome this error?

2018-03-18 19:22:45Z

I am getting the following log_count/ERROR while scraping a site with Scrapy. I can see that it has made 43 requests and got 43 responses. Everything looks fine. Then what the error for?:and this is my code for spider:I am absolutely new to scraping world. How to overcome this error?That output shows that 21 error was logged; you can also see all of those were AttributeErrors.If you look at the rest of the log output, you will see the errors themselves:From this, you can see that your regex for end_date_final doesn't always find a match.

shutil copy files but deletes data

Daniel

[shutil copy files but deletes data](https://stackoverflow.com/questions/49352076/shutil-copy-files-but-deletes-data)

The following code works sometimes, it pushes it to the directory it needs to be in and IT always copy the file, however sometimes there is no data in the csv files in the leadparser directory. 

2018-03-18 19:34:59Z

The following code works sometimes, it pushes it to the directory it needs to be in and IT always copy the file, however sometimes there is no data in the csv files in the leadparser directory. You have to close the file, to get all data written:

Python Scheduler - “Process finished” after first job, wont run following jobs

Hendrik J.

[Python Scheduler - “Process finished” after first job, wont run following jobs](https://stackoverflow.com/questions/49316461/python-scheduler-process-finished-after-first-job-wont-run-following-jobs)

I implemented a web-crawler using Python and Scrapy (https://scrapy.org/).There are three different processes which need to run successively. The first one writes profile keys to a MongoDB collection, the second one crawls all subdomains for a given profile and writes them in another collection, and the third one writes specific profile data to a SQL database.Now I want to run these processes automatically. I looked at the module Schedule (https://github.com/dbader/schedule, https://schedule.readthedocs.io/en/stable/index.html) and wrote the following code:When running this code, the first job gets started at the given time and I get a Process finished with exit code 0. After that the scheduler stops and the other jobs don't run.I suspect that this is the normal behaviour for "Process finished", but is there any way around it?

Or should i use some different scheduling module?I am not deploying the spiders via scrapyd, because my third process is not a spider.Thank you in advance!

2018-03-16 08:48:57Z

I implemented a web-crawler using Python and Scrapy (https://scrapy.org/).There are three different processes which need to run successively. The first one writes profile keys to a MongoDB collection, the second one crawls all subdomains for a given profile and writes them in another collection, and the third one writes specific profile data to a SQL database.Now I want to run these processes automatically. I looked at the module Schedule (https://github.com/dbader/schedule, https://schedule.readthedocs.io/en/stable/index.html) and wrote the following code:When running this code, the first job gets started at the given time and I get a Process finished with exit code 0. After that the scheduler stops and the other jobs don't run.I suspect that this is the normal behaviour for "Process finished", but is there any way around it?

Or should i use some different scheduling module?I am not deploying the spiders via scrapyd, because my third process is not a spider.Thank you in advance!I suggest you read this part of Scrapy documentation:https://doc.scrapy.org/en/latest/topics/practices.htmlSpecifically the part where it says: running the spiders sequentially by chaining the deferreds.What you can do is chain both spiders and data export script into one runner script. Than use the python scheduler module to call that script periodically.

Scrapy spider doesn't call 'callback' function

Elgin Cahangirov

[Scrapy spider doesn't call 'callback' function](https://stackoverflow.com/questions/49350210/scrapy-spider-doesnt-call-callback-function)

I'm trying to perform the crawl with the spider below, but it doesn't call 'callback' function. My spider:When I run this spider it doesn't print 'lalala' in the terminal, i.e. doesn't call 'parse_screener' function. I wrote this spider as exactly shown in the documentation. What's the problem?

2018-03-18 16:25:11Z

I'm trying to perform the crawl with the spider below, but it doesn't call 'callback' function. My spider:When I run this spider it doesn't print 'lalala' in the terminal, i.e. doesn't call 'parse_screener' function. I wrote this spider as exactly shown in the documentation. What's the problem?The problem is your allow clause. It's regex so you must escape special symbols like '?'. This works fine (backslash before ?):

Request not getting expected response

Felipe Trenk

[Request not getting expected response](https://stackoverflow.com/questions/49323689/request-not-getting-expected-response)

I'm using scrapy, a python framework for web crawling to extract info from a website.The website I want to extract the information from is: http://apps.who.int/classifications/icd10/browse/2016/enFrom the index tree in the left of the website, I want to extract both L## codes as well as L##.# codes with their names (where L represents a letter and # a number). So the first two extractions I want are:The problem is, when I open the scrapy shell and run:The response don't seem to include the index I want. Why isn't scrapy able to get this index and is there a way to fix it?

2018-03-16 14:54:40Z

I'm using scrapy, a python framework for web crawling to extract info from a website.The website I want to extract the information from is: http://apps.who.int/classifications/icd10/browse/2016/enFrom the index tree in the left of the website, I want to extract both L## codes as well as L##.# codes with their names (where L represents a letter and # a number). So the first two extractions I want are:The problem is, when I open the scrapy shell and run:The response don't seem to include the index I want. Why isn't scrapy able to get this index and is there a way to fix it?If you  view source of the url you will not find the text or element you are looking for. Probably they are generated by javascript on load.

So you have to look into the javascript code that is doing it. 

How to extract a number using scrapy?

boi

[How to extract a number using scrapy?](https://stackoverflow.com/questions/49266200/how-to-extract-a-number-using-scrapy)

Hello and thanks for taking the time to read this. I am quite new to scrapy and am trying to scrape just one number on a website. I tried using: but this seems only to work for text as using with this number (which is contained within span) a the value of none is returned. Thanks in advance for any help!

2018-03-13 21:33:41Z

Hello and thanks for taking the time to read this. I am quite new to scrapy and am trying to scrape just one number on a website. I tried using: but this seems only to work for text as using with this number (which is contained within span) a the value of none is returned. Thanks in advance for any help!

Scrapy: crawled and scraped 0 items

SY9

[Scrapy: crawled and scraped 0 items](https://stackoverflow.com/questions/49273719/scrapy-crawled-and-scraped-0-items)

I'm trying to get companies' info from government website and using Scrapy for it.My spider code is the following one. It doesn't show any error when running but doesn't extract any info.

"Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)" message is shown in command line after running."items.py" file is as followsOutput is as follows.Any advice would be highly appreciated. Thanks in advance.

2018-03-14 09:17:54Z

I'm trying to get companies' info from government website and using Scrapy for it.My spider code is the following one. It doesn't show any error when running but doesn't extract any info.

"Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)" message is shown in command line after running."items.py" file is as followsOutput is as follows.Any advice would be highly appreciated. Thanks in advance.You dont have def start_requets(self) but start_url so Scrapy will scrape URLs from list start_url and use your callback method parse.I mean, you are missing def parse(self, response) change def crawling(self, response) to def parse(self, response)Also your code's logic is completely wrong, just think of code's flow before writing it.Put the page in start_url  which has company links, listing page i meanThen create def parse(self, response) and create a for loop to iterate over each company link.Because Scrapy, by default, read the first addresses to scrape in start_urls (not start_url) and start parsing with parse method (not crawling). Try a rename operation and relaunch your spider.

irregular Python/Scrapy scraping behaviour: Status 200 but data not always returned

Simon Ridley

[irregular Python/Scrapy scraping behaviour: Status 200 but data not always returned](https://stackoverflow.com/questions/49209771/irregular-python-scrapy-scraping-behaviour-status-200-but-data-not-always-retur)

I've written a Python Scrapy spider to scrape 60,000 pages from a website. Each of the items extracted work greats, and when scraping approximately 4000 pages, I have no issues, I get a 200 status from each page and all data required is extracted. However when I start to scrape 8000 pages upwards, I am sometimes able to scrape all the data with 200 status codes, but on other occasions I get very few items returned. The output still displays all 8000 urls and a 200 status code for each, but very few associated items extracted.I notice that if the crawler has successfully scraped all 8000 pages, and is then run almost immediately again, this behaviour occurs. Its the same behaviour seen when trying to scrape all 60,000 pages. I have the following enabled within the settings.py file:ROBOTSTXT_OBEY = TrueMy code is as follows:Successful scrape log:Ironically at the time of updating this, I'm unable to produce an unsuccessful scrape log as the spider scrapes the 8000 odd pages at the moment. Could this the website managing the volume of traffic its receiving, therefore the behaviour of the spider is sporadic?  Any ideas on how to resolve this would be greatly appreciated.

2018-03-10 13:46:47Z

I've written a Python Scrapy spider to scrape 60,000 pages from a website. Each of the items extracted work greats, and when scraping approximately 4000 pages, I have no issues, I get a 200 status from each page and all data required is extracted. However when I start to scrape 8000 pages upwards, I am sometimes able to scrape all the data with 200 status codes, but on other occasions I get very few items returned. The output still displays all 8000 urls and a 200 status code for each, but very few associated items extracted.I notice that if the crawler has successfully scraped all 8000 pages, and is then run almost immediately again, this behaviour occurs. Its the same behaviour seen when trying to scrape all 60,000 pages. I have the following enabled within the settings.py file:ROBOTSTXT_OBEY = TrueMy code is as follows:Successful scrape log:Ironically at the time of updating this, I'm unable to produce an unsuccessful scrape log as the spider scrapes the 8000 odd pages at the moment. Could this the website managing the volume of traffic its receiving, therefore the behaviour of the spider is sporadic?  Any ideas on how to resolve this would be greatly appreciated.

how to remove first characters of an item within array in python scrapy

BARNOWL

[how to remove first characters of an item within array in python scrapy](https://stackoverflow.com/questions/49209595/how-to-remove-first-characters-of-an-item-within-array-in-python-scrapy)

I am trying to remove the first 7 characters of an item within an array, more specifically im trying to remove "mailto" so it will just render out the emailI thought using [:7] would do the trick however python ignores the request.any suggestions ?this is where i need to remove the characters

2018-03-10 13:26:50Z

I am trying to remove the first 7 characters of an item within an array, more specifically im trying to remove "mailto" so it will just render out the emailI thought using [:7] would do the trick however python ignores the request.any suggestions ?this is where i need to remove the charactersClearly business.css('a.email-business::attr(href)').extract() returns a list. You need to remove mailto: from an item in list.OrYou need to use [7:] and not [:7]the syntax is [<start>:<end>] and when omitted it will be automatically from start or end of the string.For example:The count starts at 0:So you probably need

How to iterate multiple URLs in Scrapy and save them after each iteration

Dhaval Thakkar

[How to iterate multiple URLs in Scrapy and save them after each iteration](https://stackoverflow.com/questions/49213861/how-to-iterate-multiple-urls-in-scrapy-and-save-them-after-each-iteration)

I am trying to crawl all historical coin data from https://coinmarketcap.com. So, I am trying to scrape data using scrapy. I am able to scrape all data from the website but I am unable to save them all. It only saves about 2000 entries whereas actually, it might be above 20000.Also, I think the code that I've written can be optimized but I am unable to do so.The folder format is :This is the utils.py file code: This is the hist.py file code: Run the scrapy file above using:And here is the csv file link:

https://drive.google.com/file/d/13UR5TWGEfz124R9yRaYvafbfxGvCZ6vZ/view?usp=sharingAny help is appreciated!

2018-03-10 20:45:19Z

I am trying to crawl all historical coin data from https://coinmarketcap.com. So, I am trying to scrape data using scrapy. I am able to scrape all data from the website but I am unable to save them all. It only saves about 2000 entries whereas actually, it might be above 20000.Also, I think the code that I've written can be optimized but I am unable to do so.The folder format is :This is the utils.py file code: This is the hist.py file code: Run the scrapy file above using:And here is the csv file link:

https://drive.google.com/file/d/13UR5TWGEfz124R9yRaYvafbfxGvCZ6vZ/view?usp=sharingAny help is appreciated!The problem might lie in the fact that you're overwriting the output .csv file for each crawled URL.Try to substitutewithEDIT:This appends data to the file.

Scrapy LinkExtractor - restrict_paths - exclude tags

Milano

[Scrapy LinkExtractor - restrict_paths - exclude tags](https://stackoverflow.com/questions/49221014/scrapy-linkextractor-restrict-paths-exclude-tags)

When extracting links from a webpage, I want to get all links except those inside the header. To not extract links from the header, I've created an XPath which matches everything except header or footer:The problem is that when I put as a restricted_paths argument, it doesn't work correctly. I get even more links than I get without specifying restricted_paths argument. returns 120 links.returns 199 links including those inside header (<div id="header"> .... </div>)

2018-03-11 14:20:48Z

When extracting links from a webpage, I want to get all links except those inside the header. To not extract links from the header, I've created an XPath which matches everything except header or footer:The problem is that when I put as a restricted_paths argument, it doesn't work correctly. I get even more links than I get without specifying restricted_paths argument. returns 120 links.returns 199 links including those inside header (<div id="header"> .... </div>)

Substring before and after xpath - using scrapy

vforbiedronka

[Substring before and after xpath - using scrapy](https://stackoverflow.com/questions/49223081/substring-before-and-after-xpath-using-scrapy)

I am using scrapy to scrap a list of movies:On a desired page you have date and title of a movie stuck together. Using xpath i wanted to extract date which is in between parentheses. However i am constanlty recieving a syntax error. Why is that? Or maybe any other good ideas to scrap a year in which movie was created?

2018-03-11 17:40:37Z

I am using scrapy to scrap a list of movies:On a desired page you have date and title of a movie stuck together. Using xpath i wanted to extract date which is in between parentheses. However i am constanlty recieving a syntax error. Why is that? Or maybe any other good ideas to scrap a year in which movie was created?You should correctly mix single and double quotes in your XPath. Try

How to get HtmlResponse type in custom CacheStorage in scrapy

rrmerugu

[How to get HtmlResponse type in custom CacheStorage in scrapy](https://stackoverflow.com/questions/49215863/how-to-get-htmlresponse-type-in-custom-cachestorage-in-scrapy)

i am writing a custom cache backend in scrapy and I want to save the body into elasticsearch for full-text search. The default response type i am getting is scrapy.http.response.Response which contains bytes and when i try to encode to string, it goes to something like this \u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0003�}�r۸��o�j�\u0001f���\u0019�C�-۲��|N2;�q�Φ���\u000b\"!�6Er\bҲN���\u001a�o�e\u001fe�d�\u001b EJ�,[ά�Ιql\u0012\u0004\u001a�F���h . So the question is; how can I can get HtmlResponse type response in the store_response, so that I can get the actual text ! I looked into settings, count find any. 

2018-03-11 01:53:06Z

i am writing a custom cache backend in scrapy and I want to save the body into elasticsearch for full-text search. The default response type i am getting is scrapy.http.response.Response which contains bytes and when i try to encode to string, it goes to something like this \u001f�\b\u0000\u0000\u0000\u0000\u0000\u0000\u0003�}�r۸��o�j�\u0001f���\u0019�C�-۲��|N2;�q�Φ���\u000b\"!�6Er\bҲN���\u001a�o�e\u001fe�d�\u001b EJ�,[ά�Ιql\u0012\u0004\u001a�F���h . So the question is; how can I can get HtmlResponse type response in the store_response, so that I can get the actual text ! I looked into settings, count find any. COMPRESSION_ENABLED': False in the settings.py fixed this issue. 

Scrapy Crawl Page and Supage but crawls only one item

user1796346

[Scrapy Crawl Page and Supage but crawls only one item](https://stackoverflow.com/questions/49221269/scrapy-crawl-page-and-supage-but-crawls-only-one-item)

I Have an issue with my Spider. I tried to follow some tutorial to understand the scrapy a little bit better and extended the tutorial to crawl also subpages. The issue of my spider is that it only crawls one element of the entry page and not 25 as it should be on the page. I have no clue where the failure is. Perhaps somebody of you can help me here:Thanks for your help.

BR

2018-03-11 14:46:38Z

I Have an issue with my Spider. I tried to follow some tutorial to understand the scrapy a little bit better and extended the tutorial to crawl also subpages. The issue of my spider is that it only crawls one element of the entry page and not 25 as it should be on the page. I have no clue where the failure is. Perhaps somebody of you can help me here:Thanks for your help.

BRThanks for your comments:

Indeed I have to yield request it, rather return request.

Now it is working.

scrapy selector fails when large lines are present response

Ayushman Koul

[scrapy selector fails when large lines are present response](https://stackoverflow.com/questions/49157923/scrapy-selector-fails-when-large-lines-are-present-response)

Originally encoutered by github_user when scraping Amazon Restaurant

This page contains multiple script tag with lines greater then 64,000 character in one line.

The selector (xpath and css) does not search beyond these lines.Due to this the following xpath '//h1[contains(@class, "hw-dp-restaurant-name")]/text()' to extract name of the restaurant returns empty even though there is a matching tag is present.Please help me how to fix this bug.Thank you in advance

2018-03-07 17:29:08Z

Originally encoutered by github_user when scraping Amazon Restaurant

This page contains multiple script tag with lines greater then 64,000 character in one line.

The selector (xpath and css) does not search beyond these lines.Due to this the following xpath '//h1[contains(@class, "hw-dp-restaurant-name")]/text()' to extract name of the restaurant returns empty even though there is a matching tag is present.Please help me how to fix this bug.Thank you in advance

scrapy spider not following link to another page

BARNOWL

[scrapy spider not following link to another page](https://stackoverflow.com/questions/49177153/scrapy-spider-not-following-link-to-another-page)

I'm referencing from this tutorial and it works for getting data on the first page, and the following a link afterwards.However, in my example, I am trying to check if listing has 3 things before I click on the listing link:If so, I want scrapy to click on the business link that goes to the business profile where I am able to retrieve the email.After that, I want scrapy to go back to the main page and repeat the process for the rest of 19 listings on that page.Yet, it outputs a list of duplicates like this:Any suggestions on how to improve the code?

2018-03-08 15:49:10Z

I'm referencing from this tutorial and it works for getting data on the first page, and the following a link afterwards.However, in my example, I am trying to check if listing has 3 things before I click on the listing link:If so, I want scrapy to click on the business link that goes to the business profile where I am able to retrieve the email.After that, I want scrapy to go back to the main page and repeat the process for the rest of 19 listings on that page.Yet, it outputs a list of duplicates like this:Any suggestions on how to improve the code?Read the guide thorougly before doing a spider. To populate items, you should use an Item Loader, that can have post and pre-processors useful for your purpose.

For duplication, you can use a custom pipeline.

Scrapy pagination is not working and optimized spider

Samsul Islam

[Scrapy pagination is not working and optimized spider](https://stackoverflow.com/questions/49100849/scrapy-pagination-is-not-working-and-optimized-spider)

Please help me to optimize my scrapy spider. Specially next page pagination is not working. There are lot of page per page has 50 items.

I catch first page 50 items(link) in parse_items and next page items also scrap in parse_items.

2018-03-04 21:49:29Z

Please help me to optimize my scrapy spider. Specially next page pagination is not working. There are lot of page per page has 50 items.

I catch first page 50 items(link) in parse_items and next page items also scrap in parse_items.When I try running your code (after fixing the many indentation, spelling and letter case errors), this line is shown in scrapy's log:Scrapy will filter duplicate requests by default, and your parse_items2() method does nothing but create duplicate requests. I fail to see any reason for that method existing.What you should do instead is specify the ˙parse()` method as callback for your requests, and avoid having an extra method that does nothing:Try this for pagination:

Scrapy InitSpider unable to click correct login button

ToriTompkins

[Scrapy InitSpider unable to click correct login button](https://stackoverflow.com/questions/49111534/scrapy-initspider-unable-to-click-correct-login-button)

I am trying to log into a Tor forum with a Scrapy InitSpider however I have encountered the following problem.Below is my code that handles the login:And I receive the following error:Once I remove the attribute 'value': 'Login' from the clickdata list the spider then selects the first clickable element on the page, and not the element that would log in, and the login fails.Below is the relevant HTML for the login portion of the page:Could anyone tell me how I can resolve this? Thanks!

2018-03-05 13:26:19Z

I am trying to log into a Tor forum with a Scrapy InitSpider however I have encountered the following problem.Below is my code that handles the login:And I receive the following error:Once I remove the attribute 'value': 'Login' from the clickdata list the spider then selects the first clickable element on the page, and not the element that would log in, and the login fails.Below is the relevant HTML for the login portion of the page:Could anyone tell me how I can resolve this? Thanks!I don't know why clickdata don't work (it's really poorly described and may be it only works with name attribute) but this code works for me:

How to let Scrapy access Tor after deploy to Scapinghub

Terence Goh

[How to let Scrapy access Tor after deploy to Scapinghub](https://stackoverflow.com/questions/49114184/how-to-let-scrapy-access-tor-after-deploy-to-scapinghub)

I had configure the spider to access the Tor with setup Privoxy but this only work when I use in localhost as the setting I configure is pointed to 127.0.0.1: port. But when i deploy to the Scapinghub, the server side do not setup tor and privoxy as i do. Is that any solution that i can use to let the spider go through my machine through my network and port ?As i know, if on the same network, we can use the internal IP. Can I just replace the public IP to 127.0.0.1 but i wonder how the network to forward to which machine. Below is the configuration to access tor:middlewares.pysetting.py 

2018-03-05 15:44:12Z

I had configure the spider to access the Tor with setup Privoxy but this only work when I use in localhost as the setting I configure is pointed to 127.0.0.1: port. But when i deploy to the Scapinghub, the server side do not setup tor and privoxy as i do. Is that any solution that i can use to let the spider go through my machine through my network and port ?As i know, if on the same network, we can use the internal IP. Can I just replace the public IP to 127.0.0.1 but i wonder how the network to forward to which machine. Below is the configuration to access tor:middlewares.pysetting.py You can deploy a custom docker image with tor set up on it.

And then point to the 127.0.0.1. 

https://shub.readthedocs.io/en/stable/deploy-custom-image.html#deploy-custom-image

Fold second-level links recursively in Scrapy

tat

[Fold second-level links recursively in Scrapy](https://stackoverflow.com/questions/49018580/fold-second-level-links-recursively-in-scrapy)

Using Scrapy, I am trying to scrape a link network from Wikipedia across all languages. Each Wikipedia page should contain a link to a Wikidata item that uniquely identifies the topic of the page across all languages. The process I am trying to implement looks like this:Basically, I want to skip over the intermediate link on a given source page and instead grab its corresponding Wikidata link.Here is the (semi-working) code that I have so far:The spider runs and scrapes links for a while (the scraped item count typically reaches 620 or so), but eventually it builds up a massive queue, stops scraping altogether, and just continues to crawl. Should I expect it to begin scraping again at some point?It seems as though there should be an easy way to do this kind of second-level scraping in Scrapy, but the other questions I've read so far seem to be mostly about how to handle paging in Scrapy, but not how to "fold" a link in this way.    

2018-02-27 21:55:31Z

Using Scrapy, I am trying to scrape a link network from Wikipedia across all languages. Each Wikipedia page should contain a link to a Wikidata item that uniquely identifies the topic of the page across all languages. The process I am trying to implement looks like this:Basically, I want to skip over the intermediate link on a given source page and instead grab its corresponding Wikidata link.Here is the (semi-working) code that I have so far:The spider runs and scrapes links for a while (the scraped item count typically reaches 620 or so), but eventually it builds up a massive queue, stops scraping altogether, and just continues to crawl. Should I expect it to begin scraping again at some point?It seems as though there should be an easy way to do this kind of second-level scraping in Scrapy, but the other questions I've read so far seem to be mostly about how to handle paging in Scrapy, but not how to "fold" a link in this way.    As long you spider has no issue, what you really want is that when you run It should yield sooner then the queued requests of below typeIf you look at the documentationhttps://doc.scrapy.org/en/latest/topics/request-response.htmlSo you will use andThis will make sure that the scraper gives priority to links which will results in data to scraped first

Scrapy not working on OBD site

traiantomescu

[Scrapy not working on OBD site](https://stackoverflow.com/questions/49027294/scrapy-not-working-on-obd-site)

I'm trying to use scrapy-spider on oneblockdown.it to get all the products from the latest products and to store them into a DB.Some sites into my monitor are working, but someone such as OBD is not working and not uploading nothing to the db. This is my function:I guess I'm using the wrong xpath, but I can't solve it

2018-02-28 10:28:42Z

I'm trying to use scrapy-spider on oneblockdown.it to get all the products from the latest products and to store them into a DB.Some sites into my monitor are working, but someone such as OBD is not working and not uploading nothing to the db. This is my function:I guess I'm using the wrong xpath, but I can't solve itFirst of all the site is Cloudflare protected (prevent scraping).Also you have several issues with your code:You should start all your xpaths with '.' when using a relative selector like product:Otherwise, It will try to get the element with this xpath: /body/div[@class='catalogue-product-cover']

scrapy deny certain length of words

Omega

[scrapy deny certain length of words](https://stackoverflow.com/questions/49027413/scrapy-deny-certain-length-of-words)

I am trying to implement a deny rule where i don't want to crawl a certain length of words. Example:https://example.com/a/commentshttps://example.com/z/bloghttps://example.com/t/publicthis is my deny rule:i want to deny anything that 1-2 letter short between * / *.with this command it basically just ends the crawl instantly with no errors.Thank you UPDATEIf there is 1 or 2 letters between the slashes (For example the letter a: "https://example/a/comments")  i don't want to crawl it.I know i can deny comments (this is easy) but there also "https://example.com/a/all"... using same letter a but different path after it..

2018-02-28 10:34:07Z

I am trying to implement a deny rule where i don't want to crawl a certain length of words. Example:https://example.com/a/commentshttps://example.com/z/bloghttps://example.com/t/publicthis is my deny rule:i want to deny anything that 1-2 letter short between * / *.with this command it basically just ends the crawl instantly with no errors.Thank you UPDATEIf there is 1 or 2 letters between the slashes (For example the letter a: "https://example/a/comments")  i don't want to crawl it.I know i can deny comments (this is easy) but there also "https://example.com/a/all"... using same letter a but different path after it..There are two problems with your pattern:This pattern should work as intended: r'/[a-zA-Z]{1,2}/' (using a raw string for simplicity).Also (not a problem here), (a) is the same as a, if you want a one-tuple, you should use (a,).

Scrapy LinkExtractor Specific Url

Snooze

[Scrapy LinkExtractor Specific Url](https://stackoverflow.com/questions/49053035/scrapy-linkextractor-specific-url)

i'm using to crawl a website. However, the current code redirects me and does not crawl from the URL I want. URL: 

http://www.example.com/book/diff/ 

Where diff can be anything except /. 

To add on, I only want to crawl url that match the url.Here is my current code:

2018-03-01 15:26:39Z

i'm using to crawl a website. However, the current code redirects me and does not crawl from the URL I want. URL: 

http://www.example.com/book/diff/ 

Where diff can be anything except /. 

To add on, I only want to crawl url that match the url.Here is my current code:This should be enough.

Python download image on folder

Ciungan Vali

[Python download image on folder](https://stackoverflow.com/questions/49030840/python-download-image-on-folder)

I have a problem with Python and Scrapy, i maked the script is still working and put all the data on MongoDB, but when he scraping he still take the photos only in database but i want to download in this structure /Project/photos/link-page/name.jpgYou have my code here!

This is Itmes.pyThis is from setting.pyHere i have the scrapper.py

2018-02-28 13:34:32Z

I have a problem with Python and Scrapy, i maked the script is still working and put all the data on MongoDB, but when he scraping he still take the photos only in database but i want to download in this structure /Project/photos/link-page/name.jpgYou have my code here!

This is Itmes.pyThis is from setting.pyHere i have the scrapper.pyIf you want to store your images  like: {IMAGES_STORE}/link-page/name.jpg, you will need to extend the default ImagesPipeline class and override the method file_path. For example:And then include it as a pipeline in your settings file instead of the default ImagePipeline:

Running multiple instances of a CrawlSpider

Pantheos Max

[Running multiple instances of a CrawlSpider](https://stackoverflow.com/questions/48982416/running-multiple-instances-of-a-crawlspider)

i'm just getting started using scrapy and i'd like to do the followingto do this, the Spider needs to receive the domain it has to crawl as an argument.I already successfully created the CrawlSpider:If i call it with scrapy crawl subsites -a starturl=http://example.com -a allowed=example.com -o output.jl

the result is exactly as i want it, so this part is fine already.What i fail to do is create multiple instances of SubsiteSpider, each with a different domain as argument.I tried (in SpiderRunner.py)Variant:But i get an error that occurs, i presume, because the argument is not properly passed to __init__, for example TypeError: __init__() missing 1 required positional argument: 'allowed' or TypeError: __init__() missing 2 required positional arguments: 'starturl' and 'allowed'

(Loop is yet to be implemented)So, here are my questions:

1) What is the proper way to pass arguments to init, if i do not start crawling via scrapy shell, but from within python code?

2) How can i also pass the -o output.jl argument? (or maybe, use allowed argument as filename?)

3) I am fine with this running each spider after another - would it still be considered best / good practice to do it that way? Could you point to a more extensive tutorial about "running the same spider again and again, with different arguments(=target domains), optionally parallel", if there is one?Thank you all very much in advance!

If there are any spelling mistakes (not an english native speaker), or if question / details are not precise enough, please tell me how to correct them.

2018-02-26 06:08:54Z

i'm just getting started using scrapy and i'd like to do the followingto do this, the Spider needs to receive the domain it has to crawl as an argument.I already successfully created the CrawlSpider:If i call it with scrapy crawl subsites -a starturl=http://example.com -a allowed=example.com -o output.jl

the result is exactly as i want it, so this part is fine already.What i fail to do is create multiple instances of SubsiteSpider, each with a different domain as argument.I tried (in SpiderRunner.py)Variant:But i get an error that occurs, i presume, because the argument is not properly passed to __init__, for example TypeError: __init__() missing 1 required positional argument: 'allowed' or TypeError: __init__() missing 2 required positional arguments: 'starturl' and 'allowed'

(Loop is yet to be implemented)So, here are my questions:

1) What is the proper way to pass arguments to init, if i do not start crawling via scrapy shell, but from within python code?

2) How can i also pass the -o output.jl argument? (or maybe, use allowed argument as filename?)

3) I am fine with this running each spider after another - would it still be considered best / good practice to do it that way? Could you point to a more extensive tutorial about "running the same spider again and again, with different arguments(=target domains), optionally parallel", if there is one?Thank you all very much in advance!

If there are any spelling mistakes (not an english native speaker), or if question / details are not precise enough, please tell me how to correct them.There are a few problems with your code:You can achieve the same effect using custom_settings, giving each instance a different FEED_URI setting.

Can't scrape the links to next pages when using xPath selectors, returns empty. (Using Scrapy)

Ro991

[Can't scrape the links to next pages when using xPath selectors, returns empty. (Using Scrapy)](https://stackoverflow.com/questions/48967679/cant-scrape-the-links-to-next-pages-when-using-xpath-selectors-returns-empty)

I am using Scrapy and trying to scrape this url, when I request any data about the products on the page I get it out. But the div with the paginator class and id=paginator1 is returned as empty even though it is a table with references to next pages. I have tried using xPath selectors for the table and css selectors, but both return empty. 

This is what I tried, using cssIn [29]: response.css('span a::attr(href)').extract()

Out[29]: 

['/registration/formregistration/new',

 '/',

 '/catalog/solntsezaschitnye_ochki',

 'http://wezom.com.ua/prodvizhenie']

andIn [31]: response.xpath('//*[@id="paginator1"]/table/tbody/tr[1]/td[2]/span')

Out[31]: []



2018-02-24 21:14:49Z

I am using Scrapy and trying to scrape this url, when I request any data about the products on the page I get it out. But the div with the paginator class and id=paginator1 is returned as empty even though it is a table with references to next pages. I have tried using xPath selectors for the table and css selectors, but both return empty. 

This is what I tried, using cssIn [29]: response.css('span a::attr(href)').extract()

Out[29]: 

['/registration/formregistration/new',

 '/',

 '/catalog/solntsezaschitnye_ochki',

 'http://wezom.com.ua/prodvizhenie']

andIn [31]: response.xpath('//*[@id="paginator1"]/table/tbody/tr[1]/td[2]/span')

Out[31]: []

The pagination is generated using JavaScript, as you can see in the HTML:You can extract all of the relevant information out of the <script> block:You can then build the pagination URLs according to the logic in the pagination script (or just see what the URLs look like).If you take a look at the actual html source (response.text), you will see the following:As you can see, the div is indeed empty, and being populated through javascript.You have two options to get those links:

Delay per each thread in Scrapy

Dmitrii Mikhailov

[Delay per each thread in Scrapy](https://stackoverflow.com/questions/48970724/delay-per-each-thread-in-scrapy)

DOWNLOAD_DELAY parameter sets a delay per each downloaded page. But what I would like to have instead is a delay set per each scrapy thread. Is there any way to do that?

2018-02-25 06:06:10Z

DOWNLOAD_DELAY parameter sets a delay per each downloaded page. But what I would like to have instead is a delay set per each scrapy thread. Is there any way to do that?

Login to website using scrapy

haider

[Login to website using scrapy](https://stackoverflow.com/questions/48966964/login-to-website-using-scrapy)

I am writing a spider. In which I am trying to scraping a website using scraping by logging into that website. I have write a spider but still getting problem in logging into the website. I had write the whole spider but can't resolve the issue of getting logging in. Please have a look at my code.

2018-02-24 19:52:45Z

I am writing a spider. In which I am trying to scraping a website using scraping by logging into that website. I have write a spider but still getting problem in logging into the website. I had write the whole spider but can't resolve the issue of getting logging in. Please have a look at my code.The problem is simple: you've created the login() method, but you never call it.The simplest way to solve this is to rename that method to start_requests().

This method will then be called by scrapy to generate the initial requests, instead of generating them from start_urls.

Scrapy doesn't scrap items from my URLs : Crawled (200) / Referer : None

VioGeo

[Scrapy doesn't scrap items from my URLs : Crawled (200) / Referer : None](https://stackoverflow.com/questions/48989448/scrapy-doesnt-scrap-items-from-my-urls-crawled-200-referer-none)

I'm trying to scrap several pages from a website. For this, I've got different start URLs and a method to crawl the next pages. 

The issue is that the spider doesn't scrap the items and doesn't seem to crawl indicated pages. I've got no outcome.

Do you have any idea to solve this ?Here is the codeExtract of the shell window

2018-02-26 13:15:39Z

I'm trying to scrap several pages from a website. For this, I've got different start URLs and a method to crawl the next pages. 

The issue is that the spider doesn't scrap the items and doesn't seem to crawl indicated pages. I've got no outcome.

Do you have any idea to solve this ?Here is the codeExtract of the shell windowCopying element xpath from your browser's developer tools will give you something that matches only that 1 element.

Even then, browsers sometimes need to modify the html to be able to display it, and since your xpath is super-specific, there's a possibility that you won't get even that 1 match.How to fix this?Take a look at the html, find relevant elements, classes and ids, and write an xpath yourself.

For example, something as simple as //tr matches all the elements you're trying to match with //*[@id="td-outer-wrap"]/div[3]/div/div/div[1]/div/div[2]/div[3]/table/tbody/tr.As @stranac said, the issue comes from the Xpath. Currently, when I copied the Xpath of my element in Google console there was a tbody tag. But this tag doesn't in the source code. As @gangabass explained here, this is "a common problem: sometimes there is no tbody tag in source HTML for tables (modern browsers add it to the DOM automatically)". I removed it, the extraction works, but it doesn't organise as I want (one line for one event) I have all the extract data in one cell. 

Scrapy iterate through starting urls and domains

Anthony

[Scrapy iterate through starting urls and domains](https://stackoverflow.com/questions/48933464/scrapy-iterate-through-starting-urls-and-domains)

I am attempting to read a list of urls and domains from csv and have a Scrapy spider iterate through the list of domains and starting urls with the goal of having all urls within that domain exported to a csv file through my pipeline.When I run the spider it will either give me an indention error, or when I make adjustments to indention then it will only recognize the last domain in the list.Any recommendations on how to accomplish this are appreciated.  

2018-02-22 17:13:54Z

I am attempting to read a list of urls and domains from csv and have a Scrapy spider iterate through the list of domains and starting urls with the goal of having all urls within that domain exported to a csv file through my pipeline.When I run the spider it will either give me an indention error, or when I make adjustments to indention then it will only recognize the last domain in the list.Any recommendations on how to accomplish this are appreciated.  This code you pasted has terrible indentation I am not surprised the interpreter complains. But most likely this is your problem:It creates a new list containing just one domain and assigns it to allowed_domains. So the last domain overrides everything that was saved there before. Fix it by doing:or even like that (without the loop):Fix your indentation and try this:

Test or Mock Scrapy Pipeline

Malik A. Rumi

[Test or Mock Scrapy Pipeline](https://stackoverflow.com/questions/48939892/test-or-mock-scrapy-pipeline)

I was looking into testing the scrapy pipeline, (I already know the spider works) when it occurred to me I could just use a local copy of a page from the target website instead of repeatedly hitting it with my spider online. But I did not see anything suggesting that option. Is there some reason why this won't work, or is not a best practice?

2018-02-23 01:38:35Z

I was looking into testing the scrapy pipeline, (I already know the spider works) when it occurred to me I could just use a local copy of a page from the target website instead of repeatedly hitting it with my spider online. But I did not see anything suggesting that option. Is there some reason why this won't work, or is not a best practice?

calling another page on click from the result of first click in scrapy-splash

Priyanka D

[calling another page on click from the result of first click in scrapy-splash](https://stackoverflow.com/questions/48942238/calling-another-page-on-click-from-the-result-of-first-click-in-scrapy-splash)

I am scrapping a website where multiple ajax calls are made and contents are displayed.First i am navigating to a page by clicking an option from the home page.From the resulting HTML,i have to make another click and navigate to another page.How to do this using scrapy-splash?Here is my code

2018-02-23 06:19:06Z

I am scrapping a website where multiple ajax calls are made and contents are displayed.First i am navigating to a page by clicking an option from the home page.From the resulting HTML,i have to make another click and navigate to another page.How to do this using scrapy-splash?Here is my code

How to scrape product details which is inbetween <div> tag using Xpath

VASUDHA INAMADUGU

[How to scrape product details which is inbetween <div> tag using Xpath](https://stackoverflow.com/questions/48945364/how-to-scrape-product-details-which-is-inbetween-div-tag-using-xpath)

I am trying to get the word BIBA. I am getting output like \n,\n,\nBIBA,\n But I only want "BIBA".

Please help me to get that name using xpath.Thank you.

2018-02-23 10:06:38Z

I am trying to get the word BIBA. I am getting output like \n,\n,\nBIBA,\n But I only want "BIBA".

Please help me to get that name using xpath.Thank you.I highly recommend to use Scrapy Item Loaders and Input and Output processors:How about this:

My Scrapy item['img_urls'] doesn't download the file

Jordan Guillonneau

[My Scrapy item['img_urls'] doesn't download the file](https://stackoverflow.com/questions/48906294/my-scrapy-itemimg-urls-doesnt-download-the-file)

I'm currently working on a student's data scientist project which consist of building a fish recognition system by picture. We will use tensorflow to make sense from data & scrapy to find a massive amount of data (fish picture & his scientific name).I'm new to scrapy, but I've been working a lot since 3 days, I 've written a basic fishbase spider (you'll find the url in the spider's code):Here is the item file :and the setting file :I'm getting the results I want, but the images won't download. I don't understand why... Plus, I've downloaded a buck of images from other sites.

2018-02-21 12:45:10Z

I'm currently working on a student's data scientist project which consist of building a fish recognition system by picture. We will use tensorflow to make sense from data & scrapy to find a massive amount of data (fish picture & his scientific name).I'm new to scrapy, but I've been working a lot since 3 days, I 've written a basic fishbase spider (you'll find the url in the spider's code):Here is the item file :and the setting file :I'm getting the results I want, but the images won't download. I don't understand why... Plus, I've downloaded a buck of images from other sites.There are two problems:

scrapy pipelines cant insert data into sqlite3

wen

[scrapy pipelines cant insert data into sqlite3](https://stackoverflow.com/questions/48941220/scrapy-pipelines-cant-insert-data-into-sqlite3)

It works fine when creating table but doesn't insert any data at all. Does anyone know what might be the issues?This is my pipelineThis is itemsThis is the main programmetell me if there is any info missing 

2018-02-23 04:37:51Z

It works fine when creating table but doesn't insert any data at all. Does anyone know what might be the issues?This is my pipelineThis is itemsThis is the main programmetell me if there is any info missing cool, the logs do show the problem 

Unexpected Behavior When Scraping Pages with Scrapy

Keenan Burke-Pitts

[Unexpected Behavior When Scraping Pages with Scrapy](https://stackoverflow.com/questions/48916745/unexpected-behavior-when-scraping-pages-with-scrapy)

I am struggling to get my code to extract out data from each json object of the current request call and then once it goes through each json object, move on to the next request for the next batch of json objects.  It appears that my script just scrapes the first request call over and over.  Can someone assist me what I'm missing in my for loop and/or while loop?  Thanks in advance!!

2018-02-21 22:39:44Z

I am struggling to get my code to extract out data from each json object of the current request call and then once it goes through each json object, move on to the next request for the next batch of json objects.  It appears that my script just scrapes the first request call over and over.  Can someone assist me what I'm missing in my for loop and/or while loop?  Thanks in advance!!Try running this code. I have only cleaned it a bit.Here are the logs when running it

Parsing stray text with Scrapy

Ace

[Parsing stray text with Scrapy](https://stackoverflow.com/questions/48919935/parsing-stray-text-with-scrapy)

Any idea how to extract 'TEXT TO GRAB' from this piece of markup:

2018-02-22 04:58:52Z

Any idea how to extract 'TEXT TO GRAB' from this piece of markup:It's not an ideal solution but it should do the trick:OR like this:Output:Not ideal:

Python - Scrapy ecommerce website

Slyds

[Python - Scrapy ecommerce website](https://stackoverflow.com/questions/48945567/python-scrapy-ecommerce-website)

I'm trying to scrape the price of this producthttp://www.asos.com/au/fila/fila-vintage-plus-ringer-t-shirt-with-small-logo-in-green/prd/9065343?clr=green&SearchQuery=&cid=7616&gridcolumn=2&gridrow=1&gridsize=4&pge=1&pgesize=72&totalstyles=4699With the following code but it returns an empty arrayAny help is appreciated, Thanks.

2018-02-23 10:17:49Z

I'm trying to scrape the price of this producthttp://www.asos.com/au/fila/fila-vintage-plus-ringer-t-shirt-with-small-logo-in-green/prd/9065343?clr=green&SearchQuery=&cid=7616&gridcolumn=2&gridrow=1&gridsize=4&pge=1&pgesize=72&totalstyles=4699With the following code but it returns an empty arrayAny help is appreciated, Thanks.Because the site is dynamic(this is what I got when I use view(response) command in scrapy shell:



As you can see, the price info doesn't come out. Solutions:

1. splash. 

2. selenium+phantomJSIt might help also by checking this answer:Empty List From Scrapy When Using Xpath to Extract ValuesThe price is later added by the browser which renders the page using javascript code found in the html. If you disable javascript in your browser, you would notice that the page would look a bit different. Also, take a look at the page source, usually that's unaltered, to see that the tag you're looking for doesn't exist (yet).Scrapy doesn't execute any javascript code. It receives the plain html and that's what you have to work with.If you want to extract data from pages which look the same as in the browser, I recommend using an headless browser like Splash (if you're already using scrapy): https://github.com/scrapinghub/splash

You can programaticaly tell it to download your page, render it and select the data points you're interested in.The other way is to check for the request made to the Asos API which asks for the product data. In your case, for this product:

http://www.asos.com/api/product/catalogue/v2/stockprice?productIds=9065343&currency=AUD&keyStoreDataversion=0ggz8b-4.1&store=AUI got this url by taking a look at all the XMLHttpRequest (XHR) requests sent in the Network tab found in Developers Tools (on Google Chrome).You can try to find JSON inside HTML (using regular expression) and parse it:

Scrapy saved scrapped data in csv in one line instead of multiple line

x_P0seid0N_x

[Scrapy saved scrapped data in csv in one line instead of multiple line](https://stackoverflow.com/questions/48927727/scrapy-saved-scrapped-data-in-csv-in-one-line-instead-of-multiple-line)

I'm using scrapy to scrap URL from a website and save the results in a csv file. But it is saving in one line only instead of multiple line.I tried to search for an answer in stackoverflow but in vain.Here is my file: i used : scrapy crawl uom -o uom.csv -t csvwhere did i go wrong in my code?

2018-02-22 12:43:01Z

I'm using scrapy to scrap URL from a website and save the results in a csv file. But it is saving in one line only instead of multiple line.I tried to search for an answer in stackoverflow but in vain.Here is my file: i used : scrapy crawl uom -o uom.csv -t csvwhere did i go wrong in my code?You need to process each URL separatelly:

How to scrape data from multiple pages using Scrapy?

Ayushman Koul

[How to scrape data from multiple pages using Scrapy?](https://stackoverflow.com/questions/48856187/how-to-scrape-data-from-multiple-pages-using-scrapy)

I'm trying to scrape data from multiple pages using Scrapy. I'musing the code below, what am I doing wrong?

2018-02-18 20:23:06Z

I'm trying to scrape data from multiple pages using Scrapy. I'musing the code below, what am I doing wrong?Why do you think you are doing something wrong? Does it show any error? If so, the output should be included in the question in the first place. If it's not doing what you expected, again, you should tell us.Anyway, looking at the code, there are at least two possible errors:

Scrapy: Issues with scraping multiple pages

Morrow

[Scrapy: Issues with scraping multiple pages](https://stackoverflow.com/questions/48852029/scrapy-issues-with-scraping-multiple-pages)

I'm trying to build a spider using Scrapy, that returns the data of multiple pages. So far, I'm good with scraping data from the first page, but I'm having trouble to go further. 

This is my code so far: I tried using a LinkExtractor to grab the url's of the following pages: Therefore, I made sure to adjust the parse function to parse_item in order to prevent overwritting the base function of scrapy. 

I think I'm missing something in the restrict_css argument but I'm not sure what it is. 

2018-02-18 13:08:15Z

I'm trying to build a spider using Scrapy, that returns the data of multiple pages. So far, I'm good with scraping data from the first page, but I'm having trouble to go further. 

This is my code so far: I tried using a LinkExtractor to grab the url's of the following pages: Therefore, I made sure to adjust the parse function to parse_item in order to prevent overwritting the base function of scrapy. 

I think I'm missing something in the restrict_css argument but I'm not sure what it is. Looking at the page source, you can see that the navigation links aren't defined in the html, instead there is a template, which is later populated by javascript:From some simple testing I did, it seems that just adding a page parameter is enough to get to a different page in the listing.

However, it seems that both size and page are limited to 20, so you'll be limited to 400 results from a single search.

Scrapy not returning any scrap item

Vikas Dhiman

[Scrapy not returning any scrap item](https://stackoverflow.com/questions/48819761/scrapy-not-returning-any-scrap-item)

I have just started using Scrapy for Web Scraping. I have read few documents which points to html pages for scraping. I have tried it in eentertainment website, I was trying to scrap only title of the Image. later on Price and image. On writing i am not able to get anything. Can anyone please point where i am doing wrong.Here is the code.and here is the webpage inspect element:-

2018-02-16 03:45:22Z

I have just started using Scrapy for Web Scraping. I have read few documents which points to html pages for scraping. I have tried it in eentertainment website, I was trying to scrap only title of the Image. later on Price and image. On writing i am not able to get anything. Can anyone please point where i am doing wrong.Here is the code.and here is the webpage inspect element:-There are couple of problems with your spider:From what I can tell based on the screenshot provided, you are trying to scrape image titles from the page. For that, and taking into account notes above, see adapted spider code that works:

scraping some dynamic data over and over (a browser like scraper!!)

Num Num

[scraping some dynamic data over and over (a browser like scraper!!)](https://stackoverflow.com/questions/60179282/scraping-some-dynamic-data-over-and-over-a-browser-like-scraper)

i want to scrape some data from two bet websites. (that using javascript.)

these websites data is completely dynamic so they change minutes by minutes. and i need a web scraper to scrape these data and store it in a database. (each site just 5 games and for 12 hours.)but the problem is this: when a scraper wants to extract data from web pages, it crawls into the first page and after that it finished the job, it'll close the first page and go to the second page and do it until last page.

if i scrape data in this way, i should send too much requests to the site and probably will get banned!so i think there might be another way to do this: like a browser, if there were a scraper that opens some tabs and prevent these tabs from suspending and losing connection, data on those tabs would be updated automatically and will extracted simply + the websites won't ban it!so i want to know is there any scraper like this? can selenium/splash + scrapy do something like this? or i should look for something else? (something like browser extensions?)

do you have any suggestions for me? :(thanks and sorry for grammatical problems. :[[

2020-02-12 00:16:00Z

i want to scrape some data from two bet websites. (that using javascript.)

these websites data is completely dynamic so they change minutes by minutes. and i need a web scraper to scrape these data and store it in a database. (each site just 5 games and for 12 hours.)but the problem is this: when a scraper wants to extract data from web pages, it crawls into the first page and after that it finished the job, it'll close the first page and go to the second page and do it until last page.

if i scrape data in this way, i should send too much requests to the site and probably will get banned!so i think there might be another way to do this: like a browser, if there were a scraper that opens some tabs and prevent these tabs from suspending and losing connection, data on those tabs would be updated automatically and will extracted simply + the websites won't ban it!so i want to know is there any scraper like this? can selenium/splash + scrapy do something like this? or i should look for something else? (something like browser extensions?)

do you have any suggestions for me? :(thanks and sorry for grammatical problems. :[[Use scrapy/splash and read this. https://docs.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned

How does scrapy upload files? Just like requests.post(url,files=files)

Dora Liu

[How does scrapy upload files? Just like requests.post(url,files=files)](https://stackoverflow.com/questions/60148554/how-does-scrapy-upload-files-just-like-requests-posturl-files-files)

I'm writing code with scrapy, and I need to upload a file to a url. When I used requsts before, I could use requests.post(url,files=files), but how do I use scrapy to achieve the same effect? I searched the Internet for a long time, but I couldn't find the answer.Here is my sample code:How to use scrapy to upload files？

2020-02-10 10:35:59Z

I'm writing code with scrapy, and I need to upload a file to a url. When I used requsts before, I could use requests.post(url,files=files), but how do I use scrapy to achieve the same effect? I searched the Internet for a long time, but I couldn't find the answer.Here is my sample code:How to use scrapy to upload files？

CSS selector for a single attribute out of multiple attributes

Xquared

[CSS selector for a single attribute out of multiple attributes](https://stackoverflow.com/questions/60041149/css-selector-for-a-single-attribute-out-of-multiple-attributes)

how do you select a single attribute within the elementjust need the img srcseems to be an overlooked question

as all assumed implementations yield the entire tag still

2020-02-03 14:23:09Z

how do you select a single attribute within the elementjust need the img srcseems to be an overlooked question

as all assumed implementations yield the entire tag stillAssuming your element is actually something likethe xpath expressionShould get you xyz.jpg.

Scrapy splash is not able to render javascript

Jimit Bavishi

[Scrapy splash is not able to render javascript](https://stackoverflow.com/questions/60012666/scrapy-splash-is-not-able-to-render-javascript)

I've been learning to scrape social media links from a website that dynamically loads HTML using Javascript. However, when I try to use scrapy-splash, the SplashTextResponse contains the original HTML source code instead.Upon inspecting the Facebook icon in the footer of Doordash page using Google Chrome, I find the HTML that I want to locate programmatically. Note that this element is not present in the source code of Doordash website.What can I do to find such "hidden" links present in webpages that dynamically generate HTML using Javascript?

2020-01-31 23:40:42Z

I've been learning to scrape social media links from a website that dynamically loads HTML using Javascript. However, when I try to use scrapy-splash, the SplashTextResponse contains the original HTML source code instead.Upon inspecting the Facebook icon in the footer of Doordash page using Google Chrome, I find the HTML that I want to locate programmatically. Note that this element is not present in the source code of Doordash website.What can I do to find such "hidden" links present in webpages that dynamically generate HTML using Javascript?

How to get a parameter of a XHR request with scrapy

Sander

[How to get a parameter of a XHR request with scrapy](https://stackoverflow.com/questions/59812274/how-to-get-a-parameter-of-a-xhr-request-with-scrapy)

I need to scrape some documents on the website docplayer. As described here https://stackoverflow.com/a/57380171/6548527, it is possible to grab the PDF url which always has the template 'http://docplayer.net/storage/{0}/{1}/{1}.pdf'.But requests the URL alone results in a 403 Forbidden. There are two headers that are also needed, "s" and "ex". As shown in the image below, when visiting http://docplayer.net/72489212-Excellence-in-prevention-descriptions-of-the-prevention-programs-and-strategies-with-the-greatest-evidence-of-success.html you can see with the network tool the request with the 's' and the 'ex'. 

How can i get these parameters? Result: 

2020-01-19 16:52:34Z

I need to scrape some documents on the website docplayer. As described here https://stackoverflow.com/a/57380171/6548527, it is possible to grab the PDF url which always has the template 'http://docplayer.net/storage/{0}/{1}/{1}.pdf'.But requests the URL alone results in a 403 Forbidden. There are two headers that are also needed, "s" and "ex". As shown in the image below, when visiting http://docplayer.net/72489212-Excellence-in-prevention-descriptions-of-the-prevention-programs-and-strategies-with-the-greatest-evidence-of-success.html you can see with the network tool the request with the 's' and the 'ex'. 

How can i get these parameters? Result: 

How can I get all values by same class through Xpath?

luis liu

[How can I get all values by same class through Xpath?](https://stackoverflow.com/questions/59744585/how-can-i-get-all-values-by-same-class-through-xpath)

Try to learn about Xpath scraping but can not make it.When I use Xpath helper plugin in Chrome, I can get the data like that. about 99 ports, the last one is "$PORT"Xpath info screenshotWith this CODE, only return the "$PORT" to me, and I want to know why I can not get the other 98 ports data from this Xpath?

2020-01-15 02:57:50Z

Try to learn about Xpath scraping but can not make it.When I use Xpath helper plugin in Chrome, I can get the data like that. about 99 ports, the last one is "$PORT"Xpath info screenshotWith this CODE, only return the "$PORT" to me, and I want to know why I can not get the other 98 ports data from this Xpath?Data of your page filled dynamically using Javascript from JSON. But JSON does not load through the XHR. You can find JSON is in HTML and you can extract JSON using Regex and convert JSON to Dictionary.Output:Or you can use Selenium ChromeDriver which load Javascript into HTML. So you can extract those data using lxml.Output:You can download ChromeDriver from here.

Scrapy unable to access child div class

adrian

[Scrapy unable to access child div class](https://stackoverflow.com/questions/59241984/scrapy-unable-to-access-child-div-class)

I am using Scrapy to scrape href link in the table in this webpage https://researchgrant.gov.sg/eservices/advanced-search/?keyword=&source=sharepoint&type=project&status=open&page=2&_pp_projectstatus=&_pp_hiname=ab&_pp_piname=pua&_pp_source=sharepoint&_pp_details=#project. I am able to access the div MVCGridTableHolder_advancesearchawardedprojectsp_ but couldn't access to its child which are the div class row and div style, my attempt is shown below. Is it because of the partial view?html code:Scrapy shell attempt:

2019-12-09 02:59:30Z

I am using Scrapy to scrape href link in the table in this webpage https://researchgrant.gov.sg/eservices/advanced-search/?keyword=&source=sharepoint&type=project&status=open&page=2&_pp_projectstatus=&_pp_hiname=ab&_pp_piname=pua&_pp_source=sharepoint&_pp_details=#project. I am able to access the div MVCGridTableHolder_advancesearchawardedprojectsp_ but couldn't access to its child which are the div class row and div style, my attempt is shown below. Is it because of the partial view?html code:Scrapy shell attempt:If you open browser developer tools in your browser when loading this page, you would see that there is a separate XHR request sent to load that partial view content. You could simulate that request in your code.Example using requests:In Scrapy, you could do it with a FormRequest:

Scrap the data from a website that loads the data from JavaScript / JSON Array (POST request) using Python

Hoque MD Zahidul

[Scrap the data from a website that loads the data from JavaScript / JSON Array (POST request) using Python](https://stackoverflow.com/questions/59150737/scrap-the-data-from-a-website-that-loads-the-data-from-javascript-json-array)

I am trying to scrape the data from this linkI have tried this way I got the HTML dom but the product list dom are missing. Actually I guess the product list dom manages by a JSON array that requests from this link but I am not sure about the product list dom load method. I am right or wrong.

I want to scrape the all product details from this site and export in Excel.

2019-12-03 05:35:49Z

I am trying to scrape the data from this linkI have tried this way I got the HTML dom but the product list dom are missing. Actually I guess the product list dom manages by a JSON array that requests from this link but I am not sure about the product list dom load method. I am right or wrong.

I want to scrape the all product details from this site and export in Excel.The requests library does not load the Javascript. If you want to download completely rendered website, use selenium library : https://selenium-python.readthedocs.io/

Crawler with Web Scrapy - error selecting Tag

Costa.Gustavo

[Crawler with Web Scrapy - error selecting Tag](https://stackoverflow.com/questions/59098070/crawler-with-web-scrapy-error-selecting-tag)

I'm following this tutorial to Crawler using the Scrapy Web Library on this site. Considering the image below, I need to collect the text inside the span tag ("Mãe cria sozinha ...")Using the scrapy shell, I'm trying to use response.css to collect, but I'm returning an empty list:I believe I am wrong in passing the tags, so what is the correct way to do this?

2019-11-29 02:27:49Z

I'm following this tutorial to Crawler using the Scrapy Web Library on this site. Considering the image below, I need to collect the text inside the span tag ("Mãe cria sozinha ...")Using the scrapy shell, I'm trying to use response.css to collect, but I'm returning an empty list:I believe I am wrong in passing the tags, so what is the correct way to do this?If you open the source of the URL using ctrl/cmd + U, you will be unable to find class_band thus your response is returned empty are not getting your desired results. Moreover bstn-hl-title class is also not available in the source of the webpage as well. Thus all fields of item will empty as well. In scrapy you have access to the source that you see in the browser using ctrl/cmd + U.

Scrapy doesn't crawl

luiz coutinho

[Scrapy doesn't crawl](https://stackoverflow.com/questions/59102017/scrapy-doesnt-crawl)

When I run this program, to crawl this page, I get: . 

For example, if i run the same program on this page, I get an exact copy of this page. Why does it not work with the first page?

2019-11-29 09:22:22Z

When I run this program, to crawl this page, I get: . 

For example, if i run the same program on this page, I get an exact copy of this page. Why does it not work with the first page?As a result of missing <base href="https://folhadirigida.com.br/"> in your HTML page, it does not look fine to you. Adding it to your HTML body like this code will make page looks fine.

Can I yield item in Spider Idle

ubuntu

[Can I yield item in Spider Idle](https://stackoverflow.com/questions/58994277/can-i-yield-item-in-spider-idle)

I need to yield saved items in spider Idle. Is it possible?Not working!

2019-11-22 12:25:52Z

I need to yield saved items in spider Idle. Is it possible?Not working!spider_idle is a signal to perform some action when the spider is in idle condition. 

For more information refer here : https://docs.scrapy.org/en/latest/topics/signals.html#spider-idleYou cannot yield the item in spiders_close as all the results will go to parse method.

Python web scraper issues

Davey Boy

[Python web scraper issues](https://stackoverflow.com/questions/58957612/python-web-scraper-issues)

I had a web scraper coded for me earlier on in the year via Upwork.

The person who coded the scraper is no longer available.

The error message is as follows:I have no idea what these error messages mean.Help (and thanks).

Dave.

2019-11-20 15:12:25Z

I had a web scraper coded for me earlier on in the year via Upwork.

The person who coded the scraper is no longer available.

The error message is as follows:I have no idea what these error messages mean.Help (and thanks).

Dave.You are trying to run the script with bash; since you're missing a shebang it's not interpreted via python.Please add the shebang #!/usr/bin/env python to the top of your file alternatively run it via python your_script.py

How can I convert a string to a number in xpath to run a calculation against it?

Annie Cushing

[How can I convert a string to a number in xpath to run a calculation against it?](https://stackoverflow.com/questions/58731481/how-can-i-convert-a-string-to-a-number-in-xpath-to-run-a-calculation-against-it)

The Goal

I'm trying to: 1) scrape all instances of price on a page; 2) find the count of prices to get the count of products on a page; 3) calculate the average price. Why I'm Doing This

I want to drop the xpath expression into Screaming Frog to extract the count and avg into separate columns when I crawl the site.Where I'm Stuck

I can't calculate the avg b/c the prices are strings, not integers.Contrived Ex

Since I can't share my real ex, I've replicated the issue with a scrape of this Hipmunk query: https://www.hipmunk.com/flights#f=NYC;t=SEA;d=2019-11-08;r=2019-11-10;is_search_for_business=false.The xpath I'm using to grab the prices: //div[@class = 'FlightPrice']You can see from the Results pane in the screenshot below, it works swimmingly.screenshot of functional xpathIs it possible to convert the currency strings to an integer so I can run calculations on them? I tried wrapping it in a number() function but just got an NaN error. Not sure if it's because of the $s.

2019-11-06 13:38:24Z

The Goal

I'm trying to: 1) scrape all instances of price on a page; 2) find the count of prices to get the count of products on a page; 3) calculate the average price. Why I'm Doing This

I want to drop the xpath expression into Screaming Frog to extract the count and avg into separate columns when I crawl the site.Where I'm Stuck

I can't calculate the avg b/c the prices are strings, not integers.Contrived Ex

Since I can't share my real ex, I've replicated the issue with a scrape of this Hipmunk query: https://www.hipmunk.com/flights#f=NYC;t=SEA;d=2019-11-08;r=2019-11-10;is_search_for_business=false.The xpath I'm using to grab the prices: //div[@class = 'FlightPrice']You can see from the Results pane in the screenshot below, it works swimmingly.screenshot of functional xpathIs it possible to convert the currency strings to an integer so I can run calculations on them? I tried wrapping it in a number() function but just got an NaN error. Not sure if it's because of the $s.float the item or variable. put it in the parenthesis below and you can use it in an equationfloat()You don’t do the conversion in XPath, you extract the string with XPath and parse it afterwards.For prices, I recommend that you use price-parser.

Scrapy output items - multiple parse methods, one row per item

jrbwt

[Scrapy output items - multiple parse methods, one row per item](https://stackoverflow.com/questions/58088385/scrapy-output-items-multiple-parse-methods-one-row-per-item)

I am continuing a scrapy project from an earlier question: scrapy output item as 1 list element per row

I have my scrapy code returning data from ufc events in one parse method and subsequently returning totals and round-by-round data for each event match in an additional parse method (separate links).The scraped data returned in the resulting csv file is correct. However the formatting is problematic:First, the items from the first and second parse methods appear on separate rows. These second items are kind of subset as a separate block completely to the right and below the first parse method items.Subsequently, within the second parse method items (below and to the right of the first block of item rows) the items skip a row to accommodate round-by-round data from an if-elif-else condition. This data which is slotted between these rows. I am using items and itemloaders but I am not currently using any custom item pipelines. I run the spider from the command line and output to csv with: Abbreviated code:I would like for each item to be output as one csv row. So if the csv current csv output is like:I want my csv to be:

2019-09-24 21:29:32Z

I am continuing a scrapy project from an earlier question: scrapy output item as 1 list element per row

I have my scrapy code returning data from ufc events in one parse method and subsequently returning totals and round-by-round data for each event match in an additional parse method (separate links).The scraped data returned in the resulting csv file is correct. However the formatting is problematic:First, the items from the first and second parse methods appear on separate rows. These second items are kind of subset as a separate block completely to the right and below the first parse method items.Subsequently, within the second parse method items (below and to the right of the first block of item rows) the items skip a row to accommodate round-by-round data from an if-elif-else condition. This data which is slotted between these rows. I am using items and itemloaders but I am not currently using any custom item pipelines. I run the spider from the command line and output to csv with: Abbreviated code:I would like for each item to be output as one csv row. So if the csv current csv output is like:I want my csv to be:It took me a while to understand your question/problem, so apologies if my answer is not correct. scrapy will write a new line to the output each time you yield an item, so you should only yield when you have a complete StatsItem. If it is essential that your data must be parsed from two different pages, you can create your item in parse_event and then pass it through to the parse_match function partially filled using either cb_kwargs (introduced in scrapy-1.7) or the meta argument of Request. So in parse_event you'd have and then you can modify parse_match to take item as an argumentIn conclusion, try to only do yield il.load_item() once.

Error: 'ascii' codec can't encode character '\xa7' in position 227: ordinal not in range(128)

shahrukh ijaz

[Error: 'ascii' codec can't encode character '\xa7' in position 227: ordinal not in range(128)](https://stackoverflow.com/questions/58009866/error-ascii-codec-cant-encode-character-xa7-in-position-227-ordinal-not)

i am using scrapy and when i run my spider i got error 

TypeError: Object of type 'bytes' is not JSON serializable

2019-09-19 11:22:09 [scrapy.utils.signal] ERROR: Error caught on signal handler: >and when i tried using pdb and try to print the  item['title'] 

but still found an error given below:'ascii' codec can't encode character '\xa7' in position 227: ordinal not in range(128)if anybody have idea please share it with me why this issue occurs

2019-09-19 11:21:48Z

i am using scrapy and when i run my spider i got error 

TypeError: Object of type 'bytes' is not JSON serializable

2019-09-19 11:22:09 [scrapy.utils.signal] ERROR: Error caught on signal handler: >and when i tried using pdb and try to print the  item['title'] 

but still found an error given below:'ascii' codec can't encode character '\xa7' in position 227: ordinal not in range(128)if anybody have idea please share it with me why this issue occursusing this i have done with this issue

can not send request in parse callback in scrapy

mo.nasiri

[can not send request in parse callback in scrapy](https://stackoverflow.com/questions/58013251/can-not-send-request-in-parse-callback-in-scrapy)

i have a class to scrap some data:which in CommentParser class i have:but scrapy dose not send request in CommentParser class and so i can not get response in CommentParser.parse

2019-09-19 14:25:54Z

i have a class to scrap some data:which in CommentParser class i have:but scrapy dose not send request in CommentParser class and so i can not get response in CommentParser.parseYou have to play with OOP, notice SiteSpider(CommentParser): that means SiteSpider will have access to methods of CommentParser

Find the right selector css to crawl a webpage on scrapy

Melissa A

[Find the right selector css to crawl a webpage on scrapy](https://stackoverflow.com/questions/57813429/find-the-right-selector-css-to-crawl-a-webpage-on-scrapy)

I'm trying to crawl this webpage "https://www.woolworths.com.au/shop/browse/drinks/cordials-juices-iced-teas/iced-teas" to extract the products name but I can't find the right selector, even for the price, h1 or the title! I tried :How can I proceed?

2019-09-05 21:56:55Z

I'm trying to crawl this webpage "https://www.woolworths.com.au/shop/browse/drinks/cordials-juices-iced-teas/iced-teas" to extract the products name but I can't find the right selector, even for the price, h1 or the title! I tried :How can I proceed?Content is dynamically loaded from a POST xhr returning json you can find in network tab of browser.Request goes to: Payload:with response in scrapy use:

how to run this code for image crawling with scrapy

xaner

[how to run this code for image crawling with scrapy](https://stackoverflow.com/questions/57646435/how-to-run-this-code-for-image-crawling-with-scrapy)

I want to try to crawling images from a website with scrapy libraryI found a code from github (https://github.com/imikay/ImageGrabber)

but how to run the code?

can someone tell me step by step please?

2019-08-25 13:28:35Z

I want to try to crawling images from a website with scrapy libraryI found a code from github (https://github.com/imikay/ImageGrabber)

but how to run the code?

can someone tell me step by step please?Clone the repo - git clone git@github.com:imikay/ImageGrabber.git

Go to this folder - cd ImageGrabber

Here https://github.com/imikay/ImageGrabber/blob/master/ImageGrabber/spiders/ImageSpider.py we can see the code of spider. You need to rewrite it for your goal.

 Anyway, this code from 2011 and a better way to write a new project for your goal.

Why is it necessary to return item in pipelines.py in scrapy?

it_is_a_literature

[Why is it necessary to return item in pipelines.py in scrapy?](https://stackoverflow.com/questions/57536000/why-is-it-necessary-to-return-item-in-pipelines-py-in-scrapy)

The pipelines.py send data parsed  into database or file or json. pipelines, from scrapy's architecture overview ,there are 9 steps ,step 1-8 constitute a whole loop,step 9 create a new process which contains 8 other same processes such like previous. In some sample pipelines.py,we often see such codes as below:pipelines.py is the last step in a whole 8 processes,why return item in pipelines.py after send item into database or other storage way?

item had been insert into database,it is no use to call the item by any component in scrapy again?

I have tested return item can't be omitted,want to know why?

2019-08-17 11:46:19Z

The pipelines.py send data parsed  into database or file or json. pipelines, from scrapy's architecture overview ,there are 9 steps ,step 1-8 constitute a whole loop,step 9 create a new process which contains 8 other same processes such like previous. In some sample pipelines.py,we often see such codes as below:pipelines.py is the last step in a whole 8 processes,why return item in pipelines.py after send item into database or other storage way?

item had been insert into database,it is no use to call the item by any component in scrapy again?

I have tested return item can't be omitted,want to know why?Scrapy supports multiple pipelines being enabled simultaneously.That is why it is important that pipelines pass items forward, unless they are pipelines that are specifically crafted to drop certain items.For example, you could later decide to add a second pipeline that stores items locally, in addition to storing them in the database.From documntation:

Try to scrap email address [duplicate]

Hosam Gamal

[Try to scrap email address [duplicate]](https://stackoverflow.com/questions/57267820/try-to-scrap-email-address)

I was trying to scrap this website[www.united-church.ca/search/locator/all?keyw=&mission_units_ucc_ministry_type_advanced=10&locll=][1]I did scrape it, but I couldn't scrap email addresses

Can you help me scrap it ?

I was using scrapyi have found a little bit of solution but it still not complete 

The output now is likeCan you help me replace [at] with @ and combine it in one string?

2019-07-30 08:53:30Z

I was trying to scrap this website[www.united-church.ca/search/locator/all?keyw=&mission_units_ucc_ministry_type_advanced=10&locll=][1]I did scrape it, but I couldn't scrap email addresses

Can you help me scrap it ?

I was using scrapyi have found a little bit of solution but it still not complete 

The output now is likeCan you help me replace [at] with @ and combine it in one string?Join the list elements and replace,

How to get recommendations on the webpage

Newbie

[How to get recommendations on the webpage](https://stackoverflow.com/questions/57178002/how-to-get-recommendations-on-the-webpage)

Consider this page:I asked a question here on Stackoverflow a few days back and it was recommended that in order to scrape recommendations I should look into scrapy-splash. With splash, I am able to scrape most of the JS,however, I am stuck at scraping recommendations at the bottom of page. This is what I have tried so far:This returns nothing.

2019-07-24 08:00:03Z

Consider this page:I asked a question here on Stackoverflow a few days back and it was recommended that in order to scrape recommendations I should look into scrapy-splash. With splash, I am able to scrape most of the JS,however, I am stuck at scraping recommendations at the bottom of page. This is what I have tried so far:This returns nothing.Did you try this selector:response.css('div.you-may-also-like-section div.product-tile-container a::attr(href)').extract()Also, you can try to set wait time in Splash browser.But if you check Browser -> Network -> XHR, you will find this request https://api.rfksrv.com/search-rec/263221008/3

Everything that you should do is to make a request like that, but with your data, that was taken from a source page. I would recommend this wayin curl it looks so:

need to append / inbetween text in python

asaika

[need to append / inbetween text in python](https://stackoverflow.com/questions/57175334/need-to-append-inbetween-text-in-python)

"}I need / in between city,state/city,stateActual output:L"}Expected output:"}

2019-07-24 04:36:22Z

"}I need / in between city,state/city,stateActual output:L"}Expected output:"}Output Try this,Output:Edit 1:(From comments by OP)Output:

Scrapy - Spider scheduling

M. Coppée

[Scrapy - Spider scheduling](https://stackoverflow.com/questions/57017318/scrapy-spider-scheduling)

I would like to schedule my spider to a given point in time. For example, I want to schedule my spider for midnight before going to bed. Is there any "easy" way, through the command line to schedule this or should I use scrapyd ? 

2019-07-13 08:05:57Z

I would like to schedule my spider to a given point in time. For example, I want to schedule my spider for midnight before going to bed. Is there any "easy" way, through the command line to schedule this or should I use scrapyd ? You can use crontab command to schedule your scraping job if your spiders run on your local machine. For example, if you want to schedule your spider in order to run everyday at 23:00 :For further reading : https://www.cyberciti.biz/faq/how-do-i-add-jobs-to-cron-under-linux-or-unix-oses/depending on your system you could set up a cron job:

https://wiki.ubuntuusers.de/Cron/you could also go with frontera or scrapinghub: https://support.scrapinghub.com/support/solutions/articles/22000200419-scheduling-periodic-jobs

I get no response scraping a “google maps” map using scrapy

Th3FreeSpirit

[I get no response scraping a “google maps” map using scrapy](https://stackoverflow.com/questions/56980997/i-get-no-response-scraping-a-google-maps-map-using-scrapy)

I am trying to scrape this webpage:

https://www.google.com/maps/d/u/0/viewer?mid=10gfc4vm6VKjxIf6UhKLlMLePqTjTYXYC&ll=50.65039081184933%2C3.040291506005474&z=11

to get information about producers.

However, when I send my requests (through scrapy shell) I get an empty response:Here is the code I am using

2019-07-11 02:51:17Z

I am trying to scrape this webpage:

https://www.google.com/maps/d/u/0/viewer?mid=10gfc4vm6VKjxIf6UhKLlMLePqTjTYXYC&ll=50.65039081184933%2C3.040291506005474&z=11

to get information about producers.

However, when I send my requests (through scrapy shell) I get an empty response:Here is the code I am usingResolved:

You change ROBOTSTXT_OBEY to False in settings.py

Scrapy: how to check websites if they contain a certain string

Preth Doth

[Scrapy: how to check websites if they contain a certain string](https://stackoverflow.com/questions/56962227/scrapy-how-to-check-websites-if-they-contain-a-certain-string)

I want to iterate through many websites and see if they contain a certain string. E.g. "biscuit". Using scrapy

2019-07-10 01:17:57Z

I want to iterate through many websites and see if they contain a certain string. E.g. "biscuit". Using scrapyLearn and use Scrapy.Scrapy Docs: https://docs.scrapy.org/en/latest/

Email as a string

vezunchik

[Email as a string](https://stackoverflow.com/questions/56693311/email-as-a-string)

I need to login to a website using scrapy and I need to use an email as a username, but when it uses the username instead of using an @ symbol in the email it does %40 instead.  Is there a way to prevent this?  Because I obviously cannot login right now because of this problem.LoginSpider.pyPartial Output

2019-06-20 20:16:37Z

I need to login to a website using scrapy and I need to use an email as a username, but when it uses the username instead of using an @ symbol in the email it does %40 instead.  Is there a way to prevent this?  Because I obviously cannot login right now because of this problem.LoginSpider.pyPartial OutputBy your log it seems that you call wrong form on the page, not login form.Try to specify it in code with:Now I got html-response with:But I am sure if you pul correct credentials, everything will be fine.

Get Boolean Value for each line of a list from xpath response using scrapy

Martin

[Get Boolean Value for each line of a list from xpath response using scrapy](https://stackoverflow.com/questions/56580576/get-boolean-value-for-each-line-of-a-list-from-xpath-response-using-scrapy)

While using scrapy, I have an xpath response which returns a list. I would like to check for each line of the list whether it contains a string or not. So the result should be a list of booleans.How is that done using scrapy and response.xpath?This is my Workaround:

Since scrapy is using XPath 1, I had to build a workaround because in Xpath 1 every node is unique. As a result Xpath 1 can return each heading only once. This is what I did to solve the problem: Problem:



Heading 1 

Text 1 

Text 2

Text 3Heading 2

Text 4

Text 5

Text 6Result:

True

True

What I wanted to receive something like this:

True

True

False

True

True

FalseSo instead of using Xpath selectors to retrieve the boolean values, I used scrapy to retrieve the full page source code. Then, I iterated over this string according to my needs using standard python string operations and loops. Main problem was that Xpath 1 can return each node only once.

2019-06-13 12:32:00Z

While using scrapy, I have an xpath response which returns a list. I would like to check for each line of the list whether it contains a string or not. So the result should be a list of booleans.How is that done using scrapy and response.xpath?This is my Workaround:

Since scrapy is using XPath 1, I had to build a workaround because in Xpath 1 every node is unique. As a result Xpath 1 can return each heading only once. This is what I did to solve the problem: Problem:



Heading 1 

Text 1 

Text 2

Text 3Heading 2

Text 4

Text 5

Text 6Result:

True

True

What I wanted to receive something like this:

True

True

False

True

True

FalseSo instead of using Xpath selectors to retrieve the boolean values, I used scrapy to retrieve the full page source code. Then, I iterated over this string according to my needs using standard python string operations and loops. Main problem was that Xpath 1 can return each node only once.If you want to check if each list item contains a string, then you can use the other approach.If you consider the below sample html, find out the list items which does not have fruit in list item text.You can get the list items that does not contains the string using the below xpath.

How to fix 'Parsing email from modal' in Post request

palash babu

[How to fix 'Parsing email from modal' in Post request](https://stackoverflow.com/questions/56552785/how-to-fix-parsing-email-from-modal-in-post-request)

I am using windows 10 with python 3 and scrapy. Here is the site link that's i need to parsing email addresshttps://find.plasticsurgery.org/city/new-yorkto getting individual people email it's need to click each time but i got a POST query from network section and developed a scrapy spider but still it's doesn't parsing any email.below my spider codeend of the results i found just {'email': '#'}we expect the result for email address such as {'email': any@anyemail.com}

2019-06-11 23:18:13Z

I am using windows 10 with python 3 and scrapy. Here is the site link that's i need to parsing email addresshttps://find.plasticsurgery.org/city/new-yorkto getting individual people email it's need to click each time but i got a POST query from network section and developed a scrapy spider but still it's doesn't parsing any email.below my spider codeend of the results i found just {'email': '#'}we expect the result for email address such as {'email': any@anyemail.com}This is something you might wanna do to get the names and their email addresses. Feel free to use different search id's to get different results, as in 38078106 or 38066000 e.t.c.Maybe you need to use actual headers?

Crontab jobs doesn't run multiple Scrapy spider properly

Nabat Farsi

[Crontab jobs doesn't run multiple Scrapy spider properly](https://stackoverflow.com/questions/56503821/crontab-jobs-doesnt-run-multiple-scrapy-spider-properly)

I have a crontab job that runs a myautorun.sh file which contains 3 spider:crontab -eand myautorun.sh looks like this:each spider scrapes 20 pages.when cronjob runs it ends up with variable scraped pages around 30, and never is 60. 

Each spider reads few pages but not all 20 pages. However if I comment two spiders and run them separately one at a time, it gives me 60.So I am puzzled why it cant run parallel spiders properly,

I am running the spiders using Crawlera, and they are on a 1GB ram virtual server.Is there any settings or anything that I am missing?

Thanks,  

2019-06-08 05:43:41Z

I have a crontab job that runs a myautorun.sh file which contains 3 spider:crontab -eand myautorun.sh looks like this:each spider scrapes 20 pages.when cronjob runs it ends up with variable scraped pages around 30, and never is 60. 

Each spider reads few pages but not all 20 pages. However if I comment two spiders and run them separately one at a time, it gives me 60.So I am puzzled why it cant run parallel spiders properly,

I am running the spiders using Crawlera, and they are on a 1GB ram virtual server.Is there any settings or anything that I am missing?

Thanks,  & means you are running them in parallel, so that maybe was issue, your spiders were taking too much RAM and hence were being KILLEDAnyways, you should also redirect spider's outputs to logs files so in future there is any error, you will be able to see what has happened I created another virtual server with 3GB ram and it resolved the issue!

error attributing items from scrapy into a database

how to change

[error attributing items from scrapy into a database](https://stackoverflow.com/questions/56265046/error-attributing-items-from-scrapy-into-a-database)

am trying to insert items scraped through scrapy into a MySQL database (create a new database if none is present before), I followed an online tutorial since I have no idea how to do this but an error keeps happening.am trying to store an item that contains 5 text fields into a databasehere's my pipelinehere's a part of my spider I expect the item to be stored in my "link" database but I keep running into this error 

" self.cursor.execute("""insert into link_tb values (%s,%s,%s,%s,%s)""", (

AttributeError: 'LinkPipeline' object has no attribute 'cursor'"

2019-05-22 21:08:22Z

am trying to insert items scraped through scrapy into a MySQL database (create a new database if none is present before), I followed an online tutorial since I have no idea how to do this but an error keeps happening.am trying to store an item that contains 5 text fields into a databasehere's my pipelinehere's a part of my spider I expect the item to be stored in my "link" database but I keep running into this error 

" self.cursor.execute("""insert into link_tb values (%s,%s,%s,%s,%s)""", (

AttributeError: 'LinkPipeline' object has no attribute 'cursor'"You defined the constructor as _init_ instead of __init__

How to remove \r\n from language prices?

Zaryab

[How to remove \r\n from language prices?](https://stackoverflow.com/questions/56068953/how-to-remove-r-n-from-language-prices)

When I run the code, it gives me \r\n with space. I have tried to remove \r\n from the result but it didn't. This is code. Please check it out.

2019-05-09 23:48:07Z

When I run the code, it gives me \r\n with space. I have tried to remove \r\n from the result but it didn't. This is code. Please check it out.Not sure what do you want exactly but this code works:

First Scrapy Spider

AlbertWolfgang

[First Scrapy Spider](https://stackoverflow.com/questions/56012860/first-scrapy-spider)

I'm trying to build a Scrapy Spider to parse the artist and track info from SoundCloud.Using the developer tools in FireFox I've determined an API call can be made that returns a JSON object that converts to a python dictionary. This API call needs an artist ID, and as far as I can tell these IDs have been auto-incremented. This means I don't need to crawl the site, and can just have a list of starting URLs that make the initial API call and then parse the pages that follow from that. I believe this should make me more friendly to the site?From the returned response the artists' URL can be obtained, and visiting and parsing this URL will give more information about the artistFrom the artists' URL we can visit their tracks and scrape a list of tracks alongside the tracks' attributes.I think the issues I'm having stem from not understanding Scrapy's framework...

If I directly put in the artists' URL is start_urls Scrapy passes a scrapy.http.response.html.HtmlResponse Object to parse_artist. This allows me to extract the data I need (I didn't include all the code to parse the page to keep the code snippet shorter). However, if I pass that same Object to the same function from the function parse_api_call it results in an error...I cannot understand why this is, and any help would be appreciated.Side Note:

The initial API call grabs tracks from the artist, and the offset and limit can be changed and the function called recursively to collect the tracks. This, however, has proven unreliable, and even when it doesn't result in an error that terminates the program, it doesn't get a full list of tracks from the artist. Here's the current code:

2019-05-06 21:50:54Z

I'm trying to build a Scrapy Spider to parse the artist and track info from SoundCloud.Using the developer tools in FireFox I've determined an API call can be made that returns a JSON object that converts to a python dictionary. This API call needs an artist ID, and as far as I can tell these IDs have been auto-incremented. This means I don't need to crawl the site, and can just have a list of starting URLs that make the initial API call and then parse the pages that follow from that. I believe this should make me more friendly to the site?From the returned response the artists' URL can be obtained, and visiting and parsing this URL will give more information about the artistFrom the artists' URL we can visit their tracks and scrape a list of tracks alongside the tracks' attributes.I think the issues I'm having stem from not understanding Scrapy's framework...

If I directly put in the artists' URL is start_urls Scrapy passes a scrapy.http.response.html.HtmlResponse Object to parse_artist. This allows me to extract the data I need (I didn't include all the code to parse the page to keep the code snippet shorter). However, if I pass that same Object to the same function from the function parse_api_call it results in an error...I cannot understand why this is, and any help would be appreciated.Side Note:

The initial API call grabs tracks from the artist, and the offset and limit can be changed and the function called recursively to collect the tracks. This, however, has proven unreliable, and even when it doesn't result in an error that terminates the program, it doesn't get a full list of tracks from the artist. Here's the current code:You have to use to get data from new url. But you can't execute it as normal function and get result at once. You have to use return Request() or yield Request() and scrapy puts it in queue to get data later. After it gets data it uses method parse() to parse data from response. But you can set own method in request But in parse_artist() you will not have access to data which you get in previous function so you have to send it in request using meta - ie. Full working code. You can put all in one file and run it without creating project. It also saves result in output.csv ouput.csv

How to capture response.code for each url that is attempted to scrape?

CubanGT

[How to capture response.code for each url that is attempted to scrape?](https://stackoverflow.com/questions/55955322/how-to-capture-response-code-for-each-url-that-is-attempted-to-scrape)

I have a large list of URLs to scrape and after multiple tests, I noticed that in the output from the execution of the spider there are a results sections that show all the response codes that the crawler encountered. But when I run my code that has this simple line in it, ALL the urls come back with a Code = 200In the debug window the break down is like this and was hoping to capture the same thing in my file so that I can easily identify which URLs I need to go validate and adjust the code if needed. I know what they all mean, but I would like to capture these actual codes in my CSV file that is creating with the scrape so that I can investigate the troubled URLS.

2019-05-02 15:06:45Z

I have a large list of URLs to scrape and after multiple tests, I noticed that in the output from the execution of the spider there are a results sections that show all the response codes that the crawler encountered. But when I run my code that has this simple line in it, ALL the urls come back with a Code = 200In the debug window the break down is like this and was hoping to capture the same thing in my file so that I can easily identify which URLs I need to go validate and adjust the code if needed. I know what they all mean, but I would like to capture these actual codes in my CSV file that is creating with the scrape so that I can investigate the troubled URLS.I don’t think you want to capture 301 response codes. When Scrapy find a 301, by default it yields a new request for the redirect target (a new URL), and your callback only receives the response to the final URL (after following all redirects).As for 404 responses, they never reach your callback by default. If you want your callback to receive these responses, you have two options:

How to make loop in scrapy class?

Rizwan

[How to make loop in scrapy class?](https://stackoverflow.com/questions/55920351/how-to-make-loop-in-scrapy-class)

I am scraping a site. But I have a problem. I made a class and a variable and gave it a link to scrape data from. But in website there are many categories in one page. Means there are countries names listed by alphabets A, B upto Z. I made a loop to just enter the first alphabet of a country starting from A, B upto Z, so that I could not repeat the links again and again. But it is not working. Its just giving me the names of the Countries which is in end of the variable eleventh_category list. If the list ends with C it will give me just the names of the countries starting from C not the B and A. I hope you understood..

2019-04-30 11:51:02Z

I am scraping a site. But I have a problem. I made a class and a variable and gave it a link to scrape data from. But in website there are many categories in one page. Means there are countries names listed by alphabets A, B upto Z. I made a loop to just enter the first alphabet of a country starting from A, B upto Z, so that I could not repeat the links again and again. But it is not working. Its just giving me the names of the Countries which is in end of the variable eleventh_category list. If the list ends with C it will give me just the names of the countries starting from C not the B and A. I hope you understood..You are overriding the value in save at each iteration of your loop, so the final value will always be the last value of eleventh_category.You can put the loop in the parse method:I am lost in your logics about variables outside functions (or you have problems with indents), but check this solution:Here you have all logics and calls inside parse function, that is much easier and more readable.

Is there a way to get url present of an element who's value is < a href = “#”>

ram

[Is there a way to get url present of an element who's value is < a href = “#”>](https://stackoverflow.com/questions/55746264/is-there-a-way-to-get-url-present-of-an-element-whos-value-is-a-href)

I am trying to scrape data  the website Bestbuy.com (USA),when we go to the product page URL for the first time It asks us to choose the country, product page. I am trying to get the URL of the page that directs to USA, when user clicks on United States flagNote: You may try to open the link of product page in incognito windowThe code snippet of the element on the retailers site, where we have to select the country is as follows:When i run the scrapy command to get the url of the element of href, I get the value as '#', which is correctWhen i hover on the '#' in the source code, I can see the URL , how can i fetch this value?Image>> image Link

2019-04-18 12:39:30Z

I am trying to scrape data  the website Bestbuy.com (USA),when we go to the product page URL for the first time It asks us to choose the country, product page. I am trying to get the URL of the page that directs to USA, when user clicks on United States flagNote: You may try to open the link of product page in incognito windowThe code snippet of the element on the retailers site, where we have to select the country is as follows:When i run the scrapy command to get the url of the element of href, I get the value as '#', which is correctWhen i hover on the '#' in the source code, I can see the URL , how can i fetch this value?Image>> image LinkAs Luiz commented, response.url contains the current URL.If you want to resolve relative URLs to absolute URLs in general, taking advantage of response.urljoin() or replacing Request() with response.follow() might be a better approach.

Connection to the other side was lost - webscraping

mkc

[Connection to the other side was lost - webscraping](https://stackoverflow.com/questions/55746364/connection-to-the-other-side-was-lost-webscraping)

I am tring to scrape all of the companies names from https://www.gpw.pl/spolki, futhermore I want to press "Pokaż więcej..." (in english show more) to scrape all of the companies names.My initial code is:However at the end I am left with the following error:[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]If I want to have a csv with all of the names for all of the companies how can I achieve that? What am I doing wrong I mean is this website simply blocking me from scraping?EDIT: My best guess is that the website is blocking all web crawlers, I ve tried using different IP addresses and nothing has helped. BTW: If you down-vote this question do not hesitate to write why :) 

2019-04-18 12:45:49Z

I am tring to scrape all of the companies names from https://www.gpw.pl/spolki, futhermore I want to press "Pokaż więcej..." (in english show more) to scrape all of the companies names.My initial code is:However at the end I am left with the following error:[<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]If I want to have a csv with all of the names for all of the companies how can I achieve that? What am I doing wrong I mean is this website simply blocking me from scraping?EDIT: My best guess is that the website is blocking all web crawlers, I ve tried using different IP addresses and nothing has helped. BTW: If you down-vote this question do not hesitate to write why :) Yes, the website can be blocking you.Try to enable the Autothrottle feature to avoid hitting the website too hard.You can also try to set user-agent to a different value. Eg.If none of those helps you, consider using a proxy or VPN.

How to use XPATH correctly?

smithchao

[How to use XPATH correctly?](https://stackoverflow.com/questions/55283528/how-to-use-xpath-correctly)

I use scrapy to scraping bookmaker and I have trouble in using XPATHHow do I to scrap 3.40, 1.58, 5.50 by using a node contains 'bet365'ex: https://i.stack.imgur.com/O1xHu.png

2019-03-21 15:06:35Z

I use scrapy to scraping bookmaker and I have trouble in using XPATHHow do I to scrap 3.40, 1.58, 5.50 by using a node contains 'bet365'ex: https://i.stack.imgur.com/O1xHu.pngHere is the correct xpath.From what you show it looks like you could use css selector in scrapy of 

scrap a web page with scrapy dosen't return page content

Mohammad Reza

[scrap a web page with scrapy dosen't return page content](https://stackoverflow.com/questions/55105807/scrap-a-web-page-with-scrapy-dosent-return-page-content)

I'm trying to scrap a web page with scrapy I noticed it won't work when I parsed web page throw my ipython shell it returned this:more like a json response. how can I scrap throw it? by the way my scrapper looks like this:

2019-03-11 15:59:25Z

I'm trying to scrap a web page with scrapy I noticed it won't work when I parsed web page throw my ipython shell it returned this:more like a json response. how can I scrap throw it? by the way my scrapper looks like this:The Website content is not render by server side.The Content of the website is  rendered by JavaScript:In this case you need use either.There might be other possible Solutions. 

Scraper: Adding Delay between Iterations

M. S.

[Scraper: Adding Delay between Iterations](https://stackoverflow.com/questions/54832527/scraper-adding-delay-between-iterations)

For a scraping project, I want to add a 1 second delay between each iteration of the script. In other threads, I have already read that a delay can be included through the "time" feature.However, the code below is despite the inclusion of "time" still processing multiple requests per second, which is simply too fast for the scraper. Does anyone know how to make the 1 second delay work properly?

2019-02-22 17:46:21Z

For a scraping project, I want to add a 1 second delay between each iteration of the script. In other threads, I have already read that a delay can be included through the "time" feature.However, the code below is despite the inclusion of "time" still processing multiple requests per second, which is simply too fast for the scraper. Does anyone know how to make the 1 second delay work properly?There is a special setting for that, called download-delayYou can read more in docs for scrapy: https://doc.scrapy.org/en/latest/topics/settings.html#download-delay

Callback returns NameError - Scrapy

Matija Žiberna

[Callback returns NameError - Scrapy](https://stackoverflow.com/questions/54773894/callback-returns-nameerror-scrapy)

I'm having troubles when using callback at last method named last. I get the following errorOne method above works absolutely fine even-though it uses the same logic. If anyone has any idea I would greatly appreciate the help.EDIT: I've tried putting self.last but I get the same issue - this time self is not defined.Thanks!

2019-02-19 19:48:11Z

I'm having troubles when using callback at last method named last. I get the following errorOne method above works absolutely fine even-though it uses the same logic. If anyone has any idea I would greatly appreciate the help.EDIT: I've tried putting self.last but I get the same issue - this time self is not defined.Thanks!In addition to configure callback parameter to with self.last.

Spider methods should have strictly self and response as method arguments(not region and region_2). Otherwise SplashRequest will be unable to recognise callback functionYou forgot self. Fix to callback=self.last.

How to use Tkinter or any other python GUI with Scrapy [closed]

S. Sinha

[How to use Tkinter or any other python GUI with Scrapy [closed]](https://stackoverflow.com/questions/54160736/how-to-use-tkinter-or-any-other-python-gui-with-scrapy)

I want a gui with Tkinter or any other python GUI with scrapy. So the scraped links and data should be displayed On gui rather than Command Prompt 

2019-01-12 14:55:37Z

I want a gui with Tkinter or any other python GUI with scrapy. So the scraped links and data should be displayed On gui rather than Command Prompt I am not aware of any Tkinter (or Qt/GTK) GUIs for Scrapy, but you can certainly create your own.  Most Scrapy GUIs or integrations come in the form of webclients, some of which I have seen areand maybeTo interact with Scrapy, and create your own GUI, you have the choices betweenHere is a project I found a long time ago when I was looking to do the same.HirojiSawatari's Github - "FindGoods"HirojiSawatari's Project Documentation/blogThere might be some... encoding issues lol.The Idea is there though, if you reverse engineer the project, you should get an Idea on how to achieve what you want. It does entail proficiency with Tkinter and scrapy (obviously lol I mention because it isn't something for people just learning these frameworks.)

Scraping multiple links by scrapy

S. Sinha

[Scraping multiple links by scrapy](https://stackoverflow.com/questions/53962591/scraping-multiple-links-by-scrapy)

I scraped a web page & all the useful links I stored in a list & now I want to scrap those links which are in the list. So how may I do it? 

2018-12-28 18:06:10Z

I scraped a web page & all the useful links I stored in a list & now I want to scrap those links which are in the list. So how may I do it? This what i use for that:Iterate through the list of links with a for loop.

Scraping of data with scrapy

Shreyas Ramachandran

[Scraping of data with scrapy](https://stackoverflow.com/questions/53575655/scraping-of-data-with-scrapy)

I am trying to scrape data from the following link https://timesofindia.indiatimes.com/archive/year-2018,month-1.cms. Specifically, I am trying to take the URLs present inside the calendar table.

My current code for doing this is But this gives me no results. Where am I going wrong and what would be the solution?

2018-12-01 22:37:59Z

I am trying to scrape data from the following link https://timesofindia.indiatimes.com/archive/year-2018,month-1.cms. Specifically, I am trying to take the URLs present inside the calendar table.

My current code for doing this is But this gives me no results. Where am I going wrong and what would be the solution?If you take a look at the page's source, you'll see that the links you're trying to extract do not exist.

Instead, there is a block of javascript that generates them.You have two possibilities for scraping this data:

scrapy does not follow all URLs within a page

merlin

[scrapy does not follow all URLs within a page](https://stackoverflow.com/questions/53439000/scrapy-does-not-follow-all-urls-within-a-page)

I am new to scrapy and just successfully crawled a page, retrieving 58 results while I was looking for 120 results that are available.The problem seems to be, that if one website contains 4 links, scrapy follows the first one and the other three will never be visited as the links to those pages are only within that one page and it will never be visited again. I am assuming this, since within the result set those 3 are missing but the links are OK if I visit the page in the browser.The spider:Output:I am using the default settings from the template spider.If I run it again, slightly other amounts will be fetchedHow can I debug this problem in order to retrieve all results?

2018-11-22 23:19:39Z

I am new to scrapy and just successfully crawled a page, retrieving 58 results while I was looking for 120 results that are available.The problem seems to be, that if one website contains 4 links, scrapy follows the first one and the other three will never be visited as the links to those pages are only within that one page and it will never be visited again. I am assuming this, since within the result set those 3 are missing but the links are OK if I visit the page in the browser.The spider:Output:I am using the default settings from the template spider.If I run it again, slightly other amounts will be fetchedHow can I debug this problem in order to retrieve all results?Found the problem. The regex with the second rule was prohibiting those URLs. Worked on the regex and not it runs OK.

scrapy to get into next page and download all files

Vinod kumar

[scrapy to get into next page and download all files](https://stackoverflow.com/questions/53320572/scrapy-to-get-into-next-page-and-download-all-files)

I am new to scrapy and python, I am able to get details from URL, I want enter into link and download all files(.htm and .txt).My CodeAnd I need to enter into link and download all the files with ends with .htm and .txt files. Below code is not working..Can Anyone help me with this ? Thanks in Advance.

2018-11-15 13:28:24Z

I am new to scrapy and python, I am able to get details from URL, I want enter into link and download all files(.htm and .txt).My CodeAnd I need to enter into link and download all the files with ends with .htm and .txt files. Below code is not working..Can Anyone help me with this ? Thanks in Advance.Try the following to get the files downloaded in your desktop or wherever you mention within the script:To be clearer: you need to specify explicitly dirf = r"C:\Users\WCS\Desktop\Storage" where C:\Users\WCS\Desktop or something will be your desired location. However, the script will automatically create Storage folder to save those files within.

Know if one of start urls is finished

asa

[Know if one of start urls is finished](https://stackoverflow.com/questions/53170496/know-if-one-of-start-urls-is-finished)

I am using scrapy and I want to scrape many urls, and my question is how can I know that scrapy changes to the second start urls.start_urls = ['link1', 'link2']Because I want to execute some code when scrapy switch from link1 to link2.Thanks by advance, and sorry for my bad english.

2018-11-06 10:56:43Z

I am using scrapy and I want to scrape many urls, and my question is how can I know that scrapy changes to the second start urls.start_urls = ['link1', 'link2']Because I want to execute some code when scrapy switch from link1 to link2.Thanks by advance, and sorry for my bad english.If you use start_urls, Scrapy automatically sends requests asynchronously, which means that callback for link2 may be invoked earlier than link1, so there's no "switching" between these links.If you want to have these requests sent in a specific order, you can do the following:

scrapy get a tag thanks to the precedent tag

joe

[scrapy get a tag thanks to the precedent tag](https://stackoverflow.com/questions/53144275/scrapy-get-a-tag-thanks-to-the-precedent-tag)

hello I have this html script: And I would like  to get the dd value (5) based on the dt value Nr. of Doors.this the code that I am using (which is wrong): Below the output that I get:this result is duplicated a lot of time!!

2018-11-04 18:56:36Z

hello I have this html script: And I would like  to get the dd value (5) based on the dt value Nr. of Doors.this the code that I am using (which is wrong): Below the output that I get:this result is duplicated a lot of time!!

Scrapping the AJAX websites with Scrapy

aftab qaisrani

[Scrapping the AJAX websites with Scrapy](https://stackoverflow.com/questions/52988392/scrapping-the-ajax-websites-with-scrapy)

I am searching the answer for my question since last two days but now i am frustrated. I found a lot of stuff about this but nothing helped me in achieving my goal. The purpose of this long paragraph was to aware you that please don't link the previous answers of stackoverflow for reference. Kindly provide me simple & neat solution/answer.

I want to scrape the players data from https://www.premierleague.com/players

but i face ajax while scrolling down the page.

My teammate asked me not to use selenium. He asked me to only use Scrapy for this purpose.

So kindly tell me in easy word if it is possible with Scrapy or not.? & if possible then how.

I will be thankful to you.

2018-10-25 11:38:28Z

I am searching the answer for my question since last two days but now i am frustrated. I found a lot of stuff about this but nothing helped me in achieving my goal. The purpose of this long paragraph was to aware you that please don't link the previous answers of stackoverflow for reference. Kindly provide me simple & neat solution/answer.

I want to scrape the players data from https://www.premierleague.com/players

but i face ajax while scrolling down the page.

My teammate asked me not to use selenium. He asked me to only use Scrapy for this purpose.

So kindly tell me in easy word if it is possible with Scrapy or not.? & if possible then how.

I will be thankful to you.Its easyOpen Inspect Element or FirebugSwitch to Network tab and then click on XHRAnd then start scrolling down the pageYou will see they are sending AJAX requestsJust check which URL they are sending requests to, and check what form-data is being sent along

Creating a script in Python that will search for a specific URL on all pages of a website using BeautifulSoup, requests, and possibly Scrapy

GeoMoon

[Creating a script in Python that will search for a specific URL on all pages of a website using BeautifulSoup, requests, and possibly Scrapy](https://stackoverflow.com/questions/52875310/creating-a-script-in-python-that-will-search-for-a-specific-url-on-all-pages-of)

I am a new Stack Overflow member so please let me know if and how I can improve this question. I am working on a Python script which will take a link to a website's home page, and then search for a specific URL throughout the entire website (not just that first homepage). The reason for this is that my research team would like to query a list of websites for a URL to a particular database, without having to click through every single page to find it. It is essentially a task of saying "Does this website reference this database? If so, how many times?" and then keeping that information for our records. So far, I have been able to use resources on SO and other pages to create a script that will scrape the HTML of the specific webpage I have referenced, and I have included this script for review.  As you can see, I am looking for a URL linking to a particular database (in this case, DataONE) after being given a website URL by the user. This script works great, but it only scrapes that particular page that I link -- NOT the entire website. So, if I provide the website: https://www.lib.utk.edu/, it will only search for references to DataONE within this page but it will not search for references across all of the pages under the UTK Libraries website. **I do not have a high enough reputation on this site yet to post pictures, so I am unable to include an image of this script "in action." **I've heavily researched this on SO to try and gain insight, but none of the questions asked or answered thus far apply to my specific problem. Examples:

1. How can I loop scraping data for multiple pages in a website using python and beautifulsoup4: in this particular question, the OP can find out how many pages they need to search through because their problem refers to a specific search made on a site. However, in my case, I will not know how many pages there are in each website.

2. Use BeautifulSoup to loop through and retrieve specific URLs: Again, this is dealing with parsing through URLs but it is not looking through an entire website for URLs.

3. How to loop through each page of website for web scraping with BeautifulSoup: The OP here seems to be struggling with the same problem I am having, but the accepted answer there does not provide enough detail for understanding HOW to approach a problem like this.  I've scoured the BeautifulSoup documentation but I have not found any help with web scraping an entire website from a single URL (and not knowing how many total pages are in the website). I've looked into using Scrapy, but I'm not sure it's what I need for my purposes on this project, because I am not trying to download or store data -- I am simply trying to see when and where a certain URL is referenced on an entire website.  My question: Is doing something like this possible with BeautifulSoup, and if so, can you suggest how I should change my current code to handle my research problem? Or is there another program I should look into using? 

2018-10-18 13:34:55Z

I am a new Stack Overflow member so please let me know if and how I can improve this question. I am working on a Python script which will take a link to a website's home page, and then search for a specific URL throughout the entire website (not just that first homepage). The reason for this is that my research team would like to query a list of websites for a URL to a particular database, without having to click through every single page to find it. It is essentially a task of saying "Does this website reference this database? If so, how many times?" and then keeping that information for our records. So far, I have been able to use resources on SO and other pages to create a script that will scrape the HTML of the specific webpage I have referenced, and I have included this script for review.  As you can see, I am looking for a URL linking to a particular database (in this case, DataONE) after being given a website URL by the user. This script works great, but it only scrapes that particular page that I link -- NOT the entire website. So, if I provide the website: https://www.lib.utk.edu/, it will only search for references to DataONE within this page but it will not search for references across all of the pages under the UTK Libraries website. **I do not have a high enough reputation on this site yet to post pictures, so I am unable to include an image of this script "in action." **I've heavily researched this on SO to try and gain insight, but none of the questions asked or answered thus far apply to my specific problem. Examples:

1. How can I loop scraping data for multiple pages in a website using python and beautifulsoup4: in this particular question, the OP can find out how many pages they need to search through because their problem refers to a specific search made on a site. However, in my case, I will not know how many pages there are in each website.

2. Use BeautifulSoup to loop through and retrieve specific URLs: Again, this is dealing with parsing through URLs but it is not looking through an entire website for URLs.

3. How to loop through each page of website for web scraping with BeautifulSoup: The OP here seems to be struggling with the same problem I am having, but the accepted answer there does not provide enough detail for understanding HOW to approach a problem like this.  I've scoured the BeautifulSoup documentation but I have not found any help with web scraping an entire website from a single URL (and not knowing how many total pages are in the website). I've looked into using Scrapy, but I'm not sure it's what I need for my purposes on this project, because I am not trying to download or store data -- I am simply trying to see when and where a certain URL is referenced on an entire website.  My question: Is doing something like this possible with BeautifulSoup, and if so, can you suggest how I should change my current code to handle my research problem? Or is there another program I should look into using? You will need to implement some form of crawler.This can be done manually; essentially, you'd do this:I'd recommend looking into Scrapy though. It lets you define Spiders that you feed with information about what URLs to start at and how to generate further links to visit. The Spider has a parse method that you can utilize to search for your database. In case of a match, you could update a local SQLite-DB or simply write a count to a textfile.TL;DR: from visiting a single page, it is hard to identify what other pages exist. You have to parse all internal links. A robots.txt can be helpful in this effort, but is not guaranteed to exist.You could use two python sets to keep track of pages you already visited and of pages you need to visit. Also: you if condition is wrong, to test both , you cannot use a and b in c you need to do a in c and b in cSomething like this:

Scrapy ModuleNotFoundError: No module named 'MySQLdb'

Tomzski

[Scrapy ModuleNotFoundError: No module named 'MySQLdb'](https://stackoverflow.com/questions/52515795/scrapy-modulenotfounderror-no-module-named-mysqldb)

Just started out with Scrapy and I am trying to write to a MySQL database rather than outputting to a csv.I have found the code here: https://gist.github.com/tzermias/6982723 that I am using to try to make this work, but unfortunately having an error that I can't get my head around.This is my pipelines.py:This is what is in my settings.py:This is the spider.py:I have been unable to find a working example of what I am looking to do to be able to work out where I am going wrong so thank you to anyone who looks at this or might be able to help.

2018-09-26 10:39:23Z

Just started out with Scrapy and I am trying to write to a MySQL database rather than outputting to a csv.I have found the code here: https://gist.github.com/tzermias/6982723 that I am using to try to make this work, but unfortunately having an error that I can't get my head around.This is my pipelines.py:This is what is in my settings.py:This is the spider.py:I have been unable to find a working example of what I am looking to do to be able to work out where I am going wrong so thank you to anyone who looks at this or might be able to help.you will need MySQL-python installed in your python environment, along with libmysql installed on the operating system.On Ubuntu this would be achieved in the folllowing manner.

pip install MySQL-python

sudo apt-get install libmysql-dev

do you have these packages installed "MySQLdb, scrapy, twisted".Else try installing using PIP and then try running the script.

Check scrapy result in bash

heyarne

[Check scrapy result in bash](https://stackoverflow.com/questions/52425741/check-scrapy-result-in-bash)

I have multiple spiders that i run in a bash script like so:Since they should run for a long time I'd like to have a simple way of monitoring their success rate; my plan was to ping https://healtchecks.io when both scrapers run successfully (i.e. they don't have any error messages). I've sprinkled some assert statements over the code to be reasonably confident about this.My problem is that each scrapy runspider command always returns 0 no matter what. That means I can't really check whether they have been succesful.Is there a way to influence this behavior? Some command line flag I haven't found? If not, how would I run the two spiders from a python script and save their output to a defined location? I found this link but it doesn't mention how to handle the returned items.

2018-09-20 12:59:40Z

I have multiple spiders that i run in a bash script like so:Since they should run for a long time I'd like to have a simple way of monitoring their success rate; my plan was to ping https://healtchecks.io when both scrapers run successfully (i.e. they don't have any error messages). I've sprinkled some assert statements over the code to be reasonably confident about this.My problem is that each scrapy runspider command always returns 0 no matter what. That means I can't really check whether they have been succesful.Is there a way to influence this behavior? Some command line flag I haven't found? If not, how would I run the two spiders from a python script and save their output to a defined location? I found this link but it doesn't mention how to handle the returned items.The way I eventually solved this was assigning the log output to a variable and grepping it for ERROR: Spider error processing. Scrapy has the very nice behavior of not failing unnecessarily early; if I exited the python script myself I would have lost that. This way I could just run one scraper after another and handle the errors in the end, so I could still collect as much as possible while being notified in the case that something doesn't run 100% smoothly.

Xpath starting retuning None on Scrapy

Guilherme Resende

[Xpath starting retuning None on Scrapy](https://stackoverflow.com/questions/52245371/xpath-starting-retuning-none-on-scrapy)

I'm trying to crawl a site and to do so, I'm using Scrapy. So, when doing requests to nested pages, the procedure usually gets the the information correctly on the first trials, but, on later requests the nodes starts to return None. I'm using xpath's functionality. Below I'm pasting some lines of the parse function:(I tried this one with the approach of explicitly comparing the class value)(With this one I used the contains function)(I've also used this one when I found more suitable)Am I doing something wrong on paths?

Is there any reason for the crawler to stop reading the nodes correctly?

2018-09-09 14:14:26Z

I'm trying to crawl a site and to do so, I'm using Scrapy. So, when doing requests to nested pages, the procedure usually gets the the information correctly on the first trials, but, on later requests the nodes starts to return None. I'm using xpath's functionality. Below I'm pasting some lines of the parse function:(I tried this one with the approach of explicitly comparing the class value)(With this one I used the contains function)(I've also used this one when I found more suitable)Am I doing something wrong on paths?

Is there any reason for the crawler to stop reading the nodes correctly?Cannot say what the problem is without the log messages or the spider code but..

What happens most of the time is that websites fo not follow a strict html structure .For some properties the 'title' may be inside the span 

but for the next iteration it may be 

span[@class="inlineFree"]/h1/text() or  or any other tag

so you should check the html for those returning None

Python, extract XHR response data from website

da409

[Python, extract XHR response data from website](https://stackoverflow.com/questions/52228851/python-extract-xhr-response-data-from-website)

I am trying to extract some data from https://www.barchart.com/stocks/signals/top-bottom/top?viewName=main.I am able to extract data from normal html using the xpath method, however i noticed that this website gets its data from a network.I have found the location of where the data I want is (the table from the barchart website) which is shown in the picture below.Picture of XHR responseHow can i scrape just the response portion?Thanks!

2018-09-07 19:52:12Z

I am trying to extract some data from https://www.barchart.com/stocks/signals/top-bottom/top?viewName=main.I am able to extract data from normal html using the xpath method, however i noticed that this website gets its data from a network.I have found the location of where the data I want is (the table from the barchart website) which is shown in the picture below.Picture of XHR responseHow can i scrape just the response portion?Thanks!Try this code.

is there a way to put a blank entry in place of a node value for the particular classes that don't contain that node with Scrapy and Python

Hooton

[is there a way to put a blank entry in place of a node value for the particular classes that don't contain that node with Scrapy and Python](https://stackoverflow.com/questions/51988850/is-there-a-way-to-put-a-blank-entry-in-place-of-a-node-value-for-the-particular)

I'm using python and scrapy to pull information from a database of companies online. Each company's information is completely contained in a parent node but not every company has a child node containing its website, some only have the company name. This means when i pull the data with xpath i'm getting 20 company names but only 18 web addresses (per page) which means when i zip up the lists and export i'm only getting the first 18 companies and the websites don't match. is there a way to insert a blank entry into the website list for the companies that don't have the website information node as one of the child nodes. Thank youfrom the above, when i run ideally i'd get ['www.company.co.uk',''] with a blank entry for the second company node since they don't have a p node for the website. when i run xpath i'm getting a longer list of company names than websites so the lists don't match when i zip them together

2018-08-23 15:03:06Z

I'm using python and scrapy to pull information from a database of companies online. Each company's information is completely contained in a parent node but not every company has a child node containing its website, some only have the company name. This means when i pull the data with xpath i'm getting 20 company names but only 18 web addresses (per page) which means when i zip up the lists and export i'm only getting the first 18 companies and the websites don't match. is there a way to insert a blank entry into the website list for the companies that don't have the website information node as one of the child nodes. Thank youfrom the above, when i run ideally i'd get ['www.company.co.uk',''] with a blank entry for the second company node since they don't have a p node for the website. when i run xpath i'm getting a longer list of company names than websites so the lists don't match when i zip them togetherPls, attach some code so people could get better understanding of you problem...Overall, you should follow next pattern:

How to build a price comparison program that scrapes the prices of a product across several websites

darbulix

[How to build a price comparison program that scrapes the prices of a product across several websites](https://stackoverflow.com/questions/51675269/how-to-build-a-price-comparison-program-that-scrapes-the-prices-of-a-product-acr)

I am trying to build a price comparison program for personal use (and for practice) that allows me to compare prices of the same item across different websites. I have just started using the Scrapy library and played around by scraping websites. These are my steps whenever I scrape a new website:1) Find the website's search url, understand its pattern, and store it. For instance, Target's search url is composed by a fixed url="https://www.target.com/s?searchTerm=" plus the search terms (in parsed url)2)Once I know the website's search url, I send a SplashRequest using the Splash library. I do this because many pages are heavily loaded with JS3)Look up the HTML structure of the results page and determine the correct xpath expression to parse the prices. However, many websites have results page in different formats depending on the search terms or product category, changing thus the page's HTML code. Therefore, I have to examine all the possible results page's formats and come up with an xpath that can account for all the different formatsI find this process to be very inefficient, slow, and inaccurate. For instance, at step 3, even though I have the correct xpath, I am still unable to scrape all the prices in the page (sometimes I also get prices of items that are not present in the HTML rendered page), which I dont understand. Also, I dont know whether the websites know that my requests come from a bot, thus maybe sending me a faulty or incorrect HTML code. Moreover, this process cannot be automated. For example, I have to repeat step 1 and 2 for every new website. Therefore, I was wondering if there was a more efficient process, library, or approach that I could use to help me finish this program. I also heard something about using the website's API, although I dont quite understand how it works. This is my first time doing scraping and I dont know too much about web technologies, so any help/advice is highly appreciate!

2018-08-03 14:53:54Z

I am trying to build a price comparison program for personal use (and for practice) that allows me to compare prices of the same item across different websites. I have just started using the Scrapy library and played around by scraping websites. These are my steps whenever I scrape a new website:1) Find the website's search url, understand its pattern, and store it. For instance, Target's search url is composed by a fixed url="https://www.target.com/s?searchTerm=" plus the search terms (in parsed url)2)Once I know the website's search url, I send a SplashRequest using the Splash library. I do this because many pages are heavily loaded with JS3)Look up the HTML structure of the results page and determine the correct xpath expression to parse the prices. However, many websites have results page in different formats depending on the search terms or product category, changing thus the page's HTML code. Therefore, I have to examine all the possible results page's formats and come up with an xpath that can account for all the different formatsI find this process to be very inefficient, slow, and inaccurate. For instance, at step 3, even though I have the correct xpath, I am still unable to scrape all the prices in the page (sometimes I also get prices of items that are not present in the HTML rendered page), which I dont understand. Also, I dont know whether the websites know that my requests come from a bot, thus maybe sending me a faulty or incorrect HTML code. Moreover, this process cannot be automated. For example, I have to repeat step 1 and 2 for every new website. Therefore, I was wondering if there was a more efficient process, library, or approach that I could use to help me finish this program. I also heard something about using the website's API, although I dont quite understand how it works. This is my first time doing scraping and I dont know too much about web technologies, so any help/advice is highly appreciate!The most common problem with crawling is that in general, they are determining everything to be scraped syntactically, while conceptualizing the entities you are to be working with helps a lot, I am speaking from my own experience.In a research about scraping I was involved in we have reached to the conclusion that we need to use a semantic tree. This tree should contain nodes, which represent important data for your purpose and a parent-child relation means that the parent encapsulates the child in the HTML, XML or other hierarchical structure.You will therefore need some kind of concept about how you will want to represent the semantic tree and how it will be mapped with site structures. If your search method allows you to use the logical OR, then you will be able to define the same semantic tree for multiple online sources.On the other hand, if the owners of some sites are willing to allow you to scrape their data, then you might ask them to define the semantic tree.If a given website's structure is changed, then using a semantic tree more often than not you will be able to comply to the change by just changing the selector of a few elements, if the semantic tree's node structure remains the same. If some owners are partners in allowing scraping, then you will be able to just download their semantic trees.If a website provides an API, then you can use that, read about REST APIs to do so. However, these APIs are probably not uniform.

How to run ScraPy Spiders in Play, Java/Scala environment?

Quinten Lootens

[How to run ScraPy Spiders in Play, Java/Scala environment?](https://stackoverflow.com/questions/51692778/how-to-run-scrapy-spiders-in-play-java-scala-environment)

For current project I made a Web API in Scala, however I need to scrape +- 70 urls on each user request. How can I do this with Scrapy while using Scala/Java? Are there other ways to do this?

2018-08-05 08:43:59Z

For current project I made a Web API in Scala, however I need to scrape +- 70 urls on each user request. How can I do this with Scrapy while using Scala/Java? Are there other ways to do this?Try executing a background task using an Actor. Although one way to do this is by scheduling something to occur 2 every N seconds, using an Actor adds a queue (mailbox). A queue is important so you don't overload the server.You can have a controller like this 1:You haven't provided much detail in terms of specifications, but this is a good start that will work for many purposes.

I want to avoid Redirecting (301) with Scrapy and create a json file

OKY

[I want to avoid Redirecting (301) with Scrapy and create a json file](https://stackoverflow.com/questions/51698016/i-want-to-avoid-redirecting-301-with-scrapy-and-create-a-json-file)

I want to scrape Web with Scrapy and make json file.

I am currently studying at "Data Visualization with Python and JavaScript". Attempting to make a json file failed. I think that the cause is "Redirecting (301)", but I do not know how to handle it.Enter the following code in nwinners_list_spider.py in / nobel_winners / spiders.Enter the following code in the root directory.The following display appears and no data is entered in the json file.1.I thought that the function was a problem, I confirmed the movement of each function. Each function itself operated without problems.2.I tried to find out whether Redirecting (301) was the cause and tried to change settings.py. However, I did not know what kind of processing should be concrete.Thank you.

2018-08-05 20:29:14Z

I want to scrape Web with Scrapy and make json file.

I am currently studying at "Data Visualization with Python and JavaScript". Attempting to make a json file failed. I think that the cause is "Redirecting (301)", but I do not know how to handle it.Enter the following code in nwinners_list_spider.py in / nobel_winners / spiders.Enter the following code in the root directory.The following display appears and no data is entered in the json file.1.I thought that the function was a problem, I confirmed the movement of each function. Each function itself operated without problems.2.I tried to find out whether Redirecting (301) was the cause and tried to change settings.py. However, I did not know what kind of processing should be concrete.Thank you.Don't know if this solves!

If redirecting is the issue you can use:This will prevent spider from redirecting.

How use dir() function to see inside scrapy module

MagicHat

[How use dir() function to see inside scrapy module](https://stackoverflow.com/questions/51567524/how-use-dir-function-to-see-inside-scrapy-module)

From the documentation:So i try see inside the scrapy moduleimport scrapy is a module right, or im wrong?Im complete newb in python and just try understand how works.How can i see inside modules like documentation examples

2018-07-28 02:25:07Z

From the documentation:So i try see inside the scrapy moduleimport scrapy is a module right, or im wrong?Im complete newb in python and just try understand how works.How can i see inside modules like documentation examplesTry this from your python interpreter:This worked for me in both Python 2 and 3. I have also confirmed that it works in both iPython and the standard interpreter. If it does not work for you even with the import, your environment may have gotten messed up in some way, and we can troubleshoot further.In this case scrapy is a module, and import scrapy is the syntax for making that module available in whatever context you are invoking the import from. This section of the Python tutorial has information on modules and importing them. 

Unescape Hex code point \u0026 without altering the encoding in Python

Vishal Sharma

[Unescape Hex code point \u0026 without altering the encoding in Python](https://stackoverflow.com/questions/51458016/unescape-hex-code-point-u0026-without-altering-the-encoding-in-python)

While scraping, I'm getting hex code point in the extracted data like \u0026#39; and \u003c. The problem is while extracting, they are getting escaped by getting a "\" before them like \\u0026#39 and \\u003c. So to tackle that I used,  But the problem with using "unicode_escape" is that it alters some special symbols like 🎈❤️🎈❤️🎈 and converting it into Ã°ÂŸÂŽÂˆÃ¢ÂÂ¤Ã¯Â¸ÂÃ°ÂŸÂŽÂˆÃ¢ÂÂ¤Ã¯Â¸ÂÃ°ÂŸÂŽÂˆ. So how do I tackle that?The script from the source code is like:and from this I want to extract : Best "ad-free" entertainment for kids!🎈❤️🎈❤️🎈

2018-07-21 16:22:38Z

While scraping, I'm getting hex code point in the extracted data like \u0026#39; and \u003c. The problem is while extracting, they are getting escaped by getting a "\" before them like \\u0026#39 and \\u003c. So to tackle that I used,  But the problem with using "unicode_escape" is that it alters some special symbols like 🎈❤️🎈❤️🎈 and converting it into Ã°ÂŸÂŽÂˆÃ¢ÂÂ¤Ã¯Â¸ÂÃ°ÂŸÂŽÂˆÃ¢ÂÂ¤Ã¯Â¸ÂÃ°ÂŸÂŽÂˆ. So how do I tackle that?The script from the source code is like:and from this I want to extract : Best "ad-free" entertainment for kids!🎈❤️🎈❤️🎈You can use tag.encode('utf-8') to encode the string correctly this returns bytes which means you must then use wb when writing to the file eg f = open('filename, 'wb'). The below script now encodes the string.It prints: Best &quot;ad-free&quot; entertainment for kids!<br>🎈❤️🎈❤️🎈I'm not sure where you are getting the double slashes from, but they may actually be needed to escape the slashes in the string, so may not pose a problem.

Why is Scrapy executed even if the item pipeline setting is not enough?

Keisuke URAGO

[Why is Scrapy executed even if the item pipeline setting is not enough?](https://stackoverflow.com/questions/51386349/why-is-scrapy-executed-even-if-the-item-pipeline-setting-is-not-enough)

In Scrapy, I was trying to incorporate the ImagesPipeline and run it, but it took me a while to notice the spelling mistakes in the settings. (It expects "IMAGES_STORE" but I typed to "IMAEGS_STORE"). I found that the ImagesPipeline was not loaded in the debug log, but the cause was not immediately understood. It is mysterious why it will not cause an error when running Scrapy.

2018-07-17 16:34:01Z

In Scrapy, I was trying to incorporate the ImagesPipeline and run it, but it took me a while to notice the spelling mistakes in the settings. (It expects "IMAGES_STORE" but I typed to "IMAEGS_STORE"). I found that the ImagesPipeline was not loaded in the debug log, but the cause was not immediately understood. It is mysterious why it will not cause an error when running Scrapy.It did not throw error because you can put your custom settings in settings.py of your choice.So Scrapy thought that IMAEGS_STORE was some your custom variable and that maybe used somewhere.You can put Keisuke_URAGO=123 in settings.py and scrapy will not throw error. This is what settings.py is meant to be.

Python-Scrapy : Yielding a Variable name defined in one function through Another

Vishal Sharma

[Python-Scrapy : Yielding a Variable name defined in one function through Another](https://stackoverflow.com/questions/51014511/python-scrapy-yielding-a-variable-name-defined-in-one-function-through-another)

After declaring a variable in parse function as "self.Title" and yielding the data through another function, it is returning the data of only one URL across all others.What can be going wrong. Here is the code snippet.And the output is coming like Download_URL are different for all 3 scraped URLs but Title although different are coming same for all 3 request.

2018-06-24 22:34:42Z

After declaring a variable in parse function as "self.Title" and yielding the data through another function, it is returning the data of only one URL across all others.What can be going wrong. Here is the code snippet.And the output is coming like Download_URL are different for all 3 scraped URLs but Title although different are coming same for all 3 request.You can't store per-item data on an instance of the Spider class.When parse yields the Request, pass your Title as metadata, as described in the docs. It's then available for use in parse_download on the response.meta property. As a solution, I did wrote this snippet in the first function:And called it into another via :

ERROR: Spider error processing <GET https://www.imovirtual.com/comprar/apartamento/lisboa/> (referer: None)

Joana

[ERROR: Spider error processing <GET https://www.imovirtual.com/comprar/apartamento/lisboa/> (referer: None)](https://stackoverflow.com/questions/50950802/error-spider-error-processing-get-https-www-imovirtual-com-comprar-apartamen)

I'm trying to create a web crawler (in python, using scrapy) that extracts information from an ad, extract what is on the main page and enter the sub page of the same ad and extract the remaining information, but is giving this error when I run the code. Any suggestion?

2018-06-20 14:36:57Z

I'm trying to create a web crawler (in python, using scrapy) that extracts information from an ad, extract what is on the main page and enter the sub page of the same ad and extract the remaining information, but is giving this error when I run the code. Any suggestion?There are a few things to change to make it run:You have to set subpage_link (It does not seem to be defined)Request callbacks have only one parameter which is (Scrapy response) so you 

should replace parse_subpage(self,youritem) by parse_subpage(self, reponse)To send your item with the Request you'd better use Request meta parameter which allows you to transfer data from one scrapy response to another. If you replace scrapy.Request(subpage_link, callback=self.parse_subpage) by scrapy.Request(subpage_link, callback=self.parse_subpage, meta={'item': youritem}) you will have access to youritem when scrapy will call parse_subpage by doing response.meta.get('item')This should work.

unable to fetch facebook friend list using scrapy

Aaditya Pareek

[unable to fetch facebook friend list using scrapy](https://stackoverflow.com/questions/50885091/unable-to-fetch-facebook-friend-list-using-scrapy)

I am trying to fetch the friendlist of a user using scrapy script i am able to get the data for the about and the other basic info sections however with the friend section the similar code does not seem to work. I am using my facebook account logged in the browser also the friendlist of the desired profile is also public. I have tried implementing the following methods:

2018-06-16 04:57:22Z

I am trying to fetch the friendlist of a user using scrapy script i am able to get the data for the about and the other basic info sections however with the friend section the similar code does not seem to work. I am using my facebook account logged in the browser also the friendlist of the desired profile is also public. I have tried implementing the following methods:In [2]: response.xpath('//div[@class="_3i9"]')

Out[2]: []In [3]: response.xpath('//div[@class="_3i9"]/div/a/@href')

Out[3]: []In [4]: response.xpath('//div[@class="_3i9"]/div/ul/li/a/@href')

Out[4]: []In [5]: response.xpath('//div[@class="clearfix _5qo4"]/a/@href')

Out[5]: []In [6]: response.xpath('//div[@class="clearfix _5qo4"]')

Out[6]: []In [7]: response.xpath('//div[@class="uiList _262m _4kg"]')

Out[7]: []In [8]: response.xpath('//ul[@class="uiList _262m _4kg"]')

Out[8]: []In [9]: response.xpath('//ul[@class="collection_wrapper_2356318349"]')

Out[9]: []In [10]: response.xpath('//[@class="collection_wrapper_2356318349"]')

share edit

How to use 2 yield items from 2 different methods in scrapy?

Raj725

[How to use 2 yield items from 2 different methods in scrapy?](https://stackoverflow.com/questions/50780420/how-to-use-2-yield-items-from-2-different-methods-in-scrapy)

I am new to python and scrapy. I am yielding 2 items from 2 different methods, first one is for first-page data, the second one is for second-page data. I am not able to save the data in same order, the second item saving after the first item, but I need to save both at a time.

    Thanks in advance.

2018-06-10 03:57:29Z

I am new to python and scrapy. I am yielding 2 items from 2 different methods, first one is for first-page data, the second one is for second-page data. I am not able to save the data in same order, the second item saving after the first item, but I need to save both at a time.

    Thanks in advance.Raj725, your question is actual for beginners in Scrapy and may be in Python. I had same question before I read Scrapy documentation. It's not possible to understand Scrapy without reading documentation. You can start to read tutorial, then Item section and Pipeline section.It's example how to yield several type of data.1 Need to prepair Items you need in items.py file:2 Now you can use Items in your scrapy code. It's possible to yield Items in any place where you have data to save:3 Example of pipeline.py file. Do not forgot to check type of Item before saving it. In the end of "process_item" you have to return item. 4 Do no forgot to declare your Pipelines in settings.py. Scrapy will not use Pilelines without it.I do not provided ready to use code. I provided examples of code to understand how it work. It's possible to put it to your code and make changes you need.

I didn't show how to save Items to CSV files. You can import "csv" module. Also you can use CsvItemExported in your pipeline.py from "scrapy.exporters". I provided  link with example how to save different Items to different CSV files.

How to properly pass arguments to scrapy spider on scrapinghub?

Emilz

[How to properly pass arguments to scrapy spider on scrapinghub?](https://stackoverflow.com/questions/50387627/how-to-properly-pass-arguments-to-scrapy-spider-on-scrapinghub)

I am trying to pass paramters to my spider (ideally a Dataframe or csv) with:I tried using the *args and **kwargs argument type but each time I only get the last result. For example:When I try to print them from inside my spider I only get the element 3:I think that there is some easy explanation that i just can't seem to understand.Thanks in advance!

2018-05-17 09:12:58Z

I am trying to pass paramters to my spider (ideally a Dataframe or csv) with:I tried using the *args and **kwargs argument type but each time I only get the last result. For example:When I try to print them from inside my spider I only get the element 3:I think that there is some easy explanation that i just can't seem to understand.Thanks in advance!For passing arguments and tags you can do like this

How can I open multiple links on a webpage and scrap there data?

Umair Hanif

[How can I open multiple links on a webpage and scrap there data?](https://stackoverflow.com/questions/49856461/how-can-i-open-multiple-links-on-a-webpage-and-scrap-there-data)

I hope you guys are best with your health and R&D work.I want to open a webpage and that webpage contains many other links, I want to open all those and wants Scrapy to scrap all those web pages. Please help me out.

Thanks in Advance. 

2018-04-16 11:42:17Z

I hope you guys are best with your health and R&D work.I want to open a webpage and that webpage contains many other links, I want to open all those and wants Scrapy to scrap all those web pages. Please help me out.

Thanks in Advance. I have tried with monsterindia.com and open page using scrapy, that page contain multiple links. I have scraped all the data in the respective link and also we can do pagination. The following code may useful.

What triggers the from_crawler classmethod?

user61629

[What triggers the from_crawler classmethod?](https://stackoverflow.com/questions/49604090/what-triggers-the-from-crawler-classmethod)

I'm using scrapy and I have the following functioning pipeline class :class DynamicSQLlitePipeline(object):In my spider I have:From a recent question I was pointed to What is the 'cls' variable used for in Python classes?If I understand correctly in order for the pipeline object to be instantiated (using the init function), it requires a docket number. The docket number only becomes available once the from_crawler class method is run. But what triggers the from_crawler method. Again the code is working.

2018-04-02 00:57:54Z

I'm using scrapy and I have the following functioning pipeline class :class DynamicSQLlitePipeline(object):In my spider I have:From a recent question I was pointed to What is the 'cls' variable used for in Python classes?If I understand correctly in order for the pipeline object to be instantiated (using the init function), it requires a docket number. The docket number only becomes available once the from_crawler class method is run. But what triggers the from_crawler method. Again the code is working.The caller of a classmethod has to have an instance of the class. They may just access it by name, like this:… or:Or maybe you pass the class object to someone, and they store it and use it later like this:In Scrapy, the usual way to register a set of pipelines with the framework, according to the docs, is like this:(Also see the Extensions user guide, which explains how this fits into a scrapy project.)Presumably you've done something similar in code you haven't shown us, putting something like 'sqlscraper.pipelines.DynamicSQLlitePipeline' in that dict. At some point, Scrapy goes through that dict, sorts it in order by the values, and instantiates each pipeline. (Because it has the name of the class, as a string, instead of the class object, this is a little trickier, but the details really aren't relevant here.)

Can't find data using this xpath? in Python Scrapy

Omar Riaz

[Can't find data using this xpath? in Python Scrapy](https://stackoverflow.com/questions/49516102/cant-find-data-using-this-xpath-in-python-scrapy)

https://www.volkswagen-vans.co.uk/en/offers-finance/offers/caddy-panel-van.html

2018-03-27 14:50:20Z

https://www.volkswagen-vans.co.uk/en/offers-finance/offers/caddy-panel-van.htmlThe xpath is not the problem but your (missing) code; in Chrome Dev Tools -> console, I get :When you post here on SO, like @some-progammer said, always post your code.Please, read MCVEI have sort it out by changing my xpath.

Can't extract information without unnecessary characters

Deba

[Can't extract information without unnecessary characters](https://stackoverflow.com/questions/49381342/cant-extract-information-without-unnecessary-characters)

I was doing webscraping with Scrapy and Python and not able to write a proper xpath command to extract full event description from this URL : [https://www.......com/events/9275-big-talks] without any unnecessary character but it's fetching many unnecessary chars like ' ', '\r\n' etc. I am writing this command: And getting this output:How can I get rid of those characters? Please help me !

2018-03-20 10:08:29Z

I was doing webscraping with Scrapy and Python and not able to write a proper xpath command to extract full event description from this URL : [https://www.......com/events/9275-big-talks] without any unnecessary character but it's fetching many unnecessary chars like ' ', '\r\n' etc. I am writing this command: And getting this output:How can I get rid of those characters? Please help me !Consider building an input processor of ItemLoader that removes unnecessary char. For example I'm using:

How do I use a generator expression or list comprehension to generate a series of numbers starting with 000?

eizanendoso

[How do I use a generator expression or list comprehension to generate a series of numbers starting with 000?](https://stackoverflow.com/questions/49268687/how-do-i-use-a-generator-expression-or-list-comprehension-to-generate-a-series-o)

I'm crawling and the pages I'm dealing with have sequential page IDs.

There is no link such as 'Next Page' for the crawler to follow.So, I want to produce a list of numbers starting with00000001and ending with00013099But am having trouble writing an expression/comprehension which captures that. Will be very grateful for any help!Thank you!

2018-03-14 02:03:05Z

I'm crawling and the pages I'm dealing with have sequential page IDs.

There is no link such as 'Next Page' for the crawler to follow.So, I want to produce a list of numbers starting with00000001and ending with00013099But am having trouble writing an expression/comprehension which captures that. Will be very grateful for any help!Thank you!Scrapy's response.follow(href, callback) works not only with actual hrefs to be scraped from the page, but also with auto-generated ones... simply use a for cicle, then generate links by string concatenation, and pass it to follow.

Xpath or css selector attrbute value in scrapy

haider

[Xpath or css selector attrbute value in scrapy](https://stackoverflow.com/questions/49031703/xpath-or-css-selector-attrbute-value-in-scrapy)

Hi there I am new to scrapy and I want to extract an attribute value form an html element. So what could be the right way to extract that attribute value form that html . I want to extract "data-next-url" attributeI am using that xpath but it is not working

2018-02-28 14:19:12Z

Hi there I am new to scrapy and I want to extract an attribute value form an html element. So what could be the right way to extract that attribute value form that html . I want to extract "data-next-url" attributeI am using that xpath but it is not workingIf you check source HTML you'll find this:But you can get next page URL anyway:=>or =>

How to check if key exists in a for loop in python using scrapy

BARNOWL

[How to check if key exists in a for loop in python using scrapy](https://stackoverflow.com/questions/48968376/how-to-check-if-key-exists-in-a-for-loop-in-python-using-scrapy)

I'm trying to see if a business name matches with a previous business name and if it does break the iteration, if not continue with the iteration.The problem It is ignoring the break and I still see duplicates.run.py

2018-02-24 22:41:21Z

I'm trying to see if a business name matches with a previous business name and if it does break the iteration, if not continue with the iteration.The problem It is ignoring the break and I still see duplicates.run.pyI think business_names and business_name both are of list type and if you simply apply in operator it will return FALSE always. so better to use extract_first or any other logic while searching business_name in business_names.Check for existance using the in operator and have some value for item. Right now your code is checking to see if an element of business_name is equal tobusiness_nameTo:

Extract all href using css/xpath selectors

vcovo

[Extract all href using css/xpath selectors](https://stackoverflow.com/questions/48818898/extract-all-href-using-css-xpath-selectors)

I'm trying to extract all hrefs on a page. I have tried the following:

response.css('a::attr(href)').extract()

response.xpath('//@href').extract()It's extracting a significant chunk of the links, but not all of them... More concretely, I'm unable to scrape the twitter link from this site:

https://www.acchain.org/Any insight is appreciated.

2018-02-16 01:26:18Z

I'm trying to extract all hrefs on a page. I have tried the following:

response.css('a::attr(href)').extract()

response.xpath('//@href').extract()It's extracting a significant chunk of the links, but not all of them... More concretely, I'm unable to scrape the twitter link from this site:

https://www.acchain.org/Any insight is appreciated.The website uses javascript to generate some of the content, including the sidebar (generated by https://www.acchain.org/js/sidebar.js)The simplest way to scrape these links would be executing the javascript, e.g. using a browser.

There are multiple ways you could do this, but probably the simplest is using the scrapy-splash middleware.You can use reference of Scrapy Tutorial to write code for this page since it involves javascript to generate the content of body.It should be //a/@href

Tested on Linux bash withxmllint --html --recover --xpath '//a/@href' test.html | sed -e 's/href/\nhref/g'

How to extract link with (.pdf) extension using css and xpath selector's?

Rohan Maran

[How to extract link with (.pdf) extension using css and xpath selector's?](https://stackoverflow.com/questions/60147382/how-to-extract-link-with-pdf-extension-using-css-and-xpath-selectors)

form this block of code I would like to extract pdf link.<a class="btn btn-success btn-responsive" href="http://www.eso-garden.com/specials/living_in_the_light.pdf" target="_blank" rel="nofollow" onclick="c();">Go to PDF</a>

2020-02-10 09:26:39Z

form this block of code I would like to extract pdf link.<a class="btn btn-success btn-responsive" href="http://www.eso-garden.com/specials/living_in_the_light.pdf" target="_blank" rel="nofollow" onclick="c();">Go to PDF</a>

How extract phone Number from script with Xpath

Sweet Vice

[How extract phone Number from script with Xpath](https://stackoverflow.com/questions/58581632/how-extract-phone-number-from-script-with-xpath)

(Scrapy)I need help with the next code:ml-item['datos'] is the script contains the phone number, i need extract only phone number, i try extract with regex and xpath but i cant do it.

The script contains a lot of info, but i only need a phone number, i need extract it with a regex expresion because the phone number change in the next page.

The script is:

2019-10-27 17:16:09Z

(Scrapy)I need help with the next code:ml-item['datos'] is the script contains the phone number, i need extract only phone number, i try extract with regex and xpath but i cant do it.

The script contains a lot of info, but i only need a phone number, i need extract it with a regex expresion because the phone number change in the next page.

The script is:Data in your script tag saved in JSON format.

It can be converted into python data scruture with python built-in json module

My SQL Python ProgrammingError: 1064 (42000) [closed]

edleciel

[My SQL Python ProgrammingError: 1064 (42000) [closed]](https://stackoverflow.com/questions/58487890/my-sql-python-programmingerror-1064-42000)

I am trying to store scraped data with scrapy to a sql database but I get the following error: ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 7Please find my pipelines.py bellow

2019-10-21 13:59:21Z

I am trying to store scraped data with scrapy to a sql database but I get the following error: ProgrammingError: 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 7Please find my pipelines.py bellowYou have a coma too much, use instead

Crawling case-sensitive URLs but Scraping case-insensitive with Scrapy

user3004549

[Crawling case-sensitive URLs but Scraping case-insensitive with Scrapy](https://stackoverflow.com/questions/58015441/crawling-case-sensitive-urls-but-scraping-case-insensitive-with-scrapy)

I am using Scrapy to crawl and scrape numerous websites. Scrapy needs to crawl the URLs in a case-sensitive mode as this is an important information when requesting a web page. Many websites link to some of the webpages using different casings of the same URLs which fools Scrapy into creating duplicates scrapes. For example, the page https://www.example.com/index.html links to https://www.example.com/User1.php and https://www.example.com/user1.phpWe need Scrapy to collect both pages as when we see the page User1.php, we do not know yet that we will collect a clone of it later through user1.php. We cannot lowercase https://www.example.com/User1.php either during the crawl as the server may return a 404 error when the page https://www.example.com/user1.php is not available.So what I am looking for is a solution to tell Scrapy to crawl URLs in a case-sensitive mode, but to duplicate filter the pages, once collected, in a case-insensitive mode before they are scraped to eliminate the risks of duplicates. Does anyone know how to operate Scrapy under both modes at the same time.

2019-09-19 16:34:55Z

I am using Scrapy to crawl and scrape numerous websites. Scrapy needs to crawl the URLs in a case-sensitive mode as this is an important information when requesting a web page. Many websites link to some of the webpages using different casings of the same URLs which fools Scrapy into creating duplicates scrapes. For example, the page https://www.example.com/index.html links to https://www.example.com/User1.php and https://www.example.com/user1.phpWe need Scrapy to collect both pages as when we see the page User1.php, we do not know yet that we will collect a clone of it later through user1.php. We cannot lowercase https://www.example.com/User1.php either during the crawl as the server may return a 404 error when the page https://www.example.com/user1.php is not available.So what I am looking for is a solution to tell Scrapy to crawl URLs in a case-sensitive mode, but to duplicate filter the pages, once collected, in a case-insensitive mode before they are scraped to eliminate the risks of duplicates. Does anyone know how to operate Scrapy under both modes at the same time.You will likely want to create a custom DupeFilter that extends BaseDupeFilter, then set DUPEFILTER_CLASS = "my_package.MyDupeFilter" in your settings.pyYou may have plenty of luck just subclassing the existing RFPDupeFilter and inserting a line into def request_seen(self, request) that case-folds the URL before fingerprinting it:In fact, that sounds like such a common feature, if you find that change works for you, then submit a PR to Scrapy to add case_fold = settings.getbool("DUPEFILTER_CASE_INSENSITIVE") so others can benefit from that change

TypeError: Object of type 'bytes' is not JSON serializable python3

shahrukh ijaz

[TypeError: Object of type 'bytes' is not JSON serializable python3](https://stackoverflow.com/questions/58023234/typeerror-object-of-type-bytes-is-not-json-serializable-python3)

i received this error after running my spider i also have a pipeline and i convert everything into JSON but still got this error after my item return my code is 

2019-09-20 06:55:43Z

i received this error after running my spider i also have a pipeline and i convert everything into JSON but still got this error after my item return my code is This means you are using Scrapy's Item classSolution is that either do thisOr in your Spider, do NOT use Item class to create item, just use a Dict like item = {}

Getting list of urls form wikipedia page

Faisal

[Getting list of urls form wikipedia page](https://stackoverflow.com/questions/57241960/getting-list-of-urls-form-wikipedia-page)

I have a list of names of fortune 500 companies.

here is an example [Abbott Laboratories,Progressive,Arrow Electronics,Kraft Heinz

Plains GP Holdings,Gilead Sciences,Mondelez International,Northrop Grumman]Now I want to get the complete url from Wikipedia for each element in the list.https://en.wikipedia.org/wiki/Abbott_Laboratories (this is only one example)

2019-07-28 14:44:52Z

I have a list of names of fortune 500 companies.

here is an example [Abbott Laboratories,Progressive,Arrow Electronics,Kraft Heinz

Plains GP Holdings,Gilead Sciences,Mondelez International,Northrop Grumman]Now I want to get the complete url from Wikipedia for each element in the list.https://en.wikipedia.org/wiki/Abbott_Laboratories (this is only one example)The biggest problem is looking for possible sites and only selecting the one belonging to the company.One somewhat wrong way would be just just appending the company name to the wiki url and hoping that it works. That results in a) it works (like Abbott Laboratories), b) it produces a page, but not the right one (Progressive, should be Progressive_Corporation) or c) it produces no result at all.Another (way better) option would be using the wikipedia package (https://pypi.org/project/wikipedia/) and its built-in search function. The problem of selecting the right site still remains, so you basically have to do this by hand (or create a good automatic selection like searching for the word "company")

How to scrap the invisible dynamic elements without selenium?

pajarnas

[How to scrap the invisible dynamic elements without selenium?](https://stackoverflow.com/questions/56743993/how-to-scrap-the-invisible-dynamic-elements-without-selenium)

I am working on a simulated login on nike.com. The login content is dynamic, which means it was set invisible in the source codes:After analyzing the source code, I found the following javascript source code may be related to the issue:The display:none codes will not be viewed before the execution of the javascript codes. How to access the codes without using selenium? Any help will be appreciated!

2019-06-24 21:11:18Z

I am working on a simulated login on nike.com. The login content is dynamic, which means it was set invisible in the source codes:After analyzing the source code, I found the following javascript source code may be related to the issue:The display:none codes will not be viewed before the execution of the javascript codes. How to access the codes without using selenium? Any help will be appreciated!Use WebdriverWait and following xpath to access the login form.Maybe webbot?  webbot even works web pages which have dynamically changing id and classnames and has more methods and features than selenium or mechanize.Here's a snippet :)Or, keep it super-simple.SplashJS works for me. 

Splash is a javascript rendering service. It’s a lightweight web browser with an HTTP API, implemented in Python 3 using Twisted and QT5. The (twisted) QT reactor is used to make the service fully asynchronous allowing to take advantage of webkit concurrency via QT main loop. Some of Splash features:

How to make a loop in scraping a page?

Bilal Khan

[How to make a loop in scraping a page?](https://stackoverflow.com/questions/55770603/how-to-make-a-loop-in-scraping-a-page)

I am scraping a page but I have a problem. I don't want to print items['Paragraphs'] = response.css('p::text').extract() in a function again and again. Instead I want to make a loop of it. I tried several times but failed. Here is the code..

2019-04-20 05:37:57Z

I am scraping a page but I have a problem. I don't want to print items['Paragraphs'] = response.css('p::text').extract() in a function again and again. Instead I want to make a loop of it. I tried several times but failed. Here is the code..Can you describe, what output do you want to get?As far as I understood your problem, you can apply zip to your dicts, it will merge your values and make iteration possible in more clear way. And better to yield item in the end of cycle.Or just why not to write proper dict from the very beginning?

Scrapy Request somehow cuts the URLs

Tribic

[Scrapy Request somehow cuts the URLs](https://stackoverflow.com/questions/55408189/scrapy-request-somehow-cuts-the-urls)

i like to spider a url that looks like this:

https://steamcommunity.com/market/search?appid=730#p1_popular_descBecause the End is dynamic, i create the list of urls in parse and then make a request loop.The Problem is, that he cuts the url after appid=730 - so each url looks the same. If i switch to dont_filter=true, i see he loops again and again over page1. I dont get the Problem :( the "x" in the code will get dynamic later (thats the start_url is needed for), think that has nothing to do with the issue.Seems he always crawls from the referer url, not the one i gave him. the url may not end by 730.expected: 150 results, 10 by each url in the loop.actual: 15 results, but each 10 times - all from the first url.

2019-03-28 23:08:46Z

i like to spider a url that looks like this:

https://steamcommunity.com/market/search?appid=730#p1_popular_descBecause the End is dynamic, i create the list of urls in parse and then make a request loop.The Problem is, that he cuts the url after appid=730 - so each url looks the same. If i switch to dont_filter=true, i see he loops again and again over page1. I dont get the Problem :( the "x" in the code will get dynamic later (thats the start_url is needed for), think that has nothing to do with the issue.Seems he always crawls from the referer url, not the one i gave him. the url may not end by 730.expected: 150 results, 10 by each url in the loop.actual: 15 results, but each 10 times - all from the first url.The URL on the address bar appears as you say, but if you inspect the requests on the Network tab of your browser developer tool you will see the request that returns new items is this:This Json contains the page HTML on the field results_html, you can create a selector with this value if you want to get the data using xpath.Reading the response of this URL you can also notice there is a tip saying it is also possible to add a parameter &norender=1 to the URL and don't work with HTML at all. So it is up to you to choose what you are most comfortable with.Lots of sites do this so you have to keep an eye on the requests and don't always trust what appears on the the address bar. I advise you to never even trust what appear on the "inspector" and always check the source code (right click > See page source code).

Not getting full json object from Scrapy response

h s

[Not getting full json object from Scrapy response](https://stackoverflow.com/questions/54580454/not-getting-full-json-object-from-scrapy-response)

I am new to scraping. Learning it to collect data for deep learning projects. So, I made a spider to scrape restaurant information from airbnb website. Spider:Ignore last parse method.Now the data I want to scrape is in sections list of this Json object. But when I run the spider. sections is returned as empty list in response( check bold letters). Here's the object from response:Why this is happening?

2019-02-07 19:01:18Z

I am new to scraping. Learning it to collect data for deep learning projects. So, I made a spider to scrape restaurant information from airbnb website. Spider:Ignore last parse method.Now the data I want to scrape is in sections list of this Json object. But when I run the spider. sections is returned as empty list in response( check bold letters). Here's the object from response:Why this is happening?If you take out all the parameters, some of which I assume are filtering the result set, then you get several items in the sections list: Potentially add the other parameters back in one by one until you work out what is causing the result set to be empty.

Scrapy a link with a protection?

sgr

[Scrapy a link with a protection?](https://stackoverflow.com/questions/54160817/scrapy-a-link-with-a-protection)

I want to scrapy the link between this protection ?Someone can tell me if really possible to do this ?Best regards

2019-01-12 15:04:44Z

I want to scrapy the link between this protection ?Someone can tell me if really possible to do this ?Best regardsI don't like giving newcomers to StackOverflow a down vote. You should become more acquainted with the official SO docs on asking a proper question here.As to your question...If you are asking if you can obtain the download link, then yes. I won't actually try to download this file but the "continue" button has a token and callback to a function JS called "onSubmit()". This function the I ASSUME takes you to the download page.You should always include what you are trying to accomplish when posting on here; also writing elaborate well thought out question is key to getting the response your looking for!I scrapy shelled the site with no user-agents and got a 200 response so seems like there should be no reason why you would not be able to "scrapy" the site. 

scrapy hyphen ignored in xpath attribute

Scrubster

[scrapy hyphen ignored in xpath attribute](https://stackoverflow.com/questions/53786694/scrapy-hyphen-ignored-in-xpath-attribute)

At basketball-reference.com there is a table I am wanting to parse with Xpath and Scrapy. When I try to scrape all of the rows that have the attribute 'data-row' I get nothing.  However, I am able to access the children of the data-rows as long as I don't specifically say 'data-row'.  I believe this has to do with the hyphen being in the attribute name.  Output = [ ]I have made a workaround to this, however if I could know what is wrong that would be great.

2018-12-14 20:48:33Z

At basketball-reference.com there is a table I am wanting to parse with Xpath and Scrapy. When I try to scrape all of the rows that have the attribute 'data-row' I get nothing.  However, I am able to access the children of the data-rows as long as I don't specifically say 'data-row'.  I believe this has to do with the hyphen being in the attribute name.  Output = [ ]I have made a workaround to this, however if I could know what is wrong that would be great.eLRuLL was kind enough to give an answer in the comments of my question.  I believe the '-row' portion was dynamically added via javascript.  I ran in to the problem again on another site.  By using google chrome and going to developer tools->Network->JS, I viewed a script that adds to the class attribute.  I am assuming this is what was happening before based on eLRuLL's comment and that the other website was indeed doing it like that.

Need to Download all .pdf file in given URL using scrapy

Vinod kumar

[Need to Download all .pdf file in given URL using scrapy](https://stackoverflow.com/questions/52989220/need-to-download-all-pdf-file-in-given-url-using-scrapy)

**I Tried to Run this scrapy Query to download the all the related PDF from given URL **I tried to execute this using "scrapy crawl mySpider"Can anyone help me with this  ? Thanks in Advance.

2018-10-25 12:24:37Z

**I Tried to Run this scrapy Query to download the all the related PDF from given URL **I tried to execute this using "scrapy crawl mySpider"Can anyone help me with this  ? Thanks in Advance.Flaws in the code: http://www.pwc.com/us/en/tax-services/publications/research-and-insights.html 

this url is redirecting to https://www.pwc.com/us/en/services/tax/library.htmlAlso there is no div with the id all_results so no div#all_results exists in the html response returned to the crawler. So the first line of code in the parse method should generate error.  For the scrapy crawl command to work you should be in a directory where the configuration file scrapy.cfg exists. Edit: I hope this code helps you. It downloads all the pdfs from the given link.Code: The code repository can be found at: 

https://github.com/NilanshBansal/File_download_ScrapyYou should run the command inside the directory where scrapy.cfg is present.

Curl Scraper working on localhost but not on online servers

Abhishek Kumar

[Curl Scraper working on localhost but not on online servers](https://stackoverflow.com/questions/52940178/curl-scraper-working-on-localhost-but-not-on-online-servers)

I am trying to scrape https://www.gst.gov.in  This code works perfect on localhost but not on the server.

I have tried using different serves with various curl methods with custom headers and referrers but no luck. On the server, I get a connection timeout error. If I use any other https URL or another site it works fine. the problem is with this specific URL can anyone help in scraping this page also if anyone can tell if the remote server is blocking the request then how to bypass this.

2018-10-23 02:12:27Z

I am trying to scrape https://www.gst.gov.in  This code works perfect on localhost but not on the server.

I have tried using different serves with various curl methods with custom headers and referrers but no luck. On the server, I get a connection timeout error. If I use any other https URL or another site it works fine. the problem is with this specific URL can anyone help in scraping this page also if anyone can tell if the remote server is blocking the request then how to bypass this.It's a government website - it's most likely blocking any IPs from geographical region that is not local. i.e. your server needs to be in India or have Indian proxy since you are trying to download Indian government page.

how to extract one element from a site

Yan Zhang

[how to extract one element from a site](https://stackoverflow.com/questions/51345428/how-to-extract-one-element-from-a-site)

i want to extract contact information from the site:

http://www.smtnet.com/company/index.cfm?fuseaction=view_company&company_id=49509

i already done by the code as below:but i failed to exact the company website,it should be https://www.europlacer.com/.ay one can tell me how to extract it?

2018-07-15 05:05:00Z

i want to extract contact information from the site:

http://www.smtnet.com/company/index.cfm?fuseaction=view_company&company_id=49509

i already done by the code as below:but i failed to exact the company website,it should be https://www.europlacer.com/.ay one can tell me how to extract it?If you just want the href attribute of "Visit Website" button, then use this:But, the above code will return you only this:Since the URL of the company (i.e. 'https://www.europlacer.com/') is NOT directly stored in the href attribute. (It is resolved later using a javascript) But if you closely look at the source:You can see the direct URL is present as an argument to the function in onclickattribute so you need to extract it out from there. First, to extract the onclick attribute's value, do this:Then, extract your required URL from it like this:Another method to extract the URL would be to actually resolve the value of the href attribute. You can see, when you click on the link, it becomes something like:So, the trick would be to prepend the hostname ("http://www.smtnet.com"), load the URL and then extract the loaded URL once it changes. But the first method I described in my answer would be lot easier.Additionally for the company name, I think you should try this:Since, the above line prints only the company name (i.e. "EUROPLACER"). Your code takes in some text as well.When you inspect the Visit website button in the developer console, you see thisYou want to grab the Anchor element and retrieve the URL from the onclick attribute like soFirstFind a unique element: you can't use CSS class or element's id to get the element, so you have to find a unique element that helps you to get the targeted element. This img can help you:So, you can get it like this:SecondGet targeted element: how can this unique element help you? The element with the company URL is its parent node (we can reach it with /..) and we need its onclick:Final stepExtract demanded text: you can use many methods and tools, I just test regex and it works properly:Note that I make little changes and use the regex pattern mentioned here. Don't forget to import re package also.

Scrapy: How do I select the next `td` in this `tr`?

Username

[Scrapy: How do I select the next `td` in this `tr`?](https://stackoverflow.com/questions/51218193/scrapy-how-do-i-select-the-next-td-in-this-tr)

I want to select the next sibling of a td tag in a tr element. The tr element is this:My Scrapy code looks like this: response.xpath("//text()[contains(.,'Created On:')]/following-sibling::td"). But that gives me an empty list [].How do I select the next td?

2018-07-06 22:07:31Z

I want to select the next sibling of a td tag in a tr element. The tr element is this:My Scrapy code looks like this: response.xpath("//text()[contains(.,'Created On:')]/following-sibling::td"). But that gives me an empty list [].How do I select the next td?Try this XPath expression:You were trying to use the following-sibling axis from the wrong context node. Going back one level fixes this problem.An alternative is matching the td element in the first place like in this expression:

How to scrap a web forum using Python

Muhammad Magdi Youssif

[How to scrap a web forum using Python](https://stackoverflow.com/questions/51058409/how-to-scrap-a-web-forum-using-python)

I want to know the best methodology to create a python program that periodically scraps a web forum that has many threads and each thread has many posts from different users. I am not inquiring about the packages to be used, I want the high level architecture of the solution. A referral to posts that addressed scrapping huge web forums will be highly appreciated.

2018-06-27 08:49:32Z

I want to know the best methodology to create a python program that periodically scraps a web forum that has many threads and each thread has many posts from different users. I am not inquiring about the packages to be used, I want the high level architecture of the solution. A referral to posts that addressed scrapping huge web forums will be highly appreciated.There are many ways of doing this. But remember few things:

1. The Content you are trying to extract/scrape should be the part of source code of the Page and not generated by javascript or other such similar way. 

2. If there is complex authentication process, you might have to wreck your brains too much. For such case, its better you use PhantomJS on Selenium. Coming to the tools you can use:

1.To send HTTP GET and POST requests you can use Requests module of Python. 

2.The module has feature to download the source code of the page. 

3.To Parse that sourcecode and get your content, you can use minidom parser or BeautifulSoup. 

python scrapy code not giving any output

AbeerK K

[python scrapy code not giving any output](https://stackoverflow.com/questions/49384608/python-scrapy-code-not-giving-any-output)

I am trying to run the code using python(scrapy) but there is no output.

I am also tyring to login to a webpage, let me know if there are any errorsThe code i am using is this:I tried calling the function and this is the error I received: 

2018-03-20 12:45:45Z

I am trying to run the code using python(scrapy) but there is no output.

I am also tyring to login to a webpage, let me know if there are any errorsThe code i am using is this:I tried calling the function and this is the error I received: There is no output because you don't call anything.In other worlds, you defined what is MySpider but you didn't used it. Here's a link that could help youThe error message could not be plainer: The spider must have a name. There is no name in the code you have posted. This is basic to creating a spider in Scrapy. Also, your Python spacing is terrible, you need an editor with Pylint or something that will tell you about PEP8.Change your code toand run your spider by for more information 

How to use make sure scrapy is downloaded correctly?

sbarnes123

[How to use make sure scrapy is downloaded correctly?](https://stackoverflow.com/questions/49101880/how-to-use-make-sure-scrapy-is-downloaded-correctly)

I have scrapy downloaded in anaconda and I am trying to open it inside the command prompt and I am getting the message "'scrapy' is not recognized as an internal or external command, operable program or batch file', can anyone tell me how to resolve this?

2018-03-05 00:12:41Z

I have scrapy downloaded in anaconda and I am trying to open it inside the command prompt and I am getting the message "'scrapy' is not recognized as an internal or external command, operable program or batch file', can anyone tell me how to resolve this?I believe you have used conda install scrapy to install the package.Then the only reason is: you did not add /path/to/conda/Scripts (E.g. C:\Anaconda2\Scripts) to your PATH.This results in you cannot find scrapy, or you can use the full path something like /path/to/conda/Scripts/scrapy to call it.

Python regex to get first occurance only from ecommerce data inside script tag

Sumit Kumar Mallick

[Python regex to get first occurance only from ecommerce data inside script tag](https://stackoverflow.com/questions/53865926/python-regex-to-get-first-occurance-only-from-ecommerce-data-inside-script-tag)

I'm using this regular expresion to get data from an eCommerce website which has required data inside script tag   - "quantity":"(.*?)"This regular expression is giving me all the matches in the data. I have tried many ways to construct it using non-greedy method but no luck. Which expression should I use to stop the search at the first match from the data?TIAsample Script data :

2018-12-20 09:39:13Z

I'm using this regular expresion to get data from an eCommerce website which has required data inside script tag   - "quantity":"(.*?)"This regular expression is giving me all the matches in the data. I have tried many ways to construct it using non-greedy method but no luck. Which expression should I use to stop the search at the first match from the data?TIAsample Script data :There are 2 approaches here:(Better) Non-Regex approachThe "text" you are trying to match looks (almost) like a valid JSON object / Python dictionary (except for some extra parentheses and the HOLIDAY part which may originate from bad copy-pasting). Use this fact to convert it to an object then just grab the top-level 'quantity' key:(Last resort) Regex approachIf you insist, you can use re.search which by default stops at the first match as written in its documentation:You can try this:

Syntax error while trying to run scrapy crawl on Mac? [duplicate]

Jessica

[Syntax error while trying to run scrapy crawl on Mac? [duplicate]](https://stackoverflow.com/questions/52105782/syntax-error-while-trying-to-run-scrapy-crawl-on-mac)

I installed Scrapy as suggested on Scrapy.org, installing Xcode, homebrew, and then using the command "pip3 install scrapy". I also used "sudo easy_install pip" after a few errors with the pip command. Now I'm getting this SyntaxError whenever I try running my FirstSpider, scrapy crawl FirstSpider: Can anyone explain what's happening or how to solve it?Full error report: 

2018-08-30 21:46:05Z

I installed Scrapy as suggested on Scrapy.org, installing Xcode, homebrew, and then using the command "pip3 install scrapy". I also used "sudo easy_install pip" after a few errors with the pip command. Now I'm getting this SyntaxError whenever I try running my FirstSpider, scrapy crawl FirstSpider: Can anyone explain what's happening or how to solve it?Full error report: The code you are using needs to be updated for the newer Python version you are using, as async has become a reserved keyword in Python 3.7.Ask the project that you are using to fix this. In the meantime, downgrading to Python 3.6 will let you run the software.The traceback shows you that the error is in the twisted project, which does not yet support Python 3.7. The project is aware of this and actively working towards fixing the problem.

Issue with index not outputting even though it shows in the list

crein147

[Issue with index not outputting even though it shows in the list](https://stackoverflow.com/questions/59340439/issue-with-index-not-outputting-even-though-it-shows-in-the-list)

I am web scraping a site to get item specs. For some reason I am not able to pull a certain spec for some item even though its on the site.When I output the item's spec's xpath text, the index I want to pull shows, '<td>\nColor\n</td>',Here is how I am going about it all.Pull all the specs for said item:Get the spec I want by finding it with .indexGet the spec infoThen I just yield it as the spec name with the info.What is going wrong? Also, hope I explained well enough!https://pastebin.com/vgSaiawq

2019-12-15 01:02:54Z

I am web scraping a site to get item specs. For some reason I am not able to pull a certain spec for some item even though its on the site.When I output the item's spec's xpath text, the index I want to pull shows, '<td>\nColor\n</td>',Here is how I am going about it all.Pull all the specs for said item:Get the spec I want by finding it with .indexGet the spec infoThen I just yield it as the spec name with the info.What is going wrong? Also, hope I explained well enough!https://pastebin.com/vgSaiawqSo the \n was messing with my outputting to a csv file. I fixed it with the following:and adding this to my spec variables:

How to get data to crawl reviews from any url page

Siddhant Singh

[How to get data to crawl reviews from any url page](https://stackoverflow.com/questions/55454220/how-to-get-data-to-crawl-reviews-from-any-url-page)

Suppose you have a play store comments page in which hundreds of people commenting regularly. My idea is to take data from the play store comments page not only on the same page but also when you're scrolling. 

So it should take

All the data from the comment pages 

Who has commented on it?

If he has given any ratings or not. 

Including his/her photos. How do I approach this problem and what tools I can use for this? 

Share all the links where I can read more about this type of problem statement.

2019-04-01 11:38:57Z

Suppose you have a play store comments page in which hundreds of people commenting regularly. My idea is to take data from the play store comments page not only on the same page but also when you're scrolling. 

So it should take

All the data from the comment pages 

Who has commented on it?

If he has given any ratings or not. 

Including his/her photos. How do I approach this problem and what tools I can use for this? 

Share all the links where I can read more about this type of problem statement.I would use scrapy with looping requests to the ajax URL. Logic to terminate the loop can handled a few ways depending on what data is available.How to scrape all contents from infinite scroll website? scrapyScraping Infinite Scrolling Pages with "load more" button using Scrapyhttps://stackoverflow.com/search?q=scrapy+infinite+scroll

webcrawler only loops over starturl

English 100

[webcrawler only loops over starturl](https://stackoverflow.com/questions/54099111/webcrawler-only-loops-over-starturl)

i know there is some key concenpt im mising can some1 fix the code or point me in the right direction.

ill google and try some things but i dont know why its not workingany help appreciated/ will +1if you need more detail please comment 

    import scrapy

    import timeplease link some resources (ineed more detail)

2019-01-08 20:16:43Z

i know there is some key concenpt im mising can some1 fix the code or point me in the right direction.

ill google and try some things but i dont know why its not workingany help appreciated/ will +1if you need more detail please comment 

    import scrapy

    import timeplease link some resources (ineed more detail)remove this lineyield scrapy.Request(start_url, dont_filter=True, priority=-1)because after extracting the data, the parse() method yields a new request on start_url again and again.

i want to scrap this part

joes

[i want to scrap this part](https://stackoverflow.com/questions/55088806/i-want-to-scrap-this-part)

I'm trying to scrap each panel in the screenshot but i didn't get the right xpath to scrap those parts .Any one can help me please.https://www.seloger.com/annonces/achat/appartement/paris-15eme-75/saint-lambert/142632059.htm?cp=75&idtt=2,5&idtypebien=2,1&LISTING-LISTpg=2&naturebien=1,2,4&tri=initial&

2019-03-10 14:41:26Z

I'm trying to scrap each panel in the screenshot but i didn't get the right xpath to scrap those parts .Any one can help me please.https://www.seloger.com/annonces/achat/appartement/paris-15eme-75/saint-lambert/142632059.htm?cp=75&idtt=2,5&idtypebien=2,1&LISTING-LISTpg=2&naturebien=1,2,4&tri=initial&This data is taken from additional request to https://www.seloger.com/detail,json,caracteristique_bien.json?idannonce=142632059. There you will get json with whole information.UPD:

How to retrieve data from json response with scrapy?

Rahul Dudharejiya

[How to retrieve data from json response with scrapy?](https://stackoverflow.com/questions/53295129/how-to-retrieve-data-from-json-response-with-scrapy)

I am using scrapy with python.This is my url:https://www.workingnomads.co/jobsapi/job/_search?sort=expired:asc,premium:desc,pub_date:desc&_source=company,category_name,description,location_base,instructions,id,external_id,slug,title,pub_date,tags,source,apply_url,premium,expired,use_atMy code:It returns the response in JSON format which looks like this.How can I get the value of a specific key? this one is the postman response i want to retrive apply_url key value.

2018-11-14 07:34:34Z

I am using scrapy with python.This is my url:https://www.workingnomads.co/jobsapi/job/_search?sort=expired:asc,premium:desc,pub_date:desc&_source=company,category_name,description,location_base,instructions,id,external_id,slug,title,pub_date,tags,source,apply_url,premium,expired,use_atMy code:It returns the response in JSON format which looks like this.How can I get the value of a specific key? this one is the postman response i want to retrive apply_url key value.You'll want to access:Where x is the number of items/nodes under hits. See https://jsoneditoronline.org/?id=3757afd4ef634f99ae7264372eaf0ff4As you can see, there are 10 items or nodes under hits -> hits. apply_url is under _source for each item.For example, print(jsonresponse['hits']['hits'][0]['_source']['apply_url']) would produce:https://boards.greenhouse.io/mesosphere/jobs/1422922?gh_jid=1422922

