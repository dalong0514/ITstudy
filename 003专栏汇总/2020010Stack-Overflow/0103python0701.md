What does the / mean in Python 3.4's help output for range before the closing parenthesis?It signifies the end of the positional only parameters, parameters you cannot use as keyword parameters. Such parameters can only be specified in the C API.It means the key argument to __contains__ can only be passed in by position (range(5).__contains__(3)), not as a keyword argument (range(5).__contains__(key=3)), something you can do with positional arguments in pure-python functions.Also see the Argument Clinic documentation:and the (very recent addition to) the Python FAQ:The syntax is now part of the Python language specification, as of version 3.8, see PEP 570 – Python Positional-Only Parameters. Before PEP 570, the syntax was already reserved for possible future inclusion in Python, see PEP 457 - Syntax For Positional-Only Parameters. Positional-only parameters can lead to cleaner and clearer APIs, make pure-Python implementations of otherwise C-only modules more consistent and easier to maintain, and because positional-only parameters require very little processing, they lead to faster Python code.I asked this question myself. :) Found out that / was originally proposed by Guido in here.Then his proposal won.I think the very relevant document covering this is PEP 570.

Where recap section looks nice.If the function ends with /This means all functional arguments are positional. Forward Slash (/) indicates all arguments prior to it are positional only argument. Positional only arguments feature was added in python 3.8 after PEP 570 was accepted. Initially this notation was defined in PEP 457 - Notation for Notation For Positional-Only Parameters Let's take following exampleHere in the above function definition parameters a and b are positional-only, while x or y can be either positional or keyword. Following function calls are valid But, following function call is not valid which raises an exception TypeError since a, b are not passed as positional arguments instead passed as keywordMany built in function in python accept positional only arguments where passing arguments by keyword doesn't make sense. For example built-in function len accepts only one positional(only) argument, Where calling len as len(obj="hello world")  impairs readability, check help(len).  Positional only parameters make underlying c/library functions easy to maintain. It allows parameters names of positional only parameters to be changes in future without risk of breaking client code that uses API Last but not least, positional only parameters allow us to use their names to be used in variable length keyword arguments. Check following example Positional only parameters is better Explained here at Types of function arguments in python: Positional Only Parameters Positional-only parameters syntax was officially added to python3.8. Checkout what's new python3.8 - positional only arguments PEP Related: PEP 570 -- Python Positional-Only Parameters

Getting realtime output using subprocess

Chris Lieb

[Getting realtime output using subprocess](https://stackoverflow.com/questions/803265/getting-realtime-output-using-subprocess)

I am trying to write a wrapper script for a command line program (svnadmin verify) that will display a nice progress indicator for the operation.  This requires me to be able to see each line of output from the wrapped program as soon as it is output.I figured that I'd just execute the program using subprocess.Popen, use stdout=PIPE, then read each line as it came in and act on it accordingly.  However, when I ran the following code, the output appeared to be buffered somewhere, causing it to appear in two chunks, lines 1 through 332, then 333 through 439 (the last line of output)After looking at the documentation on subprocess a little, I discovered the bufsize parameter to Popen, so I tried setting bufsize to 1 (buffer each line) and 0 (no buffer), but neither value seemed to change the way the lines were being delivered.At this point I was starting to grasp for straws, so I wrote the following output loop:but got the same result.Is it possible to get 'realtime' program output of a program executed using subprocess?  Is there some other option in Python that is forward-compatible (not exec*)?

2009-04-29 16:45:48Z

I am trying to write a wrapper script for a command line program (svnadmin verify) that will display a nice progress indicator for the operation.  This requires me to be able to see each line of output from the wrapped program as soon as it is output.I figured that I'd just execute the program using subprocess.Popen, use stdout=PIPE, then read each line as it came in and act on it accordingly.  However, when I ran the following code, the output appeared to be buffered somewhere, causing it to appear in two chunks, lines 1 through 332, then 333 through 439 (the last line of output)After looking at the documentation on subprocess a little, I discovered the bufsize parameter to Popen, so I tried setting bufsize to 1 (buffer each line) and 0 (no buffer), but neither value seemed to change the way the lines were being delivered.At this point I was starting to grasp for straws, so I wrote the following output loop:but got the same result.Is it possible to get 'realtime' program output of a program executed using subprocess?  Is there some other option in Python that is forward-compatible (not exec*)?I tried this, and for some reason while the codebuffers aggressively, the variantdoes not. Apparently this is a known bug: http://bugs.python.org/issue3907 (The issue is now "Closed" as of Aug 29, 2018)You can direct the subprocess output to the streams directly. Simplified example:You can try this:If you use readline instead of read, there will be some cases where the input message is not printed. Try it with a command the requires an inline input and see for yourself.The Streaming subprocess stdin and stdout with asyncio in Python blog post by Kevin McCarthy shows how to do it with asyncio:I ran into the same problem awhile back.  My solution was to ditch iterating for the read method, which will return immediately even if your subprocess isn't finished executing, etc.Real Time Output Issue resolved:

I did encountered similar issue in Python, while capturing the real time output from c program.  I added "fflush(stdout);" in my C code. It worked for me. Here is the snip the code << C Program >><< Python Program >><< OUTPUT>>

Print: Count  1

Print: Count  2

Print: Count  3Hope it helps.~sairamDepending on the use case, you might also want to disable the buffering in the subprocess itself.If the subprocess will be a Python process, you could do this before the call:Or alternatively pass this in the env argument to Popen.Otherwise, if you are on Linux/Unix, you can use the stdbuf tool. E.g. like:See also here about stdbuf or other options.(See also here for the same answer.)I used this solution to get realtime output on a subprocess. This loop will stop as soon as the process completes leaving out a need for a break statement or possible infinite loop. You may use an iterator over each byte in the output of the subprocess. This allows inline update (lines ending with '\r' overwrite previous output line) from the subprocess:In Python 3.x the process might hang because the output is a byte array instead of a string. Make sure you decode it into a string. Starting from Python 3.6 you can do it using the parameter encoding in Popen Constructor. The complete example:Note that this code redirects stderr to stdout and handles output errors.Using pexpect [ http://www.noah.org/wiki/Pexpect ] with non-blocking readlines will resolve this problem. It stems from the fact that pipes are buffered, and so your app's output is getting buffered by the pipe, therefore you can't get to that output until the buffer fills or the process dies.Found this "plug-and-play" function here. Worked like a charm!Complete solution:This is the basic skeleton that I always use for this. It makes it easy to implement timeouts and is able to deal with inevitable hanging processes.(This solution has been tested with Python 2.7.15)

You just need to sys.stdout.flush() after each line read/write:

Why do you have to call .iteritems() when iterating over a dictionary in python?

Falmarri

[Why do you have to call .iteritems() when iterating over a dictionary in python?](https://stackoverflow.com/questions/3744568/why-do-you-have-to-call-iteritems-when-iterating-over-a-dictionary-in-python)

Why do you have to call iteritems() to iterate over key, value pairs in a dictionary? ieWhy isn't that the default behavior of iterating over a dictionary

2010-09-19 05:03:33Z

Why do you have to call iteritems() to iterate over key, value pairs in a dictionary? ieWhy isn't that the default behavior of iterating over a dictionaryFor every python container C, the expectation is thatwill pass just fine -- wouldn't you find it astonishing if one sense of in (the loop clause) had a completely different meaning from the other (the presence check)?  I sure would!  It naturally works that way for lists, sets, tuples, ...So, when C is a dictionary, if in were to yield key/value tuples in a for loop, then, by the principle of least astonishment, in would also have to take such a tuple as its left-hand operand in the containment check.How useful would that be?  Pretty useless indeed, basically making if (key, value) in C a synonym for if C.get(key) == value -- which is a check I believe I may have performed, or wanted to perform, 100 times more rarely than what if k in C actually means, checking the presence of the key only and completely ignoring the value.On the other hand, wanting to loop just on keys is quite common, e.g.:having the value as well would not help particularly:actually somewhat less clear and less concise.  (Note that items was the original spelling of the "proper" methods to use to get key/value pairs: unfortunately that was back in the days when such accessors returned whole lists, so to support "just iterating" an alternative spelling had to be introduced, and iteritems  it was -- in Python 3, where backwards compatibility constraints with previous Python versions were much weakened, it became items again).My guess: Using the full tuple would be more intuitive for looping, but perhaps less so for testing for membership using in.That code wouldn't really work if you had to specify both key and value for in. I am having a hard time imagining use case where you'd check if both the key AND value are in the dictionary. It is far more natural to only test the keys.Now it's not necessary that the in operator and for ... in operate over the same items. Implementation-wise they are different operations (__contains__ vs. __iter__). But that little inconsistency would be somewhat confusing and, well, inconsistent.

Better to 'try' something and catch the exception or test if it's possible first to avoid an exception?

chown

[Better to 'try' something and catch the exception or test if it's possible first to avoid an exception?](https://stackoverflow.com/questions/7604636/better-to-try-something-and-catch-the-exception-or-test-if-its-possible-first)

Should I test if something is valid or just try to do it and catch the exception?For example, should I:Or:Some thoughts...

PEP 20 says:Should using a try instead of an if be interpreted as an error passing silently?  And if so, are you explicitly silencing it by using it in this way, therefore making it OK?I'm not referring to situations where you can only do things 1 way; for example:

2011-09-29 23:55:29Z

Should I test if something is valid or just try to do it and catch the exception?For example, should I:Or:Some thoughts...

PEP 20 says:Should using a try instead of an if be interpreted as an error passing silently?  And if so, are you explicitly silencing it by using it in this way, therefore making it OK?I'm not referring to situations where you can only do things 1 way; for example:You should prefer try/except over if/else if that results inOften, these go hand-in-hand.speed-upsIn the case of trying to find an element in a long list by:the try, except is the best option when the index is probably in the list and the IndexError is usually not raised. This way you avoid the need for an extra lookup by if index < len(my_list).Python encourages the use of exceptions, which you handle is a phrase from Dive Into Python. Your example not only handles the exception (gracefully), rather than letting it silently pass, also the exception occurs only in the exceptional case of index not being found (hence the word exception!).cleaner codeThe official Python Documentation mentions EAFP: Easier to ask for forgiveness than permission and Rob Knight notes that catching errors rather than avoiding them, can result in cleaner, easier to read code. His example says it like this:Worse (LBYL 'look before you leap'):Better (EAFP: Easier to ask for forgiveness than permission):In this particular case, you should use something else entirely:In general, though: If you expect the test to fail frequently, use if. If the test is expensive relative to just trying the operation and catching the exception if it fails, use try. If neither one of these conditions applies, go with whatever reads easier.Using try and except directly rather than inside an if guard should always be done if there is any possibility of a race condition.  For example, if you want to ensure that a directory exists, do not do this:If another thread or process creates the directory between isdir and mkdir, you'll exit.  Instead, do this:That will only exit if the 'foo' directory can't be created.If it's trivial to check whether something will fail before you do it, you should probably favor that. After all, constructing exceptions (including their associated tracebacks) takes time.Exceptions should be used for:Note that oftentimes, the real answer is "neither" - for instance, in your first example, what you really should do is just use .get() to provide a default:Using try is acknowledging that an error may pass, which is the opposite of having it pass silently. Using except is causing it not to pass at all.Using try: except: is preferred in cases where if: else: logic is more complicated. Simple is better than complex; complex is better than complicated; and it's easier to ask for forgiveness than permission.What "errors should never pass silently" is warning about, is the case where code could raise an exception that you know about, and where your design admits the possibility, but you haven't designed in a way to deal with the exception. Explicitly silencing an error, in my view, would be doing something like pass in an except block, which should only be done with an understanding that "doing nothing" really is the correct error handling in the particular situation. (This is one of the few times where I feel like a comment in well-written code is probably really needed.)However, in your particular example, neither is appropriate:The reason everyone is pointing this out - even though you acknowledge your desire to understand in general, and inability to come up with a better example - is that equivalent side-steps actually exist in quite a lot of cases, and looking for them is the first step in solving the problem.As the other posts mention, it depends on the situation.  There are a few dangers with using try/except in place of checking the validity of your data in advance, especially when using it on bigger projects.e.g., suppose you had:The IndexError says nothing about whether it occurred when trying to get an element of index_list or my_list.Whenever you use try/except for control flow, ask yourself:If the answer to one or more of these questions is 'no', there might be a lot of forgiveness to ask for; most likely from your future self.An example.

I recently saw code in a larger project that looked like this:Talking to the programmer it turned that the intended control flow was:This worked because foo made a database query and the query would be successful if x was an integer and throw a ProgrammingError if x was a list.Using try/except is a bad choice here:For a general meaning, you may consider reading Idioms and Anti-Idioms in Python: Exceptions.In your particular case, as others stated, you should use dict.get():

Mock vs MagicMock

Vladimir Ignatov

[Mock vs MagicMock](https://stackoverflow.com/questions/17181687/mock-vs-magicmock)

My understanding is that MagicMock is a superset of Mock that automatically does "magic methods" thus seamlessly providing support for lists, iterations and so on... Then what is the reason for plain Mock existing? Isn't that just a stripped down version of MagicMock that can be practically ignored? Does Mock class know any tricks that are not available in MagicMock?

2013-06-19 01:49:02Z

My understanding is that MagicMock is a superset of Mock that automatically does "magic methods" thus seamlessly providing support for lists, iterations and so on... Then what is the reason for plain Mock existing? Isn't that just a stripped down version of MagicMock that can be practically ignored? Does Mock class know any tricks that are not available in MagicMock?Mock's author, Michael Foord, addressed a very similar question at Pycon 2011 (31:00):With Mock you can mock magic methods but you have to define them. MagicMock has "default implementations of most of the magic methods.".If you don't need to test any magic methods, Mock is adequate and doesn't bring a lot of extraneous things into your tests. If you need to test a lot of magic methods MagicMock will save you some time.To begin with, MagicMock is a subclass of Mock.As a result, MagicMock provides everything that Mock provides and more.  Rather than thinking of Mock as being a stripped down version of MagicMock, think of MagicMock as an extended version of Mock.  This should address your questions about why Mock exists and what does Mock provide on top of MagicMock.Secondly, MagicMock provides default implementations of many/most magic methods, whereas Mock doesn't.  See here for more information on the magic methods provided.Some examples of provided magic methods:And these which may not be as intuitive (at least not intuitive to me):You can "see" the methods added to MagicMock as those methods are invoked for the first time:So, why not use MagicMock all the time?The question back to you is: Are you okay with the default magic method implementations?  For example, is it okay for mocked_object[1] to not error?  Are you okay with any unintended consequences due to the magic method implementations being already there?If the answer to these questions is a yes, then go ahead and use MagicMock.  Otherwise, stick to Mock.This is what python's official documentation

says:I've found another particular case where simple Mock may turn more useful than MagicMock:Comparing against ANY can be useful, for example, comparing almost every key between two dictionaries where some value is calculated using a mock.This will be valid if you're using Mock:while it will raise an AssertionError if you've used MagicMock 

Activate a virtualenv via fabric as deploy user

Thomas Schreiber

[Activate a virtualenv via fabric as deploy user](https://stackoverflow.com/questions/1180411/activate-a-virtualenv-via-fabric-as-deploy-user)

I want to run my fabric script locally, which will in turn, log into my server, switch user to deploy, activate the projects .virtualenv, which will change dir to the project and issue a git pull.I typically use the workon command from virtualenvwrapper which sources the activate file and the postactivate file will put me in the project folder. In this case, it seems that because fabric runs from within shell, control is give over to fabric, so I can't use bash's source built-in to '$source ~/.virtualenv/myvenv/bin/activate'Anybody have an example and explanation of how they have done this?

2009-07-24 22:03:57Z

I want to run my fabric script locally, which will in turn, log into my server, switch user to deploy, activate the projects .virtualenv, which will change dir to the project and issue a git pull.I typically use the workon command from virtualenvwrapper which sources the activate file and the postactivate file will put me in the project folder. In this case, it seems that because fabric runs from within shell, control is give over to fabric, so I can't use bash's source built-in to '$source ~/.virtualenv/myvenv/bin/activate'Anybody have an example and explanation of how they have done this?Right now, you can do what I do, which is kludgy but works perfectly well* (this usage assumes you're using virtualenvwrapper -- which you should be -- but you can easily substitute in the rather longer 'source' call you mentioned, if not):Since version 1.0, Fabric has a prefix context manager which uses this technique so you can for example:* There are bound to be cases where using the command1 && command2 approach may blow up on you, such as when command1 fails (command2 will never run) or if command1 isn't properly escaped and contains special shell characters, and so forth.As an update to bitprophet's forecast: With Fabric 1.0 you can make use of prefix() and your own context managers.I'm just using a simple wrapper function virtualenv() that can be called instead of run(). It doesn't use the cd context manager, so relative paths can be used.virtualenvwrapper can make this a little simplerThis is my approach on using virtualenv with local deployments.Using fabric's path() context manager you can run pip or python with binaries from virtualenv.Thanks to all answers posted and I would like to add one more alternative for this. There is an module, fabric-virtualenv, which can provide the function as the same code:fabric-virtualenv makes use of fabric.context_managers.prefix, which might be a good way :)If you want to install the packages to environment or want to run commands according to the packages you have in environment, I have found this hack to solve my problem, instead of writing complex methods of fabric or installing new OS packages:This way you might not need to activate the environment, but you can execute commands under the environment.Here is code for a decorator that will result in the use of Virtual Environment for any run/sudo calls:and then to use the decorator, note the order of the decorators is important:This approach worked for me, you can apply this too.Assuming venv is your virtual env directory and add this method wherever appropriate.

Why are trailing commas allowed in a list?

Burhan Khalid

[Why are trailing commas allowed in a list?](https://stackoverflow.com/questions/11597901/why-are-trailing-commas-allowed-in-a-list)

I am curious why in Python a trailing comma in a list is valid syntax, and it seems that Python simply ignores it:It makes sense when its a tuple since ('a') and ('a',) are two different things, but in lists?

2012-07-22 05:11:30Z

I am curious why in Python a trailing comma in a list is valid syntax, and it seems that Python simply ignores it:It makes sense when its a tuple since ('a') and ('a',) are two different things, but in lists?The main advantages are that it makes multi-line lists easier to edit and that it reduces clutter in diffs.Changing:to:involves only a one-line change in the diff:This beats the more confusing multi-line diff when the trailing comma was omitted:The latter diff makes it harder to see that only one line was added and that the other line didn't change content.It also reduces the risk of doing this:and triggering implicit string literal concatenation, producing s = ['manny', 'mo', 'jackroger'] instead of the intended result.It's a common syntactical convention to allow trailing commas in an array, languages like C and Java allow it, and Python seems to have adopted this convention for its list data structure. It's particularly useful when generating code for populating a list: just generate a sequence of elements and commas, no need to consider the last one as a special case that shouldn't have a comma at the end.It helps to eliminate a certain kind of bug. It's sometimes clearer to write lists on multiple lines.

But in, later maintenace you may want to rearrange the items.But if you allow trailing commas, and use them, you can easily rearrange the lines without introducing an error.A tuple is different because ('a') is expanded using implicit continuation and ()s as a precendence operator, whereas ('a',) refers to a length 1 tuple.Your original example would have been tuple('a')The main reason is to make diff less complicated.

For example you have a list :and you want to add another element to it. Then you will be end up doing this:thus, diff will show that two lines have been changed, first adding ',' in line with 'c' and adding 'd' at last line.So, python allows trailing ',' in last element of list, to prevent extra diff which can cause confusion.

How to read the RGB value of a given pixel in Python?

Josh Hunt

[How to read the RGB value of a given pixel in Python?](https://stackoverflow.com/questions/138250/how-to-read-the-rgb-value-of-a-given-pixel-in-python)

If I open an image with open("image.jpg"), how can I get the RGB values of a pixel assuming I have the coordinates of the pixel?Then, how can I do the reverse of this? Starting with a blank graphic, 'write' a pixel with a certain RGB value?I would prefer if I didn't have to download any additional libraries.

2008-09-26 08:10:50Z

If I open an image with open("image.jpg"), how can I get the RGB values of a pixel assuming I have the coordinates of the pixel?Then, how can I do the reverse of this? Starting with a blank graphic, 'write' a pixel with a certain RGB value?I would prefer if I didn't have to download any additional libraries.It's probably best to use the Python Image Library to do this which I'm afraid is a separate download.The easiest way to do what you want is via the load() method on the Image object which returns a pixel access object which you can manipulate like an array:Alternatively, look at ImageDraw which gives a much richer API for creating images.Using Pillow (which works with Python 3.X as well as Python 2.7+), you can do the following:Now you have all pixel values. If it is RGB or another mode can be read by im.mode. Then you can get pixel (x, y) by:Alternatively, you can use Numpy and reshape the array:A complete, simple to use solution isPyPNG - lightweight PNG decoder/encoderAlthough the question hints at JPG, I hope my answer will be useful to some people.Here's how to read and write PNG pixels using PyPNG module:PyPNG is a single pure Python module less than 4000 lines long, including tests and comments.PIL is a more comprehensive imaging library, but it's also significantly heavier.As Dave Webb said:Image manipulation is a complex topic, and it's best if you do use a library. I can recommend gdmodule which provides easy access to many different image formats from within Python.There's a really good article on wiki.wxpython.org entitled Working With Images. The article mentions the possiblity of using wxWidgets (wxImage), PIL or PythonMagick. Personally, I've used PIL and wxWidgets and both make image manipulation fairly easy.You can use pygame's surfarray module. This module has a 3d pixel array returning method called pixels3d(surface). I've shown usage below:I hope been helpful. Last word: screen is locked for lifetime of screenpix.install PIL using the command "sudo apt-get install python-imaging" and run the following program. It will print RGB values of the image. If the image is large redirect the output to a file using '>' later open the file to see RGB valuesYou could use the Tkinter module, which is the standard Python interface to the Tk GUI toolkit and you don't need extra download. See https://docs.python.org/2/library/tkinter.html.(For Python 3, Tkinter is renamed to tkinter)Here is how to set RGB values:And get RGB:If you are looking to have three digits in the form of an RGB colour code, the following code should do just that.This may work for you.

Showing line numbers in IPython/Jupyter Notebooks

Ruggero Turra

[Showing line numbers in IPython/Jupyter Notebooks](https://stackoverflow.com/questions/10979667/showing-line-numbers-in-ipython-jupyter-notebooks)

Error reports from most language kernels running in IPython/Jupyter Notebooks indicate the line on which the error occurred; but (at least by default) no line numbers are indicated in Notebooks.Is it possibile to add the line numbers to IPython/Jupyter Notebooks?

2012-06-11 11:59:15Z

Error reports from most language kernels running in IPython/Jupyter Notebooks indicate the line on which the error occurred; but (at least by default) no line numbers are indicated in Notebooks.Is it possibile to add the line numbers to IPython/Jupyter Notebooks?CTRL - ML toggles line numbers in the CodeMirror area.  See the QuickHelp for other keyboard shortcuts.In more details CTRL - M (or ESC) bring you to command mode, then pressing the L keys should toggle the visibility of current cell line numbers. In more recent notebook versions Shift-L should toggle for all cells.If you can't remember the shortcut, bring up the command palette Ctrl-Shift+P (Cmd+Shift+P on Mac), and search for "line numbers"), it should allow to toggle and show you the shortcut.On IPython 2.2.0, just typing l (lowercase L) on command mode (activated by typing Esc) works.  See [Help] - [Keyboard Shortcuts] for other shortcuts.Also, you can set default behavior to display line numbers by editing custom.js.Select the Toggle Line Number Option from the  View -> Toggle Line Number.For me, ctrl + m is used to save the webpage as png, so it does not work properly. But I find another way.On the toolbar, there is a bottom named open the command paletee, you can click it and type in the line, and you can see the toggle cell line number here. To turn line numbers on by default in all cells at startup I recommend this link:https://www.webucator.com/blog/2015/11/show-line-numbers-by-default-in-ipython-notebook/I quote ...Navigate to your jupyter config directory, which you can find by typing the following at the command line:jupyter --config-dirFrom there, open or create the custom folder. In that folder, you should find a custom.js file. If there isn’t one, you should be able to create one. Open it in a text editor and add this code:

define([

    'base/js/namespace',

    'base/js/events'

    ],

    function(IPython, events) {

        events.on("app_initialized.NotebookApp",

            function () {

                IPython.Cell.options_default.cm_config.lineNumbers = true;

            }

        );

    }

);

Here is how to know active shortcut (depending on your OS and notebook version, it might change)Help > Keyboard Shortcuts > toggle line numbersOn OSX running ipython3 it was ESC LYou can also find Toggle Line Numbers under View on the top toolbar of the Jupyter notebook in your browser.

This adds/removes the lines numbers in all notebook cells.For me, Esc+l only added/removed the line numbers of the active cell.Was looking for this: Shift-L in JupyterLab 1.0.01.press esc to enter the command mode

2.perss l(it L in lowcase) to show the line number

List all base classes in a hierarchy of given class?

Sridhar Ratnakumar

[List all base classes in a hierarchy of given class?](https://stackoverflow.com/questions/1401661/list-all-base-classes-in-a-hierarchy-of-given-class)

Given a class Foo (whether it is a new-style class or not), how do you generate all the base classes - anywhere in the inheritance hierarchy - it issubclass of?

2009-09-09 19:41:52Z

Given a class Foo (whether it is a new-style class or not), how do you generate all the base classes - anywhere in the inheritance hierarchy - it issubclass of?inspect.getmro(cls) works for both new and old style classes and returns the same as NewClass.mro(): a list of the class and all its ancestor classes, in the order used for method resolution.See the __bases__ property available on a python class, which contains a tuple of the bases classes:inspect.getclasstree() will create a nested list of classes and their bases. 

Usage:you can use the __bases__ tuple of the class object:The tuple returned by __bases__ has all its base classes.Hope it helps!attention that in python 3.x every class inherits from base object class.According to the Python doc, we can also simply use class.__mro__ attribute or class.mro() method:Although Jochen's answer is very helpful and correct, as you can obtain the class hierarchy using the .getmro() method of the inspect module, it's also important to highlight that Python's inheritance hierarchy is as follows:ex: 

An inheriting class ex: 

An inherited classOne class can inherit from another

 - The class' attributed are inherited

 - in particular, its methods are inherited

 - this means that instances of an inheriting (child) class can access attributed of the inherited (parent) classinstance -> class -> then inherited classesusing

will show you the hierarchy, within Python.

Python division

Adam Nelson

[Python division](https://stackoverflow.com/questions/2958684/python-division)

I was trying to normalize a set of numbers from -100 to 0 to a range of 10-100 and was having problems only to notice that even with no variables at all, this does not evaluate the way I would expect it to:Float division doesn't work either:If either side of the division is cast to a float it will work:Each side in the first example is evaluating as an int which means the final answer will be cast to an int. Since 0.111 is less than .5, it rounds to 0.  It is not transparent in my opinion, but I guess that's the way it is.What is the explanation?

2010-06-02 14:33:00Z

I was trying to normalize a set of numbers from -100 to 0 to a range of 10-100 and was having problems only to notice that even with no variables at all, this does not evaluate the way I would expect it to:Float division doesn't work either:If either side of the division is cast to a float it will work:Each side in the first example is evaluating as an int which means the final answer will be cast to an int. Since 0.111 is less than .5, it rounds to 0.  It is not transparent in my opinion, but I guess that's the way it is.What is the explanation?You're using Python 2.x, where integer divisions will truncate instead of becoming a floating point number.You should make one of them a float:or from __future__ import division, which the forces / to adopt Python 3.x's behavior that always returns a float.You're putting Integers in so Python is giving you an integer back:If if you cast this to a float afterwards the rounding will have already been done, in other words, 0 integer will always become 0 float.If you use floats on either side of the division then Python will give you the answer you expect.So in your case:You need to change it to a float BEFORE you do the division. That is:In Python 2.7, the / operator is an integer division if inputs are integers:In Python 3.3, the / operator is a float division even if the inputs are integer.For integer division in Python 3, we will use the // operator.The // operator is an integer division operator in both Python 2.7 and Python 3.3.In Python 2.7 and Python 3.3:Now, see the comparisonFor the above program, the output will be False in Python 2.7 and True in Python 3.3.In Python 2.7 a = 1.75 and b = 1.In Python 3.3 a = 1.75 and b = 1.75, just because / is a float division.It has to do with the version of python that you use. Basically it adopts the C behavior: if you divide two integers, the results will be rounded down to an integer. Also keep in mind that Python does the operations from left to right, which plays a role when you typecast.Example:

Since this is a question that always pops in my head when I am doing arithmetic operations (should I convert to float and which number), an example from that aspect is presented:When we divide integers, not surprisingly it gets lower rounded.If we typecast the last integer to float, we will still get zero, since by the time our number gets divided by the float has already become 0 because of the integer division.Same scenario as above but shifting the float typecast a little closer to the left side.Finally, when we typecast the first integer to float, the result is the desired one, since beginning from the first division, i.e. the leftmost one, we use floats.Extra 1: If you are trying to answer that to improve arithmetic evaluation, you should check this Extra 2: Please be careful of the following scenario:Specifying a float by placing a '.' after the number will also cause it to default to float. Make at least one of them float, then it will be float division, not integer:Casting the result to float is too late.In python cv2 not updated the division calculation. so, you must include from __future__ import division  in first line of the program.Either way, it's integer division.  10/90 = 0.  In the second case, you're merely casting 0 to a float.Try casting one of the operands of "/" to be a float:You're casting to float after the division has already happened in your second example. Try this:I'm somewhat surprised that no one has mentioned that the original poster might have liked rational numbers to result.  Should you be interested in this, the Python-based program Sage has your back.  (Currently still based on Python 2.x, though 3.x is under way.)This isn't a solution for everyone, because it does do some preparsing so these numbers aren't ints, but Sage Integer class elements.  Still, worth mentioning as a part of the Python ecosystem.Personally I preferred to insert a 1. * at the very beginning. So the expression become something like this:As I always do a division for some formula like:so it is impossible to simply add a .0 like 20.0. And in my case, wrapping with a float() may lose a little bit readability.

How to check if a word is an English word with Python?

Barthelemy

[How to check if a word is an English word with Python?](https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python)

I want to check in a Python program if a word is in the English dictionary.I believe nltk wordnet interface might be the way to go but I have no clue how to use it for such a simple task.In the future, I might want to check if the singular form of a word is in the dictionary (e.g., properties -> property -> english word). How would I achieve that?

2010-09-24 16:01:15Z

I want to check in a Python program if a word is in the English dictionary.I believe nltk wordnet interface might be the way to go but I have no clue how to use it for such a simple task.In the future, I might want to check if the singular form of a word is in the dictionary (e.g., properties -> property -> english word). How would I achieve that?For (much) more power and flexibility, use a dedicated spellchecking library like PyEnchant. There's a tutorial, or you could just dive straight in:PyEnchant comes with a few dictionaries (en_GB, en_US, de_DE, fr_FR), but can use any of the OpenOffice ones if you want more languages.There appears to be a pluralisation library called inflect, but I've no idea whether it's any good.It won't work well with WordNet, because WordNet does not contain all english words.

Another possibility based on NLTK without enchant is NLTK's words corpusUsing NLTK:You should refer to this article if you have trouble installing wordnet or want to try other approaches.Using a set to store the word list because looking them up will be faster:To answer the second part of the question, the plurals would already be in a good word list, but if you wanted to specifically exclude those from the list for some reason, you could indeed write a function to handle it. But English pluralization rules are tricky enough that I'd just include the plurals in the word list to begin with.As to where to find English word lists, I found several just by Googling "English word list". Here is one: http://www.sil.org/linguistics/wordlists/english/wordlist/wordsEn.txt  You could Google for British or American English if you want specifically one of those dialects.For a faster NLTK-based solution you could hash the set of words to avoid a linear search.I find that there are 3 package-based solutions to solve the problem. They are pyenchant, wordnet and corpus(self-defined or from ntlk). Pyenchant couldn't installed easily in win64 with py3. Wordnet doesn't work very well because it's corpus isn't complete. So for me, I choose the solution answered by @Sadik, and use 'set(words.words())' to speed up.First:Then:For a semantic web approach, you could run a sparql query against WordNet in RDF format.  Basically just use urllib module to issue GET request and return results in JSON format, parse using python 'json' module.  If it's not English word you'll get no results.As another idea, you could query Wiktionary's API.With pyEnchant.checker SpellChecker:

Underscore vs Double underscore with variables and methods [duplicate]

Jordon Bedwell

[Underscore vs Double underscore with variables and methods [duplicate]](https://stackoverflow.com/questions/6930144/underscore-vs-double-underscore-with-variables-and-methods)

Somebody was nice enough to explain to me that __method() mangles but instead of bothering him further since there are a lot of other people who need help I was wondering if somebody could elaborate the differences further.For example I don't need mangling but does _ stay private so somebody couldn't do instance._method()? Or does it just keep it from overwriting another variable by making it unique?  I don't need my internal methods "hidden" but since they are specific to use I don't want them being used outside of the class.

2011-08-03 16:52:14Z

Somebody was nice enough to explain to me that __method() mangles but instead of bothering him further since there are a lot of other people who need help I was wondering if somebody could elaborate the differences further.For example I don't need mangling but does _ stay private so somebody couldn't do instance._method()? Or does it just keep it from overwriting another variable by making it unique?  I don't need my internal methods "hidden" but since they are specific to use I don't want them being used outside of the class.From PEP 8:Also, from David Goodger's Code Like a Pythonista:A single leading underscore is simply a convention that means, "You probably shouldn't use this."  It doesn't do anything to stop someone from using the attribute.A double leading underscore actually changes the name of the attribute so that two classes in an inheritance hierarchy can use the same attribute name, and they will not collide.There is no access control in Python. You can access all attributes of a class, and that includes mangled names (as _class__variable). Concentrate on your code and API instead of trying to protect developers from themselves.

Keras, How to get the output of each layer?

GoingMyWay

[Keras, How to get the output of each layer?](https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer)

I have trained a binary classification model with CNN, and here is my codeAnd here, I wanna get the output of each layer just like TensorFlow, how can I do that?

2017-01-18 04:07:16Z

I have trained a binary classification model with CNN, and here is my codeAnd here, I wanna get the output of each layer just like TensorFlow, how can I do that?You can easily get the outputs of any layer by using: model.layers[index].outputFor all layers use this:Note: To simulate Dropout use learning_phase as 1. in layer_outs otherwise use 0.Edit: (based on comments)K.function creates theano/tensorflow tensor functions which is later used to get the output from the symbolic graph given the input. Now K.learning_phase() is required as an input as many Keras layers like Dropout/Batchnomalization depend on it to change behavior during training and test time. So if you remove the dropout layer in your code you can simply use:Edit 2: More optimizedI just realized that the previous answer is not that optimized as for each function evaluation the data will be transferred CPU->GPU memory and also the tensor calculations needs to be done for the lower layers over-n-over. Instead this is a much better way as you don't need multiple functions but a single function giving you the list of all outputs:From https://keras.io/getting-started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layerOne simple way is to create a new Model that will output the layers that you are interested in:Alternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:Based on all the good answers of this thread, I wrote a library to fetch the output of each layer. It abstracts all the complexity and has been designed to be as user-friendly as possible:https://github.com/philipperemy/keractIt handles almost all the edge casesHope it helps!Following looks very simple to me:Above is a tensor object, so you can modify it using operations that can be applied to a tensor object. For example, to get the shape model.layers[idx].output.get_shape()idx is the index of the layer and you can find it from model.summary()I wrote this function for myself (in Jupyter) and it was inspired by indraforyou's answer. It will plot all the layer outputs automatically. Your images must have a (x, y, 1) shape where 1 stands for 1 channel. You just call plot_layer_outputs(...) to plot.Wanted to add this as a comment (but don't have high enough rep.) to @indraforyou's answer to correct for the issue mentioned in @mathtick's comment. To avoid the InvalidArgumentError: input_X:Y is both fed and fetched. exception, simply replace the line outputs = [layer.output for layer in model.layers] with outputs = [layer.output for layer in model.layers][1:], i.e.adapting indraforyou's minimal working example:p.s. my attempts trying things such as outputs = [layer.output for layer in model.layers[1:]] did not work. From: https://github.com/philipperemy/keras-visualize-activations/blob/master/read_activations.pyWell, other answers are very complete, but there is a very basic way to "see", not to "get" the shapes.Just do a model.summary(). It will print all layers and their output shapes. "None" values will indicate variable dimensions, and the first dimension will be the batch size. Assuming you have:1- Keras pre-trained model. 2- Input x as image or set of images. The resolution of image should be compatible with dimension of the input layer. For example 80*80*3 for 3-channels (RGB) image. 3- The name of the output layer to get the activation. For example, "flatten_2" layer. This should be include in the layer_names variable, represents name of layers of the given model.4- batch_size is an optional argument.Then you can easily use get_activation function to get the activation of the output layer for a given input x and pre-trained model:In case you have one of the following cases:  You need to do the following changes:Minimum example:

Convert int to ASCII and back in Python

mlissner

[Convert int to ASCII and back in Python](https://stackoverflow.com/questions/3673428/convert-int-to-ascii-and-back-in-python)

I'm working on making a URL shortener for my site, and my current plan (I'm open to suggestions) is to use a node ID to generate the shortened URL. So, in theory, node 26 might be short.com/z, node 1 might be short.com/a, node 52 might be short.com/Z, and node 104 might be short.com/ZZ. When a user goes to that URL, I need to reverse the process (obviously).I can think of some kludgy ways to go about this, but I'm guessing there are better ones. Any suggestions?

2010-09-09 02:47:08Z

I'm working on making a URL shortener for my site, and my current plan (I'm open to suggestions) is to use a node ID to generate the shortened URL. So, in theory, node 26 might be short.com/z, node 1 might be short.com/a, node 52 might be short.com/Z, and node 104 might be short.com/ZZ. When a user goes to that URL, I need to reverse the process (obviously).I can think of some kludgy ways to go about this, but I'm guessing there are better ones. Any suggestions?ASCII to int:gives 97And back to a string:gives 'a'If multiple characters are bound inside a single integer/long, as was my issue:Yields '0123456789' and x = 227581098929683594426425LWhat about BASE58 encoding the URL? Like for example flickr does.Turning that back into a number isn't a big deal either.Use hex(id)[2:] and int(urlpart, 16).  There are other options.  base32 encoding your id could work as well, but I don't know that there's any library that does base32 encoding built into Python.Apparently a base32 encoder was introduced in Python 2.4 with the base64 module.  You might try using b32encode and b32decode.  You should give True for both the casefold and map01 options to b32decode in case people write down your shortened URLs.Actually, I take that back.  I still think base32 encoding is a good idea, but that module is not useful for the case of URL shortening.  You could look at the implementation in the module and make your own for this specific case.  :-)

How to check if string input is a number? [duplicate]

Trufa

[How to check if string input is a number? [duplicate]](https://stackoverflow.com/questions/5424716/how-to-check-if-string-input-is-a-number)

How do I check if a user's string input is a number (e.g. -1, 0, 1, etc.)?The above won't work since input always returns a string.

2011-03-24 19:51:53Z

How do I check if a user's string input is a number (e.g. -1, 0, 1, etc.)?The above won't work since input always returns a string.Simply try converting it to an int and then bailing out if it doesn't work.Apparently this will not work for negative values, but it will for positive numbers.  Use isdigit()The method isnumeric() will do the job (Documentation for python3.x):But remember:isnumeric() returns True if all characters in the string are numeric characters, and there is at least one character.So negative numbers are not accepted.For Python 3 the following will work.EDITED:

You could also use this below code to find out if its a number or also a negativeyou could also change your format to your specific requirement.

I am seeing this post a little too late.but hope this helps other persons who are looking for answers :) .  let me know if anythings wrong in the given code.If you specifically need an int or float, you could try "is not int" or "is not float":If you only need to work with ints, then the most elegant solution I've seen is the ".isdigit()" method:Works fine for check if an input is

   a positive Integer AND in a specific rangeI would recommend this, @karthik27, for negative numbers Then do whatever you want with that regular expression, match(), findall() etcthe most elegant solutions would be the already proposed,Unfortunatelly it doesn't work both for negative integers and for general float values of a. If your point is to check if 'a' is a generic number beyond integers i'd suggest the following one, which works for every kind of float and integer :). Here is the test:I hope you find it useful :)You can use the isdigit() method for strings.

In this case, as you said the input is always a string: natural: [0, 1, 2 ... ∞]Python 2Python 3integer: [-∞, .., -2, -1, 0, 1, 2, ∞]float: [-∞, .., -2, -1.0...1, -1, -0.0...1, 0, 0.0...1, ..., 1, 1.0...1, 

..., ∞] This solution will accept only integers and nothing but integers.This works with any number, including a fraction:Why not divide the input by a number? This way works with everything. Negatives, floats, and negative floats. Also Blank spaces and zero.Result:I know this is pretty late but its to help anyone else that had to spend 6 hours trying to figure this out. (thats what I did):This works flawlessly: (checks if any letter is in the input/checks if input is either integer or float)Here is a simple function that checks input for INT and RANGE.  Here, returns 'True' if input is integer between 1-100, 'False' otherwiseI've been using a different approach I thought I'd share. Start with creating a valid range:Now ask for a number and if not in list continue asking:Lastly convert to int (which will work because list only contains integers as strings:Here is the simplest solution:This makes a loop to check whether input is an integer or not, result would look like below:If you wanted to evaluate floats, and you wanted to accept NaNs as input but not other strings like 'abc', you could do the following:I also ran into problems this morning with users being able to enter non-integer responses to my specific request for an integer.This was the solution that ended up working well for me to force an answer I wanted:I would get exceptions before even reaching the try: statement when I used and the user entered "J" or any other non-integer character.  It worked out best to take it as raw input, check to see if that raw input could be converted to an integer, and then convert it afterward.try this! it worked for me even if I input negative numbers.Based on inspiration from answer. I defined a function as below. Looks like its working fine. Please let me know if you find any issue

Sorting dictionary keys in python [duplicate]

George

[Sorting dictionary keys in python [duplicate]](https://stackoverflow.com/questions/575819/sorting-dictionary-keys-in-python)

I have a dict where each key references an int value. What's the best way to sort the keys into a list depending on the values?

2009-02-22 21:19:51Z

I have a dict where each key references an int value. What's the best way to sort the keys into a list depending on the values?I like this one:

How to make a class property? [duplicate]

deft_code

[How to make a class property? [duplicate]](https://stackoverflow.com/questions/5189699/how-to-make-a-class-property)

In python I can add a method to a class with the @classmethod decorator.  Is there a similar decorator to add a property to a class?  I can better show what I'm talking about.Is the syntax I've used above possible or would it require something more?The reason I want class properties is so I can lazy load class attributes, which seems reasonable enough.

2011-03-04 04:34:01Z

In python I can add a method to a class with the @classmethod decorator.  Is there a similar decorator to add a property to a class?  I can better show what I'm talking about.Is the syntax I've used above possible or would it require something more?The reason I want class properties is so I can lazy load class attributes, which seems reasonable enough.Here's how I would do this:The setter didn't work at the time we call  Bar.bar, because we are calling

TypeOfBar.bar.__set__, which is not Bar.bar.__set__.Adding a metaclass definition solves this:Now all will be fine.If you define classproperty as follows, then your example works exactly as you requested.The caveat is that you can't use this for writable properties.  While e.I = 20 will raise an AttributeError, Example.I = 20 will overwrite the property object itself.I think you may be able to do this with the metaclass.  Since the metaclass can be like a class for the class (if that makes sense).  I know you can assign a __call__() method to the metaclass to override calling the class, MyClass().  I wonder if using the property decorator on the metaclass operates similarly.  (I haven't tried this before, but now I'm curious...)[update:]Wow, it does work:Note: This is in Python 2.7.  Python 3+ uses a different technique to declare a metaclass.  Use: class MyClass(metaclass=MetaClass):, remove __metaclass__, and the rest is the same.[answer written based on python 3.4; the metaclass syntax differs in 2 but I think the technique will still work]You can do this with a metaclass...mostly. Dappawit's almost works, but I think it has a flaw:This gets you a classproperty on Foo, but there's a problem...What the hell is going on here? Why can't I reach the class property from an instance?I was beating my head on this for quite a while before finding what I believe is the answer. Python @properties are a subset of descriptors, and, from the descriptor documentation (emphasis mine):So the method resolution order doesn't include our class properties (or anything else defined in the metaclass). It is possible to make a subclass of the built-in property decorator that behaves differently, but (citation needed) I've gotten the impression googling that the developers had a good reason (which I do not understand) for doing it that way.That doesn't mean we're out of luck; we can access the properties on the class itself just fine...and we can get the class from type(self) within the instance, which we can use to make @property dispatchers:Now Foo().thingy works as intended for both the class and the instances! It will also continue to do the right thing if a derived class replaces its underlying _thingy (which is the use case that got me on this hunt originally).This isn't 100% satisfying to me -- having to do setup in both the metaclass and object class feels like it violates the DRY principle. But the latter is just a one-line dispatcher; I'm mostly okay with it existing, and you could probably compact it down to a lambda or something if you really wanted.If you use Django, it has a built in @classproperty decorator.As far as I can tell, there is no way to write a setter for a class property without creating a new metaclass.I have found that the following method works. Define a metaclass with all of the class properties and setters you want. IE, I wanted a class with a title property with a setter. Here's what I wrote:Now make the actual class you want as normal, except have it use the metaclass you created above.It's a bit weird to define this metaclass as we did above if we'll only ever use it on the single class. In that case, if you're using the Python 2 style, you can actually define the metaclass inside the class body. That way it's not defined in the module scope.If you only need lazy loading, then you could just have a class initialisation method.But the metaclass approach seems cleaner, and with more predictable behavior.Perhaps what you're looking for is the Singleton design pattern. There's a nice SO QA about implementing shared state in Python.I happened to come up with a solution very similar to @Andrew, only DRYThis has the best of all answers:The "metaproperty" is added to the class, so that it will still be a property of the instanceIn my case, I actually customized _thingy to be different for every child, without defining it in each class (and without a default value) by:

virtualenv --no-site-packages and pip still finding global packages?

ianw

[virtualenv --no-site-packages and pip still finding global packages?](https://stackoverflow.com/questions/1382925/virtualenv-no-site-packages-and-pip-still-finding-global-packages)

I was under the impression that virtualenv --no-site-packages would create a completely separate and isolated Python environment, but it doesn't seem to.For example, I have python-django installed globally, but wish to create a virtualenv with a different Django version.From what I can tell, the pip -E foo install above is supposed to re-install a new version of Django.  Also, if I tell pip to freeze the environment, I get a whole lot of packages.  I would expect that for a fresh environment with --no-site-packages this would be blank?Am I misunderstanding how --no-site-packages is supposed to work?  

2009-09-05 09:59:17Z

I was under the impression that virtualenv --no-site-packages would create a completely separate and isolated Python environment, but it doesn't seem to.For example, I have python-django installed globally, but wish to create a virtualenv with a different Django version.From what I can tell, the pip -E foo install above is supposed to re-install a new version of Django.  Also, if I tell pip to freeze the environment, I get a whole lot of packages.  I would expect that for a fresh environment with --no-site-packages this would be blank?Am I misunderstanding how --no-site-packages is supposed to work?  I had  a problem like this, until I realized that (long before I had discovered virtualenv), I had gone adding directories to the PYTHONPATH in my .bashrc file. As it had been over a year beforehand, I didn't think of that straight away.  Eventually I found that, for whatever reason, pip -E was not working.  However, if I actually activate the virtualenv, and use easy_install provided by virtualenv to install pip, then use pip directly from within, it seems to work as expected and only show the packages in the virtualenvYou have to make sure you are running the pip binary in the virtual environment you created, not the global one.See a test:We create the virtualenv with the --no-site-packages option:We check the output of freeze from the newly created pip:But if we do use the global pip, this is what we get:That is, all the packages that pip has installed in the whole system. By checking which pip we get (at least in my case) something like /usr/local/bin/pip, meaning that when we do pip freeze it is calling this binary instead of mytest/bin/pip.I know this is a very old question but for those arriving here looking for a solution:Don't forget to activate the virtualenv (source bin/activate) before running pip freeze. Otherwise you'll get a list of all global packages.--no-site-packages should, as the name suggests, remove the standard site-packages directory from sys.path. Anything else that lives in the standard Python path will remain there.Temporarily clear the PYTHONPATH with:Then create and activate the virtual environment:Only then:A similar problem can occur on Windows if you call scripts directly as script.py which then uses the Windows default opener and opens Python outside the virtual environment. Calling it with python script.py will use Python with the virtual environment.This also seems to happen when you move the virtualenv directory to another directory (on linux), or rename a parent directory.One of the possible reasons why virtualenv pip won't work is if any of the parent folders had space in its name /Documents/project name/app

renaming it to /Documents/projectName/app solves the problem.I came accross the same problem where pip in venv still works as global pip.

After searching many pages, i figure it out this way.

1. Create a new venv by virtualenv with option "--no-site-packages"please note that although the "--no-site-packages" option was default true since 1.7.0 in the doc file of virtualenv, but i found it not working unless you set it on manually. In order to get a pure venv, i strongly suggest turning this option on

2. Activate the new env you createdWish this answer helps you!Here's the list of all the pip install options - I didn't find any '-E' option, may be older version had it. Below I am sharing a plain english usage and working of virtualenv for the upcoming SO users.Every thing seems fine, accept activating the virtualenv (foo). All it does is allow us to have multiple (and varying) python environment i.e. various Python versions, or various Django versions, or any other Python package - in case we have a previous version in production and want to test the latest Django release with our application. In short creating and using (activating) virtual environment (virtualenv) makes it possible to run or test our application or simple python scripts with different Python interpreter i.e. Python 2.7 and 3.3 - can be a fresh installation (using --no-site-packages option) or all the packages from existing/last setup (using --system-site-packages option). To use it we have to activate it:   $ pip install django will install it into the global site-packages, and similarly getting the pip freeze will give names of the global site-packages.while inside the venv dir (foo) executing $ source /bin/activate will activate venv i.e. now anything installed with pip will only be installed in the virtual env, and only now the pip freeze will not give the list of global site-packages python packages. Once activated:(foo) before the $ sign indicates we are using a virtual python environment i.e. any thing with pip - install, freeze, uninstall will be limited to this venv, and no effect on global/default Python installation/packages.I was having this same problem. The issue for me (on Ubuntu) was that my path name contained $. When I created a virtualenv outside of the $ dir, it worked fine.Weird.

Filtering Pandas DataFrames on dates

AMM

[Filtering Pandas DataFrames on dates](https://stackoverflow.com/questions/22898824/filtering-pandas-dataframes-on-dates)

I have a Pandas DataFrame with a 'date' column. Now I need to filter out all rows in the DataFrame that have dates outside of the next two months. Essentially, I only need to retain the rows that are within the next two months. What is the best way to achieve this?

2014-04-06 19:24:42Z

I have a Pandas DataFrame with a 'date' column. Now I need to filter out all rows in the DataFrame that have dates outside of the next two months. Essentially, I only need to retain the rows that are within the next two months. What is the best way to achieve this?If date column is the index, then use .loc for label based indexing or .iloc for positional indexing.For example:See details here http://pandas.pydata.org/pandas-docs/stable/dsintro.html#indexing-selectionIf the column is not the index you have two choices:See here for the general explanationNote: .ix is deprecated.Previous answer is not correct in my experience, you can't pass it a simple string, needs to be a datetime object. So:And if your dates are standardized by importing datetime package, you can simply use:For standarding your date string using datetime package, you can use this function:If your datetime column have the Pandas datetime type (e.g. datetime64[ns]), for proper filtering you need the pd.Timestamp object, for example:If the dates are in the index then simply:You can use pd.Timestamp to perform a query and a local referencewith the outputHow about using pyjanitorIt has cool features.After pip install pyjanitor

It is more efficient to use if-return-return or if-else-return?

Jorge Leitao

[It is more efficient to use if-return-return or if-else-return?](https://stackoverflow.com/questions/9191388/it-is-more-efficient-to-use-if-return-return-or-if-else-return)

Suppose I have an if statement with a return. From the efficiency perspective, should I useorShould I prefer one or another when using a compiled language (C) or a scripted one (Python)?

2012-02-08 10:20:53Z

Suppose I have an if statement with a return. From the efficiency perspective, should I useorShould I prefer one or another when using a compiled language (C) or a scripted one (Python)?Since the return statement terminates the execution of the current function, the two forms are equivalent (although the second one is arguably more readable than the first).The efficiency of both forms is comparable, the underlying machine code has to perform a jump if the if condition is false anyway.Note that Python supports a syntax that allows you to use only one return statement in your case:From Chromium's style guide:Don't use else after return:Regarding coding style:Most coding standards no matter language ban multiple return statements from a single function as bad practice. (Although personally I would say there are several cases where multiple return statements do make sense: text/data protocol parsers, functions with extensive error handling etc)The consensus from all those industry coding standards is that the expression should be written as:Regarding efficiency:The above example and the two examples in the question are all completely equivalent in terms of efficiency. The machine code in all these cases have to compare A > B, then branch to either the A+1 or the A-1 calculation, then store the result of that in a CPU register or on the stack.EDIT :Sources:With any sensible compiler, you should observe no difference; they should be compiled to identical machine code as they're equivalent.Version A is simpler and that's why I would use it.And if you turn on all compiler warnings in Java you will get a warning on the second Version because it is unnecesarry and turns up code complexity.I know the question is tagged python, but it mentions dynamic languages so thought I should mention that in ruby the if statement actually has a return type so you can do something likeOr because it also has implicit return simply which gets around the style issue of not having multiple returns quite nicely.I personally avoid else blocks when possible. See the Anti-if CampaignAlso, they don't charge 'extra' for the line, you know :p "Simple is better than complex" & "Readability is king"This is a question of style (or preference) since the interpreter does not care. Personally I would try not to make the final statement of a function which returns a value at an indent level other than the function base.  The else in example 1 obscures, if only slightly, where the end of the function is.By preference I use:As it obeys both the good convention of having a single return statement as the last statement in the function (as already mentioned) and the good functional programming paradigm of avoiding imperative style intermediate results. For more complex functions I prefer to break the function into multiple sub-functions to avoid premature returns if possible. Otherwise I revert to using an imperative style variable called rval.  I try not to use multiple return statements unless the function is trivial or the return statement before the end is as a result of an error.  Returning prematurely highlights the fact that you cannot go on. For complex functions that are designed to branch off into multiple subfunctions I try to code them as case statements (driven by a dict for instance). Some posters have mentioned speed of operation. Speed of Run-time is secondary for me since if you need speed of execution  Python is not the best language to use. I use Python as its the efficiency of coding (i.e. writing error free code) that matters to me. 

What is the EAFP principle in Python?

Unpaid Oracles

[What is the EAFP principle in Python?](https://stackoverflow.com/questions/11360858/what-is-the-eafp-principle-in-python)

What is meant by "using the EAFP principle" in Python? Could you provide any examples?

2012-07-06 10:55:04Z

What is meant by "using the EAFP principle" in Python? Could you provide any examples?From the glossary:An example would be an attempt to access a dictionary key.EAFP:LBYL:The LBYL version has to search the key inside the dictionary twice, and might also be considered slightly less readable.I'll try to explain it with another example.Here we're trying to access the file and print the contents in console. We might want to check if we can access the file and if we can, we'll open it and print the contents. If we can't access the file we'll hit the else part. The reason that this is a race condition is because we first make an access-check. By the time we reach with open(my_file) as f: maybe we can't access it anymore due to some permission issues (for example another process gains an exclusive file lock). This code will likely throw an error and we won't be able to catch that error because we thought that we could access the file.In this example, we're just trying to open the file and if we can't open it, it'll throw an IOError. If we can, we'll open the file and print the contents. So instead of asking something we're trying to do it. If it works, great! If it doesn't we catch the error and handle it.I call it "optimistic programming".  The idea is that most times people will do the right thing, and errors should be few.  So code first for the "right thing" to happen, and then catch the errors if they don't.  My feeling is that if a user is going to be making mistakes, they should be the one to suffer the time consequences.  People who use the tool the right way are sped through.

What is the most efficient way to store a list in the Django models?

grieve

[What is the most efficient way to store a list in the Django models?](https://stackoverflow.com/questions/1110153/what-is-the-most-efficient-way-to-store-a-list-in-the-django-models)

Currently I have a lot of python objects in my code similar to the following:Now I want to turn this into a Django model, where self.myName is a string field, and self.myFriends is a list of strings.Since the list is such a common data structure in python, I sort of expected there to be a Django model field for it. I know I can use a ManyToMany or OneToMany relationship, but I was hoping to avoid that extra indirection in the code.Edit:I added this related question, which people may find useful.

2009-07-10 15:16:19Z

Currently I have a lot of python objects in my code similar to the following:Now I want to turn this into a Django model, where self.myName is a string field, and self.myFriends is a list of strings.Since the list is such a common data structure in python, I sort of expected there to be a Django model field for it. I know I can use a ManyToMany or OneToMany relationship, but I was hoping to avoid that extra indirection in the code.Edit:I added this related question, which people may find useful.Would this relationship not be better expressed as a one-to-many foreign key relationship to a Friends table?  I understand that myFriends are just strings but I would think that a better design would be to create a Friend model and have MyClass contain a foreign key realtionship to the resulting table.With that firmly in mind, let's do this! Once your apps hit a certain point, denormalizing data is very common. Done correctly, it can save numerous expensive database lookups at the cost of a little more housekeeping.To return a list of friend names we'll need to create a custom Django Field class that will return a list when accessed.David Cramer posted a guide to creating a SeperatedValueField on his blog. Here is the code:The logic of this code deals with serializing and deserializing values from the database to Python and vice versa. Now you can easily import and use our custom field in the model class:A simple way to store a list in Django is to just convert it into a JSON string, and then save that as Text in the model. You can then retrieve the list by converting the (JSON) string back into a python list. Here's how:The "list" would be stored in your Django model like so:In your view/controller code:Storing the list in the database:Retrieving the list from the database:Conceptually, here's what's going on:If you are using Django >= 1.9 with Postgres you can make use of ArrayField advantages It is also possible to nest array fields:As @thane-brimhall mentioned it is also possible to query elements directly. Documentation referenceAs this is an old question, and Django techniques must have changed significantly since, this answer reflects Django version 1.4, and is most likely applicable for v 1.5.Django by default uses relational databases; you should make use of 'em. Map friendships to database relations (foreign key constraints) with the use of ManyToManyField. Doing so allows you to use RelatedManagers for friendlists, which use smart querysets. You can use all available methods such as filter or values_list.Using ManyToManyField relations and properties:You can access a user's friend list this way:Note however that these relations are symmetrical: if Joseph is a friend of Bob, then Bob is a friend of Joseph.Remember that this eventually has to end up in a relational database. So using relations really is the common way to solve this problem. If you absolutely insist on storing a list in the object itself, you could make it for example comma-separated, and store it in a string, and then provide accessor functions that split the string into a list. With that, you will be limited to a maximum number of strings, and you will lose efficient queries.In case you're using postgres, you can use something like this:if you need more details you can read in the link below:

https://docs.djangoproject.com/pt-br/1.9/ref/contrib/postgres/fields/You can store virtually any object using a Django Pickle Field, ala this snippet:http://www.djangosnippets.org/snippets/513/Storing a list of strings in Django model:and you can call it like this:Using one-to-many relation (FK from Friend to parent class) will make your app more scalable (as you can trivially extend the Friend object with additional attributes beyond the simple name). And thus this is the best way My solution, may be it helps someone:

Pandas dataframe fillna() only some columns in place

Sait

[Pandas dataframe fillna() only some columns in place](https://stackoverflow.com/questions/38134012/pandas-dataframe-fillna-only-some-columns-in-place)

I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.When I do:The output:It replaces every None with 0's. What I want to do is, only replace Nones in columns a and b, but not c.What is the best way of doing this?

2016-06-30 22:06:16Z

I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.When I do:The output:It replaces every None with 0's. What I want to do is, only replace Nones in columns a and b, but not c.What is the best way of doing this?You can select your desired columns and do it by assignment:The resulting output is as expected:You can using dict , fillna with different value for different columnAfter assign it back You can avoid making a copy of the object using Wen's solution and inplace=True:Which yields:Here's how you can do it all in one line:Breakdown: df[['a', 'b']] selects the columns you want to fill NaN values for, value=0 tells it to fill NaNs with zero, and inplace=True will make the changes permanent, without having to make a copy of the object.using the top answer produces a warning about making changes to a copy of a df slice. Assuming that you have other columns, a better way to do this is to pass a dictionary: 

df.fillna({'A': 'NA', 'B': 'NA'}, inplace=True)Or something like:and if there is more:Sometimesdf[['col1','col2']] = df[['col1','col2']].filna ... this syntax wont work.Use the following instead

Python script to copy text to clipboard [duplicate]

iamsiva11

[Python script to copy text to clipboard [duplicate]](https://stackoverflow.com/questions/11063458/python-script-to-copy-text-to-clipboard)

I just need a python script that copies text to the clipboard.After the script gets executed i need the output of the text to be pasted to another source.

Is it possible to write a python script that does this job?

2012-06-16 12:32:47Z

I just need a python script that copies text to the clipboard.After the script gets executed i need the output of the text to be pasted to another source.

Is it possible to write a python script that does this job?See Pyperclip.  Example (taken from Pyperclip site):Also, see Xerox.  But it appears to have more dependencies.On macOS, use subprocess.run to pipe your text to pbcopy:It will copy "hello world" to the clipboard.Use Tkinter: https://stackoverflow.com/a/4203897/2804197(Original author: https://stackoverflow.com/users/449571/atomizer)This is the only way that worked for me using Python 3.5.2 plus it's the easiest to implement w/ using the standard PyData suiteShout out to https://stackoverflow.com/users/4502363/gadi-oron for the answer (I copied it completely) from How do I copy a string to the clipboard on Windows using Python?I wrote a little wrapper for it that I put in my ipython profile <3 Pyperclip seems to be up to the task.To use native Python directories, use:on Mac, instead:Then use:to call the function.GTK3:PyQt5:One more answer to improve on:

https://stackoverflow.com/a/4203897/2804197

and https://stackoverflow.com/a/25476462/1338797 (Tkinter).Tkinter is nice, because it's either included with Python (Windows) or easy to install (Linux), and thus requires little dependencies for the end user.Here I have a "full-blown" example, which copies the arguments or the standard input, to clipboard, and - when not on Windows - waits for the user to close the application:This showcases:This is an altered version of @Martin Thoma's answer for GTK3. I found that the original solution resulted in the process never ending and my terminal hung when I called the script. Changing the script to the following resolved the issue for me.You will probably want to change what clipboardText gets assigned to, in this script it is assigned to the parameter that the script is called with.On a fresh ubuntu 16.04 installation, I found that I had to install the python-gobject package for it to work without a module import error.I try this clipboard 0.0.4 and it works well.https://pypi.python.org/pypi/clipboard/0.0.4

In Python, how can you load YAML mappings as OrderedDicts?

Eric Naeseth

[In Python, how can you load YAML mappings as OrderedDicts?](https://stackoverflow.com/questions/5121931/in-python-how-can-you-load-yaml-mappings-as-ordereddicts)

I'd like to get PyYAML's loader to load mappings (and ordered mappings) into the Python 2.7+ OrderedDict type, instead of the vanilla dict and the list of pairs it currently uses.What's the best way to do that?

2011-02-25 19:52:12Z

I'd like to get PyYAML's loader to load mappings (and ordered mappings) into the Python 2.7+ OrderedDict type, instead of the vanilla dict and the list of pairs it currently uses.What's the best way to do that?Update: In python 3.6+ you probably don't need OrderedDict at all due to the new dict implementation that has been in use in pypy for some time (although considered CPython implementation detail for now).Update: In python 3.7+, the insertion-order preservation nature of dict objects has been declared to be an official part of the Python language spec, see What's New In Python 3.7.I like @James' solution for its simplicity. However, it changes the default global yaml.Loader class, which can lead to troublesome side effects. Especially, when writing library code this is a bad idea. Also, it doesn't directly work with yaml.safe_load().Fortunately, the solution can be improved without much effort:For serialization, I don't know an obvious generalization, but at least this shouldn't have any side effects:The yaml module allow you to specify custom 'representers' to convert Python objects to text and 'constructors' to reverse the process.oyaml is a drop-in replacement for PyYAML which preserves dict ordering. Both Python 2 and Python 3 are supported. Just pip install oyaml, and import as shown below:You'll no longer be annoyed by screwed-up mappings when dumping/loading.Note: I'm the author of oyaml.ruamel.yaml is a drop in replacement for PyYAML (disclaimer: I am the author of that package). Preserving the order of the mappings was one of the things added in the first version (0.1) back in 2015. Not only does it preserve the order of your dictionaries, it will also preserve comments, anchor names, tags and does support the YAML 1.2 specification (released 2009)The specification says that the ordering is not guaranteed, but of course there is ordering in the YAML file and the appropriate parser can just hold on to that and transparently generate an object that keeps the ordering. You just need to choose the right parser, loader and dumper¹:will give you:data is of type CommentedMap which functions like a dict, but has extra information that is kept around until being dumped (including the preserved comment!)Note: there is a library, based on the following answer, which implements also the CLoader and CDumpers: Phynix/yamlloaderI doubt very much that this is the best way to do it, but this is the way I came up with, and it does work. Also available as a gist.Update: the library was deprecated in favor of the yamlloader (which is based on the yamlordereddictloader)I've just found a Python library (https://pypi.python.org/pypi/yamlordereddictloader/0.1.1) which was created based on answers to this question and is quite simple to use:On my For PyYaml installation for Python 2.7 I updated __init__.py, constructor.py, and loader.py. Now supports object_pairs_hook option for load commands. Diff of changes I made is below.here's a simple solution that also checks for duplicated top level keys in your map.

Sound alarm when code finishes

mtigger

[Sound alarm when code finishes](https://stackoverflow.com/questions/16573051/sound-alarm-when-code-finishes)

I am in a situation where my code takes extremely long to run and I don't want to be staring at it all the time but want to know when it is done.How can I make the (Python) code sort of sound an "alarm" when it is done? I was contemplating making it play a .wav file when it reaches the end of the code... Is this even a feasible idea? 

If so, how could I do it?

2013-05-15 19:04:57Z

I am in a situation where my code takes extremely long to run and I don't want to be staring at it all the time but want to know when it is done.How can I make the (Python) code sort of sound an "alarm" when it is done? I was contemplating making it play a .wav file when it reaches the end of the code... Is this even a feasible idea? 

If so, how could I do it?Where freq is the frequency in Hz and the duration is in milliseconds.In order to use this example, you must install sox.On Debian / Ubuntu / Linux Mint, run this in your terminal:On Mac, run this in your terminal (using macports):You need to install the speech-dispatcher package in Ubuntu (or the corresponding package on other distributions):plays the bell soundThis one seems to work on both Windows and Linux* (from this question):In Windows, can put at the end:To work on Linux, you may need to do the following (from QO's comment):ubuntu speech dispatcher can be used:I'm assuming you want the standard system bell, and don't want to concern yourself with frequencies and durations etc., you just want the standard windows bell.Kuchi's answer didn't work for me on OS X Yosemite (10.10.1). I did find the afplay command (here), which you can just call from Python. This works regardless of whether the Terminal audible bell is enabled and without a third-party library.See: Python Sound ("Bell")

This helped me when i wanted to do the same.

All credits go to gbcHave you tried :That works for me here on Mac OS 10.5Actually, I think your original attempt works also with a little modification:(You just need the single quotes around the character sequence).Why use python at all?  You might forget to remove it and check it into a repository.  Just run your python command with && and another command to run to do the alerting.or drop a function into your .bashrc. I use apython here but you could override 'python'It can be done by code as follows:

Why is TensorFlow 2 much slower than TensorFlow 1?

OverLordGoldDragon

[Why is TensorFlow 2 much slower than TensorFlow 1?](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1)

It's been cited by many users as the reason for switching to Pytorch, but I've yet to find a justification / explanation for sacrificing the most important practical quality, speed, for eager execution.Below is code benchmarking performance, TF1 vs. TF2 - with TF1 running anywhere from 47% to 276% faster. My question is: what is it, at the graph or hardware level, that yields such a significant slowdown?Looking for a detailed answer - am already familiar with broad concepts. Relevant GitSpecs: CUDA 10.0.130, cuDNN 7.4.2, Python 3.7.4, Windows 10, GTX 1070Benchmark results:UPDATE: Disabling Eager Execution per below code does not help. The behavior, however, is inconsistent: sometimes running in graph mode helps considerably, other times it runs slower relative to Eager.As TF devs don't appear around anywhere, I'll be investigating this matter myself - can follow progress in the linked Github issue.UPDATE 2: tons of experimental results to share, along explanations; should be done today.Benchmark code:Functions used:

2019-10-17 22:28:33Z

It's been cited by many users as the reason for switching to Pytorch, but I've yet to find a justification / explanation for sacrificing the most important practical quality, speed, for eager execution.Below is code benchmarking performance, TF1 vs. TF2 - with TF1 running anywhere from 47% to 276% faster. My question is: what is it, at the graph or hardware level, that yields such a significant slowdown?Looking for a detailed answer - am already familiar with broad concepts. Relevant GitSpecs: CUDA 10.0.130, cuDNN 7.4.2, Python 3.7.4, Windows 10, GTX 1070Benchmark results:UPDATE: Disabling Eager Execution per below code does not help. The behavior, however, is inconsistent: sometimes running in graph mode helps considerably, other times it runs slower relative to Eager.As TF devs don't appear around anywhere, I'll be investigating this matter myself - can follow progress in the linked Github issue.UPDATE 2: tons of experimental results to share, along explanations; should be done today.Benchmark code:Functions used:UPDATE 2/18/2020: I've benched 2.1 and 2.1-nightly; the results are mixed. All but one configs (model & data size) are as fast as or much faster than the best of TF2 & TF1. The one that's slower, and slower dramatically, is Large-Large - esp. in Graph execution (1.6x to 2.5x slower).Furthermore, there are extreme reproducibility differences between Graph and Eager for a large model I tested - one not explainable via randomness/compute-parallelism. I can't currently present reproducible code for these claims per time constraints, so instead I strongly recommend testing this for your own models.Haven't opened a Git issue on these yet, but I did comment on the original - no response yet. I'll update the answer(s) once progress is made.VERDICT: it isn't, IF you know what you're doing. But if you don't, it could cost you, lots - by a few GPU upgrades on average, and by multiple GPUs worst-case.THIS ANSWER: aims to provide a high-level description of the issue, as well as guidelines for how to decide on the training configuration specific to your needs. For a detailed, low-level description, which includes all benchmarking results + code used, see my other answer.I'll be updating my answer(s) w/ more info if I learn any - can bookmark / "star" this question for reference.ISSUE SUMMARY: as confirmed by a TensorFlow developer, Q. Scott Zhu, TF2 focused development on Eager execution & tight integration w/ Keras, which involved sweeping changes in TF source - including at graph-level. Benefits: greatly expanded processing, distribution, debug, and deployment capabilities. The cost of some of these, however, is speed.The matter, however, is fairly more complex. It isn't just TF1 vs. TF2 - factors yielding significant differences in train speed include:Unfortunately, almost none of the above are independent of the other, and each can at least double execution time relative to another. Fortunately, you can determine what'll work best systematically, and with a few shortcuts - as I'll be showing.WHAT SHOULD I DO? Currently, the only way is - experiment for your specific model, data, and hardware. No single configuration will always work best - but there are do's and don't's to simplify your search:>> DO:>> DON'T:Refer to code at bottom of my other answer for an example benchmarking setup. The list above is based mainly on the "BENCHMARKS" tables in the other answer.LIMITATIONS of the above DO's & DON'T's:Why did TF2 sacrifice the most practical quality, speed, for eager execution? It hasn't, clearly - graph is still available. But if the question is "why eager at all":HOW TO ENABLE/DISABLE EAGER?ADDITIONAL INFO:REQUESTS TO TENSORFLOW DEVS: ACKNOWLEDGEMENTS: Thanks to UPDATES:THIS ANSWER: aims to provide a detailed, graph/hardware-level description of the issue - including TF2 vs. TF1 train loops, input data processors, and Eager vs. Graph mode executions. For an issue summary & resolution guidelines, see my other answer.PERFORMANCE VERDICT: sometimes one is faster, sometimes the other, depending on configuration. As far as TF2 vs TF1 goes, they're about on par on average, but significant config-based differences do exist, and TF1 trumps TF2 more often than vice versa. See "BENCHMARKING" below.EAGER VS. GRAPH: the meat of this entire answer for some: TF2's eager is slower than TF1's, according to my testing. Details further down.The fundamental difference between the two is: Graph sets up a computational network proactively, and executes when 'told to' - whereas Eager executes everything upon creation. But the story only begins here:TF2 vs. TF1: quoting relevant portions of a TF dev's, Q. Scott Zhu's, response - w/ bit of my emphasis & rewording:With the last sentence of last paragraph above, and last clause of below paragraph:I disagree - per my profiling results, which show Eager's input data processing to be substantially slower than Graph's. Also, unsure about tf.data.Dataset in particular, but Eager does repeatedly call multiple of the same data conversion methods - see profiler.Lastly, dev's linked commit: Significant number of changes to support the Keras v2 loops.Train Loops: depending on (1) Eager vs. Graph; (2) input data format, training in will proceed with a distinct train loop - in TF2, _select_training_loop(), training.py, one of:Each handles resource allocation differently, and bears consequences on performance & capability. Train Loops: fit vs train_on_batch, keras vs. tf.keras: each of the four uses different train loops, though perhaps not in every possible combination. keras' fit, for example, uses a form of fit_loop, e.g. training_arrays.fit_loop(), and its train_on_batch may use K.function(). tf.keras has a more sophisticated hierarchy described in part in previous section.Train Loops: documentation -- relevant source docstring on some of the different execution methods:Input data processors: similar to above, the processor is selected case-by-case, depending on internal flags set according to runtime configurations (execution mode, data format, distribution strategy). The simplest case's with Eager, which works directly w/ Numpy arrays. For some specific examples, see this answer.MODEL SIZE, DATA SIZE: BENCHMARKS: the grinded meat. -- Word Document -- Excel SpreadsheetTerminology:PROFILER:PROFILER - Explanation: Spyder 3.3.6 IDE profiler.TESTING ENVIRONMENT:METHODOLOGY:LIMITATIONS: a "complete" answer would explain every possible train loop & iterator, but that's surely beyond my time ability, nonexistent paycheck, or general necessity. The results are only as good as the methodology - interpret with an open mind.CODE:

How to remove specific element from an array using python

locoboy

[How to remove specific element from an array using python](https://stackoverflow.com/questions/7118276/how-to-remove-specific-element-from-an-array-using-python)

I want to write something that removes a specific element from an array. I know that I have to for loop through the array to find the element that matches the content. Let's say that I have an array of emails and I want to get rid of the element that matches some email string.I'd actually like to use the for loop structure because I need to use the same index for other arrays as well. Here is the code that I have:

2011-08-19 07:27:02Z

I want to write something that removes a specific element from an array. I know that I have to for loop through the array to find the element that matches the content. Let's say that I have an array of emails and I want to get rid of the element that matches some email string.I'd actually like to use the for loop structure because I need to use the same index for other arrays as well. Here is the code that I have:You don't need to iterate the array. Just:This will remove the first occurence that matches the string.EDIT: After your edit, you still don't need to iterate over. Just do:Using filter() and lambda would provide a neat and terse method of removing unwanted values:This does not modify emails. It creates the new list newEmails containing only elements for which the anonymous function returned True.The sane way to do this is to use zip() and a List Comprehension / Generator Expression:Also, if your'e not using array.array() or numpy.array(), then most likely you are using [] or list(), which give you Lists, not Arrays. Not the same thing.Your for loop is not right, if you need the index in the for loop use:In your case, Bogdan solution is ok, but your data structure choice is not so good. Having to maintain these two lists with data from one related to data from the other at same index is clumsy.A list of tupple (email, otherdata) may be better, or a dict with email as key. There is an alternative solution to this problem which also deals with duplicate matches.We start with 2 lists of equal length: emails, otherarray. The objective is to remove items from both lists for each index i where emails[i] == 'something@something.com'.This can be achieved using a list comprehension and then splitting via zip:

What does '# noqa' mean in Python comments?

Ishpreet

[What does '# noqa' mean in Python comments?](https://stackoverflow.com/questions/45346575/what-does-noqa-mean-in-python-comments)

While searching through a Python project, I found a few lines commented with # noqa.What does noqa mean in Python? Is it specific to Python only?

2017-07-27 09:25:10Z

While searching through a Python project, I found a few lines commented with # noqa.What does noqa mean in Python? Is it specific to Python only?Adding # noqa to a line indicates that the linter (a program that automatically checks code quality) should not check this line. Any warnings that code may have generated will be ignored.That line may have something that "looks bad" to the linter, but the developer understands and intends it to be there for some reason.For more information, see the Flake8 documentation for Selecting and Ignoring Violations.It's generally referred in Python Programming to ignore the PEP8 warnings. In simple words, lines having #noqa at the end will be ignored by the linter programs and they won't raise any warnings.You know what? Even Guido van Rossum (the creator of Python) asked this question before :DA bit Etymology of # noqa:Some basic usages of # noqa (with flake8):

Windows path in Python

Gareth

[Windows path in Python](https://stackoverflow.com/questions/2953834/windows-path-in-python)

What is the best way to represent a Windows directory, for example "C:\meshes\as"? I have been trying to modify a script but it never works because I can't seem to get the directory right, I assume because of the '\' acting as escape character?

2010-06-01 22:29:06Z

What is the best way to represent a Windows directory, for example "C:\meshes\as"? I have been trying to modify a script but it never works because I can't seem to get the directory right, I assume because of the '\' acting as escape character?you can use always:this works both in linux and windows.

Other posibility isif you have problems with some names you can also try raw string literals:however best practice is to use the os.path module functions that always select the correct configuration for your OS:From python 3.4 you can also use the pathlib module. This is equivelent to the above:orUse the os.path module.Or use raw stringsI would also recommend no spaces in the path or file names. And you could use double backslashes in your strings. Yes, \ in Python string literals denotes the start of an escape sequence. In your path you have a valid two-character escape sequence \a, which is collapsed into one character that is ASCII Bell:Other common escape sequences include \t (tab), \n (line feed), \r (carriage return):As you can see, in all these examples the backslash and the next character in the literal were grouped together to form a single character in the final string. The full list of Python's escape sequences is here.There are a variety of ways to deal with that:In Windows, you can use / in your path just like Linux or macOS in all places as long as you use PowerShell as your command-line interface. It comes pre-installed on Windows and it supports many Linux commands like ls command.If you use Windows Command Prompt (the one that appears when you type cmd in Windows Start Menu), you need to specify paths with \ just inside it. You can use / paths in all other places (code editor, Python interactive mode, etc.).

What exactly is Python's file.flush() doing?

geek

[What exactly is Python's file.flush() doing?](https://stackoverflow.com/questions/7127075/what-exactly-is-pythons-file-flush-doing)

I found this in the Python documentation for File Objects:So my question is: what exactly is Python's flush doing? I thought that it forces to write data to the disk, but now I see that it doesn't. Why? 

2011-08-19 20:32:23Z

I found this in the Python documentation for File Objects:So my question is: what exactly is Python's flush doing? I thought that it forces to write data to the disk, but now I see that it doesn't. Why? There's typically two levels of buffering involved:The internal buffers are buffers created by the runtime/library/language that you're programming against and is meant to speed things up by avoiding system calls for every write. Instead, when you write to a file object, you write into its buffer, and whenever the buffer fills up, the data is written to the actual file using system calls.However, due to the operating system buffers, this might not mean that the data is written to disk. It may just mean that the data is copied from the buffers maintained by your runtime into the buffers maintained by the operating system.If you write something, and it ends up in the buffer (only), and the power is cut to your machine, that data is not on disk when the machine turns off.So, in order to help with that you have the flush and fsync methods, on their respective objects.The first, flush, will simply write out any data that lingers in a program buffer to the actual file. Typically this means that the data will be copied from the program buffer to the operating system buffer.Specifically what this means is that if another process has that same file open for reading, it will be able to access the data you just flushed to the file. However, it does not necessarily mean it has been "permanently" stored on disk.To do that, you need to call the os.fsync method which ensures all operating system buffers are synchronized with the storage devices they're for, in other words, that method will copy data from the operating system buffers to the disk.Typically you don't need to bother with either method, but if you're in a scenario where paranoia about what actually ends up on disk is a good thing, you should make both calls as instructed.Addendum in 2018.Note that disks with cache mechanisms is now much more common than back in 2013, so now there are even more levels of caching and buffers involved. I assume these buffers will be handled by the sync/flush calls as well, but I don't really know.Because the operating system may not do so. The flush operation forces the file data into the file cache in RAM, and from there it's the OS's job to actually send it to the disk.It flushes the internal buffer, which is supposed to cause the OS to write out the buffer to the file.[1] Python uses the OS's default buffering unless you configure it do otherwise.But sometimes the OS still chooses not to cooperate. Especially with wonderful things like write-delays in Windows/NTFS. Basically the internal buffer is flushed, but the OS buffer is still holding on to it. So you have to tell the OS to write it to disk with os.fsync() in those cases.[1] http://docs.python.org/library/stdtypes.htmlBasically, flush() cleans out your RAM buffer, its real power is that it lets you continue to write to it afterwards - but it shouldn't be thought of as the best/safest write to file feature.  It's flushing your RAM for more data to come, that is all.  If you want to ensure data gets written to file safely then use close() instead. 

Conditional import of modules in Python

Tim

[Conditional import of modules in Python](https://stackoverflow.com/questions/3496592/conditional-import-of-modules-in-python)

In my program I want to import simplejson or json based on whether the OS the user is on is Windows or Linux. I take the OS name as input from the user. Now, is it correct to do the following?

2010-08-16 19:28:29Z

In my program I want to import simplejson or json based on whether the OS the user is on is Windows or Linux. I take the OS name as input from the user. Now, is it correct to do the following?I've seen this idiom used a lot, so you don't even have to do OS sniffing:To answer the question in your title but not the particular case you provide, it's perfectly correct, tons of packages do this.  It's probably better to figure out the OS yourself instead of relying on the user; here's pySerial doing it as an example.serial/__init__.pyThis should be only used in cases where you're assuming and need a strong guarantee that certain interfaces/features will be there: e.g. a 'file' called /dev/ttyX. In your case: dealing with JSON, there's nothing that is actually OS-specific and you are only checking if the package exists or not. In that case, just try to import it, and fall-back with an except if it fails:It is not advisable to use to bind json or simplejson with OS platform. simplejson is newer and advanced version of json so we should try to import it first.Based on python version you can try below way to import json or simplejson

Python, creating objects

Mohsen M. Alrasheed

[Python, creating objects](https://stackoverflow.com/questions/15081542/python-creating-objects)

I'm trying to learn python and I now I am trying to get the hang of classes and how to manipulate them with instances.I can't seem to understand this practice problem:Create and return a student object whose name, age, and major are

the same as those given as inputI just don't get what it means by object, do they mean I should create an array inside the function that holds these values? or create a class and let this function be inside it, and assign instances? (before this question i was asked to set up a student class with name, age, and major inside)

2013-02-26 04:47:18Z

I'm trying to learn python and I now I am trying to get the hang of classes and how to manipulate them with instances.I can't seem to understand this practice problem:Create and return a student object whose name, age, and major are

the same as those given as inputI just don't get what it means by object, do they mean I should create an array inside the function that holds these values? or create a class and let this function be inside it, and assign instances? (before this question i was asked to set up a student class with name, age, and major inside)Note that even though one of the principles in Python's philosophy is "there should be one—and preferably only one—obvious way to do it", there are still multiple ways to do this. You can also use the two following snippets of code to take advantage of Python's dynamic capabilities:I prefer the former, but there are instances where the latter can be useful – one being when working with document databases like MongoDB. Create a class and give it an __init__ method:Now, you can initialize an instance of the Student class:Although I'm not sure why you need a make_student student function if it does the same thing as Student.__init__.Objects are instances of classes.  Classes are just the blueprints for objects.  So given your class definition -You can create a make_student function by explicitly assigning the attributes to a new instance of Student -But it probably makes more sense to do this in a constructor (__init__) -The constructor is called when you use Student().  It will take the arguments defined in the __init__ method.  The constructor signature would now essentially be Student(name, age, major).If you use that, then a make_student function is trivial (and superfluous) -For fun, here is an example of how to create a make_student function without defining a class.  Please do not try this at home.when you create an object using predefine class, at first you want to create a variable for storing that object. Then you can create object and store variable that you created.Actually this init method is the constructor of class.you can initialize that method using some attributes.. In that point , when you creating an object , you will have to pass some values for particular attributes..

Remove duplicate dict in list in Python

Brenden

[Remove duplicate dict in list in Python](https://stackoverflow.com/questions/9427163/remove-duplicate-dict-in-list-in-python)

I have a list of dicts, and I'd like to remove the dicts with identical key and value pairs.For this list: [{'a': 123}, {'b': 123}, {'a': 123}]I'd like to return this: [{'a': 123}, {'b': 123}]Another example:For this list: [{'a': 123, 'b': 1234}, {'a': 3222, 'b': 1234}, {'a': 123, 'b': 1234}]I'd like to return this: [{'a': 123, 'b': 1234}, {'a': 3222, 'b': 1234}]

2012-02-24 07:46:41Z

I have a list of dicts, and I'd like to remove the dicts with identical key and value pairs.For this list: [{'a': 123}, {'b': 123}, {'a': 123}]I'd like to return this: [{'a': 123}, {'b': 123}]Another example:For this list: [{'a': 123, 'b': 1234}, {'a': 3222, 'b': 1234}, {'a': 123, 'b': 1234}]I'd like to return this: [{'a': 123, 'b': 1234}, {'a': 3222, 'b': 1234}]Try this:The strategy is to convert the list of dictionaries to a list of tuples where the tuples contain the items of the dictionary. Since the tuples can be hashed, you can remove duplicates using set (using a set comprehension here, older python alternative would be set(tuple(d.items()) for d in l)) and, after that, re-create the dictionaries from tuples with dict.where:Edit: If you want to preserve ordering, the one-liner above won't work since set won't do that. However, with a few lines of code, you can also do that:Example output:Note: As pointed out by @alexis it might happen that two dictionaries with the same keys and values, don't result in the same tuple. That could happen if they go through a different adding/removing keys history. If that's the case for your problem, then consider sorting d.items() as he suggests.Another one-liner based on list comprehensions:Here since we can use dict comparison, we only keep the elements that are not in the rest of the initial list (this notion is only accessible through the index n, hence the use of enumerate).Other answers would not work if you're operating on nested dictionaries such as deserialized JSON objects. For this case you could use:Sometimes old-style loops are still useful. This code is little longer than jcollado's, but very easy to read:If using a third-party package would be okay then you could use iteration_utilities.unique_everseen:It preserves the order of the original list and ut can also handle unhashable items like dictionaries by falling back on a slower algorithm (O(n*m) where n are the elements in the original list and m the unique elements in the original list instead of O(n)). In case both keys and values are hashable you can use the key argument of that function to create hashable items for the "uniqueness-test" (so that it works in O(n)).In the case of a dictionary (which compares independent of order) you need to map it to another data-structure that compares like that, for example frozenset:Note that you shouldn't use a simple tuple approach (without sorting) because equal dictionaries don't necessarily have the same order (even in Python 3.7 where insertion order - not absolute order - is guaranteed):And even sorting the tuple might not work if the keys aren't sortable:I thought it might be useful to see how the performance of these approaches compares, so I did a small benchmark. The benchmark graphs are time vs. list-size based on a list containing no duplicates (that was chosen arbitrarily, the runtime doesn't change significantly if I add some or lots of duplicates). It's a log-log plot so the complete range is covered.The absolute times:The timings relative to the fastest approach:The second approach from thefourtheye is fastest here. The unique_everseen approach with the key function is on the second place, however it's the fastest approach that preserves order. The other approaches from jcollado and thefourtheye are almost as fast. The approach using unique_everseen without key and the solutions from Emmanuel and Scorpil are very slow for longer lists and behave much worse O(n*n) instead of O(n). stpks approach with json isn't O(n*n) but it's much slower than the similar O(n) approaches.The code to reproduce the benchmarks:For completeness here is the timing for a list containing only duplicates:The timings don't change significantly except for unique_everseen without key function, which in this case is the fastest solution. However that's just the best case (so not representative) for that function with unhashable values because it's runtime depends on the amount of unique values in the list: O(n*m) which in this case is just 1 and thus it runs in O(n). Disclaimer: I'm the author of iteration_utilities.If you want to preserve the Order, then you can doIf the order doesn't matter, then you can doIf you are using Pandas in your workflow, one option is to feed a list of dictionaries directly to the pd.DataFrame constructor. Then use drop_duplicates and to_dict methods for the required result.Not a universal answer, but if your list happens to be sorted by some key, like this:then the solution is as simple as:Result:Works with nested dictionaries and (obviously) preserves order.You can use a set, but you need to turn the dicts into a hashable type.Unique now equalsTo get dicts back:Here's a quick one-line solution with a doubly-nested list comprehension (based on @Emmanuel 's solution). This uses a single key (for example, a) in each dict as the primary key, rather than checking if the entire dict matchesIt's not what OP asked for, but it's what brought me to this thread, so I figured I'd post the solution I ended up withNot so short but easy to read:Now, list list_of_data_uniq will have unique dicts. 

How to check BLAS/LAPACK linkage in NumPy and SciPy?

Woltan

[How to check BLAS/LAPACK linkage in NumPy and SciPy?](https://stackoverflow.com/questions/9000164/how-to-check-blas-lapack-linkage-in-numpy-and-scipy)

I am builing my numpy/scipy environment based on blas and lapack more or less based on this walk through. When I am done, how can I check, that my numpy/scipy functions really do use the previously built blas/lapack functionalities?

2012-01-25 09:15:34Z

I am builing my numpy/scipy environment based on blas and lapack more or less based on this walk through. When I am done, how can I check, that my numpy/scipy functions really do use the previously built blas/lapack functionalities?The method numpy.show_config() (or numpy.__config__.show()) outputs information about linkage gathered at build time. My output looks like this. I think it means I am using the BLAS/LAPACK that ships with Mac OS. What you are searching for is this:

system infoI compiled numpy/scipy with atlas and i can check this with:Check the documentation for more commands.As it uses the dynamically loaded versions, you can just do this:where anyoftheCmodules.so could be, for example, numpy/core/_dotblas.so, which links to libblas.so.You can use the link loader dependency tool to look at the C level hook components of your build and see whether they have external dependencies on your blas and lapack of choice. I am not near a linux box right now, but on an OS X machine you can do this inside the site-packages directory which holds the installations:substitute ldd in place of otool on a gnu/Linux system and you should get the answers you need.You can display BLAS, LAPACK, MKL linkage using show_config():Which for me gives output: 

Docker「ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network」

Kurt Peek

[Docker「ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network」](https://stackoverflow.com/questions/43720339/docker-error-could-not-find-an-available-non-overlapping-ipv4-address-pool-am)

I have a directory apkmirror-scraper-compose with the following structure:I'm trying to run the following docker-compose.yml:where the Dockerfile for tor isthat for privoxy iswhere config consists of the two linesand the Dockerfile for scraper iswhere requirements.txt contains the single line requests. Finally, the program newnym.py is designed to simply test whether changing the IP address using Tor is working:The docker-compose build builds successfully, but if I try docker-compose up, I get the following error message:I tried searching for help on this error message, but couldn't find any. What is causing this error?

2017-05-01 13:59:29Z

I have a directory apkmirror-scraper-compose with the following structure:I'm trying to run the following docker-compose.yml:where the Dockerfile for tor isthat for privoxy iswhere config consists of the two linesand the Dockerfile for scraper iswhere requirements.txt contains the single line requests. Finally, the program newnym.py is designed to simply test whether changing the IP address using Tor is working:The docker-compose build builds successfully, but if I try docker-compose up, I get the following error message:I tried searching for help on this error message, but couldn't find any. What is causing this error?I've seen it suggested docker may be at its maximum of created networks. The command docker network prune can be used to remove all networks not used by at least one container.My issue ended up being, as Robert commented about: an issue with openvpn service openvpn stop 'solved' the problem.I ran into this problem because I had OpenVPN running. As soon as I killed OpenVPN, docker-compose up fired right up, and the error disappeared.Following Peter Hauge's comment, upon running docker network ls I saw (among other lines) the following:The line with NAME and DRIVER as both host seems to be what he is referring to with "networks already created on your host". So, following https://gist.github.com/bastman/5b57ddb3c11942094f8d0a97d461b430, I ran the commandNow docker-compose up works (although newnym.py produces an error).I have the same problem. I ran docker system prune -a --volumes, docker network prune, but neither helped me.I use a VPN, I turned off the VPN and, after it docker started normal and was able to create a network. After that, you can enable VPN again. I ran in this problem with OpenVPN working as well and I've found a solution where you should NOT stop/start OpenVPN server.Idea that You should specify what exactly subnet you want to use. In docker-compose.yml write:That's it. Now, default network will be used and if Your VPN did not assigned you something from 172.177.57.* subnet You're fine.As other answers mentioned, Docker's default local bridge network only supports 30 different networks (each one of them uniquely identifiable by their name). If you are not using them, then docker network prune will do the trick.However, you might be interested in establishing more than 30 containers, each with their own network. Were you interested in doing so then you would need to define an overlay network. This is a bit more tricky but extremely well documented here.I had an identical problem with the same error message but the solution with removal of unused docker networks didn't help me. I've deleted all non-default docker networks (and all images and containers as well) but it didn't help - docker still was not able to create a new network.The cause of the problem was in network interfaces that were left after OpenVpn installation. (It was installed on the host previously.) I found them by running ifconfig command:I've found that I can remove them with a couple of commands:After this the problem has disappeared.I encoutered the same problem, the reason why is that you reached the max of networks: do an : docker network ls

Choose one to remove using: docker network rm networkname_defaultYou can try Worked for me.This happened to me because I was using OpenVPN. I found a way that I don't need to stop using the VPN or manually add a network to the docker-compose file nor run any crazy script. I switched to WireGuard instead of OpenVPN. More specifically, as I am running the nordvpn solution, I installed WireGuard and used their version of it, NordLynx.AddRead about network_mode in the

documentation.Disclaimer: I am not very knowledgeable about Docker networking, so the

"solution" that worked for me is like a magic incantation and YMMV.When I ran docker run my-image the networking gave me no problems, but when

I converted this command to a docker-compose.yml file, I got the same error

as the OP.I read Arenim's answer and

some other stuff on the internet that suggested to re-use an existing network.You can find existing networks like this:I wanted to reuse the default bridge network, so I addedto the the root of my docker-compose.yml (so not inside one of my

services, but at the root indentation).I now got the following error:This led met to this Docker Github

issue, that plainly stated

that I should add the network_mode object to my docker-compose:I was using Docker version 18.09.8, docker-compose version 1.24.1 and

the compose file format 3.7.Killing the vpn is not needed.This other comment about using a new network comes pretty close to the solution for me, and was working for a while, but I found a better way thanks to some talk over in another questionCreate a network with:Then, at the bottom of docker-compose.yaml, put this:Done.  No need to add networks to all container definitions etc.  and you can re-use the network with other docker-compose files as well if you'd like.I fixed this issue by steps :

How to divide flask app into multiple py files?

Rolando

[How to divide flask app into multiple py files?](https://stackoverflow.com/questions/11994325/how-to-divide-flask-app-into-multiple-py-files)

My flask application currently consists of a single test.py file with multiple routes and the main() route defined. Is there some way I could create a test2.py file that contains routes that were not handled in test.py?I am concerned that there are too many routes in test.py and would like to make it such that I can run python test.py, which will also pick up the routes on test.py as if it were part of the same file. What changes to I have to make in test.py and/or include in test2.py to get this to work?

2012-08-16 19:43:25Z

My flask application currently consists of a single test.py file with multiple routes and the main() route defined. Is there some way I could create a test2.py file that contains routes that were not handled in test.py?I am concerned that there are too many routes in test.py and would like to make it such that I can run python test.py, which will also pick up the routes on test.py as if it were part of the same file. What changes to I have to make in test.py and/or include in test2.py to get this to work?You can use the usual Python package structure to divide your App into multiple modules, see the Flask docs.However,You can create a sub-component of your app as a Blueprint in a separate file:And then use it in the main part:Blueprints can also bundle specific resources: templates or static files. Please refer to the Flask docs for all the details.I would like to recommend flask-empty at GitHub.It provides an easy way to understand Blueprints, multiple views and extensions.Dividing the app into blueprints is a great idea. However, if this isn't enough, and if you want to then divide the Blueprint itself into multiple py files, this is also possible using the regular Python module import system, and then looping through all the routes that get imported from the other files.I created a Gist with the code for doing this:https://gist.github.com/Jaza/61f879f577bc9d06029eAs far as I'm aware, this is the only feasible way to divide up a Blueprint at the moment. It's not possible to create "sub-blueprints" in Flask, although there's an issue open with a lot of discussion about this:https://github.com/mitsuhiko/flask/issues/593Also, even if it were possible (and it's probably do-able using some of the snippets from that issue thread), sub-blueprints may be too restrictive for your use case anyway - e.g. if you don't want all the routes in a sub-module to have the same URL sub-prefix.You can use simple trick which is import flask app variable from main inside another file, like:test-routes.pyand in your main files, where you declared flask app, import test-routes, like:app.pyIt works from my side.This task can be accomplished without blueprints and tricky imports using Centralized URL Mapapp.pyviews.py

Why can't non-default arguments follow default arguments?

dotslash

[Why can't non-default arguments follow default arguments?](https://stackoverflow.com/questions/16932825/why-cant-non-default-arguments-follow-default-arguments)

Why does this piece of code throw a SyntaxError?While the following piece of code runs without visible errors:

2013-06-05 06:17:41Z

Why does this piece of code throw a SyntaxError?While the following piece of code runs without visible errors:All required parameters must be placed before any default arguments. Simply because they are mandatory, whereas default arguments are not. Syntactically, it would be impossible for the interpreter to decide which values match which arguments if mixed modes were allowed. A SyntaxError is raised if the arguments are not given in the correct order:Let us take a look at keyword arguments, using your function.Suppose its allowed to declare function as above,

Then with the above declarations, we can make the following (regular) positional or keyword argument calls:How you will suggest the assignment of variables in the function call, how default arguments are going to be used along with keyword arguments.Reference O'Reilly - Core-Python

                                                               Where as this function make use of the default arguments syntactically correct for above function calls.

Keyword arguments calling prove useful for being able to provide for out-of-order positional arguments, but, coupled with default arguments, they can also be used to "skip over" missing arguments as well.If you were to allow this, the default arguments would be rendered useless because you would never be able to use their default values, since the non-default arguments  come after. In Python 3 however, you may do the following:which makes x and y keyword only so you can do this:This works because there is no longer any ambiguity. Note you still can't do fun1(2, 2) (that would set the default arguments).Let me clear two points here :def example(a, b, c=None, r="w" , d=[], *ae,  **ab):(a,b) are positional parameter(c=none) is optional parameter(r="w") is keyword parameter (d=[]) is list parameter (*ae) is keyword-only(**ab) is var-keyword parameterargument is not defined when default values are saved,Python computes

and saves default values when you define the functionc and d are not defined, does not exist, when this happens (it exists only when the function is executed) "a,a=b" its not allowed in parameter.Required arguments (the ones without defaults), must be at the start to allow client code to only supply two.  If the optional arguments were at the start, it would be confusing:What would that do in your first example?  In the last, x is "who is who", y is 3 and a = "jack".

Displaying better error message than「No JSON object could be decoded」

OJW

[Displaying better error message than「No JSON object could be decoded」](https://stackoverflow.com/questions/14899506/displaying-better-error-message-than-no-json-object-could-be-decoded)

Python code to load data from some long complicated JSON file:(note: the best code version should be:but both exhibit similar behavior)For many types of JSON error (missing delimiters, incorrect backslashes in strings, etc), this prints a nice helpful message containing the line and column number where the JSON error was found.However, for other types of JSON error (including the classic "using comma on the last item in a list", but also other things like capitalising true/false), Python's output is just:For that type of ValueError, how do you get Python to tell you where is the error in the JSON file?

2013-02-15 16:54:12Z

Python code to load data from some long complicated JSON file:(note: the best code version should be:but both exhibit similar behavior)For many types of JSON error (missing delimiters, incorrect backslashes in strings, etc), this prints a nice helpful message containing the line and column number where the JSON error was found.However, for other types of JSON error (including the classic "using comma on the last item in a list", but also other things like capitalising true/false), Python's output is just:For that type of ValueError, how do you get Python to tell you where is the error in the JSON file?I've found that the simplejson module gives more descriptive errors in many cases where the built-in json module is vague. For instance, for the case of having a comma after the last item in a list:which is not very descriptive. The same operation with simplejson:Much better! Likewise for other common errors like capitalizing True.You wont be able to get python to tell you where the JSON is incorrect. You will need to use a linter online somewhere like thisThis will show you error in the JSON you are trying to decode. You could try the rson library found here: http://code.google.com/p/rson/ . I it also up on PYPI: https://pypi.python.org/pypi/rson/0.9 so you can use easy_install or pip to get it.for the example given by tom:RSON is a designed to be a superset of JSON, so it can parse JSON files.  It also has an alternate syntax which is much nicer for humans to look at and edit.  I use it quite a bit for input files.As for the capitalizing of boolean values: it appears that rson reads incorrectly capitalized booleans as strings.I had a similar problem and it was due to singlequotes. The JSON standard(http://json.org) talks only about using double quotes so it must be that the python json library supports only double quotes.For my particular version of this problem, I went ahead and searched the function declaration of load_json_file(path) within the packaging.py file, then smuggled a print line into it:That way it would print the content of the json file before entering the try-catch, and that way – even with my barely existing Python knowledge – I was able to quickly figure out why my configuration couldn't read the json file.

(It was because I had set up my text editor to write a UTF-8 BOM … stupid)Just mentioning this because, while maybe not a good answer to the OP's specific problem, this was a rather quick method in determining the source of a very oppressing bug. And I bet that many people will stumble upon this article who are searching a more verbose solution for a MalformedJsonFileError: No JSON object could be decoded when reading …. So that might help them.As to me, my json file is very large, when use common json in python it gets the above error.After install simplejson by sudo pip install simplejson.And then  I solved it.I had a similar problem this was my code:the problem was i had forgotten to file.close()  I did it and fixed the problem.The accepted answer is the easiest one to fix the problem. But in case you are not allowed to install the simplejson due to your company policy, I propose below solution to fix the particular issue of "using comma on the last item in a list":Just hit the same issue and in my case the problem was related to BOM (byte order mark) at the beginning of the file. json.tool would refuse to process even empty file (just curly braces) until i removed the UTF BOM mark.What I have done is:This resolved the problem with json.tool. Hope this helps!When your file is created. Instead of creating a file with content is empty. Replace with:You could use cjson, that claims to be up to 250 times faster than pure-python implementations, given that you have "some long complicated JSON file" and you will probably need to run it several times (decoders fail and report the first error they encounter only).

How to duplicate virtualenv

dolma33

[How to duplicate virtualenv](https://stackoverflow.com/questions/7438681/how-to-duplicate-virtualenv)

I have an existing virtualenv with a lot of packages but an old version of Django.What I want to do is duplicate this environment so I have another environment with the exact same packages but a newer version of Django. How can I do this?

2011-09-15 23:43:01Z

I have an existing virtualenv with a lot of packages but an old version of Django.What I want to do is duplicate this environment so I have another environment with the exact same packages but a newer version of Django. How can I do this?The easiest way is to use pip to generate a requirements file. A requirements file is basically a file that contains a list of all the python packages you want to install (or have already installed in case of file generated by pip), and what versions they're at.To generate a requirements file, go into your original virtualenv, and run:This will generate the requirements.txt file for you. If you open that file up in your favorite text editor, you'll see something like:Now, edit the line that says Django==x.x to say Django==1.3 (or whatever version you want to install in your new virtualenv).Lastly, activate your new virtualenv, and run:And pip will automatically download and install all the python modules listed in your requirements.txt file, at whatever versions you specified!Another option is to use virtualenv-clone package:virtualenvwrapper provides a command to duplicate virtualenvIf you are using Anaconda you can just run:This will copy myenv to the newly created environment called myclone.Easiest option is using virtualenv-clone package.To duplicate venv1 to venv2, follow these steps:Can you not simply:

Combining node.js and Python

Cartesius00

[Combining node.js and Python](https://stackoverflow.com/questions/10775351/combining-node-js-and-python)

Node.js is a perfect match for our web project, but there are few computational tasks for which we would prefer Python. We also already have a Python code for them.

We are highly concerned about speed, what is the most elegant way how to call a Python "worker" from node.js in an asynchronous non-blocking way?

2012-05-27 16:03:52Z

Node.js is a perfect match for our web project, but there are few computational tasks for which we would prefer Python. We also already have a Python code for them.

We are highly concerned about speed, what is the most elegant way how to call a Python "worker" from node.js in an asynchronous non-blocking way?For communication between node.js and Python server, I would use Unix sockets if both processes run on the same server and TCP/IP sockets otherwise. For marshaling protocol I would take JSON or protocol buffer. If threaded Python shows up to be a bottleneck, consider using Twisted Python, which

provides the same event driven concurrency as do node.js.If you feel adventurous, learn clojure (clojurescript, clojure-py) and you'll get the same language that runs and interoperates with existing code on Java, JavaScript (node.js included), CLR and Python. And you get superb marshalling protocol by simply using clojure data structures.This sounds like a scenario where zeroMQ would be a good fit. It's a messaging framework that's similar to using TCP or Unix sockets, but it's much more robust (http://zguide.zeromq.org/py:all)There's a library that uses zeroMQ to provide a RPC framework that works pretty well. It's called zeroRPC (http://www.zerorpc.io/). Here's the hello world. Python "Hello x" server:And the node.js client:Or vice-versa, node.js server:And the python clientIf you arrange to have your Python worker in a separate process (either long-running server-type process or a spawned child on demand), your communication with it will be asynchronous on the node.js side. UNIX/TCP sockets and stdin/out/err communication are inherently async in node.I'd consider also Apache Thrift http://thrift.apache.org/It can bridge between several programming languages, is highly efficient and has support for async or sync calls. See full features here http://thrift.apache.org/docs/features/The multi language can be useful for future plans, for example if you later want to do part of the computational task in C++ it's very easy to do add it to the mix using Thrift.I've had a lot of success using thoonk.js along with thoonk.py. Thoonk leverages Redis (in-memory key-value store) to give you feed (think publish/subscribe), queue and job patterns for communication.Why is this better than unix sockets or direct tcp sockets? Overall performance may be decreased a little, however Thoonk provides a really simple API that simplifies having to manually deal with a socket. Thoonk also helps make it really trivial to implement a distributed computing model that allows you to scale your python workers to increase performance, since you just spin up new instances of your python workers and connect them to the same redis server.I'd recommend using some work queue using, for example, the excellent Gearman, which will provide you with a great way to dispatch background jobs, and asynchronously get their result once they're processed.The advantage of this, used heavily at Digg (among many others) is that it provides a strong, scalable and robust way to make workers in any language to speak with clients in any language.Update 2019There are several ways to achieve this and here is the list in increasing order of complexityApproach 1 Python Shell Simplest approach source.js filedestination.py fileNotes: Make a folder called subscriber which is at the same level as source.js file and put destination.py inside it. Dont forget to change your virtualenv environment

Where does this come from: -*- coding: utf-8 -*-

hamstergene

[Where does this come from: -*- coding: utf-8 -*-](https://stackoverflow.com/questions/4872007/where-does-this-come-from-coding-utf-8)

Python recognizes the following as instruction which defines file's encoding:I definitely saw this kind of instructions before (-*- var: value -*-). Where does it come from? What is the full specification, e.g. can the value include spaces, special symbols, newlines, even -*- itself?My program will be writing plain text files and I'd like to include some metadata in them using this format.

2011-02-02 08:01:23Z

Python recognizes the following as instruction which defines file's encoding:I definitely saw this kind of instructions before (-*- var: value -*-). Where does it come from? What is the full specification, e.g. can the value include spaces, special symbols, newlines, even -*- itself?My program will be writing plain text files and I'd like to include some metadata in them using this format.This way of specifying the encoding of a Python file comes from PEP 0263 - Defining Python Source Code Encodings.It is also recognized by GNU Emacs (see Python Language Reference, 2.1.4 Encoding declarations), though I don't know if it was the first program to use that syntax.This is so called file local variables, that are understood by Emacs and set correspondingly.  See corresponding section in Emacs manual - you can define them either in header or in footer of fileIn PyCharm, I'd leave it out. It turns off the UTF-8 indicator at the bottom with a warning that the encoding is hard-coded. Don't think you need the PyCharm comment mentioned above.# -*- coding: utf-8 -*- is a Python 2 thing. In Python 3+, the default encoding of source files is UTF-8 and that line is useless.See: Should I use encoding declaration in Python 3?

Python ElementTree module: How to ignore the namespace of XML files to locate matching element when using the method「find」,「findall」

KevinLeng

[Python ElementTree module: How to ignore the namespace of XML files to locate matching element when using the method「find」,「findall」](https://stackoverflow.com/questions/13412496/python-elementtree-module-how-to-ignore-the-namespace-of-xml-files-to-locate-ma)

I want to use the method of "findall" to locate some elements of the source xml file in the ElementTree module.However, the source xml file (test.xml) has namespace. I truncate part of xml file as sample:The sample python code is below:Although it can works, because there is a namespace "{http://www.test.com}", it's very inconvenient to add a namespace in front of each tag.How can I ignore the namespace when using the method of "find", "findall" and so on?

2012-11-16 07:53:17Z

I want to use the method of "findall" to locate some elements of the source xml file in the ElementTree module.However, the source xml file (test.xml) has namespace. I truncate part of xml file as sample:The sample python code is below:Although it can works, because there is a namespace "{http://www.test.com}", it's very inconvenient to add a namespace in front of each tag.How can I ignore the namespace when using the method of "find", "findall" and so on?Instead of modifying the XML document itself, it's best to parse it and then modify the tags in the result. This way you can handle multiple namespaces and namespace aliases:This is based on the discussion here:

http://bugs.python.org/issue18304Update: rpartition instead of partition makes sure you get the tag name in postfix even if there is no namespace. Thus you could condense it:If you remove the xmlns attribute from the xml before parsing it then there won't be a namespace prepended to each tag in the tree.The answers so far explicitely put the namespace value in the script. For a more generic solution, I would rather extract the namespace from the xml:And use it in find method:Here's an extension to nonagon's answer, which also strips namespaces off attributes:Improving on the answer by ericspod:Instead of changing the parse mode globally we can wrap this in an object supporting the with construct.This can then be used as followsThe beauty of this way is that it does not change any behaviour for unrelated code outside the with block. I ended up creating this after getting errors in unrelated libraries after using the version by ericspod which also happened to use expat.You can use the elegant string formatting construct as well:or, if you're sure that PAID_OFF only appears in one level in tree:If you're using ElementTree and not cElementTree you can force Expat to ignore namespace processing by replacing ParserCreate():ElementTree tries to use Expat by calling ParserCreate() but provides no option to not provide a namespace separator string, the above code will cause it to be ignore but be warned this could break other things.Let's combine nonagon's answer with mzjn's answer to a related question:Using this function we:I think this is the best approach all around as there's no manipulation either of a source XML or resulting parsed xml.etree.ElementTree output whatsoever involved. I'd like also to credit barny's answer with providing an essential piece of this puzzle (that you can get the parsed root from the iterator). Until that I actually traversed XML tree twice in my application (once to get namespaces, second for a root).I might be late for this but I dont think re.sub is a good solution.However the rewrite xml.parsers.expat does not work for Python 3.x versions,The main culprit is the xml/etree/ElementTree.py see bottom of the source codeWhich is kinda sad.The solution is to get rid of it first.Tested on Python 3.6.Try try statement is useful in case somewhere in your code you reload or import a module twice you get some strange errors like btw damn the etree source code looks really messy.

How to normalize a NumPy array to within a certain range?

endolith

[How to normalize a NumPy array to within a certain range?](https://stackoverflow.com/questions/1735025/how-to-normalize-a-numpy-array-to-within-a-certain-range)

After doing some processing on an audio or image array, it needs to be normalized within a range before it can be written back to a file.  This can be done like so:Is there a less verbose, convenience function way to do this? matplotlib.colors.Normalize() doesn't seem to be related.

2009-11-14 17:52:38Z

After doing some processing on an audio or image array, it needs to be normalized within a range before it can be written back to a file.  This can be done like so:Is there a less verbose, convenience function way to do this? matplotlib.colors.Normalize() doesn't seem to be related.Using /= and *= allows you to eliminate an intermediate temporary array, thus saving some memory.  Multiplication is less expensive than division, so is marginally faster than Since we are using basic numpy methods here, I think this is about as efficient a solution in numpy as can be.In-place operations do not change the dtype of the container array. Since the desired normalized values are floats, the audio and image arrays need to have floating-point point dtype before the in-place operations are performed.

If they are not already of floating-point dtype, you'll need to convert them using astype. For example,If the array contains both positive and negative data, I'd go with:also, worth mentioning even if it's not OP's question, standardization:You can also rescale using sklearn. The advantages are that you can adjust normalize the standard deviation, in addition to mean-centering the data, and that you can do this on either axis, by features, or by records.The keyword arguments axis, with_mean, with_std are self explanatory, and are shown in their default state. The argument copy performs the operation in-place if it is set to False. Documentation here.You can use the "i" (as in idiv, imul..) version, and it doesn't look half bad:For the other case you can write a function to normalize an n-dimensional array by colums:You are trying to min-max scale the values of audio between -1 and +1 and image between 0 and 255.Using sklearn.preprocessing.minmax_scale, should easily solve your problem. e.g.:andnote: Not to be confused with the operation that scales the norm (length) of a vector to a certain value (usually 1), which is also commonly referred to as normalization.A simple solution is using the scalers offered by the sklearn.preprocessing library. The error X_rec-X will be zero. You can adjust the feature_range for your needs, or even use a standart scaler sk.StandardScaler()I tried following this, and got the error The numpy array I was trying to normalize was an integer array. It seems they deprecated type casting in versions > 1.10, and you have to use numpy.true_divide() to resolve that.img was an PIL.Image object.

class method generates「TypeError: … got multiple values for keyword argument …」

drevicko

[class method generates「TypeError: … got multiple values for keyword argument …」](https://stackoverflow.com/questions/18950054/class-method-generates-typeerror-got-multiple-values-for-keyword-argument)

If I define a class method with a keyword argument thus:calling the method generates a TypeError:What's going on?

2013-09-23 00:08:45Z

If I define a class method with a keyword argument thus:calling the method generates a TypeError:What's going on?The problem is that the first argument passed to class methods in python is always a copy of the class instance on which the method is called, typically labelled self. If the class is declared thus:it behaves as expected. Explanation:Without self as the first parameter, when myfoo.foodo(thing="something") is executed, the foodo method is called with arguments (myfoo, thing="something"). The instance myfoo is then assigned to thing (since thing is the first declared parameter), but python also attempts to assign "something" to thing, hence the Exception.To demonstrate, try running this with the original code:You'll output like:You can see that 'thing' has been assigned a reference to the instance 'myfoo' of the class 'foo'. This section of the docs explains how function arguments work a bit more.Thanks for the instructive posts. I'd just like to keep a note that if you're getting "TypeError: foodo() got multiple values for keyword argument 'thing'", it may also be that you're mistakenly passing the 'self' as a parameter when calling the function (probably because you copied the line from the class declaration - it's a common error when one's in a hurry).This might be obvious, but it might help someone who has never seen it before.  This also happens for regular functions if you mistakenly assign a parameter by position and explicitly by name.just add 'staticmethod' decorator to function and problem is fixedI want to add one more answer :there is difference between parameter and argument you can read in detail about here Arguments and Parameter in pythonsince we have three parameters :a is positional parameterb=1 is keyword and default parameter *args is variable length parameter so we first assign a as positional parameter , means we have to provide value to positional argument in its position order, here order matter.

but we are passing argument 1 at the place of a in calling function and then we are also providing value to a , treating as keyword argument.

now a have two values :one is positional value: a=1second is keyworded value which is a=12We have to change hello(1, 2, 3, 4,a=12) to hello(1, 2, 3, 4,12)

so now a will get only one positional value which is 1 and b will get value 2 and rest of values will get *args (variable length parameter)if we want that *args should get 2,3,4 and a  should get 1 and b should get  12 then we can do like this

def hello(a,*args,b=1):

     pass

hello(1, 2, 3, 4,b=12)Something more :output :This error can also happen if you pass a key word argument for which one of the keys is similar (has same string name) to a positional argument."fire" has been accepted into the 'bar' argument. And yet there is another 'bar' argument present in kwargs.You would have to remove the keyword argument from the kwargs before passing it to the method.Also this can happen in Django if you are using jquery ajax to url that reverses to a function that doesn't contain 'request' parameter

Best way to structure a tkinter application?

Chris Aung

[Best way to structure a tkinter application?](https://stackoverflow.com/questions/17466561/best-way-to-structure-a-tkinter-application)

The following is the overall structure of my typical python tkinter program.funA funB and funC will bring up another Toplevel windows with widgets when user click on button 1, 2, 3.  I am wondering if this is the right way to write a python tkinter program? Sure, it will work even if I write this way, but is it the best way? It sounds stupid but when I see the codes other people written, their code is not messed up with bunch of functions and mostly they have classes.Is there any specific structure that we should follow as good practice? How should I plan before start writing a python program?I know there is no such thing as best practice in programming and I am not asking for it either. I just want some advice and explanations to keep me on the right direction as I am learning Python by myself.

2013-07-04 09:21:47Z

The following is the overall structure of my typical python tkinter program.funA funB and funC will bring up another Toplevel windows with widgets when user click on button 1, 2, 3.  I am wondering if this is the right way to write a python tkinter program? Sure, it will work even if I write this way, but is it the best way? It sounds stupid but when I see the codes other people written, their code is not messed up with bunch of functions and mostly they have classes.Is there any specific structure that we should follow as good practice? How should I plan before start writing a python program?I know there is no such thing as best practice in programming and I am not asking for it either. I just want some advice and explanations to keep me on the right direction as I am learning Python by myself.I advocate an object oriented approach. This is the template that I start out with:The important things to notice are:If your app has additional toplevel windows, I recommend making each of those a separate class, inheriting from tk.Toplevel. This gives you all of the same advantages mentioned above -- the windows are atomic, they have their own namespace, and the code is well organized. Plus, it makes it easy to put each into its own module once the code starts to get large. Finally, you might want to consider using classes for every major portion of your interface. For example, if you're creating an app with a toolbar, a navigation pane, a statusbar, and a main area, you could make each one of those classes. This makes your main code quite small and easy to understand:Since all of those instances share a common parent, the parent effectively becomes the "controller" part of a model-view-controller architecture. So, for example, the main window could place something on the statusbar by calling self.parent.statusbar.set("Hello, world"). This allows you to define a simple interface between the components, helping to keep coupling to a minimun. Putting each of your top-level windows into it's own separate class gives you code re-use and better code organization. Any buttons and relevant methods that are present in the window should be defined inside this class. Here's an example (taken from here):Also see:Hope that helps.This isn't a bad structure; it will work just fine. However, you do have to have functions in a function to do commands when someone clicks on a button or somethingSo what you could do is write classes for these then have methods in the class that handle commands for the button clicks and such.Here's an example:Usually tk programs with multiple windows are multiple big classes and in the __init__ all the entries, labels etc are created and then each method is to handle button click eventsThere isn't really a right way to do it, whatever works for you and gets the job done as long as its readable and you can easily explain it because if you cant easily explain your program, there probably is a better way to do it.Take a look at Thinking in Tkinter.OOP should be the approach and frame should be a class variable instead of instance variable.Reference: http://www.python-course.eu/tkinter_buttons.phpProbably the best way to learn how to structure your program is by reading other people's code, especially if it's a large program to which many people have contributed. After looking at the code of many projects, you should get an idea of what the consensus style should be. Python, as a language, is special in that there are some strong guidelines as to how you should format your code. The first is the so-called "Zen of Python":On a more practical level, there is PEP8, the style guide for Python.With those in mind, I would say that your code style doesn't really fit, particularly the nested functions. Find a way to flatten those out, either by using classes or moving them into separate modules. This will make the structure of your program much easier to understand.I personally do not use the objected oriented approach, mostly because it a) only get in the way; b) you will never reuse that as a module.but something that is not discussed here, is that you must use threading or multiprocessing. Always. otherwise your application will be awful.just do a simple test: start a window, and then fetch some URL or anything else. changes are your UI will not be updated while the network request is happening. Meaning, your application window will be broken. depend on the OS you are on, but most times, it will not redraw, anything you drag over the window will be plastered on it, until the process is back to the TK mainloop.

pytest: assert almost equal

Vladimir Keleshev

[pytest: assert almost equal](https://stackoverflow.com/questions/8560131/pytest-assert-almost-equal)

How to do assert almost equal with py.test for floats without resorting to something like:More specifically it will be useful to know a neat solution for quickly compare pairs of float, without unpacking them:

2011-12-19 10:41:54Z

How to do assert almost equal with py.test for floats without resorting to something like:More specifically it will be useful to know a neat solution for quickly compare pairs of float, without unpacking them:I noticed that this question specifically asked about py.test. py.test 3.0 includes an approx() function (well, really class) that is very useful for this purpose.The documentation is here: https://docs.pytest.org/en/latest/reference.html#pytest-approxYou will have to specify what is "almost" for you:to apply to tuples (or any sequence):If you have access to NumPy it has great functions for floating point comparison that already do pairwise comparison with numpy.testing.Then you can do something like:Something likeThat is what unittest doesFor the second partProbably better to wrap that in a functionThese answers have been around for a long time, but I think the easiest and also most readable way is to use unittest for it's many nice assertions without using it for the testing structure.(based on this answer)Just use * to unpack your return value without needing to introduce new names.If you want something that works not only with floats but for example Decimals you can use python's math.isclose:Docs - https://docs.python.org/3/library/math.html#math.iscloseI'd use nose.tools. It plays well with py.test runner and have other equally useful asserts - assert_dict_equal(), assert_list_equal(), etc.

Can I install Python windows packages into virtualenvs?

Ned Batchelder

[Can I install Python windows packages into virtualenvs?](https://stackoverflow.com/questions/3271590/can-i-install-python-windows-packages-into-virtualenvs)

Virtualenv is great: it lets me keep a number of distinct Python installations so that different projects' dependencies aren't all thrown together into a common pile.But if I want to install a package on Windows that's packaged as a .exe installer, how can I direct it to install into the virtualenv?  For example, I have pycuda-0.94rc.win32-py2.6.exe.  When I run it, it examines the registry, and finds only one Python26 to install into, the common one that my virtualenv is based off of.How can I direct it to install into the virtualenv?

2010-07-17 13:10:51Z

Virtualenv is great: it lets me keep a number of distinct Python installations so that different projects' dependencies aren't all thrown together into a common pile.But if I want to install a package on Windows that's packaged as a .exe installer, how can I direct it to install into the virtualenv?  For example, I have pycuda-0.94rc.win32-py2.6.exe.  When I run it, it examines the registry, and finds only one Python26 to install into, the common one that my virtualenv is based off of.How can I direct it to install into the virtualenv?Yes, you can. All you need isSurprised? It looks like binary installers for Windows made with distutils combine .exe with .zip into one .exe file. Change extension to .zip to see it's a valid zip file. I discovered this after reading answers to my question Where can I download binary eggs with psycopg2 for Windows?UPDATEAs noted by Tritium21 in his answer nowadays you should use pip instead of easy_install. Pip can't install binary packages created by distutils but it can install binary packages in the new wheel format. You can convert from old format to the new one using wheel package, which you have to install first.I know this is quite an old question, and predates the tools I am about to talk about, but for the sake of Google, I think it is a good idea to mention it.  easy_install is the black sheep of python packaging.  No one wants to admit using it with the new hotness of pip around.  Also, while playing registry tricks will work best for non-standard EXE installers (someone built the installer themselves instead of using distutils, and is checking the registry for the installation path), there is now a Better Way(c) for standard EXE installers.The wheel format, introduced recently as of this post, is the replacement for the egg format, filling much the same role.  This format is also supported by pip (a tool already installed in your virtualenv).if for some reason pip install WHEELFILE does not work, try wheel install WHEELFILEI ended up adapting a script (http://effbot.org/zone/python-register.htm) to register a Python installation in the registry.  I can pick the Python to be the Python in the registry, run the Windows installer, then set the registry back:Run this script with the Python you want to be registered, and it will be entered into the registry. Note that on Windows 7 and Vista, you'll need Administrator privileges.easy_install is able to install .exe packages as long as they were built using distutils' bdist_wininst target, which covers many popular packages. However, there are many others that aren't (wxPython is one that I've struggled with)You can use environment's easy_install to install PyCUDA.it will give you the same version 0.94rc.On Windows easy_install.exe will be in Scripts directory.If it's a .msi, you might be able to specify command line options using msiexec.  The Python installer itself allows TARGETDIR, but I'm not sure if distutils bakes this into distribution installers.  If you're using a .exe, I don't think there's a clean way.  One option is to use a program like 7Zip (or winzip, etc) to directly extract the contents of the exe, then copy the relevent folders into your virtual site-packages folder.  For example, if I extract "processing-0.5.2.win32-py2.5.exe", I find a folder "PLATLIB\processing" which I copy to a virtualenv path and use without any runtime problems.  (I'm not sure it's always that simple though.)

How to merge multiple lists into one list in python? [duplicate]

user1452759

[How to merge multiple lists into one list in python? [duplicate]](https://stackoverflow.com/questions/11574195/how-to-merge-multiple-lists-into-one-list-in-python)

I have many lists which looks like I want the above to look like How do I achieve that?

2012-07-20 06:48:02Z

I have many lists which looks like I want the above to look like How do I achieve that?Just add them:You should read the Python tutorial to learn basic info like this.Just another method....

Add SUM of values of two LISTS into new LIST

Prashant

[Add SUM of values of two LISTS into new LIST](https://stackoverflow.com/questions/14050824/add-sum-of-values-of-two-lists-into-new-list)

I have the following two lists:Now I want to add the items from both of these lists into a new list.output should be 

2012-12-27 07:09:52Z

I have the following two lists:Now I want to add the items from both of these lists into a new list.output should be The zip function is useful here, used with a list comprehension.If you have a list of lists (instead of just two lists):From docsAssuming both lists a and b have same length, you do not need zip, numpy or anything else.Python 2.x and 3.x:Default behavior in numpy is add componentwisewhich outputsThis extends itself to any number of lists:In your case, myListOfLists would be [first, second]Try the following code:The easy way and fast way to do this is:Alternatively, you can use numpy sum:My answer is repeated with Thiru's that answered it in Mar 17 at 9:25.It was simpler and quicker, here are his solutions:You need numpy!one-liner solution You can use zip(), which will "interleave" the two arrays together, and then map(), which will apply a function to each element in an iterable:Here is another way to do it. We make use of the internal __add__ function of python:OutputIf you want to add also the rest of the values in the lists you can use this (this is working in Python3.5)Here is another way to do it.It is working fine for me . Perhaps the simplest approach:If you consider your lists as numpy array, then you need to easily sum them:If you have an unknown number of lists of the same length, you can use the below function.Here the *args accepts a variable number of list arguments (but only sums the same number of elements in each). 

The * is used again to unpack the elements in each of the lists.Output:Or with 3 listsOutput:You can use this method but it will work only if both the list are of the same size:

How to filter rows containing a string pattern from a Pandas dataframe [duplicate]

John Knight

[How to filter rows containing a string pattern from a Pandas dataframe [duplicate]](https://stackoverflow.com/questions/27975069/how-to-filter-rows-containing-a-string-pattern-from-a-pandas-dataframe)

Assume we have a data frame in Python Pandas that looks like this:Or, in table form:How do I filter rows which contain the key word "ball?" For example, the output should be:

2015-01-15 23:44:22Z

Assume we have a data frame in Python Pandas that looks like this:Or, in table form:How do I filter rows which contain the key word "ball?" For example, the output should be:Step-by-step explanation (from inner to outer):If you want to set the column you filter on as a new index, you could also consider to use .filter; if you want to keep it as a separate column then str.contains is the way to go.Let's say you have and your plan is to filter all rows in which ids contains ball AND set ids as new index, you can dowhich givesBut filter also allows you to pass a regex, so you could also filter only those rows where the column entry ends with ball. In this case you useNote that now the entry with ballxyz is not included as it starts with ball and does not end with it.If you want to get all entries that start with ball you can simple useyieldingThe same works with columns; all you then need to change is the axis=0 part. If you filter based on columns, it would be axis=1.

range() for floats

Jonathan

[range() for floats](https://stackoverflow.com/questions/7267226/range-for-floats)

Is there a range() equivalent for floats in Python?

2011-09-01 07:30:04Z

Is there a range() equivalent for floats in Python?

I don't know a built-in function, but writing one like this shouldn't be too complicated.As the comments mention, this could produce unpredictable results like:To get the expected result, you can use one of the other answers in this question, or as @Tadhg mentioned, you can use decimal.Decimal as the jump argument. Make sure to initialize it with a string rather than a float.Or even:And then:You can either use:or use lambda / map:I used to use numpy.arange but had some complications controlling the number of elements it returns, due to floating point errors.  So now I use linspace, e.g.:Pylab has frange (a wrapper, actually, for matplotlib.mlab.frange):Eagerly evaluated (2.x range):Lazily evaluated (2.x xrange, 3.x range):Alternately:using itertools: lazily evaluated floating point range:I helped add the function numeric_range to the package more-itertools.more_itertools.numeric_range(start, stop, step) acts like the built in function range but can handle floats, Decimal, and Fraction types.There is no such built-in function, but you can use the following (Python 3 code) to do the job as safe as Python allows you to.You can verify all of it by running a few assertions:Code available on GitHubWhy Is There No Floating Point Range Implementation In The Standard Library?As made clear by all the posts here, there is no floating point version of range(). That said, the omission makes sense if we consider that the range() function is often used as an index (and of course, that means an accessor) generator. So, when we call range(0,40), we're in effect saying we want 40 values starting at 0, up to 40, but non-inclusive of 40 itself. When we consider that index generation is as much about the number of indices as it is their values, the use of a float implementation of range() in the standard library makes less sense. For example, if we called the function frange(0, 10, 0.25), we would expect both 0 and 10 to be included, but that would yield a vector with 41 values. Thus, an frange() function depending on its use will always exhibit counter intuitive behavior; it either has too many values as perceived from the indexing perspective or is not inclusive of a number that reasonably should be returned from the mathematical perspective.The Mathematical Use CaseWith that said, as discussed, numpy.linspace() performs the generation with the mathematical perspective nicely:The Indexing Use CaseAnd for the indexing perspective, I've written a slightly different approach with some tricksy string magic that allows us to specify the number of decimal places.Similarly, we can also use the built-in round function and specify the number of decimals:A Quick Comparison & PerformanceOf course, given the above discussion, these functions have a fairly limited use case. Nonetheless, here's a quick comparison:The results are identical for each:And some timings:Looks like the string formatting method wins by a hair on my system. The LimitationsAnd finally, a demonstration of the point from the discussion above and one last limitation:Further, when the skip parameter is not divisible by the stop value, there can be a yawning gap given the latter issue:There are ways to address this issue, but at the end of the day, the best approach would probably be to just use Numpy.A solution without numpy etc dependencies was provided by kichik but due to the floating point arithmetics, it often behaves unexpectedly. As noted by me and blubberdiblub, additional elements easily sneak into the result. For example naive_frange(0.0, 1.0, 0.1) would yield 0.999... as its last value and thus yield 11 values in total.A robust version is provided here: Because the multiplication, the rounding errors do not accumulate. The use of epsilon takes care of possible rounding error of the multiplication, even though issues of course might rise in the very small and very large ends. Now, as expected:And with somewhat larger numbers:The code is also available as a GitHub Gist.Aw, heck -- I'll toss in a simple library-less version.  Feel free to improve on it[*]:The core idea is that nsteps is the number of steps to get you from start to stop and range(nsteps) always emits integers so there's no loss of accuracy.  The final step is to map [0..nsteps] linearly onto [start..stop].If, like alancalvitti you'd like the series to have exact rational representation, you can always use Fractions:[*] In particular, frange() returns a list, not a generator.  But it sufficed for my needs.Note 1:

From the discussion in the comment section here, "never use numpy.arange() (the numpy documentation itself recommends against it). Use numpy.linspace as recommended by wim, or one of the other suggestions in this answer"Note 2:

I have read the discussion in a few comments here, but after coming back to this question for the third time now, I feel this information should be placed in a more readable position.i wrote a function that returns a tuple of a range of double precision floating point numbers without any decimal places beyond the hundredths. it was simply a matter of parsing the range values like strings and splitting off the excess. I use it for displaying ranges to select from within a UI. I hope someone else finds it useful.If in doubt, try the four tests cases above.Please note the first letter of Range is capital. This naming method is not encouraged for functions in Python. You can change Range to something like drange or frange if you want. The "Range" function behaves just as you want it to. You can check it's manual here [ http://reference.wolfram.com/language/ref/Range.html ].I think that there is a very simple answer that really emulates all the features of range but for both float and integer. In this solution, you just suppose that your approximation by default is 1e-7 (or the one you choose) and you can change it when you call the function.Is there a range() equivalent for floats in Python?

NO

Use this:There several answers here that don't handle simple edge cases like negative step, wrong start, stop etc. Here's the version that handles many of these cases correctly giving same behaviour as native range():Note that this would error out step=0 just like native range. One difference is that native range returns object that is indexable and reversible while above doesn't.You can play with this code and test cases here.

How do I perform HTML decoding/encoding using Python/Django?

rksprst

[How do I perform HTML decoding/encoding using Python/Django?](https://stackoverflow.com/questions/275174/how-do-i-perform-html-decoding-encoding-using-python-django)

I have a string that is HTML encoded: I want to change that to:I want this to register as HTML so that it is rendered as an image by the browser instead of being displayed as text. The string is stored like that because I am using a web-scraping tool called BeautifulSoup, it "scans" a web-page and gets certain content from it, then returns the string in that format.I've found how to do this in C# but not in Python. Can someone help me out?

2008-11-08 20:44:30Z

I have a string that is HTML encoded: I want to change that to:I want this to register as HTML so that it is rendered as an image by the browser instead of being displayed as text. The string is stored like that because I am using a web-scraping tool called BeautifulSoup, it "scans" a web-page and gets certain content from it, then returns the string in that format.I've found how to do this in C# but not in Python. Can someone help me out?Given the Django use case, there are two answers to this.  Here is its django.utils.html.escape function, for reference:To reverse this, the Cheetah function described in Jake's answer should work, but is missing the single-quote.  This version includes an updated tuple, with the order of replacement reversed to avoid symmetric problems:This, however, is not a general solution; it is only appropriate for strings encoded with django.utils.html.escape.  More generally, it is a good idea to stick with the standard library:As a suggestion: it may make more sense to store the HTML unescaped in your database.  It'd be worth looking into getting unescaped results back from BeautifulSoup if possible, and avoiding this process altogether.With Django, escaping only occurs during template rendering; so to prevent escaping you just tell the templating engine not to escape your string.  To do that, use one of these options in your template:With the standard library:For html encoding, there's cgi.escape from the standard library:For html decoding, I use the following:For anything more complicated, I use BeautifulSoup.Use daniel's solution if the set of encoded characters is relatively restricted.

Otherwise, use one of the numerous HTML-parsing libraries.I like BeautifulSoup because it can handle malformed XML/HTML :http://www.crummy.com/software/BeautifulSoup/for your question, there's an example in their documentation In Python 3.4+:See at the bottom of this page at Python wiki, there are at least 2 options to "unescape" html.Daniel's comment as an answer:"escaping only occurs in Django during template rendering. Therefore, there's no need for an unescape - you just tell the templating engine not to escape. either {{ context_var|safe }} or {% autoescape off %}{{ context_var }}{% endautoescape %}"I found a fine function at: http://snippets.dzone.com/posts/show/4569If anyone is looking for a simple way to do this via the django templates, you can always use filters like this:I had some data coming from a vendor and everything I posted had html tags actually written on the rendered page as if you were looking at the source.  The above code helped me greatly.

Hope this helps others.Cheers!!Even though this is a really old question, this may work.Django 1.5.5I found this in the Cheetah source code (here)not sure why they reverse the list,

 I think it has to do with the way they encode, so with you it may not need to be reversed.

Also if I were you I would change htmlCodes to be a list of tuples rather than a list of lists...

this is going in my library though :)i noticed your title asked for encode too, so here is Cheetah's encode function.You can also use django.utils.html.escapeBelow is a python function that uses module htmlentitydefs.  It is not perfect.  The version of htmlentitydefs that I have is incomplete and it assumes that all entities decode to one codepoint which is wrong for entities like &NotEqualTilde;:http://www.w3.org/TR/html5/named-character-references.htmlWith those caveats though, here's the code.This is the easiest solution for this problem - From this page.Searching the simplest solution of this question in Django and Python I found you can use builtin theirs functions to escape/unescape html code.I saved your html code in scraped_html and clean_html:You need Django >= 1.0To unescape your scraped html code you can use django.utils.text.unescape_entities which:To escape your clean html code you can use django.utils.html.escape which:You need Python >= 3.4To unescape your scraped html code you can use html.unescape which:To escape your clean html code you can use html.escape which:

Python safe method to get value of nested dictionary

Arti

[Python safe method to get value of nested dictionary](https://stackoverflow.com/questions/25833613/python-safe-method-to-get-value-of-nested-dictionary)

I have a nested dictionary. Is there only one way to get values out safely?Or maybe python has a method like get() for nested dictionary ?

2014-09-14 13:11:40Z

I have a nested dictionary. Is there only one way to get values out safely?Or maybe python has a method like get() for nested dictionary ?You could use get twice:This will return None if either key1 or key2 does not exist.Note that this could still raise an AttributeError if example_dict['key1'] exists but is not a dict (or a dict-like object with a get method). The try..except code you posted would raise a TypeError instead if example_dict['key1'] is unsubscriptable.Another difference is that the try...except short-circuits immediately after the first missing key. The chain of get calls does not.If you wish to preserve the syntax, example_dict['key1']['key2'] but do not want it to ever raise KeyErrors, then you could use the Hasher recipe:Note that this returns an empty Hasher when a key is missing.Since Hasher is a subclass of dict you can use a Hasher in much the same way you could use a dict. All the same methods and syntax is available, Hashers just treat missing keys differently.You can convert a regular dict into a Hasher like this:and convert a Hasher to a regular dict just as easily:Another alternative is to hide the ugliness in a helper function:So the rest of your code can stay relatively readable:You could also use python reduce:By combining all of these answer here and small changes that I made, I think this function would be useful. its safe, quick, easily maintainable.Example :Building up on Yoav's answer, an even safer approach:A recursive solution. It's not the most efficient but I find it a bit more readable than the other examples and it doesn't rely on functools.ExampleA more polished versionWhile the reduce approach is neat and short, I think a simple loop is easier to grok. I've also included a default parameter.As an exercise to understand how the reduce one-liner worked, I did the following. But ultimately the loop approach seems more intuitive to me.UsageI suggest you to try python-benedict.It is a dict subclass that provides keypath support and much more.Installation: pip install python-benedictnow you can access nested values using keypath:or access nested values using keys list:It is well tested and open-source on GitHub:https://github.com/fabiocaccamo/python-benedictA simple class that can wrap a dict, and retrieve based on a key:For example:If the key doesn't exist, it returns None by default. You can override that using a default= key in the FindDict wrapper -- for example`:for a second level key retrieving, you can do this:After seeing this for deeply getting attributes, I made the following to safely get nested dict values using dot notation. This works for me because my dicts are deserialized MongoDB objects, so I know the key names don't contain .s. Also, in my context, I can specify a falsy fallback value (None) that I don't have in my data, so I can avoid the try/except pattern when calling the function.Yet another function for the same thing, also returns a boolean to represent whether the key was found or not and handles some unexpected errors.example usage:An adaptation of unutbu's answer that I found useful in my own code:It generates a dictionary entry for key1 if it does not have that key already so that you avoid the KeyError. If you want to end up a nested dictionary that includes that key pairing anyway like I did, this seems like the easiest solution.Since raising an key error if one of keys is missing is a reasonable thing to do, we can even not check for it and get it as single as that:Little improvement to reduce approach to make it work with list. Also using data path as string divided by dots instead of array.A solution I've used that is similar to the double get but with the additional ability to avoid a TypeError using if else logic:However, the more nested the dictionary the more cumbersome this becomes.For nested dictionary/JSON lookups, you can use dictorpip install dictordict objectto get Lonestar's items, simply provide a dot-separated path, ieyou can provide fallback value in case the key isnt in paththeres tons more options you can do, like ignore letter casing and using other characters besides '.' as a path separator,https://github.com/perfecto25/dictor

Replacing column values in a pandas DataFrame

Black

[Replacing column values in a pandas DataFrame](https://stackoverflow.com/questions/23307301/replacing-column-values-in-a-pandas-dataframe)

I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. I have tried the following:But receive the exact same copy of the previous results.I would ideally like to get some output which resembles the following loop element-wise.I've looked through the gotchas documentation (http://pandas.pydata.org/pandas-docs/stable/gotchas.html) but cannot figure out why nothing happens.Any help will be appreciated.

2014-04-26 06:04:24Z

I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. I have tried the following:But receive the exact same copy of the previous results.I would ideally like to get some output which resembles the following loop element-wise.I've looked through the gotchas documentation (http://pandas.pydata.org/pandas-docs/stable/gotchas.html) but cannot figure out why nothing happens.Any help will be appreciated.If I understand right, you want something like this:(Here I convert the values to numbers instead of strings containing numbers.  You can convert them to "1" and "0", if you really want, but I'm not sure why you'd want that.)The reason your code doesn't work is because using ['female'] on a column (the second 'female' in your w['female']['female']) doesn't mean "select rows where the value is 'female'".  It means to select rows where the index is 'female', of which there may not be any in your DataFrame.You can edit a subset of a dataframe by using loc:In this case:Slight variation:See pandas.DataFrame.replace() docs.This should also work:You can also use apply with .get i.e.w['female'] = w['female'].apply({'male':0, 'female':1}.get):Dataframe w:Using apply to replace values from the dictionary:Result:Note: apply with dictionary should be used if all the possible values of the columns in the dataframe are defined in the dictionary else, it will have empty for those not defined in dictionary. This is very compact:Another good one:Alternatively there is the built-in function pd.get_dummies for these kinds of assignments:This gives you a data frame with two columns, one for each value that occurs in w['female'], of which you drop the first (because you can infer it from the one that is left). The new column is automatically named as the string that you replaced. This is especially useful if you have categorical variables with more than two possible values. This function creates as many dummy variables needed to distinguish between all cases. Be careful then that you don't assign the entire data frame to a single column, but instead, if w['female'] could be 'male', 'female' or 'neutral', do something like this:Then you are left with two new columns giving you the dummy coding of 'female' and you got rid of the column with the strings. There is also a function in pandas called factorize which you can use to automatically do this type of work. It converts labels to numbers: ['male', 'female', 'male'] -> [0, 1, 0]. See this answer for more information.If your column contains more strings than only female and male, Series.map will fail in this case since it will return NaN for other values.That's why we have to chain it with fillna:Example why .map fails:For the correct method, we chain map with fillna, so we fill the NaN with values from the original column:I think that in answer should be pointed which type of object do you get in all methods suggested above: is it Series or DataFrame.When you get column by w.female. or w[[2]] (where, suppose, 2 is number of your column) you'll get back DataFrame.

So in this case you can use DataFrame methods like .replace.When you use .loc or iloc you get back Series, and Series don't have .replace method, so you should use methods like apply, map and so on.

Getting number of elements in an iterator in Python

Tomasz Wysocki

[Getting number of elements in an iterator in Python](https://stackoverflow.com/questions/3345785/getting-number-of-elements-in-an-iterator-in-python)

Is there an efficient way to know how many elements are in an iterator in Python, in general, without iterating through each and counting?

2010-07-27 16:32:21Z

Is there an efficient way to know how many elements are in an iterator in Python, in general, without iterating through each and counting?No. It's not possible.Example:Length of iterator is unknown until you iterate through it.This code should work:Although it does iterate through each item and count them, it is the fastest way to do so.It also works for when the iterator has no item:Of course, it runs forever for an infinite input, so remember that iterators can be infinite:Also, be aware that the iterator will be exhausted by doing this, and further attempts to use it will see no elements. That's an unavoidable consequence of the Python iterator design. If you want to keep the elements, you'll have to store them in a list or something.No, any method will require you to resolve every result. You can do but running that on an infinite iterator will of course never return. It also will consume the iterator and it will need to be reset if you want to use the contents.Telling us what real problem you're trying to solve might help us find you a better way to accomplish your actual goal.Edit: Using list() will read the whole iterable into memory at once, which may be undesirable. Another way is to doas another person posted. That will avoid keeping it in memory.You cannot (except the type of a particular iterator implements some specific methods that make it possible).Generally, you may count iterator items only by consuming the iterator. One of probably the most efficient ways:(For Python 3.x replace itertools.izip with zip).Kinda. You could check the __length_hint__ method, but be warned that (at least up to Python 3.4, as gsnedders helpfully points out) it's a undocumented implementation detail (following message in thread), that could very well vanish or summon nasal demons instead.Otherwise, no. Iterators are just an object that only expose the next() method. You can call it as many times as required and they may or may not eventually raise StopIteration. Luckily, this behaviour is most of the time transparent to the coder. :)I like the cardinality package for this, it is very lightweight and tries to use the fastest possible implementation available depending on the iterable.Usage:The actual count() implementation is as follows:So, for those who would like to know the summary of that discussion. The final top scores for counting a 50 million-lengthed generator expression using: sorted by performance of execution (including memory consumption), will make you surprised:```('list, sec', 1.9684218849870376)('list_compr, sec', 2.5885991149989422)('sum, sec', 3.441088170016883)('ilen, sec', 9.812256851990242)('reduce, sec', 13.436614598002052)

```So, len(list(gen)) is the most frequent and less memory consumableAn iterator is just an object which has a pointer to the next object to be read by some kind of buffer or stream, it's like a LinkedList where you don't know how many things you have until you iterate through them. Iterators are meant to be efficient because all they do is tell you what is next by references instead of using indexing (but as you saw you lose the ability to see how many entries are next).Regarding your original question, the answer is still that there is no way in general to know the length of an iterator in Python.Given that you question is motivated by an application of the pysam library, I can give a more specific answer: I'm a contributer to PySAM and the definitive answer is that SAM/BAM files do not provide an exact count of aligned reads.  Nor is this information easily available from a BAM index file.  The best one can do is to estimate the approximate number of alignments by using the location of the file pointer after reading a number of alignments and extrapolating based on the total size of the file.  This is enough to implement a progress bar, but not a method of counting alignments in constant time.A quick benchmark:The results:I.e. the simple count_iter_items is the way to go.Adjusting this for python3:There are two ways to get the length of "something" on a computer.The first way is to store a count - this requires anything that touches the file/data to modify it (or a class that only exposes interfaces -- but it boils down to the same thing).The other way is to iterate over it and count how big it is.It's common practice to put this type of information in the file header, and for pysam to give you access to this.  I don't know the format, but have you checked the API?As others have said, you can't know the length from the iterator.This is against the very definition of an iterator, which is a pointer to an object, plus information about how to get to the next object.An iterator does not know how many more times it will be able to iterate until terminating.  This could be infinite, so infinity might be your answer.Although it's not possible in general to do what's been asked, it's still often useful to have a count of how many items were iterated over after having iterated over them. For that, you can use jaraco.itertools.Counter or similar. Here's an example using Python 3 and rwt to load the package.Presumably, you want count the number of items without iterating through, so that the iterator is not exhausted, and you use it again later. This is possible with copy or deepcopyThe output is "Finding the length did not exhaust the iterator!"Optionally (and unadvisedly), you can shadow the built-in len function as follows:

What does a . in an import statement in Python mean?

Vlad the Impala

[What does a . in an import statement in Python mean?](https://stackoverflow.com/questions/7279810/what-does-a-in-an-import-statement-in-python-mean)

I'm looking over the code for Python's multiprocessing module, and it contains this line:instead ofthe subtle difference being the period before _multiprocessing. What does that mean? Why the period?

2011-09-02 06:12:36Z

I'm looking over the code for Python's multiprocessing module, and it contains this line:instead ofthe subtle difference being the period before _multiprocessing. What does that mean? Why the period?That's the new syntax for explicit relative imports. It means import from the current package.The dot in the module name is used for relative module import (see here and here,  section 6.4.2). You can use more than one dot, referring not to the curent package but its parent(s). This should only be used within packages, in the main module one should always use absolute module names.

Best programming aids for a quadriplegic programmer

Peter Rowell

[Best programming aids for a quadriplegic programmer](https://stackoverflow.com/questions/2710537/best-programming-aids-for-a-quadriplegic-programmer)

Before you jump to conclusions, yes, this is programming related. It covers a situation that comes under the heading of, "There, but for the grace of God, go you or I." This is brand new territory for me so I'm asking for some serious help here.A young man, Honza Ripa, in a nearby town did the classic Dumb Thing two weeks after graduating from High School -- he dove into shallow water in the Russian River and had a C-4/C-5 break, sometimes called a Swimming Pool break. In a matter of seconds he went from an exceptional golfer and wrestler to a quadriplegic. (Read the story ... all of us should have been so lucky as to have a girlfriend like Brianna.) That was 10 months ago and he has regained only tiny amounts of control of his right index finger and a couple of other hand/foot motions, none of them fine-grained.His total control of his computer (currently running Win7, but we can change that as needed) is via voice command. Honza's not dumb. He had a 3.7 GPA with AP math and physics.The Problems:I've done a ton of googling on this, but I know there are things I'm missing. I'm asking the SO community to step up to the plate here. I know this group has the answers, so let me hear them! Overwhelm me with the opportunities that any of us might have/need to still program after such a life-changing event.Update: I just registered computingforquads.org and I'll be creating pages for all sorts of solutions to all sorts of problems. Thanks for you help so far and keep those answers coming!

2010-04-26 00:30:21Z

Before you jump to conclusions, yes, this is programming related. It covers a situation that comes under the heading of, "There, but for the grace of God, go you or I." This is brand new territory for me so I'm asking for some serious help here.A young man, Honza Ripa, in a nearby town did the classic Dumb Thing two weeks after graduating from High School -- he dove into shallow water in the Russian River and had a C-4/C-5 break, sometimes called a Swimming Pool break. In a matter of seconds he went from an exceptional golfer and wrestler to a quadriplegic. (Read the story ... all of us should have been so lucky as to have a girlfriend like Brianna.) That was 10 months ago and he has regained only tiny amounts of control of his right index finger and a couple of other hand/foot motions, none of them fine-grained.His total control of his computer (currently running Win7, but we can change that as needed) is via voice command. Honza's not dumb. He had a 3.7 GPA with AP math and physics.The Problems:I've done a ton of googling on this, but I know there are things I'm missing. I'm asking the SO community to step up to the plate here. I know this group has the answers, so let me hear them! Overwhelm me with the opportunities that any of us might have/need to still program after such a life-changing event.Update: I just registered computingforquads.org and I'll be creating pages for all sorts of solutions to all sorts of problems. Thanks for you help so far and keep those answers coming!I have sports injuries, and I cannot type more than few characters without serious pain.Instead, I use emacs together with Dragon NaturallySpeaking.

I have written macros and scripts to help it get by.  The system is not perfect, but it works.

I program mostly in C++, but I also use python.If you want to, I will help you with that.

I have to warn you, it takes few months to get used to speech to text software and train it.  moreover, I am not native English speaker, am sure that gets in the wayDo not despair, there are solutions.here a link to emacs and Dragon files (unfortunately have not documented yet)http://code.google.com/p/asadchev/source/browse/#svn/trunk/home/Dragonhttp://code.google.com/p/asadchev/source/browse/#svn/trunk/emacs/elispalso, if you need more info, my number is 515 230 9363 (United States, Iowa).

I will be glad to help you if I canIt's worth looking at the Dasher Project, which makes it possible to enter text reasonably quickly even for the severly disabled.  Dasher is built on a probabilistic model of languages, so that more likely utterances are easier to enter into the system.  The demonstration system comes with a fairly impressive collection of natural languages.  It should be easy to get a large corpus of programs written in Python, load Dasher with the corpus, and create a special-purpose version for entering Python programs.This isn't part of any professional software, but when I saw this, I've thought it would be good for text entry using eye movement tracking or minimal mouse movement.  See Ken Perlin's Processing page, and look at the applets for "pen input".I know someone in a village in India who is a paraplegic, who uses Dragon Speech to Text software to write on her computer. I don't know how well suited it is for a programmer (she is not a programmer), but it's a start.You might also want to look into Natural Point. It's an eye controlled mouse, which might help HonzaHope this helpsiPython with completionOn the python side, iPython shows parameters, functions, etc, and has command completion.  Perhaps it could also be customized to respond to the various input devices as well? One thing that may help (i got it from this question) is http://shorttalk-emacs.sourceforge.net/ . It seems to be an interface between emacs and speech recognition. And regarding languages, i would recommend using Lua over Python. It has a more natural English flow to it.I know this question is quite old now. I wonder how things are going for Honza with respect to programming. It would be nice to hear back.For what it's worth, I suffer from RSI and now try to minimize use of the keyboard and especially the mouse.My own experience of voice recognition is that this stuff DOES work. I use Windows's inbuilt speech recognition software for Windows 7 (WSR). I've also used voice finger (http://voicefinger.cozendey.com/) to help move the mouse pointer. Some key points I would mention are:Learn the shortcuts. You can do almost anything using shortcuts and speaking them works great using Voice Recognition when in "typing mode" (see below).Use Typing mode. Unless you are dictating text this is great for speaking short cuts to the computer or for spelling weird words. Interestingly it is not a clearly "advertised" function of WSR.Phonetic Alphabet. To make good use of typing mode learn the phonetic alphabet: http://en.wikipedia.org/wiki/NATO_phonetic_alphabet  You can't realistically get by without this when using any form of speech recognition.VIM. (or emacs I guess - not sure). Vim is a great for editing text without touching the mouse - ever. This makes it great for editing texts using WSR. I am only a VIM beginner myself but find it incredibly helpful.Web browsing. In my experience web browsing is still an extremely difficult thing to do without a mouse. There are simply too many situations which require you to hover with the mouse in order to get to the underlying commands. This is a great shame. Nevertheless there are some really good Firefox plugins to help browsing without a mouse. These are just my own personal experiences. It would be great to hear back about how Honza is getting on.

What does the「-U」option stand for in pip install -U

zakdances

[What does the「-U」option stand for in pip install -U](https://stackoverflow.com/questions/12435209/what-does-the-u-option-stand-for-in-pip-install-u)

Despite a ton of Googling, I can't find any docs for pip's command line options/arguments. What does pip install -U mean? Does anyone have a link to a list of pip's options and arguments?

2012-09-15 06:48:54Z

Despite a ton of Googling, I can't find any docs for pip's command line options/arguments. What does pip install -U mean? Does anyone have a link to a list of pip's options and arguments?Type pip install -h to list help:So, if you already have a package installed, it will upgrade the package for you. Without the -U switch it'll tell you the package is already installed and exit.Each pip subcommand has its own help listing. pip -h shows you overall help, and pip [subcommand] -h gives you help for that sub command, such as install.You can also find the full reference documentation online; the General Options section covers switches available for every pip subcommand, while each subcommand has a separate Options section to cover subcommand-specific switches; see the pip install options section, for example.

Is it feasible to compile Python to machine code?

Andy Balaam

[Is it feasible to compile Python to machine code?](https://stackoverflow.com/questions/138521/is-it-feasible-to-compile-python-to-machine-code)

How feasible would it be to compile Python (possibly via an intermediate C representation) into machine code?Presumably it would need to link to a Python runtime library, and any parts of the Python standard library which were Python themselves would need to be compiled (and linked in) too.Also, you would need to bundle the Python interpreter if you wanted to do dynamic evaluation of expressions, but perhaps a subset of Python that didn't allow this would still be useful.Would it provide any speed and/or memory usage advantages?  Presumably the startup time of the Python interpreter would be eliminated (although shared libraries would still need loading at startup).

2008-09-26 09:51:51Z

How feasible would it be to compile Python (possibly via an intermediate C representation) into machine code?Presumably it would need to link to a Python runtime library, and any parts of the Python standard library which were Python themselves would need to be compiled (and linked in) too.Also, you would need to bundle the Python interpreter if you wanted to do dynamic evaluation of expressions, but perhaps a subset of Python that didn't allow this would still be useful.Would it provide any speed and/or memory usage advantages?  Presumably the startup time of the Python interpreter would be eliminated (although shared libraries would still need loading at startup).Try ShedSkin Python-to-C++ compiler, but it is far from perfect. Also there is Psyco - Python JIT if only speedup is needed. But IMHO this is not worth the effort. For speed-critical parts of code best solution would be to write them as C/C++ extensions. As @Greg Hewgill says it, there are good reasons why this is not always possible. However, certain kinds of code (like very algorithmic code) can be turned into "real" machine code. There are several options:After that, you can use one of the existing packages (freeze, Py2exe, PyInstaller) to put everything into one binary.All in all: there is no general answer for your question. If you have Python code that is performance-critical, try to use as much builtin functionality as possible (or ask a "How do I make my Python code faster" question). If that doesn't help, try to identify the code and port it to C (or Cython) and use the extension.py2c ( http://code.google.com/p/py2c) can convert python code to c/c++

I am the solo developer of py2c.Nuitka is a Python to C++ compiler that links against libpython. It appears to be a relatively new project. The author claims a speed improvement over CPython on the pystone benchmark. PyPy is a project to reimplement Python in Python, using compilation to native code as one of the implementation strategies (others being a VM with JIT, using JVM, etc.). Their compiled C versions run slower than CPython on average but much faster for some programs.Shedskin is an experimental Python-to-C++ compiler.Pyrex is a language specially designed for writing Python extension modules. It's designed to bridge the gap between the nice, high-level, easy-to-use world of Python and the messy, low-level world of C.Pyrex is a subset of the Python language that compiles to C, done by the guy that first built list comprehensions for Python.  It was mainly developed for building wrappers but can be used in a more general context.  Cython is a more actively maintained fork of pyrex.This might seem reasonable at first glance, however there are a lot of ordinary things in Python that aren't directly mappable to to a C representation without carrying over a lot of the Python runtime support. For example, duck typing comes to mind. Many functions in Python that read input can take a file or file-like object, as long as it supports certain operations, eg. read() or readline(). If you think about what it would take to map this type of support to C, you begin to imagine exactly the sorts of things that the Python runtime system already does.There are utilities such as py2exe that will bundle a Python program and runtime into a single executable (as far as possible).Some extra references:Jython has a compiler targeting JVM bytecode. The bytecode is fully dynamic, just like the Python language itself! Very cool. (Yes, as Greg Hewgill's answer alludes, the bytecode does use the Jython runtime, and so the Jython jar file must be distributed with your app.)Psyco is a kind of just-in-time (JIT) compiler: dynamic compiler for Python, runs code 2-100 times faster, but it needs much memory.In short: it run your existing Python software much faster, with no change in your source but it doesn't compile to object code the same way a C compiler would.The answer is "Yes, it is possible". You could take Python code and attempt to compile it into the equivalent C code using the CPython API. In fact, there used to be a Python2C project that did just that, but I haven't heard about it in many years (back in the Python 1.5 days is when I last saw it.)You could attempt to translate the Python code into native C as much as possible, and fall back to the CPython API when you need actual Python features. I've been toying with that idea myself the last month or two. It is, however, an awful lot of work, and an enormous amount of Python features are very hard to translate into C: nested functions, generators, anything but simple classes with simple methods, anything involving modifying module globals from outside the module, etc, etc.This doesn't compile Python to machine code. But allows to create a shared library to call Python code.If what you are looking for is an easy way to run Python code from C without relying on execp stuff. You could generate a shared library from python code wrapped with a few calls to Python embedding API. Well the application is a shared library, an .so that you can use in many other libraries/applications.Here is a simple example which create a shared library, that you can link with a C program. The shared library executes Python code.The python file that will be executed is pythoncalledfromc.py:You can try it with python2 -c "import pythoncalledfromc; pythoncalledfromc.main('HELLO'). It will output:The shared library will be defined by the following by callpython.h:The associated callpython.c is:You can compile it with the following command:Create a file named callpythonfromc.c that contains the following:Compile it and run:This is a very basic example. It can work, but depending on the library it might be still difficult to serialize C data structures to Python and from Python to C. Things can be automated somewhat...Nuitka might be helpful.Also there is numba but they both don't aim to do what you want exactly. Generating a C header from Python code is possible, but only if you specify the how to convert the Python types to C types or can infer that information. See python astroid for a Python ast analyzer.

Feedback on using Google App Engine? [closed]

agartzke

[Feedback on using Google App Engine? [closed]](https://stackoverflow.com/questions/110186/feedback-on-using-google-app-engine)

Looking to do a very small, quick 'n dirty side project. I like the fact that the Google App Engine is running on Python with Django built right in - gives me an excuse to try that platform... but my question is this:Has anyone made use of the app engine for anything other than a toy problem? I see some good example apps out there, so I would assume this is good enough for the real deal, but wanted to get some feedback.Any other success/failure notes would be great.

2008-09-21 03:41:19Z

Looking to do a very small, quick 'n dirty side project. I like the fact that the Google App Engine is running on Python with Django built right in - gives me an excuse to try that platform... but my question is this:Has anyone made use of the app engine for anything other than a toy problem? I see some good example apps out there, so I would assume this is good enough for the real deal, but wanted to get some feedback.Any other success/failure notes would be great.I have tried app engine for my small quake watch application

http://quakewatch.appspot.com/My purpose was to see the capabilities of app engine, so here are the main points:But overall I think it is excellent for creating apps which do not need lot of background processing.Edit:

Now task queues can be used for running batch processing or scheduled tasksEdit:

after working/creating a real application on GAE for a year, now my opnion is that unless you are making a application which needs to scale to million and million of users, don't use GAE. Maintaining and doing trivial tasks in GAE is a headache due to distributed nature, to avoid deadline exceeded errors, count entities or do complex queries requires complex code, so small complex application should stick to LAMP.Edit:

Models should be specially designed considering all the transactions you wish to have in future, because entities only in same entity group can be used in a transaction and it makes the process of updating two different groups a nightmare e.g. transfer money from user1 to user2 in transaction is impossible unless they are in same entity group, but making them same entity group may not be best for frequent update purposes....

read this http://blog.notdot.net/2009/9/Distributed-Transactions-on-App-EngineI am using GAE to host several high-traffic applications.  Like on the order of 50-100 req/sec.  It is great, I can't recommend it enough.My previous experience with web development was with Ruby (Rails/Merb).  Learning Python was easy.  I didn't mess with Django or Pylons or any other framework, just started from the GAE examples and built what I needed out of the basic webapp libraries that are provided.If you're used to the flexibility of SQL the datastore can take some getting used to.  Nothing too traumatic! The biggest adjustment is moving away from JOINs. You have to shed the idea that normalizing is crucial.BenOne of the compelling reasons I have come across for using Google App Engine is its integration with Google Apps for your domain. Essentially it allows you to create custom, managed web applications that are restricted to the (controlled) logins of your domain. Most of my experience with this code was building a simple time/task tracking application. The template engine was simple and yet made a multi-page application very approachable. The login/user awareness api is similarly useful. I was able to make a public page/private page paradigm without too much issue. (a user would log in to see the private pages. An anonymous user was only shown the public page.)I was just getting into the datastore portion of the project when I got pulled away for "real work".I was able to accomplish a lot (it still is not done yet) in a very little amount of time. Since I had never used Python before, this was particularly pleasant (both because it was a new language for me, and also because the development was still fast despite the new language). I ran into very little that led me to believe that I wouldn't be able to accomplish my task. Instead I have a fairly positive impression of the functionality and features. That is my experience with it. Perhaps it doesn't represent more than an unfinished toy project, but it does represent an informed trial of the platform, and I hope that helps.The "App Engine running Django" idea is a bit misleading. App Engine replaces the entire Django model layer so be prepared to spend some time getting acclimated with App Engine's datastore which requires a different way of modeling and thinking about data. I used GAE to build http://www.muspy.comIt's a bit more than a toy project but not overly complex either. I still depend on a few issues to be addressed by Google, but overall developing the website was an enjoyable experience.If you don't want to deal with hosting issues, server administration, etc, I can definitely recommend it. Especially if you already know Python and Django.I think App Engine is pretty cool for small projects at this point. There's a lot to be said for never having to worry about hosting. The API also pushes you in the direction of building scalable apps, which is good practice.This question has been fully answered. Which is good. 

But one thing perhaps is worth mentioning.

The google app engine has a plugin for the eclipse ide which is a joy to work with. If you already do your development with eclipse you are going to be so happy about that. To deploy on the google app engine's web site all I need to do is click one little button - with the airplane logo - super. Take a look the the sql game, it is very stable and actually pushed traffic limits at one point so that it was getting throttled by Google.  I have seen nothing but good news about App Engine, other than hosting you app on servers someone else controls completely.I used GAE to build a simple application which accepts some parameters, formats and send email. It was extremely simple and fast. I also made some performance benchmarks on the GAE datastore and memcache services (http://dbaspects.blogspot.com/2010/01/memcache-vs-datastore-on-google-app.html ). It is not that fast. My opinion is that GAE is serious platform which enforce certain methodology. I think it will evolve to the truly scalable platform, where bad practices simply not allowed.I used GAE for my flash gaming site, Bearded Games. GAE is a great platform. I used Django templates which are so much easier than the old days of PHP. It comes with a great admin panel, and gives you really good logs. The datastore is different than a database like MySQL, but it's much easier to work with. Building the site was easy and straightforward and they have lots of helpful advice on the site.I used GAE and Django to build a Facebook application. I used http://code.google.com/p/app-engine-patch as my starting point as it has Django 1.1 support. I didn't try to use any of the manage.py commands because I assumed they wouldn't work, but I didn't even look into it. The application had three models and also used pyfacebook, but that was the extent of the complexity. I'm in the process of building a much more complicated application which I'm starting to blog about on http://brianyamabe.com.

Handling very large numbers in Python

Yes - that Jake.

[Handling very large numbers in Python](https://stackoverflow.com/questions/538551/handling-very-large-numbers-in-python)

I've been considering fast poker hand evaluation in Python. It occurred to me that one way to speed the process up would be to represent all the card faces and suits as prime numbers and multiply them together to represent the hands. To whit:ANDThis would give each hand a numeric value that, through modulo could tell me how many kings are in the hand or how many hearts. For example, any hand with five or more clubs in it would divide evenly by 2^5; any hand with four kings would divide evenly by 59^4, etc.The problem is that a seven-card hand like AcAdAhAsKdKhKs has a hash value of approximately 62.7 quadrillion, which would take considerably more than 32 bits to represent internally. Is there a way to store such large numbers in Python that will allow me to perform arithmetic operations on it?

2009-02-11 20:13:45Z

I've been considering fast poker hand evaluation in Python. It occurred to me that one way to speed the process up would be to represent all the card faces and suits as prime numbers and multiply them together to represent the hands. To whit:ANDThis would give each hand a numeric value that, through modulo could tell me how many kings are in the hand or how many hearts. For example, any hand with five or more clubs in it would divide evenly by 2^5; any hand with four kings would divide evenly by 59^4, etc.The problem is that a seven-card hand like AcAdAhAsKdKhKs has a hash value of approximately 62.7 quadrillion, which would take considerably more than 32 bits to represent internally. Is there a way to store such large numbers in Python that will allow me to perform arithmetic operations on it?Python supports a "bignum" integer type which can work with arbitrarily large numbers. In Python 2.5+, this type is called long and is separate from the int type, but the interpreter will automatically use whichever is more appropriate. In Python 3.0+, the int type has been dropped completely.That's just an implementation detail, though — as long as you have version 2.5 or better, just perform standard math operations and any number which exceeds the boundaries of 32-bit math will be automatically (and transparently) converted to a bignum.You can find all the gory details in PEP 0237.python supports arbitrarily large integers naturally:example:You could even get, for example of a huge integer value, fib(4000000).But still it does not (for now) supports an arbitrarily large float !!If you need one big, large, float then check up on the decimal Module. There are examples of use on these foruns: OverflowError: (34, 'Result too large')Another reference: http://docs.python.org/2/library/decimal.htmlYou can even using the gmpy module if you need a speed-up (which is likely to be of your interest): Handling big numbers in codeAnother reference: https://code.google.com/p/gmpy/You could do this for the fun of it, but other than that it's not a good idea.  It would not speed up anything I can think of.python supports arbitrarily large integers naturally:The python interpreter will handle it for you, you just have to do your operations (+, -, *, /), and it will work as normal.The int value is unlimited.Careful when doing division, by default the quotient is turned into float, but float does not support such large numbers. If you get an error message saying float does not support such large numbers, then it means the quotient is too large to be stored in float you’ll have to use floor division (//). It ignores any decimal that comes after the decimal point, this way, the result will be int, so you can have a large number result.10//3 Outputs 310//4 outputs 2

Why is「if not someobj:」better than「if someobj == None:」in Python?

Sylvain Defresne

[Why is「if not someobj:」better than「if someobj == None:」in Python?](https://stackoverflow.com/questions/100732/why-is-if-not-someobj-better-than-if-someobj-none-in-python)

I've seen several examples of code like this:But I'm wondering why not doing:Is there any difference? Does one have an advantage over the other?

2008-09-19 09:38:10Z

I've seen several examples of code like this:But I'm wondering why not doing:Is there any difference? Does one have an advantage over the other?In the first test, Python try to convert the object to a bool value if it is not already one. Roughly, we are asking the object : are you meaningful or not ? This is done using the following algorithm :In the second test, the object is compared for equality to None. Here, we are asking the object, "Are you equal to this other value?" This is done using the following algorithm :There is another test possible using the is operator. We would be asking the object, "Are you this particular object?"Generally, I would recommend to use the first test with non-numerical values, to use the test for equality when you want to compare objects of the same nature (two strings, two numbers, ...) and to check for identity only when using sentinel values (None meaning not initialized for a member field for exemple, or when using the getattr or the __getitem__ methods).To summarize, we have :These are actually both poor practices.  Once upon a time, it was considered OK to casually treat None and False as similar.   However, since Python 2.2 this is not the best policy.First, when you do an if x or if not x kind of test, Python has to implicitly convert x to boolean.  The rules for the bool function describe a raft of things which are False; everything else is True.  If the value of x wasn't properly boolean to begin with, this implicit conversion isn't really the clearest way to say things.  Before Python 2.2, there was no bool function, so it was even less clear.Second, you shouldn't really test with == None.  You should use is None and is not None.See PEP 8, Style Guide for Python Code.  How many singletons are there?  Five: None, True, False, NotImplemented and Ellipsis.  Since you're really unlikely to use NotImplemented or Ellipsis, and you would never say if x is True (because simply if x is a lot clearer), you'll only ever test None.Because None is not the only thing that is considered false.False, 0, (), [], {} and "" are all different from None, so your two code snippets are not equivalent.Moreover, consider the following:if object: is not an equality check. 0, (), [], None, {}, etc. are all different from each other, but they all evaluate to False.This is the "magic" behind short circuiting expressions like:which is shorthand for:although you really should write:If you askthe __nonzero__ method of spam gets called. From the Python manual:If you askthe __eq__ method of spam gets called with the argument None.For more information of the customization possibilities have a look at the Python documenation at https://docs.python.org/reference/datamodel.html#basic-customizationPEP 8 -- Style Guide for Python Code recommends to use is or is not if you are testing for None-nessOn the other hand if you are testing for more than None-ness, you should use the boolean operator.These two comparisons serve different purposes. The former checks for boolean value of something, the second checks for identity with None value.For one the first example is shorter and looks nicer. As per the other posts what you choose also depends on what you really want to do with the comparison.The answer is "it depends".I use the first example if I consider 0, "", [] and False (list not exhaustive) to be equivalent to None in this context.Personally, I chose a consistent approach across languages: I do if (var) (or equivalent) only if var is declared as boolean (or defined as such, in C we don't have a specific type). I even prefix these variables with a b (so it would be bVar actually) to be sure I won't accidentally use another type here.

I don't really like implicit casting to boolean, even less when there are numerous, complex rules.Of course, people will disagree. Some go farther, I see if (bVar == true) in the Java code at my work (too redundant for my taste!), others love too much compact syntax, going while (line = getNextLine()) (too ambiguous for me).

How to get the parent dir location

zjm1126

[How to get the parent dir location](https://stackoverflow.com/questions/2817264/how-to-get-the-parent-dir-location)

this code is get the templates/blog1/page.html  in b.py:but i want to get the parent dir location:and  how to get the aParent locationthanksupdated:this is right:or 

2010-05-12 08:57:45Z

this code is get the templates/blog1/page.html  in b.py:but i want to get the parent dir location:and  how to get the aParent locationthanksupdated:this is right:or You can apply dirname repeatedly to climb higher: dirname(dirname(file)). This can only go as far as the root package, however. If this is a problem, use os.path.abspath: dirname(dirname(abspath(file))).os.path.abspath doesn't validate anything, so if we're already appending strings to __file__ there's no need to bother with dirname or joining or any of that. Just treat __file__ as a directory and start climbing:That's far less convoluted than os.path.abspath(os.path.join(os.path.dirname(__file__),"..")) and about as manageable as dirname(dirname(__file__)). Climbing more than two levels starts to get ridiculous.But, since we know how many levels to climb, we could clean this up with a simple little function:Use relative path with the pathlib module in Python 3.4+:You can use multiple calls to parent to go further in the path:As an alternative to specifying parent twice, you can use:Should give you the path to a.But if b.py is the file that is currently executed, then you can achieve the same by just doingos.pardir is a better way for ../ and more readable. This will return the parent path of the given_pathA simple way can be:May be join two .. folder, to get parent of the parent folder?Use the following to jump to previous folder:If you need multiple jumps a good and easy solution will be to use a simple decorator in this case.Here is another relatively simple solution that:it just uses normpath and join:Result:I think use this is better:I tried: 

n-grams in python, four, five, six grams?

Shifu

[n-grams in python, four, five, six grams?](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)

I'm looking for a way to split a text into n-grams.

Normally I would do something like:I am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams?Thanks!

2013-07-08 16:35:31Z

I'm looking for a way to split a text into n-grams.

Normally I would do something like:I am aware that nltk only offers bigrams and trigrams, but is there a way to split my text in four-grams, five-grams or even hundred-grams?Thanks!Great native python based answers given by other users. But here's the nltk approach (just in case, the OP gets penalized for reinventing what's already existing in the nltk library). There is an ngram module that people seldom use in nltk. It's not because it's hard to read ngrams, but training a model base on ngrams where n > 3 will result in much data sparsity.I'm surprised that this hasn't shown up yet:here is another simple way for do n-grams Using only nltk toolsExample outputIn order to keep the ngrams in array format just remove ' '.join  People have already answered pretty nicely for the scenario where you need bigrams or trigrams but if you need everygram for the sentence in that case you can use nltk.util.everygrams Incase you have a limit like in case of trigrams where the max length should be 3 then you can use max_len param to specify it.You can just modify the max_len param to achieve whatever gram i.e four gram, five gram, six or even hundred gram.The previous mentioned solutions can be modified to implement the above mentioned solution but this solution is much straight forward than that.For further reading click hereAnd when you just need a specific gram like bigram or trigram etc you can use the nltk.util.ngrams as mentioned in M.A.Hassan's answer.You can easily whip up your own function to do this using itertools: A more elegant approach to build bigrams with python’s builtin zip(). 

Simply convert the original string into a list by split(), then pass the list once normally and once offset by one element. I have never dealt with nltk but did N-grams as part of some small class project. If you want to find the frequency of all N-grams occurring in the string, here is a way to do that. D would give you the histogram of your N-words.For four_grams it is already in NLTK, here is a piece of code that can help you toward this:I hope it helps.If efficiency is an issue and you have to build multiple different n-grams (up to a hundred as you say), but you want to use pure python I would do:    Usage : ~Same speed as NLTK:Repost from my previous answer.You can use sklearn.feature_extraction.text.CountVectorizer:outputs:You can set to ngram_size to any positive integer. I.e. you can split a text in four-grams, five-grams or even hundred-grams.Nltk is great, but sometimes is a overhead for some projects:Example use:You can get all 4-6gram using the code without other package below:the output is below:you can find more detail on this blogAfter about seven years, here's a more elegant answer using collections.deque:Output:

What are the drawbacks of Stackless Python? [closed]

Ryszard Szopa

[What are the drawbacks of Stackless Python? [closed]](https://stackoverflow.com/questions/588958/what-are-the-drawbacks-of-stackless-python)

I've been reading recently about Stackless Python and it seems to have many advantages compared with vanilla cPython. It has all those cool features like infinite recursion, microthreads, continuations, etc. and at the same time is faster than cPython (around 10%, if the Python wiki is to be believed) and compatible with it (at least versions 2.5, 2.6 and 3.0).All these looks almost too good to be true. However, TANSTAAFL, I don't see much enthusiasm for Stackless among the Python community, and PEP 219 has never come into realization. Why is that? What are the drawbacks of Stackless? What skeletons are hidden in Stackless' closet?(I know Stackless doesn't offer real concurrency, just an easier way of programming in the concurrent way. It doesn't really bother me.)

2009-02-26 03:17:18Z

I've been reading recently about Stackless Python and it seems to have many advantages compared with vanilla cPython. It has all those cool features like infinite recursion, microthreads, continuations, etc. and at the same time is faster than cPython (around 10%, if the Python wiki is to be believed) and compatible with it (at least versions 2.5, 2.6 and 3.0).All these looks almost too good to be true. However, TANSTAAFL, I don't see much enthusiasm for Stackless among the Python community, and PEP 219 has never come into realization. Why is that? What are the drawbacks of Stackless? What skeletons are hidden in Stackless' closet?(I know Stackless doesn't offer real concurrency, just an easier way of programming in the concurrent way. It doesn't really bother me.)I don't know where that "Stackless is 10% faster" on the Wiki came from, but then again I've never tried to measure those performance numbers. I can't think of what Stackless does to make a difference that big.Stackless is an amazing tool with several organizational/political problems.The first comes from history. Christian Tismer started talking about what eventually became Stackless about 10 years ago. He had an idea of what he wanted, but had a hard time explaining what he was doing and why people should use it. This is partially because his background didn't have the CS training regarding ideas like coroutines and because his presentations and discussion are very implementation oriented, which is hard for anyone not already hip-deep in continuations to understand how to use it as a solution to their problems.For that reason, the initial documentation was poor. There were some descriptions of how to use it, with the best from third-party contributors. At PyCon 2007 I gave a talk on "Using Stackless" which went over quite well, according to the PyCon survey numbers. Richard Tew has done a great job collecting these, updating stackless.com, and maintaining the distribution when new Python releases comes up. He's an employee of CCP Games, developers of EVE Online, which uses Stackless as an essential part of their gaming system.CCP games is also the biggest real-world example people use when they talk about Stackless. The main tutorial for Stackless is Grant Olson's "Introduction to Concurrent Programming with Stackless Python", which is also game oriented. I think this gives people a skewed idea that Stackless is games-oriented, when it's more that games are more easily continuation oriented.Another difficulty has been the source code. In its original form it required changes to many parts of Python, which made Guido van Rossum, the Python lead, wary. Part of the reason, I think, was support for call/cc that was later removed as being "too much like supporting a goto when there are better higher-level forms." I'm not certain about this history, so just read this paragraph as "Stackless used to require too many changes."Later releases didn't require the changes, and Tismer continued to push for its inclusion in Python. While there was some consideration, the official stance (as far as I know) is that CPython is not only a Python implementation but it's meant as a reference implementation, and it won't include Stackless functionality because it can't be implemented by Jython or Iron Python.There are absolutely no plans for "significant changes to the code base". That quote and  reference hyperlink from Arafangion's (see the comment) are from roughly 2000/2001. The structural changes have long been done, and it's what I mentioned above. Stackless as it is now is stable and mature, with only minor tweaks to the code base over the last several years. One final limitation with Stackless - there is no strong advocate for Stackless. Tismer is now deeply involved with PyPy, which is an implementation of Python for Python. He has implemented the Stackless functionality in PyPy and considers it much superior to Stackless itself, and feels that PyPy is the way of the future. Tew maintains Stackless but he isn't interested in advocacy. I considered being in that role, but couldn't see how I could make an income from it.Though if you want training in Stackless, feel free to contact me! :)it took quite long to find this discussion. At that

time I was not on PyPy but had a 2-years affair with psyco, until health stopped this all quite abruptly. I'm now active again and designing an

alternative approach - will present it on EuroPython 2012.Most of Andrews statements are correct. Some

minor additions:Stackless was significantly faster than CPython, 10 years ago, because I optimized the interpreter loop. At that time, Guido was not ready for that. A few years later, people did similar optimizations and even more and better ones, which makes Stackless a little bit slower, as expected. On inclusion: well, in the beginning I was very pushy and convinced that Stackless is the way to go. Later, when it was almost possible to get included, I lost interest in that and preferred to let

it stay this way, partially out of frustration, partially to

keep control of Stackless. The arguments like "other implementations cannot do it" felt always lame to me, as there are other examples where this argument could also be used. I thought I better forget about that and stay in good friendship with Guido, having my own distro.Meanwhile things are changing again. I'm working on PyPy and Stackless as an extension  Will talk about that sometimes later Cheers -- ChrisIf I recall correctly, Stackless was slated for inclusion into the official CPython, but the author of stackless told the CPython folks not to do so, because he planned to do some significant changes to the code base - presumeably he wanted the integration done later when the project was more mature.I'm also interested in the answers here. I've played a bit with Stackless and it looks like it would be a good solid addition to standard Python.PEP 219 does mention potential difficulties with calling Python code from C code, if Python wants to change to a different stack. There would need to be ways to detect and prevent this (to avoid trashing the C stack). I think this is tractable though, so I'm also wondering why Stackless must stand on its own.

How to identify whether a file is normal file or directory using python

PTBNL

[How to identify whether a file is normal file or directory using python](https://stackoverflow.com/questions/955941/how-to-identify-whether-a-file-is-normal-file-or-directory-using-python)

How do you check whether a file is a normal file or a directory using python?

2009-06-05 13:47:31Z

How do you check whether a file is a normal file or a directory using python?os.path.isdir() and os.path.isfile() should give you what you want.  See: 

http://docs.python.org/library/os.path.htmlAs other answers have said, os.path.isdir() and os.path.isfile() are what you want.  However, you need to keep in mind that these are not the only two cases.  Use os.path.islink() for symlinks for instance.  Furthermore, these all return False if the file does not exist, so you'll probably want to check with os.path.exists() as well.Python 3.4 introduced the pathlib module into the standard library, which provides an object oriented approach to handle filesystem paths. The relavant methods would be .is_file() and .is_dir():Pathlib is also available on Python 2.7 via the pathlib2 module on PyPi.try this:If you're just stepping through a set of directories you might be better just to try os.chdir and give an error/warning if it fails:

How to add a constant column in a Spark DataFrame?

Evan Zamir

[How to add a constant column in a Spark DataFrame?](https://stackoverflow.com/questions/32788322/how-to-add-a-constant-column-in-a-spark-dataframe)

I want to add a column in a DataFrame with some arbitrary value (that is the same for each row). I get an error when I use withColumn as follows:It seems that I can trick the function into working as I want by adding and subtracting one of the other columns (so they add to zero) and then adding the number I want (10 in this case):This is supremely hacky, right? I assume there is a more legit way to do this?

2015-09-25 18:17:14Z

I want to add a column in a DataFrame with some arbitrary value (that is the same for each row). I get an error when I use withColumn as follows:It seems that I can trick the function into working as I want by adding and subtracting one of the other columns (so they add to zero) and then adding the number I want (10 in this case):This is supremely hacky, right? I assume there is a more legit way to do this?Spark 2.2+Spark 2.2 introduces typedLit to support  Seq, Map, and Tuples (SPARK-19254) and following calls should be supported (Scala):Spark 1.3+ (lit), 1.4+ (array, struct), 2.0+ (map):The second argument for DataFrame.withColumn should be a Column so you have to use a literal:If you need complex columns you can build these using blocks like array:Exactly the same methods can be used in Scala.To provide names for structs use either alias on each field:or cast on the whole objectIt is also possible, although slower, to use an UDF.Note:The same constructs can be used to pass constant arguments to UDFs or SQL functions.In spark 2.2 there are two ways to add constant value in a column in DataFrame: 1) Using lit 2) Using typedLit. The difference between the two is that typedLit can also handle parameterized scala types e.g. List, Seq, and MapSample DataFrame:1) Using lit: Adding constant string value in new column named newcol:Result:2) Using typedLit:Result:

Shell Script: Execute a python program from within a shell script

Harpal

[Shell Script: Execute a python program from within a shell script](https://stackoverflow.com/questions/4377109/shell-script-execute-a-python-program-from-within-a-shell-script)

I've tried googling the answer but with no luck.I need to use my works supercomputer server, but for my python script to run, it must be executed via a shell script.For example I want job.sh to execute python_script.pyHow can this be accomplished?

2010-12-07 13:30:53Z

I've tried googling the answer but with no luck.I need to use my works supercomputer server, but for my python script to run, it must be executed via a shell script.For example I want job.sh to execute python_script.pyHow can this be accomplished?Just make sure the python executable is in your PATH environment variable then add in your scriptDetails:Suppose you have a python file hello.py

Create a file called job.sh that containsmark it executable usingthen run itModify your script hello.py and add this as the first linemark it executable usingthen run itImho, writing Is quite wrong, especially in these days. Which python? python2.6? 2.7? 3.0? 3.1? Most of times you need to specify the python version in shebang tag of python file. I encourage to use In such case, is much better to have the script executable and invoke it directly:This way the version of python you need is only written in one file. Most of system these days are having python2 and python3 in the meantime, and it happens that the symlink python points to python3, while most people expect it pointing to python2.Save the following program as print.py:Then in the terminal type:you should be able to invoke it as python scriptname.pyeg also make sure the script has permissions to runyou can make it executable by using chmod u+x scriptname.pyEDIT: beaten to it :-pThis works best for me:

Add this at the top of the script:(C:\Python27\python.exe is the path to the python.exe on my machine)

Then run the script via:This works for me: Done with python_file.pyDone with python_file1.pyI use this usually when I have to run multiple python files with different arguments, pre defined. Note: Just a quick heads up on what's going on here: I use this and it works fineSince the other posts say everything (and I stumbled upon this post while looking for the following). 

Here is a way how to execute a python script from another python script:Python 2:Python 3:and you can supply args by providing some other sys.argv

How to add an integer to each element in a list?

Ned Batchelder

[How to add an integer to each element in a list?](https://stackoverflow.com/questions/9304408/how-to-add-an-integer-to-each-element-in-a-list)

If I have list=[1,2,3] and I want to add 1 to each element to get the output [2,3,4],

how would I do that?I assume I would use a for loop but not sure exactly how.

2012-02-16 01:52:43Z

If I have list=[1,2,3] and I want to add 1 to each element to get the output [2,3,4],

how would I do that?I assume I would use a for loop but not sure exactly how.list-comprehensions python.The other answers on list comprehension are probably the best bet for simple addition, but if you have a more complex function that you needed to apply to all the elements then map may be a good fit.In your example it would be:if you want to use numpy there is another method as followsEdit: this isn't in-placeFirstly don't use the word 'list' for your variable. It shadows the keyword list.The best way is to do it in place using splicing, note the [:] denotes a splice:My intention here is to expose if the item in the list is an integer it supports various built-in functions. Python 2+:Python 3+:Came across a not so efficient, but unique way of doing it. So sharing it across.And yes it requires extra space for another list.

Difference between two dates in Python

mauguerra

[Difference between two dates in Python](https://stackoverflow.com/questions/8419564/difference-between-two-dates-in-python)

I have two different dates and I want to know the difference in days between them. The format of the date is YYYY-MM-DD.I have a function that can ADD or SUBTRACT a given number to a date:where A is the date and x the number of days I want to add. And the result is another date.I need a function where I can give two dates and the result would be an int with date difference in days.

2011-12-07 17:17:20Z

I have two different dates and I want to know the difference in days between them. The format of the date is YYYY-MM-DD.I have a function that can ADD or SUBTRACT a given number to a date:where A is the date and x the number of days I want to add. And the result is another date.I need a function where I can give two dates and the result would be an int with date difference in days.Use - to get the difference between two datetime objects and take the days member.Another short solution:I tried the code posted by larsmans above but, there are a couple of problems:1) The code as is will throw the error as mentioned by mauguerra

2) If you change the code to the following:This will convert your datetime objects to strings but, two things1) Trying to do d2 - d1 will fail as you cannot use the minus operator on strings and

2) If you read the first line of the above answer it stated, you want to use the - operator on two datetime objects but, you just converted them to stringsWhat I found is that you literally only need the following:Try this:pd.date_range('2019-01-01', '2019-02-01').shape[0]

Days between two dates? [duplicate]

Bemmu

[Days between two dates? [duplicate]](https://stackoverflow.com/questions/8258432/days-between-two-dates)

What's the shortest way to see how many full days have passed between two dates?

Here's what I'm doing now.

2011-11-24 14:15:06Z

What's the shortest way to see how many full days have passed between two dates?

Here's what I'm doing now.Assuming you’ve literally got two date objects, you can subtract one from the other and query the resulting timedelta object for the number of days:And it works with datetimes too — I think it rounds down to the nearest day:Do you mean full calendar days, or groups of 24 hours?For simply 24 hours, assuming you're using Python's datetime, then the timedelta object already has a days property:For calendar days, you'll need to round a down to the nearest day, and b up to the nearest day, getting rid of the partial day on either side:Try:I tried with b and a of type datetime.date.Referencing my comments on other answers. This is how I would work out the difference in days based on 24 hours and calender days. the days attribute works well for 24 hours and the function works best for calendar checks.

Remove all values within one list from another list? [duplicate]

ariel

[Remove all values within one list from another list? [duplicate]](https://stackoverflow.com/questions/2514961/remove-all-values-within-one-list-from-another-list)

I am looking for a way to remove all values within a list from another list.Something like this:

2010-03-25 11:20:00Z

I am looking for a way to remove all values within a list from another list.Something like this:I was looking for fast way to do the subject, so I made some experiments with suggested ways. And I was surprised by results, so I want to share it with you.Experiments were done using pythonbenchmark tool and with Results:5 tries, average time 12.8 sec5 tries, average time 12.6 sec5 tries, average time 0.27 sec5 tries, average time 0.0057 secAlso I made another measurement with bigger inputs size for the last two functionsAnd the results:For modification (remove method) - average time is 252 seconds

For set approach - average time is 0.75 secondsSo you can see that approach with sets is significantly faster than others. Yes, it doesn't keep similar items, but if you don't need it  - it's for you.

And there is almost no difference between list comprehension and using filter function. Using 'remove' is ~50 times faster, but it modifies source list.

And the best choice is using sets - it's more than 1000 times faster than list comprehension!If you don't have repeated values, you could use set difference.and then convert back to list, if needed.orDon't create the set inside the lambda or inside the comprehension. If you do, it'll be recreated on every iteration, defeating the point of using a set at all.The simplest way isOne possible problem here is that each time you call remove(), all the items are shuffled down the list to fill the hole. So if a grows very large this will end up being quite slow.This way builds a brand new list. The advantage is that we avoid all the shuffling of the first approachIf you want to modify a in place, just one small change is requiredOthers have suggested ways to make newlist after filtering e.g.orbut from your question it looks you want in-place modification for that you can do this, this will also be much much faster if original list is long and items to be removed lessoutput:

    [1, 4, 5, 6, 8, 9]I am checking for ValueError exception so it works even if items are not in orginal list.Also if you do not need in-place modification solution by S.Mark is simpler.

From ND to 1D arrays

Amelio Vazquez-Reina

[From ND to 1D arrays](https://stackoverflow.com/questions/13730468/from-nd-to-1d-arrays)

Say I have an array a:I would like to convert it to a 1D array (i.e. a column vector):but this returnswhich is not the same as:I can take the first element of this array to manually convert it to a 1D array:but this requires me to know how many dimensions the original array has (and concatenate [0]'s when working with higher dimensions)Is there a dimensions-independent way of getting a column/row vector from an arbitrary ndarray?

2012-12-05 18:59:13Z

Say I have an array a:I would like to convert it to a 1D array (i.e. a column vector):but this returnswhich is not the same as:I can take the first element of this array to manually convert it to a 1D array:but this requires me to know how many dimensions the original array has (and concatenate [0]'s when working with higher dimensions)Is there a dimensions-independent way of getting a column/row vector from an arbitrary ndarray?Use np.ravel (for a 1D view) or np.flatten (for a 1D copy) or np.flat (for an 1D iterator):Note that ravel() returns a view of a when possible. So modifying b also modifies a. ravel() returns a view when the 1D elements are contiguous in memory, but would return a copy if, for example, a were made from slicing another array using a non-unit step size (e.g. a = x[::2]).If you want a copy rather than a view, useIf you just want an iterator, use np.flat:or, simply:One of the simplest way is to use flatten(), like this example :My array it was like this : After using flatten(): It's also the solution of errors of this type :[1 2 3 4 5 6 7 8]I wanted to see a benchmark result of functions mentioned in answers including unutbu's.Also want to point out that numpy doc recommend to use arr.reshape(-1) in case view is preferable. (even though ravel is tad faster in the following result)Functions:numpy version: '1.18.0'Although this isn't using the np array format, (to lazy to modify my code) this should do what you want...  If, you truly want a column vector you will want to transpose the vector result.  It all depends on how you are planning to use this.So if you need to transpose, you can do something like this:

Is it possible to sort two lists(which reference each other) in the exact same way?

Error_404

[Is it possible to sort two lists(which reference each other) in the exact same way?](https://stackoverflow.com/questions/9764298/is-it-possible-to-sort-two-listswhich-reference-each-other-in-the-exact-same-w)

Okay, this may not be the smartest idea but I was a bit curious if this is possible.  Say I have two lists:If I run list1.sort(), it'll sort it to [1,1,2,3,4] but is there a way to get to keep list2 in sync as well(so I can say item 4 belongs to 'three')?  My problem is I have a pretty complex program that is working fine with lists but I sort of need to start referencing some data. I know this is a perfect situation for dictionaries but I'm trying to avoid dictionaries in my processing because I do need to sort the key values(if I must use dictionaries I know how to use them).Basically the nature of this program is, the data comes in a random order(like above), I need to sort it, process it and then send out the results(order doesn't matter but users need to know which result belongs to which key). I thought about putting it in a dictionary first, then sorting list one but I would have no way of differentiating of items in the with the same value if order is not maintained(it may have an impact when communicating the results to users).  So ideally, once I get the lists I would rather figure out a way to sort both lists together.  Is this possible?

2012-03-19 02:35:10Z

Okay, this may not be the smartest idea but I was a bit curious if this is possible.  Say I have two lists:If I run list1.sort(), it'll sort it to [1,1,2,3,4] but is there a way to get to keep list2 in sync as well(so I can say item 4 belongs to 'three')?  My problem is I have a pretty complex program that is working fine with lists but I sort of need to start referencing some data. I know this is a perfect situation for dictionaries but I'm trying to avoid dictionaries in my processing because I do need to sort the key values(if I must use dictionaries I know how to use them).Basically the nature of this program is, the data comes in a random order(like above), I need to sort it, process it and then send out the results(order doesn't matter but users need to know which result belongs to which key). I thought about putting it in a dictionary first, then sorting list one but I would have no way of differentiating of items in the with the same value if order is not maintained(it may have an impact when communicating the results to users).  So ideally, once I get the lists I would rather figure out a way to sort both lists together.  Is this possible?One classic approach to this problem is to use the "decorate, sort, undecorate" idiom, which is especially simple using python's built-in zip function:These of course are no longer lists, but that's easily remedied, if it matters:It's worth noting that the above may sacrifice speed for terseness; the in-place version, which takes up 3 lines, is a tad faster on my machine for small lists:On the other hand, for larger lists, the one-line version could be faster:As Quantum7 points out, JSF's suggestion is a bit faster still, but it will probably only ever be a little bit faster, because Python uses the very same DSU idiom internally for all key-based sorts. It's just happening a little closer to the bare metal. (This shows just how well optimized the zip routines are!)I think the zip-based approach is more flexible and is a little more readable, so I prefer it.You can sort indexes using values as keys:To get sorted lists given sorted indexes:In your case you shouldn't have list1, list2 but rather a single list of pairs:It is easy to create; it is easy to sort in Python:Sort by the first value only:I have used the answer given by senderle for a long time until I discovered np.argsort.

Here is how it works.I find this solution more intuitive, and it works really well. The perfomance:Even though np.argsort isn't the fastest one, I find it easier to use.Schwartzian transform. The built-in Python sorting is stable, so the two 1s don't cause a problem.What about:You can use the key argument in sorted() method unless you have two same values in list2.The code is given below:It sorts list2 according to corresponding values in list1, but make sure that while using this, no two values in list2 evaluate to be equal because list.index() function give the first valueOne way is to track where each index goes to by sorting the identity [0,1,2,..n]This works for any number of lists.Then move each item to its position. Using splices is best.Note we could have iterated the lists without even sorting them:You can use the zip() and sort() functions to accomplish this:Hope this helpsan algorithmic solution:Outputs: -> Output speed: 0.2sAnother approach to retaining the order of a string list when sorting against another list is as follows:outputI would like to expand open jfs's answer, which worked great for my problem: sorting two lists by a third, decorated list:We can create our decorated list in any way, but in this case we will create it from the elements of one of the two original lists, that we want to sort:Now we can apply jfs's solution to sort our two lists by the thirdEdit: Hey guys I made a block post about this, check it out if you feel like it :)  🐍🐍🐍If you are using numpy you can use np.argsort to get the sorted indices and apply those indices to the list. This works for any number of list that you would want to sort.

Nested defaultdict of defaultdict

Corley Brigman

[Nested defaultdict of defaultdict](https://stackoverflow.com/questions/19189274/nested-defaultdict-of-defaultdict)

Is there a way to make a defaultdict also be the default for the defaultdict? (i.e. infinite-level recursive defaultdict?)I want to be able to do:So, I can do x = defaultdict(defaultdict), but that's only a second level:There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?Note this is asking how to do an infinite-level recursive defaultdict, so it's distinct to Python: defaultdict of defaultdict?, which was how to do a two-level defaultdict.I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.

2013-10-04 19:28:50Z

Is there a way to make a defaultdict also be the default for the defaultdict? (i.e. infinite-level recursive defaultdict?)I want to be able to do:So, I can do x = defaultdict(defaultdict), but that's only a second level:There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?Note this is asking how to do an infinite-level recursive defaultdict, so it's distinct to Python: defaultdict of defaultdict?, which was how to do a two-level defaultdict.I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.For an arbitrary number of levels:Of course you could also do this with a lambda, but I find lambdas to be less readable.  In any case it would look like this:The other answers here tell you how to create a defaultdict which contains "infinitely many" defaultdict, but they fail to address what I think may have been your initial need which was to simply have a two-depth defaultdict.You may have been looking for: The reasons why you might prefer this construct are:There is a nifty trick for doing that:Then you can create your x with x = tree().Similar to BrenBarn's solution, but doesn't contain the name of the variable tree twice, so it works even after changes to the variable dictionary:Then you can create each new x with x = tree().For the def version, we can use function closure scope to protect the data structure from the flaw where existing instances stop working if the tree name is rebound.  It looks like this:I would also propose more OOP-styled implementation, which supports infinite nesting as well as properly formatted repr.Usage:

How can I manually generate a .pyc file from a .py file

zJay

[How can I manually generate a .pyc file from a .py file](https://stackoverflow.com/questions/5607283/how-can-i-manually-generate-a-pyc-file-from-a-py-file)

For some reason, I can not depend on Python's "import" statement to generate .pyc file automaticallyIs there a way to implement a function as following?

2011-04-09 19:28:20Z

For some reason, I can not depend on Python's "import" statement to generate .pyc file automaticallyIs there a way to implement a function as following?You can use compileall in the terminal. The following command will go recursively into sub directories and make pyc files for all the python files it finds. The compileall module is part of the python standard library, so you don't need to install anything extra to use it. This works exactly the same way for python2 and python3.You can compile individual files(s) from the command line with:It's been a while since I last used Python, but I believe you can use py_compile:I found several ways to compile python scripts into bytecodeTake a look at the links below:https://docs.python.org/3/library/py_compile.htmlhttps://docs.python.org/3/library/compileall.htmlI would use compileall. It works nicely both from scripts and from the command line. It's a bit higher level module/tool than the already mentioned py_compile that it also uses internally.which makes compile all .py to .pyc in the project which contained the subfolders.which makes compile all .py to the __pycache__ folders in the project which contained the subfolders.Or with browning from this post:To match the original question requirements (source path and destination path) the code should be like that:If the input code has errors then the py_compile.PyCompileError exception is raised.

Change a Django form field to a hidden field

Rory

[Change a Django form field to a hidden field](https://stackoverflow.com/questions/6862250/change-a-django-form-field-to-a-hidden-field)

I have a Django form with a RegexField, which is very similar to a normal text input field.In my view, under certain conditions I want to hide it from the user, and trying to keep the form as similar as possible. What's the best way to turn this field into a HiddenInput field?I know I can set attributes on the field with:And I can set the desired initial value with:However, that won't change the form of the widget.What's the best / most "django-y" / least "hacky" way to make this field a <input type="hidden"> field?

2011-07-28 16:19:39Z

I have a Django form with a RegexField, which is very similar to a normal text input field.In my view, under certain conditions I want to hide it from the user, and trying to keep the form as similar as possible. What's the best way to turn this field into a HiddenInput field?I know I can set attributes on the field with:And I can set the desired initial value with:However, that won't change the form of the widget.What's the best / most "django-y" / least "hacky" way to make this field a <input type="hidden"> field?If you have a custom template and view you may  exclude the field and use {{ modelform.instance.field }} to get the value.also you may prefer to use in the view:but I'm not sure it will protect save method on post.Hope it helps.This may also be useful: {{ form.field.as_hidden }}an option that worked for me, define the field in the original form as:then when you override it in the new Class it will keep it's place.Firstly, if you don't want the user to modify the data, then it seems cleaner to simply exclude the field.  Including it as a hidden field just adds more data to send over the wire and invites a malicious user to modify it when you don't want them to.  If you do have a good reason to include the field but hide it, you can pass a keyword arg to the modelform's constructor.  Something like this perhaps:Then in your view:I prefer this approach to modifying the modelform's internals in the view, but it's a matter of taste.For normal form you can doIf you have model form you can do the followingYou can also override __init__ methodYou can just use css :

This will make the field and its label invisible.

Installing Python 3 on RHEL

Chander Shivdasani

[Installing Python 3 on RHEL](https://stackoverflow.com/questions/8087184/installing-python-3-on-rhel)

I'm trying to install python3 on RHEL using the following steps:Which returned  No matches found for: python3Followed by:None of the search results contained python3. What should I try next?

2011-11-10 22:21:44Z

I'm trying to install python3 on RHEL using the following steps:Which returned  No matches found for: python3Followed by:None of the search results contained python3. What should I try next?It is easy to install it manually:Now if you want an alternative installation directory, you can pass --prefix to the configurecommand.Example: for 'installing' Python in /opt/local, just add --prefix=/opt/local. After the make install step: In order to use your new Python installation, it could be, that you still have to add the [prefix]/bin to the $PATH and [prefix]/lib to the $LD_LIBRARY_PATH (depending of the --prefix you passed)Installing from RPM is generally better, because:Red Hat has added through the EPEL repository:You can create your virtualenv using pyvenv:With CentOS7, pip3.6 is provided as a package :)You can create your virtualenv using pyvenv:If you use the pyvenv script, you'll get a WARNING:The IUS Community provides some up-to-date packages for RHEL & CentOS. The guys behind are from Rackspace, so I think that they are quite trustworthy...https://ius.io/Check the right repo for you here:https://ius.io/GettingStarted/You can create your virtualenv using pyvenv:You can create your virtualenv using pyvenv:In addition to gecco's answer I would change step 3 from:to:Then after installation you could also:It is to ensure that installation will not conflict with python installed with yum.See explanation I have found on Internet:http://www.hosting.com/support/linux/installing-python-3-on-centosredhat-5x-from-sourceUse the SCL repos.(This last command will have to be run each time you want to use python27 rather than the system default.)Along with Python 2.7 and 3.3, Red Hat Software Collections now includes Python 3.4 - all work on both RHEL 6 and 7.RHSCL 2.0 docs are at https://access.redhat.com/documentation/en-US/Red_Hat_Software_Collections/Plus lot of articles at developerblog.redhat.com.editYou can download a source RPMs and binary RPMs for RHEL6 / CentOS6 from

hereThis is a backport from the newest Fedora development

source rpm to RHEL6 / CentOS6Python3 was recently added to EPEL7 as Python34.There is ongoing (currently) effort to make packaging guidelines about how to package things for Python3 in EPEL7.See https://bugzilla.redhat.com/show_bug.cgi?id=1219411

and https://lists.fedoraproject.org/pipermail/python-devel/2015-July/000721.htmlI see all the answers as either asking to compile python3 from code or installing the binary RPM package. Here is another answer to enable EPEL (Extra Packages for Enterprise Linux) and then install python using yum. Steps for RHEL 7.5 (Maipo)Also see linkI was having the same issue using the python 2.7. Follow the below steps to upgrade successfully to 3.6.

You can also try this one-Three steps using Python 3.5 by Software Collections:Note that sudo is not needed for the last command. Now we can see that python 3 is the default for the current shell:Simply skip the last command if you'd rather have Python 2 as the default for the current shell.Now let's say that your Python 3 scripts give you an error like /usr/bin/env: python3: No such file or directory. That's because the installation is usually done to an unusual path:The above would normally be a symlink. If you want python3 to be automatically added to the $PATH for all users on startup, one way to do this is adding a file like:Which would have something like:And now after a reboot, if we doIt should just work. One exception would be an auto-generated user like "jenkins" in a Jenkins server which doesn't have a shell. In that case, manually adding the path to $PATH in scripts would be one way to go.Finally, if you're using sudo pip3 to install packages, but it tells you that pip3 cannot be found, it could be that you have a secure_path in /etc/sudoers. Checking with sudo visudo should confirm that. To temporarily use the standard PATH when running commands you can do, for example:See this question for more details.NOTE: There is a newer Python 3.6 by Software Collections, but I wouldn't recommend it at this time, because I had major headaches trying to install Pycurl. For Python 3.5 that isn't an issue because I just did sudo yum install sclo-python35-python-pycurl which worked out of the box.If you are on RHEL and want a Red Hat supported Python, use Red Hat Software collections (RHSCL). The EPEL and IUS packages are not supported by Red Hat.  Also many of the answers above point to the CentOS software collections. While you can install those, they aren't the Red Hat supported packages for RHEL.  Also, the top voted answer gives bad advice - On RHEL you do not want to change /usr/bin/python, /usr/bin/python2 because you will likely break yum and other RHEL admin tools. Take a look at /bin/yum, it is a Python script that starts with #!/usr/bin/python.  If you compile Python from source, do not do a make install as root.  That will overwrite /usr/bin/python. If you break yum it can be difficult to restore your system.For more info, see How to install Python 3, pip, venv, virtualenv, and pipenv on RHEL on developers.redhat.com.  It covers installing and using Python 3 from RHSCL, using Python Virtual Environments, and a number of tips for working with software collections and working with Python on RHEL.In a nutshell, to install Python 3.6 via Red Hat Software Collections:To use a software collection you have to enable it:However if you want Python 3 permanently enabled, you can add the following to your ~/.bashrc and then log out and back in again. Now Python 3 is permanently in your path.Note: once you do that, typing python now gives you Python 3.6 instead of Python 2.7.See the above article for all of this and a lot more detail. If you want official RHEL packages you can use RHSCL (Red Hat Software Collections)More details:You have to have access to Red Hat Customer Portal to read full articles.Here are the steps i followed to install Python3:yum install python34.x86_64 works if you have epel-release installed, which this answer explains how to, and I confirmed it worked on RHEL 7.3For RHEL on Amazon Linux, using python3 I had to do :Full working 36 when SCL is not available (based on Joys input)Finally activate the environment...Then python3You can install miniconda (https://conda.io/miniconda.html). That's a bit more than just python 3.7 but the installation is very straightforward and simple.You'll have to accept the license agreement and choose some options in interactive mode (accept the defaults).

I believe it can be also installed silently somehow.For those working on AWS EC2 RHEL 7.5, (use sudo) enable required reposInstall Python 3.6Install other dependencies

One line if-condition-assignment

bdhar

[One line if-condition-assignment](https://stackoverflow.com/questions/7872838/one-line-if-condition-assignment)

I have the following codeI need to set the value of num1 to 20 if someBoolValue is True; and do nothing otherwise. So, here is my code for thatIs there someway I could avoid the ...else num1 part to make it look cleaner? An equivalent to I tried replacing it with ...else pass like this: num1=20 if someBoolValue else pass. All I got was syntax error. Nor I could just omit the ...else num1 part.

2011-10-24 08:15:37Z

I have the following codeI need to set the value of num1 to 20 if someBoolValue is True; and do nothing otherwise. So, here is my code for thatIs there someway I could avoid the ...else num1 part to make it look cleaner? An equivalent to I tried replacing it with ...else pass like this: num1=20 if someBoolValue else pass. All I got was syntax error. Nor I could just omit the ...else num1 part.I don't think this is possible in Python, since what you're actually trying to do probably gets expanded to something like this:If you exclude else num1, you'll receive a syntax error since I'm quite sure that the assignment must actually return something.As others have already mentioned, you could do this, but it's bad because you'll probably just end up confusing yourself when reading that piece of code the next time:I'm not a big fan of the num1 = someBoolValue and 20 or num1 for the exact same reason. I have to actually think twice on what that line is doing.The best way to actually achieve what you want to do is the original version:The reason that's the best verison is because it's very obvious what you want to do, and you won't confuse yourself, or whoever else is going to come in contact with that code later.Also, as a side note, num1 = 20 if someBoolValue is valid Ruby code, because Ruby works a bit differently.Use this:In one line:But don’t do that. This style is normally not expected. People prefer the longer form for clarity and consistency.(Equally, camel caps should be avoided. So rather use some_bool_value.)Note that an in-line expression some_value if predicate without an else part does not exist because there would not be a return value if the predicate were false. However, expressions must have a clearly defined return value in all cases. This is different from usage as in, say, Ruby or Perl.you can use one of the following:No. I guess you were hoping that something like num1 = 20 if someBoolValue would work, but it doesn't. I think the best way is with the if statement as you have written it:That's my new final answer.

Prior answer was as follows and was overkill for the stated problem. Getting_too_clever == not Good.  Here's the prior answer... still good if you want add one thing for True cond and another for False:You mentioned num1 would already have a value that should be left alone.  I assumed num1 = 10 since that's the first statement of the post, so the operation to get to 20 is to add 10.produced this outputIf you wish to invoke a method if some boolean is true, you can put else None to terminate the trinary.  You can definitely use num1 = (20 if someBoolValue else num1) if you want.Here is what i can suggest.

 Use another variable to derive the if clause and assign it to num1.Code:Another way

num1 = (20*boolVar)+(num1*(not boolVar))You can do it this way.You can solve your problem this way but, using 'try/except block' is not the best practice for python.

Useful code which uses reduce()? [closed]

cnu

[Useful code which uses reduce()? [closed]](https://stackoverflow.com/questions/15995/useful-code-which-uses-reduce)

Does anyone here have any useful code which uses reduce() function in python? Is there any code other than the usual + and * that we see in the examples?Refer Fate of reduce() in Python 3000 by GvR

2008-08-19 11:16:58Z

Does anyone here have any useful code which uses reduce() function in python? Is there any code other than the usual + and * that we see in the examples?Refer Fate of reduce() in Python 3000 by GvRThe other uses I've found for it besides + and * were with and and or, but now we have any and all to replace those cases. foldl and foldr do come up in Scheme a lot... Here's some cute usages:Flatten a listGoal: turn [[1, 2, 3], [4, 5], [6, 7, 8]] into [1, 2, 3, 4, 5, 6, 7, 8].List of digits to a numberGoal: turn [1, 2, 3, 4, 5, 6, 7, 8] into 12345678.Ugly, slow way:Pretty reduce way:reduce() can be used to find Least common multiple for 3 or more numbers:Example:reduce() could be used to resolve dotted names (where eval() is too unsafe to use):Find the intersection of N given lists:returns:via: Python - Intersection of two listsI think reduce is a silly command. Hence:The usage of reduce that I found in my code involved the situation where I had some class structure for logic expression and I needed to convert a list of these expression objects to a conjunction of the expressions. I already had a function make_and to create a conjunction given two expressions, so I wrote reduce(make_and,l). (I knew the list wasn't empty; otherwise it would have been something like reduce(make_and,l,make_true).)This is exactly the reason that (some) functional programmers like reduce (or fold functions, as such functions are typically called). There are often already many binary functions like +, *, min, max, concatenation and, in my case, make_and and make_or. Having a reduce makes it trivial to lift these operations to lists (or trees or whatever you got, for fold functions in general).Of course, if certain instantiations (such as sum) are often used, then you don't want to keep writing reduce. However, instead of defining the sum with some for-loop, you can just as easily define it with reduce.Readability, as mentioned by others, is indeed an issue. You could argue, however, that only reason why people find reduce less "clear" is because it is not a function that many people know and/or use.You could replace value = json_obj['a']['b']['c']['d']['e'] with:If you already have the path a/b/c/.. as a list. For example, Change values in dict of nested dicts using items in a list.Function composition: If you already have a list of functions that you'd like to apply in succession, such as:Then you can apply them all consecutively with:In this case, method chaining may be more readable. But sometimes it isn't possible, and this kind of composition may be more readable and maintainable than a f1(f2(f3(f4(x)))) kind of syntax.@Blair Conrad: You could also implement your glob/reduce using sum, like so:This is less verbose than either of your two examples, is perfectly Pythonic, and is still only one line of code.So to answer the original question, I personally try to avoid using reduce because it's never really necessary and I find it to be less clear than other approaches.  However, some people get used to reduce and come to prefer it to list comprehensions (especially Haskell programmers).  But if you're not already thinking about a problem in terms of reduce, you probably don't need to worry about using it.reduce can be used to support chained attribute lookups:Of course, this is equivalent tobut it's useful when your code needs to accept an arbitrary list of attributes.(Chained attributes of arbitrary length are common when dealing with Django models.)reduce is useful when you need to find the union or intersection of a sequence of set-like objects. (Apart from actual sets, an example of these are Django's Q objects.)On the other hand, if you're dealing with bools, you should use any and all:After grepping my code, it seems the only thing I've used reduce for is calculating the factorial:I'm writing a compose function for a language, so I construct the composed function using reduce along with my apply operator.In a nutshell, compose takes a list of functions to compose into a single function. If I have a complex operation that is applied in stages, I want to put it all together like so:This way, I can then apply it to an expression like so:And I want it to be equivalent to:Now, to build my internal objects, I want it to say:(The Lambda class builds a user-defined function, and Apply builds a function application.)Now, reduce, unfortunately, folds the wrong way, so I wound up using, roughly:To figure out what reduce produces, try these in the REPL:reduce can be used to get the list with the maximum nth elementwould return [5, 2, 5, 7] as it is the list with max 3rd element +Reduce isn't limited to scalar operations; it can also be used to sort things into buckets. (This is what I use reduce for most often).Imagine a case in which you have a list of objects, and you want to re-organize it hierarchically based on properties stored flatly in the object. In the following example, I produce a list of metadata objects related to articles in an XML-encoded newspaper with the articles function. articles generates a list of XML elements, and then maps through them one by one, producing objects that hold some interesting info about them. On the front end, I'm going to want to let the user browse the articles by section/subsection/headline. So I use reduce to take the list of articles and return a single dictionary that reflects the section/subsection/article hierarchy.I give both functions here because I think it shows how map and reduce can complement each other nicely when dealing with objects. The same thing could have been accomplished with a for loop, ... but spending some serious time with a functional language has tended to make me think in terms of map and reduce.By the way, if anybody has a better way to set properties like I'm doing in extract, where the parents of the property you want to set might not exist yet, please let me know.Not sure if this is what you are after but you can search source code on Google.Follow the link for a search on 'function:reduce() lang:python' on Google Code searchAt first glance the following projects use reduce()etc. etc. but then these are hardly surprising since they are huge projects.The functionality of reduce can be done using function recursion which I guess Guido thought was more explicit.Update:Since Google's Code Search was discontinued on 15-Jan-2012, besides reverting to regular Google searches, there's something called Code Snippets Collection that looks promising. A number of other resources are mentioned in answers this (closed) question Replacement for Google Code Search?.Update 2 (29-May-2017):A good source for Python examples (in open-source code) is the Nullege search engine.I used reduce to concatenate a list of PostgreSQL search vectors with the || operator in sqlalchemy-searchable:I have an old Python implementation of pipegrep that uses reduce and the glob module to build a list of files to process:I found it handy at the time, but it's really not necessary, as something similar is just as good, and probably more readableLet say that there are some yearly statistic data stored a list of Counters.

We want to find the MIN/MAX values in each month across the different years. 

For example, for January it would be 10. And for February it would be 15. 

We need to store the results in a new Counter.I have objects representing some kind of overlapping intervals (genomic exons), and redefined their intersection using __and__:

Then when I have a collection of them (for instance, in the same gene), I use

I just found useful usage of reduce: splitting string without removing the delimiter. The code is entirely from Programatically Speaking blog. Here's the code:Here's the result:Note that it handles edge cases that popular answer in SO doesn't. For more in-depth explanation, I am redirecting you to original blog post.Using reduce() to find out if a list of dates are consecutive:

Multiple linear regression in Python

Zach

[Multiple linear regression in Python](https://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python)

I can't seem to find any python libraries that do multiple regression. The only things I find only do simple regression. I need to regress my dependent variable (y) against several independent variables (x1, x2, x3, etc.).For example, with this data:(output for above:)How would I regress these in python, to get the linear regression formula:

2012-07-13 22:14:40Z

I can't seem to find any python libraries that do multiple regression. The only things I find only do simple regression. I need to regress my dependent variable (y) against several independent variables (x1, x2, x3, etc.).For example, with this data:(output for above:)How would I regress these in python, to get the linear regression formula:sklearn.linear_model.LinearRegression will do it:Then clf.coef_ will have the regression coefficients.sklearn.linear_model also has similar interfaces to do various kinds of regularizations on the regression.Here is a little work around that I created. I checked it with R and it works correct.Result:Output:pandas provides a convenient way to run OLS as given in this answer:Run an OLS regression with Pandas Data FrameJust to clarify, the example you gave is multiple linear regression, not multivariate linear regression refer. Difference:In short:(Another source.)You can use numpy.linalg.lstsq:Result:You can see the estimated output with:Result:Use scipy.optimize.curve_fit. And not only for linear fit.Once you convert your data to a pandas dataframe (df), The intercept term is included by default.See this notebook for more examples.I think this may the most easy way to finish this work: Multiple Linear Regression can be handled using the sklearn library as referenced above.  I'm using the Anaconda install of Python 3.6.Create your model as follows:You can use numpy.linalg.lstsqYou can use the function below and pass it a DataFrame:Scikit-learn is a machine learning library for Python which can do this job for you. 

Just import sklearn.linear_model module into your script.Find the code template for Multiple Linear Regression using sklearn in Python:That's it. You can use this code as a template for implementing Multiple Linear Regression in any dataset. 

For a better understanding with an example, Visit: Linear Regression with an exampleHere is an alternative and basic method:Instead of sm.OLS you can also use sm.Logit or sm.Probit and etc.

What are the differences between the threading and multiprocessing modules?

lucacerone

[What are the differences between the threading and multiprocessing modules?](https://stackoverflow.com/questions/18114285/what-are-the-differences-between-the-threading-and-multiprocessing-modules)

I am learning how to use the threading and the multiprocessing modules in Python to run certain operations in parallel and speed up my code.I am finding this hard (maybe because I don't have any theoretical background about it) to understand what the difference is between a threading.Thread() object and a multiprocessing.Process() one.Also, it is not entirely clear to me how to instantiate a queue of jobs and having only 4 (for example) of them running in parallel, while the other wait for resources to free before being executed.I find the examples in the documentation clear, but not very exhaustive; as soon as I try to complicate things a bit, I receive a lot of weird errors (like a method that can't be pickled, and so on).So, when should I use the threading and multiprocessing modules?Can you link me to some resources that explain the concepts behind these two modules and how to use them properly for complex tasks?

2013-08-07 21:37:06Z

I am learning how to use the threading and the multiprocessing modules in Python to run certain operations in parallel and speed up my code.I am finding this hard (maybe because I don't have any theoretical background about it) to understand what the difference is between a threading.Thread() object and a multiprocessing.Process() one.Also, it is not entirely clear to me how to instantiate a queue of jobs and having only 4 (for example) of them running in parallel, while the other wait for resources to free before being executed.I find the examples in the documentation clear, but not very exhaustive; as soon as I try to complicate things a bit, I receive a lot of weird errors (like a method that can't be pickled, and so on).So, when should I use the threading and multiprocessing modules?Can you link me to some resources that explain the concepts behind these two modules and how to use them properly for complex tasks?What Giulio Franco says is true for multithreading vs. multiprocessing in general.However, Python* has an added issue: There's a Global Interpreter Lock that prevents two threads in the same process from running Python code at the same time. This means that if you have 8 cores, and change your code to use 8 threads, it won't be able to use 800% CPU and run 8x faster; it'll use the same 100% CPU and run at the same speed. (In reality, it'll run a little slower, because there's extra overhead from threading, even if you don't have any shared data, but ignore that for now.)There are exceptions to this. If your code's heavy computation doesn't actually happen in Python, but in some library with custom C code that does proper GIL handling, like a numpy app, you will get the expected performance benefit from threading. The same is true if the heavy computation is done by some subprocess that you run and wait on.More importantly, there are cases where this doesn't matter. For example, a network server spends most of its time reading packets off the network, and a GUI app spends most of its time waiting for user events. One reason to use threads in a network server or GUI app is to allow you to do long-running "background tasks" without stopping the main thread from continuing to service network packets or GUI events. And that works just fine with Python threads. (In technical terms, this means Python threads give you concurrency, even though they don't give you core-parallelism.)But if you're writing a CPU-bound program in pure Python, using more threads is generally not helpful.Using separate processes has no such problems with the GIL, because each process has its own separate GIL. Of course you still have all the same tradeoffs between threads and processes as in any other languages—it's more difficult and more expensive to share data between processes than between threads, it can be costly to run a huge number of processes or to create and destroy them frequently, etc. But the GIL weighs heavily on the balance toward processes, in a way that isn't true for, say, C or Java. So, you will find yourself using multiprocessing a lot more often in Python than you would in C or Java.Meanwhile, Python's "batteries included" philosophy brings some good news: It's very easy to write code that can be switched back and forth between threads and processes with a one-liner change.If you design your code in terms of self-contained "jobs" that don't share anything with other jobs (or the main program) except input and output, you can use the concurrent.futures library to write your code around a thread pool like this:You can even get the results of those jobs and pass them on to further jobs, wait for things in order of execution or in order of completion, etc.; read the section on Future objects for details.Now, if it turns out that your program is constantly using 100% CPU, and adding more threads just makes it slower, then you're running into the GIL problem, so you need to switch to processes. All you have to do is change that first line:The only real caveat is that your jobs' arguments and return values have to be pickleable (and not take too much time or memory to pickle) to be usable cross-process. Usually this isn't a problem, but sometimes it is.But what if your jobs can't be self-contained? If you can design your code in terms of jobs that pass messages from one to another, it's still pretty easy. You may have to use threading.Thread or multiprocessing.Process instead of relying on pools. And you will have to create queue.Queue or multiprocessing.Queue objects explicitly. (There are plenty of other options—pipes, sockets, files with flocks, … but the point is, you have to do something manually if the automatic magic of an Executor is insufficient.)But what if you can't even rely on message passing? What if you need two jobs to both mutate the same structure, and see each others' changes? In that case, you will need to do manual synchronization (locks, semaphores, conditions, etc.) and, if you want to use processes, explicit shared-memory objects to boot. This is when multithreading (or multiprocessing) gets difficult. If you can avoid it, great; if you can't, you will need to read more than someone can put into an SO answer.From a comment, you wanted to know what's different between threads and processes in Python. Really, if you read Giulio Franco's answer and mine and all of our links, that should cover everything… but a summary would definitely be useful, so here goes:* It's not actually Python, the language, that has this issue, but CPython, the "standard" implementation of that language. Some other implementations don't have a GIL, like Jython.** If you're using the fork start method for multiprocessing—which you can on most non-Windows platforms—each child process gets any resources the parent had when the child was started, which can be another way to pass data to children.Multiple threads can exist in a single process.

The threads that belong to the same process share the same memory area (can read from and write to the very same variables, and can interfere with one another).

On the contrary, different processes live in different memory areas, and each of them has its own variables. In order to communicate, processes have to use other channels (files, pipes or sockets).If you want to parallelize a computation, you're probably going to need multithreading, because you probably want the threads to cooperate on the same memory.Speaking about performance, threads are faster to create and manage than processes (because the OS doesn't need to allocate a whole new virtual memory area), and inter-thread communication is usually faster than inter-process communication. But threads are harder to program. Threads can interfere with one another, and can write to each other's memory, but the way this happens is not always obvious (due to several factors, mainly instruction reordering and memory caching), and so you are going to need synchronization primitives to control access to your variables.I believe this link answers your question in an elegant way.To be short, if one of your sub-problems has to wait while another finishes, multithreading is good (in I/O heavy operations, for example); by contrast, if your sub-problems could really happen at the same time, multiprocessing is suggested. However, you won't create more processes than your number of cores.Here's some performance data for python 2.6.x that calls to question the  notion that threading is more performant that multiprocessing in IO-bound scenarios. These results are from a 40-processor IBM System x3650 M4 BD.IO-Bound Processing : Process Pool performed better than Thread PoolCPU-Bound Processing : Process Pool performed better than Thread PoolThese aren't rigorous tests, but they tell me that multiprocessing isn't entirely unperformant in comparison to threading.Code used in the interactive python console for the above testsPython documentation quotesI've highlighted the key Python documentation quotes about Process vs Threads and the GIL at: What is the global interpreter lock (GIL) in CPython?Process vs thread experimentsI did a bit of benchmarking in order to show the difference more concretely.In the benchmark, I timed CPU and IO bound work for various numbers of threads on an 8 hyperthread CPU. The work supplied per thread is always the same, such that more threads means more total work supplied.The results were:Plot data.Conclusions:Test code:GitHub upstream + plotting code on same directory.Tested on Ubuntu 18.10, Python 3.6.7, in a Lenovo ThinkPad P51 laptop with CPU: Intel Core i7-7820HQ CPU (4 cores / 8 threads), RAM: 2x Samsung M471A2K43BB1-CRC (2x 16GiB), SSD: Samsung MZVLB512HAJQ-000L7 (3,000 MB/s).Visualize which threads are running at a given timeThis post https://rohanvarma.me/GIL/ taught me that you can run a callback whenever a thread is scheduled with the target= argument of threading.Thread and the same for multiprocessing.Process.This allows us to view exactly which thread runs at each time. When this is done, we would see something like (I made this particular graph up): which would show that:Well, most of the question is answered by Giulio Franco. I will further elaborate on the consumer-producer problem, which I suppose will put you on the right track for your solution to using a multithreaded app.You could read more on the synchronization primitives from:The pseudocode is above. I suppose you should search the producer-consumer-problem to get more references.

Remove all line breaks from a long string of text

Ian Zane

[Remove all line breaks from a long string of text](https://stackoverflow.com/questions/16566268/remove-all-line-breaks-from-a-long-string-of-text)

Basically, I'm asking the user to input a string of text into the console, but the string is very long and includes many line breaks.  How would I take the user's string and delete all line breaks to make it a single line of text.  My method for acquiring the string is very simple.Is there a different way I should be grabbing the string from the user?  I'm running Python 2.7.4 on a Mac.P.S. Clearly I'm a noob, so even if a solution isn't the most efficient, the one that uses the most simple syntax would be appreciated.

2013-05-15 13:25:01Z

Basically, I'm asking the user to input a string of text into the console, but the string is very long and includes many line breaks.  How would I take the user's string and delete all line breaks to make it a single line of text.  My method for acquiring the string is very simple.Is there a different way I should be grabbing the string from the user?  I'm running Python 2.7.4 on a Mac.P.S. Clearly I'm a noob, so even if a solution isn't the most efficient, the one that uses the most simple syntax would be appreciated.How do you enter line breaks with raw_input? But, once you have a string with some characters in it you want to get rid of, just replace them.In the example above, I replaced all spaces. The string '\n' represents newlines. And \r represents carriage returns (if you're on windows, you might be getting these and a second replace will handle them for you!).basically:Note also, that it is a bad idea to call your variable string, as this shadows the module string. Another name I'd avoid but would love to use sometimes: file. For the same reason.You can try using string replace:You can split the string with no separator arg, which will treat consecutive whitespace as a single separator (including newlines and tabs). Then join using a space:https://docs.python.org/2/library/stdtypes.html#str.splitupdated based on Xbello comment:read more hereAnother option is regex:A method taking into considerationit takes such a multi-line string which may be messy e.g.and produces nice one-line stringUPDATE:

To fix multiple new-line character producing redundant spaces:This works for the following too

test_str = '\nhej ho \n aaa\r\n\n\n\n\n   a\n 'The problem with rstrip is that it does not work in all cases (as I myself have seen few). Instead you can use -

text= text.replace("\n"," ")

this will remove all new line \n with a space.Thanks in advance guys for your upvotes.

How can I consume a WSDL (SOAP) web service in Python?

DavidM

[How can I consume a WSDL (SOAP) web service in Python?](https://stackoverflow.com/questions/115316/how-can-i-consume-a-wsdl-soap-web-service-in-python)

I want to use a WSDL SOAP based web service in Python. I have looked at the Dive Into Python code but the SOAPpy module does not work under Python 2.5.I have tried using suds which works partly, but breaks with certain types (suds.TypeNotFound: Type not found: 'item').I have also looked at Client but this does not appear to support WSDL.And I have looked at ZSI but it looks very complex. Does anyone have any sample code for it?The WSDL is https://ws.pingdom.com/soap/PingdomAPI.wsdl and works fine with the PHP 5 SOAP client.

2008-09-22 14:58:53Z

I want to use a WSDL SOAP based web service in Python. I have looked at the Dive Into Python code but the SOAPpy module does not work under Python 2.5.I have tried using suds which works partly, but breaks with certain types (suds.TypeNotFound: Type not found: 'item').I have also looked at Client but this does not appear to support WSDL.And I have looked at ZSI but it looks very complex. Does anyone have any sample code for it?The WSDL is https://ws.pingdom.com/soap/PingdomAPI.wsdl and works fine with the PHP 5 SOAP client.I would recommend that you have a look at SUDS"Suds is a lightweight SOAP python client for consuming Web Services."There is a relatively new library which is very promising and albeit still poorly documented, seems very clean and pythonic: python zeep.See also this answer for an example.I recently stumbled up on the same problem. Here is the synopsis of my solution:Basic constituent code blocks neededThe following are the required basic code blocks of your client applicationWhat modules do you need?Many suggested to use Python modules such as urllib2 ; however, none of the modules work-at least for this particular project.So, here is the list of the modules you need to get. 

First of all, you need to download and install the latest version of suds from the following link:Additionally, you need to download and install requests and suds_requests modules from the following links respectively ( disclaimer: I am new to post in here, so I can't post more than one link for now).Once you successfully download and install these modules, you are good to go.The codeFollowing the steps outlined earlier, the code looks like the following:

Imports:Session request and authentication:Create the Client:Add WS-Security Header:Please note that this method creates the security header depicted in Fig.1. So, your implementation may vary depending on the correct security header format provided by the owner of the service you are consuming.Consume the relevant method (or operation) :Logging:One of the best practices in such implementations as this one is logging to see how the communication is executed. In case there is some issue, it makes debugging easy. The following code does basic logging. However, you can log many aspects of the communication in addition to the ones depicted in the code.Result:Here is the result in my case. Note that the server returned HTTP 200. This is the standard success code for HTTP request-response.Right now (as of 2008), all the SOAP libraries available for Python suck. I recommend avoiding SOAP if possible. The last time we where forced to use a SOAP web service from Python, we wrote a wrapper in C# that handled the SOAP on one side and spoke COM out the other. Zeep is a decent SOAP library for Python that matches what you're asking for: http://docs.python-zeep.orgI periodically search for a satisfactory answer to this, but no luck so far. I use soapUI + requests + manual labour.I gave up and used Java the last time I needed to do this, and simply gave up a few times the last time I wanted to do this, but it wasn't essential.Having successfully used the requests library last year with Project Place's RESTful API, it occurred to me that maybe I could just hand-roll the SOAP requests I want to send in a similar way.Turns out that's not too difficult, but it is time consuming and prone to error, especially if fields are inconsistently named (the one I'm currently working on today has 'jobId', JobId' and 'JobID'. I use soapUI to load the WSDL to make it easier to extract endpoints etc and perform some manual testing.  So far I've been lucky not to have been affected by changes to any WSDL that I'm using.It's not true SOAPpy does not work with Python 2.5 - it works, although it's very simple and really, really basic. If you want to talk to any more complicated webservice, ZSI is your only friend.The really useful demo I found is at http://www.ebi.ac.uk/Tools/webservices/tutorials/python - this really helped me to understand how ZSI works.If you're rolling your own I'd highly recommend looking at http://effbot.org/zone/element-soap.htm.SOAPpy is now obsolete, AFAIK, replaced by ZSL. It's a moot point, because I can't get either one to work, much less compile, on either Python 2.5 or Python 2.6

Does python have a sorted list?

ilya n.

[Does python have a sorted list?](https://stackoverflow.com/questions/1109804/does-python-have-a-sorted-list)

By which I mean a structure with:I also had a related question about performance of list(...).insert(...) which is now here.

2009-07-10 14:18:00Z

By which I mean a structure with:I also had a related question about performance of list(...).insert(...) which is now here.The standard Python list is not sorted in any form. The standard heapq module can be used to append in O(log n) to an existing list and remove the smallest one in O(log n), but isn't a sorted list in your definition.There are various implementations of balanced trees for Python that meet your requirements, e.g. rbtree, RBTree, or pyavl.Is there a particular reason for your big-O requirements? Or do you just want it to be fast? The sortedcontainers module is pure-Python and fast (as in fast-as-C implementations like blist and rbtree).The performance comparison shows it benchmarks faster or on par with blist's sorted list type. Note also that rbtree, RBTree, and PyAVL provide sorted dict and set types but don't have a sorted list type.If performance is a requirement, always remember to benchmark. A module that substantiates the claim of being fast with Big-O notation should be suspect until it also shows benchmark comparisons.Disclaimer: I am the author of the Python sortedcontainers module.Installation:Usage:Though I have still never checked the "big O" speeds of basic Python list operations, 

the bisect standard module is probably also worth mentioning in this context:  PS. Ah, sorry, bisect is mentioned in the referenced question. Still, I think it won't be much harm if this information will be here )PPS. And CPython lists are actually arrays (not, say, skiplists or etc) . Well, I guess they have to be something simple, but as for me, the name is a little bit misleading.So, if I am not mistaken, the bisect/list speeds would probably be:Upd. Following a discussion in the comments, let me link here these SO questions: How is Python's List Implemented and What is the runtime complexity of python list functionsThough it does not (yet) provide a custom search function, the heapq module may suit your needs. It implements a heap queue using a regular list. You'd have to write your own efficient membership test that makes use of the queue's internal structure (that can be done in O(log n), I'd say...). There is one downside: extracting a sorted list has complexity O(n log n).I would use the biscect or sortedcontainers modules. I don't really am experienced, but I think the heapq module works. It contains a Heap QueueIt may not be hard to implement your own sortlist on Python. Below is a proof of concept:========= Results ============[3, 10, 14, 17, 23, 44, 45, 45, 50, 66, 73, 77, 79, 84, 85, 86, 91, 95, 101][3, 10, 14, 17, 23, 44, 45, 45, 50, 66, 73, 77, 79, 84, 85, 86, 91, 95, 99, 101]101350

Dictionary vs Object - which is more efficient and why?

tkokoszka

[Dictionary vs Object - which is more efficient and why?](https://stackoverflow.com/questions/1336791/dictionary-vs-object-which-is-more-efficient-and-why)

What is more efficient in Python in terms of memory usage and CPU consumption - Dictionary or Object?Background:

I have to load huge amount of data into Python. I created an object that is just a field container. Creating 4M instances and putting them into a dictionary took about 10 minutes and ~6GB of memory. After dictionary is ready, accessing it is a blink of an eye.Example:

To check the performance I wrote two simple programs that do the same - one is using objects, other dictionary:Object (execution time ~18sec):Dictionary (execution time ~12sec):Question:

Am I doing something wrong or dictionary is just faster than object? If indeed dictionary performs better, can somebody explain why?

2009-08-26 18:55:45Z

What is more efficient in Python in terms of memory usage and CPU consumption - Dictionary or Object?Background:

I have to load huge amount of data into Python. I created an object that is just a field container. Creating 4M instances and putting them into a dictionary took about 10 minutes and ~6GB of memory. After dictionary is ready, accessing it is a blink of an eye.Example:

To check the performance I wrote two simple programs that do the same - one is using objects, other dictionary:Object (execution time ~18sec):Dictionary (execution time ~12sec):Question:

Am I doing something wrong or dictionary is just faster than object? If indeed dictionary performs better, can somebody explain why?Have you tried using __slots__?From the documentation:So does this save time as well as memory?Comparing the three approaches on my computer:test_slots.py:test_obj.py:test_dict.py:test_namedtuple.py (supported in 2.6):Run benchmark (using CPython 2.5):Using CPython 2.6.2, including the named tuple test:So yes (not really a surprise), using __slots__ is a performance optimization. Using a named tuple has similar performance to __slots__.Attribute access in an object uses dictionary access behind the scenes - so by using attribute access you are adding extra overhead. Plus in the object case, you are incurring additional overhead because of e.g. additional memory allocations and code execution (e.g. of the __init__ method).In your code, if o is an Obj instance, o.attr is equivalent to o.__dict__['attr'] with a small amount of extra overhead.Have you considered using a namedtuple?  (link for python 2.4/2.5)It's the new standard way of representing structured data that gives you the performance of a tuple and the convenience of a class.It's only downside compared with dictionaries is that (like tuples) it doesn't give you the ability to change attributes after creation.Results:Here is a copy of @hughdbrown answer for python 3.6.1, I've made the count 5x larger and added some code to test the memory footprint of the python process at the end of each run.Before the downvoters have at it, Be advised that this method of counting the size of objects is not accurate.And these are my resultsMy conclusion is:There is no question.

You have data, with no other attributes (no methods, nothing). Hence you have a data container (in this case, a dictionary).I usually prefer to think in terms of data modeling. If there is some huge performance issue, then I can give up something in the abstraction, but only with very good reasons.

Programming is all about managing complexity, and the maintaining the correct abstraction is very often one of the most useful way to achieve such result.About the reasons an object is slower, I think your measurement is not correct.

You are performing too little assignments inside the for loop, and therefore what you see there is the different time necessary to instantiate a dict (intrinsic object) and a "custom" object. Although from the language perspective they are the same, they have quite a different implementation.

After that, the assignment time should be almost the same for both, as in the end members are maintained inside a dictionary.There is yet another way to reduce memory usage if data structure isn't supposed to contain reference cycles.Let's compare two classes:andIt became possible since structclass-based classes doesn't support cyclic garbage collection, which is not needed in such cases.There is also one advantage over __slots__-based class: you are able to add extra attributes:

Can't connect to local MySQL server through socket '/tmp/mysql.sock

Alex Gaynor

[Can't connect to local MySQL server through socket '/tmp/mysql.sock](https://stackoverflow.com/questions/16325607/cant-connect-to-local-mysql-server-through-socket-tmp-mysql-sock)

When I attempted to connect to a local MySQL server during my test suite, it

fails with the error:However, I'm able to at all times, connect to MySQL by running the command line

mysql program. A ps aux | grep mysql shows the server is running, and

stat /tmp/mysql.sock confirm that the socket exists. Further, if I open a

debugger in except clause of that exception, I'm able to reliably connect

with the exact same parameters.This issue reproduces fairly reliably, however it doesn't appear to be 100%,

because every once in a blue moon, my test suite does in fact run without

hitting this error. When I attempted to run with sudo dtruss it did not reproduce.All the client code is in Python, though I can't figure how that'd be relevant.Switching to use host 127.0.0.1 produces the error:

2013-05-01 20:06:17Z

When I attempted to connect to a local MySQL server during my test suite, it

fails with the error:However, I'm able to at all times, connect to MySQL by running the command line

mysql program. A ps aux | grep mysql shows the server is running, and

stat /tmp/mysql.sock confirm that the socket exists. Further, if I open a

debugger in except clause of that exception, I'm able to reliably connect

with the exact same parameters.This issue reproduces fairly reliably, however it doesn't appear to be 100%,

because every once in a blue moon, my test suite does in fact run without

hitting this error. When I attempted to run with sudo dtruss it did not reproduce.All the client code is in Python, though I can't figure how that'd be relevant.Switching to use host 127.0.0.1 produces the error:This worked for me. However, if this doesnt work then make sure that mysqld is running and try connecting.The relevant section of the MySQL manual is here. I'd start by going through the debugging steps listed there.Also, remember that localhost and 127.0.0.1 are not the same thing in this context:So, for example, you can check if your database is listening for TCP connections vi netstat -nlp. It seems likely that it IS listening for TCP connections because you say that mysql -h 127.0.0.1 works just fine. To check if you can connect to your database via sockets, use mysql -h localhost. If none of this helps, then you probably need to post more details about your MySQL config, exactly how you're instantiating the connection, etc.For me the problem was I wasn't running mysql server.

Run server first and then execute mysql.I've seen this happen at my shop when my devs have a stack manager like MAMP installed that comes preconfigured with MySQL installed in a non standard place.at your terminal run that will give you your path to the sock file. take that path and use it in your DATABASES HOST paramater.What you need to do is point your  also run which mysql_config if you somehow have multiple instances of mysql server installed on the machine you may be connecting to the wrong one.I just changed the HOST from localhost to 127.0.0.1 and it works fine:When, if you lose your daemon mysql in mac OSx but is present in other path for exemple in private/var do the following command1)2) restart your connexion to mysql with :Run the below cmd in terminal Then restart the machine to take effect. It works!!After attempting a few of these solutions and not having any success, this is what worked for me:Check number of open files for the mysql process using lsof command. Increase the open files limit and run again.This may be one of following problems.and then put it in your db connection code: /tmp/mysql.sock is the returned from grep2.Incorrect mysql port

  solution: You have to find out the correct mysql port:and then in your code:3306 is the port returned from the grepI think first option will resolve your problem. To those who upgraded from 5.7 to 8.0 via homebrew, this error is likely caused by the upgrade not being complete. In my case, mysql.server start got me the following error:I then checked the log file via cat /usr/local/var/mysql/YOURS.err | tail -n 50, and found the following:If you are on the same boat, first install mysql@5.7 via homebrew, stop the server, and then start the 8.0 system again.Then,This would get your MySQL (8.0) working again.I think i saw this same behavior some time ago, but can't remember the details.

In our case, the problem was the moment the testrunner initialises database connections relative to first database interaction required, for instance, by import of a module in  settings.py or some __init__.py.

I'll try to digg up some more info, but this might already ring a bell for your case.I have two sneaky conjectures on this oneLook into the possibility of not being able to access the /tmp/mysql.sock file. When I setup MySQL databases, I normally let the socket file site in /var/lib/mysql. If you login to mysql as root@localhost, your OS session needs access to the /tmp folder. Make sure /tmp has the correct access rights in the OS. Also, make sure the sudo user can always read file in /tmp.Accessing mysql via 127.0.0.1 can cause some confusion if you are not paying attention. How?From the command line, if you connect to MySQL with 127.0.0.1, you may need to specify the TCP/IP protocol.or try the DNS nameThis will bypass logging in as root@localhost, but make sure you have root@'127.0.0.1' defined.Next time you connect to MySQL, run this:What does this give you?If these functions return with the same values, then you are connecting and authenticating as expected. If the values are different, you may need to create the corresponding user root@127.0.0.1.Had this same problem. Turned out mysqld had stopped running (I'm on Mac OSX). I restarted it and the error went away.I figured out that mysqld was not running largely because of this link:

http://dev.mysql.com/doc/refman/5.6/en/can-not-connect-to-server.htmlNotice the first tip!if you get an error like below :Then just find your mysqld.sock file location and add it to "HOST".Like i am using xampp on linux so my mysqld.sock file is in another location. so it is not working for '/var/run/mysqld/mysqld.sock'Make sure your /etc/hosts has 127.0.0.1 localhost in it and it should work fineCheck that your mysql has not reached maximum connections, or is not in some sort of booting loop as happens quite often if the settings are incorrect in my.cnf.Use ps aux | grep mysql to check if the PID is changing.Looked around online too long not to contribute. After trying to type in the mysql prompt from the command line, I was continuing to receive this message:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)This was due to the fact that my local mysql server was no longer running. In order to restart the server, I navigated towhere my mysql.server was located. From here, simply type: This will relaunch the local mysql server.From there you can reset the root password if need be..I had to kill off all instances of mysql by first finding all the process IDs:And then killing them off:Then:Worked for me.The socket is located in /tmp. On Unix system, due to modes & ownerships on /tmp, this could cause some problem. But, as long as you tell us that you CAN use your mysql connexion normally, I guess it is not a problem on your system. A primal check should be to relocate mysql.sock in a more neutral directory.The fact that the problem occurs "randomly" (or not every time) let me think that it could be a server problem. Configure your DB connection in the 'Manage DB Connections dialog. Select 'Standard (TCP/IP)' as connection method.See this page for more details 

http://dev.mysql.com/doc/workbench/en/wb-manage-db-connections.htmlAccording to this other page a socket file is used even if you specify localhost.It also shows how to check on your server by running these commands:in ubuntu14.04 you can do this to slove this problem.For me, I'm sure mysqld is started, and command line mysql can work properly. But the httpd server show the issue(can't connect to mysql through socket).finally, I found when I start the mysqld service with service mysqld start, there are issues(selinux permission issue), and when I fix the selinux issue, and start the mysqld with "service mysqld start", the httpd connection issue disappear. But when I start the mysqld with mysqld_safe&, mysqld can be worked. (mysql client can work properly). But there are still issue when connect with httpd.If it's socket related read this fileand see what is the standard socket location. It's a line like:now create an alias for your shell like:This way you don't need root privileges.Simply try to run mysqld. This was what was not working for me on mac.

If it doesn't work try go to /usr/local/var/mysql/<your_name>.err to see detailed error logs.May be helpful.Using MacOS Mojave 10.14.6 for MySQL 8.0.19 installed via HomebrewWorked for a time then eventually the error returned. Uninstalled the Homebrew version of MySQL and installed the .dmg file directly from hereHappily connecting since then.

How to update SQLAlchemy row entry?

webminal.org

[How to update SQLAlchemy row entry?](https://stackoverflow.com/questions/9667138/how-to-update-sqlalchemy-row-entry)

Assume table has three columns: username, password and no_of_logins.When user tries to login, it's checked for an entry with a query likeIf password matches, he proceeds further. What I would like to do is count how many times the user logged in. Thus whenever he successfully logs in, I would like to increment the no_of_logins field and store it back to the user table. I'm not sure how to run update query with SqlAlchemy.

2012-03-12 12:37:12Z

Assume table has three columns: username, password and no_of_logins.When user tries to login, it's checked for an entry with a query likeIf password matches, he proceeds further. What I would like to do is count how many times the user logged in. Thus whenever he successfully logs in, I would like to increment the no_of_logins field and store it back to the user table. I'm not sure how to run update query with SqlAlchemy.There are several ways to UPDATE using sqlalchemyI didn't understand it until I played around with it myself, so I figured there would be others who were confused as well.  Say you are working on the user whose id == 6 and whose no_of_logins == 30 when you start.By referencing the class instead of the instance, you can get SQLAlchemy to be smarter about incrementing, getting it to happen on the database side instead of the Python side.  Doing it within the database is better since it's less vulnerable to data corruption (e.g. two clients attempt to increment at the same time with a net result of only one increment instead of two).  I assume it's possible to do the incrementing in Python if you set locks or bump up the isolation level, but why bother if you don't have to?If you are going to increment twice via code that produces SQL like SET no_of_logins = no_of_logins + 1, then you will need to commit or at least flush in between increments, or else you will only get one increment in total:With the help of user=User.query.filter_by(username=form.username.data).first() statement you will get the specified user in user variable. Now you can change the value of the new object variable like user.no_of_logins += 1 and save the changes with the session's commit method.I wrote telegram bot, and have some problem with update rows. 

Use this example, if you have ModelWhy use db.session.flush()? That's why >>> SQLAlchemy: What's the difference between flush() and commit()?

seek() function?

Gmenfan83

[seek() function?](https://stackoverflow.com/questions/11696472/seek-function)

Please excuse my confusion here but I have read the documentation regarding the seek() function in python (after having to use it) and although it helped me I am still a bit confused on the actual meaning of what it does, any explanations are much appreciated, thank you.

2012-07-27 22:28:47Z

Please excuse my confusion here but I have read the documentation regarding the seek() function in python (after having to use it) and although it helped me I am still a bit confused on the actual meaning of what it does, any explanations are much appreciated, thank you.Regarding seek() there's not too much to worry about.First of all, it is useful when operating over an open file.It's important to note that its syntax is as follows:where fp is the file pointer you're working with; offset means how many positions you will move; from_what defines your point of reference:if omitted, from_what defaults to 0.Never forget that when managing files, there'll always be a position inside that file where you are currently working on. When just open, that position is the beginning of the file, but as you work with it, you may advance.

seek will be useful to you when you need to walk along that open file, just as a path you are traveling into.When you open a file, the system points to the beginning of the file. Any read or write you do will happen from the beginning. A seek() operation moves that pointer to some other part of the file so you can read or write at that place.So, if you want to read the whole file but skip the first 20 bytes, open the file, seek(20) to move to where you want to start reading, then continue with reading the file.Or say you want to read every 10th byte, you could write a loop that does seek(9, 1) (moves 9 bytes forward relative to the current positions), reads one byte, repeat.The seek function expect's an offset in bytes. So if you have a text file with the following content: simple.txtYou can jump 1 byte to skip over the first character as following:For strings, forget about using WHENCE: use f.seek(0) to position at beginning of file and f.seek(len(f)+1) to position at the end of file.  Use open(file, "r+") to read/write anywhere in a file. If you use "a+" you'll only be able to write (append) at the end of the file regardless of where you position the cursor.

Numpy where function multiple conditions

user1654183

[Numpy where function multiple conditions](https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions)

I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:However this selects only for the condition If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work? Cheers

2013-05-02 17:03:19Z

I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:However this selects only for the condition If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work? CheersThe best way in your particular case would just be to change your two criteria to one criterion:It only creates one boolean array, and in my opinion is easier to read because it says, is dist within a dr or r? (Though I'd redefine r to be the center of your region of interest instead of the beginning, so r = r + dr/2.)  But that doesn't answer your question.The answer to your question:

You don't actually need where if you're just trying to filter out the elements of dists that don't fit your criteria:Because the & will give you an elementwise and (the parentheses are necessary).Or, if you do want to use where for some reason, you can do:Why:

The reason it doesn't work is because np.where returns a list of indices, not a boolean array.  You're trying to get and between two lists of numbers, which of course doesn't have the True/False values that you expect.  If a and b are both True values, then a and b returns b.  So saying something like [0,1,2] and [2,3,4] will just give you [2,3,4].  Here it is in action:What you were expecting to compare was simply the boolean array, for exampleNow you can call np.where on the combined boolean array:Or simply index the original array with the boolean array using fancy indexingThe accepted answer explained the problem well enough. However, the the more Numpythonic approach for applying multiple conditions is to use numpy logical functions. In this ase you can use np.logical_and: One interesting thing to point here; the usual way of using OR and AND too will work in this case, but with a small change. Instead of "and" and instead of "or", rather use Ampersand(&) and Pipe Operator(|) and it will work.When we use 'and':When we use Ampersand(&):And this is same in the case when we are trying to apply multiple filters in case of pandas Dataframe. Now the reasoning behind this has to do something with Logical Operators and Bitwise Operators and for more understanding about same, I'd suggest to go through this answer or similar Q/A in stackoverflow.A user asked, why is there a need for giving (ar>3) and (ar<6) inside the parenthesis. Well here's the thing. Before I start talking about what's happening here, one needs to know about Operator precedence in Python.Similar to what BODMAS is about, python also gives precedence to what should be performed first. Items inside the parenthesis are performed first and then the bitwise operator comes to work. I'll show below what happens in both the cases when you do use and not use "(", ")".Case1:Since there are no brackets here, the bitwise operator(&) is getting confused here that what are you even asking it to get logical AND of, because in the operator precedence table if you see, & is given precedence over < or > operators. Here's the table from from lowest precedence to highest precedence.It's not even performing the < and > operation and being asked to perform a logical AND operation. So that's why it gives that error.Now to Case 2:If you do use the bracket, you clearly see what happens.Two arrays of True and False. And you can easily perform logical AND operation on them. Which gives you:And rest you know, np.where, for given cases, wherever True, assigns first value(i.e. here 'yo') and if False, the other(i.e. here, keeping the original).That's all. I hope I explained the query well.I like to use np.vectorize for such tasks. Consider the following:You can also use np.argwhere instead of np.where for clear output. But that is your call :)Hope it helps.Try:This should work:The most elegant way~~Try: Output: (array([2, 3]),)You can see Logic functions for more details.I have worked out this simple example 

Requests — how to tell if you're getting a 404

user1427661

[Requests — how to tell if you're getting a 404](https://stackoverflow.com/questions/15258728/requests-how-to-tell-if-youre-getting-a-404)

I'm using the Requests library and accessing a website to gather data from it with the following code: I want to add error testing for when an improper URL is entered and a 404 error is returned. If I intentionally enter an invalid URL, when I do this:I get this:EDIT:I want to know how to test for that. The object type is still the same. When I do r.content or r.text, I simply get the HTML of a custom 404 page. 

2013-03-06 21:46:37Z

I'm using the Requests library and accessing a website to gather data from it with the following code: I want to add error testing for when an improper URL is entered and a 404 error is returned. If I intentionally enter an invalid URL, when I do this:I get this:EDIT:I want to know how to test for that. The object type is still the same. When I do r.content or r.text, I simply get the HTML of a custom 404 page. Look at the r.status_code attribute:Demo:If you want requests to raise an exception for error codes (4xx or 5xx), call r.raise_for_status():You can also test the response object in a boolean context; if the status code is not an error code (4xx or 5xx), it is considered ‘true’:If you want to be more explicit, use if r.ok:. 

Redirecting stdout to「nothing」in python

Pushpak Dagade

[Redirecting stdout to「nothing」in python](https://stackoverflow.com/questions/6735917/redirecting-stdout-to-nothing-in-python)

I have a large project consisting of sufficiently large number of modules, each printing something to the standard output. Now as the project has grown in size, there are large no. of print statements printing a lot on the std out which has made the program considerably slower.So, I now want to decide at runtime whether or not to print anything to the stdout. I cannot make changes in the modules as there are plenty of them. (I know I can redirect the stdout to a file but even this is considerably slow.)  So my question is how do I redirect the stdout to nothing ie how do I make the print statement do nothing?Currently the only idea I have is to make a class which has a write method (which does nothing) and redirect the stdout to an instance of this class.Is there an inbuilt mechanism in python for this? Or is there something better than this?

2011-07-18 16:12:28Z

I have a large project consisting of sufficiently large number of modules, each printing something to the standard output. Now as the project has grown in size, there are large no. of print statements printing a lot on the std out which has made the program considerably slower.So, I now want to decide at runtime whether or not to print anything to the stdout. I cannot make changes in the modules as there are plenty of them. (I know I can redirect the stdout to a file but even this is considerably slow.)  So my question is how do I redirect the stdout to nothing ie how do I make the print statement do nothing?Currently the only idea I have is to make a class which has a write method (which does nothing) and redirect the stdout to an instance of this class.Is there an inbuilt mechanism in python for this? Or is there something better than this?Cross-platform:On Windows:On Linux:A nice way to do this is to create a small context processor that you wrap your prints in. You then just use is in a with-statement to silence all output.Running this code only prints the second line of output, not the first:This works cross-platform (Windows + Linux + Mac OSX), and is cleaner than the ones other answers imho.If you're in python 3.4 or higher, there's a simple and safe solution using the standard library:(at least on my system) it appears that writing to os.devnull is about 5x faster than writing to a DontPrint class, i.e.gave the following output:If you're in a Unix environment (Linux included), you can redirect output to /dev/null:And for Windows:How about this:This uses the features in the contextlib module to hide the output of whatever command you are trying to run, depending on the result of should_hide_output(), and then restores the output behavior after that function is done running.If you want to hide standard error output, then import redirect_stderr from contextlib and add a line saying stack.enter_context(redirect_stderr(null_stream)).The main downside it that this only works in Python 3.4 and later versions.Your class will work just fine (with the exception of the write() method name -- it needs to be called write(), lowercase). Just make sure you save a copy of sys.stdout in another variable.If you're on a *NIX, you can do sys.stdout = open('/dev/null'), but this is less portable than rolling your own class.You can just mock it.Why don't you try this?It is OK for print() case. But it can cause an error if you call any method of sys.stdout, e.g. sys.stdout.write().There is a note in docs:Supplement to iFreilicht's answer - it works for both python 2 & 3.

How to get rid of punctuation using NLTK tokenizer?

lizarisk

[How to get rid of punctuation using NLTK tokenizer?](https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer)

I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use nltk.word_tokenize(), I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also word_tokenize doesn't work with multiple sentences: dots are added to the last word.

2013-03-21 12:22:07Z

I'm just starting to use NLTK and I don't quite understand how to get a list of words from text. If I use nltk.word_tokenize(), I get a list of words and punctuation. I need only the words instead. How can I get rid of punctuation? Also word_tokenize doesn't work with multiple sentences: dots are added to the last word.Take a look at the other tokenizing options that nltk provides here. For example, you can define a tokenizer that picks out sequences of alphanumeric characters as tokens and drops everything else:Output:You do not really need NLTK to remove punctuation. You can remove it with simple python. For strings:Or for unicode:and then use this string in your tokenizer.P.S. string module have some other sets of elements that can be removed (like digits).Below code will remove all punctuation marks as well as non alphabetic characters. Copied from their book.http://www.nltk.org/book/ch01.html outputAs noticed in comments start with sent_tokenize(), because word_tokenize() works only on a single sentence. You can filter out punctuation with filter(). And if you have an unicode strings make sure that is a unicode object (not a 'str' encoded with some encoding like 'utf-8'). I just used the following code, which removed all the punctuation:I think you need some sort of regular expression matching (the following code is in Python 3):Output:Should work well in most cases since it removes punctuation while preserving tokens like "n't", which can't be obtained from regex tokenizers such as wordpunct_tokenize.I use this code to remove punctuation:And If you want to check whether a token is a valid English word or not, you may need PyEnchantTutorial:Sincerely asking, what is a word? If your assumption is that a word consists of alphabetic characters only, you are wrong since words such as can't will be destroyed into pieces (such as can and t) if you remove punctuation before tokenisation, which is very likely to affect your program negatively.Hence the solution is to tokenise and then remove punctuation tokens....and then if you wish, you can replace certain tokens such as 'm with am.Remove punctuaion(It will remove . as well as part of punctuation handling using below code)Sample Input/Output:['direct', 'flat', 'oberoi', 'esquire', '3', 'bhk', '2195', 'saleable', '1330', 'carpet', 'rate', '14500', 'final', 'plus', '1', 'floor', 'rise', 'tax', 'approx', '9', 'flat', 'cost', 'parking', '389', 'cr', 'plus', 'taxes', 'plus', 'possession', 'charger', 'middle', 'floor', 'north', 'door', 'arey', 'oberoi', 'woods', 'facing', '53', 'paymemt', 'due', '1', 'transfer', 'charge', 'buyer', 'total', 'cost', 'around', '420', 'cr', 'approx', 'plus', 'possession', 'charges', 'rahul', 'soni']

You can do it in one line without nltk (python 3.x).Just adding to the solution by @rmalouf, this will not include any numbers because \w+ is equivalent to [a-zA-Z0-9_]

Type annotations for *args and **kwargs

Praxeolitic

[Type annotations for *args and **kwargs](https://stackoverflow.com/questions/37031928/type-annotations-for-args-and-kwargs)

I'm trying out Python's type annotations with abstract base classes to write some interfaces. Is there a way to annotate the possible types of *args and **kwargs?For example, how would one express that the sensible arguments to a function are either an int or two ints? type(args) gives Tuple so my guess was to annotate the type as Union[Tuple[int, int], Tuple[int]], but this doesn't work.Error messages from mypy:It makes sense that mypy doesn't like this for the function call because it expects there to be a tuple in the call itself. The addition after unpacking also gives a typing error that I don't understand.How does one annotate the sensible types for *args and **kwargs?

2016-05-04 15:21:26Z

I'm trying out Python's type annotations with abstract base classes to write some interfaces. Is there a way to annotate the possible types of *args and **kwargs?For example, how would one express that the sensible arguments to a function are either an int or two ints? type(args) gives Tuple so my guess was to annotate the type as Union[Tuple[int, int], Tuple[int]], but this doesn't work.Error messages from mypy:It makes sense that mypy doesn't like this for the function call because it expects there to be a tuple in the call itself. The addition after unpacking also gives a typing error that I don't understand.How does one annotate the sensible types for *args and **kwargs?For variable positional arguments (*args) and variable keyword arguments (**kw) you only need to specify the expected value for one such argument.From the Arbitrary argument lists and default argument values section of the Type Hints PEP:So you'd want to specify your method like this:However, if your function can only accept either one or two integer values, you should not use *args at all, use one explicit positional argument and a second keyword argument:Now your function is actually limited to one or two arguments, and both must be integers if specified. *args always means 0 or more, and can't be limited by type hints to a more specific range.As a short addition to the previous answer, if you're trying to use mypy on Python 2 files and need to use comments to add types instead of annotations, you need to prefix the types for args and kwargs with * and ** respectively:This is treated by mypy as being the same as the below, Python 3.5 version of foo:The proper way to do this is using @overloadNote that you do not add @overload or type annotations to the actual implementation, which must come last.You'll need a newish version of both typing and mypy to get support for @overload outside of stub files.You can also use this to vary the returned result in a way that makes explicit which argument types correspond with which return type. e.g.:

How should I structure a Python package that contains Cython code

Craig McQueen

[How should I structure a Python package that contains Cython code](https://stackoverflow.com/questions/4505747/how-should-i-structure-a-python-package-that-contains-cython-code)

I'd like to make a Python package containing some Cython code. I've got the the Cython code working nicely. However, now I want to know how best to package it.For most people who just want to install the package, I'd like to include the .c file that Cython creates, and arrange for setup.py to compile that to produce the module. Then the user doesn't need Cython installed in order to install the package.But for people who may want to modify the package, I'd also like to provide the Cython .pyx files, and somehow also allow for setup.py to build them using Cython (so those users would need Cython installed).How should I structure the files in the package to cater for both these scenarios?The Cython documentation gives a little guidance. But it doesn't say how to make a single setup.py that handles both the with/without Cython cases.

2010-12-22 02:44:31Z

I'd like to make a Python package containing some Cython code. I've got the the Cython code working nicely. However, now I want to know how best to package it.For most people who just want to install the package, I'd like to include the .c file that Cython creates, and arrange for setup.py to compile that to produce the module. Then the user doesn't need Cython installed in order to install the package.But for people who may want to modify the package, I'd also like to provide the Cython .pyx files, and somehow also allow for setup.py to build them using Cython (so those users would need Cython installed).How should I structure the files in the package to cater for both these scenarios?The Cython documentation gives a little guidance. But it doesn't say how to make a single setup.py that handles both the with/without Cython cases.I've done this myself now, in a Python package simplerandom (BitBucket repo - EDIT: now github) (I don't expect this to be a popular package, but it was a good chance to learn Cython).This method relies on the fact that building a .pyx file with Cython.Distutils.build_ext (at least with Cython version 0.14) always seems to create a .c file in the same directory as the source .pyx file.Here is a cut-down version of setup.py which I hope shows the essentials:I also edited MANIFEST.in to ensure that mycythonmodule.c is included in a source distribution (a source distribution that is created with python setup.py sdist):I don't commit mycythonmodule.c to version control 'trunk' (or 'default' for Mercurial). When I make a release, I need to remember to do a python setup.py build_ext first, to ensure that mycythonmodule.c is present and up-to-date for the source code distribution. I also make a release branch, and commit the C file into the branch. That way I have a historical record of the C file that was distributed with that release.Adding to Craig McQueen's answer: see below for how to override the sdist command to have Cython automatically compile your source files before creating a source distribution.That way your run no risk of accidentally distributing outdated C sources. It also helps in the case where you have limited control over the distribution process e.g. when automatically creating distributions from continuous integration etc.http://docs.cython.org/en/latest/src/userguide/source_files_and_compilation.html#distributing-cython-modulesThe easiest is to include both but just use the c-file? Including the .pyx file is nice, but it's not needed once you have the .c file anyway. People who want to recompile the .pyx can install Pyrex and do it manually.Otherwise you need to have a custom build_ext command for distutils that builds the C file first. Cython already includes one. http://docs.cython.org/src/userguide/source_files_and_compilation.htmlWhat that documentation doesn't do is say how to make this conditional, but Should handle it.Including (Cython) generated .c files are pretty weird. Especially when we include that in git. I'd prefer to use setuptools_cython. When Cython is not available, it will build an egg which has built-in Cython environment, and then build your code using the egg.A possible example: https://github.com/douban/greenify/blob/master/setup.pyUpdate(2017-01-05):Since setuptools 18.0, there's no need to use setuptools_cython. Here is an example to build Cython project from scratch without setuptools_cython.This is a setup script I wrote which makes it easier to include nested directories inside the build. One needs to run it from folder within a package. Givig structure like this:setup.pyHappy compiling ;) The simple hack I came up with:Just install Cython if it could not be imported. One should probably not share this code, but for my own dependencies it's good enough.The easiest way I found using only setuptools instead of the feature limited distutils isAll other answers either rely on A modern solution is to use setuptools instead, see this answer (automatic handling of Cython extensions requires setuptools 18.0, i.e., it's available for many years already). A modern standard setup.py with requirements handling, an entry point, and a cython module could look like this:I think I found a pretty good way of doing this by providing a custom build_ext command.  The idea is the following:Here's the code:This allows one to just write the setup() arguments without worrying about imports and whether one has cython available:

How to convert a string to a list in Python?

Clinteney Hui

[How to convert a string to a list in Python?](https://stackoverflow.com/questions/5387208/how-to-convert-a-string-to-a-list-in-python)

How do you convert a string into a list?Say the string is like text = "a,b,c". After the conversion, text == ['a', 'b', 'c'] and hopefully text[0] == 'a', text[1] == 'b'?

2011-03-22 05:26:58Z

How do you convert a string into a list?Say the string is like text = "a,b,c". After the conversion, text == ['a', 'b', 'c'] and hopefully text[0] == 'a', text[1] == 'b'?Like this:Alternatively, you can use eval() if you trust the string to be safe:Just to add on to the existing answers: hopefully, you'll encounter something more like this in the future:But what you're dealing with right now, go with @Cameron's answer.The following Python code will turn your string into a list of strings:In python you seldom need to convert a string to a list, because strings and lists are very similarIf you really have a string which should be a character array, do this:Note that Strings are very much like lists in pythonStrings are lists. Almost.If you actually want arrays:If you do not need arrays, and only want to look by index at your characters, remember a string is an iterable, just like a list except the fact that it is immutable:In case you want to split by spaces, you can just use .split():Output:All answers are good, there is another way of doing, which is list comprehension, see the solution below. for comma separated list do the following I usually use:the strip remove spaces around words.To convert a string having the form a="[[1, 3], [2, -6]]" I wrote yet not optimized code:You should use the built-in translate method for strings.  Type help('abc'.translate) at Python shell for more info.Using functional Python:

Pandas convert dataframe to array of tuples

enrishi

[Pandas convert dataframe to array of tuples](https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples)

I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a "row" of the dataframe.My DataFrame looks something like:I want to convert it to an array of tuples like:Any suggestion on how I can efficiently do this?

2012-03-18 12:53:06Z

I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a "row" of the dataframe.My DataFrame looks something like:I want to convert it to an array of tuples like:Any suggestion on how I can efficiently do this?How about:for pandas < 0.24 use As of 17.1, the above will return a list of namedtuples.If you want a list of ordinary tuples, pass name=None as an argument:A generic way:Motivation

Many data sets are large enough that we need to concern ourselves with speed/efficiency.  So I offer this solution in that spirit.  It happens to also be succinct.For the sake of comparison, let's drop the index columnSolution

I'll propose the use of zip and mapIt happens to also be flexible if we wanted to deal with a specific subset of columns.  We'll assume the columns we've already displayed are the subset we want.Turn's out records is quickest followed by asymptotically converging zipmap and iter_tuplesI'll use a library simple_benchmarks that I got from this postCheck the resultsHere's a vectorized approach (assuming the dataframe, data_set to be defined as df instead) that returns a list of tuples as shown:produces:The idea of setting datetime column as the index axis is to aid in the conversion of the Timestamp value to it's corresponding datetime.datetime format equivalent by making use of the convert_datetime64 argument in DF.to_records which does so for a DateTimeIndex dataframe.This returns a recarray which could be then made to return a list using .tolistMore generalized solution depending on the use case would be:The most efficient and easy way:You can filter the columns you need before this call.This answer doesn't add any answers that aren't already discussed, but here are some speed results. I think this should resolve questions that came up in the comments. All of these look like they are O(n), based on these three values.TL;DR: tuples = list(df.itertuples(index=False, name=None)) and tuples = list(zip(*[df[c].values.tolist() for c in df])) are tied for the fastest.I did a quick speed test on results for three suggestions here:Small size:Gives:Larger:Gives:As much patience as I have:Gives:The zip version and the itertuples version are within the confidence intervals each other. I suspect that they are doing the same thing under the hood.These speed tests are probably irrelevant though. Pushing the limits of my computer's memory doesn't take a huge amount of time, and you really shouldn't be doing this on a large data set. Working with those tuples after doing this will end up being really inefficient. It's unlikely to be a major bottleneck in your code, so just stick with the version you think is most readable.More pythonic way:

Check if key exists and iterate the JSON array using Python

pravi

[Check if key exists and iterate the JSON array using Python](https://stackoverflow.com/questions/24898797/check-if-key-exists-and-iterate-the-json-array-using-python)

I have a bunch of JSON data from Facebook posts like the one below:The JSON data is semi-structured and all is not the same. 

Below is my code:I want the code to print the to_id as 1543 else print 'null'I am not sure how to do this.

2014-07-22 22:16:51Z

I have a bunch of JSON data from Facebook posts like the one below:The JSON data is semi-structured and all is not the same. 

Below is my code:I want the code to print the to_id as 1543 else print 'null'I am not sure how to do this.Output:If all you want is to check if key exists or notIf you want to check if there is a value for keyReturn a default value if actual value is missingIt is a good practice to create helper utility methods for things like that so that whenever you need to change the logic of attribute validation it would be in one place, and the code will be more readable for the followers.For example create a helper method (or class JsonUtils with static methods) in json_utils.py:and then use it in your project:IMPORTANT NOTE:There is a reason I am using data.get(attribute) or default_value instead of simply data.get(attribute, default_value):In my applications getting attribute with value 'null' is the same as not getting the attribute at all. If your usage is different, you need to change this.Try it:Or, if you just want to skip over values missing ids instead of printing 'null':So:Of course in real life, you probably don't want to print each id, but to store them and do something with them, but that's another issue.I wrote a tiny function for this purpose. Feel free to repurpose,

Python: Continuing to next iteration in outer loop

Sahas

[Python: Continuing to next iteration in outer loop](https://stackoverflow.com/questions/1859072/python-continuing-to-next-iteration-in-outer-loop)

I wanted to know if there are any built-in ways to continue to next iteration in outer loop in python.  For example,  consider the code:I want this continue statement to exit the jj loop and goto next item in the ii loop.  I can implement this logic in some other way (by setting a flag variable), but is there an easy way to do this, or is this like asking for too much?

2009-12-07 10:14:11Z

I wanted to know if there are any built-in ways to continue to next iteration in outer loop in python.  For example,  consider the code:I want this continue statement to exit the jj loop and goto next item in the ii loop.  I can implement this logic in some other way (by setting a flag variable), but is there an easy way to do this, or is this like asking for too much?In a general case, when you have multiple levels of looping and break does not work for you (because you want to continue one of the upper loops, not the one right above the current one), you can do one of the followingThe disadvantage is that you may need to pass to that new function some variables, which were previously in scope. You can either just pass them as parameters, make them instance variables on an object (create a new object just for this function, if it makes sense), or global variables, singletons, whatever (ehm, ehm).Or you can define inner as a nested function and let it just capture what it needs (may be slower?)Philosophically, this is what exceptions are for, breaking the program flow through the structured programming building blocks (if, for, while) when necessary.The advantage is that you don't have to break the single piece of code into multiple parts. This is good if it is some kind of computation that you are designing while writing it in Python. Introducing abstractions at this early point may slow you down.Bad thing with this approach is that interpreter/compiler authors usually assume that exceptions are exceptional and optimize for them accordingly.Create a special exception class for this, so that you don't risk accidentally silencing some other exception.I am sure there are still other solutions.Break will break the inner loop, and block1 won't be executed (it will run only if the inner loop is exited normally).In other languages you can label the loop and break from the labelled loop.  Python Enhancement Proposal (PEP) 3136 suggested adding these to Python but Guido rejected it:So if that's what you were hoping for you're out of luck, but look at one of the other answers as there are good options there.I think you could do something like this:Another way to deal with this kind of problem is to use Exception().For example:result:Assuming we want to jump to the outer n loop from m loop if m =3:result:Reference link:http://www.programming-idioms.org/idiom/42/continue-outer-loop/1264/pythonI think one of the easiest ways to achieve this is to replace "continue" with "break" statement,i.e.For example, here is the easy code to see how exactly it goes on:We want to find something and then stop the inner iteration. I use a flag system.I just did something like this. My solution for this was to replace the interior for loop with a list comprehension. where op is some boolean operator acting on a combination of ii and jj. In my case, if any of the operations returned true, I was done.This is really not that different from breaking the code out into a function, but I thought that using the "any" operator to do a logical OR on a list of booleans and doing the logic all in one line was interesting. It also avoids the function call.

How to sort Counter by value? - python

alvas

[How to sort Counter by value? - python](https://stackoverflow.com/questions/20950650/how-to-sort-counter-by-value-python)

Other than doing list comprehensions of reversed list comprehension, is there a pythonic way to sort Counter by value? If so, it is faster than this:

2014-01-06 13:02:52Z

Other than doing list comprehensions of reversed list comprehension, is there a pythonic way to sort Counter by value? If so, it is faster than this:Use the Counter.most_common() method, it'll sort the items for you:It'll do so in the most efficient manner possible; if you ask for a Top N instead of all values, a heapq is used instead of a straight sort:Outside of counters, sorting can always be adjusted based on a key function; .sort() and sorted() both take callable that lets you specify a value on which to sort the input sequence; sorted(x, key=x.get, reverse=True) would give you the same sorting as x.most_common(), but only return the keys, for example:or you can sort on only the value given (key, value) pairs:See the Python sorting howto for more information.A rather nice addition to @MartijnPieters answer is to get back a dictionary sorted by occurrence since Collections.most_common only returns a tuple. I often couple this with a json output for handy log files: With the output:Yes:Using the sorted keyword key and a lambda function:This works for all dictionaries. However Counter has a special function which already gives you the sorted items (from most frequent, to least frequent). It's called most_common():You can also specify how many items you want to see:More general sorted, where the key keyword defines the sorting method, minus before numerical type indicates descending: 

Why can't Python find shared objects that are in directories in sys.path?

Vinay Sajip

[Why can't Python find shared objects that are in directories in sys.path?](https://stackoverflow.com/questions/1099981/why-cant-python-find-shared-objects-that-are-in-directories-in-sys-path)

I'm trying to import pycurl:Now, libcurl.so.4 is in /usr/local/lib. As you can see, this is in sys.path:Any help will be greatly appreciated.

2009-07-08 19:06:17Z

I'm trying to import pycurl:Now, libcurl.so.4 is in /usr/local/lib. As you can see, this is in sys.path:Any help will be greatly appreciated.sys.path is only searched for Python modules. For dynamic linked libraries, the paths searched must be in LD_LIBRARY_PATH. Check if your LD_LIBRARY_PATH includes /usr/local/lib, and if it doesn't, add it and try again.Some more information (source):Update: to set LD_LIBRARY_PATH, use one of the following, ideally in your ~/.bashrc 

or equivalent file:orUse the first form if it's empty (equivalent to the empty string, or not present at all), and the second form if it isn't. Note the use of export.Ensure your libcurl.so module is in the system library path, which is distinct and separate from the python library path.A "quick fix" is to add this path to a LD_LIBRARY_PATH variable. However, setting that system wide (or even account wide) is a BAD IDEA, as it is possible to set it in such a way that some programs will find a library it shouldn't, or even worse, open up security holes.If your "locally installed libraries" are installed in, for example, /usr/local/lib, add this directory to /etc/ld.so.conf (it's a text file) and run "ldconfig"The command will run a caching utility, but will also create all the necessary "symbolic links" required for the loader system to function. It is surprising that the "make install" for libcurl did not do this already, but it's possible it could not if /usr/local/lib is not in /etc/ld.so.conf already.PS: it's possible that your /etc/ld.so.conf contains nothing but "include ld.so.conf.d/*.conf". You can still add a directory path after it, or just create a new file inside the directory it's being included from. Dont forget to run "ldconfig" after it.Be careful. Getting this wrong can screw up your system. Additionally: make sure your python module is compiled against THAT version of libcurl. If you just copied some files over from another system, this wont always work. If in doubt, compile your modules on the system you intend to run them on.You can also set LD_RUN_PATH to /usr/local/lib in your user environment when you compile pycurl in the first place. This will embed /usr/local/lib in the RPATH attribute of the C extension module .so so that it automatically knows where to find the library at run time without having to have LD_LIBRARY_PATH set at run time.Had the exact same issue. I installed curl 7.19 to /opt/curl/ to make sure that I would not affect current curl on our production servers. 

Once I linked libcurl.so.4 to /usr/lib:sudo ln -s /opt/curl/lib/libcurl.so /usr/lib/libcurl.so.4I still got the same error! Durf.But running ldconfig make the linkage for me and that worked. No need to set the LD_RUN_PATH or LD_LIBRARY_PATH at all. Just needed to run ldconfig.As a supplement to above answers - I'm just bumping into a similar problem, and working completely of the default installed python. When I call the example of the shared object library I'm looking for with LD_LIBRARY_PATH, I get something like this: Notably, it doesn't even complain about the import - it complains about the source file!But if I force loading of the object using LD_PRELOAD:... I immediately get a more meaningful error message - about a missing dependency!Just thought I'd jot this down here - cheers!I use python setup.py build_ext -R/usr/local/lib -I/usr/local/include/libcalg-1.0 and the compiled .so file is under the build folder.

you can type python setup.py --help build_ext to see the explanations of -R and -I

What is the difference between lemmatization vs stemming?

TIMEX

[What is the difference between lemmatization vs stemming?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)

When do I use each ?Also...is the NLTK lemmatization dependent upon Parts of Speech?

Wouldn't it be more accurate if it was?

2009-11-24 00:48:11Z

When do I use each ?Also...is the NLTK lemmatization dependent upon Parts of Speech?

Wouldn't it be more accurate if it was?Short and dense: http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.htmlFrom the NLTK docs:Source: https://en.wikipedia.org/wiki/LemmatisationThere are two aspects to show their differences:Reference http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatizationThe purpose of both stemming and lemmatization is to reduce morphological variation. This is in contrast to the the more general "term conflation" procedures, which may also address lexico-semantic, syntactic, or orthographic variations. The real difference between stemming and lemmatization is threefold:Lemmatization may also be backed up by a part-of-speech tagger in order to disambiguate homonyms.As MYYN pointed out, stemming is the process of removing inflectional and sometimes derivational affixes to a base form that all of the original words are probably related to.  Lemmatization is concerned with obtaining the single word that allows you to group together a bunch of inflected forms.  This is harder than stemming because it requires taking the context into account (and thus the meaning of the word), while stemming ignores context.As for when you would use one or the other, it's a matter of how much your application depends on getting the meaning of a word in context correct.  If you're doing machine translation, you probably want lemmatization to avoid mistranslating a word.  If you're doing information retrieval over a billion documents with 99% of your queries ranging from 1-3 words, you can settle for stemming.As for NLTK, the WordNetLemmatizer does use the part of speech, though you have to provide it (otherwise it defaults to nouns).  Passing it "dove" and "v" yields "dive" while "dove" and "n" yields "dove".An example-driven explanation on the differenes between lemmatization and stemming:Lemmatization handles matching「car」to「cars」along

with matching「car」to「automobile」.  Stemming handles matching「car」to「cars」.http://www.ideaeng.com/stemming-lemmatization-0601ianacl

but i think Stemming is a rough hack people use to get all the different forms of the same word down to a base form which need not be a legit word on its own

Something like the Porter Stemmer can uses simple regexes to eliminate common word suffixesLemmatization brings a word down to its actual base form which, in the case of irregular verbs, might look nothing like the input word

Something like Morpha which uses FSTs to bring nouns and verbs to their base formStemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context.  Here are the examples to illustrate all the differences and use cases: 

How to sort my paws?

Ivo Flipse

[How to sort my paws?](https://stackoverflow.com/questions/4502656/how-to-sort-my-paws)

In my previous question I got an excellent answer that helped me detect where a paw hit a pressure plate, but now I'm struggling to link these results to their corresponding paws:I manually annotated the paws (RF=right front, RH= right hind, LF=left front, LH=left hind).As you can see there's clearly a repeating pattern and it comes back in almost every measurement. Here's a link to a presentation of 6 trials that were manually annotated.My initial thought was to use heuristics to do the sorting, like:However, I’m a bit skeptical about my heuristics, as they would fail on me as soon as I encounter a variation I hadn’t thought off. They also won’t be able to cope with measurements from lame dogs, whom probably have rules of their own. Furthermore, the annotation suggested by Joe sometimes get's messed up and doesn't take into account what the paw actually looks like.Based on the answers I received on my question about peak detection within the paw, I’m hoping there are more advanced solutions to sort the paws. Especially because the pressure distribution and the progression thereof are different for each separate paw, almost like a fingerprint. I hope there's a method that can use this to cluster my paws, rather than just sorting them in order of occurrence. So I'm looking for a better way to sort the results with their corresponding paw. For anyone up to the challenge, I have pickled a dictionary with all the sliced arrays that contain the pressure data of each paw (bundled by measurement) and the slice that describes their location (location on the plate and in time). To clarfiy: walk_sliced_data is a dictionary that contains ['ser_3', 'ser_2', 'sel_1', 'sel_2', 'ser_1', 'sel_3'], which are the names of the measurements. Each measurement contains another dictionary, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (example from 'sel_1') which represent the impacts that were extracted. Also note that 'false' impacts, such as where the paw is partially measured (in space or time) can be ignored. They are only useful because they can help recognizing a pattern, but 

won't be analyzed. And for anyone interested, I’m keeping a blog with all the updates regarding the project!

2010-12-21 18:32:19Z

In my previous question I got an excellent answer that helped me detect where a paw hit a pressure plate, but now I'm struggling to link these results to their corresponding paws:I manually annotated the paws (RF=right front, RH= right hind, LF=left front, LH=left hind).As you can see there's clearly a repeating pattern and it comes back in almost every measurement. Here's a link to a presentation of 6 trials that were manually annotated.My initial thought was to use heuristics to do the sorting, like:However, I’m a bit skeptical about my heuristics, as they would fail on me as soon as I encounter a variation I hadn’t thought off. They also won’t be able to cope with measurements from lame dogs, whom probably have rules of their own. Furthermore, the annotation suggested by Joe sometimes get's messed up and doesn't take into account what the paw actually looks like.Based on the answers I received on my question about peak detection within the paw, I’m hoping there are more advanced solutions to sort the paws. Especially because the pressure distribution and the progression thereof are different for each separate paw, almost like a fingerprint. I hope there's a method that can use this to cluster my paws, rather than just sorting them in order of occurrence. So I'm looking for a better way to sort the results with their corresponding paw. For anyone up to the challenge, I have pickled a dictionary with all the sliced arrays that contain the pressure data of each paw (bundled by measurement) and the slice that describes their location (location on the plate and in time). To clarfiy: walk_sliced_data is a dictionary that contains ['ser_3', 'ser_2', 'sel_1', 'sel_2', 'ser_1', 'sel_3'], which are the names of the measurements. Each measurement contains another dictionary, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (example from 'sel_1') which represent the impacts that were extracted. Also note that 'false' impacts, such as where the paw is partially measured (in space or time) can be ignored. They are only useful because they can help recognizing a pattern, but 

won't be analyzed. And for anyone interested, I’m keeping a blog with all the updates regarding the project!Alright! I've finally managed to get something working consistently! This problem pulled me in for several days... Fun stuff! Sorry for the length of this answer, but I need to elaborate a bit on some things... (Though I may set a record for the longest non-spam stackoverflow answer ever!)As a side note, I'm using the full dataset that Ivo provided a link to in his original question.  It's a series of rar files (one-per-dog) each containing several different experiment runs stored as ascii arrays. Rather than try to copy-paste stand-alone code examples into this question, here's a bitbucket mercurial repository with full, stand-alone code. You can clone it with hg clone https://joferkington@bitbucket.org/joferkington/paw-analysisOverviewThere are essentially two ways to approach the problem, as you noted in your question. I'm actually going to use both in different ways.Basically, the first method works with the dog's paws follow the trapezoidal-like pattern shown in Ivo's question above, but fails whenever the paws don't follow that pattern. It's fairly easy to programatically detect when it doesn't work.  Therefore, we can use the measurements where it did work to build up a training dataset (of ~2000 paw impacts from ~30 different dogs) to recognize which paw is which, and the problem reduces to a supervised classification (With some additional wrinkles... Image recognition is a bit harder than a "normal" supervised classification problem).Pattern AnalysisTo elaborate on the first method, when a dog is walking (not running!) normally (which some of these dogs may not be), we expect paws to impact in the order of: Front Left, Hind Right, Front Right, Hind Left, Front Left, etc. The pattern may start with either the front left or front right paw.If this were always the case, we could simply sort the impacts by initial contact time and use a modulo 4 to group them by paw.  However, even when everything is "normal", this doesn't work.  This is due to the trapezoid-like shape of the pattern. A hind paw spatially falls behind the previous front paw.Therefore, the hind paw impact after the initial front paw impact often falls off the sensor plate, and isn't recorded.  Similarly, the last paw impact is often not the next paw in the sequence, as the paw impact before it occured off the sensor plate and wasn't recorded.Nonetheless, we can use the shape of the paw impact pattern to determine when this has happened, and whether we've started with a left or right front paw.  (I'm actually ignoring problems with the last impact here. It's not too hard to add it, though.)In spite of all of this, it frequently doesn't work correctly. Many of the dogs in the full dataset appear to be running, and the paw impacts don't follow the same temporal order as when the dog is walking. (Or perhaps the dog just has severe hip problems...)Fortunately, we can still programatically detect whether or not the paw impacts follow our expected spatial pattern:Therefore, even though the simple spatial classification doesn't work all of the time, we can determine when it does work with reasonable confidence.  Training DatasetFrom the pattern-based classifications where it worked correctly, we can build up a very large training dataset of correctly classified paws (~2400 paw impacts from 32 different dogs!).  We can now start to look at what an "average" front left, etc, paw looks like.To do this, we need some sort of "paw metric" that is the same dimensionality for any dog. (In the full dataset, there are both very large and very small dogs!)  A paw print from an Irish elkhound will be both much wider and much "heavier" than a paw print from a toy poodle.  We need to rescale each paw print so that a) they have the same number of pixels, and b) the pressure values are standardized.  To do this, I resampled each paw print onto a 20x20 grid and rescaled the pressure values based on the maximum, mininum, and mean pressure value for the paw impact. After all of this, we can finally take a look at what an average left front, hind right, etc paw looks like.  Note that this is averaged across >30 dogs of greatly different sizes, and we seem to be getting consistent results!However, before we do any analysis on these, we need to subtract the mean (the average paw for all legs of all dogs).Now we can analyize the differences from the mean, which are a bit easier to recognize:Image-based Paw RecognitionOk... We finally have a set of patterns that we can begin to try to match the paws against.  Each paw can be treated as a 400-dimensional vector (returned by the paw_image function) that can be compared to these four 400-dimensional vectors.  Unfortunately, if we just use a "normal" supervised classification algorithm (i.e. find which of the 4 patterns is closest to a particular paw print using a simple distance), it doesn't work consistently. In fact, it doesn't do much better than random chance on the training dataset.This is a common problem in image recognition. Due to the high dimensionality of the input data, and the somewhat "fuzzy" nature of images (i.e. adjacent pixels have a high covariance), simply looking at the difference of an image from a template image does not give a very good measure of the similarity of their shapes.  EigenpawsTo get around this we need to build a set of "eigenpaws" (just like "eigenfaces" in facial recognition), and describe each paw print as a combination of these eigenpaws.  This is identical to principal components analysis, and basically provides a way to reduce the dimensionality of our data, so that distance is a good measure of shape.Because we have more training images than dimensions (2400 vs 400), there's no need to do "fancy" linear algebra for speed. We can work directly with the covariance matrix of the training data set:These basis_vecs are the "eigenpaws".To use these, we simply dot (i.e. matrix multiplication) each paw image (as a 400-dimensional vector, rather than a 20x20 image) with the basis vectors. This gives us a 50-dimensional vector (one element per basis vector) that we can use to classify the image. Instead of comparing a 20x20 image to the 20x20 image of each "template" paw, we compare the 50-dimensional, transformed image to each 50-dimensional transformed template paw.  This is much less sensitive to small variations in exactly how each toe is positioned, etc, and basically reduces the dimensionality of the problem to just the relevant dimensions.Eigenpaw-based Paw ClassificationNow we can simply use the distance between the 50-dimensional vectors and the "template" vectors for each leg to classify which paw is which:Here are some of the results:

Remaining ProblemsThere are still some problems, particularly with dogs too small to make a clear pawprint... (It works best with large dogs, as the toes are more clearly seperated at the sensor's resolution.) Also, partial pawprints aren't recognized with this system, while they can be with the trapezoidal-pattern-based system. However, because the eigenpaw analysis inherently uses a distance metric, we can classify the paws both ways, and fall back to the trapezoidal-pattern-based system when the eigenpaw analysis's smallest distance from the "codebook" is over some threshold. I haven't implemented this yet, though.Phew... That was long! My hat is off to Ivo for having such a fun question!Using the information purely based on duration, I think you could apply techniques from modeling kinematics; namely Inverse Kinematics. Combined with orientation, length, duration, and total weight it gives some level of periodicity which, I would hope could be the first step trying to solve your "sorting of paws" problem.All that data could be used to create a list of bounded polygons (or tuples), which you could use to sort by step size then by paw-ness [index].Can you have the technician running the test manually enter the first paw (or first two)? The process might be:

Get selected subcommand with argparse

Adam Schmideg

[Get selected subcommand with argparse](https://stackoverflow.com/questions/4575747/get-selected-subcommand-with-argparse)

When I use subcommands with python argparse, I can get the selected arguments.So args doesn't contain 'foo'.  Simply writing sys.argv[1] doesn't work because of the possible global args. How can I get the subcommand itself?

2011-01-01 20:59:17Z

When I use subcommands with python argparse, I can get the selected arguments.So args doesn't contain 'foo'.  Simply writing sys.argv[1] doesn't work because of the possible global args. How can I get the subcommand itself?The very bottom of the Python docs on argparse sub-commands explains how to do this:You can also use the set_defaults() method referenced just above the example I found.ArgumentParser.add_subparsers has dest formal argument described as:In the example below of a simple task function layout using subparsers, the selected subparser is in parser.parse_args().subparser.

Plotting in a non-blocking way with Matplotlib

opetroch

[Plotting in a non-blocking way with Matplotlib](https://stackoverflow.com/questions/28269157/plotting-in-a-non-blocking-way-with-matplotlib)

I have been playing with Numpy and matplotlib in the last few days. I am having problems trying to make matplotlib plot a function without blocking execution. I know there are already many threads here on SO asking similar questions, and I 've googled quite a lot but haven't managed to make this work.I have tried using show(block=False) as some people suggest, but all I get is a frozen window. If I simply call show(), the result is plotted properly but execution is blocked until the window is closed. From other threads I 've read, I suspect that whether show(block=False) works or not depends on the backend. Is this correct? My back end is Qt4Agg. Could you have a look at my code and tell me if you see something wrong? Here is my code. Thanks for any help.PS. I forgot to say that I would like to update the existing window every time I plot something, instead of creating a new one.

2015-02-01 23:23:00Z

I have been playing with Numpy and matplotlib in the last few days. I am having problems trying to make matplotlib plot a function without blocking execution. I know there are already many threads here on SO asking similar questions, and I 've googled quite a lot but haven't managed to make this work.I have tried using show(block=False) as some people suggest, but all I get is a frozen window. If I simply call show(), the result is plotted properly but execution is blocked until the window is closed. From other threads I 've read, I suspect that whether show(block=False) works or not depends on the backend. Is this correct? My back end is Qt4Agg. Could you have a look at my code and tell me if you see something wrong? Here is my code. Thanks for any help.PS. I forgot to say that I would like to update the existing window every time I plot something, instead of creating a new one.I spent a long time looking for solutions, and found this answer.It looks like, in order to get what you (and I) want, you need the combination of plt.ion(), plt.show() (not with block=False) and, most importantly, plt.pause(.001) (or whatever time you want). The pause is needed because the GUI events happen while the main code is sleeping, including drawing. It's possible that this is implemented by picking up time from a sleeping thread, so maybe IDEs mess with that—I don't know.Here's an implementation that works for me on python 3.5: A simple trick that works for me is the following:Example:Note: plt.show() is the last line of my script.You can avoid blocking execution by writing the plot to an array, then displaying the array in a different thread. Here is an example of generating and displaying plots simultaneously using pf.screen from pyformulas 0.2.8:Result:Disclaimer: I'm the maintainer for pyformulas.Reference: Matplotlib: save plot to numpy arrayA lot of these answers are super inflated and from what I can find, the answer isn't all that difficult to understand.You can use plt.ion() if you want, but I found using plt.draw() just as effectiveFor my specific project I'm plotting images, but you can use plot() or scatter() or whatever instead of figimage(), it doesn't matter.OrIf you're using an actual figure.

I used @krs013, and @Default Picture's answers to figure this out

Hopefully this saves someone from having launch every single figure on a separate thread, or from having to read these novels just to figure this outif the amount of data is too much you can lower the update rate with a simple counterThis was my actual problem that couldn't find satisfactory answer for, I wanted plotting that didn't close after the script was finished (like MATLAB), If you think about it, after the script is finished, the program is terminated and there is no logical way to hold the plot this way, so there are two optionsthis wasn't satisfactory for me so I found another solution outside of the boxFor this the saving and viewing should be both fast and the viewer shouldn't lock the file and should update the content automaticallyvector based formats are both small and fastFor PDF there are several good optionsSample code for outputing plot to a fileafter first run, open the output file in one of the viewers mentioned above and enjoy.Here is a screenshot of VSCode alongside SumatraPDF, also the process is fast enough to get semi-live update rate (I can get near 10Hz on my setup just use time.sleep() between intervals)

Iggy's answer was the easiest for me to follow, but I got the following error when doing a subsequent subplot command that was not there when I was just doing show:In order to avoid this error, it helps to close (or clear) the plot after the user hits enter.Here's the code that worked for me:The Python package drawnow allows to update a plot in real time in a non blocking way.

It also works with a webcam and OpenCV for example to plot measures for each frame.

See the original post.

How to avoid explicit 'self' in Python?

bguiz

[How to avoid explicit 'self' in Python?](https://stackoverflow.com/questions/1984104/how-to-avoid-explicit-self-in-python)

I have been learning Python by following some pygame tutorials.Therein I found extensive use of the keyword self, and coming from a primarily Java background, I find that I keep forgetting to type self.  For example, instead of self.rect.centerx I would type rect.centerx, because, to me, rect is already a member variable of the class.The Java parallel I can think of for this situation is having to prefix all references to member variables with this.Am I stuck prefixing all member variables with self, or is there a way to declare them that would allow me to avoid having to do so?Even if what I am suggesting isn't pythonic, I'd still like to know if it is possible.I have taken a look at these related SO questions, but they don't quite answer what I am after:

2009-12-31 05:50:07Z

I have been learning Python by following some pygame tutorials.Therein I found extensive use of the keyword self, and coming from a primarily Java background, I find that I keep forgetting to type self.  For example, instead of self.rect.centerx I would type rect.centerx, because, to me, rect is already a member variable of the class.The Java parallel I can think of for this situation is having to prefix all references to member variables with this.Am I stuck prefixing all member variables with self, or is there a way to declare them that would allow me to avoid having to do so?Even if what I am suggesting isn't pythonic, I'd still like to know if it is possible.I have taken a look at these related SO questions, but they don't quite answer what I am after:Python requires specifying self.  The result is there's never any confusion over what's a member and what's not, even without the full class definition visible.  This leads to useful properties, such as: you can't add members which accidentally shadow non-members and thereby break code.One extreme example: you can write a class without any knowledge of what base classes it might have, and always know whether you are accessing a member or not:That's the complete code!  (some_function returns the type used as a base.)Another, where the methods of a class are dynamically composed:Remember, both of these examples are extreme and you won't see them every day, nor am I suggesting you should often write code like this, but they do clearly show aspects of self being explicitly required.Previous answers are all basically variants of "you can't" or "you shouldn't". While I agree with the latter sentiment, the question is technically still unanswered. Furthermore, there are legitimate reasons why someone might want to do something along the lines of what the actual question is asking. One thing I run into sometimes is lengthy math equations where using long names makes the equation unrecognizable. Here are a couple ways of how you could do this in a canned example:The third example - i.e. using for k in self.__dict__ : exec(k+'= self.'+k) is basically what the question is actually asking for, but let me be clear that I don't think it is generally a good idea.For more info, and ways to iterate through class variables, or even functions, see answers and discussion to this question. For a discussion of other ways to dynamically name variables, and why this is usually not a good idea see this blog post.Actually self is not a keyword, it's just the name conventionally given to the first parameter of instance methods in Python. And that first parameter can't be skipped, as it's the only mechanism a method has of knowing which instance of your class it's being called on.You can use whatever name you want, for exampleor evenbut you are stuck with using a name for the scope.I do not recommend you use something different to self unless you have a convincing reason, as it would make it alien for experienced pythonistas. yes, you must always specify self, because explicit is better than implicit, according to python philosophy.You will also find out that the way you program in python is very different from the way you program in java, hence the use of self tends to decrease because you don't project everything inside the object. Rather, you make larger use of module-level function, which can be better tested.by the way. I hated it at first, now I hate the opposite. same for indented-driven flow control.self is part of the python syntax to access members of objects, so I'm afraid you're stuck with itThe "self" is the conventional placeholder of the current object instance of a class. Its used when you want to refer to the object's property or field or method inside a class as if you're referring to "itself". But to make it shorter someone in the Python programming realm started to use "self" , other realms use "this" but they make it as a keyword which cannot be replaced. I rather used "its" to increase the code readability. Its one of the good things in Python - you have a freedom to choose your own placeholder for the object's instance other than "self".

Example for self:Now we replace 'self' with 'its':which is more readable now?Yeah, self is tedious. But, is it better?Actually you can use recipe "Implicit self" from Armin Ronacher presentation "5 years of bad ideas" ( google it).It's a very clever recipe, as almost everything from Armin Ronacher, but I don't think this idea is very appealing. I think I'd prefer explicit this in C#/Java.Update. Link to "bad idea recipe": https://speakerdeck.com/mitsuhiko/5-years-of-bad-ideas?slide=58From: Self Hell - More stateful functions.

No such file or directory「limits.h」when installing Pillow on Alpine Linux

uberma

[No such file or directory「limits.h」when installing Pillow on Alpine Linux](https://stackoverflow.com/questions/30624829/no-such-file-or-directory-limits-h-when-installing-pillow-on-alpine-linux)

I'm running alpine-linux on a Raspberry Pi 2.  I'm trying to install Pillow via this command:This is the output from the command:I think this is probably the relevant section:My research shows it's probably something with the header files.  I have installed these:

2015-06-03 15:45:03Z

I'm running alpine-linux on a Raspberry Pi 2.  I'm trying to install Pillow via this command:This is the output from the command:I think this is probably the relevant section:My research shows it's probably something with the header files.  I have installed these:Alpine Linux uses musl libc. You probably need to install musl-dev.@zakaria answer is correct, but if you stumble uponthen you need the package linux-headers (notice the prefix linux before limits.hlimits.h is located in libc-dev:I had very similar problem with installing python library regex in docker pyhton:3.6-alpine image, Alpine linux >= 3.3.I had to add gcc and musl-dev packagesI've found some python packages fail to install via pip install but work if you install the associated alpine linux package. For example pip install uwsgi fails complaining about limits.h, but apk add uwsgi-python works fine. Suggest trying apk add py-pillow instead of pip install pillow.

initialize a numpy array

Curious2learn

[initialize a numpy array](https://stackoverflow.com/questions/4535374/initialize-a-numpy-array)

Is there way to initialize a numpy array of a shape and add to it? I will explain what I need with a list example. If I want to create a list of objects generated in a loop, I can do:I want to do something similar with a numpy array. I know about vstack, concatenate etc. However, it seems these require two numpy arrays as inputs. What I need is:The big_array should have a shape (10,4). How to do this?EDIT:I want to add the following clarification. I am aware that I can define big_array = numpy.zeros((10,4)) and then fill it up. However, this requires specifying the size of big_array in advance. I know the size in this case, but what if I do not? When we use the .append function for extending the list in python, we don't need to know its final size in advance. I am wondering if something similar exists for creating a bigger array from smaller arrays, starting with an empty array.

2010-12-26 20:52:45Z

Is there way to initialize a numpy array of a shape and add to it? I will explain what I need with a list example. If I want to create a list of objects generated in a loop, I can do:I want to do something similar with a numpy array. I know about vstack, concatenate etc. However, it seems these require two numpy arrays as inputs. What I need is:The big_array should have a shape (10,4). How to do this?EDIT:I want to add the following clarification. I am aware that I can define big_array = numpy.zeros((10,4)) and then fill it up. However, this requires specifying the size of big_array in advance. I know the size in this case, but what if I do not? When we use the .append function for extending the list in python, we don't need to know its final size in advance. I am wondering if something similar exists for creating a bigger array from smaller arrays, starting with an empty array.ororHowever, the mentality in which we construct an array by appending elements to a list is not much used in numpy, because it's less efficient (numpy datatypes are much closer to the underlying C arrays). Instead, you should preallocate the array to the size that you need it to be, and then fill in the rows. You can use numpy.append if you must, though.The way I usually do that is by creating a regular list, then append my stuff into it, and finally transform the list to a numpy array as follows :of course your final object takes twice the space in the memory at the creation step, but appending on python list is very fast, and creation using np.array() also.Introduced in numpy 1.8:Examples:Array analogue for the python'sis:numpy.fromiter() is what you are looking for:It also works with generator expressions, e.g.:If you know the length of the array in advance, you can specify it with an optional 'count' argument.You do want to avoid explicit loops as much as possible when doing array computing, as that reduces the speed gain from that form of computing. There are multiple ways to initialize a numpy array. If you want it filled with zeros, do as katrielalex said:big_array = numpy.zeros((10,4))EDIT: What sort of sequence is it you're making? You should check out the different numpy functions that create arrays, like numpy.linspace(start, stop, size) (equally spaced number), or numpy.arange(start, stop, inc). Where possible, these functions will make arrays substantially faster than doing the same work in explicit loopsFor your first array example use,To initialize big_array, useThis assumes you want to initialize with zeros, which is pretty typical, but there are many other ways to initialize an array in numpy.Edit:

If you don't know the size of big_array in advance, it's generally best to first build a Python list using append, and when you have everything collected in the list, convert this list to a numpy array using numpy.array(mylist).  The reason for this is that lists are meant to grow very efficiently and quickly, whereas numpy.concatenate would be very inefficient since numpy arrays don't change size easily.  But once everything is collected in a list, and you know the final array size, a numpy array can be efficiently constructed.To initialize a numpy array with a specific matrix:output:Whenever you are in the following situation:and you want something similar in numpy, several previous answers have pointed out ways to do it, but as @katrielalex pointed out these methods are not efficient. The efficient way to do this is to build a long list and then reshape it the way you want after you have a long list. For example, let's say I am reading some lines from a file and each row has a list of numbers and I want to build a numpy array of shape (number of lines read, length of vector in each row). Here is how I would do it more efficiently:I realize that this is a bit late, but I did not notice any of the other answers mentioning indexing into the empty array:This way, you preallocate the entire result array with numpy.empty and fill in the rows as you go using indexed assignment.It is perfectly safe to preallocate with empty instead of zeros in the example you gave since you are guaranteeing that the entire array will be filled with the chunks you generate.I'd suggest defining shape first. 

Then iterate over it to insert values.Maybe something like this will fit your needs..Which produces the following output

Importing variables from another file?

Ofek

[Importing variables from another file?](https://stackoverflow.com/questions/17255737/importing-variables-from-another-file)

How can I import variables from one file to another?example: file1 has the variables x1 and x2 how to pass them to file2?How can I import all of the variables from one to another?

2013-06-22 22:04:40Z

How can I import variables from one file to another?example: file1 has the variables x1 and x2 how to pass them to file2?How can I import all of the variables from one to another?will import all objects and methods in file1Import file1 inside file2:To import all variables from file1 without flooding file2's namespace, use:To import all variables from file1 to file2's namespace( not recommended):From the docs:Best to import x1 and x2 explicitly:This allows you to avoid unnecessary namespace conflicts with variables and functions from file1 while working in file2.But if you really want, you can import all the variables:Actually this is not really the same to import a variable with:andAltough at import time x1 and file1.x1 have the same value, they are not the same variables. For instance, call a function in file1 that modifies x1 and then try to print the variable from the main file: you will not see the modified value.Marc response is correct. Actually, you can print the memory address for the variables print(hex(id(libvar)) and you can see the addresses are different.script1.pyscript2.py is where we using script1 variableMethod 1:Method 2:In Python you can access the contents of other files like as if they

 are some kind of a library, compared to other languages like java or any 

 oop base languages , This is really cool ; This makes accessing the contents of the file or import it to to process 

 it or to do anything with it ; 

 And that is the Main reason why Python is highly preferred Language for 

 Data Science and Machine Learning etc. ;And this is the picture of project structure  Where I am accessing variables from .env file where the API links and 

 Secret keys reside .General Structure:

What's the meaning of「(1,) == 1,」in Python?

Pythoner

[What's the meaning of「(1,) == 1,」in Python?](https://stackoverflow.com/questions/37313471/whats-the-meaning-of-1-1-in-python)

I'm testing the tuple structure, and I found it's strange when I use the == operator like:When I assign these two expressions to a variable, the result is true:This questions is different from Python tuple trailing comma syntax rule in my view. I ask the group of expressions between == operator. 

2016-05-19 03:29:25Z

I'm testing the tuple structure, and I found it's strange when I use the == operator like:When I assign these two expressions to a variable, the result is true:This questions is different from Python tuple trailing comma syntax rule in my view. I ask the group of expressions between == operator. Other answers have already shown you that the behaviour is due to operator precedence, as documented here.  I'm going to show you how to find the answer yourself next time you have a question similar to this.  You can deconstruct how the expression parses using the ast module:From this we can see that the code gets parsed as Tim Peters explained: This is just operator precedence.  Your firstgroups like so:so builds a tuple with a single element from the result of comparing the one-element tuple 1, to the integer 1 for equality  They're not equal, so you get the 1-tuple False, for a result.When you do it builds a tuple with the result from comparing the tuple (1,) with an integer and thus returning False.Instead when you assign to variables, the two equal tuples are compared with each other.You can try:

If list index exists, do X

user1569317

[If list index exists, do X](https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x)

In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.I need to code such that if a certain list index exists, then run a function.This is made more complicated by the fact that I have nested if statements about len(my_list).Here's a simplified version of what I have now, which isn't working:

2012-08-02 21:36:20Z

In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.I need to code such that if a certain list index exists, then run a function.This is made more complicated by the fact that I have nested if statements about len(my_list).Here's a simplified version of what I have now, which isn't working:Could it be more useful for you to use the length of the list len(n) to inform your decision rather than checking n[i] for each possible length? I need to code such that if a certain list index exists, then run a function.This is the perfect use for a try block:However, by definition, all items in a Python list between 0 and len(the_list)-1 exist (i.e., there is no need for a try, except if you know 0 <= index < len(the_list)). You can use enumerate if you want the indexes between 0 and the last element:If you are looking for some defined 'index' thought, I think you are asking the wrong question. Perhaps you should consider using a mapping container (such as a dict) versus a sequence container (such as a list). You could rewrite your code like this:Runs like this:You can also use .get method rather than try/except for a shorter version:len(nams) should be equal to n in your code. All indexes 0 <= i < n "exist".It can be done simply using the following code:Using the length of the list would be the fastest solution to check if an index exists:This also tests for negative indices, and most sequence types (Like ranges and strs) that have a length.If you need to access the item at that index afterwards anyways, it is easier to ask forgiveness than permission, and it is also faster and more Pythonic. Use try: except:.This would be as opposed to:You already know how to test for this and in fact are already performing such tests in your code.The valid indices for a list of length n are 0 through n-1 inclusive.Thus, a list has an index i if and only if the length of the list is at least i + 1.If you want to iterate the inserted actors data:ok, so I think it's actually possible (for the sake of argument):You can try something like this Oneliner:Full example:Just for info. Imho, try ... except IndexError is better solution.Do not let any space in front of your brackets.Example:Tip:

You should add comments over and/or under your code. Not behind your code.Have a nice day.A lot of answers, not the simple one.To check if a index 'id' exists at dictionary dict:returns true if 'age' exists.

Matplotlib transparent line plots

Gus

[Matplotlib transparent line plots](https://stackoverflow.com/questions/4320021/matplotlib-transparent-line-plots)

I am plotting two similar trajectories in matplotlib and I'd like to plot each of the lines with partial transparency so that the red (plotted second) doesn't obscure the blue.EDIT: Here's the image with transparent lines.

2010-12-01 00:05:16Z

I am plotting two similar trajectories in matplotlib and I'd like to plot each of the lines with partial transparency so that the red (plotted second) doesn't obscure the blue.EDIT: Here's the image with transparent lines.Plain and simple:(I know I add nothing new, but the straightforward answer should be visible).After I plotted all the lines, I was able to set the transparency of all of them as follows:EDIT: please see Joe's answer in the comments.It really depends on what functions you're using to plot the lines, but try see if the on you're using takes an alpha value and set it to something like 0.5. If that doesn't work, try get the line objects and set their alpha values directly.

Project structure for Google App Engine

Chris Marasti-Georg

[Project structure for Google App Engine](https://stackoverflow.com/questions/48458/project-structure-for-google-app-engine)

I started an application in Google App Engine right when it came out, to play with the technology and work on a pet project that I had been thinking about for a long time but never gotten around to starting.  The result is BowlSK.  However, as it has grown, and features have been added, it has gotten really difficult to keep things organized - mainly due to the fact that this is my first python project, and I didn't know anything about it until I started working.What I have:Example:

http://www.bowlsk.com/ maps to HomePage (default package), template at "index.html"

http://www.bowlsk.com/games/view-series.html?series=7130 maps to ViewSeriesPage (again, default package), template at "games/view-series.html"It's nasty.  How do I restructure?  I had 2 ideas:Is there a best practice?  With Django 1.0 on the horizon, is there something I can do now to improve my ability to integrate with it when it becomes the official GAE templating engine?  I would simply start trying these things, and seeing which seems better, but pyDev's refactoring support doesn't seem to handle package moves very well, so it will likely be a non-trivial task to get all of this working again.

2008-09-07 14:08:47Z

I started an application in Google App Engine right when it came out, to play with the technology and work on a pet project that I had been thinking about for a long time but never gotten around to starting.  The result is BowlSK.  However, as it has grown, and features have been added, it has gotten really difficult to keep things organized - mainly due to the fact that this is my first python project, and I didn't know anything about it until I started working.What I have:Example:

http://www.bowlsk.com/ maps to HomePage (default package), template at "index.html"

http://www.bowlsk.com/games/view-series.html?series=7130 maps to ViewSeriesPage (again, default package), template at "games/view-series.html"It's nasty.  How do I restructure?  I had 2 ideas:Is there a best practice?  With Django 1.0 on the horizon, is there something I can do now to improve my ability to integrate with it when it becomes the official GAE templating engine?  I would simply start trying these things, and seeing which seems better, but pyDev's refactoring support doesn't seem to handle package moves very well, so it will likely be a non-trivial task to get all of this working again.First, I would suggest you have a look at "Rapid Development with Python, Django, and Google App Engine"GvR describes a general/standard project layout on page 10 of his slide presentation.  Here I'll post a slightly modified version of the layout/structure from that page. I pretty much follow this pattern myself. You also mentioned you had trouble with packages. Just make sure each of your sub folders has an __init__.py file. It's ok if its empty.Here are some code examples that may help as well:I think this layout works great for new and relatively small to medium projects. For larger projects I would suggest breaking up the views and models to have their own sub-folders with something like:My usual layout looks something like this:I can provide examples of what my app.yaml, request.py, lib/init.py, and sample controllers look like, if this isn't clear.I implemented a google app engine boilerplate today and checked it on github. This is along the lines described by Nick Johnson above (who used to work for Google).Follow this link gae-boilerplateI think the first option is considered the best practice. And make the code folder your first package. The Rietveld project developed by Guido van Rossum is a very good model to learn from. Have a look at it: http://code.google.com/p/rietveldWith regard to Django 1.0, I suggest you start using the Django trunk code instead of the GAE built in django port. Again, have a look at how it's done in Rietveld.I like webpy so I've adopted it as templating framework on Google App Engine.

My package folders are typically organized like this:Here is an example.I am not entirely up to date on the latest best practices, et cetera when it comes to code layout, but when I did my first GAE application, I used something along your second option, where the code and templates are next to eachother.There was two reasons for this - one, it kept the code and template nearby, and secondly, I had the directory structure layout mimic that of the website - making it (for me) a bit easier too remember where everything was.

Right way to initialize an OrderedDict using its constructor such that it retains order of initial data?

click

[Right way to initialize an OrderedDict using its constructor such that it retains order of initial data?](https://stackoverflow.com/questions/25480089/right-way-to-initialize-an-ordereddict-using-its-constructor-such-that-it-retain)

What's the correct way to initialize an ordered dictionary (OD) so that it retains the order of initial data?Question: P.S. I'll just leave this here for reference: "The OrderedDict constructor and update() method both accept keyword arguments, but their order is lost because Python’s function call semantics pass-in keyword arguments using a regular unordered dictionary"P.P.S : Hopefully, in future, OrderedDict will preserve the order of kwargs also (example 1): http://bugs.python.org/issue16991

2014-08-25 06:25:39Z

What's the correct way to initialize an ordered dictionary (OD) so that it retains the order of initial data?Question: P.S. I'll just leave this here for reference: "The OrderedDict constructor and update() method both accept keyword arguments, but their order is lost because Python’s function call semantics pass-in keyword arguments using a regular unordered dictionary"P.P.S : Hopefully, in future, OrderedDict will preserve the order of kwargs also (example 1): http://bugs.python.org/issue16991The OrderedDict will preserve any order that it has access to.  The only way to pass ordered data to it to initialize is to pass a list (or, more generally, an iterable) of key-value pairs, as in your last two examples.  As the documentation you linked to says, the OrderedDict does not have access to any order when you pass in keyword arguments or a dict argument, since any order there is removed before the OrderedDict constructor sees it.Note that using a list comprehension in your last example doesn't change anything.  There's no difference between OrderedDict([(i,i) for i in l]) and OrderedDict([('b', 'b'), ('a', 'a'), ('c', 'c'), ('aa', 'aa')]).  The list comprehension is evaluated and creates the list and it is passed in; OrderedDict knows nothing about how it was created.Yes, that will work.  By definition, a list is always ordered the way it is represented.  This goes for list-comprehension too, the list generated is in the same way the data was provided (i.e. source from a list it will be deterministic, sourced from a set or dict not so much).You keep your source list of 2-tuple around for reference, and use that as your test data for your test cases when you do unit tests.  Iterate through them and ensure the order is maintained.

Class constants in python

nodwj

[Class constants in python](https://stackoverflow.com/questions/10672419/class-constants-in-python)

In python, I want a class to have some "constants" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax?

Right now I use:and I'm wondering if there is a better way to do it or a way to do it without then having to write "Animal." before the sizes.

Thanks!

edit: forgot to mention that horse inherits from animal. 

2012-05-20 09:51:25Z

In python, I want a class to have some "constants" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax?

Right now I use:and I'm wondering if there is a better way to do it or a way to do it without then having to write "Animal." before the sizes.

Thanks!

edit: forgot to mention that horse inherits from animal. Since Horse is a subclass of Animal, you can just changewithStill, you need to remember that SIZES[1] means "big", so probably you could improve your code by doing something like:Alternatively, you could create intermediate classes: HugeAnimal, BigAnimal, and so on. That would be especially helpful if each animal class will contain different logic.You can get to SIZES by means of self.SIZES (in an instance method) or cls.SIZES (in a class method).In any case, you will have to be explicit about where to find SIZES. An alternative is to put SIZES in the module containing the classes, but then you need to define all classes in a single module.Expanding on betabandido's answer, you could write a function to inject the attributes as constants into the module:

adding directory to sys.path /PYTHONPATH

UnadulteratedImagination

[adding directory to sys.path /PYTHONPATH](https://stackoverflow.com/questions/16114391/adding-directory-to-sys-path-pythonpath)

I am trying to import a module from a particular directory. The problem is that if I use sys.path.append(mod_directory) to append the path and then open the python interpreter, the directory mod_directory gets added to the end of the list sys.path. If I export the PYTHONPATH variable before opening the python interpreter, the  directory gets added to the start of the list. In the latter case I can import the module but in the former, I cannot.Can somebody explain why this is happening and give me a solution to add the mod_directory to the start, inside a python script ?

2013-04-19 22:27:28Z

I am trying to import a module from a particular directory. The problem is that if I use sys.path.append(mod_directory) to append the path and then open the python interpreter, the directory mod_directory gets added to the end of the list sys.path. If I export the PYTHONPATH variable before opening the python interpreter, the  directory gets added to the start of the list. In the latter case I can import the module but in the former, I cannot.Can somebody explain why this is happening and give me a solution to add the mod_directory to the start, inside a python script ?This is working as documented.  Any paths specified in PYTHONPATH are documented as normally coming after the working directory but before the standard interpreter-supplied paths.  sys.path.append() appends to the existing path.  See here and here.  If you want a particular directory to come first, simply insert it at the head of sys.path:That said, there are usually better ways to manage imports than either using PYTHONPATH or manipulating sys.path directly.  See, for example, the answers to this question.You could use:As to me, i need to caffe to my python path. I can add it's path to the file 

 /home/xy/.bashrc by add export PYTHONPATH=/home/xy/caffe-master/python:$PYTHONPATH.to my /home/xy/.bashrc file.But when I use pycharm, the path is still not in.So I can add path to PYTHONPATH variable, by run -> edit Configuration.Temporarily changing dirs works well for importing:When running a Python script from Powershell under Windows, this should work:

Display a float with two decimal places in Python

MarathonStudios

[Display a float with two decimal places in Python](https://stackoverflow.com/questions/6149006/display-a-float-with-two-decimal-places-in-python)

I have a function taking float arguments (generally integers or decimals with one significant digit), and I need to output the values in a string with two decimal places (5 -> 5.00, 5.5 -> 5.50, etc). How can I do this in Python?

2011-05-27 07:13:47Z

I have a function taking float arguments (generally integers or decimals with one significant digit), and I need to output the values in a string with two decimal places (5 -> 5.00, 5.5 -> 5.50, etc). How can I do this in Python?You could use the string formatting operator for that:The result of the operator is a string, so you can store it in a variable, print etc.Since this post might be here for a while, lets also point out python 3 syntax:f-string formatting: This was new in Python 3.6 - the string is placed in quotation marks as usual, prepended with f'... in the same way you would r'... for a raw string. Then you place whatever you want to put within your string, variables, numbers, inside braces f'some string text with a {variable} or {number} within that text' - and Python evaluates as with previous string formatting methods, except that this method is much more readable. You can see in this example we format with decimal places in similar fashion to previous string formatting methods. NB a can be an number, variable, or even an expression eg f'{3*my_func(3.14):02f}'.Going forward, with new code I prefer f-strings over common %s or str.format() methods as f-strings can be far more readable, and are often much faster.String formatting:Using python string formatting.If you actually want to change the number itself instead of only displaying it differently use format()Format it to 2 decimal places:example:String Formatting:ORRound Function can be used:Good thing about round() is that, we can store this result to another variable, and then use it for other purposes. I know it is an old question, but I was struggling finding the answer myself. Here is what I have come up with:Python 3:Shortest Python 3 syntax:Using Python 3 syntax:

Does 'finally' always execute in Python?

Stevoisiak

[Does 'finally' always execute in Python?](https://stackoverflow.com/questions/49262379/does-finally-always-execute-in-python)

For any possible try-finally block in Python, is it guaranteed that the finally block will always be executed?For example, let’s say I return while in an except block:Or maybe I re-raise an Exception:Testing shows that finally does get executed for the above examples, but I imagine there are other scenarios I haven't thought of.Are there any scenarios in which a finally block can fail to execute in Python?

2018-03-13 17:30:33Z

For any possible try-finally block in Python, is it guaranteed that the finally block will always be executed?For example, let’s say I return while in an except block:Or maybe I re-raise an Exception:Testing shows that finally does get executed for the above examples, but I imagine there are other scenarios I haven't thought of.Are there any scenarios in which a finally block can fail to execute in Python?"Guaranteed" is a much stronger word than any implementation of finally deserves. What is guaranteed is that if execution flows out of the whole try-finally construct, it will pass through the finally to do so. What is not guaranteed is that execution will flow out of the try-finally.The finally block is not a transaction system; it doesn't provide atomicity guarantees or anything of the sort. Some of these examples might seem obvious, but it's easy to forget such things can happen and rely on finally for too much.Yes.  Finally always wins.  The only way to defeat it is to halt execution before finally: gets a chance to execute (e.g. crash the interpreter, turn off your computer, suspend a generator forever).  Here are a couple more you may not have thought about:Depending on how you quit the interpreter, sometimes you can "cancel" finally, but not like this:Using the precarious os._exit (this falls under "crash the interpreter" in my opinion):I'm currently running this code, to test if finally will still execute after the heat death of the universe:However, I'm still waiting on the result, so check back here later.According to the Python documentation:It should also be noted that if there are multiple return statements, including one in the finally block, then the finally block return is the only one that will execute.Well, yes and no.What is guaranteed is that Python will always try to execute the finally block. In the case where you return from the block or raise an uncaught exception, the finally block is executed just before actually returning or raising the exception.(what you could have controlled yourself by simply running the code in your question)The only case I can imagine where the finally block will not be executed is when the Python interpretor itself crashes for example inside C code or because of power outage.I found this one without using a generator function:The sleep can be any code that might run for inconsistent amounts of time.What appears to be happening here is that the first parallel process to finish leaves the try block successfully, but then attempts to return from the function a value (foo) that hasn't been defined anywhere, which causes an exception. That exception kills the map without allowing the other processes to reach their finally blocks.Also, if you add the line bar = bazz just after the sleep() call in the try block. Then the first process to reach that line throws an exception (because bazz isn't defined), which causes its own finally block to be run, but then kills the map, causing the other try blocks to disappear without reaching their finally blocks, and the first process not to reach its return statement, either.What this means for Python multiprocessing is that you can't trust the exception-handling mechanism to clean up resources in all processes if even one of the processes can have an exception. Additional signal handling or managing the resources outside the multiprocessing map call would be necessary.To really understand how it works, just run these two examples:

Common xlabel/ylabel for matplotlib subplots

jolindbe

[Common xlabel/ylabel for matplotlib subplots](https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots)

I have the following plot:and now I would like to give this plot common x-axis labels and y-axis labels. With "common", I mean that there should be one big x-axis label below the whole grid of subplots, and one big y-axis label to the right. I can't find anything about this in the documentation for plt.subplots, and my googlings suggest that I need to make a big plt.subplot(111) to start with - but how do I then put my 5*2 subplots into that using plt.subplots?

2013-04-22 15:25:51Z

I have the following plot:and now I would like to give this plot common x-axis labels and y-axis labels. With "common", I mean that there should be one big x-axis label below the whole grid of subplots, and one big y-axis label to the right. I can't find anything about this in the documentation for plt.subplots, and my googlings suggest that I need to make a big plt.subplot(111) to start with - but how do I then put my 5*2 subplots into that using plt.subplots?This looks like what you actually want. It applies the same approach of this answer to your specific case:Since I consider it relevant and elegant enough (no need to specify coordinates to place text), I copy (with a slight adaptation) an answer to another related question.This results in the following (with matplotlib version 2.2.0):Without sharex=True, sharey=True you get:With it you should get it nicer:But if you want to add additional labels, you should add them only to the edge plots:Adding label for each plot would spoil it (maybe there is a way to automatically detect repeated labels, but I am not aware of one).Since the command:you used returns a tuple consisting of the figure and a list of the axes instances, it is already sufficient to do something like (mind that I've changed fig,axto fig,axes):If you happen to want to change some details on a specific subplot, you can access it via axes[i] where i iterates over your subplots.It might also be very helpful to include aat the end of the file, before the plt.show(), in order to avoid overlapping labels.It will look better if you reserve space for the common labels by making invisible labels for the subplot in the bottom left corner. It is also good to pass in the fontsize from rcParams. This way, the common labels will change size with your rc setup, and the axes will also be adjusted to leave space for the common labels.

I ran into a similar problem while plotting a grid of graphs. The graphs consisted of two parts (top and bottom). The y-label was supposed to be centered over both parts.I did not want to use a solution that depends on knowing the position in the outer figure (like fig.text()), so I manipulated the y-position of the set_ylabel() function. It is usually 0.5, the middle of the plot it is added to. As the padding between the parts (hspace) in my code was zero, I could calculate the middle of the two parts relative to the upper part.pictureDownsides:There is probably a general solution that takes padding between figures into account.Update:This feature is now part of the proplot matplotlib package that I recently released on pypi. By default, when you make figures, the labels are "shared" between axes.Original answer:I discovered a more robust method:If you know the bottom and top kwargs that went into a GridSpec initialization, or you otherwise know the edges positions of your axes in Figure coordinates, you can also specify the ylabel position in Figure coordinates with some fancy "transform" magic. For example:...and you should see that the label still appropriately adjusts left-right to keep from overlapping with ticklabels, just like normal -- but now it will adjust to be always exactly between the desired subplots.Furthermore, if you don't even use set_position, the ylabel will show up by default exactly halfway up the figure. I'm guessing this is because when the label is finally drawn, matplotlib uses 0.5 for the y-coordinate without checking whether the underlying coordinate transform has changed. 

How do I add tab completion to the Python shell?

ashchristopher

[How do I add tab completion to the Python shell?](https://stackoverflow.com/questions/246725/how-do-i-add-tab-completion-to-the-python-shell)

When starting a django application using python manage.py shell, I get an InteractiveConsole shell - I can use tab completion, etc.When just starting a python interpreter using python, it doesn't offer tab completion.Can someone tell me what django is doing to give me an interactive console, or what I need to do to start an interactive console without a django app?

2008-10-29 13:09:10Z

When starting a django application using python manage.py shell, I get an InteractiveConsole shell - I can use tab completion, etc.When just starting a python interpreter using python, it doesn't offer tab completion.Can someone tell me what django is doing to give me an interactive console, or what I need to do to start an interactive console without a django app?I may have found a way to do it.Create a file .pythonrcthen in your .bashrc file, addThat seems to work.I think django does something like https://docs.python.org/library/rlcompleter.htmlIf you want to have a really good interactive interpreter have a look at 

IPython.For the record, this is covered in the tutorial: http://docs.python.org/tutorial/interactive.htmlI use ptpython.

https://github.com/jonathanslenders/ptpython/ptpython is a wonderful tool autocomplete shell cmd.

install ptpython is very easy,use pip tooland for django shell,you should import the django env,like thisTrust me,this is the best way to you!!!It looks like python3 has it out-of box!fix for windows10 shell:In Python3 this feature is enabled by default. My system didn't have the module readline installed. I am on Manjaro. I didn't face this tab completion issue on other linux distributions (elementary, ubuntu, mint).After pip installing the module, while importing, it was throwing the following error-ImportError: libncursesw.so.5: cannot open shared object file: No such file or directory

To solve this, I ran-cd /usr/lib

ln -s libncursesw.so libncursesw.so.5

This resolved the import error. And, it also brought the tab completion in the python repl without any creation/changes of .pythonrc and .bashrc.Yes. It's built in to 3.6.fernanr@gnuruwi ~ $ python3.6

Python 3.6.3 (default, Apr 10 2019, 14:37:36)

[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux

Type "help", "copyright", "credits" or "license" for more information.For older versions (2.x) above script works like charm :)

Python code to remove HTML tags from a string [duplicate]

Bruno Rocha - rochacbruno

[Python code to remove HTML tags from a string [duplicate]](https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string)

I have a text like this:using pure Python, with no external module I want to have this:I know I can do it using lxml.html.fromstring(text).text_content() but I need to achieve the same in pure Python using builtin or std library for 2.6+How can I do that?

2012-03-12 05:55:21Z

I have a text like this:using pure Python, with no external module I want to have this:I know I can do it using lxml.html.fromstring(text).text_content() but I need to achieve the same in pure Python using builtin or std library for 2.6+How can I do that?Using a regex, you can clean everything inside <> :Some HTML texts can also contain entities, that are not enclosed in brackets such as '&nsbm'. If that is the case then you might want to write the regex asThis link contains more details on this.You could also use BeautifulSoup additional package to find out all the raw textYou will need to explicitly set a parser when calling BeautifulSoup 

I recommend "lxml" as mentioned in alternative answers (much more robust than the default one (i.e. available without additional install) 'html.parser' But it doesn't prevent you from using external libraries, so I recommend the first solution.Python has several XML modules built in. The simplest one for the case that you already have a string with the full HTML is xml.etree, which works (somewhat) similarly to the lxml example you mention:Note that this isn't perfect, since if you had something like, say, <a title=">"> it would break. However, it's about the closest you'd get in non-library Python without a really complex function:However, as lvc mentions xml.etree is available in the Python Standard Library, so you could probably just adapt it to serve like your existing lxml version:There's a simple way to this in any C-like language. The style is not Pythonic but works with pure Python:The idea based in a simple finite-state machine and is detailed explained here: http://youtu.be/2tu9LTDujbwYou can see it working here: http://youtu.be/HPkNPcYed9M?t=35sPS - If you're interested in the class(about smart debugging with python) I give you a link: https://www.udacity.com/course/software-debugging--cs259. It's free! 

Python Nose Import Error

halfak

[Python Nose Import Error](https://stackoverflow.com/questions/3073259/python-nose-import-error)

I can't seem to get the nose testing framework to recognize modules beneath my test script in the file structure.  I've set up the simplest example that demonstrates the problem. I'll explain it below.Here's the the package file structure:foo.py contains:tests/test_foo.py contains:Both init.py files are emptyIf I run nosetests -vv in the main directory (where foo.py is), I get:I get the same error when I run from inside the tests/ directory.  According to the documentation and an example I found, nose is supposed to add all parent packages to the path as well as the directory from which it is called, but this doesn't seem to be happening in my case.  I'm running Ubuntu 8.04 with Python 2.6.2.  I've built and installed nose manually (not with setup_tools) if that matters. 

