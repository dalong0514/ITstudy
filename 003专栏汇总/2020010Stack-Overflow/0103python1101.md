I have two lists, dates and values. I want to plot them using matplotlib. The following creates a scatter plot of my data.plt.plot(dates, values) creates a line graph.But what I really want is a scatterplot where the points are connected by a line.Similar to in R:, which gives me a scatterplot of points overlaid with a line connecting the points.How do I do this in python?

2013-11-21 19:31:39Z

I have two lists, dates and values. I want to plot them using matplotlib. The following creates a scatter plot of my data.plt.plot(dates, values) creates a line graph.But what I really want is a scatterplot where the points are connected by a line.Similar to in R:, which gives me a scatterplot of points overlaid with a line connecting the points.How do I do this in python?I think @Evert has the right answer:Which is pretty much the same as or whatever linestyle you prefer.For red lines an pointsor for x markers and blue linesIn addition to what provided in the other answers, the keyword "zorder" allows one to decide the order in which different objects are plotted vertically.

E.g.:plots the scatter symbols on top of the line, whileplots the line over the scatter symbols.See, e.g., the zorder demo

How to add regularizations in TensorFlow?

Lifu Huang

[How to add regularizations in TensorFlow?](https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow)

I found in many available neural network code implemented using TensorFlow that regularization terms are often implemented by manually adding an additional term to loss value.My questions are:

2016-05-09 03:04:56Z

I found in many available neural network code implemented using TensorFlow that regularization terms are often implemented by manually adding an additional term to loss value.My questions are:As you say in the second point, using the regularizer argument is the recommended way. You can use it in get_variable, or set it once in your variable_scope and have all your variables regularized.The losses are collected in the graph, and you need to manually add them to your cost function like this.Hope that helps!A few aspects of the existing answer were not immediately clear to me, so here is a step-by-step guide:I'll provide a simple correct answer since I didn't find one. You need two simple steps, the rest is done by tensorflow magic:Another option to do this with the contrib.learn library is as follows, based on the Deep MNIST tutorial on the Tensorflow website. First, assuming you've imported the relevant libraries (such as import tensorflow.contrib.layers as layers), you can define a network in a separate method:Then, in a main method, you can use the following code snippet:To get this to work you need to follow the MNIST tutorial I linked to earlier and import the relevant libraries, but it's a nice exercise to learn TensorFlow and it's easy to see how the regularization affects the output. If you apply a regularization as an argument, you can see the following:Notice that the regularization portion gives you three items, based on the items available.With regularizations of 0, 0.0001, 0.01, and 1.0, I get test accuracy values of 0.9468, 0.9476, 0.9183, and 0.1135, respectively, showing the dangers of high regularization terms.I tested tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) and tf.losses.get_regularization_loss() with one l2_regularizer in the graph, and found that they return the same value. By observing the value's quantity, I guess reg_constant has already make sense on the value by setting the parameter of tf.contrib.layers.l2_regularizer. If anyone's still looking, I'd just like to add on that in tf.keras you may add weight regularization by passing them as arguments in your layers. An example of adding L2 regularization taken wholesale from the Tensorflow Keras Tutorials site:There's no need to manually add in the regularization losses with this method as far as I know.Reference: https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#add_weight_regularizationIf you have CNN you may do the following:In your model function:In your loss function:Some answers make me more confused.Here I give two methods to make it clearly.Then,it can be added into the total losstf.GraphKeys.REGULARIZATION_LOSSES will not be added automatically, but there is a simple way to add them:tf.losses.get_regularization_loss() uses tf.add_n to sum the entries of tf.GraphKeys.REGULARIZATION_LOSSES element-wise. tf.GraphKeys.REGULARIZATION_LOSSES will typically be a list of scalars, calculated using regularizer functions. It gets entries from calls to tf.get_variable that have the regularizer parameter specified. You can also add to that collection manually. That would be useful when using tf.Variable and also when specifying activity regularizers or other custom regularizers. For instance:(In this example it would presumably be more effective to regularize x, as y really flattens out for large x.)

How to redirect output with subprocess in Python?

catatemypythoncode

[How to redirect output with subprocess in Python?](https://stackoverflow.com/questions/4965159/how-to-redirect-output-with-subprocess-in-python)

What I do in the command line:What I want to do with python:

2011-02-11 02:49:01Z

What I do in the command line:What I want to do with python:UPDATE: os.system is discouraged, albeit still available in Python 3.Use os.system:If you really want to use subprocess, here's the solution (mostly lifted from the documentation for subprocess):OTOH, you can avoid system calls entirely:In Python 3.5+ to redirect the output, just pass an open file handle for the stdout argument to subprocess.run:As others have pointed out, the use of an external command like cat for this purpose is completely extraneous.Of course: When you use subprocess , the process must be killed.This is an example.If you don't kill the process , file will be empty and you can read nothing.It can run on Windows.I can`t make sure that it can run on Unix.One interesting case would be to update a file by appending similar file to it. Then one would not have to create a new file in the process. It is particularly useful in the case where a large file need to be appended. Here is one possibility using teminal command line directly from python.

python numpy machine epsilon

Bob

[python numpy machine epsilon](https://stackoverflow.com/questions/19141432/python-numpy-machine-epsilon)

I am trying to understand what is machine epsilon. According to the Wikipedia, it can be calculated as follows:However, it is suitable only for double precision numbers. I am interested in modifying it to support also single precision numbers. I read that numpy can be used, particularly numpy.float32 class. Can anybody help with modifying the function? 

2013-10-02 16:03:01Z

I am trying to understand what is machine epsilon. According to the Wikipedia, it can be calculated as follows:However, it is suitable only for double precision numbers. I am interested in modifying it to support also single precision numbers. I read that numpy can be used, particularly numpy.float32 class. Can anybody help with modifying the function? An easier way to get the machine epsilon for a given float type is to use np.finfo():Another easy way to get epsilon is:It will already work, as David pointed out!

How to fix 'Object arrays cannot be loaded when allow_pickle=False' for imdb.load_data() function?

Kanad

[How to fix 'Object arrays cannot be loaded when allow_pickle=False' for imdb.load_data() function?](https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa)

I'm trying to implement the binary classification example using the IMDb dataset in Google Colab. I have implemented this model before. But when I tried to do it again after a few days, it returned a value error:'Object arrays cannot be loaded when allow_pickle=False' for the load_data() function.I have already tried solving this, referring to an existing answer for a similar problem: How to fix 'Object arrays cannot be loaded when allow_pickle=False' in the sketch_rnn algorithm

But turns out that just adding an allow_pickle argument isn't sufficient.My code:The error:

2019-04-28 13:39:33Z

I'm trying to implement the binary classification example using the IMDb dataset in Google Colab. I have implemented this model before. But when I tried to do it again after a few days, it returned a value error:'Object arrays cannot be loaded when allow_pickle=False' for the load_data() function.I have already tried solving this, referring to an existing answer for a similar problem: How to fix 'Object arrays cannot be loaded when allow_pickle=False' in the sketch_rnn algorithm

But turns out that just adding an allow_pickle argument isn't sufficient.My code:The error:Here's a trick to force imdb.load_data to allow pickle by, in your notebook, replacing this line:by this:This issue is still up on keras git. I hope it gets solved as soon as possible. 

Until then, try downgrading your numpy version to 1.16.2. It seems to solve the problem.This version of numpy has the default value of allow_pickle as True.Following this issue on GitHub, the official solution is to edit the imdb.py file.  This fix worked well for me without the need to downgrade numpy.   Find the imdb.py file at tensorflow/python/keras/datasets/imdb.py (full path for me was: C:\Anaconda\Lib\site-packages\tensorflow\python\keras\datasets\imdb.py - other installs will be different) and change line 85 as per the diff:The reason for the change is security to prevent the Python equivalent of an SQL injection in a pickled file.  The change above will ONLY effect the imdb data and you therefore retain the security elsewhere (by not downgrading numpy).  I just used allow_pickle = True as an argument to np.load() and it worked for me.I think the answer from cheez (https://stackoverflow.com/users/122933/cheez) is the easiest and most effective one. I'd elaborate a little bit over it so it would not modify a numpy function for the whole session period. My suggestion is below. I´m using it to download the reuters dataset from keras which is showing the same kind of error:You can try changing the flag's valuenone of the above listed solutions worked for me: i run anaconda with python 3.7.3.

What worked for me wasIn my case worked with:Yes, installing previous a version of numpy solved the problem. For those who uses PyCharm IDE:in my IDE (Pycharm), File->Settings->Project Interpreter: I found my numpy to be 1.16.3, so I revert back to 1.16.1. 

Click + and type numpy in the search, tick "specify version" : 1.16.1 and choose--> install package.  find the path to imdb.py

then just add the flag to np.load(path,...flag...)on jupyter notebook usingworked fine, but the problem appears when you use this method in spyder(you have to restart the kernel every time or you will get an error like:I solved this issue using the solution here:Its work for meWhat I have found is that TensorFlow 2.0 (I am using 2.0.0-alpha0) is not compatible with the latest version of Numpy i.e. v1.17.0 (and possibly v1.16.5+). As soon as TF2 is imported, it throws a huge list of FutureWarning, that looks something like this: This also resulted in the allow_pickle error when tried to load imdb dataset from kerasI tried to use the following solution which worked just fine, but I had to do it every single project where I was importing TF2 or tf.keras.The easiest solution I found was to either install numpy 1.16.1 globally, or use compatible versions of tensorflow and numpy in a virtual environment. My goal with this answer is to point out that its not just a problem with imdb.load_data, but a larger problem vaused by incompatibility of TF2 and Numpy versions and may result in many other hidden bugs or issues. Tensorflow has a fix in tf-nightly version.The current version is '2.0.0-dev20190511'.The answer of @cheez sometime doesn't work and recursively call the function again and again. To solve this problem you should copy the function deeply. You can do this by using the function partial, so the final code is:I don't usually post to these things but this was super annoying. The confusion comes from the fact that some of the Keras imdb.py files have already updated:to the version with allow_pickle=True. Make sure check the imdb.py file to see if this change was already implemented. If it has been adjusted, the following works fine:

Resource u'tokenizers/punkt/english.pickle' not found

Supreeth Meka

[Resource u'tokenizers/punkt/english.pickle' not found](https://stackoverflow.com/questions/26570944/resource-utokenizers-punkt-english-pickle-not-found)

My Code:ERROR Message:I'm trying to run this program in Unix machine:As per the error message, I logged into python shell from my unix machine then I used the below commands:and then I downloaded all the available things using d- down loader and l- list options but still the problem persists.I tried my best to find the solution in internet but I got the same solution what I did as I mentioned in my above steps.

2014-10-26 07:52:51Z

My Code:ERROR Message:I'm trying to run this program in Unix machine:As per the error message, I logged into python shell from my unix machine then I used the below commands:and then I downloaded all the available things using d- down loader and l- list options but still the problem persists.I tried my best to find the solution in internet but I got the same solution what I did as I mentioned in my above steps.To add to alvas' answer, you can download only the punkt corpus:Downloading all sounds like overkill to me. Unless that's what you want.If you're looking to only download the punkt model:If you're unsure which data/model you need, you can install the popular datasets, models and taggers from NLTK:With the above command, there is no need to use the GUI to download the datasets. I got the solution:Downloader> dDownload which package (l=list; x=cancel)?

  Identifier> punktFrom the shell you can execute:If you want to install the popular NLTK corpora/models:If you want to install all NLTK corpora/models:To list the resources you have downloaded:The same thing happened to me recently, you just need to download the "punkt" package and it should work.When you execute "list" (l) after having "downloaded all the available things", is everything marked like the following line?:  If you see this line with the star, it means you have it, and nltk should be able to load it.Open the Python prompt and run the above statements.The sent_tokenize function uses an instance of PunktSentenceTokenizer from the

nltk.tokenize.punkt module. This instance has already been trained and works well for

many European languages. So it knows what punctuation and characters mark the end of a

sentence and the beginning of a new sentence.Go to python console by typing in your terminal. Then, type the following 2 commands in your python shell to install the respective packages:This solved the issue for me.My issue was that I called nltk.download('all') as the root user, but the process that eventually used nltk was another user who didn't have access to /root/nltk_data where the content was downloaded.  So I simply recursively copied everything from the download location to one of the paths where NLTK was looking to find it like this:Simple nltk.download() will not solve this issue. I tried the below and it worked for me:in the nltk folder create a tokenizers folder and copy your punkt folder into tokenizers folder.This will work.!

the folder structure needs to be as shown in the pictureYou need to rearrange your folders

Move your tokenizers folder into nltk_data folder.

This doesn't work if you have nltk_data folder containing corpora folder containing  tokenizers folderFor me nothing of the above worked, so I just downloaded all the files by hand from the web site http://www.nltk.org/nltk_data/ and I put them also by hand in a file "tokenizers" inside of "nltk_data" folder. Not a pretty solution but still a solution. I faced same issue. After downloading everything, still 'punkt' error was there. I searched package on my windows machine at C:\Users\vaibhav\AppData\Roaming\nltk_data\tokenizers and I can see 'punkt.zip' present there. I realized that somehow the zip has not been extracted into C:\Users\vaibhav\AppData\Roaming\nltk_data\tokenizers\punk.

Once I extracted the zip, it worked like music. After adding this line of code, the issue will be fixed:

'python' is not recognized as an internal or external command

Dave Stallberg

['python' is not recognized as an internal or external command](https://stackoverflow.com/questions/17953124/python-is-not-recognized-as-an-internal-or-external-command)

So I have recently installed Python Version 2.7.5 and I have made a little loop thing with it but the problem is, when I go to cmd and type python testloop.py I get the error: I have tried setting the path but no avail.Here is my path:As you can see, this is where my Python is installed. I don't know what else to do. Can someone help?

2013-07-30 17:04:20Z

So I have recently installed Python Version 2.7.5 and I have made a little loop thing with it but the problem is, when I go to cmd and type python testloop.py I get the error: I have tried setting the path but no avail.Here is my path:As you can see, this is where my Python is installed. I don't know what else to do. Can someone help?You need to add that folder to your Windows Path:https://docs.python.org/2/using/windows.html Taken from this question.Try "py" instead of "python" from command line:I have found the answer... click on the installer and check the box "Add python to environment variables" DO NOT uninstall the old one rather click on modify....Click on link for picture...Firstly, be sure where your python directory. It is normally in C:\Python27. If yours is different then change it from the below command.If after you install it python still isn’t recognized, then in PowerShell enter this:Close PowerShell and then start it again to make sure Python now runs. If it doesn’t,

restart may be required.Type py -v instead of python -v in command promptIf you want to see python version then you should use py -V instead of python -VIf you want to go to python's running environment then you should use py instead of pythonHere you can run the python program as:i solved this by running CMD in administration mode, so try this.I have met same issue when I install Python, and it is resolved when I set a PATH in system, here are the steps.Just find out where you have these three files in your system then copy each path and paste it in environment variable one by one. Then click ok to all.C:\Anaconda3C:\Anaconda3\Scripts C:\Anaconda3\Library\bin Then Restart your Spyder kernel(left hand side) and type python in cmd to check if it is working.From the Python docs, set the PATH like you did as above.Then execute the Python command using the full path name to make sure that works.Another helpful but simple solution might be restarting your computer after doing the download if Python is in the PATH variable. This has been a mistake I usually make when downloading Python onto a new machine.Open CMD with administrative access(Right click then run as administrator) then type the following command there:Replace My_python_lib with the folder name of your installed python like for me it was C:\python27.

Then to check if the path variable is set, type echo %PATH% you'll see your python part in the end. Hence now python is accessible.

From this tutorialI have installed python 3.7.4. First, I tried python in my command prompt. It was saying that 'Python is not recognized command......'. Then I tried 'py' command and it works. My sample command is:If you uninstalled then re-installed, and running 'python' in CLI, make sure to open a new CMD after your installation for 'python' to be recognized. 'py' will probably be recognized with an old CLI because its not tied to any version.Option 1 : Select on add environment var during installation

Option 2 : Go to C:\Users-> AppData (hidden file) -> Local\Programs\Python\Python38-32(depends on version installed)\Scripts

Copy path  and add to env vars path.For me this path worked : C:\Users\Username\AppData\Local\Programs\Python\Python38-32\ScriptsIt was a bit more confusing with the Python instructions once SQL Server 2019 was installed with Python.   The actual path I find is as follows:Scripts run with an Execute command:There is additional documentation in reference to SQL 2019's version of Python.  There is a statement that recommends PIP be used only from a download of sqlmutils-x.x.x.zip located on git (https://www.github.com/Microsoft/sqlmutils)   But there is a caveat.  Currently this only works for R and not for Python (Anaconda and consequently pip).  Python over SQL works but pip is not yet available.  (11/25/2019)Would be great to get an update when this occurs.

How to execute Python scripts in Windows?

ton4eg

[How to execute Python scripts in Windows?](https://stackoverflow.com/questions/1934675/how-to-execute-python-scripts-in-windows)

I have a simple script blah.py (using Python 2):If I execute my script by:It prints argument but if I execute script by:error occurs:So arguments do not pass to script.python.exe in PATH. Folder with blah.py also in PATH.

python.exe is default program to execute *.py files.What is the problem?

2009-12-20 02:09:25Z

I have a simple script blah.py (using Python 2):If I execute my script by:It prints argument but if I execute script by:error occurs:So arguments do not pass to script.python.exe in PATH. Folder with blah.py also in PATH.

python.exe is default program to execute *.py files.What is the problem?When you execute a script without typing "python" in front, you need to know two things about how Windows invokes the program.  First is to find out what kind of file Windows thinks it is: Next, you need to know how Windows is executing things with that extension.  It's associated with the file type "Python.File", so this command shows what it will be doing:So on my machine, when I type "blah.py foo", it will execute this exact command, with no difference in results than if I had typed the full thing myself:If you type the same thing, including the quotation marks, then you'll get results identical to when you just type "blah.py foo".  Now you're in a position to figure out the rest of your problem for yourself.  (Or post more helpful information in your question, like actual cut-and-paste copies of what you see in the console.  Note that people who do that type of thing get their questions voted up, and they get reputation points, and more people are likely to help them with good answers.)Even if assoc and ftype display the correct information, it may happen that the arguments are stripped off. What may help in that case is directly fixing the relevant registry keys for Python. Set the key to: Likely, previously, %* was missing. Similarly, set to the same value. See http://eli.thegreenplace.net/2010/12/14/problem-passing-arguments-to-python-scripts-on-windows/

HKEY_CLASSES_ROOT\Applications\python.exe\shell\open\command The registry path may vary, use python26.exe or python.exe or whichever is already in the registry.

HKEY_CLASSES_ROOT\py_auto_file\shell\open\commandyou should make the default application to handle python files be python.exe.right click a *.py file, select "Open With" dialog.  In there select "python.exe" and check "always use this program for this file type" (something like that).then your python files will always be run using python.exeAdditionally, if you want to be able to run your python scripts without typing the .py (or .pyw) on the end of the file name, you need to add .PY (or .PY;.PYW) to the list of extensions in the PATHEXT environment variable.In Windows 7:right-click on Computer

left-click Properties

left-click Advanced system settings

left-click the Advanced tab

left-click Environment Variables...

under "system variables" scroll down until you see PATHEXT

left-click on PATHEXT to highlight it

left-click Edit...

Edit "Variable value" so that it contains ;.PY  (the End key will skip to the end)

left-click OK

left-click OK

left-click OKNote #1: command-prompt windows won't see the change w/o being closed and reopened.Note #2: the difference between the .py and .pyw extensions is that the former opens a command prompt when run, and the latter doesn't.On my computer, I added ;.PY;.PYW as the last (lowest-priority) extensions, so the "before" and "after" values of PATHEXT were:before: .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSCafter .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYWHere are some instructive commands:You could install pylauncher. It is used to launch .py, .pyw, .pyc, .pyo files and supports multiple Python installations:You can run your Python script without specifying .py extension if you have .py, .pyw in PATHEXT environment variable:It adds support for shebang (#! header line) to select desired Python version on Windows if you have multiple versions installed. You could use *nix-compatible syntax #! /usr/bin/env python.You can specify version explicitly e.g., to run using the latest installed Python 3 version:It should also fix your sys.argv issue as a side-effect.I encountered the same problem but in the context of needing to package my code for Windows users (coming from Linux).

My package contains a number of scripts with command line options.I need these scripts to get installed in the appropriate location on Windows users' machines so that they can invoke them from the command line.

As the package is supposedly user-friendly, asking my users to change their registry to run these scripts would be impossible.I came across a solution that the folks at Continuum use for Python scripts that come with their Anaconda package -- check out your Anaconda/Scripts directory for examples.For a Python script test, create two files: a test.bat and a test-script.py.test.bat looks as follows (the .bat files in Anaconda\Scripts call python.exe with a relative path which I adapted for my purposes):test-script.py is your actual Python script:If you leave these two files in your local directory you can invoke your Python script through the .bat file by doingIf you copy both files to a location that is on your PATH (such as Anaconda\Scripts) then you can even invoke your script by leaving out the .bat suffixDisclaimer: I have no idea what's going on and how this works and so would appreciate any explanation.On Windows, To run a python module without typing "python",--> Right click any python(*.py) file--> Set the open with property to "python.exe" --> Check the "always use this program for this file type"--> Append the path of python.exe to variable environment e.g. append 

C:\Python27 to PATH environment variable.To Run a python module without typing ".py" extension--> Edit PATHEXT system variable and append ".PY" extension to the list.Found an incredibly useful answer here: How to run different python versions in cmd?As J.F. Sebastian suggests, Python Launcher for Windows is the best and default choice for launching different version of Python in Windows. It used to be a third-party tool, but now it is officially supported since Python 3.3.This is a great tool just use it!Can you execute python.exe from any map?

If you do not, chek if you have proper values for python.exe in PATH enviromentAre you in same directory than blah.py. Check this by issuing command -> edit blah.py and check if you can open this fileEDIT:In that case you can not. (python arg means that you call python.exe whit some parameters which python assume that is filename of script you want to run)You can create bat file whit lines in your path map and run .bat fileExample:

In one of Path maps create blah.py.bat

Edit file and put lineYou can now run blah.py from anywere, becuase you do not need to put .bat extention when running bat filesIf that's what I understood, it's like this:COPY (not delete) python.exe and rename to py.exe and execute:Simply run the command:Assuming the file name is within same folder and Python has already been added to environment variables.

method of iterating over sqlalchemy model's defined columns?

Rick

[method of iterating over sqlalchemy model's defined columns?](https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns)

I've been trying to figure out how to iterate over the list of columns defined in a SQLAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.__dict__ since it contains a lot of SA specific items. Anyone know of a way to just get the id and desc names from the following?In this small case I could easily create a:but I'd prefer something that auto-generates the dict (for larger objects).

2010-03-29 11:27:43Z

I've been trying to figure out how to iterate over the list of columns defined in a SQLAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.__dict__ since it contains a lot of SA specific items. Anyone know of a way to just get the id and desc names from the following?In this small case I could easily create a:but I'd prefer something that auto-generates the dict (for larger objects).You could use the following function:It will exclude SA magic attributes, but will not exclude the relations. So basically it might load the dependencies, parents, children etc, which is definitely not desirable.But it is actually much easier because if you inherit from Base, you have a __table__ attribute, so that you can do:See How to discover table properties from SQLAlchemy mapped object - similar question.Edit by Mike: Please see functions such as Mapper.c and Mapper.mapped_table.  If using 0.8 and higher also see Mapper.attrs and related functions.Example for Mapper.attrs:You can get the list of defined properties from the mapper. For your case you're interested in only ColumnProperty objects.I realise that this is an old question, but I've just come across the same requirement and would like to offer an alternative solution to future readers.As Josh notes, full SQL field names will be returned by JobStatus.__table__.columns, so rather than the original field name  id, you will get jobstatus.id. Not as useful as it could be.The solution to obtaining a list of field names as they were originally defined is to look the _data attribute on the column object, which contains the full data. If we look at JobStatus.__table__.columns._data, it looks like this:From here you can simply call JobStatus.__table__.columns._data.keys() which gives you a nice, clean list:self.__table__.columns will "only" give you the columns defined in that particular class, i.e. without inherited ones.  if you need all, use self.__mapper__.columns.  in your example i'd probably use something like this:To get an as_dict method on all of my classes I used a Mixin class which uses the technics described by Ants Aasma.And then use it like this in your classesThat way you can invoke the following on an instance of MyClass.Hope this helps.I've played arround with this a bit further, I actually needed to render my instances as dict as the form of a HAL object with it's links to related objects. So I've added this little magic down here, which will crawl over all properties of the class same as the above, with the difference that I will crawl deeper into Relaionship properties and generate links for these automatically.Please note that this will only work for relationships have a single primary keyAssuming you're using SQLAlchemy's declarative mapping, you can use __mapper__ attribute to get at the class mapper. To get all mapped attributes (including relationships):If you want strictly column names, use obj.__mapper__.column_attrs.keys(). See the documentation for other views.https://docs.sqlalchemy.org/en/latest/orm/mapping_api.html#sqlalchemy.orm.mapper.Mapper.attrsreturns a dict where keys are attribute names and values the values of the object./!\ there is a supplementary attribute: '_sa_instance_state'

but you can handle it :)I know this is an old question, but what about:Then, to get column names: jobStatus.columns()That would return ['id', 'desc']Then you can loop through, and do stuff with the columns and values:

How to un-escape a backslash-escaped string?

Nick

[How to un-escape a backslash-escaped string?](https://stackoverflow.com/questions/1885181/how-to-un-escape-a-backslash-escaped-string)

Suppose I have a string which is a backslash-escaped version of another string.  Is there an easy way, in Python, to unescape the string?  I could, for example, do:However that involves passing a (possibly untrusted) string to eval() which is a security risk.  Is there a function in the standard lib which takes a string and produces a string with no security implications?

2009-12-11 01:00:01Z

Suppose I have a string which is a backslash-escaped version of another string.  Is there an easy way, in Python, to unescape the string?  I could, for example, do:However that involves passing a (possibly untrusted) string to eval() which is a security risk.  Is there a function in the standard lib which takes a string and produces a string with no security implications?You can use ast.literal_eval which is safe:Like this:All given answers will break on general Unicode strings. The following works for Python3 in all cases, as far as I can tell:In python 3, str objects don't have a decode method and you have to use a bytes object. ChristopheD's answer covers python 2.

How can I do DNS lookups in Python, including referring to /etc/hosts?

Toby White

[How can I do DNS lookups in Python, including referring to /etc/hosts?](https://stackoverflow.com/questions/2805231/how-can-i-do-dns-lookups-in-python-including-referring-to-etc-hosts)

dnspython will do my DNS lookups very nicely, but it entirely ignores the contents of /etc/hosts.Is there a python library call which will do the right thing? ie check first in etc/hosts, and only fall back to DNS lookups otherwise?

2010-05-10 18:14:42Z

dnspython will do my DNS lookups very nicely, but it entirely ignores the contents of /etc/hosts.Is there a python library call which will do the right thing? ie check first in etc/hosts, and only fall back to DNS lookups otherwise?I'm not really sure if you want to do DNS lookups yourself or if you just want a host's ip. In case you want the latter,The normal name resolution in Python works fine. Why do you need DNSpython for that. Just use socket's getaddrinfo which follows the rules configured for your operating system (on Debian, it follows /etc/nsswitch.conf:gives you a list of the addresses for www.example.com.  (ipv4 and ipv6)This code works well for returning all of the IP addresses that might belong to a particular URI. Since many systems are now in a hosted environment (AWS/Akamai/etc.), systems may return several IP addresses. The lambda was "borrowed" from @Peter Silva.The answer above was meant for Python 2. If you're using Python 3, here is the code.I found this way to expand a DNS RR hostname that expands into a list of IPs, into the list of member hostnames:Which, when I run it, lists a few 1e100.net hostnames:

adding header to python requests module

discky

[adding header to python requests module](https://stackoverflow.com/questions/8685790/adding-header-to-python-requests-module)

Earlier I used httplib module to add a header in the request. Now I am trying the same thing with the requests module.This is the python request module I am using:

http://pypi.python.org/pypi/requestsHow can I add a header to the request.post and request.get say I have to add foobar key in each request in the header.

2011-12-31 02:02:08Z

Earlier I used httplib module to add a header in the request. Now I am trying the same thing with the requests module.This is the python request module I am using:

http://pypi.python.org/pypi/requestsHow can I add a header to the request.post and request.get say I have to add foobar key in each request in the header.From http://docs.python-requests.org/en/latest/user/quickstart/You just need to create a dict with your headers (key: value pairs where the key is the name of the header and the value is, well, the value of the pair) and pass that dict to the headers parameter on the .get or .post method.So more specific to your question:You can also do this to set a header for all future gets for the Session object, where x-test will be in all s.get() calls:from: http://docs.python-requests.org/en/latest/user/advanced/#session-objects

What are the 'levels', 'keys', and names arguments for in Pandas' concat function?

piRSquared

[What are the 'levels', 'keys', and names arguments for in Pandas' concat function?](https://stackoverflow.com/questions/49620538/what-are-the-levels-keys-and-names-arguments-for-in-pandas-concat-functio)

Pandas' concat function is the Swiss Army knife of the merging utilities.  The variety of situations in which it is useful are numerous. The existing documentation leaves out a few details on some of the optional arguments. Among them are the levels and keys arguments. I set out to figure out what those arguments do.I'll pose a question that will act as a gateway into many aspects of pd.concat.Consider the data frames d1, d2, and d3:If I were to concatenate these together withI get the expected result with a pandas.MultiIndex for my columns object:However, I wanted to use the levels argument documentation:So I passedAnd get a KeyErrorThis made sense. The levels I passed were inadequate to describe the necessary levels indicated by the keys. Had I not passed anything, as I did above, the levels are inferred (as stated in the documentation). But how else can I use this argument to better effect?If I tried this instead:I and got the same results as above. But when I add one more value to the levels,I end up with the same looking data frame, but the resulting MultiIndex has an unused level.So what is the point of the level argument and should I be using keys differently?I'm using Python 3.6 and Pandas 0.22.

2018-04-03 00:25:14Z

Pandas' concat function is the Swiss Army knife of the merging utilities.  The variety of situations in which it is useful are numerous. The existing documentation leaves out a few details on some of the optional arguments. Among them are the levels and keys arguments. I set out to figure out what those arguments do.I'll pose a question that will act as a gateway into many aspects of pd.concat.Consider the data frames d1, d2, and d3:If I were to concatenate these together withI get the expected result with a pandas.MultiIndex for my columns object:However, I wanted to use the levels argument documentation:So I passedAnd get a KeyErrorThis made sense. The levels I passed were inadequate to describe the necessary levels indicated by the keys. Had I not passed anything, as I did above, the levels are inferred (as stated in the documentation). But how else can I use this argument to better effect?If I tried this instead:I and got the same results as above. But when I add one more value to the levels,I end up with the same looking data frame, but the resulting MultiIndex has an unused level.So what is the point of the level argument and should I be using keys differently?I'm using Python 3.6 and Pandas 0.22.In the process of answering this question for myself, I learned many things, and I wanted to put together a catalog of examples and some explanation.The specific answer to the point of the levels argument will come towards the end.Link To Current DocumentationThe first argument we come across is objs:For now, we'll stick with a list of some of the DataFrame and Series objects defined above.

I'll show how dictionaries can be leveraged to give very useful MultiIndex results later.The second argument we encounter is axis whose default value is 0:For values of 0 or index we mean to say: "Align along the columns and add to the index".As shown above where we used axis=0, because 0 is the default value, and we see that the index of d2 extends the index of d1 despite there being overlap of the value 2:For values 1 or columns we mean to say: "Align along the index and add to the columns",We can see that the resulting index is the union of indices and the resulting columns are the extension of columns from d1 by the columns of d2.When combining pandas.Series along axis=0, we get back a pandas.Series. The name of the resulting Series will be None unless all Series being combined have the same name. Pay attention to the 'Name: A' when we print out the resulting Series.  When it isn't present, we can assume the Series name is None.When combining pandas.Series along axis=1, it is the name attribute that we refer to in order to infer a column name in the resulting pandas.DataFrame.When performing a concatenation of a Series and DataFrame along axis=0, we convert all Series to single column DataFrames.Take special note that this is a concatenation along axis=0; that means extending the index (rows) while aligning the columns. In the examples below, we see the index becomes [2, 3, 2, 3] which is an indiscriminate appending of indices. The columns do not overlap unless I force the naming of the Series column with the argument to to_frame:You can see the results of pd.concat([s1, d1]) are the same as if I had perfromed the to_frame myself.However, I can control the name of the resulting column with a parameter to to_frame. Renaming the Series with the rename method does not control the column name in the resulting DataFrame.This is fairly intuitive. Series column name defaults to an enumeration of such Series objects when a name attribute is not available.The third argument is join that describes whether the resulting merge should be an outer merge (default) or an inner merge.It turns out, there is no left or right option as pd.concat can handle more than just two objects to merge.In the case of d1 and d2, the options look like:Fourth argument is the thing that allows us to do our left merge and more.Like when I stack d1 on top of d2, if I don't care about the index values, I could reset them or ignore them.And when using axis=1:We can pass a list of scalar values or tuples in order to assign tuple or scalar values to corresponding MultiIndex. The length of the passed list must be the same length as the number of items being concatenated.When concatenating Series objects along axis=0 (extending the index).Those keys, become a new initial level of a MultiIndex object in the index attribute.However, we can use more than scalar values in the keys argument to create an even deeper MultiIndex. Here we pass tuples of length 2 the prepend two new levels of a MultiIndex:It's a bit different when extending along columns. When we used axis=0 (see above) our keys acted as MultiIndex levels in addition to the existing index.  For axis=1, we are referring to an axis that Series objects don't have, namely the columns attribute.Notice that naming the s1 and s2 matters so long as no keys are passed, but it gets overridden if keys are passed.As with the axis=0 examples, keys add levels to a MultiIndex, but this time to the object stored in the columns attribute.This is tricky. In this case, a scalar key value cannot act as the only level of index for the Series object when it becomes a column while also acting as the first level of a MultiIndex for the DataFrame. So Pandas will again use the name attribute of the Series object as the source of the column name.Pandas only seems to infer column names from Series name, but it will not fill in the blanks when doing an analogous concatenation among data frames with a different number of column levels.Then concatenate this with another data frame with only one level in the columns object and Pandas will refuse to try and make tuples of the MultiIndex object and combine all data frames as if a single level of objects, scalars and tuples.When passing a dictionary, pandas.concat will use the keys from the dictionary as the keys parameter.This is used in conjunction with the keys argument.When levels is left as its default value of None, Pandas will take the unique values of each level of the resulting MultiIndex and use that as the object used in the resulting index.levels attribute.If Pandas already infers what these levels should be, what advantage is there to specify it ourselves? I'll show one example and leave it up to you to think up other reasons why this might be useful.Per the documentation, the levels argument is a list of sequences. This means that we can use another pandas.Index as one of those sequences.Consider the data frame df that is the concatenation of d1, d2 and d3:The levels of the columns object are:If we use sum within a groupby we get:But what if instead of ['First', 'Second', 'Fourth'] there were another missing categories named Third and Fifth? And I wanted them included in the results of a groupby aggregation? We can do this if we had a pandas.CategoricalIndex.  And we can specify that ahead of time with the levels argument.So instead, let's define df as:But the first level of the columns object is:And our groupby summation looks like:This is used to name the levels of a resulting MultiIndex. The length of the names list should match the number of levels in the resulting MultiIndex.Self explanatory documentationBecause the resulting index from concatenating d1 and d2 is not unique, it would fail the integrity check.And>

    ValueError: Indexes have overlapping values: [2]

Are for-loops in pandas really bad? When should I care?

cs95

[Are for-loops in pandas really bad? When should I care?](https://stackoverflow.com/questions/54028199/are-for-loops-in-pandas-really-bad-when-should-i-care)

Are for loops really "bad"? If not, in what situation(s) would they be better than using a more conventional "vectorized" approach?1I am familiar with the concept of "vectorization", and how pandas employs vectorized techniques to speed up computation. Vectorized functions broadcast operations over the entire series or DataFrame to achieve speedups much greater than conventionally iterating over the data. However, I am quite surprised to see a lot of code (including from answers on Stack Overflow) offering solutions to problems that involve looping through data using for loops and list comprehensions. The documentation and API say that loops are "bad", and that one should "never" iterate over arrays, series, or DataFrames. So, how come I sometimes see users suggesting loop-based solutions?1 - While it is true that the question sounds somewhat broad, the truth is that there are very specific situations when for loops are usually better than conventionally iterating over data. This post aims to capture this for posterity.  

2019-01-03 18:54:20Z

Are for loops really "bad"? If not, in what situation(s) would they be better than using a more conventional "vectorized" approach?1I am familiar with the concept of "vectorization", and how pandas employs vectorized techniques to speed up computation. Vectorized functions broadcast operations over the entire series or DataFrame to achieve speedups much greater than conventionally iterating over the data. However, I am quite surprised to see a lot of code (including from answers on Stack Overflow) offering solutions to problems that involve looping through data using for loops and list comprehensions. The documentation and API say that loops are "bad", and that one should "never" iterate over arrays, series, or DataFrames. So, how come I sometimes see users suggesting loop-based solutions?1 - While it is true that the question sounds somewhat broad, the truth is that there are very specific situations when for loops are usually better than conventionally iterating over data. This post aims to capture this for posterity.  TLDR; No, for loops are not blanket "bad", at least, not always. It is probably more accurate to say that some vectorized operations are slower than iterating, versus saying that iteration is faster than some vectorized operations. Knowing when and why is key to getting the most performance out of your code. In a nutshell, these are the situations where it is worth considering an alternative to vectorized pandas functions:Let's examine these situations individually. Pandas follows a "Convention Over Configuration" approach in its API design. This means that the same API has been fitted to cater to a broad range of data and use cases. When a pandas function is called, the following things (among others) must internally be handled by the function, to ensure working Almost every function will have to deal with these to varying extents, and this presents an overhead. The overhead is less for numeric functions (for example, Series.add), while it is more pronounced for string functions (for example, Series.str.replace).for loops, on the other hand, are faster then you think. What's even better is list comprehensions (which create lists through for loops) are even faster as they are optimized iterative mechanisms for list creation.List comprehensions follow the pattern Where seq is a pandas series or DataFrame column. Or, when operating over multiple columns,Where seq1 and seq2 are columns. Numeric Comparison

Consider a simple boolean indexing operation. The list comprehension method has been timed against Series.ne (!=) and query. Here are the functions:For simplicity, I have used the perfplot package to run all the timeit tests in this post. The timings for the operations above are below:The list comprehension outperforms query for moderately sized N, and even outperforms the vectorized not equals comparison for tiny N. Unfortunately, the list comprehension scales linearly, so it does not offer much performance gain for larger N. Value Counts

Taking another example - this time, with another vanilla python construct that is faster than a for loop - collections.Counter. A common requirement is to compute the value counts and return the result as a dictionary. This is done with value_counts, np.unique, and Counter:The results are more pronounced, Counter wins out over both vectorized methods for a larger range of small N (~3500). Of course, the take away from here is that the performance depends on your data and use case. The point of these examples is to convince you not to rule out these solutions as legitimate options. If these still don't give you the performance you need, there is always cython and numba. Let's add this test into the mix.Numba offers JIT compilation of loopy python code to very powerful vectorized code. Understanding how to make numba work involves a learning curve.String-based Comparison

Revisiting the filtering example from the first section, what if the columns being compared are strings? Consider the same 3 functions above, but with the input DataFrame cast to string.So, what changed? The thing to note here is that string operations are inherently difficult to vectorize. Pandas treats strings as objects, and all operations on objects fall back to a slow, loopy implementation.Now, because this loopy implementation is surrounded by all the overhead mentioned above, there is a constant magnitude difference between these solutions, even though they scale the same. When it comes to operations on mutable/complex objects, there is no comparison. List comprehension outperforms all operations involving dicts and lists. Accessing Dictionary Value(s) by Key

Here are timings for two operations that extract a value from a column of dictionaries: map and the list comprehension. The setup is in the Appendix, under the heading "Code Snippets".Positional List Indexing

Timings for 3 operations that extract the 0th element from a list of columns (handling exceptions), map, str.get accessor method, and the list comprehension:List Flattening

A final example is flattening lists. This is another common problem, and demonstrates just how powerful pure python is here.Both itertools.chain.from_iterable and the nested list comprehension are pure python constructs, and scale much better than the stack solution.These timings are a strong indication of the fact that pandas is not equipped to work with mixed dtypes, and that you should probably refrain from using it to do so. Wherever possible, data should be present as scalar values (ints/floats/strings) in separate columns.Lastly, the applicability of these solutions depend widely on your data. So, the best thing to do would be to test these operations on your data before deciding what to go with. Notice how I have not timed apply on these solutions, because it would skew the graph (yes, it's that slow).Pandas can apply regex operations such as str.contains, str.extract, and str.extractall, as well as other "vectorized" string operations (such as str.split, str.find,str.translate`, and so on) on string columns. These functions are slower than list comprehensions, and are meant to be more convenience functions than anything else.It is usually much faster to pre-compile a regex pattern and iterate over your data with re.compile (also see Is it worth using Python's re.compile?). The list comp equivalent to str.contains looks something like this:Or,If you need to handle NaNs, you can do something likeThe list comp equivalent to str.extract (without groups) will look something like:If you need to handle no-matches and NaNs, you can use a custom function (still faster!):The matcher function is very extensible. It can be fitted to return a list for each capture group, as needed. Just extract query the group or groups attribute of the matcher object.For str.extractall, change p.search to p.findall.String Extraction

Consider a simple filtering operation. The idea is to extract 4 digits if it is preceded by an upper case letter.More Examples

Full disclosure - I am the author (in part or whole) of these posts listed below.As shown from the examples above, iteration shines when working with small rows of DataFrames, mixed datatypes, and regular expressions.The speedup you get depends on your data and your problem, so your mileage may vary. The best thing to do is to carefully run tests and see if the payout is worth the effort. The "vectorized" functions shine in their simplicity and readability, so if performance is not critical, you should definitely prefer those. Another side note, certain string operations deal with constraints that favour the use of NumPy. Here are two examples where careful NumPy vectorization outperforms python: Additionally, sometimes just operating on the underlying arrays via .values as opposed to on the Series or DataFrames can offer a healthy enough speedup for most usual scenarios (see the Note in the Numeric Comparison section above). So, for example df[df.A.values != df.B.values] would show instant performance boosts over df[df.A != df.B]. Using .values may not be appropriate in every situation, but it is a useful hack to know. As mentioned above, it's up to you to decide whether these solutions are worth the trouble of implementing.

How do I release memory used by a pandas dataframe?

b10hazard

[How do I release memory used by a pandas dataframe?](https://stackoverflow.com/questions/39100971/how-do-i-release-memory-used-by-a-pandas-dataframe)

I have a really large csv file that I opened in pandas as follows....Once I do this my memory usage increases by 2GB, which is expected because this file contains millions of rows.  My problem comes when I need to release this memory.  I ran....However, my memory usage did not drop.  Is this the wrong approach to release memory used by a pandas data frame?  If it is, what is the proper way? 

2016-08-23 12:17:10Z

I have a really large csv file that I opened in pandas as follows....Once I do this my memory usage increases by 2GB, which is expected because this file contains millions of rows.  My problem comes when I need to release this memory.  I ran....However, my memory usage did not drop.  Is this the wrong approach to release memory used by a pandas data frame?  If it is, what is the proper way? Reducing memory usage in Python is difficult, because Python does not actually release memory back to the operating system. If you delete objects, then the memory is available to new Python objects, but not free()'d back to the system (see this question).If you stick to numeric numpy arrays, those are freed, but boxed objects are not.Python keep our memory at high watermark, but we can reduce the total number of dataframes we create. When modifying your dataframe, prefer inplace=True, so you don't create copies.Another common gotcha is holding on to copies of previously created dataframes in ipython:You can fix this by typing %reset Out to clear your history. Alternatively, you can adjust how much history ipython keeps with ipython --cache-size=5 (default is 1000).Wherever possible, avoid using object dtypes.Values with an object dtype are boxed, which means the numpy array just contains a pointer and you have a full Python object on the heap for every value in your dataframe. This includes strings.Whilst numpy supports fixed-size strings in arrays, pandas does not (it's caused user confusion). This can make a significant difference:You may want to avoid using string columns, or find a way of representing string data as numbers.If you have a dataframe that contains many repeated values (NaN is very common), then you can use a sparse data structure to reduce memory usage:You can view the memory usage (docs):As of pandas 0.17.1, you can also do df.info(memory_usage='deep') to see memory usage including objects.As noted in the comments, there are some things to try: gc.collect (@EdChum) may clear stuff, for example. At least from my experience, these things sometimes work and often don't. There is one thing that always works, however, because it is done at the OS, not language, level.Suppose you have a function that creates an intermediate huge DataFrame, and returns a smaller result (which might also be a DataFrame):Then if you do something likeThen the function is executed at a different process. When that process completes, the OS retakes all the resources it used. There's really nothing Python, pandas, the garbage collector, could do to stop that.This solves the problem of releasing the memory for me!!!the data-frame will be explicitly set to nulldel df will not be deleted if there are any reference to the df at the time of deletion. So you need to to delete all the references to it with del df to release the memory. So all the instances bound to df should be deleted to trigger garbage collection.Use objgragh to check which is holding onto the objects.

How to create a tuple with only one element

Russell

[How to create a tuple with only one element](https://stackoverflow.com/questions/12876177/how-to-create-a-tuple-with-only-one-element)

In the below example I would expect all the elements to be tuples, why is a tuple converted to a string when it only contains a single string?

2012-10-13 19:21:19Z

In the below example I would expect all the elements to be tuples, why is a tuple converted to a string when it only contains a single string?Because those first two elements aren't tuples; they're just strings. The parenthesis don't automatically make them tuples. You have to add a comma after the string to indicate to python that it should be a tuple.To fix your example code, add commas here:From the Python Docs:If you truly hate the trailing comma syntax, a workaround is to pass a list to the tuple() function:Your first two examples are not tuples, they are strings. Single-item tuples require a trailing comma, as in:('a') is not a tuple, but just a string.You need to add an extra comma at the end to make python take them as tuple: -

Python: fastest way to create a list of n lists

munchybunch

[Python: fastest way to create a list of n lists](https://stackoverflow.com/questions/5518435/python-fastest-way-to-create-a-list-of-n-lists)

So I was wondering how to best create a list of blank lists:Because of how Python works with lists in memory, this doesn't work:This does create [[],[],...] but each element is the same list:Something like a list comprehension works:But this uses the Python VM for looping. Is there any way to use an implied loop (taking advantage of it being written in C)?This is actually slower. :(

2011-04-01 20:23:59Z

So I was wondering how to best create a list of blank lists:Because of how Python works with lists in memory, this doesn't work:This does create [[],[],...] but each element is the same list:Something like a list comprehension works:But this uses the Python VM for looping. Is there any way to use an implied loop (taking advantage of it being written in C)?This is actually slower. :(The probably only way which is marginally faster than isIt does not have to create a new int object in every iteration and is about 15 % faster on my machine.Edit: Using NumPy, you can avoid the Python loop usingbut this is actually 2.5 times slower than the list comprehension.The list comprehensions actually are implemented more efficiently than explicit looping (see the dis output for example functions) and the map way has to invoke an ophaque callable object on every iteration, which incurs considerable overhead overhead.Regardless, [[] for _dummy in xrange(n)] is the right way to do it and none of the tiny (if existent at all) speed differences between various other ways should matter. Unless of course you spend most of your time doing this - but in that case, you should work on your algorithms instead. How often do you create these lists? Here are two methods, one sweet and simple(and conceptual), the other more formal and can be extended in a variety of situations, after having read a dataset.Method 1: ConceptualMethod 2 : Formal and extensibleAnother elegant way to store a list as a list of lists of different numbers - which it reads from a file. (The file here has the dataset train)

Train is a data-set with say 50 rows and 20 columns. ie. Train[0] gives me the 1st row of a csv file, train[1] gives me the 2nd row and so on. I am interested in separating the dataset with 50 rows as one list, except the column 0 , which is my explained variable here, so must be removed from the orignal train dataset, and then scaling up list after list- ie a list of a list. Here's the code that does that. Note that I am reading from "1" in the inner loop since I am interested in explanatory variables only. And I re-initialize X1=[] in the other loop, else the X2.append([0:(len(train[0])-1)]) will rewrite X1 over and over again - besides it more memory efficient.To create list and list of lists use below syntaxthis will create 1-d list and to initialize it put number in [[number] and set length of list put length in range(length)this will initialize list of lists with 10*3 dimension and with value 0So I did some speed comparisons to get the fastest way.

List comprehensions are indeed very fast. The only way to get close is to avoid bytecode getting exectuded during construction of the list.

My first attempt was the following method, which would appear to be faster in principle:(produces a list of length 2**n, of course)

This construction is twice as slow as the list comprehension, according to timeit, for both short and long (a million) lists.My second attempt was to use starmap to call the list constructor for me, There is one construction, which appears to run the list constructor at top speed, but still is slower, but only by a tiny amount:Interesting enough the execution time suggests that it is the final list call that is makes the starmap solution slow, since its execution time is almost exactly equal to the speed of:My third attempt came when I realized that list(()) also produces a list, so I tried the apperently simple:but this was slower than the starmap call.Conclusion: for the speed maniacs:

Do use the list comprehension.

Only call functions, if you have to.

Use builtins.

Static files in Flask - robot.txt, sitemap.xml (mod_wsgi)

biesiad

[Static files in Flask - robot.txt, sitemap.xml (mod_wsgi)](https://stackoverflow.com/questions/4239825/static-files-in-flask-robot-txt-sitemap-xml-mod-wsgi)

Is there any clever solution to store static files in Flask's application root directory.

robots.txt and sitemap.xml are expected to be found in /, so my idea was to create routes for them:There must be something more convenient :)

2010-11-21 19:26:59Z

Is there any clever solution to store static files in Flask's application root directory.

robots.txt and sitemap.xml are expected to be found in /, so my idea was to create routes for them:There must be something more convenient :)The best way is to set static_url_path to root url@vonPetrushev is right, in production you'll want to serve static files via nginx or apache, but for development it's nice to have your dev environment simple having your python app serving up the static content as well so you don't have to worry about changing configurations and multiple projects. To do that, you'll want to use the SharedDataMiddleware.This example assumes your static files are in the folder "static", adjust to whatever fits your environment.The cleanest answer to this question is the answer to this (identical) question:To summarize:Even though this is an old answered question, I'm answering this because this post comes up pretty high in the Google results.  While it's not covered in the documentation, if you read the API docs for the Flask Application object constructor it's covered. By passing the named parameter static_folder like so:...you can define where static files are served from.  Similarly, you can define a template_folder, the name of you static_url_path.Serving static files has nothing to do with application that is meant to deliver dynamic content. The correct way of serving static files is dependent of what server you're using. After all, when you get your app up and running, you will need to bind it to a web server. I can speak only for apache httpd, so the way of serving static files is defined in the virtual host that you are binding to your application through mod-wsgi. Here is the guide that will show you how to serve sitemaps, robots.txt or any static content:

http://code.google.com/p/modwsgi/wiki/QuickConfigurationGuide#Mounting_At_Root_Of_SiteAnother way to send static files is to use a catch-all rule like this:I use this to try to minimise the set-up when developing.  I got the idea from http://flask.pocoo.org/snippets/57/Further, I'm developing using flask on my standalone machine but deploying with Apache in production server.  I use:This might have been added since this question was asked, but I was looking through flask's "helpers.py" and I found flask.send_from_directory:... which references flask.send_file:... which seems better for more control, although send_from_directory passes **options directly through to send_file.From the documentation here: http://flask.pocoo.org/docs/quickstart/#static-filesI'm having the same dilemma as well. Did some search and found my answer(MHO):Might as well quote from the documentationIMHO: When your application is up for production, static file serving should be (or is ideally) configured on the webserver (nginx, apache); but during development, Flask made it available to serve static files. This is to help you develop rapidly - no need to setup webservers and such. Hope it helps.

Loop through list with both content and index [duplicate]

Oriol Nieto

[Loop through list with both content and index [duplicate]](https://stackoverflow.com/questions/11475748/loop-through-list-with-both-content-and-index)

It is very common for me to loop through a python list to get both the contents and their indexes. What I usually do is the following:I find this syntax a bit ugly, especially the part inside the zip function. Are there any more elegant/Pythonic ways of doing this?

2012-07-13 17:53:51Z

It is very common for me to loop through a python list to get both the contents and their indexes. What I usually do is the following:I find this syntax a bit ugly, especially the part inside the zip function. Are there any more elegant/Pythonic ways of doing this?Use the enumerate built-in function: http://docs.python.org/library/functions.html#enumerateUse enumerate():Like everyone else:but alsoIn other words, you can specify as starting value for the index/count generated by enumerate() which comes in handy if you don't want your index to start with the default value of zero.I was printing out lines in a file the other day and specified the starting value as 1 for enumerate(), which made more sense than 0 when displaying information about a specific line to the user.enumerate is what you want:enumerate() makes this prettier:See here for more.

Can Keras with Tensorflow backend be forced to use CPU or GPU at will?

mikal94305

[Can Keras with Tensorflow backend be forced to use CPU or GPU at will?](https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will)

I have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.  

2016-11-19 08:04:53Z

I have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.  If you want to force Keras to use CPUbefore Keras / Tensorflow is imported.Run your script asSee also A rather separable way of doing this is to use Here, with booleans GPU and CPU, we indicate whether we would like to run our code with the GPU or CPU by rigidly defining the number of GPUs and CPUs the Tensorflow session is allowed to access. The variables num_GPU and num_CPU define this value. num_cores then sets the number of CPU cores available for usage via intra_op_parallelism_threads and inter_op_parallelism_threads.The intra_op_parallelism_threads variable dictates the number of threads a parallel operation in a single node in the computation graph is allowed to use (intra). While the inter_ops_parallelism_threads variable defines the number of threads accessible for parallel operations across the nodes of the computation graph (inter).allow_soft_placement allows for operations to be run on the CPU if any of the following criterion are met:All of this is executed in the constructor of my class before any other operations, and is completely separable from any model or other code I use. Note: This requires tensorflow-gpu and cuda/cudnn to be installed because the option is given to use a GPU.Refs:This worked for me (win10), place before you import keras:Just import tensortflow and use keras, it's that easy.As per keras tutorial, you can simply use the same tf.device scope as in regular tensorflow:I just spent some time figure it out.

Thoma's answer is not complete.

Say your program is test.py, you want to use gpu0 to run this program, and keep other gpus free.You should write CUDA_VISIBLE_DEVICES=0 python test.pyNotice it's DEVICES not DEVICEFor people working on PyCharm, and for forcing CPU, you can add the following line in the Run/Debug configuration, under Environment variables:

Django, creating a custom 500/404 error page

Zac

[Django, creating a custom 500/404 error page](https://stackoverflow.com/questions/17662928/django-creating-a-custom-500-404-error-page)

Following the tutorial found here exactly, I cannot create a custom 500 or 404 error page. If I do type in a bad url, the page gives me the default error page. Is there anything I should be checking for that would prevent a custom page from showing up?File directories:within mysite/settings.py I have these enabled:within mysite/polls/urls.py:I can post any other code necessary, but what should I be changing to get a custom 500 error page if I use a bad url?EditSOLUTION:

I had an additionalwithin my settings.py and that was causing the problem

2013-07-15 20:07:02Z

Following the tutorial found here exactly, I cannot create a custom 500 or 404 error page. If I do type in a bad url, the page gives me the default error page. Is there anything I should be checking for that would prevent a custom page from showing up?File directories:within mysite/settings.py I have these enabled:within mysite/polls/urls.py:I can post any other code necessary, but what should I be changing to get a custom 500 error page if I use a bad url?EditSOLUTION:

I had an additionalwithin my settings.py and that was causing the problemUnder your main views.py add your own custom implementation of the following two views, and just set up the templates 404.html and 500.html with what you want to display.With this solution, no custom code needs to be added to urls.pyHere's the code:Updatehandler404 and handler500 are exported Django string configuration variables found in django/conf/urls/__init__.py. That is why the above config works.To get the above config to work, you should define the following variables in your urls.py file and point the exported Django variables to the string Python path of where these Django functional views are defined, like so:Update for Django 2.0Signatures for handler views were changed in Django 2.0:

https://docs.djangoproject.com/en/2.0/ref/views/#error-views If you use views as above, handler404 will fail with message:In such case modify your views like this:Here is the link to the official documentation on how to set up custom error views:https://docs.djangoproject.com/en/stable/topics/http/views/#customizing-error-viewsIt says to add lines like these in your URLconf (setting them anywhere else will have no effect):You can also customise the CSRF error view by modifying the setting CSRF_FAILURE_VIEW.It's worth reading the documentation of the default error handlers, page_not_found, server_error, permission_denied and bad_request. By default, they use these templates if they can find them, respectively: 404.html, 500.html, 403.html, and 400.html.So if all you want to do is make pretty error pages, just create those files in a TEMPLATE_DIRS directory, you don't need to edit URLConf at all. Read the documentation to see which context variables are available.In Django 1.10 and later, the default CSRF error view uses the template 403_csrf.html.Don't forget that DEBUG must be set to False for these to work, otherwise, the normal debug handlers will be used.Add these lines in  urls.pyand implement our custom views in views.py.From the page you referenced:So I believe you need to add something like this to your urls.py:and similar for handler500.If all you need is to show custom pages which have some fancy error messages for your site when DEBUG = False, then add two templates named 404.html and 500.html in your templates directory and it will automatically pick up this custom pages when a 404 or 500 is raised.In Django 2.* you can use this construction in views.pyIn settings.pyIn urls.pyUsually i creating default_app and handle site-wide errors, context processors in it. settings.py::::and just add your 404.html and 500.html pages in templates folder.

remove 404.html and 500.html from templates in polls app.Make an error, On the  error page find out from where django is loading templates.I mean the path stack.In base template_dir add these html pages 500.html , 404.html. When these errors occur the respective template files will be automatically loaded. You can add pages for other error codes too, like 400 and 403.Hope this help !!!As one single line (for 404 generic page):This works on django 2.0Be sure to include your custom 404.html inside the app templates folder.Try moving your error templates to .../Django/mysite/templates/ ?I'm note sure about this one, but i think these need to be "global" to the website.In Django 3.x, the accepted answer won't work because render_to_response has been removed completely as well as some more changes have been made since the version the accepted answer worked for.Some other answers are also there but I'm presenting a little cleaner answer:In your main urls.py file:In yourapp/views.py file:Ensure that you have imported render() in yourapp/views.py file:Side note: render_to_response() was deprecated in Django 2.x and it has been completely removed in verision 3.x.

Pipenv: Command Not Found

lgants

[Pipenv: Command Not Found](https://stackoverflow.com/questions/46391721/pipenv-command-not-found)

I'm new to Python development and attempting to use pipenv. I ran the command pip install pipenv, which ran successfully:However, when I run the command pipenv install in a fresh root project directory I receive the following message: -bash: pipenv: command not found. I suspect that I might need to modify my .bashrc, but I'm unclear about what to add to the file or if modification is even necessary.

2017-09-24 15:28:45Z

I'm new to Python development and attempting to use pipenv. I ran the command pip install pipenv, which ran successfully:However, when I run the command pipenv install in a fresh root project directory I receive the following message: -bash: pipenv: command not found. I suspect that I might need to modify my .bashrc, but I'm unclear about what to add to the file or if modification is even necessary.That happens because you are not installing it globally (system wide). For it to be available in your path you need to install it using sudo, like this:This fixed it for me:If you've done a user installation, you'll need to add the right folder to your PATH variable. See pipenv's installation instructionsI tried this:

python -m pipenv  # for python2

python3 -m pipenv # for python3

Hope this can help you.I have same problem with pipenv on Mac OS X 10.13 High Seirra, another Mac works just fine.  I use Heroku to deploy my Django servers, some in 2.7 and some in 3.6.  So, I need both 2.7 and 3.6.  When HomeBrew install Python, it keeps python points to original 2.7, and python3 points to 3.6.The problem might due to $ pip install pipenv.  I checked /usr/local/bin and pipenv isn't there.  So, I tried a full uninstall:Then reinstall and works now:OSX GUYS, OVER HERE!!!As @charlax answered (for me the best one), you can use a more dynamic command to set PATH, buuut for mac users this could not work, sometimes your USER_BASE path got from site is wrong, so you need to find out where your python installation is.you'll get a symlink, then you need to find the source's symlink.(this ../../../ means root)So you found the python path (/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6), then you just need to put in you ~/.bashrc as follows:

export PATH="$PATH:/Library/Frameworks/Python.framework/Versions/3.6/bin"

Before jumping into the command that will install pipenv, it is worth understanding where pip installs Python packages. Global site-packages is where Python installs packages that will be available to all users and all Python applications on the system. You can check the global site package with the commandFor example, on Linux with Python 3.7 the path is usuallyUser site-packages is where Python installs packages available only for you. But the packages will still be visible to all Python projects that you create. You can get the path withOn Linux with Python 3.7 the path is usuallyOn most Linux and other Unices, usually Python 2 and Python 3 is installed side-by-side. The default Python 3 executable is almost always python3. pip may be available as either of the following, depending on your Linux distributionAvoid using pip with sudo! Yes, it's the most convenient way to install Python packages and the executable is available at /usr/local/bin/pipenv, but it also mean that specific package is always visible for all users, and all Python projects that you create. Instead, use per-user site packages instead with --userpipenv is available atOn macOS, Homebrew is the recommended way to install Python. You can easily upgrade Python, install multiple versions of Python and switch between versions using Homebrew. If you are using Homebrew'ed Python, pip install --user is disabled. The global site-package is located atand you can safely install Python packages here. Python 3.y also searches for modules in:For legacy reasons, Python is installed in C:\Python37. The Python executable is usually named py.exe, and you can run pip with py -m pip.Global site packages is installed in Since you don't usually share your Windows devices, it is also OK to install a package globallypipenv is now available atI don't recommend install Python packages in Windows with --user, because the default user site-package directory is in your Windows roaming profileThe roaming profile is used in Terminal Services (Remote Desktop, Citrix, etc) and when you log on / off in a corporate environment. Slow login, logoff and reboot in Windows can be caused by a large roaming profile. Installing pipenv globally can have an adverse effect by overwriting the global/system-managed pip installation, thus resulting in import errors when trying to run pip.You can install pipenv at the user level:pip install --user pipenvThis should install pipenv at a user-level in /home/username/.local so that it does not conflict with the global version of pip. In my case, that still did not work after running the '--user' switch, so I ran the longer 'fix what I screwed up' command once to restore the system managed environment:sudo python3 -m pip uninstall pip && sudo apt install python3-pip --reinstall^ found here: Error after upgrading pip: cannot import name 'main'and then did the following:mkdir /home/username/.local ... if it doesn't already existexport PYTHONUSERBASE=/home/username/.localMake sure the export took effect (bit me once during this process):echo $PYTHONUSERBASEThen, I ran the pip install --user pipenv and all was well. I could then run pipenv from the CLI and it did not overwrite the global/system-managed pip module. Of course, this is specific to the user so you want to make sure you install pipenv this way while working as the user you wish to use pipenv.References:https://pipenv.readthedocs.io/en/latest/diagnose/#no-module-named-module-name https://pipenv.readthedocs.io/en/latest/install/#pragmatic-installation-of-pipenv https://pip.pypa.io/en/stable/user_guide/#user-installsAfter installing pipenv (sudo pip install pipenv), I kept getting the "Command Not Found" error when attempting to run the pipenv shell command.I finally fixed it with the following code:You might consider installing pipenv via pipsi.Unfortunately there are some issues with macOS + python3 at the time of writing, see 1, 2. In my case I had to change the bashprompt to #!/Users/einselbst/.local/venvs/pipsi/bin/pythonThis is fixed for me to:In some cases of old pip version:For window users this may be due to conflicting installation with virtualenv. For me it worked when I uninstalled virtualenv and pipenv first, and then install only pipenv.Now pipenv install xxx worked for meOn Mac OS X Catalina it appears to follow the Linux path. Using any of:Essentially installs pipenv here:But its not the executable and so is never found. The only thing that worked for me wasThis seems to result in an __init__.py file in the above directory that has contents to correctly expose the pipenv command.and everything started working, when all other posted and commented suggestions on this question failed.The pipenv package certainly seems quite picky.

Remove or adapt border of frame of legend using matplotlib

Mattijn

[Remove or adapt border of frame of legend using matplotlib](https://stackoverflow.com/questions/25540259/remove-or-adapt-border-of-frame-of-legend-using-matplotlib)

When plotting a plot using matplotlib:

2014-08-28 03:29:11Z

When plotting a plot using matplotlib:When plotting a plot using matplotlib:How to remove the box of the legend?How to change the color of the border of the legend box?How to remove only the border of the box of the legend?One more related question, since it took me forever to find the answer:How to make the legend background blank (i.e. transparent, not white):Warning, you want 'none' (the string). None means the default color instead.

Why can't I call read() twice on an open file?

helpermethod

[Why can't I call read() twice on an open file?](https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file)

For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?Here's the codeOf course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?

2010-10-11 12:25:38Z

For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?Here's the codeOf course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?Calling read() reads through the entire file and leaves the read cursor at the end of the file (with nothing more to read). If you are looking to read a certain number of lines at a time you could use  readline(), readlines() or iterate through lines with for line in handle:.To answer your question directly, once a file has been read, with read() you can use seek(0) to return the read cursor to the start of the file (docs are here). If you know the file isn't going to be too large, you can also save the read() output to a variable, using it in your findall expressions.Ps. Dont forget to close the file after you are done with it ;)yeah, as above...i'll write just an example:Everyone who has answered this question so far is absolutely right - read() moves through the file, so after you've called it, you can't call it again.What I'll add is that in your particular case, you don't need to seek back to the start or reopen the file, you can just store the text that you've read in a local variable, and use it twice, or as many times as you like, in your program:The read pointer moves to after the last read byte/character. Use the seek() method to rewind the read pointer to the beginning.Every open file has an associated position.

When you read() you read from that position.

For example read(10) reads the first 10 bytes from a newly opened file, then another read(10) reads the next 10 bytes.

read() without arguments reads all of the contents of the file, leaving the file position at the end of the file. Next time you call read() there is nothing to read.You can use seek to move the file position. Or probably better in your case would be to do one read() and keep the result for both searches.read() consumes. So, you could reset the file, or seek to the start before re-reading. Or, if it suites your task, you can use read(n) to consume only n bytes.I always find the read method something of a walk down a dark alley. You go down a bit and stop but if you are not counting your steps you are not sure how far along you are. Seek gives the solution by repositioning, the other option is Tell which returns the position along the file. May be the Python file api can combine read and seek into a read_from(position,bytes) to make it simpler - till that happens you should read this page.

How to unnest (explode) a column in a pandas DataFrame?

YOBEN_S

[How to unnest (explode) a column in a pandas DataFrame?](https://stackoverflow.com/questions/53218931/how-to-unnest-explode-a-column-in-a-pandas-dataframe)

I have the following DataFrame where one of the columns is an object (list type cell):My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?) 

2018-11-09 02:19:22Z

I have the following DataFrame where one of the columns is an object (list type cell):My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?) As a user with both R and python, I have seen this type of question a couple of times. In R, they have the built-in function from package tidyr called unnest. But in Python(pandas) there is no built-in function for this type of question. I know object columns type always make the data hard to convert with a pandas' function. When I received the data like this , the first thing that came to mind was to 'flatten' or unnest the columns . I am using pandas and python functions for this type of question. If you are worried about the speed of the above solutions, check user3483203's answer , since he is using numpy and most of the time numpy is faster . I recommend Cpython and numba if speed matters in your case.Method 0 [pandas >= 0.25]

Starting from pandas 0.25, if you only need to explode one column, you can use the explode function:Method 1

apply + pd.Series (easy to understand but in terms of performance not recommended . )Method 2

Using repeat with DataFrame constructor , re-create your dataframe (good at performance, not good at multiple columns )Method 2.1

for example besides A we have A.1 .....A.n. If we still use the method(Method 2) above it is hard for us to re-create the columns one by one . Solution : join or merge with the index after 'unnest' the single columns If you need the column order exactly the same as before, add reindex at the end.  Method 3

recreate the list If more than two columns, useMethod 4

using reindex  or locMethod 5

when the list only contains unique values:Method 6

using numpy for high performance:Method 7

using base function itertools cycle and chain: Pure python solution just for funGeneralizing to multiple columnsSelf-def function: All above method is talking about the vertical unnesting and explode , If you do need expend the list horizontal, Check with pd.DataFrame constructor Updated function Test Output Option 1If all of the sublists in the other column are the same length, numpy can be an efficient option here:Option 2If the sublists have different length, you need an additional step:Option 3I took a shot at generalizing this to work to flatten N columns and tile M columns, I'll work later on making it more efficient:FunctionsTimingsPerformanceExploding a list-like column has been simplified significantly in pandas 0.25 with the addition of the explode() method:Out:One alternative is to apply the meshgrid recipe over the rows of the columns to unnest:OutputMy 5 cents:and another 5both resulting in the sameBecause normally sublist length are different and join/merge is far more computational expensive. I retested the method for different length sublist and more normal columns.MultiIndex should be also a easier way to write and has near the same performances as numpy way.Surprisingly, in my implementation comprehension way has the best performance.Relative time of each methodSomething pretty not recommended (at least work in this case):concat + sort_index + iter + apply + next.Now:Is:If care about index:Now:Is:Any opinions on this method I thought of? or is doing both concat and melt considered too "expensive"?I generalized the problem a bit to be applicable to more columns.Summary of what my solution does:Complete example:The actual explosion is performed in 3 lines. The rest is cosmetics (multi column explosion, handling of strings instead of lists in the explosion column, ...).Credits to WeNYoBen's answerIn my case with more than one column to explode, and with variables lengths for the arrays that needs to be unnested.I ended up applying the new pandas 0.25 explode function two times, then removing generated duplicates and it does the job !

Windows can't find the file on subprocess.call()

Sri

[Windows can't find the file on subprocess.call()](https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call)

I am getting the following error:My code is:Windows 7, 64 bit. Python 3.x latest, stable.Any ideas?Thanks,

2010-06-11 10:27:39Z

I am getting the following error:My code is:Windows 7, 64 bit. Python 3.x latest, stable.Any ideas?Thanks,When the command is a shell built-in, add a 'shell=True' to the call.E.g. for dir you would type:To quote from the documentation:On Windows, I believe the subprocess module doesn't look in the PATH unless you pass shell=True because it use CreateProcess() behind the scenes. However, shell=True can be a security risk if you're passing arguments that may come from outside your program. To make subprocess nonetheless able to find the correct executable, you can use shutil.which. Suppose the executable in your PATH is named frob:(This works on Python 3.3 and above.)On Windows you have to call through cmd.exe. As Apalala mentioned, Windows commands are implemented in cmd.exe not as separate executables.e.g. /c tells cmd to run the follow commandThis is safer than using shell=True, which allows shell injections.If you are using powershell, then in it will be subprocess.call(['powershell','-command','dir']). Powershell supports a large portion of POSIX commandsAfter much head scratching, I discovered that running a file that is located in C:\Windows\System32\ while running a 32bit version of python on a 64bit machine is a potential issue, due to Windows trying to out-smart the process, and redirect calls to C:\Windows\System32 to C:\Windows\SysWOW64.I found an example of how to fix this here:

http://code.activestate.com/recipes/578035-disable-file-system-redirector/To quote from the documentation:"Prior to Python 3.5, these three functions comprised the high level API to subprocess. You can now use run() in many cases, but lots of existing code calls these functions."SO: instead of subprocess.call use subprocess.run for Python 3.5 and above

cartesian product in pandas

Idok

[cartesian product in pandas](https://stackoverflow.com/questions/13269890/cartesian-product-in-pandas)

I have two pandas dataframes:What is the best practice to get their cartesian product (of course without writing it explicitly like me)?

2012-11-07 12:33:12Z

I have two pandas dataframes:What is the best practice to get their cartesian product (of course without writing it explicitly like me)?If you have a key that is repeated for each row, then you can produce a cartesian product using merge (like you would in SQL).Output:See here for the documentation: http://pandas.pydata.org/pandas-docs/stable/merging.html#brief-primer-on-merge-methods-relational-algebraUse pd.MultiIndex.from_product as an index in an otherwise empty dataframe, then reset its index, and you're done. out:This won't win a code golf competition, and borrows from the previous answers - but clearly shows how the key is added, and how the join works. This creates 2 new data frames from lists, then adds the key to do the cartesian product on.My use case was that I needed a list of all store IDs on for each week in my list. So, I created a list of all the weeks I wanted to have, then a list of all the store IDs I wanted to map them against.The merge I chose left, but would be semantically the same as inner in this setup. You can see this in the documentation on merging, which states it does a Cartesian product if key combination appears more than once in both tables - which is what we set up.Minimal code needed for this one. Create a common 'key' to cartesian merge the two:With method chaining: As an alternative, one can rely on the cartesian product provided by itertools: itertools.product, which avoids creating a temporary key or modifying the index:Quick test: If you have no overlapping columns, don't want to add one, and the indices of the data frames can be discarded, this may be easier:You could start by taking the Cartesian product of df1.col1 and df2.col3, then merge back to df1 to get col2.Here's a general Cartesian product function which takes a dictionary of lists:Apply as:I find using pandas MultiIndex to be the best tool for the job. If you have a list of lists lists_list, call pd.MultiIndex.from_product(lists_list) and iterate over the result (or use it in DataFrame index).

SqlAlchemy - Filtering by Relationship Attribute

user1105851

[SqlAlchemy - Filtering by Relationship Attribute](https://stackoverflow.com/questions/8561470/sqlalchemy-filtering-by-relationship-attribute)

I don't have much experience with SQLAlchemy and I have a problem, which I can't solve. I tried searching and I tried a lot of code.

This is my Class (reduced to the most significant code):and I would like to query all patients, whose mother's phenoscore is (for example) == 10As told, I tried a lot of code, but I don't get it. The logically solution, in my eyes, would bebecause, you can access .mother.phenoscore for each element when outputting but, this code doesn't do it.Is there a (direct) possibility to filter by an attribute of a relationship (without writing the SQL Statement, or an extra join-statement), I need this kind of filter more than one time.Even if there is no easy solution, I am happy to get all answers.

2011-12-19 12:35:48Z

I don't have much experience with SQLAlchemy and I have a problem, which I can't solve. I tried searching and I tried a lot of code.

This is my Class (reduced to the most significant code):and I would like to query all patients, whose mother's phenoscore is (for example) == 10As told, I tried a lot of code, but I don't get it. The logically solution, in my eyes, would bebecause, you can access .mother.phenoscore for each element when outputting but, this code doesn't do it.Is there a (direct) possibility to filter by an attribute of a relationship (without writing the SQL Statement, or an extra join-statement), I need this kind of filter more than one time.Even if there is no easy solution, I am happy to get all answers.Use method has() of relationship (more readable):or join (usually faster):You have to query the relationsip with joinYou will get the example from this Self-Referential Query Strategies I used it with sessions, but an alternate way where you can access the relationship field directly isI have not tested it, but I guess this would also workGood news for you: I recently made package that gives you filtering/sorting with "magical" strings as in Django, so you can now write something likeIt's a lot shorter, especially for complex filters, say,Hope you will enjoy this packagehttps://github.com/absent1706/sqlalchemy-mixins#django-like-queries

How to exit an if clause

Roman

[How to exit an if clause](https://stackoverflow.com/questions/2069662/how-to-exit-an-if-clause)

What sorts of methods exist for prematurely exiting an if clause? There are times when I'm writing code and want to put a break statement inside of an if clause, only to remember that those can only be used for loops.Lets take the following code as an example:I can think of one way to do this: assuming the exit cases happen within nested if statements, wrap the remaining code in a big else block. Example:The problem with this is that more exit locations mean more nesting/indented code.Alternatively, I could write my code to have the if clauses be as small as possible and not require any exits.Does anyone know of a good/better way to exit an if clause?If there are any associated else-if and else clauses, I figure that exiting would skip over them. 

2010-01-15 05:20:54Z

What sorts of methods exist for prematurely exiting an if clause? There are times when I'm writing code and want to put a break statement inside of an if clause, only to remember that those can only be used for loops.Lets take the following code as an example:I can think of one way to do this: assuming the exit cases happen within nested if statements, wrap the remaining code in a big else block. Example:The problem with this is that more exit locations mean more nesting/indented code.Alternatively, I could write my code to have the if clauses be as small as possible and not require any exits.Does anyone know of a good/better way to exit an if clause?If there are any associated else-if and else clauses, I figure that exiting would skip over them. (This method works for ifs, multiple nested loops and other constructs that you can't break from easily.)Wrap the code in its own function.  Instead of break, use return.Example:(Don't actually use this, please.)You can emulate goto's functionality with exceptions:Disclaimer: I only mean to bring to your attention the possibility of doing things this way, while in no way do I endorse it as reasonable under normal circumstances. As I mentioned in a comment on the question, structuring code so as to avoid Byzantine conditionals in the first place is preferable by far. :-)may be this?For what was actually asked, my approach is to put those ifs inside a one-looped loopTest it:Generally speaking, don't. If you are nesting "ifs" and breaking from them, you are doing it wrong.However, if you must:Note, the functions don't HAVE to be declared in the if statement, they can be declared in advance ;) This would be a better choice, since it will avoid needing to refactor out an ugly if/then later on.Effectively what you're describing are goto statements, which are generally panned pretty heavily. Your second example is far easier to understand. However, cleaner still would be:There is another way which doesn't rely on defining functions (because sometimes that's less readable for small code snippets), doesn't use an extra outer while loop (which might need special appreciation in the comments to even be understandable on first sight), doesn't use goto (...) and most importantly let's you keep your indentation level for the outer if so you don't have to start nesting stuff.Yes, that also needs a second look for readability, however, if the snippets of code are small this doesn't require to track any while loops that will never repeat and after understanding what the intermediate ifs are for, it's easily readable, all in one place and with the same indentation.And it should be pretty efficient.So here i understand you're trying to break out of the outer if code blockOne way out of this is that you can test for for a false condition in the outer if block, which will then implicitly exit out of the code block, you then use an else block to nest the other ifs to do somethinguse return in the if condition will returns you out from the function,

so that you can use return to break the the if condition.

Running bash script from within python

user1638145

[Running bash script from within python](https://stackoverflow.com/questions/13745648/running-bash-script-from-within-python)

I have a problem with the following code:callBash.py:sleep.sh:I want the "end" to be printed after 10s. (I know that this is a dumb example, I could simply sleep within python, but this simple sleep.sh file was just as a test)

2012-12-06 14:25:16Z

I have a problem with the following code:callBash.py:sleep.sh:I want the "end" to be printed after 10s. (I know that this is a dumb example, I could simply sleep within python, but this simple sleep.sh file was just as a test)Making sleep.sh executable and adding shell=True to the parameter list (as suggested in previous answers) works ok. Depending on the search path, you may also need to add ./ or some other appropriate path.  (Ie, change "sleep.sh" to "./sleep.sh".)The shell=True parameter is not needed (under a Posix system like Linux) if the first line of the bash script is a path to a shell; for example, #!/bin/bash.If sleep.sh has the shebang #!/bin/sh and it has appropriate file permissions  -- run chmod u+rx sleep.sh to make sure and it is in $PATH then your code should work as is:If the script is not in the PATH then specify the full path to it e.g., if it is in the current working directory:If the script has no shebang then you need to specify shell=True:If the script has no executable permissions and you can't change it e.g., by running os.chmod('sleep.sh', 0o755) then you could read the script as a text file and pass the string to subprocess module instead:Actually, you just have to add the shell=True argument:But beware - sourceMake sure that sleep.sh has execution permissions, and run it with shell=True:If someone looking for calling a script with arguments remember to convert the args to string before passing, using str(arg).This can be used to pass as many arguments as required If chmod not working then you also trytest by me thanks Adding an answer because I was directed here after asking how to run a bash script from python. You receive an error OSError: [Errno 2] file not found if your script takes in parameters. Lets say for instance your script took in a sleep time parameter: subprocess.call("sleep.sh 10") will not work, you must pass it as an array: subprocess.call(["sleep.sh", 10])

matplotlib: format axis offset-values to whole numbers or specific number

Jonathan

[matplotlib: format axis offset-values to whole numbers or specific number](https://stackoverflow.com/questions/3677368/matplotlib-format-axis-offset-values-to-whole-numbers-or-specific-number)

I have a matplotlib figure which I am plotting data that is always referred to as nanoseconds (1e-9).  On the y-axis, if I have data that is tens of nanoseconds, ie. 44e-9, the value on the axis shows as 4.4 with a +1e-8 as an offset.  Is there anyway to force the axis to show 44 with a +1e-9 offset?The same goes for my x-axis where the axis is showing +5.54478e4, where I would rather it show an offset of +55447 (whole number, no decimal - the value here is in days).I've tried a couple things like this:for the x-axis, but this doesn't work, though I'm probably using it incorrectly or misinterpreting something from the docs, can someone point me in the correct direction?Thanks,

JonathanI tried doing something with formatters but haven't found any solution yet...:andOn a side note, I'm actually confused as to where the 'offset number' object actually resides...is it part of the major/minor ticks?

2010-09-09 14:09:11Z

I have a matplotlib figure which I am plotting data that is always referred to as nanoseconds (1e-9).  On the y-axis, if I have data that is tens of nanoseconds, ie. 44e-9, the value on the axis shows as 4.4 with a +1e-8 as an offset.  Is there anyway to force the axis to show 44 with a +1e-9 offset?The same goes for my x-axis where the axis is showing +5.54478e4, where I would rather it show an offset of +55447 (whole number, no decimal - the value here is in days).I've tried a couple things like this:for the x-axis, but this doesn't work, though I'm probably using it incorrectly or misinterpreting something from the docs, can someone point me in the correct direction?Thanks,

JonathanI tried doing something with formatters but haven't found any solution yet...:andOn a side note, I'm actually confused as to where the 'offset number' object actually resides...is it part of the major/minor ticks?I had exactly the same problem, and these two lines fixed the problem:A much easier solution is to simply customize the tick labels. Take this example:Notice how in the y-axis case, I multiplied the values by 1e9 then mentioned that constant in the y-labelEDITAnother option is to fake the exponent multiplier by manually adding its text to the top of the plot:EDIT2Also you can format the x-axis offset value in the same manner:You have to subclass ScalarFormatter to do what you need... _set_offset just adds a constant, you want to set ScalarFormatter.orderOfMagnitude. Unfortunately, manually setting orderOfMagnitude won't do anything, as it's reset when the ScalarFormatter instance is called to format the axis tick labels.  It shouldn't be this complicated, but I can't find an easier way to do exactly what you want...  Here's an example:Which yields something like:

Whereas, the default formatting would look like:

Hope that helps a bit!Edit: For what it's worth, I don't know where the offset label resides either... It would be  slightly easier to just manually set it, but I couldn't figure out how to do so...  I get the feeling that there has to be an easier way than all of this.  It works, though!Similar to Amro's answer, you can use FuncFormatterI think that a more elegant way is to use the ticker formatter. Here is an example for both xaxis and yaxis:Gonzalo's solution started working for me after having added set_scientific(False):As has been pointed out in the comments and in this answer, the offset may be switched off globally, by doing the following:For the second part, without manually resetting all the ticks again, this was my solution:obviously you can set the format string to whatever you want.

Difference between len() and .__len__()?

Mark

[Difference between len() and .__len__()?](https://stackoverflow.com/questions/2481421/difference-between-len-and-len)

Is there any difference between calling len([1,2,3]) or [1,2,3].__len__()?If there is no visible difference, what is done differently behind the scenes?

2010-03-20 00:53:24Z

Is there any difference between calling len([1,2,3]) or [1,2,3].__len__()?If there is no visible difference, what is done differently behind the scenes?len is a function to get the length of a collection. It works by calling an object's __len__ method. __something__ attributes are special and usually more than meets the eye, and generally should not be called directly.It was decided at some point long ago getting the length of something should be a function and not a method code, reasoning that len(a)'s meaning would be clear to beginners but a.len() would not be as clear. When Python started __len__ didn't even exist and len was a special thing that worked with a few types of objects. Whether or not the situation this leaves us makes total sense, it's here to stay.It's often the case that the "typical" behavior of a built-in or operator is to call (with different and nicer syntax) suitable magic methods (ones with names like __whatever__) on the objects involved.  Often the built-in or operator has "added value" (it's able to take different paths depending on the objects involved) -- in the case of len vs __len__, it's just a bit of sanity checking on the built-in that is missing from the magic method:When you see a call to the len built-in, you're sure that, if the program continues after that rather than raising an exception, the call has returned an integer, non-negative, and less than 2**31 -- when you see a call to xxx.__len__(), you have no certainty (except that the code's author is either unfamiliar with Python or up to no good;-).Other built-ins provide even more added value beyond simple sanity checks and readability. By uniformly designing all of Python to work via calls to builtins and use of operators, never through calls to magic methods, programmers are spared from the burden of remembering which case is which.  (Sometimes an error slips in: until 2.5, you had to call foo.next() -- in 2.6, while that still works for backwards compatibility, you should call next(foo), and in 3.*, the magic method is correctly named __next__ instead of the "oops-ey" next!-).So the general rule should be to never call a magic method directly (but always indirectly through a built-in) unless you know exactly why you need to do that (e.g., when you're overriding such a method in a subclass, if the subclass needs to defer to the superclass that must be done through explicit call to the magic method).You can think of len() as being roughly equivalent toOne advantage is that it allows you to write things likeinstead oforThere is slightly different behaviour though.

For example in the case of intsYou can check Pythond docs:

How to expire session due to inactivity in Django?

Akbar ibrahim

[How to expire session due to inactivity in Django?](https://stackoverflow.com/questions/3024153/how-to-expire-session-due-to-inactivity-in-django)

Our Django application has the following session management requirements.After reading the documentation, Django code and some blog posts related to this, I have come up with the following implementation approach.Requirement 1

This requirement is easily implemented by setting SESSION_EXPIRE_AT_BROWSER_CLOSE to True.Requirement 2

I have seen a few recommendations to use SESSION_COOKIE_AGE to set the session expiry period. But this method has the following problems.The following method could be used to implemented this requirement and to workaround the problems mentioned above.Requirement 3

When we detect that the session has expired (in the custom SessionMiddleware above), set an attribute on the request to indicate session expiry. This attribute can be used to display an appropriate message to the user.Requirement 4

Use JavaScript to detect user inactivity, provide the warning and also an option to extend the session. If the user wishes to extend, send a keep alive pulse to the server to extend the session.Requirement 5

Use JavaScript to detect user activity (during the long business operation) and send keep alive pulses to the server to prevent session from expiring.The above implementation approach seem very elaborate and I was wondering if there might a simpler method (especially for Requirement 2). Any insights will be highly appreciated.

2010-06-11 15:39:04Z

Our Django application has the following session management requirements.After reading the documentation, Django code and some blog posts related to this, I have come up with the following implementation approach.Requirement 1

This requirement is easily implemented by setting SESSION_EXPIRE_AT_BROWSER_CLOSE to True.Requirement 2

I have seen a few recommendations to use SESSION_COOKIE_AGE to set the session expiry period. But this method has the following problems.The following method could be used to implemented this requirement and to workaround the problems mentioned above.Requirement 3

When we detect that the session has expired (in the custom SessionMiddleware above), set an attribute on the request to indicate session expiry. This attribute can be used to display an appropriate message to the user.Requirement 4

Use JavaScript to detect user inactivity, provide the warning and also an option to extend the session. If the user wishes to extend, send a keep alive pulse to the server to extend the session.Requirement 5

Use JavaScript to detect user activity (during the long business operation) and send keep alive pulses to the server to prevent session from expiring.The above implementation approach seem very elaborate and I was wondering if there might a simpler method (especially for Requirement 2). Any insights will be highly appreciated.Here's an idea... Expire the session on browser close with the SESSION_EXPIRE_AT_BROWSER_CLOSE setting. Then set a timestamp in the session on every request like so.and add a middleware to detect if the session is expired. something like this should handle the whole process...Then you just have to make some urls and views to return relevant data to the ajax calls regarding the session expiry.when the user opts to "renew" the session, so to speak, all you have to do is set requeset.session['last_activity'] to the current time againObviously this code is only a start... but it should get you on the right pathdjango-session-security does just that...... with an additional requirement: if the server doesn't respond or an attacker disconnected the internet connection: it should expire anyway.Disclamer: I maintain this app. But I've been watching this thread for a very, very long time :)I am just pretty new to use Django.I wanted to make session expire if logged user close browser or are in idle(inactivity timeout) for some amount of time. When I googled it to figure out, this SOF question came up first. Thanks to nice answer, I looked up resources to understand how middlewares works during request/response cycle in Django. It was very helpful.I was about to apply custom middleware into my code following top answer in here. But I was still little bit suspicious because best answer in here was edited in 2011. I took more time to search little bit from recent search result and came up with simple way.I didn't check other browsers but chrome.

1. A session expired when I closed a browser even if SESSION_COOKIE_AGE set.

2. Only when I was idle for more than 10 seconds, A session expired. Thanks to SESSION_SAVE_EVERY_REQUEST, whenever you occur new request, It saves the session and updates timeout to expireTo change this default behavior, set the SESSION_SAVE_EVERY_REQUEST setting to True. When set to True, Django will save the session to the database on every single request.Note that the session cookie is only sent when a session has been created or modified. If SESSION_SAVE_EVERY_REQUEST is True, the session cookie will be sent on every request.Similarly, the expires part of a session cookie is updated each time the session cookie is sent.django manual 1.10I just leave answer so that some people who is a kind of new in Django like me don't spend much time to find out solution as a way I did. One easy way to satisfy your second requirement would be to set SESSION_COOKIE_AGE value in settings.py to a suitable amount of seconds. For instance:However, by only doing this the session will expire after 10 minutes whether or not the user exhibits some activity. To deal with this issue, expiration time can be automatically renewed (for another extra 10 minutes) every time the user performs any kind of request with the following sentence:also you can use 

stackoverflow build in functionsIn the first request, you can set the session expiry asAnd when using the access_key and token,

What is dtype('O')?

quant

[What is dtype('O')?](https://stackoverflow.com/questions/37561991/what-is-dtypeo)

I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;What does this mean?

2016-06-01 07:22:00Z

I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;What does this mean?It means:Source.Another answer helps if need check types.It means "a python object", i.e. not one of the builtin scalar types supported by numpy.What is dtype?Something that belongs to pandas or numpy, or both, or something else? If we examine pandas code:It will output like this:You can interpret the last as Pandas dtype('O') or Pandas object which is Python type string, and this corresponds to Numpy string_, or unicode_ types.Like Don Quixote is on ass, Pandas is on Numpy and Numpy understand the underlying  architecture of your system and uses the class numpy.dtype for that.Data type object is an instance of numpy.dtype class that understand the data type more precise including:In the context of this question dtype belongs to both pands and numpy and in particular dtype('O') means we expect the string.Here is some code for testing with explanation:

If we have the dataset as dictionaryThe last lines will examine the dataframe and note the output:All kind of different dtypesBut if we try to set np.nan or None this will not affect the original column dtype. The output will be like this:So np.nan or None will not change the columns dtype, unless we set the all column rows to np.nan or None. In that case column will become float64 or object respectively.You may try also setting single rows:And to note here, if we set string inside a non string column it will become string or object dtype.'O' stands for object.The first line returns: dtype('O')The line with the print statement returns the following: object

Loading initial data with Django 1.7 and data migrations

Mickaël

[Loading initial data with Django 1.7 and data migrations](https://stackoverflow.com/questions/25960850/loading-initial-data-with-django-1-7-and-data-migrations)

I recently switched from Django 1.6 to 1.7, and I began using migrations (I never used South).Before 1.7, I used to load initial data with a fixture/initial_data.json file, which was loaded with the python manage.py syncdb command (when creating the database).Now, I started using migrations, and this behavior is deprecated :The official documentation does not have a clear example on how to do it, so my question is :What is the best way to import such initial data using data migrations :I prefer the second option.I don't want to use South, as Django seems to be able to do it natively now.

2014-09-21 15:37:13Z

I recently switched from Django 1.6 to 1.7, and I began using migrations (I never used South).Before 1.7, I used to load initial data with a fixture/initial_data.json file, which was loaded with the python manage.py syncdb command (when creating the database).Now, I started using migrations, and this behavior is deprecated :The official documentation does not have a clear example on how to do it, so my question is :What is the best way to import such initial data using data migrations :I prefer the second option.I don't want to use South, as Django seems to be able to do it natively now.Update: See @GwynBleidD's comment below for the problems this solution can cause, and see @Rockallite's answer below for an approach that's more durable to future model changes.Assuming you have a fixture file in <yourapp>/fixtures/initial_data.jsonYou should NOT use loaddata management command directly in a data migration.loaddata utilizes django.core.serializers.python.Deserializer which uses the most up-to-date models to deserialize historical data in a migration. That's incorrect behavior.For example, supposed that there is a data migration which utilizes loaddata management command to load data from a fixture, and it's already applied on your development environment.Later, you decide to add a new required field to the corresponding model, so you do it and make a new migration against your updated model (and possibly provide a one-off value to the new field when ./manage.py makemigrations prompts you).You run the next migration, and all is well.Finally, you're done developing your Django application, and you deploy it on the production server. Now it's time for you to run the whole migrations from scratch on the production environment.However, the data migration fails. That's because the deserialized model from loaddata command, which represents the current code, can't be saved with empty data for the new required field you added. The original fixture lacks necessary data for it!But even if you update the fixture with required data for the new field, the data migration still fails. When the data migration is running, the next migration which adds the corresponding column to the database, is not applied yet. You can't save data to a column which does not exist!Conclusion: in a data migration, the loaddata command introduces potential inconsistency between the model and the database. You should definitely NOT use it directly in a data migration.loaddata command relies on django.core.serializers.python._get_model function to get the corresponding model from a fixture, which will return the most up-to-date version of a model. We need to monkey-patch it so it gets the historical model.(The following code works for Django 1.8.x)Inspired by some of the comments (namely n__o's) and the fact that I have a lot of initial_data.* files spread out over multiple apps I decided to create a Django app that would facilitate the creation of these data migrations.Using django-migration-fixture you can simply run the following management command and it will search through all your INSTALLED_APPS for initial_data.* files and turn them into data migrations.See django-migration-fixture for install/usage instructions.In order to give your database some initial data, write a data migration.

In the data migration, use the RunPython function to load your data.Don't write any loaddata command as this way is deprecated.Your data migrations will be run only once. The migrations are an ordered sequence of migrations. When the 003_xxxx.py migrations is run, django migrations writes in the database that this app is migrated until this one (003), and will run the following migrations only.The solutions presented above didn't work for me unfortunately. I found that every time I change my models I have to update my fixtures. Ideally I would instead write data migrations to modify created data and fixture-loaded data similarly.To facilitate this I wrote a quick function which will look in the fixtures directory of the current app and load a fixture. Put this function into a migration in the point of the model history that matches the fields in the migration.In my opinion fixtures are a bit bad. If your database changes frequently, keeping them up-to-date will came a nightmare soon. Actually, it's not only my opinion, in the book "Two Scoops of Django" it's explained much better.Instead I'll write a Python file to provide initial setup. If you need something more I'll suggest you look at Factory boy.If you need to migrate some data you should use data migrations.There's also "Burn Your Fixtures, Use Model Factories" about using fixtures.On Django 2.1, I wanted to load some models (Like country names for example) with initial data.But I wanted this to happen automatically right after the execution of initial migrations.So I thought that it would be great to have an sql/ folder inside each application that required initial data to be loaded.Then within that sql/ folder I would have .sql files with the required DMLs to load the initial data into the corresponding models, for example:To be more descriptive, this is how an app containing an sql/ folder would look:

Also I found some cases where I needed the sql scripts to be executed in a specific order. So I decided to prefix the file names with a consecutive number as seen in the image above.Then I needed a way to load any SQLs available inside any application folder automatically by doing python manage.py migrate.So I created another application named initial_data_migrations and then I added this app to the list of INSTALLED_APPS in settings.py file. Then I created a migrations folder inside and added a file called run_sql_scripts.py (Which actually is a custom migration). As seen in the image below:I created run_sql_scripts.py so that it takes care of running all sql scripts available within each application. This one is then fired when someone runs python manage.py migrate. This custom migration also adds the involved applications as dependencies, that way it attempts to run the sql statements only after the required applications have executed their 0001_initial.py migrations (We don't want to attempt running a SQL statement against a non-existent table).Here is the source of that script:I hope someone finds this helpful, it worked just fine for me!. If you have any questions please let me know.NOTE: This might not be the best solution since I'm just getting started with django, however still wanted to share this "How-to" with you all since I didn't find much information while googling about this.

Interpreting a benchmark in C, Clojure, Python, Ruby, Scala and others [closed]

defhlt

[Interpreting a benchmark in C, Clojure, Python, Ruby, Scala and others [closed]](https://stackoverflow.com/questions/11641098/interpreting-a-benchmark-in-c-clojure-python-ruby-scala-and-others)

I know that artificial benchmarks are evil. They can show results only for very specific narrow situation. I don't assume that one language is better than the other because of the some stupid bench. However I wonder why results is so different. Please see my questions at the bottom.Benchmark is simple math calculations to find pairs of prime numbers which differs by 6 (so called sexy primes)

E.g. sexy primes below 100 would be: (5 11) (7 13) (11 17) (13 19) (17 23) (23 29) (31 37) (37 43) (41 47) (47 53) (53 59) (61 67) (67 73) (73 79) (83 89) (97 103)In table: calculation time  in seconds

Running: all except Factor was running in VirtualBox (Debian unstable amd64 guest, Windows 7 x64 host)

CPU: AMD A4-3305M[*1] - I'm afraid to imagine how much time will it takeC:Ruby:Scala:Scala opimized isPrime (the same idea like in Clojure optimization):Clojure:Clojure optimized is-prime?:PythonFactorBash(zsh):

2012-07-25 00:20:44Z

I know that artificial benchmarks are evil. They can show results only for very specific narrow situation. I don't assume that one language is better than the other because of the some stupid bench. However I wonder why results is so different. Please see my questions at the bottom.Benchmark is simple math calculations to find pairs of prime numbers which differs by 6 (so called sexy primes)

E.g. sexy primes below 100 would be: (5 11) (7 13) (11 17) (13 19) (17 23) (23 29) (31 37) (37 43) (41 47) (47 53) (53 59) (61 67) (67 73) (73 79) (83 89) (97 103)In table: calculation time  in seconds

Running: all except Factor was running in VirtualBox (Debian unstable amd64 guest, Windows 7 x64 host)

CPU: AMD A4-3305M[*1] - I'm afraid to imagine how much time will it takeC:Ruby:Scala:Scala opimized isPrime (the same idea like in Clojure optimization):Clojure:Clojure optimized is-prime?:PythonFactorBash(zsh):Rough answers:Most important optimisation in the Clojure code would be to use typed primitive maths within is-prime?, something like:With this improvement, I get Clojure completing 10k in 0.635 secs (i.e. the second fastest on your list, beating Scala)P.S. note that you have printing code inside your benchmark in some cases - not a good idea as it will distort the results, especially if using a function like print for the first time causes initialisation of IO subsystems or something like that!Here's a fast Clojure version, using the same basic algorithms:It runs about 20x faster than your original on my machine. And here's a version that leverages the new reducers library in 1.5 (requires Java 7 or JSR 166):This runs about 40x faster than your original. On my machine, that's 100k in 1.5 seconds.I'll answer just #2, since it's the only one I've got anything remotely intelligent to say, but for your Python code, you're creating an intermediate list in is_prime, whereas you're using .map in your all in Ruby which is just iterating.If you change your is_prime to:they're on par.I could optimize the Python further, but my Ruby isn't good enough to know when I've given more of an advantage (e.g., using xrange makes Python win on my machine, but I don't remember if the Ruby range you used creates an entire range in memory or not).EDIT: Without being too silly, making the Python code look like:which doesn't change much more, puts it at 1.5s for me, and, with being extra silly, running it with PyPy puts it at .3s for 10K, and 21s for 100K.You can make the Scala a lot faster by modifying your isPrime method toNot quite as concise but the program runs in 40% of the time! We cut out the superfluous Range and anonymous Function objects, the Scala compiler recognizes the tail-recursion and turns it into a while-loop, which the JVM can turn into more or less optimal machine code, so it shouldn't be too far off the C version.See also: How to optimize for-comprehensions and loops in Scala?Here is my scala version in both parallel and no-parallel, just for fun:

(In my dual core compute, the parallel version takes 335ms while the no-parallel version takes 655ms)EDIT: According to Emil H's suggestion, I have changed my code to avoid the effects of IO and jvm warmup:The result shows in my compute:Never mind the benchmarks; the problem got me interested and I made some fast tweaks.  This uses the lru_cache decorator, which memoizes a function; so when we call is_prime(i-6) we basically get that prime check for free.  This change cuts the work roughly in half.  Also, we can make the range() calls step through just the odd numbers, cutting the work roughly in half again.http://en.wikipedia.org/wiki/Memoizationhttp://docs.python.org/dev/library/functools.htmlThis requires Python 3.2 or newer to get lru_cache, but could work with an older Python if you install a Python recipe that provides lru_cache.  If you are using Python 2.x you should really use xrange() instead of range().http://code.activestate.com/recipes/577479-simple-caching-decorator/The above took only a very short time to edit.  I decided to take it one step further, and make the primes test only try prime divisors, and only up to the square root of the number being tested.  The way I did it only works if you check numbers in order, so it can accumulate all the primes as it goes; but this problem was already checking the numbers in order so that was fine.On my laptop (nothing special; processor is a 1.5 GHz AMD Turion II "K625") this version produced an answer for 100K in under 8 seconds.The above code is pretty easy to write in Python, Ruby, etc. but would be more of a pain in C.You can't compare the numbers on this version against the numbers from the other versions without rewriting the others to use similar tricks.  I'm not trying to prove anything here; I just thought the problem was fun and I wanted to see what sort of easy performance improvements I could glean.Don't forget Fortran!  (Mostly joking, but I would expect similar performance to C).  The statements with exclamation points are optional, but good style. (! is a comment character in fortran 90)I couldn't resist to do a few of the most obvious optimizations for the C version which made the 100k test now take 0.3s on my machine (5 times faster than the C version in the question, both compiled with MSVC 2010 /Ox).Here is the identical implemention in Java:With Java 1.7.0_04 this runs almost exactly as fast as the C version. Client or server VM doesn't show much difference, except that JIT training seems to help the server VM a bit (~3%) while it has almost no effect with the client VM. The output in Java seems to be slower than in C. If the output is replaced with a static counter in both versions, the Java version runs a little faster than the C version.These are my times for the 100k run:and the 1M run (16386 results):While this does not really answer your questions, it shows that small tweaks can have a noteworthy impact on performance. So to be able to really compare languages you should try to avoid all algorithmic differences as much as possible.It also gives a hint why Scala seems rather fast. It runs on the Java VM and thus benefits from its impressive performance.In Scala try using Tuple2 instead of List, it should go faster. Just remove the word 'List' since (x, y) is a Tuple2.Tuple2 is specialized for Int, Long and Double meaning it won't have to box/unbox those raw datatypes. Tuple2 source. List isn't specialized. List source.Here's the code for the Go (golang.org) version:It ran just as fast as the C version.Using an Asus u81a

Intel Core 2 Duo T6500 2.1GHz, 2MB L2 cache, 800MHz FSB.

4GB RAM The 100k version: C: 2.723s Go: 2.743sWith 1000000 (1M instead of 100K): C: 3m35.458s Go: 3m36.259s But I think that it would be fair to use Go's built in multithreading capabilities and compare that version with the regular C version (without multithreading), just because it's almost too easy to do multithreading with Go.Update: I did a parallel version using Goroutines in Go:The parallelized version used in average 2.743 seconds, the exact same time that the regular version used.The parallelized version completed in 1.706 seconds. It used less than 1.5 Mb RAM.One odd thing: My dual core kubuntu 64bit never peaked in both cores.  It looked like Go was using just one core. Fixed with a call to runtime.GOMAXPROCS(4)Update: I ran the paralellized version up to 1M numbers.  One of My CPU cores was at 100% all the time, while the other wasn't used at all (odd).  It took a whole minute more than the C and the regular Go versions. :(With 1000000 (1M instead of 100K): C: 3m35.458s Go: 3m36.259s Go using goroutines:3m27.137s2m16.125s The 100k version: C: 2.723s Go: 2.743s Go using goroutines: 1.706sJust for the fun of it, here is a parallel Ruby version. On my 1.8GHz Core i5 MacBook Air, the performance results are:It looks like the JVM's JIT is giving Ruby a nice performance boost in the default case, while true multithreading helps JRuby perform 50% faster in the threaded case. What's more interesting is that JRuby 1.7 improves the JRuby 1.6 score by a healthy 17%!Based on x4u's answer, I wrote a scala version using recursion, and I improved on it by only going to the sqrt instead of x/2 for the prime check function. I get ~250ms for 100k, and ~600ms for 1M. I went ahead and went to 10M in ~6s.I also went back and wrote a CoffeeScript (V8 JavaScript) version, which gets ~15ms for 100k, 250ms for 1M, and 6s for 10M, by using a counter (ignoring I/O). If I turn on the output it takes ~150ms for 100k, 1s for 1M, and 12s for 10M. Couldn't use tail recursion here, unfortunately, so I had to convert it back into loops.The answer to your question #1 is that Yes, the JVM is incredably fast and yes static typing helps.The JVM should be faster than C in the long run, possibly even faster than "Normal" assembly language--Of course you can always hand optimize assembly to beat anything by doing manual runtime profiling and creating a separate version for each CPU, you just have to be amazingly good and knowledgable.The reasons for Java's speed are: The JVM can analyze your code while it runs and hand-optimize it--for instance, if you had a method that could be statically analyzed at compile time to be a true function and the JVM noticed that you were often calling it with the same parameters, it COULD actually eliminate the call completely and just inject the results from the last call (I'm not sure if Java actually does this exactly, but it doest a lot of stuff like this). Due to static typing, the JVM can know a lot about your code at compile time, this lets it pre-optimize quite a bit of stuff.  It also lets the compiler optimize each class individually without knowledge of how another class is planning to use it.  Also Java doesn't have arbitrary pointers to memory location, it KNOWS what values in memory may and may not be changed and can optimize accordingly.Heap allocation is MUCH more efficient than C, Java's heap allocation is more like C's stack allocation in speed--yet more versatile.  A lot of time has gone into the different algroithims used here, it's an art--for instance, all the objects with a short lifespan (like C's stack variables) are allocated to a "known" free location (no searching for a free spot with enough space) and are all freed together in a single step (like a stack pop). The JVM can know quirks about your CPU architecture and generate machine code specifically for a given CPU.The JVM can speed your code long after you shipped it.  Much like moving a program to a new CPU can speed it up, moving it to a new version of the JVM can also give you huge speed performances taylored to CPUs that didn't even exist when you initially compiled your code, something c physically cannot do without a recomiple.By the way, most of the bad rep for java speed comes from the long startup time to load the JVM (Someday someone will build the JVM into the OS and this will go away!) and the fact that many developers are really bad at writing GUI code (especially threaded) which caused Java GUIs to often become unresponsive and glitchy.  Simple to use languages like Java and VB have their faults amplified by the fact that the capibilities of the average programmer tends to be lower than more complicated languages.

How to stop Flask from initialising twice in Debug Mode? [duplicate]

Matt Alcock

[How to stop Flask from initialising twice in Debug Mode? [duplicate]](https://stackoverflow.com/questions/9449101/how-to-stop-flask-from-initialising-twice-in-debug-mode)

When building a Flask service in Python and setting the debug mode on, the Flask service will initialise twice. When the initialisation loads caches and the like, this can take a while. Having to do this twice is annoying when in development (debug) mode. When debug is off, the Flask service only initialises once.How to stop Flask from initialising twice in Debug Mode?

2012-02-25 23:35:01Z

When building a Flask service in Python and setting the debug mode on, the Flask service will initialise twice. When the initialisation loads caches and the like, this can take a while. Having to do this twice is annoying when in development (debug) mode. When debug is off, the Flask service only initialises once.How to stop Flask from initialising twice in Debug Mode?The simplest thing to do here would be to add use_reloader=False to your call to app.run - that is: app.run(debug=True, use_reloader=False)Alternatively, you can check for the value of WERKZEUG_RUN_MAIN in the environment:However, the condition is a bit more convoluted when you want the behavior to happen any time except in the loading process:You can use the before_first_request hook:

How to use subprocess popen Python

Stupid.Fat.Cat

[How to use subprocess popen Python](https://stackoverflow.com/questions/12605498/how-to-use-subprocess-popen-python)

Since os.popen is being replaced by subprocess.popen, I was wondering how would I convertto subprocess.popen()I tried:But I guess I'm not properly writing this out. Any help would be appreciated. Thanks

2012-09-26 15:42:45Z

Since os.popen is being replaced by subprocess.popen, I was wondering how would I convertto subprocess.popen()I tried:But I guess I'm not properly writing this out. Any help would be appreciated. Thankssubprocess.Popen takes a list of arguments:There's even a section of the documentation devoted to helping users migrate from os.popen to subprocess.Use sh, it'll make things a lot easier:

How to「properly」print a list?

Obaid

[How to「properly」print a list?](https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list)

So I have a list:['x', 3, 'b']And I want the output to be:[x, 3, b]How can I do this in python?If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.

2011-03-26 23:04:20Z

So I have a list:['x', 3, 'b']And I want the output to be:[x, 3, b]How can I do this in python?If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.In Python 2:In Python 3 (where print is a builtin function and not a syntax feature anymore):Both return:This is using the map() function to call str for each element of mylist, creating a new list of strings that is then joined into one string with str.join(). Then, the % string formatting operator substitutes the string in instead of %s in "[%s]".This is simple code, so if you are new you should understand it easily enough.It prints all of them without quotes, like you wanted.If you are using Python3:Using only print:Instead of using map, I'd recommend using a generator expression with the capability of join to accept an iterator:Here, join is a member function of the string class str. It takes one argument: a list (or iterator) of strings, then returns a new string with all of the elements concatenated by, in this case, ,.You can delete all unwanted characters from a string using its translate() method with None for the table argument followed by a string containing the character(s) you want removed for its deletechars argument.If you're using a version of Python before 2.6, you'll need to use the string module's translate() function instead because the ability to pass None as the table argument wasn't added until Python 2.6. Using it looks like this:Using the string.translate() function will also work in 2.6+, so using it might be preferable.Here's an interactive session showing some of the steps in @TokenMacGuy's one-liner. First he uses the map function to convert each item in the list to a string (actually, he's making a new list, not converting the items in the old list). Then he's using the string method join to combine those strings with ', ' between them. The rest is just string formatting, which is pretty straightforward. (Edit: this instance is straightforward; string formatting in general can be somewhat complex.)Note that using join is a simple and efficient way to build up a string from several substrings, much more efficient than doing it by successively adding strings to strings, which involves a lot of copying behind the scenes.Using .format for string formatting,Output:Explanation:Reference: 

.format for string formatting PEP-3101I was inspired by @AniMenon to write a pythonic more general solution. It only uses the format method. No trace of str, and it allows for the fine tuning of the elements format.

For example, if you have float numbers as elements of the list, you can adjust their format, by adding a conversion specifier, in this case :.2fThe output is quite decent:

How do I get a raw, compiled SQL query from a SQLAlchemy expression?

cce

[How do I get a raw, compiled SQL query from a SQLAlchemy expression?](https://stackoverflow.com/questions/4617291/how-do-i-get-a-raw-compiled-sql-query-from-a-sqlalchemy-expression)

I have a SQLAlchemy query object and want to get the text of the compiled SQL statement, with all its parameters bound (e.g. no %s or other variables waiting to be bound by the statement compiler or MySQLdb dialect engine, etc).Calling str() on the query reveals something like this:I've tried looking in query._params but it's an empty dict.  I wrote my own compiler using this example of the sqlalchemy.ext.compiler.compiles decorator but even the statement there still has %s where I want data.I can't quite figure out when my parameters get mixed in to create the query; when examining the query object they're always an empty dictionary (though the query executes fine and the engine prints it out when you turn echo logging on).I'm starting to get the message that SQLAlchemy doesn't want me to know the underlying query, as it breaks the general nature of the expression API's interface all the different DB-APIs.  I don't mind if the query gets executed before I found out what it was; I just want to know!

2011-01-06 16:43:05Z

I have a SQLAlchemy query object and want to get the text of the compiled SQL statement, with all its parameters bound (e.g. no %s or other variables waiting to be bound by the statement compiler or MySQLdb dialect engine, etc).Calling str() on the query reveals something like this:I've tried looking in query._params but it's an empty dict.  I wrote my own compiler using this example of the sqlalchemy.ext.compiler.compiles decorator but even the statement there still has %s where I want data.I can't quite figure out when my parameters get mixed in to create the query; when examining the query object they're always an empty dictionary (though the query executes fine and the engine prints it out when you turn echo logging on).I'm starting to get the message that SQLAlchemy doesn't want me to know the underlying query, as it breaks the general nature of the expression API's interface all the different DB-APIs.  I don't mind if the query gets executed before I found out what it was; I just want to know!This blog provides an updated answer.Quoting from the blog post, this is suggested and worked for me.Where q is defined as:Or just any kind of session.query().Thanks to Nicolas Cadou for the answer! I hope it helps others who come searching here.The documentation uses literal_binds to print a query q including parameters:The documentation also issues this warning:This should work with Sqlalchemy >= 0.6For the MySQLdb backend I modified albertov's awesome answer (thanks so much!) a bit.  I'm sure they could be merged to check if comp.positional was True but that's slightly beyond the scope of this question.Thing is, sqlalchemy never mixes the data with your query. The query and the data are passed separately to your underlying database driver - the interpolation of data happens in your database.Sqlalchemy passes the query as you've seen in str(myquery) to the database, and the values will go in a separate tuple.You could use some approach where you interpolate the data with the query yourself (as albertov suggested below), but that's not the same thing that sqlalchemy is executing.For postgresql backend using psycopg2, you can listen for the do_execute event, then use the cursor, statement and type coerced parameters along with Cursor.mogrify() to inline the parameters. You can return True to prevent actual execution of the query.Sample usage:First let me preface by saying that I assume you're doing this mainly for debugging purposes -- I wouldn't recommend trying to modify the statement outside of the SQLAlchemy fluent API.Unfortunately there doesn't seem to be a simple way to show the compiled statement with the query parameters included. SQLAlchemy doesn't actually put the parameters into the statement -- they're passed into the database engine as a dictionary. This lets the database-specific library handle things like escaping special characters to avoid SQL injection. But you can do this in a two-step process reasonably easily. To get the statement, you can do as you've already shown, and just print the query:You can get one step closer with query.statement, to see the parameter names. (Note :id_1 below vs %s above -- not really a problem in this very simple example, but could be key in a more complicated statement.)Then, you can get the actual values of the parameters by getting the params property of the compiled statement:This worked for a MySQL backend at least; I would expect it's also general enough for PostgreSQL without needing to use psycopg2.The following solution uses the SQLAlchemy Expression Language and works with SQLAlchemy 1.1. This solution does not mix the parameters with the query (as requested by the original author), but provides a way of using SQLAlchemy models to generate SQL query strings and parameter dictionaries for different SQL dialects. The example is based on the tutorial http://docs.sqlalchemy.org/en/rel_1_0/core/tutorial.htmlGiven the class,we can produce a query statement using the select function.Next, we can compile the statement into a query object.By default, the statement is compiled using a basic 'named' implementation that is compatible with SQL databases such as SQLite and Oracle. If you need to specify a dialect such as PostgreSQL, you can doOr if you want to explicitly specify the dialect as SQLite, you can change the paramstyle from 'qmark' to 'named'.From the query object, we can extract the query string and query parametersand finally execute the query.You can use events from ConnectionEvents family: after_cursor_execute or before_cursor_execute.In sqlalchemy UsageRecipes by @zzzeek you can find this example:Here you can get access to your statementI think .statement would possibly do the trick:

http://docs.sqlalchemy.org/en/latest/orm/query.html?highlight=querySo, putting together a lot of little bits of these different answers, I came up with what I needed: a simple set of code to drop in and occasionally but reliably (i.e. handles all data types) grab the exact, compiled SQL sent to my Postgres backend by just interrogating the query itself:

output to the same line overwriting previous output ? python (2.5)

Kristian

[output to the same line overwriting previous output ? python (2.5)](https://stackoverflow.com/questions/4897359/output-to-the-same-line-overwriting-previous-output-python-2-5)

I am writing a simple ftp downloader. Part of to the code is something like this:i am calling function process to handle the callback:and output is something like this:but i want it to print this line and next time reprint/refresh it so it will only show it once and i will see progress of that download...How can it be done?

2011-02-04 11:13:47Z

I am writing a simple ftp downloader. Part of to the code is something like this:i am calling function process to handle the callback:and output is something like this:but i want it to print this line and next time reprint/refresh it so it will only show it once and i will see progress of that download...How can it be done?Here's code for Python 3.x:The end= keyword is what does the work here -- by default, print() ends in a newline (\n) character, but this can be replaced with a different string. In this case, ending the line with a carriage return instead returns the cursor to the start of the current line. Thus, there's no need to import the sys module for this sort of simple usage. print() actually has a number of keyword arguments which can be used to greatly simplify code.To use the same code on Python 2.6+, put the following line at the top of the file:If all you want to do is change a single line, use \r. \r means carriage return. It's effect is solely to put the caret back at the start of the current line. It does not erase anything. Similarly, \b can be used to go one character backward. (some terminals may not support all those features)Have a look at the curses module documentation and the curses module HOWTO.Really basic example:Here's my little class that can reprint blocks of text. It properly clears the previous text so you can overwrite your old text with shorter new text without creating a mess.I found that for a simple print statement in python 2.7, just put a comma at the end after your '\r'.This is shorter than other non-python 3 solutions, but also more difficult to maintain.You can just add '\r' at the end of the string plus a comma at the end of print function. For example:I am using spyder 3.3.1 - windows 7 - python 3.6

although flush may not be needed.

based on this posting - https://github.com/spyder-ide/spyder/issues/3437to overwiting the previous line in python all wath you need is to add end='\r' to the print function, test this example:

How do I undo True = False in python interactive mode? [duplicate]

horta

[How do I undo True = False in python interactive mode? [duplicate]](https://stackoverflow.com/questions/30563716/how-do-i-undo-true-false-in-python-interactive-mode)

So I tried the "evil" thing Ned Deily mentioned in his answer here. Now I have that the type True is now always False. How would I reverse this within the interactive window?Thing to not do:Since True has now been completely overridden with False, there doesn't seem to be an obvious way to back-track. Is there a module that True comes from that I can do something like:

2015-05-31 23:20:47Z

So I tried the "evil" thing Ned Deily mentioned in his answer here. Now I have that the type True is now always False. How would I reverse this within the interactive window?Thing to not do:Since True has now been completely overridden with False, there doesn't seem to be an obvious way to back-track. Is there a module that True comes from that I can do something like:You can simply del your custom name to set it back to the default:This works:but fails if False has been fiddled with as well. Therefore this is better:as None cannot be reassigned.These also evaluate to True regardless of whether True has been reassigned to False, 5, 'foo', None, etc:Another way:For completeness: Kevin mentions that you could also fetch the real True from __builtins__:But that True can also be overriden:So better to go with one of the other options.Just do this:Or, because booleans are essentially integers:Solutions that use no object literals but are as durable as 1 == 1. Of course, you can define False once True is defined, so I'll supply solutions as half pairs.

Python Empty Generator Function

Konstantin Weitz

[Python Empty Generator Function](https://stackoverflow.com/questions/13243766/python-empty-generator-function)

In python, one can easily define an iterator function, by putting the yield keyword in the function's body, such as:How can I define a generator function that yields no value (generates 0 values), the following code doesn't work, since python cannot know that it is supposed to be an generator and not a normal function:I could do something likeBut that would be very ugly. Is there any nice way to realize an empty iterator function?

2012-11-06 03:08:13Z

In python, one can easily define an iterator function, by putting the yield keyword in the function's body, such as:How can I define a generator function that yields no value (generates 0 values), the following code doesn't work, since python cannot know that it is supposed to be an generator and not a normal function:I could do something likeBut that would be very ugly. Is there any nice way to realize an empty iterator function?You can use return once in a generator; it stops iteration without yielding anything, and thus provides an explicit alternative to letting the function run out of scope. So use yield to turn the function into a generator, but precede it with return to terminate the generator before yielding anything.I'm not sure it's that much better than what you have -- it just replaces a no-op if statement with a no-op yield statement. But it is more idiomatic. Note that just using yield doesn't work.This question asks specifically about an empty generator function. For that reason, I take it to be a question about the internal consistency of Python's syntax, rather than a question about the best way to create an empty iterator in general.If question is actually about the best way to create an empty iterator, then you might agree with Zectbumo about using iter(()) instead. However, it's important to observe that iter(()) doesn't return a function! It directly returns an empty iterable. Suppose you're working with an API that expects a callable that returns an iterable. You'll have to do something like this:(Credit should go to Unutbu for giving the first correct version of this answer.)Now, you may find the above clearer, but I can imagine situations in which it would be less clear. Consider this example of a long list of (contrived) generator function definitions:At the end of that long list, I'd rather see something with a yield in it, like this:or, in Python 3.3 and above (as suggested by DSM), this:The presence of the yield keyword makes it clear at the briefest glance that this is just another generator function, exactly like all the others. It takes a bit more time to see that the iter(()) version is doing the same thing.It's a subtle difference, but I honestly think the yield-based functions are more readable and maintainable.You don't require a generator. C'mon guys!Python 3.3 (because I'm on a yield from kick, and because @senderle stole my first thought):But I have to admit, I'm having a hard time coming up with a use case for this for which iter([]) or (x)range(0) wouldn't work equally well.Another option is:I prefer the following:The "yield" turns it into a generator while Exception means None isn't included in the result (purely empty result).Must it be a generator function? If not, how aboutThe "standard" way to make an empty iterator appears to be iter([]).

I suggested to make [] the default argument to iter(); this was rejected with good arguments, see http://bugs.python.org/issue25215

- JurjenFor those of you that actually need a function and actually need a generator

How to plot two columns of a pandas data frame using points?

Roman

[How to plot two columns of a pandas data frame using points?](https://stackoverflow.com/questions/17812978/how-to-plot-two-columns-of-a-pandas-data-frame-using-points)

I have a pandas data frame and would like to plot values from one column versus the values from another column. Fortunately, there is plot method associated with the data-frames that seems to do what I need:Unfortunately, it looks like among the plot styles (listed here after the kind parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem.

2013-07-23 14:23:42Z

I have a pandas data frame and would like to plot values from one column versus the values from another column. Fortunately, there is plot method associated with the data-frames that seems to do what I need:Unfortunately, it looks like among the plot styles (listed here after the kind parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem.You can specify the style of the plotted line when calling df.plot:The style argument can also be a dict or list, e.g.:All the accepted style formats are listed in the documentation of matplotlib.pyplot.plot.For this (and most plotting) I would not rely on the Pandas wrappers to matplotlib. Instead, just use matplotlib directly:and remember that you can access a NumPy array of the column's values with df.col_name_1.values for example.I ran into trouble using this with Pandas default plotting in the case of a column of Timestamp values with millisecond precision. In trying to convert the objects to datetime64 type, I also discovered a nasty issue: < Pandas gives incorrect result when asking if Timestamp column values have attr astype >.Pandas uses matplotlib as a library for basic plots. The easiest way in your case will using the following: However, I would recommend to use seaborn as an alternative solution if you want have more customized plots while not going into the basic level of matplotlib. In this case you the solution will be following: Now in latest pandas you can directly use df.plot.scatter functionhttps://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.plot.scatter.html

Pretty Printing a pandas dataframe

Ofer

[Pretty Printing a pandas dataframe](https://stackoverflow.com/questions/18528533/pretty-printing-a-pandas-dataframe)

How can I print a pandas dataframe as a nice text-based table, like the following?

2013-08-30 08:40:28Z

How can I print a pandas dataframe as a nice text-based table, like the following?I've just found a great tool for that need, it is called tabulate.It prints tabular data and works with DataFrame.Note:You can use prettytable to render the table as text. The trick is to convert the data_frame to an in-memory csv file and have prettytable read it. Here's the code:A simple approach is to output as html, which pandas does out of the box:If you are in Jupyter notebook, you could run the following code to interactively display the dataframe in a well formatted table.   This answer builds on the to_html('temp.html') answer above, but instead of creating a file displays the well formatted table directly in the notebook:Credit for this code due to example at: Show DataFrame as table in iPython NotebookI used Ofer's answer for a while and found it great in most cases. Unfortunately, due to inconsistencies between pandas's to_csv and prettytable's from_csv, I had to use prettytable in a different way.  One failure case is a dataframe containing commas:Prettytable raises an error of the form:The following function handles this case:If you don't care about the index, use:If you want an inbuilt function to dump your data into some github markdown, you now have one. Take a look at to_markdown:Here's what that looks like on github:Note that you will still need to have the tabulate package installed.Following up on Mark's answer, if you're not using Jupyter for some reason, e.g. you want to do some quick testing on the console, you can use the DataFrame.to_string method, which works from -- at least -- Pandas 0.12 (2014) onwards.I wanted a paper printout of a dataframe but I wanted to add some results and comments as well on the same page.

I have worked through the above and I could not get what I wanted. I ended up using

file.write(df1.to_csv()) and file.write(",,,blah,,,,,,blah") statements to get my extras on the page.

When I opened the csv file it went straight to a spreadsheet which printed everything in the right pace and format.

How to override the copy/deepcopy operations for a Python object?

Brent Writes Code

[How to override the copy/deepcopy operations for a Python object?](https://stackoverflow.com/questions/1500718/how-to-override-the-copy-deepcopy-operations-for-a-python-object)

I understand the difference between copy vs. deepcopy in the copy module. I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.Here's my scenario: I have a configuration object.  Initially, I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  However, once user interaction starts, each object needs to tweak its configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).Here's a sample object:What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?

2009-09-30 21:18:45Z

I understand the difference between copy vs. deepcopy in the copy module. I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.Here's my scenario: I have a configuration object.  Initially, I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  However, once user interaction starts, each object needs to tweak its configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).Here's a sample object:What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?The recommendations for customizing are at the very end of the docs page:Since you appear not to care about pickling customization, defining __copy__ and __deepcopy__ definitely seems like the right way to go for you.Specifically, __copy__ (the shallow copy) is pretty easy in your case...:__deepcopy__ would be similar (accepting a memo arg too) but before the return it would have to call self.foo = deepcopy(self.foo, memo) for any attribute self.foo that needs deep copying (essentially attributes that are containers -- lists, dicts, non-primitive objects which hold other stuff through their __dict__s).Putting together Alex Martelli's answer and Rob Young's comment you get the following code:printshere __deepcopy__ fills in the memo dict to avoid excess copying in case the object itself is referenced from its member.Following Peter's excellent answer, to implement a custom deepcopy, with minimal alteration to the default implementation (e.g. just modifying a field like I needed) :I might be a bit off on the specifics, but here goes;From the copy docs;In other words: copy() will copy only the top element and leave the rest as pointers into the original structure. deepcopy() will recursively copy over everything.That is, deepcopy() is what you need.If you need to do something really specific, you can override __copy__() or __deepcopy__(), as described in the manual. Personally, I'd probably implement a plain function (e.g. config.copy_config() or such) to make it plain that it isn't Python standard behaviour.Its not clear from your problem why you need to override these methods, since you don't want to do any customization to the copying methods.Anyhow, if you do want to customize the deep copy (e.g. by sharing some attributes and copying others), here is a solution:The copy module uses evantually the __getstate__()/__setstate__() pickling protocol, so these are also valid targets to override.  The default implementation just returns and sets the __dict__ of the class, so you don't have to call super() and worry about Eino Gourdin's clever trick, above.Building on Antony Hatchkins' clean answer, here's my version where the class in question derives from another custom class (s.t. we need to call super):

ValueError: unsupported pickle protocol: 3, python2 pickle can not load the file dumped by python 3 pickle?

Aleeee

[ValueError: unsupported pickle protocol: 3, python2 pickle can not load the file dumped by python 3 pickle?](https://stackoverflow.com/questions/25843698/valueerror-unsupported-pickle-protocol-3-python2-pickle-can-not-load-the-file)

I use pickle to dump a file on python 3, and I use pickle to load the file on python 2, the ValueError appears. So, python 2 pickle can not load the file dumped by python 3 pickle?If I want it? How to do?

2014-09-15 08:26:29Z

I use pickle to dump a file on python 3, and I use pickle to load the file on python 2, the ValueError appears. So, python 2 pickle can not load the file dumped by python 3 pickle?If I want it? How to do?You should write the pickled data with a lower protocol number in Python 3. Python 3 introduced a new protocol with the number 3 (and uses it as default), so switch back to a value of 2 which can be read by Python 2.Check the protocolparameter in pickle.dump. Your resulting code will look like this.There is no protocolparameter in pickle.load because pickle can determine the protocol from the file.Pickle uses different protocols to convert your data to a binary stream.You must specify in python 3 a protocol lower than 3 in order to be able to load the data in python 2. You can specify the protocol parameter when invoking pickle.dump.

How to write string literals in python without having to escape them?

kjakeb

[How to write string literals in python without having to escape them?](https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them)

Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value? I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!

2011-01-16 02:47:29Z

Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value? I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!Raw string literals:If you're dealing with very large strings, specifically multiline strings, be aware of the triple-quote syntax:There is no such thing. It looks like you want something like "here documents" in Perl and the shells, but Python doesn't have that.Using raw strings or multiline strings only means that there are fewer things to worry about. If you use a raw string then you still have to work around a terminal "\" and with any string solution you'll have to worry about the closing ", ', ''' or """ if it is included in your data.That is, there's no way to have the stringproperly stored in any Python string literal without internal escaping of some sort.You will find Python's string literal documentation here:http://docs.python.org/tutorial/introduction.html#stringsand here:http://docs.python.org/reference/lexical_analysis.html#literalsThe simplest example would be using the 'r' prefix:(Assuming you are not required to input the string from directly within Python code)to get around the Issue Andrew Dalke pointed out, simply type the literal string into a text file and then use this;This will print the literal text of whatever is in the text file, even if it is;Not fun or optimal, but can be useful, especially if you have 3 pages of code that would’ve needed character escaping. if string is a variable, use the .repr method on it:

Fast way to copy dictionary in Python

Joern

[Fast way to copy dictionary in Python](https://stackoverflow.com/questions/5861498/fast-way-to-copy-dictionary-in-python)

I have a Python program that works with dictionaries a lot. I have to make copies of dictionaries thousands of times. I need a copy of both the keys and the associated contents. The copy will be edited and must not be linked to the original (e.g. changes in the copy must not affect the original.)Keys are Strings, Values are Integers (0/1).I currently use a simple way:Profiling my Code shows that the copy operation takes most of the time.Are there faster alternatives to the dict.copy() method? What would be fastest?

2011-05-02 19:25:22Z

I have a Python program that works with dictionaries a lot. I have to make copies of dictionaries thousands of times. I need a copy of both the keys and the associated contents. The copy will be edited and must not be linked to the original (e.g. changes in the copy must not affect the original.)Keys are Strings, Values are Integers (0/1).I currently use a simple way:Profiling my Code shows that the copy operation takes most of the time.Are there faster alternatives to the dict.copy() method? What would be fastest?Looking at the C source for the Python dict operations, you can see that they do a pretty naive (but efficient) copy.  It essentially boils down to a call to PyDict_Merge:This does the quick checks for things like if they're the same object and if they've got objects in them.  After that it does a generous one-time resize/alloc to the target dict and then copies the elements one by one.  I don't see you getting much faster than the built-in copy().Appearantly dict.copy is faster, as you say.Can you provide a code sample so I can see how you are using copy() and in what context?You could use But I dont think it will be faster. I realise this is an old thread, but this is a high result in search engines for "dict copy python", and the top result for "dict copy performance", and I believe this is relevant.From Python 3.7, newDict = oldDict.copy() is up to 5.5x faster than it was previously. Notably, right now, newDict = dict(oldDict) does not seem to have this performance increase.There is a little more information here.Depending on things you leave to speculation, you may want to wrap the original dictionary and do a sort of copy-on-write.The "copy" is then a dictionary which looks up stuff in the "parent" dictionary, if it doesn't already contain the key --- but stuffs modifications in itself.This assumes that you won't be modifying the original and that the extra lookups don't end up costing more.The measurments are dependent on the dictionary size though.  For 10000 entries  copy(d) and d.copy() are almost the same.

How to integrate pep8.py in Eclipse?

David Arcos

[How to integrate pep8.py in Eclipse?](https://stackoverflow.com/questions/399956/how-to-integrate-pep8-py-in-eclipse)

A little background:I run pep8.py manually when I'm scripting, but with bigger projects I prefer to use Eclipse.

It would be really useful to integrate pep8.py in Eclipse/Pydev, so it can be run automatically in all the files in the project, and point to the lines containing the warnings.

Maybe there is an obvious way to do it, but I haven't found it yet.Question is: How to integrate pep8.py in Eclipse?

2008-12-30 10:39:53Z

A little background:I run pep8.py manually when I'm scripting, but with bigger projects I prefer to use Eclipse.

It would be really useful to integrate pep8.py in Eclipse/Pydev, so it can be run automatically in all the files in the project, and point to the lines containing the warnings.

Maybe there is an obvious way to do it, but I haven't found it yet.Question is: How to integrate pep8.py in Eclipse?As of PyDev 2.3.0, pep8 is integrated in PyDev by default, even shipping with a default version of it.Open Window > PreferencesIt must be enabled in PyDev > Editor > Code Analysis > pep8.pyErrors/Warnings should be shown as markers (as other things in the regular code analysis).In the event a file is not analyzed, see https://stackoverflow.com/a/31001619/832230.I don't know how to integrate it for whole project, but I have used it as an external tool to analyze an individual file.Note that the pycodestyle package is the official replacement for and is the newer version of the pep8 package. To install it, run:Next, in Eclipse:Go to Common tab and confirm that the Allocate Console checkbox is checked.A benefit of this approach is that you can use a very up-to-date version of the package, and are not limited to the old version included with PyDev. And if you are curious about setting up pylint in a similar manner, see this answer.That's it. Your Eclipse IDE is now integrated with PEP8.

To run pep8.py automatically, right click on your project editor. Choose PyDev and click "code analysis". In your problems tab in your workspace, you will see warnings that points to the line that you have made a violation in the PEP8 (if you have violated).CODE ANALYSIS :In Eclipse (PyDev), if you want to code analysis using pep8 style thenGo to:Windows -> Preferences -> PyDev -> Editor -> Code Analysis -> pep8.py tab and select Warning click Apply and OK button.In your python code if you validate pep8 coding style it will give you warningAUTO CODE FORMATTING :In Eclipse (PyDev), if you want to Auto Format python code using pep8 style thenGo to:Windows -> Preferences -> PyDev -> Editor -> Code Style -> Code Formatter -> click on check-box (Use autopep8.py for console formatting?) click Apply and OK button.If you want to increase length of line(pep8 default is 79) below Use autopep8.py you can set parameter type --max-line-length=150 if you set max length to 150If press auto-format shortcut ( Ctrl + Shift + f ) it will automatically format your python code like pep8 style That does not yet appear to be fully integrated into Pydev.As suggested in this post,  A request does exist for just that, but it seems to be still open 1 year after its creation...You don't :) Instead you take advantage of very good integration with PyLint and configure PyLint to check all things PEP8 checks. See How to configure PyLint to check all things PEP8 checks?

In Python, how to display current time in readable format

ensnare

[In Python, how to display current time in readable format](https://stackoverflow.com/questions/3961581/in-python-how-to-display-current-time-in-readable-format)

How can I display the current time as:in Python.  Thanks.

2010-10-18 17:19:06Z

How can I display the current time as:in Python.  Thanks.First the quick and dirty way, and second the precise way (recognizing daylight's savings or not).All you need is in the documentation.You could do something like:The full doc on the % codes are at http://docs.python.org/library/time.htmlThis may come in handy: http://strftime.org/Take a look at the facilities provided by http://docs.python.org/library/time.htmlYou have several conversion functions there.Edit: see datetime (http://docs.python.org/library/datetime.html#module-datetime) also for more OOP-like solutions. The time library linked above is kinda imperative.By this code, you'll get your live time zone.

Strings in a DataFrame, but dtype is object

Xiphias

[Strings in a DataFrame, but dtype is object](https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object)

Why does Pandas tell me that I have objects, although every item in the selected column is a string — even after explicit conversion.This is my DataFrame:Five of them are dtype object. I explicitly convert those objects to strings:Then, df["attr2"] still has dtype object, although type(df["attr2"].ix[0] reveals str, which is correct.Pandas distinguishes between int64 and float64 and object. What is the logic behind it when there is no dtype str? Why is a str covered by object?

2014-01-09 11:16:23Z

Why does Pandas tell me that I have objects, although every item in the selected column is a string — even after explicit conversion.This is my DataFrame:Five of them are dtype object. I explicitly convert those objects to strings:Then, df["attr2"] still has dtype object, although type(df["attr2"].ix[0] reveals str, which is correct.Pandas distinguishes between int64 and float64 and object. What is the logic behind it when there is no dtype str? Why is a str covered by object?The dtype object comes from NumPy, it describes the type of element in a ndarray. Every element in a ndarray must has the same size in byte. For int64 and float64, they are 8 bytes. But for strings, the length of the string is not fixed. So instead of save the bytes of strings in the ndarray directly, Pandas use object ndarray, which save pointers to objects, because of this the dtype of this kind ndarray is object.Here is an example:The accepted answer is good. Just wanted to provide an answer which referenced the documentation. The documentation says:As the leading comment says "Don't worry about it; it's supposed to be like this." (Although the accepted answer did a great job explaining the "why"; strings are variable-length)As of version 1.0.0 (January 2020), pandas has introduced as an experimental feature providing first-class support for string types through pandas.StringDtype.While you'll still be seeing object by default, the new type can be used by specifying a dtype of pd.StringDtype or simply 'string':

How to frame two for loops in list comprehension python

Shiva Krishna Bavandla

[How to frame two for loops in list comprehension python](https://stackoverflow.com/questions/18551458/how-to-frame-two-for-loops-in-list-comprehension-python)

I have two lists as belowI want to extract entries from entries when they are in tags:How can I write the two loops as a single line list comprehension?

2013-08-31 18:25:52Z

I have two lists as belowI want to extract entries from entries when they are in tags:How can I write the two loops as a single line list comprehension?This should do it:The best way to remember this is that the order of for loop inside the list comprehension is based on the order in which they appear in traditional loop approach. Outer most loop comes first, and then the inner loops subsequently.So, the equivalent list comprehension would be:In general, if-else statement comes before the first for loop, and if you have just an if statement, it will come at the end. For e.g, if you would like to add an empty list, if tag is not in entry, you would do it like this:The appropriate LC would beThe order of the loops in the LC is similar to the ones in nested loops, the if statements go to the end and the conditional expressions go in the beginning, something likeSee the Demo - EDIT - Since, you need the result to be flattened, you could use a similar list comprehension and then flatten the results.Adding this together, you could just doYou use a generator expression here instead of a list comprehension. (Perfectly matches the 79 character limit too (without the list call)) Output:

How do I run Selenium in Xvfb?

TIMEX

[How do I run Selenium in Xvfb?](https://stackoverflow.com/questions/6183276/how-do-i-run-selenium-in-xvfb)

I'm on EC2 instance. So there is no GUI.Then I do this:Great, everything should work now, right?When I run my code:I get this:

2011-05-31 05:18:33Z

I'm on EC2 instance. So there is no GUI.Then I do this:Great, everything should work now, right?When I run my code:I get this:open a terminal and run this command  xhost +. This commands needs to be run every time you restart your machine. If everything works fine may be you can add this to startup commandsAlso make sure in your /etc/environment file there is a line And then, run your tests to see if your issue is resolved.All please note the comment from sardathrion below before using this.You can use PyVirtualDisplay (a Python wrapper for Xvfb) to run headless WebDriver tests.more infoYou can also use xvfbwrapper, which is a similar module (but has no external dependencies):or better yet, use it as a context manager:The easiest way is probably to use xvfb-run:xvfb-run does the whole X authority dance for you, give it a try!This is the setup I use:Before running the tests, execute:And after the tests:The init.d file I use looks like this:If you use Maven, you can use xvfb-maven-plugin to start xvfb before tests, run them using related DISPLAY environment variable, and stop xvfb after all.

How to calculate moving average using NumPy?

goncalopp

[How to calculate moving average using NumPy?](https://stackoverflow.com/questions/14313510/how-to-calculate-moving-average-using-numpy)

There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.My question is two-fold:

2013-01-14 04:59:12Z

There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.My question is two-fold:If you just want a straightforward non-weighted moving average, you can easily implement it with np.cumsum, which may be is faster than FFT based methods:EDIT Corrected an off-by-one wrong indexing spotted by Bean in the code. EDITSo I guess the answer is: it is really easy to implement, and maybe numpy is already a little bloated with specialized functionality.NumPy's lack of a particular domain-specific function is perhaps due to the Core Team's discipline and fidelity to NumPy's prime directive: provide an N-dimensional array type, as well as functions for creating, and indexing those arrays. Like many foundational objectives, this one is not small, and NumPy does it brilliantly.The (much) larger SciPy contains a much larger collection of domain-specific libraries (called subpackages by SciPy devs)--for instance, numerical optimization (optimize), signal processsing (signal), and integral calculus (integrate).My guess is that the function you are after is in at least one of the SciPy subpackages (scipy.signal perhaps); however, i would look first in the collection of SciPy scikits, identify the relevant scikit(s) and look for the function of interest there.Scikits are independently developed packages based on NumPy/SciPy and directed to a particular technical discipline (e.g., scikits-image, scikits-learn, etc.) Several of these were (in particular, the awesome OpenOpt for numerical optimization) were highly regarded, mature projects long before choosing to reside under the relatively new scikits rubric. The Scikits homepage liked to above lists about 30 such scikits, though at least several of those are no longer under active development. Following this advice would lead you to scikits-timeseries; however, that package is no longer under active development; In effect, Pandas has become, AFAIK, the de facto NumPy-based time series library.Pandas has several functions that can be used to calculate a moving average; the simplest of these is probably rolling_mean, which you use like so:Now, just call the function rolling_mean passing in the Series object and a window size, which in my example below is 10 days.verify that it worked--e.g., compared values 10 - 15 in the original series versus the new Series smoothed with rolling meanThe function rolling_mean, along with about a dozen or so other function are informally grouped in the Pandas documentation under the rubric moving window functions; a second, related group of functions in Pandas is referred to as exponentially-weighted functions (e.g., ewma, which calculates exponentially moving weighted average). The fact that this second group is not included in the first (moving window functions) is perhaps because the exponentially-weighted transforms don't rely on a fixed-length windowA simple way to achieve this is by using np.convolve.

The idea behind this is to leverage the way the discrete convolution is computed and use it to return a rolling mean. This can be done by convolving with a sequence of np.ones of a length equal to the sliding window length we want.In order to do so we could define the following function:This function will be taking the convolution of the sequence x and a sequence of ones of length w. Note that the chosen mode is valid so that the convolution product is only given for points where the sequences overlap completely. Use caseSome examples:For a moving average with a window of length 2 we would have:And for a window of length 4: Details Lets have a more in depth look at the way the discrete convolution is being computed.

The following function aims to replicate the way np.convolve is computing the output values:Which, for the same example above would also yield:So what is being done at each step is to take the inner product between the array of ones and the current  window. In this case the multiplication by np.ones(w) is superfluous given that we are directly taking the sum of the sequence.Bellow is an example of how the first outputs are computed so that it is a little clearer. Lets suppose we want a window of w=4:And the following output would be computed as:And so on, returning a moving average of the sequence once all overlaps have been performed.This answer using Pandas is adapted from above, as rolling_mean is not part of Pandas anymoreNow, just call the function rolling on the dataframe with a window size, which in my example below is 10 days.Here are a variety of ways to do this, along with some benchmarks. The best methods are versions using optimized code from other libraries. The bottleneck.move_mean method is probably best all around. The scipy.convolve approach is also very fast, extensible, and syntactically and conceptually simple, but doesn't scale well for very large window values. The numpy.cumsum method is good if you need a pure numpy approach.Note: Some of these (e.g. bottleneck.move_mean) are not centered, and will shift your data.Timing, Small window (n=3)Timing, Large window (n=1001)Memory, Small window (n=3)Memory, Large window (n=1001)I feel this can be easily solved using bottleneckSee basic sample below:This gives move mean along each axis.The good part is Bottleneck helps to deal with nan values and it's also very efficient.In case you want to take care the edge conditions carefully (compute mean only from available elements at edges), the following function will do the trick. Try this piece of code. I think it's simpler and does the job.

lookback is the window of the moving average.In the Data[i-lookback:i, 0].sum() I have put 0 to refer to the first column of the dataset but you can put any column you like in case you have more than one column.I actually wanted a slightly different behavior than the accepted answer. I was building a moving average feature extractor for an sklearn pipeline, so I required that the output of the moving average have the same dimension as the input. What I want is for the moving average to assume the series stays constant, ie a moving average of [1,2,3,4,5] with window 2 would give [1.5,2.5,3.5,4.5,5.0]. For column vectors (my use case) we getAnd for arraysOf course, one doesn't have to assume constant values for the padding, but doing so should be adequate in most cases.talib contains a simple moving average tool, as well as other similar averaging tools (i.e. exponential moving average). Below compares the method to some of the other solutions.One caveat is that the real must have elements of dtype = float. Otherwise the following error is raisedHere is a fast implementation using numba (mind the types). Note it does contain nans where shifted.I use either the accepted answer's solution, slightly modified to have same length for output as input, or pandas' version as mentioned in a comment of another answer. I summarize both here with a reproducible example for future reference:

How to split a text into sentences?

Artyom

[How to split a text into sentences?](https://stackoverflow.com/questions/4576077/how-to-split-a-text-into-sentences)

I have a text file. I need to get a list of sentences. How can this be implemented? There are a lot of subtleties, such as a dot being used in abbreviations.My old regexp works badly:

2011-01-01 22:18:59Z

I have a text file. I need to get a list of sentences. How can this be implemented? There are a lot of subtleties, such as a dot being used in abbreviations.My old regexp works badly:The Natural Language Toolkit (nltk.org) has what you need.  This group posting indicates this does it:(I haven't tried it!) This function can split the entire text of Huckleberry Finn into sentences in about 0.1 seconds and handles many of the more painful edge cases that make sentence parsing non-trivial e.g. "Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst."Instead of using regex for spliting the text into sentences, you can also use nltk library.ref: https://stackoverflow.com/a/9474645/2877052Here is a middle of the road approach that doesn't rely on any external libraries.  I use list comprehension to exclude overlaps between abbreviations and terminators as well as to exclude overlaps between variations on terminations, for example: '.' vs. '."'I used Karl's find_all function from this entry:

Find all occurrences of a substring in PythonYou can try using Spacy instead of regex. I use it and it does the job.For simple cases (where sentences are terminated normally), this should work:The regex is *\. +, which matches a period surrounded by 0 or more spaces to the left and 1 or more to the right (to prevent something like the period in re.split being counted as a change in sentence).Obviously, not the most robust solution, but it'll do fine in most cases. The only case this won't cover is abbreviations (perhaps run through the list of sentences and check that each string in sentences starts with a capital letter?)You can also use sentence tokenization function in NLTK:@Artyom,Hi! You could make a new tokenizer for Russian (and some other languages) using this function:and then call it in this way:Good luck,

Marilena.No doubt that NLTK is the most suitable for the purpose. But getting started with NLTK is quite painful (But once you install it - you just reap the rewards)So here is simple re based code available at http://pythonicprose.blogspot.com/2009/09/python-split-paragraph-into-sentences.htmlI had to read subtitles files and split them into sentences. After pre-processing (like removing time information etc in the .srt files), the variable fullFile contained the full text of the subtitle file. The below crude way neatly split them into sentences. Probably I was lucky that the sentences always ended (correctly) with a space. Try this first and if it has any exceptions, add more checks and balances.Oh! well. I now realize that since my content was Spanish, I did not have the issues of dealing with "Mr. Smith" etc. Still, if someone wants a quick and dirty parser...

Iterate through pairs of items in a Python list [duplicate]

mrvn

[Iterate through pairs of items in a Python list [duplicate]](https://stackoverflow.com/questions/5764782/iterate-through-pairs-of-items-in-a-python-list)

Is it possible to iterate a list in the following way in Python (treat this code as pseudocode)? And it should produce

2011-04-23 14:35:09Z

Is it possible to iterate a list in the following way in Python (treat this code as pseudocode)? And it should produceFrom itertools receipes:You can zip the list with itself sans the first element:This works even if your list has no elements or only 1 element (in which case zip returns an empty iterable and the code in the for loop never executes). It doesn't work on generators, only sequences (tuple, list, str, etc).To do that you should do:Nearly verbatim from Iterate over pairs in a list (circular fashion) in Python:

How can I remove non-ASCII characters but leave periods and spaces using Python?

jterrace

[How can I remove non-ASCII characters but leave periods and spaces using Python?](https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python)

I'm working with a .txt file. I want a string of the text from the file with no non-ASCII characters. However, I want to leave spaces and periods. At present, I'm stripping those too. Here's the code:How should I modify onlyascii() to leave spaces and periods? I imagine it's not too complicated but I can't figure it out.

2011-12-31 18:23:44Z

I'm working with a .txt file. I want a string of the text from the file with no non-ASCII characters. However, I want to leave spaces and periods. At present, I'm stripping those too. Here's the code:How should I modify onlyascii() to leave spaces and periods? I imagine it's not too complicated but I can't figure it out.You can filter all characters from the string that are not printable using string.printable, like this:string.printable on my machine contains:EDIT: On Python 3, filter will return an iterable. The correct way to obtain a string back would be:An easy way to change to a different codec, is by using encode() or decode(). In your case, you want to convert to ASCII and ignore all symbols that are not supported. For example, the Swedish letter å is not an ASCII character:Edit:Python3: str -> bytes -> strPython2: unicode -> str -> unicodePython2: str -> unicode -> str  (decode and encode in reverse order)According to @artfulrobot, this should be faster than filter and lambda:See more examples here  http://stackoverflow.com/questions/20078816/replace-non-ascii-characters-with-a-single-space/20079244#20079244Your question is ambiguous; the first two sentences taken together imply that you believe that space and "period" are non-ASCII characters. This is incorrect. All chars such that ord(char) <= 127 are ASCII characters. For example, your function excludes these characters !"#$%&\'()*+,-./ but includes several others e.g. []{}.Please step back, think a bit, and edit your question to tell us what you are trying to do, without mentioning the word ASCII, and why you think that chars such that ord(char) >= 128 are ignorable. Also: which version of Python? What is the encoding of your input data?Please note that your code reads the whole input file as a single string, and your comment ("great solution") to another answer implies that you don't care about newlines in your data. If your file contains two lines like this:the result would be 'this is line 1this is line 2' ... is that what you really want?A greater solution would include:If you want printable ascii characters you probably should correct your code to:this is equivalent, to string.printable (answer from @jterrace), except for the absence of returns and tabs ('\t','\n','\x0b','\x0c' and '\r') but doesnt correspond to the range on your questionYou may use the following code to remove non-English letters:This will returnWorking my way through Fluent Python (Ramalho) - highly recommended.

List comprehension one-ish-liners inspired by Chapter 2:

Doing something before program exit

RacecaR

[Doing something before program exit](https://stackoverflow.com/questions/3850261/doing-something-before-program-exit)

How can you have a function or something that will be executed before your program quits? I have a script that will be constantly running in the background, and I need it to save some data to a file before it exits. Is there a standard way of doing this?

2010-10-03 15:02:30Z

How can you have a function or something that will be executed before your program quits? I have a script that will be constantly running in the background, and I need it to save some data to a file before it exits. Is there a standard way of doing this?Check out the atexit module:http://docs.python.org/library/atexit.htmlFor example, if I wanted to print a message when my application was terminating:Just be aware that this works great for normal termination of the script, but it won't get called in all cases (e.g. fatal internal errors).If you want something to always run, even on errors, use try: finally: like this -If you want to also handle exceptions you can insert an except: before the finally:If you stop the script by raising a KeyboardInterrupt (e.g. by pressing Ctrl-C), you can catch that just as a standard exception. You can also catch SystemExit in the same way.I mention this just so that you know about it; the 'right' way to do this is the atexit module mentioned above.

URL-parameters and logic in Django class-based views (TemplateView)

Ngenator

[URL-parameters and logic in Django class-based views (TemplateView)](https://stackoverflow.com/questions/15754122/url-parameters-and-logic-in-django-class-based-views-templateview)

It is unclear to me how it is best to access URL-parameters in class-based-views in Django 1.5. Consider the following:View:URLCONF:I want to access the year parameter in my view, so I can do logic like:How would one best access the url parameter in CBVs like the above that is subclassed of TemplateView and where should one ideally place the logic like this, eg. in a method?

2013-04-02 00:26:28Z

It is unclear to me how it is best to access URL-parameters in class-based-views in Django 1.5. Consider the following:View:URLCONF:I want to access the year parameter in my view, so I can do logic like:How would one best access the url parameter in CBVs like the above that is subclassed of TemplateView and where should one ideally place the logic like this, eg. in a method?To access the url parameters in class based views, use self.args or self.kwargs so you would access it by doing self.kwargs['year']In case you pass URL parameter like this:You can access it in class based view by using self.request.GET (its not presented in self.args nor in self.kwargs):I found this elegant solution, and for django 1.5 or higher, as pointed out here:  In your views.py:The dispatch solution found in this question.

As the view is already passed within Template context, you don't really need to worry about it. In your template file yearly.html, it is possible to access those view attributes simply by:You can keep your urlconf as it is.It's worth mentioning that getting information into your template’s context overwrites the get_context_data(), so it is somehow breaking the django's action bean flow.So far I've only been able to access these url parameters from within the get_queryset method, although I've only tried it with a ListView not a TemplateView. I'll use the url param to create an attribute on the object instance, then use that attribute in get_context_data to populate the context:How about just use Python decorators to make this intelligible:

Check if object is file-like in Python

dmeister

[Check if object is file-like in Python](https://stackoverflow.com/questions/1661262/check-if-object-is-file-like-in-python)

File-like objects are objects in Python that behave like a real file, e.g. have a read() and a write method(), but have a different implementation. It is and realization of the Duck Typing concept.It is considered good practice to allow a file-like object everywhere where a file is expected so that e.g. a StringIO or a Socket object can be used instead a real file. So it is bad to perform a check like this:What is the best way to check if an object (e.g. a parameter of a method) is "file-like"?

2009-11-02 13:13:57Z

File-like objects are objects in Python that behave like a real file, e.g. have a read() and a write method(), but have a different implementation. It is and realization of the Duck Typing concept.It is considered good practice to allow a file-like object everywhere where a file is expected so that e.g. a StringIO or a Socket object can be used instead a real file. So it is bad to perform a check like this:What is the best way to check if an object (e.g. a parameter of a method) is "file-like"?It is generally not good practice to have checks like this in your code at all unless you have special requirements. In Python the typing is dynamic, why do you feel need to check whether the object is file like, rather than just using it as if it was a file and handling the resulting error? Any check you can do is going to happen at runtime anyway so doing something like if not hasattr(fp, 'read') and raising some exception provides little more utility than just calling fp.read() and handling the resulting attribute error if the method does not exist.For 3.1+, one of the following:For 2.x, "file-like object" is too vague a thing to check for, but the documentation for whatever function(s) you're dealing with will hopefully tell you what they actually need; if not, read the code.As other answers point out, the first thing to ask is what exactly you're checking for. Usually, EAFP is sufficient, and more idiomatic.The glossary says "file-like object" is a synonym for "file object", which ultimately means it's an instance of one of the three abstract base classes defined in the io module, which are themselves all subclasses of IOBase. So, the way to check is exactly as shown above.(However, checking IOBase isn't very useful. Can you imagine a case where you need to distinguish an actual file-like read(size) from some one-argument function named read that isn't file-like, without also needing to distinguish between text files and raw binary files? So, really, you almost always want to check, e.g., "is a text file object", not "is a file-like object".)For 2.x, while the io module has existed since 2.6+, built-in file objects are not instances of io classes, neither are any of the file-like objects in the stdlib, and neither are most third-party file-like objects you're likely to encounter. There was no official definition of what "file-like object" means; it's just "something like a builtin file object", and different functions mean different things by "like". Such functions should document what they mean; if they don't, you have to look at the code.However, the most common meanings are "has read(size)", "has read()", or "is an iterable of strings", but some old libraries may expect readline instead of one of those, some libraries like to close() files you give them, some will expect that if fileno is present then other functionality is available, etc. And similarly for write(buf) (although there are a lot fewer options in that direction).As others have said you should generally avoid such checks. One exception is when the object might legitimately be different types and you want different behaviour depending on the type. The EAFP method doesn't always work here as an object could look like more than one type of duck!For example an initialiser could take a file, string or instance of its own class. You might then have code like:Using EAFP here could cause all sorts of subtle problems as each initialisation path gets partially run before throwing an exception.

Essentially this construction mimics function overloading and so isn't very Pythonic, but it can be useful if used with care.As a side note, you can't do the file check in the same way in Python 3. You'll need something like isinstance(f, io.IOBase) instead.The dominant paradigm here is EAFP: easier to ask forgiveness than permission. Go ahead and use the file interface, then handle the resulting exception, or let them propagate to the caller.It's often useful to raise an error by checking a condition, when that error normally wouldn't be raised until much later on. This is especially true for the boundary between 'user-land' and 'api' code.You wouldn't place a metal detector at a police station on the exit door, you would place it at the entrance!  If not checking a condition means an error might occur that could have been caught 100 lines earlier, or in a super-class instead of being raised in the subclass then I say there is nothing wrong with checking.Checking for proper types also makes sense when you are accepting more than one type.

It's better to raise an exception that says "I require a subclass of basestring, OR file" than just raising an exception because some variable doesn't have a 'seek' method...This doesn't mean you go crazy and do this everywhere, for the most part I agree with the concept of exceptions raising themselves, but if you can make your API drastically clear, or avoid unnecessary code execution because a simple condition has not been met do so!You can try and call the method then catch the exception:If you only want a read and a write method you could do this:If I were you I would go with the try/except method.Under most circumstances, the best way to handle this is not to.  If a method takes a file-like object, and it turns out the object it's passed isn't, the exception that gets raised when the method tries to use the object is not any less informative than any exception you might have raised explicitly.There's at least one case where you might want to do this kind of check, though, and that's when the object's not being immediately used by what you've passed it to, e.g. if it's being set in a class's constructor.  In that case, I would think that the principle of EAFP is trumped by the principle of "fail fast."  I'd check the object to make sure it implemented the methods that my class needs (and that they're methods), e.g.:I ended up running into your question when I was writing an open-like function that could accept a file name, file descriptor or pre-opened file-like object.Rather than testing for a read method, as the other answers suggest, I ended up checking if the object can be opened. If it can, it's a string or descriptor, and I have a valid file-like object in hand from the result. If open raises a TypeError, then the object is already a file.

Use logging print the output of pprint

yee379

[Use logging print the output of pprint](https://stackoverflow.com/questions/11093236/use-logging-print-the-output-of-pprint)

I want to use pprint's output to show a complex data structure, but I would like to output it using the logging module rather than stdout.

2012-06-19 01:25:13Z

I want to use pprint's output to show a complex data structure, but I would like to output it using the logging module rather than stdout.Use pprint.pformat to get a string, and then send it to your logging framework.The solution above didn't quite cut it for me because I'm also using a formatter to add name and levelname when logging. It looks a little untidy:There may be a more elegant solution, but this:produces something a little nicer:

Large, persistent DataFrame in pandas

Zelazny7

[Large, persistent DataFrame in pandas](https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas)

I am exploring switching to python and pandas as a long-time SAS user.  However, when running some tests today, I was surprised that python ran out of memory when trying to pandas.read_csv() a 128mb csv file.  It had about 200,000 rows and 200 columns of mostly numeric data.With SAS, I can import a csv file into a SAS dataset and it can be as large as my hard drive. Is there something analogous in pandas? I regularly work with large files and do not have access to a distributed computing network.

2012-07-24 00:50:49Z

I am exploring switching to python and pandas as a long-time SAS user.  However, when running some tests today, I was surprised that python ran out of memory when trying to pandas.read_csv() a 128mb csv file.  It had about 200,000 rows and 200 columns of mostly numeric data.With SAS, I can import a csv file into a SAS dataset and it can be as large as my hard drive. Is there something analogous in pandas? I regularly work with large files and do not have access to a distributed computing network.In principle it shouldn't run out of memory, but there are currently memory problems with read_csv on large files caused by some complex Python internal issues (this is vague but it's been known for a long time: http://github.com/pydata/pandas/issues/407). At the moment there isn't a perfect solution (here's a tedious one: you could transcribe the file row-by-row into a pre-allocated NumPy array or memory-mapped file--np.mmap), but it's one I'll be working on in the near future. Another solution is to read the file in smaller pieces (use iterator=True, chunksize=1000) then concatenate then with pd.concat. The problem comes in when you pull the entire text file into memory in one big slurp.Wes is of course right! I'm just chiming in to provide a little more complete example code. I had the same issue with a 129 Mb file, which was solved by:This is an older thread, but I just wanted to dump my workaround solution here. I initially tried the chunksize parameter (even with quite small values like 10000), but it didn't help much; had still technical issues with the memory size (my CSV was ~ 7.5 Gb). Right now, I just read chunks of the CSV files in a for-loop approach and add them e.g., to an SQLite database step by step:Below is my working flow. Base on your file size, you'd better optimized the chunksize.  After have all data in Database, You can query out those you need from database. If you want to load huge csv files, dask might be a good option. It mimics the pandas api, so it feels quite similar to pandaslink to dask on githubYou can use Pytable rather than pandas df.

It is designed for large data sets and the file format is in hdf5.

So the processing time is relatively fast.

Numpy array assignment with copy

mrgloom

[Numpy array assignment with copy](https://stackoverflow.com/questions/19676538/numpy-array-assignment-with-copy)

For example, if we have a numpy array A, and we want a numpy array B with the same elements.What is the difference between the following (see below) methods? When is additional memory allocated, and when is it not?

2013-10-30 07:44:25Z

For example, if we have a numpy array A, and we want a numpy array B with the same elements.What is the difference between the following (see below) methods? When is additional memory allocated, and when is it not?All three versions do different things:the last two need additional memory.To make a deep copy you need to use B = copy.deepcopy(A)This is the only working answer for me:

How can I filter lines on load in Pandas read_csv function?

benjaminwilson

[How can I filter lines on load in Pandas read_csv function?](https://stackoverflow.com/questions/13651117/how-can-i-filter-lines-on-load-in-pandas-read-csv-function)

How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in read_csv.  Am I missing something?Example: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant.

2012-11-30 18:38:41Z

How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in read_csv.  Am I missing something?Example: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant.There isn't an option to filter the rows before the CSV file is loaded into a pandas object. You can either load the file and then filter using df[df['field'] > constant], or if you have a very large file and you are worried about memory running out, then use an iterator and apply the filter as you concatenate chunks of your file e.g.:You can vary the chunksize to suit your available memory. See here for more details.I didn't find a straight-forward way to do it within context of read_csv. However, read_csv returns a DataFrame, which can be filtered by selecting rows by boolean vector df[bool_vec]:This is selecting all rows in df (assuming df is any DataFrame, such as the result of a read_csv call, that at least contains a datetime column timestamp) for which the values in the timestamp column are greater than the value of targettime. Similar question.If the filtered range is contiguous (as it usually is with time(stamp) filters), then the fastest solution is to hard-code the range of rows. Simply combine skiprows=range(1, start_row) with nrows=end_row parameters. Then the import takes seconds where the accepted solution would take minutes. A few experiments with the initial start_row are not a huge cost given the savings on import times. Notice we kept header row by using range(1,..).If you are on linux you can use grep.You can specify nrows parameter.

import pandas as pd

df = pd.read_csv('file.csv', nrows=100)

This code works well in version 0.20.3.

Should I use a class or dictionary?

deamon

[Should I use a class or dictionary?](https://stackoverflow.com/questions/4045161/should-i-use-a-class-or-dictionary)

I have a class that contains only fields and no methods, like this:This could easily be translated to a dict. The class is more flexible for future additions and could be fast with __slots__. So would there be a benefit of using a dict instead? Would a dict be faster than a class? And faster than a class with slots?

2010-10-28 16:44:35Z

I have a class that contains only fields and no methods, like this:This could easily be translated to a dict. The class is more flexible for future additions and could be fast with __slots__. So would there be a benefit of using a dict instead? Would a dict be faster than a class? And faster than a class with slots?Why would you make this a dictionary? What's the advantage? What happens if you later want to add some code? Where would your __init__ code go?Classes are for bundling related data (and usually code).Dictionaries are for storing key-value relationships, where usually the keys are all of the same type, and all the values are also of one type. Occasionally they can be useful for bundling data when the key/attribute names are not all known up front, but often this a sign that something's wrong with your design.Keep this a class.Use a dictionary unless you need the extra mechanism of a class. You could also use a namedtuple for a hybrid approach:Performance differences here will be minimal, although I would be surprised if the dictionary wasn't faster.A class in python is a dict underneath.  You do get some overhead with the class behavior, but you won't be able to notice it without a profiler.  In this case, I believe you benefit from the class because:I think that the usage of each one is way too subjective for me to get in on that, so i'll just stick to numbers.I compared the time it takes to create and to change a variable in a dict, a new_style class and a new_style class with slots.Here's the code i used to test it(it's a bit messy but it does the job.)And here is the output...So, if you're just storing variables, you need speed, and it won't require you to do many calculations, i recommend using a dict(you could always just make a function that looks like a method). But, if you really need classes, remember - always use __slots__.I tested the 'Class' with both new_style and old_style classes. It turns out that old_style classes are faster to create but slower to modify(not by much but significant if you're creating lots of classes in a tight loop (tip: you're doing it wrong)).Also the times for creating and changing variables may differ on your computer since mine is old and slow. Make sure you test it yourself to see the 'real' results.I later tested the namedtuple: i can't modify it but to create the 10000 samples (or something like that) it took 1.4 seconds so the dictionary is indeed the fastest.If i change the dict function to include the keys and values and to return the dict instead of the variable containing the dict when i create it it gives me 0.65 instead of 0.8 seconds.Creating is like a class with slots and changing the variable is the slowest (0.17 seconds) so do not use these classes. go for a dict (speed) or for the class derived from object ('syntax candy')I agree with @adw. I would never represent an "object" (in an OO sense) with a dictionary. Dictionaries aggregate name/value pairs. Classes represent objects. I've seen code where the objects are represented with dictionaries and it's unclear what the actual shape of the thing is. What happens when certain name/values aren't there? What restricts the client from putting anything at all in. Or trying to get anything at all out. The shape of the thing should always be clearly defined. When using Python it is important to build with discipline as the language allows many ways for the author to shoot him/herself in the foot.I would recommend a class, as it is all sorts of information involved with a request. Were one to use a dictionary, I'd expect the data stored to be far more similar in nature. A guideline I tend to follow myself is that if I may want to loop over the entire set of key->value pairs and do something, I use a dictionary. Otherwise, the data apparently has far more structure than a basic key->value mapping, meaning a class would likely be a better alternative.Hence, stick with the class.If all that you want to achive is syntax candy like obj.bla = 5 instead of obj['bla'] = 5, especially if you have to repeat that a lot, you maybe want to use some plain container class as in martineaus suggestion. Nevertheless, the code there is quite bloated and unnecessarily slow. You can keep it simple like that:Another reason to switch to namedtuples or a class with __slots__ could be memory usage. Dicts require significantly more memory than list types, so this could be a point to think about. Anyways, in your specific case, there doesn't seem to be any motivation to switch away from your current implementation. You don't seem to maintain millions of these objects, so no list-derived-types required. And it's actually containing some functional logic within the __init__, so you also shouldn't got with AttrDict.It may be possible to have your cake and eat it, too. In other words you can create something that provides the functionality of both a class and dictionary instance. See the ActiveState's Dɪᴄᴛɪᴏɴᴀʀʏ ᴡɪᴛʜ ᴀᴛᴛʀɪʙᴜᴛᴇ-sᴛʏʟᴇ ᴀᴄᴄᴇss  recipe and comments on ways of doing that.If you decide to use a regular class rather than a subclass, I've found the Tʜᴇ sɪᴍᴘʟᴇ ʙᴜᴛ ʜᴀɴᴅʏ "ᴄᴏʟʟᴇᴄᴛᴏʀ ᴏғ ᴀ ʙᴜɴᴄʜ ᴏғ ɴᴀᴍᴇᴅ sᴛᴜғғ" ᴄʟᴀss recipe (by Alex Martelli) to be very flexible and useful for the sort of thing it looks like you're doing (i.e. create a relative simple aggregator of information). Since it's a class you can easily extend its functionality further by adding methods.Lastly it should be noted that the names of class members must be legal Python identifiers, but dictionary keys do not—so a dictionary would provide greater freedom in that regard because keys can be anything hashable (even something that's not a string).UpdateA class object (which doesn't have a __dict__) subclass named SimpleNamespace (which does have one) was added to the types module Python 3.3, and is yet another alternative.I recommend a class. If you use a class, you can get type hint as shown. And Class support auto complete when class is argument of function.

How to access outer class from an inner class?

T. Stone

[How to access outer class from an inner class?](https://stackoverflow.com/questions/2024566/how-to-access-outer-class-from-an-inner-class)

I have a situation like so...How can I access the Outer class's method from the Inner class?

2010-01-07 23:49:05Z

I have a situation like so...How can I access the Outer class's method from the Inner class?The methods of a nested class cannot directly access the instance attributes of the outer class. Note that it is not necessarily the case that an instance of the outer class exists even when you have created an instance of the inner class.In fact, it is often recommended against using nested classes, since the nesting does not imply any particular relationship between the inner and outer classes.You're trying to access Outer's class instance, from inner class instance. So just use factory-method to build Inner instance and pass Outer instance to it.maybe I'm mad but this seems very easy indeed - the thing is to make your inner class inside a method of the outer class... Plus... "self" is only used by convention, so you could do this:It might be objected that you can't then create this inner class from outside the outer class... but this ain't true:then, somewhere miles away:even push the boat out a bit and extend this inner class (NB to get super() to work you have to change the class signature of mooble to "class mooble( object )"latermrh1997 raised an interesting point about the non-common inheritance of inner classes delivered using this technique. But it seems that the solution is pretty straightforward:Do you mean to use inheritance, rather than nesting classes like this? What you're doing doesn't make a heap of sense in Python.You can access the Outer's some_method by just referencing Outer.some_method within the inner class's methods, but it's not going to work as you expect it will. For example, if you try this:...you'll get a TypeError when initialising an Inner object, because Outer.some_method expects to receive an Outer instance as its first argument. (In the example above, you're basically trying to call some_method as a class method of Outer.)I've created some Python code to use an outer class from its inner class, based on a good idea from another answer for this question. I think it's short, simple and easy to understand.The main code, "production ready" (without comments, etc.). Remember to replace all of each value in angle brackets (e.g. <x>) with the desired value.

Making the code to assign the sub classes to the higher level class easier to copy and be used in classes derived from the higher level class that have their __init__ function changed:Insert before line 12 in the main code:

Then insert into this function lines 5-6 (of the main code) and replace lines 4-7 with the following code:

Making subclass assigning to the higher level class possible when there are many/unknown quantities of subclasses.Replace line 6 with the following code:

A class, named "a" (class a:) is created. It has subclasses that need to access it (the parent). One subclass is called "x1". In this subclass, the code a.run_func() is run.Then another class, named "b" is created, derived from class "a" (class b(a):). After that, some code runs b.x1() (calling the sub function "x1" of b, a derived sub-class). This function runs a.run_func(), calling the function "run_func" of class "a", not the function "run_func" of its parent, "b" (as it should), because the function which was defined in class "a" is set to refer to the function of class "a", as that was its parent.This would cause problems (e.g. if function a.run_func has been deleted) and the only solution without rewriting the code in class a.x1 would be to redefine the sub-class x1 with updated code for all classes derived from class "a" which would obviously be difficult and not worth it.Another possibility:You can easily access to outer class using metaclass: after creation of outer class check it's attribute dict for any classes (or apply any logic you need - mine is just trivial example) and set corresponding values:Using this approach, you can easily bind and refer two classes between each other.I found this.Tweaked to suite your question:I’m sure you can somehow write a decorator for this or somethingrelated: What is the purpose of python's inner classes?Expanding on @tsnorri's cogent thinking, that the outer method may be a static method:Now the line in question should work by the time it is actually called.The last line in the above code gives the Inner class a static method that's a clone of the Outer static method.  This takes advantage of two Python features, that functions are objects, and scope is textual....or current class in our case. So objects "local" to the definition of the Outer class (Inner and some_static_method) may be referred to directly within that definition.

How exactly does a generator comprehension work?

NONEenglisher

[How exactly does a generator comprehension work?](https://stackoverflow.com/questions/364802/how-exactly-does-a-generator-comprehension-work)

What does generator comprehension do? How does it work? I couldn't find a tutorial about it.

2008-12-13 03:55:44Z

What does generator comprehension do? How does it work? I couldn't find a tutorial about it.Do you understand list comprehensions? If so, a generator expression is like a list comprehension, but instead of finding all the items you're interested and packing them into list, it waits, and yields each item out of the expression, one by one.Because a generator expression only has to yield one item at a time, it can lead to big savings in memory usage. Generator expressions make the most sense in scenarios where you need to take one item at a time, do a lot of calculations based on that item, and then move on to the next item. If you need more than one value, you can also use a generator expression and grab a few at a time. If you need all the values before your program proceeds, use a list comprehension instead.A generator comprehension is the lazy version of a list comprehension.It is just like a list comprehension except that it returns an iterator instead of the list ie an object with a next() method that will yield the next element.If you are not familiar with list comprehensions see here and for generators see here.List/generator comprehension is a construct which you can use to create a new list/generator from an existing one.Let's say you want to generate the list of squares of each number from 1 to 10. You can do this in Python:here, range(1,11) generates the list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], but the range function is not a generator before Python 3.0, and therefore the construct I've used is a list comprehension.If I wanted to create a generator that does the same thing, I could do it like this:In Python 3, however, range is a generator, so the outcome depends only on the syntax you use (square brackets or round brackets).Generator comprehension is an easy way of creating generators with a certain structure. Lets say you want a generator that outputs one by one all the even numbers in your_list. If you create it by using the function style it would be like this:You could achieve the same result with this generator comprehension expression:In both cases, when you call next(evens) you get the next even number in your_list. Generator comprehension is an approach to create iterables, something like a cursor which moves on a resource.  If you know mysql cursor or mongodb cursor, you may be aware of that the whole actual data never gets loaded into the memory at once, but one at a time. Your cursor moves back and forth, but there is always a one row/list element in memory. In short, by using generators comprehension you can easily create cursors in python.Another example of Generator comprehension:

Post-install script with Python setuptools

Chris Simpkins

[Post-install script with Python setuptools](https://stackoverflow.com/questions/20288711/post-install-script-with-python-setuptools)

Is it possible to specify a post-install Python script file as part of the setuptools setup.py file so that a user can run the command:on a local project file archive, orfor a PyPI project and the script will be run at the completion of the standard setuptools install?  I am looking to perform post-install tasks that can be coded in a single Python script file (e.g. deliver a custom post-install message to the user, pull additional data files from a different remote source repository).I came across this SO answer from several years ago that addresses the topic and it sounds as though the consensus at that time was that you need to create an install subcommand.  If that is still the case, would it be possible for someone to provide an example of how to do this so that it is not necessary for the user to enter a second command to run the script?

2013-11-29 15:11:59Z

Is it possible to specify a post-install Python script file as part of the setuptools setup.py file so that a user can run the command:on a local project file archive, orfor a PyPI project and the script will be run at the completion of the standard setuptools install?  I am looking to perform post-install tasks that can be coded in a single Python script file (e.g. deliver a custom post-install message to the user, pull additional data files from a different remote source repository).I came across this SO answer from several years ago that addresses the topic and it sounds as though the consensus at that time was that you need to create an install subcommand.  If that is still the case, would it be possible for someone to provide an example of how to do this so that it is not necessary for the user to enter a second command to run the script?Note: The solution below only works when installing a source distribution zip or tarball, or installing in editable mode from a source tree. It will not work when installing from a binary wheel (.whl)This solution is more transparent:You will make a few additions to setup.py and there is no need for an extra file. Also you need to consider two different post-installations; one for development/editable mode and the other one for install mode.Add these two classes that includes your post-install script to setup.py:and insert cmdclass argument to setup() function in setup.py:You can even call shell commands during installation, like in this example which does pre-installation preparation:P.S. there are no any pre-install entry points available on setuptools. Read this discussion if you are wondering why there is none.Note: The solution below only works when installing a source distribution zip or tarball, or installing in editable mode from a source tree. It will not work when installing from a binary wheel (.whl)This is the only strategy that has worked for me when the post-install script requires that the package dependencies have already been installed:Note: The solution below only works when installing a source distribution zip or tarball, or installing in editable mode from a source tree. It will not work when installing from a binary wheel (.whl)A solution could be to include a post_setup.py in setup.py's directory. post_setup.py will contain a function which does the post-install and setup.py will only import and launch it at the appropriate time.In setup.py:In post_setup.py:With the common idea of launching setup.py from its directory, you will be able to import post_setup.py else it will launch an empty function. In post_setup.py, the if __name__ == '__main__': statement allows you to manually launch post-install from command line.I think the easiest way to perform the post-install, and keep the requirements, is to decorate the call to setup(...):This will run setup() when declaring setup. Once done with the requirements installation, it will run the _post_install() function, which will run the inner function _post_actions().Combining the answers from @Apalala, @Zulu and @mertyildiran; this worked for me in a Python 3.5 environment:This also gives you access the to the installation path of the package in install_path, to do some shell work on.If using atexit, there is no need to create a new cmdclass. You can simply create your atexit register right before the setup() call. It does the same thing.Also, if you need dependencies to be installed first, this does not work with pip install since your atexit handler will be called before pip moves the packages into place.I wasn't able to solve a problem with any presented recommendations, so here is what helped me.You can call function, that you want to run after installation just after setup() in setup.py, like that:

How do you determine which backend is being used by matplotlib?

Matthew Rankin

[How do you determine which backend is being used by matplotlib?](https://stackoverflow.com/questions/3580027/how-do-you-determine-which-backend-is-being-used-by-matplotlib)

Either interactively, such as from within an Ipython session, or from within a script, how can you determine which backend is being used by matplotlib?

2010-08-26 22:23:13Z

Either interactively, such as from within an Ipython session, or from within a script, how can you determine which backend is being used by matplotlib?Use the get_backend() function to obtain a string denoting which backend is in use:Another way to determine the current backend is to read rcParams dictionary:

Pandas selecting by label sometimes return Series, sometimes returns DataFrame

jobevers

[Pandas selecting by label sometimes return Series, sometimes returns DataFrame](https://stackoverflow.com/questions/20383647/pandas-selecting-by-label-sometimes-return-series-sometimes-returns-dataframe)

In Pandas, when I select a label that only has one entry in the index I get back a Series, but when I select an entry that has more then one entry I get back a data frame.Why is that?  Is there a way to ensure I always get back a data frame?

2013-12-04 19:01:40Z

In Pandas, when I select a label that only has one entry in the index I get back a Series, but when I select an entry that has more then one entry I get back a data frame.Why is that?  Is there a way to ensure I always get back a data frame?Granted that the behavior is inconsistent, but I think it's easy to imagine cases where this is convenient. Anyway, to get a DataFrame every time, just pass a list to loc. There are other ways, but in my opinion this is the cleanest.You have an index with three index items 3. For this reason df.loc[3] will return a dataframe.The reason is that you don't specify the column. So df.loc[3] selects three items of all columns (which is column 0), while df.loc[3,0] will return a Series. E.g. df.loc[1:2] also returns a dataframe, because you slice the rows.Selecting a single row (as df.loc[1]) returns a Series with the column names as the index.  If you want to be sure to always have a DataFrame, you can slice like df.loc[1:1]. Another option is boolean indexing (df.loc[df.index==1]) or the take method (df.take([0]), but this used location not labels!).Use df['columnName'] to get a Series and df[['columnName']] to get a Dataframe.   You wrote in a comment to joris' answer:A single row isn't converted in a Series.

It IS a Series: No, I don't think so, in fact; see the editThe data model of Pandas objects has been choosen like that. The reason certainly lies in the fact that it ensures some advantages I don't know (I don't fully understand the last sentence of the citation, maybe it's the reason).A DataFrame can't be composed of elements that would be Series, because the following code gives the same type "Series" as well for a row as for a column:resultSo, there is no sense to pretend that a DataFrame is composed of Series because what would these said Series be supposed to be : columns or rows ? Stupid question and vision..Then what is a DataFrame ?In the previous version of this answer, I asked this question, trying to find the answer to the Why is that? part of the question of the OP and the similar interrogation single rows to get converted into a series - why not a data frame with one row? in one of his comment,

while the Is there a way to ensure I always get back a data frame? part has been answered by Dan Allan.  Then, as the Pandas' docs cited above says that the pandas' data structures are best seen as containers of lower dimensional data, it seemed to me that the understanding of the why would be found in the characteristcs of the nature of DataFrame structures.However, I realized that this cited advice must not be taken as a precise description of the nature of Pandas' data structures.

This advice doesn't mean that a DataFrame is a container of Series.

It expresses that the mental representation of a DataFrame as a container of Series (either rows or columns according the option considered at one moment of a reasoning) is a good way to consider DataFrames, even if it isn't strictly the case in reality. "Good" meaning that this vision enables to use DataFrames with efficiency. That's all..Then what is a DataFrame object ?The DataFrame class produces instances that have a particular structure originated in the NDFrame base class, itself derived from the  PandasContainer base class that is also a parent class of the Series class.

Note that this is correct for Pandas until version 0.12. In the upcoming version 0.13, Series will derive also from NDFrame class only.resultSo my understanding is now that a DataFrame instance has certain methods that have been crafted in order to control the way data are extracted from rows and columns.    The ways these extracting methods work are described in this page:

http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing

We find in it the method given by Dan Allan and other methods.   Why these extracting methods have been crafted as they were ?

That's certainly because they have been appraised as the ones giving the better possibilities and ease in data analysis.

It's precisely what is expressed in this sentence:The why of the extraction of data from a DataFRame instance doesn't lies in its structure, it lies in the why of this structure. I guess that the structure and functionning of the Pandas' data structure have been chiseled in order to be as much intellectually intuitive as possible, and that to understand the details, one must read the blog of Wes McKinney. df.loc[:] = Dataframedf.loc[int] = Dataframe if you have more than one column and Series if you have only 1 column in the dataframedf.loc[:, ["col_name"]] = Dataframedf.loc[:, "col_name"] = Seriesdf["col_name"] = Seriesdf[["col_name"]] = DataframeIf the objective is to get a subset of the data set using the index, it is best to avoid using loc or iloc. Instead you should use syntax similar to this : If you also select on the index of the dataframe then the result can be either a DataFrame or a Series or it can be a Series or a scalar (single value).This function ensures that you always get a list from your selection (if the df, index and column are valid):

Does Python support multithreading? Can it speed up execution time?

Karim Bahgat

[Does Python support multithreading? Can it speed up execution time?](https://stackoverflow.com/questions/20939299/does-python-support-multithreading-can-it-speed-up-execution-time)

I'm slightly confused about whether multithreading works in Python or not. I know there has been a lot of questions about this and I've read many of them, but I'm still confused. I know from my own experience and have seen others post their own answers and examples here on StackOverflow that multithreading is indeed possible in Python. So why is it that everyone keep saying that Python is locked by the GIL and that only one thread can run at a time? It clearly does work. Or is there some distinction I'm not getting here? Many posters/respondents also keep mentioning that threading is limited because it does not make use of multiple cores. But I would say they are still useful because they do work simultaneously and thus get the combined workload done faster. I mean why would there even be a Python thread module otherwise?Update:Thanks for all the answers so far. The way I understand it is that multithreading will only run in parallel for some IO tasks, but can only run one at a time for CPU-bound multiple core tasks.I'm not entirely sure what this means for me in practical terms, so I'll just give an example of the kind of task I'd like to multithread. For instance, let's say I want to loop through a very long list of strings and I want to do some basic string operations on each list item. If I split up the list, send each sublist to be processed by my loop/string code in a new thread, and send the results back in a queue, will these workloads run roughly at the same time? Most importantly will this theoretically speed up the time it takes to run the script? Another example might be if I can render and save four different pictures using PIL in four different threads, and have this be faster than processing the pictures one by one after each other? I guess this speed-component is what I'm really wondering about rather than what the correct terminology is.I also know about the multiprocessing module but my main interest right now is for small-to-medium task loads (10-30 secs) and so I think multithreading will be more appropriate because subprocesses can be slow to initiate. 

2014-01-05 21:17:12Z

I'm slightly confused about whether multithreading works in Python or not. I know there has been a lot of questions about this and I've read many of them, but I'm still confused. I know from my own experience and have seen others post their own answers and examples here on StackOverflow that multithreading is indeed possible in Python. So why is it that everyone keep saying that Python is locked by the GIL and that only one thread can run at a time? It clearly does work. Or is there some distinction I'm not getting here? Many posters/respondents also keep mentioning that threading is limited because it does not make use of multiple cores. But I would say they are still useful because they do work simultaneously and thus get the combined workload done faster. I mean why would there even be a Python thread module otherwise?Update:Thanks for all the answers so far. The way I understand it is that multithreading will only run in parallel for some IO tasks, but can only run one at a time for CPU-bound multiple core tasks.I'm not entirely sure what this means for me in practical terms, so I'll just give an example of the kind of task I'd like to multithread. For instance, let's say I want to loop through a very long list of strings and I want to do some basic string operations on each list item. If I split up the list, send each sublist to be processed by my loop/string code in a new thread, and send the results back in a queue, will these workloads run roughly at the same time? Most importantly will this theoretically speed up the time it takes to run the script? Another example might be if I can render and save four different pictures using PIL in four different threads, and have this be faster than processing the pictures one by one after each other? I guess this speed-component is what I'm really wondering about rather than what the correct terminology is.I also know about the multiprocessing module but my main interest right now is for small-to-medium task loads (10-30 secs) and so I think multithreading will be more appropriate because subprocesses can be slow to initiate. The GIL does not prevent threading. All the GIL does is make sure only one thread is executing Python code at a time; control still switches between threads.What the GIL prevents then, is making use of more than one CPU core or separate CPUs to run threads in parallel.This only applies to Python code. C extensions can and do release the GIL to allow multiple threads of C code and one Python thread to run across multiple cores. This extends to I/O controlled by the kernel, such as select() calls for socket reads and writes, making Python handle network events reasonably efficiently in a multi-threaded multi-core setup.What many server deployments then do, is run more than one Python process, to let the OS handle the scheduling between processes to utilize your CPU cores to the max. You can also use the multiprocessing library to handle parallel processing across multiple processes from one codebase and parent process, if that suits your use cases.Note that the GIL is only applicable to the CPython implementation; Jython and IronPython use a different threading implementation (the native Java VM and .NET common runtime threads respectively).To address your update directly: Any task that tries to get a speed boost from parallel execution, using pure Python code, will not see a speed-up as threaded Python code is locked to one thread executing at a time. If you mix in C extensions and I/O, however (such as PIL or numpy operations) and any C code can run in parallel with one active Python thread.Python threading is great for creating a responsive GUI, or for handling multiple short web requests where I/O is the bottleneck more than the Python code. It is not suitable for parallelizing computationally intensive Python code, stick to the multiprocessing module for such tasks or delegate to a dedicated external library.Yes. :)You have the low level thread module and the higher level threading module. But it you simply want to use multicore machines, the multiprocessing module is the way to go.Quote from the docs:Threading is Allowed in Python, the only problem is that the GIL will make sure that just one thread is executed at a time (no parallelism).So basically if you want to multi-thread the code to speed up calculation it won't speed it up as just one thread is executed at a time, but if you use it to interact with a database for example it will. 

Understanding timedelta

Paul

[Understanding timedelta](https://stackoverflow.com/questions/6749294/understanding-timedelta)

Given the python code below, please help me understand what is happening there.So I get the difference between start time and end time, on line 5 I round up the duration by casting and what now, what's the further explanation?I know what delta means(average or difference), but why do I have to pass seconds = uptime to timedelta and why does the string casting works so nicely that I get HH:MM:SS ?

2011-07-19 14:59:05Z

Given the python code below, please help me understand what is happening there.So I get the difference between start time and end time, on line 5 I round up the duration by casting and what now, what's the further explanation?I know what delta means(average or difference), but why do I have to pass seconds = uptime to timedelta and why does the string casting works so nicely that I get HH:MM:SS ?Because timedelta is defined like:All arguments are optional and default to 0.You can easily say "Three days and four milliseconds" with optional arguments that way.And for str casting, it returns a nice formatted value instead of __repr__ to improve readability. From docs:Checkout documentation:http://docs.python.org/library/datetime.html#timedelta-objectsBecause timedelta objects can be passed seconds, milliseconds, days, etc... so you need to specify what are you passing in (this is why you use the explicit key). Typecasting to int is superfluous as they could also accept floats.It's not the typecasting that formats, is the internal __str__ method of the object. In fact you will achieve the same result if you write:

Checking if sys.argv[x] is defined

Cmag

[Checking if sys.argv[x] is defined](https://stackoverflow.com/questions/5423381/checking-if-sys-argvx-is-defined)

What would be the best way to check if a variable was passed along for the script:

2011-03-24 17:55:37Z

What would be the best way to check if a variable was passed along for the script:In the end, the difference between try, except and testing len(sys.argv) isn't all that significant. They're both a bit hackish compared to argparse.This occurs to me, though -- as a sort of low-budget argparse:You could even use it to generate a namedtuple with values that default to None -- all in four lines!In case you're not familiar with namedtuple, it's just a tuple that acts like an object, allowing you to access its values using tup.attribute syntax instead of tup[0] syntax. So the first line creates a new namedtuple type with values for each of the values in arg_names. The second line passes the values from the args dictionary, using get to return a default value when the given argument name doesn't have an associated value in the dictionary.Check the length of sys.argv:Some people prefer the exception-based approach you've suggested (eg, try: blah = sys.argv[1]; except IndexError: blah = 'blah'), but I don't like it as much because it doesn't「scale」nearly as nicely (eg, when you want to accept two or three arguments) and it can potentially hide errors (eg, if you used blah = foo(sys.argv[1]), but foo(...) raised an IndexError, that IndexError would be ignored).Another way I haven't seen listed yet is to set your sentinel value ahead of time. This method takes advantage of Python's lazy evaluation, in which you don't always have to provide an else statement.  Example:Or if you're going syntax CRAZY you could use Python's ternary operator:It's an ordinary Python list. The exception that you would catch for this is IndexError, but you're better off just checking the length instead.I use this - it never fails:A solution working with map built-in fonction !Then you just have to call your parameters like this:You can simply append the value of argv[1] to argv  and then check if argv[1] doesn't equal the string you inputted Example:Pretty close to what the originator was trying to do. Here is a function I use:So a usage would be something like:

How do I use vi keys in ipython under *nix?

gak

[How do I use vi keys in ipython under *nix?](https://stackoverflow.com/questions/10394302/how-do-i-use-vi-keys-in-ipython-under-nix)

Currently in Bash I use set -o vi to enable vi mode in my bash prompt.How do I get this going in ipython?Note: If an answer applies to all *nix, I'll remove the OS X from the title :)

2012-05-01 06:21:34Z

Currently in Bash I use set -o vi to enable vi mode in my bash prompt.How do I get this going in ipython?Note: If an answer applies to all *nix, I'll remove the OS X from the title :)In case someone's wandering in here recently, IPython 5.0 switched from readline to prompt_toolkit, so an updated answer to this question is to pass an option:... or to set it globally in the profile configuration (~/.ipython/profile_default/ipython_config.py; create it with ipython profile create if you don't have it) with:Looks like a solution works for many other readline compatible apps:Set the following in your ~/.inputrc file:Source: http://www.jukie.net/bart/blog/20040326082602You can also interactively switch between Vi-mode and Emacs mode. According to the the readline docs to switch between them you are supposed to be able to use the M-C-j key combination but that only seems to allow me to switch to vi-mode - on my Mac (where ESC is used as the 'Meta' key) it is: ESC+CTRL+j. To switch back to Emacs mode one can use C-e but that didn't appear to work for me - I had to instead do M-C-e - on my Mac it is: ESC+CTRL+e.FYI my ~/.inputrc is set up as follows:ipython uses the readline library and this is configurable using the ~/.inputrc file. You can add to that file to make all readline based applications use vi style keybindings instead of Emacs. I needed to be able to switch modes interactively in IPython 5 and I found you can do so by recreating the prompt manager on the fly:You may set vi in your .ipython start-up config file. Create one if you don't have it by adding a file to ~/.ipython/profile_default/startup/ called something like start.py.  Here's an example:   That last line is if you use ipython with Django, and want to import all your models by default.   

URL query parameters to dict python

Leonardo Andrade

[URL query parameters to dict python](https://stackoverflow.com/questions/21584545/url-query-parameters-to-dict-python)

Is there a way to parse a URL (with some python library) and return a python dictionary with the keys and values of a query parameters part of the URL?For example:expected return:

2014-02-05 17:43:42Z

Is there a way to parse a URL (with some python library) and return a python dictionary with the keys and values of a query parameters part of the URL?For example:expected return:Use the urllib.parse library:The urllib.parse.parse_qs() and urllib.parse.parse_qsl() methods parse out query strings, taking into account that keys can occur more than once and that order may matter.If you are still on Python 2, urllib.parse was called urlparse.For Python 3, the values of the dict from parse_qs are in a list, because there might be multiple values. If you just want the first one:If you prefer not to use a parser:So I won't delete what's above but it's definitely not what you should use.I think I read a few of the answers and they looked a little complicated, incase you're like me, don't use my solution.Use this:and for Python 2.XI know this is the same as the accepted answer, just in a one liner that can be copied.For python 2.7I agree about not reinventing the wheel but sometimes (while you're learning) it helps to build a wheel in order to understand a wheel. :) So, from a purely academic perspective, I offer this with the caveat that using a dictionary assumes that name value pairs are unique (that the query string does not contain multiple records).I'm using version 3.6.5 in the Idle IDE.For python2.7 I am using urlparse module to parse url query to dict. 

Python os.path.join on Windows

Frank E.

[Python os.path.join on Windows](https://stackoverflow.com/questions/2422798/python-os-path-join-on-windows)

I am trying to learn python and am making a program that will output a script.  I want to use os.path.join, but am pretty confused.  According to the docs if I say:I get "C:sourcedir".  According to the docs, this is normal, right?But when I use the copytree command, Python will output it the desired way, for example:Here is the error code I get:If I wrap the os.path.join with os.path.normpath I get the same error.If this os.path.join can't be used this way, then I am confused as to its purpose.According to the pages suggested by Stack Overflow, slashes should not be used in join—that is correct, I assume?

2010-03-11 05:38:19Z

I am trying to learn python and am making a program that will output a script.  I want to use os.path.join, but am pretty confused.  According to the docs if I say:I get "C:sourcedir".  According to the docs, this is normal, right?But when I use the copytree command, Python will output it the desired way, for example:Here is the error code I get:If I wrap the os.path.join with os.path.normpath I get the same error.If this os.path.join can't be used this way, then I am confused as to its purpose.According to the pages suggested by Stack Overflow, slashes should not be used in join—that is correct, I assume?Windows has a concept of current directory for each drive.  Because of that, "c:sourcedir" means "sourcedir" inside the current C: directory, and you'll need to specify an absolute directory.Any of these should work and give the same result, but I don't have a Windows VM fired up at the moment to double check:To be even more pedantic, the most python doc consistent answer would be:Since you also need os.sep for the posix root path:The reason os.path.join('C:', 'src') is not working as you expect is because of something in the documentation that you linked to:As ghostdog said, you probably want mypath=os.path.join('c:\\', 'sourcedir')To be pedantic, it's probably not good to hardcode either / or \ as the path separator.  Maybe this would be best?or For a system-agnostic solution that works on both Windows and Linux, no matter what the input path, one could use os.path.join(os.sep, rootdir + os.sep, targetdir) On WIndows:On Linux:I'd say this is a (windows)python bug.Why bug? I think this statement should be TrueBut it is False on windows machines.to join a windows path, trybasically, you will need to escape the slashYou have a few possible approaches to treat path on Windows, from the most hardcoded ones (as using raw string literals or escaping backslashes) to the least ones. Here follows a few examples that will work as expected. Use what better fits your needs.Consent with @georg-I would say then why we need lame os.path.join- better to use str.join or unicode.join e.g.answering to your comment : "the others '//'  'c:', 'c:\\' did not work (C:\\ created two backslashes, C:\ didn't work at all)"On windows using 

os.path.join('c:', 'sourcedir')

will automatically add two backslashes \\ in front of sourcedir.To resolve the path, as python works on windows also with forward slashes -> '/', simply add .replace('\\','/') with os.path.join as below:-os.path.join('c:\\', 'sourcedir').replace('\\','/')e.g: os.path.join('c:\\', 'temp').replace('\\','/')output : 'C:/temp'The proposed solutions are interesting and offer a good reference, however they are only partially satisfying. It is ok to manually add the separator when you have a single specific case or you know the format of the input string, but there can be cases where you want to do it programmatically on generic inputs.With a bit of experimenting, I believe the criteria is that the path delimiter is not added if the first segment is a drive letter, meaning a single letter followed by a colon, no matter if it corresponds to a real unit.For example:A convenient way to test for the criteria and apply a path correction can be to use os.path.splitdrive comparing the first returned element to the test value, like t+os.path.sep if os.path.splitdrive(t)[0]==t else t.Test:it can be probably be improved to be more robust for trailing spaces, and I have tested it only on windows, but I hope it gives an idea.

See also Os.path : can you explain this behavior? for interesting details on systems other then windows.

Convert RGBA PNG to RGB with PIL

Danilo Bargen

[Convert RGBA PNG to RGB with PIL](https://stackoverflow.com/questions/9166400/convert-rgba-png-to-rgb-with-pil)

I'm using PIL to convert a transparent PNG image uploaded with Django to a JPG file. The output looks broken.orBoth ways, the resulting image looks like this:Is there a way to fix this? I'd like to have white background where the transparent background used to be.Thanks to the great answers, I've come up with the following function collection:The simple non-compositing alpha_to_color function is the fastest solution, but leaves behind ugly borders because it does not handle semi transparent areas.Both the pure PIL and the numpy compositing solutions give great results, but alpha_composite_with_color is much faster (8.93 msec) than pure_pil_alpha_to_color (79.6 msec). If numpy is available on your system, that's the way to go. (Update: The new pure PIL version is the fastest of all mentioned solutions.)

2012-02-06 19:58:12Z

I'm using PIL to convert a transparent PNG image uploaded with Django to a JPG file. The output looks broken.orBoth ways, the resulting image looks like this:Is there a way to fix this? I'd like to have white background where the transparent background used to be.Thanks to the great answers, I've come up with the following function collection:The simple non-compositing alpha_to_color function is the fastest solution, but leaves behind ugly borders because it does not handle semi transparent areas.Both the pure PIL and the numpy compositing solutions give great results, but alpha_composite_with_color is much faster (8.93 msec) than pure_pil_alpha_to_color (79.6 msec). If numpy is available on your system, that's the way to go. (Update: The new pure PIL version is the fastest of all mentioned solutions.)Here's a version that's much simpler - not sure how performant it is. Heavily based on some django snippet I found while building RGBA -> JPG + BG support for sorl thumbnails. Result @80%Result @ 50%

By using Image.alpha_composite, the solution by Yuji 'Tomita' Tomita become simpler. This code can avoid a tuple index out of range error if png has no alpha channel.The transparent parts mostly have RGBA value (0,0,0,0). Since the JPG has no transparency, the jpeg value is set to (0,0,0), which is black.Around the circular icon, there are pixels with nonzero RGB values where A = 0. So they look transparent in the PNG, but funny-colored in the JPG.You can set all pixels where A == 0 to have R = G = B = 255 using numpy like this:Note that the logo also has some semi-transparent pixels used to smooth the edges around the words and icon. Saving to jpeg ignores the semi-transparency, making the resultant jpeg look quite jagged.A better quality result could be made using imagemagick's convert command:To make a nicer quality blend using numpy, you could use alpha compositing:Here's a solution in pure PIL.It's not broken. It's doing exactly what you told it to; those pixels are black with full transparency. You will need to iterate across all pixels and convert ones with full transparency to white.import Imagedef fig2img ( fig ):

    """

    @brief Convert a Matplotlib figure to a PIL Image in RGBA format and return it

    @param fig a matplotlib figure

    @return a Python Imaging Library ( PIL ) image

    """

    # put the figure pixmap into a numpy array

    buf = fig2data ( fig )

    w, h, d = buf.shape

    return Image.frombytes( "RGBA", ( w ,h ), buf.tostring( ) )def fig2data ( fig ):

    """

    @brief Convert a Matplotlib figure to a 4D numpy array with RGBA channels and return it

    @param fig a matplotlib figure

    @return a numpy 3D array of RGBA values

    """

    # draw the renderer

    fig.canvas.draw ( )def rgba2rgb(img, c=(0, 0, 0), path='foo.jpg', is_already_saved=False, if_load=True):

    if not is_already_saved:

        background = Image.new("RGB", img.size, c)

        background.paste(img, mask=img.split()[3]) # 3 is the alpha channel

asyncio.ensure_future vs. BaseEventLoop.create_task vs. simple coroutine?

crusaderky

[asyncio.ensure_future vs. BaseEventLoop.create_task vs. simple coroutine?](https://stackoverflow.com/questions/36342899/asyncio-ensure-future-vs-baseeventloop-create-task-vs-simple-coroutine)

I've seen several basic Python 3.5 tutorials on asyncio doing the same operation in various flavours.

In this code:All the three variants above that define the futures variable achieve the same result; the only difference I can see is  that with the third variant the execution is out of order (which should not matter in most cases). Is there any other difference? Are there cases where I can't just use the simplest variant (plain list of coroutines)?

2016-03-31 20:15:20Z

I've seen several basic Python 3.5 tutorials on asyncio doing the same operation in various flavours.

In this code:All the three variants above that define the futures variable achieve the same result; the only difference I can see is  that with the third variant the execution is out of order (which should not matter in most cases). Is there any other difference? Are there cases where I can't just use the simplest variant (plain list of coroutines)?Starting from Python 3.7 asyncio.create_task(coro) high-level function was added for this purpose. You should use it instead other ways of creating tasks from coroutimes. However if you need to create task from arbitrary awaitable, you should use asyncio.ensure_future(obj).ensure_future is a method to create Task from coroutine. It creates tasks in different ways based on argument (including using of create_task for coroutines and future-like objects).create_task is an abstract method of AbstractEventLoop. Different event loops can implement this function different ways.You should use ensure_future to create tasks. You'll need create_task only if you're going to implement your own event loop type.Upd:@bj0 pointed at Guido's answer on this topic:and later:It's surprising to me. My main motivation to use ensure_future all along was that it's higher-level function comparing to loop's member create_task (discussion contains some ideas like adding asyncio.spawn or asyncio.create_task).I can also point that in my opinion it's pretty convenient to use universal function that can handle any Awaitable rather than coroutines only.However, Guido's answer is clear: "When creating a task from a coroutine you should use the appropriately-named loop.create_task()"Wrap coroutine in a Task - is a way to start this coroutine "in background". Here's example:Output:You can replace asyncio.ensure_future(long_operation()) with just await long_operation() to feel the difference.As you can see the create_task is more specific.Simple invoking async function returns coroutineAnd since the gather under the hood ensures (ensure_future) that args are futures, explicitly ensure_future is redundant.Similar question What's the difference between loop.create_task, asyncio.async/ensure_future and Task?From the official docs:So now, in Python 3.7 onwards, there are 2 top-level wrapper function (similar but different):Well, utlimately both of these wrapper functions will help you call BaseEventLoop.create_task. The only difference is ensure_future accept any awaitable object and help you convert it into a Future. And also you can provide your own event_loop parameter in ensure_future. And depending if you need those capability or not, you can simply choose which wrapper to use.for your example, all the three types execute asynchronously. the only difference is that, in the third example, you pre-generated all 10 coroutines, and submitted to the loop together. so only the last one gives output randomly.

How to read a (static) file from inside a Python package?

ronszon

[How to read a (static) file from inside a Python package?](https://stackoverflow.com/questions/6028000/how-to-read-a-static-file-from-inside-a-python-package)

Could you tell me how can I read a file that is inside my Python package?A package that I load has a number of templates (text files used as strings) that I want to load from within the program. But how do I specify the path to such file?Imagine I want to read a file from:Some kind of path manipulation? Package base path tracking?

2011-05-17 08:09:45Z

Could you tell me how can I read a file that is inside my Python package?A package that I load has a number of templates (text files used as strings) that I want to load from within the program. But how do I specify the path to such file?Imagine I want to read a file from:Some kind of path manipulation? Package base path tracking?[added 2016-06-15: apparently this doesn't work in all situations. please refer to the other answers]The traditional pkg_resources from setuptools is not recommended anymore because the new method:I kept the traditional listed first, to explain the differences with the new method when porting existing code (porting also explained here).Let's assume your templates are located in a folder nested inside your module's package: You may use pkg_resources package from setuptools distribution, but that comes with a cost, performance-wise:... and notice that according to the Setuptools/pkg_resources docs, you should not use os.path.join:Use the standard library's importlib.resources module which is more efficient than setuptools, above:For the example asked in the question, we must now: In case you have this structureyou need this code:The strange "always use slash" part comes from setuptools APIsIn case you wonder where the documentation is:Before you can even worry about reading resource files, the first step is to make sure that the data files are getting packaged into your distribution in the first place - it is easy to read them directly from the source tree, but the important part is making sure these resource files are accessible from code within an installed package.Structure your project like this, putting data files into a subdirectory within the package:You should pass include_package_data=True in the setup() call. The manifest file is only needed if you want to use setuptools/distutils and build source distributions. To make sure the templates/temp_file gets packaged for this example project structure, add a line like this into the manifest file:Historical cruft note: Using a manifest file is not needed for modern build backends such as flit, poetry, which will include the package data files by default. So, if you're using pyproject.toml and you don't have a setup.py file then you can ignore all the stuff about MANIFEST.in.Now, with packaging out of the way, onto the reading part...Use standard library pkgutil APIs.  It's going to look like this in library code:It works in zips. It works on Python 2 and Python 3. It doesn't require third-party dependencies. I'm not really aware of any downsides (if you are, then please comment on the answer).This is currently the accepted answer. At best, it looks something like this:What's wrong with that? The assumption that you have files and subdirectories available is not correct. This approach doesn't work if executing code which is packed in a zip or a wheel, and it may be entirely out of the user's control whether or not your package gets extracted to a filesystem at all.This is currently the top-voted answer. It looks something like this:What's wrong with that? It adds a runtime dependency on setuptools, which should preferably be an install time dependency only. Importing and using pkg_resources can become really slow, as the code builds up a working set of all installed packages, even though you were only interested in your own package resources. That's not a big deal at install time (since installation is once-off), but it's ugly at runtime.This is a recent standard library addition (new in Python 3.7), but there is a backport available too. It looks like this:What's wrong with that? Well, unfortunately, it doesn't work...yet. This is still an incomplete API, using importlib.resources will require you to add an empty file templates/__init__.py in order that the data files will reside within a sub-package rather than in a subdirectory. It will also expose the package/templates subdirectory as an importable package.templates sub-package in its own right. If that's not a big deal and it doesn't bother you, then you can go ahead and add the __init__.py file there and use the import system to access resources. However, while you're at it you may as well make it into a my_resources.py file instead, and just define some bytes or string variables in the module, then import them in Python code. It's the import system doing the heavy lifting here either way.I've created an example project on github and uploaded on PyPI, which demonstrates all four approaches discussed above. Try it out with:See https://github.com/wimglenn/resources-example for more info.The content in "10.8. Reading Datafiles Within a Package" of Python Cookbook, Third Edition by David Beazley and Brian K. Jones giving the answers. I'll just get it to here:Suppose you have a package with files organized as follows:Now suppose the file spam.py wants to read the contents of the file somedata.dat. To do

it, use the following code:The resulting variable data will be a byte string containing the raw contents of the file.The first argument to get_data() is a string containing the package name. You can

either supply it directly or use a special variable, such as __package__. The second

argument is the relative name of the file within the package. If necessary, you can navigate

into different directories using standard Unix filename conventions as long as the

final directory is still located within the package.In this way, the package can installed as directory, .zip or .egg.Every python module in your package has a __file__ attributeYou can use it as:For egg resources see: http://peak.telecommunity.com/DevCenter/PythonEggs#accessing-package-resourcesassuming you are using an egg file; not extracted:I "solved" this in a recent project, by using a postinstall script, that extracts my templates from the egg (zip file) to the proper directory in the filesystem. It was the quickest, most reliable solution I found, since working with __path__[0] can go wrong sometimes (i don't recall the name, but i cam across at least one library, that added something in front of that list!).Also egg files are usually extracted on the fly to a temporary location called the "egg cache". You can change that location using an environment variable, either before starting your script or even later, eg.However there is pkg_resources that might do the job properly.

Passing a list of kwargs?

jwanga

[Passing a list of kwargs?](https://stackoverflow.com/questions/1496346/passing-a-list-of-kwargs)

Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:

2009-09-30 06:08:33Z

Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:Yes. You do it like this:Running this in Python confirms these produce identical results:As others have pointed out, you can do what you want by passing a dict. There are various ways to construct a dict. One that preserves the keyword=value style you attempted is to use the dict built-in:Note the versatility of dict; all of these produce the same result:Do you mean a dict?  Sure you can:So when I've come here I was looking for a way to pass several **kwargs in one function - for later use in further functions. Because this, not that surprisingly, doesn't work:With some own 'experimental' coding I came to the obviously way how to do it:This prints as expected:

Getting vertical gridlines to appear in line plot in matplotlib

Osmond Bishop

[Getting vertical gridlines to appear in line plot in matplotlib](https://stackoverflow.com/questions/16074392/getting-vertical-gridlines-to-appear-in-line-plot-in-matplotlib)

I want to get both horizontal and vertical grid lines on my plot but only the horizontal grid lines are appearing by default. I am using a pandas.DataFrame from an sql query in python to generate a line plot with dates on the x-axis. I'm not sure why they do not appear on the dates and I have tried to search for an answer to this but couldn't find one.All I have used to plot the graph is the simple code below. data is the DataFrame which contains the dates and the data from the sql query. I have also tried adding the code below but I still get the same output with no vertical grid lines.Any suggestions?

2013-04-18 04:17:10Z

I want to get both horizontal and vertical grid lines on my plot but only the horizontal grid lines are appearing by default. I am using a pandas.DataFrame from an sql query in python to generate a line plot with dates on the x-axis. I'm not sure why they do not appear on the dates and I have tried to search for an answer to this but couldn't find one.All I have used to plot the graph is the simple code below. data is the DataFrame which contains the dates and the data from the sql query. I have also tried adding the code below but I still get the same output with no vertical grid lines.Any suggestions?You may need to give boolean arg in your calls, e.g. use ax.yaxis.grid(True) instead of ax.yaxis.grid().  Additionally, since you are using both of them you can combine into ax.grid, which works on both, rather than doing it once for each dimension.That should sort you out.plt.gca().xaxis.grid(True) proved to be the solution for meAccording to matplotlib documentation, The signature of the Axes class grid() method is as follows:So in order to show grid lines for both the x axis and y axis, we can use the the following code:This method gives us finer control over what to show for grid lines.maybe this can solve the problem:

matplotlib, define size of a grid on a plotThe truth is that the grid is working, but there's only one v-grid in 00:00 and no grid in others. I meet the same problem that there's only one grid in Nov 1 among many days.For only horizontal linesThis worked

How to create Python egg file

yart

[How to create Python egg file](https://stackoverflow.com/questions/2026395/how-to-create-python-egg-file)

I have questions about egg files in Python.I have much Python code organized by package and I'm trying to create egg files.

I'm following instructions, but they are very common.According to that, it seems I need to have a setup.py file.

2010-01-08 08:46:51Z

I have questions about egg files in Python.I have much Python code organized by package and I'm trying to create egg files.

I'm following instructions, but they are very common.According to that, it seems I need to have a setup.py file.You are reading the wrong documentation. You want this: https://setuptools.readthedocs.io/en/latest/setuptools.html#develop-deploy-the-project-source-in-development-modeFor #4, the closest thing to starting java with a jar file for your app is a new feature in Python 2.6, executable zip files and directories.Where myapp.zip is a zip containing a __main__.py file which is executed as the script file to be executed. Your package dependencies can also be included in the file:You can also execute an egg, but the incantation is not as nice: This puts the myapp.egg on the Python path and uses the -m argument to run a module. Your myapp.egg will likely look something like:And python will run __init__.py (you should check that __file__=='__main__' in your app for command line use).Egg files are just zip files so you might be able to add __main__.py to your egg with a zip tool and make it executable in python 2.6 and run it like python myapp.egg instead of the above incantation where the PYTHONPATH environment variable is set.More information on executable zip files including how to make them directly executable with a shebang can be found on Michael Foord's blog post on the subject.I think you should use python wheels for distribution instead of egg now.

Why use Django on Google App Engine?

Travis Bradshaw

[Why use Django on Google App Engine?](https://stackoverflow.com/questions/1934914/why-use-django-on-google-app-engine)

When researching Google App Engine (GAE), it's clear that using Django is wildly popular for developing in Python on GAE.  I've been scouring the web to find information on the costs and benefits of using Django, to find out why it's so popular.  While I've been able to find a wide variety of sources on how to run Django on GAE and the various methods of doing so, I haven't found any comparative analysis on why Django is preferable to using the webapp framework provided by Google.To be clear, it's immediately apparent why using Django on GAE is useful for developers with an existing skillset in Django (a majority of Python web developers, no doubt) or existing code in Django (where using GAE is more of a porting exercise).  My team, however, is evaluating GAE for use on an all-new project and our existing experience is with TurboGears, not Django.It's been quite difficult to determine why Django is beneficial to a development team when the BigTable libraries have replaced Django's ORM, sessions and authentication are necessarily changed, and Django's templating (if desirable) is available without using the entire Django stack.Finally, it's clear that using Django does have the advantage of providing an "exit strategy" if we later wanted to move away from GAE and need a platform to target for the exodus.I'd be extremely appreciative for help in pointing out why using Django is better than using webapp on GAE.  I'm also completely inexperienced with Django, so elaboration on smaller features and/or conveniences that work on GAE are also valuable to me.

2009-12-20 05:03:45Z

When researching Google App Engine (GAE), it's clear that using Django is wildly popular for developing in Python on GAE.  I've been scouring the web to find information on the costs and benefits of using Django, to find out why it's so popular.  While I've been able to find a wide variety of sources on how to run Django on GAE and the various methods of doing so, I haven't found any comparative analysis on why Django is preferable to using the webapp framework provided by Google.To be clear, it's immediately apparent why using Django on GAE is useful for developers with an existing skillset in Django (a majority of Python web developers, no doubt) or existing code in Django (where using GAE is more of a porting exercise).  My team, however, is evaluating GAE for use on an all-new project and our existing experience is with TurboGears, not Django.It's been quite difficult to determine why Django is beneficial to a development team when the BigTable libraries have replaced Django's ORM, sessions and authentication are necessarily changed, and Django's templating (if desirable) is available without using the entire Django stack.Finally, it's clear that using Django does have the advantage of providing an "exit strategy" if we later wanted to move away from GAE and need a platform to target for the exodus.I'd be extremely appreciative for help in pointing out why using Django is better than using webapp on GAE.  I'm also completely inexperienced with Django, so elaboration on smaller features and/or conveniences that work on GAE are also valuable to me.We use django on our appengine instances mostly when we have to serve actual websites to the user. It has a great template engine, url routing and all the request/response/error handling built in. So even while we can't use the magic orm/admin stuff it has a lot going for it.For api services we built something very simple on top of webob. It's far more lightweight because it doesn't need everything that django offers, and therefore a little quicker in some situations.Django probably isn't the right choice for you, if you're sure that GAE is right for you. The strengths of the two technologies don't align very well - you completely lose a lot of Django's wonderful orm on GAE, and if you do use it, you write code that isn't really directly suitable to bigtable and the way GAE works.The thing about GAE is that it gets the great scalability by forcing you to write code that scales easily from the ground up. You just can't do a number of things that scale poorly (of course, you can still write poorly scaling code, but you avoid some pitfalls). The tradeoff is that you really end up coding around the framework, if you use something like Django which is designed for a different environment.If you see yourself ever leaving GAE for any reason, becoming invested in the infrastructure there is a problem for you. Coding for bigtable means that it will be harder to move to a different architecture (though the apache project is working to solve this for you with the HBase component of the Hadoop project). It would still be a lot of work to transition off of GAE.What's the driving motivator behind using GAE, besides being a Google product, and a cool buzzword? Is there a reason that scaling using something like mediatemple's offering is unlikely to work well for you? Are you sure that the ways that GAE scales are right for your application? How does the cost compare to dedicated servers, if you're expecting to get to that performance realm? Can you solve your problem well using the tools GAE provides, as compared to a more traditional load-balanced server setup?All this said, unless you absolutely positively need the borderline-ridiculous scaling that GAE offers, I'd personally suggest not letting that particular service structure your choice of framework. I like Django, so I'd say you should use it, but not on GAE.Edit (June 2010):

As an update to this comment sometime later:

Google has announced sql-like capabilitys for GAE that aren't free, but will let you easily do things like run SQL-style commands to generate reports on your data.Additionally, there are upcoming changes to the GAE query language which will allow complex queries in a far easier fashion. Look at the videos from Google I/O 2010.Furthermore, there is work being done during the Summer of Code 2010 project which should bring no-sql support to django core, and by extension, make working with GAE significantly easier.GAE is becoming more attractive as a hosting platform.Edit (August 2011):And Google just raised the cost to most users of the platform significantly by changing the pricing structure. The lockin problem has gotten better (if your application is big enough you can deploy the apache alternatives), but for most applications, running servers or VPS deployments is cheaper.Very few people really have bigdata problems. "Oh my startup might scale someday" isn't a bigdata problem. Build stuff now and get it out the door using the standard tools.I've done lots of projects on GAE. Some in django, some in their normal framework.For small things, I usually use their normal framework for simplicity and quickness. Like http://stdicon.com, http://yaml-online-parser.appspot.com/, or http://text-twist.appspot.com/. For large things, I go with django to take advantage of all the nice middleware and plugins. Like http://metaward.com. Basically my litmus test is Will this take me more than 2 weeks to write and be a REAL software project? If so, go with django for the addons.It has the added benefit of, if your project is badly suited for BigTable then you quickly port off (like I did Is BigTable slow or am I dumb?)I think all this answers are a bit obsolete.Now you can use Google Cloud SQLhttps://cloud.google.com/python/django/appengineone more fresh news is, that there is BETA support for PostgreSQLI have experience using Django and not GAE.  From my experiences with Django it was a very simplistic setup and the deployment process was incredibly easy in terms of web projects.  Granted I had to learn Python to really get a good hold on things, but at the end of the day I would use it again on a project.  This was almost 2 years ago before it reached 1.0 so I my knowledge is a bit outdated.  If you are worried about changing platforms, then this would be a better choice I suppose.I cannot answer the question but you may want to look into web2py. It is similar to Django in many respects but its database abstraction layer works on GAE and supports most of the GAE functionality (not all but we try to catch up). In this way if GAE works for you great, if it does not, you can move your code to a different db (SQLite, MySQL, PostgreSQL, Oracle, MSSQL, FireBird, DB2, Informix, Ingres, and - soon - Sybase and MongoDB).If you decide to run you app outside of GAE, you can still use Django. You won't really have that much luck with the GAE webappI am still very new to Google App engine development, but the interfaces Django provides do appear much nicer than the default. The benefits will depend on what you are using to run Django on the app engine. The Google App Engine Helper for Django allows you to use the full power of the Google App Engine with some Django functionality on the side.Django non-rel attempts to provide as much of Django's power as possible, but running on the app-engine for possible extra scalability. In particular, it includes Django models (one of Django's core features), but this is a leaky abstraction due to the differences between relational databases and bigtable. There will most likely be tradeoffs in functionality and efficiency, as well as an increased number of bugs and quirks. Of course, this might be worth it in circumstances like those described in the question, but otherwise would strongly recommend using the helper at the start as then you have the option of moving towards either pure app-engine or Django non-rel later. Also, if you do switch to Django non-rel, your increased knowledge of how app engine works will be useful if the Django abstraction ever breaks - certainly much more useful than knowledge of the quirks/workarounds for Django non-rel if you swap the other way.

How to change a module variable from another module?

shino

[How to change a module variable from another module?](https://stackoverflow.com/questions/3536620/how-to-change-a-module-variable-from-another-module)

Suppose I have a package named bar, and it contains bar.py:and __init__.py:Then I execute this script:Here's what I expect:Here's what I get:Can anyone explain my misconception?

2010-08-21 06:34:37Z

Suppose I have a package named bar, and it contains bar.py:and __init__.py:Then I execute this script:Here's what I expect:Here's what I get:Can anyone explain my misconception?You are using from bar import a. a becomes a symbol in the global scope of the importing module (or whatever scope the import statement occurs in). When you assign a new value to a, you are just changing which value a points too, not the actual value. Try to import bar.py directly with import bar in __init__.py and conduct your experiment there by setting bar.a = 1. This way, you will actually be modifying bar.__dict__['a'] which is the 'real' value of a in this context.It's a little convoluted with three layers but bar.a = 1 changes the value of a in the module called bar that is actually derived from __init__.py. It does not change the value of a that foobar sees because foobar lives in the actual file bar.py. You could set bar.bar.a if you wanted to change that.This is one of the dangers of using the from foo import bar form of the import statement: it splits bar into two symbols, one visible globally from within foo which starts off pointing to the original value and a different symbol visible in the scope where the import statement is executed. Changing a where a symbol points doesn't change the value that it pointed too.This sort of stuff is a killer when trying to reload a module from the interactive interpreter.One source of difficulty with this question is that you have a program named bar/bar.py: import bar imports either bar/__init__.py or bar/bar.py, depending on where it is done, which makes it a little cumbersome to track which a is bar.a.Here is how it works:The key to understanding what happens is to realize that in your __init__.py,in effect does something likeand defines a new variable (bar/__init__.py:a, if you wish).  Thus, your from bar import a in __init__.py binds name bar/__init__.py:a to the original bar.py:a object (None).  This is why you can do from bar import a as a2 in __init__.py: in this case, it is clear that you have both bar/bar.py:a and a distinct variable name bar/__init__.py:a2 (in your case, the names of the two variables just happen to both be a, but they still live in different namespaces: in __init__.py, they are bar.a and a).Now, when you doyou are accessing variable bar/__init__.py:a (since import bar imports your bar/__init__.py). This is the variable you modify (to 1). You are not touching the contents of variable bar/bar.py:a. So when you subsequently doyou call bar/bar.py:foobar(), which accesses variable a from bar/bar.py, which is still None (when foobar() is defined, it binds variable names once and for all, so the a in bar.py is bar.py:a, not any other a variable defined in another module—as there might be many a variables in all the imported modules).  Hence the last None output.To put another way:

Turns out this misconception is very easy to make. 

It is sneakily defined in the Python language reference: the use of object instead of symbol. I would suggest that the Python language reference make this more clear and less sparse..HOWEVER:When you import, you import the current value of the imported symbol and add it to your namespace as defined. You are not importing a reference, you are effectively importing a value.Thus, to get the updated value of i, you must import a variable that holds a reference to that symbol.In other words, importing is NOT like an import in JAVA, external declaration in C/C++ or even a use clause in PERL.Rather, the following statement in Python:is more like the following code in K&R C:(caveat: in the Python case, "a" and "x" are essentially a reference to the actual value: you're not copying the INT, you're copying the reference address) 

correct way to use super (argument passing)

cha0site

[correct way to use super (argument passing)](https://stackoverflow.com/questions/8972866/correct-way-to-use-super-argument-passing)

So I was following Python's Super Considered Harmful, and went to test out his examples.However, Example 1-3, which is supposed to show the correct way of calling super when handling __init__ methods that expect different arguments, flat-out doesn't work.This is what I get:It seems that object itself violates one of the best practices mentioned in the document, which is that methods which use super must accept *args and **kwargs.Now, obviously Mr. Knight expected his examples to work, so is this something that was changed in recent versions of Python? I checked 2.6 and 2.7, and it fails on both.So what is the correct way to deal with this problem?

2012-01-23 14:04:35Z

So I was following Python's Super Considered Harmful, and went to test out his examples.However, Example 1-3, which is supposed to show the correct way of calling super when handling __init__ methods that expect different arguments, flat-out doesn't work.This is what I get:It seems that object itself violates one of the best practices mentioned in the document, which is that methods which use super must accept *args and **kwargs.Now, obviously Mr. Knight expected his examples to work, so is this something that was changed in recent versions of Python? I checked 2.6 and 2.7, and it fails on both.So what is the correct way to deal with this problem?Sometimes two classes may have some parameter names in common. In that case, you can't pop the key-value pairs off of **kwargs or remove them from *args. Instead, you can define a Base class which unlike object, absorbs/ignores arguments:yieldsNote that for this to work, Base must be the penultimate class in the MRO.If you're going to have a lot of inheritence (that's the case here) I suggest you to pass all parameters using **kwargs, and then pop them right after you use them (unless you need them in upper classes).This is the simplest way to solve those kind of problems.As explained in Python's super() considered super, one way is to have class eat the arguments it requires, and pass the rest on. Thus, when the call-chain reaches object, all arguments have been eaten, and object.__init__ will be called without arguments (as it expects). So your code should look like this:

How to re-raise an exception in nested try/except blocks?

Tobias Kienzler

[How to re-raise an exception in nested try/except blocks?](https://stackoverflow.com/questions/18188563/how-to-re-raise-an-exception-in-nested-try-except-blocks)

I know that if I want to re-raise an exception, I simple use raise without arguments in the respective except block. But given a nested expression likehow can I re-raise the SomeError without breaking the stack trace? raise alone would in this case re-raise the more recent AlsoFailsError. Or how could I refactor my code to avoid this issue?

2013-08-12 13:42:39Z

I know that if I want to re-raise an exception, I simple use raise without arguments in the respective except block. But given a nested expression likehow can I re-raise the SomeError without breaking the stack trace? raise alone would in this case re-raise the more recent AlsoFailsError. Or how could I refactor my code to avoid this issue?You can store the exception type, value, and traceback in local variables and use the three-argument form of raise:In Python 3 the traceback is stored in the exception, so raise e will do the (mostly) right thing:The only problem with the above is that it will produce a slightly misleading traceback that tells you that SomeError occurred while handling AlsoFailsError (because of raise e inside except AlsoFailsError), where in fact the almost exact opposite occurred - we handled AlsoFailsError while trying to recover from SomeError. To disable this behavior and get a traceback that never mentions AlsoFailsError, replace raise e with raise e from None.Even if the accepted solution is right, it's good to point to the Six library which has a Python 2+3 solution, using six.reraise.So, you can write:As per Drew McGowen's suggestion, but taking care of a general case (where a return value s is present), here's an alternative to user4815162342's answer:Python 3.5+ attaches the traceback information to the error anyway, so it's no longer necessary to save it separately.

Operation on every pair of element in a list

GuiSim

[Operation on every pair of element in a list](https://stackoverflow.com/questions/942543/operation-on-every-pair-of-element-in-a-list)

Using Python, I'd like to compare every possible pair in a list.Suppose I haveI'd like to do an operation (let's call it foo) on every combination of 2 elements from the list.The final result should be the same asMy first thought was to iterate twice through the list manually, but that doesn't seem very pythonic.

2009-06-03 00:21:23Z

Using Python, I'd like to compare every possible pair in a list.Suppose I haveI'd like to do an operation (let's call it foo) on every combination of 2 elements from the list.The final result should be the same asMy first thought was to iterate twice through the list manually, but that doesn't seem very pythonic.Check out product() in the itertools module.  It does exactly what you describe.This is equivalent to:Edit: There are two very similar functions as well, permutations() and combinations().  To illustrate how they differ:product() generates every possible pairing of elements, including all duplicates:permutations() generates all unique orderings of each unique pair of elements, eliminating the x,x duplicates:Finally, combinations() only generates each unique pair of elements, in lexicographic order:All three of these functions were introduced in Python 2.6.I had a similar problem and found the solution here. It works without having to import any module.Supposing a list like:A simplified one-line solution would look like this.All possible pairs, including duplicates:All possible pairs, excluding duplicates:Unique pairs, where order is irrelevant:In case you don't want to operate but just to get the pairs, removing the function foo and using just a tuple would be enough.All possible pairs, including duplicates:Result:All possible pairs, excluding duplicates:Result:Unique pairs, where order is irrelevant:Result:Edit: After the rework to simplify this solution, I realised it is the same approach than Adam Rosenfield. I hope the larger explanation helps some to understand it better.If you're just calling a function, you can't really do much better than:If you want to collect a list of the results of calling the function, you can do:which will return you a list of the result of applying foo(i, j) to each possible pair (i, j).

Python urllib2: Receive JSON response from url

Deepak B

[Python urllib2: Receive JSON response from url](https://stackoverflow.com/questions/13921910/python-urllib2-receive-json-response-from-url)

I am trying to GET a URL using Python and the response is JSON. However, when I runThe html is of type str and I am expecting a JSON. Is there any way I can capture the response as JSON or a python dictionary instead of a str.

2012-12-17 20:46:07Z

I am trying to GET a URL using Python and the response is JSON. However, when I runThe html is of type str and I am expecting a JSON. Is there any way I can capture the response as JSON or a python dictionary instead of a str.If the URL is returning valid JSON-encoded data, use the json library to decode that:urllib, for Python 3.4

HTTPMessage, returned by r.info()Be careful about the validation and etc, but the straight solution is this:Python 3 standard library one-liner:Though I guess it has already answered I would like to add my little bit in thisNote : object passed to json.load() should support .read() , therefore urllib2.urlopen(self.name).read() would not work .

Doamin passed should be provided with protocol in this case http you can also get json by using requests as below:This is another simpler solution to your questionwhere data is the str output from the following codeNone of the provided examples on here worked for me. They were either for Python 2 (uurllib2) or those for Python 3 return the error "ImportError: No module named request". I google the error message and it apparently requires me to install a the module - which is obviously unacceptable for such a simple task. This code worked for me: 

Hide axis values in matplotlib

Luis Ramon Ramirez Rodriguez

[Hide axis values in matplotlib](https://stackoverflow.com/questions/37039685/hide-axis-values-in-matplotlib)

I've this image:And I wan't to hide the numbers, if I use I get this image:It also hide the labels, V and t. How can I keep the labels while hiding the values?

2016-05-04 23:16:46Z

I've this image:And I wan't to hide the numbers, if I use I get this image:It also hide the labels, V and t. How can I keep the labels while hiding the values?If you use the matplotlib object-oriented approach, this is a simple task using ax.set_xticklabels() and ax.set_yticklabels():Without a subplots, you can universally remove the ticks like this:This works great. Just paste this before plt.show():Boom.Not sure this is the best way, but you can certainly replace the tick labels like this:In Python 3.4 this generates a simple line plot with no tick labels on the x-axis.  A simple example is here:

http://matplotlib.org/examples/ticks_and_spines/ticklabels_demo_rotation.htmlThis related question also has some better suggestions:

Hiding axis text in matplotlib plotsI'm new to python.  Your mileage may vary in earlier versions.  Maybe others can help?to remove tickmarks entirely use:otherwise ax.set_yticklabels([]) and ax.set_xticklabels([]) will keep tickmarks.

Get Filename Without Extension in Python

user469652

[Get Filename Without Extension in Python](https://stackoverflow.com/questions/4444923/get-filename-without-extension-in-python)

If I have a filename like one of these:How could I get only the filename, without the extension? Would a regex be appropriate?

2010-12-14 22:26:57Z

If I have a filename like one of these:How could I get only the filename, without the extension? Would a regex be appropriate?In most cases, you shouldn't use a regex for that.This will also handle a filename like .bashrc correctly by keeping the whole name.If I had to do this with a regex, I'd do it like this:No need for regex. os.path.splitext is your friend:You can use stem method to get file name.Here is an example:

How to construct a timedelta object from a simple string

priestc

[How to construct a timedelta object from a simple string](https://stackoverflow.com/questions/4628122/how-to-construct-a-timedelta-object-from-a-simple-string)

I'm writing a function that needs a timedelta input to be passed in as a string. The user must enter something like "32m" or "2h32m", or even "4:13" or "5hr34m56s"... Is there a library or something that has this sort of thing already implemented?

2011-01-07 17:03:48Z

I'm writing a function that needs a timedelta input to be passed in as a string. The user must enter something like "32m" or "2h32m", or even "4:13" or "5hr34m56s"... Is there a library or something that has this sort of thing already implemented?For the first format(5hr34m56s), you should parse using regular expressionsHere is re-based solution:To me the most elegant solution, without having to resort to external libraries such as dateutil or manually parsing the input, is to use datetime's powerful strptime string parsing method.After this you can use your timedelta object as normally, convert it to seconds to make sure we did the correct thing etc.I had a bit of time on my hands yesterday, so I developed @virhilo's answer into a Python module, adding a few more time expression formats, including all those requested by @priestc.Source code is on github (MIT License) for anybody that wants it.  It's also on PyPI:Returns the time as a number of seconds:I wanted to input just a time and then add it to various dates so this worked for me:I've modified virhilo's nice answer with a few upgrades: .If you use Python 3 then here's updated version for Hari Shankar's solution, which I used:Django comes with the utility function parse_duration(). From the documentation:Use isodate library to parse ISO 8601 duration string. For example:Also see Is there an easy way to convert ISO 8601 duration to timedelta?

Python argparse: Make at least one argument required

Adam Matan

[Python argparse: Make at least one argument required](https://stackoverflow.com/questions/6722936/python-argparse-make-at-least-one-argument-required)

I've been using argparse for a Python program that can -process, -upload or both:The program is meaningless without at least one parameter. How can I configure argparse to force at least one parameter to be chosen?UPDATE:Following the comments: What's the Pythonic way to parametrize a program with at least one option?

2011-07-17 09:24:12Z

I've been using argparse for a Python program that can -process, -upload or both:The program is meaningless without at least one parameter. How can I configure argparse to force at least one parameter to be chosen?UPDATE:Following the comments: What's the Pythonic way to parametrize a program with at least one option?If not the 'or both' part (I have initially missed this) you could use something like this:Though, probably it would be a better idea to use subcommands instead.I know this is old as dirt, but the way to require one option but forbid more than one (XOR) is like this:Output:  There are also some implicit requirements when living on command line:Try to run it:Show the help:And use it:There can be even shorter variant:Usage looks like this:Note, that instead of boolean values for "process" and "upload" keys there are counters.It turns out, we cannot prevent duplication of these words:Designing good command line interface can be challenging sometime.There are multiple aspects of command line based program:argparse offers a lot, but restricts possible scenarios and can become very complex.With docopt things go much shorter while preserving readability and offering high degree of flexibility. If you manage getting parsed arguments from dictionary and do some of conversions (to integer, opening files..) manually (or by other library called schema), you may find docopt good fit for command line parsing.The best way to do this is by using python inbuilt module add_mutually_exclusive_group.If you want only one argument to be selected by command line just use required=True as an argument for group If you require a python program to run with at least one parameter, add an argument that doesn't have the option prefix  (- or -- by default) and set nargs=+ (Minimum of one argument required). The problem with this method I found is that if you do not specify the argument, argparse will generate a "too few arguments" error and not print out the help menu. If you don't need that functionality, here's how to do it in code:I think that when you add an argument with the option prefixes, nargs governs the entire argument parser and not just the option. (What I mean is, if you have an --option flag with nargs="+", then --option flag expects at least one argument. If you have option with nargs="+", it expects at least one argument overall.)For http://bugs.python.org/issue11588 I am exploring ways of generalizing the mutually_exclusive_group concept to handle cases like this.With this development argparse.py, https://github.com/hpaulj/argparse_issues/blob/nested/argparse.py 

I am able to write:which produces the following help:This accepts inputs like '-u',  '-up', '--proc --up' etc.It ends up running a test similar to https://stackoverflow.com/a/6723066/901925, though the error message needs to be clearer:I wonder:Use append_const to a list of actions and then check that the list is populated:You can even specify the methods directly within the constants.Maybe use sub-parsers?Now --help shows:You can add additional options to these sub-parsers as well. Also instead of using that dest='subparser_name' you can also bind functions to be directly called on given sub-command (see docs).This achieves the purpose and this will also be relfected in the argparse autogenerated --help output, which is imho what most sane programmers want (also works with optional arguments):Official docs on this:

https://docs.python.org/3/library/argparse.html#choices

How to convert a date string to different format [duplicate]

Chamith Malinda

[How to convert a date string to different format [duplicate]](https://stackoverflow.com/questions/14524322/how-to-convert-a-date-string-to-different-format)

I need to convert date string "2013-1-25" to string "1/25/13" in python.

I looked at the datetime.strptime but still can't find a way for this.

2013-01-25 14:55:41Z

I need to convert date string "2013-1-25" to string "1/25/13" in python.

I looked at the datetime.strptime but still can't find a way for this.I assume I have import datetime before running each of the lines of code belowprints "01/25/13".If you can't live with the leading zero, try this:This prints "1/25/13".EDIT: This may not work on every platform:If you can live with 01 for January instead of 1, then try...You can check the docs for other formatting directives.

Python hashable dicts

Unknown

[Python hashable dicts](https://stackoverflow.com/questions/1151658/python-hashable-dicts)

As an exercise, and mostly for my own amusement, I'm implementing a backtracking packrat parser.  The inspiration for this is i'd like to have a better idea about how hygenic macros would work in an algol-like language (as apposed to the syntax free lisp dialects you normally find them in).  Because of this, different passes through the input might see different grammars, so cached parse results are invalid, unless I also store the current version of the grammar along with the cached parse results.  (EDIT: a consequence of this use of key-value collections is that they should be immutable, but I don't intend to expose the interface to allow them to be changed, so either mutable or immutable collections are fine)The problem is that python dicts cannot appear as keys to other dicts.  Even using a tuple (as I'd be doing anyways) doesn't help.I guess it has to be tuples all the way down.  Now the python standard library provides approximately what i'd need, collections.namedtuple has a very different syntax, but can be used as a key.  continuing from above session:Ok.  But I have to make a class for each possible combination of keys in the rule I would want to use, which isn't so bad, because each parse rule knows exactly what parameters it uses, so that class can be defined at the same time as the function that parses the rule.  Edit: An additional problem with namedtuples is that they are strictly positional.  Two tuples that look like they should be different can in fact be the same: tl'dr: How do I get dicts that can be used as keys to other dicts?Having hacked a bit on the answers, here's the more complete solution I'm using.  Note that this does a bit extra work to make the resulting dicts vaguely immutable for practical purposes.  Of course it's still quite easy to hack around it by calling dict.__setitem__(instance, key, value) but we're all adults here.

2009-07-20 04:30:24Z

As an exercise, and mostly for my own amusement, I'm implementing a backtracking packrat parser.  The inspiration for this is i'd like to have a better idea about how hygenic macros would work in an algol-like language (as apposed to the syntax free lisp dialects you normally find them in).  Because of this, different passes through the input might see different grammars, so cached parse results are invalid, unless I also store the current version of the grammar along with the cached parse results.  (EDIT: a consequence of this use of key-value collections is that they should be immutable, but I don't intend to expose the interface to allow them to be changed, so either mutable or immutable collections are fine)The problem is that python dicts cannot appear as keys to other dicts.  Even using a tuple (as I'd be doing anyways) doesn't help.I guess it has to be tuples all the way down.  Now the python standard library provides approximately what i'd need, collections.namedtuple has a very different syntax, but can be used as a key.  continuing from above session:Ok.  But I have to make a class for each possible combination of keys in the rule I would want to use, which isn't so bad, because each parse rule knows exactly what parameters it uses, so that class can be defined at the same time as the function that parses the rule.  Edit: An additional problem with namedtuples is that they are strictly positional.  Two tuples that look like they should be different can in fact be the same: tl'dr: How do I get dicts that can be used as keys to other dicts?Having hacked a bit on the answers, here's the more complete solution I'm using.  Note that this does a bit extra work to make the resulting dicts vaguely immutable for practical purposes.  Of course it's still quite easy to hack around it by calling dict.__setitem__(instance, key, value) but we're all adults here.Here is the easy way to make a hashable dictionary. Just remember not to mutate them after embedding in another dictionary for obvious reasons.Hashables should be immutable -- not enforcing this but TRUSTING you not to mutate a dict after its first use as a key, the following approach would work:If you DO need to mutate your dicts and STILL want to use them as keys, complexity explodes hundredfolds -- not to say it can't be done, but I'll wait until a VERY specific indication before I get into THAT incredible morass!-)All that is needed to make dictionaries usable for your purpose is to add a __hash__ method:Note, the frozenset conversion will work for all dictionaries (i.e. it doesn't require the keys to be sortable).  Likewise, there is no restriction on the dictionary values.If there are many dictionaries with identical keys but with distinct values, it is necessary to have the hash take the values into account.  The fastest way to do that is:This is quicker than frozenset(self.iteritems()) for two reasons.  First, the frozenset(self) step reuses the hash values stored in the dictionary, saving unnecessary calls to hash(key).  Second, using itervalues will access the values directly and avoid the many memory allocator calls using by items to form new many key/value tuples in memory every time you do a lookup.The given answers are okay, but they could be improved by using frozenset(...) instead of tuple(sorted(...)) to generate the hash:The performance advantage depends on the content of the dictionary, but in most cases I've tested, hashing with frozenset is at least 2 times faster (mainly because it does not need to sort).A reasonably clean, straightforward implementation isI keep coming back to this topic... Here's another variation. I'm uneasy with subclassing dict to add a __hash__ method;  There's virtually no escape from the problem that dict's are mutable, and trusting that they won't change seems like a weak idea.  So I've instead looked at building a mapping based on a builtin type that is itself immutable.  although tuple is an obvious choice, accessing values in it implies a sort and a bisect;  not a problem, but it doesn't seem to be leveraging much of the power of the type it's built on.What if you jam key, value pairs into a frozenset?  What would that require, how would it work?Part 1, you need a way of encoding the 'item's in such a way that a frozenset will treat them mainly by their keys;  I'll make a little subclass for that.That alone puts you in spitting distance of an immutable mapping:D'oh! Unfortunately, when you use the set operators and the elements are equal but not the same object; which one ends up in the return value is undefined,  we'll have to go through some more gyrations.However, looking values up in this way is cumbersome, and worse, creates lots of intermediate sets; that won't do!  We'll create a 'fake' key-value pair to get around it:Which results in the less problematic:That's all the deep magic; the rest is wrapping it all up into something that has an interface like a dict.  Since we're subclassing from frozenset, which has a very different interface, there's quite a lot of methods; we get a little help from collections.Mapping, but most of the work is overriding the frozenset methods for versions that work like dicts, instead:which, ultimately, does answer my own question:The accepted answer by @Unknown, as well as the answer by @AlexMartelli work perfectly fine, but only under the following constraints:The much faster answer by @ObenSonne lifts the constraints 2 and 3, but is still bound by constraint 1 (values must be hashable). The faster yet answer by @RaymondHettinger lifts all 3 constraints because it does not include .values() in the hash calculation. However, its performance is good only if:If this condition isn't satisfied, the hash function will still be valid, but may cause too many collisions. For example, in the extreme case where all the dictionaries are generated from a website template (field names as keys, user input as values), the keys will always be the same, and the hash function will return the same value for all the inputs. As a result, a hashtable that relies on such a hash function will become as slow as a list when retrieving an item (O(N) instead of O(1)).I think the following solution will work reasonably well even if all 4 constraints I listed above are violated. It has an additional advantage that it can hash not only dictionaries, but any containers, even if they have nested mutable containers.I'd much appreciate any feedback on this, since I only tested this lightly so far.You might also want to add these two methods to get the v2 pickling protocol work with hashdict instances. Otherwise cPickle will try to use hashdict.____setitem____ resulting in a TypeError. Interestingly, with the other two versions of the protocol your code works just fine. If you don't put numbers in the dictionary and you never lose the variables containing your dictionaries, you can do this:cache[id(rule)] = "whatever"since id() is unique for every dictionaryEDIT:Oh sorry, yeah in that case what the other guys said would be better. I think you could also serialize your dictionaries as a string, like cache[ 'foo:bar' ] = 'baz' If you need to recover your dictionaries from the keys though, then you'd have to do something uglier like cache[ 'foo:bar' ] = ( {'foo':'bar'}, 'baz' )I guess the advantage of this is that you wouldn't have to write as much code.

Can I remove script tags with BeautifulSoup?

Sam

[Can I remove script tags with BeautifulSoup?](https://stackoverflow.com/questions/5598524/can-i-remove-script-tags-with-beautifulsoup)

Can script tags and all of their contents be removed from HTML with BeautifulSoup, or do I have to use Regular Expressions or something else?

2011-04-08 17:14:32Z

Can script tags and all of their contents be removed from HTML with BeautifulSoup, or do I have to use Regular Expressions or something else?Updated answer for those who might need for future reference:

The correct answer is.

decompose()

You can use different ways but decompose works in-place.Example usage:Pretty useful to get rid of detritus like 'script','img' so and so forth.As stated in the (official documentation) you can use the extract method to remove all the subtree that matches the search.

Split string using a newline delimiter with Python

Hariharan

[Split string using a newline delimiter with Python](https://stackoverflow.com/questions/22042948/split-string-using-a-newline-delimiter-with-python)

I need to delimit the string which has new line in it. How would I achieve it? Please refer below code.Input:Output desired:I have tried the below approaches:

2014-02-26 13:31:07Z

I need to delimit the string which has new line in it. How would I achieve it? Please refer below code.Input:Output desired:I have tried the below approaches:str.splitlines method should give you exactly that.  str.split, by default, splits by all the whitespace characters. If the actual string has any other whitespace characters, you might want to useOr as @Ashwini Chaudhary suggested in the comments, you can useIf you want to split only by newlines, its better to use splitlines():Example:With split() it works also:However:There is a method specifically for this purpose:Here you go:

Is python's sorted() function guaranteed to be stable?

sundar - Reinstate Monica

[Is python's sorted() function guaranteed to be stable?](https://stackoverflow.com/questions/1915376/is-pythons-sorted-function-guaranteed-to-be-stable)

The documentation doesn't guarantee that. Is there any other place that it is documented? I'm guessing it might be stable since the sort method on lists is guaranteed to be stable (Notes 9th point: "Starting with Python 2.3, the sort() method is guaranteed to be stable"), and sorted is functionally similar. However, I'm not able to find any definitive source that says so. Purpose: I need to sort based on a primary key and also a secondary key in cases where the primary key is equal in both records. If sorted() is guaranteed to be stable, I can sort on the secondary key, then sort on the primary key and get the result I need. PS: To avoid any confusion, I'm using stable in the sense of "a sort is stable if it guarantees not to change the relative order of elements that compare equal".

2009-12-16 15:30:01Z

The documentation doesn't guarantee that. Is there any other place that it is documented? I'm guessing it might be stable since the sort method on lists is guaranteed to be stable (Notes 9th point: "Starting with Python 2.3, the sort() method is guaranteed to be stable"), and sorted is functionally similar. However, I'm not able to find any definitive source that says so. Purpose: I need to sort based on a primary key and also a secondary key in cases where the primary key is equal in both records. If sorted() is guaranteed to be stable, I can sort on the secondary key, then sort on the primary key and get the result I need. PS: To avoid any confusion, I'm using stable in the sense of "a sort is stable if it guarantees not to change the relative order of elements that compare equal".Yes, the intention of the manual is indeed to guarantee that sorted is stable and indeed that it uses exactly the same algorithm as the sort method. I do realize that the docs aren't 100% clear about this identity; doc patches are always happily accepted!They are stable.By the way: you sometimes can ignore knowing whether sort and sorted are stable, by combining a multi-pass sort in a single-pass one.For example, if you want to sort objects based on their last_name, first_name attributes, you can do it in one pass:taking advantage of tuple comparison.This answer, as-is, covers the original question. For further sorting-related questions, there is the Python Sorting How-To.The documentation changed in the meantime (relevant commit) and the current documentation of sorted explicitly guarantees it:This part of the documentation was added to Python 2.7 and Python 3.4(+) so any compliant implementation of that language version should have a stable sorted.Note that for CPython the list.sort has been stable since Python 2.3I'm not 100% sure on sorted, nowadays it simple uses list.sort, but I haven't checked the history for that. But it's likely that it "always" used list.sort.The "What's New" docs for Python 2.4 effectively make the point that sorted() first creates a list, then calls sort() on it, providing you with the guarantee you need though not in the "official" docs.  You could also just check the source, if you're really concerned.The Python 3.6 doc on sorting now states thatFurthermore, in that document, there is a link to the stable Timsort, which states that 

Check if list of objects contain an object with a certain attribute value

Jiew Meng

[Check if list of objects contain an object with a certain attribute value](https://stackoverflow.com/questions/9371114/check-if-list-of-objects-contain-an-object-with-a-certain-attribute-value)

I want to check if my list of objects contain an object with a certain attribute value. I want a way of checking if list contains an object with name "t1" for example. How can it be done? I found https://stackoverflow.com/a/598415/292291, I don't want to go through the whole list every time, I just need to know if there's 1 instance which matches. Will first(...) or any(...) or something else do that?

2012-02-21 02:00:54Z

I want to check if my list of objects contain an object with a certain attribute value. I want a way of checking if list contains an object with name "t1" for example. How can it be done? I found https://stackoverflow.com/a/598415/292291, I don't want to go through the whole list every time, I just need to know if there's 1 instance which matches. Will first(...) or any(...) or something else do that?As you can easily see from the documentation, the any() function short-circuits an returns True as soon as a match has been found.

Is there an analysis speed or memory usage advantage to using HDF5 for large array storage (instead of flat binary files)?

Caleb

[Is there an analysis speed or memory usage advantage to using HDF5 for large array storage (instead of flat binary files)?](https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr)

I am processing large 3D arrays, which I often need to slice in various ways to do a variety of data analysis. A typical "cube" can be ~100GB (and will likely get larger in the future)It seems that the typical recommended file format for large datasets in python is to use HDF5 (either h5py or pytables). My question is: is there any speed or memory usage benefit to using HDF5 to store and analyze these cubes over storing them in simple flat binary files? Is HDF5 more appropriate for tabular data, as opposed to large arrays like what I am working with? I see that HDF5 can provide nice compression, but I am more interested in processing speed and dealing with memory overflow.I frequently want to analyze only one large subset of the cube. One drawback of both pytables and h5py is it seems is that when I take a slice of the array, I always get a numpy array back, using up memory. However, if I slice a numpy memmap of a flat binary file, I can get a view, which keeps the data on disk. So, it seems that I can more easily analyze specific sectors of my data without overrunning my memory.I have explored both pytables and h5py, and haven't seen the benefit of either so far for my purpose. 

2014-12-30 18:00:27Z

I am processing large 3D arrays, which I often need to slice in various ways to do a variety of data analysis. A typical "cube" can be ~100GB (and will likely get larger in the future)It seems that the typical recommended file format for large datasets in python is to use HDF5 (either h5py or pytables). My question is: is there any speed or memory usage benefit to using HDF5 to store and analyze these cubes over storing them in simple flat binary files? Is HDF5 more appropriate for tabular data, as opposed to large arrays like what I am working with? I see that HDF5 can provide nice compression, but I am more interested in processing speed and dealing with memory overflow.I frequently want to analyze only one large subset of the cube. One drawback of both pytables and h5py is it seems is that when I take a slice of the array, I always get a numpy array back, using up memory. However, if I slice a numpy memmap of a flat binary file, I can get a view, which keeps the data on disk. So, it seems that I can more easily analyze specific sectors of my data without overrunning my memory.I have explored both pytables and h5py, and haven't seen the benefit of either so far for my purpose. Some of the main advantages of HDF5 are its hierarchical structure (similar to folders/files), optional arbitrary metadata stored with each item, and its flexibility (e.g. compression).  This organizational structure and metadata storage may sound trivial, but it's very useful in practice.  Another advantage of HDF is that the datasets can be either fixed-size or flexibly sized. Therefore, it's easy to append data to a large dataset without having to create an entire new copy.Additionally, HDF5 is a standardized format with libraries available for almost any language, so sharing your on-disk data between, say Matlab, Fortran, R, C, and Python is very easy with HDF. (To be fair, it's not too hard with a big binary array, too, as long as you're aware of the C vs. F ordering and know the shape, dtype, etc of the stored array.)Just as the TL/DR: For an ~8GB 3D array, reading a "full" slice along any axis took ~20 seconds with a chunked HDF5 dataset, and 0.3 seconds (best-case) to over three hours (worst case) for a memmapped array of the same data.  Beyond the things listed above, there's another big advantage to a "chunked"* on-disk data format such as HDF5: Reading an arbitrary slice (emphasis on arbitrary) will typically be much faster, as the on-disk data is more contiguous on average. *(HDF5 doesn't have to be a chunked data format. It supports chunking, but doesn't require it. In fact, the default for creating a dataset in h5py is not to chunk, if I recall correctly.)Basically, your best case disk-read speed and your worst case disk read speed for a given slice of your dataset will be fairly close with a chunked HDF dataset (assuming you chose a reasonable chunk size or let a library choose one for you).  With a simple binary array, the best-case is faster, but the worst-case is much worse.One caveat, if you have an SSD, you likely won't notice a huge difference in read/write speed.  With a regular hard drive, though, sequential reads are much, much faster than random reads.  (i.e. A regular hard drive has long seek time.) HDF still has an advantage on an SSD, but it's more due its other features (e.g. metadata, organization, etc) than due to raw speed.First off, to clear up confusion, accessing an h5py dataset returns an object that behaves fairly similarly to a numpy array, but does not load the data into memory until it's sliced.  (Similar to memmap, but not identical.)  Have a look at the h5py introduction for more information.Slicing the dataset will load a subset of the data into memory, but presumably you want to do something with it, at which point you'll need it in memory anyway.  If you do want to do out-of-core computations, you can fairly easily for tabular data with pandas or pytables. It is possible with h5py (nicer for big N-D arrays), but you need to drop down to a touch lower level and handle the iteration yourself.However, the future of numpy-like out-of-core computations is Blaze. Have a look at it if you really want to take that route.First off, consider a  3D C-ordered array written to disk (I'll simulate it by calling arr.ravel() and printing the result, to make things more visible):The values would be stored on-disk sequentially as shown on line 4 below. (Let's ignore filesystem details and fragmentation for the moment.)In the best case scenario, let's take a slice along the first axis.  Notice that these are just the first 36 values of the array.  This will be a very fast read! (one seek, one read)  Similarly, the next slice along the first axis will just be the next 36 values. To read a complete slice along this axis, we only need one seek operation. If all we're going to be reading is various slices along this axis, then this is the perfect file structure.However, let's consider the worst-case scenario: A slice along the last axis.To read this slice in, we need 36 seeks and 36 reads, as all of the values are separated on disk. None of them are adjacent!This may seem pretty minor, but as we get to larger and larger arrays, the number and size of the seek operations grows rapidly. For a large-ish (~10Gb) 3D array stored in this way and read in via memmap, reading a full slice along the "worst" axis can easily take tens of minutes, even with modern hardware. At the same time, a slice along the best axis can take less than a second.  For simplicity, I'm only showing "full" slices along a single axis, but the exact same thing happens with arbitrary slices of any subset of the data.Incidentally there are several file formats that take advantage of this and basically store three copies of huge 3D arrays on disk: one in C-order, one in F-order, and one in the intermediate between the two.  (An example of this is Geoprobe's D3D format, though I'm not sure it's documented anywhere.)  Who cares if the final file size is 4TB, storage is cheap!  The crazy thing about that is that because the main use case is extracting a single sub-slice in each direction, the reads you want to make are very, very fast. It works very well!  Let's say we store 2x2x2 "chunks" of the 3D array as contiguous blocks on disk.  In other words, something like:So the data on disk would look like chunked:And just to show that they're 2x2x2 blocks of arr, notice that these are the first 8 values of chunked:To read in any slice along an axis, we'd read in either 6 or 9 contiguous chunks (twice as much data as we need) and then only keep the portion we wanted. That's a worst-case maximum of 9 seeks vs a maximum of 36 seeks for the non-chunked version. (But the best case is still 6 seeks vs 1 for the memmapped array.) Because sequential reads are very fast compared to seeks, this significantly reduces the amount of time it takes to read an arbitrary subset into memory. Once again, this effect becomes larger with larger arrays.HDF5 takes this a few steps farther.  The chunks don't have to be stored contiguously, and they're indexed by a B-Tree.  Furthermore, they don't have to be the same size on disk, so compression can be applied to each chunk.  By default, h5py doesn't created chunked HDF files on disk (I think pytables does, by contrast).  If you specify chunks=True when creating the dataset, however, you'll get a chunked array on disk.As a quick, minimal example:Note that chunks=True tells h5py to automatically pick a chunk size for us.  If you know more about your most common use-case, you can optimize the chunk size/shape by specifying a shape tuple (e.g. (2,2,2) in the simple example above).  This allows you to make reads along a particular axis more efficient or optimize for reads/writes of a certain size.  Just to emphasize the point, let's compare reading in slices from a chunked HDF5 dataset and a large (~8GB), Fortran-ordered 3D array containing the same exact data.I've cleared all OS caches between each run, so we're seeing the "cold" performance.For each file type, we'll test reading in a "full" x-slice along the first axis and a "full" z-slize along the last axis.  For the Fortran-ordered memmapped array, the "x" slice is the worst case, and the "z" slice is the best case.The code used is in a gist (including creating the hdf file).  I can't easily share the data used here, but you could simulate it by an array of zeros of the same shape (621, 4991, 2600) and type np.uint8.The chunked_hdf.py looks like this:memmapped_array.py is similar, but has a touch more complexity to ensure the slices are actually loaded into memory (by default, another memmapped array would be returned, which wouldn't be an apples-to-apples comparison).Let's have a look at the HDF performance first:A "full" x-slice and a "full" z-slice take about the same amount of time (~20sec).  Considering this is an 8GB array, that's not too bad.  Most of the timeAnd if we compare this to the memmapped array times (it's Fortran-ordered: A "z-slice" is the best case and an "x-slice" is the worst case.):Yes, you read that right. 0.3 seconds for one slice direction and ~3.5 hours for the other.The time to slice in the "x" direction is far longer than the amount of time it would take to load the entire 8GB array into memory and select the slice we wanted! (Again, this is a Fortran-ordered array.  The opposite x/z slice timing would be the case for a C-ordered array.)However, if we're always wanting to take a slice along the best-case direction, the big binary array on disk is very good. (~0.3 sec!) With a memmapped array, you're stuck with this I/O discrepancy (or perhaps anisotropy is a better term).  However, with a chunked HDF dataset, you can choose the chunksize such that access is either equal or is optimized for a particular use-case.  It gives you a lot more flexibility.Hopefully that helps clear up one part of your question, at any rate.  HDF5 has many other advantages over "raw" memmaps, but I don't have room to expand on all of them here.  Compression can speed some things up (the data I work with doesn't benefit much from compression, so I rarely use it), and OS-level caching often plays more nicely with HDF5 files than with "raw" memmaps.  Beyond that, HDF5 is a really fantastic container format. It gives you a lot of flexibility in managing your data, and can be used from more or less any programming language.Overall, try it and see if it works well for your use case.  I think you might be surprised.

Numpy isnan() fails on an array of floats (from pandas dataframe apply)

tim654321

[Numpy isnan() fails on an array of floats (from pandas dataframe apply)](https://stackoverflow.com/questions/36000993/numpy-isnan-fails-on-an-array-of-floats-from-pandas-dataframe-apply)

I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.For some reason, numpy.isnan is failing on this array, however as shown below, each element is a float, numpy.isnan runs correctly on each element, the type of the variable is definitely a numpy array.What's going on?!

2016-03-15 01:14:18Z

I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.For some reason, numpy.isnan is failing on this array, however as shown below, each element is a float, numpy.isnan runs correctly on each element, the type of the variable is definitely a numpy array.What's going on?!np.isnan can be applied to NumPy arrays of native dtype (such as np.float64):but raises TypeError when applied to object arrays:Since you have Pandas, you could use pd.isnull instead -- it can accept NumPy arrays of object or native dtypes:Note that None is also considered a null value in object arrays.On top of @unubtu answer, you could coerce pandas numpy object array to native (float64) type, something along the lineSpecify errors='coerce' to force strings that can't be parsed to a numeric value to become NaN. Column type would be dtype: float64, and then isnan check should workA great substitute for np.isnan() and pd.isnull() is since only nan is not equal to itself.

WhatsApp API (java/python) [closed]

rishi

[WhatsApp API (java/python) [closed]](https://stackoverflow.com/questions/17135496/whatsapp-api-java-python)

I am looking for WhatsApp API, preferably a Python or Java library.I've tried Yowsup, but could not get my number registered; I am based in India and I am not sure if that has got anything to do with it.I did try WhatsAPI (Python library) but it is not working either.Any suggestions about this? Any users of Yowsup here?

2013-06-16 16:43:31Z

I am looking for WhatsApp API, preferably a Python or Java library.I've tried Yowsup, but could not get my number registered; I am based in India and I am not sure if that has got anything to do with it.I did try WhatsAPI (Python library) but it is not working either.Any suggestions about this? Any users of Yowsup here?After trying everything, Yowsup library worked for me. The bug that I was facing was recently fixed. Anyone trying to do something with Whatsapp should try it.From my blogcourtesyThere is a secret pilot program which WhatsApp is working on with selected businessesNews coverage:For some of my technical experiments, I was trying to figure out how beneficial and feasible it is to implement bots for different chat platforms in terms of market share and so possibilities of adaptation. Especially when you have bankruptly failed twice, it's important to validate ideas and fail more faster.Popular chat platforms like Messenger, Slack, Skype etc. have happily (in the sense officially) provided APIs for bots to interact with, but WhatsApp has not yet provided any API.However, since many years, a lot of activities has happened around this - struggle towards automated interaction with WhatsApp platform:Yowsup provide best solution with example.you can download api from https://github.com/tgalal/yowsup let me know if you have any issue.WhatsApp Inc. does not provide an open API but a reverse-engineered library is made available on GitHub by the team Venomous on the GitHub. This however according to my knowledge is made possible in PHP. You can check the link here: https://github.com/venomous0x/WhatsAPIHope this helps This is the developers page of the Open WhatsApp official page: http://openwhatsapp.org/develop/You can find a lot of information there about Yowsup.Or, you can just go the the library's link (which I copied from the Open WhatsApp page anyway): https://github.com/tgalal/yowsupEnjoy!

How to pickle or store Jupyter (IPython) notebook session for later

Robin Nemeth

[How to pickle or store Jupyter (IPython) notebook session for later](https://stackoverflow.com/questions/34342155/how-to-pickle-or-store-jupyter-ipython-notebook-session-for-later)

Let's say I am doing a larger data analysis in Jupyter/Ipython notebook with lots of time consuming computations done. Then, for some reason, I have to shut down the jupyter local server I, but I would like to return to doing the analysis later, without having to go through all the time-consuming computations again.What I would like love to do is pickle or store the whole Jupyter session (all pandas dataframes, np.arrays, variables, ...) so I can safely shut down the server knowing I can return to my session in exactly the same state as before.Is it even technically possible? Is there a built-in functionality I overlooked?EDIT: based on this answer there is a %store magic which should be "lightweight pickle". However you have to store the variables manually like so:#inside a ipython/nb session

foo = "A dummy string"

%store foo

closing seesion, restarting kernel

%store -r foo # r for refresh

print(foo) # "A dummy string"which is fairly close to what I would want, but having to do it manually and being unable to distinguish between different sessions makes it less useful.

2015-12-17 18:53:24Z

Let's say I am doing a larger data analysis in Jupyter/Ipython notebook with lots of time consuming computations done. Then, for some reason, I have to shut down the jupyter local server I, but I would like to return to doing the analysis later, without having to go through all the time-consuming computations again.What I would like love to do is pickle or store the whole Jupyter session (all pandas dataframes, np.arrays, variables, ...) so I can safely shut down the server knowing I can return to my session in exactly the same state as before.Is it even technically possible? Is there a built-in functionality I overlooked?EDIT: based on this answer there is a %store magic which should be "lightweight pickle". However you have to store the variables manually like so:#inside a ipython/nb session

foo = "A dummy string"

%store foo

closing seesion, restarting kernel

%store -r foo # r for refresh

print(foo) # "A dummy string"which is fairly close to what I would want, but having to do it manually and being unable to distinguish between different sessions makes it less useful.I think Dill answers your question well.Save a Notebook session:Restore a Notebook session:Source(I'd rather comment than offer this as an actual answer, but I need more reputation to comment.) You can store most data-like variables in a systematic way. What I usually do is store all dataframes, arrays, etc. in pandas.HDFStore. At the beginning of the notebook, declareand then store any new variables as you produce themAt the end, probably a good idea to do before turning off the server. The next time you want to continue with the notebook:Truth be told, I'd prefer built-in functionality in ipython notebook, too. You can't save everything this way (e.g. objects, connections), and it's hard to keep the notebook organized with so much boilerplate codes.This question is related to: How to cache in IPython Notebook?To save the results of individual cells, the caching magic comes in handy.When rerunning the notebook, the contents of this cell is loaded from the cache.This is not exactly answering your question, but it might be enough to when the results of all the lengthy calculations are recovered fast. This in combination of hitting the run-all button on top of the notebook is for me a workable solution.The cache magic cannot save the state of a whole notebook yet. To my knowledge there is no other system yet to resume a "notebook". This would require to save all the history of the python kernel. After loading the notebook, and connecting to a kernel, this information should be loaded.

Authenticating against active directory using python + ldap

1729

[Authenticating against active directory using python + ldap](https://stackoverflow.com/questions/140439/authenticating-against-active-directory-using-python-ldap)

How do I authenticate against AD using Python + LDAP. I'm currently using the python-ldap library and all it is producing is tears.I can't even bind to perform a simple query:Running this with myusername@mydomain.co.uk password username gives me one of two errors:Invalid Credentials - When I mistype or intentionally use wrong credentials it fails to authenticate.Or What am I missing out to bind properly?I am getting the same errors on fedora and windows.

2008-09-26 16:08:11Z

How do I authenticate against AD using Python + LDAP. I'm currently using the python-ldap library and all it is producing is tears.I can't even bind to perform a simple query:Running this with myusername@mydomain.co.uk password username gives me one of two errors:Invalid Credentials - When I mistype or intentionally use wrong credentials it fails to authenticate.Or What am I missing out to bind properly?I am getting the same errors on fedora and windows.I was missing From the init.If you are open to using pywin32, you can use Win32 calls from Python. This is what we do in our CherryPy web server:That worked for me, l.set_option(ldap.OPT_REFERRALS, 0) was the key to access the ActiveDirectory. Moreover, I think that you should add an "con.unbind()" in order to close the connection before finishing the script.Here's some simple code that works for me.This is based on a previous answer.if you have Kerberos installed and talking to AD, as would be the case with, say, Centrify Express installed and running, you might just use python-kerberos. E.g.would return True a user 'joe' has password 'pizza' in the Kerberos realm X.PIZZA.COM.

(typically, I think, the latter would be the same as the name of the AD Domain)I see your comment to @Johan Buret about the DN not fixing your problem, but I also believe that is what you should look into.Given your example, the DN for the default administrator account in AD will be:

cn=Administrator,cn=Users,dc=mydomain,dc=co,dc=uk - please try that.I tried to addbut instead of an error Python just hangs and won't respond to anything any more. Maybe I'm building the search query wrong, what is the Base part of the search? I'm using the same as the DN for the simple bind (oh, and I had to do l.simple_bind, instead of l.simple_bind_s):I'm using AD LDS and the instance is registered for the current account.I had the same issue, but it was regarding the password encodingSolved the problem.Based on the excellent ldap3 tutorial:I did the above in Python3 but it's supposed to be compatible with Python 2.Use a Distinguished Name to log on your system."CN=Your user,CN=Users,DC=b2t,DC=local"

It should work on any LDAP system, including ADFor me changing from simple_bind_s() to bind() did the trick.

Numpy: Should I use newaxis or None?

nikow

[Numpy: Should I use newaxis or None?](https://stackoverflow.com/questions/944863/numpy-should-i-use-newaxis-or-none)

In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:The documentation states that one can also use None instead of newaxis, the effect is exactly the same.Is there any reason to choose one over the other? Is there any general preference or style guide? My impression is that newaxis is more popular, probably because it is more explicit. So is there any reason why None is allowed?

2009-06-03 13:48:45Z

In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:The documentation states that one can also use None instead of newaxis, the effect is exactly the same.Is there any reason to choose one over the other? Is there any general preference or style guide? My impression is that newaxis is more popular, probably because it is more explicit. So is there any reason why None is allowed?None is allowed because numpy.newaxis is merely an alias for None.The authors probably chose it because they needed a convenient constant, and None was available.As for why you should prefer newaxis over None: mainly it's because it's more explicit, and partly because someday the numpy authors might change it to something other than None.  (They're not planning to, and probably won't, but there's no good reason to prefer None.)

matplotlib: colorbars and its text labels

dimka

[matplotlib: colorbars and its text labels](https://stackoverflow.com/questions/15908371/matplotlib-colorbars-and-its-text-labels)

I'd like to create a colorbar legend for a heatmap, such that the labels are in the center of each discrete color. Example borrowed from here:This generates the following plot:Ideally I'd like to generate a legend bar which has the four colors and for each color, a label in its center: 0,1,2,>3. How can this be achieved?

2013-04-09 17:20:06Z

I'd like to create a colorbar legend for a heatmap, such that the labels are in the center of each discrete color. Example borrowed from here:This generates the following plot:Ideally I'd like to generate a legend bar which has the four colors and for each color, a label in its center: 0,1,2,>3. How can this be achieved?You were very close.  Once you have a reference to the color bar axis, you can do what ever you want to it, including putting text labels in the middle.  You might want to play with the formatting to make it more visible.To add to tacaswell's answer, the colorbar() function has an optional cax input you can use to pass an axis on which the colorbar should be drawn.  If you are using that input, you can directly set a label using that axis.

How to append multiple items in one line in Python

whatever

[How to append multiple items in one line in Python](https://stackoverflow.com/questions/16621498/how-to-append-multiple-items-in-one-line-in-python)

I have:I wanted to make the newlist.append() statements into a few statements. 

2013-05-18 06:41:09Z

I have:I wanted to make the newlist.append() statements into a few statements. No. The method for appending an entire sequence is list.extend().No.First off, append is a function, so you can't write append[i+1:i+4] because you're trying to get a slice of a thing that isn't a sequence. (You can't get an element of it, either: append[i+1] is wrong for the same reason.) When you call a function, the argument goes in parentheses, i.e. the round ones: (). Second, what you're trying to do is "take a sequence, and put every element in it at the end of this other sequence, in the original order". That's spelled extend. append is "take this thing, and put it at the end of the list, as a single item, even if it's also a list". (Recall that a list is a kind of sequence.)But then, you need to be aware that i+1:i+4 is a special construct that appears only inside square brackets (to get a slice from a sequence) and braces (to create a dict object). You cannot pass it to a function. So you can't extend with that. You need to make a sequence of those values, and the natural way to do this is with the range function.You could also:OUTPUT:  Use this :By using the (+) operator you can skip the multiple append & extend operators in just one line of code and this is valid for more then two of lists by L1+L2+L3+L4.......etc.Happy Learning...:)

Celery Received unregistered task of type (run example)

Echeg

[Celery Received unregistered task of type (run example)](https://stackoverflow.com/questions/9769496/celery-received-unregistered-task-of-type-run-example)

I'm trying to run example from Celery documentation.I run: celeryd --loglevel=INFOtasks.py:run_task.py:In same folder celeryconfig.py:When I run "run_task.py":on python consoleerrors on celeryd serverPlease explain what's the problem.

2012-03-19 11:40:50Z

I'm trying to run example from Celery documentation.I run: celeryd --loglevel=INFOtasks.py:run_task.py:In same folder celeryconfig.py:When I run "run_task.py":on python consoleerrors on celeryd serverPlease explain what's the problem.You can see the current list of registered tasks in the celery.registry.TaskRegistry class. Could be that your celeryconfig (in the current directory) is not in PYTHONPATH so celery can't find it and falls back to defaults. Simply specify it explicitly when starting celery.You can also set --loglevel=DEBUG and you should probably see the problem immediately.I think you need to restart the worker server. I meet the same problem and solve it by restarting.I had the same problem:

The reason of "Received unregistered task of type.." was that celeryd service didn't find and register the tasks on service start (btw their list is visible when you start

 ./manage.py celeryd --loglevel=info ).These tasks should be declared in CELERY_IMPORTS = ("tasks", ) in settings file.

If you have a special celery_settings.py file it has to be declared on celeryd service start as --settings=celery_settings.py as digivampire wrote.Whether you use CELERY_IMPORTS or autodiscover_tasks, the important point is the tasks are able to be found and the name of the tasks registered in Celery should match the names the workers try to fetch.When you launch the Celery, say celery worker -A project --loglevel=DEBUG, you should see the name of the tasks. For example, if I have a debug_task task in my celery.py.If you can't see your tasks in the list, please check your celery configuration imports the tasks correctly, either in --setting, --config, celeryconfig or config_from_object. If you are using celery beat, make sure the task name, task, you use in CELERYBEAT_SCHEDULE matches the name in the celery task list.I also had the same problem; I added in my celeryconfig.py file to solve it.please include=['proj.tasks']

You need go to the top dir, then exec thisnot in your celeryconfig.py input imports = ("path.ptah.tasks",)please in other module invoke task!!!!!!!!Using --settings did not work for me. I had to use the following to get it all to work:Here is the celeryconfig file that has the CELERY_IMPORTS added:My setup was a little bit more tricky because I'm using supervisor to launch celery as a daemon.For me this error was solved by ensuring the app containing the tasks was included under django's INSTALLED_APPS setting.I had this problem mysteriously crop up when I added some signal handling to my django app.  In doing so I converted the app to use an AppConfig, meaning that instead of simply reading as 'booking' in INSTALLED_APPS, it read 'booking.app.BookingConfig'.Celery doesn't understand what that means, so I added, INSTALLED_APPS_WITH_APPCONFIGS = ('booking',) to my django settings, and modified my celery.py fromtoI had the same problem running tasks from Celery Beat. Celery doesn't like relative imports so in my celeryconfig.py, I had to explicitly set the full package name:What worked for me, was to add explicit name to celery task decorator. I changed my task declaration from @app.tasks to @app.tasks(name='module.submodule.task')Here is an exampleThis, strangely, can also be because of a missing package. Run pip to install all necessary packages:

pip install -r requirements.txt autodiscover_tasks wasn't picking up tasks that used missing packages. I did not have any issue with Django. But encountered this when I was using Flask. The solution was setting the config option.celery worker -A app.celery --loglevel=DEBUG --config=settingswhile with Django, I just had:python manage.py celery worker -c 2 --loglevel=infoI encountered this problem as well, but it is not quite the same, so just FYI.  Recent upgrades causes this error message due to this decorator syntax.ERROR/MainProcess] Received unregistered task of type 'my_server_check'.@task('my_server_check')Had to be change to just@task()No clue why.If you are using the apps config in installed apps like this:Then in your config app, import the task in ready method like this:If you are running into this kind of error, there are a number of possible causes but the solution I found was that my celeryd config file in /etc/defaults/celeryd was configured for standard use, not for my specific django project. As soon as I converted it to the format specified in the celery docs, all was well.The solution for me to add this line to /etc/default/celerydBecause when I run these commands: Only the latter command was showing task names at all. I have also tried adding CELERY_APP line /etc/default/celeryd but that didn't worked either.I had the issue with PeriodicTask classes in django-celery, while their names showed up fine when starting the celery worker every execution triggered:KeyError: u'my_app.tasks.run'My task was a class named 'CleanUp', not just a method called 'run'.When I checked table 'djcelery_periodictask' I saw outdated entries and deleting them fixed the issue.Just to add my two cents for my case with this error...My path is /vagrant/devops/test with app.py and __init__.py in it.When I run cd /vagrant/devops/ && celery worker -A test.app.celery --loglevel=info I am getting this error.But when I run it like cd /vagrant/devops/test && celery worker -A app.celery --loglevel=info everything is OK.I've found that one of our programmers added the following line to one of the imports:This caused the Celery worker to change its working directory from the projects' default working directory (where it could find the tasks) to a different directory (where it couldn't find the tasks).After removing this line of code, all tasks were found and registered.Celery doesn't support relative imports so in my celeryconfig.py, you need absolute import.An additional item to a really useful list.I have found Celery unforgiving in relation to errors in tasks (or at least I haven't been able to trace the appropriate log entries) and it doesn't register them.  I have had a number of issues with running Celery as a service, which have been predominantly permissions related.The latest related to permissions writing to a log file.  I had no issues in development or running celery at the command line, but the service reported the task as unregistered. I needed to change the log folder permissions to enable the service to write to it.My 2 centsI was getting this in a docker image using alpine. The django settings referenced /dev/log for logging to syslog. The django app and celery worker were both based on the same image. The entrypoint of the django app image was launching syslogd on start, but the one for the celery worker was not. This was causing things like ./manage.py shell to fail because there wouldn't be any /dev/log. The celery worker was not failing. Instead, it was silently just ignoring the rest of the app launch, which included loading shared_task entries from applications in the django projectIn my case the error was because one container created files in a folder that were mounted on the host file-system with docker-compose.I just had to do remove the files created by the container on the host system and I was able to launch my project again.(I had to use sudo because the files were owned by the root user)Docker version: 18.03.1If you use autodiscover_tasks, make sure that your functions to be registered stay in the tasks.py, not any other file. Or celery can not find the functions you want to register.Use app.register_task will also do the job, but seems a little naive.Please refer to this official specification of autodiscover_tasks.Write the correct path to the file tasks}when running the celery with "celery -A conf worker  -l info" command all the tasks got listed in log like i was having 

. conf.celery.debug_task

 i was getting the error because I was not giving this exact task path.

So kindly recheck this by copying and pasting exact task id.The answer to your problem lies in THE FIRST LINE of the output you provided in your question: /usr/local/lib/python2.7/dist-packages/celery/loaders/default.py:64: NotConfigured: No 'celeryconfig' module found! Please make sure it exists and is available to Python.

  "is available to Python." % (configname, ))). Without the right configuration Celery is not able to do anything.Reason why it can't find the celeryconfig is most likely it is not in your PYTHONPATH.Celery won't import the task if the app was to lack some of its dependencies. For instance, an app.view imports numpy without it being installed. Best way to check for this is to try to run your django server, as you probably know:You have found the problem if this raises ImportErrors within the app that has houses the respective task. Just pip install everything, or try to remove the dependencies and try again. This isn't the cause of your issue otherwise.

Python: How to create a unique file name?

MysticCodes

[Python: How to create a unique file name?](https://stackoverflow.com/questions/2961509/python-how-to-create-a-unique-file-name)

I have a python web form with two options - File upload and textarea. I need to take the values from each and pass them to another command-line program. I can easily pass the file name with file upload options, but I am not sure how to pass the value of the textarea.I think what I need to do is:I am not sure how to generate a unique file name. Can anybody give me some tips on how to generate a unique file name? Any algorithms, suggestions, and lines of code are appreciated.Thanks for your concern

2010-06-02 20:53:04Z

I have a python web form with two options - File upload and textarea. I need to take the values from each and pass them to another command-line program. I can easily pass the file name with file upload options, but I am not sure how to pass the value of the textarea.I think what I need to do is:I am not sure how to generate a unique file name. Can anybody give me some tips on how to generate a unique file name? Any algorithms, suggestions, and lines of code are appreciated.Thanks for your concernI didn't think your question was very clear, but if all you need is a unique file name...If you want to make temporary files in Python, there's a module called tempfile in Python's standard libraries. If you want to launch other programs to operate on the file, use tempfile.mkstemp() to create files, and os.fdopen() to access the file descriptors that mkstemp() gives you.Incidentally, you say you're running commands from a Python program? You should almost certainly be using the subprocess module.So you can quite merrily write code that looks like:Running that, you should find that the cat command worked perfectly well, but the temporary file was deleted in the finally block. Be aware that you have to delete the temporary file that mkstemp() returns yourself - the library has no way of knowing when you're done with it!(Edit: I had presumed that NamedTemporaryFile did exactly what you're after, but that might not be so convenient - the file gets deleted immediately when the temp file object is closed, and having other processes open the file before you've closed it won't work on some platforms, notably Windows. Sorry, fail on my part.)The uuid module would be a good choice, I prefer to use uuid.uuid4().hex as random filename because it will return a hex string without dashes.The outputs should like this:Maybe you need unique temporary file?f is opened file. delete=False means do not delete file after closing.If you need control over the name of the file, there are optional prefix=... and suffix=... arguments that take strings.  See https://docs.python.org/3/library/tempfile.html.You can use the datetime moduleNote that:

I am using replace since the colons are not allowed in filenames in many operating systems.That's it, this will give you a unique filename every single time.I came across this question, and I will add my solution for those who may be looking for something similar. My approach was just to make a random file name from ascii characters. It will be unique with a good probability.This can be done using the unique function in ufp.path module.if current path exists 'test.ext' file. ufp.path.unique function return './test (d1).ext'.To create a unique file path if its exist, use random package to generate a new string name for file. You may refer below code for same.Now you can use this path to create file accordingly.

How to use pip on windows behind an authenticating proxy

aquavitae

[How to use pip on windows behind an authenticating proxy](https://stackoverflow.com/questions/9698557/how-to-use-pip-on-windows-behind-an-authenticating-proxy)

My computer is running windows behind a proxy on a windows server (using active directory), and I can't figure out how to get through it with pip (in python3).  I have tried using --proxy, but it still just timeouts.  I have also tried setting a long timeout (60s), but that made no difference.  My proxy settings are correct, and I compared them with those that I'm using successfully in TortoiseHG to make sure.Are there any other tricks that anyone knows of that I can try, or is there some limitation in pip with regards to windows proxies?Update: My failed attempts involved searching pypi.  I've just tried actually installing something and it worked.  Searching still fails though.  Does this indicate a bug in pip or do they work differently?

2012-03-14 08:54:46Z

My computer is running windows behind a proxy on a windows server (using active directory), and I can't figure out how to get through it with pip (in python3).  I have tried using --proxy, but it still just timeouts.  I have also tried setting a long timeout (60s), but that made no difference.  My proxy settings are correct, and I compared them with those that I'm using successfully in TortoiseHG to make sure.Are there any other tricks that anyone knows of that I can try, or is there some limitation in pip with regards to windows proxies?Update: My failed attempts involved searching pypi.  I've just tried actually installing something and it worked.  Searching still fails though.  Does this indicate a bug in pip or do they work differently?I have tried 2 options which both work on my company's NTLM authenticated proxy.

Option 1 is to use --proxy http://user:pass@proxyAddress:proxyPortIf you are still having trouble I would suggest installing a proxy authentication service (I use CNTLM) and pointing pip at it ie something like --proxy http://localhost:3128 It took me a couple hours to figure this out but I finally got it to work using CNTLM and afterwards got it to work with just a pip config file. Here is how I got it work with the pip config file...Solution:1. In Windows navigate to your user profile directory (Ex. C:\Users\Sync) and create a folder named "pip"2. Create a file named "pip.ini" in this directory (Ex. C:\Users\Sync\pip\pip.ini) and enter the following into it:Replace [domain name], [username], [password], [proxy address] and [proxy port] with your own information.Note, if your  [domain name], [username] or [password] has special characters, you have to percent-encode | encode them.3. At this point I was able to run "pip install" without any issues.Hopefully this works for others too!P.S.: This may pose a security concern because of having your password stored in plain text. If this is an issue, consider setting up CNTLM using this article (allows using hashed password instead of plain text). Afterwards  set proxy = 127.0.0.1:3128in the "pip.ini" file mentioned above.This is how I set it up:For example: I ran into the same issue on windows 7. I managed to get it working by creating a "pip" folder with a "pip.ini" file inside it. I put this folder inside "C:\Users\{my.username}\AppData\Roaming", because according to the Python documentation: In the pip.ini file I have only:So no username:password. And it is working just fine.I had a similar issue, and found that my company uses NTLM proxy authentication.  If you see this error in your pip.log, this is probably the issue:NTLMaps can be used to interface with the NTLM proxy server by becoming an intermediate proxy.Download NTLMAPs, update the included server.cfg, run the main.py file, then point pip's proxy setting to 127.0.0.1:.I also needed to change these default values in the server.cfg file to:http://ntlmaps.sourceforge.net/You may also run into problems with certificates from your proxy. There are plenty of answers here on how to retrieve your proxy's certificate.  On a Windows host, to allow pip to clear your proxy, you may want to set an environment variable such as:You can also use the --cert argument to PIP with the same result.install cntlm:  Cntlm: Fast NTLM Authentication Proxy in CConfig cntlm.ini:start it:Now in cmd.exe:works!You can also hide password: https://stormpoopersmith.com/2012/03/20/using-applications-behind-a-corporate-proxy/ same issue on windows10 and above solutions are not working for me. use a emulator console tool like cygwin and then do it the default linux way:and things are working fine.I had the same issue on a remote windows environment. I tried many solutions found here or on other similars posts but nothing worked. Finally, the solution was quite simple. I had to set NO_PROXY with cmd :You have to use double quotes and set NO_PROXY to upper case. You can also add NO_PROXY as an environment variable instead of setting it each time you use the console.I hope this will help if any other solution posted here works.In my case it worked when I opened command prompt (cmd) as an administrator only and no further information about proxy settings was needed.

How to check task status in Celery?

Marcin

[How to check task status in Celery?](https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery)

How does one check whether a task is running in celery (specifically, I'm using celery-django)?I've read the documentation, and I've googled, but I can't see a call like:My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.I'm using the current stable versions - 2.4, I believe.

2012-01-27 13:42:19Z

How does one check whether a task is running in celery (specifically, I'm using celery-django)?I've read the documentation, and I've googled, but I can't see a call like:My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.I'm using the current stable versions - 2.4, I believe.Return the task_id (which is given from .delay()) and ask the celery instance afterwards about the state:When asking, get a new AsyncResult using this task_id:Creating an AsyncResult object from the task id is the way recommended in the FAQ to obtain the task status when the only thing you have is the task id.However, as of Celery 3.x, there are significant caveats that could bite people if they do not pay attention to them. It really depends on the specific use-case scenario.In order for Celery to record that a task is running, you must set task_track_started to True. Here is a simple task that tests this:When task_track_started is False, which is the default, the state show is PENDING even though the task has started. If you set task_track_started to True, then the state will be STARTED.An AsyncResult with the state PENDING does not mean anything more than that Celery does not know the status of the task. This could be because of any number of reasons.For one thing, AsyncResult can be constructed with invalid task ids. Such "tasks" will be deemed pending by Celery:Ok, so nobody is going to feed obviously invalid ids to AsyncResult. Fair enough, but it also has for effect that AsyncResult will also consider a task that has successfully run but that Celery has forgotten as being PENDING. Again, in some use-case scenarios this can be a problem. Part of the issue hinges on how Celery is configured to keep the results of tasks, because it depends on the availability of the "tombstones" in the results backend. ("Tombstones" is the term use in the Celery documentation for the data chunks that record how the task ended.) Using AsyncResult won't work at all if task_ignore_result is True. A more vexing problem is that Celery expires the tombstones by default. The result_expires setting by default is set to 24 hours. So if you launch a task, and record the id in long-term storage, and more 24 hours later, you create an AsyncResult with it, the status will be PENDING. All "real tasks" start in the PENDING state. So getting PENDING on a task could mean that the task was requested but never progressed further than this (for whatever reason). Or it could mean the task ran but Celery forgot its state.I prefer to keep track of goals than keep track of the tasks themselves. I do keep some task information but it is really secondary to keeping track of the goals. The goals are stored in storage independent from Celery. When a request needs to perform a computation depends on some goal having been achieved, it checks whether the goal has already been achieved, if yes, then it uses this cached goal, otherwise it starts the task that will effect the goal, and sends to the client that made the HTTP request a response that indicates it should wait for a result.The variable names and hyperlinks above are for Celery 4.x. In 3.x the corresponding variables and hyperlinks are: CELERY_TRACK_STARTED, CELERY_IGNORE_RESULT, CELERY_TASK_RESULT_EXPIRES.Every Task object has a .request property, which contains it AsyncRequest object. Accordingly, the following line gives the state of a Task task:You can also create custom states and update it's value duting task execution.

This example is from docs:http://celery.readthedocs.org/en/latest/userguide/tasks.html#custom-statesOld question but I recently ran into this problem. If you're trying to get the task_id you can do it like this: Now you know exactly what the task_id is and can now use it to get the AsyncResult:Just use this API from celery FAQThis works fine.Try:task.AsyncResult(task.request.id).state this will provide the Celery Task status. If Celery Task is already is under FAILURE state it will throw an Exception: raised unexpected: KeyError('exc_type',)for simple tasks, we can use http://flower.readthedocs.io/en/latest/screenshots.html and http://policystat.github.io/jobtastic/ to do the monitoring. and for complicated tasks, say a task which deals with a lot other modules. We recommend manually record the progress and message on the specific task unit.I found helpful information in theCelery Project Workers Guide inspecting-workersFor my case, I am checking to see if Celery is running. You can play with inspect to get your needs.Apart from above Programmatic approach

Using Flower Task status can be easily seen.Real-time monitoring using Celery Events.

Flower is a web based tool for monitoring and administrating Celery clusters.Official Document: 

Flower - Celery monitoring toolInstallation:Usage:vi my_celery_apps/app1.pyvi tasks/task1.py

How to get element-wise matrix multiplication (Hadamard product) in numpy?

Malintha

[How to get element-wise matrix multiplication (Hadamard product) in numpy?](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy)

I have two matrices and I want to get the element-wise product, [[1*5,2*6], [3*7,4*8]], equaling [[5,12], [21,32]]I have triedandbut both give the result[[19 22], [43 50]]which is the matrix product, not the element-wise product. How can I get the the element-wise product (aka Hadamard product) using built-in functions?

2016-10-14 04:35:47Z

I have two matrices and I want to get the element-wise product, [[1*5,2*6], [3*7,4*8]], equaling [[5,12], [21,32]]I have triedandbut both give the result[[19 22], [43 50]]which is the matrix product, not the element-wise product. How can I get the the element-wise product (aka Hadamard product) using built-in functions?For elementwise multiplication of matrix objects, you can use numpy.multiply:Result However, you should really use array instead of matrix. matrix objects have all sorts of horrible incompatibilities with regular ndarrays. With ndarrays, you can just use * for elementwise multiplication:If you're on Python 3.5+, you don't even lose the ability to perform matrix multiplication with an operator, because @ does matrix multiplication now:just do this:Both np.multiply and * would yield element wise multiplication known as the Hadamard Product%timeit is ipython magicTry this:Here, np.array(a) returns a 2D array of type ndarray and multiplication of two ndarray would result element wise multiplication. So the result would be:If you wanna get a matrix, the do it with this:

python: getting only 1 decimal place [duplicate]

l--''''''---------''''''''''''

[python: getting only 1 decimal place [duplicate]](https://stackoverflow.com/questions/3400965/python-getting-only-1-decimal-place)

How do I convert 45.34531 to 45.3?

2010-08-03 21:41:20Z

How do I convert 45.34531 to 45.3?Are you trying to represent it with only one digit:or actually round off the other decimal places?or even round strictly down?Or use the builtin round:

pandas: best way to select all columns whose names start with X

ccsv

[pandas: best way to select all columns whose names start with X](https://stackoverflow.com/questions/27275236/pandas-best-way-to-select-all-columns-whose-names-start-with-x)

I have a DataFrame:I want to select values of 1 in columns starting with foo.. Is there a better way to do it other than:Something similar to writing something like:The answer should print out a DataFrame like this:

2014-12-03 15:15:54Z

I have a DataFrame:I want to select values of 1 in columns starting with foo.. Is there a better way to do it other than:Something similar to writing something like:The answer should print out a DataFrame like this:Just perform a list comprehension to create your columns:Another method is to create a series from the columns and use the vectorised str method startswith:In order to achieve what you want you need to add the following to filter the values that don't meet your ==1 criteria:EDITOK after seeing what you want the convoluted answer is this:Now that pandas' indexes support string operations, arguably the simplest and best way to select columns beginning with 'foo' is just:Alternatively, you can filter column (or row) labels with df.filter(). To specify a regular expression to match the names beginning with foo.:To select only the required rows (containing a 1) and the columns, you can use loc, selecting the columns using filter (or any other method) and the rows using any:The simplest way is to use str directly on column names, there is no need for pd.SeriesBased on @EdChum's answer, you can try the following solution:This will be really helpful in case not all the columns you want to select start with foo. This method selects all the columns that contain the substring foo and it could be placed in at any point of a column's name.In essence, I replaced .startswith() with .contains().My solution. It may be slower on performance:Another option for the selection of the desired entries is to use map:which gives you all the columns for rows that contain a 1:The row selection is done by as in @ajcr's answer which gives you:meaning that row 3 and 4 do not contain a 1 and won't be selected.The selection of the columns is done using Boolean indexing like this:In the example above this returnsSo, if a column does not start with foo, False is returned and the column is therefore not selected.If you just want to return all rows that contain a 1 - as your desired output suggests - you can simply dowhich returns

What is the equivalent of「none」in django templates?

goelv

[What is the equivalent of「none」in django templates?](https://stackoverflow.com/questions/11945321/what-is-the-equivalent-of-none-in-django-templates)

I want to see if a field/variable is none within a Django template. What is the correct syntax for that?This is what I currently have:In the example above, what would I use to replace "null"?

2012-08-14 03:17:29Z

I want to see if a field/variable is none within a Django template. What is the correct syntax for that?This is what I currently have:In the example above, what would I use to replace "null"?None, False and True all are available within template tags and filters. None, False, the empty string ('', "", """""") and empty lists/tuples all evaluate to False when evaluated by if, so you can easily doA hint: @fabiocerqueira is right, leave logic to models, limit templates to be the only presentation layer and calculate stuff like that in you model. An example:Hope this helps :)You can also use another built-in template default_if_noneisoperator : New in Django 1.10Look at the yesno helperEg:{% if profile.user.first_name %} works (assuming you also don't want to accept '').if in Python in general treats None, False, '', [], {}, ... all as false.You can also use the built-in template filter default:If value evaluates to False (e.g. None, an empty string, 0, False); the default "--" is displayed.Documentation:

https://docs.djangoproject.com/en/dev/ref/templates/builtins/#default

how to check which version of nltk, scikit learn installed?

nlper

[how to check which version of nltk, scikit learn installed?](https://stackoverflow.com/questions/28501072/how-to-check-which-version-of-nltk-scikit-learn-installed)

In shell script I am checking whether this packages are installed or not, if not installed then install it. So withing shell script:but it stops shell script at import linein linux terminal tried to see in this manner:which gives nothing thought it is installed.Is there any other way to verify this package installation in shell script, if not installed, also install it.

2015-02-13 13:46:40Z

In shell script I am checking whether this packages are installed or not, if not installed then install it. So withing shell script:but it stops shell script at import linein linux terminal tried to see in this manner:which gives nothing thought it is installed.Is there any other way to verify this package installation in shell script, if not installed, also install it.import nltk is Python syntax, and as such won't work in a shell script.To test the version of nltk and scikit_learn, you can write a Python script and run it. Such a script may look likeNote that not all Python packages are guaranteed to have a __version__ attribute, so for some others it may fail, but for nltk and scikit-learn at least it will work.Try this:You can simply try and that would give you a list like thisYou can visually scan the list to find the version of all installed packages... the list is in alphabetical order, so it is easy to scan.If you are in Anaconda conda list would do the same for you.For checking the version of scikit-learn in shell script, if you have pip installed, you can try this commandHope it helps!You can find NLTK version simply by doing:And similarly for scikit-learn,I'm using python3 here.you may check from a python notebook cell as followsand In my machine which is ubuntu 14.04 with python 2.7 installed, if I go here,there is a file calledVERSION.If I do a cat VERSION it prints 3.1, which is the NLTK version installed.

How do I get the filepath for a class in Python?

Staale

[How do I get the filepath for a class in Python?](https://stackoverflow.com/questions/697320/how-do-i-get-the-filepath-for-a-class-in-python)

Given a class C in Python, how can I determine which file the class was defined in? I need something that can work from either the class C, or from an instance off C.The reason I am doing this, is because I am generally a fan off putting files that belong together in the same folder. I want to create a class that uses a Django template to render itself as HTML. The base implementation should infer the filename for the template based on the filename that the class is defined in.Say I put a class LocationArtifact in the file "base/artifacts.py", then I want the default behaviour to be that the template name is "base/LocationArtifact.html".

2009-03-30 13:58:59Z

Given a class C in Python, how can I determine which file the class was defined in? I need something that can work from either the class C, or from an instance off C.The reason I am doing this, is because I am generally a fan off putting files that belong together in the same folder. I want to create a class that uses a Django template to render itself as HTML. The base implementation should infer the filename for the template based on the filename that the class is defined in.Say I put a class LocationArtifact in the file "base/artifacts.py", then I want the default behaviour to be that the template name is "base/LocationArtifact.html".You can use the inspect module, like this:try:This is the wrong approach for Django and really forcing things.The typical Django app pattern is:

What is the official「preferred」way to install pip and virtualenv systemwide?

coffee-grinder

[What is the official「preferred」way to install pip and virtualenv systemwide?](https://stackoverflow.com/questions/5585875/what-is-the-official-preferred-way-to-install-pip-and-virtualenv-systemwide)

Is it this, which people seem to recommend most often:Or this, which I got from http://www.pip-installer.org/en/latest/installing.html:Or something entirely different?

2011-04-07 18:36:35Z

Is it this, which people seem to recommend most often:Or this, which I got from http://www.pip-installer.org/en/latest/installing.html:Or something entirely different?If you can install the latest Python (2.7.9 and up) Pip is now bundled with it. 

See: https://docs.python.org/2.7//installing/index.html

If not :

Update (from the release notes):I now run the regular:Here are the official installation instructions:

http://pip.readthedocs.org/en/latest/installing.html#install-pipEDIT 25-Jul-2013:

Changed URL for setuptools install.  EDIT 10-Feb-2014:

Removed setuptools install (thanks @Ciantic)EDIT 26-Jun-2014:

Updated URL again (thanks @LarsH)EDIT 1-Mar-2015:

Pip is now bundled with Python http://www.pip-installer.org/en/latest/installing.html is really the canonical answer to this question.Specifically, the systemwide instructions are:The section quoted in the question is the virtualenv instructions rather than the systemwide ones. The easy_install instructions have been around for longer, but it isn't necessary to do it that way any more.This answer comes from @webology on Twitter:My added notes: On Ubuntu 12.04 I've had good luck just using the package manager:There is no preferred method - everything depends on your needs. Often you need to have different Python interpreters on the system for whatever reason. In this case you need to install the stuff individually for each interpreter. Apart from that: I prefer installing stuff myself instead of depending of pre-packaged stuff sometimes causing issues - but that's only one possible opionion.There really isn't a single "answer" to this question, but there are definitely some helpful concepts that can help you to come to a decision. The first question that needs to be answered in your use case is "Do I want to use the system Python?" If you want to use the Python distributed with your operating system, then using the apt-get install method may be just fine. Depending on the operating system distribution method though, you still have to ask some more questions, such as "Do I want to install multiple versions of this package?" If the answer is yes, then it is probably not a good idea to use something like apt. Dpkg pretty much will just untar an archive at the root of the filesystem, so it is up to the package maintainer to make sure the package installs safely under very little assumptions. In the case of most debian packages, I would assume (someone can feel free to correct me here) that they simply untar and provide a top level package. For example, say the package is "virtualenv", you'd end up with /usr/lib/python2.x/site-packages/virtualenv. If you install it with easy_install you'd get something like /usr/lib/python2.x/site-packages/virtualenv.egg-link that might point to /usr/lib/python2.x/site-packages/virtualenv-1.2-2.x.egg which may be a directory or zipped egg. Pip does something similar although it doesn't use eggs and instead will place the top level package directly in the lib directory. I might be off on the paths, but the point is that each method takes into account different needs. This is why tools like virtualenv are helpful as they allow you to sandbox your Python libraries such that you can have any combination you need of libraries and versions. Setuptools also allows installing packages as multiversion which means there is not a singular module_name.egg-link created. To import those packages you need to use pkg_resources and the __import__ function.Going back to your original question, if you are happy with the system python and plan on using virtualenv and pip to build environments for different applications, then installing virtualenv and / or pip at the system level using apt-get seems totally appropriate. One word of caution though is that if you plan on upgrading your distributions Python, that may have a ripple effect through your virtualenvs if you linked back to your system site packages. I should also mention that none of these options is inherently better than the others. They simply take different approaches. Using the system version is an excellent way to install Python applications, yet it can be a very difficult way to develop with Python. Easy install and setuptools is very convenient in a world without virtualenv, but if you need to use different versions of the same library, then it also become rather unwieldy. Pip and virtualenv really act more like a virtual machine. Instead of taking care to install things side by side, you just create an whole new environment. The downside here is that 30+ virtualenvs later you might have used up quite bit of diskspace and cluttered up your filesystem. As you can see, with the many options it is difficult to say which method to use, but with a little investigation into your use cases, you should be able to find a method that works. Do this:See Since virtualenvs contain pip by default, I almost never install pip globally.  What I do ends up looking more like:I then proceed to install and set up virtualenvwrapper to my liking and off I go.  it might also be worthwhile to take a look at Jeremy Avnet's virtualenv-burrito:https://github.com/brainsik/virtualenv-burrito@ericholscher says on Twitter, "The one in the official docs.."It's a great point, you should do what the docs say.Quoted from the official pip installation instructions at http://www.pip-installer.org/en/latest/installing.html:Starting from distro packages, you can either use:which lets you create virtualenvs, orwhich lets you install arbitrary packages to your home directory.If you're used to virtualenv, the first command gives you everything you need (remember, pip is bundled and will be installed in any virtualenv you create).If you just want to install packages, the second command gives you what you need.  Use pip like this:and put something likein your ~/.bashrc.If your distro is ancient and you don't want to use its packages at all (except for Python itself, probably), you can download virtualenv, either as a tarball or as a standalone script:If your distro is more of the bleeding edge kind, Python3.3 has built-in virtualenv-like abilities:This runs way faster, but setuptools and pip aren't included.To install pip on a mac (osx), the following one liner worked great for me:In Raspbian, there is even no need to mention python2.7. Indeed this is best way to install pip if python version in less then 2.7.9.Thanks to @tal-weisshttps://github.com/pypa/pip/raw/master/contrib/get-pip.py is probably the right way now.I use get-pip and virtualenv-burrito to install all this. Not sure if python-setuptools is required.On Debian the best way to do it would be sudo apt-get install python-pipThe former method is fine. The only problem I can see is that you might end up with an old version of setuptools (if the apt repository hasn't been kept up-to-date..

figure of imshow() is too small

Ruofeng

[figure of imshow() is too small](https://stackoverflow.com/questions/10540929/figure-of-imshow-is-too-small)

I'm trying to visualize a numpy array using imshow() since it's similar to imagesc() in Matlab.The resulting figure is very small at the center of the grey window, while most of the space is unoccupied. How can I set the parameters to make the figure larger? I tried figsize=(xx,xx) and it's not what I want. Thanks!

2012-05-10 19:31:35Z

I'm trying to visualize a numpy array using imshow() since it's similar to imagesc() in Matlab.The resulting figure is very small at the center of the grey window, while most of the space is unoccupied. How can I set the parameters to make the figure larger? I tried figsize=(xx,xx) and it's not what I want. Thanks!If you don't give an aspect argument to imshow, it will use the value for image.aspect in your matplotlibrc. The default for this value in a new matplotlibrc is equal.

So imshow will plot your array with equal aspect ratio.If you don't need an equal aspect you can set aspect to autowhich gives the following figureIf you want an equal aspect ratio you have to adapt your figsize according to the aspectwhich gives you:That's strange, it definitely works for me:I am using the "MacOSX" backend, btw.I'm new to python too. Here is something that looks like will do what you want toI believe this decides the size of the canvas.

matplotlib/seaborn: first and last row cut in half of heatmap plot

Flops

[matplotlib/seaborn: first and last row cut in half of heatmap plot](https://stackoverflow.com/questions/56942670/matplotlib-seaborn-first-and-last-row-cut-in-half-of-heatmap-plot)

When plotting heatmaps with seaborn (and correlation matrices with matplotlib) the first and the last row is cut in halve.

This happens also when I run this minimal code example which I found online.

The labels at the y axis are on the correct spot, but the rows aren't completely there.A few days ago, it work as intended. Since then, I installed texlive-xetex so I removed it again but it didn't solve my problem.Any ideas what I could be missing?

2019-07-08 21:18:45Z

When plotting heatmaps with seaborn (and correlation matrices with matplotlib) the first and the last row is cut in halve.

This happens also when I run this minimal code example which I found online.

The labels at the y axis are on the correct spot, but the rows aren't completely there.A few days ago, it work as intended. Since then, I installed texlive-xetex so I removed it again but it didn't solve my problem.Any ideas what I could be missing?Unfortunately matplotlib 3.1.1 broke seaborn heatmaps; and in general inverted axes with fixed ticks.

This is fixed in the current development version; you may henceIts a bug in the matplotlib regression between 3.1.0 and 3.1.1

You can correct this by:Fixed using the above and setting the heatmap limits manually. Firstchecked the current axes with Fixed withmatplotlib 3.1.2 is out - 

It is available in the Anaconda cloud via conda-forge but I was not able to install it via conda install.

The manual alternative worked:

Download matplotlib 3.1.2 from github and install via pipI solved it by adding this line in my code, with matplotlib==3.1.1: ax.set_ylim(sorted(ax.get_xlim(), reverse=True))NB. The only reason this works is because the x-axis isn't changed, so use at your own risk with future mpl versionsIt happens with matplotlib version 3.1.1 as suggested by importanceofbeingernestFollowing solved my problempip install matplotlib==3.1.0rustyDev is right about conda-forge, but I did not need to do a manual pip install from a github download. For me, on Windows, it worked directly. And the plots are all nice again.https://anaconda.org/conda-forge/matplotliboptional points, not needed for the answer:Afterwards, I tried other steps, but they are not needed: In conda prompt: conda search matplotlib --info showed no new version info, the most recent info was for 3.1.1. Thus I tried pip using pip install matplotlib==3.1.2 But pip says "Requirement already satisfied"Then getting the version according to medium.com/@rakshithvasudev/… python - import matplotlib - matplotlib.__version__ shows that 3.1.2 was successfully installedBtw, I had this error directly after updating Spyder to v4.0.0. The error was in a plot of a confusion matrix. This was mentioned already some months ago. stackoverflow.com/questions/57225685/… which is already linked to this seaborn question.conda install matplotlib=3.1.0This worked for me and downgraded  matplotlib  from 3.1.1 to 3.1.0 and the heatmaps started to behave correctly

How to get autocomplete in jupyter notebook without using tab?

physicsnoob1000

[How to get autocomplete in jupyter notebook without using tab?](https://stackoverflow.com/questions/45390326/how-to-get-autocomplete-in-jupyter-notebook-without-using-tab)

I would like to get an autocompletion feature in notebooks i.e. when I type something, a dropdown menu appears, with all the possible things I might type, without having to press the tab button. Is there such a thing?I tried :but this requires the tab button to be pressed

2017-07-29 14:22:56Z

I would like to get an autocompletion feature in notebooks i.e. when I type something, a dropdown menu appears, with all the possible things I might type, without having to press the tab button. Is there such a thing?I tried :but this requires the tab button to be pressedThere is an extension called Hinterland for jupyter, which automatically displays the drop down menu when typing. There are also some other useful extensions.In order to install extensions, you can follow the guide on this github repo. To easily activate extensions, you may want to use the extensions configurator.The auto-completion with Jupyter Notebook is so weak, even with hinterland extension. Thanks for the idea of deep-learning-based code auto-completion. I developed a Jupyter Notebook Extension based on TabNine which provides code auto-completion based on Deep Learning. Here's the Github link of my work: jupyter-tabnine.It's available on pypi index now.  Simply issue following commands, then enjoy it:)As mentioned by @physicsGuy above, You can use the hinterland extension. Simple steps to do it.Installing nbextension using conda forge channel. Simply run the below command in conda terminal:Next Step enabling the hinterland extension. Run the below command in conda terminal:That's it, done.Without doing this %config IPCompleter.greedy=True after you import a package like numpy or pandas in this way;

import numpy as np

import pandas as pd.Then you type in pd. then tap the tab button it brings out all the possible methods to use very easy and straight forward.I am using Jupiter Notebook 5.6.0. Here, to get autosuggestion I am just hitting Tab key after entering at least one character.To get the methods and properties inside the imported library use same Tab key with AliceAdd the below to your keyboard user preferences on jupyter lab (Settings->Advanced system editor)

pandas read_csv and filter columns with usecols

chip

[pandas read_csv and filter columns with usecols](https://stackoverflow.com/questions/15017072/pandas-read-csv-and-filter-columns-with-usecols)

I have a csv file which isn't coming in correctly with pandas.read_csv when I  filter the columns with usecols and use multiple indexes.

I expect that df1 and df2 should be the same except for the missing dummy column, but the columns come in mislabeled.  Also the date is getting parsed as a date.  Using column numbers instead of names give me the same problem.  I can workaround the issue by dropping the dummy column after the read_csv step, but I'm trying to understand what is going wrong.  I'm using pandas 0.10.1.edit: fixed bad header usage.

