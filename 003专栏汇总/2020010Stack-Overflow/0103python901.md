2019-12-16 15:45:06Z

f-Strings are available from Python 3.6 and are very useful for formatting strings:Reading more about them in Python 3's f-Strings: An Improved String Formatting Syntax (Guide). I found an interesting pattern:And this is exactly the case:Now if we pass from two { to three, the result is the same:So we need up to 4! ({{{{) to get two braces as an output:Why is this? What happens with two braces to have Python require an extra one from that moment on?Double braces escape the braces, so that no interpolation happens: {{ ➝ {, and }} ➝ }. And 74 remains an unchanged string, '74'.With triple braces, the outer double braces are escaped, same as above. The inner braces, on the other hand, lead to regular string interpolation of the value 74.That is, the string f'{{{74}}}' is equivalent to f'{{ {74} }}', but without spaces (or, equivalently, to '{' + f'{74}' + '}').You can see the difference when replacing the numeric constant by a variable:

Python subprocess.Popen「OSError: [Errno 12] Cannot allocate memory」

DavidM

[Python subprocess.Popen「OSError: [Errno 12] Cannot allocate memory」](https://stackoverflow.com/questions/1367373/python-subprocess-popen-oserror-errno-12-cannot-allocate-memory)

Note: This question was originally asked here but the bounty time expired even though an acceptable answer was not actually found. I am re-asking this question including all details provided in the original question.A python script is running a set of class functions every 60 seconds using the sched module:The script is running as a daemonised process using the code here.A number of class methods that are called as part of doChecks use the subprocess module to call system functions in order to get system statistics:This runs fine for a period of time before the entire script crashing with the following error:The output of free -m on the server once the script has crashed is:The server is running CentOS 5.3. I am unable to reproduce on my own CentOS boxes nor with any other user reporting the same problem.I have tried a number of things to debug this as suggested in the original question:The entire checks can be found at on GitHub here with the getProcesses function defined from line 442. This is called by doChecks() starting at line 520.The script was run with strace with the following output before the crash:

2009-09-02 12:23:43Z

Note: This question was originally asked here but the bounty time expired even though an acceptable answer was not actually found. I am re-asking this question including all details provided in the original question.A python script is running a set of class functions every 60 seconds using the sched module:The script is running as a daemonised process using the code here.A number of class methods that are called as part of doChecks use the subprocess module to call system functions in order to get system statistics:This runs fine for a period of time before the entire script crashing with the following error:The output of free -m on the server once the script has crashed is:The server is running CentOS 5.3. I am unable to reproduce on my own CentOS boxes nor with any other user reporting the same problem.I have tried a number of things to debug this as suggested in the original question:The entire checks can be found at on GitHub here with the getProcesses function defined from line 442. This is called by doChecks() starting at line 520.The script was run with strace with the following output before the crash:As a general rule (i.e. in vanilla kernels), fork/clone failures with ENOMEM occur specifically because of either an honest to God out-of-memory condition (dup_mm, dup_task_struct, alloc_pid, mpol_dup, mm_init etc. croak), or because security_vm_enough_memory_mm failed you while enforcing the overcommit policy.Start by checking the vmsize of the process that failed to fork, at the time of the fork attempt, and then compare to the amount of free memory (physical and swap) as it relates to the overcommit policy (plug the numbers in.)In your particular case, note that Virtuozzo has additional checks in overcommit enforcement.  Moreover, I'm not sure how much control you truly have, from within your container, over swap and overcommit configuration (in order to influence the outcome of the enforcement.)Now, in order to actually move forward I'd say you're left with two options:NOTE that the coding effort may be all for naught if it turns out that it's not you, but some other guy collocated in a different instance on the same server as you running amock.Memory-wise, we already know that subprocess.Popen uses fork/clone under the hood, meaning that every time you call it you're requesting once more as much memory as Python is already eating up, i.e. in the hundreds of additional MB, all in order to then exec a puny 10kB executable such as free or ps.  In the case of an unfavourable overcommit policy, you'll soon see ENOMEM.Alternatives to fork that do not have this parent page tables etc. copy problem are vfork and posix_spawn.  But if you do not feel like rewriting chunks of subprocess.Popen in terms of vfork/posix_spawn, consider using suprocess.Popen only once, at the beginning of your script (when Python's memory footprint is minimal), to spawn a shell script that then runs free/ps/sleep and whatever else in a loop parallel to your script; poll the script's output or read it synchronously, possibly from a separate thread if you have other stuff to take care of asynchronously -- do your data crunching in Python but leave the forking to the subordinate process.HOWEVER, in your particular case you can skip invoking ps and free altogether; that information is readily available to you in Python directly from procfs, whether you choose to access it yourself or via existing libraries and/or packages.  If ps and free were the only utilities you were running, then you can do away with subprocess.Popen completely.Finally, whatever you do as far as subprocess.Popen is concerned, if your script leaks memory you will still hit the wall eventually.  Keep an eye on it, and check for memory leaks.Looking at the output of free -m it seems to me that you actually do not have swap memory available. I am not sure if in Linux the swap always will be available automatically on demand, but I was having the same problem and none of the answers here really helped me. Adding some swap memory however, fixed the problem in my case so since this might help other people facing the same problem, I post my answer on how to add a 1GB swap (on Ubuntu 12.04 but it should work similarly for other distributions.)You can first check if there is any swap memory enabled.if it is empty, it means you don't have any swap enabled. To add a 1GB swap:Add the following line to the fstab to make the swap permanent.Source and more information can be found here.swap may not be the red herring previously suggested.  How big is the python process in question just before the ENOMEM?Under kernel 2.6, /proc/sys/vm/swappiness controls how aggressively the kernel will turn to swap, and overcommit* files how much and how precisely the kernel may apportion memory with a wink and a nod.  Like your facebook relationship status, it's complicated.but not according to the output of your free(1) command, which shows no swap space recognized by your server instance.  Now, your web host may certainly know much more than I about this topic, but virtual RHEL/CentOS systems I've used have reported swap available to the guest OS.Adapting Red Hat KB Article 15252:Compare your /proc/sys/vm settings to a plain CentOS 5.3 installation.  Add a swap file.  Ratchet down swappiness and see if you live any longer.For an easy fix, you couldif your're sure that your system has enough memory. See Linux over commit heuristic.I continue to suspect that your customer/user has some kernel module or driver loaded which

is interfering with the clone() system call (perhaps some obscure security enhancement,

something like LIDS but more obscure?) or is somehow filling up some of the kernel data

structures that are necessary for fork()/clone() to operate (process table, page

tables, file descriptor tables, etc).Here's the relevant portion of the fork(2) man page:I suggest having the user try this after booting into a stock, generic kernel and with only a minimal set of modules and drivers loaded (minimum necessary to run your application/script).  From there, assuming it works in that configuration, they can perform a binary search between that and the configuration which exhibits the issue.  This is standard sysadmin troubleshooting 101.The relevant line in your strace is:... I know others have talked about swap and memory availability (and I would recommend that you set up at least a small swap partition, ironically even if it's on a RAM disk ... the code paths through the Linux kernel when it has even a tiny bit of swap available have been exercised far more extensively than those (exception handling paths) in which there is zero swap available.However I suspect that this is still a red herring.The fact that free is reporting 0 (ZERO) memory in use by the cache and buffers is very disturbing.  I suspect that the free output ... and possibly your application issue here, are caused by some proprietary kernel module which is interfering with the memory allocation in some way.According to the man pages for fork()/clone() the fork() system call should return EAGAIN if your call would cause a resource limit violation (RLIMIT_NPROC) ... however, it doesn't say if EAGAIN is to be returned by other RLIMIT* violations.  In any event if your target/host has some sort of weird Vormetric or other security settings (or even if your process is running under some weird SELinux policy) then it might be causing this -ENOMEM failure.It's pretty unlikely to be a normal run-of-the-mill Linux/UNIX issue. You've got something non-standard going on there.Have you tried using:I thought this had fixed the exact same problem for me.

But then my process ended up getting killed instead of failing to spawn, which is even worse..After some testing I found that this only occurred on older versions of python: it happens with 2.6.5 but not with 2.7.2My search had led me here python-close_fds-issue, but unsetting closed_fds had not solved the issue. It is still well worth a read.I found that python was leaking file descriptors by just keeping an eye on it:Like you, I do want to capture the command's output, and I do want to avoid OOM errors... but it looks like the only way is for people to use a less buggy version of Python. Not ideal...I've seen sloppy code that looks like this:You should check to see if this is what is happening in the

python code.  Errno is only valid if the proceeding system call

failed.Edited to add:  You don't say how long this process lives.  Possible consumers of memory  

What is the perfect counterpart in Python for「while not EOF」

Allen Koo

[What is the perfect counterpart in Python for「while not EOF」](https://stackoverflow.com/questions/15599639/what-is-the-perfect-counterpart-in-python-for-while-not-eof)

To read some text file, in C or Pascal, I always use the following snippets to read the data until EOF:Thus, I wonder how can I do this simple and fast in Python? 

2013-03-24 14:25:31Z

To read some text file, in C or Pascal, I always use the following snippets to read the data until EOF:Thus, I wonder how can I do this simple and fast in Python? Loop over the file to read lines:File objects are iterable and yield lines until EOF. Using the file object as an iterable uses  a buffer to ensure performant reads.You can do the same with the stdin (no need to use raw_input():To complete the picture, binary reads can be done with:where chunk will contain up to 1024 bytes at a time from the file, and iteration stops when openfileobject.read(1024) starts returning empty byte strings.You can imitate the C idiom in Python.To read a buffer up to max_size number of bytes, you can do this:Or, a text file line by line:You need to use while True / break construct since there is no eof test in Python other than the lack of bytes returned from a read. In C, you might have:However, you cannot have this in Python:because assignments are not allowed in expressions in Python (although recent versions of Python can mimic this using assignment expressions, see below).It is certainly more idiomatic in Python to do this:Update: Since Python 3.8 you may also use assignment expressions:The Python idiom for opening a file and reading it line-by-line is:The file will be automatically closed at the end of the above code (the with construct takes care of that).Finally, it is worth noting that line will preserve the trailing newline. This can be easily removed using:You can use below code snippet to read line by line, till end of fileWhile there are suggestions above for "doing it the python way", if one wants to really have a logic based on EOF, then I suppose using exception handling is the way to do it -- Example:Or press Ctrl-Z at a raw_input() prompt (Windows, Ctrl-Z Linux)You can use the following code snippet.  readlines() reads in the whole file at once and splits it by line.

Enable access control on simple HTTP server

MChan

[Enable access control on simple HTTP server](https://stackoverflow.com/questions/21956683/enable-access-control-on-simple-http-server)

I have the following shell script for a very simple HTTP server:I was wondering how I can enable or add a CORS header like Access-Control-Allow-Origin: * to this server?

2014-02-22 16:00:26Z

I have the following shell script for a very simple HTTP server:I was wondering how I can enable or add a CORS header like Access-Control-Allow-Origin: * to this server?Unfortunately, the simple HTTP server is really that simple that it does not allow any customization, especially not for the headers it sends. You can however create a simple HTTP server yourself, using most of SimpleHTTPRequestHandler, and just add that desired header.For that, simply create a file simple-cors-http-server.py (or whatever) and, depending on the Python version you are using, put one of the following codes inside.Then you can do python simple-cors-http-server.py and it will launch your modified server which will set the CORS header for every response.With the shebang at the top, make the file executable and put it into your PATH, and you can just run it using simple-cors-http-server.py too.Python 3 uses SimpleHTTPRequestHandler and HTTPServer from the http.server module to run the server:Python 2 uses SimpleHTTPServer.SimpleHTTPRequestHandler and the BaseHTTPServer module to run the server.If you need compatibility for both Python 3 and Python 2, you could use this polyglot script that works in both versions. It first tries to import from the Python 3 locations, and otherwise falls back to Python 2:Try an alternative like http-serverAs SimpleHTTPServer is not really the kind of server you deploy to production, I'm assuming here that you don't care that much about which tool you use as long as it does the job of exposing your files at http://localhost:3000 with CORS headers in a simple command lineNeed HTTPS?If you need https in local you can also try caddy or certbotSome related tools you might find usefulI had the same problem and came to this solution:I simply created a new class inheriting from SimpleHTTPRequestHandler that only changes the send_response method.You'll need to provide your own instances of do_GET() (and do_HEAD() if choose to support HEAD operations). something like this:

Merge PDF files

Btibert3

[Merge PDF files](https://stackoverflow.com/questions/3444645/merge-pdf-files)

Is it possible, using Python, to merge separate PDF files?  Assuming so, I need to extend this a little further.  I am hoping to loop through folders in a directory and repeat this procedure.  And I may be pushing my luck, but is it possible to exclude a page that is contained in of the PDFs (my report generation always creates an extra blank page).

2010-08-09 22:23:10Z

Is it possible, using Python, to merge separate PDF files?  Assuming so, I need to extend this a little further.  I am hoping to loop through folders in a directory and repeat this procedure.  And I may be pushing my luck, but is it possible to exclude a page that is contained in of the PDFs (my report generation always creates an extra blank page).Use Pypdf or its successor PyPDF2:(and much more)Here's a sample program that works with both versions.You can use PyPdf2s PdfMerger class. File ConcatenationYou can simply concatenate files by using the append method.You can pass file handles instead file paths if you want.File MergingIf you want more fine grained control of merging there is a merge method of the PdfMerger, which allows you to specify an insertion point in the output file, meaning you can insert the pages anywhere in the file. The append method can be thought of as a merge where the insertion point is the end of the file.e.g.Here we insert the whole pdf into the output but at page 2.Page RangesIf you wish to control which pages are appended from a particular file, you can use the pages keyword argument of append and merge, passing a tuple in the form (start, stop[, step]) (like the regular range function).e.g.If you specify an invalid range you will get an IndexError.Note: also that to avoid files being left open, the PdfFileMergers close method should be called when the merged file has been written. This ensures all files are closed (input and output) in a timely manner. It's a shame that PdfFileMerger isn't implemented as a context manager, so we can use the with keyword, avoid the explicit close call and get some easy exception safety.You might also want to look at the pdfcat script provided as part of pypdf2. You can potentially avoid the need to write code altogether.The PyPdf2 github also includes some example code demonstrating merging.Put the pdf files in a dir. Launch the program. You get one pdf with all the pdfs merged.The pdfrw library can do this quite easily, assuming you don't need to preserve bookmarks and annotations, and your PDFs aren't encrypted.  cat.py is an example concatenation script, and subset.py is an example page subsetting script.The relevant part of the concatenation script -- assumes inputs is a list of input filenames, and outfn is an output file name:As you can see from this, it would be pretty easy to leave out the last page, e.g. something like:Disclaimer:  I am the primary pdfrw author.Is it possible, using Python, to merge seperate PDF files?Yes.The following example merges all files in one folder to a single new PDF file:here, http://pieceofpy.com/2009/03/05/concatenating-pdf-with-python/, gives an solution.similarly:Git Repo: https://github.com/mahaguru24/Python_Merge_PDF.gitA slight variation using a dictionary for greater flexibility (e.g. sort, dedup):I used pdf unite on the linux terminal by leveraging subprocess (assumes one.pdf and two.pdf exist on the directory) and the aim is to merge them to three.pdf

Flask SQLAlchemy query, specify column names

Matthew Scragg

[Flask SQLAlchemy query, specify column names](https://stackoverflow.com/questions/11530196/flask-sqlalchemy-query-specify-column-names)

How do I specify the column that I want in my query using a model (it selects all columns by default)? I know how to do this with the sqlalchmey session: session.query(self.col1), but how do I do it with with models?  I can't do SomeModel.query(). Is there a way?

2012-07-17 20:16:38Z

How do I specify the column that I want in my query using a model (it selects all columns by default)? I know how to do this with the sqlalchmey session: session.query(self.col1), but how do I do it with with models?  I can't do SomeModel.query(). Is there a way?You can use the with_entities() method to restrict which columns you'd like to return in the result. (documentation) Depending on your requirements, you may also find deferreds useful.  They allow you to return the full object but restrict the columns that come over the wire.is the same asfor alias, we can use .label()You can use load_only function:You can use Model.query, because the Model (or usually its base class, especially in cases where declarative extension is used) is assigned Sesssion.query_property. In this case the Model.query is equivalent to Session.query(Model).I am not aware of the way to modify the columns returned by the query (except by adding more using add_columns()).

So your best shot is to use the Session.query(Model.col1, Model.col2, ...) (as already shown by Salil).You can use Query.values, Query.valuessession.query(SomeModel).values('id', 'user')An example here:I query the db for movies with rating <> 0, and then I order them by rating with the higest rating first.Take a look here: Select, Insert, Delete in Flask-SQLAlchemy

How do I find numeric columns in Pandas?

Hanan Shteingart

[How do I find numeric columns in Pandas?](https://stackoverflow.com/questions/25039626/how-do-i-find-numeric-columns-in-pandas)

Let's say df is a pandas DataFrame.

I would like to find all columns of numeric type.

Something like:

2014-07-30 14:36:21Z

Let's say df is a pandas DataFrame.

I would like to find all columns of numeric type.

Something like:You could use select_dtypes method of DataFrame. It includes two parameters include and exclude. So isNumeric would look like:You can use the undocumented function _get_numeric_data() to filter only numeric columns:Example:Note that this is a "private method" (i.e., an implementation detail) and is subject to change or total removal in the future. Use with caution.Simple one-line answer to create a new dataframe with only numeric columns:If you want the names of numeric columns:Complete code:Simple one-liner:This is another simple code for finding numeric column in pandas data frame,Following codes will return list of names of the numeric columns of a data set.here marketing_train is my data set and select_dtypes() is function to select data types using exclude and include arguments and columns is used to fetch the column name of data set

        output of above code will be following:ThanksAdapting this answer, you could doHere, np.applymap(np.isreal) shows whether every cell in the data frame is numeric, and .axis(all=0) checks if all values in a column are True and returns a series of Booleans that can be used to index the desired columns.Please see the below code:This way you can check whether the value are numeric such as float and int or the srting values. the second if statement is used for checking the string values which is referred by the object. 

How to check if a list is empty in Python? [duplicate]

y2k

[How to check if a list is empty in Python? [duplicate]](https://stackoverflow.com/questions/1725517/how-to-check-if-a-list-is-empty-in-python)

The API I'm working with can return empty [] lists.The following conditional statements aren't working as expected:What will work?

2009-11-12 21:24:30Z

The API I'm working with can return empty [] lists.The following conditional statements aren't working as expected:What will work?Empty lists evaluate to False in boolean contexts (such as if some_list:).I like Zarembisty's answer. Although, if you want to be more explicit,  you can always do:

Ignoring NaNs with str.contains

Emre

[Ignoring NaNs with str.contains](https://stackoverflow.com/questions/28311655/ignoring-nans-with-str-contains)

I want to find rows that contain a string, like so:However, this fails because some elements are NaN:So I resort to the obfuscatedIs there a better way?

2015-02-04 00:57:17Z

I want to find rows that contain a string, like so:However, this fails because some elements are NaN:So I resort to the obfuscatedIs there a better way?There's a flag for that:See the str.replace docs:So you can do the following:In addition to the above answers, I would say for columns having no single word name, you may use:-Hope this helps.I'm not 100% on why (actually came here to search for the answer), but this also works, and doesn't require replacing all nan values.Works with or without .loc.I have no idea why this works, as I understand it when you're indexing with brackets pandas evaluates whatever's inside the bracket as either True or False. I can't tell why making the phrase inside the brackets 'extra boolean' has any effect at all.

Multiple aggregations of the same column using pandas GroupBy.agg()

ely

[Multiple aggregations of the same column using pandas GroupBy.agg()](https://stackoverflow.com/questions/12589481/multiple-aggregations-of-the-same-column-using-pandas-groupby-agg)

Is there a pandas built-in way to apply two different aggregating functions f1, f2 to the same column df["returns"], without having to call agg() multiple times?Example dataframe:The syntactically wrong, but intuitively right, way to do it would be:Obviously, Python doesn't allow duplicate keys. Is there any other manner for expressing the input to agg()? Perhaps a list of tuples [(column, function)] would work better, to allow multiple functions applied to the same column? But agg() seems like it only accepts a dictionary.Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)

2012-09-25 19:05:26Z

Is there a pandas built-in way to apply two different aggregating functions f1, f2 to the same column df["returns"], without having to call agg() multiple times?Example dataframe:The syntactically wrong, but intuitively right, way to do it would be:Obviously, Python doesn't allow duplicate keys. Is there any other manner for expressing the input to agg()? Perhaps a list of tuples [(column, function)] would work better, to allow multiple functions applied to the same column? But agg() seems like it only accepts a dictionary.Is there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)You can simply pass the functions as a list:or as a dictionary:TLDR; Pandas groupby.agg has a new, easier syntax for specifying (1) aggregations on multiple columns, and (2) multiple aggregations on a column. So, to do this for pandas >= 0.25, use OR Pandas has changed the behavior of GroupBy.agg in favour of a more intuitive syntax for specifying named aggregations. See the 0.25 docs section on Enhancements as well as relevant GitHub issues GH18366 and GH26512.From the documentation, You can now pass a tuple via keyword arguments. The tuples follow the format of (<colName>, <aggFunc>).Alternatively, you can use pd.NamedAgg (essentially a namedtuple) which makes things more explicit.It is even simpler for Series, just pass the aggfunc to a keyword argument.Lastly, if your column names aren't valid python identifiers, use a dictionary with unpacking:In more recent versions of pandas leading upto 0.24, if using a dictionary for specifying column names for the aggregation output, you will get a FutureWarning:Using a dictionary for renaming columns is deprecated in v0.20. On more recent versions of pandas, this can be specified more simply by passing a list of tuples. If specifying the functions this way, all functions for that column need to be specified as tuples of (name, function) pairs.Or,Would something like this work:

Type hints in namedtuple

Pavel Hanpari

[Type hints in namedtuple](https://stackoverflow.com/questions/34269772/type-hints-in-namedtuple)

Consider following piece of code:The Code above is just a way to demonstrate as to what I am trying to achieve.

I would like to make namedtuple with type hints. Do you know any elegant way how to achieve result as intended?

2015-12-14 14:43:24Z

Consider following piece of code:The Code above is just a way to demonstrate as to what I am trying to achieve.

I would like to make namedtuple with type hints. Do you know any elegant way how to achieve result as intended?The prefered Syntax for a typed named tuple since 3.6 isEdit

Starting Python 3.7, consider using dataclasses (your IDE may not yet support them for static type checking):You can use typing.NamedTupleFrom the docsThis is present only in Python 3.5 onwards

How to flatten only some dimensions of a numpy array

Curious

[How to flatten only some dimensions of a numpy array](https://stackoverflow.com/questions/18757742/how-to-flatten-only-some-dimensions-of-a-numpy-array)

Is there a quick way to "sub-flatten" or flatten only some of the first dimensions in a numpy array?For example, given a numpy array of dimensions (50,100,25), the resultant dimensions would be (5000,25)

2013-09-12 07:12:21Z

Is there a quick way to "sub-flatten" or flatten only some of the first dimensions in a numpy array?For example, given a numpy array of dimensions (50,100,25), the resultant dimensions would be (5000,25)Take a look at numpy.reshape .A slight generalization to Alexander's answer - np.reshape can take -1 as an argument, meaning "total array size divided by product of all other listed dimensions":e.g. to flatten all but the last dimension:A slight generalization to Peter's answer -- you can specify a range over the original array's shape if you want to go beyond three dimensional arrays.e.g. to flatten all but the last two dimensions:EDIT: A slight generalization to my earlier answer -- you can, of course, also specify a range at the beginning of the of the reshape too:An alternative approach is to use numpy.resize() as in:

What does the caret operator (^) in Python do?

Fry

[What does the caret operator (^) in Python do?](https://stackoverflow.com/questions/2451386/what-does-the-caret-operator-in-python-do)

I ran across the caret operator in python today and trying it out, I got the following output:It seems to be based on 8, so I'm guessing some sort of byte operation?  I can't seem to find much about this searching sites other than it behaves oddly for floats, does anybody have a link to what this operator does or can you explain it here?

2010-03-16 00:21:41Z

I ran across the caret operator in python today and trying it out, I got the following output:It seems to be based on 8, so I'm guessing some sort of byte operation?  I can't seem to find much about this searching sites other than it behaves oddly for floats, does anybody have a link to what this operator does or can you explain it here?It's a bitwise XOR (exclusive OR).It results to true if one (and only one) of the operands (evaluates to) true.To demonstrate:To explain one of your own examples:Think about it this way:It invokes the __xor__() or __rxor__() method of the object as needed, which for integer types does a bitwise exclusive-or.It's a bit-by-bit exclusive-or.  Binary bitwise operators are documented in chapter 5 of the Python Language Reference.Generally speaking, the symbol ^ is an infix version of the __xor__ or __rxor__ methods. Whatever data types are placed to the right and left of the symbol must implement this function in a compatible way. For integers, it is the common XOR operation, but for example there is not a built-in definition of the function for type float with type int:One neat thing about Python is that you can override this behavior in a class of your own. For example, in some languages the ^ symbol means exponentiation. You could do that this way, just as one example:Then something like this will work, and now, for instances of Foo only, the ^ symbol will mean exponentiation.When you use the ^ operator, behind the curtains the method __xor__ is called.a^b is equivalent to a.__xor__(b).Also, a ^= b is equivalent to a = a.__ixor__(b) (where __xor__ is used as a fallback when __ixor__ is implicitly called via using ^= but does not exist).In principle, what __xor__ does is completely up to its implementation. Common use cases in Python are:Demo:Demo:Explanation:

Using %f with strftime() in Python to get microseconds

user820924

[Using %f with strftime() in Python to get microseconds](https://stackoverflow.com/questions/6677332/using-f-with-strftime-in-python-to-get-microseconds)

I'm trying to use strftime() to microsecond precision, which seems possible using %f (as stated here). However when I try the following code:...I get the hour, the minutes and the seconds, but %f prints as %f, with no sign of the microseconds. I'm running Python 2.6.5 on Ubuntu, so it should be fine and %f should be supported (it's supported for 2.6 and above, as far as I know.)

2011-07-13 10:15:09Z

I'm trying to use strftime() to microsecond precision, which seems possible using %f (as stated here). However when I try the following code:...I get the hour, the minutes and the seconds, but %f prints as %f, with no sign of the microseconds. I'm running Python 2.6.5 on Ubuntu, so it should be fine and %f should be supported (it's supported for 2.6 and above, as far as I know.)You can use datetime's strftime function to get this. The problem is that time's strftime accepts a timetuple that does not carry microsecond information.Should do the trick!You are looking at the wrong documentation. The time module has different documentation. You can use the datetime module strftime like this:With Python's time module you can't get microseconds with %f.For those who still want to go with time module only, here is a workaround:You should get something like 2017-01-16 16:42:34.625 EET (yes, I use milliseconds as it's fairly enough).To break the code into details, paste the below code into a Python console:For clarification purposes, I also paste my Python 2.7.12 result here:This should do the workIt will print HH:MM:SS.microseconds like this e.g 14:38:19.425961You can also get microsecond precision from the time module using its time() function.

(time.time() returns the time in seconds since epoch. Its fractional part is the time in microseconds, which is what you want.)When the "%f" for micro seconds isn't working, please use the following method:If you want speed, try this:Where prec is precision -- how many decimal places you want.

Please note that the function does not have issues with leading zeros in fractional part like some other solutions presented here.If you want an integer, try this code:Output:

How to remove gaps between subplots in matplotlib?

user3006135

[How to remove gaps between subplots in matplotlib?](https://stackoverflow.com/questions/20057260/how-to-remove-gaps-between-subplots-in-matplotlib)

The code below produces gaps between the subplots.  How do I remove the gaps between the subplots and make the image a tight grid?

2013-11-18 20:31:38Z

The code below produces gaps between the subplots.  How do I remove the gaps between the subplots and make the image a tight grid?You can use gridspec to control the spacing between axes. There's more information here. The problem is the use of aspect='equal', which prevents the subplots from stretching to an arbitrary aspect ratio and filling up all the empty space.Normally, this would work:The result is this:However, with aspect='equal', as in the following code:This is what we get:The difference in this second case is that you've forced the x- and y-axes to have the same number of units/pixel. Since the axes go from 0 to 1 by default (i.e., before you plot anything), using aspect='equal' forces each axis to be a square. Since the figure is not a square, pyplot adds in extra spacing between the axes horizontally.To get around this problem, you can set your figure to have the correct aspect ratio. We're going to use the object-oriented pyplot interface here, which I consider to be superior in general:Here's the result:Without resorting gridspec entirely, the following might also be used to remove the gaps by setting wspace and hspace to zero:Resulting in:Have you tried plt.tight_layout()?with plt.tight_layout()

without it:

Or: something like this (use add_axes)If you don't need to share axes, then simply axLS=map(fig.add_axes, rectLS)

With recent matplotlib versions you might want to try Constrained Layout. This does not work with plt.subplot() however, so you need to use plt.subplots() instead:

In Python script, how do I set PYTHONPATH?

TIMEX

[In Python script, how do I set PYTHONPATH?](https://stackoverflow.com/questions/3108285/in-python-script-how-do-i-set-pythonpath)

I know how to set it in my /etc/profile and in my environment variables.But what if I want to set it during a script?

Is it import os, sys? How do I do it?

2010-06-24 08:25:55Z

I know how to set it in my /etc/profile and in my environment variables.But what if I want to set it during a script?

Is it import os, sys? How do I do it?You don't set PYTHONPATH, you add entries to sys.path. It's a list of directories that should be searched for Python packages, so you can just append your directories to that list.In fact, sys.path is initialized by splitting the value of PYTHONPATH on the path separator character (: on Linux-like systems, ; on Windows).You can also add directories using site.addsitedir, and that method will also take into account .pth files existing within the directories you pass. (That would not be the case with directories you specify in PYTHONPATH.)You can get and set environment variables via os.environ:But since your interpreter is already running, this will have no effect. You're better off usingwhich is the array that your PYTHONPATH will be transformed into on interpreter startup.Sorry for reopen the question, but I think that it can help someone:If you put sys.path.append('dir/to/path') without check it is already added, you could generate a long list in sys.path. For that, I recommend this:I'm sorry if I annoyed someone reopening the question.PYTHONPATH ends up in sys.path, which you can modify at runtime.you can set PYTHONPATH, by os.environ['PATHPYTHON']=/some/path, then you need to call os.system('python') to restart the python shell to make the newly added path effective.I linux this works too:

Replace first occurrence of string in Python

marks34

[Replace first occurrence of string in Python](https://stackoverflow.com/questions/4628618/replace-first-occurrence-of-string-in-python)

I have some sample string. How can I replace first occurrence of this string in a longer string with empty string?

2011-01-07 17:53:10Z

I have some sample string. How can I replace first occurrence of this string in a longer string with empty string?string replace() function perfectly solves this problem:Use re.sub directly, this allows you to specify a count:(Note that the order of arguments is replacement, original not the opposite, as might be suspected.)

Can Python test the membership of multiple values in a list?

Noe Nieto

[Can Python test the membership of multiple values in a list?](https://stackoverflow.com/questions/6159313/can-python-test-the-membership-of-multiple-values-in-a-list)

I want to test if two or more values have membership on a list, but I'm getting an unexpected result:So, Can Python test the membership of multiple values at once in a list?

What does that result mean?

2011-05-28 02:30:54Z

I want to test if two or more values have membership on a list, but I'm getting an unexpected result:So, Can Python test the membership of multiple values at once in a list?

What does that result mean?This does what you want, and will work in nearly all cases:The expression 'a','b' in ['b', 'a', 'foo', 'bar'] doesn't work as expected because Python interprets it as a tuple:There are other ways to execute this test, but they won't work for as many different kinds of inputs. As Kabie points out, you can solve this problem using sets......sometimes:Sets can only be created with hashable elements. But the generator expression all(x in container for x in items) can handle almost any container type. The only requirement is that container be re-iterable (i.e. not a generator). items can be any iterable at all.In many cases, the subset test will be faster than all, but the difference isn't shocking -- except when the question is irrelevant because sets aren't an option. Converting lists to sets just for the purpose of a test like this won't always be worth the trouble. And converting generators to sets can sometimes be incredibly wasteful, slowing programs down by many orders of magnitude.Here are a few benchmarks for illustration. The biggest difference comes when both container and items are relatively small. In that case, the subset approach is about an order of magnitude faster:This looks like a big difference. But as long as container is a set, all is still perfectly usable at vastly larger scales:Using subset testing is still faster, but only by about 5x at this scale. The speed boost is due to Python's fast c-backed implementation of set, but the fundamental algorithm is the same in both cases.If your items are already stored in a list for other reasons, then you'll have to convert them to a set before using the subset test approach. Then the speedup drops to about 2.5x:And if your container is a sequence, and needs to be converted first, then the speedup is even smaller:The only time we get disastrously slow results is when we leave container as a sequence:And of course, we'll only do that if we must. If all the items in bigseq are hashable, then we'll do this instead:That's just 1.66x faster than the alternative (set(bigseq) >= set(bigsubseq), timed above at 4.36).So subset testing is generally faster, but not by an incredible margin. On the other hand, let's look at when all is faster. What if items is ten-million values long, and is likely to have values that aren't in container?Converting the generator into a set turns out to be incredibly wasteful in this case. The set constructor has to consume the entire generator. But the short-circuiting behavior of all ensures that only a small portion of the generator needs to be consumed, so it's faster than a subset test by four orders of magnitude. This is an extreme example, admittedly. But as it shows, you can't assume that one approach or the other will be faster in all cases.Most of the time, converting container to a set is worth it, at least if all its elements are hashable. That's because in for sets is O(1), while in for sequences is O(n). On the other hand, using subset testing is probably only worth it sometimes. Definitely do it if your test items are already stored in a set. Otherwise, all is only a little slower, and doesn't require any additional storage. It can also be used with large generators of items, and sometimes provides a massive speedup in that case.Another way to do it:I'm pretty sure in is having higher precedence than , so your statement is being interpreted as 'a', ('b' in ['b' ...]), which then evaluates to 'a', True since 'b' is in the array.See previous answer for how to do what you want.If you want to check all of your input matches,if you want to check at least one match,The Python parser evaluated that statement as a tuple, where the first value was 'a', and the second value is the expression 'b' in ['b', 'a', 'foo', 'bar'] (which evaluates to True).You can write a simple function do do what you want, though:And call it like:The reason I think this is better than the chosen answer is that you really don't need to call the 'all()' function. Empty list evaluates to False in IF statements, non-empty list evaluates to True. Example:I would say we can even leave those square brackets out. Both of the answers presented here will not handle repeated elements. For example, if you are testing whether [1,2,2] is a sublist of [1,2,3,4], both will return True. That may be what you mean to do, but I just wanted to clarify.

If you want to return false for [1,2,2] in [1,2,3,4], you would need to sort both lists and check each item with a moving index on each list. Just a slightly more complicated for loop.how can you be pythonic without lambdas! .. not to be taken seriously .. but this way works too:leave out the end part if you want to test if any of the values are in the array:Here's how I did it:

Difference between python3 and python3m executables

James Mishra

[Difference between python3 and python3m executables](https://stackoverflow.com/questions/16675865/difference-between-python3-and-python3m-executables)

What is the difference between the /usr/bin/python3 and /usr/bin/python3m executibles?I am observing them on Ubuntu 13.04, but Google suggests that they exist on other distributions too.The two files have the same md5sum, but do not seem to be symbolic links or hard links; the two files have different inode numbers returned by ls -li and testing find -xdev -samefile /usr/bin/python3.3 does not return any other files.Someone asked a similar question on AskUbuntu, but I wanted to find out more about the difference between the two files. 

2013-05-21 17:19:44Z

What is the difference between the /usr/bin/python3 and /usr/bin/python3m executibles?I am observing them on Ubuntu 13.04, but Google suggests that they exist on other distributions too.The two files have the same md5sum, but do not seem to be symbolic links or hard links; the two files have different inode numbers returned by ls -li and testing find -xdev -samefile /usr/bin/python3.3 does not return any other files.Someone asked a similar question on AskUbuntu, but I wanted to find out more about the difference between the two files. Credit for this goes to chepner for pointing out that I already had the link to the solution.via PEP 3149.Regarding the m flag specifically, this is what Pymalloc is:via What's New in Python 2.3Finally, the two files may be hardlinked on some systems. While the two files have different inode numbers on my Ubuntu 13.04 system (thus are different files), a comp.lang.python post from two years ago shows that they once were hardlinked.

Mocking a function to raise an Exception to test an except block

Jesse Webb

[Mocking a function to raise an Exception to test an except block](https://stackoverflow.com/questions/28305406/mocking-a-function-to-raise-an-exception-to-test-an-except-block)

I have a function (foo) which calls another function (bar). If invoking bar() raises an HttpError, I want to handle it specially if the status code is 404, otherwise re-raise.I am trying to write some unit tests around this foo function, mocking out the call to bar(). Unfortunately, I am unable to get the mocked call to bar() to raise an Exception which is caught by my except block.Here is my code which illustrates my problem:I followed the Mock docs which say that you should set the side_effect of a Mock instance to an Exception class to have the mocked function raise the error.I also looked at some other related StackOverflow Q&As, and it looks like I am doing the same thing they are doing to cause and Exception to be raised by their mock.Why is setting the side_effect of barMock not causing the expected Exception to be raised? If I am doing something weird, how should I go about testing logic in my except block?

2015-02-03 17:46:31Z

I have a function (foo) which calls another function (bar). If invoking bar() raises an HttpError, I want to handle it specially if the status code is 404, otherwise re-raise.I am trying to write some unit tests around this foo function, mocking out the call to bar(). Unfortunately, I am unable to get the mocked call to bar() to raise an Exception which is caught by my except block.Here is my code which illustrates my problem:I followed the Mock docs which say that you should set the side_effect of a Mock instance to an Exception class to have the mocked function raise the error.I also looked at some other related StackOverflow Q&As, and it looks like I am doing the same thing they are doing to cause and Exception to be raised by their mock.Why is setting the side_effect of barMock not causing the expected Exception to be raised? If I am doing something weird, how should I go about testing logic in my except block?Your mock is raising the exception just fine, but the error.resp.status value is missing. Rather than use return_value, just tell Mock that status is an attribute:Additional keyword arguments to Mock() are set as attributes on the resulting object.I put your foo and bar definitions in a my_tests module, added in the HttpError class so I could use it too, and your test then can be ran to success:You can even see the print '404 - %s' % error.message line run, but I think you wanted to use error.content there instead; that's the attribute HttpError() sets from the second argument, at any rate.

python numpy ValueError: operands could not be broadcast together with shapes

yayu

[python numpy ValueError: operands could not be broadcast together with shapes](https://stackoverflow.com/questions/24560298/python-numpy-valueerror-operands-could-not-be-broadcast-together-with-shapes)

In numpy, I have two "arrays", X is (m,n) and y is a vector (n,1)using I am getting the errorWhen  (97,2)x(2,1) is clearly a legal matrix operation and should give me a (97,1) vectorEDIT:I have corrected this using X.dot(y) but the original question still remains.

2014-07-03 17:52:22Z

In numpy, I have two "arrays", X is (m,n) and y is a vector (n,1)using I am getting the errorWhen  (97,2)x(2,1) is clearly a legal matrix operation and should give me a (97,1) vectorEDIT:I have corrected this using X.dot(y) but the original question still remains.dot is matrix multiplication, but * does something else.We have two arrays:With Numpy arrays, the operationis done element-wise, but one or both of the values can be expanded in one or more dimensions to make them compatible. This operation are called broadcasting. Dimensions where size is 1 or which are missing can be used in broadcasting.In the example above the dimensions are incompatible, because:Here there are conflicting numbers in the first dimension (97 and 2). That is what the ValueError above is complaining about. The second dimension would be ok, as number 1 does not conflict with anything.For more information on broadcasting rules: http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html(Please note that if X and y are of type numpy.matrix, then asterisk can be used as matrix multiplication. My recommendation is to keep away from numpy.matrix, it tends to complicate more than simplify things.)Your arrays should be fine with numpy.dot; if you get an error on numpy.dot, you must have some other bug. If the shapes are wrong for numpy.dot, you get a different exception:If you still get this error, please post a minimal example of the problem. An example multiplication with arrays shaped like yours succeeds:Per numpy docs:In other words, if you are trying to multiply two matrices (in the linear algebra sense) then you want X.dot(y) but if you are trying to broadcast scalars from matrix y onto X then you need to perform X * y.T.  Example:It's possible that the error didn't occur in the dot product, but after.

For example try thisnp.dot(a,b) will be fine; however np.dot(a, b) * c is clearly wrong (12x1 X 1x5 = 12x5 which cannot element-wise multiply 5x12) but numpy will give youThe error is misleading; however there is an issue on that line.Use np.mat(x) * np.mat(y), that'll work.You are looking for np.matmul(X, y). In Python 3.5+ you can use X @ y.

Associativity of「in」in Python?

user541686

[Associativity of「in」in Python?](https://stackoverflow.com/questions/12660870/associativity-of-in-in-python)

I'm making a Python parser, and this is really confusing me:How exactly does "in" work in Python, with regards to associativity, etc.?  Why do no two of these expressions behave the same way?

2012-09-30 11:24:52Z

I'm making a Python parser, and this is really confusing me:How exactly does "in" work in Python, with regards to associativity, etc.?  Why do no two of these expressions behave the same way?1 in [] in 'a' is evaluated as (1 in []) and ([] in 'a'). Since the first condition (1 in []) is False, the whole condition is evaluated as False; ([] in 'a') is never actually evaluated, so no error is raised.Here are the statement definitions:Python does special things with chained comparisons.The following are evaluated differently:In both cases though, if the first comparison is False, the rest of the statement won't be looked at.For your particular case, Also to demonstrate the first rule above, these are statements that evaluate to True.Precedence of python operators: http://docs.python.org/reference/expressions.html#summaryFrom the documentation:What this means is, that there no associativity in x in y in z!The following are equivalent:The short answer, since the long one is already given several times here and in excellent ways, is that the boolean expression is  short-circuited, this is has stopped evaluation when a change of true in false or vice versa cannot happen by further evaluation.  (see http://en.wikipedia.org/wiki/Short-circuit_evaluation)It might be a little short (no pun intended) as an answer, but as mentioned, all other explanation is allready done quite well here, but I thought the term deserved to be mentioned.

pypi UserWarning: Unknown distribution option: 'install_requires'

Tyler Long

[pypi UserWarning: Unknown distribution option: 'install_requires'](https://stackoverflow.com/questions/8295644/pypi-userwarning-unknown-distribution-option-install-requires)

Does anybody encounter this warning when executing python setup.py install of a PyPI package?install_requires defines what the package requires. A lot of PyPI packages have this option. How can it be an "unknown distribution option"?

2011-11-28 12:13:17Z

Does anybody encounter this warning when executing python setup.py install of a PyPI package?install_requires defines what the package requires. A lot of PyPI packages have this option. How can it be an "unknown distribution option"?python setup.py uses distutils which doesn't support install_requires. setuptools does, also distribute (its successor), and pip (which uses either) do. But you actually have to use them. I.e. call setuptools through the easy_install command or pip install. Another way is to import setup from setuptools in your setup.py, but this not standard and makes everybody wanting to use your package have to have setuptools installed.This was the first result on my google search, but had no answer.

I found that upgrading setuptools resolved the issue for me (and pip for good measure)Hope this helps the next person to find this link!ATTENTION! ATTENTION! Imperfect answer ahead. To get the "latest memo" on the state of packaging in the Python universe, read this fairly detailed essay.I have just ran into this problem when trying to build/install ansible. The problem seems to be that distutils really doesn't support install_requires. Setuptools should monkey-patch distutils on-the-fly, but it doesn't, probably because the last release of setuptools is 0.6c11 from 2009, whereas distutils is a core Python project.So even after manually installing the setuptools-0.6c11-py2.7.egg running setup.py only picks up distutils dist.py, and not the one from site-packages/setuptools/.Also the setuptools documentation hints to using ez_setup and not distutils.However, setuptools is itself provided by distribute nowadays, and that flavor of setup() supports install_requires.I'm on a Mac with python 2.7.11. I have been toying with creating extremely simple and straightforward projects, where my only requirement is that I can run python setup.py install, and have setup.py use the setup command, ideally from distutils. There are literally no other imports or code aside from the kwargs to setup() other than what I note here.I get the error when the imports for my setup.py file are:When I use this, I get warnings such asIf I change the imports (and nothing else) to the following:The warnings go away.Note that I am not using setuptools, just importing it changes the behavior such that it no longer emits the warnings. For me, this is the cause of a truly baffling difference where some projects I'm using give those warnings, and some others do not.Clearly, some form of monkey-patching is going on, and it is affected by whether or not that import is done. This probably isn't the situation for everyone researching this problem, but for the narrow environment in which I'm working, this is the answer I was looking for.This is consistent with the other (community) comment, which says that distutils should monkeypatch setuptools, and that they had the problem when installing Ansible. Ansible appears to have tried to allow installs without having setuptools in the past, and then went back on that.https://github.com/ansible/ansible/blob/devel/setup.pyA lot of stuff is up in the air... but if you're looking for a simple answer for a simple project, you should probably just import setuptools.This is a warning from distutils, and is a sign that you do not have setuptools installed.

Installing it from http://pypi.python.org/pypi/setuptools will remove the warning.It will install any missing headers. It solved my issueAs far as I can tell, this is a bug in setuptools where it isn't removing the setuptools specific options before calling up to the base class in the standard library: https://bitbucket.org/pypa/setuptools/issue/29/avoid-userwarnings-emitted-when-callingIf you have an unconditional import setuptools in your setup.py (as you should if using the setuptools specific options), then the fact the script isn't failing with ImportError indicates that setuptools is properly installed.You can silence the warning as follows:Only do this if you use the unconditional import that will fail completely if setuptools isn't installed :)(I'm seeing this same behaviour in a checkout from the post-merger setuptools repo, which is why I'm confident it's a setuptools bug rather than a system config problem. I expect pre-merge distribute would have the same problem)In conclusion:distutils doesn't support install_requires or entry_points, setuptools does.change from distutils.core import setup in setup.py to from setuptools import setup or refactor your setup.py to use only distutils features.I came here because I hadn't realized entry_points was only a setuptools feature. If you are here wanting to convert setuptools to distutils like me:I've now seen this in legacy tools using Python2.7, where a build (like a Dockerfile) installs an unpinned dependancy, for example pytest. PyTest has dropped Python 2.7 support, so you may need to specify version < the new package release. Or bite the bullet and convert that app to Python 3 if that is viable.

How to correctly sort a string with a number inside? [duplicate]

Michal

[How to correctly sort a string with a number inside? [duplicate]](https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside)

I have a list of strings containing numbers and I cannot find a good way to sort them.

For example I get something like this:with the sort() method.I know that I probably need to extract the numbers somehow and then sort the list but I have no idea how to do it in the most simple way.

2011-05-11 16:21:41Z

I have a list of strings containing numbers and I cannot find a good way to sort them.

For example I get something like this:with the sort() method.I know that I probably need to extract the numbers somehow and then sort the list but I have no idea how to do it in the most simple way.Perhaps you are looking for human sorting (also known as natural sorting):yieldsPS. I've changed my answer to use Toothy's implementation of natural sorting (posted in the comments here) since it is significantly faster than my original answer.If you wish to sort text with floats, then you'll need to change the regex from one that matches ints (i.e. (\d+)) to a regex that matches floats:yields

How to print Unicode character in Python?

NoobDev4iPhone

[How to print Unicode character in Python?](https://stackoverflow.com/questions/10569438/how-to-print-unicode-character-in-python)

I want to make a dictionary where English words point to Russian and French translations. How do I print out unicode characters in Python?  Also, how do you store unicode chars in a variable?

2012-05-13 05:00:57Z

I want to make a dictionary where English words point to Russian and French translations. How do I print out unicode characters in Python?  Also, how do you store unicode chars in a variable?To include Unicode characters in your Python source code, you can use Unicode escape characters in the form \u0123 in your string, and prefix the string literal with 'u'.Here's an example running in the Python interactive console:Strings declared like this are Unicode-type variables, as described in the Python Unicode documentation.If running the above command doesn't display the text correctly for you, perhaps your terminal isn't capable of displaying Unicode characters.For information about reading Unicode data from a file, see this answer:Character reading from file in PythonPrint a unicode character directly from python interpreter:Unicode character u'\u2713' is a checkmark.  The interpreter prints the checkmark on the screen.Print a unicode character from a python script:Put this in test.py:Run it like this:If it doesn't show a checkmark for you, then the problem could be elsewhere, like the terminal settings or something you are doing with stream redirection.Store unicode characters in a file:Save this to file: foo.py:Run it and pipe output to file:Open tmp.txt and look inside, you see this:Thus you have saved unicode e with a obfuscation mark on it to a file.If you're trying to print () Unicode, and getting ascii codec errors, check out this page, the TLDR of which is do export PYTHONIOENCODING=UTF-8 before firing up python (this variable controls what sequence of bytes the console tries to encode your string data as).  Internally, Python3 uses UTF-8 by default (see the Unicode HOWTO) so that's not the problem; you can just put Unicode in strings, as seen in the other answers and comments.  It's when you try and get this data out to your console that the problem happens.  Python thinks your console can only handle ascii.  Some of the other answers say, "Write it to a file, first" but note they specify the encoding (UTF-8) for doing so (so, Python doesn't change anything in writing), and then use a method for reading the file that just spits out the bytes without any regard for encoding, which is why that works.In Python 2, you declare unicode strings with a u, as in u"猫" and use decode () and encode () to translate to and from unicode, respectively.It's quite a bit easier in Python 3. A very good overview can be found here.  That presentation clarified a lot of things for me.Considering that this is the first stack overflow result when google searching this topic, it bears mentioning that prefixing u to unicode strings is optional in Python 3. (Python 2 example was copied from the top answer) Python 3 (both work):Python 2:I use Portable winpython in Windows, it includes IPython QT console, I could achieve the following.your console interpreter should support unicode in order to show unicode characters.Just one more thing that hasn't been added yetIn Python 2, if you want to print a variable that has unicode and use .format (), then do this (make the base string that is being formatted a unicode string with u'':This fixes UTF-8 printing in python:

Iterate an iterator by chunks (of n) in Python? [duplicate]

Gerenuk

[Iterate an iterator by chunks (of n) in Python? [duplicate]](https://stackoverflow.com/questions/8991506/iterate-an-iterator-by-chunks-of-n-in-python)

Can you think of a nice way (maybe with itertools) to split an iterator into chunks of given size?Therefore l=[1,2,3,4,5,6,7] with chunks(l,3) becomes an iterator [1,2,3], [4,5,6], [7]I can think of a small program to do that but not a nice way with maybe itertools.

2012-01-24 17:44:02Z

Can you think of a nice way (maybe with itertools) to split an iterator into chunks of given size?Therefore l=[1,2,3,4,5,6,7] with chunks(l,3) becomes an iterator [1,2,3], [4,5,6], [7]I can think of a small program to do that but not a nice way with maybe itertools.The grouper()  recipe from the itertools documentation's recipes comes close to what you want:It will fill up the last chunk with a fill value, though.A less general solution that only works on sequences but does handle the last chunk as desired isFinally, a solution that works on general iterators an behaves as desired isAlthough OP asks function to return chunks as list or tuple, in case you need to return iterators, then Sven Marnach's solution can be modified:Some benchmarks: http://pastebin.com/YkKFvm8bIt will be slightly more efficient only if your function iterates through elements in every chunk. This will work on any iterable. It returns generator of generators (for full flexibility). I now realize that it's basically the same as @reclosedevs solution, but without the fluff. No need for try...except as the StopIteration propagates up, which is what we want.  The next(iterable) call is needed to raise the StopIteration when the iterable is empty, since islice will continue spawning empty generators forever if you let it.It's better because it's only two lines long, yet easy to comprehend. Note that next(iterable) is put into a tuple. Otherwise, if next(iterable) itself were iterable, then itertools.chain would flatten it out. Thanks to Jeremy Brown for pointing out this issue.I was working on something today and came up with what I think is a simple solution. It is similar to jsbueno's answer, but I believe his would yield empty groups when the length of iterable is divisible by n. My answer does a simple check when the iterable is exhausted.Here's one that returns lazy chunks; use map(list, chunks(...)) if you want lists.A succinct implementation is:This works because [iter(iterable)]*n is a list containing the same iterator n times; zipping over that takes one item from each iterator in the list, which is the same iterator, with the result that each zip-element contains a group of n items. izip_longest is needed to fully consume the underlying iterable, rather than iteration stopping when the first exhausted iterator is reached, which chops off any remainder from iterable. This results in the need to filter out the fill-value. A slightly more robust implementation would therefore be:This guarantees that the fill value is never an item in the underlying iterable. Using the definition above:This implementation almost does what you want, but it has issues:(The difference is that because islice does not raise StopIteration or anything else on calls that go beyond the end of it this will yield forever; there is also the slightly tricky issue that the islice results must be consumed before this generator is iterated).To generate the moving window functionally:So this becomes:But, that still creates an infinite iterator. So, you need takewhile (or perhaps something else might be better) to limit it:I forget where I found the inspiration for this.  I've modified it a little to work with MSI GUID's in the Windows Registry:reverse doesn't apply to your question, but it's something I use extensively with this function.Here you go."Simpler is better than complex" -

a straightforward generator a few lines long can do the job. Just place it in some utilities module or so:

How to determine whether a substring is in a different string

sunny

[How to determine whether a substring is in a different string](https://stackoverflow.com/questions/7361253/how-to-determine-whether-a-substring-is-in-a-different-string)

I have a sub-string:I have another string:How do I find if substring is a subset of string using Python?

2011-09-09 11:55:39Z

I have a sub-string:I have another string:How do I find if substring is a subset of string using Python?with in: substring in string:(By the way - try to not name a variable string, since there's a Python standard library with the same name. You might confuse people if you do that in a large project, so avoiding collisions like that is a good habit to get into.)If you're looking for more than a True/False, you'd be best suited to use the re module, like:s.group() will return the string "please help me out".People mentioned string.find(), string.index(), and string.indexOf() in the comments, and I summarize them here (according to the Python Documentation):First of all there is not a string.indexOf() method. The link posted by Deviljho shows this is a JavaScript function.Second the string.find() and string.index() actually return the index of a substring. The only difference is how they handle the substring not found situation: string.find() returns -1 while string.index() raises an ValueError.Thought I would add this in case you are looking at how to do this for a technical interview where they don't want you to use Python's built-in function in or find, which is horrible, but does happen:You can also try find() method. It determines if string str occurs in string, or in a substring of string.Can also use this method Instead Of using find(), One of the easy way is the Use of  'in' as above.if 'substring' is present in 'str' then if part will execute otherwise else part will execute.

Label axes on Seaborn Barplot

Erin Shellman

[Label axes on Seaborn Barplot](https://stackoverflow.com/questions/31632637/label-axes-on-seaborn-barplot)

I'm trying to use my own labels for a Seaborn barplot with the following code:However, I get an error that:What gives?

2015-07-26 01:08:48Z

I'm trying to use my own labels for a Seaborn barplot with the following code:However, I get an error that:What gives?Seaborn's barplot returns an axis-object (not a figure). This means you can do the following:One can avoid the AttributeError brought about by set_axis_labels() method by using the matplotlib.pyplot.xlabel and matplotlib.pyplot.ylabel.matplotlib.pyplot.xlabel sets the x-axis label while the matplotlib.pyplot.ylabel sets the y-axis label of the current axis.Solution code:Output figure:You can also set the title of your chart by adding the title parameter as follows

What rules does Pandas use to generate a view vs a copy?

orome

[What rules does Pandas use to generate a view vs a copy?](https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy)

I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.If I have, for example,I understand that a query returns a copy so that something likewill have no effect on the original dataframe, df. I also understand that scalar or named slices return a view, so that assignments to these, such as or will change df. But I'm lost when it comes to more complicated cases. For example, changes df, butdoes not.Is there a simple rule that Pandas is using that I'm just missing? What's going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I'm attempting to do in the last example above)?Note: This is not the same as this question; and I have read the documentation, but am not enlightened by it. I've also read through the "Related" questions on this topic, but I'm still missing the simple rule Pandas is using, and how I'd apply it to — for example — modify the values (or a subset of values) in a dataframe that satisfy a particular query.

2014-04-25 14:44:07Z

I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.If I have, for example,I understand that a query returns a copy so that something likewill have no effect on the original dataframe, df. I also understand that scalar or named slices return a view, so that assignments to these, such as or will change df. But I'm lost when it comes to more complicated cases. For example, changes df, butdoes not.Is there a simple rule that Pandas is using that I'm just missing? What's going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I'm attempting to do in the last example above)?Note: This is not the same as this question; and I have read the documentation, but am not enlightened by it. I've also read through the "Related" questions on this topic, but I'm still missing the simple rule Pandas is using, and how I'd apply it to — for example — modify the values (or a subset of values) in a dataframe that satisfy a particular query.Here's the rules, subsequent override:Your example of chained indexingis not guaranteed to work (and thus you shoulld never do this). Instead do:as this is faster and will always workThe chained indexing is 2 separate python operations and thus cannot be reliably intercepted by pandas (you will oftentimes get a SettingWithCopyWarning, but that is not 100% detectable either). The dev docs, which you pointed, offer a much more full explanation.

Class with Object as a parameter

mythicalprogrammer

[Class with Object as a parameter](https://stackoverflow.com/questions/7375595/class-with-object-as-a-parameter)

I'm trying to translate some python code to scala code. So I'm a total noob in Python. But why do some classes have object as a parameter but never explicitly use it? What's the reasoning for having it as a parameter in the first place?Example:Thank you for your time.

2011-09-11 00:39:45Z

I'm trying to translate some python code to scala code. So I'm a total noob in Python. But why do some classes have object as a parameter but never explicitly use it? What's the reasoning for having it as a parameter in the first place?Example:Thank you for your time.In Python2 this declares Table to be a new-style class (as opposed to "classic" class).

In Python3 all classes are new-style classes, so this is no longer necessary.New style classes have a few special attributes that classic classes lack.Also, properties and super do not work with classic classes.In Python2 it is a good idea to make all classes new-style classes. (Though a lot of classes in the standard library are still classic classes, for the sake of backward-compatibility.)In general, in a statement such asFoo is being declared as a class inheriting from base classes Base1 and Base2.object is the mother of all classes in Python. It is a new-style class, so inheriting from object makes Table a new-style class.The Table class is extending a class called object. It's not an argument. The reason you may want to extend object explicitly is it turns the class into a new-style class. If you don't explicitly specify it extends object, until Python 3, it will default to being an old-style class. (Since Python 3, all classes are new-style, whether you explicitly extend object or not.)For more information on new-style and old-style classes, please see this question.Just a note that the「new-style" vs「old-style」class distinction is specific to Python 2.x; in 3.x, all classes are「new-style」.class Table and class Table(object) are no different for Python.It's not a parameter, its extending from object (which is base Class like many other languages).All it says is that it inherits whatever is defined in "object". This is the default behaviour.object is the most base type of class object defined in python.

Attributes of object can be seen as below**>>> dir(object)['class', 'delattr', 'doc', 'format', 'getattribute', 'hash', 'init', 'new', 'reduce', 'reduce_ex', 'repr', 'setattr', 'sizeof', 'str', 'subclasshook']**So Table(object)  is just inheritance.!1)Class name (object):                                  2) class name:

They both are same but first one quite better in terms of writing,it look better while inheriting other classes to another,i t looks homogeneous.How same ?

So,every thing in Python is come under object means every thing in Python has property of object,if write or don't it will understand it.

In first we explicitly tell it in second we didn't.

What is Python used for?

Jake

[What is Python used for?](https://stackoverflow.com/questions/1909512/what-is-python-used-for)

Okay, so I am fairly new at programming (knowing only HTML, CSS, and JavaScript) and I just started diving into Python. What I want to know is, what is it used for and what is it designed for?

2009-12-15 18:46:46Z

Okay, so I am fairly new at programming (knowing only HTML, CSS, and JavaScript) and I just started diving into Python. What I want to know is, what is it used for and what is it designed for?Python is a dynamic, strongly typed, object oriented, multipurpose programming language, designed to be quick (to learn, to use, and to understand), and to enforce a clean and uniform syntax.Python can be used for any programming task, from GUI programming to web programming with everything else in between. It's quite efficient, as much of its activity is done at the C level. Python is just a layer on top of C. There are libraries for everything you can think of: game programming and openGL, GUI interfaces, web frameworks, semantic web, scientific computing...Python offers a stepping stone into the world of programming. Even though Python Programming Language has been around for 25 years, it is still rising in popularity.

Some of the biggest advantage of Python are it's As a general purpose programming language, Python can be used for multiple things. Python can be easily used for small, large, online and offline projects. The best options for utilizing Python are web development, simple scripting and data analysis. Below are a few examples of what Python will let you do:Web Development:You can use Python to create web applications on many levels of complexity. There are many excellent Python web frameworks including, Pyramid, Django and Flask, to name a few.Data Analysis:Python is the leading language of choice for many data scientists. Python has grown in popularity, within this field, due to its excellent libraries including; NumPy and Pandas and its superb libraries for data visualisation like Matplotlib and Seaborn.Machine Learning:What if you could predict customer satisfaction or analyse what factors will affect household pricing or to predict stocks over the next few days, based on previous years data? There are many wonderful libraries implementing machine learning algorithms such as Scikit-Learn, NLTK and TensorFlow.Computer Vision:You can do many interesting things such as Face detection, Color detection while using Opencv and Python.Internet Of Things With Raspberry Pi:Raspberry Pi is a very tiny and affordable computer which was developed for education and has gained enormous popularity among hobbyists with do-it-yourself hardware and automation. You can even build a robot and automate your entire home. Raspberry Pi can be used as the brain for your robot in order to perform various actions and/or react to the environment. The coding on a Raspberry Pi can be performed using Python. The Possibilities are endless!Game Development:Create a video game using module Pygame. Basically, you use Python to write the logic of the game. PyGame applications can run on Android devices.Web Scraping:If you need to grab data from a website but the site does not have an API to expose data, use Python to scraping data.Writing Scripts:If you're doing something manually and want to automate repetitive stuff, such as emails, it's not difficult to automate once you know the basics of this language.Browser Automation:Perform some neat things such as opening a browser and posting a Facebook status, you can do it with Selenium with Python.GUI Development:Build a GUI application (desktop app) using Python modules Tkinter, PyQt to support it.Rapid Prototyping:Python has libraries for just about everything. Use it to quickly built a (lower-performance, often less powerful) prototype. Python is also great for validating ideas or products for established companies and start-ups alike.Python can be used in so many different projects. If you're a programmer looking for a new language, you want one that is growing in popularity. As a newcomer to programming, Python is the perfect choice for learning quickly and easily.You can check out the Wikipedia Entry for Python.  One way that you can leverage Python is by using it on Google's AppEngine.

How to get the latest file in a folder using python

garlapak

[How to get the latest file in a folder using python](https://stackoverflow.com/questions/39327032/how-to-get-the-latest-file-in-a-folder-using-python)

I need to get the latest file of a folder using python. While using the code:I am getting the below error:  FileNotFoundError: [WinError 2] The system cannot find the file specified: 'a'

2016-09-05 08:58:23Z

I need to get the latest file of a folder using python. While using the code:I am getting the below error:  FileNotFoundError: [WinError 2] The system cannot find the file specified: 'a'Whatever is assigned to the files variable is incorrect. Use the following code.is quite incomplete code. What is files? It probably is a list of file names, coming out of os.listdir().But this list lists only the filename parts (a. k. a. "basenames"), because their path is common. In order to use it correctly, you have to combine it with the path leading to it (and used to obtain it).Such as (untested):I would suggest using glob.iglob() instead of the glob.glob(), as it is more efficient.Which means glob.iglob() will be more efficient.I mostly use below code to find the latest file matching to my pattern:LatestFile = max(glob.iglob(fileNamePattern),key=os.path.getctime)NOTE:

There are variants of max function, In case of finding the latest file we will be using below variant:

max(iterable, *[, key, default])which needs iterable so your first parameter should be iterable.

In case of finding max of nums we can use beow variant : max (num1, num2, num3, *args[, key])Try to sort items by creation time. Example below sorts files in a folder and gets first element which is latest.A much faster method on windows (0.05s), call a bat script that does this:get_latest.batwhere \\directory\in\question is the directory you want to investigate.get_latest.pyif it finds a file stdout is the path and stderr is None.Use stdout.decode("utf-8").rstrip() to get the usable string representation of the file name.(Edited to improve answer)First define a function get_latest_fileYou may also use a docstring !If you use Python 3, you can use iglob instead.Complete code to return the name of latest file:I have tried to use the above suggestions and my program crashed, than I figured out the file I'm trying to identify was used and when trying to use 'os.path.getctime' it crashed.

what finally worked for me was:this codes gets the uncommon object between the two sets of file lists

its not the most elegant, and if multiple files are created at the same time it would probably won't be stableI lack the reputation to comment but ctime from Marlon Abeykoons response did not give the correct result for me. Using mtime does the trick though. (key=os.path.getmtime))I found two answers for that problem:python os.path.getctime max does not return latest

Difference between python - getmtime() and getctime() in unix system

Writing a dict to txt file and reading it back?

Radioactive Head

[Writing a dict to txt file and reading it back?](https://stackoverflow.com/questions/11026959/writing-a-dict-to-txt-file-and-reading-it-back)

I am trying to write a dictionary to a txt file. Then read the dict values by typing the keys with raw_input. I feel like I am just missing one step but I have been looking for a while now.I get this errorMy code:

2012-06-14 05:07:15Z

I am trying to write a dictionary to a txt file. Then read the dict values by typing the keys with raw_input. I feel like I am just missing one step but I have been looking for a while now.I get this errorMy code:Your code is almost right!  You are right, you are just missing one step.  When you read in the file, you are reading it as a string; but you want to turn the string back into a dictionary.The error message you saw was because self.whip was a string, not a dictionary.I first wrote that you could just feed the string into dict() but that doesn't work!  You need to do something else.Here is the simplest way: feed the string into eval().  Like so:You can do it in one line, but I think it looks messy this way:But eval() is sometimes not recommended.  The problem is that eval() will evaluate any string, and if someone tricked you into running a really tricky string, something bad might happen.  In this case, you are just running eval() on your own file, so it should be okay.But because eval() is useful, someone made an alternative to it that is safer.  This is called literal_eval and you get it from a Python module called ast.ast.literal_eval() will only evaluate strings that turn into the basic Python types, so there is no way that a tricky string can do something bad on your computer.Actually, best practice in Python is to use a with statement to make sure the file gets properly closed.  Rewriting the above to use a with statement:In the most popular Python, known as "CPython", you usually don't need the with statement as the built-in "garbage collection" features will figure out that you are done with the file and will close it for you.  But other Python implementations, like "Jython" (Python for the Java VM) or "PyPy" (a really cool experimental system with just-in-time code optimization) might not figure out to close the file for you.  It's good to get in the habit of using with, and I think it makes the code pretty easy to understand.Have you tried the json module? JSON format is very similar to python dictionary. And it's human readable/writable:This code dumps to a text fileAlso you can load from a JSON file:To store Python objects in files, use the pickle module:Notice that I never set b = a, but instead pickled a to a file and then unpickled it into b.As for your error:self.whip was a dictionary object. deed.txt contains text, so when you load the contents of deed.txt into self.whip, self.whip becomes the string representation of itself.You'd probably want to evaluate the string back into a Python object:Notice how eval sounds like evil. That's intentional. Use the pickle module instead.I created my own functions which work really nicely:It will store the keyname first, followed by all values. Note that in this case my dict contains integers so that's why it converts to int. This is most likely the part you need to change for your situation.You can iterate through the key-value pair and write it into fileHi there is a way to write and read the dictionary to file you can turn your dictionary to JSON format and read and write quickly just do this :To write your date:and to read your data:

Python Requests and persistent sessions

ChrisGuest

[Python Requests and persistent sessions](https://stackoverflow.com/questions/12737740/python-requests-and-persistent-sessions)

I am using the requests module (version 0.10.0 with Python 2.5).

I have figured out how to submit data to a login form on a website and retrieve the session key, but I can't see an obvious way to use this session key in subsequent requests.

Can someone fill in the ellipsis in the code below or suggest another approach?

2012-10-05 00:06:36Z

I am using the requests module (version 0.10.0 with Python 2.5).

I have figured out how to submit data to a login form on a website and retrieve the session key, but I can't see an obvious way to use this session key in subsequent requests.

Can someone fill in the ellipsis in the code below or suggest another approach?You can easily create a persistent session using:After that, continue with your requests as you would:For more about sessions: https://requests.kennethreitz.org/en/master/user/advanced/#session-objectsthe other answers help to understand how to maintain such a session. Additionally, I want to provide a class which keeps the session maintained over different runs of a script (with a cache file). This means a proper "login" is only performed when required (timout or no session exists in cache). Also it supports proxy settings over subsequent calls to 'get' or 'post'. It is tested with Python3.Use it as a basis for your own code. The following snippets are release with GPL v3A code snippet for using the above class may look like this:Check out my answer in this similar question:python: urllib2 how to send cookie with urlopen requestEDIT:I see I've gotten a few downvotes for my answer, but no explaining comments. I'm guessing it's because I'm referring to the urllib libraries instead of requests. I do that because the OP asks for help with requests or for someone to suggest another approach.The documentation says that get takes in an optional cookies argument allowing you to specify cookies to use:from the docs:http://docs.python-requests.org/en/latest/user/quickstart/#cookiesUpon trying all the answers above, I found that using RequestsCookieJar instead of the regular CookieJar for subsequent requests fixed my problem. snippet to retrieve json data, password protectedThis will work for you in Python;Save only required cookies and reuse them.

Get Element value with minidom with Python

RailsSon

[Get Element value with minidom with Python](https://stackoverflow.com/questions/317413/get-element-value-with-minidom-with-python)

I am creating a GUI frontend for the Eve Online API in Python.I have successfully pulled the XML data from their server.I am trying to grab the value from a node called "name":This seems to find the node, but the output is below:How could I get it to print the value of the node?

2008-11-25 13:57:02Z

I am creating a GUI frontend for the Eve Online API in Python.I have successfully pulled the XML data from their server.I am trying to grab the value from a node called "name":This seems to find the node, but the output is below:How could I get it to print the value of the node?It should just beProbably something like this if it's the text part you want...The text part of a node is considered a node in itself placed as a child-node of the one you asked for. Thus you will want to go through all its children and find all child nodes that are text nodes. A node can have several text nodes; eg.You want both 'blabla' and 'znylpx'; hence the " ".join(). You might want to replace the space with a newline or so, or perhaps by nothing.you can use something like this.It worked out for meThe above answer is correct, namely:However for me, like others, my value was further down the tree:To find this I used the following:Running this for my simple SVG file created with Inkscape this gave me:I used xml.dom.minidom, the various fields are explained on this page, MiniDom Python.I know this question is pretty old now, but I thought you might have an easier time with ElementTreeI know that's not super specific, but I just discovered it, and so far it's a lot easier to get my head around than the minidom (since so many nodes are essentially white space).For instance, you have the tag name and the actual text together, just as you'd probably expect:I had a similar case, what worked for me was:name.firstChild.childNodes[0].dataXML is supposed to be simple and it really is and I don't know why python's minidom did it so complicated... but it's how it's madeHere is a slightly modified answer of Henrik's for multiple nodes (ie. when getElementsByTagName returns more than one instance)The question has been answered, my contribution consists in clarifying one thing that may confuse beginners:Some of the suggested and correct answers used firstChild.data and others used firstChild.nodeValue instead. In case you are wondering what is the different between them, you should remember they do the same thing because nodeValue is just an alias for data.The reference to my statement can be found as a comment on the source code of minidom:

Installing Numpy on 64bit Windows 7 with Python 2.7.3 [closed]

Chris

[Installing Numpy on 64bit Windows 7 with Python 2.7.3 [closed]](https://stackoverflow.com/questions/11200137/installing-numpy-on-64bit-windows-7-with-python-2-7-3)

It looks like the only 64 bit windows installer for Numpy is for Numpy version 1.3.0 which only works with Python 2.6http://sourceforge.net/projects/numpy/files/NumPy/It strikes me as strange that I would have to roll back to Python 2.6 to use Numpy on Windows, which makes me think I'm missing something.Am I?

2012-06-26 02:45:57Z

It looks like the only 64 bit windows installer for Numpy is for Numpy version 1.3.0 which only works with Python 2.6http://sourceforge.net/projects/numpy/files/NumPy/It strikes me as strange that I would have to roll back to Python 2.6 to use Numpy on Windows, which makes me think I'm missing something.Am I?Try the (unofficial) binaries in this site:http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpyYou can get the newest numpy x64 with or without Intel MKL libs for Python 2.7 or Python 3.Assuming you have python 2.7 64bit on your computer and have downloaded numpy from here, follow the steps below (changing numpy‑1.9.2+mkl‑cp27‑none‑win_amd64.whl as appropriate).Download numpy-1.9.2+mkl-cp27-none-win32.whl from http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy .  Copy the file to C:\Python27\Scripts  Run cmd from the above location and typeYou will hopefully get the below output:Hope that works for you.EDIT 1

Adding @oneleggedmule 's suggestion:You can also run the following command in the cmd:Basically, writing pip alone also works perfectly (as in the original answer). Writing the version 2.7 can also be done for the sake of clarity or specification.The (unofficial) binaries (http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy)  worked for me.

I've tried Mingw, Cygwin, all failed due to varies reasons. I am on Windows 7 Enterprise, 64bit.  You may also try this, anaconda

http://continuum.io/downloadsBut you need to modify your environment variable PATH, so that the anaconda folder is before the original Python folder.It is not improbable, that programmers looking for python on windows, also use the Python Tools for Visual Studio. In this case it is easy to install additional packages, by taking advantage of the included "Python Environment" Window. "Overview" is selected within the window as default. You can select "Pip" there.Then you can install numpy without additional work by entering numpy into the seach window. The coresponding "install numpy" instruction is already suggested.Nevertheless I had 2 easy to solve Problems in the beginning:Finally the installation was done. It took some time (5 minutes), so don't cancel the process to early.

pycharm convert tabs to spaces automatically

Vaibhav Mishra

[pycharm convert tabs to spaces automatically](https://stackoverflow.com/questions/11816147/pycharm-convert-tabs-to-spaces-automatically)

I am using pycharm IDE for python development it works perfectly fine for django code so suspected that converting tabs to spaces is default behaviour, however in python IDE is giving errors everywhere because it can't convert tabs to spaces automatically is there a way to achieve this.

2012-08-05 11:49:35Z

I am using pycharm IDE for python development it works perfectly fine for django code so suspected that converting tabs to spaces is default behaviour, however in python IDE is giving errors everywhere because it can't convert tabs to spaces automatically is there a way to achieve this.Change the code style to use spaces instead of tabs:Then select a folder you want to convert in the Project View and use Code | Reformat Code.For selections, you can also convert the selection using the "To spaces" function. I usually just use it via the ctrl-shift-A then find "To Spaces" from there.This only converts the tabs without changing anything else:ctrl + shift + A => open pop window to select options, select to spaces to convert all tabs as space, or to tab to convert all spaces as tab.  Open preferences, in macOS ⌘; or in Windows/Linux Ctrl + Alt + S.Go to Editor -> Code Style -> Python, and if you want to follow PEP-8, choose Tab size: 4, Indent: 4, and Continuation indent: 8 as shown below:Apply the changes, and click on OK.Option 1: You can choose in the navigation bar: Edit -> Convert Indent -> To Spaces. (see image below)Option 2: You can execute "To Spaces" action by running the Find Action shortcut: ⌘⇧A on macOS or ctrl⇧A on Windows/Linux. Then type "To Spaces", and run the action as shown in the image below.ctr+alt+shift+L ->  reformat whole file :)For me it was having a file called ~/.editorconfig that was overriding my tab settings. I removed that (surely that will bite me again someday) but it fixed my pycharm issue

Understanding Python's「is」operator

aniskhan001

[Understanding Python's「is」operator](https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator)

What does it really mean?I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.I need a clarification. Here is my code.

2012-11-30 17:39:14Z

What does it really mean?I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.I need a clarification. Here is my code.You misunderstood what the is operator tests. It tests if two variables point the same object, not if two variables have the same value.From the documentation for the is operator:Use the == operator instead:This prints True. x and y are two separate lists:If you use the id() function you'll see that x and y have different identifiers:but if you were to assign y to x then both point to the same object:and is shows both are the same object, it returns True.Remember that in Python, names are just labels referencing values; you can have multiple names point to the same object. is tells you if two names point to one and the same object. == tells you if two names refer to objects that have the same value.Another duplicate was asking why two equal strings are generally not identical, which isn't really answered here:So, why aren't they the same string? Especially given this:Let's put off the second part for a bit. How could the first one be true?The interpreter would have to have an "interning table", a table mapping string values to string objects, so every time you try to create a new string with the contents 'abc', you get back the same object. Wikipedia has a more detailed discussion on how interning works.And Python has a string interning table; you can manually intern strings with the sys.intern method.In fact, Python is allowed to automatically intern any immutable types, but not required to do so. Different implementations will intern different values.CPython (the implementation you're using if you don't know which implementation you're using) auto-interns small integers and some special singletons like False, but not strings (or large integers, or small tuples, or anything else). You can see this pretty easily:OK, but why were z and w identical?That's not the interpreter automatically interning, that's the compiler folding values.If the same compile-time string appears twice in the same module (what exactly this means is hard to define—it's not the same thing as a string literal, because r'abc', 'abc', and 'a' 'b' 'c' are all different literals but the same string—but easy to understand intuitively), the compiler will only create one instance of the string, with two references.In fact, the compiler can go even further: 'ab' + 'c' can be converted to 'abc' by the optimizer, in which case it can be folded together with an 'abc' constant in the same module.Again, this is something Python is allowed but not required to do. But in this case, CPython always folds small strings (and also, e.g., small tuples). (Although the interactive interpreter's statement-by-statement compiler doesn't run the same optimization as the module-at-a-time compiler, so you won't see exactly the same results interactively.)So, what should you do about this as a programmer?Well… nothing. You almost never have any reason to care if two immutable values are identical. If you want to know when you can use a is b instead of a == b, you're asking the wrong question. Just always use a == b except in two cases:Prompted by a duplicate question, this analogy might work:is only returns true if they're actually the same object. If they were the same, a change to one would also show up in the other. Here's an example of the difference.is and is not are the two identity operators in Python. is operator does not compare the values of the variables, but compares the identities of the variables. Consider this:The above example shows you that the identity (can also be the memory address in Cpython) is different for both a and b (even though their values are the same). That is why when you say a is b it returns false due to the mismatch in the identities of both the operands. However when you say a == b, it returns true because the == operation only verifies if both the operands have the same value assigned to them.Interesting example (for the extra grade):In the above example, even though a and b are two different variables, a is b returned True. This is because the type of a is int which is an immutable object. So python (I guess to save memory) allocated the same object to b when it was created with the same value. So in this case, the identities of the variables matched and a is b turned out to be True.This will apply for all immutable objects:Hope that helps.As you can check here to a small integers. Numbers above 257 are not an small ints, so it is calculated as a different object.It is better to use == instead in this case.Further information is here: http://docs.python.org/2/c-api/int.htmlx is y is same as id(x) == id(y), comparing identity of objects.  As @tomasz-kurgan pointed out in the comment below is operator behaves unusually with certain objects.  E.g.Ref;

https://docs.python.org/2/reference/expressions.html#is-not

https://docs.python.org/2/reference/expressions.html#id24X points to an array, Y points to a different array. Those arrays are identical, but the is operator will look at those pointers, which are not identical.It compares object identity, that is, whether the variables refer to the same object in memory. It's like the == in Java or C (when comparing pointers).A simple example with fruits   Output:If you try The output is different:That's because the == operator compares just the content of the variable. To compare the identities of 2 variable use the is operator To print the identification number: The is operator is nothing but an English version of ==.

Because the IDs of the two lists are different so the answer is false.

You can try:*Because the IDs of both the list would be same

UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 1

Markum

[UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 1](https://stackoverflow.com/questions/10561923/unicodedecodeerror-ascii-codec-cant-decode-byte-0xef-in-position-1)

I'm having a few issues trying to encode a string to UTF-8. I've tried numerous things, including using string.encode('utf-8') and unicode(string), but I get the error:This is my string:I don't see what's going wrong, any idea?Edit: The problem is that printing the string as it is does not show properly. Also, this error when I try to convert it:

2012-05-12 07:39:52Z

I'm having a few issues trying to encode a string to UTF-8. I've tried numerous things, including using string.encode('utf-8') and unicode(string), but I get the error:This is my string:I don't see what's going wrong, any idea?Edit: The problem is that printing the string as it is does not show properly. Also, this error when I try to convert it:This is to do with the encoding of your terminal not being set to UTF-8.  Here is my terminalOn my terminal the example works with the above, but if I get rid of the LANG setting then it won't workConsult the docs for your linux variant to discover how to make this change permanent.try:edit:'(\xef\xbd\xa1\xef\xbd\xa5\xcf\x89\xef\xbd\xa5\xef\xbd\xa1)\xef\xbe\x89'.decode('utf-8') gives u'(\uff61\uff65\u03c9\uff65\uff61)\uff89', which is correct.so your problem must be at some oter place, possibly if you try to do something with it were there is an implicit conversion going on (could be printing, writing to a stream...)to say more we'll need to see some code.My +1 to mata's comment at https://stackoverflow.com/a/10561979/1346705 and to the Nick Craig-Wood's demonstration.  You have decoded the string correctly.  The problem is with the print command as it converts the Unicode string to the console encoding, and the console is not capable to display the string.  Try to write the string into a file and look at the result using some decent editor that supports Unicode:Then you will see (｡･ω･｡)ﾉ.Try setting the system default encoding as utf-8 at the start of the script, so that all strings are encoded using that.If you are working on a remote host, look at /etc/ssh/ssh_config on your local PC.When this file contains a line:comment it out with adding # at the head of line. It might help.With this line, ssh sends language related environment variables of your PC to the remote host. It causes a lot of problems.No problems with my terminal. The above answers helped me looking in the right directions but it didn't work for me until I added 'ignore':As indicated in the comment below, this may lead to undesired results. OTOH it also may just do the trick well enough to get things working and you don't care about losing some characters.It's fine to use the below code in the top of your script as Andrei Krasutski suggested.But I will suggest you to also add # -*- coding: utf-8 -* line at very top of the script.Omitting it throws below error in my case when I try to execute basic.py.The following is the code present in basic.py which throws above error.Then I added # -*- coding: utf-8 -*- line at very top and executed. It worked.Thanks.this works for ubuntu 15.10:It looks like your string is encoded to utf-8, so what exactly is the problem?  Or what are you trying to do here..?In my case, it was caused by my Unicode file being saved with a "BOM". To solve this, I cracked open the file using BBEdit and did a "Save as..." choosing for encoding "Unicode (UTF-8)" and not what it came with which was "Unicode (UTF-8, with BOM)"I was getting the same type of error, and I found that the console is not capable of displaying the string in another language. Hence I made the below code changes to set default_charset as UTF-8. This is the best answer:

https://stackoverflow.com/a/4027726/2159089in linux:so sys.stdout.encoding is OK.BOM, it's so often BOM for mevi the file, useand save it. That nearly always fixes it in my caseI had the same error, with URLs containing non-ascii chars (bytes with values > 128)Worked for me, in Python 2.7, I suppose this assignment changed 'something' in the str internal representation--i.e., it forces the right decoding of the backed byte sequence in url and finally puts the string into a utf-8 str with all the magic in the right place.

Unicode in Python is black magic for me.

Hope usefuli solve that problem changing in the file settings.py with 'ENGINE': 'django.db.backends.mysql',   don´t use 'ENGINE': 'mysql.connector.django',Just convert the text explicitly to string using str(). Worked for me.

Does Python optimize away a variable that's only used as a return value?

Jayesh

[Does Python optimize away a variable that's only used as a return value?](https://stackoverflow.com/questions/43390869/does-python-optimize-away-a-variable-thats-only-used-as-a-return-value)

Is there any ultimate difference between the following two code snippets? The first assigns a value to a variable in a function and then returns that variable. The second function just returns the value directly.Does Python turn them into equivalent bytecode? Is one of them faster?Case 1:Case 2:

2017-04-13 11:20:14Z

Is there any ultimate difference between the following two code snippets? The first assigns a value to a variable in a function and then returns that variable. The second function just returns the value directly.Does Python turn them into equivalent bytecode? Is one of them faster?Case 1:Case 2:No, it doesn't.The compilation to CPython byte code is only passed through a small peephole optimizer that is designed to do only basic optimizations (See test_peepholer.py in the test suite for more on these optimizations).To take a look at what's actually going to happen, use dis* to see the instructions generated. For the first function, containing the assignment:While, for the second function:Two more (fast) instructions are used in the first: STORE_FAST and LOAD_FAST. These make a quick store and grab of the value in the fastlocals array of the current execution frame. Then, in both cases, a RETURN_VALUE is performed. So, the second is ever so slightly faster due to less commands needed to execute.In general, be aware that the CPython compiler is conservative in the optimizations it performs. It isn't and doesn't try to be as smart as other compilers (which, in general, also have much more information to work with). The main design goal, apart from obviously being correct, is to a) keep it simple and b) be as swift as possible in compiling these so you don't even notice that a compilation phase exists.In the end, you shouldn't trouble yourself with small issues like this one. The benefit in speed is tiny, constant and, dwarfed by the overhead introduced by the fact that Python is interpreted. *dis is a little Python module that dis-assembles your code, you can use it to see the Python bytecode that the VM will execute. Note: As also stated in a comment by @Jorn Vernee, this is specific to the CPython implementation of Python. Other implementations might do more aggressive optimizations if they so desire, CPython doesn't.Both are basically the same except that in the first case the object 42 is simply aassigned to a variable named a or, in other words, names (i.e. a) refer to values (i.e. 42) . It doesn't do any assignment technically, in the sense that it never copies any data.While returning, this named binding a is returned in the first case while the object 42 is return in the second case.For more reading, refer this great article by Ned Batchelder

How to implement __iter__(self) for a container object (Python)

skyeagle

[How to implement __iter__(self) for a container object (Python)](https://stackoverflow.com/questions/4019971/how-to-implement-iter-self-for-a-container-object-python)

I have written a custom container object.According to this page, I need to implement this method on my object:However, upon following up the link to Iterator Types in the Python reference manual, there are no examples given of how to implement your own.Can someone post a snippet (or link to a resource), that shows how to do this?The container I am writing, is a map (i.e. stores values by unique keys).

dicts can be iterated like this:In this case I need to be able to return two elements (a tuple?) in the iterator. 

It is still not clear how to implement such an iterator (despite the several answers that have been kindly provided). Could someone please shed some more light on how to implement an iterator for a map-like container object? (i.e. a custom class that acts like a dict)?

2010-10-26 00:59:23Z

I have written a custom container object.According to this page, I need to implement this method on my object:However, upon following up the link to Iterator Types in the Python reference manual, there are no examples given of how to implement your own.Can someone post a snippet (or link to a resource), that shows how to do this?The container I am writing, is a map (i.e. stores values by unique keys).

dicts can be iterated like this:In this case I need to be able to return two elements (a tuple?) in the iterator. 

It is still not clear how to implement such an iterator (despite the several answers that have been kindly provided). Could someone please shed some more light on how to implement an iterator for a map-like container object? (i.e. a custom class that acts like a dict)?I normally would use a generator function. Each time you use a yield statement, it will add an item to the sequence.The following will create an iterator that yields five, and then every item in some_list.Pre-3.3, yield from didn't exist, so you would have to do:Another option is to inherit from the appropriate abstract base class from the `collections module as documented here.In case the container is its own iterator, you can inherit from 

collections.Iterator. You only need to implement the next method then.An example is:While you are looking at the collections module, consider inheriting from Sequence, Mapping or another abstract base class if that is more appropriate. Here is an example for a Sequence subclass:NB: Thanks to Glenn Maynard for drawing my attention to the need to clarify the difference between iterators on the one hand and containers that are iterables rather than iterators on the other.usually __iter__() just return self if you have already define the next() method (generator object):here is a Dummy example of a generator :but __iter__() can also be used like this:

    http://mail.python.org/pipermail/tutor/2006-January/044455.htmlIf your object contains a set of data you want to bind your object's iter to, you can cheat and do this:The "iterable interface" in python consists of two methods __next__() and __iter__(). The __next__ function is the most important, as it defines the iterator behavior - that is, the function determines what value should be returned next. The __iter__() method is used to reset the starting point of the iteration. Often, you will find that  __iter__() can just return self when __init__() is used to set the starting point.See the following code for defining a Class Reverse which implements the "iterable interface" and defines an iterator over any instance from any sequence class. The __next__() method starts at the end of the sequence and returns values in reverse order of the sequence. Note that instances from a class implementing the "sequence interface" must define a __len__() and a __getitem__() method. To answer the question about mappings: your provided __iter__ should iterate over the keys of the mapping. The following is a simple example that creates a mapping x -> x * x and works on Python3 extending the ABC mapping.In case you don't want to inherit from dict as others have suggested, here is direct answer to the question on how to implement __iter__ for a crude example of a custom dict:That uses a generator, which is well described here.Since we're inheriting from Mapping, you need to also implement __getitem__ and __len__:example for inhert from dict, modify its iter, for example, skip key 2 when in for loopOne option that might work for some cases is to make your custom class inherit from dict. This seems like a logical choice if it acts like a dict; maybe it should be a dict. This way, you get dict-like iteration for free.Output:

Python split string based on regex

Ωmega

[Python split string based on regex](https://stackoverflow.com/questions/13209288/python-split-string-based-on-regex)

What is the best way to split a string like "HELLO there HOW are YOU" by upper case words (in Python)? So I'd end up with an array like such: results = ['HELLO there', 'HOW are', 'YOU']EDIT: I have tried: It doesn't seem to work, though.

2012-11-03 12:41:57Z

What is the best way to split a string like "HELLO there HOW are YOU" by upper case words (in Python)? So I'd end up with an array like such: results = ['HELLO there', 'HOW are', 'YOU']EDIT: I have tried: It doesn't seem to work, though.I suggestCheck this demo.You could use a lookahead:This will split at every space that is followed by a string of upper-case letters which end in a word-boundary.Note that the square brackets are only for readability and could as well be omitted.If it is enough that the first letter of a word is upper case (so if you would want to split in front of Hello as well) it gets even easier:Now this splits at every space followed by any upper-case letter.You don't need split, but rather findall:Your question contains the string literal "\b[A-Z]{2,}\b",

but that \b will mean backspace, because there is no r-modifier.Try: r"\b[A-Z]{2,}\b".

Can I run Keras model on gpu?

Ryan 

[Can I run Keras model on gpu?](https://stackoverflow.com/questions/45662253/can-i-run-keras-model-on-gpu)

I'm running a Keras model, with a submission deadline of 36 hours, if I train my model on the cpu it will take approx 50 hours, is there a way to run Keras on gpu?I'm using Tensorflow backend and running it on my Jupyter notebook, without anaconda installed.

2017-08-13 15:58:58Z

I'm running a Keras model, with a submission deadline of 36 hours, if I train my model on the cpu it will take approx 50 hours, is there a way to run Keras on gpu?I'm using Tensorflow backend and running it on my Jupyter notebook, without anaconda installed.Yes you can run keras models on GPU. Few things you will have to check first.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))ORoutput will be something like this:Once all this is done your model will run on GPU:To Check if keras(>=2.1.1) is using GPU:All the best.Sure. I suppose that you have already installed TensorFlow for GPU. You need to add the following block after importing keras. I am working on a machine which have 56 core cpu, and a gpu.Of course, this usage enforces my machines maximum limits. You can decrease cpu and gpu consumption values.2.0 Compatible Answer: While above mentioned answer explain in detail on how to use GPU on Keras Model, I want to explain how it can be done for Tensorflow Version 2.0.To know how many GPUs are available, we can use the below code:To find out which devices your operations and tensors are assigned to, 

put tf.debugging.set_log_device_placement(True) as the first statement of your program. Enabling device placement logging causes any Tensor allocations or operations to be printed. For example, running the below code:gives the Output shown below:For more information, refer this linkOf course. if you are running on Tensorflow or CNTk backends, your code will run on your GPU devices defaultly.But if Theano backends, you can use following 

efficient circular buffer?

jedierikb

[efficient circular buffer?](https://stackoverflow.com/questions/4151320/efficient-circular-buffer)

I want to create an efficient circular buffer in python (with the goal of taking averages of the integer values in the buffer).Is this an efficient way to use a list to collect values?What would be more efficient (and why)?

2010-11-11 04:17:18Z

I want to create an efficient circular buffer in python (with the goal of taking averages of the integer values in the buffer).Is this an efficient way to use a list to collect values?What would be more efficient (and why)?I would use collections.deque with a maxlen argThere is a recipe in the docs for deque that is similar to what you want. My assertion that it's the most efficient rests entirely on the fact that it's implemented in C by an incredibly skilled crew that is in the habit of cranking out top notch code.popping from the head of a list causes the whole list to be copied, so is inefficientYou should instead use a list/array of fixed size and an index which moves through the buffer as you add/remove itemsPython's deque is slow. You can also use numpy.roll instead

How do you rotate the numbers in an numpy array of shape (n,) or (n,1)?In this benchmark, deque is 448ms. Numpy.roll is 29ms

http://scimusing.wordpress.com/2013/10/25/ring-buffers-in-pythonnumpy/Based on MoonCactus's answer, here is a circularlist class. The difference with his version is that here  c[0] will always give the oldest-appended element, c[-1] the latest-appended element, c[-2] the penultimate... This is more natural for applications.Class:[Edited]: Added optional data parameter to allow initialization from existing lists, e.g.:ok with the use of deque class, but for the requeriments of the question (average) this is my solution:Although there are already a great number of great answers here, I could not find any direct comparison of timings for the options mentioned. Therefore, please find my humble attempt at a comparison below. For testing purposes only, the class can switch between a list-based buffer, a collections.deque-based buffer, and a Numpy.roll-based buffer.Note that the update method adds only one value at a time, to keep it simple.On my system this yields:How about the solution from the Python Cookbook, including a reclassification of the ring buffer instance when it becomes full?Credit: Sébastien KeimYou can also see this quite old Python recipe.Here is my own version with NumPy array:This one does not require any library. It grows a list and then cycle within by index.The footprint is very small (no library), and it runs twice as fast as dequeue at least. This is good to compute moving averages indeed, but be aware that the items are not kept sorted by age as above.To get the average value, e.g.:Results in:This is about 1/3 the time of the equivalent with dequeue.I've had this problem before doing serial programming. At the time just over a year ago, I couldn't find any efficient implementations either, so I ended up writing one as a C extension and it's also available on pypi under an MIT license. It's super basic, only handles buffers of 8-bit signed chars, but is of flexible length, so you can use Struct or something on top of it if you need something other than chars. I see now with a google search that there are several options these days though, so you might want to look at those too.You answer is not right.

Circular buffer main have two priciples（https://en.wikipedia.org/wiki/Circular_buffer)your code below:Let's consider a situation that the list is full, by use your code:now we append 6, the list is changed tothe items expect 1 in list has changed their positionyour code is a queue, not a circle buffer.The answer of Basj, I think is the most efficent one.By the way, a circle buffer can imporve the performance of the operation 

to add a item.From Github:https://github.com/heineman/python-data-structures/blob/master/2.%20Ubiquitous%20Lists/circBuffer.pyThe original question was: "efficient" circular buffer.

According to this efficiency asked for, the answer from aaronasterling seems to be definitively correct.

Using a dedicated class programmed in Python and comparing time processing with collections.deque shows a x5.2 times acceleration with deque!

Here is very simple code to test this:To transform a deque into a list, just use:You will then get O(1) random access to the deque items. Of course, this is only valuable if you need to do many random accesses to the deque after having set it once.This is applying the same principal to some buffers intended to hold the most recent text messages.You can check out this circular buffer based on a predefined size numpy array. The idea is that you create a buffer(allocate memory for the numpy array) and later append to it. The insertion of data and retrieval is very fast. I have created this module for a similar purpose as you need. In my case, I have a device that generates integer data. I read the data and put it in the circular buffer for future analysis and processing.

deciding among subprocess, multiprocessing, and thread in Python?

Eric O Lebigot

[deciding among subprocess, multiprocessing, and thread in Python?](https://stackoverflow.com/questions/2629680/deciding-among-subprocess-multiprocessing-and-thread-in-python)

I'd like to parallelize my Python program so that it can make use of multiple processors on the machine that it runs on.  My parallelization is very simple, in that all the parallel "threads" of the program are independent and write their output to separate files.  I don't need the threads to exchange information but it is imperative that I know when the threads finish since some steps of my pipeline depend on their output.Portability is important, in that I'd like this to run on any Python version on Mac, Linux, and Windows. Given these constraints, which is the most appropriate Python module for implementing this? I am trying to decide between thread, subprocess, and multiprocessing, which all seem to provide related functionality.Any thoughts on this?  I'd like the simplest solution that's portable.

2010-04-13 13:01:31Z

I'd like to parallelize my Python program so that it can make use of multiple processors on the machine that it runs on.  My parallelization is very simple, in that all the parallel "threads" of the program are independent and write their output to separate files.  I don't need the threads to exchange information but it is imperative that I know when the threads finish since some steps of my pipeline depend on their output.Portability is important, in that I'd like this to run on any Python version on Mac, Linux, and Windows. Given these constraints, which is the most appropriate Python module for implementing this? I am trying to decide between thread, subprocess, and multiprocessing, which all seem to provide related functionality.Any thoughts on this?  I'd like the simplest solution that's portable.multiprocessing is a great Swiss-army knife type of module.  It is more general than threads, as you can even perform remote computations.  This is therefore the module I would suggest you use.The subprocess module would also allow you to launch multiple processes, but I found it to be less convenient to use than the new multiprocessing module.Threads are notoriously subtle, and, with CPython, you are often limited to one core, with them (even though, as noted in one of the comments, the Global Interpreter Lock (GIL) can be released in C code called from Python code).I believe that most of the functions of the three modules you cite can be used in a platform-independent way.  On the portability side, note that multiprocessing only comes in standard since Python 2.6 (a version for some older versions of Python does exist, though).  But it's a great module!For me this is actually pretty simple:subprocess is for running other executables --- it's basically a wrapper around os.fork() and os.execve() with some support for optional plumbing (setting up PIPEs to and from the subprocesses.

Obviously you could other inter-process communications (IPC) mechanisms, such as sockets, or Posix or SysV shared memory. But you're going to be limited to whatever interfaces and IPC channels are supported by the programs you're calling.Commonly, one uses any subprocess synchronously --- simply calling some external utility and reading back its output or awaiting its completion (perhaps reading its results from a temporary file, or after it's posted them to some database).However one can spawn hundreds of subprocesses and poll them.  My own personal favorite utility classh does exactly that.

The biggest disadvantage of the subprocess module is that I/O support is generally blocking.  There is a draft PEP-3145 to fix that in some future version of Python 3.x and an alternative asyncproc (Warning that leads right to the download, not to any sort of documentation nor README).  I've also found that it's relatively easy to just import fcntl and manipulate your Popen PIPE file descriptors directly --- though I don't know if this is portable to non-UNIX platforms.(Update: 7 August 2019: Python 3 support for ayncio subprocesses: asyncio Subprocessses)subprocess has almost no event handling support ... though you can use the signal module and plain old-school UNIX/Linux signals --- killing your processes softly, as it were.multiprocessing is for running functions within your existing (Python) code with support for more flexible communications among this family of processes.

In particular it's best to build your multiprocessing IPC around the module's Queue objects where possible, but you can also use Event objects and various other features (some of which are, presumably, built around mmap support on the platforms where that support is sufficient).Python's multiprocessing module is intended to provide interfaces and features which are very similar to threading while allowing CPython to scale your processing among multiple CPUs/cores despite the GIL (Global Interpreter Lock).  It leverages all the fine-grained SMP locking and coherency effort that was done by developers of your OS kernel.threading is for a fairly narrow range of applications which are I/O bound (don't need to scale across multiple CPU cores) and which benefit from the extremely low latency and switching overhead of thread switching (with shared core memory) vs. process/context switching.  On Linux this is almost the empty set (Linux process switch times are extremely close to its thread-switches).threading suffers from two major disadvantages in Python.One, of course, is implementation specific --- mostly affecting CPython.  That's the GIL.  For the most part, most CPython programs will not benefit from the availability of more than two CPUs (cores) and often performance will suffer from the GIL locking contention.The larger issue which is not implementation specific, is that threads share the same memory, signal handlers, file descriptors and certain other OS resources.  Thus the programmer must be extremely careful about object locking, exception handling and other aspects of their code which are both subtle and which can kill, stall, or deadlock the entire process (suite of threads).By comparison the multiprocessing model gives each process its own memory, file descriptors, etc.  A crash or unhandled exception in any one of them will only kill that resource and robustly handling the disappearance of a child or sibling process can be considerably easier than debugging, isolating and fixing or working around similar issues in threads.It's also worth noting that Twisted offers yet another alternative which is both elegant and very challenging to understand.  Basically, at the risk of over simplifying to the point where fans of Twisted may storm my home with pitchforks and torches, Twisted provides event-driven co-operative multi-tasking within any (single) process.To understand how this is possible one should read about the features of select() (which can be built around the select() or poll() or similar OS system calls).

Basically it's all driven by the ability to make a request of the OS to sleep pending any activity on a list of file descriptors or some timeout.Awakening from each of these calls to select() is an event --- either one involving input available (readable) on some number of sockets or file descriptors, or buffering space becoming available on some other (writable) descriptors or sockets, some exceptional conditions (TCP out-of-band PUSH'd packets, for example), or a TIMEOUT.Thus the Twisted programming model is built around handling these events then looping on the resulting "main" handler, allowing it to dispatch the events to your handlers.I personally think of the name, Twisted as evocative of the programming model ... since your approach to the problem must be, in some sense, "twisted" inside out.  Rather than conceiving of your program as a series of operations on input data and outputs or results, you're writing your program as a service or daemon and defining how it reacts to various events.  (In fact the core "main loop" of a Twisted program is (usually?  always?) a reactor()).The major challenges to using Twisted involve twisting your mind around the event driven model and also eschewing the use of any class libraries or toolkits which are not written to co-operate within the Twisted framework.  This is why Twisted supplies its own modules for SSH protocol handling, for curses, and its own subprocess/Popen functions, and many other modules and protocol handlers which, at first blush, would seem to duplicate things in the Python standard libraries.I think it's useful to understand Twisted on a conceptual level even if you never intend to use it.  It may give insights into performance, contention, and event handling in your threading, multiprocessing and even subprocess handling as well as any distributed processing you undertake.(Note: Newer versions of Python 3.x are including asyncio (asynchronous I/O) features such as async def, the @async.coroutine decorator, and the await keyword, and yield from future support.  All of these are roughly similar to Twisted from a process (co-operative multitasking) perspective).

(For the current status of Twisted support for Python 3, check out: https://twistedmatrix.com/documents/current/core/howto/python3.html) Yet another realm of processing you haven't asked about, but which is worth considering, is that of distributed processing.  There are many Python tools and frameworks for distributed processing and parallel computation.  Personally I think the easiest to use is one which is least often considered to be in that space.It is almost trivial to build distributed processing around Redis.  The entire key store can be used to store work units and results, Redis LISTs can be used as Queue() like object, and the PUB/SUB support can be used for Event-like handling. You can hash your keys and use values, replicated across a loose cluster of Redis instances, to store the topology and hash-token mappings to provide consistent hashing and fail-over for scaling beyond the capacity of any single instance for co-ordinating your workers and marshaling data (pickled, JSON, BSON, or YAML) among them.Of course as you start to build a larger scale and more sophisticated solution around Redis you are re-implementing many of the features that have already been solved using, Celery, Apache Spark and Hadoop, Zookeeper, etcd, Cassandra and so on.  Those all have modules for Python access to their services.[Update: A couple of resources for consideration if you're considering Python for computationally intensive across distributed systems: IPython Parallel and PySpark.  While these are general purpose distributed computing systems, they are particularly accessible and popular subsystems data science and analytics]. There you have the gamut of processing alternatives for Python, from single threaded, with simple synchronous calls to sub-processes, pools of polled subprocesses, threaded and multiprocessing, event-driven co-operative multi-tasking, and out to distributed processing.In a similar case I opted for separate processes and the little bit of necessary communication trough network socket. It is highly portable and quite simple to do using python, but probably not the simpler (in my case I had also another constraint: communication with other processes written in C++).In your case I would probably go for multiprocess, as python threads, at least when using CPython, are not real threads. Well, they are native system threads but C modules called from Python may or may not release the GIL and allow other threads them to run when calling blocking code.To use multiple processors in CPython your only choice is the multiprocessing module. CPython keeps a lock on it's internals (the GIL) which prevents threads on other cpus to work in parallel. The multiprocessing module creates new processes ( like subprocess ) and manages communication between them.Shell out and let the unix out to do your jobs:use iterpipes to wrap subprocess and then:From Ted Ziuba's siteINPUTS_FROM_YOU | xargs -n1 -0 -P NUM ./process  #NUM parallel processesORGnu Parallel will also serveYou hang out with GIL while you send the backroom boys out to do your multicore work.

Parse date string and change format

Nimmy

[Parse date string and change format](https://stackoverflow.com/questions/2265357/parse-date-string-and-change-format)

I have a date string with the format 'Mon Feb 15 2010'.  I want to change the format to '15/02/2010'.  How can I do this?

2010-02-15 10:49:25Z

I have a date string with the format 'Mon Feb 15 2010'.  I want to change the format to '15/02/2010'.  How can I do this?datetime module could help you with that:For the specific example you could do You can install the dateutil library.  Its parse function can figure out what format a string is in without having to specify the format like you do with datetime.strptime.convert string to datetime objectAs this question comes often, here is the simple explanation.datetime or time module has two important functions. In both cases, we need a formating string. It is the representation that tells how the date or time is formatted in your string.Now lets assume we have a date object.If we want to create a string from this date in the format 'Mon Feb 15 2010'Lets assume we want to convert this s again to a datetime object.Refer This document all formatting directives regarding datetime.Just for the sake of completion: when parsing a date using strptime() and the date contains the name of a day, month, etc, be aware that you have to account for the locale.It's mentioned as a footnote in the docs as well. As an example:@codeling and @user1767754 : The following two lines will work. I saw no one posted the complete solution for the example problem that was asked. Hopefully this is enough explanation.Output:use datetime library 

http://docs.python.org/library/datetime.html look up 9.1.7. 

especiall strptime() strftime()  Behavior¶

examples 

http://pleac.sourceforge.net/pleac_python/datesandtimes.htmlYou may achieve this using pandas as well:Output:You may apply pandas approach for different datatypes as:Output:

Pandas split column of lists into multiple columns

user2938093

[Pandas split column of lists into multiple columns](https://stackoverflow.com/questions/35491274/pandas-split-column-of-lists-into-multiple-columns)

I have a pandas DataFrame with one column that looks like the following:I need to split this column of lists into 2 columns named team1 and team2 using pandas.

2016-02-18 20:01:07Z

I have a pandas DataFrame with one column that looks like the following:I need to split this column of lists into 2 columns named team1 and team2 using pandas.You can use DataFrame constructor with lists created by converting to numpy array by values with tolist:And for new DataFrame:Solution with apply(pd.Series) is very slow:Much simpler solution:Yields,If you wanted to split a column of delimited strings rather than lists, you could similarly do:This solution preserves the index of the df2 DataFrame, unlike any solution that uses tolist():Here's the result:There seems to be a syntactically simpler way, and therefore easier to remember, as opposed to the proposed solutions. I'm assuming that the column is called 'meta' in a dataframe df: Based on the previous answers, here is another solution which returns the same result as df2.teams.apply(pd.Series) with a much faster run time:Timings:The above solutions didn't work for me since I have nan observations in my dataframe. In my case df2[['team1','team2']] = pd.DataFrame(df2.teams.values.tolist(), index= df2.index) yields:I solve this using list comprehension. Here the replicable example:output:solving with list comprehension:yields:

import local function from a module housed in another directory with relative imports in jupyter notebook using python3

mpacer

[import local function from a module housed in another directory with relative imports in jupyter notebook using python3](https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im)

I have a directory structure similar to the followingWhen working in notebook.jpynb if I try to use a relative import to access a function function() in module.py with:I get the following errorIs there any way to get this to work using relative imports? Note, the notebook server is instantiated at the level of the meta_project directory, so it should have access to the information in those files. Note, also, that at least as originally intended project1 wasn't thought of as a module and therefore does not have an __init__.py file, it was just meant as a file-system directory. If the solution to the problem requires treating it as a module and including an __init__.py file (even a blank one) that is fine, but doing so is not enough to solve the problem.I share this directory between machines and relative imports allow me to use the same code everywhere, & I often use notebooks for quick prototyping, so suggestions that involve hacking together absolute paths are unlikely to be helpful.Edit: This is unlike Relative imports in Python 3, which talks about relative imports in Python 3 in general and – in particular – running a script from within a package directory. This has to do with working within a jupyter notebook trying to call a function in a local module in another directory which has both different general and particular aspects. 

2015-12-27 07:14:20Z

I have a directory structure similar to the followingWhen working in notebook.jpynb if I try to use a relative import to access a function function() in module.py with:I get the following errorIs there any way to get this to work using relative imports? Note, the notebook server is instantiated at the level of the meta_project directory, so it should have access to the information in those files. Note, also, that at least as originally intended project1 wasn't thought of as a module and therefore does not have an __init__.py file, it was just meant as a file-system directory. If the solution to the problem requires treating it as a module and including an __init__.py file (even a blank one) that is fine, but doing so is not enough to solve the problem.I share this directory between machines and relative imports allow me to use the same code everywhere, & I often use notebooks for quick prototyping, so suggestions that involve hacking together absolute paths are unlikely to be helpful.Edit: This is unlike Relative imports in Python 3, which talks about relative imports in Python 3 in general and – in particular – running a script from within a package directory. This has to do with working within a jupyter notebook trying to call a function in a local module in another directory which has both different general and particular aspects. I had almost the same example as you in this notebook where I wanted to illustrate the usage of an adjacent module's function in a DRY manner.My solution was to tell Python of that additional module import path by adding a snippet like this one to the notebook:This allows you to import the desired function from the module hierarchy:Note that it is necessary to add empty __init__.py files to project1/ and lib/ folders if you don't have them already.Came here searching for best practices in abstracting code to submodules when working in Notebooks. I'm not sure that there is a best practice. I have been proposing this. A project hierarchy as such:And from 20170609-Initial_Database_Connection.ipynb:This works because by default the Jupyter Notebook can parse the cd command. Note that this does not make use of Python Notebook magic. It simply works without prepending %bash. Considering that 99 times out of a 100 I am working in Docker using one of the Project Jupyter Docker images, the following modification is idempotentSo far, the accepted answer has worked best for me. However, my concern has always been that there is a likely scenario where I might refactor the notebooks directory into subdirectories, requiring to change the module_path in every notebook. I decided to add a python file within each notebook directory to import the required modules. Thus, having the following project structure: I added the file project_path.py in each notebook subdirectory (notebooks/explore and notebooks/explain). This file contains the code for relative imports (from @metakermit):This way, I just need to do relative imports within the project_path.py file, and not in the notebooks. The notebooks files would then just need to import project_path before importing lib. For example in 0.0-notebook.ipynb:The caveat here is that reversing the imports would not work. THIS DOES NOT WORK: Thus care must be taken during imports.Researching this topic myself and having read the answers I recommend using the path.py library since it provides a context manager for changing the current working directory.You then have something likeAlthough, you might just omit the isdir statement.Here I'll add print statements to make it easy to follow what's happeningwhich outputs in this example (where lib is at /home/jovyan/shared/notebooks/by-team/data-vis/demos/lib):Since the solution uses a context manager, you are guaranteed to go back to your previous working directory, no matter what state your kernel was in before the cell and no matter what exceptions are thrown by importing your library code.I have just found this pretty solution:You just want some functions of that fileIf python version >= 3.3 you do not need init.py file in the folder

Python Requests package: Handling xml response

Andy

[Python Requests package: Handling xml response](https://stackoverflow.com/questions/18308529/python-requests-package-handling-xml-response)

I like very much the requests package and its comfortable way to handle JSON responses. Unfortunately, I did not understand if I can also process XML responses. Has anybody experience how to handle XML responses with the requests package? Is it necessary to include another package for the XML decoding?

2013-08-19 07:29:48Z

I like very much the requests package and its comfortable way to handle JSON responses. Unfortunately, I did not understand if I can also process XML responses. Has anybody experience how to handle XML responses with the requests package? Is it necessary to include another package for the XML decoding?requests does not handle parsing XML responses, no. XML responses are much more complex in nature than JSON responses, how you'd serialize XML data into Python structures is not nearly as straightforward.Python comes with built-in XML parsers. I recommend you use the ElementTree API:or, if the response is particularly large, use an incremental approach:The external lxml project builds on the same API to give you more features and power still.

Colon (:) in Python list index [duplicate]

kuriouscoder

[Colon (:) in Python list index [duplicate]](https://stackoverflow.com/questions/4012340/colon-in-python-list-index)

I'm new to Python. I see : used in list indices especially when it's associated with function calls. Python 2.7 documentation suggests that lists.append translates to a[len(a):] = [x]. Why does one need to suffix len(a) with a colon?I understand that : is used to identify keys in dictionary.

2010-10-25 06:42:53Z

I'm new to Python. I see : used in list indices especially when it's associated with function calls. Python 2.7 documentation suggests that lists.append translates to a[len(a):] = [x]. Why does one need to suffix len(a) with a colon?I understand that : is used to identify keys in dictionary.: is the delimiter of the slice syntax to 'slice out' sub-parts in sequences , [start:end]Watch https://youtu.be/tKTZoB2Vjuk?t=41m40s at around 40:00 he starts explaining that.Works with tuples, dictionaries and lists, too.slicing operator. http://docs.python.org/tutorial/introduction.html#strings and scroll down a bita[len(a):]  - This gets you the length of a to the end. It selects a range. If you reverse    a[:len(a)]  it will get you the beginning to whatever is len(a).  

Extract traceback info from an exception object

georg

[Extract traceback info from an exception object](https://stackoverflow.com/questions/11414894/extract-traceback-info-from-an-exception-object)

Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:How can I extract the traceback from the Exception object once I have it?

2012-07-10 13:54:59Z

Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:How can I extract the traceback from the Exception object once I have it?The answer to this question depends on the version of Python you're using.It's simple: exceptions come equipped with a __traceback__ attribute that contains the traceback. This attribute is also writable, and can be conveniently set using the with_traceback method of exceptions: These features are minimally described as part of the raise documentation.All credit for this part of the answer should go to Vyctor, who first posted this information. I'm including it here only because this answer is stuck at the top, and Python 3 is becoming more common.It's annoyingly complex. The trouble with tracebacks is that they have references to stack frames, and stack frames have references to the tracebacks that have references to stack frames that have references to... you get the idea. This causes problems for the garbage collector. (Thanks to ecatmur for first pointing this out.)The nice way of solving this would be to surgically break the cycle after leaving the except clause, which is what Python 3 does. The Python 2 solution is much uglier: you are provided with an ad-hoc function,sys.exc_info(), which only works inside the except clause. It returns a tuple containing the exception, the exception type, and the traceback for whatever exception is currently being handled. So if you are inside the except clause, you can use the output of sys.exc_info() along with the traceback module to do various useful things:But as your edit indicates, you're trying to get the traceback that would have been printed if your exception had not been handled, after it has already been handled. That's a much harder question. Unfortunately, sys.exc_info returns (None, None, None) when no exception is being handled. Other related sys attributes don't help either. sys.exc_traceback is deprecated and undefined when no exception is being handled; sys.last_traceback seems perfect, but it appears only to be defined during interactive sessions.If you can control how the exception is raised, you might be able to use inspect and a custom exception to store some of the information. But I'm not entirely sure how that would work.To tell the truth, catching and returning an exception is kind of an unusual thing to do. This might be a sign that you need to refactor anyway.Since Python 3.0[PEP 3109] the built in class Exception has a __traceback__ attribute which contains a traceback object (with Python 3.2.3):The problem is that after Googling __traceback__ for a while I found only few articles but none of them describes whether or why you should (not) use __traceback__.However, the Python 3 documentation for raise says that:So I assume it's meant to be used.A way to get traceback as a string from an exception object in Python 3:traceback.format_tb(...) returns a list of strings. ''.join(...) joins them together. For more reference, please visit: https://docs.python.org/3/library/traceback.html#traceback.format_tbAs an aside, if you want to actually get the full traceback as you would see it printed to your terminal, you want this:If you use format_tb as above answers suggest you'll get less information:There's a very good reason the traceback is not stored in the exception; because the traceback holds references to its stack's locals, this would result in a circular reference and (temporary) memory leak until the circular GC kicks in.  (This is why you should never store the traceback in a local variable.)About the only thing I can think of would be for you to monkeypatch stuff's globals so that when it thinks it's catching Exception it's actually catching a specialised type and the exception propagates to you as the caller:

Python: how to print range a-z?

hhh

[Python: how to print range a-z?](https://stackoverflow.com/questions/3190122/python-how-to-print-range-a-z)

1. Print a-n: a b c d e f g h i j k l m n2. Every second in a-n: a c e g i k m3. Append a-n to index of urls{hello.com/, hej.com/, ..., hallo.com/}: hello.com/a hej.com/b ... hallo.com/n

2010-07-06 20:51:20Z

1. Print a-n: a b c d e f g h i j k l m n2. Every second in a-n: a c e g i k m3. Append a-n to index of urls{hello.com/, hej.com/, ..., hallo.com/}: hello.com/a hej.com/b ... hallo.com/nTo do the urls, you could use something like thisAssuming this is a homework ;-) - no need to summon libraries etc - it probably expect you to use range() with chr/ord, like so:For the rest, just play a bit more with the range()Hints:and andorThis solution uses the ASCII table. ord gets the ascii value from a character and chr vice versa.andThe answer to this question is simple, just make a list called ABC like so:And whenever you need to refer to it, just do:Also try this to break ur device :DTry:   OutputThis is your 2nd question: string.lowercase[ord('a')-97:ord('n')-97:2] because 97==ord('a') -- if you want to learn a bit you should figure out the rest yourself ;-)About gnibbler's answer. Zip -function, full explanation, returns a list of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. [...] construct is called list comprehension, very cool feature!I hope this helps:

How to disable Django's CSRF validation?

MrROY

[How to disable Django's CSRF validation?](https://stackoverflow.com/questions/16458166/how-to-disable-djangos-csrf-validation)

I have commented out csrf processor and middleware lines in settings.py:But when I use Ajax to send a request, Django still respond 'csrf token is incorrect or missing', and after adding X-CSRFToken to headers, the request would succeed.What is going on here ?

2013-05-09 09:07:09Z

I have commented out csrf processor and middleware lines in settings.py:But when I use Ajax to send a request, Django still respond 'csrf token is incorrect or missing', and after adding X-CSRFToken to headers, the request would succeed.What is going on here ?If you just need some views not to use CSRF, you can use @csrf_exempt:You can find more examples and other scenarios in the Django documentation:To disable CSRF for class based views the following worked for me.

Using django 1.10 and python 3.5.2In setting.py in MIDDLEWARE you can simply remove/comment this line:For Django 2:That middleware must be added to settings.MIDDLEWARE when appropriate (in your test settings for example). Note: the setting isn't not called MIDDLEWARE_CLASSES anymore.The answer might be inappropriate, but I hope it helps youHaving middleware like this helps to debug requests and to check csrf in production servers.The problem here is that SessionAuthentication performs its own CSRF validation. That is why you get the CSRF missing error even when the CSRF Middleware is commented.

You could add @csrf_exempt to every view, but if you want to disable CSRF and have session authentication for the whole app, you can add an extra middleware like this - I created this class in myapp/middle.py 

Then import this middleware in Middleware in settings.pyThat works with DRF on django 1.11If you want disable it in Global, you can write a custom middleware, like thisthen add this class youappname.middlewarefilename.DisableCsrfCheck to MIDDLEWARE_CLASSES lists, before django.middleware.csrf.CsrfViewMiddlewareCSRF can be enforced at the view level, which can't be disabled globally. In some cases this is a pain, but um, "it's for security". Gotta retain those AAA ratings. https://docs.djangoproject.com/en/dev/ref/csrf/#contrib-and-reusable-apps

Specify format for input arguments argparse python

Sohaib

[Specify format for input arguments argparse python](https://stackoverflow.com/questions/25470844/specify-format-for-input-arguments-argparse-python)

I have a python script that requires some command line inputs and I am using argparse for parsing them. I found the documentation a bit confusing and couldn't find a way to check for a format in the input parameters. What I mean by checking format is explained with this example script:I need to check for option -s and -e that the input by the user is in the format YYYY-MM-DD. Is there an option in argparse that I do not know of which accomplishes this.

2014-08-24 10:41:29Z

I have a python script that requires some command line inputs and I am using argparse for parsing them. I found the documentation a bit confusing and couldn't find a way to check for a format in the input parameters. What I mean by checking format is explained with this example script:I need to check for option -s and -e that the input by the user is in the format YYYY-MM-DD. Is there an option in argparse that I do not know of which accomplishes this.Per the documentation:You could do something like:Then use that as type:Just to add on to the answer above, you can use a lambda function if you want to keep it to a one-liner. For example:Old thread but the question was still relevant for me at least!For others who hit this via search engines: in Python 3.7, you can use the standard .fromisoformat class method instead of reinventing the wheel for ISO-8601 compliant dates, e.g.:

How to change a django QueryDict to Python Dict?

SaiyanGirl

[How to change a django QueryDict to Python Dict?](https://stackoverflow.com/questions/13349573/how-to-change-a-django-querydict-to-python-dict)

Let's pretend I have the following QueryDict: I'd like to have a dictionary out of this, eg:(I don't care if the unicode symbol u stays or goes.)If I do queryDict.dict(), as suggested by the django site, I lose the extra values belonging to var1, eg:I was thinking of doing this:Is there a better way? 

2012-11-12 18:45:07Z

Let's pretend I have the following QueryDict: I'd like to have a dictionary out of this, eg:(I don't care if the unicode symbol u stays or goes.)If I do queryDict.dict(), as suggested by the django site, I lose the extra values belonging to var1, eg:I was thinking of doing this:Is there a better way? This should work: myDict = dict(queryDict.iterlists())New in Django >= 1.4.QueryDict.dict()https://docs.djangoproject.com/en/stable/ref/request-response/#django.http.QueryDict.dictThis is what I've ended up using:From my usage this seems to get you a list you can send back to e.g. a form constructor.EDIT: maybe this isn't the best method. It seems if you want to e.g. write QueryDict to a file for whatever crazy reason, QueryDict.urlencode() is the way to go. To reconstruct the QueryDict you simply do QueryDict(urlencoded_data).If you do not want the values as Arrays you can do the following:zip is a powerful tool read more about it here http://docs.python.org/2/library/functions.html#zipI ran into a similar problem, wanting to save arbitrary values from a form as serialized values.My answer avoids explicitly iterating the dictionary contents: dict(querydict.iterlists())In order to retrieve a dictionary-like value that functions as the original, an inverse function uses QueryDict.setlist() to populate a new QueryDict value.  In this case, I don't think the explicit iteration is avoidable.My helper functions look like this:Update:Please Note : underscore _ in iterlists method of queryDict. Django version :1.5.1dict(request.POST) returns a weird python dictionary with array wrapped values.where as {x:request.POST.get(x) for x in request.POST.keys()} returns expected output.just simply add queryDict=dict(request.GET)

or queryDict=dict(QueryDict)In your view and data will be saved in querDict as python Dict.This is how I solved that problem:Like me, you probably are more familiar with Dict() methods in python. However, the QueryDict() is also an easy object to use. For example, perhaps you wanted to get the value from the request.GET QueryDict().You can do this like so: request.GET.__getitem__(<key>).QueryDict() documentation: https://docs.djangoproject.com/en/2.0/ref/request-response/#django.http.QueryDictI tried out both dict(request.POST) and request.POST.dict() and realised that if you have list values for example 'var1':['value1', 'value2'] nested in your request.POST, the later(request.POST.dict()) only gave me access to the last item in a nested list while the former(dict(request.POST)) allowed me to access all items in a nested list.I hope this helps someone.With Django 2.2 there are few clean solutions:will outputwill outputwhich is correct.

How should I write tests for Forms in Django?

Mridang Agarwalla

[How should I write tests for Forms in Django?](https://stackoverflow.com/questions/7304248/how-should-i-write-tests-for-forms-in-django)

I'd like to simulate requests to my views in Django when I'm writing tests. This is mainly to test the forms. Here's a snippet of a simple test request:The page always returns a response of 200 whether there's an form error or not. How can I check that my Form failed and that the particular field (soemthing) had an error?

2011-09-05 05:45:07Z

I'd like to simulate requests to my views in Django when I'm writing tests. This is mainly to test the forms. Here's a snippet of a simple test request:The page always returns a response of 200 whether there's an form error or not. How can I check that my Form failed and that the particular field (soemthing) had an error?I think if you just want to test the form, then you should just test the form and not the view where the form is rendered. Example to get an idea:https://docs.djangoproject.com/en/stable/topics/testing/tools/#django.test.SimpleTestCase.assertFormErrorWhere "form" is the context variable name for your form, "something" is the field name, and "This field is required." is the exact text of the expected validation error.The original 2011 answer wasBut I see now (2018) there is a whole crowd of applicable asserts available:Take your pick.

Python Unicode Encode Error

Alex B

[Python Unicode Encode Error](https://stackoverflow.com/questions/3224268/python-unicode-encode-error)

I'm reading and parsing an Amazon XML file and while the XML file shows a ' , when I try to print it I get the following error:From what I've read online thus far, the error is coming from the fact that the XML file is in UTF-8, but Python wants to handle it as an ASCII encoded character. Is there a simple way to make the error go away and have my program print the XML as it reads?

2010-07-11 19:00:48Z

I'm reading and parsing an Amazon XML file and while the XML file shows a ' , when I try to print it I get the following error:From what I've read online thus far, the error is coming from the fact that the XML file is in UTF-8, but Python wants to handle it as an ASCII encoded character. Is there a simple way to make the error go away and have my program print the XML as it reads?Likely, your problem is that you parsed it okay, and now you're trying to print the contents of the XML and you can't because theres some foreign Unicode characters.  Try to encode your unicode string as ascii first:the 'ignore' part will tell it to just skip those characters.  From the python docs:You might want to read this article: http://www.joelonsoftware.com/articles/Unicode.html, which I found very useful as a basic tutorial on what's going on.  After the read, you'll stop feeling like you're just guessing what commands to use (or at least that happened to me).A better solution:If you would like to read more about why:http://docs.plone.org/manage/troubleshooting/unicode.html#id1Don't hardcode the character encoding of your environment inside your script; print Unicode text directly instead:If your output is redirected to a file (or a pipe); you could use PYTHONIOENCODING envvar, to specify the character encoding:Otherwise, python your_script.py should work as is -- your locale settings are used to encode the text (on POSIX check: LC_ALL, LC_CTYPE, LANG envvars -- set LANG to a utf-8 locale if necessary).To print Unicode on Windows, see this answer that shows how to print Unicode to Windows console, to a file, or using IDLE.Excellent post : http://www.carlosble.com/2010/12/understanding-python-and-unicode/You can use something of the formwhich will convert a UTF-8 encoded bytestring into a Python Unicode string. But the exact procedure to use depends on exactly how you load and parse the XML file, e.g. if you don't ever access the XML string directly, you might have to use a decoder object from the codecs module.I wrote the following to fix the nuisance non-ascii quotes and force conversion to something usable.If you need to print an approximate representation of the string to the screen, rather than ignoring those nonprintable characters, please try unidecode package here:https://pypi.python.org/pypi/UnidecodeThe explanation is found here:https://www.tablix.org/~avian/blog/archives/2009/01/unicode_transliteration_in_python/This is better than using the u.encode('ascii', 'ignore') for a given string u, and can save you from unnecessary headache if character precision is not what you are after, but still want to have human readability.WirawanTry adding the following line at the top of your python script.Python 3.5, 2018If you don't know what the encoding but the unicode parser is having issues you can open the file in Notepad++ and in the top bar select Encoding->Convert to ANSI. Then you can write your python like this

Print very long string completely in pandas dataframe

Yantraguru

[Print very long string completely in pandas dataframe](https://stackoverflow.com/questions/29902714/print-very-long-string-completely-in-pandas-dataframe)

I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.Now when I try to print the same, I do not see the full string I rather see only part of the string.I tried following options Any ideas appreciated. Looks very simple, but not able to get it!  

2015-04-27 17:54:00Z

I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.Now when I try to print the same, I do not see the full string I rather see only part of the string.I tried following options Any ideas appreciated. Looks very simple, but not able to get it!  You can use options.display.max_colwidth to specify you want to see more in the default representation:And indeed, if you just want to inspect the one value, by accessing it (as a scalar, not as a row as df.iloc[2] does) you also see the full string:Use pd.set_option('display.max_colwidth', -1) for automatic linebreaks and multi-line cells.This is a great resource on how to use jupyters display with pandas to the fullest.Another, pretty simple approach is to call  list function:No worth to mention, that is not good to convent to list the whole columns, but for a simple line - why notAnother easier way to print the whole string is to call values on the dataframe.The Output will beIs this what you meant to do ?The way I often deal with the situation you describe is to use the .to_csv() method and write to stdout:Update: it should now be possible to just use None instead of sys.stdout with similar effect! This should dump the whole dataframe, including the entirety of any strings. You can use the to_csv parameters to configure column separators, whether the index is printed, etc. It will be less pretty than rendering it properly though.I posted this originally in answer to the somewhat-related question at Output data from all columns in a dataframe in pandasJust add the following line to your code before print.You can simply do the following steps for setting other additional options,this should works finePlease kindly refer the doc to change more options/settings for pandas

Creating dataframe from a dictionary where entries have different lengths

Josh

[Creating dataframe from a dictionary where entries have different lengths](https://stackoverflow.com/questions/19736080/creating-dataframe-from-a-dictionary-where-entries-have-different-lengths)

Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.How can I create a dataframe where each column holds a different entry?When I try:I get:Any way to overcome this? I am happy to have Pandas use NaN to pad those columns for the shorter entries.

2013-11-01 21:59:09Z

Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.How can I create a dataframe where each column holds a different entry?When I try:I get:Any way to overcome this? I am happy to have Pandas use NaN to pad those columns for the shorter entries.In Python 3.x:In Python 2.x:replace d.items() with d.iteritems().Here's a simple way to do that:A way of tidying up your syntax, but still do essentially the same thing as these other answers, is below:A similar syntax exists for lists, too:Another syntax for lists is:You may additionally have to transpose the result and/or change the column data types (float, integer, etc).While this does not directly answer the OP's question. I found this to be an excellent solution for my case when I had unequal arrays and I'd like to share:from pandas documentationYou can also use pd.concat along axis=1 with a list of pd.Series objects:Both the following lines work perfectly : But with %timeit on Jupyter, I've got a ratio of 4x speed for B vs A, which is quite impressive especially when working with a huge data set (mainly with a big number of columns/features).If you don't want it to show NaN and you have two particular lengths, adding a 'space' in each remaining cell would also work. If you have more than 2 lengths of entries, it is advisable to make a function which uses a similar method.pd.DataFrame([my_dict]) will do!

How to make firefox headless programmatically in Selenium with python?

Tintinabulator Zea

[How to make firefox headless programmatically in Selenium with python?](https://stackoverflow.com/questions/46753393/how-to-make-firefox-headless-programmatically-in-selenium-with-python)

I am running this code with python, selenium, and firefox but still get 'head' version of firefox:I also tried some variations of binary:

2017-10-15 08:58:25Z

I am running this code with python, selenium, and firefox but still get 'head' version of firefox:I also tried some variations of binary:To invoke Firefox Browser headlessly, you can set the headless property through Options() class as follows:There's another way to accomplish headless mode. If you need to disable or enable the headless mode in Firefox, without changing the code, you can set the environment variable MOZ_HEADLESS to whatever if you want Firefox to run headless, or don't set it at all.This is very useful when you are using for example continuous integration and you want to run the functional tests in the server but still be able to run the tests in normal mode in your PC.orHow to configure ChromeDriver to initiate Chrome browser in Headless mode through Selenium?The first answer does't work anymore. This worked for me:My answer:https://seleniumhq.github.io/selenium/docs/api/py/webdriver_firefox/selenium.webdriver.firefox.options.htmlworks for meJust a note for people who may have found this later (and want java way of achieving this); FirefoxOptions is also capable of enabling the headless mode:

How exactly does the python any() function work?

pythoniku

[How exactly does the python any() function work?](https://stackoverflow.com/questions/16505456/how-exactly-does-the-python-any-function-work)

In the python docs page for any, the equivalent code for the any() function is given as:How does this function know what element I wanna test if call it in this form?From the function definition, all I can see is that I'm passing an iterable object. How does the for loop know I am looking for something > 0?

2013-05-12 08:18:52Z

In the python docs page for any, the equivalent code for the any() function is given as:How does this function know what element I wanna test if call it in this form?From the function definition, all I can see is that I'm passing an iterable object. How does the for loop know I am looking for something > 0?If you use any(lst) you see that lst is the iterable, which is a list of some items. If it contained [0, False, '', 0.0, [], {}, None] (which all have boolean values of False) then any(lst) would be False. If lst also contained any of the following [-1, True, "X", 0.00001] (all of which evaluate to True) then any(lst) would be True.In the code you posted, x > 0 for x in lst, this is a different kind of iterable, called a generator expression. Before generator expressions were added to Python, you would have created a list comprehension, which looks very similar, but with surrounding []'s: [x > 0 for x in lst]. From the lst containing [-1, -2, 10, -4, 20], you would get this comprehended list: [False, False, True, False, True]. This internal value would then get passed to the any function, which would return True, since there is at least one True value.But with generator expressions, Python no longer has to create that internal list of True(s) and False(s), the values will be generated as the any function iterates through the values generated one at a time by the generator expression. And, since any short-circuits, it will stop iterating as soon as it sees the first True value. This would be especially handy if you created lst using something like lst = range(-1,int(1e9)) (or xrange if you are using Python2.x). Even though this expression will generate over a billion entries, any only has to go as far as the third entry when it gets to 1, which evaluates True for x>0, and so any can return True. If you had created a list comprehension, Python would first have had to create the billion-element list in memory, and then pass that to any. But by using a generator expression, you can have Python's builtin functions like any and all break out early, as soon as a True or False value is seen.It just reduce several line of code into one.

You don't have to write lengthy code like:(x > 0 for x in list) in that function call creates a generator expression eg.Which any uses, and shortcircuits on encountering the first object that evaluates TrueIt's because the iterable is Note that x > 0 returns either True or False and thus you have an iterable of booleans.Simply saying, any() does this work : according to the condition even if it encounters one fulfilling value in the list, it returns true, else it returns false.

ImportError: DLL load failed: %1 is not a valid Win32 application. But the DLL's are there

LarsH

[ImportError: DLL load failed: %1 is not a valid Win32 application. But the DLL's are there](https://stackoverflow.com/questions/19019720/importerror-dll-load-failed-1-is-not-a-valid-win32-application-but-the-dlls)

I have a situation very much like the one at ImportError: DLL load failed: %1 is not a valid Win32 application, but the answer there isn't working for me.My Python code says:But that line throws the error shown in the title of this question.I have OpenCV installed in C:\lib\opencv on this 64-bit machine. I'm using 64-bit Python.My PYTHONPATH variable: PYTHONPATH=C:\lib\opencv\build\python\2.7. This folder contains cv2.pyd and that's all.My PATH variable: Path=%OPENCV_DIR%\bin;... This folder contains 39 DLL files such as opencv_core246d.dll.OPENCV_DIR has this value: OPENCV_DIR=C:\lib\opencv\build\x64\vc11.The solution at ImportError: DLL load failed: %1 is not a valid Win32 application says to add "the new opencv binaries path (C:\opencv\build\bin\Release) to the Windows PATH environment variable". But as shown above, I already have the OpenCV binaries folder (C:\lib\opencv\build\x64\vc11\bin) in my PATH. And my OpenCV installation doesn't have any Release folders (except for an empty one under build/java).Any ideas as to what's going wrong? Can I tell Python to verbosely trace the loading process? Exactly what DLL's is it looking for?Thanks,

LarsI just noticed that, according to http://www.dependencywalker.com/, the cv2.pyd in C:\lib\opencv\build\python\2.7 is 32-bit, whereas the machine and the Python I'm running are 64-bit. Could that be the problem? And if so, where can I find a 64-bit version of cv2.pyd?

2013-09-26 04:59:00Z

I have a situation very much like the one at ImportError: DLL load failed: %1 is not a valid Win32 application, but the answer there isn't working for me.My Python code says:But that line throws the error shown in the title of this question.I have OpenCV installed in C:\lib\opencv on this 64-bit machine. I'm using 64-bit Python.My PYTHONPATH variable: PYTHONPATH=C:\lib\opencv\build\python\2.7. This folder contains cv2.pyd and that's all.My PATH variable: Path=%OPENCV_DIR%\bin;... This folder contains 39 DLL files such as opencv_core246d.dll.OPENCV_DIR has this value: OPENCV_DIR=C:\lib\opencv\build\x64\vc11.The solution at ImportError: DLL load failed: %1 is not a valid Win32 application says to add "the new opencv binaries path (C:\opencv\build\bin\Release) to the Windows PATH environment variable". But as shown above, I already have the OpenCV binaries folder (C:\lib\opencv\build\x64\vc11\bin) in my PATH. And my OpenCV installation doesn't have any Release folders (except for an empty one under build/java).Any ideas as to what's going wrong? Can I tell Python to verbosely trace the loading process? Exactly what DLL's is it looking for?Thanks,

LarsI just noticed that, according to http://www.dependencywalker.com/, the cv2.pyd in C:\lib\opencv\build\python\2.7 is 32-bit, whereas the machine and the Python I'm running are 64-bit. Could that be the problem? And if so, where can I find a 64-bit version of cv2.pyd?Unofficial Windows Binaries for Python Extension Packagesyou can find any python libs from herePlease check if the python version you are using is also 64 bit. If not then that could be the issue. You would be using a 32 bit python version and would have installed a 64 bit binaries for the OPENCV library. Wow, I found yet another case for this problem. None of the above worked. Eventually I used python's ability to introspect what was being loaded. For python 2.7 this means:This turned up a completely unexpected "cv2.pyd" file in an Anaconda DLL directory that wasn't touched by multiple uninstall/install attempts. Python was looking there first and not finding my good installation. I deleted that cv2.pyd file and tried imp.find_module("cv2") again and python immediately found the right file and cv2 started working.So if none of the other solutions work for you, make sure you use python introspection to see what file python is trying to load.In my case, I have 64bit python, and it was lxml that was the wrong version--I should have been using the x64 version of that as well. I solved this by downloading the 64-bit version of lxml here:https://pypi.python.org/pypi/lxml/3.4.1This was the simplest answer to a frustrating issue.I just had this problem, it turns it was just because I was using x64 version of the opencv file.  Tried the x86 and it worked.If your build-system (CMake in my case) copies the file from <name>.dll to <name>.pyd, you will get this error if the original file wasn't actually a dll.  In my case, building shared libraries got switched off, so the underlying file was actually a *.lib.I discovered this error by loading the pyd file in DependencyWalker and finding that it wasn't valid.I had the same problem. Here's what I did:It solvs the problem for me. You may also like to give it a try. Hope it work for you as well.I copied cv2.pyd file from /opencv/build/python/2.7/x86 folder instead of from /x64 folder to C:/Python27/Lib/site-packeges. I followed rest of the instructions provided here.Added by someone else, not verified: I also copy file cv2.pyd to folder C:/Python27/Lib/site-packages/cv2. It works.For me the problem was that I was using different versions of Python in the same Eclipse project. My setup was not consistent with the Project Properties and the Run Configuration Python versions.In Project > Properties > PyDev, I had the Interpreter set to Python2.7.11.In Run Configurations > Interpreter, I was using the Default Interpreter. Changing it to Python 2.7.11 fixed the problem.I faced the same issue when I uninstalled and reinstalled a different version of 2.7.x of Python on my system using a 32 bit Windows Installer. I got the same error on most of my import statements.

I uninstalled the newly installed Python and downloaded a 64 bit Windows installer and reinstalled Python again and it worked.

Hope this helps you.So I had problems installing vtk under windows (as I use python 3.7 there is no binary available so far just for older python versions pip install vtk is not working)I did wrote python in my cmd:Python 3.7.3 on win32So I now know I have python 3.7.3 runing on a 32 bit.I then downloaded the correct wheel at VTK‑8.2.0‑cp37‑cp37m‑win32.whlNext I instlled that wheel:Then I tested it and it worked:Update numpy.pip install numpy --upgradeWork for me!! First I copied cv2.pyd from /opencv/build/python/2.7/x86 to C:/Python27/Lib/site-packeges. The error was Then I installed numpy-1.8.0-win32-superpack-python2.7.exe and opencv works fine.You can install opencv from official or unofficial sites.Refer to this question and this issue if you are using Anaconda.It has a very simple solution.

After installing opencv

placecv2.pyd from C:\opencv\build\python\2.7\ **x64** to C:\Python27\Lib\site-packagesinstead of, place cv2.pyd from C:\opencv\build\python\2.7\ **x86** to C:\Python27\Lib\site-packages I got this error when trying to import MySQLdb. What worked for me was to uninstall Python and then reinstall it. I got the error after installing npm (https://www.npmjs.com/get-npm). One thing it did was install Python even though I already had it.This has worked for me. I have tried different methods but this was my best solution.Open command prompt and type the following;

pip install opencv-python. 

(make sure your internet is on).

after that try importing it again.I found the solution, maybe you can try to use the cmd window rather than the anaconda prompt window to start you first scrapy test.

How do I configure a Python interpreter in IntelliJ IDEA with the PyCharm plugin?

kousen

[How do I configure a Python interpreter in IntelliJ IDEA with the PyCharm plugin?](https://stackoverflow.com/questions/24769117/how-do-i-configure-a-python-interpreter-in-intellij-idea-with-the-pycharm-plugin)

There is a tutorial in the IDEA docs on how to add a Python interpreter in PyCharm, which involves accessing the "Project Interpreter" page. Even after installing the Python plugin, I don't see that setting anywhere.Am I missing something obvious?

2014-07-15 22:18:30Z

There is a tutorial in the IDEA docs on how to add a Python interpreter in PyCharm, which involves accessing the "Project Interpreter" page. Even after installing the Python plugin, I don't see that setting anywhere.Am I missing something obvious?With the Python plugin installed, navigate to File > Project Structure.  Under the Project menu for Project SDK, select "New" and select "Python SDK", then select "Local".  Provided you have a Python SDK installed, the flow should be natural from there - navigate to the location your Python installation lives.So here is a simple project, where I have used Selenium and added that using external pathNow you need to open Project Structure and go to SDK SectionNow Select your project's virtual environment. In the Classpath tab add the PYTHONPATH by clicking + button  and now the modules will be recognizedIf you have multiple modules in your project, with different languages, you can set the interpreter in the following way:Follow these steps:For Pycharm users:File ⟶ Settings ⟶ Project: MyProject ⟶ Project Interpreter

matplotlib taking time when being imported

Ricky Robinson

[matplotlib taking time when being imported](https://stackoverflow.com/questions/34771191/matplotlib-taking-time-when-being-imported)

I just upgraded to the latest stable release of matplotlib (1.5.1) and everytime I import matplotlib I get this message:... which always stalls for a few seconds.Is this the expected behaviour? Was it the same also before, but just without the printed message?

2016-01-13 15:54:39Z

I just upgraded to the latest stable release of matplotlib (1.5.1) and everytime I import matplotlib I get this message:... which always stalls for a few seconds.Is this the expected behaviour? Was it the same also before, but just without the printed message?As tom suggested in the comment above, deleting the files: solve the problem. 

In my case the files were under:EDITEDA couple of days ago the message appeared again, I deleted the files in the locations mention above without any success. I found that as suggested here by T Mudau there's an extra location with text cache files is: ~/.cache/fontconfigConfirmed Hugo's approach works for Ubuntu 14.04 LTS/matplotlib 1.5.1:On OSX Yosemite (version 10.10.15), the following worked for me:I ran the python code using sudo just once, and it resolved the warning for me. 

Now it runs faster. Running without sudo gives no warning at all. CheersI ran the python code w. sudo and it cured it...my guess was that there wasn't permission to write that table... good luck!HI you must find this file : font_manager.py in my case : C:\Users\gustavo\Anaconda3\Lib\site-packages\matplotlib\ font_manager.pyand FIND def win32InstalledFonts(directory=None, fontext='ttf') and replace by :def win32InstalledFonts(directory=None, fontext='ttf'):

    """

    Search for fonts in the specified font directory, or use the

    system directories if none given.  A list of TrueType font

    filenames are returned by default, or AFM fonts if fontext ==

    'afm'.

    """This worked for me on Ubuntu 16.04 LST with Python 3.5.2 | Anaconda 4.2.0 (64-bit). I deleted all of the files in ~/.cache/matplotlib/.At first I thought it wouldn't work, because I got the warning afterward. But after the cache files were rebuilt the warning went away. So, close your file, and reopen again(open again), it has no warning.This worked for me:

Argparse: Required argument 'y' if 'x' is present

asudhak

[Argparse: Required argument 'y' if 'x' is present](https://stackoverflow.com/questions/19414060/argparse-required-argument-y-if-x-is-present)

I have a requirement as follows:for the argument prox , I use action='store_true' to check if it is present or not. 

I do not require any of the arguments. But, if --prox is set I require rport and lport as well. Is there an easy way of doing this with argparse without writing custom conditional coding.More Code:

2013-10-16 21:16:45Z

I have a requirement as follows:for the argument prox , I use action='store_true' to check if it is present or not. 

I do not require any of the arguments. But, if --prox is set I require rport and lport as well. Is there an easy way of doing this with argparse without writing custom conditional coding.More Code:No, there isn't any option in argparse to make mutually inclusive sets of options.The simplest way to deal with this would be:You're talking about having conditionally required arguments. Like @borntyping said you could check for the error and do parser.error(), or you could just apply a requirement related to --prox when you add a new argument.A simple solution for your example could be:This way required receives either True or False depending on whether the user as used --prox. This also guarantees that -lport and -rport have an independent behavior between each other.How about using parser.parse_known_args() method and then adding the --lport and --rport args as required args if --prox is present.Also keep in mind that you can supply the namespace opts generated after the first parsing while parsing the remaining arguments the second time. That way, in the the end, after all the parsing is done, you'll have a single namespace with all the options. Drawbacks: Do you use lport when prox is not set.  If not, why not make lport and rport arguments of prox?  e.g.That saves your users typing.  It is just as easy to test if args.prox is not None: as if args.prox:.The accepted answer worked great for me! Since all code is broken without tests here is how I tested the accepted answer. parser.error() does not raise an argparse.ArgumentError error it instead exits the process. You have to test for SystemExit. with pytestwith unittestsinspired from: Using unittest to test argparse - exit errors

Python - What is exactly sklearn.pipeline.Pipeline?

farhawa

[Python - What is exactly sklearn.pipeline.Pipeline?](https://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline)

I can't figure out how the sklearn.pipeline.Pipeline works exactly.There are a few explanation in the doc. For example what do they mean by:To make my question clearer, what are steps? How do they work?EditThanks to the answers I can make my question clearer:When I call pipeline and pass, as steps, two transformers and one estimator, e.g:What happens when I call this?I can't figure out how an estimator can be a transformer and how a transformer can be fitted.

2015-10-12 22:42:46Z

I can't figure out how the sklearn.pipeline.Pipeline works exactly.There are a few explanation in the doc. For example what do they mean by:To make my question clearer, what are steps? How do they work?EditThanks to the answers I can make my question clearer:When I call pipeline and pass, as steps, two transformers and one estimator, e.g:What happens when I call this?I can't figure out how an estimator can be a transformer and how a transformer can be fitted.Transformer in scikit-learn - some class that have fit and transform method, or fit_transform method.Predictor - some class that has fit and predict methods, or fit_predict method.Pipeline is just an abstract notion, it's not some existing ml algorithm. Often in ML tasks you need to perform sequence of different transformations (find set of features, generate new features, select only some good features) of raw dataset before applying final estimator.Here is a good example of Pipeline usage.

Pipeline gives you a single interface for all 3 steps of transformation and resulting estimator. It encapsulates transformers and predictors inside, and now you can do something like:With just:With pipelines you can easily perform a grid-search over set of parameters for each step of this meta-estimator. As described in the link above. All steps except last one must be transforms, last step can be transformer or predictor.

Answer to edit:

When you call pipln.fit() - each transformer inside pipeline will be fitted on outputs of previous transformer (First transformer is learned on raw dataset).  Last estimator may be transformer or predictor, you can call fit_transform() on pipeline only if your last estimator is transformer (that implements fit_transform, or transform and fit methods separately), you can call fit_predict() or predict() on pipeline only if your last estimator is predictor. So you just can't call fit_transform or transform on pipeline, last step of which is predictor.I think that M0rkHaV has the right idea. Scikit-learn's pipeline class is a useful tool for encapsulating multiple different transformers alongside an estimator into one object, so that you only have to call your important methods once (fit(), predict(), etc). Let's break down the two major components:As for your edit: let's go through a text-based example. Using LabelBinarizer, we want to turn a list of labels into a list of binary values. Now, when the binarizer is fitted on some data, it will have a structure called classes_ that contains the unique classes that the transformer 'knows' about. Without calling fit() the binarizer has no idea what the data looks like, so calling transform() wouldn't make any sense. This is true if you print out the list of classes before trying to fit the data.I get the following error when trying this:But when you fit the binarizer on the vec list:and try again I get the following:And now, after calling transform on the vec object, we get the following:As for estimators being used as transformers, let us use the DecisionTree classifier as an example of a feature-extractor. Decision Trees are great for a lot of reasons, but for our purposes, what's important is that they have the ability to rank features that the tree found useful for predicting. When you call transform() on a Decision Tree, it will take your input data and find what it thinks are the most important features. So you can think of it transforming your data matrix (n rows by m columns) into a smaller matrix (n rows by k columns), where the k columns are the k most important features that the Decision Tree found.A pipeline is a series of steps in which data is transformed. It comes from the old "pipe and filter" design pattern (for instance, you could think of unix bash commands with pipes「|」or redirect operators「>」). However, pipelines are objects in the code. Thus, you may have a class for each filter (a.k.a. each pipeline step), and then another class to combine those steps into the final pipeline. Some pipelines may combine other pipelines in series or in parallel, have multiple inputs or outputs, and so on. We like to view Machine Learning pipelines as:Pipelines (or steps in the pipeline) must have those two methods:It's also possible to call this method to chain both: Scikit-Learn had its first release in 2007, which was a pre deep learning era. However, it’s one of the most known and adopted machine learning library, and is still growing. On top of all, it uses the Pipe and Filter design pattern as a software architectural style - it’s what makes Scikit-Learn so fabulous, added to the fact it provides algorithms ready for use. However, it has massive issues when it comes to do the following, which we should be able to do in 2020 already:For sure, Scikit-Learn is very convenient and well-built. However, it needs a refresh. Here are our solutions with Neuraxle to make Scikit-Learn fresh and useable within modern computing projects!Note: if a step of a pipeline doesn’t need to have one of the fit or transform methods, it could inherit from NonFittableMixin or NonTransformableMixin to be provided a default implementation of one of those methods to do nothing.As a starter, it is possible for pipelines or their steps to also optionally define those methods:The following methods are provided by default to allow for managing hyperparameters:For more info on our suggested solutions, read the entries in the big list with links above. 

How do I find the maximum of 2 numbers?

Shilpa

[How do I find the maximum of 2 numbers?](https://stackoverflow.com/questions/3357369/how-do-i-find-the-maximum-of-2-numbers)

How to find the maximum of 2 numbers?I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?

2010-07-28 20:49:09Z

How to find the maximum of 2 numbers?I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?Use the builtin function max.Example: 

max(2, 4) returns 4.Just for giggles, there's a min as well...should you need it. :Pmax()max(number_one, number_two)You can use max(value, run)The function max takes any number of arguments, or (alternatively) an iterable, and returns the maximum value.should do it.Just for the fun of it, after the party has finished and the horse bolted.The answer is: max() !You could also achieve the same result by using a Conditional Expression:a bit more flexible than max but admittedly longer to type.(num1>=num2)*num1+(num2>num1)*num2 will return the maximum of two values.I noticed that if you have divisions it rounds off to integer, it would be better to use:c=float(max(a1,...,an))/bSorry for the late post!gives largest number out of the numberslist without using a Max statement

Python - Extracting and Saving Video Frames

GShocked

[Python - Extracting and Saving Video Frames](https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames)

So I've followed this tutorial but it doesn't seem to do anything. Simply nothing. It waits a few seconds and closes the program. What is wrong with this code? Also, in the comments it says that this limits the frames to 1000? Why?EDIT:

I tried doing success = True first but that didn't help. It only created one image that was 0 bytes. 

2015-10-23 20:48:20Z

So I've followed this tutorial but it doesn't seem to do anything. Simply nothing. It waits a few seconds and closes the program. What is wrong with this code? Also, in the comments it says that this limits the frames to 1000? Why?EDIT:

I tried doing success = True first but that didn't help. It only created one image that was 0 bytes. From here download this video so we have the same video file for the test. Make sure to have that mp4 file in the same directory of your python code. Then also make sure to run the python interpreter from the same directory.Then modify the code, ditch waitKey that's wasting time also without a window it cannot capture the keyboard events. Also we print the success value to make sure it's reading the frames successfully.How does that go?So here was the final code that worked:So for this to work, you'll have to get some stuff. First, download OpenCV2. Then install this for Python 2.7.x. Go to the FFmpeg folder inside the 3rd party folder (something like C:\OpenCV\3rdparty\ffmpeg, but I'm not sure). Copy opencv_ffmpeg.dll (or the x64 version if your python version is x64) and paste it into your Python folder (probably C:\Python27). Rename it opencv_ffmpeg300.dll if your OpenCV version is 3.0.0 (you can find it here), and change accordingly to your version. Btw, you must have your python folder in your environment path.To extend on this question (& answer by @user2700065) for a slightly different cases, if anyone does not want to extract every frame but wants to extract frame every one second. So a 1-minute video will give 60 frames(images).This is a tweak from previous answer for python 3.x from @GShocked, I would post it to the comment, but dont have enough reputationThis is Function which will convert most of the video formats to number of frames there are in the video. It works on Python3 with OpenCV 3+It supports .mts and normal files like .mp4 and .avi. Tried and Tested on .mts files. Works like a Charm. After a lot of research on how to convert frames to video I have created this function hope this helps. We require opencv for this: change the value of fps(frames per second),input folder path and output folder path according to your own local locationsThe previous answers have lost the first frame. And it will be nice to store the images in a folder.By the way, you can check the frame rate by VLC.  Go to windows -> media information -> codec detailsThis code  extract frames from the video and save the frames in .jpg formate I am using Python via Anaconda's Spyder software. Using the original code listed in the question of this thread by @Gshocked, the code does not work (the python won't read the mp4 file). So I downloaded OpenCV 3.2 and copied "opencv_ffmpeg320.dll" and "opencv_ffmpeg320_64.dll" from the "bin" folder. I pasted both of these dll files to Anaconda's "Dlls" folder. Anaconda also has a "pckgs" folder...I copied and pasted the entire "OpenCV 3.2" folder that I downloaded to the Anaconda "pckgs" folder. Finally, Anaconda has a "Library" folder which has a "bin" subfolder. I pasted the "opencv_ffmpeg320.dll" and "opencv_ffmpeg320_64.dll" files to that folder.After closing and restarting Spyder, the code worked. I'm not sure which of the three methods worked, and I'm too lazy to go back and figure it out. But it works so, cheers!This function extracts images from video with 1 fps, IN ADDITION it identifies the last frame and stops reading also:

How to make the python interpreter correctly handle non-ASCII characters in string operations?

adergaard

[How to make the python interpreter correctly handle non-ASCII characters in string operations?](https://stackoverflow.com/questions/1342000/how-to-make-the-python-interpreter-correctly-handle-non-ascii-characters-in-stri)

I have a string that looks like so:The clear cut way to trim this string (as I understand Python) is simply to say the string is in a variable called s, we get:That should do the trick. But of course it complains that the non-ASCII character '\xc2' in file blabla.py is not encoded.I never quite could understand how to switch between different encodings.Here's the code, it really is just the same as above, but now it's in context. The file is saved as UTF-8 in notepad and has the following header:The code:It gets no further than s.replace...

2009-08-27 15:53:31Z

I have a string that looks like so:The clear cut way to trim this string (as I understand Python) is simply to say the string is in a variable called s, we get:That should do the trick. But of course it complains that the non-ASCII character '\xc2' in file blabla.py is not encoded.I never quite could understand how to switch between different encodings.Here's the code, it really is just the same as above, but now it's in context. The file is saved as UTF-8 in notepad and has the following header:The code:It gets no further than s.replace...Python 2 uses ascii as the default encoding for source files, which means you must specify another encoding at the top of the file to use non-ascii unicode characters in literals. Python 3 uses utf-8 as the default encoding for source files, so this is less of an issue.See:

http://docs.python.org/tutorial/interpreter.html#source-code-encodingTo enable utf-8 source encoding, this would go in one of the top two lines:The above is in the docs, but this also works:Additional considerations:edit: my first impulse is always to use a filter, but the generator expression is more memory efficient (and shorter)...Keep in mind that this is guaranteed to work with UTF-8 encoding (because all bytes in multi-byte characters have the highest bit set to 1).The following code will replace all non ASCII characters with question marks.Using Regex:Way too late for an answer, but the original string was in UTF-8 and '\xc2\xa0' is UTF-8 for NO-BREAK SPACE.  Simply decode the original string as s.decode('utf-8') (\xa0 displays as a space when decoded incorrectly as Windows-1252 or latin-1:This will print out 6 918 417 712I know it's an old thread, but I felt compelled to mention the translate method, which is always a good way to replace all character codes above 128 (or other if necessary).Usage : str.translate(table[, deletechars])Starting with Python 2.6, you can also set the table to None, and use deletechars to delete the characters you don't want as in the examples shown in the standard docs at http://docs.python.org/library/stdtypes.html.With unicode strings, the translation table is not a 256-character string but a dict with the ord() of relevant characters as keys. But anyway getting a proper ascii string from a unicode string is simple enough, using the method mentioned by truppo above, namely : unicode_string.encode("ascii", "ignore")As a summary, if for some reason you absolutely need to get an ascii string (for instance, when you raise a standard exception with raise Exception, ascii_message ), you can use the following function:The good thing with translate is that you can actually convert accented characters to relevant non-accented ascii characters instead of simply deleting them or replacing them by '?'. This is often useful, for instance for indexing purposes.and make your .py file unicode.This is a dirty hack, but may work.For what it was worth, my character set was utf-8 and I had included the classic "# -*- coding: utf-8 -*-" line.However, I discovered that I didn't have Universal Newlines when reading this data from a webpage. My text had two words, separated by "\r\n". I was only splitting on the \n and replacing the "\n".Once I looped through and saw the character set in question, I realized the mistake.So, it could also be within the ASCII character set, but a character that you didn't expect.

Making heatmap from pandas DataFrame

Curious

[Making heatmap from pandas DataFrame](https://stackoverflow.com/questions/12286607/making-heatmap-from-pandas-dataframe)

I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. 

2012-09-05 17:18:21Z

I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. You want matplotlib.pcolor:For people looking at this today, I would recommend the Seaborn heatmap() as documented here.The example above would be done as follows:

Where %matplotlib is an IPython magic function for those unfamiliar.If you don't need a plot per say, and you're simply interested in adding color to represent the values in a table format, you can use the style.background_gradient() method of the pandas data frame. This method colorizes the HTML table that is displayed when viewing pandas data frames in e.g. the JupyterLab Notebook and the result is similar to using "conditional formatting" in spreadsheet software:For detailed usage, please see the more elaborate answer I provided on the same topic previously and the styling section of the pandas documentation.Useful sns.heatmap api is here.  Check out the parameters, there are a good number of them.  Example:If you want an interactive heatmap from a Pandas DataFrame and you are running a Jupyter notebook, you can try the interactive Widget Clustergrammer-Widget, see interactive notebook on NBViewer here, documentation hereAnd for larger datasets you can try the in-development Clustergrammer2 WebGL widget (example notebook here)Please note that the authors of seaborn only want seaborn.heatmap to work with categorical dataframes. It's not general.If your index and columns are numeric and/or datetime values, this code will serve you well.Matplotlib heat-mapping function pcolormesh requires bins instead of indices, so there is some fancy code to build bins from your dataframe indices (even if your index isn't evenly spaced!). The rest is simply np.meshgrid and plt.pcolormesh.Call it using heatmap(df), and see it using plt.show().

error installing psycopg2, library not found for -lssl

tscizzle

[error installing psycopg2, library not found for -lssl](https://stackoverflow.com/questions/26288042/error-installing-psycopg2-library-not-found-for-lssl)

I run and I get a bunch of output that looks like:  And at the end it says:Running easy_install or doing it from source both give me the same error at the end (the part about library not found for -lssl).Running brew install (or upgrade) openssl yields the belowCan anyone help me out?

2014-10-09 21:17:31Z

I run and I get a bunch of output that looks like:  And at the end it says:Running easy_install or doing it from source both give me the same error at the end (the part about library not found for -lssl).Running brew install (or upgrade) openssl yields the belowCan anyone help me out?For anyone looking for a solution for this on macOS Sierra 10.12 (or later, most likely): I fixed this by installing the command line tools:After that, pip install psycopg2 should work.If it doesn't, you could also try to link against brew's openssl:with openssl installed via brew. Note that the brew link openssl --force does not work anymore:As @macho points out below if this still does not work, you might need to use the --no-cache option of pip, e.g.I had OpenSSL installed from brew (brew install openssl)The following worked for me:When running brew link openssl I get the following message:Following this advice here's the pip command you need to use:On mojave I added these to the .bash_profilewas then able to install psycopg 2.8.3 in a python 3.7.4 virtualenv.This after reinstalling xcode and the command line tools.All the answers above helped!What worked for me was the hint provided in the command to link openssl,This's the problem of new macOs version, where pip cannot install cryptography. What fixed my problem is to provide the env to the install command:You can replace <YOUR COMMAND HERE> with pip install cryptography, or pip install <SOMETHING THAT REQUIRES cryptography> for example.Credit to this article: Fixing macOS Sierra fatal error: 'openssl/opensslv.h' or 'openssl/aes.h' file not foundUsing Fish, the following two commands solved this issue for me after installing OpenSSL with Homebrew.Use brew info openssl to get up-to-date info.Recently had this problem in High Sierra, having just installed Python 3.7 in a virtualenv.The solution is to use a later version of psycopg2. Version 2.7.7 worked, where 2.7.1 did not.Instead of installing psycopg2, install psycopg2-binary, from the same authors:This is what the documentation says about this PyPI package:I was having this issue on Mojave. Mojave does not create a /usr/include directory, which psycopg2 needs to install. This was not obvious. I found the solution here:

How to update Xcode from command line, which references:

https://forums.developer.apple.com/thread/104296I had this same error and got it to resolve after I pip installed cythonRunning PyCharm from conda environment, solved my issue using:I've managed to fix it by using: I am not sure how this differs from the brew uninstall/upgrades that I did on OpenSSL in prior attempts I've made. My assumption is that these operations left some of the "faulty" shared libraries which were preventing this from working. Note that this also fixed issues with installing python cryptography module.

Integrating MySQL with Python in Windows

Sammy

[Integrating MySQL with Python in Windows](https://stackoverflow.com/questions/645943/integrating-mysql-with-python-in-windows)

I am finding it difficult to use MySQL with Python in my windows system.I am currently using Python 2.6. I have tried to compile MySQL-python-1.2.3b1 (which is supposed to work for Python 2.6 ?) source code using the provided setup scripts. The setup script runs and it doesn't report any error but it doesn't generate _mysql module. I have also tried setting up MySQL for Python 2.5 with out success. The problem with using 2.5 is that Python 2.5 is compiled with visual studio 2003 (I installed it using the provided binaries). I have visual studio 2005 on my windows system. Hence setuptools fails to generate _mysql module.Any help ?

2009-03-14 13:53:28Z

I am finding it difficult to use MySQL with Python in my windows system.I am currently using Python 2.6. I have tried to compile MySQL-python-1.2.3b1 (which is supposed to work for Python 2.6 ?) source code using the provided setup scripts. The setup script runs and it doesn't report any error but it doesn't generate _mysql module. I have also tried setting up MySQL for Python 2.5 with out success. The problem with using 2.5 is that Python 2.5 is compiled with visual studio 2003 (I installed it using the provided binaries). I have visual studio 2005 on my windows system. Hence setuptools fails to generate _mysql module.Any help ?Download page for python-mysqldb. The page includes binaries for 32 and 64 bit versions of for Python 2.5, 2.6 and 2.7. There's also discussion on getting rid of the deprecation warning.UPDATE: This is an old answer. Currently, I would recommend using PyMySQL. It's pure python, so it supports all OSes equally, it's almost a drop-in replacement for mysqldb, and it also works with python 3. The best way to install it is using pip. You can install it from here (more instructions here), and then run:This may read like your grandpa givin advice, but all answers here did not mention the best way: go nd install ActivePython instead of python.org windows binaries. I was really wondering for a long time why Python development on windows was such a pita - until I installed activestate python. I am not affiliated with them. It is just the plain truth. Write it on every wall: Python development on Windows = ActiveState! 

you then just pypm install mysql-python and everything works smoothly. no compile orgy. no strange errors. no terror. Just start coding and doing real work after five minutes. 

This is the only way to go on windows. Really.As Python newbie learning the Python ecosystem I've just completed this.I found a location were one person had successfully built mysql for python2.6, sharing the link, http://www.technicalbard.com/files/MySQL-python-1.2.2.win32-py2.6.exe...you might see a warning while import MySQLdb which is fine and that won’t hurt anything,C:\Python26\lib\site-packages\MySQLdb__init__.py:34: DeprecationWarning: the sets module is deprecated

from sets import ImmutableSetWhat about pymysql?  It's pure Python, and I've used it on Windows with considerable success, bypassing the difficulties of compiling and installing mysql-python.You're not the only person having problems with Python 2.6 and MySQL (http://blog.contriving.net/2009/03/04/using-python-26-mysql-on-windows-is-nearly-impossible/). Here's an explanation how it should run under Python 2.5 http://i.justrealized.com/2008/04/08/how-to-install-python-and-django-in-windows-vista/

Good luckThere are Windows binaries for MySQL-Python (2.4 & 2.5) available on Sourceforge.  Have you tried those?The precompiled binaries on http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python is just worked for me.On Python 3.4 I've installed mysqlclient from http://www.lfd.uci.edu/~gohlke/pythonlibs/ with pip install mysqlclient and it's working.You can try to use myPySQL. It's really easy to use; no compilation for windows, and even if you need to compile it for any reason, you only need Python and Visual C installed (not mysql).http://code.google.com/p/mypysql/Good luckBecause I am running python in a (pylons/pyramid) virtualenv, I could not run the binary installers (helpfully) linked to previously. I had problems following the steps with Willie's answer, but I determined that the problem is (probably) that I am running windows 7 x64 install, which puts the registry key for mysql in a slightly different location, specifically in my case (note: I am running version 5.5) in: "HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\MySQL AB\MySQL Server 5.5".HOWEVER, "HKEY_LOCAL_MACHINE\" cannot be included in the path or it will fail.Also, I had to do a restart between steps 3 and 4.After working through all of this, IMO it would have been smarter to run the entire python dev environment from cygwin.If you are looking for Python 3.2 this seems the best solution I found so far Source: http://wiki.python.org/moin/MySQLYou might want to also consider making use of Cygwin, it has mysql python libraries in the repository. You can also use pyodbc with the MySQL Connector/ODBC to use MySQL on Windows.  Unixodbc is also available to make the code compatible on Linux.  Pyodbc uses the standard Python DB API 2.0 so if you stick with that switching between MySQL/PostgreSQL/SQLite/ODBC/JDBC drivers etc. should be relatively painless.upvoted itsadok's answer because it led me to the installation for python 2.7 as well, which is located here: http://www.codegood.com/archives/129Got sick of the installation troubles with MySQLdb and tried pymysql instead.Easy setup;And APIs are pretty much the same.

Python Regex - How to Get Positions and Values of Matches

Greg

[Python Regex - How to Get Positions and Values of Matches](https://stackoverflow.com/questions/250271/python-regex-how-to-get-positions-and-values-of-matches)

How can I get the start and end positions of all matches using the re module? For example given the pattern r'[a-z]' and the string 'a1b2c3d4' I'd want to get the positions where it finds each letter. Ideally, I'd like to get the text of the match back too.

2008-10-30 14:04:33Z

How can I get the start and end positions of all matches using the re module? For example given the pattern r'[a-z]' and the string 'a1b2c3d4' I'd want to get the positions where it finds each letter. Ideally, I'd like to get the text of the match back too.Taken from Regular Expression HOWTOCombine that with:In Python 2.2, the finditer() method is also available, returning a sequence of MatchObject instances as an iterator.you should be able to do something on the order ofFor Python 3.x You shall get \n separated tuples (comprising first and last indices of the match, respectively) and the match itself, for each hit in the string.note that the span & group are indexed for multi capture groups in a regex

Check if argparse optional argument is set or not

Madeleine P. Vincent

[Check if argparse optional argument is set or not](https://stackoverflow.com/questions/30487767/check-if-argparse-optional-argument-is-set-or-not)

I would like to check whether an optional argparse argument has been set by the user or not.Can I safely check using isset?Something like this:Does this work the same for float / int / string type arguments?I could set a default parameter and check it (e.g., set myArg = -1, or "" for a string, or "NOT_SET").  However, the value I ultimately want to use is only calculated later in the script.  So I would be setting it to -1 as a default, and then updating it to something else later. This seems a little clumsy in comparison with simply checking if the value was set by the user.

2015-05-27 16:08:23Z

I would like to check whether an optional argparse argument has been set by the user or not.Can I safely check using isset?Something like this:Does this work the same for float / int / string type arguments?I could set a default parameter and check it (e.g., set myArg = -1, or "" for a string, or "NOT_SET").  However, the value I ultimately want to use is only calculated later in the script.  So I would be setting it to -1 as a default, and then updating it to something else later. This seems a little clumsy in comparison with simply checking if the value was set by the user.I think that optional arguments (specified with --) are initialized to None if they are not supplied. So you can test with is not None. Try the example below:As @Honza notes is None is a good test.  It's the default default, and the user can't give you a string that duplicates it.You can specify another default='mydefaultvalue, and test for that.  But what if the user specifies that string?  Does that count as setting or not?You can also specify default=argparse.SUPPRESS.  Then if the user does not use the argument, it will not appear in the args namespace.  But testing that might be more complicated:Internally the parser keeps a list of seen_actions, and uses it for 'required' and 'mutually_exclusive' testing.  But it isn't available to you out side of parse_args.I think using the option default=argparse.SUPPRESS makes most sense. Then, instead of checking if the argument is not None, one checks if the argument is in the resulting namespace.You can check an optionally passed flag with store_true and store_false argument action options:This way, you don't have to worry about checking by conditional is not None. You simply check for True or False. Read more about these options in the docs hereIf your argument is positional (ie it doesn't have a "-" or a "--" prefix, just the argument, typically a file name) then you can use the nargs parameter to do this:Here is my solution to see if I am using an argparse variable This might give more insight to the above answer which I used and adapted to work for my program.

Check if Python Package is installed

Kevin

[Check if Python Package is installed](https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed)

What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script. I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.My ideas for accomplishing the check

2009-06-26 20:54:38Z

What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script. I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.My ideas for accomplishing the checkIf you mean a python script, just do something like this:A better way of doing this is:The result:Check if requests is installed:Why this way? Sometimes you have app name collisions. Importing from the app namespace doesn't give you the full picture of what's installed on the system.Note, that proposed solution works:Cases when it might not work:A better way of doing this is:For pip>=10.x use:Why this way? Sometimes you have app name collisions. Importing from the app namespace doesn't give you the full picture of what's installed on the system.As a result, you get a list of pkg_resources.Distribution objects. See the following as an example:Make a list of it:Check if requests is installed:As of Python 3.3, you can use the find_spec() methodIf you want to have the check from the terminal, you can runand if nothing is returned, the package is not installed.If perhaps you want to automate this check, so that for example you can install it if missing, you can have the following in your bash script:As an extension of this answer:For Python 2.*, pip show <package_name> will perform the same task.For example pip show numpy will return the following or alike:You can use the pkg_resources module from setuptools. For example:Note that there is a difference between python module and a python package. A package can contain multiple modules and module's names might not match the package name. I'd like to add some thoughts/findings of mine to this topic.

I'm writing a script that checks all requirements for a custom made program. There are many checks with python modules too. There's a little issue with the solution. 

In my case one of the python modules called python-nmap, but you import it with import nmap and as you see the names mismatch. Therefore the test with the above solution returns a False result, and it also imports the module on hit, but maybe no need to use a lot of memory for a simple test/check.I also found that installed_packages will have only the packages has been installed with pip.

On my system pip freeze returns over 40 python modules, while installed_packages has only 1, the one I installed manually (python-nmap).Another solution below that I know it may not relevant to the question, but I think it's a good practice to keep the test function separate from the one that performs the install it might be useful for some.The solution that worked for me. It based on this answer How to check if a python module exists without importing itNOTE: this solution can't find the module by the name python-nmap too, I have to use nmap instead (easy to live with) but in this case the module won't be loaded to the memory whatsoever.If you'd like your script to install missing packages and continue, you could do something like this (on example of 'krbV' module in 'python-krbV' package):A quick way is to use python command line tool.

Simply type import <your module name>

You see an error if module is missing.Hmmm ... the closest I saw to a convenient answer was using the command line to try the import. But I prefer to even avoid that.How about 'pip freeze | grep pkgname'? I tried it and it works well. It also shows you the version it has and whether it is installed under version control (install) or editable (develop).class myError(exception):

 pass #or do some thing

try:

 import mymodule

except ImportError as e:

 raise myError("error was occured")In the Terminal typeExampleOpen your command prompt typeGo option #2.  If ImportError is thrown, then the package is not installed (or not in sys.path).

python NameError: global name '__file__' is not defined

Kaibing Yang

[python NameError: global name '__file__' is not defined](https://stackoverflow.com/questions/16771894/python-nameerror-global-name-file-is-not-defined)

When I run this code in python 2.7, I get this error:code is:

2013-05-27 11:11:59Z

When I run this code in python 2.7, I get this error:code is:This error comes when you append this line os.path.join(os.path.dirname(__file__)) in python interactive shell. Python Shell doesn't detect current file path in __file__ and it's related to your filepath in which you added this lineSo you should write this line os.path.join(os.path.dirname(__file__)) in file.py. and then run python file.py, It works because it takes your filepath.I had the same problem with PyInstaller and Py2exe so I came across the resolution on the FAQ from cx-freeze.When using your script from the console or as an application, the functions hereunder will deliver you the "execution path", not the "actual file path":Source:

http://cx-freeze.readthedocs.org/en/latest/faq.htmlYour old line (initial question):Substitute your line of code with the following snippet.With the above code you could add your application to the path of your os, you could execute it anywhere without the problem that your app is unable to find it's data/configuration files.Tested with python:change your codes as follows! it works for me.

`Are you using the interactive interpreter? You can useYou should read: How do I get the path of the current executed file in Python?I solved it by treating file as a string, i.e. put "__file__" (together with the quotes!) instead of __file__This works fine for me:If all you are looking for is to get your current working directory os.getcwd() will give you the same thing as os.path.dirname(__file__) as long as you have not changed the working directory elsewhere in your code.  os.getcwd() also works in interactive mode.So

    os.path.join(os.path.dirname(__file__))

becomes

    os.path.join(os.getcwd())I've run into cases where __file__ doesn't work as expected. But the following hasn't failed me so far:This is the closest thing to a Python analog to C's __FILE__.The behavior of Python's __file__ is much different than C's __FILE__. The C version will give you the original path of the source file. This is useful in logging errors and knowing which source file has the bug.Python's __file__ only gives you the name of the currently executing file, which may not be very useful in log output.You will get this if you are running the commands from the python shell:You need to execute the file directly, by passing it in as an argument to the python command:In your case, it should really be python setup.py installWhat you can do is to use the followingNote here that using the string '__file__' does indeed refer to the actual variable __file__. You can test this out yourself of course..The added bonus of this solution is the flexibilty when you are running a script partly interactively (e.g. to test/develop it), and can run it via the commandline If you're exec'ing a file via command line, you can use this hackThis worked for me in the UnrealEnginePython console, calling py.exec myfile.pyI'm having exacty the same problem and using probably the same tutorial. The function definition:is buggy, since os.path.dirname(__file__) will not return what you need. Try replacing os.path.dirname(__file__) with os.path.dirname(os.path.abspath(__file__)):I've just posted Andrew that the code snippet in current docs don't work, hopefully, it'll be corrected.

can we use xpath with BeautifulSoup?

Shiva Krishna Bavandla

[can we use xpath with BeautifulSoup?](https://stackoverflow.com/questions/11465555/can-we-use-xpath-with-beautifulsoup)

I am using BeautifulSoup to scrape a url and I had the following code Now in the above code we can use findAll to get tags and information related to them, but I want to use xpath. Is it possible to use xpath with BeautifulSoup? If possible, can anyone please provide me an example code so that it will be more helpful?

2012-07-13 06:55:19Z

I am using BeautifulSoup to scrape a url and I had the following code Now in the above code we can use findAll to get tags and information related to them, but I want to use xpath. Is it possible to use xpath with BeautifulSoup? If possible, can anyone please provide me an example code so that it will be more helpful?Nope, BeautifulSoup, by itself, does not support XPath expressions.An alternative library, lxml, does support XPath 1.0. It has a BeautifulSoup compatible mode where it'll try and parse broken HTML the way Soup does. However, the default lxml HTML parser does just as good a job of parsing broken HTML, and I believe is faster.Once you've parsed your document into an lxml tree, you can use the .xpath() method to search for elements.There is also a dedicated lxml.html() module with additional functionality.Note that in the above example I passed the response object directly to lxml, as having the parser read directly from the stream is more efficient than reading the response into a large string first. To do the same with the requests library, you want to set stream=True and pass in the response.raw object after enabling transparent transport decompression:Of possible interest to you is the CSS Selector support; the CSSSelector class translates CSS statements into XPath expressions, making your search for td.empformbody that much easier:Coming full circle: BeautifulSoup itself does have very complete CSS selector support:I can confirm that there is no XPath support within Beautiful Soup.As others have said, BeautifulSoup doesn't have xpath support.  There are probably a number of ways to get something from an xpath, including using Selenium.  However, here's a solution that works in either Python 2 or 3:I used this as a reference.BeautifulSoup has a function named findNext from current element directed childern,so:Above code can imitate the following xpath:when you use lxml all simple:but when use BeautifulSoup BS4 all simple too:try this magic:as you see, this does not support sub-tag, so i remove "/@href" partI've searched through their docs and it seems there is not xpath option. Also, as you can see here on a similar question on SO, the OP is asking for a translation from xpath to BeautifulSoup, so my conclusion would be - no, there is no xpath parsing available. Maybe you can try the following without XPathThis is a pretty old thread, but there is a work-around solution now, which may not have been in BeautifulSoup at the time. Here is an example of what I did. I use the "requests" module to read an RSS feed and get its text content in a variable called "rss_text". With that, I run it thru BeautifulSoup, search for the xpath /rss/channel/title, and retrieve its contents. It's not exactly XPath in all its glory (wildcards, multiple paths, etc.), but if you just have a basic path you want to locate, this works. 

How do I send a POST request as a JSON?

TIMEX

[How do I send a POST request as a JSON?](https://stackoverflow.com/questions/9746303/how-do-i-send-a-post-request-as-a-json)

I want to send a POST request, but one of the fields should be a list of numbers. How can I do that ? (JSON?)

2012-03-17 00:53:49Z

I want to send a POST request, but one of the fields should be a list of numbers. How can I do that ? (JSON?)If your server is expecting the POST request to be json, then you would need to add a header, and also serialize the data for your request...Python 2.xPython 3.xhttps://stackoverflow.com/a/26876308/496445If you don't specify the header, it will be the default application/x-www-form-urlencoded type.I recommend using the incredible requests module.http://docs.python-requests.org/en/v0.10.7/user/quickstart/#custom-headersfor python 3.4.2 I found the following will work:This works perfect for Python 3.5, if the URL contains Query String / Parameter value,Request URL = https://bah2.com/ws/rest/v1/concept/

Parameter value = 21f6bb43-98a1-419d-8f0c-8133669e40caYou have to add header,or you will get http 400 error.

The code works well on python2.6,centos5.4code:Here is an example of how to use urllib.request object from Python standard library.In the lastest requests package, you can use json parameter in requests.post() method to send a json dict, and the Content-Type in header will be set to application/json. There is no need to specify header explicitly.This one works fine for me with apis

Python3: ImportError: No module named '_ctypes' when using Value from module multiprocessing

htc_m8

[Python3: ImportError: No module named '_ctypes' when using Value from module multiprocessing](https://stackoverflow.com/questions/27022373/python3-importerror-no-module-named-ctypes-when-using-value-from-module-mul)

I am using Ubuntu and have installed Python 2.7.5 and 3.4.0. In Python 2.7.5 I am able to successfully assign a variable x = Value('i', 2), but not in 3.4.0. I am getting:I just updated to 3.3.2 through installing the source of 3.4.0. It installed in /usr/local/lib/python3.4.Did I update to Python 3.4 correctly?One thing I noticed that Python 3.4 is installed in usr/local/lib, while Python 3.3.2 is still installed in usr/lib, so it was not overwritten.

2014-11-19 16:49:31Z

I am using Ubuntu and have installed Python 2.7.5 and 3.4.0. In Python 2.7.5 I am able to successfully assign a variable x = Value('i', 2), but not in 3.4.0. I am getting:I just updated to 3.3.2 through installing the source of 3.4.0. It installed in /usr/local/lib/python3.4.Did I update to Python 3.4 correctly?One thing I noticed that Python 3.4 is installed in usr/local/lib, while Python 3.3.2 is still installed in usr/lib, so it was not overwritten.Installing libffi-dev and re-installing python3.7 fixed the problem for me.to cleanly build py 3.7  libffi-dev  is required or else later stuff will failIf using RHEL/Fedora:orIf using Debian/Ubuntu:  On a fresh Debian image, cloning https://github.com/python/cpython and running:Now execute the configure file cloned above:Got 3.7 installed and working for me.Looks like I said I would update this answer with some more explanation and two years later I don't have much to add.Aside from that I guess the choice would be to either read through the cpython codebase looking for #include directives that need to be met, but what I usually do is keep trying to install the package and just keep reading through the output installing the required packages until it succeeds.Reminds me of the story of the Engineer, the Manager and the Programmer whose car rolls down a hill.Detailed steps to install Python 3.7 in CentOS or any redhat linux machine: Thought I'd add the Centos installs:Check python version:python3 -VCreate virtualenv:virtualenv -p python3 venvI run into this error when I tried to install Python 3.7.3 in Ubuntu 18.04 with next command: $ pyenv install 3.7.3.

Installation succeeded after running $ sudo apt-get update && sudo apt-get install libffi-dev (as suggested here).

The issue was solved there.None of the solution worked. You have to recompile your python again; once all the required packages were completely installed.Follow this:https://gist.github.com/jerblack/798718c1910ccdd4ede92481229043be Refer to this thread, for customized installation of libffi, it is difficult for Python3.7 to find the library location of libffi. An alternative method is to set the CONFIGURE_LDFLAGS variable in the Makefile, for example CONFIGURE_LDFLAGS="-L/path/to/libffi-3.2.1/lib64".If you use pyenv and get error "No module named '_ctypes'" (like i am) on Debian/Raspbian/Ubuntu you need to run this commands:Put your version of python instead of 3.7.6

Chained method calls indentation style in Python [duplicate]

katspaugh

[Chained method calls indentation style in Python [duplicate]](https://stackoverflow.com/questions/8683178/chained-method-calls-indentation-style-in-python)

From reading PEP-8, I get it that you should put the closing parenthesis on the same line as the last argument in function calls:Probably, long expressions are best to avoid at all. But if it's undesirable, how would you go about multiple chained method calls? Should the closing paren be on a new line?What about no-arguments methods? How to write them on multiple lines without referencing the intermediate return values?Update: There's a duplicate question of How to break a line of chained methods in Python?. The accepted answer suggests a familiar from jQuery style of starting each new line with a dot. The author doesn't provide any reasons or authoritative references, so I'd like to get a confirmation on such style or an alternative.

2011-12-30 19:23:46Z

From reading PEP-8, I get it that you should put the closing parenthesis on the same line as the last argument in function calls:Probably, long expressions are best to avoid at all. But if it's undesirable, how would you go about multiple chained method calls? Should the closing paren be on a new line?What about no-arguments methods? How to write them on multiple lines without referencing the intermediate return values?Update: There's a duplicate question of How to break a line of chained methods in Python?. The accepted answer suggests a familiar from jQuery style of starting each new line with a dot. The author doesn't provide any reasons or authoritative references, so I'd like to get a confirmation on such style or an alternative.This is a case where a line continuation character is preferred to open parentheses.The need for this style becomes more obvious as method names get longer and as methods start taking arguments:PEP 8 is intend to be interpreted with a measure of common-sense and an eye for both the practical and the beautiful.  Happily violate any PEP 8 guideline that results in ugly or hard to read code.That being said, if you frequently find yourself at odds with PEP 8, it may be a sign that there are readability issues that transcend your choice of whitespace :-)I think the best is to use () to force line joining, and to do this:It's not ideal, but I like that it stands out visually and makes it somewhat obvious what the chain of calls is. It allows end-of-line comments, which \ newline does not.

Remove leading and trailing spaces?

fpena06

[Remove leading and trailing spaces?](https://stackoverflow.com/questions/10443400/remove-leading-and-trailing-spaces)

I'm having a hard time trying to use .strip with the following line of code. Thanks for the help.

2012-05-04 05:56:34Z

I'm having a hard time trying to use .strip with the following line of code. Thanks for the help.You can use the strip() to remove trailing and leading spaces.Note: the internal spaces are preservedExpand your one liner into multiple lines. Then it becomes easy:Should be noted that strip() method would trim any leading and trailing whitespace characters from the string (if there is no passed-in argument). For guys like me who want to trim space character while keeping the others (like newline), this answer might be helpful:strip([chars]): You can pass in optional characters to strip([chars]) method. Python will look for occurrences of these characters and trim the given string accordingly.

Is there a Python function to determine which quarter of the year a date is in?

Jason Christa

[Is there a Python function to determine which quarter of the year a date is in?](https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in)

Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?

2009-09-10 15:54:08Z

Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?Given an instance x of datetime.date, (x.month-1)//3 will give you the quarter (0 for first quarter, 1 for second quarter, etc -- add 1 if you need to count from 1 instead;-).Originally two answers, multiply upvoted and even originally accepted (both currently deleted), were buggy -- not doing the -1 before the division, and dividing by 4 instead of 3.  Since .month goes 1 to 12, it's easy to check for yourself what formula is right:gives 1 1 1 2 2 2 2 3 3 3 3 4 -- two four-month quarters and a single-month one (eep).gives 1 1 1 2 2 2 3 3 3 4 4 4 -- now doesn't this look vastly preferable to you?-)This proves that the question is well warranted, I think;-).I don't think the datetime module should necessarily have every possible useful calendric function, but I do know I maintain a (well-tested;-) datetools module for the use of my (and others') projects at work, which has many little functions to perform all of these calendric computations -- some are complex, some simple, but there's no reason to do the work over and over (even simple work) or risk bugs in such computations;-).IF you are already using pandas, it's quite simple.If you have a date column in a dataframe, you can easily create a new quarter column:I would suggest another arguably cleaner solution. If X is a datetime.datetime.now() instance, then the quarter is:ceil has to be imported from math module as it can't be accessed directly. For anyone trying to get the quarter of the fiscal year, which may differ from the calendar year, I wrote a Python module to do just this.Installation is simple. Just run:There are no dependencies, and fiscalyear should work for both Python 2 and 3.It's basically a wrapper around the built-in datetime module, so any datetime commands you are already familiar with will work. Here's a demo:fiscalyear is hosted on GitHub and PyPI. Documentation can be found at Read the Docs. If you're looking for any features that it doesn't currently have, let me know!if m is the month number...This method works for any mapping:We have just generated a function int->intThis method is also fool-proofHere is an example of a function that gets a datetime.datetime object and returns a unique string for each quarter:And the output is:This is an old question but still worthy of discussion.Here is my solution, using the excellent dateutil module.So first_day is the first day of the quarter, and last_day is the last day of the quarter (calculated by finding the first day of the next quarter, minus one day).For those, who are looking for financial year quarter data,

using pandas,reference: 

pandas period indexhmmm so calculations can go wrong, here is a better version (just for the sake of it)Here is a verbose, but also readable solution that will work for datetime and date instancesusing dictionaries, you can pull this off by This is very simple and works in python3:

Python equivalent of D3.js

Eiyrioü von Kauyf

[Python equivalent of D3.js](https://stackoverflow.com/questions/12977517/python-equivalent-of-d3-js)

Can anyone recommend a Python library that can do interactive graph visualization?I specifically want something like d3.js but for python and ideally it would be 3D as well. I have looked at:

2012-10-19 15:26:16Z

Can anyone recommend a Python library that can do interactive graph visualization?I specifically want something like d3.js but for python and ideally it would be 3D as well. I have looked at:You could use d3py a python module that generate xml pages embedding d3.js script. For example :Plotly supports interactive 2D and 3D graphing. Graphs are rendered with D3.js and can be created with a Python API, matplotlib, ggplot for Python, Seaborn, prettyplotlib, and pandas. You can zoom, pan, toggle traces on and off, and see data on the hover. Plots can be embedded in HTML, apps, dashboards, and IPython Notebooks. Below is a temperature graph showing interactivity. See the gallery of IPython Notebooks tutorials for more examples. 

  

The docs provides examples of supported plot types and code snippets.

Specifically to your question, you can also make interactive plots from NetworkX. 

For 3D plotting with Python, you can make 3D scatter, line, and surface plots that are similarly interactive. Plots are rendered with WebGL. For example, see a 3D graph of UK Swap rates.

Disclosure: I'm on the Plotly team.Have you looked at vincent?  Vincent takes Python data objects and converts them to Vega visualization grammar.  Vega is a higher-level visualization tool built on top of D3.  As compared to D3py, the vincent repo has been updated more recently.  Though the examples are all static D3.more info:The graphs can be viewed in Ipython, just add this codeOr output to JSON where you can view the JSON output graph in the Vega online editor (http://trifacta.github.io/vega/editor/) or view them on your Python server locally.  More info on viewing can be found in the pypi link above.Not sure when, but the Pandas package should have D3 integration at some point.  http://pandas.pydata.org/developers.htmlBokeh is a Python visualization library that supports interactive visualization. Its primary output backend is HTML5 Canvas and uses client/server model.examples: http://continuumio.github.io/bokehjs/One recipe that I have used (described here: Co-Director Network Data Files in GEXF and JSON from OpenCorporates Data via Scraperwiki and networkx ) runs as follows:The networkx JSON exporter takes the form:Alternatively you can export the network as a GEXF XML file and then import this representation into the sigma.js Javascript visualisation library.Another option is bokeh which just went to version 0.3.For those who recommended pyd3, it is no longer under active development and points you to vincent. vincent is also no longer under active development and recommends using altair.So if you want a pythonic d3, use altair.Check out python-nvd3. It is a python wrapper for nvd3. Looks cooler than d3.py and also has more chart options. Try https://altair-viz.github.io/ - the successor of d3py and vincent. See also I would suggest using mpld3 which combines D3js javascript visualizations with matplotlib of python.The installation and usage is really simple and it has some cool plugins and interactive stuffs.http://mpld3.github.io/Plotly can do some cool stuffs for you https://plot.ly/Produces highly interactive graphs that can be easily embedded withing the HTML pages for your private server or website using its off line API. Update:

I am note sure about its 3D plotting capabilities, for 2D graphs is awesome

Thanks  You can also choose to serialize your data and then visualize it in D3.js, as done here:

Use Python & Pandas to Create a D3 Force Directed Network Diagram (It comes with a jupyter notebook as well!)Here is the gist. You serialize your graph data in this format:Then you load the data in with d3.js:For the routine drawGraph I refer you to the link, however.There is an interesting port of NetworkX to Javascript that might do what you want.  See http://felix-kling.de/JSNetworkX/See:Is there a good interactive 3D graph library out there?The accepted answer suggests the following program, which apparently has python bindings: http://ubietylab.net/ubigraph/EditI'm not sure about the interactivity of NetworkX, but you can definitely make 3D graphs. There is at least one example in the gallery:http://networkx.lanl.gov/examples/drawing/edge_colormap.htmlAnd another example in the 'examples'. This one, however, requires that you have Mayavi.http://networkx.lanl.gov/examples/3d_drawing/mayavi2_spring.htmlI've got a good example of automatically generating D3.js network diagrams using Python here: http://brandonrose.org/ner2snaThe cool thing is that you end up with auto-generated HTML and JS and can embed the interactive D3 chart in a notebook with an IFrame

Using .sort with PyMongo

KennyPowers

[Using .sort with PyMongo](https://stackoverflow.com/questions/10242149/using-sort-with-pymongo)

With PyMongo, when I try to retrieve objects sorted by their 'number' and 'date' fields like this:I get this error:What's wrong with my sort query?

2012-04-20 07:39:59Z

With PyMongo, when I try to retrieve objects sorted by their 'number' and 'date' fields like this:I get this error:What's wrong with my sort query?sort should be a list of key-direction pairs, that isThe reason why this has to be a list is that the ordering of the arguments matters and dicts are not ordered in Python < 3.6

Can you list the keyword arguments a function receives?

dbr

[Can you list the keyword arguments a function receives?](https://stackoverflow.com/questions/196960/can-you-list-the-keyword-arguments-a-function-receives)

I have a dict, which I need to pass key/values as keyword arguments.. For example..This works fine, but if there are values in the d_args dict that are not accepted by the example function, it obviously dies.. Say, if the example function is defined as def example(kw2):This is a problem since I don't control either the generation of the d_args, or the example function.. They both come from external modules, and example only accepts some of the keyword-arguments from the dict..Ideally I would just doI will probably just filter the dict, from a list of valid keyword-arguments, but I was wondering: Is there a way to programatically list the keyword arguments the a specific function takes?

2008-10-13 07:55:18Z

I have a dict, which I need to pass key/values as keyword arguments.. For example..This works fine, but if there are values in the d_args dict that are not accepted by the example function, it obviously dies.. Say, if the example function is defined as def example(kw2):This is a problem since I don't control either the generation of the d_args, or the example function.. They both come from external modules, and example only accepts some of the keyword-arguments from the dict..Ideally I would just doI will probably just filter the dict, from a list of valid keyword-arguments, but I was wondering: Is there a way to programatically list the keyword arguments the a specific function takes?A little nicer than inspecting the code object directly and working out the variables is to use the inspect module.If you want to know if its callable with a particular set of args, you need the args without a default already specified.  These can be got by:Then a function to tell what you are missing from your particular dict is:Similarly, to check for invalid args, use:And so a full test if it is callable is :(This is good only as far as python's arg parsing.  Any runtime checks for invalid values in kwargs obviously can't be detected.)This will print names of all passable arguments, keyword and non-keyword ones:This is because first co_varnames are always parameters (next are local variables, like y in the example above).So now you could have a function:Which you then could use like this:EDIT: A small addition: if you really need only keyword arguments of a function, you can use the func_defaults attribute to extract them:You could now call your function with known args, but extracted kwargs, e.g.:This assumes that func uses no *args or **kwargs magic in its signature.In Python 3.0:For a Python 3 solution, you can use inspect.signature and filter according to the kind of parameters you'd like to know about. Taking a sample function with positional or keyword, keyword-only, var positional and var keyword parameters:You can create a signature object for it:and then filter with a list comprehension to find out the details you need:and, similarly, for var positionals using p.VAR_POSITIONAL and var keyword with VAR_KEYWORD. In addition, you can add a clause to the if to check if a default value exists by checking if p.default equals p.empty.Extending DzinX's answer:

How do I run a Python program?

Sergio Tapia

[How do I run a Python program?](https://stackoverflow.com/questions/1522564/how-do-i-run-a-python-program)

So I'm starting like Python a bit, but I'm having trouble erm...running it. LolI'm using IDLE for now, but its no use whatsoever because you can only run a couple of lines at a time.I'm also using Komodo Edit to create the actual .py files.My question is, how can I run the .py files to test out the actual program? I'm using Windows 7, and Komodo Edit 5 as my IDE. Pressing F5 in Komodo doesn't do anythin at all.

2009-10-05 21:42:30Z

So I'm starting like Python a bit, but I'm having trouble erm...running it. LolI'm using IDLE for now, but its no use whatsoever because you can only run a couple of lines at a time.I'm also using Komodo Edit to create the actual .py files.My question is, how can I run the .py files to test out the actual program? I'm using Windows 7, and Komodo Edit 5 as my IDE. Pressing F5 in Komodo doesn't do anythin at all.I'm very glad you asked! I was just working on explaining this very thing in our wikibook (which is obviously incomplete).  We're working with Python novices, and had to help a few through exactly what you're asking!  Command-line Python in Windows: If you get this message:  then python (the interpreter program that can translate Python into 'computer instructions') isn't on your path (see Putting Python in Your Path below).  Then try calling it like this (assuming Python2.6, installed in the usual location):> C:\Python26\python.exe first.py(Advanced users:  instead of first.py, you could write out first.py's full path of C:\Documents and Settings\Gregg\Desktop\pyscripts\first.py)Putting Python In Your PathWindowsIn order to run programs, your operating system looks in various places,

and tries to match the name of the program / command you typed with some 

programs along the way.  In windows:control panel > system >  advanced > |Environmental Variables| > system variables -> Paththis needs to include:  C:\Python26; (or equivalent).  If you put it at the front,

it will be the first place looked.  You can also add it at the end, which is possibly saner.Then restart your prompt, and try typing 'python'.  If it all worked, you should

get a ">>>" prompt.You can just callIn IDLE press F5You can open your .py file with IDLE and press F5 to run it. You can open that same file with other editor ( like Komodo as you said ) save it and press F5 again; F5 works with IDLE ( even when the editing is done with another tool ).If you want to run it directly from Komodo according to this article: Executing Python Code Within Komodo Edit you have to:Python itself comes with an editor that you can access from the IDLE File > New File menu option.Write the code in that file, save it as [filename].py and then (in that same file editor window) press F5 to execute the code you created in the IDLE Shell window.Note: it's just been the easiest and most straightforward way for me so far.if you dont want call filename.py you can add .PY to the PATHEXT, that way you will just call filenameIf this helps anyone, neither "python [filename].py" or "python.exe [filename.py]" worked for me, but "start python [filename].py" did. If anyone else is experiencing issues with the former two commands, try the latter one.What I just did, to open a simple python script by double clicking. I just added a batch file to the directory containing the script:(I have the python executable on my system path. If not one would need include its complete path of course.)Then I just can double click on the batch file to run the script. The third line keeps the cmd window from being dismissed as soon as the script ends, so you can see the results. :) When you're done just close the command window.Navigate your file location just press Shift button and click file name. Click tab Open command window here and write in your command prompt python file_name.pyIf you want to run the #'.py' file

just write in print() in your code to actually see it get printed. 

Unlike python IDLE, you need to specify what you want to print using print() command. 

For eg. OUTPUT

[1, 2, 3, 4, 5]

PythonI have tried many of the commands listed above, however none worked, even after setting my path to include the directory where I installed Python. The command py -3 file.py always works for me, and if I want to run Python 2 code, as long as Python 2 is in my path, just changing the command to py -2 file.py works perfectly.I am using Windows, so I'm not too sure if this command will work on Linux, or Mac, but it's worth a try.

Matplotlib - Move X-Axis label downwards, but not X-Axis Ticks

victorhooi

[Matplotlib - Move X-Axis label downwards, but not X-Axis Ticks](https://stackoverflow.com/questions/6406368/matplotlib-move-x-axis-label-downwards-but-not-x-axis-ticks)

I'm using Matplotlib to plot a histogram.

Using tips from my previous question: Matplotlib - label each bin,

I've more or less go the kinks worked out.There's one final issue - previously - the x-axis label ("Time (in milliseconds)") was being rendered underneath the x-axis tickmarks (0.00, 0.04, 0.08, 0.12 etc.)Using the advice from Joe Kingston (see question above), I tried using:However, this moves both the x-axis tickmarks (0.00, 0.04, 0.08, 0.12 etc.), as well as the x-axis label ("Time (in milliseconds)"):Is there any way to move only the x-axis label to underneath the three rows of figures?Nb: You may need to open the PNGs below directly - Right Click on the image, then View Image (in FF), or Open image in new tab (Chrome). The image resize done by SO has rendered them nigh unreadable

2011-06-20 02:36:48Z

I'm using Matplotlib to plot a histogram.

Using tips from my previous question: Matplotlib - label each bin,

I've more or less go the kinks worked out.There's one final issue - previously - the x-axis label ("Time (in milliseconds)") was being rendered underneath the x-axis tickmarks (0.00, 0.04, 0.08, 0.12 etc.)Using the advice from Joe Kingston (see question above), I tried using:However, this moves both the x-axis tickmarks (0.00, 0.04, 0.08, 0.12 etc.), as well as the x-axis label ("Time (in milliseconds)"):Is there any way to move only the x-axis label to underneath the three rows of figures?Nb: You may need to open the PNGs below directly - Right Click on the image, then View Image (in FF), or Open image in new tab (Chrome). The image resize done by SO has rendered them nigh unreadableuse labelpad parameter:or set it after:If the variable ax.xaxis._autolabelpos = True, matplotlib sets the label position in function _update_label_position in axis.py according to (some excerpts):You can set the label position independently of the ticks by using:that sets _autolabelpos to False or as mentioned above by changing the labelpad parameter.

Should I use encoding declaration in Python 3?

Mateusz Jagiełło

[Should I use encoding declaration in Python 3?](https://stackoverflow.com/questions/14083111/should-i-use-encoding-declaration-in-python-3)

Python 3 uses UTF-8 encoding for source-code files by default. Should I still use the encoding declaration at the beginning of every source file? Like # -*- coding: utf-8 -*-

2012-12-29 15:27:17Z

Python 3 uses UTF-8 encoding for source-code files by default. Should I still use the encoding declaration at the beginning of every source file? Like # -*- coding: utf-8 -*-Because the default is UTF-8, you only need to use that declaration when you deviate from the default, or if you rely on other tools (like your IDE or text editor) to make use of that information.In other words, as far as Python is concerned, only when you want to use an encoding that differs do you have to use that declaration.Other tools, such as your editor, can support similar syntax, which is why the PEP 263 specification allows for considerable flexibility in the syntax (it must be a comment, the text coding must be there, followed by either a : or = character and optional whitespace, followed by a recognised codec).Note that it only applies to how Python reads the source code. It doesn't apply to executing that code, so not to how printing, opening files, or any other I/O operations translate between bytes and Unicode. For more details on Python, Unicode, and encodings, I strongly urge you to read the Python Unicode HOWTO, or the  very thorough Pragmatic Unicode talk by Ned Batchelder.For multi-encodings projects:configuring encoding for specific file in pycharm

Best way to handle list.index(might-not-exist) in python?

Draemon

[Best way to handle list.index(might-not-exist) in python?](https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python)

I have code which looks something like this:ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.I could do this:But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.

2010-01-25 13:59:37Z

I have code which looks something like this:ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.I could do this:But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.There is nothing "dirty" about using try-except clause. This is the pythonic way. ValueError will be raised by the .index method only, because it's the only code you have there!To answer the comment:

In Python, easier to ask forgiveness than to get permission philosophy is well established, and no index will not raise this type of error for any other issues. Not that I can think of any. One line. Simple. No exceptions.The dict type has a get function, where if the key doesn't exist in the dictionary, the 2nd argument to get is the value that it should return.  Similarly there is setdefault, which returns the value in the dict if the key exists, otherwise it sets the value according to your default parameter and then returns your default parameter.You could extend the list type to have a getindexdefault method.Which could then be used like:There is nothing wrong with your code that uses ValueError. Here's yet another one-liner if you'd like to avoid exceptions:This issue is one of language philosophy. In Java for example there has always been a tradition that exceptions should really only be used in "exceptional circumstances" that is when errors have happened, rather than for flow control. In the beginning this was for performance reasons as Java exceptions were slow but now this has become the accepted style.In contrast Python has always used exceptions to indicate normal program flow, like raising a ValueError as we are discussing here. There is nothing "dirty" about this in Python style and there are many more where that came from. An even more common example is StopIteration exception which is raised by an iterator‘s next() method to signal that there are no further values.If you are doing this often then it is better to stove it away in a helper function:What about this:Rather than expose something so implementation-dependent like a list index in a function interface, pass the collection and the thing and let otherfunction deal with the "test for membership" issues.  If otherfunction is written to be collection-type-agnostic, then it would probably start with:which will work if thing_collection is a list, tuple, set, or dict.This is possibly clearer than:which is the code you already have in otherfunction.What about like this:What about this 😃 : I have the same issue with the ".index()" method on lists. I have no issue with the fact that it throws an exception but I strongly disagree with the fact that it's a non-descriptive ValueError. I could understand if it would've been an IndexError, though.I can see why returning "-1" would be an issue too because it's a valid index in Python. But realistically, I never expect a ".index()" method to return a negative number.Here goes a one liner (ok, it's a rather long line ...), goes through the list exactly once and returns "None" if the item isn't found. It would be trivial to rewrite it to return -1, should you so desire.How to use:I don't know why you should think it is dirty... because of the exception? if you want a oneliner, here it is:but i would advise against using it; I think Ross Rogers solution is the best, use an object to encapsulate your desiderd behaviour, don't try pushing the language to its limits at the cost of readability.I'd suggest:

Validating with an XML schema in Python

Eli Courtwright

[Validating with an XML schema in Python](https://stackoverflow.com/questions/299588/validating-with-an-xml-schema-in-python)

I have an XML file and an XML schema in another file and I'd like to validate that my XML file adheres to the schema.  How do I do this in Python?I'd prefer something using the standard library, but I can install a third-party package if necessary.

2008-11-18 17:59:56Z

I have an XML file and an XML schema in another file and I'd like to validate that my XML file adheres to the schema.  How do I do this in Python?I'd prefer something using the standard library, but I can install a third-party package if necessary.I am assuming you mean using XSD files. Surprisingly there aren't many python XML libraries that support this. lxml does however. Check Validation with lxml. The page also lists how to use lxml to validate with other schema types.As for "pure python" solutions: the package index lists:Installation lxmlIf you get an error like "Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?", try to do this first:The simplest validatorLet's create simplest validator.pythen write and run main.pyA little bit of OOPIn order to validate more than one file, there is no need to create an XMLSchema object every time, therefore:validator.pyNow we can validate all files in the directory as follows:main.pyFor more options read here: Validation with lxmlThe PyXB package at http://pyxb.sourceforge.net/ generates validating bindings for Python from XML schema documents.  It handles almost every schema construct and supports multiple namespaces.There are two ways(actually there are more) that you could do this.

1. using lxml

pip install lxml    >> xmllint --format --pretty 1 --load-trace --debug --schema /path/to/my_schema_file.xsd /path/to/my_xml_file.xmlYou can easily validate an XML file or tree against an XML Schema (XSD) with the xmlschema Python package. It's pure Python, available on PyPi and doesn't have many dependencies.Example - validate a file:The method raises an exception if the file doesn't validate against the XSD. That exception then contains some violation details.If you want to validate many files you only have to load the XSD once:If you don't need the exception you can validate like this:Alternatively, xmlschema directly works on file objects and in memory XML trees (either created with xml.etree.ElementTree or lxml). Example:lxml provides etree.DTDfrom the tests on http://lxml.de/api/lxml.tests.test_dtd-pysrc.html

Where should virtualenvs be created?

Ray

[Where should virtualenvs be created?](https://stackoverflow.com/questions/12184846/where-should-virtualenvs-be-created)

I'm confused as to where I should put my virtualenvs.With my first django project, I created the project with the commandI then cd'd into the djangoproject directory and ran the commandwhich created the virtual environment directory at the same level as the inner djangoproject directory.Is this the wrong place in which to create the virtualenv for this particular project?I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.Is there a correct way to do this?

2012-08-29 19:04:00Z

I'm confused as to where I should put my virtualenvs.With my first django project, I created the project with the commandI then cd'd into the djangoproject directory and ran the commandwhich created the virtual environment directory at the same level as the inner djangoproject directory.Is this the wrong place in which to create the virtualenv for this particular project?I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.Is there a correct way to do this?Many people use the virtualenvwrapper tool, which keeps all virtualenvs in the same place (the ~/.virtualenvs directory) and allows shortcuts for creating and keeping them there. For example, you might do:and then later:It's probably a bad idea to keep the virtualenv directory in the project itself, since you don't want to distribute it (it might be specific to your computer or operating system). Instead, keep a requirements.txt file using pip:and distribute that. This will allow others using your project to reinstall all the same requirements into their virtualenv with:Changing the location of the virtualenv directory breaks itThis is a major advantage of putting the directory outside of the repository tree, e.g. under ~/.virtualenvs with virutalenvwrapper.Otherwise, if you keep it in the project tree, moving the project location will break the virtualenv.See: Renaming a virtualenv folder without breaking itThere is --relocatable but it is known to not be perfect.Another minor advantage: you don't have to .gitignore it.If it weren't for that, I'd just leave my virtualenvs gitignored in the project tree itself to keep related stuff close together.This is fine since you you will likely never reuse a given virtualenv across projects.The generally accepted place to put them is the same place that the default installation of virtualenvwrapper puts them: ~/.virtualenvsRelated: virtualenvwrapper is an excellent tool that provides shorthands for the common virtualenv commands. http://www.doughellmann.com/projects/virtualenvwrapper/If you use pyenv install Python, then pyenv-virtualenv will be a best practice. If set .python-version file, it can auto activate or deactivate virtual env when you change work folder. Pyenv-virtualenv also put all virtual env into $HOME/.pyenv/versions folder.

Saving and loading objects and using pickle

Peterstone

[Saving and loading objects and using pickle](https://stackoverflow.com/questions/4530611/saving-and-loading-objects-and-using-pickle)

I´m trying to save and load objects using pickle module. 

First I declare my objects:After that I open a file called 'Fruits.obj'(previously I created a new .txt file and I renamed 'Fruits.obj'):After do this I close my session and I began a new one and I put the next (trying to access to the object that it supposed to be saved):But I have this message:I don´t know what to do because I don´t understand this message.

Does anyone know How I can load my object 'banana'?

Thank you!EDIT:

As some of you have sugested I put:There were no problem, but the next I put was:And I have error:

2010-12-25 15:17:17Z

I´m trying to save and load objects using pickle module. 

First I declare my objects:After that I open a file called 'Fruits.obj'(previously I created a new .txt file and I renamed 'Fruits.obj'):After do this I close my session and I began a new one and I put the next (trying to access to the object that it supposed to be saved):But I have this message:I don´t know what to do because I don´t understand this message.

Does anyone know How I can load my object 'banana'?

Thank you!EDIT:

As some of you have sugested I put:There were no problem, but the next I put was:And I have error:As for your second problem:After you have read the contents of the file, the file pointer will be at the end of the file - there will be no further data to read. You have to rewind the file so that it will be read from the beginning again:What you usually want to do though, is to use a context manager to open the file and read data from it. This way, the file will be automatically closed after the block finishes executing, which will also help you organize your file operations into meaningful chunks.Finally, cPickle is a faster implementation of the pickle module in C. So:The following works for me:You're forgetting to read it as binary too.In your write part you have:In the read part you have:So replace it with:And it will work :)As for your second error, it is most likely cause by not closing/syncing the file properly.Try this bit of code to write:And this (unchanged) to read:A neater version would be using the with statement.For writing:For reading:Always open in binary mode, in this caseYou didn't open the file in binary mode.Should work.For your second error, the file is most likely empty, which mean you inadvertently emptied it or used the wrong filename or something.(This is assuming you really did close your session. If not, then it's because you didn't close the file between the write and the read).I tested your code, and it works.It seems you want to save your class instances across sessions, and using pickle is a decent way to do this.  However, there's a package called klepto that abstracts the saving of objects to a dictionary interface, so you can choose to pickle objects and save them to a file (as shown below), or pickle the objects and save them to a database, or instead of use pickle use json, or many other options.  The nice thing about klepto is that by abstracting to a common interface, it makes it easy so you don't have to remember the low-level details of how to save via pickling to a file, or otherwise.Note that It works for dynamically added class attributes, which pickle cannot do...Then we restart…Klepto works on python2 and python3.Get the code here:

  https://github.com/uqfoundationYou can use anycache to do the job for you. Assuming you have a function myfunc which creates the instance:Anycache calls myfunc at the first time and pickles the result to a 

file in cachedir using an unique identifier (depending on the the function name and the arguments) as filename.

On any consecutive run, the pickled object is loaded.If the cachedir is preserved between python runs, the pickled object is taken from the previous python run.The function arguments are also taken into account. 

A refactored implementation works likewise:

How can I perform two-dimensional interpolation using scipy?

Andras Deak

[How can I perform two-dimensional interpolation using scipy?](https://stackoverflow.com/questions/37872171/how-can-i-perform-two-dimensional-interpolation-using-scipy)

I have a set of scattered two-dimensional data points, and I would like to plot them as a nice surface, preferably using something like contourf or plot_surface in matplotlib.pyplot. How can I interpolate my two-dimensional or multidimensional data to a mesh using scipy?I've found the scipy.interpolate sub-package, but I keep getting errors when using interp2d or bisplrep or griddata or rbf. What is the proper syntax of these methods?

2016-06-17 02:27:48Z

I have a set of scattered two-dimensional data points, and I would like to plot them as a nice surface, preferably using something like contourf or plot_surface in matplotlib.pyplot. How can I interpolate my two-dimensional or multidimensional data to a mesh using scipy?I've found the scipy.interpolate sub-package, but I keep getting errors when using interp2d or bisplrep or griddata or rbf. What is the proper syntax of these methods?Disclaimer: I'm mostly writing this post with syntactical considerations and general behaviour in mind. I'm not familiar with the memory and CPU aspect of the methods described, and I aim this answer at those who have reasonably small sets of data, such that the quality of the interpolation can be the main aspect to consider. I am aware that when working with very large data sets, the better-performing methods (namely griddata and Rbf) might not be feasible.I'm going to compare three kinds of multi-dimensional interpolation methods (interp2d/splines, griddata and Rbf). I will subject them to two kinds of interpolation tasks and two kinds of underlying functions (points from which are to be interpolated). The specific examples will demonstrate two-dimensional interpolation, but the viable methods are applicable in arbitrary dimensions. Each method provides various kinds of interpolation; in all cases I will use cubic interpolation (or something close1). It's important to note that whenever you use interpolation you introduce bias compared to your raw data, and the specific methods used affect the artifacts that you will end up with. Always be aware of this, and interpolate responsibly.The two interpolation tasks will beThe two functions (over the domain [x,y] in [-1,1]x[-1,1]) will beHere's how they look:I will first demonstrate how the three methods behave under these four tests, then I'll detail the syntax of all three. If you know what you should expect from a method, you might not want to waste your time learning its syntax (looking at you, interp2d).For the sake of explicitness, here is the code with which I generated the input data. While in this specific case I'm obviously aware of the function underlying the data, I will only use this to generate input for the interpolation methods. I use numpy for convenience (and mostly for generating the data), but scipy alone would suffice too.Let's start with the easiest task. Here's how an upsampling from a mesh of shape [6,7] to one of [20,21] works out for the smooth test function:Even though this is a simple task, there are already subtle differences between the outputs. At a first glance all three outputs are reasonable. There are two features to note, based on our prior knowledge of the underlying function: the middle case of griddata distorts the data most. Note the y==-1 boundary of the plot (nearest the x label): the function should be strictly zero (since y==-1 is a nodal line for the smooth function), yet this is not the case for griddata. Also note the x==-1 boundary of the plots (behind, to the left): the underlying function has a local maximum (implying zero gradient near the boundary) at [-1, -0.5], yet the griddata output shows clearly non-zero gradient in this region. The effect is subtle, but it's a bias none the less. (The fidelity of Rbf is even better with the default choice of radial functions, dubbed multiquadratic.)A bit harder task is to perform upsampling on our evil function:Clear differences are starting to show among the three methods. Looking at the surface plots, there are clear spurious extrema appearing in the output from interp2d (note the two humps on the right side of the plotted surface). While griddata and Rbf seem to produce similar results at first glance, the latter seems to produce a deeper minimum near [0.4, -0.4] that is absent from the underlying function.However, there is one crucial aspect in which Rbf is far superior: it respects the symmetry of the underlying function (which is of course also made possible by the symmetry of the sample mesh). The output from griddata breaks the symmetry of the sample points, which is already weakly visible in the smooth case.Most often one wants to perform interpolation on scattered data. For this reason I expect these tests to be more important. As shown above, the sample points were chosen pseudo-uniformly in the domain of interest. In realistic scenarios you might have additional noise with each measurement, and you should consider whether it makes sense to interpolate your raw data to begin with.Output for the smooth function:Now there's already a bit of a horror show going on. I clipped the output from interp2d to between [-1, 1] exclusively for plotting, in order to preserve at least a minimal amount of information. It's clear that while some of the underlying shape is present, there are huge noisy regions where the method completely breaks down. The second case of griddata reproduces the shape fairly nicely, but note the white regions at the border of the contour plot. This is due to the fact that griddata only works inside the convex hull of the input data points (in other words, it doesn't perform any extrapolation). I kept the default NaN value for output points lying outside the convex hull.2 Considering these features, Rbf seems to perform best.And the moment we've all been waiting for:It's no huge surprise that interp2d gives up. In fact, during the call to interp2d you should expect some friendly RuntimeWarnings complaining about the impossibility of the spline to be constructed. As for the other two methods, Rbf seems to produce the best output, even near the borders of the domain where the result is extrapolated.So let me say a few words about the three methods, in decreasing order of preference (so that the worst is the least likely to be read by anybody).The Rbf class stands for "radial basis functions". To be honest I've never considered this approach until I started researching for this post, but I'm pretty sure I'll be using these in the future.Just like the spline-based methods (see later), usage comes in two steps: first one creates a callable Rbf class instance based on the input data, and then calls this object for a given output mesh to obtain the interpolated result. Example from the smooth upsampling test:Note that both input and output points were 2d arrays in this case, and the output z_dense_smooth_rbf has the same shape as x_dense and y_dense without any effort. Also note that Rbf supports arbitrary dimensions for interpolation.So, scipy.interpolate.RbfMy former favourite, griddata, is a general workhorse for interpolation in arbitrary dimensions. It doesn't perform extrapolation beyond setting a single preset value for points outside the convex hull of the nodal points, but since extrapolation is a very fickle and dangerous thing, this is not necessarily a con. Usage example:Note the slightly kludgy syntax. The input points have to be specified in an array of shape [N, D] in D dimensions. For this we first have to flatten our 2d coordinate arrays (using ravel), then concatenate the arrays and transpose the result. There are multiple ways to do this, but all of them seem to be bulky. The input z data also have to be flattened. We have a bit more freedom when it comes to the output points: for some reason these can also be specified as a tuple of multidimensional arrays. Note that the help of griddata is misleading, as it suggests that the same is true for the input points (at least for version 0.17.0):In a nutshell, scipy.interpolate.griddataThe only reason I'm discussing interp2d and its relatives is that it has a deceptive name, and people are likely to try using it. Spoiler alert: don't use it (as of scipy version 0.17.0). It's already more special than the previous subjects in that it's specifically used for two-dimensional interpolation, but I suspect this is by far the most common case for multivariate interpolation.As far as syntax goes, interp2d is similar to Rbf in that it first needs constructing an interpolation instance, which can be called to provide the actual interpolated values. There's a catch, however: the output points have to be located on a rectangular mesh, so inputs going into the call to the interpolator have to be 1d vectors which span the output grid, as if from numpy.meshgrid:One of the most common mistakes when using interp2d is putting your full 2d meshes into the interpolation call, which leads to explosive memory consumption, and hopefully to a hasty MemoryError.Now, the greatest problem with interp2d is that it often doesn't work. In order to understand this, we have to look under the hood. It turns out that interp2d is a wrapper for the lower-level functions bisplrep+bisplev, which are in turn wrappers for FITPACK routines (written in Fortran). The equivalent call to the previous example would beNow, here's the thing about interp2d: (in scipy version 0.17.0) there is a nice comment in interpolate/interpolate.py for interp2d:and indeed in interpolate/fitpack.py, in bisplrep there's some setup and ultimatelyAnd that's it. The routines underlying interp2d are not really meant to perform interpolation. They might suffice for sufficiently well-behaved data, but under realistic circumstances you will probably want to use something else.Just to conclude, interpolate.interp2d1I'm fairly certain that the cubic and linear kind of basis functions of Rbf do not exactly correspond to the other interpolators of the same name.

2These NaNs are also the reason for why the surface plot seems so odd: matplotlib historically has difficulties with plotting complex 3d objects with proper depth information. The NaN values in the data confuse the renderer, so parts of the surface that should be in the back are plotted to be in the front. This is an issue with visualization, and not interpolation.

How to return images in flask response? [duplicate]

wong2

[How to return images in flask response? [duplicate]](https://stackoverflow.com/questions/8637153/how-to-return-images-in-flask-response)

As an example, this URL:should return a response with a image/gif MIME type. I have two static .gif images,

and if type is 1, it should return ok.gif, else return error.gif. How to do that in flask?

2011-12-26 15:47:34Z

As an example, this URL:should return a response with a image/gif MIME type. I have two static .gif images,

and if type is 1, it should return ok.gif, else return error.gif. How to do that in flask?You use something liketo send back ok.gif or error.gif, depending on the type query parameter. See the documentation for the send_file function and the request object for more information.

matplotlib get ylim values

synaptik

[matplotlib get ylim values](https://stackoverflow.com/questions/26131607/matplotlib-get-ylim-values)

I'm using matplotlib to plot data (using plot and errorbar functions) from Python.  I have to plot a set of totally separate and independent plots, and then adjust their ylim values so they can be easily visually compared.How can I retrieve the ylim values from each plot, so that I can take the min and max of the lower and upper ylim values, respectively, and adjust the plots so they can be visually compared?Of course, I could just analyze the data and come up with my own custom ylim values... but I'd like to use matplotlib to do that for me.  Any suggestions on how to easily (and efficiently) do this?Here's my Python function that plots using matplotlib:

2014-09-30 23:05:30Z

I'm using matplotlib to plot data (using plot and errorbar functions) from Python.  I have to plot a set of totally separate and independent plots, and then adjust their ylim values so they can be easily visually compared.How can I retrieve the ylim values from each plot, so that I can take the min and max of the lower and upper ylim values, respectively, and adjust the plots so they can be visually compared?Of course, I could just analyze the data and come up with my own custom ylim values... but I'd like to use matplotlib to do that for me.  Any suggestions on how to easily (and efficiently) do this?Here's my Python function that plots using matplotlib:Just use axes.get_ylim(), it is very similar to set_ylim. From the docs:If you are using the plt api directly, you can avoid calls to the axes altogether:Leveraging from the good answers above and assuming you were only using plt as in then you can get all four plot limits using plt.axis() as in the following example.The above code should produce the following output plot.

Is module __file__ attribute absolute or relative?

goh

[Is module __file__ attribute absolute or relative?](https://stackoverflow.com/questions/7116889/is-module-file-attribute-absolute-or-relative)

I'm having trouble understanding __file__. From what I understand, __file__ returns the absolute path from which the module was loaded. I'm having problem producing this: I have a abc.py with one statement print __file__, running from /d/projects/ python abc.py returns abc.py. running from /d/ returns projects/abc.py. Any reasons why?

2011-08-19 04:18:08Z

I'm having trouble understanding __file__. From what I understand, __file__ returns the absolute path from which the module was loaded. I'm having problem producing this: I have a abc.py with one statement print __file__, running from /d/projects/ python abc.py returns abc.py. running from /d/ returns projects/abc.py. Any reasons why?From the documentation:From the mailing list thread linked by @kindall in a comment to the question:For the rest of this, consider sys.path not to include ''.So, if you are outside the part of sys.path that contains the module, you'll get an absolute path. If you are inside the part of sys.path that contains the module, you'll get a relative path.If you load a module in the current directory, and the current directory isn't in sys.path, you'll get an absolute path.If you load a module in the current directory, and the current directory is in sys.path, you'll get a relative path.__file__ is absolute since Python 3.4, except when executing a script directly using a relative path:Not sure if it resolves symlinks though.Example of passing a relative path:Late simple example:Under Python-2.*, the second call incorrectly determines the path.abspath(__file__) based on the current directory:As noted by @techtonik, in Python 3.4+, this will work fine since __file__ returns an absolute path.With the help of the of Guido mail provided by @kindall, we can understand the standard import process as trying to find the module in each member of sys.path, and file as the result of this lookup (more details in PyMOTW Modules and Imports.). So if the module is located in an absolute path in sys.path the result is absolute, but if it is located in a relative path in sys.path the result is relative.Now the site.py startup file takes care of delivering only absolute path in sys.path, except the initial '', so if you don't change it by other means than setting the PYTHONPATH (whose path are also made absolute, before prefixing sys.path), you will get always an absolute path, but when the module is accessed through the current directory.Now if you trick sys.path in a funny way you can get anything.As example if you have a sample module foo.py in /tmp/ with the code:If you go in /tmp you get:When in  in /home/user, if you add /tmp your PYTHONPATH you get:Even if you add ../../tmp, it will be normalized and the result is the same.But if instead of using PYTHONPATH you use directly some funny path

you get a result as funny as the cause.Guido explains in the above cited thread, why python do not try to transform all entries in absolute paths:So your path is used as it is.

How to set adaptive learning rate for GradientDescentOptimizer?

displayname

[How to set adaptive learning rate for GradientDescentOptimizer?](https://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer)

I am using TensorFlow to train a neural network. This is how I am initializing the GradientDescentOptimizer:The thing here is that I don't know how to set an update rule for the learning rate or a decay value for that. How can I use an adaptive learning rate here?

2015-11-25 15:08:58Z

I am using TensorFlow to train a neural network. This is how I am initializing the GradientDescentOptimizer:The thing here is that I don't know how to set an update rule for the learning rate or a decay value for that. How can I use an adaptive learning rate here?First of all, tf.train.GradientDescentOptimizer is designed to use a constant learning rate for all variables in all steps. TensorFlow also provides out-of-the-box adaptive optimizers including the tf.train.AdagradOptimizer and the tf.train.AdamOptimizer, and these can be used as drop-in replacements.However, if you want to control the learning rate with otherwise-vanilla gradient descent, you can take advantage of the fact that the learning_rate argument to the tf.train.GradientDescentOptimizer constructor can be a Tensor object. This allows you to compute a different value for the learning rate in each step, for example:Alternatively, you could create a scalar tf.Variable that holds the learning rate, and assign it each time you want to change the learning rate.Tensorflow provides an op to automatically apply an exponential decay to a learning rate tensor: tf.train.exponential_decay.  For an example of it in use, see this line in the MNIST convolutional model example.  Then use @mrry's suggestion above to supply this variable as the learning_rate parameter to your optimizer of choice.The key excerpt to look at is:Note the global_step=batch parameter to minimize.  That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.Gradient descent algorithm uses the constant learning rate which you can provide in during the initialization. You can pass various learning rates in a way showed by Mrry.But instead of it you can also use more advanced optimizers which have faster convergence rate and adapts to the situation.Here is a brief explanation based on my understanding:From tensorflow official docsIf you want to set specific learning rates for intervals of epochs like  0 < a < b < c < .... Then you can define your learning rate as a conditional tensor, conditional on the global step, and feed this as normal to the optimiser. You could achieve this with a bunch of nested tf.cond statements, but its easier to build the tensor recursively:Then to use it you need to know how many training steps there are in a single epoch, so that we can use the global step to switch at the right time, and finally define the epochs and learning rates you want. So if I want the learning rates [0.1, 0.01, 0.001, 0.0001] during the epoch intervals of [0, 19], [20, 59], [60, 99], [100, \infty] respectively, I would do:

Compile (but do not run) a Python script [duplicate]

asmeurer

[Compile (but do not run) a Python script [duplicate]](https://stackoverflow.com/questions/4537411/compile-but-do-not-run-a-python-script)

How do I compile a Python script without running it?  I just want to check the script for syntax errors.  I was hoping for a simple command line switch, but I didn't see anything in python --help.  I'd like an answer for both Python 2 and Python 3.

2010-12-27 08:15:41Z

How do I compile a Python script without running it?  I just want to check the script for syntax errors.  I was hoping for a simple command line switch, but I didn't see anything in python --help.  I'd like an answer for both Python 2 and Python 3.py_compile — Compile Python source filesYou can use pylint to find syntax errors as well as more subtle errors, such as accessing undefined variables in some rarely-used conditional branch.One way is to do something like this (for test.py):This works for Python 2.x.

Python: Tuples/dictionaries as keys, select, sort

Nico Schlömer

[Python: Tuples/dictionaries as keys, select, sort](https://stackoverflow.com/questions/4878881/python-tuples-dictionaries-as-keys-select-sort)

suppose I have quantities of fruits of different colors, e.g., 24 blue bananas, 12 green apples, 0 blue strawberries and so on.

I'd like to organize them in a data structure in Python that allows for easy selection and sorting. My idea was to put them into a dictionary with tuples as keys, e.g.,or even dictionaries, e.g.,I'd like to retrieve a list of all blue fruit, or bananas of all colors, for example, or to sort this dictionary by the name of the fruit. Are there ways to do this in a clean way?It might well be that dictionaries with tuples as keys are not the proper way to handle this situation.All suggestions welcome!

2011-02-02 19:24:11Z

suppose I have quantities of fruits of different colors, e.g., 24 blue bananas, 12 green apples, 0 blue strawberries and so on.

I'd like to organize them in a data structure in Python that allows for easy selection and sorting. My idea was to put them into a dictionary with tuples as keys, e.g.,or even dictionaries, e.g.,I'd like to retrieve a list of all blue fruit, or bananas of all colors, for example, or to sort this dictionary by the name of the fruit. Are there ways to do this in a clean way?It might well be that dictionaries with tuples as keys are not the proper way to handle this situation.All suggestions welcome!Personally, one of the things I love about python is the tuple-dict combination. What you have here is effectively a 2d array (where x = fruit name and y = color), and I am generally a supporter of the dict of tuples for implementing 2d arrays, at least when something like numpy or a database isn't more appropriate. So in short, I think you've got a good approach.Note that you can't use dicts as keys in a dict without doing some extra work, so that's not a very good solution. That said, you should also consider namedtuple(). That way you could do this:Now you can use your fruitcount dict:Other tricks:Echoing chmullig, to get a list of all colors of one fruit, you would have to filter the keys, i.e. Your best option will be to create a simple data structure to model what you have.  Then you can store these objects in a simple list and sort/retrieve them any way you wish.For this case, I'd use the following class:Then you can simply construct "Fruit" instances and add them to a list, as shown in the following manner:The simple list fruits will be much easier, less confusing, and better-maintained.  Some examples of use:All outputs below is the result after running the given code snippet followed by:Unsorted list:Displays:Sorted alphabetically by name:Displays:Sorted by quantity:Displays:Where color == red:Displays:Database, dict of dicts, dictionary of list of dictionaries, named tuple (it's a subclass), sqlite, redundancy... 

I didn't believe my eyes. What else ?Yeah! I thoughtSo, in my opinion, a list of tuples is plenty enough :resultA dictionary probably isn't what you should be using in this case. A more full featured library would be a better alternative. Probably a real database. The easiest would be sqlite. You can keep the whole thing in memory by passing in the string ':memory:' instead of a filename.If you do want to continue down this path, you can do it with the extra attributes in the key or the value. However a dictionary can't be the key to a another dictionary, but a tuple can. The docs explain what's allowable. It must be an immutable object, which includes strings, numbers and tuples that contain only strings and numbers (and more tuples containing only those types recursively...). You could do your first example with d = {('apple', 'red') : 4}, but it'll be very hard to query for what you want. You'd need to do something like this:With keys as tuples, you just filter the keys with given second component and sort it:Sorting works because tuples have natural ordering if their components have natural ordering.With keys as rather full-fledged objects, you just filter by k.color == 'blue'. You can't really use dicts as keys, but you can create a simplest class like class Foo(object): pass and add any attributes to it on the fly:These instances can serve as dict keys, but beware their mutability!You could have a dictionary where the entries are a list of other dictionaries:Output:Edit:  As eumiro pointed out, you could use a dictionary of dictionaries:Output:This type of data is efficiently pulled from a Trie-like data structure.  It also allows for fast sorting. The memory efficiency might not be that great though.A traditional trie stores each letter of a word as a node in the tree.  But in your case your "alphabet" is different.  You are storing strings instead of characters.it might look something like this:see this link: trie in pythonYou want to use two keys independently, so you have two choices:

saving images in python at a very high quality

dustin

[saving images in python at a very high quality](https://stackoverflow.com/questions/16183462/saving-images-in-python-at-a-very-high-quality)

How can I save python plots at very high quality?That is, when I keep zooming in on the object saved in a pdf file, there is no blurring?Also, what would be the best mode to save it in?png, eps?  Or some other?  I can't do pdf because there is a hidden number that happens that mess with Latexmk compilation.

2013-04-24 04:38:27Z

How can I save python plots at very high quality?That is, when I keep zooming in on the object saved in a pdf file, there is no blurring?Also, what would be the best mode to save it in?png, eps?  Or some other?  I can't do pdf because there is a hidden number that happens that mess with Latexmk compilation.If you are using matplotlib and trying to get good figures in a latex document, save as an eps. Specifically, try something like this after running the commands to plot the image:I have found that eps files work best and the dpi parameter is what really makes them look good in a document.UPDATE:To specify the orientation of the figure before saving simply call the following before the plt.savefig call, but after creating the plot (assuming you have plotted using an axes with the name ax):Where elevation_angle is a number (in degrees) specifying the polar angle (down from vertical z axis) and the azimuthal_angle specifies the azimuthal angle (around the z axis).I find that it is easiest to determine these values by first plotting the image and then rotating it and watching the current values of the angles appear towards the bottom of the window just below the actual plot. Keep in mind that the x, y, z, positions appear by default, but are replaced with the two angles when you start to click+drag+rotate the image.Just to add my results, also using matplotlib..eps made all my text bold and removed transparency. .svg gave me high-res pictures that actually looked like my graph. I used 1200 dpi because a lot of scientific journals require images in 1200 / 600 / 300 dpi depending on what the image is of. Convert to desired dpi and format in GiMP or Inkscape. EDIT: Obviously the dpi doesn't matter since .svg are vector graphics and have "infinite resolution".Okay, I found spencerlyon2's answer working, however in case anybody would find himself not knowing what to do with that one line, I had to do it this way:In case you are working with seaborn plots, instead of matplotlib, you can save a .png image like this:Let's suppose you have a matrix object (either pandas or numpy), and you want to take a heatmap:This code is compatible with the latest version of seaborn. Other codes around stackoverflow worked only for previous versions.Another way I like is this. I set the size of the next image as follows:And then later I plot the output in the console, from which I can copy-paste it where I want.

(Since seaborn is built on top of matplotlib, there will be no problem.)You can save to a figure that is 1920x1080 (or 1080p) using:You can also go much higher or lower. The above solutions work well for printing, but these days you want the created image to go into a PNG/JPG or appear in a wide screen format.

Some built-in to pad a list in python

newtover

[Some built-in to pad a list in python](https://stackoverflow.com/questions/3438756/some-built-in-to-pad-a-list-in-python)

I have a list of size < N and I want to pad it up to the size N with a value.Certainly, I can use something like the following, but I feel that there should be something I missed:

2010-08-09 09:30:23Z

I have a list of size < N and I want to pad it up to the size N with a value.Certainly, I can use something like the following, but I feel that there should be something I missed:or if you don't want to change a in placeyou can always create a subclass of list and call the method whatever you pleaseI think this approach is more visual and pythonic.There is no built-in function for this. But you could compose the built-ins for your task (or anything :p).(Modified from itertool's padnone and take recipes)Usage:gnibbler's answer is nicer, but if you need a builtin, you could use itertools.izip_longest (zip_longest in Py3k):which will return a list of tuples ( i, list[ i ] ) filled-in to None. If you need to get rid of the counter, do something like:You could also use a simple generator without any build ins.

But I would not pad the list, but let the application logic deal with an empty list.Anyhow, iterator without buildinsIf you want to pad with None instead of '', map() does the job:more-itertools is a library that includes a special padded tool for this kind of problem:Alternatively, more_itertools also implements Python itertools recipes including padnone and take as mentioned by @kennytm, so they don't have to be reimplemented:If you wish to replace the default None padding, use a list comprehension:To go off of kennytm:This avoids any extra allocation, unlike any solution that depends on creating and appending the list [value] * extra_length. The "extend" method first calls __length_hint__ on the iterator, and extends the allocation for l by that much before filling it in from the iterator.you can use * iterable unpacking operator:output:

Python - use list as function parameters

Jonathan

[Python - use list as function parameters](https://stackoverflow.com/questions/4979542/python-use-list-as-function-parameters)

How can I use a Python list (e.g. params = ['a',3.4,None]) as parameters to a function, e.g.:

2011-02-12 17:43:28Z

How can I use a Python list (e.g. params = ['a',3.4,None]) as parameters to a function, e.g.:You can do this using the splat operator:This causes the function to receive each list item as a separate parameter. There's a description here: http://docs.python.org/tutorial/controlflow.html#unpacking-argument-listsThis has already been answered perfectly, but since I just came to this page and did not understand immediately I am just going to add a simple but complete example.Use an asterisk:You want the argument unpacking operator *.

Is there a way to delete created variables, functions, etc from the memory of the interpreter?

funghorn

[Is there a way to delete created variables, functions, etc from the memory of the interpreter?](https://stackoverflow.com/questions/26545051/is-there-a-way-to-delete-created-variables-functions-etc-from-the-memory-of-th)

I've been searching for the accurate answer to this question for a couple of days now but haven't got anything good. I'm not a complete beginner in programming, but not yet even on the intermediate level.When I'm in the shell of Python, I type: dir() and I can see all the names of all the objects in the current scope (main block), there are 6 of them:Then, when I'm declaring a variable, for example x = 10, it automatically adds to that lists of objects under built-in module dir(), and when I type dir() again, it shows now:The same goes for functions, classes and so on.  How do I delete all those new objects without erasing the standard 6 which where available at the beginning?I've read here about "memory cleaning", "cleaning of the console", which erases all the text from the command prompt window:  But all this has nothing to do with what I'm trying to achieve, it doesn't clean out all used objects.

2014-10-24 09:20:33Z

I've been searching for the accurate answer to this question for a couple of days now but haven't got anything good. I'm not a complete beginner in programming, but not yet even on the intermediate level.When I'm in the shell of Python, I type: dir() and I can see all the names of all the objects in the current scope (main block), there are 6 of them:Then, when I'm declaring a variable, for example x = 10, it automatically adds to that lists of objects under built-in module dir(), and when I type dir() again, it shows now:The same goes for functions, classes and so on.  How do I delete all those new objects without erasing the standard 6 which where available at the beginning?I've read here about "memory cleaning", "cleaning of the console", which erases all the text from the command prompt window:  But all this has nothing to do with what I'm trying to achieve, it doesn't clean out all used objects.You can delete individual names with del:or you can remove them from the globals() object:This is just an example loop; it defensively only deletes names that do not start with an underscore, making a (not unreasoned) assumption that you only used names without an underscore at the start in your interpreter. You could use a hard-coded list of names to keep instead (whitelisting) if you really wanted to be thorough. There is no built-in function to do the clearing for you, other than just exit and restart the interpreter.Modules you've imported (import os) are going to remain imported because they are referenced by sys.modules; subsequent imports will reuse the already imported module object. You just won't have a reference to them in your current global namespace.Yes. There is a simple way to remove everything in iPython. 

In iPython console, just type:Then system will ask you to confirm. Press y.

If you don't want to see this prompt, simply type:This should work..You can use python garbage collector:If you are in an interactive environment like Jupyter or ipython you might be interested in  clearing unwanted var's if they are getting heavy.The magic-commands reset  and reset_selective is vailable on interactive python sessions like  ipython and Jupyter1) resetin and the out parameters specify whether you want to flush the in/out caches. The directory history is flushed with the dhist parameter.Another interesting one is array that only removes numpy Arrays:2) reset_selectiveClean Array Example:Source: http://ipython.readthedocs.io/en/stable/interactive/magics.htmlThis worked for me.You need to run it twice once for globals followed by localsActually python will reclaim the memory which is not in use anymore.This is called garbage collection which is automatic process in python. But still if you want to do it then you can delete it by del variable_name. You can also do it by assigning the variable to None The only way to truly reclaim memory from unreferenced Python objects is via the garbage collector. The del keyword simply unbinds a name from an object, but the object still needs to be garbage collected. You can force garbage collector to run using the gc module, but this is almost certainly a premature optimization but it has its own risks. Using del has no real effect, since those names would have been deleted as they went out of scope anyway.

What is the equivalent of MATLAB's repmat in NumPy

vernomcrp

[What is the equivalent of MATLAB's repmat in NumPy](https://stackoverflow.com/questions/1721802/what-is-the-equivalent-of-matlabs-repmat-in-numpy)

I would like to execute the equivalent of the following MATLAB code using NumPy: repmat([1; 1], [1 1 1]).  How would I accomplish this?

2009-11-12 12:20:32Z

I would like to execute the equivalent of the following MATLAB code using NumPy: repmat([1; 1], [1 1 1]).  How would I accomplish this?Here is a much better (official) NumPy for Matlab Users link - I'm afraid the mathesaurus one is quite out of date.The numpy equivalent of repmat(a, m, n) is tile(a, (m, n)). This works with multiple dimensions and gives a similar result to matlab. (Numpy gives a 3d output array as you would expect - matlab for some reason gives 2d output - but the content is the same).Matlab:Python:Note that some of the reasons you'd need to use MATLAB's repmat are taken care of by NumPy's broadcasting mechanism, which allows you to do various types of math with arrays of similar shape. So if you had, say, a 1600x1400x3 array representing a 3-color image, you could (elementwise) multiply it by [1.0 0.25 0.25] to reduce the amount of green and blue at each pixel. See the above link for more information.See NumPy for Matlab users.Matlab:Numpy:This is how I understood it out of a bit of fiddling around. Happy to be corrected and hope this helps.Say you have a matrix M of 2x3 elements. This has two dimensions, obviously.I could see no difference between Matlab and Python while asking to manipulate the input matrix along the dimensions the matrix already has. 

Thus the two commandsare really equivalent for a matrix of rank 2 (two dimensions). The matters goes counter-intuitive when you ask for repetition/tiling over more dimensions than the input matrix has. Going back to the matrix M of rank two and shape 2x3, it is sufficient to look at what happens to the size/shape of the output matrix. Say the sequence for manipulation is now 1,1,2.In Matlabit has copied the first two dimensions (rows and columns) of the input matrix and has repeated that once into a new third dimension (copied twice, that is). True to the naming repmat for repeat matrix. In Pythonit has applied a different procedure since, I presume, the sequence (1,1,2) is read differently than in Matlab. The number of copies in the direction of columns, rows and out-of-plane dimension are being read from right to left. The resulting object has a different shape from Matlab. One can no longer assert that repmat and tile are equivalent instructions.In order to get tile to behave like repmat, in Python one has to make sure that the input matrix has as many dimensions as the elements are in the sequence. This is done, for example, by a little preconditioning and creating a related object NThen, at the input side one has N.shape = (2,3,1) rather than M.shape = (2,3) and at the output sidewhich was the answer of size(repmat(M,1,1,2)). I presume this is because we have guided Python to add the third dimension to the right of (2,3) rather than to its left, so that Python works out the sequence (1,1,2) as it was intended in the Matlab way of reading it.The element in [:,:,0] in the Python answer for N will contain the same values as the element (:,:,1) the Matlab answer for M.Finally, I can't seem to find an equivalent for repmat when one uses the Kronecker product out ofunless I then precondition M into N as above. So I would argue that the most general way to move on is to use the ways of np.newaxis.The game gets trickier when we consider a matrix L of rank 3 (three dimensions) and the simple case of no new dimensions being added in the output matrix. These two seemingly equivalent instructions will not produce the same resultsbecause the row, column, out-of-plane directions are (p,q,r) in Matlab and (q,r,p) in Python, which was not visible with rank-2 arrays. There, one has to be careful and obtaining the same results with the two languages would require more preconditioning. I am aware that this reasoning may well not be general, but I could work it out only this far. Hopefully this invites other fellows to put it to a harder test.Know both tile and repeat.numpy.matlib has a repmat function with a similar interface as the matlab function

Heatmap in matplotlib with pcolor?

Jason Sundram

[Heatmap in matplotlib with pcolor?](https://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor)

I'd like to make a heatmap like this (shown on FlowingData):

The source data is here, but random data and labels would be fine to use, i.e.Making the heatmap is easy enough in matplotlib:And I even found a colormap arguments that look about right: heatmap = plt.pcolor(data, cmap=matplotlib.cm.Blues)But beyond that, I can't figure out how to display labels for the columns and rows and display the data in the proper orientation (origin at the top left instead of bottom left).Attempts to manipulate heatmap.axes (e.g. heatmap.axes.set_xticklabels = column_labels) have all failed. What am I missing here?

2013-01-18 03:31:17Z

I'd like to make a heatmap like this (shown on FlowingData):

The source data is here, but random data and labels would be fine to use, i.e.Making the heatmap is easy enough in matplotlib:And I even found a colormap arguments that look about right: heatmap = plt.pcolor(data, cmap=matplotlib.cm.Blues)But beyond that, I can't figure out how to display labels for the columns and rows and display the data in the proper orientation (origin at the top left instead of bottom left).Attempts to manipulate heatmap.axes (e.g. heatmap.axes.set_xticklabels = column_labels) have all failed. What am I missing here?This is late, but here is my python implementation of the flowingdata NBA heatmap.updated:1/4/2014: thanks everyoneThe output looks like this:

There's an ipython notebook with all this code here. I've learned a lot from 'overflow so hopefully someone will find this useful.The python seaborn module is based on matplotlib, and produces a very nice heatmap.Below is an implementation with seaborn, designed for the ipython/jupyter notebook.The output looks like this:

I used the matplotlib Blues color map, but personally find the default colors quite beautiful. I used matplotlib to rotate the x-axis labels, as I couldn't find the seaborn syntax. As noted by grexor, it was necessary to specify the dimensions (fig.set_size_inches) by trial and error, which I found a bit frustrating.As noted by Paul H, you can easily add the values to heat maps (annot=True), but in this case I didn't think it improved the figure. Several code snippets were taken from the excellent answer by joelotz. Main issue is that you first need to set the location of your x and y ticks. Also, it helps to use the more object-oriented interface to matplotlib. Namely, interact with the axes object directly.Hope that helps.Someone edited this question to remove the code I used, so I was forced to add it as an answer. Thanks to all who participated in answering this question! I think most of the other answers are better than this code, I'm just leaving this here for reference purposes.With thanks to Paul H, and unutbu (who answered this question), I have some pretty nice-looking output:And here's the output:

What does a b prefix before a python string mean?

kriss

[What does a b prefix before a python string mean?](https://stackoverflow.com/questions/2592764/what-does-a-b-prefix-before-a-python-string-mean)

In a python source code I stumbled upon I've seen a small b before a string like in:I know about the u prefix signifying a unicode string, and the r prefix for a raw string literal. What does the b stand for and in which kind of source code is it useful as it seems to be exactly like a plain string without any prefix?

2010-04-07 13:28:36Z

In a python source code I stumbled upon I've seen a small b before a string like in:I know about the u prefix signifying a unicode string, and the r prefix for a raw string literal. What does the b stand for and in which kind of source code is it useful as it seems to be exactly like a plain string without any prefix?This is Python3 bytes literal. This prefix is absent in Python 2.5 and older (it is equivalent to a plain string of 2.x, while plain string of 3.x is equivalent to a literal with u prefix in 2.x). In Python 2.6+ it is equivalent to a plain string, for compatibility with 3.x.The b prefix signifies a bytes string literal.If you see it used in Python 3 source code, the expression creates a bytes object, not a regular Unicode str object. If you see it echoed in your Python shell or as part of a list, dict or other container contents, then you see a bytes object represented using this notation.bytes objects basically contain a sequence of integers in the range 0-255, but when represented, Python displays these bytes as ASCII codepoints to make it easier to read their contents. Any bytes outside the printable range of ASCII characters are shown as escape sequences (e.g. \n, \x82, etc.). Inversely, you can use both ASCII characters and escape sequences to define byte values; for ASCII values their numeric value is used (e.g. b'A' == b'\x41')Because a bytes object consist of a sequence of integers, you can construct a bytes object from any other  sequence of integers with values in the 0-255 range, like a list:and indexing gives you back the integers (but slicing produces a new bytes value; for the above example, value[0] gives you 72, but value[:1] is b'H' as 72 is the ASCII code point for the capital letter H).bytes model binary data, including encoded text. If your bytes value does contain text, you need to first decode it, using the correct codec. If the data is encoded as UTF-8, for example, you can obtain a Unicode str value with:Conversely, to go from text in a str object to bytes you need to encode. You need to decide on an encoding to use; the default is to use UTF-8, but what you will need is highly dependent on your use case:You can also use the constructor, bytes(strvalue, encoding) to do the same. Both the decoding and encoding methods take an extra argument to specify how errors should be handled.Python 2, versions 2.6 and 2.7 also support creating string literals using b'..' string literal syntax, to ease  code that works on both Python 2 and 3.bytes objects are immutable, just like str strings are. Use a bytearray() object if you need to have a mutable bytes value. 

How to export plots from matplotlib with transparent background?

Cupitor

[How to export plots from matplotlib with transparent background?](https://stackoverflow.com/questions/15857647/how-to-export-plots-from-matplotlib-with-transparent-background)

I am using matplotlib to make some graphs and unfortunately I cannot export them without the white background. In other words, when I export a plot like this and position it on top of another image, the white background hides what is behind it rather than allowing it to show through. How can I export plots with a transparent background instead?

2013-04-07 00:47:01Z

I am using matplotlib to make some graphs and unfortunately I cannot export them without the white background. In other words, when I export a plot like this and position it on top of another image, the white background hides what is behind it rather than allowing it to show through. How can I export plots with a transparent background instead?Use the matplotlib savefig function with the keyword argument transparent=True to save the image as a png file.Result:

Of course, that plot doesn't demonstrate the transparency.  Here's a screenshot of the PNG file displayed using the ImageMagick display command.  The checkerboard pattern is the background that is visible through the transparent parts of the PNG file.Png files can handle transparency.

So you could use this question Save plot to image file instead of displaying it using Matplotlib so as to save you graph as a png file.And if you want to turn all white pixel transparent, there's this other question : Using PIL to make all white pixels transparent?If you want to turn an entire area to transparent, then there's this question: And then use the PIL library like in this question Python PIL: how to make area transparent in PNG? so as to make your graph transparent.

Python's many ways of string formatting — are the older ones (going to be) deprecated?

gerrit

[Python's many ways of string formatting — are the older ones (going to be) deprecated?](https://stackoverflow.com/questions/13451989/pythons-many-ways-of-string-formatting-are-the-older-ones-going-to-be-depre)

Python has at least six ways of formatting a string:A brief history of the different methods:My questions are:Similar questions and why I think they're not duplicates:

2012-11-19 10:33:16Z

Python has at least six ways of formatting a string:A brief history of the different methods:My questions are:Similar questions and why I think they're not duplicates:While there are various indications in the docs that .format and f-strings are superior to % strings, there's no surviving plan to ever deprecate the latter.In commit Issue #14123: Explicitly mention that old style % string formatting has caveats but is not going away any time soon., inspired by issue Indicate that there are no current plans to deprecate printf-style formatting, the docs on %-formatting were edited to contain this phrase:(Emphasis mine.)This phrase was removed later, in commit Close #4966: revamp the sequence docs in order to better explain the state of modern Python. This might seem like a sign that a plan to deprecate % formatting was back on the cards... but diving into the bug tracker reveals that the intent was the opposite. On the bug tracker, the author of the commit characterises the change like this:In other words, we've had two consecutive changes to the %-formatting docs intended to explicitly emphasise that it will not be deprecated, let alone removed. The docs remain opinionated on the relative merits of different kinds of string formatting, but they're also clear the %-formatting isn't going to get deprecated or removed.What's more, the most recent change to that paragraph, in March 2017, changed it from this...... to this:Notice the change from "helps avoid" to "may help avoid", and how the clear recommendation of .format and f-strings has been replaced by fluffy, equivocal prose about how each style "provides their own trade-offs and benefits". That is, not only is a formal deprecation no longer on the cards, but the current docs are openly acknowledging that % formatting at least has some "benefits" over the other approaches.I'd infer from all this that the movement to deprecate or remove % formatting has not only faltered, but been defeated thoroughly and permanently.The new .format() method is meant to replace the old % formatting syntax. The latter has been de-emphasised, (but not officially deprecated yet). The method documentation states as much:(Emphasis mine).To maintain backwards compatibility and to make transition easier, the old format has been left in place for now. From the original PEP 3101 proposal:Note the until it comes time to deprecate the older system; it hasn't been deprecated, but the new system is to be used whenever you write new code.The new system has as an advantage that you can combine the tuple and dictionary approach of the old % formatter:and is extensible through the object.__format__() hook used to handle formatting of individual values.Note that the old system had % and the Template class, where the latter allows you to create subclasses that add or alter its behaviour. The new-style system has the Formatter class to fill the same niche.Python 3 has further stepped away from deprecation, instead giving you warning in the printf-style String Formatting section:Python 3.6 also added formatted string literals, which in-line the expressions into the format strings. These are the fastest method of creating strings with interpolated values, and should be used instead of str.format() wherever you can use a literal. The % operator for string formatting is not deprecated, and is not going to be removed - despite the other answers.

Every time the subject is raised on Python development list, there is strong controversy on which is better, but no controversy on whether to remove the classic way - it will stay. Despite being denoted on PEP 3101, Python 3.1 had come and gone, and % formatting is still around.The statements for the keeping classic style are clear: it is simple, it is fast, it is quick to do for short things. Using the .format method is not always more readable - and barely anyone - even among the core developers, can use the full syntax provided by .format without having to look at the reference

 Even back in 2009, one had messages like this: http://mail.python.org/pipermail/python-dev/2009-October/092529.html  - the subject had barely showed up in the lists since.2016 updateIn current Python development version (which will become Python 3.6) there is a third method of string interpolation, described on PEP-0498. It defines a new quote prefix f"" (besides the current u"", b"" and r"").Prefixing a string by f will call a method on the string object at runtime, which will automatically interpolate variables from the current scope into the string:Guido's latest position on this seems to be indicated here:What’s New In Python 3.0And the PEP3101 itself, which has the last modified dating back to (Fri, 30 Sep 2011), so no progress as of late on that one, I suppose.Looking at the older Python docs and PEP 3101 there was a statement that the % operator will be deprecated and removed from the language in the future. The following statement was in the Python docs for Python 3.0, 3.1, and 3.2:If you go to the same section in Python 3.3 and 3.4 docs, you will see that statement has been removed. I also cannot find any other statement anywhere else in the documentation indicating that the operator will be deprecated or removed from the language. It's also important to note that PEP3101 has not been modified in over two and a half years (Fri, 30 Sep 2011).UpdatePEP461 Adding % formatting to bytes and bytearray is accepted and should be part of Python 3.5 or 3.6. It's another sign that the % operator is alive and kicking.

Combine Date and Time columns using python pandas

richie

[Combine Date and Time columns using python pandas](https://stackoverflow.com/questions/17978092/combine-date-and-time-columns-using-python-pandas)

I have a pandas dataframe with the following columns;How do I combine data['Date'] & data['Time']  to get the following? Is there a way of doing it using pd.to_datetime?

2013-07-31 18:27:40Z

I have a pandas dataframe with the following columns;How do I combine data['Date'] & data['Time']  to get the following? Is there a way of doing it using pd.to_datetime?It's worth mentioning that you may have been able to read this in directly e.g. if you were using read_csv using parse_dates=[['Date', 'Time']].Assuming these are just strings you could simply add them together (with a space), allowing you to apply to_datetime:Note: surprisingly (for me), this works fine with NaNs being converted to NaT, but it is worth worrying that the conversion (perhaps using the raise argument).The accepted answer works for columns that are of datatype string. For completeness: I come across this question when searching how to do this when the columns are of datatypes: date and time. You can use this to merge date and time into the same column of dataframe.Reading .csv file with merged columns Date_Time:You can use this line to keep both other columns also.    I don't have enough reputation to comment on jka.ne so:I had to amend jka.ne's line for it to work:This might help others.Also, I have tested a different approach, using replace instead of combine:which in the OP's case would be:I have timed both approaches for a relatively large dataset (>500.000 rows), and they both have similar runtimes, but using combine is faster (59s for replace vs 50s for combine).You can cast the columns if the types are different (datetime and timestamp or str) and use to_datetime : Result : Best, The answer really depends on what your column types are. In my case, I had datetime and timedelta.If this is your case, then you just need to add the columns:First make sure to have the right data types:Then you easily combine them:You can also convert to datetime without string concatenation, by combining datetime and timedelta objects. Combined with pd.DataFrame.pop, you can remove the source series simultaneously:Use the  combine function:My dataset had 1second resolution data for a few days and parsing by the suggested methods here was very slow. Instead I used:Note the use of cache=True makes parsing the dates very efficient since there are only a couple unique dates in my files, which is not true for a combined date and time column.

How to remove leading and trailing zeros in a string? Python

alvas

[How to remove leading and trailing zeros in a string? Python](https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python)

I have several alphanumeric strings like theseThe desired output for removing trailing zeros would be:The desired output for leading trailing zeros would be:The desire output for removing both leading and trailing zeros would be:For now i've been doing it the following way, please suggest a better way if there is:

2012-10-30 15:29:20Z

I have several alphanumeric strings like theseThe desired output for removing trailing zeros would be:The desired output for leading trailing zeros would be:The desire output for removing both leading and trailing zeros would be:For now i've been doing it the following way, please suggest a better way if there is:What about a basicto remove both trailing and leading zeros ? If you're only interested in removing trailing zeros, use .rstrip instead (and .lstrip for only the leading ones).[More info in the doc.]You could use some list comprehension to get the sequences you want like so:Remove leading + trailing '0':Remove leading '0':Remove trailing '0':You can simply do this with a bool:Did you try with strip() :str.strip is the best approach for this situation, but more_itertools.strip is also a general solution that strips both leading and trailing elements from an iterable:CodeDetailsNotice, here we strip both leading and trailing "0"s among other elements that satisfy a predicate.  This tool is not limited to strings.  See also docs for more examples of more_itertools is a third-party library installable via > pip install more_itertools.Assuming you have other data types (and not only string) in your list try this. This removes trailing and leading zeros from strings and leaves other data types untouched. This also handles the special case s = '0'e.g 

split string in to 2 based on last occurrence of a separator

Yashwanth Kumar

[split string in to 2 based on last occurrence of a separator](https://stackoverflow.com/questions/7351744/split-string-in-to-2-based-on-last-occurrence-of-a-separator)

I would like to know if there is any built in function in python to break the string in to 2 parts, based on the last occurrence of a separator.for eg:

consider the string "a b c,d,e,f" , after the split over separator ",", i want the output as"a b c,d,e" and "f". I know how to manipulate the string to get the desired output, but i want to know if there is any in built function in python.

2011-09-08 16:56:50Z

I would like to know if there is any built in function in python to break the string in to 2 parts, based on the last occurrence of a separator.for eg:

consider the string "a b c,d,e,f" , after the split over separator ",", i want the output as"a b c,d,e" and "f". I know how to manipulate the string to get the desired output, but i want to know if there is any in built function in python.Use rpartition(s). It does exactly that.You can also use rsplit(s, 1).You can split a string by the last occurrence of a separator with rsplit:To split by the last comma:

bash: mkvirtualenv: command not found

Mike Pennington

[bash: mkvirtualenv: command not found](https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found)

After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.I'm running CentOS 6 and python 2.6.6, if this matters.

2012-12-13 08:03:18Z

After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.I'm running CentOS 6 and python 2.6.6, if this matters.Solution 1:For some reason, virtualenvwrapper.sh installed in /usr/bin/virtualenvwrapper.sh, instead of under /usr/local/bin.The following in my .bash_profile works...My install seems to work fine without sourcing virtualenvwrapper_bashrcSolution 2:Alternatively as mentioned below, you could leverage the chance that virtualenvwrapper.sh is already in your shell's PATH and just issue a source `which virtualenvwrapper.sh`

Try:The backticks are command substitution - they take whatever the program prints out and put it in the expression. In this case "which" checks the $PATH to find virtualenvwrapper.sh and outputs the path to it. The script is then read by the shell via 'source'.If you want this to happen every time you restart your shell, it's probably better to grab the output from the "which" command first, and then put the "source" line in your shell, something like this:echo "source /path/to/virtualenvwrapper.sh" >> ~/.profile^ This may differ slightly based on your shell. Also, be careful not to use the a single > as this will truncate your ~/.profile :-oI had the same issue on OS X 10.9.1 with python 2.7.5.  No issues with WORKON_HOME for me, but I did have to manually add source "/usr/local/bin/virtualenvwrapper.sh" to ~/.bash_profile (or ~/.bashrc in unix) after I ran pip install virtualenvwrapperPrerequisites to execute this command -1) pip (recursive acronym of Pip Install Python)  is a package management system used to install and manage software packages written in Python. Many packages can be found in the Python Package Index (PyPI).2) Install Virtual Environment. Used to create virtual environment, to install packages and dependencies of multiple projects isolated from each other.3) Install virtual environment wrapper About virtual env wrapperAfter Installing prerequisites you need to bring virtual environment wrapper into action to create virtual environment. Following are the steps -1) set virtual environment directory in path variable-

export WORKON_HOME=(directory you need to save envs)2) source /usr/local/bin/virtualenvwrapper.sh -p $WORKON_HOMEAs mentioned by @Mike, source `which virtualenvwrapper.sh` or which virtualenvwrapper.sh can used to locate virtualenvwrapper.sh file.It's best to put above two lines in ~/.bashrc to avoid executing the above commands every time you open new shell. That's all you need to create environment using mkvirtualenvPoints to keep in mind -Use this procedure to create virtual env in ubuntustep 1Install pipstep 2Install virtualenvstep 3Create a dir to store your virtualenvs (I use ~/.virtualenvs) or use this command to install specific version of python in envstep 4step 5step 6Add this two line code at the end of the bashrc filestep 7Open new terminal (recommended)step 8Create a new virtualenvstep 9To load or switch between virtualenvs, use the workon command:step 10To exit your new virtualenv, use and make sure using pip vs pip3  OR follow the steps below to install virtual environment using python3Install envand activate your virtual environment using the following command:Since I just went though a drag, I'll try to write the answer I'd have wished for two hours ago. This is for people who don't just want the copy&paste solutionFirst: Do you wonder why copying and pasting paths works for some people while it doesn't work for others?** The main reason, solutions differ are different python versions, 2.x or 3.x. There are actually distinct versions of virtualenv and virtualenvwrapper that work with either python 2 or 3. If you are on python 2 install like so: If you are planning to use python 3 install the related python 3 versionsYou've successfully installed the packages for your python version and are all set, right? Well, try it. Type workon into your terminal. Your terminal will not be able to find the command (workon is a command of virtualenvwrapper). Of course it won't. Workon is an executable that will only be available to you once you load/source the file virtualenvwrapper.sh. But the official installation guide has you covered on this one, right?. Just open your .bash_profile and insert the following, it says in the documentation: Especially the command source /usr/local/bin/virtualenvwrapper.sh seems helpful since the command seems to load/source the desired file virtualenvwrapper.sh that contains all the commands you want to work with like workon and mkvirtualenv. But yeah, no. When following the official installation guide, you are very likely to receive the error from the initial post: mkvirtualenv: command not found. Still no command is being found and you are still frustrated. So whats the problem here? The problem is that virtualenvwrapper.sh is not were you are looking for it right now. Short reminder ... you are looking here: But there is a pretty straight forward way to finding the desired file. Just type to your terminal. This will search your PATH for the file, since it is very likely to be in some folder that is included in the PATH of your system.If your system is very exotic, the desired file will hide outside of a PATH folder. In that case you can find the path to virtalenvwrapper.sh with the shell command find / -name virtualenvwrapper.shYour result may look something like this: /Library/Frameworks/Python.framework/Versions/3.7/bin/virtualenvwrapper.sh

Congratulations. You have found your missing file!. Now all you have to do is changing one command in your .bash_profile. Just change: to: Congratulations. Virtualenvwrapper does now work on your system. But you can do one more thing to enhance your solution. If you've found the file virtualenvwrapper.sh with the command which virtualenvwrapper.sh you know that it is inside of a folder of the PATH. So if you just write the filename, your file system will assume the file is inside of a PATH folder. So you you don't have to write out the full path. Just type:Thats it. You are no longer frustrated. You have solved your problem. Hopefully.On Windows 7 and Git Bash this helps me:Restart your git bash and mkvirtualenv command now will work nicely.Using Git Bash on Windows 10 and Python36 for Windows I found the virtualenvwrapper.sh in a slightly different place and running this resolved the issueIn order to successfully install the virtualenvwrapper on Ubuntu 18.04.3 you need to do the following:Solved my issue in Ubuntu 14.04 OS with python 2.7.6, by adding below two lines into ~/.bash_profile (or ~/.bashrc in unix) files.source "/usr/local/bin/virtualenvwrapper.sh"export WORKON_HOME="/opt/virtual_env/"And then executing both these lines onto the terminal.On Windows 10, to create the virtual environment, I replace "pip mkvirtualenv myproject" by "mkvirtualenv myproject" and that works well.

List all the modules that are part of a python package?

static_rtti

[List all the modules that are part of a python package?](https://stackoverflow.com/questions/1707709/list-all-the-modules-that-are-part-of-a-python-package)

Is there a straightforward way to find all the modules that are part of a python package? I've found this old discussion, which is not really conclusive, but I'd love to have a definite answer before I roll out my own solution based on os.listdir().

2009-11-10 12:47:30Z

Is there a straightforward way to find all the modules that are part of a python package? I've found this old discussion, which is not really conclusive, but I'd love to have a definite answer before I roll out my own solution based on os.listdir().Yes, you want something based on pkgutil or similar -- this way you can treat all packages alike regardless if they are in eggs or zips or so (where os.listdir won't help).How to import them too? You can just use __import__ as normal:The right tool for this job is pkgutil.walk_packages.To list all the modules on your system:Be aware that walk_packages imports all subpackages, but not submodules.If you wish to list all submodules of a certain package then you can use something like this:iter_modules only lists the modules which are one-level deep. 

walk_packages gets all the submodules.

In the case of scipy, for example, walk_packages returns while iter_modules only returnsThe documentation on pkgutil (http://docs.python.org/library/pkgutil.html)

does not list all the interesting functions defined in 

/usr/lib/python2.6/pkgutil.py.Perhaps this means the functions are not part of the "public" interface and are subject to change.However, at least as of Python 2.6 (and perhaps earlier versions?)

pkgutil comes with a walk_packages method which recursively walks through all the

modules available.This works for me:I was looking for a way to reload all submodules that I'm editing live in my package. It is a combination of the answers/comments above, so I've decided to post it here as an answer rather than a comment.Here's one way, off the top of my head:It could certainly be cleaned up and improved.EDIT: Here's a slightly nicer version:NOTE: This will also find modules that might not necessarily be located in a subdirectory of the package, if they're pulled in in its __init__.py file, so it depends on what you mean by "part of" a package.

Line continuation for list comprehensions or generator expressions in python

sasker

[Line continuation for list comprehensions or generator expressions in python](https://stackoverflow.com/questions/5809059/line-continuation-for-list-comprehensions-or-generator-expressions-in-python)

How are you supposed to break up a very long list comprehension?I have also seen somewhere that people that dislike using '\' to break up lines,

but never understood why. What is the reason behind this?

2011-04-27 18:55:12Z

How are you supposed to break up a very long list comprehension?I have also seen somewhere that people that dislike using '\' to break up lines,

but never understood why. What is the reason behind this?works fine, so you can pretty much do as you please. I'd personally preferThe reason why \ isn't appreciated very much is that it appears at the end of a line, where it either doesn't stand out or needs extra padding, which has to be fixed when line lengths change:In such cases, use parens:I'm not opposed to:You don't need \ in this case. In general, I think people avoid \ because it's slightly ugly, but also can give problems if it's not the very last thing on the line (make sure no whitespace follows it). I think it's much better to use it than not, though, in order to keep your line lengths down.Since \ isn't necessary in the above case, or for parenthesized expressions, I actually find it fairly rare that I even need to use it.You can also make use of multiple indentations in cases where you're dealing with a list of several data structures.Notice how it also filters onto another list using an if statement. Dropping the if statement to its own line is useful as well.

Disable IPython Exit Confirmation

Naftuli Kay

[Disable IPython Exit Confirmation](https://stackoverflow.com/questions/7438112/disable-ipython-exit-confirmation)

It's really irritating that every time I type exit(), I get prompted with a confirmation to exit; of course I want to exit! Otherwise, I would not have written exit()!!!Is there a way to override IPython's default behaviour to make it exit without a prompt?

2011-09-15 22:21:16Z

It's really irritating that every time I type exit(), I get prompted with a confirmation to exit; of course I want to exit! Otherwise, I would not have written exit()!!!Is there a way to override IPython's default behaviour to make it exit without a prompt?If you also want Ctrl-D to exit without confirmation, in IPython 0.11, add c.TerminalInteractiveShell.confirm_exit = False to your config file *.If you don't have a config file yet, run ipython profile create to create one.Note this ticket if you're working within the Django shell.* The config file is located at: $HOME/.ipython/profile_default/ipython_config.pyIn ipython version 0.11 or higher, just type Exit, with capital E.Alternatively, start IPython with:Or for newer versions of IPython:I like the config suggestions, but until I learned them I've started using "Quit" key combination.orThis just kills what is running. No time to ask questions on confirmation.

Conda: Installing / upgrading directly from github

Amelio Vazquez-Reina

[Conda: Installing / upgrading directly from github](https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github)

Can I install/upgrade packages from GitHub using conda?For example, with pip I can do:to install scrappy directly from the master branch in GitHub. Can I do something equivalent with conda?If this is not possible, would it make any sense to install pip with conda and manage such local installations with pip? 

2013-09-27 03:50:44Z

Can I install/upgrade packages from GitHub using conda?For example, with pip I can do:to install scrappy directly from the master branch in GitHub. Can I do something equivalent with conda?If this is not possible, would it make any sense to install pip with conda and manage such local installations with pip? There's better support for this now through conda-env.  You can, for example, now do:It's still calling pip under the covers, but you can now unify your conda and pip package specifications in a single environment.yml file.If you wanted to update your root environment with this file, you would need to save this to a file (for example, environment.yml), then run the command: conda env update -f environment.yml.It's more likely that you would want to create a new environment:conda env create -f environment.yml (changed as supposed in the comments)The answers are outdated. You simply have to conda install pip and git. Then you can use pip normally:conda doesn't support this directly because it installs from binaries, whereas git install would be from source. conda build does support recipes that are built from git. On the other hand, if all you want to do is keep up-to-date with the latest and greatest of a package, using pip inside of Anaconda is just fine, or alternately, use setup.py develop against a git clone. I found a reference to this in condas issues. The following should now work.  

How do you unit test a Celery task?

DavidM

[How do you unit test a Celery task?](https://stackoverflow.com/questions/12078667/how-do-you-unit-test-a-celery-task)

The Celery documentation mentions testing Celery within Django but doesn't explain how to test a Celery task if you are not using Django. How do you do this?

2012-08-22 17:47:52Z

The Celery documentation mentions testing Celery within Django but doesn't explain how to test a Celery task if you are not using Django. How do you do this?It is possible to test tasks synchronously using any unittest lib out there. I normaly do 2 different test sessions when working with celery tasks. The first one (as I'm suggesting bellow) is completely synchronous and should be the one that makes sure the algorithm does what it should do. The second session uses the whole system (including the broker) and makes sure I'm not having serialization issues or any other distribution, comunication problem.So:And your test:Hope that helps!I use this:Docs: http://docs.celeryproject.org/en/3.1/configuration.html#celery-always-eagerCELERY_ALWAYS_EAGER lets you run your task synchronous, and you don't need a celery server.Depends on what exactly you want to be testing. For those on Celery 4 it's:Because the settings names have been changed and need updating if you choose to upgrade, seehttp://docs.celeryproject.org/en/latest/whatsnew-4.0.html#lowercase-setting-namesAs of Celery 3.0, one way to set CELERY_ALWAYS_EAGER in Django is:Since Celery v4.0, py.test fixtures are provided to start a celery worker just for the test and are shut down when done:Among other fixtures described on http://docs.celeryproject.org/en/latest/userguide/testing.html#py-test, you can change the celery default options by redefining the celery_config fixture this way:By default, the test worker uses an in-memory broker and result backend. No need to use a local Redis or RabbitMQ if not testing specific features.In my case (and I assume many others), all I wanted was to test the inner logic of a task using pytest.TL;DR; ended up mocking everything away (OPTION 2)Example Use Case:proj/tasks.pytests/test_tasks.pybut, since shared_task decorator does a lot of celery internal logic, it isn't really a unit tests.So, for me, there were 2 options:OPTION 1: Separate internal logicproj/tasks_logic.pyproj/tasks.pyThis looks very odd, and other than making it less readable, it requires to manually extract and pass attributes that are part of the request, for instance the task_id in case you need it, which make the logic less pure.OPTION 2: mocks

mocking away celery internalstests/__init__.pywhich then allows me to mock the request object (again, in case you need things from the request, like the id, or the retries counter.tests/test_tasks.pyThis solution is much more manual, but, it gives me the control I need to actually unit test, without repeating myself, and without losing the celery scope.

How can I set the aspect ratio in matplotlib?

jtlz2

[How can I set the aspect ratio in matplotlib?](https://stackoverflow.com/questions/7965743/how-can-i-set-the-aspect-ratio-in-matplotlib)

I'm trying to make a square plot (using imshow), i.e. aspect ratio of 1:1, but I can't. None of these work:It seems like the calls are just being ignored (a problem I often seem to have with matplotlib).

2011-11-01 11:27:29Z

I'm trying to make a square plot (using imshow), i.e. aspect ratio of 1:1, but I can't. None of these work:It seems like the calls are just being ignored (a problem I often seem to have with matplotlib).Third times the charm. My guess is that this is a bug and Zhenya's answer suggests it's fixed in the latest version. I have version 0.99.1.1 and I've created the following solution: This is 'force.png':

Below are my unsuccessful, yet hopefully informative attempts. Second Answer:My 'original answer' below is overkill, as it does something similar to axes.set_aspect(). I think you want to use axes.set_aspect('auto'). I don't understand why this is the case, but it produces a square image plot for me, for example this script:Produces an image plot with 'equal' aspect ratio:

and one with 'auto' aspect ratio:

The code provided below in the 'original answer' provides a starting off point for an explicitly controlled aspect ratio, but it seems to be ignored once an imshow is called. Original Answer:Here's an example of a routine that will adjust the subplot parameters so that you get the desired aspect ratio:This produces a figure like so:

I can imagine if your having multiple subplots within the figure, you would want to include the number of y and x subplots as keyword parameters (defaulting to 1 each) to the routine provided. Then using those numbers and the hspace and wspace keywords, you can make all the subplots have the correct aspect ratio.What is the matplotlib version you are running? I have recently had to upgrade to 1.1.0, and with it, add_subplot(111,aspect='equal') works for me.you should try with figaspect. It works for me. From the docs:Edit: I am not sure of what you are looking for. The above code changes the canvas (the plot size). If you want to change the size of the matplotlib window, of the figure, then use: this does produce a window of 5x1 (wxh).This answer is based on  Yann's answer.  It will set the aspect ratio for linear or log-log plots.  I've used additional information from https://stackoverflow.com/a/16290035/2966723 to test if the axes are log-scale.Obviously you can use any version of log you want, I've used scipy, but numpy or math should be fine.After many years of success with the answers above, I have found this not to work again - but I did find a working solution for subplots athttps://jdhao.github.io/2017/06/03/change-aspect-ratio-in-mplWith full credit of course to the author above (who can perhaps rather post here), the relevant lines are:The link also has a crystal clear explanation of the different coordinate systems used by matplotlib.Thanks for all great answers received - especially @Yann's which will remain the winner.

Select rows in pandas MultiIndex DataFrame

cs95

[Select rows in pandas MultiIndex DataFrame](https://stackoverflow.com/questions/53927460/select-rows-in-pandas-multiindex-dataframe)

What are the most common pandas ways to select/filter rows of a dataframe whose index is a MultiIndex?Assumptions for simplicity:How do I select rows having "a" in level "one"? Additionally, how would I be able to drop level "one" in the output?Question 1b

How do I slice all rows with value "t" on level "two"?How can I select rows corresponding to items "b" and "d" in level "one"?Question 2b

How would I get all values corresponding to "t" and "w" in level "two"?How do I retrieve a cross section, i.e., a single row having a specific values for the index from df? Specifically, how do I retrieve the cross section of ('c', 'u'), given byHow do I select the two rows corresponding to ('c', 'u'), and ('a', 'w')?How can I retrieve all rows corresponding to "a" in level "one" or "t" in level "two"?How can I slice specific cross sections? For "a" and "b", I would like to select all rows with sub-levels "u" and "v", and for "d", I would like to select rows with sub-level "w".How do I get all rows where values in level "two" are greater than 5?Note: This post will not go through how to create MultiIndexes, how to perform assignment operations on them, or any performance related discussions (these are separate topics for another time). 

2018-12-26 04:36:11Z

What are the most common pandas ways to select/filter rows of a dataframe whose index is a MultiIndex?Assumptions for simplicity:How do I select rows having "a" in level "one"? Additionally, how would I be able to drop level "one" in the output?Question 1b

How do I slice all rows with value "t" on level "two"?How can I select rows corresponding to items "b" and "d" in level "one"?Question 2b

How would I get all values corresponding to "t" and "w" in level "two"?How do I retrieve a cross section, i.e., a single row having a specific values for the index from df? Specifically, how do I retrieve the cross section of ('c', 'u'), given byHow do I select the two rows corresponding to ('c', 'u'), and ('a', 'w')?How can I retrieve all rows corresponding to "a" in level "one" or "t" in level "two"?How can I slice specific cross sections? For "a" and "b", I would like to select all rows with sub-levels "u" and "v", and for "d", I would like to select rows with sub-level "w".How do I get all rows where values in level "two" are greater than 5?Note: This post will not go through how to create MultiIndexes, how to perform assignment operations on them, or any performance related discussions (these are separate topics for another time). Here is an introduction to some common idioms (henceforth referred to as the Four Idioms) we will be frequently re-visitingIt will be beneficial to look at the various slicing and filtering problems in terms of the Four Idioms to gain a better understanding what can be applied to a given situation. It is very important to understand that not all of the idioms will work equally well (if at all) in every circumstance. If an idiom has not been listed as a potential solution to a problem below, that means that idiom cannot be applied to that problem effectively.You can use loc, as a general purpose solution applicable to most situations:At this point, if you getThat means you're using an older version of pandas. Consider upgrading! Otherwise, use df.loc[('a', slice(None)), :].Alternatively, you can use xs here, since we are extracting a single cross section. Note the levels and axis arguments (reasonable defaults can be assumed here). Here, the drop_level=False argument is needed to prevent xs from dropping level "one" in the result (the level we sliced on).Yet another option here is using query:If the index did not have a name, you would need to change your query string to be "ilevel_0 == 'a'".Finally, using get_level_values:This can be easily done using eitherOr,Notice that we can omit the drop_level argument (it is assumed to be True by default).Intuitively, you would want something involving slice(): It Just Works!™ But it is clunky. We can facilitate a more natural slicing syntax using the pd.IndexSlice API here.This is much, much cleaner.With xs, it is With query, it is And finally, with get_level_values, you may doAll to the same effect.Using loc, this is done in a similar fashion by specifying a list.To solve the above problem of selecting "b" and "d", you can also use query:And, with get_level_values + Index.isin:With loc, this is possible only in conjuction with pd.IndexSlice.The first colon : in pd.IndexSlice[:, ['t', 'w']] means to slice across the first level. As the depth of the level being queried increases, you will need to specify more slices, one per level being sliced across. You will not need to specify more levels beyond the one being sliced, however. With query, this is With get_level_values and Index.isin (similar to above):Use loc by specifying a tuple of keys:Or,With xs, this is again simply passing a single tuple as the first argument, with all other arguments set to their appropriate defaults:With query, things become a bit clunky:You can see now that this is going to be relatively difficult to generalize. But is still OK for this particular problem.With accesses spanning multiple levels, get_level_values can still be used, but is not recommended:With loc, this is still as simple as:With query, you will need to dynamically generate a query string by iterating over your cross sections and levels:100% DO NOT RECOMMEND! But it is possible. This is actually very difficult to do with loc while ensuring correctness and still maintaining code clarity. df.loc[pd.IndexSlice['a', 't']] is incorrect, it is interpreted as df.loc[pd.IndexSlice[('a', 't')]] (i.e., selecting a cross section). You may think of a solution with pd.concat to handle each label separately:But you'll notice one of the rows is duplicated. This is because that row satisfied both slicing conditions, and so appeared twice. You will instead need to doBut if your DataFrame inherently contains duplicate indices (that you want), then this will not retain them.  Use with extreme caution.With query, this is stupidly simple:With get_level_values, this is still simple, but not as elegant:This is a special case that I've added to help understand the applicability of the Four Idioms—this is one case where none of them will work effectively, since the slicing is very specific, and does not follow any real pattern. Usually, slicing problems like this will require explicitly passing a list of keys to loc. One way of doing this is with:If you want to save some typing, you will recognise that there is a pattern to slicing "a", "b" and its sublevels, so we can separate the slicing task into two portions and concat the result:Slicing specification for "a" and "b" is slightly cleaner (('a', 'b'), ('u', 'v')) because the same sub-levels being indexed are the same for each level.This can be done using query,And get_level_values.Actually, most solutions here are applicable to columns as well, with minor changes. Consider:These are the following changes you will need to make to the Four Idioms to have them working with columns.Recently I came across a use case where I had a 3+ level multi-index dataframe in which I couldn't make any of the solutions above produce the results I was looking for. It's quite possible that the above solutions do of course work for my use case, and I tried several, however I was unable to get them to work with the time I had available. I am far from expert, but I stumbled across a solution that was not listed in the comprehensive answers above. I offer no guarantee that the solutions are in any way optimal.This is a different way to get a slightly different result to Question #6 above. (and likely other questions as well)Specifically I was looking for:As a monkey wrench in the gears (however totally fixable):On the toy dataframe below:Using the below works, of course:But I wanted a different result, so my method to get that result was:And if I wanted two+ values from one level and a single (or 2+) value from another level:The above method is probably a bit clunky, however I found it filled my needs and as a bonus was easier for me to understand and read. 

Is it Pythonic to use list comprehensions for just side effects?

sinan

[Is it Pythonic to use list comprehensions for just side effects?](https://stackoverflow.com/questions/5753597/is-it-pythonic-to-use-list-comprehensions-for-just-side-effects)

Think about a function that I'm calling for it's side effects, not return values (like printing to screen, updating GUI, printing to a file, etc.).Now, is it Pythonic to use list comprehensions to call this func:Note that I don't save the list anywhereOr should I call this func like this:Which is better and why?

2011-04-22 08:22:01Z

Think about a function that I'm calling for it's side effects, not return values (like printing to screen, updating GUI, printing to a file, etc.).Now, is it Pythonic to use list comprehensions to call this func:Note that I don't save the list anywhereOr should I call this func like this:Which is better and why?It is very anti-Pythonic to do so, and any seasoned Pythonista will give you hell over it. The intermediate list is thrown away after it is created, and it could potentially be very, very large, and therefore expensive to create.You shouldn't use a list comprehension, because as people have said that will build a large temporary list that you don't need. The following two methods are equivalent:with the definition of consume from the itertools man page:Of course, the latter is clearer and easier to understand.List comprehensions are for creating lists. And unless you are actually creating a list, you should not use list comprehensions.So I would got for the second option, just iterating over the list and then call the function when the conditions apply.Second is better.Think of the person who would need to understand your code. You can get bad karma easily with the first :)You could go middle between the two by using filter(). Consider the example:Depends on your goal.If you are trying to do some operation on each object in a list, the second approach should be adopted.If you are trying to generate a list from another list, you may use list comprehension.You can dobut it's not very pretty.Using a list comprehension for its side effects is ugly, non-Pythonic, inefficient, and I wouldn't do it. I would use a for loop instead, because a for loop signals a procedural style in which side-effects are important.But, if you absolutely insist on using a list comprehension for its side effects, you should avoid the inefficiency by using a generator expression instead. If you absolutely insist on this style, do one of these two:or:These are generator expressions, and they do not generate a random list that gets tossed out. I think the all form is perhaps slightly more clear, though I think both of them are confusing and shouldn't be used.I think this is ugly and I wouldn't actually do it in code. But if you insist on implementing your loops in this fashion, that's how I would do it.I tend to feel that list comprehensions and their ilk should signal an attempt to use something at least faintly resembling a functional style. Putting things with side effects that break that assumption will cause people to have to read your code more carefully, and I think that's a bad thing.

How do I get a list of all the duplicate items using pandas in python?

BigHandsome

[How do I get a list of all the duplicate items using pandas in python?](https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python)

I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas duplicated method, it only returns the first duplicate.  Is there a a way to get all of the duplicates and not just the first one?A small subsection of my dataset looks like this:My code looks like this currently:There area a couple duplicate items. But, when I use the above code, I only get the first item.  In the API reference, I see how I can get the last item, but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy.  So, in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries, instead of the just first one.  Any help is most appreciated.

2013-02-02 00:22:08Z

I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas duplicated method, it only returns the first duplicate.  Is there a a way to get all of the duplicates and not just the first one?A small subsection of my dataset looks like this:My code looks like this currently:There area a couple duplicate items. But, when I use the above code, I only get the first item.  In the API reference, I see how I can get the last item, but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy.  So, in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries, instead of the just first one.  Any help is most appreciated.Method #1: print all rows where the ID is one of the IDs in duplicated:but I couldn't think of a nice way to prevent repeating ids so many times.  I prefer method #2: groupby on the ID.With Pandas version 0.17, you can set 'keep = False' in the duplicated function to get all the duplicate items.it'll return all duplicated rows back to you.According to documentation:As I am unable to comment, hence posting as a separate answerTo find duplicates on the basis of more than one column, mention every column name 

as below, and it will return you all the duplicated rows set:This worked for meUsing an element-wise logical or and setting the take_last argument of the pandas duplicated method to both True and False you can obtain a set from your dataframe that includes all of the duplicates.This may not be a solution to the question, but to illustrate examples:The outputs:sort("ID") does not seem to be working now, seems deprecated as per sort doc, so use sort_values("ID") instead to sort after duplicate filter, as following:df[df.duplicated(['ID'])==True].sort_values('ID')For my database duplicated(keep=False) did not work until the column was sorted.

Unable to set default python version to python3 in ubuntu

RejeeshChandran

[Unable to set default python version to python3 in ubuntu](https://stackoverflow.com/questions/41986507/unable-to-set-default-python-version-to-python3-in-ubuntu)

I was trying to set default python version to python3 in Ubuntu 16.04. By default it is python2 (2.7). I followed below steps :  but I'm getting the following error for the second statement,    I'm new to Ubuntu and Idon't know what I'm doing wrong.

2017-02-01 17:57:06Z

I was trying to set default python version to python3 in Ubuntu 16.04. By default it is python2 (2.7). I followed below steps :  but I'm getting the following error for the second statement,    I'm new to Ubuntu and Idon't know what I'm doing wrong.Open your .bashrc file nano ~/.bashrc. Type alias python=python3 on to a new line at the top of the file then save the file with ctrl+o and close the file with ctrl+x. Then, back at your command line type source ~/.bashrc. Now your alias should be permanent. EDIT: For update alternatives, the priority is an integer. The priority represents which program should be the first used. This article sums it all up pretty well. The second line mentioned can be changed toupdate-alternatives --install /usr/bin/python python /usr/bin/python3 10This gives a priority of 10 for the path of python3. The disadvantage of editing .bashrc file is that it will not work while using the commands with sudo.To change to python3, you can use the following command in terminal alias python=python3. To change Python 3.6.8 as the default in Ubuntu 18.04 to Python 3.7.Install Python 3.7Steps to install Python3.7 and configure it as the default interpreter.A simple safe way would be to use an alias. Place this into ~/.bashrc file:

if you have gedit editor useto go into the bashrc file and then at the top of the bashrc file make the following change.After adding the above in the file. run the below commandexample:As an added extra, you can add an alias for pip as well (in .bashrc or bash_aliases):  You many find that a clean install of python3 actually points to python3.x so you may need:  Do then write eitheror Save the file, close the terminal and open it again.

You should be fine now! LinkAs it says, update-alternatives --install needs <link> <name> <path> and <priority> arguments.You have link (/usr/bin/python), name (python), and path (/usr/bin/python3), you're missing priority.update-alternatives --help says:So just put a 100 or something at the endAt First Install python3 and pip3then in your terminal run Check the version of python in your machine.get python path fromthen set your python versionFor another non-invasive, current-user only approach:python pip will be ready in a new shell.The best way in ubuntu 18.04 which will work for all users isSave the changes and restart .      After restart what ever version of python 3 you have in the system along with python 2.7 will be taken as default. You could be more specific by saying the following in alias if you have multiple version of python 3.Just follow these steps to help change the default python to the newly upgrade python version. Worked well for me. At first, Make sure Python3 is installed on your computerGo to your terminal and type:cd ~/  to go to your home directoryIf you didn't set up your .bash_profile yet, type touch .bash_profile to create your .bash_profile.Or, type open -e .bash_profile to edit the file.Copy and save alias python=python3 in the .bash_profile. Close and reopen your Terminal. Then type the following command to check if Python3 is your default version now:python --versionYou should see python 3.x.y is your default version.Cheers!

Regular expression to return text between parenthesis

user469652

[Regular expression to return text between parenthesis](https://stackoverflow.com/questions/4894069/regular-expression-to-return-text-between-parenthesis)

All I need is the contents inside the parenthesis.

2011-02-04 02:46:50Z

All I need is the contents inside the parenthesis.If your problem is really just this simple, you don't need regex:Use re.search(r'\((.*?)\)',s).group(1):If you want to find all occurences: Building on tkerwin's answer, if you happen to have nested parentheses like in his answer will not work if you need to take everything between the first opening parenthesis and the last closing parenthesis to get (a+b)/(c+d), because find searches from the left of the string, and would stop at the first closing parenthesis.To fix that, you need to use rfind for the second part of the operation, so it would become 

How do you validate a URL with a regular expression in Python?

Zee Spencer

[How do you validate a URL with a regular expression in Python?](https://stackoverflow.com/questions/827557/how-do-you-validate-a-url-with-a-regular-expression-in-python)

I'm building an app on Google App Engine. I'm incredibly new to Python and have been beating my head against the following problem for the past 3 days.I have a class to represent an RSS Feed and in this class I have a method called setUrl. Input to this method is a URL. I'm trying to use the re python module to validate off of the RFC 3986 Reg-ex (http://www.ietf.org/rfc/rfc3986.txt)Below is a snipped which should work? 

2009-05-06 00:40:30Z

I'm building an app on Google App Engine. I'm incredibly new to Python and have been beating my head against the following problem for the past 3 days.I have a class to represent an RSS Feed and in this class I have a method called setUrl. Input to this method is a URL. I'm trying to use the re python module to validate off of the RFC 3986 Reg-ex (http://www.ietf.org/rfc/rfc3986.txt)Below is a snipped which should work? An easy way to parse (and validate) URL's is the urlparse (py2, py3) module.  A regex is too much work.There's no "validate" method because almost anything is a valid URL.  There are some punctuation rules for splitting it up.  Absent any punctuation, you still have a valid URL.Check the RFC carefully and see if you can construct an "invalid" URL.  The rules are very flexible.  For example ::::: is a valid URL.  The path is ":::::".  A pretty stupid filename, but a valid filename.Also, ///// is a valid URL.  The netloc ("hostname") is "".  The path is "///".  Again, stupid.  Also valid.  This URL normalizes to "///" which is the equivalent.Something like "bad://///worse/////" is perfectly valid.  Dumb but valid.Bottom Line.  Parse it, and look at the pieces to see if they're displeasing in some way.  Do you want the scheme to always be "http"?  Do you want the netloc to always be "www.somename.somedomain"?  Do you want the path to look unix-like?  Or windows-like?  Do you want to remove the query string?  Or preserve it?These are not RFC-specified validations.  These are validations unique to your application.Here's the complete regexp to parse a URL.Given its complexibility, I think you should go the urlparse way.For completeness, here's the pseudo-BNF of the above regex (as a documentation):I admit, I find your regular expression totally incomprehensible.  I wonder if you could use urlparse instead?  Something like:It might be slower, and maybe you'll miss conditions, but it seems (to me) a lot easier to read and debug than a regular expression for URLs.I'm using the one used by Django and it seems to work pretty well:You can always check the latest version here: https://github.com/django/django/blob/master/django/core/validators.py#L74http://pypi.python.org/pypi/rfc3987 gives regular expressions for consistency with the rules in RFC 3986 and RFC 3987 (that is, not with scheme-specific rules).A regexp for IRI_reference is:In one line:note - Lepl is no longer maintained or supported.RFC 3696 defines "best practices" for URL validation - http://www.faqs.org/rfcs/rfc3696.htmlThe latest release of Lepl (a Python parser library) includes an implementation of RFC 3696.  You would use it something like:Although the validators are defined in Lepl, which is a recursive descent parser, they are largely compiled internally to regular expressions.  That combines the best of both worlds - a (relatively) easy to read definition that can be checked against RFC 3696 and an efficient implementation.  There's a post on my blog showing how this simplifies the parser - http://www.acooke.org/cute/LEPLOptimi0.htmlLepl is available at http://www.acooke.org/lepl and the RFC 3696 module is documented at http://www.acooke.org/lepl/rfc3696.htmlThis is completely new in this release, so may contain bugs.  Please contact me if you have any problems and I will fix them ASAP.  Thanks.urlparse quite happily takes invalid URLs, it is more a string string-splitting library than any kind of validator. For example:Depending on the situation, this might be fine..If you mostly trust the data, and just want to verify the protocol is HTTP, then urlparse is perfect.If you want to make the URL is actually a legal URL, use the ridiculous regexIf you want to make sure it's a real web address,Nowadays In 90% of case if you working with URL in python than you probably use python-requests. So now is the question "Why do not reuse URL validation from requests" ?feature:The regex provided should match any url of the form http://www.ietf.org/rfc/rfc3986.txt; and does when tested in the python interpreter.What format have the URLs you've been having trouble parsing had?I've needed to do this many times over the years and always end up copying someone else's regular expression who has thought about it way more than I want to think about it.Having said that, there is a regex in the Django forms code which should do the trick:http://code.djangoproject.com/browser/django/trunk/django/forms/fields.py#L534NOTE: As ugly as it looks in your browser just copy paste and the formatting should be goodFound at the python mailing lists and used for the gnome-terminalsource: http://mail.python.org/pipermail/python-list/2007-January/595436.htmlsource: https://github.com/django/django/blob/master/django/core/validators.py#L74

binning data in python with scipy/numpy

Sven Marnach

[binning data in python with scipy/numpy](https://stackoverflow.com/questions/6163334/binning-data-in-python-with-scipy-numpy)

is there a more efficient way to take an average of an array in prespecified bins? for example, i have an array of numbers and an array corresponding to bin start and end positions in that array, and I want to just take the mean in those bins? I have code that does it below but i am wondering how it can be cut down and improved. thanks.

2011-05-28 17:43:00Z

is there a more efficient way to take an average of an array in prespecified bins? for example, i have an array of numbers and an array corresponding to bin start and end positions in that array, and I want to just take the mean in those bins? I have code that does it below but i am wondering how it can be cut down and improved. thanks.It's probably faster and easier to use numpy.digitize():An alternative to this is to use numpy.histogram():Try for yourself which one is faster... :)The Scipy (>=0.11) function scipy.stats.binned_statistic specifically addresses the above question.For the same example as in the previous answers, the Scipy solution would beNot sure why this thread got necroed; but here is a 2014 approved answer, which should be far faster:The numpy_indexed package (disclaimer: I am its author) contains functionality to efficiently perform operations of this type:This is essentially the same solution as the one I posted earlier; but now wrapped in a nice interface, with tests and all :)I would add, and also to answer the question find mean bin values using histogram2d python that the scipy also have a function specially designed to compute a bidimensional binned statistic for one or more sets of datathe function scipy.stats.binned_statistic_dd is a generalization of this funcion for higher dimensions datasetsAnother alternative is to use the ufunc.at. This method applies in-place a desired operation at specified indices.

We can get the bin position for each datapoint using the searchsorted method. 

Then we can use at to increment by 1 the position of histogram at the index given by bin_indexes, every time we encounter an index at bin_indexes.  

Python Logging (function name, file name, line number) using a single file

user1126425

[Python Logging (function name, file name, line number) using a single file](https://stackoverflow.com/questions/10973362/python-logging-function-name-file-name-line-number-using-a-single-file)

I am trying to learn how an application works. And for this I am inserting debug commands as the first line of each function's body with the goal of logging the function's name as well as the line number (within the code) where I send a message to the log output. Finally, since this application comprises of many files, I want to create a single log file so that I can better understand the control flow of the application.Here is what I know:I would greatly appreciate any help.Thanks!

2012-06-11 00:07:12Z

I am trying to learn how an application works. And for this I am inserting debug commands as the first line of each function's body with the goal of logging the function's name as well as the line number (within the code) where I send a message to the log output. Finally, since this application comprises of many files, I want to create a single log file so that I can better understand the control flow of the application.Here is what I know:I would greatly appreciate any help.Thanks!You have a few marginally related questions here.I'll start with the easiest: (3). Using logging you can aggregate all calls to a single log file or other output target: they will be in the order they occurred in the process.Next up: (2). locals() provides a dict of the current scope. Thus, in a method that has no other arguments, you have self in scope, which contains a reference to the current instance. The trick being used that is stumping you is the string formatting using a dict as the RHS of the % operator. "%(foo)s" % bar will be replaced by whatever the value of bar["foo"] is.Finally, you can use some introspection tricks, similar to those used by pdb that can log more info:This will log the message passed in, plus the (original) function name, the filename in which the definition appears, and the line in that file. Have a look at inspect - Inspect live objects for more details.As I mentioned in my comment earlier, you can also drop into a pdb interactive debugging prompt at any time by inserting the line import pdb; pdb.set_trace() in, and re-running your program. This enables you to step through the code, inspecting data as you choose.The correct answer for this is to use the already provided funcName variable Then anywhere you want, just add:Example output from a script I'm working on right now:funcname, linename and lineno provide information about the last function that did the logging.If you have wrapper of logger (e.g singleton logger), then @synthesizerpatel's answer might not work for you.To find out the other callers in the call stack you can do:

Loop through all nested dictionary values?

Takkun

[Loop through all nested dictionary values?](https://stackoverflow.com/questions/10756427/loop-through-all-nested-dictionary-values)

I'm trying to loop through a dictionary and print out all key value pairs where the value is not a nested dictionary. If the value is a dictionary I want to go into it and print out its key value pairs...etc. Any help?EDITHow about this? It still only prints one thing.Full Test CaseDictionary:Result:

2012-05-25 14:41:23Z

I'm trying to loop through a dictionary and print out all key value pairs where the value is not a nested dictionary. If the value is a dictionary I want to go into it and print out its key value pairs...etc. Any help?EDITHow about this? It still only prints one thing.Full Test CaseDictionary:Result:As said by Niklas, you need recursion, i.e. you want to define a function to print your dict, and if the value is a dict, you want to call your print function using this new dict.Something like :Since a dict is iterable, you can apply the classic nested container iterable formula to this problem with only a couple of minor changes. Here's a Python 2 version (see below for 3):Test:In Python 2, It might be possible to create a custom Mapping that qualifies as a Mapping but doesn't contain iteritems, in which case this will fail. The docs don't indicate that iteritems is required for a Mapping; on the other hand, the source gives Mapping types an iteritems method. So for custom Mappings, inherit from collections.Mapping explicitly just in case.In Python 3, there are a number of improvements to be made. As of Python 3.3, abstract base classes live in collections.abc. They remain in collections too for backwards compatibility, but it's nicer having our abstract base classes together in one namespace. So this imports abc from collections. Python 3.3 also adds yield from, which is designed for just these sorts of situations. This is not empty syntactic sugar; it may lead to faster code and more sensible interactions with coroutines.There are potential problems if you write your own recursive implementation or the iterative equivalent with stack. See this example:In the normal sense, nested dictionary will be a n-nary tree like data structure. But the definition doesn't exclude the possibility of a cross edge or even a back edge (thus no longer a tree). For instance, here key2.2 holds to the dictionary from key1, key2.3 points to the entire dictionary(back edge/cycle). When there is a back edge(cycle), the stack/recursion will run infinitely.If you print this dictionary with this implementation from ScharronYou would see this error:The same goes with the implementation from senderle.Similarly, you get an infinite loop with this implementation from Fred Foo:However, Python actually detects cycles in nested dictionary:"{...}" is where a cycle is detected.As requested by Moondra this is a way to avoid cycles (DFS):Alternative iterative solution:Slightly different version I wrote that keeps track of the keys along the way to get thereOn your data, it'll printIt's also easy to modify it to track the prefix as a tuple of keys rather than a string if you need it that way.Here is pythonic way to do it. This function will allow you to loop through key-value pair in all the levels. It does not save the whole thing to the memory but rather walks through the dict as you loop through itPrintsIterative solution as an alternative:A alternative solution to work with lists based on Scharron's solutionHere's a modified version of Fred Foo's answer for Python 2. In the original response, only the deepest level of nesting is output. If you output the keys as lists, you can keep the keys for all levels, although to reference them you need to reference a list of lists. Here's the function: To reference the keys: for a three-level dictionary. You need to know the number of levels before to access multiple keys and the number of levels should be constant (it may be possible to add a small bit of script to check the number of nesting levels when iterating through values, but I haven't yet looked at this). I find this approach a bit more flexible, here you just providing generator function that emits key, value pairs and can be easily extended to also iterate over lists.Then you can write your own myprint function, then would print those key value pairs.A test:Output:I tested this on Python 3.6.I am using the following code to print all the values of a nested dictionary, taking into account where the value could be a list containing dictionaries. This was useful to me when parsing a JSON file into a dictionary and needing to quickly check whether any of its values are None.Output:These answers work for only 2 levels of sub-dictionaries. For more try this:

How to get UTC time in Python?

James Clarke

[How to get UTC time in Python?](https://stackoverflow.com/questions/15940280/how-to-get-utc-time-in-python)

I've search a bunch on StackExchange for a solution but nothing does quite what I need.  In JavaScript, I'm using the following to calculate UTC time since Jan 1st 1970:What would be the equivalent Python code?

2013-04-11 03:33:34Z

I've search a bunch on StackExchange for a solution but nothing does quite what I need.  In JavaScript, I'm using the following to calculate UTC time since Jan 1st 1970:What would be the equivalent Python code?Try this code that uses datetime.utcnow():For your purposes when you need to calculate an amount of time spent between two dates all that you need is to substract end and start dates. The results of such substraction is a timedelta object.From the python docs:And this means that by default you can get any of the fields mentioned in it's definition - 

