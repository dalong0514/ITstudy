you may want to try portablepy to have your app run on Windows environment

without Python.I'm not sure that this is the best way to do it, but when I'm deploying Ruby GUI apps (not Python, but has the same "problem" as far as .exe's are concerned) on Windows, I just write a short launcher in C# that calls on my main script. It compiles to an executable, and I then have an application executable.Then you should use py2exe, for example, to bring in one folder all the files needed to run the app, even if the user has not python on his pc (I am talking of windows... for the apple os there is no need of an executable file, I think, as it come with python in it without any need of installing it.with this code:2) Put your program in the same folder of setup.py

put in this folder the program you want to make it distribuitable:

es: l4h.py3) Run cmd from that folder (on the folder, right click + shift and choose start cmd here)

4) write in cmd:>python setup.py py2exe

5) in the dist folder there are all the files you need

6) you can zip it and distribute it  ********PySimpleGUI wraps tkinter and works on Python 3 and 2.7. It also runs on Qt, WxPython and in a web browser, using the same source code for all platforms.  You can make custom GUIs that utilize all of the same widgets that you find in tkinter (sliders, checkboxes, radio buttons, ...).  The code tends to be very compact and readable.As explained in the PySimpleGUI Documentation, to build the .EXE file you run:You don't need to compile python for Mac/Windows/Linux.  It is an interpreted language, so you simply need to have the Python interpreter installed on the system of your choice (it is available for all three platforms).As for a GUI library that works cross platform, Python's Tk/Tcl widget library works very well, and I believe is sufficiently cross platform.Tkinter is the python interface to Tk/TclFrom the python project webpage: You can use appJar for basic GUI development.See documentation at appJar site.Installation is made with pip install appjar from command line.I was amazed seeing that no one mentioned Kivy!!!I have once done a project using Tkinter, although they do advocate that it has improved a lot, it still gives me a feel of windows 98, so I switched to Kivy.I have been following a tutorial series if it helps...Just to give an idea of how kivy looks, see this (The project I am working on):And I have been working on it for barely a week now !

The benefits for Kivy you ask? Check thisThe reason why I chose this is, its look and that it can be used in mobile as well.

Efficient way to rotate a list in python

dzhelil

[Efficient way to rotate a list in python](https://stackoverflow.com/questions/2150108/efficient-way-to-rotate-a-list-in-python)

What is the most efficient way to rotate a list in python? 

Right now I have something like this:Is there a better way?

2010-01-27 20:44:07Z

What is the most efficient way to rotate a list in python? 

Right now I have something like this:Is there a better way?A collections.deque is optimized for pulling and pushing on both ends. They even have a dedicated rotate() method. What about just using pop(0)?Numpy can do this using the roll command:It depends on what you want to have happen when you do this:You might want to change your:to:Simplest way I can think of:                                        If you just want to iterate over these sets of elements rather than construct a separate data structure, consider using iterators to construct a generator expression:This also depends on if you want to shift the list in place (mutating it), or if you want the function to return a new list.  Because, according to my tests, something like this is at least twenty times faster than your implementation that adds two lists:In fact, even adding a l = l[:] to the top of that to operate on a copy of the list passed in is still twice as fast.Various implementations with some timing at http://gist.github.com/288272Just some notes on timing:If you're starting with a list, l.append(l.pop(0)) is the fastest method you can use. This can be shown with time complexity alone:So if you are starting with deque objects, you can deque.rotate() at the cost of O(k). But, if the starting point is a list, the time complexity of using deque.rotate() is O(n). l.append(l.pop(0) is faster at O(1).Just for the sake of illustration, here are some sample timings on 1M iterations:Methods which require type conversion:List methods mentioned here:Timing code used is below.Showing that creating deques from lists is O(n):If you need to create deque objects:1M iterations @ 6.853878974914551 secondsIf you already have deque objects:1M iterations @ 0.12380790710449219 secondsIf you need to create nparrays1M iterations @ 27.558452129364014 secondsIf you already have nparrays:1M iterations @ 6.0491721630096436 secondsRequires no type conversion1M iterations @ 4.819645881652832 secondsRequires no type conversion1M iterations @ 0.32483696937561035I also got interested in this and compared some of the suggested solutions with perfplot (a small project of mine).It turns out thatis by far the fastest method for small shifts n.For larger n, isn't bad.Essentially, perfplot performs the shift for increasing large arrays and measures the time. Here are the results:shift = 1:shift = 100:Code to reproduce the plot:Possibly a ringbuffer is more suitable. It is not a list, although it is likely that it can behave enough like a list for your purposes.The problem is that the efficiency of a shift on a list is O(n), which becomes significant for large enough lists.Shifting in a ringbuffer is simply updating the head location which is O(1)For an immutable implementation, you could use something like this:If efficiency is your goal, (cycles? memory?) you may be better off looking at the array module: http://docs.python.org/library/array.htmlArrays do not have the overhead of lists.  As far as pure lists go though, what you have is about as good as you can hope to do.I think you are looking for this:Another alternative:I take this cost model as a reference:http://scripts.mit.edu/~6.006/fall07/wiki/index.php?title=Python_Cost_ModelYour method of slicing the list and concatenating two sub-lists are linear-time operations. I would suggest using pop, which is a constant-time operation, e.g.:I don't know if this is 'efficient', but it also works:EDIT: Hello again, I just found a big problem with this solution!

Consider the following code:The shift_classlist() method executes the same code as my x.insert(0,x.pop())-solution, otherlist is a list indipendent from the class. After passing the content of otherlist to the MyClass.classlist list, calling the shift_classlist() also changes the otherlist list:CONSOLE OUTPUT:I use Python 2.7. I don't know if thats a bug, but I think it's more likely that I missunderstood something here.Does anyone of you know why this happens?The following method is O(n) in place with constant auxiliary memory:Note that in python, this approach is horribly inefficient compared to others as it can't take advantage of native implementations of any of the pieces.I have similar thing. For example, to shift by two...I think you've got the most efficient wayFor a list X = ['a', 'b', 'c', 'd', 'e', 'f'] and a desired shift value of shift less than list length, we can define the function list_shift() as belowExamples, list_shift(X,1) returns ['b', 'c', 'd', 'e', 'f', 'a']

list_shift(X,3) returns ['d', 'e', 'f', 'a', 'b', 'c']What is the use case? Often, we don't actually need a fully shifted array --we just need to access a handful of elements in the shifted array. Getting Python slices is runtime O(k) where k is the slice, so a sliced rotation is runtime N. The deque rotation command is also O(k). Can we do better? Consider an array that is extremely large (let's say, so large it would be computationally slow to slice it). An alternative solution would be to leave the original array alone and simply calculate the index of the item that would have existed in our desired index after a shift of some kind. Accessing a shifted element thus becomes O(1). Following function copies sent list to a templist, so that pop function does not affect the original list: Testing: Output:Jon Bentley in Programming Pearls (Column 2) describes an elegant and efficient algorithm for rotating an n-element vector x left by i positions:This can be translated to Python as follows:Demo:For example, giventhe function should return [9, 7, 6, 3, 8]. Three rotations were made:For another example, giventhe function should return [0, 0, 0]Giventhe function should return [1, 2, 3, 4]I was looking for in place solution to this problem. This solves the purpose in O(k). for similar functionality as shift in other languages:

How to use MySQLdb with Python and Django in OSX 10.6?

Joe

[How to use MySQLdb with Python and Django in OSX 10.6?](https://stackoverflow.com/questions/2952187/how-to-use-mysqldb-with-python-and-django-in-osx-10-6)

This is a much discussed issue for OSX 10.6 users, but I haven't been able to find a solution that works. Here's my setup:Python 2.6.1 64bit

Django 1.2.1

MySQL 5.1.47 osx10.6 64bitI create a virtualenvwrapper with --no-site-packages, then installed Django. When I activate the virtualenv and run python manage.py syncdb, I get this error:I've also installed the MySQL for Python adapter, but to no avail (maybe I installed it improperly?). Anyone dealt with this before?

2010-06-01 18:18:10Z

This is a much discussed issue for OSX 10.6 users, but I haven't been able to find a solution that works. Here's my setup:Python 2.6.1 64bit

Django 1.2.1

MySQL 5.1.47 osx10.6 64bitI create a virtualenvwrapper with --no-site-packages, then installed Django. When I activate the virtualenv and run python manage.py syncdb, I get this error:I've also installed the MySQL for Python adapter, but to no avail (maybe I installed it improperly?). Anyone dealt with this before?I had the same error and pip install MySQL-python solved it for me.Alternate installs:Below, Soli notes that if you receive the following error:... then you have a further system dependency issue. Solving this will vary from system to system, but for Debian-derived systems:sudo apt-get install python-mysqldbRunning Ubuntu, I had to do:Adding to other answers, the following helped me finish the installation mysql-python:virtualenv, mysql-python, pip: anyone know how?On Ubuntu...Don't forget to add 'sudo' to the beginning of commands if you don't have the proper permissions.Try this the commands below. They work for me:mysql_config must be on the path. On Mac, doraised an error:fixed the problem.How I got it working:After sourcing my env:Then, I ran the startproject and inside the manage.py, I added this:Also, updated this inside settings:I also have configparser==3.5.0 installed in my virtualenv, not sure if that was required or not...Hope it helps, The following worked perfectly for me, running Ubuntu 13.10 64-bit:Now, navigate to your virtualenv (such as env folder) and execute the following:I actually found the solution in a separate question and I am quoting it below:I think this should also work with OSX. The only problem would be getting an equivalent command for installing libmysqlclient-dev and python-dev as they are needed to compile 

mysql-python I guess.Hope this helps.Try this: This solved the issue for me .This issue was the result of an incomplete / incorrect installation of the MySQL for Python adapter. Specifically, I had to edit the path to the mysql_config file to point to /usr/local/mysql/bin/mysql_config - discussed in greater detail in this article: http://dakrauth.com/blog/entry/python-and-django-setup-mac-os-x-leopard/sudo apt-get install python-mysqldb works perfectly in ubuntupip install mysql-python raises an Environment ErrorThis worked for Red Hat Enterprise Linux Server release 6.4You can install as  pip install mysqlclientI made the upgrade to OSX Mavericks and Pycharm 3 and start to get this error, i used pip and easy install and got the error: So i need to update to Xcode 5 and tried again to install using pip.That fix all the problems.The error raised here is in importing the python module. This can be solved by adding the python site-packages folder to the environment variable $PYTHONPATH on OS X. So we can add the following command to the .bash_profile file:*replace x.x with the python version you are usingIf you are using python3, then try this(My OS is Ubuntu 16.04):pip did not work for me on windows 8 64 bits system. 

easy_install mysql-python works for me.

You can use easy_install to avoid building binaries on windows if pip does not work. I had the same problem on OSX 10.6.6. But just a simple easy_install mysql-python on terminal did not solve it as another hiccup followed:error: command 'gcc-4.2' failed with exit status 1. Apparently, this issue arises after upgrading from XCode3 (which is natively shipped with OSX 10.6) to XCode4. This newer ver removes support for building ppc arch. If its the same case, try doing as follows before easy_install mysql-pythonMany thanks to Ned Deily for this solution. Check hereFor me the problem got solved by simply reinstalling mysql-pythonInstall Command Line Tools Works for me:I overcame the same problem by installing MySQL-python library using pip. You can see the message displayed on my console when I first changed my database settings in settings.py and executed makemigrations command(The solution is following the below message, just see that).Finally I overcame this problem as follows:Run this command now you can run your command.I encountered similar situations like yours that I am using python3.7 and django 2.1 in virtualenv on mac osx. 

Try to run command:And edit __init__.py file in your project folder and add following:Then run: python3 manage.py runserver

or  python manage.py runserver

How to use a dot「.」to access members of dictionary?

bodacydo

[How to use a dot「.」to access members of dictionary?](https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary)

How do I make Python dictionary members accessible via a dot "."?For example, instead of writing mydict['val'], I'd like to write mydict.val.Also I'd like to access nested dicts this way. For examplewould refer to 

2010-02-28 18:51:42Z

How do I make Python dictionary members accessible via a dot "."?For example, instead of writing mydict['val'], I'd like to write mydict.val.Also I'd like to access nested dicts this way. For examplewould refer to You can do it using this class I just made. With this class you can use the Map object like another dictionary(including json serialization) or with the dot notation. I hope to help you:Usage examples:I've always kept this around in a util file. You can use it as a mixin on your own classes too.Install dotmap via pipIt does everything you want it to do and subclasses dict, so it operates like a normal dictionary:On top of that, you can convert it to and from dict objects:This means that if something you want to access is already in dict form, you can turn it into a DotMap for easy access:Finally, it automatically creates new child DotMap instances so you can do things like this:Full disclosure: I am the creator of the DotMap. I created it because Bunch was missing these featuresDerive from dict and and implement __getattr__ and __setattr__.Or you can use Bunch which is very similar.I don't think it's possible to monkeypatch built-in dict class.Fabric has a really nice, minimal implementation. Extending that to allow for nested access, we can use a defaultdict, and the result looks something like this:Make use of it as follows:That elaborates a bit on Kugel's answer of "Derive from dict and and implement __getattr__ and __setattr__". Now you know how!I tried this:you can try __getattribute__ too.make every dict a type of dotdict would be good enough, if you want to init this from a multi-layer dict, try implement __init__ too.Don't. Attribute access and indexing are separate things in Python, and you shouldn't want them to perform the same. Make a class (possibly one made by namedtuple) if you have something that should have accessible attributes and use [] notation to get an item from a dict.If you want to pickle your modified dictionary, you need to add few state methods to above answers:Building on Kugel's answer and taking Mike Graham's words of caution into consideration, what if we make a wrapper?  I like the Munch and it gives lot of handy options on top of dot access. Use __getattr__, very simple, works in 

Python 3.4.3Output:The language itself doesn't support this, but sometimes this is still a useful requirement. Besides the Bunch recipe, you can also write a little method which can access a dictionary using a dotted string:which would support something like this:To build upon epool's answer, this version allows you to access any dict inside via the dot operator:For instance, foo.bar.baz[1].baba returns "loo".I recently came across the 'Box' library which does the same thing. I found it to be more effective than other existing libraries like dotmap, which generate python recursion error when you have large nested dicts.Use SimpleNamespace:If one decides to permanently convert that dict to object this should do. You can create a throwaway object just before accessing. I ended up trying BOTH the AttrDict and the Bunch libraries and found them to be way to slow for my uses.  After a friend and I looked into it, we found that the main method for writing these libraries results in the library aggressively recursing through a nested object and making copies of the dictionary object throughout.  With this in mind, we made two key changes.  1) We made attributes lazy-loaded 2) instead of creating copies of a dictionary object, we create copies of a light-weight proxy object.  This is the final implementation.  The performance increase of using this code is incredible.  When using AttrDict or Bunch, these two libraries alone consumed 1/2 and 1/3 respectively of my request time(what!?).  This code reduced that time to almost nothing(somewhere in the range of 0.5ms).  This of course depends on your needs, but if you are using this functionality quite a bit in your code, definitely go with something simple like this.  See the original implementation here by https://stackoverflow.com/users/704327/michael-merickel.The other thing to note, is that this implementation is pretty simple and doesn't implement all of the methods you might need.  You'll need to write those as required on the DictProxy or ListProxy objects.I'd like to throw my own solution into the ring:https://github.com/skorokithakis/jsaneIt allows you to parse JSON into something you can access with.attribute.lookups.like.this.r(), mostly because I hadn't seen this answer before starting to work on it.Not a direct answer to the OP's question, but inspired by and perhaps useful for some.. I've created an object-based solution using the internal __dict__ (In no way optimized code)One simple way to get dot access (but not array access), is to use a plain object in Python. Like this:...and use it like this:... to convert it to a dict:This solution is a refinement upon the one offered by epool to address the requirement of the OP to access nested dicts in a consistent manner. The solution by epool did not allow for accessing nested dicts.With this class, one can now do something like: A.B.C.D. This also works with nested dicts and makes sure that dicts which are appended later behave the same:The answer of @derek73 is very neat, but it cannot be pickled nor (deep)copied, and it returns None for missing keys. The code below fixes this.Edit: I did not see the answer above that addresses the exact same point (upvoted). I'm leaving the answer here for reference.A solution kind of delicate

How to dynamically build a JSON object with Python?

Backo

[How to dynamically build a JSON object with Python?](https://stackoverflow.com/questions/23110383/how-to-dynamically-build-a-json-object-with-python)

I am new to Python and I am playing with JSON data. I would like to dynamically build a JSON object by adding some key-value to an existing JSON object.I tried the following but I get TypeError: 'str' object does not support item assignment:

2014-04-16 13:06:52Z

I am new to Python and I am playing with JSON data. I would like to dynamically build a JSON object by adding some key-value to an existing JSON object.I tried the following but I get TypeError: 'str' object does not support item assignment:You build the object before encoding it to a JSON string:JSON is a serialization format, textual data representing a structure. It is not, itself, that structure.You can create the Python dictionary and serialize it to JSON in one line and it's not even ugly.There is already a solution provided which allows building a dictionary, (or nested dictionary for more complex data), but if you wish to build an object, then perhaps try 'ObjDict'.  This gives much more control over the json to be created, for example retaining order, and allows building as an object which may be a preferred representation of your concept.pip install objdict first.[Installation]:All previous answers are correct, here is one more and easy way to do it. For example, create a Dict data structure to serialize and deserialize an object(Notice None is Null in python and I'm intentionally using this to demonstrate how you can store null and convert it to json null)

What's the scope of a variable initialized in an if statement?

froadie

[What's the scope of a variable initialized in an if statement?](https://stackoverflow.com/questions/2829528/whats-the-scope-of-a-variable-initialized-in-an-if-statement)

I'm new to Python, so this is probably a simple scoping question. The following code in a Python file (module) is confusing me slightly:In other languages I've worked in, this code would throw an exception, as the x variable is local to the if statement and should not exist outside of it. But this code executes, and prints 1. Can anyone explain this behavior? Are all variables created in a module global/available to the entire module?

2010-05-13 19:07:45Z

I'm new to Python, so this is probably a simple scoping question. The following code in a Python file (module) is confusing me slightly:In other languages I've worked in, this code would throw an exception, as the x variable is local to the if statement and should not exist outside of it. But this code executes, and prints 1. Can anyone explain this behavior? Are all variables created in a module global/available to the entire module?Python variables are scoped to the innermost function, class, or module in which they're assigned. Control blocks like if and while blocks don't count, so a variable assigned inside an if is still scoped to a function, class, or module.(Implicit functions defined by a generator expression or list/set/dict comprehension do count, as do lambda expressions. You can't stuff an assignment statement into any of those, but lambda parameters and for clause targets are implicit assignment.)Yes, they're in the same "local scope", and actually code like this is common in Python:Note that x isn't declared or initialized before the condition, like it would be in C or Java, for example.In other words, Python does not have block-level scopes. Be careful, though, with examples such aswhich would clearly raise a NameError exception.Scope in python follows this order:(source)Notice that if and other looping/branching constructs are not listed - only classes, functions, and modules provide scope in Python, so anything declared in an if block has the same scope as anything decleared outside the block. Variables aren't checked at compile time, which is why other languages throw an exception. In python, so long as the variable exists at the time you require it, no exception will be thrown.As Eli said, Python doesn't require variable declaration. In C you would say:but in Python declaration is implicit, so when you assign to x it is automatically declared. It's because Python is dynamically typed - it wouldn't work in a statically typed language, because depending on the path used, a variable might be used without being declared. This would be caught at compile time in a statically typed language, but with a dynamically typed language it's allowed.The only reason that a statically typed language is limited to having to declare variables outside of if statements in because of this problem. Embrace the dynamic!Unlike languages such as C, a Python variable is in scope for the whole of the function (or class, or module) where it appears, not just in the innermost "block".  It is as though you declared int x at the top of the function (or class, or module), except that in Python you don't have to declare variables.Note that the existence of the variable x is checked only at runtime -- that is, when you get to the print x statement.  If __name__ didn't equal "__main__" then you would get an exception: NameError: name 'x' is not defined.Yes. It is also true for for scope. But not functions of course.In your example: if the condition in the if statement is false, x will not be defined though.you're executing this code from command line therefore if conditions is true and x is set. Compare:And note that since Python types are only checked at runtime you can have code like:But I'm having trouble thinking of other ways in which the code would operate without an error because of type issues.

Split a string by spaces — preserving quoted substrings — in Python

Adam Pierce

[Split a string by spaces — preserving quoted substrings — in Python](https://stackoverflow.com/questions/79968/split-a-string-by-spaces-preserving-quoted-substrings-in-python)

I have a string which is like this:I'm trying to write something in Python to split it up by space while ignoring spaces within quotes. The result I'm looking for is:PS. I know you are going to ask "what happens if there are quotes within the quotes, well, in my application, that will never happen.

2008-09-17 04:25:15Z

I have a string which is like this:I'm trying to write something in Python to split it up by space while ignoring spaces within quotes. The result I'm looking for is:PS. I know you are going to ask "what happens if there are quotes within the quotes, well, in my application, that will never happen.You want split, from the built-in shlex module.This should do exactly what you want.Have a look at the shlex module, particularly shlex.split.I see regex approaches here that look complex and/or wrong.  This surprises me, because regex syntax can easily describe "whitespace or thing-surrounded-by-quotes", and most regex engines (including Python's) can split on a regex.  So if you're going to use regexes, why not just say exactly what you mean?:Explanation:shlex probably provides more features, though.Depending on your use case, you may also want to check out the csv module:Output: I use shlex.split to process 70,000,000 lines of squid log, it's so slow. So I switched to re.Please try this, if you have performance problem with shlex.Since this question is tagged with regex, I decided to try a regex approach. I first replace all the spaces in the quotes parts with \x00, then split by spaces, then replace the \x00 back to spaces in each part.Both versions do the same thing, but splitter is a bit more readable then splitter2.It seems that for performance reasons re is faster. Here is my solution using a least greedy operator that preserves the outer quotes:Result:It leaves constructs like aaa"bla blub"bbb together as these tokens are not separated by spaces. If the string contains escaped characters, you can match like that:Please note that this also matches the empty string "" by means of the \S part of the pattern.To preserve quotes use this function:The main problem with the accepted shlex approach is that it does not ignore escape characters outside quoted substrings, and gives slightly unexpected results in some corner cases.I have the following use case, where I need a split function that splits input strings such that either single-quoted or double-quoted substrings are preserved, with the ability to escape quotes within such a substring. Quotes within an unquoted string should not be treated differently from any other character. Some example test cases with the expected output:I ended up with the following function to split a string such that the expected output results for all input strings:The following test application checks the results of other approaches (shlex and csv for now) and the custom split implementation:Output:So performance is much better than shlex, and can be improved further by precompiling the regular expression, in which case it will outperform the csv approach.Speed test of different answers:Hmm, can't seem to find the "Reply" button... anyway, this answer is based on the approach by Kate, but correctly splits strings with substrings containing escaped quotes and also removes the start and end quotes of the substrings:This works on strings like 'This is " a \\\"test\\\"\\\'s substring"' (the insane markup is unfortunately necessary to keep Python from removing the escapes).If the resulting escapes in the strings in the returned list are not wanted, you can use this slightly altered version of the function:To get around the unicode issues in some Python 2 versions, I suggest:I suggest:test string:to capture also "" and '':result:to ignore empty "" and '':result:If you don't care about sub strings than a simplePerformance:Or string modulePerformance: String module seems to perform better than string methodsOr you can use RE enginePerformanceFor very long strings you should not load the entire string into memory and instead either split the lines or use an iterative loopTry this:Some test strings:

Python 3: ImportError「No Module named Setuptools」

user1994934

[Python 3: ImportError「No Module named Setuptools」](https://stackoverflow.com/questions/14426491/python-3-importerror-no-module-named-setuptools)

I'm having troubles with installing packages in Python 3.I have always installed packages with setup.py install. But now, when I try to install the ansicolors package I get:I have no idea what to do because I didn't have setuptools installed in the past. Still, I was able to install many packages with setup.py install without setuptools. Why should I get setuptools now?I can't even install setuptools because I have Python 3.3 and setuptools doesn't support Python 3.Why doesn't my install command work anymore?

2013-01-20 16:20:25Z

I'm having troubles with installing packages in Python 3.I have always installed packages with setup.py install. But now, when I try to install the ansicolors package I get:I have no idea what to do because I didn't have setuptools installed in the past. Still, I was able to install many packages with setup.py install without setuptools. Why should I get setuptools now?I can't even install setuptools because I have Python 3.3 and setuptools doesn't support Python 3.Why doesn't my install command work anymore?Your setup.py file needs setuptools. Some Python packages used to use distutils for distribution, but most now use setuptools, a more complete package. Here is a question about the differences between them.To install setuptools on Debian:For Python 3.x:EDIT: Official setuptools dox page:Therefore the rest of this post is probably obsolete (e.g. some links don't work).Distribute - is a setuptools fork which "offers Python 3 support". Installation instructions for distribute(setuptools) + pip: Similar issue here. UPDATE: Distribute seems to be obsolete, i.e. merged into Setuptools: Distribute is a deprecated fork of the Setuptools project. Since the Setuptools 0.7 release, Setuptools and Distribute have merged and Distribute is no longer being maintained. All ongoing effort should reference the Setuptools project and the Setuptools documentation.You may try with instructions found on setuptools pypi page (I haven't tested this, sorry :( ):I was doing this inside a virtualenv on Oracle Linux 6.4 using python-2.6 so the apt-based solutions weren't an option for me, nor were the python-2.7 ideas. My fix was to upgrade my version of setuptools that had been installed by virtualenv:After that, I was able to install packages into the virtualenv. I know this question has already had an answer selected but I hope this answer will help others in my situation.pip uninstall setuptoolsand then:pip install setuptoolsThis works for me and fix my issue.The distribute package provides a Python 3-compatible version of setuptools: http://pypi.python.org/pypi/distributeAlso, use pip to install the modules. It automatically finds dependencies and installs them for you.It works just fine for me with your package:Windows 7:I have given a complete solution here for python selenium webdriverThe PyPA recommended tool for installing and managing Python packages is pip. pip is included with Python 3.4 (PEP 453), but for older versions here's how to install it (on Windows):Download https://bootstrap.pypa.io/get-pip.pyA few years ago I inherited a python (2.7.1) project running under Django-1.2.3 and now was asked to enhance it with QR possibilities. Got the same problem and did not find pip or apt-get either. So I solved it in a totally different but easy way.

I /bin/vi-ed the setup.py and changed the line 

"from setuptools import setup"

into:

"from distutils.core import setup"

That did it for me, so I thought I should post this for other users running old pythons.

Regards,

Roger Vermeir

I can't install python-ldap

VacuumTube

[I can't install python-ldap](https://stackoverflow.com/questions/4768446/i-cant-install-python-ldap)

When I run the following command:I get this error:Any ideas how to fix this?

2011-01-22 14:44:26Z

When I run the following command:I get this error:Any ideas how to fix this?The python-ldap is based on OpenLDAP, so you need to have the development files (headers) in order to compile the Python module. If you're on Ubuntu, the package is called libldap2-dev.Debian/Ubuntu:RedHat/CentOS:To install python-ldap successfully with pip, following development libraries are needed (package names taken from ubuntu environment):  On CentOS/RHEL 6, you need to install:and yum will also install cyrus-sasl-devel as a dependency.  Then you can run:In Ubuntu it looks like this :Windows: I completely agree with the accepted answer, but digging through the comments took a while to get to the meat of what I needed.  I ran across this specific problem with Reviewboard on Windows using the Bitnami.  To give an answer for windows then, I used this link mentioned in the comments:Then, executed the following commands (because I had python 2.7 and a 32bit install at that)In a Ubuntu/Debian based distro, you could use apt-file to find the name of the exact package that includes the missing header file.As you could see from the output of apt-file search lber.h, you'd just need to install the package libldap2-dev.For those having the same issue of missing Iber.h on Alpine Linux, in a docker image that you are trying to adapt to Alpine for instance. The package you are looking for is: openldap-dev So runapk add openldap-dev

Available from version 3.3 up to EdgeAvailable for both armhf and x86_64 Architectures.On openSUSE you need to install the packages openldap2-devel, cyrus-sasl-devel, python-devel and libopenssl-devel.zypper install openldap2-devel  cyrus-sasl-devel python-devel libopenssl-develOn Fedora 22, you need to do this instead:On OSX, you need the xcode CLI tools.  Just open a terminal and run:For most systems, the build requirements are now mentioned in python-ldap's documentation, in the "Installing" section.If anything is missing for your system (or your system is missing entirely), please let maintainer know!

(As of 2018, I am the maintainer, so a comment here should be enough. Or you can send a pull request or mail.)python3 does not support python-ldap. Rather to install ldap3.To correct the error due to dependencies to install the python-ldap : Windows 7/10download the whl filehttp://www.lfd.uci.edu/~gohlke/pythonlibs/#python-ldap.python 3.6 suit withDeploy the file in :install it withIn FreeBSD 11:As a general solution to install Python packages with binary dependencies [1] on Debian/Ubuntu:You'll have to check the name of your Python package on Ubuntu versus PyPI. In this case they're the same.Obviously doesn't work if the Python package is not in the Ubuntu repos.[1] I learnt this trick when trying to pip install matplotlib on Ubuntu.For alpine dockerif the python version is 3 and above tryIf you're working with windows machines, you can find 'python-ldap' wheel in this Link and then you can install itfor those who are using alphine linux,

apk add openldap-dev

How do you run a Python script as a service in Windows?

Hanno Fietz

[How do you run a Python script as a service in Windows?](https://stackoverflow.com/questions/32404/how-do-you-run-a-python-script-as-a-service-in-windows)

I am sketching the architecture for a set of programs that share various interrelated objects stored in a database. I want one of the programs to act as a service which provides a higher level interface for operations on these objects, and the other programs to access the objects through that service.I am currently aiming for Python and the Django framework as the technologies to implement that service with. I'm pretty sure I figure how to daemonize the Python program in Linux. However, it is an optional spec item that the system should support Windows. I have little experience with Windows programming and no experience at all with Windows services.Is it possible to run a Python programs as a Windows service (i. e. run it automatically without user login)? I won't necessarily have to implement this part, but I need a rough idea how it would be done in order to decide whether to design along these lines.Edit: Thanks for all the answers so far, they are quite comprehensive. I would like to know one more thing: How is Windows aware of my service? Can I manage it with the native Windows utilities? What is the equivalent of putting a start/stop script in /etc/init.d?

2008-08-28 14:28:04Z

I am sketching the architecture for a set of programs that share various interrelated objects stored in a database. I want one of the programs to act as a service which provides a higher level interface for operations on these objects, and the other programs to access the objects through that service.I am currently aiming for Python and the Django framework as the technologies to implement that service with. I'm pretty sure I figure how to daemonize the Python program in Linux. However, it is an optional spec item that the system should support Windows. I have little experience with Windows programming and no experience at all with Windows services.Is it possible to run a Python programs as a Windows service (i. e. run it automatically without user login)? I won't necessarily have to implement this part, but I need a rough idea how it would be done in order to decide whether to design along these lines.Edit: Thanks for all the answers so far, they are quite comprehensive. I would like to know one more thing: How is Windows aware of my service? Can I manage it with the native Windows utilities? What is the equivalent of putting a start/stop script in /etc/init.d?Yes you can. I do it using the pythoncom libraries that come included with ActivePython or can be installed with pywin32 (Python for Windows extensions).This is a basic skeleton for a simple service:Your code would go in the main() method—usually with some kind of infinite loop that might be interrupted by checking a flag, which you set in the SvcStop methodAlthough I upvoted the chosen answer a couple of weeks back, in the meantime I struggled a lot more with this topic. It feels like having a special Python installation and using special modules to run a script as a service is simply the wrong way. What about portability and such?I stumbled across the wonderful Non-sucking Service Manager, which made it really simple and sane to deal with Windows Services. I figured since I could pass options to an installed service, I could just as well select my Python executable and pass my script as an option.I have not yet tried this solution, but I will do so right now and update this post along the process. I am also interested in using virtualenvs on Windows, so I might come up with a tutorial sooner or later and link to it here.The simplest way to achieve this is to use native command sc.exe:References:There are a couple alternatives for installing as a service virtually any Windows executable.For Windows Home Server or Windows Server 2003 (works with WinXP too), the Windows Server 2003 Resource Kit Tools comes with utilities that can be used in tandem for this, called instsrv.exe and srvany.exe.  See this Microsoft KB article KB137890 for details on how to use these utils.  For Windows Home Server, there is a great user friendly wrapper for these utilities named aptly "Any Service Installer".  There is another alternative using ServiceInstaller for Windows NT (download-able here) with python instructions available.  Contrary to the name, it works with both Windows 2000 and Windows XP as well.  Here are some instructions for how to install a python script as a service.After my initial answer, I noticed there were closely related Q&A already posted on SO. See also:Can I run a Python script as a service (in Windows)? How?How do I make Windows aware of a service I have written in Python?The simplest way is to use the: NSSM - the Non-Sucking Service Manager:1 - make download on https://nssm.cc/download2 - install the python program as a service: Win prompt as adminc:>nssm.exe install WinService3 - On NSSM´s console:path: C:\Python27\Python27.exeStartup directory: C:\Python27Arguments: c:\WinService.py4 - check the created services on services.mscStep by step explanation how to make it work :1- First create a python file according to the basic skeleton mentioned above. And save it to a path for example :  "c:\PythonFiles\AppServerSvc.py"2 - On this step we should register our service. Run command prompt as administrator and type as: the first argument of binpath is the path of python.exe second argument of binpath  is the path of your python file that we created alreadyDon't miss that you should put one space after every "=" sign. Then if everything is ok, you should see Now your python service is installed as windows service now. You can see it in Service Manager and registry  under : HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\TestService3- Ok now. You can start your service on service manager. You can execute every python file that provides this service skeleton.  I started hosting as a service with pywin32.Everything was well but I met the problem that service was not able to start within 30 seconds (default timeout for Windows) on system startup. It was critical for me because Windows startup took place simultaneous on several virtual machines hosted on one physical machine, and IO load was huge.

Error messages were:Error 1053: The service did not respond to the start or control request in a timely fashion.Error 7009: Timeout (30000 milliseconds) waiting for the <ServiceName> service to connect.I fought a lot with pywin, but ended up with using NSSM as it was proposed in this answer. It was very easy to migrate to it.pysc: Service Control Manager on PythonExample script to run as a service taken from pythonhosted.org:nssm in python 3+(I converted my .py file to .exe with pyinstaller)nssm:

as said beforeIf you don't want to convert your project to .exe After working on this on and off for a few days, here is the answer I would have wished to find, using pywin32 to keep it nice and self contained.This is complete working code for one loop-based and one thread-based solution.

It may work on both python 2 and 3, although I've only tested the latest version on 2.7 and Win7. The loop should be good for polling code, and the tread should work with more server-like code. It seems to work nicely with the waitress wsgi server that does not have a standard way to shut down gracefully.I would also like to note that there seems to be loads of examples out there, like this that are almost useful, but in reality misleading, because they have cut and pasted other examples blindly. I could be wrong. but why create an event if you never wait for it?That said I still feel I'm on somewhat shaky ground here, especially with regards to how clean the exit from the thread version is, but at least I believe there are nothing misleading here.To run simply copy the code to a file and follow the instructions.Use a simple flag to terminate thread. The important bit is that "thread done" prints.

For a more elaborate example exiting from an uncooperative server thread see my post about the waitress wsgi server.The accepted answer using win32serviceutil works but is complicated and makes debugging and changes harder. It is far easier to use NSSM (the Non-Sucking Service Manager). You write and comfortably debug a normal python program and when it finally works you use NSSM to install it as a service in less than a minute:From an elevated (admin) command prompt you run nssm.exe install NameOfYourService and you fill-in these options:By the way, if your program prints useful messages that you want to keep in a log file NSSM can also handle this and a lot more for you.After reading all the anwsers and create some scripts, if you can run python service.py install and python service.py debug, but python service.py start has no response.Maybe it's caused by venv problem, because windows service start your service by exec PROJECT\venv\Lib\site-packages\win32\pythonservice.exe.You can use powershell or cmd to test your service to find more error's details.If you get some error like me, then you can check my answer in another question, I fixed it and post my code here.

How to use「raise」keyword in Python [duplicate]

Capurnicus

[How to use「raise」keyword in Python [duplicate]](https://stackoverflow.com/questions/13957829/how-to-use-raise-keyword-in-python)

I have read the official definition of "raise", but I still don't quite understand what it does. In simplest terms, what is "raise"?Example usage would help.

2012-12-19 17:27:28Z

I have read the official definition of "raise", but I still don't quite understand what it does. In simplest terms, what is "raise"?Example usage would help.It has 2 purposes.yentup has given the first one.The second is to reraise the current exception in an exception handler, so that it can be handled further up the call stack.It's used for raising errors.Some examples hereraise without any arguments is a special use of python syntax. It means get the exception and re-raise it. If this usage it could have been called reraise.From The Python Language Reference:If raise is used alone without any argument is strictly used for reraise-ing. If done in the situation that is not at a reraise of another exception, the following error is shown:

RuntimeError: No active exception to reraiseBesides raise Exception("message") and raise Python 3 introduced a new form, raise Exception("message") from e. It's called exception chaining, it allows you to preserve the original exception (the root cause) with its traceback.It's very similar to inner exceptions from C#.More info:

https://www.python.org/dev/peps/pep-3134/raise causes an exception to be raised. Some other languages use the verb 'throw' instead.It's intended to signal an error situation; it flags that the situation is exceptional to the normal flow.Raised exceptions can be caught again by code 'upstream' (a surrounding block, or a function earlier on the stack) to handle it, using a try, except combination.You can use it to raise errors as part of error-checking:Or handle some errors, and then pass them on as part of error-handling:

Standard way to embed version into python package?

Dimitri Tcaciuc

[Standard way to embed version into python package?](https://stackoverflow.com/questions/458550/standard-way-to-embed-version-into-python-package)

Is there a standard way to associate version string with a python package in such way that I could do the following?I would imagine there's some way to retrieve that data without any extra hardcoding, since minor/major strings are specified in setup.py already. Alternative solution that I found was to have import __version__ in my foo/__init__.py and then have __version__.py generated by setup.py.

2009-01-19 18:05:54Z

Is there a standard way to associate version string with a python package in such way that I could do the following?I would imagine there's some way to retrieve that data without any extra hardcoding, since minor/major strings are specified in setup.py already. Alternative solution that I found was to have import __version__ in my foo/__init__.py and then have __version__.py generated by setup.py.Not directly an answer to your question, but you should consider naming it __version__, not version.This is almost a quasi-standard. Many modules in the standard library use __version__, and this is also used in lots of 3rd-party modules, so it's the quasi-standard.Usually, __version__ is a string, but sometimes it's also a float or tuple.Edit: as mentioned by S.Lott (Thank you!), PEP 8 says it explicitly:You should also make sure that the version number conforms to the format described in PEP 440 (PEP 386 a previous version of this standard).I use a single _version.py file as the "once cannonical place" to store version information:Here is how it works: the "one canonical place" to store the version number is a .py file, named "_version.py" which is in your Python package, for example in myniftyapp/_version.py. This file is a Python module, but your setup.py doesn't import it! (That would defeat feature 3.) Instead your setup.py knows that the contents of this file is very simple, something like:And so your setup.py opens the file and parses it, with code like:Then your setup.py passes that string as the value of the "version" argument to setup(), thus satisfying feature 2.To satisfy feature 1, you can have your package (at run-time, not at setup time!) import the _version file from myniftyapp/__init__.py like this:Here is an example of this technique that I've been using for years.The code in that example is a bit more complicated, but the simplified example that I wrote into this comment should be a complete implementation.Here is example code of importing the version.If you see anything wrong with this approach, please let me know.After more than ten year of writing Python code and managing various packages I came to the conclusion that DIY is maybe not the best approach. I started using pbr package for dealing with versioning in my packages. If you are using git as your SCM, this will fit into your workflow like magic, saving your weeks of work (you will be surprised about how complex the issue can be).As of today pbr is ranked #11 most used python package and reaching this level didn't include any dirty tricks: was only one: fixing a common packaging problem in a very simple way.pbr can do more of the package maintenance burden, is not limited to versioning but it does not force you to adopt all its benefits.So to give you an idea about how it looks to adopt pbr in one commit have a look swiching packaging to pbrProbably you would observed that the version is not stored at all in the repository. PBR does detect it from Git branches and tags.No need to worry about what happens when you do not have a git repository because pbr does "compile" and cache the version when you package or install the applications, so there is no runtime dependency on git.Here is the best solution I've seen so far and it also explains why:Inside yourpackage/version.py:Inside yourpackage/__init__.py:Inside setup.py:If you know another approach that seems to be better let me know.Per the deferred PEP 396 (Module Version Numbers), there is a proposed way to do this. It describes, with rationale, an (admittedly optional) standard for modules to follow. Here's a snippet: Though this is probably far too late, there is a slightly simpler alternative to the previous answer:(And it would be fairly simple to convert auto-incrementing portions of version numbers to a string using str().)Of course, from what I've seen, people tend to use something like the previously-mentioned version when using __version_info__, and as such store it as a tuple of ints; however, I don't quite see the point in doing so, as I doubt there are situations where you would perform mathematical operations such as addition and subtraction on portions of version numbers for any purpose besides curiosity or auto-incrementation (and even then, int() and str() can be used fairly easily). (On the other hand, there is the possibility of someone else's code expecting a numerical tuple rather than a string tuple and thus failing.)This is, of course, my own view, and I would gladly like others' input on using a numerical tuple.As shezi reminded me, (lexical) comparisons of number strings do not necessarily have the same result as direct numerical comparisons; leading zeroes would be required to provide for that. So in the end, storing __version_info__ (or whatever it would be called) as a tuple of integer values would allow for more efficient version comparisons.Many of these solutions here ignore git version tags which still means you have to track version in multiple places (bad). I approached this with the following goals:The release target always increments the 3rd version digit, but you can use the next_minor_ver or next_major_ver to increment the other digits. The commands rely on the versionbump.py script that is checked into the root of the repoThis does the heavy lifting how to process and increment the version number from git.The my_module/_version.py file is imported into my_module/__init__.py. Put any static install config here that you want distributed with your module.The last step is to read the version info from the my_module module.Of course, for all of this to work you'll have to have at least one version tag in your repo to start.I use a JSON file in the package dir. This fits Zooko's requirements.Inside pkg_dir/pkg_info.json:Inside setup.py:Inside pkg_dir/__init__.py:I also put other information in pkg_info.json, like author. I

like to use JSON because I can automate management of metadata.Also worth noting is that as well as __version__ being a semi-std. in python so is __version_info__ which is a tuple, in the simple cases you can just do something like:...and you can get the __version__ string from a file, or whatever.There doesn't seem to be a standard way to embed a version string in a python package. Most packages I've seen use some variant of your solution, i.e. eitnerI also saw another style:arrow handles it in an interesting way.Now (since 2e5031b)In arrow/__init__.py:In setup.py:BeforeIn arrow/__init__.py:In setup.py:There is not a standard way, but the standard way to manage your packages is setuptools.The best solution I've found overall for managing version is to use setuptools with the pbr extension.  This is now my standard way of managing version.PBR moves most metadata out of the setup.py tools and into a setup.cfg file that is then used as a source for most metadata, which can include version. This allows the metadata to be packaged into an executable using something like pyinstaller if needed (if so, you will probably need this info), and separates the metadata from the other package management/setup scripts.When using Git for VCS/SCM, this setup is even better, as it will pull in a lot of the metadata from Git so that your repo can be your primary source of truth for some of the metadata, including version, authors, changelogs, etc.As PBR will pull version, author, changelog and other info directly from your git repo, so some of the metadata in setup.cfg can be left out and auto generated whenever a distribution is created for your package (using setup.py)setuptools will pull the latest info in real-time using setup.py:This will pull the latest version either from the setup.cfg file, or from the git repo, based on the latest commit that was made and tags that exist in the repo. This command doesn't update the version in a distribution though.When you create a distribution with setup.py (i.e. py setup.py sdist, for example), then all the current info will be extracted and stored in the distribution. This essentially runs the setup.py --version command and then stores that version info into the package.egg-info folder in a set of files that store distribution metadata.You can access the metadata from the current build within Python scripts in the package itself. For version, for example, there are several ways to do this I have found so far:You can put one of these directly in your __init__.py for the package to extract the version info as follows, similar to some other answers:For what it's worth, if you're using NumPy distutils, numpy.distutils.misc_util.Configuration has a make_svn_version_py() method that embeds the revision number inside package.__svn_version__ in the variable version .Since we don't want to manually change the version in the file every time we create a new tag (ready to release a new package version), we can use the following..I highly recommend bumpversion package. I've been using it for years to bump a version.start by adding version=<VERSION> to your setup.py file if you don't have it already.You should use a short script like this every time you bump a version:Then add one file per repo called: .bumpversion.cfg:Note: If you use CVS (or RCS) and want a quick solution, you can use:(Of course, the revision number will be substituted for you by CVS.)This gives you a print-friendly version and a version info that you can use to check that the module you are importing has at least the expected version:

How to get MD5 sum of a string using python?

super9

[How to get MD5 sum of a string using python?](https://stackoverflow.com/questions/5297448/how-to-get-md5-sum-of-a-string-using-python)

In the Flickr API docs, you need to find the MD5 sum of a string to generate the [api_sig] value.How does one go about generating an MD5 sum from a string? Flickr's example:string: 000005fab4534d05api_key9a0554259914a86fb9e7eb014e4e5d52permswriteMD5 sum: a02506b31c1cd46c2e0b6380fb94eb3d

2011-03-14 10:38:56Z

In the Flickr API docs, you need to find the MD5 sum of a string to generate the [api_sig] value.How does one go about generating an MD5 sum from a string? Flickr's example:string: 000005fab4534d05api_key9a0554259914a86fb9e7eb014e4e5d52permswriteMD5 sum: a02506b31c1cd46c2e0b6380fb94eb3dFor Python 2.x, use python's hashlibOutput: a02506b31c1cd46c2e0b6380fb94eb3dYou can do the following:Python 2.xPython 3.xHowever in this case you're probably better off using this helpful Python module for interacting with the Flickr API:... which will deal with the authentication for you.Official documentation of hashlibHave you tried using the MD5 implementation in hashlib? Note that hashing algorithms typically act on binary data rather than text data, so you may want to be careful about which character encoding is used to convert from text to binary data before hashing.The result of a hash is also binary data - it looks like Flickr's example has then been converted into text using hex encoding. Use the hexdigest function in hashlib to get this.You can Try withYou can use b character in front of a string literal:Out:

Django Model() vs Model.objects.create()

0leg

[Django Model() vs Model.objects.create()](https://stackoverflow.com/questions/26672077/django-model-vs-model-objects-create)

What it the difference between running two commands:andDoes the second one immediately create a BarModel in the database, while for FooModel, the save() method has to be called explicitly to add it to the database?

2014-10-31 10:09:49Z

What it the difference between running two commands:andDoes the second one immediately create a BarModel in the database, while for FooModel, the save() method has to be called explicitly to add it to the database?https://docs.djangoproject.com/en/stable/topics/db/queries/#creating-objectsThe two syntaxes are not equivalent and it can lead to unexpected errors.

Here is a simple example showing the differences.

If you have a model:And you create a first object:Then you try to create an object with the same primary key:UPDATE 15.3.2017:I have opened a Django-issue on this and it seems to be preliminary accepted here:

https://code.djangoproject.com/ticket/27825My experience is that when using the Constructor (ORM) class by references with Django 1.10.5 there might be some inconsistencies in the data (i.e. the attributes of the created object may get the type of the input data instead of the casted type of the ORM object property)

example: modelssome_test.py - object.createsome_test.py - Constructor()

What does numpy.random.seed(0) do?

covariance

[What does numpy.random.seed(0) do?](https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do)

What does np.random.seed do in the below code from a Scikit-Learn tutorial? I'm not very familiar with NumPy's random state generator stuff, so I'd really appreciate a layman's terms explanation of this.

2014-02-01 05:28:33Z

What does np.random.seed do in the below code from a Scikit-Learn tutorial? I'm not very familiar with NumPy's random state generator stuff, so I'd really appreciate a layman's terms explanation of this.np.random.seed(0) makes the random numbers predictableWith the seed reset (every time), the same set of numbers will appear every time.If the random seed is not reset, different numbers appear with every invocation:(pseudo-)random numbers work by starting with a number (the seed), multiplying it by a large number, adding an offset, then taking modulo of that sum.  The resulting number is then used as the seed to generate the next "random" number.  When you set the seed (every time), it does the same thing every time, giving you the same numbers.If you want seemingly random numbers, do not set the seed.  If you have code that uses random numbers that you want to debug, however, it can be very helpful to set the seed before each run so that the code does the same thing every time you run it.To get the most random numbers for each run, call numpy.random.seed().  This will cause numpy to set the seed to a random number obtained from /dev/urandom or its Windows analog or, if neither of those is available, it will use the clock.For more information on using seeds to generate pseudo-random numbers, see wikipedia.If you set the np.random.seed(a_fixed_number) every time you call the numpy's other random function, the result will be the same:However, if you just call it once and use various random functions, the results will still be different:As noted, numpy.random.seed(0) sets the random seed to 0, so the pseudo random numbers you get from random will start from the same point. This can be good for debuging in some cases.  HOWEVER, after some reading, this seems to be the wrong way to go at it, if you have threads because it is not thread safe. from differences-between-numpy-random-and-random-random-in-python:example of how to go about this: may give:Lastly, note that there might be cases where initializing to 0 (as opposed to a seed that has not all bits 0) may result to non-uniform distributions for some few first iterations because of the way xor works, but this depends on the algorithm, and is beyond my current worries and the scope of this question.I have used this very often in neural networks. It is well known that when we start training a neural network we randomly initialise the weights. The model is trained on these weights on a particular dataset. After number of epochs you get trained set of weights. Now suppose you want to again train from scratch or you want to pass the model to others to reproduce your results, the weights will be again initialised to a random numbers which mostly will be different from earlier ones. The obtained trained weights after same number of epochs ( keeping same data and other parameters ) as earlier one will differ. The problem is your model is no more reproducible that is every time you train your model from scratch it provides you different sets of weights. This is because the model is being initialized by different random numbers every time.What if every time you start training from scratch the model is initialised to the same set of random initialise weights? In this case  your model could become reproducible. This is achieved by numpy.random.seed(0). By mentioning seed() to a particular number, you are hanging on to same set of random numbers always.Imagine you are showing someone how to code something with a bunch of "random" numbers. By using numpy seed they can use the same seed number and get the same set of "random" numbers.So it's not exactly random because an algorithm spits out the numbers but it looks like a randomly generated bunch.A random seed specifies the start point when a computer generates a random number sequence.For example, let’s say you wanted to generate a random number in Excel (Note: Excel sets a limit of 9999 for the seed). If you enter a number into the Random Seed box during the process, you’ll be able to use the same set of random numbers again. If you typed「77」into the box, and typed「77」the next time you run the random number generator, Excel will display that same set of random numbers. If you type「99」, you’ll get an entirely different set of numbers. But if you revert back to a seed of 77, then you’ll get the same set of random numbers you started with.For example,「take a number x, add 900 +x, then subtract 52.」In order for the process to start, you have to specify a starting number, x (the seed). Let’s take the starting number 77:Add 900 + 77 = 977

Subtract 52 = 925

Following the same algorithm, the second「random」number would be:900 + 925 = 1825

Subtract 52 = 1773

This simple example follows a pattern, but the algorithms behind computer number generation are much more complicatedAll the random numbers generated after setting particular seed value are same across all the platforms/systems.There is a nice explanation in Numpy docs:

https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.RandomState.html

it refers to Mersenne Twister pseudo-random number generator.  More details on the algorithm here: https://en.wikipedia.org/wiki/Mersenne_Twister

Reverse Y-Axis in PyPlot

DarkAnt

[Reverse Y-Axis in PyPlot](https://stackoverflow.com/questions/2051744/reverse-y-axis-in-pyplot)

I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.

2010-01-12 19:31:47Z

I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0.There is a new API that makes this even simpler.and/orDisplacedAussie's answer is correct, but usually a shorter method is just to reverse the single axis in question:where the gca() function returns the current Axes instance and the [::-1] reverses the list.Use matplotlib.pyplot.axis()axis([xmin, xmax, ymin, ymax])So you could add something like this at the end:Although you might want padding at each end so that the extreme points don't sit on the border.If you're in ipython in pylab mode, thenthe show() is required to make it update the current figure.Another similar method to those described above is to use plt.ylim for example:This method works for me when I'm attempting to compound multiple datasets on Y1 and/or Y2using ylim() might be the best approach for your purpose:will result:You could also use function exposed by the axes object of the scatter  plotAlternatively, you can use the matplotlib.pyplot.axis() function, which allows you inverting any of the plot axisOr if you prefer to only reverse the X-axis, thenIndeed, you can invert both axis:If using matplotlib you can try:

matplotlib.pyplot.xlim(l, r)

matplotlib.pyplot.ylim(b, t)

These two lines set the limits of the x and y axes respectively. For the x axis, the first argument l sets the left most value, and the second argument r sets the right most value. For the y axis, the first argument b sets the bottom most value, and the second argument t sets the top most value.

Shuffle an array with python, randomize array item order with python

davethegr8

[Shuffle an array with python, randomize array item order with python](https://stackoverflow.com/questions/473973/shuffle-an-array-with-python-randomize-array-item-order-with-python)

What's the easiest way to shuffle an array with python?

2009-01-23 18:34:29Z

What's the easiest way to shuffle an array with python?Alternative way to do this using sklearnOutput:Advantage: You can random multiple arrays simultaneously without disrupting the mapping. And 'random_state' can control the shuffling for reproducible behavior.The other answers are the easiest, however it's a bit annoying that the random.shuffle method doesn't actually return anything - it just sorts the given list.  If you want to chain calls or just be able to declare a shuffled array in one line you can do:Then you can do lines like:When dealing with regular Python lists, random.shuffle() will do the job just as the previous answers show.But when it come to ndarray(numpy.array), random.shuffle seems to break the original ndarray. Here is an example:Just use: np.random.shuffle(a)Like random.shuffle, np.random.shuffle shuffles the array in-place.Just in case you want a new array you can use sample:You can sort your array with random keykey only be read once so comparing item during sort still efficient.but look like random.shuffle(array) will be faster since it written in CIn addition to the previous replies, I would like to introduce another function.numpy.random.shuffle as well as random.shuffle perform in-place shuffling. However, if you want to return a shuffled array numpy.random.permutation is the function to use.I don't know I used random.shuffle() but it return 'None' to me, so I wrote this, might helpful to someoneBe aware that random.shuffle() should not be used on multi-dimensional arrays as it causes repetitions. Imagine you want to shuffle an array along its first dimension, we can create the following test example,so that along the first axis, the i-th element corresponds to a 2x3 matrix where all the elements are equal to i.If we use the correct shuffle function for multi-dimensional arrays, i.e. np.random.shuffle(x), the array will be shuffled along the first axis as desired. However, using random.shuffle(x) will cause repetitions. You can check this by running len(np.unique(x)) after shuffling which gives you 10 (as expected) with np.random.shuffle() but only around 5 when using random.shuffle().

What's the best way to parse a JSON response from the requests library?

RickD

[What's the best way to parse a JSON response from the requests library?](https://stackoverflow.com/questions/16877422/whats-the-best-way-to-parse-a-json-response-from-the-requests-library)

I'm using the python requests module to send a RESTful GET to a server, for which I get a response in JSON. The JSON response is basically just a list of lists.What's the best way to coerce the response to a native Python object so I can either iterate or print it out using pprint?

2013-06-01 21:26:07Z

I'm using the python requests module to send a RESTful GET to a server, for which I get a response in JSON. The JSON response is basically just a list of lists.What's the best way to coerce the response to a native Python object so I can either iterate or print it out using pprint?You can use json.loads:This converts a given string into a dictionary which allows you to access your JSON data easily within your code.Or you can use @Martijn's helpful suggestion, and the higher voted answer, response.json().Since you're using requests, you should use the response's json method.It autodetects which decoder to use.

What is the purpose of class methods?

Dave Webb

[What is the purpose of class methods?](https://stackoverflow.com/questions/38238/what-is-the-purpose-of-class-methods)

I'm teaching myself Python and my most recent lesson was that Python is not Java, and so I've just spent a while turning all my Class methods into functions.I now realise that I don't need to use Class methods for what I would done with static methods in Java, but now I'm not sure when I would use them.  All the advice I can find about Python Class methods is along the lines of newbies like me should steer clear of them, and the standard documentation is at its most opaque when discussing them.Does anyone have a good example of using a Class method in Python or at least can someone tell me when Class methods can be sensibly used?

2008-09-01 18:16:41Z

I'm teaching myself Python and my most recent lesson was that Python is not Java, and so I've just spent a while turning all my Class methods into functions.I now realise that I don't need to use Class methods for what I would done with static methods in Java, but now I'm not sure when I would use them.  All the advice I can find about Python Class methods is along the lines of newbies like me should steer clear of them, and the standard documentation is at its most opaque when discussing them.Does anyone have a good example of using a Class method in Python or at least can someone tell me when Class methods can be sensibly used?Class methods are for when you need to have methods that aren't specific to any particular instance, but still involve the class in some way. The most interesting thing about them is that they can be overridden by subclasses, something that's simply not possible in Java's static methods or Python's module-level functions.If you have a class MyClass, and a module-level function that operates on MyClass (factory, dependency injection stub, etc), make it a classmethod. Then it'll be available to subclasses.Factory methods (alternative constructors) are indeed a classic example of class methods.Basically, class methods are suitable anytime you would like to have a method which naturally fits into the namespace of the class, but is not associated with a particular instance of the class.As an example, in the excellent unipath module:As the current directory is process wide, the cwd method has no particular instance with which it should be associated.  However, changing the cwd to the directory of a given Path instance should indeed be an instance method.Hmmm... as Path.cwd() does indeed return a Path instance, I guess it could be considered to be a factory method...Think about it this way: normal methods are useful to hide the details of dispatch: you can type myobj.foo() without worrying about whether the foo() method is implemented by the myobj object's class or one of its parent classes. Class methods are exactly analogous to this, but with the class object instead: they let you call MyClass.foo() without having to worry about whether foo() is implemented specially by MyClass because it needed its own specialized version, or whether it is letting its parent class handle the call.Class methods are essential when you are doing set-up or computation that precedes the creation of an actual instance, because until the instance exists you obviously cannot use the instance as the dispatch point for your method calls. A good example can be viewed in the SQLAlchemy source code; take a look at the dbapi() class method at the following link:https://github.com/zzzeek/sqlalchemy/blob/ab6946769742602e40fb9ed9dde5f642885d1906/lib/sqlalchemy/dialects/mssql/pymssql.py#L47You can see that the dbapi() method, which a database backend uses to import the vendor-specific database library it needs on-demand, is a class method because it needs to run before instances of a particular database connection start getting created — but that it cannot be a simple function or static function, because they want it to be able to call other, supporting methods that might similarly need to be written more specifically in subclasses than in their parent class. And if you dispatch to a function or static class, then you "forget" and lose the knowledge about which class is doing the initializing.I recently wanted a very light-weight logging class that would output varying amounts of output depending on the logging level that could be programmatically set.  But I didn't want to instantiate the class every time I wanted to output a debugging message or error or warning.  But I also wanted to encapsulate the functioning of this logging facility and make it reusable without the declaration of any globals.So I used class variables and the @classmethod decorator to achieve this.With my simple Logging class, I could do the following:Then, in my code, if I wanted to spit out a bunch of debugging information, I simply had to codeErrors could be out put with In the "production" environment, I can specify and now, only the error message will be output.  The debug message will not be printed.Here's my class:And some code that tests it just a bit:Alternative constructors are the classic example.When a user logs in on my website, a User() object is instantiated from the username and password.If I need a user object without the user being there to log in (e.g. an admin user might want to delete another users account, so i need to instantiate that user and call its delete method):I have class methods to grab the user object.I think the most clear answer is AmanKow's one. It boils down to how u want to organize your code. You can write everything as module level functions which are wrapped in the namespace of the module i.eThe above procedural code is not well organized, as you can see after only 3 modules it gets confusing, what is each method do ? You can use long descriptive names for functions(like in java) but still  your code gets unmanageable very quick.The object oriented way is to break down your code into manageable blocks i.e Classes & objects and functions can be associated with objects instances or with classes. With class functions you gain another level of division in your code compared with module level functions.

So you can group related functions within a class to make them more specific to a task  that you assigned to that class. For example you can create a file utility class :This way is more flexible and more maintainable, you group functions together and its more obvious to what each function do. Also you prevent name conflicts, for example the function copy may exist in another imported module(for example network copy) that you use in your code, so when you use the full name FileUtil.copy() you remove the problem and both copy functions can be used side by side.Honestly?  I've never found a use for staticmethod or classmethod.  I've yet to see an operation that can't be done using a global function or an instance method.It would be different if python used private and protected members more like Java does.  In Java, I need a static method to be able to access an instance's private members to do stuff.  In Python, that's rarely necessary.Usually, I see people using staticmethods and classmethods when all they really need to do is use python's module-level namespaces better.It allows you to write generic class methods that you can use with any compatible class.For example:If you don't use @classmethod you can do it with self keyword but it needs an instance of Class:I used to work with PHP and recently I was asking myself, whats going on with this classmethod? Python manual is very technical and very short in words so it wont help with understanding that feature. I was googling and googling and I found answer -> http://code.anjanesh.net/2007/12/python-classmethods.html.If you are lazy to click it. My explanation is shorter and below. :)in PHP (maybe not all of you know PHP, but this language is so straight forward that everybody should understand what I'm talking about) we have static variables like this:The output will be in both cases 20.However in python we can add @classmethod decorator and thus it is possible to have output 10 and 20 respectively. Example:Smart, ain't?Class methods provide a "semantic sugar" (don't know if this term is widely used) - or "semantic convenience".Example: you got a set of classes representing objects. You might want to have the class method all() or find() to write User.all() or User.find(firstname='Guido'). That could be done using module level functions of course...What just hit me, coming from Ruby, is that a so-called class method and a so-called instance method is just a function with semantic meaning applied to its first parameter, which is silently passed when the function is called as a method of an object (i.e. obj.meth()).Normally that object must be an instance but the @classmethod method decorator changes the rules to pass a class. You can call a class method on an instance (it's just a function) - the first argument will be its class. Because it's just a function, it can only be declared once in any given scope (i.e. class definition). If follows therefore, as a surprise to a Rubyist, that you can't have a class method and an instance method with the same name.Consider this:You can call foo on an instance But not on a class:Now add @classmethod:Calling on an instance now passes its class:as does calling on a class:It's only convention that dictates that we use self for that first argument on an instance method and cls on a class method. I used neither here to illustrate that it's just an argument. In Ruby, self is a keyword.Contrast with Ruby:The Python class method is just a decorated function and you can use the same techniques to create your own decorators. A decorated method wraps the real method (in the case of @classmethod it passes the additional class argument). The underlying method is still there, hidden but still accessible.footnote: I wrote this after a name clash between a class and instance method piqued my curiosity. I am far from a Python expert and would like comments if any of this is wrong.This is an interesting topic. My take on it is that python classmethod operates like a singleton rather than a factory (which returns a produced an instance of a class). The reason it is a singleton is that there is a common object that is produced (the dictionary) but only once for the class but shared by all instances.To illustrate this here is an example. Note that all instances have a reference to the single dictionary. This is not Factory pattern as I understand it. This is probably very unique to python.I was asking myself the same question few times. And even though the guys here tried hard to explain it, IMHO the best answer (and simplest) answer I have found is the description of the Class method in the Python Documentation.There is also reference to the Static method. And in case someone already know instance methods (which I assume), this answer might be the final piece to put it all together...Further and deeper elaboration on this topic can be found also in the documentation:

The standard type hierarchy (scroll down to Instance methods section)A class defines a set of instances, of course.  And the methods of a class work on the individual instances.  The class methods (and variables) a place to hang other information that is related to the set of instances over all.For example if your class defines a the set of students you might want class variables or methods which define things like the set of grade the students can be members of.You can also use class methods to define tools for working on the entire set.  For example Student.all_of_em() might return all the known students.  Obviously if your set of instances have more structure than just a set you can provide class methods to know about that structure.  Students.all_of_em(grade='juniors')Techniques like this tend to lead to storing members of the set of instances into data structures that are rooted in class variables.  You need to take care to avoid frustrating the garbage collection then.@classmethod can be useful for easily instantiating objects of that class from outside resources. Consider the following:Then in another file:Accessing inst.x will give the same value as settings['x'].

Django gives Bad Request (400) when DEBUG = False

codeimplementer

[Django gives Bad Request (400) when DEBUG = False](https://stackoverflow.com/questions/19875789/django-gives-bad-request-400-when-debug-false)

I am new to django-1.6. When I run the django server with DEBUG = True, it's running perfectly. But when I change DEBUG to False in the settings file, then the server stopped and it gives the following error on the command prompt:After I changed ALLOWED_HOSTS to ["http://127.0.0.1:8000",], in the browser I get the error:Is it possible to run Django without debug mode?

2013-11-09 12:11:41Z

I am new to django-1.6. When I run the django server with DEBUG = True, it's running perfectly. But when I change DEBUG to False in the settings file, then the server stopped and it gives the following error on the command prompt:After I changed ALLOWED_HOSTS to ["http://127.0.0.1:8000",], in the browser I get the error:Is it possible to run Django without debug mode?The ALLOWED_HOSTS list should contain fully qualified host names, not urls. Leave out the port and the protocol. If you are using 127.0.0.1, I would add localhost to the list too:You could also use * to match any host:Quoting the documentation:Bold emphasis mine.The status 400 response you get is due to a SuspiciousOperation exception being raised when your host header doesn't match any values in that list.For me, I got this error by not setting USE_X_FORWARDED_HOST to true.  From the docs: My hosting service wrote explicitly in their documentation that this setting must be used, and I get this 400 error if I forget it.  I had the same problem and I fixed it by setting ALLOWED_HOSTS = ['*'] and to solve the problem with the static images you have to change the virtual paths in the environment configuration like this:Virtual Path 

                Directory

/static/                          /opt/python/current/app/yourpj/static/

/media/                        /opt/python/current/app/Nuevo/media/I hope it helps you.PD: sorry for my bad english.I had the same problem and none of the answers resolved my problem, for resolving the situation like this it's better to enable logging by adding the following config to settings.py temporaryand try to tail -f /tmp/debug.log.

and when you see your issue you can handle it much easier than blind debugging.My issue was about to Invalid HTTP_HOST header: 'pt_web:8000'. The domain name provided is not valid according to RFC 1034/1035.and resolve it by adding proxy_set_header Host $host; to Nginx config file and enabling port forwarding by USE_X_FORWARDED_PORT = True in the settings.py ( it's because in my case I've listened to request in Nginx on port 8080 and pass it to guni on port 8000With DEBUG = False in you settings file, you also need ALLOWED_HOST list set up.

Try including ALLOWED_HOST = ['127.0.0.1', 'localhost', 'www.yourdomain.com']Otherwise you might receive a Bad Request(400) error from django.For me as I have already xampp on 127.0.0.1 and django on 127.0.1.1

and i kept trying adding hosts and i got the same error or (400) bad request

 so I change the url to 127.0.1.1:(the used port)/project

and voila !you have to check what is your virtual network address, for me as i use bitnami django stack 2.2.3-1 on Linux i can check which port django is using.

if you have an error ( 400 bad request ) then i guess django on different virtual network ..

good luck 

Try to run your server with the --insecure  just like this: python manage.py runserver --insecureNavigate to settings and locate the base.py file

Set the allowed hosts to 

 ALLOWED_HOSTS = ['*']

grouping rows in list in pandas groupby

Abhishek Thakur

[grouping rows in list in pandas groupby](https://stackoverflow.com/questions/22219004/grouping-rows-in-list-in-pandas-groupby)

I have a pandas data frame like:I want to group by the first column and get second column as lists in rows:Is it possible to do something like this using pandas groupby?

2014-03-06 08:31:09Z

I have a pandas data frame like:I want to group by the first column and get second column as lists in rows:Is it possible to do something like this using pandas groupby?You can do this using groupby to group on the column of interest and then apply list to every group:A handy way to achieve this would be:Look into writing Custom Aggregations: https://www.kaggle.com/akshaysehgal/how-to-group-by-aggregate-using-pyAs you were saying the groupby method of a pd.DataFrame object can do the job.Examplewhich gives and index-wise description of the groups.To get elements of single groups, you can do, for instanceTo solve this for several columns of a dataframe:This answer was inspired from Anamika Modi's answer. Thank you!Use any of the following groupby and agg recipes.To aggregate multiple columns as lists, use any of the following:To group-listify a single column only, convert the groupby to a SeriesGroupBy object, then call SeriesGroupBy.agg. Use,If looking for a unique list while grouping multiple columns this could probably help:Let us using df.groupby with list and Series constructor Here I have grouped elements with "|" as a separator

    import pandas as pd

How to calculate the angle between a line and the horizontal axis?

orlp

[How to calculate the angle between a line and the horizontal axis?](https://stackoverflow.com/questions/7586063/how-to-calculate-the-angle-between-a-line-and-the-horizontal-axis)

In a programming language (Python, C#, etc) I need to determine how to calculate the angle between a line and the horizontal axis?I think an image describes best what I want:Given (P1x,P1y) and (P2x,P2y) what is the best way to calculate this angle? The origin is in the topleft and only the positive quadrant is used.

2011-09-28 15:58:40Z

In a programming language (Python, C#, etc) I need to determine how to calculate the angle between a line and the horizontal axis?I think an image describes best what I want:Given (P1x,P1y) and (P2x,P2y) what is the best way to calculate this angle? The origin is in the topleft and only the positive quadrant is used.First find the difference between the start point and the end point (here, this is more of a directed line segment, not a "line", since lines extend infinitely and don't start at a particular point).Then calculate the angle (which runs from the positive X axis at P1 to the positive Y axis at P1).But arctan may not be ideal, because dividing the differences this way will erase the distinction needed to distinguish which quadrant the angle is in (see below). Use the following instead if your language includes an atan2 function:EDIT (Feb. 22, 2017): In general, however, calling atan2(deltaY,deltaX) just to get the proper angle for cos and sin may be inelegant. In those cases, you can often do the following instead:EDIT (Feb. 28, 2017): Even without normalizing (deltaX, deltaY):An implementation in Python using radians (provided on July 19, 2015 by Eric Leschinski, who edited my answer):All tests pass. See https://en.wikipedia.org/wiki/Unit_circleSorry, but I'm pretty sure Peter's answer is wrong. Note that the y axis goes down the page (common in graphics). As such the deltaY calculation has to be reversed, or you get the wrong answer.Consider:givesSo if in the example above, P1 is (1,1) and P2 is (2,2) [because Y increases down the page], the code above will give 45.0 degrees for the example shown, which is wrong. Change the order of the deltaY calculation and it works properly.I have found a solution in Python that is working well !Considering the exact question, putting us in a "special" coordinates system where positive axis means moving DOWN (like a screen or an interface view), you need to adapt this function like this, and negative the Y coordinates:Example in Swift 2.0This function gives a correct answer to the question. Answer is in radians, so the usage, to view angles in degrees, is: Based on reference "Peter O".. Here is the java versionmatlab function:A formula for an angle from 0 to 2pi.There is x=x2-x1 and y=y2-y1.The formula is working forany value of x and y. For x=y=0 the result is undefined.f(x,y)=pi()-pi()/2*(1+sign(x))*(1-sign(y^2))

How to print the value of a Tensor object in TensorFlow?

Dawny33

[How to print the value of a Tensor object in TensorFlow?](https://stackoverflow.com/questions/33633370/how-to-print-the-value-of-a-tensor-object-in-tensorflow)

I have been using the introductory example of matrix multiplication in TensorFlow.When I print the product, it is displaying it as a Tensor object:But how do I know the value of product?The following doesn't help:I know that graphs run on Sessions, but isn't there any way I can check the output of a Tensor object without running the graph in a session?

2015-11-10 15:19:58Z

I have been using the introductory example of matrix multiplication in TensorFlow.When I print the product, it is displaying it as a Tensor object:But how do I know the value of product?The following doesn't help:I know that graphs run on Sessions, but isn't there any way I can check the output of a Tensor object without running the graph in a session?The easiest[A] way to evaluate the actual value of a Tensor object is to pass it to the Session.run() method, or call Tensor.eval() when you have a default session (i.e. in a with tf.Session(): block, or see below). In general[B], you cannot print the value of a tensor without running some code in a session.If you are experimenting with the programming model, and want an easy way to evaluate tensors, the tf.InteractiveSession lets you open a session at the start of your program, and then use that session for all Tensor.eval() (and Operation.run()) calls. This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a Session object everywhere. For example, the following works in a Jupyter notebook:This might seem silly for such a small expression, but one of the key ideas in Tensorflow is deferred execution: it's very cheap to build a large and complex expression, and when you want to evaluate it, the back-end (to which you connect with a Session) is able to schedule its execution more efficiently (e.g. executing independent parts in parallel and using GPUs).[A]: To print the value of a tensor without returning it to your Python program, you can use the tf.Print() operator, as Andrzej suggests in another answer. Note that you still need to run part of the graph to see the output of this op, which is printed to standard output. If you're running distributed TensorFlow, tf.Print() will print its output to the standard output of the task where that op runs. This means that if you use https://colab.research.google.com for example, or any other Jupyter Notebook, then you will not see the output of tf.Print() in the notebook; in that case refer to this answer on how to get it to print still.[B]: You might be able to use the experimental tf.contrib.util.constant_value() function to get the value of a constant tensor, but it isn't intended for general use, and it isn't defined for many operators.While other answers are correct that you cannot print the value until you evaluate the graph, they do not talk about one easy way of actually printing a value inside the graph, once you evaluate it. The easiest way to see a value of a tensor whenever the graph is evaluated (using run or eval) is to use the Print operation as in this example:Now, whenever we evaluate the whole graph, e.g. using b.eval(), we get:Reiterating what others said, its not possible to check the values without running the graph.A simple snippet for anyone looking for an easy example to print values is as below. The code can be executed without any modification in ipython notebookOutput:No, you can not see the content of the tensor without running the graph (doing session.run()). The only things you can see are:I have not found this in documentation, but I believe that the values of the variables (and some of the constants are not calculated at the time of assignment).Take a look at this example:The first example where I just initiate a constant Tensor of random numbers run approximately the same time irrespectibly of dim (0:00:00.003261)In the second case, where the constant is actually gets evaluated and the values are assigned, the time clearly depends on dim (0:00:01.244642)And you can make it more clear by calculating something (d = tf.matrix_determinant(m1), keeping in mind that the time will run in O(dim^2.8))P.S. I found were it is explained in documentation:I think you need to get some fundamentals right. With the examples above you have created tensors (multi dimensional array). But for tensor flow to really work you have to initiate a "session" and run your "operation" in the session. Notice the word "session" and "operation".

You need to know 4 things to work with  tensorflow: Now from what you wrote out you have given the tensor, and the operation but you have no session running nor a graph. Tensor (edges of the graph) flow through graphs and are manipulated by operations (nodes of the graph). There is default graph but you can initiate yours in a session.When you say print , you only access the shape of the variable or constant you defined.So you can see what you are missing :Hope it helps!In the recent Tensorflow 1.13.1With Tensorflow 2.0, eager mode is enabled by default. so the following code works with TF2.0.Based on the answers above, with your particular code snippet you can print the product like this:You can check the output of a TensorObject without running the graph in a session, by enabling eager execution.Simply add the following two lines of code:

import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

right after you import tensorflow.The output of print product in your example will now be:

tf.Tensor([[ 12.]], shape=(1, 1), dtype=float32)

Note that as of now (November 2017) you'll have to install a Tensorflow nightly build to enable eager execution. Pre-built wheels can be found here.In Tensorflow 2.0+ (or in Eager mode environment) you can call .numpy() method:tf.keras.backend.eval is useful for evaluating small expressions.TF 1.x and TF 2.0 compatible.Minimal Verifiable ExampleThis is useful because you do not have to explicitly create a Session or InteractiveSession. You should think of TensorFlow Core programs as consisting of two discrete sections:So for the code below you just Build the computational graph.  You need also To initialize all the variables in a TensorFlow program , you must explicitly call a special operation as follows:Now you build the graph and initialized all variables ,next step is to evaluate the nodes, you must run the computational graph within a session. A session encapsulates the control and state of the TensorFlow runtime.The following code creates a Session object and then invokes its run method to run enough of the computational graph to evaluate product :Please note that tf.Print() will change the tensor name.

If the tensor you seek to print is a placeholder, feeding data to it will fail as the original name will not be found during feeding.

For example:Output is:You can use Keras, one-line answer will be to use eval method like so:Try this simple code!  (it is self explanatory) I didn't find it easy to understand what is required even after reading all the answers until I executed this. TensofFlow is new to me too.But still you may need the value returned by executing the session.Basically, in tensorflow when you create a tensor of any sort they are created and stored inside which can be accessed only when you run a tensorflow session. Say you have created a constant tensor 

c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

Without running a session, you can get

- op: An Operation. Operation that computes this tensor.

- value_index: An int. Index of the operation's endpoint that produces this tensor.

- dtype: A DType. Type of elements stored in this tensor.To get the values you can run a session with the tensor you require as:The output will be something like this:Enable the eager execution which is introduced in tensorflow after version 1.10.

It's very easy to use.Using tips provided in https://www.tensorflow.org/api_docs/python/tf/print I use the log_d function to print formatted strings.tf.Print is now deprecated, here's how to use tf.print (lowercase p) instead.While running a session is a good option, it is not always the way to go. For instance, you may want to print some tensor in a particular session.The new print method returns a print operation which has no output tensors:Since it has no outputs, you can't insert it in a graph the same way as you could with tf.Print. Instead, you can you can add it to control dependencies in your session in order to make it print.Sometimes, in a larger graph, maybe created partly in subfunctions, it is cumbersome to propagate the print_op to the session call. Then, tf.tuple can be used to couple the print operation with another operation, which will then run with that operation whichever session executes the code. Here's how that is done: Question: How to print the value of a Tensor object in TensorFlow?Answer:

How can I rename a conda environment?

pkowalczyk

[How can I rename a conda environment?](https://stackoverflow.com/questions/42231764/how-can-i-rename-a-conda-environment)

I have a conda environment named old_name, how can I change its name to new_name without breaking references?

2017-02-14 16:51:16Z

I have a conda environment named old_name, how can I change its name to new_name without breaking references?You can't.One workaround is to create clone environment, and then remove original one:(remember about deactivating current environment with deactivate on Windows and source deactivate on macOS/Linux)There are several drawbacks of this method:There is an open issue requesting this feature.Based upon dwanderson's helpful comment, I was able to do this in a Bash one-liner:My badly named env was "env1" and the new one I wish to clone from it is "envpython2".Just open the folder"\Anaconda\envs" and rename it.Before Rename

After Rename

Actual meaning of 'shell=True' in subprocess

user225312

[Actual meaning of 'shell=True' in subprocess](https://stackoverflow.com/questions/3172470/actual-meaning-of-shell-true-in-subprocess)

I am calling different processes with the subprocess module. However, I have a question.In the following codes:and Both work. After reading the docs, I came to know that shell=True means executing the code through the shell. So that means in absence, the process is directly started. So what should I prefer for my case - I need to run a process and get its output. What benefit do I have from calling it from within the shell or outside of it.

2010-07-03 18:39:55Z

I am calling different processes with the subprocess module. However, I have a question.In the following codes:and Both work. After reading the docs, I came to know that shell=True means executing the code through the shell. So that means in absence, the process is directly started. So what should I prefer for my case - I need to run a process and get its output. What benefit do I have from calling it from within the shell or outside of it.The benefit of not calling via the shell is that you are not invoking a 'mystery program.'  On POSIX, the environment variable SHELL controls which binary is invoked as the "shell."  On Windows, there is no bourne shell descendent, only cmd.exe.So invoking the shell invokes a program of the user's choosing and is platform-dependent.  Generally speaking, avoid invocations via the shell.Invoking via the shell does allow you to expand environment variables and file globs according to the shell's usual mechanism.  On POSIX systems, the shell expands file globs to a list of files.  On Windows, a file glob (e.g., "*.*") is not expanded by the shell, anyway (but environment variables on a command line are expanded by cmd.exe).If you think you want environment variable expansions and file globs, research the ILS attacks of 1992-ish on network services which performed subprogram invocations via the shell.  Examples include the various sendmail backdoors involving ILS.In summary, use shell=False.source: Subprocess ModuleExecuting programs through the shell means that all user input passed to the program is interpreted according to the syntax and semantic rules of the invoked shell.  At best, this only causes inconvenience to the user, because the user has to obey these rules.  For instance, paths containing special shell characters like quotation marks or blanks must be escaped.  At worst, it causes security leaks, because the user can execute arbitrary programs.shell=True is sometimes convenient to make use of specific shell features like word splitting or parameter expansion.  However, if such a feature is required, make use of other modules are given to you (e.g. os.path.expandvars() for parameter expansion or shlex for word splitting).  This means more work, but avoids other problems.In short: Avoid shell=True by all means.An example where things could go wrong with Shell=True is shown hereCheck the doc here: subprocess.call()The other answers here adequately explain the security caveats which are also mentioned in the subprocess documentation.  But in addition to that, the overhead of starting a shell to start the program you want to run is often unnecessary and definitely silly for situations where you don't actually use any of the shell's functionality.  Moreover, the additional hidden complexity should scare you, especially if you are not very familiar with the shell or the services it provides.Where the interactions with the shell are nontrivial, you now require the reader and maintainer of the Python script (which may or may not be your future self) to understand both Python and shell script. Remember the Python motto "explicit is better than implicit"; even when the Python code is going to be somewhat more complex than the equivalent (and often very terse) shell script, you might be better off removing the shell and replacing the functionality with native Python constructs. Minimizing the work done in an external process and keeping control within your own code as far as possible is often a good idea simply because it improves visibility and reduces the risks of -- wanted or unwanted -- side effects.Wildcard expansion, variable interpolation, and redirection are all simple to replace with native Python constructs.  A complex shell pipeline where parts or all cannot be reasonably rewritten in Python would be the one situation where perhaps you could consider using the shell.  You should still make sure you understand the performance and security implications.In the trivial case, to avoid shell=True, simply replacewithNotice how the first argument is a list of strings to pass to execvp(), and how quoting strings and backslash-escaping shell metacharacters is generally not necessary (or useful, or correct).

Maybe see also When to wrap quotes around a shell variable?As an aside, you very often want to avoid Popen if one of the simpler wrappers in the subprocess package does what you want. If you have a recent enough Python, you should probably use subprocess.run.If not, for many tasks, you want check_output to obtain the output from a command, whilst checking that it succeeded, or check_call if there is no output to collect.I'll close with a quote from David Korn: "It's easier to write a portable shell than a portable shell script." Even subprocess.run('echo "$HOME"', shell=True) is not portable to Windows.

Concatenating two one-dimensional NumPy arrays

highBandWidth

[Concatenating two one-dimensional NumPy arrays](https://stackoverflow.com/questions/9236926/concatenating-two-one-dimensional-numpy-arrays)

I have two simple one-dimensional arrays in NumPy. I should be able to concatenate them using numpy.concatenate. But I get this error for the code below:Why?

2012-02-11 01:11:06Z

I have two simple one-dimensional arrays in NumPy. I should be able to concatenate them using numpy.concatenate. But I get this error for the code below:Why?The line should be:The arrays you want to concatenate need to passed in as a sequence, not as separate arguments.From the NumPy documentation:It was trying to interpret your b as the axis parameter, which is why it complained it couldn't convert it into a scalar.The first parameter to concatenate should itself be a sequence of arrays to concatenate: There are several possibilities for concatenating 1D arrays, e.g.,All those options are equally fast for large arrays; for small ones, concatenate has a slight edge:The plot was created with perfplot:An alternative ist to use the short form of "concatenate" which is either "r_[...]" or "c_[...]" as shown in the example code beneath (see http://wiki.scipy.org/NumPy_for_Matlab_Users for additional information):Which results in:Here are more approaches for doing this by using numpy.ravel(), numpy.array(), utilizing the fact that 1D arrays can be unpacked into plain elements:

`from … import` vs `import .` [duplicate]

wchargin

[`from … import` vs `import .` [duplicate]](https://stackoverflow.com/questions/9439480/from-import-vs-import)

I'm wondering if there's any difference between the code fragmentand the fragmentor if they are interchangeable. If they are interchangeable, which is the "standard"/"preferred" syntax (if there is one)?Thanks!

2012-02-24 23:24:31Z

I'm wondering if there's any difference between the code fragmentand the fragmentor if they are interchangeable. If they are interchangeable, which is the "standard"/"preferred" syntax (if there is one)?Thanks!It depends on how you want to access the import when you refer to it.You can also alias things yourself when you import for simplicity or to avoid masking built ins:Many people have already explained about import vs from, so I want to try to explain a bit more under the hood, where the actual difference lies.First of all, let me explain exactly what the basic import statements do.Now let's see what happens when we do import X.Y:Check sys.modules with name os and os.path:Check globals() and locals() namespace dict with name os and os.path:From the above example, we found that only os is added to the local and global namespaces.

So, we should be able to use os:…but not path:Once you delete the os from locals() namespace, you won't be able to access either os or os.path, even though they do exist in sys.modules:Now let's look at from.Check sys.modules with name os and os.path:So sys.modules looks the same as it did when we imported using import name.Okay. Let's check what it the locals() and globals() namespace dicts look like:You can access by using path, but not by os.path:Let's delete 'path' from locals():One final example using aliasing:And no path defined:When you are import same name from two different modules:Import stat from shutil again:THE LAST IMPORT WILL WINThere is a difference. In some cases, one of those will work and the other won't. Here is an example: say we have the following structure:Now, I want to import b.py into a.py. And I want to import a.py to foo. How do I do this? Two statements, in a I write: In foo.py I write:Well, this will generate an ImportError when trying to run foo.py. The interpreter will complain about the import statement in a.py (import b) saying there is no module b. So how can one fix this? In such a situation, changing the import statement in a to import mylib.b

will not work since a and b are both in mylib. The solution here (or at least one solution) is to use absolute import:Source: Python: importing a module that imports a moduleYou are using Python3 were urllib in the package. Both forms are acceptable and no one form of import is preferred over the other. Sometimes when there are multiple package directories involved you may to use the former from x.y.z.a import sIn this particular case with urllib package, the second way import urllib.request and use of urllib.request is how standard library uniformly uses it.In python 2.x at least you cannot do import urllib2.urlopenYou have to do from urllib2 import urlopenMy main complaint with import urllib.request is that you can still reference urllib.parse even though it isn't imported.Also request for me is under urllib3. Python 2.7.4 ubuntu

Python Pandas: Get index of rows which column matches certain value

I want badges

[Python Pandas: Get index of rows which column matches certain value](https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-which-column-matches-certain-value)

Given a DataFrame with a column "BoolCol", we want to find the indexes of the DataFrame in which the values for "BoolCol" == TrueI currently have the iterating way to do it, which works perfectly:But this is not the correct panda's way to do it.

After some research, I am currently using this code:This one gives me a list of indexes, but they dont match, when I check them by doing:The result is actually False!!Which would be the correct Pandas way to do this?

2014-02-15 16:18:11Z

Given a DataFrame with a column "BoolCol", we want to find the indexes of the DataFrame in which the values for "BoolCol" == TrueI currently have the iterating way to do it, which works perfectly:But this is not the correct panda's way to do it.

After some research, I am currently using this code:This one gives me a list of indexes, but they dont match, when I check them by doing:The result is actually False!!Which would be the correct Pandas way to do this?df.iloc[i] returns the ith row of df. i does not refer to the index label, i is a 0-based index.In contrast, the attribute index returns actual index labels, not numeric row-indices:or equivalently,You can see the difference quite clearly by playing with a DataFrame with

a non-default index that does not equal to the row's numerical position:If you want to use the index, then you can select the rows using loc instead of iloc:Note that loc can also accept boolean arrays:If you have a boolean array, mask, and need ordinal index values, you can compute them using np.flatnonzero:Use df.iloc to select rows by ordinal index:Can be done using numpy where() function:Though you don't always need index for a match, but incase if you need:First you may check query when the target column is type bool  (PS: about how to use it please check link )After we filter the original df by the Boolean column we can pick the index .Also pandas have nonzero, we just select the position of True row and using it slice the DataFrame or index Simple way is to reset the index of the DataFrame prior to filtering:Bit hacky, but it's quick!I extended this question that is how to gets the row, columnand value of all matches value?here is solution:Output:If you want to use your dataframe object only once, use:

Is False == 0 and True == 1 an implementation detail or is it guaranteed by the language?

Eric O Lebigot

[Is False == 0 and True == 1 an implementation detail or is it guaranteed by the language?](https://stackoverflow.com/questions/2764017/is-false-0-and-true-1-an-implementation-detail-or-is-it-guaranteed-by-the)

Is it guaranteed that False == 0 and True == 1, in Python (assuming that they are not reassigned by the user)?  For instance, is it in any way guaranteed that the following code will always produce the same results, whatever the version of Python (both existing and, likely, future ones)?Any reference to the official documentation would be much appreciated!Edit: As noted in many answers, bool inherits from int.  The question can therefore be recast as: "Does the documentation officially say that programmers can rely on booleans inheriting from integers, with the values 0 and 1?".  This question is relevant for writing robust code that won't fail because of implementation details!

2010-05-04 09:03:30Z

Is it guaranteed that False == 0 and True == 1, in Python (assuming that they are not reassigned by the user)?  For instance, is it in any way guaranteed that the following code will always produce the same results, whatever the version of Python (both existing and, likely, future ones)?Any reference to the official documentation would be much appreciated!Edit: As noted in many answers, bool inherits from int.  The question can therefore be recast as: "Does the documentation officially say that programmers can rely on booleans inheriting from integers, with the values 0 and 1?".  This question is relevant for writing robust code that won't fail because of implementation details!In Python 2.x this is not guaranteed as it is possible for True and False to be reassigned.  However, even if this happens, boolean True and boolean False are still properly returned for comparisons.In Python 3.x True and False are keywords and will always be equal to 1 and 0.Under normal circumstances in Python 2, and always in Python 3:False object is of type bool which is a subclass of int:It is the only reason why in your example, ['zero', 'one'][False] does work. It would not work with an object which is not a subclass of integer, because list indexing only works with integers, or objects that define a __index__ method (thanks mark-dickinson).Edit:It is true of the current python version, and of that of Python 3. The docs for python 2.6 and the docs for Python 3 both say:and in the boolean subsection:There is also, for Python 2:So booleans are explicitly considered as integers in Python 2.6 and 3.So you're safe until Python 4 comes along. ;-)Link to the PEP discussing the new bool type in Python 2.3: http://www.python.org/dev/peps/pep-0285/.When converting a bool to an int, the integer value is always 0 or 1, but when converting an int to a bool, the boolean value is True for all integers except 0.In Python 2.x, it is not guaranteed at all:So it could change. In Python 3.x, True, False, and None are reserved words, so the above code would not work.In general, with booleans you should assume that while False will always have an integer value of 0 (so long as you don't change it, as above), True could have any other value. I wouldn't necessarily rely on any guarantee that True==1, but on Python 3.x, this will always be the case, no matter what.Very simple. As bool relates to evaluating an integer as a bool, ONLY zero gives a false answer. ALL Non-Zero values, floats, integers, including negative numbers, or what have you, will return true. A nice example of why this is useful is determining the power status of a device. On is any non-zero value, off is zero. Electronically speaking this makes sense. To determine true or false relatively between values, you must have something to compare it to. This applies to strings and number values, using == or != or <, > >=, <=, etc.You can assign an integer to a variable and then get true or false based on that variable value. Just write int(False) and you will get 0, if you type int(True) it will output 1False is a bool. It has a different type. It is a different object from 0 which is an integer.0 == False returns True because False is cast to an integer. int(False) returns 0The python documentation of the == operator says (help('==')):As a consequence False is converted to an integer for the need of the comparison. But it is different from 0.

Python try…except comma vs 'as' in except

Peter Graham

[Python try…except comma vs 'as' in except](https://stackoverflow.com/questions/2535760/python-try-except-comma-vs-as-in-except)

What is the difference between ',' and 'as' in except statements, eg:and:Is the second syntax legal in 2.6?  It works in CPython 2.6 on Windows but the 2.5 interpreter in cygwin complains that it is invalid.If they are both valid in 2.6 which should I use?

2010-03-29 04:16:56Z

What is the difference between ',' and 'as' in except statements, eg:and:Is the second syntax legal in 2.6?  It works in CPython 2.6 on Windows but the 2.5 interpreter in cygwin complains that it is invalid.If they are both valid in 2.6 which should I use?The definitive document is PEP-3110: Catching ExceptionsSummary:Yes it's legal. I'm running Python 2.6Update: There is another reason to use the as syntax. Using , makes things a lot more ambiguous, as others have pointed out; and here's what makes the difference. As of Python 2.6, there is multicatch which allows you to catch multiple exceptions in one except block. In such a situation, it's more expressive and pythonic to sayrather than to saywhich would still workthe "as" syntax is the preferred one going forward, however if your code needs to work with older Python versions (2.6 is the first to support the new one) then you'll need to use the comma syntax.If you want to support all python versions you can use the sys.exc_info() function like this:(source:http://python3porting.com/noconv.html)As of Python 3.7 (not sure about other versions) the 'comma' syntax is not supported any more:Source file exception_comma.py:

How to add an empty column to a dataframe?

kjo

[How to add an empty column to a dataframe?](https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe)

What's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something likeIs there a less perverse method?

2013-05-01 21:46:47Z

What's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something likeIs there a less perverse method?If I understand correctly, assignment should fill:To add to DSM's answer and building on this associated question, I'd split the approach into two cases:Here is an example adding multiple columns:or You can also always concatenate a new (empty) dataframe to the existing dataframe, but that doesn't feel as pythonic to me :)an even simpler solution is: where "header_list" is a list of the headers you want to appear.any header included in the list that is not found already in the dataframe will be added with blank cells below.so if then c and d will be added as columns with blank cellsStarting with v0.16.0, DF.assign() could be used to assign new columns (single/multiple) to a DF. These columns get inserted in alphabetical order at the end of the DF.This becomes advantageous compared to simple assignment in cases wherein you want to perform a series of chained operations directly on the returned dataframe.Consider the same DF sample demonstrated by @DSM:Note that this returns a copy with all the previous columns along with the newly created ones. Inorder for the original DF to be modified accordingly, use it like : df = df.assign(...) as it does not support inplace operation currently.I like:This makes sure that a df with zero rows stays with zero rows.if you want to add column name from a list@emunsing's answer is really cool for adding multiple columns, but I couldn't get it to work for me in python 2.7.  Instead, I found this works:The below code address the question "How do I add n number of empty columns to my existing dataframe". In the interest of keeping solutions to similar problems in one place, I am adding it here.Approach 1 (to create 64 additional columns with column names from 1-64)Approach 2 (to create 64 additional columns with column names from 1-64)You can do 

Python Anaconda - How to Safely Uninstall

william tell

[Python Anaconda - How to Safely Uninstall](https://stackoverflow.com/questions/22585235/python-anaconda-how-to-safely-uninstall)

I installed Python Anaconda on Mac (OS Mavericks). I wanted to revert to the default version of Python on my Mac. What's the best way to do this? Should I delete the ~/anaconda directory? Any other changes required? Currently when I run which python I get this path:/Users/username/anaconda/bin/python

2014-03-22 23:51:54Z

I installed Python Anaconda on Mac (OS Mavericks). I wanted to revert to the default version of Python on my Mac. What's the best way to do this? Should I delete the ~/anaconda directory? Any other changes required? Currently when I run which python I get this path:/Users/username/anaconda/bin/pythonFrom the docs:Further notes:The anaconda installer adds a line in your ~/.bash_profile script that prepends the anaconda bin directory to your $PATH environment variable. Deleting the anaconda directory should be all you need to do, but it's good housekeeping to remove this line from your setup script too.Package "anaconda clean", available from Anaconda platform, should uninstall safely.Refer: https://docs.anaconda.com/anaconda/install/uninstall for more details.Removing the Anaconda directory helps, but I don't think that's a good idea as you might need to use anaconda sometimes in near future. So, as suggested by mwaskom, anaconda installer automatically adds PATH variable which points to anaconda/bin directory in the ~/.bashrc file.It looks like thisSo, just comment out the line (add # in the beginning of the line).

Then reload the ~/.bashrc file executing source ~/.bashrcNow, verify the changes executing which python in the new terminal.It was pretty easy. It switched my pointer to Python:

https://docs.continuum.io/anaconda/install#os-x-uninstallIf you're uninstalling Anaconda to be able to use the base Python installation in the system, you could temporarily disable the path by following these steps and not uninstalling Anaconda.Go to your home directory. Just a cd command will do.Edit the file .bashrc.Look for something like export PATH="/home/ubuntu/anaconda3/bin:$PATH" in the file.Put a # at the beginning to comment it from the script.Open a new terminal and you should be running the base python installation. This works on Linux systems. Should work on Mac too.To uninstall Anaconda, you can do a simple remove of the program. This will leave a few files behind, which for most users is just fine. See Option A.If you also want to remove all traces of the configuration files and directories from Anaconda and its programs, you can download and use the Anaconda-Clean program first, then do a simple remove. See Option B.Use simple remove to uninstall Anaconda:macOS–Open the Terminal.app or iTerm2 terminal application, and then remove your entire Anaconda directory, which has a name such as anaconda2 or anaconda3, by entering rm -rf ~/anaconda3.Full uninstall using Anaconda-Clean and simple remove.NOTE: Anaconda-Clean must be run before simple remove.Install the Anaconda-Clean package from Anaconda Prompt or a terminal window:In the same window, run one of these commands:Remove all Anaconda-related files and directories with a confirmation prompt before deleting each one:Or, remove all Anaconda-related files and directories without being prompted to delete each one:Anaconda-Clean creates a backup of all files and directories that might be removed, such as .bash_profile, in a folder named .anaconda_backup in your home directory. Also note that Anaconda-Clean leaves your data files in the AnacondaProjects directory untouched.

After using Anaconda-Clean, follow the instructions above in Option A to uninstall Anaconda.

Removing Anaconda path from .bash_profileIf you use Linux or macOS, you may also wish to check the .bash_profilefile in your home directory for a line such as:NOTE: Replace /Users/jsmith/anaconda3/ with your actual path.This line adds the Anaconda path to the PATH environment variable. It may refer to either Anaconda or Miniconda. After uninstalling Anaconda, you may delete this line and save the file.by official uninstalling wayWhen you’re done editing the file, type Ctrl+X to exit and y to save changes.Anaconda is now removed from your server.In case you have multiple version of anaconda,rm -rf ~/anaconda2 [for version 2]rm -rf ~/anaconda3 [for version 3]Open .bashrc file in a text editorvim .bashrcremove anaconda directory from your PATH.export PATH="/home/{username}/anaconda2/bin:$PATH" [for version 2]export PATH="/home/{username}/anaconda3/bin:$PATH" [for version 3]I simply: ...this removed conda also.Then: ...and removed the path line added at the very bottom (clearly identified by Anaconda as 'added by Anaconda'.Worth noting that anaconda3 created a backup of my .bashrc file before modification, and named it as:...so I could always have just renamed this and deleted my modified .bashrcTo uninstall anaconda you have to:1) Remove the entire anaconda install directory with:rm -rf ~/anaconda22) And (OPTIONAL): ->Edit ~/.bash_profile to remove the anaconda directory from your PATH environment variable.->Remove the following hidden file and folders that may have been created in the home directory:rm -rf ~/.condarc ~/.conda ~/.continuumsourceTo uninstall Anaconda Fully from your System :It was enoughIn my case Anaconda3 was not installed in home directory. Instead, it was installed in root. Therefore, I had to do the following to get it uninstalled: I always try to follow the developers advice, since they are usually the ones that now how it would affect your system. Theoretically this should be the safest way:Install the Anaconda-Clean package from Anaconda Prompt (terminal on Linux or macOS):In the same window, run one of these commands:Remove all Anaconda-related files and directories with a confirmation prompt before deleting each one:Or, remove all Anaconda-related files and directories without being prompted to delete each one:Anaconda-Clean creates a backup of all files and directories that might be removed in a folder named .anaconda_backup in your home directory. Also note that Anaconda-Clean leaves your data files in the AnacondaProjects directory untouched.https://docs.anaconda.com/anaconda/install/uninstall/For windowsHope, it helps.

correct way to define class variables in Python [duplicate]

jeanc

[correct way to define class variables in Python [duplicate]](https://stackoverflow.com/questions/9056957/correct-way-to-define-class-variables-in-python)

I noticed that in Python, people initialize their class attributes in two different ways.The first way is like this:The other style looks like:Which is the correct way to initialize class attributes?

2012-01-29 21:26:39Z

I noticed that in Python, people initialize their class attributes in two different ways.The first way is like this:The other style looks like:Which is the correct way to initialize class attributes?Neither way is necessarily correct or incorrect, they are just two different kinds of class elements:You'll see it more clearly with some code:As you can see, when we changed the class element, it changed for both objects. But, when we changed the object element, the other object remained unchanged.I think this sample explains the difference between the styles:element1 is bound to the class, element2 is bound to an instance of the class.

Why compile Python code?

ryeguy

[Why compile Python code?](https://stackoverflow.com/questions/471191/why-compile-python-code)

Why would you compile a Python script? You can run them directly from the .py file and it works fine, so is there a performance advantage or something? I also notice that some files in my application get compiled into .pyc while others do not, why is this?

2009-01-22 22:57:34Z

Why would you compile a Python script? You can run them directly from the .py file and it works fine, so is there a performance advantage or something? I also notice that some files in my application get compiled into .pyc while others do not, why is this?It's compiled to bytecode which can be used much, much, much faster.The reason some files aren't compiled is that the main script, which you invoke with python main.py is recompiled every time you run the script. All imported scripts will be compiled and stored on the disk.Important addition by Ben Blank:The .pyc file is Python that has already been compiled to byte-code.  Python automatically runs a .pyc file if it finds one with the same name as a .py file you invoke."An Introduction to Python" says this about compiled Python files:The advantage of running a .pyc file is that Python doesn't have to incur the overhead of compiling it before running it.  Since Python would compile to byte-code before running a .py file anyway, there shouldn't be any performance improvement aside from that.How much improvement can you get from using compiled .pyc files?  That depends on what the script does.  For a very brief script that simply prints "Hello World," compiling could constitute a large percentage of the total startup-and-run time.  But the cost of compiling a script relative to the total run time diminishes for longer-running scripts.The script you name on the command-line is never saved to a .pyc file. Only modules loaded by that "main" script are saved in that way.Pluses:First: mild, defeatable obfuscation.Second: if compilation results in a significantly smaller file, you will get faster load times. Nice for the web.Third: Python can skip the compilation step. Faster at intial load. Nice for the CPU and the web.Fourth: the more you comment, the smaller the .pyc or .pyo file will be in comparison to the source .py file.Fifth: an end user with only a .pyc or .pyo file in hand is much less likely to present you with a bug they caused by an un-reverted change they forgot to tell you about.Sixth: if you're aiming at an embedded system, obtaining a smaller size

file to embed may represent a significant plus, and the architecture is stable so drawback one, detailed below, does not come into play.Top level compilationIt is useful to know that you can compile a top level python source file into a .pyc file this way:This removes comments. It leaves docstrings intact. If you'd like to get rid of the docstrings as well (you might want to seriously think about why you're doing that) then compile this way instead......and you'll get a .pyo file instead of a .pyc file; equally distributable in terms of the code's essential functionality, but smaller by the size of the stripped-out docstrings (and less easily understood for subsequent employment if it had decent docstrings in the first place). But see drawback three, below.Note that python uses the .py file's date, if it is present, to decide whether it should execute the .py file as opposed to the .pyc or .pyo file --- so edit your .py file, and the .pyc or .pyo is obsolete and whatever benefits you gained are lost. You need to recompile it in order to get the .pyc or .pyo benefits back again again, such as they may be.Drawbacks:First: There's a "magic cookie" in .pyc and .pyo files that indicates the system architecture that the python file was compiled in. If you distribute one of these files into an environment of a different type, it will break. If you distribute the .pyc or .pyo without the associated .py to recompile or touch so it supersedes the .pyc or .pyo, the end user can't fix it, either.Second: If docstrings are skipped with the use of the -OO command line option as described above, no one will be able to get at that information, which can make use of the code more difficult (or impossible.)Third: Python's -OO option also implements some optimizations as per the -O command line option; this may result in changes in operation. Known optimizations are:Fourth: if you had intentionally made your python script executable with something on the order of #!/usr/bin/python on the first line, this is stripped out in .pyc and .pyo files and that functionality is lost.Fifth: somewhat obvious, but if you compile your code, not only can its use be impacted, but the potential for others to learn from your work is reduced, often severely.There is a performance increase in running compiled python. However when you run a .py file as an imported module, python will compile and store it, and as long as the .py file does not change it will always use the compiled version.With any interpeted language when the file is used the process looks something like this:

1. File is processed by the interpeter.

2. File is compiled

3. Compiled code is executed.obviously by using pre-compiled code you can eliminate step 2, this applies python, PHP and others.Heres an interesting blog post explaining the differences http://julipedia.blogspot.com/2004/07/compiled-vs-interpreted-languages.html

And here's an entry that explains the Python compile process http://effbot.org/zone/python-compile.htmAs already mentioned, you can get a performance increase from having your python code compiled into bytecode. This is usually handled by python itself, for imported scripts only.Another reason you might want to compile your python code, could be to protect your intellectual property from being copied and/or modified.You can read more about this in the Python documentation.There's certainly a performance difference when running a compiled script. If you run normal .py scripts, the machine compiles it every time it is run and this takes time. On modern machines this is hardly noticeable but as the script grows it may become more of an issue.Something not touched upon is source-to-source-compiling. For example, nuitka translates Python code to C/C++, and compiles it to binary code which directly runs on the CPU, instead of Python bytecode which runs on the slower virtual machine.This can lead to significant speedups, or it would let you work with Python while your environment depends on C/C++ code.We use compiled code to distribute to users who do not have access to the source code. Basically to stop inexperienced programers accidentally changing something or fixing bugs without telling us.Yep, performance is the main reason and, as far as I know, the only reason.If some of your files aren't getting compiled, maybe Python isn't able to write to the .pyc file, perhaps because of the directory permissions or something.  Or perhaps the uncompiled files just aren't ever getting loaded... (scripts/modules only get compiled when they first get loaded)Beginners assume Python is compiled because of .pyc files. The .pyc file is the compiled bytecode, which is then interpreted. So if you've run your Python code before and have the .pyc file handy, it will run faster the second time, as it doesn't have to re-compile the bytecodecompiler:

    A compiler is a piece of code that translates the high level language into machine languageInterpreters:

Interpreters also convert the high level language into machine readable binary equivalents. Each time when an interpreter gets a high level language code to be executed, it converts the code into an intermediate code before converting it into the machine code. Each part of the code is interpreted and then execute separately in a sequence and an error is found in a part of the code it will stop the interpretation of the code without translating the next set of the codes.  Sources:

http://www.toptal.com/python/why-are-there-so-many-pythons

http://www.engineersgarage.com/contribution/difference-between-compiler-and-interpreter

What is the global interpreter lock (GIL) in CPython?

e-satis

[What is the global interpreter lock (GIL) in CPython?](https://stackoverflow.com/questions/1294382/what-is-the-global-interpreter-lock-gil-in-cpython)

What is a global interpreter lock and why is it an issue?A lot of noise has been made around removing the GIL from Python, and I'd like to understand why that is so important. I have never written a compiler nor an interpreter myself, so don't be frugal with details, I'll probably need them to understand.

2009-08-18 14:50:06Z

What is a global interpreter lock and why is it an issue?A lot of noise has been made around removing the GIL from Python, and I'd like to understand why that is so important. I have never written a compiler nor an interpreter myself, so don't be frugal with details, I'll probably need them to understand.Python's GIL is intended to serialize access to interpreter internals from different threads. On multi-core systems, it means that multiple threads can't effectively make use of multiple cores. (If the GIL didn't lead to this problem, most people wouldn't care about the GIL - it's only being raised as an issue because of the increasing prevalence of multi-core systems.) If you want to understand it in detail, you can view this video or look at this set of slides. It might be too much information, but then you did ask for details :-)Note that Python's GIL is only really an issue for CPython, the reference implementation. Jython and IronPython don't have a GIL. As a Python developer, you don't generally come across the GIL unless you're writing a C extension. C extension writers need to release the GIL when their extensions do blocking I/O, so that other threads in the Python process get a chance to run.Suppose you have multiple threads which don't really touch each other's data. Those should execute as independently as possible. If you have a "global lock" which you need to acquire in order to (say) call a function, that can end up as a bottleneck. You can wind up not getting much benefit from having multiple threads in the first place.To put it into a real world analogy: imagine 100 developers working at a company with only a single coffee mug. Most of the developers would spend their time waiting for coffee instead of coding.None of this is Python-specific - I don't know the details of what Python needed a GIL for in the first place. However, hopefully it's given you a better idea of the general concept.Let's first understand what the python GIL provides:Any operation/instruction is executed in the interpreter. GIL ensures that interpreter is held by a single thread at a particular instant of time. And your python program with multiple threads works in a single interpreter. At any particular instant of time, this interpreter is held by a single thread. It means that only the thread which is holding the interpreter is running at any instant of time.Now why is that an issue:Your machine could be having multiple cores/processors. And multiple cores allow multiple threads to execute simultaneously i.e multiple threads could execute at any particular instant of time..

But since the interpreter is held by a single thread, other threads are not doing anything even though they have access to a core. So, you are not getting any advantage provided by multiple cores because at any instant only a single core, which is the core being used by the thread currently holding the interpreter, is being used. So, your program will take as long to execute as if it were a single threaded program.However, potentially blocking or long-running operations, such as I/O, image processing, and NumPy number crunching, happen outside the GIL. Taken from here. So for such operations, a multithreaded operation will still be faster than a single threaded operation despite the presence of GIL. So, GIL is not always a bottleneck.Edit: GIL is an implementation detail of CPython. IronPython and Jython don't have GIL, so a truly multithreaded program should be possible in them, thought I have never used PyPy and Jython and not sure of this.Python doesn't allow multi-threading in the truest sense of the word. It has a multi-threading package but if you want to multi-thread to speed your code up, then it's usually not a good idea to use it. Python has a construct called the Global Interpreter Lock (GIL). https://www.youtube.com/watch?v=ph374fJqFPEThe GIL makes sure that only one of your 'threads' can execute at any one time. A thread acquires the GIL, does a little work, then passes the GIL onto the next thread. This happens very quickly so to the human eye it may seem like your threads are executing in parallel, but they are really just taking turns using the same CPU core. All this GIL passing adds overhead to execution. This means that if you want to make your code run faster then using the threading package often isn't a good idea. There are reasons to use Python's threading package. If you want to run some things simultaneously, and efficiency is not a concern, then it's totally fine and convenient. Or if you are running code that needs to wait for something (like some IO) then it could make a lot of sense. But the threading library wont let you use extra CPU cores.Multi-threading can be outsourced to the operating system (by doing multi-processing), some external application that calls your Python code (eg, Spark or Hadoop), or some code that your Python code calls (eg: you could have your Python code call a C function that does the expensive multi-threaded stuff).Whenever two threads have access to the same variable you have a problem.

In C++ for instance, the way to avoid the problem is to define some mutex lock to prevent two thread to, let's say, enter the setter of an object at the same time.Multithreading is possible in python, but two threads cannot be executed at the same time 

at a granularity finer than one python instruction.

The running thread is getting a global lock called GIL.This means if you begin write some multithreaded code in order to take advantage of your multicore processor, your performance won't improve.

The usual workaround consists of going multiprocess.Note that it is possible to release the GIL if you're inside a method you wrote in C for instance.The use of a GIL is not inherent to Python but to some of its interpreter, including the most common CPython. 

(#edited, see comment)The GIL issue is still valid in Python 3000.Python 3.7 documentationI would also like to highlight the following quote from the Python threading documentation:This links to the Glossary entry for global interpreter lock which explains that the GIL implies that threaded parallelism in Python is unsuitable for CPU bound tasks:This quote also implies that dicts and thus variable assignment are also thread safe as a CPython implementation detail:Next, the docs for the multiprocessing package explain how it overcomes the GIL by spawning process while exposing an interface similar to that of threading:And the docs for concurrent.futures.ProcessPoolExecutor explain that it uses multiprocessing as a backend:which should be contrasted to the other base class ThreadPoolExecutor that uses threads instead of processesfrom which we conclude that ThreadPoolExecutor is only suitable for I/O bound tasks, while ProcessPoolExecutor can also handle CPU bound tasks.The following question asks why the GIL exists in the first place: Why the Global Interpreter Lock?Process vs thread experimentsAt Multiprocessing vs Threading Python I've done an experimental analysis of process vs threads in Python.Quick preview of the results:Why Python (CPython and others) uses the GILFrom http://wiki.python.org/moin/GlobalInterpreterLockIn CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython's memory management is not thread-safe.How to remove it from Python?Like Lua, maybe Python could start multiple VM, But python doesn't do that, I guess there should be some other reasons.In Numpy or some other python extended library, sometimes, releasing the GIL to other threads could boost the efficiency of the whole programme.I want to share an example from the book multithreading for Visual Effects. So here is a classic dead lock situationNow consider the events in the sequence resulting a dead-lock. 

What's the best way to parse command line arguments? [closed]

kamens

[What's the best way to parse command line arguments? [closed]](https://stackoverflow.com/questions/20063/whats-the-best-way-to-parse-command-line-arguments)

What's the easiest, tersest, and most flexible method or library for parsing Python command line arguments?

2008-08-21 14:24:41Z

What's the easiest, tersest, and most flexible method or library for parsing Python command line arguments?This answer suggests optparse which is appropriate for older Python versions. For Python 2.7 and above, argparse replaces optparse. See this answer for more information.As other people pointed out, you are better off going with optparse over getopt.  getopt is pretty much a one-to-one mapping of the standard getopt(3) C library functions, and not very easy to use.optparse, while being a bit more verbose, is much better structured and simpler to extend later on.Here's a typical line to add an option to your parser:It pretty much speaks for itself; at processing time, it will accept -q or --query as options, store the argument in an attribute called query and has a default value if you don't specify it.  It is also self-documenting in that you declare the help argument (which will be used when run with -h/--help) right there with the option.Usually you parse your arguments with:This will, by default, parse the standard arguments passed to the script (sys.argv[1:])options.query will then be set to the value you passed to the script.You create a parser simply by doingThese are all the basics you need.  Here's a complete Python script that shows this:5 lines of python that show you the basics.Save it in sample.py, and run it once withand once withBeyond that, you will find that optparse is very easy to extend.

In one of my projects, I created a Command class which allows you to nest subcommands in a command tree easily.  It uses optparse heavily to chain commands together.  It's not something I can easily explain in a few lines, but feel free to browse around in my repository for the main class, as well as a class that uses it and the option parserargparse is the way to go. Here is a short summary of how to use it:1) Initialize2) Add Arguments3) Parse4) Access5) Check ValuesCorrect use:Incorrect arguments:Full help:Since 2012 there is a very easy, powerful and really cool module for argument parsing called docopt. Here is an example taken from its documentation: So this is it: 2 lines of code plus your doc string which is essential and you get your arguments parsed and available in your arguments object.Since 2017 there's another cool module called python-fire. It can generate a CLI interface for your code with you doing zero argument parsing. Here's a simple example from the documentation (this small program exposes the function double to the command line):From the command line, you can run:The new hip way is argparse for these reasons.  argparse > optparse > getoptupdate: As of py2.7 argparse is part of the standard library and optparse is deprecated.I prefer Click. It abstracts managing options and allows "(...) creating beautiful command line interfaces in a composable way with as little code as necessary".Here's example usage:It also automatically generates nicely formatted help pages:Pretty much everybody is using getoptHere is the example code for the doc :So in a word, here is how it works.You've got two types of options. Those who are receiving arguments, and those who are

just like switches.sys.argv is pretty much your char** argv in C. Like in C you skip the first element which is the name of your program and parse only the arguments : sys.argv[1:]Getopt.getopt will parse it according to the rule you give in argument."ho:v" here describes the short arguments : -ONELETTER. The : means that -o accepts one argument.Finally ["help", "output="] describes long arguments ( --MORETHANONELETTER ).

The = after output once again means that output accepts one arguments.The result is a list of couple (option,argument)If an option doesn't accept any argument (like --help here) the arg part is an empty string.

You then usually want to loop on this list and test the option name as in the example.I hope this helped you.Use optparse which comes with the standard library. For example:Source: Using Python to create UNIX command line toolsHowever as of Python 2.7 optparse is deprecated, see: Why use argparse rather than optparse?Just in case you might need to, this may help if you need to grab unicode arguments on Win32 (2K, XP etc):Lightweight command line argument defaultsAlthough argparse is great and is the right answer for fully documented command line switches and advanced features, you can use function argument defaults to handles straightforward positional arguments very simply.The 'name' argument captures the script name and is not used. Test output looks like this:For simple scripts where I just want some default values, I find this quite sufficient. You might also want to include some type coercion in the return values or command line values will all be strings.I prefer optparse to getopt. It's very declarative: you tell it the names of the options and the effects they should have (e.g., setting a boolean field), and it hands you back a dictionary populated according to your specifications.http://docs.python.org/lib/module-optparse.htmlI think the best way for larger projects is optparse, but if you are looking for an easy way, maybe http://werkzeug.pocoo.org/documentation/script is something for you.So basically every function action_* is exposed to the command line and a nice

help message is generated for free. Argparse code can be longer than actual implementation code!That's a problem I find with most popular argument parsing options is that if your parameters are only modest, the code to document them becomes disproportionately large to the benefit they provide.  A relative new-comer to the argument parsing scene (I think) is plac.It makes some acknowledged trade-offs with argparse, but uses inline documentation and wraps simply around main() type function function:consoleargs deserves to be mentioned here. It is very easy to use. Check it out:Now in console:Here's a method, not a library, which seems to work for me.The goals here are to be terse, each argument parsed by a single line, the args line up for readability, the code is simple and doesn't depend on any special modules (only os + sys), warns about missing or unknown arguments gracefully, use a simple for/range() loop, and works across python 2.x and 3.xShown are two toggle flags (-d, -v), and two values controlled by arguments (-i xxx and -o xxx).The goal of NextArg() is to return the next argument while checking for missing data, and 'skip' skips the loop when NextArg() is used, keeping the flag parsing down to one liners.I extended Erco's approach to allow for required positional arguments and for optional arguments. These should precede the -d, -v etc. arguments.Positional and optional arguments can be retrieved with PosArg(i) and OptArg(i, default) respectively.

When an optional argument is found the start position of searching for options (e.g. -i) is moved 1 ahead to avoid causing an 'unexpected' fatal.

Automatically create requirements.txt

Igor Barinov

[Automatically create requirements.txt](https://stackoverflow.com/questions/31684375/automatically-create-requirements-txt)

Sometimes I download the python source code from github and don't know how to install all the dependencies. If there is no requirements.txt file I have to create it by hands. 

The question is:

Given the python source code directory is it possible to create requirements.txt automatically from the import section?

2015-07-28 18:29:03Z

Sometimes I download the python source code from github and don't know how to install all the dependencies. If there is no requirements.txt file I have to create it by hands. 

The question is:

Given the python source code directory is it possible to create requirements.txt automatically from the import section?If you use virtual environment, pip freeze > requirements.txt just fine. IF NOT, pigar will be a good choice for you.By the way, I do not ensure it will work with 2.6.UPDATE:Pipenv or other tools is recommended for improving your development flow.For Python 3 use belowYou can use the following code to generate a requirements.txt file:more info related to pipreqs can be found here.Sometimes you come across pip freeze, but this saves all packages in the environment including those that you don't use in your current project. In my case, I use Anaconda, so running the following command from conda terminal inside my environment solved it, and created this requirements txt file for me automatically:This was taken from this Github link pratos/condaenv.txt If an error been seen, and you are using anaconda, try to use the .yml option:For other person to use the environment...Or if you are creating a new enviroment on other machine:

conda env create -f .yml.yml option been found hereMake sure to run pip3 for python3.7.Before executing the above command make sure you have created a virtual environment. python3:python2: After that put your source code in the directory. If you run the python file now, probably It won't launch If you are using non-native modules. You can install those modules runing This will not affect you entire module list except the environment you are In.Now you can execute the command at the top and now you have a requirements file which contains only the modules you installed in the virtual environment. Now you can run the command at the top.I advise everyone to use environments as It makes things easier when It comes to stuff like this.Hope this helped.If Facing the same issue as mine i.e. not on the virtual environment and wants requirements.txt for a specific project or from the selected folder(includes children) and pipreqs is not supporting. You can use : P.S: It may have a few additional libraries as it checks on fuzzylogic.

How to delete an item in a list if it exists?

Zeynel

[How to delete an item in a list if it exists?](https://stackoverflow.com/questions/4915920/how-to-delete-an-item-in-a-list-if-it-exists)

I am getting new_tag from a form text field with self.response.get("new_tag") and selected_tags from checkbox fields with I combine them like this:(f1.striplist is a function that strips white spaces inside the strings in the list.)But in the case that tag_list is empty (no new tags are entered) but there are some selected_tags, new_tag_list contains an empty string " ".For example, from logging.info:How do I get rid of the empty string?If there is an empty string in the list:But if there is no empty string:But this gives:Why does this happen, and how do I work around it?

2011-02-06 20:34:51Z

I am getting new_tag from a form text field with self.response.get("new_tag") and selected_tags from checkbox fields with I combine them like this:(f1.striplist is a function that strips white spaces inside the strings in the list.)But in the case that tag_list is empty (no new tags are entered) but there are some selected_tags, new_tag_list contains an empty string " ".For example, from logging.info:How do I get rid of the empty string?If there is an empty string in the list:But if there is no empty string:But this gives:Why does this happen, and how do I work around it?Test for presence using the in operator, then apply the remove method.The removemethod will remove only the first occurrence of thing, in order to remove all occurrences you can use while instead of if.This shoot-first-ask-questions-last attitude is common in Python. Instead of testing in advance if the object is suitable, just carry out the operation and catch relevant Exceptions:Off course the second except clause in the example above is not only of questionable humor but totally unnecessary (the point was to illustrate duck-typing for people not familiar with the concept).If you expect multiple occurrences of thing:However, with contextlib's suppress() contextmanager (introduced in python 3.4) the above code can be simplified to this:Again, if you expect multiple occurrences of thing:Around 1993, Python got lambda, reduce(), filter() and map(), courtesy of a Lisp hacker who missed them and submitted working patches*. You can use filter to remove elements from the list:There is a shortcut that may be useful for your case: if you want to filter out empty items (in fact items where bool(item) == False, like None, zero, empty strings or other empty collections), you can pass None as the first argument:List comprehensions became the preferred style for list manipulation in Python since introduced in version 2.0 by PEP 202. The rationale behind it is that List comprehensions provide a more concise way to create lists in situations where map() and filter() and/or nested loops would currently be used.Generator expressions were introduced in version 2.4 by PEP 289. A generator expression is better for situations where you don't really need (or want) to have a full list created in memory - like when you just want to iterate over the elements one at a time. If you are only iterating over the list, you can think of a generator expression as a lazy evaluated list comprehension:Note that this will only remove one instance of the empty string from your list (as your code would have, too). Can your list contain more than one?If index doesn't find the searched string, it throws the ValueError you're seeing. Either 

catch the ValueError:or use find, which returns -1 in that case.Adding this answer for completeness, though it's only usable under certain conditions.If you have very large lists, removing from the end of the list avoids CPython internals having to memmove, for situations where you can re-order the list. It gives a performance gain to remove from the end of the list, since it won't need to memmove every item after the one your removing - back one step (1).

For one-off removals the performance difference may be acceptable, but if you have a large list and need to remove many items - you will likely notice a performance hit.Although admittedly, in these cases, doing a full list search is likely to be a performance bottleneck too, unless items are mostly at the front of the list.This method can be used for more efficient removal,as long as re-ordering the list is acceptable. (2)You may want to avoid raising an error when the item isn't in the list.Eek, don't do anything that complicated : )Just filter() your tags.  bool() returns False for empty strings, so instead ofyou should writeor better yet, put this logic inside striplist() so that it doesn't return empty strings in the first place.Here's another one-liner approach to throw out there:It doesn't create a list copy, doesn't make multiple passes through the list, doesn't require additional exception handling, and returns the matched object or None if there isn't a match. Only issue is that it makes for a long statement.In general, when looking for a one-liner solution that doesn't throw exceptions, next() is the way to go, since it's one of the few Python functions that supports a default argument.All you have to do is thisbut that method has an issue. You have to put something in the except place

so i found this:

In Python, if I return inside a「with」block, will the file still close?

Lightbreeze

[In Python, if I return inside a「with」block, will the file still close?](https://stackoverflow.com/questions/9885217/in-python-if-i-return-inside-a-with-block-will-the-file-still-close)

Consider the following:Will the file be closed properly, or does using return somehow bypass the context manager?

2012-03-27 07:37:30Z

Consider the following:Will the file be closed properly, or does using return somehow bypass the context manager?Yes, it acts like the finally block after a try block, i.e. it always executes (unless the python process terminates in an unusual way of course).It is also mentioned in one of the examples of PEP-343 which is the specification for the with statement:Something worth mentioning is however, that you cannot easily catch exceptions thrown by the open() call without putting the whole with block inside a try..except block which is usually not what one wants.Yes...is pretty much equivalent to:More accurately, the __exit__ method in a context manager is always called when exiting the block (regardless of exceptions, returns etc). The file object's __exit__ method just calls f.close() (e.g here in CPython)Yes. More generally, the __exit__ method of a With Statement Context Manager will indeed be called in the event of a return from inside the context. This can be tested with the following:The output is:The output above confirms that __exit__ was called despite the early return. As such, the context manager is not bypassed.Yes, but there may be some side effect in other cases, because it may should do something (like flushing buffer) in __exit__ block

Adding a legend to PyPlot in Matplotlib in the simplest manner possible

Games Brainiac

[Adding a legend to PyPlot in Matplotlib in the simplest manner possible](https://stackoverflow.com/questions/19125722/adding-a-legend-to-pyplot-in-matplotlib-in-the-simplest-manner-possible)

Please consider the graphing script below:As you can see, this is a very basic use of matplotlib's PyPlot. This ideally generates a graph like the one below:Nothing special, I know. However, it is unclear what data is being plotted where (I'm trying to plot the data of some sorting algorithms, length against time taken, and I'd like to make sure people know which line is which). Thus, I need a legend, however, taking a look at the following example below(from the official site):You will see that I need to create an extra variable ax. How can I add a legend to my graph without having to create this extra variable and retaining the simplicity of my current script?

2013-10-01 20:53:05Z

Please consider the graphing script below:As you can see, this is a very basic use of matplotlib's PyPlot. This ideally generates a graph like the one below:Nothing special, I know. However, it is unclear what data is being plotted where (I'm trying to plot the data of some sorting algorithms, length against time taken, and I'd like to make sure people know which line is which). Thus, I need a legend, however, taking a look at the following example below(from the official site):You will see that I need to create an extra variable ax. How can I add a legend to my graph without having to create this extra variable and retaining the simplicity of my current script?Add a label= to each of your plot() calls, and then call legend(loc='upper left').Consider this sample (tested with Python 3.8.0):

Slightly modified from this tutorial: http://jakevdp.github.io/mpl_tutorial/tutorial_pages/tut1.htmlYou can access the Axes instance (ax) with plt.gca().  In this case, you can useYou can do this either by using the label= keyword in each of your plt.plot() calls or by assigning your labels as a tuple or list within legend, as in this working example:However, if you need to access the Axes instance more that once, I do recommend saving it to the variable ax withand then calling ax instead of plt.gca().Here's an example to help you out ...A simple plot for sine and cosine curves with a legend.Used matplotlib.pyplotAdd labels to each argument in your plot call corresponding to the series it is graphing, i.e. label = "series 1"Then simply add Pyplot.legend() to the bottom of your script and the legend will display these labels.You can add a custom legend documentation

Remove rows with duplicate indices (Pandas DataFrame and TimeSeries)

Paul H

[Remove rows with duplicate indices (Pandas DataFrame and TimeSeries)](https://stackoverflow.com/questions/13035764/remove-rows-with-duplicate-indices-pandas-dataframe-and-timeseries)

I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame looks something like this:The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file. Simple example of such a case is illustrated below:And so I need df3 to evenutally become:I thought that adding a column of row numbers (df3['rownum'] = range(df3.shape[0])) would help me select out the bottom-most row for any value of the DatetimeIndex, but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work.

2012-10-23 17:11:04Z

I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame looks something like this:The problem I'm having is that sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file. Simple example of such a case is illustrated below:And so I need df3 to evenutally become:I thought that adding a column of row numbers (df3['rownum'] = range(df3.shape[0])) would help me select out the bottom-most row for any value of the DatetimeIndex, but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work.I would suggest using the duplicated method on the Pandas Index itself:While all the other methods work, the currently accepted answer is by far the least performant for the provided example. Furthermore, while the groupby method is only slightly less performant, I find the duplicated method to be more readable.Using the sample data provided:Note that you can keep the last element by changing the keep argument.It should also be noted that this method works with MultiIndex as well (using df1 as specified in Paul's example):A simple solution is to use drop_duplicatesFor me, this operated quickly on large data sets.This requires that 'rownum' be the column with duplicates.  In the modified example, 'rownum' has no duplicates, therefore nothing gets eliminated.  What we really want is to have the 'cols' be set to the index.  I've not found a way to tell drop_duplicates to only consider the index.Here is a solution that adds the index as a dataframe column, drops duplicates on that, then removes the new column:And if you want things back in the proper order, just call sort on the dataframe.Oh my. This is actually so simple!Follow up edit 2013-10-29

In the case where I have a fairly complex MultiIndex, I think I prefer the groupby approach. Here's simple example for posterity:and here's the important partUnfortunately, I don't think Pandas allows one to drop dups off the indices. I would suggest the following:Remove duplicates (Keeping First)Remove duplicates (Keeping Last)Tests: 10k loops using OP's dataIf anyone like me likes chainable data manipulation using the pandas dot notation (like piping), then the following may be useful: This enables chaining statements like this:

How to retry after exception?

FurtiveFelon

[How to retry after exception?](https://stackoverflow.com/questions/2083987/how-to-retry-after-exception)

I have a loop starting with for i in range(0, 100). Normally it runs correctly, but sometimes it fails due to network conditions. Currently I have it set so that on failure, it will continue in the except clause (continue on to the next number for i).Is it possible for me to reassign the same number to i and run through the failed iteration of the loop again?

2010-01-18 05:00:19Z

I have a loop starting with for i in range(0, 100). Normally it runs correctly, but sometimes it fails due to network conditions. Currently I have it set so that on failure, it will continue in the except clause (continue on to the next number for i).Is it possible for me to reassign the same number to i and run through the failed iteration of the loop again?Do a while True inside your for loop, put your try code inside, and break from that while loop only when your code succeeds.I prefer to limit the number of retries, so that if there's a problem with that specific item you will eventually continue onto the next one, thus:The retrying package is a nice way to retry a block of code on failure.For example:Here is a solution similar to others, but it will raise the exception if it doesn't succeed in the prescribed number or retries.The more "functional" approach without using those ugly while loops:The clearest way would be to explicitly set i. For example:A generic solution with a timeout:Usage:There is something similar in the Python Decorator Library.Please bear in mind that it does not test for exceptions, but the return value. It retries until the decorated function returns True.A slightly modified version should do the trick.Using recursionMy version is similar to several of the above, but doesn't use a separate while loop, and re-raises the latest exception if all retries fail. Could explicitly set err = None at the top, but not strictly necessary as it should only execute the final else block if there was an error and therefore err is set.Using while and a counter:You can use Python retrying package.

RetryingIt is written in Python to simplify the task of adding retry behavior to just about anything.If you want a solution without nested loops and invoking break on success you could developer a quick wrap retriable for any iterable. Here's an example of a networking issue that I run into often - saved authentication expires. The use of it would read like this: I use following in my codes,Here's my idea on how to fix this:i recently worked with my python on a solution to this problem and i am happy to share it with stackoverflow visitors please give feedback if it is needed.increment your loop variable only when the try clause succeeds

matplotlib Legend Markers Only Once

carl

[matplotlib Legend Markers Only Once](https://stackoverflow.com/questions/6146778/matplotlib-legend-markers-only-once)

I often plot a point on a matplotlib plot with:However, this causes the legend to put a star in the legend twice, such that it looks like:when I really want it to look like:How do I do this?

2011-05-27 01:08:06Z

I often plot a point on a matplotlib plot with:However, this causes the legend to put a star in the legend twice, such that it looks like:when I really want it to look like:How do I do this?This should work:BTW, if you add the lineto your matplotlibrc file, then this will be the new default.[See also scatterpoints, depending on your plot.]API: Link to API docsI like to change my matplotlib rc parameters dynamically in every python script. To achieve this goal I simply use somthing like that at the beginning of my python files.This will apply to all plots generated from my python file.EDIT: For those who do not like to import pylab, the long answer is

TypeError: method() takes 1 positional argument but 2 were given

Zero Piraeus

[TypeError: method() takes 1 positional argument but 2 were given](https://stackoverflow.com/questions/23944657/typeerror-method-takes-1-positional-argument-but-2-were-given)

If I have a class......which I use to create an object......on which I call method("foo") like so......why does Python tell me I gave it two arguments, when I only gave one?

2014-05-29 23:27:32Z

If I have a class......which I use to create an object......on which I call method("foo") like so......why does Python tell me I gave it two arguments, when I only gave one?In Python, this:...is syntactic sugar, which the interpreter translates behind the scenes into:...which, as you can see, does indeed have two arguments - it's just that the first one is implicit, from the point of view of the caller.This is because most methods do some work with the object they're called on, so there needs to be some way for that object to be referred to inside the method. By convention, this first argument is called self inside the method definition:If you call method("foo") on an instance of MyNewClass, it works as expected:Occasionally (but not often), you really don't care about the object that your method is bound to, and in that circumstance, you can decorate the method with the builtin staticmethod() function to say so:...in which case you don't need to add a self argument to the method definition, and it still works:Something else to consider when this type of error is encountered:I was running into this error message and found this post helpful. Turns out in my case I had overridden an __init__() where there was object inheritance.The inherited example is rather long, so I'll skip to a more simple example that doesn't use inheritance:Result is:PyCharm didn't catch this typo. Nor did Notepad++ (other editors/IDE's might).Granted, this is a "takes no parameters" TypeError, it isn't much different than "got two" when expecting one, in terms of object initialization in Python.Addressing the topic: An overloading initializer will be used if syntactically correct, but if not it will be ignored and the built-in used instead. The object won't expect/handle this and the error is thrown.In the case of the sytax error: The fix is simple, just edit the custom init statement:In simple words.In Python you should add self argument as the first argument to all defined methods in classes:Then you can use your method according to your intuition:This should solve your problem :)For a better understanding, you can also read the answers to this question: What is the purpose of self?It occurs when you don't specify the no of parameters the __init__() or any other method looking for.For example:When you run the above programme, it gives you an error like that:How we can get rid of this thing?Just pass the parameters, what __init__() method looking for Newcomer to Python, I had this issue when I was using the Python's ** feature in a wrong way. Trying to call this definition from somewhere:using a call without a double star was causing the problem:The solution is to add ** to the argument:You should actually create a class:Pass cls parameter into @classmethod to resolve this problem.

Django - what is the difference between render(), render_to_response() and direct_to_template()?

Ryan

[Django - what is the difference between render(), render_to_response() and direct_to_template()?](https://stackoverflow.com/questions/5154358/django-what-is-the-difference-between-render-render-to-response-and-direc)

Whats the difference (in language a python/django noob can understand) in a view between render(), render_to_response() and direct_to_template()?e.g. from Nathan Borror's basic apps examplesBut I've also seenAndWhats the difference, what to use in any particular situation?

2011-03-01 12:13:50Z

Whats the difference (in language a python/django noob can understand) in a view between render(), render_to_response() and direct_to_template()?e.g. from Nathan Borror's basic apps examplesBut I've also seenAndWhats the difference, what to use in any particular situation?https://docs.djangoproject.com/en/1.8/topics/http/shortcuts/#renderrender() is a brand spanking new shortcut for render_to_response in 1.3 that will automatically use RequestContext that I will most definitely be using from now on.https://docs.djangoproject.com/en/1.8/topics/http/shortcuts/#render-to-responserender_to_response is your standard render function used in the tutorials and such. To use RequestContext you'd have to specify context_instance=RequestContext(request)https://docs.djangoproject.com/en/1.8/ref/generic-views/#django-views-generic-simple-direct-to-templatedirect_to_template is a generic view that I use in my views (as opposed to in my urls) because like the new render() function, it automatically uses RequestContext and all its context_processors.But direct_to_template should be avoided as function based generic views are deprecated. Either use render or an actual class, see https://docs.djangoproject.com/en/1.3/topics/generic-views-migration/I'm happy I haven't typed RequestContext in a long, long time. Rephrasing Yuri, Fábio, and Frosts answers for the Django noob (i.e. me) - almost certainly a simplification, but a good starting point?Render isSo there is really no difference between render_to_response except it wraps your context making the template pre-processors work.Direct to template is a generic view.There is really no sense in using it here because there is overhead over render_to_response in the form of view function.From django docs: direct_to_template is something different. It's a generic view that uses a data dictionary to render the html without the need of the views.py, you use it in urls.py. Docs hereJust one note I could not find in the answers above. In this code:What the third parameter context_instance actually does? Being RequestContext it sets up some basic context which is then added to user_context. So the template gets this extended context. What variables are added is given by TEMPLATE_CONTEXT_PROCESSORS in settings.py. For instance django.contrib.auth.context_processors.auth adds variable user and variable perm which are then accessible in the template.

Replace non-ASCII characters with a single space

dotancohen

[Replace non-ASCII characters with a single space](https://stackoverflow.com/questions/20078816/replace-non-ascii-characters-with-a-single-space)

I need to replace all non-ASCII (\x00-\x7F) characters with a space. I'm surprised that this is not dead-easy in Python, unless I'm missing something. The following function simply removes all non-ASCII characters:And this one replaces non-ASCII characters with the amount of spaces as per the amount of bytes in the character code point (i.e. the – character is replaced with 3 spaces):How can I replace all non-ASCII characters with a single space?Of the myriad of similar SO questions, none address character replacement as opposed to stripping, and  additionally address all non-ascii characters not  a  specific  character.

2013-11-19 18:09:03Z

I need to replace all non-ASCII (\x00-\x7F) characters with a space. I'm surprised that this is not dead-easy in Python, unless I'm missing something. The following function simply removes all non-ASCII characters:And this one replaces non-ASCII characters with the amount of spaces as per the amount of bytes in the character code point (i.e. the – character is replaced with 3 spaces):How can I replace all non-ASCII characters with a single space?Of the myriad of similar SO questions, none address character replacement as opposed to stripping, and  additionally address all non-ascii characters not  a  specific  character.Your ''.join() expression is filtering, removing anything non-ASCII; you could use a conditional expression instead:This handles characters one by one and would still use one space per character replaced.Your regular expression should just replace consecutive non-ASCII characters with a space:Note the + there.For you the get the most alike representation of your original string I recommend the unidecode module:Then you can use it in a string:For character processing, use Unicode strings:But note you will still have a problem if your string contains decomposed Unicode characters (separate character and combining accent marks, for example):If the replacement character can be '?' instead of a space, then I'd suggest result = text.encode('ascii', 'replace').decode():Results:What about this one?As a native and efficient approach, you don't need to use ord or any loop over the characters. Just encode with ascii and ignore the errors.The following will just remove the non-ascii characters:Now if you want to replace the deleted characters just do the following:Potentially for a different question, but I'm providing my version of @Alvero's answer (using unidecode).  I want to do a "regular" strip on my strings, i.e. the beginning and end of my string for whitespace characters, and then replace only other whitespace characters with a "regular" space, i.e.to,We first replace all non-unicode spaces with a regular space (and join it back again),And then we split that again, with python's normal split, and strip each "bit",And lastly join those back again, but only if the string passes an if test,And with that, safely_stripped('ㅤㅤㅤㅤCeñíaㅤmañanaㅤㅤㅤㅤ') correctly returns 'Ceñía mañana'.

Extracting text from HTML file using Python

John D. Cook

[Extracting text from HTML file using Python](https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python)

I'd like to extract the text from an HTML file using Python.  I want essentially the same output I would get if I copied the text from a browser and pasted it into notepad.  I'd like something more robust than using regular expressions that may fail on poorly formed HTML.  I've seen many people recommend Beautiful Soup, but I've had a few problems using it.  For one, it picked up unwanted text, such as JavaScript source.  Also, it did not interpret HTML entities.  For example, I would expect &#39; in HTML source to be converted to an apostrophe in text, just as if I'd pasted the browser content into notepad.Update html2text looks promising. It handles HTML entities correctly and ignores JavaScript.  However, it does not exactly produce plain text; it produces markdown that would then have to be turned into plain text. It comes with no examples or documentation, but the code looks clean.Related questions:

2008-11-30 02:28:04Z

I'd like to extract the text from an HTML file using Python.  I want essentially the same output I would get if I copied the text from a browser and pasted it into notepad.  I'd like something more robust than using regular expressions that may fail on poorly formed HTML.  I've seen many people recommend Beautiful Soup, but I've had a few problems using it.  For one, it picked up unwanted text, such as JavaScript source.  Also, it did not interpret HTML entities.  For example, I would expect &#39; in HTML source to be converted to an apostrophe in text, just as if I'd pasted the browser content into notepad.Update html2text looks promising. It handles HTML entities correctly and ignores JavaScript.  However, it does not exactly produce plain text; it produces markdown that would then have to be turned into plain text. It comes with no examples or documentation, but the code looks clean.Related questions:html2text is a Python program that does a pretty good job at this.The best piece of code I found for extracting text without getting javascript or not wanted things :You just have to install BeautifulSoup before :NOTE: NTLK no longer supports clean_html functionOriginal answer below, and an alternative in the comments sections.Use NLTK  I wasted my 4-5 hours fixing the issues with html2text.  Luckily i could encounter NLTK.

It works magically.    Found myself facing just the same problem today. I wrote a very simple HTML parser to strip incoming content of all markups, returning the remaining text with only a minimum of formatting.Here is a version of xperroni's answer which is a bit more complete. It skips script and style sections and translates charrefs (e.g., &#39;) and HTML entities (e.g., &amp;).It also includes a trivial plain-text-to-html inverse converter.I know there are a lot of answers already, but the most elegent and pythonic solution I have found is described, in part, here.Based on Fraser's comment, here is more elegant solution:You can use html2text method in the stripogram library also.To install stripogram run sudo easy_install stripogramThere is Pattern library for data mining.http://www.clips.ua.ac.be/pages/pattern-webYou can even decide what tags to keep:PyParsing does a great job.  The PyParsing wiki was killed so here is another location where there are examples of the use of PyParsing (example link). One reason for investing a little time with pyparsing is that he has also written a very brief very well organized O'Reilly Short Cut manual that is also inexpensive.Having said that, I use BeautifulSoup a lot and it is not that hard to deal with the entities issues, you can convert them before you run BeautifulSoup.  Goodluck   This isn't exactly a Python solution, but it will convert text Javascript would generate into text, which I think is important (E.G. google.com). The browser Links (not Lynx) has a Javascript engine, and will convert source to text with the -dump option.So you could do something like:Instead of the HTMLParser module, check out htmllib.  It has a similar interface, but does more of the work for you.  (It is pretty ancient, so it's not much help in terms of getting rid of javascript and css.  You could make a derived class, but and add methods with names like start_script and end_style (see the python docs for details), but it's hard to do this reliably for malformed html.)  Anyway, here's something simple that prints the plain text to the consoleI recommend a Python Package called goose-extractor

Goose will try to extract the following information:Main text of an article

Main image of article

Any Youtube/Vimeo movies embedded in article

Meta Description

Meta tagsMore :https://pypi.python.org/pypi/goose-extractor/if you need more speed and less accuracy then you could use raw lxml.install html2text using then,Beautiful soup does convert html entities. It's probably your best bet considering HTML is often buggy and filled with unicode and html encoding issues. This is the code I use to convert html to raw text:Another option is to run the html through a text based web browser and dump it. For example (using Lynx):This can be done within a python script as follows:It won't give you exactly just the text from the HTML file, but depending on your use case it may be preferable to the output of html2text. I know there's plenty of answers here already but I think newspaper3k also deserves a mention. I recently needed to complete a similar task of extracting the text from articles on the web and this library has done an excellent job of achieving this so far in my tests. It ignores the text found in menu items and side bars as well as any JavaScript that appears on the page as the OP requests. If you already have the HTML files downloaded you can do something like this:It even has a few NLP features for summarizing the topics of articles:Another non-python solution: Libre Office:The reason I prefer this one over other alternatives is that every HTML paragraph gets converted into a single text line (no line breaks), which is what I was looking for. Other methods require post-processing. Lynx does produce nice output, but not exactly what I was looking for. Besides, Libre Office can be used to convert from all sorts of formats...Anyone has tried bleach.clean(html,tags=[],strip=True) with bleach? it's working for me.Best worked for me is inscripts . https://github.com/weblyzard/inscriptisThe results are really goodI've had good results with Apache Tika. Its purpose is the extraction of metadata and text from content, hence the underlying parser is tuned accordingly out of the box.Tika can be run as a server, is trivial to run / deploy in a Docker container, and from there can be accessed via Python bindings.in a simple waythis code finds all parts of the html_text started with '<' and ending with '>' and replace all found by an empty string@PeYoTIL's answer using BeautifulSoup and eliminating style and script content didn't work for me. I tried it using decompose instead of extract but it still didn't work. So I created my own which also formats the text using the <p> tags and replaces <a> tags with the href link. Also copes with links inside text. Available at this gist with a test doc embedded.In Python 3.x you can do it in a very easy way by importing 'imaplib' and 'email' packages. Although this is an older post but maybe my answer can help new comers on this post.Now you can print body variable and it will be in plaintext format :) If it is good enough for you then it would be nice to select it as accepted answer.you can extract only text from HTML with BeautifulSoupWhile alot of people mentioned using regex to strip html tags, there are a lot of downsides.for example:Should be parsed to:Here's a snippet I came up with, you can cusomize it to your specific needs, and it works like a charmAnother example using BeautifulSoup4 in Python 2.7.9+includes:Code:Explained:Read in the url data as html (using BeautifulSoup), remove all script and style elements, and also get just the text using .get_text(). Break into lines and remove leading and trailing space on each, then break multi-headlines into a line each chunks = (phrase.strip() for line in lines for phrase in line.split("  ")). Then using text = '\n'.join, drop blank lines, finally return as sanctioned utf-8.Notes: Here's the code I use on a regular basis.I hope that helps.The LibreOffice writer comment has merit since the application can employ python macros. It seems to offer multiple benefits both for answering this question and furthering the macro base of LibreOffice. If this resolution is a one-off implementation, rather than to be used as part of a greater production program, opening the HTML in writer and saving the page as text would seem to resolve the issues discussed here.Perl way (sorry mom, i'll never do it in production).

Url decode UTF-8 in Python

swordholder

[Url decode UTF-8 in Python](https://stackoverflow.com/questions/16566069/url-decode-utf-8-in-python)

I have spent plenty of time as far as I am newbie in Python.

How could I ever decode such a URL:to this one in python 2.7: example.com?title==правовая+защита  url=urllib.unquote(url.encode("utf8")) is returning something very ugly.Still no solution, any help is appreciated.

2013-05-15 13:16:54Z

I have spent plenty of time as far as I am newbie in Python.

How could I ever decode such a URL:to this one in python 2.7: example.com?title==правовая+защита  url=urllib.unquote(url.encode("utf8")) is returning something very ugly.Still no solution, any help is appreciated.The data is UTF-8 encoded bytes escaped with URL quoting, so you want to decode, with urllib.parse.unquote(), which handles decoding from percent-encoded data to UTF-8 bytes and then to text, transparently:Demo:The Python 2 equivalent is urllib.unquote(), but this returns a bytestring, so you'd have to decode manually:If you are using Python 3, you can use urllib.parsegives:

How to check if a user is logged in (how to properly use user.is_authenticated)?

Rick

[How to check if a user is logged in (how to properly use user.is_authenticated)?](https://stackoverflow.com/questions/3644902/how-to-check-if-a-user-is-logged-in-how-to-properly-use-user-is-authenticated)

I am looking over this website but just can't seem to figure out how to do this as it's not working. I need to check if the current site user is logged in (authenticated), and am trying:despite being sure that the user is logged in, it returns just:I'm able to do other requests (from the first section in the url above), such as:which returns a successful response.

2010-09-05 03:30:04Z

I am looking over this website but just can't seem to figure out how to do this as it's not working. I need to check if the current site user is logged in (authenticated), and am trying:despite being sure that the user is logged in, it returns just:I'm able to do other requests (from the first section in the url above), such as:which returns a successful response.Update for Django 1.10+: is_authenticated is now an attribute in Django 1.10. The method still exists for backwards compatibility, but will be removed in Django 2.0.For Django 1.9 and older:is_authenticated is a function. You should call it likeAs Peter Rowell pointed out, what may be tripping you up is that in the default Django template language, you don't tack on parenthesis to call functions. So you may have seen something like this in template code:However, in Python code, it is indeed a method in the User class.Django 1.10+Use an attribute, not a method:The use of the method of the same name is deprecated in Django 2.0, and is no longer mentioned in the Django documentation.that after updated to the property request.user.is_authenticated was throwing the exception TypeError: Object of type 'CallableBool' is not JSON serializable.  The solution was to use JsonResponse, which could handle the CallableBool object properly when serializing:Following block should work: In your view:In you controller functions add decorator:For Django 2.0+ versions use:For more info visit https://www.django-rest-framework.org/api-guide/requests/#authrequest.user.is_authenticated() has been removed in Django 2.0+ versions.

Is generator.next() visible in python 3.0?

jottos

[Is generator.next() visible in python 3.0?](https://stackoverflow.com/questions/1073396/is-generator-next-visible-in-python-3-0)

I have a generator that generates a series, for example:in python 2.6 I am able to make the following calls:however in 3.0 if I execute the same two lines of code I'm getting the following error:but, the loop iterator syntax does work in 3.0I've not been able to find anything yet that explains this difference in behavior for 3.0.

2009-07-02 09:29:44Z

I have a generator that generates a series, for example:in python 2.6 I am able to make the following calls:however in 3.0 if I execute the same two lines of code I'm getting the following error:but, the loop iterator syntax does work in 3.0I've not been able to find anything yet that explains this difference in behavior for 3.0.Correct, g.next() has been renamed to g.__next__(). The reason for this is consistency: Special methods like __init__() and __del__ all have double underscores (or "dunder" in the current vernacular), and .next() was one of the few exceptions to that rule. This was fixed in Python 3.0. [*]But instead of calling g.__next__(), as Paolo says, use next(g).[*] There are other special attributes that have gotten this fix; func_name, is now __name__, etc.Try:Check out this neat table that shows the differences in syntax between 2 and 3 when it comes to this.If your code must run under Python2 and Python3, use the 2to3 six library like this:

Read file from line 2 or skip header row

super9

[Read file from line 2 or skip header row](https://stackoverflow.com/questions/4796764/read-file-from-line-2-or-skip-header-row)

How can I skip the header row and start reading a file from line2?

2011-01-25 17:25:08Z

How can I skip the header row and start reading a file from line2?If you want the first line and then you want to perform some operation on file this code will helpful.If slicing could work on iterators...To generalize the task of reading multiple header lines and to improve readability I'd use method extraction. Suppose you wanted to tokenize the first three lines of coordinates.txt to use as header information.Example Then method extraction allows you to specify what you want to do with the header information (in this example we simply tokenize the header lines based on the comma and return it as a list but there's room to do much more). OutputIf coordinates.txt contains another headerline, simply change numberheaderlines. Best of all, it's clear what __readheader(rh, numberheaderlines=2) is doing and we avoid the ambiguity of having to figure out or comment on why author of the the accepted answer uses next() in his code. If you want to read multiple CSV files starting from line 2, this works like a charm(this is part of Parfait's answer to a different question)

How to initialize a dict with keys from a list and empty value in Python?

Juanjo Conti

[How to initialize a dict with keys from a list and empty value in Python?](https://stackoverflow.com/questions/2241891/how-to-initialize-a-dict-with-keys-from-a-list-and-empty-value-in-python)

I'd like to get from this:to this:Is there a pythonic way of doing it?This is an ugly way to do it:

2010-02-11 02:43:19Z

I'd like to get from this:to this:Is there a pythonic way of doing it?This is an ugly way to do it:dict.fromkeys([1, 2, 3, 4])This is actually a classmethod, so it works for dict-subclasses (like collections.defaultdict) as well. The optional second argument specifies the value to use for the keys (defaults to None.)nobody cared to give a dict-comprehension solution ?Output:In many workflows where you want to attach a default / initial value for arbitrary keys, you don't need to hash each key individually ahead of time. You can use collections.defaultdict. For example:This is more efficient, it saves having to hash all your keys at instantiation. Moreover, defaultdict is a subclass of dict, so there's usually no need to convert back to a regular dictionary.For workflows where you require controls on permissible keys, you can use dict.fromkeys as per the accepted answer:

Cannot install Lxml on Mac os x 10.9

David O'Regan

[Cannot install Lxml on Mac os x 10.9](https://stackoverflow.com/questions/19548011/cannot-install-lxml-on-mac-os-x-10-9)

I want to install Lxml so I can then install Scrapy.When I updated my Mac today it wouldn't let me reinstall lxml, I get the following error:I have tried using brew to install libxml2 and libxslt, both installed fine but I still cannot install lxml.Last time I was installing I needed to enable the developer tools on Xcode but since its updated to Xcode 5 it doesnt give me that option anymore.Does anyone know what I need to do?

2013-10-23 17:07:32Z

I want to install Lxml so I can then install Scrapy.When I updated my Mac today it wouldn't let me reinstall lxml, I get the following error:I have tried using brew to install libxml2 and libxslt, both installed fine but I still cannot install lxml.Last time I was installing I needed to enable the developer tools on Xcode but since its updated to Xcode 5 it doesnt give me that option anymore.Does anyone know what I need to do?You should install or upgrade the commandline tool for xcode.

Try this in a terminal:I solved this issue on Yosemite by both installing and linking libxml2 and libxslt through brew:If you have solved the problem using this method but it pops up again at a later time, you might need to run this before the four lines above:If you are having permission errors with Homebrew, especially on El Capitan, this is a helpful document. In essence, regardless of OS X version, try running:You may solve your problem by running this on the commandline:It sure helped me.

Explanations on docsI tried most of the solutions above, but none of them worked for me. I'm running Yosemite 10.10, the only solution that worked for me was to type this in the terminal:EDIT: If you are using virtualenv, the sudo in beginning is not needed.This has been bothering me as well for a while.  I don't know the internals enough about python distutils etc, but the include path here is wrong.  I made the following ugly hack to hold me over until the python lxml people can do the proper fix.Installing globally... OS X 10.9.2instalation instructions on http://lxml.de/installation.html explain:None of the above worked for me on 10.9.2, as compilation bails out with following error: Which actually lead to cleanest solution (see more details in [1]): or following if installing globally[1] clang error: unknown argument: '-mno-fused-madd' (python package installation failure)I solved this issue on Yosemite by running the following commands:With homebrew, libxml2 is hidden to not interfere with the system libxml2, so pip must be helped a little in order to find it.With bash:With fish:OSX 10.9.2I tried all the answers on this page, none of them worked for me. I'm running OS X Version 10.9.2But this definitely works....like a charm:ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future pip install lxmlUnfortunately xcode-select --install did not work for me as I already had the latest version.It's very strange but I solved the issue by opening XCode and accepting the Terms & Conditions. Re-running pip install lxml returned no errors after.After successful install from pip (lxml 3.6.4) I was getting an error when importing the lxml.etree module.I was searching endlessly to install this as a requisite for scrapy, and tried all the options, but finally this worked for me (mac osx 10.11 python 2.7):The older version of lxml seem to work with etree module. Pip can often ignore the specified version of a package, for example when you have the newer version in the pip cache, thus the easy_install. The '-2.7' option is for python version, omit this if you are installing for python 3.x. In my case, I must shutdown Kaspersky Antivirus before installing lxml by:I am using OSX 10.9.2 and I get the same error.Installation of the XCode command line tools does not help for this particular version of OSX.I think a better approach to fix this is to install with the following command:This is similar to jdkoftinoff' fix, but does not alter your system in a permanent way.I met the same question and after days of working I resolved this problem on my OS X 10.9.4, with Python 3.4.1.Here's my solution,According to installing lxml from lxml.de,If you do not have MacPort, install it from MacPort.org. It's quite easy. You may also need a compiler, to install XCode compiling tools, use xcode-select --installFirstly I updated my port to the latest version via sudo port selfupdate, Then I just type sudo port install libxml2 and several minutes later you should see libxml2 installed successfully. Probably you may also need libxslt to install lxml. To install libxslt, use:sudo port install libxslt.Now, just type pip install lxml, it should work fine.before compiling add the path that to xmlversion.h into your environment.But make sure the path I've provided has the xmlversion.h file located inside. Then,pip did not work for me. I went to

https://pypi.python.org/pypi/lxml/2.3

and downloaded the macosx .egg file:https://pypi.python.org/packages/2.7/l/lxml/lxml-2.3-py2.7-macosx-10.6-intel.egg#md5=52322e4698d68800c6b6aedb0dbe5f34Then used command line easy_install to install the .egg file.This post links to a solution that worked for me

Python3, lxml and "Symbol not found: _lzma_auto_decoder" on Mac OS X 10.9hthAfter much tearing of the hair and gnashing of the teeth, I uninstalled xcode with pip and ran:easy_install lxmlAnd all was well.Try:Or:It works!

How do I keep Python print from adding newlines or spaces? [duplicate]

Bart

[How do I keep Python print from adding newlines or spaces? [duplicate]](https://stackoverflow.com/questions/255147/how-do-i-keep-python-print-from-adding-newlines-or-spaces)

In python, if I sayI get the letter h and a newline.  If I say I get the letter h and no newline.  If I sayI get the letter h, a space, and the letter m.  How can I prevent Python from printing the space?The print statements are different iterations of the same loop so I can't just use the + operator.

2008-10-31 22:33:21Z

In python, if I sayI get the letter h and a newline.  If I say I get the letter h and no newline.  If I sayI get the letter h, a space, and the letter m.  How can I prevent Python from printing the space?The print statements are different iterations of the same loop so I can't just use the + operator.You need to call sys.stdout.flush() because otherwise it will hold the text in a buffer and you won't see it.In Python 3, useto suppress the endline terminator, andto suppress the whitespace separator between items. See the documentation for printGreg is right-- you can use sys.stdout.writePerhaps, though, you should consider refactoring your algorithm to accumulate a list of <whatevers> and thenOr use a +, i.e.:Just make sure all are concatenate-able objects.But really, you should use sys.stdout.write directly.For completeness, one other way is to clear the softspace value after performing the write.prints helloworld !Using stdout.write() is probably more convenient for most cases though.This may look stupid, but seems to be the simplest:Regain control of your console! Simply:where __past__.py contains:then:Bonus extra: If you don't like print >> f, ..., you can extending this caper to fprintf(f, ...).I am not adding a new answer. I am just putting the best marked answer in a better format.

I can see that the best answer by rating is using sys.stdout.write(someString). You can try this out:will yield:That is all.In python 2.6:So using print_function from __future__ you can set explicitly the sep and end parameteres of print function.You can use print like the printf function in C.e.g.print "%s%s" % (x, y)sys.stdout.write is (in Python 2) the only robust solution. Python 2 printing is insane. Consider this code:This will print a b, leading you to suspect that it is printing a trailing space. But this is not correct. Try this instead:This will print a0b. How do you explain that? Where have the spaces gone?I still can't quite make out what's really going on here. Could somebody look over my best guess:My attempt at deducing the rules when you have a trailing , on your print:First, let's assume that print    , (in Python 2) doesn't print any whitespace (spaces nor newlines).Python 2 does, however, pay attention to how you are printing - are you using print, or sys.stdout.write, or something else? If you make two consecutive calls to print, then Python will insist on putting in a space in between the two.it will produce 

Should I use「camel case」or underscores in python? [duplicate]

tdc

[Should I use「camel case」or underscores in python? [duplicate]](https://stackoverflow.com/questions/8908760/should-i-use-camel-case-or-underscores-in-python)

So which is better and why?or

2012-01-18 10:45:05Z

So which is better and why?orfor everything related to Python's style guide: i'd recommend you read PEP8.To answer your question:PEP 8 advises the first form for readability. You can find it here.Check out its already been answered, click here

What are type hints in Python 3.5?

Vaulstein

[What are type hints in Python 3.5?](https://stackoverflow.com/questions/32557920/what-are-type-hints-in-python-3-5)

One of the most talked about features in Python 3.5 is type hints.An example of type hints is mentioned in this article and this one while also mentioning to use type hints responsibly. Can someone explain more about them and when they should be used and when not?

2015-09-14 05:37:33Z

One of the most talked about features in Python 3.5 is type hints.An example of type hints is mentioned in this article and this one while also mentioning to use type hints responsibly. Can someone explain more about them and when they should be used and when not?I would suggest reading PEP 483 and PEP 484 and watching this presentation by Guido on Type Hinting.In a nutshell: Type hinting is literally what the words mean, you hint the type of the object(s) you're using. Due to the dynamic nature of Python, inferring or checking the type of an object being used is especially hard. This fact makes it hard for developers to understand what exactly is going on in code they haven't written and, most importantly, for type checking tools found in many IDEs [PyCharm, PyDev come to mind] that are limited due to the fact that they don't have any indicator of what type the objects are. As a result they resort to trying to infer the type with (as mentioned in the presentation) around 50% success rate. To take two important slides from the Type Hinting presentation:As a closing note for this small introduction: This is an optional feature and, from what I understand, it has been introduced in order to reap some of the benefits of static typing. You generally do not need to worry about it and definitely don't need to use it (especially in cases where you use Python as an auxiliary scripting language). It should be helpful when developing large projects as it offers much needed robustness, control and additional debugging capabilities.In order to make this answer more complete, I think a little demonstration would be suitable. I'll be using mypy, the library which inspired Type Hints as they are presented in the PEP. This is mainly written for anybody bumping into this question and wondering where to begin.Before I do that let me reiterate the following: PEP 484 doesn't enforce anything; it is simply setting a direction for function

annotations and proposing guidelines for how type checking can/should be performed. You can annotate your functions and

hint as many things as you want; your scripts will still run regardless of the presence of annotations because Python itself doesn't use them.Anyways, as noted in the PEP, hinting types should generally take three forms:Additionally, you'll want to use type hints in conjunction with the new typing module introduced in Py3.5. In it, many (additional) ABCs (Abstract Base Classes) are defined along with helper functions and decorators for use in static checking. Most ABCs in collections.abc are included but in a Generic form in order to allow subscription (by defining a __getitem__() method).For anyone interested in a more in-depth explanation of these, the mypy documentation is written very nicely and has a lot of code samples demonstrating/describing the functionality of their checker; it is definitely worth a read.First, it's interesting to observe some of the behavior we can get when using special comments. Special # type: type comments

can be added during variable assignments to indicate the type of an object if one cannot be directly inferred. Simple assignments are

generally easily inferred but others, like lists (with regard to their contents), cannot.Note: If we want to use any derivative of Containers and need to specify the contents for that container we must use the generic types from the typing module. These support indexing.If we add these commands to a file and execute them with our interpreter, everything works just fine and print(a) just prints

the contents of list a. The # type comments have been discarded, treated as plain comments which have no additional semantic meaning.By running this with mypy, on the other hand, we get the following responce:Indicating that a list of str objects cannot contain an int, which, statically speaking, is sound. This can be fixed by either abiding to the type of a and only appending str objects or by changing the type of the contents of a to indicate that any value is acceptable (Intuitively performed with List[Any] after Any has been imported from typing).Function annotations are added in the form param_name : type after each parameter in your function signature and a return type is specified using the -> type notation before the ending function colon; all annotations are stored in the __annotations__ attribute for that function in a handy dictionary form. Using a trivial example (which doesn't require extra types from the typing module):The annotated.__annotations__ attribute now has the following values:If we're a complete noobie, or we are familiar with Py2.7 concepts and are consequently unaware of the TypeError lurking in the comparison of annotated, we can perform another static check, catch the error and save us some trouble:Among other things, calling the function with invalid arguments will also get caught:These can be extended to basically any use-case and the errors caught extend further than basic calls and operations. The types you

can check for are really flexible and I have merely given a small sneak peak of its potential. A look in the typing module, the

PEPs or the mypy docs will give you a more comprehensive idea of the capabilities offered.Stub files can be used in two different non mutually exclusive cases:What stub files (with an extension of .pyi) are is an annotated interface of the module you are making/want to use. They contain

the signatures of the functions you want to type-check with the body of the functions discarded. To get a feel of this, given a set

of three random functions in a module named randfunc.py:We can create a stub file randfunc.pyi, in which we can place some restrictions if we wish to do so. The downside is that

somebody viewing the source without the stub won't really get that annotation assistance when trying to understand what is supposed

to be passed where.Anyway, the structure of a stub file is pretty simplistic: Add all function definitions with empty bodies (pass filled) and

supply the annotations based on your requirements. Here, let's assume we only want to work with int types for our Containers.The combine function gives an indication of why you might want to use annotations in a different file, they some times clutter up

the code and reduce readability (big no-no for Python). You could of course use type aliases but that sometime confuses more than it

helps (so use them wisely).This should get you familiarized with the basic concepts of Type Hints in Python. Even though the type checker used has been

mypy you should gradually start to see more of them pop-up, some internally in IDEs (PyCharm,) and others as standard python modules.

I'll try and add additional checkers/related packages in the following list when and if I find them (or if suggested).Checkers I know of:Related Packages/Projects:The typeshed project is actually one of the best places you can look to see how type hinting might be used in a project of your own. Let's take as an example the __init__ dunders of the Counter class in the corresponding .pyi file: Where _T = TypeVar('_T') is used to define generic classes. For the Counter class we can see that it can either take no arguments in its initializer, get a single Mapping from any type to an int or take an Iterable of any type.  Notice: One thing I forgot to mention was that the typing module has been introduced on a provisional basis. From PEP 411:So take things here with a pinch of salt; I'm doubtfull it will be removed  or altered in significant ways but one can never know.** Another topic altogether but valid in the scope of type-hints: PEP 526: Syntax for Variable Annotations is an effort to replace # type comments by introducing new syntax which allows users to annotate the type of variables in simple varname: type statements. See What are variable annotations in Python 3.6?, as previously mentioned, for a small intro on these.Adding to Jim's elaborate answer:Check the typing module -- this module supports type hints as specified by PEP 484.For example, the function below takes and returns values of type str and is annotated as follows:The typing module also supports:The newly released PyCharm 5 supports type hinting. In their blog post about it (see Python 3.5 type hinting in PyCharm 5) they offer a great explanation of what type hints are and aren't along with several examples and illustrations for how to use them in your code. Additionally, it is supported in Python 2.7, as explained in this comment:Type-hints is for maintenability and doesn't get interpreted by Python. In the below code, the line "def add(self, ic:int)" doesn't incur an error until the next "return..." line.Type hint are a recent addition to a dynamic language where for decades folks swore naming conventions as simple as Hungarian (object label with first letter b = boolian, c = character, d = dictionary, i = integer, l = list, n = numeric, s = string, t= tuple) were not needed, too cumbersome, but now have decided that, oh wait ... it is way too much trouble to use the language (type()) to recognize objects, and our fancy IDEs need help doing anything that complicated, and that dynamically assigned object values make them completely useless anyhow, whereas a simple naming convention could have resolved all of it, for any developer, at a mere glance.

Mapping over values in a python dictionary

Tarrasch

[Mapping over values in a python dictionary](https://stackoverflow.com/questions/12229064/mapping-over-values-in-a-python-dictionary)

Given a dictionary { k1: v1, k2: v2 ... } I want to get { k1: f(v1), k2: f(v2) ... } provided I pass a function f.Is there any such built in function? Or do I have to doIdeally I would just writeorThat is, it doesn't matter to me if the original dictionary is mutated or a copy is created.

2012-09-01 15:31:37Z

Given a dictionary { k1: v1, k2: v2 ... } I want to get { k1: f(v1), k2: f(v2) ... } provided I pass a function f.Is there any such built in function? Or do I have to doIdeally I would just writeorThat is, it doesn't matter to me if the original dictionary is mutated or a copy is created.There is no such function; the easiest way to do this is to use a dict comprehension:In python 2.7, use the .iteritems() method instead of .items() to save memory. The dict comprehension syntax wasn't introduced until python 2.7.Note that there is no such method on lists either; you'd have to use a list comprehension or the map() function.As such, you could use the map() function for processing your dict as well:but that's not that readable, really.These toolz are great for this kind of simple yet repetitive logic.http://toolz.readthedocs.org/en/latest/api.html#toolz.dicttoolz.valmapGets you right where you want to be.You can do this in-place, rather than create a new dict, which may be preferable for large dictionaries (if you do not need a copy).results in my_dictionary containing:Due to PEP-0469 which renamed iteritems() to items() and PEP-3113 which removed Tuple parameter unpacking, in Python 3.x you should write Martijn Pieters♦ answer like this:While my original answer missed the point (by trying to solve this problem with the solution to Accessing key in factory of defaultdict), I have reworked it to propose an actual solution to the present question.Here it is:Usage:The idea is to subclass the original dict to give it the desired functionality: "mapping" a function over all the values.The plus point is that this dictionary can be used to store the original data as if it was a dict, while transforming any data on request with a callback.Of course, feel free to name the class and the function the way you want (the name chosen in this answer is inspired by PHP's array_walk() function).Note: Neither the try-except block nor the return statements are mandatory for the functionality, they are there to further mimic the behavior of the PHP's array_walk.To avoid doing indexing from inside lambda, like:You can also do:Just came accross this use case. I implemented gens's answer, adding a recursive approach for handling values that are also dicts:This can be useful when dealing with json or yaml files that encode strings as bytes in Python 2

Initializing a list to a known number of elements in Python [duplicate]

Joan Venge

[Initializing a list to a known number of elements in Python [duplicate]](https://stackoverflow.com/questions/521674/initializing-a-list-to-a-known-number-of-elements-in-python)

Right now I am using a list, and was expecting something like:Should I use array instead?

2009-02-06 18:58:53Z

Right now I am using a list, and was expecting something like:Should I use array instead?The first thing that comes to mind for me is:But do you really need to preinitialize it?Not quite sure why everyone is giving you a hard time for wanting to do this - there are several scenarios where you'd want a fixed size initialised list. And you've correctly deduced that arrays are sensible in these cases.For the non-pythonistas, the (0,)*1000 term is creating a tuple containing 1000 zeros. The comma forces python to recognise (0) as a tuple, otherwise it would be evaluated as 0.I've used a tuple instead of a list because they are generally have lower overhead.One obvious and probably not efficient way is Note that this can be extended to 2-dimension easily. 

For example, to get a 10x100 "array" you can do Wanting to initalize an array of fixed size is a perfectly acceptable thing to do in any programming language; it isn't like the programmer wants to put a break statement in a while(true) loop. Believe me, especially if the elements are just going to be overwritten and not merely added/subtracted, like is the case of many dynamic programming algorithms, you don't want to mess around with append statements and checking if the element hasn't been initialized yet on the fly (that's a lot of code gents).object = [0 for x in range(1000)]This will work for what the programmer is trying to achieve.@Steve already gave a good answer to your question:Warning: As @Joachim Wuttke pointed out, the list must be initialized with an immutable element. [[]] * 1000 does not work as expected because you will get a list of 1000 identical lists (similar to a list of 1000 points to the same list in C). Immutable objects like int, str or tuple will do fine. Resizing lists is slow. The following results are not very surprising:But resizing is not very slow if you don't have very large lists. Instead of initializing the list with a single element (e.g. None) and a fixed length to avoid list resizing, you should consider using list comprehensions and directly fill the list with correct values. For example:For huge data set numpy or other optimized libraries are much faster:You could do this:That would give you a list of 1000 elements in size and which happens to be initialised with values from 0-999.  As list does a __len__ first to size the new list it should be fairly efficient. You should consider using a dict type instead of pre-initialized list. The cost of a dictionary look-up is small and comparable to the cost of accessing arbitrary list element.And when using a mapping you can write:And the putElement function can store item at any given position. And if you need to check if your collection contains element at given index it is more Pythonic to write:Than:  Since the latter assumes that no real element in your collection can be None. And if that happens - your code misbehaves.And if you desperately need performance and that's why you try to pre-initialize your variables, and write the fastest code possible - change your language. The fastest code can't be written in Python. You should try C instead and implement wrappers to call your pre-initialized and pre-compiled code from Python.Without knowing more about the problem domain, it's hard to answer your question.

Unless you are certain that you need to do something more, the pythonic way to initialize a list is:Are you actually seeing a performance problem?  If so, what is the performance bottleneck?

Don't try to solve a problem that you don't have.   It's likely that performance cost to dynamically fill an array to 1000 elements is completely irrelevant to the program  that you're really trying to write.The array class is useful if the things in your list are always going to be a specific primitive fixed-length type (e.g. char, int, float).  But, it doesn't require pre-initialization either.This:creates a list, elements are initialized 8but this:would create 7 lists which have one element

Daemon Threads Explanation

Corey Goldberg

[Daemon Threads Explanation](https://stackoverflow.com/questions/190010/daemon-threads-explanation)

In the Python documentation

it says:Does anyone have a clearer explanation of what that means or a practical example showing where you would set threads as daemonic?Clarify it for me: so the only situation you wouldn't set threads as daemonic, is when you want them to continue running after the main thread exits?

2008-10-10 03:24:07Z

In the Python documentation

it says:Does anyone have a clearer explanation of what that means or a practical example showing where you would set threads as daemonic?Clarify it for me: so the only situation you wouldn't set threads as daemonic, is when you want them to continue running after the main thread exits?Some threads do background tasks, like sending keepalive packets, or performing periodic garbage collection, or whatever. These are only useful when the main program is running, and it's okay to kill them off once the other, non-daemon, threads have exited.Without daemon threads, you'd have to keep track of them, and tell them to exit, before your program can completely quit. By setting them as daemon threads, you can let them run and forget about them, and when your program quits, any daemon threads are killed automatically.Let's say you're making some kind of dashboard widget.  As part of this, you want it to display the unread message count in your email box.  So you make a little thread that will:When your widget starts up, it would create this thread, designate it a daemon, and start it.  Because it's a daemon, you don't have to think about it; when your widget exits, the thread will stop automatically.A simpler way to think about it, perhaps: when main returns, your process will not exit if there are non-daemon threads still running.A bit of advice: Clean shutdown is easy to get wrong when threads and synchronization are involved - if you can avoid it, do so. Use daemon threads whenever possible.Other posters gave some examples for situations in which you'd use daemon threads.  My recommendation, however, is never to use them.It's not because they're not useful, but because there are some bad side effects you can experience if you use them.  Daemon threads can still execute after the Python runtime starts tearing down things in the main thread, causing some pretty bizarre exceptions.More info here:https://joeshaw.org/python-daemon-threads-considered-harmful/https://mail.python.org/pipermail/python-list/2005-February/343699.htmlStrictly speaking you never need them, it just makes implementation easier in some cases.Chris already explained what daemon threads are, so let's talk about practical usage. Many thread pool implementations use daemon threads for task workers. Workers are threads which execute tasks from task queue. Worker needs to keep waiting for tasks in task queue indefinitely as they don't know when new task will appear. Thread which assigns tasks (say main thread) only knows when tasks are over. Main thread waits on task queue to get empty and then exits. If workers are user threads i.e. non-daemon, program won't terminate. It will keep waiting for these indefinitely running workers, even though workers aren't doing anything useful. Mark workers daemon threads, and main thread will take care of killing them as soon as it's done handling tasks. Quoting Chris: "... when your program quits, any daemon threads are killed automatically.". I think that sums it up. You should be careful when you use them as they abruptly terminate when main program executes to completion.When your second thread is non-Daemon, your application's primary main thread cannot quit because its exit criteria is being tied to the exit also of non-Daemon thread(s). Threads cannot be forcibly killed in python, therefore your app will have to really wait for the non-Daemon thread(s) to exit. If this behavior is not what you want, then set your second thread as daemon so that it won't hold back your application from exiting.  

Convert a timedelta to days, hours and minutes

Oli

[Convert a timedelta to days, hours and minutes](https://stackoverflow.com/questions/2119472/convert-a-timedelta-to-days-hours-and-minutes)

I've got a timedelta. I want the days, hours and minutes from that - either as a tuple or a dictionary... I'm not fussed.I must have done this a dozen times in a dozen languages over the years but Python usually has a simple answer to everything so I thought I'd ask here before busting out some nauseatingly simple (yet verbose) mathematics.Mr Fooz raises a good point.I'm dealing with "listings" (a bit like ebay listings) where each one has a duration. I'm trying to find the time left by doing when_added + duration - nowAm I right in saying that wouldn't account for DST? If not, what's the simplest way to add/subtract an hour?

2010-01-22 18:20:38Z

I've got a timedelta. I want the days, hours and minutes from that - either as a tuple or a dictionary... I'm not fussed.I must have done this a dozen times in a dozen languages over the years but Python usually has a simple answer to everything so I thought I'd ask here before busting out some nauseatingly simple (yet verbose) mathematics.Mr Fooz raises a good point.I'm dealing with "listings" (a bit like ebay listings) where each one has a duration. I'm trying to find the time left by doing when_added + duration - nowAm I right in saying that wouldn't account for DST? If not, what's the simplest way to add/subtract an hour?If you have a datetime.timedelta value td, td.days already gives you the "days" you want. timedelta values keep fraction-of-day as seconds (not directly hours or minutes) so you'll indeed have to perform "nauseatingly simple mathematics", e.g.:This is a bit more compact, you get the hours, minutes and seconds in two lines.As for DST, I think the best thing is to convert both datetime objects to seconds. This way the system calculates DST for you.I don't understand how about thisYou get minutes and seconds of a minute as a float.I used the following:timedeltas have a days and seconds attribute .. you can convert them yourself with ease.

How to unzip a list of tuples into individual lists? [duplicate]

VaidAbhishek

[How to unzip a list of tuples into individual lists? [duplicate]](https://stackoverflow.com/questions/12974474/how-to-unzip-a-list-of-tuples-into-individual-lists)

I have a list of tuples, where I want to unzip this list into two independent lists. I'm looking for some standardized operation in Python. I'm looking for a succinct and pythonic way to achieve this. Basically, I'm hunting for inverse operation of zip() function. 

2012-10-19 12:38:40Z

I have a list of tuples, where I want to unzip this list into two independent lists. I'm looking for some standardized operation in Python. I'm looking for a succinct and pythonic way to achieve this. Basically, I'm hunting for inverse operation of zip() function. Use zip(*list):The zip() function pairs up the elements from all inputs, starting with the first values, then the second, etc. By using *l you apply all tuples in l as separate arguments to the zip() function, so zip() pairs up 1 with 3 with 8 first, then 2 with 4 and 9. Those happen to correspond nicely with the columns, or the transposition of l.zip() produces tuples; if you must have mutable list objects, just map() the tuples to lists or use a list comprehension to produce a list of lists:If you want a list of lists:If a list of tuples is OK:

Set variable in jinja [duplicate]

MyTux

[Set variable in jinja [duplicate]](https://stackoverflow.com/questions/3727045/set-variable-in-jinja)

I would like to know how can I set a variable with another variable in jinja. I will explain, I have got a submenu and I would like show which link is active. I tried this:where recordtype is a variable given for my template.

2010-09-16 13:18:12Z

I would like to know how can I set a variable with another variable in jinja. I will explain, I have got a submenu and I would like show which link is active. I tried this:where recordtype is a variable given for my template.{{ }} tells the template to print the value, this won't work in expressions like you're trying to do.  Instead, use the {% set %} template tag and then assign the value the same way you would in normal python code.Result:Nice shorthand for Multiple variable assignmentsJust Set it up like this

How to count the frequency of the elements in a list?

Bruce

[How to count the frequency of the elements in a list?](https://stackoverflow.com/questions/2161752/how-to-count-the-frequency-of-the-elements-in-a-list)

I need to find the frequency of elements in a listoutput->Also I want to remove the duplicates from a

2010-01-29 12:08:20Z

I need to find the frequency of elements in a listoutput->Also I want to remove the duplicates from aSince the list is ordered you can do this:Output:In Python 2.7 (or newer), you can use collections.Counter:If you are using Python 2.6 or older, you can download it here. Python 2.7+ introduces Dictionary Comprehension. Building the dictionary from the list will get you the count as well as get rid of duplicates.To count the number of appearances:To remove duplicates:Counting the frequency of elements is probably best done with a dictionary:To remove the duplicates, use a set:In Python 2.7+, you could use collections.Counter to count itemsHere's another succint alternative using itertools.groupby which also works for unordered input:resultsYou can do this:Output:The first array is values, and the second array is the number of elements with these values.So If you want to get just array with the numbers you should use this:I would simply use scipy.stats.itemfreq in the following manner:you may check the documentation here: http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.itemfreq.htmlFor your first question, iterate the list and use a dictionary to keep track of an elements existsence. For your second question, just use the set operator.This answer is more explicit...I am quite late, but this will also work, and will help others: will produce this..OutputSimple solution using a dictionary.To remove duplicates and Maintain order:i'm using Counter to generate a freq. dict from text file words in 1 line of codeAnother approach of doing this, albeit by using a heavier but powerful library - NLTK.Yet another solution with another algorithm without using collections:You can use the in-built function provided in pythonThe above code automatically removes duplicates in a list and also prints the frequency of each element in original list and the list without duplicates.Two birds for one shot ! X DThis approach can be tried if you don't want to use any library and keep it simple and short!o/pFor the record, a functional answer:It's cleaner if you count zeroes too:An explanation:The elements do not have to be sorted (itertools.groupby). You'll get weird results if you have negative numbers.One more way is to use a dictionary and the list.count, below a naive way to do it.

Reverse colormap in matplotlib

Mermoz

[Reverse colormap in matplotlib](https://stackoverflow.com/questions/3279560/reverse-colormap-in-matplotlib)

I would like to know how to simply reverse the color order of a given colormap in order to use it with plot_surface.

2010-07-19 08:44:21Z

I would like to know how to simply reverse the color order of a given colormap in order to use it with plot_surface.The standard colormaps also all have reversed versions. They have the same names with _r tacked on to the end. (Documentation here.)In matplotlib a color map isn't a list, but it contains the list of its colors as colormap.colors. And the module matplotlib.colors provides a function ListedColormap() to generate a color map from a list. So you can reverse any color map by doing The solution is pretty straightforward. Suppose you want to use the "autumn" colormap scheme. The standard version:To reverse the colormap color spectrum, use get_cmap() function and append '_r' to the colormap title like this:As a LinearSegmentedColormaps is based on a dictionary of red, green and blue, it's necessary to reverse each item:See that it works:EDITI don't get the comment of user3445587. It works fine on the rainbow colormap:But it especially works nice for custom declared colormaps, as there is not a default _r for custom declared colormaps. Following example taken from http://matplotlib.org/examples/pylab_examples/custom_cmap.html:As of Matplotlib 2.0, there is a reversed() method for ListedColormap and LinearSegmentedColorMap objects, so you can just docmap_reversed = cmap.reversed()

Here is the documentation.There are two types of LinearSegmentedColormaps. In some, the _segmentdata is given explicitly, e.g., for jet:For rainbow, _segmentdata is given as follows:We can find the functions in the source of matplotlib, where they are given asEverything you want is already done in matplotlib, just call cm.revcmap, which reverses both types of segmentdata, soshould do the job - you can simply create a new LinearSegmentData from that. In revcmap, the reversal of function based SegmentData is done withwhile the other lists are reversed as usualSo actually the whole thing you want, isThere is no built-in way (yet) of reversing arbitrary colormaps, but one simple solution is to actually not modify the colorbar but to create an inverting Normalize object:You can then use this with plot_surface and other Matplotlib plotting functions by doing e.g.This will work with any Matplotlib colormap.

Using pickle.dump - TypeError: must be str, not bytes

John Rowland

[Using pickle.dump - TypeError: must be str, not bytes](https://stackoverflow.com/questions/13906623/using-pickle-dump-typeerror-must-be-str-not-bytes)

I'm using python3.3 and I'm having a cryptic error when trying to pickle a simple dictionary. Here is the code:and I get:

2012-12-16 23:42:03Z

I'm using python3.3 and I'm having a cryptic error when trying to pickle a simple dictionary. Here is the code:and I get:The output file needs to be opened in binary mode:needs to be:Just had same issue.  In Python 3, Binary modes 'wb', 'rb' must be specified whereas in Python 2x,  they are not needed.  When you follow tutorials that are based on Python 2x,  that's why you are here. 

How to convert 'binary string' to normal string in Python3?

Hanfei Sun

[How to convert 'binary string' to normal string in Python3?](https://stackoverflow.com/questions/17615414/how-to-convert-binary-string-to-normal-string-in-python3)

For example, I have a string like this(return value of subprocess.check_output):Whatever I did to it, it is always printed with the annoying b' before the string:Does anyone have any ideas about how to use it as a normal string or convert it into a normal string? 

2013-07-12 12:55:06Z

For example, I have a string like this(return value of subprocess.check_output):Whatever I did to it, it is always printed with the annoying b' before the string:Does anyone have any ideas about how to use it as a normal string or convert it into a normal string? Decode it.To get bytes from string, encode it.If the answer from falsetru didn't work you could also try:

How do I filter query objects by date range in Django?

user469652

[How do I filter query objects by date range in Django?](https://stackoverflow.com/questions/4668619/how-do-i-filter-query-objects-by-date-range-in-django)

I've got a field in one model like:Now, I need to filter the objects by a date range.How do I filter all the objects that have a date between 1-Jan-2011 and 31-Jan-2011?

2011-01-12 12:09:58Z

I've got a field in one model like:Now, I need to filter the objects by a date range.How do I filter all the objects that have a date between 1-Jan-2011 and 31-Jan-2011?Use Or if you are just trying to filter month wise: As Bernhard Vallant said, if you want a queryset which excludes the specified range ends you should consider his solution, which utilizes gt/lt (greater-than/less-than).You can use django's filter with datetime.date objects:When doing django ranges with a filter make sure you know the difference between using a date object vs a datetime object. __range is inclusive on dates but if you use a datetime object for the end date it will not include the entries for that day if the time is not set.returns all entries from startdate to enddate including entries on those dates. Bad example since this is returning entries a week into the future, but you get the drift.will be missing 24 hours worth of entries depending on what the time for the date fields is set to.You can get around the "impedance mismatch" caused by the lack of precision in the DateTimeField/date object comparison -- that can occur if using range -- by using a datetime.timedelta to add a day to last date in the range. This works like:As discussed previously, without doing something like this, records are ignored on the last day.Edited to avoid the use of datetime.combine -- seems more logical to stick with date instances when comparing against a DateTimeField, instead of messing about with throwaway (and confusing) datetime objects. See further explanation in comments below.Is simple, Works for meTo make it more flexible, you can design a FilterBackend as below:Still relevant today.

You can also do:

Convert sqlalchemy row object to python dict

Anurag Uniyal

[Convert sqlalchemy row object to python dict](https://stackoverflow.com/questions/1958219/convert-sqlalchemy-row-object-to-python-dict)

Is there a simple way to iterate over column name and value pairs?My version of sqlalchemy is 0.5.6Here is the sample code where I tried using dict(row), but it throws exception , TypeError: 'User' object is not iterableRunning this code on my system outputs:

2009-12-24 12:42:34Z

Is there a simple way to iterate over column name and value pairs?My version of sqlalchemy is 0.5.6Here is the sample code where I tried using dict(row), but it throws exception , TypeError: 'User' object is not iterableRunning this code on my system outputs:You may access the internal __dict__ of a SQLAlchemy object, like the following::I couldn't get a good answer so I use this:Edit: if above function is too long and not suited for some tastes here is a one liner (python 2.7+)As per @zzzeek in comments:In SQLAlchemy v0.8 and newer, use the inspection system.Note that .key is the attribute name, which can be different from the column name, e.g. in the following case:This method also works for column_property.rows have an _asdict() function which gives a dictas @balki mentioned:The _asdict() method can be used if you're querying a specific field because it is returned as a KeyedTuple.Whereas, if you do not specify a column you can use one of the other proposed methods - such as the one provided by @charlax. Note that this method is only valid for 2.7+.Old question, but since this the first result for "sqlalchemy row to dict" in Google it deserves a better answer.The RowProxy object that SqlAlchemy returns has the items() method:

http://docs.sqlalchemy.org/en/latest/core/connections.html#sqlalchemy.engine.RowProxy.itemsIt simply returns a list of (key, value) tuples. So one can convert a row to dict using the following:In Python <= 2.6:In Python >= 2.7:Following @balki answer, since SQLAlchemy 0.8 you can use _asdict(), available for KeyedTuple objects. This renders a pretty straightforward answer to the original question. Just, change in your example the last two lines (the for loop) for this one:This works because in the above code u is an object of type class KeyedTuple, since .all() returns a list of KeyedTuple. Therefore it has the method _asdict(), which nicely returns u as a dictionary.WRT the answer by @STB: AFAIK, anithong that .all() returns is a list of KeypedTuple. Therefore, the above works either if you specify a column or not, as long as you are dealing with the result of .all() as applied to a Query object.Assuming the following functions will be added to the class User the following will return all key-value pairs of all columns:unlike the other answers all but only those attributes of the object are returned which are Column attributes at class level of the object. Therefore no _sa_instance_state or any other attribute SQLalchemy or you add to the object are included. ReferenceEDIT: Forget to say, that this also works on inherited Columns.If you also want to include hybrid_property attributes the following will work:I assume here that you mark Columns with an beginning _ to indicate that you want to hide them, either because you access the attribute by an hybrid_property or you simply do not want to show them. ReferenceTipp all_orm_descriptors also returns hybrid_method and AssociationProxy if you also want to include them.Every answer (like 1, 2 )  which based on the __dict__ attribute simply returns all attributes of the object. This could be much more attributes then you want. Like I sad this includes _sa_instance_state or any other attribute you define on this object.Every answer (like 1, 2 ) which is based on the dict() function only works on SQLalchemy row objects returned by session.execute() not on the classes you define to work with, like the class User from the question.The solving answer which is based on row.__table__.columns will definitely not work. row.__table__.columns contains the column names of the SQL Database. These can only be equal to the attributes name of the python object. If not you get an AttributeError.

For answers (like 1, 2 ) based on class_mapper(obj.__class__).mapped_table.c it is the same.The expression you are iterating through evaluates to list of model objects, not rows. So the following is correct usage of them:Do you realy need to convert them to dicts? Sure, there is a lot of ways, but then you don't need ORM part of SQLAlchemy:Update: Take a look at sqlalchemy.orm.attributes module. It has a set of functions to work with object state, that might be useful for you, especially instance_dict().Refer to Alex Brasetvik's Answer, you can use one line of code to solve the problemUnder the comment section of Alex Brasetvik's Answer, zzzeek the creator of SQLAlchemy stated this is the "Correct Method" for the problem.I've found this post because I was looking for a way to convert a SQLAlchemy row into a dict. I'm using SqlSoup... but the answer was built by myself, so, if it could helps someone here's my two cents:You could try to do it in this way.It use a built-in method in the query object that return a dictonary object of the query object. references: https://docs.sqlalchemy.org/en/latest/orm/query.htmlYou can convert sqlalchemy object to dictionary like this and return it as json/dictionary.Helper functions:Driver Function:Two ways:1.2.The docs offer a very simple solution: ResultRow._asdict()Here is how Elixir does it. The value of this solution is that it allows recursively including the dictionary representation of relations.With this code you can also to add to your query "filter" or "join" and this work!That should work.I have a variation on Marco Mariani's answer, expressed as a decorator. The main difference is that it'll handle lists of entities, as well as safely ignoring some other types of return values (which is very useful when writing tests using mocks):To complete @Anurag Uniyal 's answer, here is a method that will recursively follow relationships:I am a newly minted Python programmer and ran into problems getting to JSON with Joined tables.  Using information from the answers here I built a function to return reasonable results to JSON where the table names are included avoiding having to alias, or have fields collide.Simply pass the result of a session query:test = Session().query(VMInfo, Customer).join(Customer).order_by(VMInfo.vm_name).limit(50).offset(10)json = sqlAl2json(test)if your models table column is not equie mysql column.such as :Need to use:if you use this way you can get modify_time and create_time both are NoneBecause Class Attributes name not equal with column store in mysqlReturn the contents of this :class:.KeyedTuple as a dictionaryFor the sake of everyone and myself, here is how I use it:This function might help. 

I can't find better solution to solve problem when attribute name is different then column names.You'll need it everywhere in your project, I apriciate @anurag answered it works fine. till this point I was using it, but it'll mess all your code and also wont work with entity change.Rather try this,

inherit your base query class in SQLAlchemyafter that wherever you'll define your object "as_dict" method will be there.Python 3.6.8+The builtin str() method automatically converts datetime.datetime objects to iso-8806-1.NOTE: The default func will only be applied to a value if there's an error so int and float values won't be converted... unless there's an error :).With python 3.7+, we can do this with dataclass, and the asdict method that comes with it:The key is to use the @dataclass decorator, and annotate each column with its type (the : str part of the name: str = Column(String) line).   Also note that since the email is not annotated, it is not included in query_result_dict.

Test if a variable is a list or tuple

interstar

[Test if a variable is a list or tuple](https://stackoverflow.com/questions/2184955/test-if-a-variable-is-a-list-or-tuple)

In python, what's the best way to test if a variable contains a list or a tuple? (ie. a collection)Is isinstance() as evil as suggested here? http://www.canonical.org/~kragen/isinstance/Update: the most common reason I want to distinguish a list from a string is when I have some indefinitely deep nested tree / data-structure of lists of lists of lists of strings etc. which I'm exploring with a recursive algorithm and I need to know when I've hit the "leaf" nodes.

2010-02-02 15:00:23Z

In python, what's the best way to test if a variable contains a list or a tuple? (ie. a collection)Is isinstance() as evil as suggested here? http://www.canonical.org/~kragen/isinstance/Update: the most common reason I want to distinguish a list from a string is when I have some indefinitely deep nested tree / data-structure of lists of lists of lists of strings etc. which I'm exploring with a recursive algorithm and I need to know when I've hit the "leaf" nodes.Go ahead and use isinstance if you need it.  It is somewhat evil, as it excludes custom sequences, iterators, and other things that you might actually need.  However, sometimes you need to behave differently if someone, for instance, passes a string.  My preference there would be to explicitly check for str or unicode like so:N.B. Don't mistake types.StringType for types.StringTypes.  The latter incorporates str and unicode objects.The types module is considered by many to be obsolete in favor of just checking directly against the object's type, so if you'd rather not use the above, you can alternatively check explicitly against str and unicode, like this:Edit:Better still is:End editAfter either of these, you can fall back to behaving as if you're getting a normal sequence, letting non-sequences raise appropriate exceptions.See the thing that's "evil" about type checking is not that you might want to behave differently for a certain type of object, it's that you artificially restrict your function from doing the right thing with unexpected object types that would otherwise do the right thing.  If you have a final fallback that is not type-checked, you remove this restriction.  It should be noted that too much type checking is a code smell that indicates that you might want to do some refactoring, but that doesn't necessarily mean you should avoid it from the getgo.There's nothing wrong with using isinstance as long as it's not redundant. If a variable should only be a list/tuple then document the interface and just use it as such. Otherwise a check is perfectly reasonable:This type of check does have some good use-cases, such as with the standard string startswith / endswith methods (although to be accurate these are implemented in C in CPython using an explicit check to see if it's a tuple - there's more than one way to solve this problem, as mentioned in the article you link to).An explicit check is often better than trying to use the object as a container and handling the exception - that can cause all sorts of problems with code being run partially or unnecessarily.Document the argument as needing to be a sequence, and use it as a sequence. Don't check the type.How about: hasattr(a, "__iter__") ?It tells if the object returned can be iterated over as a generator.  By default, tuples and lists can, but not the string types.On Python 2.8 type(list) is list returns false  

I would suggest comparing the type in this horrible way:(well at least on my system, using anaconda on Mac OS X Yosemite)Python uses "Duck typing", i.e. if a variable kwaks like a duck, it must be a duck. In your case, you probably want it to be iterable, or you want to access the item at a certain index. You should just do this: i.e. use the object in for var: or var[idx] inside a try block, and if you get an exception it wasn't a duck...If you just need to know if you can use the foo[123] notation with the variable, you can check for the existence of a __getitem__ attribute (which is what python calls when you access by index) with hasattr(foo, '__getitem__')Another easy way to find out if a variable is either list or tuple or generally check variable type would be :In principle, I agree with Ignacio, above, but you can also use type to check if something is a tuple or a list.Has to be more complex test if you really want to handle just about anything as function argument.Although, usually it's enough to just spell out that a function expects iterable  and then check only type(a) != type(''). Also it may happen that for a string you have a simple processing path or you are going to be nice and do a split etc., so you don't want to yell at strings and if someone sends you something weird, just let him have an exception.

How to sort a dataFrame in python pandas by two or more columns?

Rakesh Adhikesavan

[How to sort a dataFrame in python pandas by two or more columns?](https://stackoverflow.com/questions/17141558/how-to-sort-a-dataframe-in-python-pandas-by-two-or-more-columns)

Suppose I have a dataframe with columns a, b and c, I want to sort the dataframe by column b in ascending order, and by column c in descending order, how do I do this?

2013-06-17 06:28:47Z

Suppose I have a dataframe with columns a, b and c, I want to sort the dataframe by column b in ascending order, and by column c in descending order, how do I do this?As of the 0.17.0 release, the sort method was deprecated in favor of sort_values.  sort was completely removed in the 0.20.0 release. The arguments (and results) remain the same:You can use the ascending argument of sort:For example:As commented by @renadeenthat is, if you want to reuse df1 as a sorted DataFrame:orAs of pandas 0.17.0, DataFrame.sort() is deprecated, and set to be removed in a future version of pandas. The way to sort a dataframe by its values is now is DataFrame.sort_valuesAs such, the answer to your question would now beFor large dataframes of numeric data, you may see a significant performance improvement via numpy.lexsort, which performs an indirect sort using a sequence of keys:One peculiarity is that the defined sorting order with numpy.lexsort is reversed: (-'b', 'a') sorts by series a first. We negate series b to reflect we want this series in descending order.Be aware that np.lexsort only sorts with numeric values, while pd.DataFrame.sort_values works with either string or numeric values. Using np.lexsort with strings will give: TypeError: bad operand type for unary -: 'str'.

how to concatenate two dictionaries to create a new one in Python? [duplicate]

timy

[how to concatenate two dictionaries to create a new one in Python? [duplicate]](https://stackoverflow.com/questions/1781571/how-to-concatenate-two-dictionaries-to-create-a-new-one-in-python)

Say I have three dicts How do I create a new d4 that combines these three dictionaries? i.e.:

2009-11-23 07:23:32Z

Say I have three dicts How do I create a new d4 that combines these three dictionaries? i.e.:I recommend approach (2), and I particularly recommend avoiding (1) (which also takes up O(N) extra auxiliary memory for the concatenated list of items temporary data structure).alternatively (and supposedly faster):Previous SO question that both of these answers came from is here.You can use the update() method to build a new dictionary containing all the items:Or, in a loop:Use the dict constructorAs a functionThe overhead of creating intermediate dictionaries can be eliminated by using thedict.update() method:Here's a one-liner (imports don't count :) that can easily be generalized to concatenate N dictionaries:Output:Generalized to concatenate N dicts:and:I'm a little late to this party, I know, but I hope this helps someone.

How to get method parameter names?

Staale

[How to get method parameter names?](https://stackoverflow.com/questions/218616/how-to-get-method-parameter-names)

Given the Python function:How can I extract the number and names of the arguments. I.e., given that I have a reference to func, I want the func.[something] to return ("arg1", "arg2").The usage scenario for this is that I have a decorator, and I wish to use the method arguments in the same order that they appear for the actual function as a key. I.e., how would the decorator look that printed "a,b" when I call a_method("a", "b")?

2008-10-20 14:22:02Z

Given the Python function:How can I extract the number and names of the arguments. I.e., given that I have a reference to func, I want the func.[something] to return ("arg1", "arg2").The usage scenario for this is that I have a decorator, and I wish to use the method arguments in the same order that they appear for the actual function as a key. I.e., how would the decorator look that printed "a,b" when I call a_method("a", "b")?Take a look at the inspect module - this will do the inspection of the various code object properties for you.The other results are the name of the *args and **kwargs variables, and the defaults provided.  ie.Note that some callables may not be introspectable in certain implementations of Python. For Example, in CPython, some built-in functions defined in C provide no metadata about their arguments. As a result, you will get a ValueError if you use inspect.getfullargspec() on a built-in function.Since Python 3.3, you can use inspect.signature() to see the call signature of a callable object:In CPython, the number of arguments isand their names are in the beginning ofThese are implementation details of CPython, so this probably does not work in other implementations of Python, such as IronPython and Jython.One portable way to admit "pass-through" arguments is to define your function with the signature func(*args, **kwargs). This is used a lot in e.g. matplotlib, where the outer API layer passes lots of keyword arguments to the lower-level API.In a decorator method, you can list arguments of the original method in this way:If the **kwargs are important for you, then it will be a bit complicated:Example:I think what you're looking for is the locals method - The Python 3 version is:The method returns a dictionary containing both args and kwargs.Here is something I think will work for what you want, using a decorator.Run it, it will yield the following output:Python 3.5+:So previously:Now:To test:Given that we have function 'function' which takes argument 'arg', this will evaluate as True, otherwise as False.Example from the Python console:In Python 3.+ with the Signature object at hand, an easy way to get a mapping between argument names to values, is using the Signature's bind() method!For example, here is a decorator for printing a map like that:Here is another way to get the function parameters without using any module.Output:Returns a list of argument names, takes care of partials and regular functions:Update for Brian's answer:If a function in Python 3 has keyword-only arguments, then you need to use inspect.getfullargspec:yields this:In python 3, below is to make *args and **kwargs into a dict (use OrderedDict for python < 3.6 to maintain dict orders):To update a little bit Brian's answer, there is now a nice backport of inspect.signature that you can use in older python versions: funcsigs.

So my personal preference would go forFor fun, if you're interested in playing with Signature objects and even creating functions with random signatures dynamically you can have a look at my makefun project.inspect.signature is very slow. Fastest way is What about dir() and vars() now?Seems doing exactly what is being asked super simply…Must be called from within the function scope.But be wary that it will return all local variables so be sure to do it at the very beginning of the function if needed.Also note that, as pointed out in the comments, this doesn't allow it to be done from outside the scope. So not exactly OP's scenario but still matches the question title. Hence my answer.

Django: Display Choice Value

Shankze

[Django: Display Choice Value](https://stackoverflow.com/questions/4320679/django-display-choice-value)

models.py:views.py:On the template, when I call person.gender, I get 'M' or 'F' instead of 'Male' or 'Female'.How to display the value ('Male' or 'Female') instead of the code ('M'/'F')?

2010-12-01 02:25:39Z

models.py:views.py:On the template, when I call person.gender, I get 'M' or 'F' instead of 'Male' or 'Female'.How to display the value ('Male' or 'Female') instead of the code ('M'/'F')?It looks like you were on the right track - get_FOO_display() is most certainly what you want:In templates, you don't include () in the name of a method. Do the following:In ViewsIn TemplateDocumentation of get_FOO_display()

How to make inline plots in Jupyter Notebook larger?

donlan

[How to make inline plots in Jupyter Notebook larger?](https://stackoverflow.com/questions/36367986/how-to-make-inline-plots-in-jupyter-notebook-larger)

I have made my plots inline on my Ipython Notebook with "%matplotlib inline."Now, the plot appears.  However, it is very small.  Is there a way to make it appear larger using either notebook settings or plot settings?

2016-04-02 00:51:37Z

I have made my plots inline on my Ipython Notebook with "%matplotlib inline."Now, the plot appears.  However, it is very small.  Is there a way to make it appear larger using either notebook settings or plot settings?Yes, play with figuresize like so (before you call your subplot):The default figure size (in inches) is controlled byFor example:creates a figure with 10 (width) x 5 (height) inchesI have found that %matplotlib notebook works better for me than inline with Jupyter notebooks.Note that you may need to restart the kernel if you were using %matplotlib inline before.Update 2019:

If you are running Jupyter Lab you might want to use

%matplotlib widgetIf you only want the image of your figure to appear larger without changing the general appearance of your figure increase the figure resolution. Changing the figure size as suggested in most other answers will change the appearance since font sizes do not scale accordingly.The question is about matplotlib, but for the sake of any R users that end up here given the language-agnostic title:If you're using an R kernel, just use:To adjust the size of one figure:To change the default settings, and therefore all your plots:A small but important detail for adjusting figure size on a one-off basis (as several commenters above reported "this doesn't work for me"):You should do plt.figure(figsize=(,)) PRIOR to defining your actual plot.  For example:This should correctly size the plot according to your specified figsize:Whereas this will show the plot with the default settings, seeming to "ignore" figsize:

What is the difference between ndarray and array in numpy?

flxb

[What is the difference between ndarray and array in numpy?](https://stackoverflow.com/questions/15879315/what-is-the-difference-between-ndarray-and-array-in-numpy)

What is the difference between ndarray and array in Numpy? And where can I find the implementations in the numpy source code?

2013-04-08 12:41:55Z

What is the difference between ndarray and array in Numpy? And where can I find the implementations in the numpy source code?numpy.array is just a convenience function to create an ndarray; it is not a class itself.  You can also create an array using numpy.ndarray, but it is not the recommended way.  From the docstring of numpy.ndarray:  Most of the meat of the implementation is in C code, here in multiarray, but you can start looking at the ndarray interfaces here:https://github.com/numpy/numpy/blob/master/numpy/core/numeric.pynumpy.array is a function that returns a numpy.ndarray.  There is no object type numpy.array.Just a few lines of example code to show the difference between numpy.array and numpy.ndarrayWarm up step: Construct a listCheck the type You will getConstruct an array (from a list) using np.arrayOr, you can skip the warm up step, directly have Check the type You will getwhich tells you the type of the numpy array is numpy.ndarrayYou can also check the type by and you will get Either of the following two lines will give you an error messagenumpy.ndarray() is a class, while numpy.array() is a method / function to create ndarray. In numpy docs if you want to create an array from ndarray class you can do it with 2 ways as quoted:1- using array(), zeros() or empty() methods:

Arrays should be constructed using array, zeros or empty (refer to the See Also section below). The parameters given here refer to a low-level method (ndarray(…)) for instantiating an array.2- from ndarray class directly:

There are two modes of creating an array using __new__:

    If buffer is None, then only shape, dtype, and order are used.

    If buffer is an object exposing the buffer interface, then all keywords are interpreted.The example below gives a random array because we didn't assign buffer value:another example is to assign array object to the buffer

example:from above example we notice that we can't assign a list to "buffer" and we had to use numpy.array() to return ndarray object for the bufferConclusion: use numpy.array() if you want to make a numpy.ndarray() object"I think with np.array() you can only create C like though you mention the order, when you check using np.isfortran() it says false. but with np.ndarrray() when you specify the order it creates based on the order provided. 

How should I log while using multiprocessing in Python?

cdleary

[How should I log while using multiprocessing in Python?](https://stackoverflow.com/questions/641420/how-should-i-log-while-using-multiprocessing-in-python)

Right now I have a central module in a framework that spawns multiple processes using the Python 2.6 multiprocessing module. Because it uses multiprocessing, there is module-level multiprocessing-aware log, LOG = multiprocessing.get_logger(). Per the docs, this logger has process-shared locks so that you don't garble things up in sys.stderr (or whatever filehandle) by having multiple processes writing to it simultaneously.The issue I have now is that the other modules in the framework are not multiprocessing-aware. The way I see it, I need to make all dependencies on this central module use multiprocessing-aware logging. That's annoying within the framework, let alone for all clients of the framework. Are there alternatives I'm not thinking of?

2009-03-13 04:02:31Z

Right now I have a central module in a framework that spawns multiple processes using the Python 2.6 multiprocessing module. Because it uses multiprocessing, there is module-level multiprocessing-aware log, LOG = multiprocessing.get_logger(). Per the docs, this logger has process-shared locks so that you don't garble things up in sys.stderr (or whatever filehandle) by having multiple processes writing to it simultaneously.The issue I have now is that the other modules in the framework are not multiprocessing-aware. The way I see it, I need to make all dependencies on this central module use multiprocessing-aware logging. That's annoying within the framework, let alone for all clients of the framework. Are there alternatives I'm not thinking of?The only way to deal with this non-intrusively is to:I just now wrote a log handler of my own that just feeds everything to the parent process via a pipe.  I've only been testing it for ten minutes but it seems to work pretty well. (Note: This is hardcoded to RotatingFileHandler, which is my own use case.)This now uses a queue for correct handling of concurrency, and also recovers from errors correctly.  I've now been using this in production for several months, and the current version below works without issue.QueueHandler is native in Python 3.2+, and does exactly this. It is easily replicated in previous versions.Python docs have two complete examples: Logging to a single file from multiple processesFor those using Python < 3.2, just copy QueueHandler into your own code from: https://gist.github.com/vsajip/591589 or alternatively import logutils.Each process (including the parent process) puts its logging on the Queue, and then a listener thread or process (one example is provided for each) picks those up and writes them all to a file - no risk of corruption or garbling.Yet another alternative might be the various non-file-based logging handlers in the logging package: (and others)This way, you could easily have a logging daemon somewhere that you could write to safely and would handle the results correctly. (E.g., a simple socket server that just unpickles the message and emits it to its own rotating file handler.)The SyslogHandler would take care of this for you, too. Of course, you could use your own instance of syslog, not the system one.Below is another solution with a focus on simplicity for anyone else (like me) who get here from Google.  Logging should be easy!  Only for 3.2 or higher.A variant of the others that keeps the logging and queue thread separate.All current solutions are too coupled to the logging configuration by using a handler. My solution has the following architecture and features:Code with usage example and output can be found at the following Gist: https://gist.github.com/schlamar/7003737Since we can represent multiprocess logging as many publishers and one subscriber (listener), using ZeroMQ to implement PUB-SUB messaging is indeed an option. Moreover, PyZMQ module, the Python bindings for ZMQ, implements PUBHandler, which is object for publishing logging messages over a zmq.PUB socket.There's a solution on the web, for centralized logging from distributed application using PyZMQ and PUBHandler, which can be easily adopted for working locally with multiple publishing processes.I also like zzzeek's answer but Andre is correct that a queue is required to prevent garbling. I had some luck with the pipe, but did see garbling which is somewhat expected. Implementing it turned out to be harder than I thought, particularly due to running on Windows, where there are some additional restrictions about global variables and stuff (see: How's Python Multiprocessing Implemented on Windows?)But, I finally got it working. This example probably isn't perfect, so comments and suggestions are welcome. It also does not support setting the formatter or anything other than the root logger. Basically, you have to reinit the logger in each of the pool processes with the queue and set up the other attributes on the logger.Again, any suggestions on how to make the code better are welcome. I certainly don't know all the Python tricks yet :-)just publish somewhere your instance of the logger. that way, the other modules and clients can use your API to get the logger without having to import multiprocessing.I liked zzzeek's answer. I would just substitute the Pipe for a Queue since if multiple threads/processes use the same pipe end to generate log messages they will get garbled.How about delegating all the logging to another process that reads all log entries from a Queue?Simply share LOG_QUEUE via any of the multiprocess mechanisms or even inheritance and it all works out fine!I have a solution that's similar to ironhacker's except that I use logging.exception in some of my code and found that I needed to format the exception before passing it back over the Queue since tracebacks aren't pickle'able:Below is a class that can be used in Windows environment, requires ActivePython.

You can also inherit for other logging handlers (StreamHandler etc.)And here is an example that demonstrates usage:Here's my simple hack/workaround... not the most comprehensive, but easily modifiable and simpler to read and understand I think than any other answers I found before writing this:There is this great packagePackage:

https://pypi.python.org/pypi/multiprocessing-logging/code:

https://github.com/jruere/multiprocessing-loggingInstall:Then add:One of the alternatives is to write the mutliprocessing logging to a known file and register an atexit handler to join on those processes read it back on stderr; however, you won't get a real-time flow to the output messages on stderr that way.If you have deadlocks occurring in a combination of locks, threads and forks in the logging module, that is reported in bug report 6721 (see also related SO question).There is a small fixup solution posted here.However, that will just fix any potential deadlocks in logging. That will not fix that things are maybe garbled up. See the other answers presented here.Simplest idea as mentioned:For whoever might need this, I wrote a decorator for multiprocessing_logging package that adds the current process name to logs, so it becomes clear who logs what.It also runs install_mp_handler() so it becomes unuseful to run it before creating a pool.This allows me to see which worker creates which logs messages.Here's the blueprint with an example:To my children who meet the same issue in decades and found this question on this site I leave this answer.Simplicity vs overcomplicating. Just use other tools. Python is awesome, but it was not designed to do some things.The following snippet for logrotate daemon works for me and does not overcomplicate things. Schedule it to run hourly and This is how I install it (symlinks do not work for logrotate):

Reusable library to get human readable version of file size?

Sridhar Ratnakumar

[Reusable library to get human readable version of file size?](https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size)

There are various snippets on the web that would give you a function to return human readable size from bytes size:But is there a Python library that provides this?

2009-07-07 20:59:32Z

There are various snippets on the web that would give you a function to return human readable size from bytes size:But is there a Python library that provides this?Addressing the above "too small a task to require a library" issue by a straightforward implementation:Supports:Example:by Fred CireraA library that has all the functionality that it seems you're looking for is humanize.  humanize.naturalsize() seems to do everything you're looking for.Here's my version. It does not use a for-loop. It has constant complexity, O(1), and is in theory more efficient than the answers here that use a for-loop.To make it more clear what is going on, we can omit the code for the string formatting. Here are the lines that actually do the work:While I know this question is ancient, I recently came up with a version that avoids loops, using log2 to determine the size order which doubles as a shift and an index into the suffix list:Could well be considered unpythonic for its readability, though :)The following works in Python 3.6+, is, in my opinion, the easiest to understand answer on here, and lets you customize the amount of decimal places used.If you're using Django installed you can also try filesizeformat:There's always got to be one of those guys. Well today it's me. Here's a one-liner solution -- or two lines if you count the function signature. One such library is hurry.filesize.Using either powers of 1000 or kibibytes would be more standard-friendly:P.S. Never trust a library that prints thousands with the K (uppercase) suffix :)Riffing on the snippet provided as an alternative to hurry.filesize(), here is a snippet that gives varying precision numbers based on the prefix used. It isn't as terse as some snippets, but I like the results.This will do what you need in almost any situation, is customizable with optional arguments, and as you can see, is pretty much self-documenting:Example output:Advanced customizations:This code is both Python 2 and Python 3 compatible. PEP8 compliance is an exercise for the reader. Remember, it's the output that's pretty.Update: If you need thousands commas, just apply the obvious extension:For example:You should use "humanize".Drawing from all the previous answers, here is my take on it. It's an object which will store the file size in bytes as an integer. But when you try to print the object, you automatically get a human readable version.The HumanFriendly project helps with this.The above code will give 1KB as answer.

Examples can be found here.I like the fixed precision of senderle's decimal version, so here's a sort of hybrid of that with joctee's answer above (did you know you could take logs with non-integer bases?):DiveIntoPython3 also talks about this function.Modern Django have self template tag filesizeformat:Formats the value like a human-readable file size (i.e. '13 KB', '4.1 MB', '102 bytes', etc.).For example:If value is 123456789, the output would be 117.7 MB.More info: https://docs.djangoproject.com/en/1.10/ref/templates/builtins/#filesizeformatHow about a simple 2 liner:Here is how it works under the hood:It however doesn't work if filesize is 0 or negative (because log is undefined for 0 and -ve numbers). You can add extra checks for them:Examples:NOTE - There is a difference between Kb and KiB. KB means 1000 bytes, whereas KiB means 1024 bytes. KB,MB,GB are all multiples of 1000, whereas KiB, MiB, GiB etc are all multiples of 1024. More about it here What you're about to find below is by no means the most performant or shortest solution among the ones already posted. Instead, it focuses on one particular issue that many of the other answers miss.Namely the case when input like 999_995 is given:which, being truncated to the nearest integer and applied back to the input givesThis seems to be exactly what we'd expect until we're required to control output precision. And this is when things start to get a bit difficult.With the precision set to 2 digits we get:instead of 1M.How can we counter that?Of course, we can check for it explicitly:But can we do better? Can we get to know which way the order should be cut before we do the final step?It turns out we can.Assuming 0.5 decimal rounding rule, the above if condition translates into:resulting ingivingrefer Sridhar Ratnakumar's answer, updated to:and example output is:This solution might also appeal to you, depending on how your mind works:

numpy: most efficient frequency counts for unique values in an array

Abe

[numpy: most efficient frequency counts for unique values in an array](https://stackoverflow.com/questions/10741346/numpy-most-efficient-frequency-counts-for-unique-values-in-an-array)

In numpy / scipy, is there an efficient way to get frequency counts for unique values in an array?Something along these lines:( For you, R users out there, I'm basically looking for the table() function )

2012-05-24 16:15:18Z

In numpy / scipy, is there an efficient way to get frequency counts for unique values in an array?Something along these lines:( For you, R users out there, I'm basically looking for the table() function )Take a look at np.bincount:http://docs.scipy.org/doc/numpy/reference/generated/numpy.bincount.htmlAnd then:or:or however you want to combine the counts and the unique values.As of Numpy 1.9, the easiest and fastest method is to simply use numpy.unique, which now has a return_counts keyword argument:Which gives:A quick comparison with scipy.stats.itemfreq:Update: The method mentioned in the original answer is deprecated, we should use the new way instead:Original answer:you can use scipy.stats.itemfreqI was also interested in this, so I did a little performance comparison (using perfplot, a pet project of mine). Result:is by far the fastest. (Note the log-scaling.)Code to generate the plot:Using pandas module:This is by far the most general and performant solution; surprised it hasn't been posted yet.Unlike the currently accepted answer, it works on any datatype that is sortable (not just positive ints), and it has optimal performance; the only significant expense is in the sorting done by np.unique.numpy.bincount is the probably the best choice. If your array contains anything besides small dense integers it might be useful to wrap it something like this:For example:Even though it has already been answered, I suggest a different approach that makes use of numpy.histogram. Such function given a sequence it returns the frequency of its elements grouped in bins.Beware though: it works in this example because numbers are integers. If they where real numbers, then this solution would not apply as nicely.This gives you:

{1: 5, 2: 3, 5: 1, 25: 1}To count unique non-integers - similar to Eelco Hoogendoorn's answer but considerably faster (factor of 5 on my machine), I used weave.inline to combine numpy.unique with a bit of c-code; Profile infoEelco's pure numpy version: NoteThere's redundancy here (unique performs a sort also), meaning that the code could probably be further optimized by putting the unique functionality inside the c-code loop. Old question, but I'd like to provide my own solution which turn out to be the fastest, use normal list instead of np.array as input (or transfer to list firstly), based on my bench test.Check it out if you encounter it as well.For example, 100000 loops, best of 3: 2.26 µs per loop100000 loops, best of 3: 8.8 µs per loop100000 loops, best of 3: 5.85 µs per loopWhile the accepted answer would be slower, and the scipy.stats.itemfreq solution is even worse.A more indepth testing did not confirm the formulated expectation.Ref. comments below on cache and other in-RAM side-effects that influence a small dataset massively repetitive testing results.some thing like this should do it:Also, this previous post on  Efficiently counting unique elements  seems pretty similar to your question, unless I'm missing something.multi-dimentional frequency count, i.e. counting arrays.from collections import Counter

Counter(x)

How can I format a decimal to always show 2 decimal places?

orokusaki

[How can I format a decimal to always show 2 decimal places?](https://stackoverflow.com/questions/1995615/how-can-i-format-a-decimal-to-always-show-2-decimal-places)

I want to display:49 as 49.00and:54.9 as 54.90Regardless of the length of the decimal or whether there are are any decimal places, I would like to display a Decimal with 2 decimal places, and I'd like to do it in an efficient way. The purpose is to display money values.eg, 4898489.00

2010-01-03 17:30:44Z

I want to display:49 as 49.00and:54.9 as 54.90Regardless of the length of the decimal or whether there are are any decimal places, I would like to display a Decimal with 2 decimal places, and I'd like to do it in an efficient way. The purpose is to display money values.eg, 4898489.00I suppose you're probably using the Decimal() objects from the decimal module? (If you need exactly two digits of precision beyond the decimal point with arbitrarily large numbers, you definitely should be, and that's what your question's title suggests...)If so, the Decimal FAQ section of the docs has a question/answer pair which may be useful for you:The next question readsIf you need the answer to that (along with lots of other useful information), see the aforementioned section of the docs. Also, if you keep your Decimals with two digits of precision beyond the decimal point (meaning as much precision as is necessary to keep all digits to the left of the decimal point and two to the right of it and no more...), then converting them to strings with str will work fine:You should use the new format specifications to define how your value should be represented:The documentation can be a bit obtuse at times, so I recommend the following, easier readable references:Python 3.6 introduced literal string interpolation (also known as f-strings) so now you can write the above even more succinct as:The String Formatting Operations section of the Python documentation contains the answer you're looking for.  In short:Some examples:You can use the string formatting operator as so:I'm not sure what you mean by "efficient" -- this is almost certainly not the bottleneck of your application.  If your program is running slowly, profile it first to find the hot spots, and then optimize those.You can change 2 in 2f to any number of decimal points you want to show.From Python3.6, this translates to:.format is a more readable way to handle variable formatting:In python 3, a way of doing this would beif you have multiple parameters you can useThe Easiest way example to show you how to do that is :Code :>>> points = 19.5

>>> total = 22

>>>'Correct answers: {:.2%}'.format(points/total)

`Output : Correct answers: 88.64%If you're using this for currency, and also want the value to be seperated by ,'s you can use $ {:,.f2}.format(currency_value).e.g.:currency_value = 1234.50$ {:,.f2}.format(currency_value) --> $ 1,234.50Here is a bit of code I wrote some time ago:print("> At the end of year  " + year_string + "  total paid is \t$ {:,.2f}".format(total_paid))what about 

How to install both Python 2.x and Python 3.x in Windows

dln385

[How to install both Python 2.x and Python 3.x in Windows](https://stackoverflow.com/questions/3809314/how-to-install-both-python-2-x-and-python-3-x-in-windows)

I do most of my programming in Python 3.x on Windows 7, but now I need to use the Python Imaging Library (PIL), ImageMagick, and wxPython, all of which require Python 2.x.Can I have both Python 2.x and Python 3.x installed in Windows 7? When I run a script, how would I "choose" which version of Python should run it? Will the aforementioned programs be able to handle multiple versions of Python installed at once? I have searched for hours and hours for how to do this to no avail.Thanks.

2010-09-28 02:40:30Z

I do most of my programming in Python 3.x on Windows 7, but now I need to use the Python Imaging Library (PIL), ImageMagick, and wxPython, all of which require Python 2.x.Can I have both Python 2.x and Python 3.x installed in Windows 7? When I run a script, how would I "choose" which version of Python should run it? Will the aforementioned programs be able to handle multiple versions of Python installed at once? I have searched for hours and hours for how to do this to no avail.Thanks.I found that the formal way to do this is as follows:Just install two (or more, using their installers) versions of Python on Windows 7 (for me work with 3.3 and 2.7). Follow the instuctions below, changing the parameters for your needs.Create the following environment variable (to default on double click):To launch a script in a particular interpreter, add the following shebang (beginning of script):To execute a script using a specific interpreter, use the following prompt command:To launch a specific interpreter:To launch the default interpreter (defined by the PY_PYTHON variable):ResourcesDocumentation: Using Python on WindowsPEP 397 - Python launcher for WindowsWhat I did was download both 2.7.6 and 3.3.4. Python 3.3.4 has the option to add the path to it in the environment variable so that was done. So basically I just manually added Python 2.7.6.How to...Note: (so as not to break pip commands in step 4 and 5, keep copy of python.exe in the same directory as the renamed file)I have multiple versions in windows.

I just change the exe name of the version I'm not defaulting to.As for package installers, most exe installers allow you to choose the python install to add the package too.

For manual installation check out the --prefix option to define where the package should be installed:http://docs.python.org/install/index.html#alternate-installation-windows-the-prefix-schemeIf you use Anaconda Python, you can easily install various environments.Say you had Anaconda Python 2.7 installed and you wanted a python 3.4 environment:Then to activate the environment:And to deactive:(With Linux, you should use source activate py34.)Links:Download Anaconda PythonInstructions for environmentsTo install and run any version of Python in the same system follow my guide below. For example say you want to install Python 2.x and Python 3.x on the same Windows system. In my example I have Python 2.7.14 installed first and Python 3.5.3. This is how my PATH variable starts with: PATH=C:\Program Files\Microsoft MPI\Bin\;C:\Python27;C:\Program Files\Python_3.6\Scripts\;C:\Program Files\Python_3.6\;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Common Files\Intel\Shared... Note that Python 2.7 is first and Python 3.5 second. Now py -4 or py -5 etc. on my system outputs: Requested Python version (4) not installed or Requested Python version (5) not installed etc. Hopefully this is clear enough. Starting version 3.3 Windows version has Python launcher, please take a look at section 3.4. Python Launcher for WindowsHere's what you can do:Install cmder.

Open and use Cmder as you would with you cmd terminal.

Use the command alias to create command aliases.I did the following:And that's it! ;-)I actually just thought of an interesting solution. While Windows will not allow you to easily alias programs, you can instead create renamed batch files that will call the current program.Instead of renaming the executable which will break a lot of thing including pip, create the file python2.bat in the same directory as the python2.exe. Then add the following line:What does this archaic syntax mean? Well, it's a batch script, (Windows version of bash). %~dp0 gets the current directory and %* will just pass all the arguments to python that were passed to the script. Repeat for python3.batYou can also do the same for pip and other utilities, just replace the word python in the file with pip or whathever the filename. The alias will be whatever the file is named.Best of all, when added to the PATH, Windows ignores the extension so runningWill launch the python3 version and and the command python2 will launch the python2 version.BTW, this is the same technique Spyder uses to add itself to the path on Windows. :)You can install multiple versions of Python one machine, and during setup, you can choose to have one of them associate itself with Python file extensions. If you install modules, there will be different setup packages for different versions, or you can choose which version you want to target. Since they generally install themselves into the site-packages directory of the interpreter version, there shouldn't be any conflicts (but I haven't tested this). To choose which version of python, you would have to manually specify the path to the interpreter if it is not the default one. As far as I know, they would share the same PATH and PYTHONPATH variables, which may be a problem.Note: I run Windows XP. I have no idea if any of this changes for other versions, but I don't see any reason that it would.What I have done on my own windows computer where I have Python 2.7 and Python 3.4 installed is I wrote a simple .bat file in the same directory as my Python.exe files. They look something like,The %* allows you to add arguments (Python files) afterwards. I believe /k keeps the prompt open after it finishes running the script. Then I save that as python27.bat Then I go to my Python 3 directory and make a bat file there. Now in my command line I can writeOrAnd they will run in their respective versions of Python. Make sure that c:\python27 and c:\python34 are in your environment variables.I got my answer from hereI did this in three steps by following the instructions here: This is all taken directly from here: http://ipython.readthedocs.io/en/stable/install/kernel_install.html. I'm currently running Python 2.x on Windows 8 and have Anaconda 4.2.13 installed. 1) First install the latest version of python:2) Next activate python33) Install the kernel:If you have Python 3 installed and want to install 2, switch the 2 and the 3 above. When you open a new notebook, you can now choose between Python 2 or 3. Check your system environment variables after installing Python, python 3's directories should be first in your PATH variable, then python 2.Whichever path variable matches first is the one Windows uses.As always py -2 will launch python2 in this scenario.I have encountered that problem myself and I made my launchers in a .bat so you could choose the version you want to launch. The only problem is your .py must be in the python folder, but anyway here is the code:For Python2For Python3Save them as .bat and follow the instructions inside.Install the one you use most (3.3 in my case) over the top of the other. That'll force IDLE to use the one you want.Alternatively (from the python3.3 README):On Unix and Mac systems if you intend to install multiple versions of Python

using the same installation prefix (--prefix argument to the configure script)

you must take care that your primary python executable is not overwritten by the

installation of a different version.  All files and directories installed using

"make altinstall" contain the major and minor version and can thus live

side-by-side.  "make install" also creates ${prefix}/bin/python3 which refers to

${prefix}/bin/pythonX.Y.  If you intend to install multiple versions using the

same prefix you must decide which version (if any) is your "primary" version.

Install that version using "make install".  Install all other versions using

"make altinstall".For example, if you want to install Python 2.6, 2.7 and 3.3 with 2.7 being the

primary version, you would execute "make install" in your 2.7 build directory

and "make altinstall" in the others.I just had to install them. Then I used the free (and portable) soft at http://defaultprogramseditor.com/ under "File type settings"/"Context menu"/search:"py", chose .py file and added an 'open' command for the 2 IDLE by copying the existant command named 'open with IDLE, changing names to IDLE 3.4.1/2.7.8, and remplacing the files numbers of their respective versions in the program path. Now I have just to right click the .py file and chose which IDLE I want to use. Can do the same with direct interpreters if you prefer.Only Works if your running your code in your Python IDEI have both Python 2.7 and Python 3.3 installed on my windows operating system. If I try to launch a file, it will usually open up on the python 2.7 IDE. How I solved this issue, was when I choose to run my code on python 3.3, I open up python 3.3 IDLE(Python GUI), select file, open my file with the IDLE and save it. Then when I run my code, it runs to the IDLE that I currently opened it with. It works vice versa with 2.7.I have installed both python 2.7.13 and python 3.6.1 on windows 10pro and I was getting the same "Fatal error" when I tried pip2 or pip3. What I did to correct this was to go to the location of python.exe for python 2 and python 3 files and create a copy of each, I then renamed each copy to python2.exe and python3.exe depending on the python version in the installation folder. I therefore had in each python installation folder both a python.exe file and a python2.exe or python3.exe depending on the python version. This resolved my problem when I typed either pip2 or pip3.If you can't get anything else to work, open an interpreter in whichever version you choose (I prefer using iPython) and:This uses whichever python version you are currently operating under. Works fine for a single script, but will quickly get out of hand if there are lots of scripts you run, in which case you can always make a batch file with all of these calls inside. Not the most elegant answer, but it works.Is there a way to make aliases for different python version a la Linux?

How to make good reproducible pandas examples

Marius

[How to make good reproducible pandas examples](https://stackoverflow.com/questions/20109391/how-to-make-good-reproducible-pandas-examples)

Having spent a decent amount of time watching both the r and pandas tags on SO, the impression that I get is that pandas questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like this, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.How can we create good reproducible examples for pandas questions? Simple dataframes can be put together, e.g.:But many example datasets need more complicated structure, e.g.:For datasets that are hard to mock up using a few lines of code, is there an equivalent to R's dput() that allows you to generate copy-pasteable code to regenerate your datastructure?

2013-11-20 23:31:39Z

Having spent a decent amount of time watching both the r and pandas tags on SO, the impression that I get is that pandas questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like this, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.How can we create good reproducible examples for pandas questions? Simple dataframes can be put together, e.g.:But many example datasets need more complicated structure, e.g.:For datasets that are hard to mock up using a few lines of code, is there an equivalent to R's dput() that allows you to generate copy-pasteable code to regenerate your datastructure?Note: The ideas here are pretty generic for Stack Overflow, indeed questions.This is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code.After importing numpy and pandas, be sure to provide a random seed if you want folks to be able to exactly reproduce your data and results.Here's an example showing a variety of things you can do.  All kinds of useful sample dataframes could be created from a subset of this:This produces:Some notes:In addition to taking subsets of the above code, you can further combine the techniques to do just about anything.  For example, here's a short example that combines np.tile and date_range to create sample ticker data for 4 stocks covering the same dates:Now we have a sample dataset with 100 lines (25 dates per ticker), but we have only used 4 lines to do it, making it easy for everyone else to reproduce without copying and pasting 100 lines of code.  You can then display subsets of the data if it helps to explain your question:My best advice for asking questions would be to play on the psychology of the people who answer questions.  Being one of those people, I can give insight into why I answer certain questions and why I don't answer others.I'm motivated to answer questions for several reasonsAll my purest intentions are great and all, but I get that satisfaction if I answer 1 question or 30.  What drives my choices for which questions to answer has a huge component of point maximization.I'll also spend time on interesting problems but that is few and far between and doesn't help an asker who needs a solution to a non-interesting question.  Your best bet to get me to answer a question is to serve that question up on a platter ripe for me to answer it with as little effort as possible.  If I'm looking at two questions and one has code I can copy paste to create all the variables I need... I'm taking that one!  I'll come back to the other one if I have time, maybe.Make it easy for the people answering questions.I like points (I mentioned that above).  But those points aren't really really my reputation.  My real reputation is an amalgamation of what others on the site think of me.  I strive to be fair and honest and I hope others can see that.  What that means for an asker is, we remember the behaviors of askers.  If you don't select answers and upvote good answers, I remember.  If you behave in ways I don't like or in ways I do like, I remember.  This also plays into which questions I'll answer.Anyway, I can probably go on, but I'll spare all of you who actually read this.The Challenge One of the most challenging aspects of responding to SO questions is the time it takes to recreate the problem (including the data).  Questions which don't have a clear way to reproduce the data are less likely to be answered.  Given that you are taking the time to write a question and you have an issue that you'd like help with, you can easily help yourself by providing data that others can then use to help solve your problem.The instructions provided by @Andy for writing good Pandas questions are an excellent place to start.  For more information, refer to how to ask and how to create Minimal, Complete, and Verifiable examples.Please clearly state your question upfront.  After taking the time to write your question and any sample code, try to read it and provide an 'Executive Summary' for your reader which summarizes the problem and clearly states the question.Original question:Depending on the amount of data, sample code and error stacks provided, the reader needs to go a long way before understanding what the problem is.  Try restating your question so that the question itself is on top, and then provide the necessary details.Revised Question:PROVIDE SAMPLE DATA IF NEEDED!!!Sometimes just the head or tail of the DataFrame is all that is needed.  You can also use the methods proposed by @JohnE to create larger datasets that can be reproduced by others.  Using his example to generate a 100 row DataFrame of stock prices:If this was your actual data, you may just want to include the head and/or tail of the dataframe as follows (be sure to anonymize any sensitive data):You may also want to provide a description of the DataFrame (using only the relevant columns).  This makes it easier for others to check the data types of each column and identify other common errors (e.g. dates as string vs. datetime64 vs. object):NOTE:  If your DataFrame has a MultiIndex:If your DataFrame has a multiindex, you must first reset before calling to_dict.  You then need to recreate the index using set_index:Here is my version of dput - the standard R tool to produce reproducible reports - for Pandas DataFrames.

It will probably fail for more complex frames, but it seems to do the job in simple cases:now, Note that this produces a much more verbose output than DataFrame.to_dict, e.g.,vsfor du above, but it preserves column types. 

E.g., in the above test case,because du.dtypes is uint8 and pd.DataFrame(du.to_dict()).dtypes is int64.

Pandas: drop a level from a multi-level column index?

David Wolever

[Pandas: drop a level from a multi-level column index?](https://stackoverflow.com/questions/22233488/pandas-drop-a-level-from-a-multi-level-column-index)

If I've got a multi-level column index:How can I drop the "a" level of that index, so I end up with:

2014-03-06 18:58:06Z

If I've got a multi-level column index:How can I drop the "a" level of that index, so I end up with:You can use MultiIndex.droplevel:Another way to drop the index is to use a list comprehension: This strategy is also useful if you want to combine the names from both levels like in the example below where the bottom level contains two 'y's:Dropping the top level would leave two columns with the index 'y'. That can be avoided by joining the names with the list comprehension.That's a problem I had after doing a groupby and it took a while to find this other question that solved it. I adapted that solution to the specific case here.Another way to do this is to reassign df based on a cross section of df, using the .xs method.As of Pandas 0.24.0, we can now use DataFrame.droplevel():This is very useful if you want to keep your DataFrame method-chain rolling.You could also achieve that by renaming the columns:df.columns = ['a', 'b']This involves a manual step but could be an option especially if you would eventually rename your data frame.A small trick using sum  with level=1(work when level=1 is all unique)More common solution get_level_valuesI have struggled with this problem since I don’t know why my droplevel() function does not work. Work through several and learn that ‘a’ in your table is columns name and ‘b’, ‘c’ are index. Do like this will helpOne line super simple answer:-

df.columns=[df.columns.get_level_values(0)[i] + '_' + df.columns.get_level_values(1)[i]  for i in range(0,len(df.columns.get_level_values(0)))]this will give you a data frame with:-a_b  b_c

0  1    2

1  3    4

All combinations of a list of lists

Lin

[All combinations of a list of lists](https://stackoverflow.com/questions/798854/all-combinations-of-a-list-of-lists)

I'm basically looking for a python version of Combination of List<List<int>>Given a list of lists, I need a new list that gives all the possible combinations of items between the lists.The number of lists is unknown, so I need something that works for all cases. Bonus points for elegance!

2009-04-28 16:44:47Z

I'm basically looking for a python version of Combination of List<List<int>>Given a list of lists, I need a new list that gives all the possible combinations of items between the lists.The number of lists is unknown, so I need something that works for all cases. Bonus points for elegance!you need itertools.product:The most elegant solution is to use itertools.product in python 2.6.If you aren't using Python 2.6, the docs for itertools.product actually show an equivalent function to do the product the "manual" way:I hope you find that as elegant as I did when I first encountered it.Numpy can do it:Nothing wrong with straight up recursion for this task, and if you need a version that works with strings, this might fit your needs:One can use base python for this. The code needs a function to flatten lists of lists: Then one can run: Output: Output: 

Why does「pip install」inside Python raise a SyntaxError?

Nacht

[Why does「pip install」inside Python raise a SyntaxError?](https://stackoverflow.com/questions/8548030/why-does-pip-install-inside-python-raise-a-syntaxerror)

I'm trying to use pip to install a package.  I try to run pip install from the Python shell, but I get a SyntaxError.  Why do I get this error?  How do I use pip to install the package?

2011-12-17 21:23:32Z

I'm trying to use pip to install a package.  I try to run pip install from the Python shell, but I get a SyntaxError.  Why do I get this error?  How do I use pip to install the package?pip is run from the command line, not the Python interpreter. It is a program that installs modules, so you can use them from Python. Once you have installed the module, then you can open the Python shell and do import selenium.The Python shell is not a command line, it is an interactive interpreter. You type Python code into it, not commands.Use the command line, not the Python shell (DOS, PowerShell in Windows).If you installed Python into your PATH using the latest installers, you don't need to be in that folder to run pip Terminal in Mac or LinuxAs @sinoroc suggested correct way of installing a package via pip is using separate process since pip may cause closing a thread or may require a restart of interpreter to load new installed package so this is the right way of using the API: subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'SomeProject']) but since Python allows to access internal API and you know what you're using the API for you may want to use internal API anyway eg. if you're building own GUI package manager with alternative resourcess like https://www.lfd.uci.edu/~gohlke/pythonlibs/Following soulution is OUT OF DATE, instead of downvoting suggest updates. see https://github.com/pypa/pip/issues/7498 for reference.

UPDATE ca. v.18 get_installed_distributions() has been removed. Instead you may use generator freeze like this:If you need to update every installed package, use following:If you want to stop installing other packages if any installation fails, use it in one single pip.main([]) call:Note: When you install from list in file with -r / --requirement parameter you do NOT need open() function.Warning: Some parameters as simple --help may cause python interpreter to stop.Curiosity: By using pip.exe you actually use python interpreter and pip module anyway. If you unpack pip.exe or pip3.exe regardless it's python 2.x or 3.x, inside is the SAME single file __main__.py:To run pip in Python 3.x, just follow the instructions on Python's page: Installing Python Modules.Note that this is run from the command line and not the python shell (the reason for syntax error in the original question).you need to type it in cmd not in the IDLE. becuse IDLE is not an command prompt if you want to install something from IDLE type thisthis is calling pip like pip <commands> in terminal. The commands will be seperated by spaces that you are doing there to.Initially I too faced this same problem, I installed python and when I run pip command it used to throw me an error like shown in pic below.  Make Sure pip path is added in environmental variables. For me, the python and pip installation path is::

Python: C:\Users\fhhz\AppData\Local\Programs\Python\Python38\

pip: C:\Users\fhhz\AppData\Local\Programs\Python\Python38\Scripts

Both these paths were added to path in environmental variables.Now Open a new cmd window and type pip, you should be seeing a screen as below.  Now type pip install <<package-name>>. Here I'm installing package spyder so my command line statement will be as pip install spyder and here goes my running screen.. and I hope we are done with this!!Programmatically, the following currently works.  I see all the answers post 10.0 and all, but none of them are the correct path for me.  Within Kaggle for sure, this apporach worksTry upgrade pip with the below command and retry

Python not working in the command line of git bash

Caroline Morris

[Python not working in the command line of git bash](https://stackoverflow.com/questions/32597209/python-not-working-in-the-command-line-of-git-bash)

Python will not run in git bash (Windows). When I type python in the command line, it takes me to a blank line without saying that it has entered python 2.7.10 like its does in Powershell. It doesn't give me an error message, but python just doesn't run.I have already made sure the environmental variables in PATH included c:\python27. What else can I check?A session wherein this issue occurs looks like the following:...sitting there without returning to the prompt.

2015-09-15 23:18:07Z

Python will not run in git bash (Windows). When I type python in the command line, it takes me to a blank line without saying that it has entered python 2.7.10 like its does in Powershell. It doesn't give me an error message, but python just doesn't run.I have already made sure the environmental variables in PATH included c:\python27. What else can I check?A session wherein this issue occurs looks like the following:...sitting there without returning to the prompt.Just enter this in your git shell on windows - > alias python='winpty python.exe', that is all and you are going to have alias to the python executable. Enjoy P.S. For permanent alias addition see below,then open .bashrc, add your command from above and save the file.

You need to create the file through the console or you cannot save it with the proper name. You also need to restart the shell to apply the change.I don't see next option in a list of answers, but I can get interactive prompt with "-i" key:This is a known bug in MSys2, which provides the terminal used by Git Bash. You can work around it by running a Python build without ncurses support, or by using WinPTY, used as follows:The prebuilt binaries for msys are likely to work with Git Bash. (Do check whether there's a newer version if significant time has passed since this answer was posted!).As of Git for Windows 2.7.1, also try using winpty c:Python27/python.exe; WinPTY may be included out-of-the-box.I am windows 10 user and I have installed GIT in my system by just accepting the defaults.After reading the above answers, I got 2 solutions for my own and these 2 solutions perfectly works on GIT bash and facilitates me to execute Python statements on GIT bash.I am attaching 3 images of my GIT bash terminal. 1st with problem and the latter 2 as solutions.PROBLEM - Cursor is just waiting after hitting python commandSOLUTION 1Execute winpty <path-to-python-installation-dir>/python.exe on GIT bash terminal. Note: Do not use C:\Users\Admin like path style in GIT bash, instead use /C/Users/Admin.In my case, I executed winpty  /C/Users/SJV/Anaconda2/python.exe command on GIT bashOr if you do not know your username then execute winpty  /C/Users/$USERNAME/Anaconda2/python.exeSOLUTION 2Just type python -i and that is it.Thanks.In addition to the answer of @Charles-Duffy, you can use winpty directly without installing/downloading anything extra. Just run winpty c:/Python27/python.exe. The utility winpty.exe can be found at Git\usr\bin. I'm using Git for Windows v2.7.1The prebuilt binaries from @Charles-Duffy is version 0.1.1(according to the file name), while the included one is 0.2.2Try python -i instead of python, it's a cursor thing.HI. This is (for me) the best solution to run both Python (Python 2.7 and Python 3.x) directly from Git Bash on Win 10 => adding aliases into the aliases file that Git Bash uses for.for ex: in my case the file is in C:\Software\Develop\Git\etc\profile.d\aliases.shIn my case the python.exe are installed in:So you must create 2 aliases, one for Python 2 (I named python2) and the other for Python 3 (I named just python)

Git Bash uses linux file structure so you need to change the "\" for "/"

and if you have a path like my example Network Automation you put it with " ""Network Automation", for ex.winpty is the magic command that will call the executable.So add these lines at the beginning of aliases.shI modified also the ll alias to show all the files and in a human readable list:Now, permanently you could launch both Python directly from Git shell just writting$ python   -> launch Python 3$ python2  -> launch Python 2$ ll      -> enters a ls -lah to quickly show your detailed file listtype: 'winpty python' and it will work gitbash has some issues when running any command that starts with python. this goes for any python manage.py commands as well. Always start with 'winpty python manage.py' At least this is what works for me. Running Windows 10. You can change target for Git Bash shortcut from:to This is the way ConEmu used to start git bash (version 16). Recent version starts it normally and it's how I got there...In addition to @Vitaliy Terziev answertry touch .bash_profile and then add alias into the file.I am using MINGW64 via Visual Studio Code on Windows 10 and trying to install node-sass (which requires python2). I followed felixrieseberg/windows-build-tools #56 on Github which solved my issue.This is a special case, but I'm posting in case someone has the same problem:npm --add-python-to-path='true' --debug install --global windows-build-tools

This installs python and other required build tools to %USERPROFILE%\.windows-build-tools\python27.For python version 3.7.3 in vscode with gitbash as the default terminal I was dealing with this for a while and then followed @Vitaliy Terziev advice of adding the alias to .bashrc but with the following specification:alias python=’「/c/Users/my user name/AppData/Local/Programs/Python/Python37/python.exe」’Notice the combination of single and double quotes because of「my user name」spaces.For me, "winpty" couldn't resolve python path in vscode.Another example of this issue is using the AWS Elastic Beanstalk command line interface (awsebcli, eb cli) from the git bash (MINGW64, Mintty) in windows (using git version 2.19.0.windows.1). I'm just posting this because it took me a while to end up here, searching for eb-cli specific issues.Commands such as eb init or eb config save, which require user input, appear to cause a freeze/hang. In reality I guess the console is not updated with the text requesting user input. Moreover, eb deploy only updates the console text after the command has finished, so I don't get to see progress updates until finished.As mentioned in the git for windows release notes (for v2.19.0) and e.g. in Xun Yang's answer, a workaround is to runwinpty eb <command> (instead of just eb <command>)A alternative, as suggested in this git for windows issue, could be to use the windows native console instead of mintty (option during git installation).The one worked for me is as mentioned earlier in these great answers above is the alias as follows:

(I'm using anaconda, so first find where is the python path, then add it into the alias on git bash).

 1. on anaconda terminal I run: where python

 2. on git bash I run: alias python='winpty "C:\ProgramData\Anaconda3\envs\your_env_name\python.exe"'

 3. Done. Python is defined inside the git Bash using the alias.Thanks to (Vitaliy Terziev  & hygull) for their very helpful answers.I just don't like these "magic" aliases which you're always forgetting where it's coming from, and sometimes leads to issues in some cases.2 workarounds, rather than a solution: In my Git Bash, following command hangs and I don't get the prompt back:So I just use:As some people have noted above, you can also use:.Have a look at this answer:Git Bash won't run my python files?the path in Git Bash should be set like this:

How to activate virtualenv?

larry

[How to activate virtualenv?](https://stackoverflow.com/questions/14604699/how-to-activate-virtualenv)

I have been through search and tried various alternatives without success and spent several days on it now - driving me mad.Running on Red Hat Linux with Python 2.5.2

Began using most recent Virtualenv but could not activate it, I found somewhere suggesting needed earlier version so I have used Virtualenv 1.6.4 as that should work with Python 2.6.Still no joy...

2013-01-30 13:02:35Z

I have been through search and tried various alternatives without success and spent several days on it now - driving me mad.Running on Red Hat Linux with Python 2.5.2

Began using most recent Virtualenv but could not activate it, I found somewhere suggesting needed earlier version so I have used Virtualenv 1.6.4 as that should work with Python 2.6.Still no joy...Here is my workflow after creating a folder and cd'ing into it:You forgot to do source bin/activate where source is a executable name.

Struck me first few times as well, easy to think that manual is telling "execute this from root of the environment folder". No need to make activate executable via chmod.You can do or just go to the directoryand then Good Luck.Cd to the environment path, go to the bin folder.

At this point when you use ls command, you should see the "activate" file.now typeGo to the project directory. In my case microblog is the flask project directory and under microblog directory there should be app and venv folders. then run the below command, This is one worked for me in Ubuntu.The problem there is the /bin/. command. That's really weird, since . should always be a link to the directory it's in. (Honestly, unless . is a strange alias or function, I don't even see how it's possible.) It's also a little unusual that your shell doesn't have a . builtin for source.One quick fix would be to just run the virtualenv in a different shell. (An obvious second advantage being that instead of having to deactivate you can just exit.)If your shell supports it, you may also have the nonstandard source command, which should do the same thing as ., but may not exist. (All said, you should try to figure out why your environment is strange or it will cause you pain again in the future.)By the way, you didn't need to chmod +x those files. Files only need to be executable if you want to execute them directly. In this case you're trying to launch them from ., so they don't need it.$ mkdir <YOURPROJECT>

 Create a new project$ cd <YOURPROJECT>

Change directory to that project$ virtualenv <NEWVIRTUALENV>

Creating new virtualenv$ source <NEWVIRTUALENV>/bin/activate

Activating that new virtualenvinstead of ./activateuse  source activateFor Windows You can perform as:TO create the virtual env as: virtualenv envName –python=python.exe (if not create environment variable)It fine works on the new python version .I would recommend virtualenvwrapper as well. It works wonders for me and how I always have problems with activating. http://virtualenvwrapper.readthedocs.org/en/latest/Create your own Python virtual environment called <Your Env _name >:.

I have given it VE.To activate your new virtual environment, run (notice it's not ./ here):Sample output (note prompt changed):Once your virtual environment is set, you can remove the Virtualenv repo.On Mac, change shell to BASH (keep note that virtual env works only in bash shell )Bingo , it worked. See prompt changed.On Ubuntu:Note : prompt changedI had trouble getting running source /bin/activate then I realized I was using tcsh as my terminal shell instead of bash.  once I switched I was able to activate venv. Probably a little late to post my answer here but still I'll post, it might benefit someone though,I had faced the same problem, The main reason being that I created the virtualenv as a "root" user

But later was trying to activate it using another user.chmod won't work as you're not the owner of the file, hence the alternative is to use chown (to change the ownership)For e.g. : If you have your virtualenv created at /home/abc/ENVThen CD to /home/abc and run the command : chown -Rv [user-to-whom-you want-change-ownership] [folder/filename whose ownership needs to be changed]In this example the commands would be : chown -Rv abc ENVAfter the ownership is successfully changed you can simply run source /ENV/bin/./activate and your should be able to activate the virtualenv correctly.1- open powershell and navigate to your application folder 

2- enter your virtualenv folder ex : cd .\venv\Scripts\

3- active virtualenv by type .\activate Windows 10In Windows these directories are created : To activate Virtual Environment in Windows 10.\scripts directory contain activate file.Linux Ubuntu In Ubuntu these directories are created : To activate Virtual Environment in Linux Ubuntu./bin directory contain activate file.Virtual Environment copied from Windows to Linux Ubuntu vice versa If Virtual environment folder copied from Windows to Linux Ubuntu then according to directories:

Find string between two substrings [duplicate]

John Howard

[Find string between two substrings [duplicate]](https://stackoverflow.com/questions/3368969/find-string-between-two-substrings)

How do I find a string between two substrings ('123STRINGabc' -> 'STRING')?My current method is like this:However, this seems very inefficient and un-pythonic. What is a better way to do something like this?Forgot to mention:

The string might not start and end with start and end. They may have more characters before and after.

2010-07-30 05:50:28Z

How do I find a string between two substrings ('123STRINGabc' -> 'STRING')?My current method is like this:However, this seems very inefficient and un-pythonic. What is a better way to do something like this?Forgot to mention:

The string might not start and end with start and end. They may have more characters before and after.gives:I thought it should be noted - depending on what behavior you need, you can mix index and rindex calls or go with one of the above versions (it's equivalent of regex (.*) and (.*?) groups).gives String formatting adds some flexibility to what Nikolaus Gradwohl suggested. start and end can now be amended as desired.Just converting the OP's own solution into an answer:Here is one way to do itAnother way using regexporIf you don't want to import anything, try the string method .index():must show:

here0, here1, here2the regex is better but it will require additional lib an you may want to go for python onlyTo extract STRING, try:These solutions assume the start string and final string are different. Here is a solution I use for an entire file when the initial and final indicators are the same, assuming the entire file is read using readlines():Example:Gives:You can simply use this code or copy the function below. All neatly in one line.If you run the function as follows.You will pobably be left with the output:rather thanIf you want to have the sub-strings on the end of the output the code must look like below.But if you don't want the substrings on the end the +1 must be on the first value.Here is a function I did to return a list with a string(s) inbetween string1 and string2 searched.This is essentially cji's answer -  Jul 30 '10 at 5:58.

I changed the try except structure for a little more clarity on what was causing the exception.My method will be to do something like,This I posted before as code snippet in Daniweb:Result:re_find was almost 20 times slower than index_find in this example.Parsing text with delimiters from different email platforms posed a larger-sized version of this problem.  They generally have a START and a STOP. Delimiter characters for wildcards kept choking regex.  The problem with split is mentioned here & elsewhere - oops, delimiter character gone. It occurred to me to use replace() to give split() something else to consume.  Chunk of code:Further from Nikolaus Gradwohl answer, I needed to get version number (i.e., 0.0.2) between('ui:' and '-') from below file content (filename: docker-compose.yml):and this is how it worked for me (python script):This seems much more straight forward to me:

Getting command-line password input in Python

Nacht

[Getting command-line password input in Python](https://stackoverflow.com/questions/9202224/getting-command-line-password-input-in-python)

You know how in Linux when you try some Sudo stuff it tells you to enter the password and, as you type, nothing is shown in the terminal window (the password is not shown)?Is there a way to do that in Python? I'm working on a script that requires so sensitive info and would like for it to be hidden when I'm typing it. In other words, I want to get the password from the user without showing the password. 

2012-02-08 22:03:37Z

You know how in Linux when you try some Sudo stuff it tells you to enter the password and, as you type, nothing is shown in the terminal window (the password is not shown)?Is there a way to do that in Python? I'm working on a script that requires so sensitive info and would like for it to be hidden when I'm typing it. In other words, I want to get the password from the user without showing the password. Use getpass.getpass():An optional prompt can be passed as parameter; the default is "Password: ".Note that this function requires a proper terminal, so it can turn off echoing of typed characters – see「GetPassWarning: Can not control echo on the terminal」when running from IDLE for further details.getpass works on Linux, Windows, and Mac.Use getpass for this purpose.This code will print an asterisk instead of every letter.

Cron and virtualenv

John-Scott

[Cron and virtualenv](https://stackoverflow.com/questions/3287038/cron-and-virtualenv)

I am trying to run a Django management command from cron. I am using virtualenv to keep my project sandboxed.I have seen examples here and elsewhere that show running management commands from within virtualenv's like:However, even though syslog shows an entry when the task should have started, this task never actually runs (the log file for the script is empty). If I run the line manually from the shell, it works as expected. The only way I can currently get the command to run via cron, is to break the commands up and put them in a dumb bash wrapper script:EDIT:ars came up with a working combination of commands:At least in my case, invoking the activate script for the virtualenv did nothing. This works, so on with the show.

2010-07-20 04:33:15Z

I am trying to run a Django management command from cron. I am using virtualenv to keep my project sandboxed.I have seen examples here and elsewhere that show running management commands from within virtualenv's like:However, even though syslog shows an entry when the task should have started, this task never actually runs (the log file for the script is empty). If I run the line manually from the shell, it works as expected. The only way I can currently get the command to run via cron, is to break the commands up and put them in a dumb bash wrapper script:EDIT:ars came up with a working combination of commands:At least in my case, invoking the activate script for the virtualenv did nothing. This works, so on with the show.You should be able to do this by using the python in your virtual environment:EDIT: If your django project isn't in the PYTHONPATH, then you'll need to switch to the right directory:You can also try to log the failure from cron:Another thing to try is to make the same change in your manage.py script at the very top:Running source from a cronfile won't work as cron uses /bin/sh as its default shell, which doesn't support source.  You need to set the SHELL environment variable to be /bin/bash:It's tricky to spot why this fails as /var/log/syslog doesn't log the error details.  Best to alias yourself to root so you get emailed with any cron errors.  Simply add yourself to /etc/aliases and run sendmail -bi.More info here:

http://codeinthehole.com/archives/43-Running-django-cronjobs-within-a-virtualenv.htmlthe link above is changed to:

https://codeinthehole.com/tips/running-django-cronjobs-within-a-virtualenv/Don't look any further:Generic approach:The beauty about this is you DO NOT need to change the SHELL variable for crontab from sh to bashRather than mucking around with virtualenv-specific shebangs, just prepend PATH onto the crontab. From an activated virtualenv, run these three commands and python scripts should just work:The crontab's first line should now look like this:The only correct way to run python cron jobs when using a virtualenv is to activate the environment and then execute the environment's python to run your code.One way to do this is use virtualenv's activate_this in your python script, see: http://virtualenv.readthedocs.org/en/latest/userguide.html#using-virtualenv-without-bin-pythonAnother solution is echoing the complete command including activating the environment and piping it into /bin/bash. Consider this for your /etc/crontab:The best solution for me was to both man python mentions modifying the path in shell at $PYTHONPATH or in python with sys.pathOther answers mention ideas for doing this using the shell.  From python, adding the following lines to my script allows me to successfully run it directly from cron.Here's how it looks in an interactive session --I'd like to add this because I spent some time solving the issue and did not find an answer here for combination of variables usage in cron and virtualenv. So maybe it'll help someone.It did not work well when it was configured likeThanks @davidwinterbottom, @reed-sandberg and @mkb for giving the right direction. The accepted answer actually works fine until your python need to run a script which have to run another python binary from venv/bin directory.

How to pip install a package with min and max version range?

coredumperror

[How to pip install a package with min and max version range?](https://stackoverflow.com/questions/8795617/how-to-pip-install-a-package-with-min-and-max-version-range)

I'm wondering if there's any way to tell pip, specifically in a requirements file, to install a package with both a minimum version (pip install package>=0.2) and a maximum version which should never be installed (theoretical api: pip install package<0.3).I ask because I am using a third party library that's in active development. I'd like my pip requirements file to specify that it should always install the most recent minor release of the 0.5.x branch, but I don't want pip to ever try to install any newer major versions (like 0.6.x) since the API is different.  This is important because even though the 0.6.x branch is available, the devs are still releasing patches and bugfixes to the 0.5.x branch, so I don't want to use a static package==0.5.9 line in my requirements file.Is there any way to do that?

2012-01-09 21:44:41Z

I'm wondering if there's any way to tell pip, specifically in a requirements file, to install a package with both a minimum version (pip install package>=0.2) and a maximum version which should never be installed (theoretical api: pip install package<0.3).I ask because I am using a third party library that's in active development. I'd like my pip requirements file to specify that it should always install the most recent minor release of the 0.5.x branch, but I don't want pip to ever try to install any newer major versions (like 0.6.x) since the API is different.  This is important because even though the 0.6.x branch is available, the devs are still releasing patches and bugfixes to the 0.5.x branch, so I don't want to use a static package==0.5.9 line in my requirements file.Is there any way to do that?You can do:And pip will look for the best match, assuming the version is at least 0.2, and less than 0.3.This also applies to pip requirements files.  See the full details on version specifiers in PEP 440.you can also use:which is more consistent and easy to read.An elegant method would be to use the ~= compatible release operator according to PEP 440. In your case this would amount to:As an example, if the following versions exist, it would choose 0.5.9:For clarification, each pair is equivalent:

How to round to 2 decimals with Python?

Dolcens

[How to round to 2 decimals with Python?](https://stackoverflow.com/questions/20457038/how-to-round-to-2-decimals-with-python)

I am getting a lot of decimals in the output of this code (Fahrenheit to Celsius converter).My code currently looks like this:So my question is, how do I make the program round every answer to the 2nd decimal place?

2013-12-08 18:13:41Z

I am getting a lot of decimals in the output of this code (Fahrenheit to Celsius converter).My code currently looks like this:So my question is, how do I make the program round every answer to the 2nd decimal place?You can use the round function, which takes as its first argument the number and the second argument is the precision after the decimal point.In your case, it would be:Using str.format()'s syntax to display answer with two decimal places (without altering the underlying value of answer):Where:Most answers suggested round or format. round sometimes rounds up, and in my case I needed the value of my variable to be rounded down and not just displayed as such.I found the answer here: How do I round a floating point number up to a certain decimal place?or:You want to round your answer.round(value,significantDigit) is the ordinary solution to do this, however this sometimes does not operate as one would expect from a math perspective when the digit immediately inferior (to the left of) the digit you're rounding to has a 5.Here's some examples of this unpredictable behavior:Assuming your intent is to do the traditional rounding for statistics in the sciences, this is a handy wrapper to get the round function working as expected needing to import extra stuff like Decimal.Aha! So based on this we can make a function...Basically this adds a really small value to the string to force it to round up properly on the unpredictable instances where it doesn't ordinarily with the round function when you expect it to.  A convenient value to add is 1e-X where X is the length of the number string you're trying to use round on plus 1. The approach of using 10**(-len(val)-1) was deliberate, as it the largest small number you can add to force the shift, while also ensuring that the value you add never changes the rounding even if the decimal . is missing.  I could use just 10**(-len(val)) with a condiditional if (val>1) to subtract 1 more... but it's simpler to just always subtract the 1 as that won't change much the applicable range of decimal numbers this workaround can properly handle.  This approach will fail if your values reaches the limits of the type, this will fail, but for nearly the entire range of valid decimal values it should work.So the finished code will be something like:...should give you the results you expect.You can also use the decimal library to accomplish this, but the wrapper I propose is simpler and may be preferred in some cases.Edit: Thanks Blckknght for pointing out that the 5 fringe case occurs only for certain values here.Just use the formatting with %.2f which gives you rounding down to 2 decimals.You can use the string formatting operator of python "%".

"%.2f" means 2 digits after the decimal point.http://docs.python.org/2/library/stdtypes.html#string-formattingYou can use the round function.will give you an answer of 80.234In your case, use Hope this helps :)Here is an example that I used:Not sure why, but '{:0.2f}'.format(0.5357706) gives me '0.54'.

The only solution that works for me (python 3.6) is the following:You can use round operator for up to 2 decimal As you want your answer in decimal number so you dont need to typecast your answer variable to str in printC() function. and then use printf-style String FormattingThe answer is from: https://stackoverflow.com/a/29651462/8025086

Get month name from number

Rajeev

[Get month name from number](https://stackoverflow.com/questions/6557553/get-month-name-from-number)

How can I get the month name from the month number?For instance, if I have 3, I want to return march How to get the string march?

2011-07-02 14:17:22Z

How can I get the month name from the month number?For instance, if I have 3, I want to return march How to get the string march?Calendar APIFrom that you can see that calendar.month_name[3] would return March, and the array index of 0 is the empty string, so there's no need to worry about zero-indexing either.Returns: DecemberSome more info on the Python doc website[EDIT : great comment from @GiriB] You can also use %b which returns the short notation for month name.For the example above, it would return Dec.This is not so helpful if you need to just know the month name for a given number (1 - 12), as the current day doesn't matter. calendar.month_name[i] or calendar.month_abbr[i] are more useful here. Here is an example:Sample output:This Is What I Would Do:It Outputs:(This Was The Real Date When I Wrote This)I'll offer this in case (like me) you have a column of month numbers in a dataframe:For arbitaray range of month numbers will yield correct list. Adjust start-parameter from where January begins in the month-integer list.8.1. datetime — Basic date and time types — Python 2.7.17 documentation

https://docs.python.org/2/library/datetime.html#strftime-strptime-behaviorA list of all the strftime arguments.  Names of months and nice stuff like formatting left zero fill.  Read the full page for stuff like rules for "naive" arguments.  Here is the list in brief:

%a  Sun, Mon, …, Sat %A  Sunday, Monday, …, Saturday %w  Weekday as number, where 0 is Sunday %d  Day of the month 01, 02, …, 31%b  Jan, Feb, …, Dec%B  January, February, …, December%m  Month number as a zero-padded 01, 02, …, 12%y  2 digit year zero-padded 00, 01, …, 99%Y  4 digit Year 1970, 1988, 2001, 2013%H  Hour (24-hour clock) zero-padded 00, 01, …, 23%I  Hour (12-hour clock) zero-padded 01, 02, …, 12%p  AM or PM.%M  Minute zero-padded 00, 01, …, 59%S  Second zero-padded 00, 01, …, 59%f  Microsecond zero-padded 000000, 000001, …, 999999%z  UTC offset in the form +HHMM or -HHMM   +0000, -0400, +1030%Z  Time zone name  UTC, EST, CST%j  Day of the year zero-padded 001, 002, …, 366%U  Week number of the year zero padded, Days before the first Sunday are week 0%W  Week number of the year (Monday as first day)%c  Locale’s date and time representation. Tue Aug 16 21:30:00 1988%x  Locale’s date representation. 08/16/1988 (en_US)%X  Locale’s time representation. 21:30:00 %%  literal '%' character.I created my own function converting numbers to their corresponding month.Then I can call the function. For example:Outputs:

Is there any difference between「foo is None」and「foo == None」?

Joe Shaw

[Is there any difference between「foo is None」and「foo == None」?](https://stackoverflow.com/questions/26595/is-there-any-difference-between-foo-is-none-and-foo-none)

Is there any difference between:andThe convention that I've seen in most Python code (and the code I myself write) is the former, but I recently came across code which uses the latter.  None is an instance (and the only instance, IIRC) of NoneType, so it shouldn't matter, right?  Are there any circumstances in which it might?

2008-08-25 18:27:16Z

Is there any difference between:andThe convention that I've seen in most Python code (and the code I myself write) is the former, but I recently came across code which uses the latter.  None is an instance (and the only instance, IIRC) of NoneType, so it shouldn't matter, right?  Are there any circumstances in which it might?is always returns True if it compares the same object instanceWhereas == is ultimately determined by the __eq__() methodi.e.You may want to read this object identity and equivalence.The statement 'is' is used for object identity, it checks if objects refer to the same instance (same address in memory).And the '==' statement refers to equality (same value).A word of caution: Is not exactly the same as:The former is a boolean value test and can evaluate to false in different contexts. There are a number of things that represent false in a boolean value tests for example empty containers, boolean values. None also evaluates to false in this situation but other things do too.(ob1 is ob2) equal to (id(ob1) == id(ob2))The reason foo is None is the preferred way is that you might be handling an object that defines its own __eq__, and that defines the object to be equal to None. So, always use foo is None if you need to see if it is infact None.There is no difference because objects which are identical will of course be equal. However, PEP 8 clearly states you should use is:is tests for identity, not equality. For your statement foo is none, Python simply compares the memory address of objects. It means you are asking the question "Do I have two names for the same object?"== on the other hand tests for equality as determined by the __eq__() method. It doesn't cares about identity.None is a singleton operator. So None is None is always true.For None there shouldn't be a difference between equality (==) and identity (is). The NoneType probably returns identity for equality. Since None is the only instance you can make of NoneType (I think this is true), the two operations are the same. In the case of other types this is not always the case. For example:This would print "Equal" since lists have a comparison operation that is not the default returning of identity.@Jason:I don't like using "if foo:" unless foo truly represents a boolean value (i.e. 0 or 1). If foo is a string or an object or something else, "if foo:" may work, but it looks like a lazy shortcut to me. If you're checking to see if x is None, say "if x is None:".Some more details:E.g.And since NoneType can only have one instance of itself in the python's "look-up" table therefore the former and the latter are more of a programming style of the developer who wrote the code(maybe for consistency) rather then having any subtle logical reason to choose one over the other.John Machin's conclusion that None is a singleton is a conclusion bolstered by this code.Since None is a singleton, x == None and x is None would have the same result.  However, in my aesthetical opinion, x == None is best.

How to use a variable inside a regular expression?

Pedro Lobito

[How to use a variable inside a regular expression?](https://stackoverflow.com/questions/6930982/how-to-use-a-variable-inside-a-regular-expression)

I'd like to use a variable inside a regex, how can I do this in Python?

2011-08-03 17:59:41Z

I'd like to use a variable inside a regex, how can I do this in Python?From python 3.6 on you can also use Literal String Interpolation, "f-strings". In your particular case the solution would be:You have to build the regex as a string:Note the use of re.escape so that if your text has special characters, they won't be interpreted as such.This will insert what is in TEXTO into the regex as a string.I find it very convenient to build a regular expression pattern by stringing together multiple smaller patterns.Output:I agree with all the above unless: sys.argv[1] was something like Chicken\d{2}-\d{2}An\s*important\s*anchoryou would not want to use re.escape, because in that case you would like it to behave like a regexI needed to search for usernames that are similar to each other, and what Ned Batchelder said was incredibly helpful. However, I found I had cleaner output when I used re.compile to create my re search term:Output can be printed using the following:you can try another usage using format grammer suger:You can use format keyword as well for this.Format method will replace {} placeholder to the variable which you passed to the format method as an argument. more exampleI have configus.yml

with flows filesin python code I use

Configuring so that pip install can work from github

ccgillett

[Configuring so that pip install can work from github](https://stackoverflow.com/questions/8247605/configuring-so-that-pip-install-can-work-from-github)

We'd like to use pip with github to install private packages to our production servers.  This question concerns what needs to be in the github repo in order for the install to be successful.Assuming the following command line (which authenticates just fine and tries to install):What needs to reside in the ProductName?  Is it the contents of what would normally be in the tar file after running setup.py with the sdist option, or is the actual tar.gz file, or something else?I'm asking here because I've tried several variations and can't make it work.  Any help appreciated.

2011-11-23 18:56:54Z

We'd like to use pip with github to install private packages to our production servers.  This question concerns what needs to be in the github repo in order for the install to be successful.Assuming the following command line (which authenticates just fine and tries to install):What needs to reside in the ProductName?  Is it the contents of what would normally be in the tar file after running setup.py with the sdist option, or is the actual tar.gz file, or something else?I'm asking here because I've tried several variations and can't make it work.  Any help appreciated.You need the whole python package, with a setup.py file in it.A package named foo would be:And install from github like:More info at https://pip.pypa.io/en/stable/reference/pip_install/#vcs-supportI had similar issue when I had to install from github repo, but did not want to install git , etc.The simple way to do it is using zip archive of the package. Add /zipball/master to the repo URL:This way you will make pip work with github source repositories.If you want to use requirements.txt file, you will need git and something like the entry below to anonymously fetch the master branch in your requirements.txt.Editable mode downloads the project's source code into ./src in the current directory. It allows pip freeze to output the correct github location of the package.Clone target repository same way like you cloning any other project:Then install it in develop mode:You can change anything you wan't and every code using foo package will use modified code. There 2 benefits ot this solution:

Saving an Object (Data persistence)

Peterstone

[Saving an Object (Data persistence)](https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence)

I've created an object like this:I would like to save this object. How can I do that?

2010-12-25 09:02:03Z

I've created an object like this:I would like to save this object. How can I do that?You could use the pickle module in the standard library.

Here's an elementary application of it to your example:You could also define your own simple utility like the following which opens a file and writes a single object to it:Since this is such a popular answer, I'd like touch on a few slightly advanced usage topics.It's almost always preferable to actually use the cPickle module rather than pickle because the former is written in C and is much faster. There are some subtle differences between them, but in most situations they're equivalent and the C version will provide greatly superior performance. Switching to it couldn't be easier, just change the import statement to this:In Python 3, cPickle was renamed _pickle, but doing this is no longer necessary since the pickle module now does it automatically—see What difference between pickle and _pickle in python 3?.The rundown is you could use something like the following to ensure that your code will always use the C version when it's available in both Python 2 and 3:pickle can read and write files in several different, Python-specific, formats, called protocols as described in the documentation, "Protocol version 0" is ASCII and therefore "human-readable". Versions > 1 are binary and the highest one available depends on what version of Python is being used. The default also depends on Python version. In Python 2 the default was Protocol version 0, but in Python 3.8.1, it's Protocol version 4. In Python 3.x the module had a pickle.DEFAULT_PROTOCOL added to it, but that doesn't exist in Python 2.Fortunately there's shorthand for writing pickle.HIGHEST_PROTOCOL in every call (assuming that's what you want, and you usually do), just use the literal number -1 — similar to referencing the last element of a sequence via a negative index.

So, instead of writing:You can just write:Either way, you'd only have specify the protocol once if you created a Pickler object for use in multiple pickle operations:Note: If you're in an environment running different versions of Python, then you'll probably want to explicitly use (i.e. hardcode) a specific protocol number that all of them can read (later versions can generally read files produced by earlier ones).While a pickle file can contain any number of pickled objects, as shown in the above samples, when there's an unknown number of them, it's often easier to store them all in some sort of variably-sized container, like a list, tuple, or dict and write them all to the file in a single call:and restore the list and everything in it later with:The major advantage is you don't need to know how many object instances are saved in order to load them back later (although doing so without that information is possible, it requires some slightly specialized code). See the answers to the related question Saving and loading multiple objects in pickle file? for details on different ways to do this. Personally I like @Lutz Prechelt's answer the best. Here's it adapted to the examples here:I think it's a pretty strong assumption to assume that the object is a class. What if it's not a class?  There's also the assumption that the object was not defined in the interpreter.  What if it was defined in the interpreter?  Also, what if the attributes were added dynamically? When some python objects have attributes added to their __dict__ after creation, pickle doesn't respect the addition of those attributes (i.e. it 'forgets' they were added -- because pickle serializes by reference to the object definition).In all these cases, pickle and cPickle can fail you horribly.If you are looking to save an object (arbitrarily created), where you have attributes (either added in the object definition, or afterward)… your best bet is to use dill, which can serialize almost anything in python.We start with a class…Now shut down, and restart...Oops… pickle can't handle it.  Let's try dill.  We'll throw in another object type (a lambda) for good measure.And now read the file.It works. The reason pickle fails, and dill doesn't, is that dill treats __main__ like a module (for the most part), and also can pickle class definitions instead of pickling by reference (like pickle does).  The reason dill can pickle a lambda is that it gives it a name… then pickling magic can happen.Actually, there's an easier way to save all these objects, especially if you have a lot of objects you've created.  Just dump the whole python session, and come back to it later.Now shut down your computer, go enjoy an espresso or whatever, and come back later...The only major drawback is that dill is not part of the python standard library. So if you can't install a python package on your server, then you can't use it.However, if you are able to install python packages on your system, you can get the latest dill with git+https://github.com/uqfoundation/dill.git@master#egg=dill. And you can get the latest released version with pip install dill.You can use anycache to do the job for you. It considers all the details:Assuming you have a function myfunc which creates the instance:Anycache calls myfunc at the first time and pickles the result to a 

file in cachedir using an unique identifier (depending on the the function name and its arguments) as filename.

On any consecutive run, the pickled object is loaded.

If the cachedir is preserved between python runs, the pickled object is taken from the previous python run.For any further details see the documentationQuick example using company1 from your question, with python3. However, as this answer noted, pickle often fails. So you should really use dill.

How often does python flush to a file?

Tim McJilton

[How often does python flush to a file?](https://stackoverflow.com/questions/3167494/how-often-does-python-flush-to-a-file)

I'm unsure about (1).As for (2), I believe Python flushes to stdout after every new line.  But, if you overload stdout to be to a file, does it flush as often?

2010-07-02 16:30:41Z

I'm unsure about (1).As for (2), I believe Python flushes to stdout after every new line.  But, if you overload stdout to be to a file, does it flush as often?For file operations, Python uses the operating system's default buffering unless you configure it do otherwise.  You can specify a buffer size, unbuffered, or line buffered.For example, the open function takes a buffer size argument.http://docs.python.org/library/functions.html#open"The optional buffering argument specifies the file’s desired buffer size:" code:You can also force flush the buffer to a file programmatically with the flush() method.I have found this useful when tailing an output file with tail -f.I don't know if this applies to python as well, but I think it depends on the operating system that you are running.On Linux for example, output to terminal flushes the buffer on a newline, whereas for output to files it only flushes when the buffer is full (by default).  This is because it is more efficient to flush the buffer fewer times, and the user is less likely to notice if the output is not flushed on a newline in a file.  You might be able to auto-flush the output if that is what you need.EDIT:  I think you would auto-flush in python this way (based 

from here)You can also check the default buffer size by calling the read only DEFAULT_BUFFER_SIZE attribute from io module.Here is another approach, up to the OP to choose which one he prefers.When including the code below in the __init__.py file before any other code, messages printed with print and any errors will no longer be logged to Ableton's Log.txt but to separate files on your disk:(for Mac, change #username# to the name of your user folder. On Windows the path to your user folder will have a different format)When you open the files in a text editor that refreshes its content when the file on disk is changed (example for Mac: TextEdit does not but TextWrangler does), you will see the logs being updated in real-time.Credits: this code was copied mostly from the liveAPI control surface scripts by Nathan Ramella

python: How do I know what type of exception occurred?

Shang Wang

[python: How do I know what type of exception occurred?](https://stackoverflow.com/questions/9823936/python-how-do-i-know-what-type-of-exception-occurred)

I have a function called by the main program:but in the middle of the execution of the function it raises exception, so it jumps to the except part.How can I see exactly what happened in the someFunction() that caused the exception to happen?

2012-03-22 14:08:54Z

I have a function called by the main program:but in the middle of the execution of the function it raises exception, so it jumps to the except part.How can I see exactly what happened in the someFunction() that caused the exception to happen?The other answers all point out that you should not catch generic exceptions, but no one seems to want to tell you why, which is essential to understanding when you can break the "rule". Here is an explanation. Basically, it's so that you don't hide:So as long as you take care to do none of those things, it's OK to catch the generic exception. For instance, you could provide information about the exception to the user another way, like:So how to catch the generic exception? There are several ways. If you just want the exception object, do it like this:Make sure message is brought to the attention of the user in a hard-to-miss way! Printing it, as shown above, may not be enough if the message is buried in lots of other messages. Failing to get the users attention is tantamount to swallowing all exceptions, and if there's one impression you should have come away with after reading the answers on this page, it's that this is not a good thing. Ending the except block with a raise statement will remedy the problem by transparently reraising the exception that was caught.The difference between the above and using just except: without any argument is twofold:If you also want the same stacktrace you get if you do not catch the exception, you can get that like this (still inside the except clause):If you use the logging module, you can print the exception to the log (along with a message) like this:If you want to dig deeper and examine the stack, look at variables etc., use the post_mortem function of the pdb module inside the except block:I've found this last method to be invaluable when hunting down bugs.Get the name of the class that exception object belongs:and using print_exc() function will also print stack trace which is essential info for any error message.Like this:You will get output like this:And after print and analysis, the code can decide not to handle exception and just execute raise:Output:And interpreter prints exception:After raise original exception continues to propagate further up the call stack. (Beware of possible pitfall) If you raise new exception it caries new (shorter) stack trace.Output:Notice how traceback does not include calculate() function from line 9 which is the origin of original exception e.You usually should not catch all possible exceptions with try: ... except as this is overly broad. Just catch those that are expected to happen for whatever reason. If you really must, for example if you want to find out more about some problem while debugging, you should doUnless somefunction is a very bad coded legacy function, you shouldn't need what you're asking.Use multiple except clause to handle in different ways different exceptions:The main point is that you shouldn't catch generic exception, but only the ones that you need to. I'm sure that you don't want to shadow unexpected errors or bugs.Most answers point to except (…) as (…): syntax (rightly so) but at the same time nobody wants to talk about an elephant in the room, where the elephant is sys.exc_info() function.

From the documentation of sys module (emphasis mine):I think the sys.exc_info() could be treated as the most direct answer to the original question of How do I know what type of exception occurred?try:

    someFunction()

except Exception, exc:Here's how I'm handling my exceptions.  The idea is to do try solving the issue if that's easy, and later add a more desirable solution if possible.  Don't solve the issue in the code that generates the exception, or that code loses track of the original algorithm, which should be written to-the-point.  However, pass what data is needed to solve the issue, and return a lambda just in case you can't solve the problem outside of the code that generates it.For now, since I don't want to think tangentially to my app's purpose, I haven't added any complicated solutions.  But in the future, when I know more about possible solutions (since the app is designed more), I could add in a dictionary of solutions indexed by during.  In the example shown, one solution might be to look for app data stored somewhere else, say if the 'app.p' file got deleted by mistake.For now, since writing the exception handler is not a smart idea (we don't know the best ways to solve it yet, because the app design will evolve), we simply return the easy fix which is to act like we're running the app for the first time (in this case).These answers are fine for debugging, but for programmatically testing the exception, isinstance(e, SomeException) can be handy, as it tests for subclasses of SomeException too, so you can create functionality that applies to hierarchies of exceptions.To add to Lauritz's answer, I created a decorator/wrapper for exception handling and the wrapper logs which type of exception occurred.This can be called on a class method or a standalone function with the decorator:@general_function_handlerSee my blog about for the full example: http://ryaneirwin.wordpress.com/2014/05/31/python-decorators-and-exception-handling/You can start as Lauritz recommended, with:and then just to print ex like so:The actual exception can be captured in the following way:You can learn more about exceptions from The Python Tutorial.Your question is: "How can I see exactly what happened in the someFunction() that caused the exception to happen?"It seems to me that you are not asking about how to handle unforeseen exceptions in production code (as many answers assumed), but how to find out what is causing a particular exception during development.The easiest way is to use a debugger that can stop where the uncaught exception occurs, preferably not exiting, so that you can inspect the variables. For example, PyDev in the Eclipse open source IDE can do that. To enable that in Eclipse, open the Debug perspective, select Manage Python Exception Breakpoints in the Run menu, and check Suspend on uncaught exceptions.Just refrain from catching the exception and the traceback that Python prints will tell you what exception occurred.

NumPy array initialization (fill with identical values)

max

[NumPy array initialization (fill with identical values)](https://stackoverflow.com/questions/5891410/numpy-array-initialization-fill-with-identical-values)

I need to create a NumPy array of length n, each element of which is v.Is there anything better than:I know zeros and ones would work for v = 0, 1. I could use v * ones(n), but it won't work when v is None, and also would be much slower.

2011-05-05 00:34:20Z

I need to create a NumPy array of length n, each element of which is v.Is there anything better than:I know zeros and ones would work for v = 0, 1. I could use v * ones(n), but it won't work when v is None, and also would be much slower.NumPy 1.8 introduced np.full(), which is a more direct method than empty() followed by fill() for creating an array filled with a certain value:This is arguably the way of creating an array filled with certain values, because it explicitly describes what is being achieved (and it can in principle be very efficient since it performs a very specific task).Updated for Numpy 1.7.0:(Hat-tip to @Rolf Bartstra.)a=np.empty(n); a.fill(5) is fastest.In descending speed order:I believe fill is the fastest way to do this.You should also always avoid iterating like you are doing in your example.  A simple a[:] = v will accomplish what your iteration does using numpy broadcasting.Apparently, not only the absolute speeds but also the speed order (as reported by user1579844) are machine dependent; here's what I found:a=np.empty(1e4); a.fill(5) is fastest;In descending speed order:So, try and find out, and use what's fastest on your platform.I had in mind, but apparently that is slower than all other suggestions for large enough n.Here is full comparison with perfplot (a pet project of mine).The two empty alternatives are still the fastest (with NumPy 1.12.1). full catches up for large arrays.Code to generate the plot:You can use numpy.tile, e.g. :Although tile is meant to 'tile' an array (instead of a scalar, as in this case), it will do the job, creating pre-filled arrays of any size and dimension.without numpy

Get the Row(s) which have the max value in groups using groupby

jojo12

[Get the Row(s) which have the max value in groups using groupby](https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby)

How do I find all rows in a pandas dataframe which have the max value for count column, after grouping by ['Sp','Mt'] columns?Example 1: the following dataFrame, which I group by ['Sp','Mt']:Expected output: get the result rows whose count is max between the groups, like:Example 2: this dataframe, which I group by ['Sp','Mt']: For the above example, I want to get all the rows where count equals max, in each group e.g :

2013-03-29 14:48:13Z

How do I find all rows in a pandas dataframe which have the max value for count column, after grouping by ['Sp','Mt'] columns?Example 1: the following dataFrame, which I group by ['Sp','Mt']:Expected output: get the result rows whose count is max between the groups, like:Example 2: this dataframe, which I group by ['Sp','Mt']: For the above example, I want to get all the rows where count equals max, in each group e.g :To get the indices of the original DF you can do:Note that if you have multiple max values per group, all will be returned.UpdateOn a hail mary chance that this is what the OP is requesting:You can sort the dataFrame by count and then remove duplicates. I think it's easier:Easy solution would be to apply : idxmax() function to get indices of rows with max values. 

This would filter out all the rows with max value in the group.Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.You may not need to do with group by , using sort_values+ drop_duplicatesAlso almost same logic by using tailFor me, the easiest solution would be keep value when count is equal to the maximum. Therefore, the following one line command is enough : Use groupby and idxmax methods:Out[54]:I've been using this functional style for many group operations:.reset_index(drop=True) gets you back to the original index by dropping the group-index.Realizing that "applying" "nlargest" to groupby object works just as fine:Additional advantage - also can fetch top n values if required:Try using "nlargest" on the groupby object. The advantage of using nlargest is that it returns the index of the rows where "the nlargest item(s)" were fetched from. 

Note: we slice the second(1) element of our index since our index in this case consist of tuples(eg.(s1, 0)).enter image description here

How do I create a slug in Django?

Johnd

[How do I create a slug in Django?](https://stackoverflow.com/questions/837828/how-do-i-create-a-slug-in-django)

I am trying to create a SlugField in Django.I created this simple model:I then do this:I was expecting b-b-b-b.

2009-05-08 01:17:11Z

I am trying to create a SlugField in Django.I created this simple model:I then do this:I was expecting b-b-b-b.You will need to use the slugify function. You can call slugify automatically by overriding the save method:Be aware that the above will cause your URL to change when the q field is edited, which can cause broken links. It may be preferable to generate the slug only once when you create a new object:There is corner case with some utf-8 charactersExample:This can be solved with UnidecodeA small correction to Thepeer's answer: To override save() function in model classes, better add arguments to it:Otherwise, test.objects.create(q="blah blah blah") will result in a force_insert error (unexpected argument).If you're using the admin interface to add new items of your model, you can set up a ModelAdmin in your admin.py and utilize prepopulated_fields to automate entering of a slug:Here, when the user enters a value in the admin form for the name field, the slug will be automatically populated with the correct slugified name. In most cases the slug should not change, so you really only want to calculate it on first save:Use prepopulated_fields in your admin class:If you don't want to set the slugfield to Not be editable, then I believe you'll want to set the Null and Blank properties to False.  Otherwise you'll get an error when trying to save in Admin.So a modification to the above example would be::I'm using Django 1.7Create a SlugField in your model like this:Then in admin.py define prepopulated_fields;You can look at the docs for the SlugField to get to know more about it in more descriptive way.

Microsoft Visual C++ 14.0 is required (Unable to find vcvarsall.bat)

Umanda

[Microsoft Visual C++ 14.0 is required (Unable to find vcvarsall.bat)](https://stackoverflow.com/questions/29846087/microsoft-visual-c-14-0-is-required-unable-to-find-vcvarsall-bat)

I've installed Python 3.5 and while running it gives me the following error I have added the following lines to my PathI have a 64bit win 7 setup in my PC.What could be the solution for mitigating this error and installing the modules correctly via pip.

2015-04-24 11:16:44Z

I've installed Python 3.5 and while running it gives me the following error I have added the following lines to my PathI have a 64bit win 7 setup in my PC.What could be the solution for mitigating this error and installing the modules correctly via pip.Your path only lists Visual Studio 11 and 12, it wants 14, which is Visual Studio 2015. If you install that, and remember to tick the box for Languages->C++ then it should work. On my Python 3.5 install, the error message was a little more useful, and included the URL to get it fromEdit: New working linkEdit: As suggested by Lightfire228, you may also need to upgrade setuptools package for the error to disappear:I can't believe no one has suggested this already - use the binary-only option for pip. For example, for mysqlclient:Many packages don't create a build for every single release which forces your pip to build from source. If you're happy to use the latest pre-compiled binary version, use --only-binary :all: to allow pip to use an older binary version.To solve any of the following errors:The Solution is:Note if you already installed Visual Studio then when you run the installer, you can modify yours (click modify button under Visual Studio Community 2017) and do steps 3 and 4Final Note : If you don't want to install all modules, having the 3 ones below (or a newer version of the  VC++ 2017) would be sufficient. (you can also install the Visual Studio Build Tools with only these options so you dont need to install Visual Studio Community Edition itself) => This minimal install is already a 4.5GB, so saving off anything is helpful

As the other responses pointed out, one solution is to install Visual Studio 2015. However, it takes a few GBs of disk space. One way around is to install precompiled binaries. The webpage http://www.lfd.uci.edu/~gohlke/pythonlibs  (mirror)  contains precompiled binaries for many Python packages. After downloading the package of interest to you, you can install it using pip install, e.g. pip install mysqlclient‑1.3.10‑cp35‑cp35m‑win_amd64.whl.So I also had the common error: Microsoft Visual C++ 14.0 is required when pip installing a library.After looking across many web pages and the solutions to this thread, with none of them working. I figured these steps (most taken from previous solutions) allowed this to work.

I had the same problem when installing spaCy module. And I checked control panel I have several visual C++ redistributables installed already.What I did was select "Microsoft Visual Studio Community 2015" which is already installed on my PC --> "Modify" -->check "Common Tools for Visual C++ 2015". Then it will take some time and download more than 1 GB to install it.This fixed my issue. Now I have spaCy installed.I had this same problem. A solution for updating setuptoolsorAfter reading a lot of answers in SO and none of them working,  I finally managed to solve it following the steps in this thread, I will leave here the steps in case the page dissapears:Hope it helps as it did for me.You should now no longer use Visual Studio Tools 2015 since a newer version is available.  As indicated by the Python documentation you should be using Visual Studio Tools 2017 instead.Download it from hereYou will require also need setuptools,  if you don't have setup tools run:Or if you already have it, be sure to upgrade it.For the Python documentation link above you will see that setuptools version must be at least 34.4.0. for VS Tools to workMake sure that you've installed these required packages.Worked perfectly in my case as i installed the checked packages

I had exactly the same issue and solved it by installing mysql-connector-python with:I am on python3.7 & windows 10 and installing Microsoft Build Tools for Visual Studio 2017 (as described here) did not solve my problem that was identical to yours.Use this link to download and install Visual C++ 2015 Build Tools. It will automatically download visualcppbuildtools_full.exe and install Visual C++ 14.0 without actually installing Visual Studio. After the installation completes, retry pip install and you won't get the error again.I have tested it on following platform and versions:I have same suggestion as a comment to the question, however, I have been requested to post this as an answer as it helped a lot of people. So I posted it as an answer.I had the same issue. Downloading the Build Tools for Visual Studio 2017 worked for me. Find it hereJust had the same issue while using the latest Python 3.6. With Windows OS 10 Home Edition and 64 Bit Operation SystemSteps to solve this issue :had a similar situation installing pymssqlpip was trying to build the package because there were no official wheels

for python 3.6 & windows.solved it by downloading an unoffical wheel from here:

http://www.lfd.uci.edu/~gohlke/pythonlibs/specifically for your case -> 

http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-pythonI had the same problem. I needed a 64-bit version of Python so I installed 3.5.0 (the most recent as of writing this). After switching to 3.4.3 all of my module installations worked.Python Releases for Windowsin my case, I got another error regarding lxml as belowI had to install lxml‑4.2.3‑cp37‑cp37m‑win_amd64.whl same way as in the answer of @Sushant Chaudhary to successfully complete installation of Scrapy.now you can run pip install scrapyNone of the solutions here and elsewhere worked for me. Turns out an incompatible 32bit version of mysqlclient is being installed on my 64bit Windows 10 OS because I'm using a 32bit version of PythonI had to uninstall my current Python 3.7 32bit, and reinstalled Python 3.7 64bit and everything is working fine nowLook if the package have an official fork that include the necessary binary wheels. I needed the package python-Levenshtein, had this error, and find the package python-Levenshtein-wheels instead.Use the link to Visual C++ 2015 Build Tools. That will install Visual C++ 14.0 without installing Visual Studio.Oops! Looks like they don't have Windows wheels on PyPI.In the meantime, installing from source probably works or try downloading MSVC++ 14 as suggested in the error message and by others on this page.Christoph's site also has unofficial Windows Binaries for Python Extension Packages (.whl files).Follow steps mentioned in following links to install binaries :Also check :Which binary to download??I was facing the same problem. The following worked for me:

Download the unoffical binaries file from Christoph Gohlke installers site as per the python version installed on your system.

Navigate to the folder where you have installed the file and runFor me python_ldap‑3.0.0‑cp35‑cp35m‑win_amd64.whl worked as my machine is 64 bit and python version is 3.5.

This successfully installed python-ldap on my windows machine. You can try the same for mysql-pythonfor Python 3.7.4 following set of commands worked:

Before those command, you need to confirm Desktop with C++ and Python is installed in Visual Studio.I had the same issue while installing mysqlclient for the Django project.In my case, it's the system architecture mismatch causing the issue. I have Windows 7 64bit version on my system. But, I had installed Python 3.7.2 32 bit version by mistake. So, I re-installed Python interpreter (64bit) and ran the commandI hope this would work with other Python packages as well.I had the same exact issue on my windows 10 python version 3.8.

In my case, I needed to install mysqlclient were the error occurred Microsoft Visual C++ 14.0 is required. Because installing visual studio and it's packages could be a tedious process, Here's what I did:step 1 - Go to unofficial python binaries from any browser and open its website. step 2 - press ctrl+F and type whatever you want. In my case it was mysqlclient.step 3 - Go into it and choose according to your python version and windows system. In my case it was mysqlclient‑1.4.6‑cp38‑cp38‑win32.whl and download it. step 4 - open command prompt and specify the path where you downloaded your file. In my case it was C:\Users\user\Downloadsstep 5 - type pip install .\mysqlclient‑1.4.6‑cp38‑cp38‑win32.whl and press enter.Thus it was installed successfully, after which I went my project terminal re-entered the required command. This solved my problemNote that, while working on the project in pycharm, I also tried installing mysql-client from the project interpreter. But mysql-client and mysqlclient are different things. I have no idea why and it did not work.

Moving matplotlib legend outside of the axis makes it cutoff by the figure box

jbbiomed

[Moving matplotlib legend outside of the axis makes it cutoff by the figure box](https://stackoverflow.com/questions/10101700/moving-matplotlib-legend-outside-of-the-axis-makes-it-cutoff-by-the-figure-box)

I'm familiar with the following questions:Matplotlib savefig with a legend outside the plotHow to put the legend out of the plotIt seems that the answers in these questions have the luxury of being able to fiddle with the exact shrinking of the axis so that the legend fits. Shrinking the axes, however, is not an ideal solution because it makes the data smaller making it actually more difficult to interpret; particularly when its complex and there are lots of things going on ... hence needing a large legendThe example of a complex legend in the documentation demonstrates the need for this because the legend in their plot actually completely obscures multiple data points.http://matplotlib.sourceforge.net/users/legend_guide.html#legend-of-complex-plots What I would like to be able to do is dynamically expand the size of the figure box to accommodate the expanding figure legend.Notice how the final label 'Inverse tan' is actually outside the figure box (and looks badly cutoff - not publication quality!)

Finally, I've been told that this is normal behaviour in R and LaTeX, so I'm a little confused why this is so difficult in python... Is there a historical reason? Is Matlab equally poor on this matter?I have the (only slightly) longer version of this code on pastebin http://pastebin.com/grVjc007

2012-04-11 07:32:08Z

I'm familiar with the following questions:Matplotlib savefig with a legend outside the plotHow to put the legend out of the plotIt seems that the answers in these questions have the luxury of being able to fiddle with the exact shrinking of the axis so that the legend fits. Shrinking the axes, however, is not an ideal solution because it makes the data smaller making it actually more difficult to interpret; particularly when its complex and there are lots of things going on ... hence needing a large legendThe example of a complex legend in the documentation demonstrates the need for this because the legend in their plot actually completely obscures multiple data points.http://matplotlib.sourceforge.net/users/legend_guide.html#legend-of-complex-plots What I would like to be able to do is dynamically expand the size of the figure box to accommodate the expanding figure legend.Notice how the final label 'Inverse tan' is actually outside the figure box (and looks badly cutoff - not publication quality!)

Finally, I've been told that this is normal behaviour in R and LaTeX, so I'm a little confused why this is so difficult in python... Is there a historical reason? Is Matlab equally poor on this matter?I have the (only slightly) longer version of this code on pastebin http://pastebin.com/grVjc007Sorry EMS, but I actually just got another response from the matplotlib mailling list (Thanks goes out to Benjamin Root).The code I am looking for is adjusting the savefig call to:This is apparently similar to calling tight_layout, but instead you allow savefig to consider extra artists in the calculation. This did in fact resize the figure box as desired.This produces:[edit] The intent of this question was to completely avoid the use of arbitrary coordinate placements of arbitrary text as was the traditional solution to these problems. Despite this, numerous edits recently have insisted on putting these in, often in ways that led to the code raising an error. I have now fixed the issues and tidied the arbitrary text to show how these are also considered within the bbox_extra_artists algorithm. Added: I found something that should do the trick right away, but the rest of the code below also offers an alternative.Use the subplots_adjust() function to move the bottom of the subplot up:Then play with the offset in the legend bbox_to_anchor part of the legend command, to get the legend box where you want it. Some combination of setting the figsize and using the subplots_adjust(bottom=...) should produce a quality plot for you.Alternative:

I simply changed the line:to:and changedtoand it shows up fine on my screen (a 24-inch CRT monitor). Here figsize=(M,N) sets the figure window to be M inches by N inches. Just play with this until it looks right for you. Convert it to a more scalable image format and use GIMP to edit if necessary, or just crop with the LaTeX viewport option when including graphics.Here is another, very manual solution. You can define the size of the axis and paddings are considered accordingly (including legend and tickmarks). Hope it is of use to somebody.Example (axes size are the same!):Code:

_csv.Error: field larger than field limit (131072)

user1251007

[_csv.Error: field larger than field limit (131072)](https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072)

I have a script reading in a csv file with very huge fields:However, this throws the following error on some csv files:How can I analyze csv files with huge fields? Skipping the lines with huge fields is not an option as the data needs to be analyzed in subsequent steps.

2013-02-25 09:38:02Z

I have a script reading in a csv file with very huge fields:However, this throws the following error on some csv files:How can I analyze csv files with huge fields? Skipping the lines with huge fields is not an option as the data needs to be analyzed in subsequent steps.The csv file might contain very huge fields, therefore increase the field_size_limit:sys.maxsize works for Python 2.x and 3.x. sys.maxint would only work with Python 2.x (SO: what-is-sys-maxint-in-python-3)As Geoff pointed out, the code above might result in the following error: OverflowError: Python int too large to convert to C long. 

To circumvent this, you could use the following quick and dirty code (which should work on every system with Python 2 and Python 3):This could be because your CSV file has embedded single or double quotes. If your CSV file is tab-delimited try opening it as:Below is to check the current limitOut[20]: 131072Below is to increase the limit. Add it to the codeTry checking the limit againOut[22]: 100000000Now you won't get the error "_csv.Error: field larger than field limit (131072)"csv field sizes are controlled via [Python 3.Docs]: csv.field_size_limit([new_limit]):It is set by default to 128k or 0x20000 (131072), which should be enough for any decent .csv:However, when dealing with a .csv file (with the correct quoting and delimiter) having (at least) one field longer than this size, the error pops up. To get rid of the error, the size limit should be increased (to avoid any worries, the maximum possible value is attempted).Behind the scenes (check [GitHub]: python/cpython - (master) cpython/Modules/_csv.c for implementation details), the variable that holds this value is a C long ([Wikipedia]: C data types), whose size varies depending on CPU architecture and OS (ILP). The classical difference: for a 64 bit OS, the long type size (in bits) is:When attempting to set it, the new value is checked to be in the long boundaries, that's why in some cases another exception pops up (this case is common on Win):To avoid running into this problem, set the (maximum possible) limit (LONG_MAX) using an artifice (thanks to [Python 3.Docs]: ctypes - A foreign function library for Python). It should work on Python 3 and Python 2, on any CPU / OS.For more details on playing with C types boundaries from Python, check [SO]: Maximum and minimum value of C types integers from Python (@CristiFati's answer).Sometimes, a row contain double quote column. When csv reader try read this row, not understood end of column and fire this raise.

Solution is below:I just had this happen to me on a 'plain' CSV file. Some people might call it an invalid formatted file. No escape characters, no double quotes and delimiter was a semicolon.A sample line from this file would look like this:the single quote in the second cell would throw the parser off its rails. What worked was:Find the cqlshrc file usually placed in .cassandra directory.In that file append,You can use read_csv from pandas to skip these lines.

PATH issue with pytest 'ImportError: No module named YadaYadaYada'

MattoTodd

[PATH issue with pytest 'ImportError: No module named YadaYadaYada'](https://stackoverflow.com/questions/10253826/path-issue-with-pytest-importerror-no-module-named-yadayadayada)

I used easy_install to install pytest on a mac and started writing tests for a project with a file structure likes so:run py.test while in the repo directory, everything behaves as you would expectbut when I try that same thing on either linux or windows (both have pytest 2.2.3 on them) it barks whenever it hits its first import of something from my application path. Say for instance from app import some_def_in_appDo I need to be editing my PATH to run py.test on these systems? Has Anyone experienced this? 

2012-04-20 21:32:03Z

I used easy_install to install pytest on a mac and started writing tests for a project with a file structure likes so:run py.test while in the repo directory, everything behaves as you would expectbut when I try that same thing on either linux or windows (both have pytest 2.2.3 on them) it barks whenever it hits its first import of something from my application path. Say for instance from app import some_def_in_appDo I need to be editing my PATH to run py.test on these systems? Has Anyone experienced this? Yes, the source folder is not in Python's path if you cd to the tests directory. You have 2 choices:I'm not sure why py.test does not add the current directory in the PYTHONPATH itself, but here's a workaround (to be executed from the root of your repository):It works because Python adds the current directory in the PYTHONPATH for you.The least invasive solution is adding an empty file named conftest.py in the repo/ directory:That's it. No need to write custom code for mangling the sys.path or remember to drag PYTHONPATH along, or placing __init__.py into dirs where it doesn't belong.The project directory afterwards:pytest looks for the conftest modules on test collection to gather custom hooks and fixtures, and in order to import the custom objects from them, pytest adds the parent directory of the conftest.py to the sys.path (in this case the repo directory).If you have other project structure, place the conftest.py in the package root dir (the one that contains packages but is not a package itself, so does not contain an __init__.py), for example:Although this approach can be used with the src layout (place conftest.py in the src dir):beware that adding src to PYTHONPATH mitigates the meaning and benefits of the src layout! You will end up with testing the code from repository and not the installed package. If you need to do it, maybe you don't need the src dir at all.Of course, conftest modules are not just some files to help the source code discovery; it's where all the project-specific enhancements of the pytest framework and the customization of your test suite happen. pytest has a lot of information on conftest modules scattered throughout their docs; start with conftest.py: local per-directory pluginsAlso, SO has an excellent question on conftest modules: In py.test, what is the use of conftest.py files?I had the same problem. I fixed it by adding an empty __init__.py file to my tests directory.Run pytest itself as a module with:

python -m pytest testsYou can run with PYTHONPATH in project rootOr use pip install as editable importI created this as an answer to your question and my own confusion. I hope it helps. Pay attention to PYTHONPATH in both the py.test command line and in the tox.ini.https://github.com/jeffmacdonald/pytest_testSpecifically: You have to tell py.test and tox where to find the modules you are including. With py.test you can do this:And with tox, add this to your tox.ini:I started getting weird ConftestImportFailure: ImportError('No module named ... errors when I had accidentally added __init__.py file to my src directory (which was not supposed to be a Python package, just a container of all source).I fixed it by removing the top-level __init__.py in the parent folder of my sources.I was getting this error due to something even simpler (you could even say trivial). I hadn't installed the pytest module. So a simple apt install python-pytest fixed it for me.'pytest' would have been listed in setup.py as a test dependency. Make sure you install the test requirements as well. I had a similar issue. pytest did not recognize a module installed in the environment I was working in.I resolved it by also installing pytest into the same environment.For me the problem was tests.py generated by Django along with tests directory. Removing tests.py solved the problem.I got this error as I used relative imports incorrectly. In the OP example, test_app.py should import functions using e.g.However liberally __init__.py files are scattered around the file structure, this does not work and creates the kind of ImportError seen unless the files and test files are in the same directory. Here's an example of what I had to do with one of my projects:Here’s my project structure:To be able to access activity_indicator.py from test_activity_indicator.py I needed to:Very often the tests were interrupted due to module being unable to be imported,After research, I found out that the system is looking at the file in the wrong place and we can easily overcome the problem by copying the file, containing the module, in the same folder as stated, in order to be properly imported. Another solution proposal would be to change the declaration for the import and show MutPy the correct path of the unit. However, due to the fact that multiple units can have this dependency, meaning we need to commit changes also in their declarations, we prefer to simply move the unit to the folder. According to a post on Medium by Dirk Avery (and supported by my personal experience) if you're using a virtual environment for your project then you can't use a system-wide install of pytest; you have to install it in the virtual environment and use that install. In particular, if you have it installed in both places then simply running the pytest command won't work because it will be using the system install. As the other answers have described, one simple solution is to run python -m pytest instead of pytest; this works because it uses the environment's version of pytest. Alternatively, you can just uninstall the system's version of pytest; after reactivating the virtual environment the pytest command should work.I was having the same problem when following the Flask tutorial and I found the answer on the official Pytest docs

It's a little shift from the way I (and I think many others) are used to do things.You have to create a setup.py file in your project's root directory with at least the following two lines:where PACKAGENAME is your app's name. Then you have to install it with pip:The -e flag tells pip to intall the package in editable or "develop" mode. So the next time you run pytest it should find your app in the standard PYTHONPATH.I had the same problem in Flask.When I added:to tests folder, problem disappeared :)Probably application couldn't recognize folder tests as module

Pandas read in table without headers

user308827

[Pandas read in table without headers](https://stackoverflow.com/questions/29287224/pandas-read-in-table-without-headers)

How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do usecols

2015-03-26 19:27:49Z

How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do usecolsIn order to read a csv in that doesn't have a header and for only certain columns you need to pass params header=None and usecols=[3,6] for the 4th and 7th columns:See the docsPrevious answers were good and correct, but in my opinion, an extra names parameter will make it perfect, and it should be the recommended way, especially when the csv has no headers.or use header=None to explicitly tells people that the csv has no headers (anyway both lines are identical)So that you can retrieve your data by instead of Based on read_csv, when names are passed explicitly, then header will be behaving like None instead of 0, so one can skip header=None when names exist.Make sure you specify pass header=None and add usecols=[3,6] for the 4th and 7th columns.

Converting numpy dtypes to native python types

conradlee

[Converting numpy dtypes to native python types](https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types)

If I have a numpy dtype, how do I automatically convert it to its closest python data type?  For example,I could try to come up with a mapping of all of these cases, but does numpy provide some automatic way of converting its dtypes into the closest possible native python types?  This mapping need not be exhaustive, but it should convert the common dtypes that have a close python analog.  I think this already happens somewhere in numpy.

2012-02-26 11:40:11Z

If I have a numpy dtype, how do I automatically convert it to its closest python data type?  For example,I could try to come up with a mapping of all of these cases, but does numpy provide some automatic way of converting its dtypes into the closest possible native python types?  This mapping need not be exhaustive, but it should convert the common dtypes that have a close python analog.  I think this already happens somewhere in numpy.Use val.item() to convert most NumPy values to a native Python type:(Another method is np.asscalar(val), however it is deprecated since NumPy 1.16).For the curious, to build a table of conversions of NumPy array scalars for your system:There are a few NumPy types that have no native Python equivalent on some systems, including: clongdouble, clongfloat, complex192, complex256, float128, longcomplex, longdouble and longfloat. These need to be converted to their nearest NumPy equivalent before using .item().found myself having mixed set of numpy types and standard python. as all numpy types derive from numpy.generic, here's how you can convert everything to python standard types:If you want to convert (numpy.array OR numpy scalar OR native type OR numpy.darray) TO native type you can simply do :tolist will convert your scalar or array to python native type. The default lambda function takes care of the case where value is already native.How about:You can also call the item() method of the object you want to convert:tolist() is a more general approach to accomplish this. It works in any primitive dtype and also in arrays or matrices.I doesn't actually yields a list if called from primitive types:numpy == 1.15.2I think you can just write general type convert function like so:This means there is no fixed lists and your code will scale with more types.numpy holds that information in a mapping exposed as typeDict so you could do something like the below::If you want the actual python types rather than their names, you can do ::Sorry to come late to the partly, but I was looking at a problem of converting numpy.float64 to regular Python float only. I saw 3 ways of doing that: Here are the relevant timings from IPython:It sounds like float(npValue) seems much faster.My approach is a bit forceful, but seems to play nice for all cases:Usage:A side note about array scalars for those who don't need automatic conversion and know the numpy dtype of the value:SourceThus, for most cases conversion might not be needed at all, and the array scalar could be used directly. The effect should be identical to using Python scalar:But if, for some reason, the explicit conversion is needed, using the corresponding Python built-in function is the way to go. As shown in the other answer it's also faster than array scalar item() method.Translate the whole ndarray instead one unit data object:However, it takes some minutes when handling large dataframes. I am also looking for a more efficient solution.

Hope a better answer.

When is「i += x」different from「i = i + x」in Python?

MarJamRob

[When is「i += x」different from「i = i + x」in Python?](https://stackoverflow.com/questions/15376509/when-is-i-x-different-from-i-i-x-in-python)

I was told that += can have different effects than the standard notation of i = i +. Is there a case in which i += 1 would be different from i = i + 1?

2013-03-13 03:24:44Z

I was told that += can have different effects than the standard notation of i = i +. Is there a case in which i += 1 would be different from i = i + 1?This depends entirely on the object i.  += calls the __iadd__ method (if it exists -- falling back on __add__ if it doesn't exist) whereas + calls the __add__ method1 or the __radd__ method in a few cases2.  From an API perspective, __iadd__ is supposed to be used for modifying mutable objects in place (returning the object which was mutated) whereas __add__ should return a new instance of something.  For immutable objects, both methods return a new instance, but __iadd__ will put the new instance in the current namespace with the same name that the old instance had.  This is why seems to increment i.  In reality, you get a new integer and assign it "on top of" i -- losing one reference to the old integer.  In this case, i += 1 is exactly the same as i = i + 1.  But, with most mutable objects, it's a different story:As a concrete example:compared to:notice how in the first example, since b and a reference the same object, when I use += on b, it actually changes b (and a sees that change too -- After all, it's referencing the same list).  In the second case however, when I do b = b + [1, 2, 3], this takes the list that b is referencing and concatenates it with a new list [1, 2, 3].  It then stores the concatenated list in the current namespace as b -- With no regard for what b was the line before.1In the expression x + y, if x.__add__ isn't implemented or if x.__add__(y) returns NotImplemented and x and y have different types, then x + y tries to call y.__radd__(x).  So, in the case where you have foo_instance += bar_instanceif Foo doesn't implement __add__ or __iadd__ then the result here is the same as foo_instance = bar_instance.__radd__(bar_instance, foo_instance)2In the expression foo_instance + bar_instance, bar_instance.__radd__ will be tried before foo_instance.__add__ if the type of bar_instance is a subclass of the type of foo_instance (e.g. issubclass(Bar, Foo)).  The rational for this is because Bar is in some sense a "higher-level" object than Foo so Bar should get the option of overriding Foo's behavior.Under the covers, i += 1 does something like this:While i = i + 1 does something like this:This is a slight oversimplification, but you get the idea: Python gives types a way to handle += specially, by creating an __iadd__ method as well as an __add__.The intention is that mutable types, like list, will mutate themselves in __iadd__ (and then return self, unless you're doing something very tricky), while immutable types, like int, will just not implement it.For example:Because l2 is the same object as l1, and you mutated l1, you also mutated l2.But:Here, you didn't mutate l1; instead, you created a new list, l1 + [3], and rebound the name l1 to point at it, leaving l2 pointing at the original list.(In the += version, you were also rebinding l1, it's just that in that case you were rebinding it to the same list it was already bound to, so you can usually ignore that part.)Here is an example that directly compares i += x with i = i + x:

what is the right way to treat Python argparse.Namespace() as a dictionary?

Jason S

[what is the right way to treat Python argparse.Namespace() as a dictionary?](https://stackoverflow.com/questions/16878315/what-is-the-right-way-to-treat-python-argparse-namespace-as-a-dictionary)

If I want to use the results of argparse.ArgumentParser(), which is a Namespace object, with a method that expects a dictionary or mapping-like object (see collections.Mapping), what is the right way to do it?Is it proper to "reach into" an object and use its __dict__ property?I would think the answer is no: __dict__ smells like a convention for implementation, but not for an interface, the way __getattribute__ or __setattr__ or __contains__ seem to be.

2013-06-01 23:33:56Z

If I want to use the results of argparse.ArgumentParser(), which is a Namespace object, with a method that expects a dictionary or mapping-like object (see collections.Mapping), what is the right way to do it?Is it proper to "reach into" an object and use its __dict__ property?I would think the answer is no: __dict__ smells like a convention for implementation, but not for an interface, the way __getattribute__ or __setattr__ or __contains__ seem to be.You can access the namespace's dictionary with vars():You can modify the dictionary directly if you wish:Yes, it is okay to access the __dict__ attribute.  It is a well-defined, tested, and guaranteed behavior.Straight from the horse's mouth:In general, I would say "no". However Namespace has struck me as over-engineered, possibly from when classes couldn't inherit from built-in types. On the other hand, Namespace does present a task-oriented approach to argparse, and I can't think of a situation that would call for grabbing the __dict__, but the limits of my imagination are not the same as yours.

Generate 'n' unique random numbers within a range [duplicate]

Chris Headleand

[Generate 'n' unique random numbers within a range [duplicate]](https://stackoverflow.com/questions/22842289/generate-n-unique-random-numbers-within-a-range)

I know how to generate a random number within a range in Python.And I know I can put this in a loop to generate n amount of these numbersHowever, I need to make sure each number in that list is unique. Other than a load of conditional statements, is there a straightforward way of generating n number of unique random numbers?The important thing is that each number in the list is different to the others..So[12, 5, 6, 1] = goodBut[12, 5, 5, 1] = bad, because the number 5 occurs twice.

2014-04-03 15:29:57Z

I know how to generate a random number within a range in Python.And I know I can put this in a loop to generate n amount of these numbersHowever, I need to make sure each number in that list is unique. Other than a load of conditional statements, is there a straightforward way of generating n number of unique random numbers?The important thing is that each number in the list is different to the others..So[12, 5, 6, 1] = goodBut[12, 5, 5, 1] = bad, because the number 5 occurs twice.If you just need sampling without replacement:random.sample takes a population and a sample size k and returns k random members of the population.If you have to control for the case where k is larger than len(population), you need to be prepared to catch a ValueError:Generate the range of data first and then shuffle it like thisBy doing this way, you will get all the numbers in the particular range but in a random order.But you can use random.sample to get the number of elements you need, from a range of numbers like thisYou could add to a set until you reach n:Be careful of having a smaller range than will fit in n. It will loop forever, unable to find new numbers to insert up to nYou could use the random.sample function from the standard library to select k elements from a population:In case of a rather large range of possible numbers, you could use itertools.islice with an infinite random generator:After the question update it is now clear that you need n distinct (unique) numbers.

In Python, how do I iterate over a dictionary in sorted key order?

mike

[In Python, how do I iterate over a dictionary in sorted key order?](https://stackoverflow.com/questions/364519/in-python-how-do-i-iterate-over-a-dictionary-in-sorted-key-order)

There's an existing function that ends in the following, where d is a dictionary:that returns an unsorted iterator for a given dictionary. I would like to return an iterator that goes through the items sorted by key. How do I do that?

2008-12-12 23:57:05Z

There's an existing function that ends in the following, where d is a dictionary:that returns an unsorted iterator for a given dictionary. I would like to return an iterator that goes through the items sorted by key. How do I do that?Haven't tested this very extensively, but works in Python 2.5.2.If you are used to doing for key, value in d.iteritems(): ... instead of iterators, this will still work with the solution aboveWith Python 3.x, use d.items() instead of d.iteritems() to return an iterator.Use the sorted() function:If you want an actual iterator over the sorted results, since sorted() returns a list, use:A dict's keys are stored in a hashtable so that is their 'natural order', i.e. psuedo-random. Any other ordering is a concept of the consumer of the dict.sorted() always returns a list, not a dict. If you pass it a dict.items() (which produces a list of tuples), it will return a list of tuples [(k1,v1), (k2,v2), ...] which can be used in a loop in a way very much like a dict, but it is not in anyway a dict!The following feels like a dict in a loop, but it's not, it's a list of tuples being unpacked into k,v:Roughly equivalent to:Greg's answer is right. Note that in Python 3.0 you'll have to do as iteritems will be gone.You can now use OrderedDict in Python 2.7 as well:Here you have the what's new page for 2.7 version and the OrderedDict API.In general, one may sort a dict like so:For the specific case in the question, having a "drop in replacement" for d.iteritems(), add a function like:and so the ending line changes fromtoorThis method still has an O(N log N) sort, however, after a short linear heapify, it yields the items in sorted order as it goes, making it theoretically more efficient when you do not always need the whole list. If you want to sort by the order that items were inserted instead of of the order of the keys, you should have a look to Python's collections.OrderedDict. (Python 3 only)sorted returns a list, hence your error when you try to iterate over it,

but because you can't order a dict you will have to deal with a list.I have no idea what the larger context of your code is, but you could try adding an

iterator to the resulting list. 

like this maybe?:of course you will be getting back tuples now because sorted turned your dict into a list of tuplesex:

 say your dict was:

 {'a':1,'c':3,'b':2}

sorted turns it into a list:so when you actually iterate over the list you get back (in this example) a tuple

composed of a string and an integer, but at least you will be able to iterate over it.Assuming you are using CPython 2.x and have a large dictionary mydict, then using sorted(mydict) is going to be slow because sorted builds a sorted list of the keys of mydict.In that case you might want to look at my ordereddict package which includes a C implementation of sorteddict in C. Especially if you have to go over the sorted list of keys multiple times at different stages (ie. number of elements) of the dictionaries lifetime.http://anthon.home.xs4all.nl/Python/ordereddict/

input() error - NameError: name '…' is not defined

chillpenguin

[input() error - NameError: name '…' is not defined](https://stackoverflow.com/questions/21122540/input-error-nameerror-name-is-not-defined)

I am getting an error when I try to run this simple python script:Lets say I type in "dude", the error I am getting is:I am running Mac OS X 10.9.1 and I am using the Python Launcher app that came with the install of python 3.3 to run the script.Edit: I realized I am somehow running these scripts with 2.7. I guess the real question is how do I run my scripts with version 3.3? I thought if I dragged and dropped my scripts on top of the Python Launcher app that is inside the Python 3.3 folder in my applications folder that it would launch my scripts using 3.3. I guess this method still launches scripts with 2.7. So How do I use 3.3?

2014-01-14 19:44:14Z

I am getting an error when I try to run this simple python script:Lets say I type in "dude", the error I am getting is:I am running Mac OS X 10.9.1 and I am using the Python Launcher app that came with the install of python 3.3 to run the script.Edit: I realized I am somehow running these scripts with 2.7. I guess the real question is how do I run my scripts with version 3.3? I thought if I dragged and dropped my scripts on top of the Python Launcher app that is inside the Python 3.3 folder in my applications folder that it would launch my scripts using 3.3. I guess this method still launches scripts with 2.7. So How do I use 3.3?TL;DRinput function in Python 2.7, evaluates whatever your enter, as a Python expression. If you simply want to read strings, then use raw_input function in Python 2.7, which will not evaluate the read strings.If you are using Python 3.x, raw_input has been renamed to input. Quoting the Python 3.0 release notes,In Python 2.7, there are two functions which can be used to accept user inputs. One is input and the other one is raw_input. You can think of the relation between them as followsConsider the following piece of code to understand this betterinput accepts a string from the user and evaluates the string in the current Python context. When I type dude as input, it finds that dude is bound to the value thefourtheye and so the result of evaluation becomes thefourtheye and that gets assigned to input_variable.If I enter something else which is not there in the current python context, it will fail will the NameError.Security considerations with Python 2.7's input:Since whatever user types is evaluated, it imposes security issues as well. For example, if you have already loaded os module in your program with import os, and then the user types inthis will be evaluated as a function call expression by python and it will be executed. If you are executing Python with elevated privileges, /etc/hosts file will be deleted. See, how dangerous it could be?To demonstrate this, let's try to execute input function again.Now, when input("Enter your name: ") is executed, it waits for the user input and the user input is a valid Python function invocation and so that is also invoked. That is why we are seeing Enter your name again: prompt again.So, you are better off with raw_input function, like thisIf you need to convert the result to some other type, then you can use appropriate functions to convert the string returned by raw_input. For example, to read inputs as integers, use the int function, like shown in this answer.In python 3.x, there is only one function to get user inputs and that is called input, which is equivalent to Python 2.7's raw_input.You are running Python 2, not Python 3.  For this to work in Python 2, use raw_input.Since you are writing for Python 3.x, you'll want to begin your script with:If you use:It will default to Python 2.x. These go on the first line of your script, if there is nothing that starts with #! (aka the shebang).If your scripts just start with:Then you can change it to:Although this shorter formatting is only recognized by a few programs, such as the launcher, so it is not the best choice. The first two examples are much more widely used and will help ensure your code will work on any machine that has Python installed.You should use raw_input because you are using python-2.7. When you use input() on a variable (for example: s = input('Name: ')), it will execute the command ON the Python environment without saving what you wrote on the variable (s) and create an error if what you wrote is not defined.raw_input() will save correctly what you wrote on the variable (for example: f = raw_input('Name : ')), and it will not execute it in the Python environment without creating any possible error: For python 3 and aboveit will solve the problem on pycharm IDE

if you are solving on online site exactly hackerrank then use:You could either do:or:You have to enter input in either single or double quotesWe are using the following that works both python 2 and python 3I also encountered this issue with a module that was supposed to be compatible for python 2.7 and 3.7what i found to fix the issue was importing:this fixed the usability for both interpreters you can read more about the six library hereFor anyone else that may run into this issue, turns out that even if you include #!/usr/bin/env python3 at the beginning of your script, the shebang is ignored if the file isn't executable.To determine whether or not your file is executable:If you've included import sys; print(sys.version) as Kevin suggested, you'll now see that the script is being interpreted by python3Good contributions the previous ones.Thanks!There are two ways to fix these issues,As error implies: name 'dude' is not defined 

i.e. for python 'dude' become variable here and it's not having any value of python defined type assignedso only its crying like baby so if we define a 'dude' variable and assign any value and pass to it, it will work but that's not what we want as we don't know what user will enter and moreover we want to capture the user input.You can change which python you're using with your IDE, if you've already downloaded python 3.x it shouldn't be too hard to switch. But your script works fine on python 3.x, I would just changetoBecause with the comma it prints with a whitespace in between your name is and whatever the user inputted. AND: if you're using 2.7 just use raw_input instead of input. 

What is the preferred syntax for initializing a dict: curly brace literals {} or the dict() function?

daotoad

[What is the preferred syntax for initializing a dict: curly brace literals {} or the dict() function?](https://stackoverflow.com/questions/2853683/what-is-the-preferred-syntax-for-initializing-a-dict-curly-brace-literals-or)

I'm putting in some effort to learn Python, and I am paying close attention to common coding standards.  This may seem like a pointlessly nit-picky question, but I am trying to focus on best-practices as I learn, so I don't have to unlearn any 'bad' habits.I see two common methods for initializing a dict:Which is considered to be "more pythonic"?  Which do you use? Why?

2010-05-17 23:37:08Z

I'm putting in some effort to learn Python, and I am paying close attention to common coding standards.  This may seem like a pointlessly nit-picky question, but I am trying to focus on best-practices as I learn, so I don't have to unlearn any 'bad' habits.I see two common methods for initializing a dict:Which is considered to be "more pythonic"?  Which do you use? Why?Curly braces. Passing keyword arguments into dict(), though it works beautifully in a lot of scenarios, can only initialize a map if the keys are valid Python identifiers.The first, curly braces. Otherwise, you run into consistency issues with keys that have odd characters in them, like =.The first version is preferable:I think the first option is better because you are going to access the values as a['a'] or a['another']. The keys in your dictionary are strings, and there is no reason to pretend they are not. To me the keyword syntax looks clever at first, but obscure at a second look. This only makes sense to me if you are working with __dict__, and the keywords are going to become attributes later, something like that.FYI, in case you need to add attributes to your dictionary (things that are attached to the dictionary, but are not one of the keys), then you'll need the second form.  In that case, you can initialize your dictionary with keys having arbitrary characters, one at a time, like so:Sometimes dict() is a good choice:[random.randint(0,100) for x in range(0,7)]))I almost always use curly-braces; however, in some cases where I'm writing tests, I do keyword packing/unpacking, and in these cases dict() is much more maintainable, as I don't need to change:to:It also helps in some circumstances where I think I might want to turn it into a namedtuple or class instance at a later time.In the implementation itself, because of my obsession with optimisation, and when I don't see a particularly huge maintainability benefit, I'll always favour curly-braces.In tests and the implementation, I would never use dict() if there is a chance that the keys added then, or in the future, would either:

Convert floats to ints in Pandas?

MJP

[Convert floats to ints in Pandas?](https://stackoverflow.com/questions/21291259/convert-floats-to-ints-in-pandas)

I've been working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers, or, without comma. Is there a way to convert them to integers or not display the comma?

2014-01-22 18:42:15Z

I've been working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers, or, without comma. Is there a way to convert them to integers or not display the comma?To modify the float output do this:Use the pandas.DataFrame.astype(<type>) function to manipulate column dtypes.EDIT:To handle missing values:Considering the following data frame:Using a list of column names, change the type for multiple columns with applymap():Or for a single column with apply():This is a quick solution in case you want to convert more columns of your pandas.DataFrame from float to integer considering also the case that you can have NaN values.I tried with else x) and else None), but the result is still having the float number, so I used else "".Expanding on @Ryan G mentioned usage of the pandas.DataFrame.astype(<type>) method, one can use the errors=ignore argument to only convert those columns that do not produce an error, which notably simplifies the syntax. Obviously, caution should be applied when ignoring errors, but for this task it comes very handy. From pandas.DataFrame.astype docs:Here's a simple function that will downcast floats into the smallest possible integer type that doesn't lose any information.  For examples,Code examples:

What exactly does the .join() method do?

Matt McCormick

[What exactly does the .join() method do?](https://stackoverflow.com/questions/1876191/what-exactly-does-the-join-method-do)

I'm pretty new to Python and am completely confused by .join() which I have read is the preferred method for concatenating strings.I tried:and got something like:Why does it work like this?  Shouldn't the 595 just be automatically appended?

2009-12-09 19:22:04Z

I'm pretty new to Python and am completely confused by .join() which I have read is the preferred method for concatenating strings.I tried:and got something like:Why does it work like this?  Shouldn't the 595 just be automatically appended?Look carefully at your output:I've highlighted the "5", "9", "5" of your original string. The Python join() method is a string method, and takes a list of things to join with the string. A simpler example might help explain:The "," is inserted between each element of the given list. In your case, your "list" is the string representation "595", which is treated as the list ["5", "9", "5"].It appears that you're looking for + instead:join takes an iterable thing as an argument.  Usually it's a list.  The problem in your case is that a string is itself iterable, giving out each character in turn. Your code breaks down to this:which acts the same as this:and so produces your string:Strings as iterables is one of the most confusing beginning issues with Python.To append a string, just concatenate it with the + sign.E.g.join connects strings together with a separator. The separator is what you

place right before the join. E.g.Join takes a list of strings as a parameter.join() is for concatenating all list elements. For concatenating just two strings "+" would make more sense:To expand a bit more on what others are saying, if you wanted to use join to simply concatenate your two strings, you would do this:There is a good explanation of why it is costly to use + for concatenating a large number of strings hereOn providing this as input ,Python returns this as output :If this is an input, using the JOIN method, we can add the distance between the words and also convert the list to the string.This is Python output "".join may be used to copy the string in a list to a variable

Redirecting to URL in Flask

iJade

[Redirecting to URL in Flask](https://stackoverflow.com/questions/14343812/redirecting-to-url-in-flask)

I'm new to Python and Flask and I'm trying to do the equivalent of Response.redirect as in C# - ie: redirect to a specific URL - how do I go about this?Here is my code:

2013-01-15 17:55:45Z

I'm new to Python and Flask and I'm trying to do the equivalent of Response.redirect as in C# - ie: redirect to a specific URL - how do I go about this?Here is my code:You have to return a redirect:See the documentation on flask docs. The default value for code is 302 so code=302 can be omitted or replaced by other redirect code (one in 301, 302, 303, 305, and 307).Take a look at the example in the documentation.From the Flask API Documentation (v. 0.10):Here is how you do redirection (3xx) from one url to another in Flask (0.12.2):For other official references, here.Docs can be found here.Flask includes the redirect function for redirecting to any url. Futhermore, you can abort a request early with an error code with abort:By default a black and white error page is shown for each error code.The redirect method takes by default the code 302. A list for http status codes here.For this you can simply use the redirect function that is included in flaskAnother useful tip(as you're new to flask), is to add app.debug = True after initializing the flask object as the debugger output helps a lot while figuring out what's wrong.You can use like this:Here is the referenced link to this code.

How do I disable the security certificate check in Python requests

Paul Draper

[How do I disable the security certificate check in Python requests](https://stackoverflow.com/questions/15445981/how-do-i-disable-the-security-certificate-check-in-python-requests)

I am usingbut I get a request.exceptions.SSLError.

The website has an expired certficate, but I am not sending sensitive data, so it doesn't matter to me.

I would imagine there is an argument like 'verifiy=False' that I could use, but I can't seem to find it.

2013-03-16 05:38:58Z

I am usingbut I get a request.exceptions.SSLError.

The website has an expired certficate, but I am not sending sensitive data, so it doesn't matter to me.

I would imagine there is an argument like 'verifiy=False' that I could use, but I can't seem to find it.From the documentation:If you're using a third-party module and want to disable the checks, here's a context manager that monkey patches requests and changes it so that verify=False is the default and suppresses the warning.Here's how you use it:    Note that this code closes all open adapters that handled a patched request once you leave the context manager. This is because requests maintains a per-session connection pool and certificate validation happens only once per connection so unexpected things like this will happen:Use requests.packages.urllib3.disable_warnings() and verify=False on requests methods.To add to Blender's answer, you can disable SSL for all requests using Session.verify = FalseNote that urllib3, (which Requests uses), strongly discourages making unverified HTTPS requests and will raise an InsecureRequestWarning.If you want to send exactly post request with verify=False option, fastest way is to use this code:Also can be done from the environment variable:

Programmatically stop execution of python script? [duplicate]

Joan Venge

[Programmatically stop execution of python script? [duplicate]](https://stackoverflow.com/questions/543309/programmatically-stop-execution-of-python-script)

Is it possible to stop execution of a python script at any line with a command?Like

2009-02-12 21:18:06Z

Is it possible to stop execution of a python script at any line with a command?Likesys.exit() will do exactly what you want.You could raise SystemExit(0) instead of going to all the trouble to import sys; sys.exit(0).You want sys.exit(). From Python's docs:So, basically, you'll do something like this:The exit() and quit() built in functions do just what you want. No import of sys needed.Alternatively, you can raise SystemExit, but you need to be careful not to catch it anywhere (which shouldn't happen as long as you specify the type of exception in all your try.. blocks).

Why is the use of len(SEQUENCE) in condition values considered incorrect by Pylint?

E_net4 fixes your mistakes

[Why is the use of len(SEQUENCE) in condition values considered incorrect by Pylint?](https://stackoverflow.com/questions/43121340/why-is-the-use-of-lensequence-in-condition-values-considered-incorrect-by-pyli)

