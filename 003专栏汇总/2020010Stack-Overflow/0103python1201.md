2013-02-22 04:50:55Z

I have a csv file which isn't coming in correctly with pandas.read_csv when I  filter the columns with usecols and use multiple indexes.

I expect that df1 and df2 should be the same except for the missing dummy column, but the columns come in mislabeled.  Also the date is getting parsed as a date.  Using column numbers instead of names give me the same problem.  I can workaround the issue by dropping the dummy column after the read_csv step, but I'm trying to understand what is going wrong.  I'm using pandas 0.10.1.edit: fixed bad header usage.The answer by @chip completely misses the point of two keyword arguments.This solution corrects those oddities:Which gives us:This code achieves what you want --- also its weird and certainly buggy:I observed that it works when:a) you specify the index_col rel. to the number of columns you really use -- so its three columns in this example, not four (you drop dummy and start counting from then onwards)b) same for parse_datesc) not so for usecols ;) for obvious reasonsd) here I adapted the names to mirror this behaviourwhich printsIf your csv file contains extra data, columns can be deleted from the DataFrame after import.   Which gives us:You have to just add the index_col=False parameter import csv first and use csv.DictReader its easy to process...

How to set a cell to NaN in a pandas dataframe

Mark Morrisson

[How to set a cell to NaN in a pandas dataframe](https://stackoverflow.com/questions/34794067/how-to-set-a-cell-to-nan-in-a-pandas-dataframe)

I'd like to replace bad values in a column of a dataframe by NaN's.Though, the last line fails and throws a warning because it's working on a copy of df. So, what's the correct way to handle this? I've seen many solutions with iloc or ix but here, I need to use a boolean condition.

2016-01-14 16:00:09Z

I'd like to replace bad values in a column of a dataframe by NaN's.Though, the last line fails and throws a warning because it's working on a copy of df. So, what's the correct way to handle this? I've seen many solutions with iloc or ix but here, I need to use a boolean condition.just use replace:What you're trying is called chain indexing: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copyYou can use loc to ensure you operate on the original dF:While using replace seems to solve the problem, I would like to propose an alternative. Problem with mix of numeric and some string values in the column not to have strings replaced with np.nan, but to make whole column proper. I would bet that original column most likely is of an object typeWhat you really need is to make it a numeric column (it will have proper type and would be quite faster), with all non-numeric values replaced by NaN.Thus, good conversion code would beSpecify errors='coerce' to force strings that can't be parsed to a numeric value to become NaN. Column type would beYou can use replace:Also be aware of the inplace parameter for replace. You can do something like:This will replace all instances in the df without creating a copy.Similarly, if you run into other types of unknown values such as empty string or None value:Reference: Pandas Latest - ReplaceThis solve your problem. With the double [], you are working on a copy of the DataFrame. You have to specify exact location in one call to be able to modify it.You can try these snippets.

pip installs packages successfully, but executables not found from command line

Sanket_Diwale

[pip installs packages successfully, but executables not found from command line](https://stackoverflow.com/questions/35898734/pip-installs-packages-successfully-but-executables-not-found-from-command-line)

I am working on mac OS X Yosemite, version 10.10.3.I installed python2.7 and pip using macport as done in 

http://johnlaudun.org/20150512-installing-and-setting-pip-with-macports/I can successfully install packages and import them inside my python environment and python scripts. However any executable associated with a package that can be called from the command line in the terminal are not found.Does anyone know what might be wrong? (More details below)For example while installing a package called "rosdep" as instructed in http://wiki.ros.org/jade/Installation/SourceI can run: sudo pip install -U rosdep

which installs without errors and corresponding files are located in /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packagesHowever if I try to run : sudo rosdep init,

it gives an error : "sudo: rosdep: command not found"This is not a package specific error. I get this for any package installed using pip on my computer. I even tried adding to my $PATH.

But the executables are not found on the command line, even though the packages work perfectly from within python.

2016-03-09 17:38:44Z

I am working on mac OS X Yosemite, version 10.10.3.I installed python2.7 and pip using macport as done in 

http://johnlaudun.org/20150512-installing-and-setting-pip-with-macports/I can successfully install packages and import them inside my python environment and python scripts. However any executable associated with a package that can be called from the command line in the terminal are not found.Does anyone know what might be wrong? (More details below)For example while installing a package called "rosdep" as instructed in http://wiki.ros.org/jade/Installation/SourceI can run: sudo pip install -U rosdep

which installs without errors and corresponding files are located in /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packagesHowever if I try to run : sudo rosdep init,

it gives an error : "sudo: rosdep: command not found"This is not a package specific error. I get this for any package installed using pip on my computer. I even tried adding to my $PATH.

But the executables are not found on the command line, even though the packages work perfectly from within python.check your $PATHtox has a command line mode:  where is it?and what's in my $PATH?  Notice the /opt/local/Library/Frameworks/Python.framework/Versions/2.7/bin?  That's what allows finding my pip-installed stuffNow, to see where things are from Python, try doing this (substitute rosdep for tox).that prints out:Now, cd to the directory right above lib in the above.  Do you see a bin directory?  Do you see rosdep in that bin?  If so try adding the bin to your $PATH.output:On macOS with the default python installation you need to add /Users/<you>/Library/Python/2.7/bin/ to your $PATH.Add this to your .bash_profile:That's where pip installs the executables.Tip: For non-default python version which python to find the location of your python installation and replace that portion in the path above. (Thanks for the hint Sanket_Diwale)If you're installing using --user (e.g. pip3.6 install --user tmuxp), it is possible to get the platform-specific user install directory from Python itself using the site module.  For example, on macOS:By appending /bin to this, we now have the path where package executables will be installed.  We can dynamically populate the PATH in your shell's rc file based on the output; I'm using bash, but with any luck this is portable:I use the precise Python versions to reduce the chance of the executables just "disappearing" when Python upgrades a minor version, e.g. from 3.5 to 3.6.  They'll disappear because, as can be seen above, the user installation path may include the Python version.  So while python3 could point to 3.5 or 3.6, python3.6 will always point to 3.6.  This needs to be kept in mind when installing further packages, e.g. use pip3.6 over pip3.If you don't mind the idea of packages disappearing, you can use python2 and python3 instead:I know the question asks about macOS, but here is a solution for Linux users who arrive here via Google.I was having the issue described in this question, having installed the pdfx package via pip.When I ran it however, nothing...Yet:The problem on Linux is that pip install ... drops scripts into ~/.local/bin and this is not on the default Debian/Ubuntu $PATH.Here's a GitHub issue going into more detail: https://github.com/pypa/pip/issues/3813To fix, just add ~/.local/bin to your $PATH, for example by adding the following line to your .bashrc file:After that, restart your shell and things should work as expected.I stumbled upon this question because I created, successfully built and published a PyPI Package, but couldn't execute it after installation. The $PATHvariable was correctly set.In my case the problem was that I hadn't set the entry_pointin the setup.py file:On Windows, you need to add the path %USERPROFILE%\AppData\Roaming\Python\Scripts to your path.In addition to adding python's bin directory to $PATH variable, I also had to change the owner of that directory, to make it work. No idea why I wasn't the owner already.

Remove multiple items from a Python list in just one statement

RandomCoder

[Remove multiple items from a Python list in just one statement](https://stackoverflow.com/questions/36268749/remove-multiple-items-from-a-python-list-in-just-one-statement)

In python, I know how to remove items from a list.This above code removes the values 5 and 'item' from item_list.

But when there is a lot of stuff to remove, I have to write many lines of If I know the index of what I am removing, I use:where x is the index of the item I want to remove.If I know the index of all of the numbers that I want to remove, I'll use some sort of loop to del the items at the indices. But what if I don't know the indices of the items I want to remove?I tried item_list.remove('item', 'foo'), but I got an error saying that remove only takes one argument. Is there a way to remove multiple items from a list in a single statement?P.S. I've used del and remove. Can someone explain the difference between these two, or are they the same?Thanks

2016-03-28 18:38:13Z

In python, I know how to remove items from a list.This above code removes the values 5 and 'item' from item_list.

But when there is a lot of stuff to remove, I have to write many lines of If I know the index of what I am removing, I use:where x is the index of the item I want to remove.If I know the index of all of the numbers that I want to remove, I'll use some sort of loop to del the items at the indices. But what if I don't know the indices of the items I want to remove?I tried item_list.remove('item', 'foo'), but I got an error saying that remove only takes one argument. Is there a way to remove multiple items from a list in a single statement?P.S. I've used del and remove. Can someone explain the difference between these two, or are they the same?ThanksIn Python, create a new object is often better than modify an existing one:Which is equivalent to:In case of a big list of filtered out values (here, ('item', 5) is a small set of element), use a set can lead to performance improvement, as the in operation is in O(1) :Note that, as explained in comments and suggested here, the following could save even more time, avoiding the set to be built at each loop:A bloom filter is also a good solution if memory is not cheap.final list after removing should be as followSingle Line Codeoutput would be as follow I don't know why everyone forgot to mention the amazing capability of sets in python. You can simply cast your list into a set and then remove whatever you want to remove in a simple expression like so:I'm reposting my answer from here because I saw it also fits in here.

It allows removing multiple values or removing only duplicates of these values 

and returns either a new list or modifies the given list in place.Docstring extension:You should be clear about what you really want to do, modify an existing list, or make a new list with

the specific items missing. It's important to make that distinction in case you have a second reference pointing

to the existing list. If you have, for example...This may or may not be the behaviour you want.You can use filterfalse function from itertools moduleExampleOutput:I do not exactly understand why you do not like .remove but to get the first index corresponding to a value use .index(value):then remove the corresponding value:.index(value) gets the first occurrence of value, and .remove(value) removes the first occurrence of value

Django template Path

shaytac

[Django template Path](https://stackoverflow.com/questions/3038459/django-template-path)

I'm following the tutorial on http://docs.djangoproject.com/en/dev/intro/tutorial02/#intro-tutorial02 in a Windows 7 environment. My settings file is:I got the base_template from  the template admin/base_site.html from within the default Django admin template directory in the source code of Django itself (django/contrib/admin/templates) into an admin subdirectory of myapp directory as the tutorial instructed. It doesn't seem to take affect for some reason. Any clue of what might be the problem? Do I have to do a sync db?

2010-06-14 15:24:49Z

I'm following the tutorial on http://docs.djangoproject.com/en/dev/intro/tutorial02/#intro-tutorial02 in a Windows 7 environment. My settings file is:I got the base_template from  the template admin/base_site.html from within the default Django admin template directory in the source code of Django itself (django/contrib/admin/templates) into an admin subdirectory of myapp directory as the tutorial instructed. It doesn't seem to take affect for some reason. Any clue of what might be the problem? Do I have to do a sync db?I know this isn't in the Django tutorial, and shame on them, but it's better to set up relative paths for your path variables. You can set it up like so:This way you can move your Django project and your path roots will update automatically. This is useful when you're setting up your production server.Second, there's something suspect to your TEMPLATE_DIRS path. It should point to the root of your template directory. Also, it should also end in a trailing /.I'm just going to guess here that the .../admin/ directory is not your template root. If you still want to write absolute paths you should take out the reference to the admin template directory.With that being said, the template loaders by default should be set up to recursively traverse into your app directories to locate template files.You shouldn't need to copy over the admin templates unless if you specifically want to overwrite something.You will have to run a syncdb if you haven't run it yet. You'll also need to statically server your media files if you're hosting django through runserver.If using Django settings as installed, then why not just use its baked-in, predefined BASE_DIR and TEMPLATES? In the pip installed Django(v1.8), I get: Smart solution in Django 2.0.3 for keeping templates in project directory (/root/templates/app_name):settings.pyin views.py just add such template path:For Django 1.6.6:Also static and media for debug and production mode:Into urls.py you must add:In Django 1.8 you can set template paths, backend and other parameters for templates in one dictionary (settings.py):Official docs.I also had issues with this part of the tutorial (used tutorial for version 1.7).My mistake was that I only edited the 'Django administration' string, and did not pay enough attention to the manual.This is the line from django/contrib/admin/templates/admin/base_site.html:But after some time and frustration it became clear that there was the 'site_header or default:_' statement, which should be removed. So after removing the statement (like the example in the manual everything worked like expected).Example manual:Alright üòÅ Let's say you have a brand new project, if so you would go to settings.py file and search for TEMPLATES once you found it you just paste this line os.path.join(BASE_DIR, 'template') in 'DIRS' At the end, you should get somethings like this : If you want to know where your BASE_DIR directory is located type these 3 simple commands: Once you're in the shell :PS: If you named your template folder with another name, you would change it here too. In django 2.2 this is explained here https://docs.djangoproject.com/en/2.2/howto/overriding-templates/

Titlecasing a string with exceptions

yassin

[Titlecasing a string with exceptions](https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions)

Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?

2010-09-16 16:25:36Z

Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?There are a few problems with this. If you use split and join, some white space characters will be ignored. The built-in capitalize and title methods do not ignore white space. If a sentence starts with an article, you do not want the first word of a title in lowercase.Keeping these in mind:Use the titlecase.py module! Works only for English.GitHub: https://github.com/ppannuto/python-titlecaseThere are these methods:There's no lowercase article option. You'd have to code that yourself, probably by using a list of articles you want to lower.Stuart Colville has made a Python port of a Perl script written by John Gruber to convert strings into title case but avoids capitalizing small words based on rules from the New York Times Manual of style, as well as catering for several special cases.Some of the cleverness of these scripts:You can download it here.This should do. I get it differently.Ok as said in reply above, you have to make a custom capitalize:mytext = u'i am a foobar bazbar'This outputsPython 2.7's title method has a flaw in it.will return Carpenter'S Assistant when value is Carpenter's AssistantThe best solution is probably the one from @BioGeek using titlecase from Stuart Colville.  Which is the same solution proposed by @Etienne.The title starts with capitalized word and that does not match the article.One-liner using list comprehension and the ternary operatorBreakdown:for word in "Wow, a python one liner for titles".split(" ") Splits the string into an list and initiates a for loop (in the list comprehenstion)word.title() if word not in "the a on in of an" else word uses native method title() to title case the string if it's not an article" ".join joins the list elements with a seperator of (space)

Test if numpy array contains only zeros

IUnknown

[Test if numpy array contains only zeros](https://stackoverflow.com/questions/18395725/test-if-numpy-array-contains-only-zeros)

We initialize a numpy array with zeros as bellow:But how do we check whether all elements in a given n*n numpy array matrix is zero.

The method just need to return a True if all the values are indeed zero.

2013-08-23 05:55:33Z

We initialize a numpy array with zeros as bellow:But how do we check whether all elements in a given n*n numpy array matrix is zero.

The method just need to return a True if all the values are indeed zero.Check out numpy.count_nonzero.The other answers posted here will work, but the clearest and most efficient function to use is numpy.any():orI'd use np.all here, if you have an array a:As another answer says, you can take advantage of truthy/falsy evaluations if you know that 0 is the only falsy element possibly in your array. All elements in an array are falsy iff there are not any truthy elements in it.*However, the answer claimed that any was faster than other options due partly to short-circuiting. As of 2018, Numpy's all and any do not short-circuit. If you do this kind of thing often, it's very easy to make your own short-circuiting versions using numba:These tend to be faster than Numpy's versions even when not short-circuiting. count_nonzero is the slowest. Some input to check performance:Check:* Helpful all and any equivalences:If you're testing for all zeros to avoid a warning on another numpy function then wrapping the line in a try, except block will save having to do the test for zeros before the operation you're interested in i.e. 

django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb

getitstarted

[django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb](https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo)

The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.Databse Settings::Thanks a lot for the help !!

2013-03-09 16:00:59Z

The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.Databse Settings::Thanks a lot for the help !!It looks like you don't have the python mysql package installed, try:or if not using a virtual environment (on *nix hosts):you have to install python-mysqldb - Python interface to MySQLTry 

sudo apt-get install python-mysqldbIf you get errors trying to install mysqlclient with pip, you may lack the mysql dev library. Install it by running:and try again to install mysqlclient:When I set up Django development environment for PyCharm in Mac OS X Mountain Lion with python, mysql, sequel pro application I got error same as owner of this thread.

However, my answer for them who is running python-mysqldb under Mac OS Mountain Lion x86_x64 (MySql and Python also should be same architecture) and already tried everything like pip and etc. In order fix this problem do following steps:Currently everything works fine. So I hope it will be helpful for somebody who uses Mac. :)My answer is similar to @Ron-E, but I got a few more errors/corrections so I'm putting my steps below for Mac OSX on Mavericks and Python 2.7.6.You are missing the python mysqldb library. Use this command (for Debian/Ubuntu) to install it:

sudo apt-get install python-mysqldbDownload and install Mysql-python from here for windows environment.  http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python. It is because it did not find sql connector. try:On Ubuntu it is advised to use the distributions repository.This happened with me as well and I believe this has become a common error not only for Django developers but also for Flask as well, so the way I solved this issue was using brew.This way every single issue was solved and both frameworks work absolutely fine.P.S.: For those who use macports (such as myself), this can be an issue as brew works in a different level, my advice is to use brew instead of macportsI hope I could be helpful.For Ubuntu 16.04 and 18.04 or python 3 versionsIf you are using pyhton version 3.4 or above. you have to install in terminal. then install pip install mysqlclient on your virtual env or where you installed pip.I've solved this issue in this environment:./settings.pyIf you use 'ENGINE': 'mysql.connector.django' , install driver executing:  Note that $ pip install mysql-pythondidn't work for me.   Note that if you use  'ENGINE': 'django.db.backends.mysql' , you should install driver executing:

$ pip install mysqlclientFinally execute:

$ python manage.py migrateIf it's all right, python creates all these tables id database: With the same error message as Will, it worked for me to install mysql first as the missing file will be added during the installation.

So afterran without errors.I was having the same problem.The following solved my issueRun pip install pymysql in your shellThen, edit the init.py file in your project origin directory(the same as settings.py)

and thenadd:import pymysqlpymysql.install_as_MySQLdb()this should solve the problem.Just to add to other answers, if you're using Django, it is advisable that you install mysql-python BEFORE installing Django.I wasted a lot of time on this. Turns out that the default database library is not supported for Python 3. You have to use a different one.if the error looks like this then try :Faced similar issue. I tried installing mysql-python using pip, but it failed due to gcc dependency errors.The solution that worked for mePlease note that I already had anaconda installed, which didn't had gcc dependency.try:I recommend to use it in a virtual environment.Maybe you can try the following mode of operation:

sudo python manage.py runserver 0.0.0.0:8000Seems like you don't have permission to the Python folder. Try sudo chown -R $USER /Library/Python/2.7

How to document Python code with doxygen [closed]

Hanno Fietz

[How to document Python code with doxygen [closed]](https://stackoverflow.com/questions/58622/how-to-document-python-code-with-doxygen)

I like doxygen to create documentation of C or PHP code. I have an upcoming Python project and I think I remember that Python doesn't have /* .. */ comments, and also has its own self-documentation facility which seems to be the pythonic way to document.Since I'm familiar with doxygen, how can I use it to produce my Python documentation? Is there anything in particular that I need to be aware of?

2008-09-12 10:26:40Z

I like doxygen to create documentation of C or PHP code. I have an upcoming Python project and I think I remember that Python doesn't have /* .. */ comments, and also has its own self-documentation facility which seems to be the pythonic way to document.Since I'm familiar with doxygen, how can I use it to produce my Python documentation? Is there anything in particular that I need to be aware of?This is documented on the doxygen website, but to summarize here:You can use doxygen to document your Python code. You can either use the Python documentation string syntax:In which case the comments will be extracted by doxygen, but you won't be able to use any of the special doxygen commands.Or you can (similar to C-style languages under doxygen) double up the comment marker (#) on the first line before the member:In that case, you can use the special doxygen commands. There's no particular Python output mode, but you can apparently improve the results by setting OPTMIZE_OUTPUT_JAVA to YES.Honestly, I'm a little surprised at the difference - it seems like once doxygen can detect the comments in ## blocks or """ blocks, most of the work would be done and you'd be able to use the special commands in either case. Maybe they expect people using """ to adhere to more Pythonic documentation practices and that would interfere with the special doxygen commands?The doxypy input filter allows you to use pretty much all of Doxygen's formatting tags in a standard Python docstring format.  I use it to document a large mixed C++ and Python game application framework, and it's working well.In the end, you only have two options:You generate your content using Doxygen, or you generate your content using Sphinx*.There are other options to note:Sphinx is mainly a tool for formatting docs written independently from the source code, as I understand it.For generating API docs from Python docstrings, the leading tools are pdoc and pydoctor. Here's pydoctor's generated API docs for Twisted and Bazaar.Of course, if you just want to have a look at the docstrings while you're working on stuff, there's the "pydoc" command line tool and as well as the help() function available in the interactive interpreter.An other very good documentation tool is sphinx. It will be used for the upcoming python 2.6 documentation and is used by django and a lot of other python projects.From the sphinx website:

What does x[x < 2] = 0 mean in Python?

aberger

[What does x[x < 2] = 0 mean in Python?](https://stackoverflow.com/questions/36603042/what-does-xx-2-0-mean-in-python)

I came across some code with a line similar toPlaying around with variations, I am still stuck on what this syntax does.Examples:

2016-04-13 15:27:15Z

I came across some code with a line similar toPlaying around with variations, I am still stuck on what this syntax does.Examples:This only makes sense with NumPy arrays. The behavior with lists is useless, and specific to Python 2 (not Python 3). You may want to double-check if the original object was indeed a NumPy array (see further below) and not a list.But in your code here, x is a simple list.Sinceis False

i.e 0, thereforex[x<2] is x[0]x[0] gets changed.Conversely, x[x>2] is x[True] or x[1]So, x[1] gets changed.Why does this happen?The rules for comparison are:So, we have the following ordernumeric < list < string < tupleSee the accepted answer for How does Python compare string and int?.If x is a NumPy array, then the syntax makes more sense because of boolean array indexing. In that case, x < 2 isn't a boolean at all; it's an array of booleans representing whether each element of x was less than 2. x[x < 2] = 0 then selects the elements of x that were less than 2 and sets those cells to 0. See Indexing.The bool is simply converted to an integer. The index is either 0 or 1.The original code in your question works only in Python 2. If x is a list in Python 2, the comparison x < y is False if y is an integer. This is because it does not make sense to compare a list with an integer. However in Python 2, if the operands are not comparable, the comparison is based in CPython on the alphabetical ordering of the names of the types; additionally all numbers come first in mixed-type comparisons. This is not even spelled out in the documentation of CPython 2, and different Python 2 implementations could give different results. That is [1, 2, 3, 4, 5] < 2 evaluates to False because 2 is a number and thus "smaller" than a list in CPython. This mixed comparison was eventually deemed to be too obscure a feature, and was removed in Python 3.0.Now, the result of < is a bool; and bool is a subclass of int:So basically you're taking the element 0 or 1 depending on whether the comparison is true or false.If you try the code above in Python 3, you will get TypeError: unorderable types: list() < int() due to a change in Python 3.0:There are many datatypes that overload the comparison operators to do something different (dataframes from pandas, numpy's arrays). If the code that you were using did something else, it was because x was not a list, but an instance of some other class with operator < overridden to return a value that is not a bool; and this value was then handled specially by x[] (aka __getitem__/__setitem__)This has one more use: code golf. Code golf is the art of writing programs that solve some problem in as few source code bytes as possible. is roughly equivalent toexcept that both a and b are evaluated in the first version, but not in the second version.c<d evaluates to True or False.

(a, b) is a tuple.

Indexing on a tuple works like indexing on a list: (3,5)[1] == 5.

True is equal to 1 and False is equal to 0.  or for False:There's a good list on the stack exchange network of many nasty things you can do to python in order to save a few bytes. https://codegolf.stackexchange.com/questions/54/tips-for-golfing-in-pythonAlthough in normal code this should never be used, and in your case it would mean that x acts both as something that can be compared to an integer and as a container that supports slicing, which is a very unusual combination. It's probably Numpy code, as others have pointed out.In general it could mean anything. It was already explained what it means if x is a list or numpy.ndarray but in general it only depends on how the comparison operators (<, >, ...) and also how the get/set-item ([...]-syntax) are implemented.Because:This can be customized to do anything you want. Just as an example (mimics a bit numpys-boolean indexing):So now let's see what happens if you use it:Notice this is just one possibility. You are free to implement almost everything you want.

python location on mac osx

goh

[python location on mac osx](https://stackoverflow.com/questions/6819661/python-location-on-mac-osx)

I'm a little confused with the python on osx. I do not know if the previous owner of the laptop has installed macpython using macport. And I remembered that osx has an builtin version of python. I tried using type -a python and the result returned However running both python at these locations give me [GCC 4.2.1 (Apple Inc. build 5646)] on darwin. Do they both refer to the same builtin python mac provided?I also read that installing macpython one would I looked at Applications, and theres a MacPort folder with python2.6 and the mentioned stuff in it. But running IDLE, i find the same message as above.Hmm I'm a little confused. Which is which?

2011-07-25 16:56:20Z

I'm a little confused with the python on osx. I do not know if the previous owner of the laptop has installed macpython using macport. And I remembered that osx has an builtin version of python. I tried using type -a python and the result returned However running both python at these locations give me [GCC 4.2.1 (Apple Inc. build 5646)] on darwin. Do they both refer to the same builtin python mac provided?I also read that installing macpython one would I looked at Applications, and theres a MacPort folder with python2.6 and the mentioned stuff in it. But running IDLE, i find the same message as above.Hmm I'm a little confused. Which is which?[GCC 4.2.1 (Apple Inc. build 5646)] is the version of GCC that the Python(s) were built with, not the version of Python itself.  That information should be on the previous line.  For example:Items in /usr/bin should always be or link to files supplied by Apple in OS X, unless someone has been ill-advisedly changing things there.  To see exactly where the /usr/local/bin/python is linked to:In this case, that is typical for a python.org installed Python instance or it could be one built from source.On Mac OS X, it's in the Python framework in /System/Library/Frameworks/Python.framework/Resources.Full path is: Btw it's easy to find out where you can find a specific binary: which Python will show you the path of your Python binary (which is probably the same as I posted above).I found the easiest way to locate it, you can use which pythonit will show something like this: /usr/bin/pythonThis one will solve all your problems dealing with Python and Mac:If you have a Mac and you installed python3 like most of us do :) with brew installyour file is located in:How do you know? 

Run:You should get:Now this is a symbolic link, how do you know? Run:and you'll get:which means that your is actually pointing to: If, for some reason, your is not pointing to the place you want, which in our case: just backup it:and run:now create a new symbolic link:and now your is pointing to Check it by running: On High Sierra shows the default python but if you downloaded and installed the latest version from python.org you can find it by:which on my machine showsinstalled with 'brew install python3', found it here 

I checked a few similar discussions and found out the best way to locate all python2/python3 builds is:i found it here: 

/Library/Frameworks/Python.framework/Versions/3.6/binwhich python3 simply result in a path in which the interpreter settles down.Run this in your interactive terminalIt will give you the folder where python is installedrun the following code in a .py file:I have a cook recipe for finding things in linux/macosFirst update the locate db then do a do a /find to find what you are looking for. to update your locate db in macos do this:it sometimes takes a while. Hope this helps :)

Does pandas iterrows have performance issues?

KieranPC

[Does pandas iterrows have performance issues?](https://stackoverflow.com/questions/24870953/does-pandas-iterrows-have-performance-issues)

I have noticed very poor performance when using iterrows from pandas.Is this something that is experienced by others? Is it specific to iterrows and should this function be avoided for data of a certain size (I'm working with 2-3 million rows)?This discussion on GitHub led me to believe it is caused when mixing dtypes in the dataframe, however the simple example below shows it is there even when using one dtype (float64). This takes 36 seconds on my machine:Why are vectorized operations like apply so much quicker? I imagine there must be some row by row iteration going on there too. I cannot figure out how to not use iterrows in my case (this I'll save for a future question). Therefore I would appreciate hearing if you have consistently been able to avoid this iteration. I'm making calculations based on data in separate dataframes. Thank you!---Edit: simplified version of what I want to run has been added below---

2014-07-21 17:19:17Z

I have noticed very poor performance when using iterrows from pandas.Is this something that is experienced by others? Is it specific to iterrows and should this function be avoided for data of a certain size (I'm working with 2-3 million rows)?This discussion on GitHub led me to believe it is caused when mixing dtypes in the dataframe, however the simple example below shows it is there even when using one dtype (float64). This takes 36 seconds on my machine:Why are vectorized operations like apply so much quicker? I imagine there must be some row by row iteration going on there too. I cannot figure out how to not use iterrows in my case (this I'll save for a future question). Therefore I would appreciate hearing if you have consistently been able to avoid this iteration. I'm making calculations based on data in separate dataframes. Thank you!---Edit: simplified version of what I want to run has been added below---Generally, iterrows should only be used in very very specific cases. This is the general order of precedence for performance of various operations:Using a custom cython routine is usually too complicated, so let's skip that for now.1) Vectorization is ALWAYS ALWAYS the first and best choice. However,  there are a small set of cases which cannot be vectorized in obvious ways (mostly involving a recurrence). Further, on a smallish frame, it may be faster to do other methods.3) Apply involves can usually be done by an iterator in Cython space (this is done internally in pandas) (this is a) case.This is dependent on what is going on inside the apply expression. e.g. df.apply(lambda x: np.sum(x)) will be executed pretty swiftly (of course df.sum(1) is even better). However something like: df.apply(lambda x: x['b'] + 1) will be executed in python space, and consequently is slower.4) itertuples does not box the data into a Series, just returns it as a tuple5) iterrows DOES box the data into a Series. Unless you really need this, use another method.6) updating an empty frame a-single-row-at-a-time. I have seen this method used WAY too much. It is by far the slowest. It is probably common place (and reasonably fast for some python structures), but a DataFrame does a fair number of checks on indexing, so this will always be very slow to update a row at a time. Much better to create new structures and concat.Vector operations in Numpy and pandas are much faster than scalar operations in vanilla Python for several reasons:Moral of the story: use the vector operations in Numpy and pandas. They are faster than scalar operations in Python for the simple reason that these operations are exactly what a C programmer would have written by hand anyway. (Except that the array notion is much easier to read than explicit loops with embedded SIMD instructions.)Here's the way to do your problem. This is all vectorized.Another option is to use to_records(), which is faster than both itertuples and iterrows.But for your case, there is much room for other types of improvements.Here's my final optimized versionBenchmark test:Full code:The final version is almost 10x faster than the original code. The strategy is:Yes, Pandas itertuples() is faster than iterrows().

you can refer the documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html"To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally faster than iterrows."

UnicodeEncodeError: 'latin-1' codec can't encode character

ensnare

[UnicodeEncodeError: 'latin-1' codec can't encode character](https://stackoverflow.com/questions/3942888/unicodeencodeerror-latin-1-codec-cant-encode-character)

What could be causing this error when I try to insert a foreign character into the database?And how do I resolve it?Thanks!

2010-10-15 13:57:07Z

What could be causing this error when I try to insert a foreign character into the database?And how do I resolve it?Thanks!Character U+201C Left Double Quotation Mark is not present in the Latin-1 (ISO-8859-1) encoding.It is present in code page 1252 (Western European). This is a Windows-specific encoding that is based on ISO-8859-1 but which puts extra characters into the range 0x80-0x9F. Code page 1252 is often confused with ISO-8859-1, and it's an annoying but now-standard web browser behaviour that if you serve your pages as ISO-8859-1, the browser will treat them as cp1252 instead. However, they really are two distinct encodings:If you are using your database only as a byte store, you can use cp1252 to encode„Äå and other characters present in the Windows Western code page. But still other Unicode characters which are not present in cp1252 will cause errors.You can use encode(..., 'ignore') to suppress the errors by getting rid of the characters, but really in this century you should be using UTF-8 in both your database and your pages. This encoding allows any character to be used. You should also ideally tell MySQL you are using UTF-8 strings (by setting the database connection and the collation on string columns), so it can get case-insensitive comparison and sorting right.I ran into this same issue when using the Python MySQLdb module.  Since MySQL will let you store just about any binary data you want in a text field regardless of character set, I found my solution here:Using UTF8 with Python MySQLdbEdit: Quote from the above URL to satisfy the request in the first comment...The best solution is detail see :I hope your database is at least UTF-8. Then you will need to run yourstring.encode('utf-8') before you try putting it into the database.You are trying to store a Unicode codepoint \u201c using an encoding ISO-8859-1 / Latin-1 that can't describe that codepoint. Either you might need to alter the database to use utf-8, and store the string data using an appropriate encoding, or you might want to sanitise your inputs prior to storing the content; i.e. using something like Sam Ruby's excellent i18n guide. That talks about the issues that windows-1252 can cause, and suggests how to process it, plus links to sample code!SQLAlchemy users can simply specify their field as convert_unicode=True.Example:

sqlalchemy.String(1000, convert_unicode=True)SQLAlchemy will simply accept unicode objects and return them back, handling the encoding itself.DocsLatin-1 (aka ISO 8859-1) is a single octet character encoding scheme, and you can't fit \u201c („Äå) into a byte.Did you mean to use UTF-8 encoding?Use the below snippet to convert the text from Latin to Englishoutput: Python:  You will need to add 

# - * - coding: UTF-8 - * - (remove the spaces around * )

 to the first line of the python file. and then add the following to the text to encode: .encode('ascii', 'xmlcharrefreplace'). This will replace all the unicode characters with it's ASCII equivalent.

filter items in a python dictionary where keys contain a specific string

memo

[filter items in a python dictionary where keys contain a specific string](https://stackoverflow.com/questions/23862406/filter-items-in-a-python-dictionary-where-keys-contain-a-specific-string)

I'm a C coder developing something in python. I know how to do the following in C (and hence in C-like logic applied to python), but I'm wondering what the 'Python' way of doing it is.I have a dictionary d, and I'd like to operate on a subset of the items, only those who's key (string) contains a specific substring.i.e. the C logic would be:I'm imagining the python version would be something likeI've found a lot of posts on here regarding filtering dictionaries, but couldn't find one which involved exactly this.My dictionary is not nested and i'm using python 2.7

2014-05-26 03:46:05Z

I'm a C coder developing something in python. I know how to do the following in C (and hence in C-like logic applied to python), but I'm wondering what the 'Python' way of doing it is.I have a dictionary d, and I'd like to operate on a subset of the items, only those who's key (string) contains a specific substring.i.e. the C logic would be:I'm imagining the python version would be something likeI've found a lot of posts on here regarding filtering dictionaries, but couldn't find one which involved exactly this.My dictionary is not nested and i'm using python 2.7How about a dict comprehension:One you see it, it should be self-explanatory, as it reads like English pretty well.This syntax requires Python 2.7 or greater.In Python 3, there is only dict.items(), not iteritems() so you would use:Go for whatever is most readable and easily maintainable. Just because you can write it out in a single line doesn't mean that you should. Your existing solution is close to what I would use other than I would user iteritems to skip the value lookup, and I hate nested ifs if I can avoid them:However if you realllly want something to let you iterate through a filtered dict then I would not do the two step process of building the filtered dict and then iterating through it, but instead use a generator, because what is more pythonic (and awesome) than a generator?First we create our generator, and good design dictates that we make it abstract enough to be reusable:And then we can use the generator to solve your problem nice and cleanly with simple, understandable code:    In short: generators are awesome.You can use the built-in filter function to filter dictionaries, lists, etc. based on specific conditions.The advantage is that you can use it for different data structures.Jonathon gave you an approach using dict comprehensions in his answer. Here is an approach that deals with your do something part.If you want to do something with the values of the dictionary, you don't need a dictionary comprehension at all:I'm using iteritems() since you tagged your question with python-2.7Now the result will be in a list with some_function applied to each key/value pair of the dictionary, that has foo in its key.If you just want to deal with the values and ignore the keys, just change the list comprehension:some_function can be any callable, so a lambda would work as well:The inner list is actually not required, as you can pass a generator expression to map as well:

copy 2D array into 3rd dimension, N times (Python)

anon01

[copy 2D array into 3rd dimension, N times (Python)](https://stackoverflow.com/questions/32171917/copy-2d-array-into-3rd-dimension-n-times-python)

I'd like to copy a numpy 2D array into a third dimension. For example, given the (2D) numpy array:convert it into a 3D matrix with N such copies in a new dimension.  Acting on arr with N=3, the output should be:

2015-08-23 21:51:14Z

I'd like to copy a numpy 2D array into a third dimension. For example, given the (2D) numpy array:convert it into a 3D matrix with N such copies in a new dimension.  Acting on arr with N=3, the output should be:Probably the cleanest way is to use np.repeat:Having said that, you can often avoid repeating your arrays altogether by using broadcasting. For example, let's say I wanted to add a (3,) vector:to a. I could copy the contents of a 3 times in the third dimension, then copy the contents of c twice in both the first and second dimensions, so that both of my arrays were (2, 2, 3), then compute their sum. However, it's much simpler and quicker to do this:Here, a[..., None] has shape (2, 2, 1) and c[None, None, :] has shape (1, 1, 3)*. When I compute the sum, the result gets 'broadcast' out along the dimensions of size 1, giving me a result of shape (2,  2,  3):Broadcasting is a very powerful technique because it avoids the additional overhead involved in creating repeated copies of your input arrays in memory.* Although I included them for clarity, the None indices into c aren't actually necessary - you could also do a[..., None] + c, i.e. broadcast a (2, 2, 1) array against a (3,) array. This is because if one of the arrays has fewer dimensions than the other then only the trailing dimensions of the two arrays need to be compatible. To give a more complicated example:Another way is to use numpy.dstack.  Supposing that you want to repeat the matrix a num_repeats times:The trick is to wrap the matrix a into a list of a single element, then using the * operator to duplicate the elements in this list num_repeats times.For example, if:This repeats the array of [1 2; 1 2] 5 times in the third dimension.  To verify (in IPython):At the end we can see that the shape of the matrix is 2 x 2, with 5 slices in the third dimension.Introduced in NumPy 1.10.0, we can leverage numpy.broadcast_to to simply generate a 3D view into the 2D input array. The benefit would be no extra memory overhead and virtually free runtime. This would be essential in cases where the arrays are big and we are okay to work with views. Also, this would work with generic n-dim cases.I would use the word stack in place of copy, as readers might confuse it with the copying of arrays that creates memory copies.Stack along first axisIf we want to stack input arr along the first axis, the solution with np.broadcast_to to create 3D view would be -Stack along third/last axisTo stack input arr along the third axis, the solution to create 3D view would be -If we actually need a memory copy, we can always append .copy() there. Hence, the solutions would be -Here's how the stacking works for the two cases, shown with their shape information for a sample case -Same solution(s) would work to extend a n-dim input to n+1-dim view output along the first and last axes. Let's explore some higher dim cases -3D input case :4D input case :and so on.Let's use a large sample 2D case and get the timings and verify output being a view.Let's prove that the proposed solution is a view indeed. We will use stacking along first axis (results would be very similar for stacking along the third axis) -Let's get the timings to show that it's virtually free -Being a view, increasing N from 3 to 3000 changed nothing on timings and both are negligible on timing units. Hence, efficient both on memory and performance!Edit @Mr.F, to preserve dimension order:Here's a broadcasting example that does exactly what was requested.Then b*a is the desired result and (b*a)[:,:,0] produces array([[1, 2],[1, 2]]), which is the original a, as does (b*a)[:,:,1], etc.

Creating functions in a loop

sharvey

[Creating functions in a loop](https://stackoverflow.com/questions/3431676/creating-functions-in-a-loop)

I'm trying to create functions inside of a loop:The problem is that all functions end up being the same. Instead of returning 0, 1, and 2, all three functions return 2:Why is this happening, and what should I do to get 3 different functions that output 0, 1, and 2 respectively?

2010-08-07 19:04:33Z

I'm trying to create functions inside of a loop:The problem is that all functions end up being the same. Instead of returning 0, 1, and 2, all three functions return 2:Why is this happening, and what should I do to get 3 different functions that output 0, 1, and 2 respectively?You're running into a problem with late binding -- each function looks up i as late as possible (thus, when called after the end of the loop, i will be set to 2).  Easily fixed by forcing early binding: change def f(): to def f(i=i): like this:Default values (the right-hand i in i=i is a default value for argument name i, which is the left-hand i in i=i) are looked up at def time, not at call time, so essentially they're a way to specifically looking for early binding.If you're worried about f getting an extra argument (and thus potentially being called erroneously), there's a more sophisticated way which involved using a closure as a "function factory":and in your loop use f = make_f(i) instead of the def statement.The issue here is that the value of i is not saved when the function f is created. Rather, f looks up the value of i when it is called.If you think about it, this behavior makes perfect sense. In fact, it's the only reasonable way functions can work. Imagine you have a function that accesses a global variable, like this:When you read this code, you would - of course - expect it to print "bar", not "foo", because the value of global_var has changed after the function was declared. The same thing is happening in your own code: By the time you call f, the value of i has changed and been set to 2.There are actually many ways to solve this problem. Here are a few options:Caveat: These solutions only work if you assign a new value to the variable. If you modify the object stored in the variable, you'll experience the same problem again:Notice how i still changed even though we turned it into a default argument! If your code mutates i, then you must bind a copy of i to your function, like so:

Save a large file using the Python requests library [duplicate]

Matt Williamson

[Save a large file using the Python requests library [duplicate]](https://stackoverflow.com/questions/14114729/save-a-large-file-using-the-python-requests-library)

I know that fetching a url is as simple as requests.get and I can get at the raw response body and save it to a file, but for large files, is there a way to stream directly to a file? Like if I'm downloading a movie with it or something?

2013-01-01 22:13:59Z

I know that fetching a url is as simple as requests.get and I can get at the raw response body and save it to a file, but for large files, is there a way to stream directly to a file? Like if I'm downloading a movie with it or something?Oddly enough, requests doesn't have anything simple for this. You'll have to iterate over the response and write those chunks to a file:I usually just use urllib.urlretrieve(). It works, but if you need to use a session or some sort of authentication, the above code works as well.

How to„Äåselect distinct„Äçacross multiple data frame columns in pandas?

Jody

[How to„Äåselect distinct„Äçacross multiple data frame columns in pandas?](https://stackoverflow.com/questions/30530663/how-to-select-distinct-across-multiple-data-frame-columns-in-pandas)

I'm looking for a way to do the equivalent to the sql The pandas sql comparison doesn't have anything about "distinct".unique() only works for a single column, so I suppose I could concat the columns, or put them in a list/tuple and compare that way, but this seems like something pandas should do in a more native way.  Am I missing something obvious, or is there no way to do this?

2015-05-29 13:17:32Z

I'm looking for a way to do the equivalent to the sql The pandas sql comparison doesn't have anything about "distinct".unique() only works for a single column, so I suppose I could concat the columns, or put them in a list/tuple and compare that way, but this seems like something pandas should do in a more native way.  Am I missing something obvious, or is there no way to do this?You can use the drop_duplicates method to get the unique rows in a DataFrame:You can also provide the subset keyword argument if you only want to use certain columns to determine uniqueness. See the docstring.I've tried different solutions. First was:and it works well for not object data

Another way to do this and to avoid error (for object columns type) is to apply drop_duplicates() You can also use SQL to do this, but it worked very slow in my case:There is no unique method for a df, if the number of unique values for each column were the same then the following would work: df.apply(pd.Series.unique) but if not then you will get an error. Another approach would be to store the values in a dict which is keyed on the column name:To solve a similar problem, I'm using groupby:Whether that's appropriate will depend on what you want to do with the result, though (in my case, I just wanted the equivalent of COUNT DISTINCT as shown).I think use drop duplicate sometimes will not so useful depending dataframe.I found this:And work for me!https://riptutorial.com/pandas/example/26077/select-distinct-rows-across-dataframeYou can take the sets of the columns and just subtract the smaller set from the larger set:

ValueError: numpy.dtype has the wrong size, try recompiling

Amber Chen

[ValueError: numpy.dtype has the wrong size, try recompiling](https://stackoverflow.com/questions/17709641/valueerror-numpy-dtype-has-the-wrong-size-try-recompiling)

I just installed pandas and statsmodels package on my python 2.7

When I tried  "import pandas as pd", this error message comes out.

Can anyone help? Thanks!!!

2013-07-17 20:30:55Z

I just installed pandas and statsmodels package on my python 2.7

When I tried  "import pandas as pd", this error message comes out.

Can anyone help? Thanks!!!(to expand a bit on my comment)Numpy developers follow in general a policy of keeping a backward compatible binary interface (ABI). However, the ABI is not forward compatible.What that means:A package, that uses numpy in a compiled extension, is compiled against a specific version of numpy. Future version of numpy will be compatible with the compiled extension of the package (for exception see below).

Distributers of those other packages do not need to recompile their package against a newer  versions of numpy and users do not need to update these other packages, when users update to a newer version of numpy.However, this does not go in the other direction. If a package is compiled against a specific numpy version, say 1.7, then there is no guarantee that the binaries of that package will work with older numpy versions, say 1.6, and very often or most of the time they will not.The binary distribution of packages like pandas and statsmodels, that are compiled against a recent version of numpy, will not work when an older version of numpy is installed.

Some packages, for example matplotlib, if I remember correctly, compile their extensions against the oldest numpy version that they support. In this case, users with the same old or any more recent version of numpy can use those binaries.The error message in the question is a typical result of binary incompatibilities.The solution is to get a binary compatible version, either by updating numpy to at least the version against which pandas or statsmodels were compiled, or to recompile pandas and statsmodels against the older version of numpy that is already installed.Breaking the ABI backward compatibility:Sometimes improvements or refactorings in numpy break ABI backward compatibility. This happened (unintentionally) with numpy 1.4.0.

As a consequence, users that updated numpy to 1.4.0, had binary incompatibilities with all other compiled packages, that were compiled against a previous version of numpy. This requires that all packages with binary extensions that use numpy have to be recompiled to work with the ABI incompatible version. For me (Mac OS X Maverics, Python 2.7)helped. After this you can install up-to-date packages pandas, scikit-learn, e.t.c. using pip:I found it to be a simple version being outdated or mismatch and was fixed with: Or might work with the one liner:I had a similar error with another library and realized that I had several versions of numpy installed on my system. The fix for me was to edit my PYTHONPATH and put the site-packages that contained the latest version of numpy in first position.As in here, for me only sudo pip install pandas==0.13.1 worked I also encounter this error when use pandas to access MYSQL.

This error message indicates a binary compatible issue and can be resolved by 

using latest version of pandas and numpy package. 

Here is my steps to resolve this issue, and it works well on my Ubuntu 12.04:In my case, I had installed pandas-0.10.0.win-amd64-py2.7 but was checking to see if a bug had been fixed in a more recent version of pandas.  So I did an easy_install -U to force the upgrade, but then got the above error due to some incompatibilities with numpy etc... when I did To fix, I just reinstalled the pandas-0.10.0.win-amd64-py2.7 binary and everything works. I didn't see this answer (suggests to use pip) which may have helped me (though not sure) Install particular version with easy_installAlso this highlights why one should use virtualenv (which I wasn't).For me (Mac OS X Mavericks) it worked to install the version for python2.6:then run:The problem I solved on Webfaction was old numpy library(1.5) which was in conflict with my fresh installation in .virtualenv. The problem was solved after I did pip install pandas out of the virtual environment.

The idea came from discussion on https://github.com/pydata/pandas/issues/3711, thanks, cpcloud!I just meet this 'ValueError' issue and have addressed it. Definitely there's something wrong with numpy package. But when I try to pip install --upgrade numpy it failed, so I uninstall and download the newest numpy.zip file.

Then manually uncompress and python setup.py install it.Luckly, it works!Like @user333700 said, required versions of libraries may not meet for each other. You get one library as another's dependency. Then without knowing it was already installed as dependency, you need that specific library and you install one version. With such ways dependencies may mess up.I lived such a case and looked for a solution. Found this:

https://stackoverflow.com/a/12975518/1694344I had two different versions for egg-info file and folder name of numpy:I removed them all and reinstalled numpy with pip.I had a similar issue, and simply re-installing using pip install ... as suggested in previous comments didn't work.What worked for me was re-installing with the added flag pip install --no-cache-dir ..., seems there was an incompatible numpy version somewhere in the cache.There are cases where you want to keep a specific NumPy version and the upgrade option mentioned here will not work. 

An example that occurred to me was the Python distribution preinstalled with ArcGIS. For ArcPy to work in ArcGIS 10.5.1, that distribution needs to be Python 2.7.12 with NumPy 1.9.3 and any other version of NumPy is probably going to cause issues with your ArcPy functionality. What you can do with this case is try to install a specific, older version of the problematic third-party library that is supposed to be compatible with the older NumPy version that ArcGIS has. For instance, scikit-learn 0.19.1 would NOT operate with NumPy 1.9.3 and would result in the same error you mentioned. However, scikit-learn 0.15 works fine. You can test different versions to find the one that works. Just mention the version number through pip:

Getting„Äåglobal name 'foo' is not defined„Äçwith Python's timeit

Kyle Cronin

[Getting„Äåglobal name 'foo' is not defined„Äçwith Python's timeit](https://stackoverflow.com/questions/551797/getting-global-name-foo-is-not-defined-with-pythons-timeit)

I'm trying to find out how much time it takes to execute a Python statement, so I looked online and found that the standard library provides a module called timeit that purports to do exactly that:However, this produces an error:I'm still new to Python and I don't fully understand all the scoping issues it has, but I don't know why this snippet doesn't work. Any thoughts?

2009-02-15 23:15:40Z

I'm trying to find out how much time it takes to execute a Python statement, so I looked online and found that the standard library provides a module called timeit that purports to do exactly that:However, this produces an error:I'm still new to Python and I don't fully understand all the scoping issues it has, but I don't know why this snippet doesn't work. Any thoughts?Change this line:To this:Check out the link you provided at the very bottom.I just tested it on my machine and it worked with the changes.You can try this hack:With Python 3, you can use globals=globals() From the documentation:Since timeit doesn't have your stuff in scope.add into your setup "import thisfile; "then when you call the setup function myfunc() use  "thisfile.myfunc()"eg  "thisfile.py"

Check if file is symlink in python

Bandicoot

[Check if file is symlink in python](https://stackoverflow.com/questions/11068419/check-if-file-is-symlink-in-python)

In python, is there a function to check if a given file/directory is a symlink ? For example, for the below files, my wrapper function should return True.

2012-06-17 02:03:57Z

In python, is there a function to check if a given file/directory is a symlink ? For example, for the below files, my wrapper function should return True.To determine if a directory entry is a symlink use this:For instance, given:For python 3.4 and up, you can use the Path classYou have to be careful when using the is_symlink() method.  It will return True even the target of the link is non-existent as long as the the named object is a symlink.  For example (Linux/Unix):Then, in your current directory fire up pythonThe programmer has to decide what he/she realy wants.  Python 3 seems to have renamed a lots of classes. It might be worthwhile to read the manual page for the Path class: https://docs.python.org/3/library/pathlib.htmlWithout the intention to bloat this topic, but I was redirected to this page as I was looking for symlink's to find them and convert them to real files and found this script within the python tools library.

Dead simple example of using Multiprocessing Queue, Pool and Locking

thclpr

[Dead simple example of using Multiprocessing Queue, Pool and Locking](https://stackoverflow.com/questions/20887555/dead-simple-example-of-using-multiprocessing-queue-pool-and-locking)

I tried to read the documentation at http://docs.python.org/dev/library/multiprocessing.html but I'm  still struggling with multiprocessing Queue, Pool and Locking. And for now I was able to build the example below.Regarding Queue and Pool, I'm not sure if I understood the concept in the right way, so correct me if I'm wrong.  What I'm trying to achieve is to 

process 2 requests at time ( data list have 8 in this example ) so, what should I use? Pool to create 2 processes that can handle two different queues ( 2 at max ) or should I just use Queue to process 2 inputs each time? The lock would be to print the outputs correctly.

2014-01-02 16:45:52Z

I tried to read the documentation at http://docs.python.org/dev/library/multiprocessing.html but I'm  still struggling with multiprocessing Queue, Pool and Locking. And for now I was able to build the example below.Regarding Queue and Pool, I'm not sure if I understood the concept in the right way, so correct me if I'm wrong.  What I'm trying to achieve is to 

process 2 requests at time ( data list have 8 in this example ) so, what should I use? Pool to create 2 processes that can handle two different queues ( 2 at max ) or should I just use Queue to process 2 inputs each time? The lock would be to print the outputs correctly.The best solution for your problem is to utilize a Pool. Using Queues and having a separate "queue feeding" functionality is probably overkill.Here's a slightly rearranged version of your program, this time with only 2 processes coralled in a Pool. I believe it's the easiest way to go, with minimal changes to original code: Note that mp_worker() function now accepts a single argument (a tuple of the two previous arguments) because the map() function chunks up your input data into sublists, each sublist given as a single argument to your worker function.Output:Edit as per @Thales comment below:If you want "a lock for each pool limit" so that your processes run in tandem pairs, ala:A waiting B waiting | A done , B done | C waiting , D waiting | C done, D done | ...then change the handler function to launch pools (of 2 processes) for each pair of data:Now your output is:This might be not 100% related to the question, but on my search for an example of using multiprocessing with a queue this shows up first on google.This is a basic example class that you can instantiate and put items in a queue and can wait until queue is finished. That's all I needed.Here is my personal goto for this topic:Gist here, (pull requests welcome!):

https://gist.github.com/thorsummoner/b5b1dfcff7e7fdd334ecFor everyone using editors like Komodo Edit (win10) add sys.stdout.flush() to:or as first line to:This helps to see what goes on during the run of the script; in stead of having to look at the black command line box.Here is an example from my code (for threaded pool, but just change class name and you'll have process pool): Basically: 

Opencv - Depth map from uncalibrated stereo system

user3601754

[Opencv - Depth map from uncalibrated stereo system](https://stackoverflow.com/questions/36172913/opencv-depth-map-from-uncalibrated-stereo-system)

I'm trying to get a depth map from an uncalibrated method.

I can obtain the fundamental matrix via different correspondent points from SIFT method and cv2.findFundamentalMat. Then with cv2.stereoRectifyUncalibrated I can get the rectification matrix. Finally I can use cv2.warpPerspective to rectify and compute the disparity but this latter doesn't conduct to a good depth map. The values are very high so I'm wondering if I have to use warpPerspective or I have to calculate rotation matrix from homography matrix got with stereoRectifyUncalibrated.I'm not sure of the projective matrix with the case of homography matrix obtained with the stereoRectifyUncalibrated to rectify.A part of the code :Here the rectified pictures with uncalibrated method (and warpPerspective) :

Here the rectified pictures with calibrated method :

I don't know how the difference is so important between the two kind of pictures...and for the calibrated method, it doesn't seem aligned...strange

The disparity map of the uncalibrated method :  And the depth map are calculated with : C1[0,0]*T[0]/(disp)

with T from the stereoCalibrate but the values are very high...-------- EDIT LATER ------------I tried to "mount" the reconstruction matrix ([Devernay97], [Garcia01]) with the homography matrix obtained with the "stereoRectifyUncalibrated" but the result are not good... Is my use correct? 

2016-03-23 08:17:53Z

I'm trying to get a depth map from an uncalibrated method.

I can obtain the fundamental matrix via different correspondent points from SIFT method and cv2.findFundamentalMat. Then with cv2.stereoRectifyUncalibrated I can get the rectification matrix. Finally I can use cv2.warpPerspective to rectify and compute the disparity but this latter doesn't conduct to a good depth map. The values are very high so I'm wondering if I have to use warpPerspective or I have to calculate rotation matrix from homography matrix got with stereoRectifyUncalibrated.I'm not sure of the projective matrix with the case of homography matrix obtained with the stereoRectifyUncalibrated to rectify.A part of the code :Here the rectified pictures with uncalibrated method (and warpPerspective) :

Here the rectified pictures with calibrated method :

I don't know how the difference is so important between the two kind of pictures...and for the calibrated method, it doesn't seem aligned...strange

The disparity map of the uncalibrated method :  And the depth map are calculated with : C1[0,0]*T[0]/(disp)

with T from the stereoCalibrate but the values are very high...-------- EDIT LATER ------------I tried to "mount" the reconstruction matrix ([Devernay97], [Garcia01]) with the homography matrix obtained with the "stereoRectifyUncalibrated" but the result are not good... Is my use correct? 

How to plot a histogram using Matplotlib in Python with a list of data?

DataVizGuys

[How to plot a histogram using Matplotlib in Python with a list of data?](https://stackoverflow.com/questions/33203645/how-to-plot-a-histogram-using-matplotlib-in-python-with-a-list-of-data)

I am trying to plot a histogram using the matplotlib.hist() function but I am not sure how to do it.I have a list and a list of names(strings).How do I make the probability as my y-value of each bar and names as x-values? 

2015-10-18 21:46:59Z

I am trying to plot a histogram using the matplotlib.hist() function but I am not sure how to do it.I have a list and a list of names(strings).How do I make the probability as my y-value of each bar and names as x-values? If you want a histogram, you don't need to attach any 'names' to x-values, as on x-axis you would have bins:However, if you have limited number of data points, and you want a bar plot, then you may attach labels to x-axis:Let me know if this solves your problem.EDIT 26 November 2018As per comment below, the following code will suffice as of Matplotlib 3.0.2:EDIT 23 May 2019As far as histogram is concerned, the normed param is deprecated:So, as from Matplolib 3.1 instead of:one has to write:If you haven't installed matplotlib yet just try the command.Though the question appears to be demanding plotting a histogram using matplotlib.hist() function, it can arguably be not done using the same as the latter part of the question demands to use the given probabilities as the y-values of bars and given names(strings) as the x-values.I'm assuming a sample list of names corresponding to given probabilities to draw the plot. A simple bar plot serves the purpose here for the given problem. The following code can be used:This is a very round-about way of doing it but if you want to make a histogram where you already know the bin values but dont have the source data, you can use the np.random.randint function to generate the correct number of values within the range of each bin for the hist function to graph, for example:as for labels you can align x ticks with bins to get something like this:

What is the currently correct way to dynamically update plots in Jupyter/iPython?

Nathaniel

[What is the currently correct way to dynamically update plots in Jupyter/iPython?](https://stackoverflow.com/questions/34486642/what-is-the-currently-correct-way-to-dynamically-update-plots-in-jupyter-ipython)

In the answers to how to dynamically update a plot in a loop in ipython notebook (within one cell), an example is given of how to dynamically update a plot inside a Jupyter notebook within a Python loop. However, this works by destroying and re-creating the plot on every iteration, and a comment in one of the threads notes that this situation can be improved by using the new-ish %matplotlib nbagg magic, which provides an interactive figure embedded in the notebook, rather than a static image.However, this wonderful new nbagg feature seems to be completely undocumented as far as I can tell, and I'm unable to find an example of how to use it to dynamically update a plot. Thus my question is, how does one efficiently update an existing plot in a Jupyter/Python notebook, using the nbagg backend? Since dynamically updating plots in matplotlib is a tricky issue in general, a simple working example would be an enormous help. A pointer to any documentation on the topic would also be extremely helpful.To be clear what I'm asking for: what I want to do is to run some simulation code for a few iterations, then draw a plot of its current state, then run it for a few more iterations, then update the plot to reflect the current state, and so on. So the idea is to draw a plot and then, without any interaction from the user, update the data in the plot without destroying and re-creating the whole thing.Here is some slightly modified code from the answer to the linked question above, which achieves this by re-drawing the whole figure every time. I want to achieve the same result, but more efficiently using nbagg.

2015-12-28 01:21:11Z

In the answers to how to dynamically update a plot in a loop in ipython notebook (within one cell), an example is given of how to dynamically update a plot inside a Jupyter notebook within a Python loop. However, this works by destroying and re-creating the plot on every iteration, and a comment in one of the threads notes that this situation can be improved by using the new-ish %matplotlib nbagg magic, which provides an interactive figure embedded in the notebook, rather than a static image.However, this wonderful new nbagg feature seems to be completely undocumented as far as I can tell, and I'm unable to find an example of how to use it to dynamically update a plot. Thus my question is, how does one efficiently update an existing plot in a Jupyter/Python notebook, using the nbagg backend? Since dynamically updating plots in matplotlib is a tricky issue in general, a simple working example would be an enormous help. A pointer to any documentation on the topic would also be extremely helpful.To be clear what I'm asking for: what I want to do is to run some simulation code for a few iterations, then draw a plot of its current state, then run it for a few more iterations, then update the plot to reflect the current state, and so on. So the idea is to draw a plot and then, without any interaction from the user, update the data in the plot without destroying and re-creating the whole thing.Here is some slightly modified code from the answer to the linked question above, which achieves this by re-drawing the whole figure every time. I want to achieve the same result, but more efficiently using nbagg.Here is an example that updates a plot in a loop. It updates the data in the figure and does not redraw the whole figure every time. It does block execution, though if you're interested in running a finite set of simulations and saving the results somewhere, it may not be a problem for you.I put this up on nbviewer here.There is an IPython Widget version of nbagg that is currently a work in progress at the Matplotlib repository. When that is available, that will probably be the best way to use nbagg.EDIT: updated to show multiple plotsI'm using jupyter-lab and this works for me (adapt it to your case):Then in a loop you populate a dictionary and you pass it to live_plot():make sure you have a few cells below the plot, otherwise the view snaps in place each time the plot is redrawn.

Django signals vs. overriding save method

imjoevasquez

[Django signals vs. overriding save method](https://stackoverflow.com/questions/170337/django-signals-vs-overriding-save-method)

I'm having trouble wrapping my head around this.  Right now I have some models that looks kind of like this:A Review is has several "scores", the overall_score is the average of the scores.  When a review or a score is saved, I need to recalculate the overall_score average.  Right now I'm using a overridden save method.  Would there be any benefits to using Django's signal dispatcher?

2008-10-04 13:37:12Z

I'm having trouble wrapping my head around this.  Right now I have some models that looks kind of like this:A Review is has several "scores", the overall_score is the average of the scores.  When a review or a score is saved, I need to recalculate the overall_score average.  Right now I'm using a overridden save method.  Would there be any benefits to using Django's signal dispatcher?Save/delete signals are generally favourable in situations where you need to make changes which aren't completely specific to the model in question, or could be applied to models which have something in common, or could be configured for use across models.One common task in overridden save methods is automated generation of slugs from some text field in a model. That's an example of something which, if you needed to implement it for a number of models, would benefit from using a pre_save signal, where the signal handler could take the name of the slug field and the name of the field to generate the slug from. Once you have something like that in place, any enhanced functionality you put in place will also apply to all models - e.g. looking up the slug you're about to add for the type of model in question, to ensure uniqueness.Reusable applications often benefit from the use of signals - if the functionality they provide can be applied to any model, they generally (unless it's unavoidable) won't want users to have to directly modify their models in order to benefit from it.With django-mptt, for example, I used the pre_save signal to manage a set of fields which describe a tree structure for the model which is about to be created or updated and the pre_delete signal to remove tree structure details for the object being deleted and its entire sub-tree of objects before it and they are deleted. Due to the use of signals, users don't have to add or modify save or delete methods on their models to have this management done for them, they just have to let django-mptt know which models they want it to manage.You asked: Would there be any benefits to using Django's signal dispatcher?I found this in the django docs:From: Overriding predefined model methodsIf you'll use signals you'd be able to update Review score each time related score model gets saved. But if don't need such functionality i don't see any reason to put this into signal, that's pretty model-related stuff.It is a kind sort of denormalisation. Look at this pretty solution. In-place composition field definition.Small addition from Django docs about bulk delete (.delete() method on QuerySet objects):https://docs.djangoproject.com/en/1.11/topics/db/queries/#deleting-objectsAnd bulk update (.update() method on QuerySet objects):https://docs.djangoproject.com/en/2.1/ref/models/querysets/#updateSignals are useful when you have to execute some long term process and don't want to block your user waiting for save to complete.

Python nested functions variable scoping [duplicate]

Stefan Manastirliu

[Python nested functions variable scoping [duplicate]](https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping)

I've read almost all the other questions about the topic, but my code still doesn't work.I think I'm missing something about python variable scope.Here is my code:And I get I know the problem is on the _total assignment, but I can't understand why.

Shouldn't recurse() have access to the parent function's variables?Can someone explain to me what I'm missing about python variable scope?

2011-03-07 11:05:45Z

I've read almost all the other questions about the topic, but my code still doesn't work.I think I'm missing something about python variable scope.Here is my code:And I get I know the problem is on the _total assignment, but I can't understand why.

Shouldn't recurse() have access to the parent function's variables?Can someone explain to me what I'm missing about python variable scope?When I run your code I get this error:This problem is caused by this line:The documentation about Scopes and Namespaces  says this:So since the line is effectively saying:it creates _total in the namespace of recurse().  Since _total is then new and unassigned you can't use it in the addition.Here's an illustration that gets to the essence of David's answer. With the statement b = 4 commented out, this code outputs 0 1, just what you'd expect.But if you uncomment that line, on the line print b, you get the errorIt seems mysterious that the presence of b = 4 might somehow make b disappear on the lines that precede it. But the text David quotes explains why: during static analysis, the interpreter determines that b is assigned to in inner, and that it is therefore a local variable of inner. The print line attempts to print the b in that inner scope before it has been assigned.In Python 3, you can use the nonlocal statement to access non-local, non-global scopes.Rather than declaring a special object or map or array,

one can also use a function attribute.

This makes the scoping of the variable really clear.Of course this attribute belongs to the function (defintion), and not to the function call.

So one must be mindful of threading and recursion.This is a variation of redman's solution, but using a proper namespace instead of an array to encapsulate the variable:I'm not sure if using a class object this way is considered an ugly hack or a proper coding technique in the python community, but it works fine in python 2.x and 3.x (tested with 2.7.3 and 3.2.3). I'm also unsure about the run-time efficiency of this solution.You probably have gotten the answer to your question. But i wanted to indicate a way i ussually get around this and that is by using lists. For instance, if i want to do this: I would instead do this:This way X is never a local variableWhile I used to use @redman's list-based approach, it's not optimal in terms of readability.Here is a modified @Hans' approach, except I use an attribute of the inner function, rather than the outer. This should be more compatible with recursion, and maybe even multithreading:This prints:If I s/inner.attribute/outer.attribute/g, we get:So, indeed, it seems better to make them the inner function's attributes.Also, it seems sensible in terms of readability: because then the variable conceptually relates to the inner function, and this notation reminds the reader that the variable is shared between the scopes of the inner and the outer functions. A slight downside for the readability is that the inner.attribute may only be set syntactically after the def inner(): ....More from a philosophical point of view, one answer might be "if you're having namespace problems, give it a namespace of its very own!"Providing it in its own class not only allows you to encapsulate the problem but also makes testing easier, eliminates those pesky globals, and reduces the need to shovel variables around between various top-level functions (doubtless there'll be more than just get_order_total).Preserving the OP's code to focus on the essential change,As a PS, one hack which is a variant on the list idea in another answer, but perhaps clearer,as you see, total is in the local scope of the main function, but it's not in the local scope of recurse (obviously) but neither it is in the global scope, 'cause it's defined only in the local scope of get_order_totalMy way around...

How do I gzip compress a string in Python?

Bdfy

[How do I gzip compress a string in Python?](https://stackoverflow.com/questions/8506897/how-do-i-gzip-compress-a-string-in-python)

How do I gzip compress a string in Python?gzip.GzipFile exists, but that's for file objects - what about with plain strings?

2011-12-14 15:16:18Z

How do I gzip compress a string in Python?gzip.GzipFile exists, but that's for file objects - what about with plain strings?If you want to produce a complete gzip-compatible binary string, with the header etc, you could use gzip.GzipFile together with StringIO:The easiest way is the zlib encoding:Then you decompress it with:Python3 version of Sven Marnach's 2011 answer:For those who want to compress a Pandas dataframe in JSON format:Tested with Python 3.6 and Pandas 0.23

Removing python module installed in develop mode

copyninja

[Removing python module installed in develop mode](https://stackoverflow.com/questions/3606457/removing-python-module-installed-in-develop-mode)

Hi I was trying the python packaging using setuptools and to test I installed the module in develop mode.

i.eThis has added my modules directory to sys.path. Now I want to remove the module is there any way to do this?Thanks in advance

2010-08-31 06:13:54Z

Hi I was trying the python packaging using setuptools and to test I installed the module in develop mode.

i.eThis has added my modules directory to sys.path. Now I want to remove the module is there any way to do this?Thanks in advanceUse the --uninstall or -u option to develop, i.e:This will remove it from easy-install.pth and delete the .egg-link.  The only thing it doesn't do is delete scripts (yet).Edit easy-install.pth in your site-packages directory and remove the line that points to your development version of that package.I have had a similar problem to this before. What I did was I loaded up the Python shell, imported the module and then printed its __file__ attribute. From there I would just remove the folder or file that was being associated.What you may want to look into is using virtualenv this system allows you to create a instance of python separate from your system. Any modules you install or use in this instance are self contained including the version of the module. I keep all my projects now inside of there own contained virtualenv, which allows me to install and use whatever modules I want without worrying about screwing up modules from other projects.

How to normalize a 2-dimensional numpy array in python less verbose?

Aufwind

[How to normalize a 2-dimensional numpy array in python less verbose?](https://stackoverflow.com/questions/8904694/how-to-normalize-a-2-dimensional-numpy-array-in-python-less-verbose)

Given a 3 times 3 numpy arrayTo normalize the rows of the 2-dimensional array I thought ofThere must be a better way, isn't there?Perhaps to clearify: By normalizing I mean, the sum of the entrys per row must be one. But I think that will be clear to most people.

2012-01-18 03:12:41Z

Given a 3 times 3 numpy arrayTo normalize the rows of the 2-dimensional array I thought ofThere must be a better way, isn't there?Perhaps to clearify: By normalizing I mean, the sum of the entrys per row must be one. But I think that will be clear to most people.Broadcasting is really good for this:row_sums[:, numpy.newaxis] reshapes row_sums from being (3,) to being (3, 1). When you do a / b, a and b are broadcast against each other.You can learn more about broadcasting here or even better here.Scikit-learn has a normalize function that lets you apply various normalizations. The "make it sum to 1" is the L1 norm, and to take that do:Now your rows will sum to 1.I think this should work,In case you are trying to normalize each row such that its magnitude is one (i.e. a row's unit length is one or the sum of the square of each element in a row is one):Verifying:it appears that this also worksYou could also use matrix transposition:You could use built-in numpy function: 

np.linalg.norm(a, axis = 1, keepdims = True)Or using lambda function, likeeach vector of vec will have a unit norm.I think you can normalize the row elements sum to 1 by this:

    new_matrix = a / a.sum(axis=1, keepdims=1).

And the column normalization can be done with new_matrix = a / a.sum(axis=0, keepdims=1). Hope this can hep.where input_data is the name of your 2D array

Python: tf-idf-cosine: to find document similarity

add-semi-colons

[Python: tf-idf-cosine: to find document similarity](https://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity)

I was following a tutorial which was available at Part 1 & Part 2. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from stackoverflow, included is the code mentioned in the above link (just so as to make life easier)as a result of the above code I have the following matrixI am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors.

2012-08-25 02:41:26Z

I was following a tutorial which was available at Part 1 & Part 2. Unfortunately the author didn't have the time for the final section which involved using cosine similarity to actually find the distance between two documents. I followed the examples in the article with the help of the following link from stackoverflow, included is the code mentioned in the above link (just so as to make life easier)as a result of the above code I have the following matrixI am not sure how to use this output in order to calculate cosine similarity, I know how to implement cosine similarity with respect to two vectors of similar length but here I am not sure how to identify the two vectors.First off, if you want to extract count features and apply TF-IDF normalization and row-wise euclidean normalization you can do it in one operation with TfidfVectorizer:Now to find the cosine distances of one document (e.g. the first in the dataset) and all of the others you just need to compute the dot products of the first vector with all of the others as the tfidf vectors are already row-normalized.  As explained by Chris Clark in comments and here Cosine Similarity does not take into account the magnitude of the vectors. Row-normalised have a magnitude of 1 and so the Linear Kernel is sufficient to calculate the similarity values.The scipy sparse matrix API is a bit weird (not as flexible as dense N-dimensional numpy arrays). To get the first vector you need to slice the matrix row-wise to get a submatrix with a single row:scikit-learn already provides pairwise metrics (a.k.a. kernels in machine learning parlance) that work for both dense and sparse representations of vector collections. In this case we need a dot product that is also known as the linear kernel:Hence to find the top 5 related documents, we can use argsort and some negative array slicing (most related documents have highest cosine similarity values, hence at the end of the sorted indices array):The first result is a sanity check: we find the query document as the most similar document with a cosine similarity score of 1 which has the following text:The second most similar document is a reply that quotes the original message hence has many common words:WIth the Help of @excray's comment, I manage to figure it out the answer, What we need to do is actually write a simple for loop to iterate over the two arrays that represent the train data and test data. First implement a simple lambda function to hold formula for the cosine calculation:And then just write a simple for loop to iterate over the to vector, logic is for every "For each vector in trainVectorizerArray, you have to find the cosine similarity with the vector in testVectorizerArray."Here is the output:I know its an old post. but I tried the http://scikit-learn.sourceforge.net/stable/ package. here is my code to find the cosine similarity. The question was how will you calculate the cosine similarity with this package and here is my code for thatHere suppose the query is the first element of train_set and doc1,doc2 and doc3 are the documents which I want to rank with the help of cosine similarity. then I can use this code. Also the tutorials provided in the question was very useful. Here are all the parts for it 

part-I,part-II,part-IIIthe output will be as follows :here 1 represents that query is matched with itself and the other three are the scores for matching the query with the respective documents.Let me give you another tutorial written by me. It answers your question, but also makes an explanation why we are doing some of the things. I also tried to make it concise. So you have a list_of_documents which is just an array of strings and another document which is just a string. You need to find such document from the list_of_documents that is the most similar to document.Let's combine them together: documents = list_of_documents + [document]Let's start with dependencies. It will become clear why we use each of them.One of the approaches that can be uses is a bag-of-words approach, where we treat each word in the document independent of others and just throw all of them together in the big bag. From one point of view, it looses a lot of information (like how the words are connected), but from another point of view it makes the model simple.In English and in any other human language there are a lot of "useless" words like 'a', 'the', 'in' which are so common that they do not possess a lot of meaning. They are called stop words and it is a good idea to remove them. Another thing that one can notice is that words like 'analyze', 'analyzer', 'analysis' are really similar. They have a common root and all can be converted to just one word. This process is called stemming and there exist different stemmers which differ in speed, aggressiveness and so on. So we transform each of the documents to list of stems of words without stop words. Also we discard all the punctuation.So how will this bag of words help us? Imagine we have 3 bags: [a, b, c], [a, c, a] and [b, c, d]. We can convert them to vectors in the basis [a, b, c, d]. So we end up with vectors: [1, 1, 1, 0], [2, 0, 1, 0] and [0, 1, 1, 1]. The similar thing is with our documents (only the vectors will be way to longer). Now we see that we removed a lot of words and stemmed other also to decrease the dimensions of the vectors. Here there is just interesting observation. Longer documents will have way more positive elements than shorter, that's why it is nice to normalize the vector. This is called term frequency TF, people also used additional information about how often the word is used in other documents - inverse document frequency IDF. Together we have a metric TF-IDF which have a couple of flavors. This can be achieved with one line in sklearn :-)  Actually vectorizer allows to do a lot of things like removing stop words and lowercasing. I have done them in a separate step only because sklearn does not have non-english stopwords, but nltk has.So we have all the vectors calculated. The last step is to find which one is the most similar to the last one. There are various ways to achieve that, one of them is Euclidean distance which is not so great for the reason discussed here. Another approach is cosine similarity. We iterate all the documents and calculating cosine similarity between the document and the last one:Now minimum will have information about the best document and its score.This should help you.  and output will be:Here is a function that compares your test data against the training data, with the Tf-Idf transformer fitted with the training data. Advantage is that you can quickly pivot or group by to find the n closest elements, and that the calculations are down matrix-wise.

Log all requests from the python-requests module

dangonfast

[Log all requests from the python-requests module](https://stackoverflow.com/questions/16337511/log-all-requests-from-the-python-requests-module)

I am using python Requests. I need to debug some OAuth activity, and for that I would like it to log all requests being performed. I could get this information with ngrep, but unfortunately it is not possible to grep https connections (which are needed for OAuth)How can I activate logging of all URLs (+ parameters) that Requests is accessing?

2013-05-02 11:57:58Z

I am using python Requests. I need to debug some OAuth activity, and for that I would like it to log all requests being performed. I could get this information with ngrep, but unfortunately it is not possible to grep https connections (which are needed for OAuth)How can I activate logging of all URLs (+ parameters) that Requests is accessing?The underlying urllib3 library logs all new connections and URLs with the logging module, but not POST bodies. For GET requests this should be enough:which gives you the most verbose logging option; see the logging HOWTO for more details on how to configure logging levels and destinations.Short demo:Depending on the exact version of urllib3, the following messages are logged:This doesn't include headers or bodies. urllib3 uses the http.client.HTTPConnection class to do the grunt-work, but that class doesn't support logging, it can normally only be configured to print to stdout. However, you can rig it to send all debug information to logging instead by introducing an alternative print name into that module:Calling httpclient_logging_patch() causes http.client connections to output all debug information to a standard logger, and so are picked up by logging.basicConfig():You need to enable debugging at httplib level (requests ‚Üí urllib3 ‚Üí httplib).Here's some functions to both toggle (..._on() and ..._off()) or temporarily have it on:Demo use:You will see the REQUEST, including HEADERS and DATA, and RESPONSE with HEADERS but without DATA. The only thing missing will be the response.body which is not logged.SourceFor those using python 3+When trying to get the Python logging system (import logging) to emit low level debug log messages, it suprised me to discover that given:that only urllib3 actually uses the Python logging system:Sure, you can extract debug messages from HTTPConnection by setting:but these outputs are merely emitted via the print statement. To prove this, simply grep the Python 3.7 client.py source code and view the print statements yourself (thanks @Yohann):Presumably redirecting stdout in some way might work to shoe-horn stdout into the logging system and potentionally capture to e.g. a log file. To capture urllib3 debug information through the Python 3 logging system,  contrary to much advice on the internet, and as @MikeSmith points out, you won‚Äôt have much luck intercepting:instead you need to:Here is some code which logs urllib3 workings to a log file using the Python logging system:the result:If you set HTTPConnection.debuglevel = 1 you'll get the print statement output of additional juicy low level info:Remember this output uses print and not the Python logging system, and thus cannot be captured using a traditional logging stream or file handler (though it may be possible to capture output to a file by redirecting stdout).To maximise all possible logging, you must settle for console/stdout output with this:giving the full range of output:I'm using python 3.4, requests 2.19.1:'urllib3' is the logger to get now (no longer 'requests.packages.urllib3').  Basic logging will still happen without setting http.client.HTTPConnection.debuglevel

Python: How would you save a simple settings/config file?

user1438098

[Python: How would you save a simple settings/config file?](https://stackoverflow.com/questions/19078170/python-how-would-you-save-a-simple-settings-config-file)

I don't care if it's JSON, pickle, YAML, or whatever.All other implementations I have seen are not forwards compatible, so if I have a config file, add a new key in the code, then load that config file, it'll just crash.Are there any simple way to do this?

2013-09-29 12:44:22Z

I don't care if it's JSON, pickle, YAML, or whatever.All other implementations I have seen are not forwards compatible, so if I have a config file, add a new key in the code, then load that config file, it'll just crash.Are there any simple way to do this?There are several ways to do this depending on the file format required.I would use the standard configparser approach unless there were compelling reasons to use a different format.Write a file like so:The file format is very simple with sections marked out in square brackets:Values can be extracted from the file like so:JSON data can be very complex and has the advantage of being highly portable.Write data to a file:Read data from a file:A basic YAML example is provided in this answer. More details can be found on the pyYAML website.If you want to use something like an INI file to hold settings, consider using configparser which loads key value pairs from a text file, and can easily write back to the file. INI file has the format:The file can be loaded and used like this:which outputsAs you can see, you can use a standard data format that is easy to read and write. Methods like getboolean and getint allow you to get the datatype instead of a simple string.Writing configurationresults inSeems not to be used at all for configuration files by the Python community. However, parsing / writing XML is easy and there are plenty of possibilities to do so with Python. One is BeautifulSoup:where the config.xml might look like thisSave and load a dictionary. You will have arbitrary keys, values and arbitrary number of key, values pairs.Try using ReadSettings:

How do I get interactive plots again in Spyder/IPython/matplotlib?

endolith

[How do I get interactive plots again in Spyder/IPython/matplotlib?](https://stackoverflow.com/questions/23585126/how-do-i-get-interactive-plots-again-in-spyder-ipython-matplotlib)

I upgraded from Python(x,y) 2.7.2.3 to 2.7.6.0 in Windows 7 (and was happy to see that I can finally type function_name? and see the docstring in the Object Inspector again) but now the plotting doesn't work as it used to.Previously (Spyder 2.1.9, IPython 0.10.2, matplotlib 1.2.1), when I plotted this script, for instance, it would plot the subplots side-by-side in an interactive window:Now (Spyder 2.2.5, IPython 1.2.0, Matplotlib 1.3.1) when I try to plot things, it does the subplots as tiny inline PNGs, which is a change in IPython:So I went into options and found this:which seems to say that I can get the old interactive plots back, with the 4 subplots displayed side-by-side, but when I switch to "Automatic", and try to plot something, it does nothing.  No plots at all.If I switch this drop-down to Qt, or uncheck "Activate support", it only plots the first subplot, or part of it, and then stops:How do I get the old behavior of 4 side-by-side subplots in a single figure that I can interact with?

2014-05-10 19:04:18Z

I upgraded from Python(x,y) 2.7.2.3 to 2.7.6.0 in Windows 7 (and was happy to see that I can finally type function_name? and see the docstring in the Object Inspector again) but now the plotting doesn't work as it used to.Previously (Spyder 2.1.9, IPython 0.10.2, matplotlib 1.2.1), when I plotted this script, for instance, it would plot the subplots side-by-side in an interactive window:Now (Spyder 2.2.5, IPython 1.2.0, Matplotlib 1.3.1) when I try to plot things, it does the subplots as tiny inline PNGs, which is a change in IPython:So I went into options and found this:which seems to say that I can get the old interactive plots back, with the 4 subplots displayed side-by-side, but when I switch to "Automatic", and try to plot something, it does nothing.  No plots at all.If I switch this drop-down to Qt, or uncheck "Activate support", it only plots the first subplot, or part of it, and then stops:How do I get the old behavior of 4 side-by-side subplots in a single figure that I can interact with?Change the backend to automatic:Tools > preferences > IPython console > Graphics > Graphics backend > Backend: AutomaticThen close and open Spyder.You can quickly control this by typing built-in magic commands in Spyder's IPython console, which I find faster than picking these from the preferences menu. Changes take immediate effect, without needing to restart Spyder or the kernel.To switch to "automatic" (i.e. interactive) plots, type:then if you want to switch back to "inline", type this:(Note: these commands don't work in non-IPython consoles)See more background on this topic: Purpose of "%matplotlib inline"After applying : Tools > preferences > Graphics > Backend > Automatic   Just restart the kernel And you will surely get Interactive Plot.

Happy Coding!As said in the comments, the problem lies in your script. Actually, there are 2 problems:

How to add a title to Seaborn Facet Plot

I am not George

[How to add a title to Seaborn Facet Plot](https://stackoverflow.com/questions/29813694/how-to-add-a-title-to-seaborn-facet-plot)

How do I add a title to this Seaborne plot? Let's give it a title 'I AM A TITLE'.

2015-04-23 04:28:39Z

How do I add a title to this Seaborne plot? Let's give it a title 'I AM A TITLE'.After those lines: If you add a suptitle without adjusting the axis, the seaborn facet titles overlap it. (With different data):In ipython notebook, this worked for me! More info here: http://matplotlib.org/api/figure_api.htmlWhat worked for me was:sns.plt.suptitle('YOUR TITLE HERE')The answers using sns.plt.title() and sns.plt.suptitle() don't work anymore.Instead, you need to use matplotlib's title() function:

Python: BeautifulSoup - get an attribute value based on the name attribute

Ruth

[Python: BeautifulSoup - get an attribute value based on the name attribute](https://stackoverflow.com/questions/11205386/python-beautifulsoup-get-an-attribute-value-based-on-the-name-attribute)

I want to print an attribute value based on its name, take for exampleI want to do something like thisThe above code give a KeyError: 'name', I believe this is because name is used by BeatifulSoup so it can't be used as a keyword argument.

2012-06-26 10:29:43Z

I want to print an attribute value based on its name, take for exampleI want to do something like thisThe above code give a KeyError: 'name', I believe this is because name is used by BeatifulSoup so it can't be used as a keyword argument.It's pretty simple, use the following -Leave a comment if anything is not clear.theharshest answered the question but here is another way to do the same thing.

Also, In your example you have NAME in caps and in your code you have name in lowercase. The following works: theharshest's answer is the best solution, but FYI the problem you were encountering has to do with the fact that a Tag object in Beautiful Soup acts like a Python dictionary. If you access tag['name'] on a tag that doesn't have a 'name' attribute, you'll get a KeyError.6 years late to the party but I've been searching for how to extract an html element's tag attribute value, so for:I want "addressLocality". I kept being directed back here, but the answers didn't really solve my problem.How I managed to do it eventually:As it's a dict, you can then also use keys and 'values'Hopefully it helps someone else!One can also try this solution : To find the value, which is written in span of tablehtmlContentPython code

Getting Python error„Äåfrom: can't read /var/mail/Bio„Äç

brucezepplin

[Getting Python error„Äåfrom: can't read /var/mail/Bio„Äç](https://stackoverflow.com/questions/16069816/getting-python-error-from-cant-read-var-mail-bio)

I am running a (bio)python script which results in the following error:seeing as my script doesn't have anything to with mail, I don't understand why my script is looking in /var/mail.What seems to be the problem here? i doubt it will help as the script doesn't seem to be the problem, but here's my script anyway:what is the problem here? bad python setup? I really don't think it's the script.

2013-04-17 20:37:12Z

I am running a (bio)python script which results in the following error:seeing as my script doesn't have anything to with mail, I don't understand why my script is looking in /var/mail.What seems to be the problem here? i doubt it will help as the script doesn't seem to be the problem, but here's my script anyway:what is the problem here? bad python setup? I really don't think it's the script.No, it's not the script, it's the fact that your script is not executed by Python at all. If your script is stored in a file named script.py, you have to execute it as python script.py, otherwise the default shell will execute it and it will bail out at the from keyword. (Incidentally, from is the name of a command line utility which prints names of those who have sent mail to the given username, so that's why it tries to access the mailboxes).Another possibility is to add the following line to the top of the script:This will instruct your shell to execute the script via python instead of trying to interpret it on its own.I ran into a similar error"from: can't read /var/mail/django.test.utils"when trying to run a commandin the tutorial at https://docs.djangoproject.com/en/1.8/intro/tutorial05/after reading the answer by Tam√°s

I realized I was not trying this command in the python shell but in the termnial (this can happen to those new to linux)solution was to first enter in the python shell with the command python

and when you get these >>>

then run any python commandsSame here. I had this error when running an import command from terminal without activating python3 shell through manage.py in a django project (yes, I am a newbie yet). As one must expect, activating shell allowed the command to be interpreted correctly.and only thenPut this at the top of your .py file (for python 2.x)or for python 3.xThis should look up the python environment, without it, it will execute the code as if it were not python code, but straight to the CLI. If you need to specify a manual location of python environment put

How can I save all the variables in the current python session?

user10

[How can I save all the variables in the current python session?](https://stackoverflow.com/questions/2960864/how-can-i-save-all-the-variables-in-the-current-python-session)

I want to save all the variables in my current python environment. It seems one option is to use the 'pickle' module. However, I don't want to do this for 2 reasons:1) I have to call pickle.dump() for each variable

2) When I want to retrieve the variables, I must remember the order in which I saved the variables, and then do a pickle.load() to retrieve each variable.I am looking for some command which would save the entire session, so that when I load this saved session, all my variables are restored. Is this possible?Thanks a lot!

GauravEdit: I guess I don't mind calling pickle.dump() for each variable that I would like to save, but remembering the exact order in which the variables were saved seems like a big restriction. I want to avoid that.

2010-06-02 19:17:45Z

I want to save all the variables in my current python environment. It seems one option is to use the 'pickle' module. However, I don't want to do this for 2 reasons:1) I have to call pickle.dump() for each variable

2) When I want to retrieve the variables, I must remember the order in which I saved the variables, and then do a pickle.load() to retrieve each variable.I am looking for some command which would save the entire session, so that when I load this saved session, all my variables are restored. Is this possible?Thanks a lot!

GauravEdit: I guess I don't mind calling pickle.dump() for each variable that I would like to save, but remembering the exact order in which the variables were saved seems like a big restriction. I want to avoid that.If you use shelve, you do not have to remember the order in which the objects are pickled, since shelve gives you a dictionary-like object:To shelve your work:To restore:Having sat here and failed to save the globals() as a dictionary, I discovered you can pickle a session using the dill library. This can be done by using:One very easy way that might satisfy your needs. For me, it did pretty well:Simply, click on this icon on the Variable Explorer (right side of Spider):Saving all the variables in *.spydata formatLoading all the variables or pics etc.Here is a way saving the Spyder workspace variables using the spyderlib functions Let me know if it works for you.

David B-HWhat you're trying to do is to hibernate your process. This was discussed already. The conclusion is that there are several hard-to-solve problems exist while trying to do so. For example with restoring open file descriptors.It is better to think about serialization/deserialization subsystem for your program. It is not trivial in many cases, but is far better solution in long-time perspective.Although if I've exaggerated the problem. You can try to pickle your global variables dict. Use globals() to access the dictionary. Since it is varname-indexed you haven't to bother about the order.If you want the accepted answer abstracted to function you can use:to get/load the workspace:it worked when I ran it. I will admit I don't understand dir() and globals() 100% so I am not sure if there might be some weird caveat, but so far it seems to work. Comments are welcome :)after some more research if you call save_workspace as I suggested with globals and save_workspace is within a function it won't work as expected if you want to save the veriables in a local scope. For that use locals(). This happens because globals takes the globals from the module where the function is defined, not from where it is called would be my guess.You can save it as a text file or a CVS file. People use Spyder for example to save variables but it has a known issue: for specific data types it fails to import down in the road.

Python SQL query string formatting

ssoler

[Python SQL query string formatting](https://stackoverflow.com/questions/5243596/python-sql-query-string-formatting)

I'm trying to find the best way to format an sql query string. When I'm debugging 

my application I'd like to log to file all the sql query strings, and it is

important that the string is properly formated.Option 1Option 2Note: I have replaced white spaces with underscore _, because they are trimmed by the editorOption 3Option 4For me the best solution would be Option 2 but I don't like the extra whitespaces when I print the sql string.Do you know of any other options?

2011-03-09 09:20:08Z

I'm trying to find the best way to format an sql query string. When I'm debugging 

my application I'd like to log to file all the sql query strings, and it is

important that the string is properly formated.Option 1Option 2Note: I have replaced white spaces with underscore _, because they are trimmed by the editorOption 3Option 4For me the best solution would be Option 2 but I don't like the extra whitespaces when I print the sql string.Do you know of any other options?Sorry for posting to such an old thread -- but as someone who also shares a passion for pythonic 'best', I thought I'd share our solution.The solution is to build SQL statements using python's String Literal Concatenation (http://docs.python.org/), which could be qualified a somewhere between Option 2 and Option 4Code Sample:You've obviously considered lots of ways to write the SQL such that it prints out okay, but how about changing the 'print' statement you use for debug logging, rather than writing your SQL in ways you don't like?  Using your favourite option above, how about a logging function such as this:This would also make it trivial to add additional logic to split the logged string across multiple lines if the line is longer than your desired length.Cleanest way I have come across is inspired by the sql style guide.Essentially, the keywords that begin a clause should be right-aligned and the field names etc, should be left aligned. This looks very neat and is easier to debug as well.if the value of condition should be a string, you can do like this:you could put the field names into an array "fields", and then:I would suggest sticking to option 2 (I'm always using it for queries any more complex than SELECT * FROM table) and if you want to print it in a nice way you may always use a separate module.For short queries that can fit on one or two lines, I use the string literal solution in the top-voted solution above. For longer queries, I break them out to .sql files. I then use a wrapper function to load the file and execute the script, something like:Of course this often lives inside a class so I don't usually have to pass cursor explicitly. I also generally use codecs.open(), but this gets the general idea across. Then SQL scripts are completely self-contained in their own files with their own syntax highlighting.In Addition to @user590028 : Using format was helpful for what I was working on like so:And:To avoid formatting entirely, I think a great solution is to use procedures.Calling a procedure gives you the result of whatever query you want to put in this procedure. You can actually process multiple queries within a procedure. The call will just return the last query that was called.[edit in responese to comment]

Having an SQL string inside a method does NOT mean that you have to "tabulate" it:

Set Django's FileField to an existing file

Guard

[Set Django's FileField to an existing file](https://stackoverflow.com/questions/8332443/set-djangos-filefield-to-an-existing-file)

I have an existing file on disk (say /folder/file.txt) and a FileField model field in Django.When I do it re-saves the file as file_1.txt (the next time it's _2, etc.).I understand why, but I don't want this behavior - I know the file I want the field to be associated with is really there waiting for me, and I just want Django to point to it.How?

2011-11-30 20:20:28Z

I have an existing file on disk (say /folder/file.txt) and a FileField model field in Django.When I do it re-saves the file as file_1.txt (the next time it's _2, etc.).I understand why, but I don't want this behavior - I know the file I want the field to be associated with is really there waiting for me, and I just want Django to point to it.How?If you want to do this permanently, you need to create your own FileStorage classNow in your model, you use your modified MyFileStoragejust set instance.field.name to the path of your filee.g.try this (doc):It's right to write own storage class. However get_available_name is not the right method to override.get_available_name is called when Django sees a file with same name and tries to get a new available file name. It's not the method that causes the rename. the method caused that is _save. Comments in _save is pretty good and you can easily find it opens file for writing with flag os.O_EXCL which will throw an OSError if same file name already exists. Django catches this Error then calls get_available_name to get a new name.So I think the correct way is to override _save and call os.open() without flag os.O_EXCL. The modification is quite simple however the method is a little be long so I don't paste it here. Tell me if you need more help :)I had exactly the same problem! then I realize that my Models were causing that. example I hade my models like this:Then, I wanted to have more the one tile referencing the same file in the disk! The way that I found to solve that was change my Model structure to this:Which after I realize that make more sense, because if I want the same file being saved more then one in my DB I have to create another table for it! I guess you can solve your problem like that too, just hoping that you can change the models! EDITAlso I guess you can use a different storage, like this for instance: SymlinkOrCopyStoragehttp://code.welldev.org/django-storages/src/11bef0c2a410/storages/backends/symlinkorcopy.py

Which is the recommended way to plot: matplotlib or pylab? [closed]

Ashwin Nanjappa

[Which is the recommended way to plot: matplotlib or pylab? [closed]](https://stackoverflow.com/questions/16849483/which-is-the-recommended-way-to-plot-matplotlib-or-pylab)

I see that I can plot in Python using either:Or:Both of these use the same matplotlib plotting code.So, which of these do the Matplotlib developers / docs currently recommend as the better method to plot? Why?

2013-05-31 03:52:05Z

I see that I can plot in Python using either:Or:Both of these use the same matplotlib plotting code.So, which of these do the Matplotlib developers / docs currently recommend as the better method to plot? Why?Official docs: Matplotlib, pyplot and pylab: how are they related?Both of those imports boil down do doing exactly the same thing and will run the exact same code, it is just different ways of importing the modules. Also note that matplotlib has two interface layers, a state-machine layer managed by pyplot and the OO interface pyplot is built on top of, see How can I attach a pyplot function to a figure instance?pylab is a clean way to bulk import a whole slew of helpful functions (the pyplot state machine function, most of numpy) into a single name space.  The main reason this exists (to my understanding) is to work with ipython to make a very nice interactive shell which more-or-less replicates MATLAB (to make the transition easier and because it is good for playing around). See pylab.py and matplotlib/pylab.pyAt some level, this is purely a matter of taste and depends a bit on what you are doing. If you are not embedding in a gui (either using a non-interactive backend for bulk scripts or using one of the provided interactive backends) the typical thing to do iswhich doesn't pollute the name space. I prefer this so I can keep track of where stuff came from.If you use this is equivalent to runningIt is now recommended that for new versions of ipython you usewhich will set up all the proper background details to make the interactive backends to work nicely, but will not bulk import anything.  You will need to explicitly import the modules want.is a good start.If you are embedding matplotlib in a gui you don't want to import pyplot as that will start extra gui main loops, and exactly what you should import depends on exactly what you are doing.The documentation at https://matplotlib.org/faq/usage_faq.html#matplotlib-pyplot-and-pylab-how-are-they-related, which also describes the difference between pyglot and pylab, states: "Although many examples use pylab, it is no longer recommended.". So, I don't see any reason to use pylab or worry about it.

How to explode a list inside a Dataframe cell into separate rows

SpicyClubSauce

[How to explode a list inside a Dataframe cell into separate rows](https://stackoverflow.com/questions/32468402/how-to-explode-a-list-inside-a-dataframe-cell-into-separate-rows)

I'm looking to turn a pandas cell containing a list into rows for each of those values.So, take this:  If I'd like to unpack and stack the values in the nearest_neighbors column so that each value would be a row within each opponent index, how would I best go about this? Are there pandas methods that are meant for operations like this?

2015-09-08 22:43:05Z

I'm looking to turn a pandas cell containing a list into rows for each of those values.So, take this:  If I'd like to unpack and stack the values in the nearest_neighbors column so that each value would be a row within each opponent index, how would I best go about this? Are there pandas methods that are meant for operations like this?In the code below, I first reset the index to make the row iteration easier.  I create a list of lists where each element of the outer list is a row of the target DataFrame and each element of the inner list is one of the columns.  This nested list will ultimately be concatenated to create the desired DataFrame.I use a lambda function together with list iteration to create a row for each element of the nearest_neighbors paired with the relevant name and opponent.  Finally, I create a new DataFrame from this list (using the original column names and setting the index back to name and opponent).EDIT JUNE 2017An alternative method is as follows:Use apply(pd.Series) and stack, then reset_index and to_frameDetailsExploding a list-like column has been simplified significantly in pandas 0.25 with the addition of the

explode() method:Out:I think this a really good question, in Hive you would use EXPLODE, I think there is a case to be made that Pandas should include this functionality by default. I would probably explode the list column with a nested generator comprehension like this:The fastest method I found so far is extending the DataFrame with .iloc and assigning back the flattened target column.Given the usual input (replicated a bit):Given the following suggested alternatives:I find that extend_iloc() is the fastest:Nicer alternative solution with apply(pd.Series):Similar to Hive's EXPLODE functionality: So all of these answers are good but I wanted something ^really simple^ so here's my contribution:That's it.. just use this when you want a new series where the lists are 'exploded'. Here's an example where we do value_counts() on taco choices :)Here is a potential optimization for larger dataframes. This runs faster when there are several equal values in the "exploding" field. (The larger the dataframe is compared to the unique value count in the field, the better this code will perform.)Extending Oleg's .iloc answer to automatically flatten all list-columns:This assumes that each list-column has equal list length.Instead of using apply(pd.Series) you can flatten the column. This improves performance.

Python inheritance: TypeError: object.__init__() takes no parameters

Lucas Kauffman

[Python inheritance: TypeError: object.__init__() takes no parameters](https://stackoverflow.com/questions/11179008/python-inheritance-typeerror-object-init-takes-no-parameters)

I get this error:when running my code, I don't really see what I'm doing wrong here though:

2012-06-24 16:13:54Z

I get this error:when running my code, I don't really see what I'm doing wrong here though:You are calling the wrong class name in your super() call:Essentially what you are resolving to is the __init__ of the object base class which takes no params.Its a bit redundant, I know, to have to specify the class that you are already inside of, which is why in python3 you can just do:  super().__init__()This has bitten me twice recently (I know I should have learned from my mistake the first time) and the accepted answer hasn't helped me either time so while it is fresh in my mind I thought I would submit my own answer just in case anybody else is running into this (or I need this again in future).In my case the issue was that I was passing a kwarg into the initialisation of the subclass but in the superclass that keyword arg was then being passed though into the super() call.I always think these types of things are best with an example:So to resolve this I just need to alter the order that I do things in the Foo.__init__ method; e.g.:

Why don't I get any syntax errors when I execute my Python script with Perl?

Dacav

[Why don't I get any syntax errors when I execute my Python script with Perl?](https://stackoverflow.com/questions/29563832/why-dont-i-get-any-syntax-errors-when-i-execute-my-python-script-with-perl)

I just wrote some testing python code into test.py, and I'm launching it as follows:After a while I realized my mistake. I say "after a while", because the

Python code gets actually correctly executed, as if in Python interpreter!Why is my Perl interpreting my Python? test.py looks like this:Interestingly, if I do the opposite (i.e. call python something.pl) I get a good deal of syntax errors.

2015-04-10 14:19:28Z

I just wrote some testing python code into test.py, and I'm launching it as follows:After a while I realized my mistake. I say "after a while", because the

Python code gets actually correctly executed, as if in Python interpreter!Why is my Perl interpreting my Python? test.py looks like this:Interestingly, if I do the opposite (i.e. call python something.pl) I get a good deal of syntax errors.From perlrun,For example,

What is the difference between NaN and None?

user1083734

[What is the difference between NaN and None?](https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none)

I am reading two columns of a csv file using pandas readcsv() and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be None but instead nan is assigned. Surely None is more descriptive of an empty cell as it has a null value, whereas nan just says that the value read is not a number.Is my understanding correct, what IS the difference between None and nan? Why is nan assigned instead of None?Also, my dictionary check for any empty cells has been using numpy.isnan():But this gives me an error saying that I cannot use this check for v. I guess it is because an integer or float variable, not a string is meant to be used. If this is true, how can I check v for an "empty cell"/nan case?

2013-07-08 19:06:17Z

I am reading two columns of a csv file using pandas readcsv() and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be None but instead nan is assigned. Surely None is more descriptive of an empty cell as it has a null value, whereas nan just says that the value read is not a number.Is my understanding correct, what IS the difference between None and nan? Why is nan assigned instead of None?Also, my dictionary check for any empty cells has been using numpy.isnan():But this gives me an error saying that I cannot use this check for v. I guess it is because an integer or float variable, not a string is meant to be used. If this is true, how can I check v for an "empty cell"/nan case?NaN is used as a placeholder for missing data consistently in pandas, consistency is good. I usually read/translate NaN as "missing". Also see the 'working with missing data' section in the docs.Wes writes in the docs 'choice of NA-representation':Note: the "gotcha" that integer Series containing missing data are upcast to floats.In my opinion the main reason to use NaN (over None) is that it can be stored with numpy's float64 dtype, rather than the less efficient object dtype, see NA type promotions.Jeff comments (below) on this:Saying that, many operations may still work just as well with None vs NaN (but perhaps are not supported i.e. they may sometimes give surprising results):To answer the second question:

You should be using pd.isnull and pd.notnull to test for missing data (NaN).NaN can be used as a numerical value on mathematical operations, while None cannot (or at least shouldn't).NaN is a numeric value, as defined in IEEE 754 floating-point standard.

None is an internal Python tipe (NoneType) and would be more like "inexistent" or "empty" than "numerically invalid" in this context.The main "symptom" of that is that, if you perform, say, an average or a sum on an array containing NaN, even a single one, you get NaN as a result...In the other hand, you cannot perform mathematical operations using None as operand.So, depending on the case, you could use None as a way to tell your algorithm not to consider invalid or inexistent values on computations. That would mean the algorithm should test each value to see if it is None.Numpy has some functions to avoid NaN values to contaminate your results, such as nansum and nan_to_num for example.The function isnan() checks to see if something is "Not A Number" and will return whether or not a variable is a number, for example isnan(2) would return falseThe conditional myVar is not None returns whether or not the variable is definedYour numpy array uses isnan() because it is intended to be an array of numbers and it initializes all elements of the array to NaN these elements are considered "empty"Below are the differences:I found the below article very helpful:

https://medium.com/analytics-vidhya/dealing-with-missing-values-nan-and-none-in-python-6fc9b8fb4f31NaN stants for NOT a number.

None might stand for any.

How to concatenate two layers in keras?

rdo

[How to concatenate two layers in keras?](https://stackoverflow.com/questions/43196636/how-to-concatenate-two-layers-in-keras)

I have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this:So, I'd created a model with two layers and tried to merge them but it returns an error: The first layer in a Sequential model must get an "input_shape" or "batch_input_shape" argument. on the line result.add(merged).Model:

2017-04-04 00:56:54Z

I have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this:So, I'd created a model with two layers and tried to merge them but it returns an error: The first layer in a Sequential model must get an "input_shape" or "batch_input_shape" argument. on the line result.add(merged).Model:You're getting the error because result defined as Sequential() is just a container for the model and you have not defined an input for it. Given what you're trying to build set result to take the third input x3.However, my preferred way of building a model that has this type of input structure would be to use the functional api.Here is an implementation of your requirements to get you started:To answer the question in the comments: 1) How are result and merged connected? Assuming you mean how are they concatenated.Concatenation works like this:i.e rows are just joined.2) Now, x1 is input to first, x2 is input into second and x3 input into third.You can experiment with model.summary() (notice the concatenate_XX (Concatenate) layer size)You can view notebook here for detail:

https://nbviewer.jupyter.org/github/anhhh11/DeepLearning/blob/master/Concanate_two_layer_keras.ipynbAdding to the above-accepted answer so that it helps those who are using tensorflow 2.0 Result:

Generate temporary file names without creating actual file in Python

Hill

[Generate temporary file names without creating actual file in Python](https://stackoverflow.com/questions/26541416/generate-temporary-file-names-without-creating-actual-file-in-python)

The question, number 10501247, in stackoverflow gives answer how to create temporary file in Python.

I only need to have temporary file name in my case.

Calling tempfile.NamedTemporaryFile() returns file handle after actual file creation.

Is there way to get file name only?

2014-10-24 04:19:59Z

The question, number 10501247, in stackoverflow gives answer how to create temporary file in Python.

I only need to have temporary file name in my case.

Calling tempfile.NamedTemporaryFile() returns file handle after actual file creation.

Is there way to get file name only?If you want a temp file name only you can call inner tempfile function _get_candidate_names():Calling next again, will return another name, etc. This does not give you the path to temp folder. To get default 'tmp' directory, use: I think the easiest, most secure way of doing this is something like:A temporary directory is created that only you can access, so there should be no security issues, but there will be no files created in it, so you can just pick any filename you want to create in that directory.It may be a little late, but is there anything wrong with this?tempfile.mktemp() do this.But note that it's deprecated. However it will not create the file and it is a public function in tempfile compared to using the _get_candidate_names().The reason it's deprecated is due to the time gap between calling this and actually trying to create the file. However in my case the chance of that is so slim and even if it would fail that would be acceptable. But it's up to you to evaluate for your usecase.As Joachim Isaksson said in the comments, if you just get a name you may have problems if some other program happens to use that name before your program does. The chances are slim, but not impossible.So the safe thing to do in this situation is to use the full GzipFile() constructor, which has the signature GzipFile( [filename[, mode[, compresslevel[, fileobj]]]]). So you can pass it the open fileobj, and a filename as well, if you like. See the gzip docs for details.Combining the previous answers, my solution is:Make some_id optional if not needed for you.

Very large matrices using Python and NumPy

Peter

[Very large matrices using Python and NumPy](https://stackoverflow.com/questions/1053928/very-large-matrices-using-python-and-numpy)

NumPy is an extremely useful library, and from using it I've found that it's capable of handling matrices which are quite large (10000 x 10000) easily, but begins to struggle with anything much larger (trying to create a matrix of 50000 x 50000 fails). Obviously, this is because of the massive memory requirements.Is there is a way to create huge matrices natively in NumPy (say 1 million by 1 million) in some way (without having several terrabytes of RAM)?

2009-06-28 00:32:21Z

NumPy is an extremely useful library, and from using it I've found that it's capable of handling matrices which are quite large (10000 x 10000) easily, but begins to struggle with anything much larger (trying to create a matrix of 50000 x 50000 fails). Obviously, this is because of the massive memory requirements.Is there is a way to create huge matrices natively in NumPy (say 1 million by 1 million) in some way (without having several terrabytes of RAM)?PyTables and NumPy are the way to go.PyTables will store the data on disk in HDF format, with optional compression. My datasets often get 10x compression, which is handy when dealing with tens or hundreds of millions of rows. It's also very fast; my 5 year old laptop can crunch through data doing SQL-like GROUP BY aggregation at 1,000,000 rows/second. Not bad for a Python-based solution!Accessing the data as a NumPy recarray again is as simple as:The HDF library takes care of reading in the relevant chunks of data and converting to NumPy.numpy.arrays are meant to live in memory.  If you want to work with matrices larger than your RAM, you have to work around that.  There are at least two approaches you can follow:You should be able to use numpy.memmap to memory map a file on disk.  With newer python and 64-bit machine, you should have the necessary address space, without loading everything into memory.  The OS should handle only keep part of the file in memory.To handle sparse matrices, you need the scipy package that sits on top of numpy -- see here for more details about the sparse-matrix options that scipy gives you.Stefano Borini's post got me to look into how far along this sort of thing already is.  This is it.  It appears to do basically what you want.  HDF5 will let you store very large datasets, and then access and use them in the same ways NumPy does.  Make sure you're using a 64-bit operating system and a 64-bit version of Python/NumPy. Note that on 32-bit architectures you can address typically 3GB of memory (with about 1GB lost to memory mapped I/O and such). With 64-bit and things arrays larger than the available RAM you can get away with virtual memory, though things will get slower if you have to swap. Also, memory maps (see numpy.memmap) are a way to work with huge files on disk without loading them into memory, but again, you need to have a 64-bit address space to work with for this to be of much use. PyTables will do most of this for you as well.It's a bit alpha, but http://blaze.pydata.org/ seems to be working on solving this. Sometimes one simple solution is using a custom type for your matrix items. Based on the range of numbers you need, you can use a manual dtype and specially smaller for your items. Because Numpy considers the largest type for object by default this might be a helpful idea in many cases. Here is an example:And with custom type:Are you asking how to handle a 2,500,000,000 element matrix without terabytes of RAM?  The way to handle 2 billion items without 8 billion bytes of RAM is by not keeping the matrix in memory.That means much more sophisticated algorithms to fetch it from the file system in pieces.Usually when we deal with large matrices we implement them as Sparse Matrices.I don't know if numpy supports sparse matrices but I found this instead.As far as I know about numpy, no, but I could be wrong. I can propose you this alternative solution: write the matrix on the disk and access it in chunks. I suggest you the HDF5 file format. If you need it transparently, you can reimplement the ndarray interface to paginate your disk-stored matrix into memory. Be careful if you modify the data to sync them back on the disk. 

sqlalchemy IS NOT NULL select

salamey

[sqlalchemy IS NOT NULL select](https://stackoverflow.com/questions/21784851/sqlalchemy-is-not-null-select)

How can I add the filter as in SQL to select values that are NOT NULL from a certain column ?How can I do the same with SQLAlchemy filters?

2014-02-14 16:44:02Z

How can I add the filter as in SQL to select values that are NOT NULL from a certain column ?How can I do the same with SQLAlchemy filters?column_obj != None will produce a IS NOT NULL constraint:or use isnot() (new in 0.7.9):Demo:Starting in version 0.7.9 you can use the filter operator .isnot instead of comparing constraints, like this:query.filter(User.name.isnot(None))This method is only necessary if pep8 is a concern.source: sqlalchemy documentationIn case anyone else is wondering, you can use is_ to generate foo IS NULL:

How to select rows in a DataFrame between two values, in Python Pandas?

user131983

[How to select rows in a DataFrame between two values, in Python Pandas?](https://stackoverflow.com/questions/31617845/how-to-select-rows-in-a-dataframe-between-two-values-in-python-pandas)

I am trying to modify a DataFrame df to only contain rows for which the values in the column closing_price are between 99 and 101 and trying to do this with the code below. However, I get the error and I am wondering if there is a way to do this without using loops.

2015-07-24 18:56:10Z

I am trying to modify a DataFrame df to only contain rows for which the values in the column closing_price are between 99 and 101 and trying to do this with the code below. However, I get the error and I am wondering if there is a way to do this without using loops.You should use () to group your boolean vector to remove ambiguity. Consider also series between:there is a nicer alternative - use query() method:UPDATE: answering the comment:you can also use .between() methodor Instead of this You should use thisWe have to use NumPy's bitwise Logic operators |, &, ~, ^ for compounding queries.

Also, the parentheses are important for operator precedence.For more info, you can visit the link

:Comparisons, Masks, and Boolean LogicIf you're dealing with multiple values and multiple inputs you could also set up an apply function like this. In this case filtering a dataframe for GPS locations that fall withing certain ranges.

Prepend a level to a pandas MultiIndex

Yawar

[Prepend a level to a pandas MultiIndex](https://stackoverflow.com/questions/14744068/prepend-a-level-to-a-pandas-multiindex)

I have a DataFrame with a MultiIndex created after some grouping:How do I prepend a level to the MultiIndex so that I turn it into something like:

2013-02-07 05:16:07Z

I have a DataFrame with a MultiIndex created after some grouping:How do I prepend a level to the MultiIndex so that I turn it into something like:A nice way to do this in one line using pandas.concat():This can be generalized to many data frames, see the docs.You can first add it as a normal column and then append it to the current index, so:And change the order if needed with:Which results in:I think this is a more general solution:Some advantages over the other answers:I made a little function out of cxrodgers answer, which IMHO is the best solution since it works purely on an index, independent of any data frame or series.There is one fix I added: the to_frame() method will invent new names for index levels that don't have one. As such the new index will have names that don't exist in the old index. I added some code to revert this name-change.Below is the code, I've used it myself for a while and it seems to work fine. If you find any issues or edge cases, I'd be much obliged to adjust my answer.It passed the following unittest code:

Python display string multiple times

Hick

[Python display string multiple times](https://stackoverflow.com/questions/963161/python-display-string-multiple-times)

I want to print a character or string like '-' n number of times.Can I do it without using a loop?.. Is there a function like..which would mean printing the - 3 times, like this:

2009-06-08 01:11:57Z

I want to print a character or string like '-' n number of times.Can I do it without using a loop?.. Is there a function like..which would mean printing the - 3 times, like this:Python 2.x:Python 3.x:The accepted answer is short and sweet, but here is an alternate syntax allowing to provide a separator in Python 3.x.

Python sorting list of dictionaries by multiple keys

simi

[Python sorting list of dictionaries by multiple keys](https://stackoverflow.com/questions/1143671/python-sorting-list-of-dictionaries-by-multiple-keys)

I have a list of dicts:and I need to use a multi key sort reversed by Total_Points, then not reversed by TOT_PTS_Misc.This can be done at the command prompt like so:But I have to run this through a function, where I pass in the list and the sort keys. For example, def multikeysort(dict_list, sortkeys):.How can the lambda line be used which will sort the list, for an arbitrary number of keys that are passed in to the multikeysort function, and take into consideration that the sortkeys may have any number of keys and those that need reversed sorts will be identified with a '-' before it?

2009-07-17 14:36:48Z

I have a list of dicts:and I need to use a multi key sort reversed by Total_Points, then not reversed by TOT_PTS_Misc.This can be done at the command prompt like so:But I have to run this through a function, where I pass in the list and the sort keys. For example, def multikeysort(dict_list, sortkeys):.How can the lambda line be used which will sort the list, for an arbitrary number of keys that are passed in to the multikeysort function, and take into consideration that the sortkeys may have any number of keys and those that need reversed sorts will be identified with a '-' before it?This answer works for any kind of column in the dictionary -- the negated column need not be a number.You can call it like this:Try it with either column negated. You will see the sort order reverse.Next: change it so it does not use extra class....2016-01-17Taking my inspiration from this answer What is the best way to get the first item from an iterable matching a condition?, I shortened the code:In case you like your code terse.Later 2016-01-17This works with python3 (which eliminated the cmp argument to sort):Inspired by this answer How should I do custom sort in Python 3?This article has a nice rundown on various techniques for doing this.  If your requirements are simpler than "full bidirectional multikey", take a look.  It's clear the accepted answer and the blog post I just 

referenced influenced each other in some way, though I don't know which order.In case the link dies here's a very quick synopsis of examples not covered above:I know this is a rather old question, but none of the answers mention that Python guarantees a stable sort order for its sorting routines such as list.sort() and sorted(), which means items that compare equal retain their original order.This means that the equivalent of ORDER BY name ASC, age DESC (using SQL notation) for a list of dictionaries can be done like this:The reversing/inverting works for all orderable types, not just numbers which you can negate by putting a minus sign in front.And because of the Timsort algorithm used in (at least) CPython, this is actually rather fast in practice.I use the following for sorting a 2d array on a number of columnsThis could be extended to work on an arbitrary number of items. I tend to think finding a better access pattern to your sortable keys is better than writing a fancy comparator.I had a similar issue today - I had to sort dictionary items by descending numeric values and by ascending string values. To solve the issue of conflicting directions, I negated the integer values. Here's a variant of my solution - as applicable to OPVery simple - and works like a charmDemonstration:The parsing is a bit fragile, but at least it allows for variable number of spaces between the keys.Since you're already comfortable with lambda, here's a less verbose solution.

How can Python dict have multiple keys with same hash?

Praveen Gollakota

[How can Python dict have multiple keys with same hash?](https://stackoverflow.com/questions/9010222/how-can-python-dict-have-multiple-keys-with-same-hash)

I am trying to understand python hash function under the hood. I created a custom class where all instances return the same hash value. I just assumed that only one instance of the above class can be in a set at any time, but in fact a set can have multiple elements with same hash.I experimented a little more and found that if I override the __eq__ method such that all the instances of the class compare equal, then the set only allows one instance.So I am curious to know how can a dict have multiple elements with same hash. Thanks!Note: Edited the question to give example of dict (instead of set) because all the discussion in the answers is about dicts. But the same applies to sets; sets can also have multiple elements with same hash value.

2012-01-25 20:59:46Z

I am trying to understand python hash function under the hood. I created a custom class where all instances return the same hash value. I just assumed that only one instance of the above class can be in a set at any time, but in fact a set can have multiple elements with same hash.I experimented a little more and found that if I override the __eq__ method such that all the instances of the class compare equal, then the set only allows one instance.So I am curious to know how can a dict have multiple elements with same hash. Thanks!Note: Edited the question to give example of dict (instead of set) because all the discussion in the answers is about dicts. But the same applies to sets; sets can also have multiple elements with same hash value.For a detailed description of how Python's hashing works see my answer to Why is early return slower than else?Basically it uses the hash to pick a slot in the table. If there is a value in the slot and the hash matches, it compares the items to see if they are equal.If the hash doesn't match or the items aren't equal, then it tries another slot. There's a formula to pick this (which I describe in the referenced answer), and it gradually pulls in unused parts of the hash value; but once it has used them all up, it will eventually work its way through all slots in the hash table. That guarantees eventually we either find a matching item or an empty slot. When the search finds an empty slot, it inserts the value or gives up (depending whether we are adding or getting a value).The important thing to note is that there are no lists or buckets: there is just a hash table with a particular number of slots, and each hash is used to generate a sequence of candidate slots.Here is everything about Python dicts that I was able to put together (probably more than anyone would like to know; but the answer is comprehensive). A shout out to Duncan for pointing out that Python dicts use slots and leading me down this rabbit hole.There you go! The Python implementation of dict checks for both hash equality of two keys and the normal equality (==) of the keys when inserting items. So in summary, if there are two keys, a and b and hash(a)==hash(b), but a!=b, then both can exist harmoniously in a Python dict. But if hash(a)==hash(b) and a==b, then they cannot both be in the same dict. Because we have to probe after every hash collision, one side effect of too many hash collisions is that the lookups and insertions will become very slow (as Duncan points out in the comments).I guess the short answer to my question is, "Because that's how it's implemented in the source code ;)"While this is good to know (for geek points?), I am not sure how it can be used in real life. Because unless you are trying to explicitly break something, why would two objects that are not equal, have same hash? Edit: the answer below is one of possible ways to deal with hash collisions, it is however not how Python does it. Python's wiki referenced below is also incorrect. The best source given by @Duncan below is the implementation itself: http://svn.python.org/projects/python/trunk/Objects/dictobject.c I apologize for the mix-up.It stores a list (or bucket) of elements at the hash then iterates through that list until it finds the actual key in that list. A picture says more than a thousand words:Here you see John Smith and Sandra Dee both hash to 152. Bucket 152 contains both of them. When looking up Sandra Dee it first finds the list in bucket 152, then loops through that list until Sandra Dee is found and returns 521-6955.The following is wrong it's only here for context: On Python's wiki you can find (pseudo?) code how Python performs the lookup.There's actually several possible solutions to this problem, check out the wikipedia article for a nice overview: http://en.wikipedia.org/wiki/Hash_table#Collision_resolutionHash tables, in general have to allow for hash collisions! You will get unlucky and two things will eventually hash to the same thing. Underneath, there is a set of objects in a list of items that has that same hash key. Usually, there is only one thing in that list, but in this case, it'll keep stacking them into the same one. The only way it knows they are different is through the equals operator.When this happens, your performance will degrade over time, which is why you want your hash function to be as "random as possible".In the thread I did not see what exactly python does with instances of a user-defined classes when we put it into a dictionary as a keys. Let's read some documentation: it declares that only hashable objects can be used as a keys. Hashable are all immutable built-in classes and all user-defined classes.So if you have a constantly __hash__ in your class, but not providing any __cmp__ or __eq__ method, then all your instances are unequal for the dictionary.

In the other hand, if you providing any __cmp__ or __eq__ method, but not providing __hash__, your instances are still unequal in terms of dictionary.Output

Conda command is not recognized on Windows 10

alex

[Conda command is not recognized on Windows 10](https://stackoverflow.com/questions/44597662/conda-command-is-not-recognized-on-windows-10)

I installed Anaconda 4.4.0 (Python 3.6 version) on Windows 10 by following the instructions here: https://www.continuum.io/downloads. However, when I open the Command prompt window and try to write I get the error. I tried to runbut it didn't help. I also read that I might need to edit my .bashrc file, but I don't know how to access this file, and how I should edit it. 

2017-06-16 20:40:54Z

I installed Anaconda 4.4.0 (Python 3.6 version) on Windows 10 by following the instructions here: https://www.continuum.io/downloads. However, when I open the Command prompt window and try to write I get the error. I tried to runbut it didn't help. I also read that I might need to edit my .bashrc file, but I don't know how to access this file, and how I should edit it. In Windows, you will have to set the path to the location where you installed Anaconda3 to.For me, I installed anaconda3 into C:\Anaconda3. Therefore you need to add C:\Anaconda3 as well as C:\Anaconda3\Scripts\ to your path variable, e.g. set PATH=%PATH%;C:\Anaconda3;C:\Anaconda3\Scripts\.You can do this via powershell (see above, https://msdn.microsoft.com/en-us/library/windows/desktop/bb776899(v=vs.85).aspx ), or hit the windows key ‚Üí enter environment ‚Üí choose from settings ‚Üí edit environment variables for your account ‚Üí select Path variable ‚Üí Edit ‚Üí New.To test it, open a new dos shell, and you should be able to use conda commands now. E.g., try conda --version.When you install anaconda on windows now, it doesn't automatically add Python or Conda. If you don‚Äôt know where your conda and/or python is, you type the following commands into your anaconda promptNext, you can add Python and Conda to your path by using the setx command in your command prompt.

Next close that command prompt and open a new one. Congrats you can now use conda and python Source: https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444The newest version of the Anaconda installer for Windows will also install a windows launcher for "Anaconda Prompt" and "Anaconda Powershell Prompt". If you use one of those instead of the regular windows cmd shell, the conda command, python etc. should be available by default in this shell.Things have been changed after conda 4.6.Programs "Anaconda Prompt" and "Anaconda Powershell" expose the command conda for you automatically. Find them in your startup menu.If you don't wanna use the prompts above and try to make conda available in a normal cmd.exe and a Powershell. Read the following content.The purpose of the following content is to make command conda available both in cmd.exe and Powershell on Windows.If you have already checked "Add Anaconda to my PATH environment variable" during Anaconda installation, skip step 1.These steps make sure the conda command is exposed into your cmd.exe and Powershell.Caveat: Add the new \path\to\anaconda3\condabin but not \path\to\anaconda3\Scripts into your PATH. This is a big change introduced in conda 4.6.Activation script initialization fron conda 4.6 release logIn the old days, \path\to\anaconda3\Scripts is the one to be put into your PATH. It exposes command conda and the default Python from "base" environment at the same time.After conda 4.6, conda related commands are separated into condabin. This makes it possible to expose ONLY command conda without activating the Python from "base" environment.If you want to use Anaconda in regular cmd on windows you need to add several paths to your Path env variable.Those paths are (instead of Anaconda3 the folder may be Anaconda2 depending on the Anaconda version on your PC):I had also faced the same problem just an hour back. I was trying to install QuTip Quantum Toolbox in Python

Unfortunately, I didn't stumble onto this page in time.

Say you have downloaded Anaconda installer and run it until the end.

 Naively, I opened the command prompt in windows 10 and proceded to type the following commands as given in the qutip installation docs.But as soon as I typed the first line I got the following responseerror messsageI went ahead and tried some other things as seen in this figures

error message

Finally after going through a number conda websites, I understood how one fixes this problem.

Type Anaconda prompt in the search bar at the bottom like this (same place where you hail Cortana)

Anaconda promptOnce you are here all the conda commands will work as usual If you have installed Visual studio 2017 (profressional)The install location:If you do not want the hassle of putting this in your path environment variable on windows and restarting you can run it by simply:Even I got the same problem when I've first installed Anaconda. It said 'conda' command not found.So I've just setup two values[added two new paths of Anaconda] system environment variables in the PATH variable which are: 

                  C:\Users\mshas\Anaconda2\    &

                  C:\Users\mshas\Anaconda2\ScriptsLot of people forgot to add the second variable which is "Scripts" just add that then 'conda' command works.You need to add the python.exe in C://.../Anaconda3 installation file as well as C://.../Anaconda3/Scripts to PATH. First go to your installation directory, in my case it is installed in C://Users/user/Anaconda3 and shift+right click and press "Open command window here" or it might be "Open powershell here", if it is powershell, just write cmd and hit enter to run command window. Then run the following command setx PATH %cd% Then go to C://Users/user/Anaconda3/Scripts and open the command window there as above, then run the same command "setx PATH %cd%" case #1

You should set 3 path:It will solve problem:case #2

Also you can use Anaconda Promd (for Win10) instead CLI (cmd.exe)To prevent having further issues with SSL you should add all those to Path : Requests (Caused by SSLError("Can't connect to HTTPS URL because the SSL module is not available.") Error in PyCharm requesting website

Stripping non printable characters from a string in python

Vinko Vrsalovic

[Stripping non printable characters from a string in python](https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python)

I use to runon Perl to get rid of non printable characters. In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not. What would you do? EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output. 

curses.ascii.isprint will return false for any unicode character.

2008-09-18 13:17:06Z

I use to runon Perl to get rid of non printable characters. In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not. What would you do? EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output. 

curses.ascii.isprint will return false for any unicode character.Iterating over strings is unfortunately rather slow in Python. Regular expressions are over an order of magnitude faster for this kind of thing. You just have to build the character class yourself. The unicodedata module is quite helpful for this, especially the unicodedata.category() function. See Unicode Character Database for descriptions of the categories.As far as I know, the most pythonic/efficient method would be:You could try setting up a filter using the unicodedata.category() function:See Table 4-9 on page 175 in the Unicode database character properties for the available categoriesIn Python 3,See this StackOverflow post on removing punctuation for how .translate() compares to regex & .replace()This function uses list comprehensions and str.join, so it runs in linear time instead of O(n^2):The following will work with Unicode input and is rather fast...My own testing suggests this approach is faster than functions that iterate over the string and return a result using str.join.The best I've come up with now is (thanks to the python-izers above) This is the only way I've found out that works with Unicode characters/stringsAny better options?The one below performs faster than the others above. Take a lookThere are when using the regex library: https://pypi.org/project/regex/It is well maintained and supports Unicode regex, Posix regex and many more. The usage (method signatures) is very similar to Python's re.From the documentation:(I'm not affiliated, just a user.)Yet another option in python 3:To remove 'whitespace',

Insert an element at specific index in a list and return updated list

ATOzTOA

[Insert an element at specific index in a list and return updated list](https://stackoverflow.com/questions/14895599/insert-an-element-at-specific-index-in-a-list-and-return-updated-list)

I have this:Is there anyway I can get the updated list as result, instead of updating the original list in place?

2013-02-15 13:16:03Z

I have this:Is there anyway I can get the updated list as result, instead of updating the original list in place?l.insert(index, obj) doesn't actually return anything, it just updates the list.

As ATO said, you can do b = a[:index] + [obj] + a[index:].

However, another way is:You may also insert the element using the slice indexing in the list. For example:For inserting multiple elements together at a given index, all you need to do is to use a list of multiple elements that you want to insert. For example:Alternative using List Comprehension (but very slow in terms of performance):As an alternative, it can be achieved using list comprehension with enumerate too. (But please don't do it this way. It is just for illustration):Here's the timeit comparison of all the answers with list of 1000 elements for Python 3.4.5:Shortest I got: b = a[:2] + [3] + a[2:]Use the Python list insert() Method. Usage:Example:Returns [1, 2, 3, 4, 5]

Python: Select subset from list based on index set

fuenfundachtzig

[Python: Select subset from list based on index set](https://stackoverflow.com/questions/3179106/python-select-subset-from-list-based-on-index-set)

I have several lists having all the same number of entries (each specifying an object property):and list with flags of the same length(which could easily be substituted with an equivalent index list:What is the easiest way to generate new lists property_asel, property_bsel, ... which contain only the values indicated either by the True entries or the indices?

2010-07-05 11:30:33Z

I have several lists having all the same number of entries (each specifying an object property):and list with flags of the same length(which could easily be substituted with an equivalent index list:What is the easiest way to generate new lists property_asel, property_bsel, ... which contain only the values indicated either by the True entries or the indices?You could just use list comprehension:orThe latter one is faster because there are fewer good_indices than the length of property_a, assuming good_indices are precomputed instead of generated on-the-fly.Edit: The first option is equivalent to itertools.compress available since Python 2.7/3.1. See @Gary Kerr's answer.I see 2 options.Use the built in function zipJust looking at the new features of 2.7. There is now a function in the itertools module which is similar to the above code.http://docs.python.org/library/itertools.html#itertools.compressAssuming you only have the list of items and a list of true/required indices, this should be the fastest:This means the property selection will only do as many rounds as there are true/required indices. If you have a lot of property lists that follow the rules of a single tags (true/false) list you can create an indices list using the same list comprehension principles:This iterates through each item in good_objects (while remembering its index with enumerate) and  returns only the indices where the item is true.For anyone not getting the list comprehension, here is an English prose version with the code highlighted in bold:list the index for every group of index, item that exists in an enumeration of good objects, if (where) the item is TrueMatlab and Scilab languages offer a simpler and more elegant syntax than Python for the question you're asking, so I think the best you can do is to mimic Matlab/Scilab by using the Numpy package in Python. By doing this the solution to your problem is very concise and elegant:Numpy tries to mimic Matlab/Scilab but it comes at a cost: you need to declare every list with the keyword "array", something which will overload your script (this problem doesn't exist with Matlab/Scilab). Note that this solution is restricted to arrays of number, which is the case in your example.

Capture stdout from a script?

Paolo

[Capture stdout from a script?](https://stackoverflow.com/questions/5136611/capture-stdout-from-a-script)

suppose there is a script doing something like this:Now suppose I want to capture the output of the write function and store it in a variable for further processing. The naive solution was:But this doesn't work. I come up with another solution and it works, but please, let me know if there is a better way to solve the problem. Thanks

2011-02-27 22:49:33Z

suppose there is a script doing something like this:Now suppose I want to capture the output of the write function and store it in a variable for further processing. The naive solution was:But this doesn't work. I come up with another solution and it works, but please, let me know if there is a better way to solve the problem. ThanksSetting stdout is a reasonable way to do it.  Another is to run it as another process:Here is a context manager version of your code.  It yields a list of two values; the first is stdout, the second is stderr.For future visitors: Python 3.4 contextlib provides for this directly (see Python contextlib help) via the redirect_stdout context manager:Or maybe use functionality that is already there...This is the decorator counterpart of my original code.writer.py remains the same:mymodule.py sligthly gets modified:And here is the decorator:Starting with Python 3 you can also use sys.stdout.buffer.write() to write (already) encoded byte strings to stdout (see stdout in Python 3).

When you do that, the simple StringIO approach doesn't work because neither sys.stdout.encoding nor sys.stdout.buffer would be available.Starting with Python 2.6 you can use the TextIOBase API, which includes the missing attributes:This solution works for Python 2 >= 2.6 and Python 3.

Please note that our sys.stdout.write() only accepts unicode strings and sys.stdout.buffer.write() only accepts byte strings.

This might not be the case for old code, but is often the case for code that is built to run on Python 2 and 3 without changes.If you need to support code that sends byte strings to stdout directly without using stdout.buffer, you can use this variation:You don't have to set the encoding of the buffer the sys.stdout.encoding, but this helps when using this method for testing/comparing script output.The question here (the example of how to redirect output, not the tee part) uses os.dup2 to redirect a stream at the OS level.  That is nice because it will apply to commands that you spawn from your program as well.I think You should look at these four objects:Example:UPD: As Eric said in a comment, one shouldn't use they directly, so I copied and pasted it.I like the contextmanager solution however if you need the buffer stored with the open file and fileno support you could do something like this.use

Connecting to a host listed in ~/.ssh/config when using Fabric

Brian M. Hunt

[Connecting to a host listed in ~/.ssh/config when using Fabric](https://stackoverflow.com/questions/3077281/connecting-to-a-host-listed-in-ssh-config-when-using-fabric)

I'm having trouble with Fabric not recognizing hosts that I have in ~/.ssh/config.My fabfile.py is as follows:Running $ fab whoami gives:The name lulu is in my ~/.ssh/config, like this:My first thought to solving this is adding something like lulu.lulu to /etc/hosts (I'm on a Mac), but then I have to also pass in the identity file to Fabric - and I'd rather keep my authentication (i.e. ~/.ssh/config) separate from my deployment (i.e. fabfile.py).As well, incidentally, if you try to connect to a host in the hosts file, fabric.contrib.projects.rsync_project doesn't seem to acknowledge 'ports' in the hosts.env (i.e. if you use hosts.env = [lulu:2100] a call to rsync_project seems to try connecting to lulu:21).Is there a reason Fabric doesn't recognize this lulu name?

2010-06-19 21:21:23Z

I'm having trouble with Fabric not recognizing hosts that I have in ~/.ssh/config.My fabfile.py is as follows:Running $ fab whoami gives:The name lulu is in my ~/.ssh/config, like this:My first thought to solving this is adding something like lulu.lulu to /etc/hosts (I'm on a Mac), but then I have to also pass in the identity file to Fabric - and I'd rather keep my authentication (i.e. ~/.ssh/config) separate from my deployment (i.e. fabfile.py).As well, incidentally, if you try to connect to a host in the hosts file, fabric.contrib.projects.rsync_project doesn't seem to acknowledge 'ports' in the hosts.env (i.e. if you use hosts.env = [lulu:2100] a call to rsync_project seems to try connecting to lulu:21).Is there a reason Fabric doesn't recognize this lulu name?Since version 1.4.0, Fabric uses your ssh config (partly). However, you need to explicitly enable it, withsomewhere near the top of your fabfile. Once you do this, Fabric should read your ssh config (from ~/.ssh/config by default, or from env.ssh_config_path).One warning: if you use a version older than 1.5.4, an abort will occur if env.use_ssh_config is set but there is no config file present. In that case, you can use a workaround like:Note that this also happens when the name is not in /etc/hosts. I had the same problem and had to add the host name to both that file and ~/.ssh/config.update: This Answer is now outdated.Fabric doesn't have support currently for the .ssh/config file. You can set these up in a function to then call on the cli, eg: fab production task; where production sets the username, hostname, port, and ssh identity. As for rsync project, that should now have port setting ability, if not, you can always run local("rsync ...") as that is essentially what that contributed function does.One can use following code to read the config (original code taken from: http://markpasc.typepad.com/blog/2010/04/loading-ssh-config-settings-for-fabric.html):

Using ConfigParser to read a file without section name

Escualo

[Using ConfigParser to read a file without section name](https://stackoverflow.com/questions/2885190/using-configparser-to-read-a-file-without-section-name)

I am using ConfigParser to read the runtime configuration of a script.I would like to have the flexibility of not providing a section name (there are scripts which are simple enough; they don't need a 'section'). ConfigParser will throw a NoSectionError exception, and will not accept the file.How can I make ConfigParser simply retrieve the (key, value) tuples of a config file without section names? For instance:I would rather not write to the config file.

2010-05-21 20:01:34Z

I am using ConfigParser to read the runtime configuration of a script.I would like to have the flexibility of not providing a section name (there are scripts which are simple enough; they don't need a 'section'). ConfigParser will throw a NoSectionError exception, and will not accept the file.How can I make ConfigParser simply retrieve the (key, value) tuples of a config file without section names? For instance:I would rather not write to the config file.Alex Martelli provided a solution for using ConfigParser to parse .properties files (which are apparently section-less config files).His solution is a file-like wrapper that will automagically insert a dummy section heading to satisfy ConfigParser's requirements.Enlightened by this answer by jterrace, I come up with this solution:

EDIT for future googlers: As of Python 3.4+ readfp is deprecated, and StringIO is not needed anymore. Instead we can use read_string directly:You can do this in a single line of code.In python 3, prepend a fake section header to your config file data, and pass it to read_string().You could also use itertools.chain() to simulate a section header for read_file(). This might be more memory-efficient than the above approach, which might be helpful if you have large config files in a constrained runtime environment.In python 2, prepend a fake section header to your config file data, wrap the result in a StringIO object, and pass it to readfp().With any of these approaches, your config settings will be available in parser.items('top').You could use StringIO in python 3 as well, perhaps for compatibility with both old and new python interpreters, but note that it now lives in the io package and readfp() is now deprecated.Alternatively, you might consider using a TOML parser instead of ConfigParser.You can use the ConfigObj library to do that simply : http://www.voidspace.org.uk/python/configobj.htmlUpdated: Find latest code here.If you are under Debian/Ubuntu, you can install this module using your package manager :An example of use:The easiest way to do this is to use python's CSV parser, in my opinion.  Here's a read/write function demonstrating this approach as well as a test driver. This should work provided the values are not allowed to be multi-line.  :)Having ran into this problem myself, I wrote a complete wrapper to ConfigParser (the version in Python 2) that can read and write files without sections transparently, based on Alex Martelli's approach linked on the accepted answer. It should be a drop-in replacement to any usage of ConfigParser. Posting it in case anyone in need of that finds this page. Blueicefield's answer mentioned configobj, but the original lib only supports Python 2. It now has a Python 3+ compatible port: https://github.com/DiffSK/configobjAPIs haven't changed, see it's doc.

Windows is not passing command line arguments to Python programs executed from the shell

mckoss

[Windows is not passing command line arguments to Python programs executed from the shell](https://stackoverflow.com/questions/2640971/windows-is-not-passing-command-line-arguments-to-python-programs-executed-from-t)

I'm having trouble getting command line arguments passed to Python programs if I try to execute them directly as executable commands from a Windows command shell.  For example, if I have this program (test.py):And execute:as compared to:My configuration has:

2010-04-14 20:56:11Z

I'm having trouble getting command line arguments passed to Python programs if I try to execute them directly as executable commands from a Windows command shell.  For example, if I have this program (test.py):And execute:as compared to:My configuration has:I think I solved this.  For some reason there is a SECOND place in the registry (besides that shown by the file associations stored in HKEY_CLASSES_ROOT\Python.File\shell\open\command):This seems to be the controlling setting on my system.  The registry setting above adds the "%*" to pass all arguments to python.exe (it was missing in my registry for some reason).My setting was under yet another registry key, HKEY_CLASSES_ROOT\py_auto_file. The other keys mentioned also existed, but Windows was using this one for some reason.For Python 3.3 on Windows 7, my setting was under another registry key; the key I changed to make the arguments get passed wasHKEY_USERS\S-1-5-21-3922133726-554333396-2662258059-1000_Classes\py_auto_file\shell\open\commandIt was "C:\Python\Python33\python.exe" "%1". I only appended %* to it.  The key's value is now "C:\Python\Python33\python.exe" "%1" %*.I had several (at least five) other keys with the value "C:\Python\Python33\python.exe" "%1", but this is the one I changed that made it work.To make it working for me, I had to use the registry path:and added a %*Here are .reg files to fix for Python 3.6, 2.7 and Anaconda3:python-3.6.0.regpython-2.7.0.regananconda3.reg (change username)Interesting. Works here using python 2.6 and Windows XP (5.1.2600):Your program associations for .py files might be messed up. Just re-associate .py files with your python executable.Right click a .py file > Open with > Choose default program ... > [find C:\PythonXY\python.exe]I checked all registry keys with python.exe and py_auto_file and made them point to my current python installation including th %* at the end that passes arguments. They were quite a few:But that didn't do the job for me. I had to change my default python application as well.As one can see I have 3 Python versions installed. It is impossible to see which is which here so I tried all three of them as my default python application. Eventually I was able to get my script arguments with one of these three.By looking through the Windows registry, I found all the places where anything like

Python36\pythonw.exe "%1" %* appears.When I type python app.py args at the command prompt, everything works properly.When I use just the app name (app.py args) Windows opens app.py in Python, but the app fails when it tries to access argv[1], because len(argv) is 1.Apparently Windows knows enough to pass a py file to Python, but I can't figure out from looking at registry entries how it constructs the command. It appears to be using "%1" rather than "%1" %*.If fixed this on my Windows 10 system by editing the following registry keys:to this value:

data type not understood

Bob

[data type not understood](https://stackoverflow.com/questions/5446522/data-type-not-understood)

I'm trying to use a matrix to compute stuff. The code is thisbut I get 'data type not understood', and it works if I do it from terminal.

2011-03-27 01:10:10Z

I'm trying to use a matrix to compute stuff. The code is thisbut I get 'data type not understood', and it works if I do it from terminal.Try:Since the shape parameter has to be an int or sequence of intshttp://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.htmlOtherwise you are passing ncols to np.zeros as the dtype.

Python twisted: where to start [closed]

Oleg Tarasenko

[Python twisted: where to start [closed]](https://stackoverflow.com/questions/1888139/python-twisted-where-to-start)

I am trying to start learning twisted for socket servers creation. I want to add some useful features (like auth, and maybe some other). Maybe someone can point me to  a good tutorial which will help me to start (+ maybe some other ideas)

2009-12-11 13:40:12Z

I am trying to start learning twisted for socket servers creation. I want to add some useful features (like auth, and maybe some other). Maybe someone can point me to  a good tutorial which will help me to start (+ maybe some other ideas)Look here: Twisted Web in 60 seconds. That's a group of blog posts describing step by step how to do lots of common stuff with Twisted, all written by Jean-Paul Calderone, the biggest contributor of Twisted. It's really where you should start.After that, look at the Twisted core documentation then refer to the API and then into the source code.Have fun! There's a great tutorial here - it's usually the one I send to new Twisty's :-)http://krondo.com/blog/?page_id=1327Its worth remembering that Twisted programming is more of a thinking paradigm than a programming one. Also, it's worth doing away with the client server model too - in Twisted there's just one broker talking in either direction to another.Take a look here after the above too:http://twistedmatrix.com/documents/current/core/howto/pb-intro.htmlEnjoy :-)

What's so cool about Twisted? [closed]

Anton Gogolev

[What's so cool about Twisted? [closed]](https://stackoverflow.com/questions/5458631/whats-so-cool-about-twisted)

I'm increasingly hearing that Python's Twisted framework rocks and other frameworks pale in comparison.Can anybody shed some light on this and possibly compare Twisted with other network programming frameworks.

2011-03-28 11:56:50Z

I'm increasingly hearing that Python's Twisted framework rocks and other frameworks pale in comparison.Can anybody shed some light on this and possibly compare Twisted with other network programming frameworks.There are a lot of different aspects of Twisted that you might find cool.Twisted includes lots and lots of protocol implementations, meaning that more likely than not there will be an API you can use to talk to some remote system (either client or server in most cases) - be it HTTP, FTP, SMTP, POP3, IMAP4, DNS, IRC, MSN, OSCAR, XMPP/Jabber, telnet, SSH, SSL, NNTP, or one of the really obscure protocols like Finger, or ident, or one of the lower level protocol-building-protocols like DJB's netstrings, simple line-oriented protocols, or even one of Twisted's custom protocols like Perspective Broker (PB) or Asynchronous Messaging Protocol (AMP).Another cool thing about Twisted is that on top of these low-level protocol implementations, you'll often find an abstraction that's somewhat easier to use.  For example, when writing an HTTP server, Twisted Web provides a "Resource" abstraction which lets you construct URL hierarchies out of Python objects to define how requests will be responded to.All of this is tied together with cooperating APIs, mainly due to the fact that none of this functionality is implemented by blocking on the network, so you don't need to start a thread for every operation you want to do.  This contributes to the scalability that people often attribute to Twisted (although it is the kind of scalability that only involves a single computer, not the kind of scalability that lets your application grow to use a whole cluster of hosts) because Twisted can handle thousands of connections in a single thread, which tends to work better than having thousands of threads, each for a single connection.Avoiding threading is also beneficial for testing and debugging (and hence reliability in general).  Since there is no pre-emptive context switching in a typical Twisted-based program, you generally don't need to worry about locking.  Race conditions that depend on the order of different network events happening can easily be unit tested by simulating those network events (whereas simulating a context switch isn't a feature provided by most (any?) threading libraries).Twisted is also really, really concerned with quality.  So you'll rarely find regressions in a Twisted release, and most of the APIs just work, even if you aren't using them in the common way (because we try to test all the ways you might use them, not just the common way).  This is particularly true for all of the code added to Twisted (or modified) in the last 3 or 4 years, since 100% line coverage has been a minimum testing requirement since then.Another often overlooked strength of Twisted is its ten years of figuring out different platform quirks.  There are lots of undocumented socket errors on different platforms and it's really hard to learn that they even exist, let alone handle them.  Twisted has gradually covered more and more of these, and it's pretty good about it at this point.  Younger projects don't have this experience, so they miss obscure failure modes that will probably only happen to users of any project you release, not to you.All that say, what I find coolest about Twisted is that it's a pretty boring library that lets me ignore a lot of really boring problems and just focus on the interesting and fun things. :)Well it's probably according to taste. Twisted allows you to easily create event driven network servers/clients, without really worrying about everything that goes into accomplishing this. And thanks to the MIT License, Twisted can be used almost anywhere. But I haven't done any benchmarking so I have no idea how it scales, but I'm guessing quite good.Another plus would be the Twisted Projects, with which you can quickly see how to implement most of the server/services that you would want to.Twisted also has some great documentation, when I started with it a couple of weeks ago I was able to quickly get a working prototype.Quite new to the python scene please correct me if i'm in the wrong.

Validate SSL certificates with Python

Eli Courtwright

[Validate SSL certificates with Python](https://stackoverflow.com/questions/1087227/validate-ssl-certificates-with-python)

I need to write a script that connects to a bunch of sites on our corporate intranet over HTTPS and verifies that their SSL certificates are valid; that they are not expired, that they are issued for the correct address, etc.  We use our own internal corporate Certificate Authority for these sites, so we have the public key of the CA to verify the certificates against.Python by default just accepts and uses SSL certificates when using HTTPS, so even if a certificate is invalid, Python libraries such as urllib2 and Twisted will just happily use the certificate.Is there a good library somewhere that will let me connect to a site over HTTPS and verify its certificate in this way?How do I verify a certificate in Python?

2009-07-06 14:17:34Z

I need to write a script that connects to a bunch of sites on our corporate intranet over HTTPS and verifies that their SSL certificates are valid; that they are not expired, that they are issued for the correct address, etc.  We use our own internal corporate Certificate Authority for these sites, so we have the public key of the CA to verify the certificates against.Python by default just accepts and uses SSL certificates when using HTTPS, so even if a certificate is invalid, Python libraries such as urllib2 and Twisted will just happily use the certificate.Is there a good library somewhere that will let me connect to a site over HTTPS and verify its certificate in this way?How do I verify a certificate in Python?From release version 2.7.9/3.4.3 on, Python by default attempts to perform certificate validation.This has been proposed in PEP 467, which is worth a read: https://www.python.org/dev/peps/pep-0476/The changes affect all relevant stdlib modules (urllib/urllib2, http, httplib).Relevant documentation:https://docs.python.org/2/library/httplib.html#httplib.HTTPSConnectionhttps://docs.python.org/3/library/http.client.html#http.client.HTTPSConnectionNote that the new built-in verification is based on the system-provided certificate database. Opposed to that, the requests package ships its own certificate bundle. Pros and cons of both approaches are discussed in the Trust database section of PEP 476.I have added a distribution to the Python Package Index which makes the match_hostname() function from the Python 3.2 ssl package available on previous versions of Python.http://pypi.python.org/pypi/backports.ssl_match_hostname/You can install it with:Or you can make it a dependency listed in your project's setup.py. Either way, it can be used like this:You can use Twisted to verify certificates.  The main API is CertificateOptions, which can be provided as the contextFactory argument to various functions such as listenSSL and startTLS.Unfortunately, neither Python nor Twisted comes with a the pile of CA certificates required to actually do HTTPS validation, nor the HTTPS validation logic.  Due to a limitation in PyOpenSSL, you can't do it completely correctly just yet, but thanks to the fact that almost all certificates include a subject commonName, you can get close enough.Here is a naive sample implementation of a verifying Twisted HTTPS client which ignores wildcards and subjectAltName extensions, and uses the certificate-authority certificates present in the 'ca-certificates' package in most Ubuntu distributions.  Try it with your favorite valid and invalid certificate sites :).PycURL does this beautifully.Below is a short example. It will throw a pycurl.error if something is fishy, where you get a tuple with error code and a human readable message.You will probably want to configure more options, like where to store the results, etc. But no need to clutter the example with non-essentials.Example of what exceptions might be raised:Some links that I found useful are the libcurl-docs for setopt and getinfo.Here's an example script which demonstrates certificate validation:Or simply make your life easier by using the requests library:A few more words about its usage.M2Crypto can do the validation. You can also use M2Crypto with Twisted if you like. The Chandler desktop client uses Twisted for networking and M2Crypto for SSL, including certificate validation.Based on Glyphs comment it seems like M2Crypto does better certificate verification by default than what you can do with pyOpenSSL currently, because M2Crypto checks subjectAltName field too.I've also blogged on how to get the certificates Mozilla Firefox ships with in Python and usable with Python SSL solutions.Jython DOES carry out certificate verification by default, so using standard library modules, e.g. httplib.HTTPSConnection, etc, with jython will verify certificates and give exceptions for failures, i.e. mismatched identities, expired certs, etc.In fact, you have to do some extra work to get jython to behave like cpython, i.e. to get jython to NOT verify certs. I have written a blog post on how to disable certificate checking on jython, because it can be useful in testing phases, etc.Installing an all-trusting security provider on java and jython.

http://jython.xhaus.com/installing-an-all-trusting-security-provider-on-java-and-jython/The following code allows you to benefit from all SSL validation checks (e.g. date validity, CA certificate chain ...) EXCEPT a pluggable verification step e.g. to verify the hostname or do other additional certificate verification steps.pyOpenSSL is an interface to the OpenSSL library. It should provide everything you need.I was having the same problem but wanted to minimize 3rd party dependencies (because this one-off script was to be executed by many users). My solution was to wrap a curl call and make sure that the exit code was 0. Worked like a charm.

Built in Python hash() function

SilentGhost

[Built in Python hash() function](https://stackoverflow.com/questions/793761/built-in-python-hash-function)

Windows XP, Python 2.5:Google App Engine (http://shell.appspot.com/):Why is that? How can I have a hash function that will give me same results across different platforms (Windows, Linux, Mac)? 

2009-04-27 14:31:00Z

Windows XP, Python 2.5:Google App Engine (http://shell.appspot.com/):Why is that? How can I have a hash function that will give me same results across different platforms (Windows, Linux, Mac)? Use hashlib as hash() was designed to be used to:and therefore does not guarantee that it will be the same across Python implementations.As stated in the documentation, built-in hash() function is not designed for storing resulting hashes somewhere externally. It is used to provide object's hash value, to store them in dictionaries and so on. It's also implementation-specific (GAE uses a modified version of Python). Check out:As you can see, they are different, as hash() uses object's __hash__ method instead of 'normal' hashing algorithms, such as SHA.Given the above, the rational choice is to use the hashlib module.The response is absolutely no surprise: in fact so if you want to get reliable responses on ASCII strings, just get the lower 32 bits as uint. The hash function for strings is 32-bit-safe and almost portable.On the other side, you can't rely at all on getting the hash() of any object over which you haven't explicitly defined the __hash__ method to be invariant. Over ASCII strings it works just because the hash is calculated on the single characters forming the string, like the following:where the c_mul function is the "cyclic" multiplication (without overflow) as in C.Most answers suggest this is because of different platforms, but there is more to it. From the documentation of object.__hash__(self):Even running on the same machine will yield varying results across invocations:While:See also the environment variable PYTHONHASHSEED:For example:Hash results varies between 32bit and 64bit platformsIf a calculated hash shall be the same on both platforms consider usingAt a guess, AppEngine is using a 64-bit implementation of Python (-5768830964305142685 won't fit in 32 bits) and your implementation of Python is 32 bits. You can't rely on object hashes being meaningfully comparable between different implementations.This is the hash function that Google uses in production for python 2.5:What about sign bit?For example:Hex value 0xADFE74A5 represents unsigned 2919134373 and signed -1375832923.

Currect value must be signed (sign bit = 1) but python converts it as unsigned and we have an incorrect hash value after translation from 64 to 32 bit.Be careful using:Polynomial hash for strings. 1000000009 and 239 are arbitrary prime numbers. Unlikely to have collisions by accident. Modular arithmetic is not very fast, but for preventing collisions this is more reliable than taking it modulo a power of 2. Of course, it is easy to find a collision on purpose.The value of PYTHONHASHSEED might be used to initialize the hash values.Try:It probably just asks the operating system provided function, rather than its own algorithm. As other comments says, use hashlib or write your own hash function.

How do I pass extra arguments to a Python decorator?

balki

[How do I pass extra arguments to a Python decorator?](https://stackoverflow.com/questions/10176226/how-do-i-pass-extra-arguments-to-a-python-decorator)

I have a decorator like below.I want to enhance this decorator to accept another argument like belowBut this code gives the error,Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?

2012-04-16 14:38:33Z

I have a decorator like below.I want to enhance this decorator to accept another argument like belowBut this code gives the error,Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?Since you are calling the decorator like a function, it needs to return another function which is the actual decorator:The outer function will be given any arguments you pass explicitly, and should return the inner function. The inner function will be passed the function to decorate, and return the modified function.Usually you want the decorator to change the function behavior by wrapping it in a wrapper function. Here's an example that optionally adds logging when the function is called:The functools.wraps call copies things like the name and docstring to the wrapper function, to make it more similar to the original function.Example usage:Just to provide a different viewpoint: the syntaxis equivalent to In particular, expr can be anything you like, as long as it evaluates to a callable. In particular particular, expr can be a decorator factory: you give it some parameters and it gives you a decorator. So maybe a better way to understand your situation is aswhich can then be shortened toOf course, since it looks like decorator_factory is a decorator, people tend to name it to reflect that. Which can be confusing when you try to follow the levels of indirection.Just want to add some usefull trick that will allow to make decorator arguments optional. It will also alows to reuse decorator and decrease nestingJust another way of doing decorators.

I find this way the easiest to wrap my head around.

Counterintuitive behaviour of int() in python

StefanS

[Counterintuitive behaviour of int() in python](https://stackoverflow.com/questions/36085185/counterintuitive-behaviour-of-int-in-python)

It's clearly stated in the docs that int(number) is a flooring type conversion:and int(string) returns an int if and only if the string is an integer literal.Is there any special reason for that? I find it counterintuitive that the function floors in one case, but not the other.

2016-03-18 13:05:59Z

It's clearly stated in the docs that int(number) is a flooring type conversion:and int(string) returns an int if and only if the string is an integer literal.Is there any special reason for that? I find it counterintuitive that the function floors in one case, but not the other.There is no special reason. Python is simply applying its general principle of not performing implicit conversions, which are well-known causes of problems, particularly for newcomers, in languages such as Perl and Javascript.int(some_string) is an explicit request to convert a string to integer format; the rules for this conversion specify that the string must contain a valid integer literal representation. int(float) is an explicit request to convert a float to an integer; the rules for this conversion specify that the float's fractional portion will be truncated.In order for int("3.1459") to return 3 the interpreter would have to implicitly convert the string to a float. Since Python doesn't support implicit conversions, it chooses to raise an exception instead.This is almost certainly a case of applying three of the principles from the Zen of Python: Some percentage of the time, someone doing int('1.23') is calling the wrong conversion for their use case, and wants something like float or decimal.Decimal instead. In these cases, it's clearly better for them to get an immediate error that they can fix, rather than silently giving the wrong value. In the case that you do want to truncate that to an int, it is trivial to explicitly do so by passing it through float first, and then calling one of int, round, trunc, floor or ceil as appropriate. This also makes your code more self-documenting, guarding against a later modification "correcting" a hypothetical silently-truncating int call to float by making it clear that the rounded value is what you want.Sometimes a thought experiment can be useful.With behavior A, it's straightforward and trivial to get the effect of behavior B: use int(float('1.23')) instead.On the other hand, with behavior B, getting the effect of behavior A is significantly more complicated:(and even with the code above, I don't have complete confidence that there isn't some corner case that it mishandles.)Behavior A therefore is more expressive than behavior B.Another thing to consider: '1.23' is a string representation of a floating-point value.  Converting '1.23' to an integer conceptually involves two conversions (string to float to integer), but int(1.23) and int('1') each involve only one conversion.Edit:And indeed, there are corner cases that the above code would not handle: 1e-2 and 1E-2 are both floating point values too.In simple words - they're not the same function. They are 2 different functions with the same name that return an integer but they are different functions.'int' is short and easy to remember and its meaning applied to each type is intuitive to most programmers which is why they chose it. There's no implication they are providing the same or combined functionality, they simply have the same name and return the same type. They could as easily be called 'floorDecimalAsInt' and 'convertStringToInt', but they went for 'int' because it's easy to remember, (99%) intuitive and confusion would rarely occur.Parsing text as an Integer for text which included a decimal point such as "4.5" would throw an error in majority of computer languages and be expected to throw an error by majority of programmers, since the text-value does not represent an integer and implies they are providing erroneous data

Is there an easy way to request a URL in python and NOT follow redirects?

John

[Is there an easy way to request a URL in python and NOT follow redirects?](https://stackoverflow.com/questions/110498/is-there-an-easy-way-to-request-a-url-in-python-and-not-follow-redirects)

Looking at the source of urllib2 it looks like the easiest way to do it would be to subclass HTTPRedirectHandler and then use build_opener to override the default HTTPRedirectHandler, but this seems like a lot of (relatively complicated) work to do what seems like it should be pretty simple.

2008-09-21 07:49:10Z

Looking at the source of urllib2 it looks like the easiest way to do it would be to subclass HTTPRedirectHandler and then use build_opener to override the default HTTPRedirectHandler, but this seems like a lot of (relatively complicated) work to do what seems like it should be pretty simple.Here is the Requests way:Dive Into Python has a good chapter on handling redirects with urllib2. Another solution is httplib.This is a urllib2 handler that will not follow redirects:The redirections keyword in the httplib2 request method is a red herring. Rather than return the first request it will raise a RedirectLimit exception if it receives a redirection status code. To return the inital response you need to set follow_redirects to False on the Http object:i suppose this would helpI second olt's pointer to Dive into Python. Here's an implementation using urllib2 redirect handlers, more work than it should be? Maybe, shrug.The shortest way however is

Best method for reading newline delimited files and discarding the newlines?

solarce

[Best method for reading newline delimited files and discarding the newlines?](https://stackoverflow.com/questions/544921/best-method-for-reading-newline-delimited-files-and-discarding-the-newlines)

I am trying to determine the best way to handle getting rid of newlines when reading in newline delimited files in Python.What I've come up with is the following code, include throwaway code to test.Suggestions?

2009-02-13 06:31:11Z

I am trying to determine the best way to handle getting rid of newlines when reading in newline delimited files in Python.What I've come up with is the following code, include throwaway code to test.Suggestions?Here's a generator that does what you requested. In this case, using rstrip is sufficient and slightly faster than strip.However, you'll most likely want to use this to get rid of trailing whitespaces too.What do you think about this approach?Generator expression avoids loading whole file into memory and with ensures closing the fileJust use generator expressions:Also I want to advise you against reading whole file in memory -- looping over generators is much more efficient on big datasets.I use thisThen I can do things like this.Or, I can extend cleaned with extra functions to, for example, drop blank lines or skip comment lines or whatever.I'd do it like this:

Plot a bar using matplotlib using a dictionary

otmezger

[Plot a bar using matplotlib using a dictionary](https://stackoverflow.com/questions/16010869/plot-a-bar-using-matplotlib-using-a-dictionary)

Is there any way to plot a bar plot using matplotlib using data directly from a dict? My dict looks like this: I was expecting to work, but it does not. Here is the error:

2013-04-15 08:34:47Z

Is there any way to plot a bar plot using matplotlib using data directly from a dict? My dict looks like this: I was expecting to work, but it does not. Here is the error:You can do it in two lines by first plotting the bar chart and then setting the appropriate ticks:Note that the penultimate line should read plt.xticks(range(len(D)), list(D.keys())) in python3, because D.keys() returns a generator, which matplotlib cannot use directly.For future reference, the above code does not work with Python 3. For Python 3, the D.keys() needs to be converted to a list.It's a little simpler than most answers here suggest:The best way to implement it using matplotlib.pyplot.bar(range, height, tick_label) where the range provides scalar values for the positioning of the corresponding bar in the graph. tick_label does the same work as xticks(). One can replace it with an integer also and use multiple plt.bar(integer, height, tick_label). For detailed information please refer the documentation. Additionally the same plot can be generated without using range(). But the problem encountered was that tick_label just worked for the last plt.bar() call. Hence xticks() was used for labelling:I often load the dict into a pandas DataFrame then use the plot function of the DataFrame. 

Here is the one-liner:Why not just:

Scatter plot and Color mapping in Python

Vincent

[Scatter plot and Color mapping in Python](https://stackoverflow.com/questions/17682216/scatter-plot-and-color-mapping-in-python)

I have a range of points x and y stored in numpy arrays.

Those represent x(t) and y(t) where t=0...T-1I am plotting a scatter plot usingI would like to have a colormap representing the time (therefore coloring the points depending on the index in the numpy arrays) What is the easiest way to do so?

2013-07-16 16:36:31Z

I have a range of points x and y stored in numpy arrays.

Those represent x(t) and y(t) where t=0...T-1I am plotting a scatter plot usingI would like to have a colormap representing the time (therefore coloring the points depending on the index in the numpy arrays) What is the easiest way to do so?Here is an exampleHere you are setting the color based on the index, t, which is just an array of [1, 2, ..., 100].

Perhaps an easier-to-understand example is the slightly simplerNote that the array you pass as c doesn't need to have any particular order or type, i.e. it doesn't need to be sorted or integers as in these examples.  The plotting routine will scale the colormap such that the minimum/maximum values in c correspond to the bottom/top of the colormap.You can change the colormap by addingImporting matplotlib.cm is optional as you can call colormaps as cmap="cmap_name" just as well.  There is a reference page of colormaps showing what each looks like.  Also know that you can reverse a colormap by simply calling it as cmap_name_r.  So eitherwill work.  Examples are "jet_r" or cm.plasma_r.  Here's an example with the new 1.5 colormap viridis:You can add a colorbar by usingNote that if you are using figures and subplots explicitly (e.g. fig, ax = plt.subplots() or ax = fig.add_subplot(111)), adding a colorbar can be a bit more involved. Good examples can be found here for a single subplot colorbar and here for 2 subplots 1 colorbar.To add to wflynny's answer above, you can find the available colormaps hereExample:

or alternatively,

Subplot ColorbarFor subplots with scatter, you can trick a colorbar onto your axes by building the "mappable" with the help of a secondary figure and then adding it to your original plot.As a continuation of the above example:Note that you will also output a secondary figure that you can ignore.

List comprehension: Returning two (or more) items for each item

Hashmush

[List comprehension: Returning two (or more) items for each item](https://stackoverflow.com/questions/11868964/list-comprehension-returning-two-or-more-items-for-each-item)

Is it possible to return 2 (or more) items for each item in a list comprehension?What I want (example):should return [f(0), g(0), f(1), g(1), ..., f(n-1), g(n-1)]So, something to replace this block of code:

2012-08-08 16:27:53Z

Is it possible to return 2 (or more) items for each item in a list comprehension?What I want (example):should return [f(0), g(0), f(1), g(1), ..., f(n-1), g(n-1)]So, something to replace this block of code:Timings:Double list comprehension:Demo:This is equivalent to [f(1),g(1)] + [f(2),g(2)] + [f(3),g(3)] + ...You can also think of it as:note: The right way is to use itertools.chain.from_iterable or the double list comprehension. (It does not require recreating the list on every +, thus has O(N) performance rather than O(N^2) performance.) I'll still use sum(..., []) when I want a quick one-liner or I'm in a hurry, or when the number of terms being combined is bounded (e.g. <= 10). That is why I still mention it here, with this caveat. You can also use tuples: ((f(x),g(x)) for ...), () (or per khachik's comment, having a generator fg(x) which yields a two-tuple).This lambda function zips two lists into a single one:Example:

django 1.4 - can't compare offset-naive and offset-aware datetimes

meepmeep

[django 1.4 - can't compare offset-naive and offset-aware datetimes](https://stackoverflow.com/questions/10652819/django-1-4-cant-compare-offset-naive-and-offset-aware-datetimes)

I am in the process of migrating an application from django 1.2 To 1.4.I have a daily task object which contains a time of day that task should be completed:In order to check if a task is still required to be completed today, I have the following code:This worked fine under 1.2, But under 1.4 I get the error:due to the lineand both comparison clauses throw this error.I have tried making timeDue timezone aware by adding pytz.UTC as an argument, but this still raises the same error.I've read some of the docs on timezones but am confused as to whether I just need to make timeDue timezone aware, or whether I need to make a fundamental change to my db and existing data.

2012-05-18 12:39:51Z

I am in the process of migrating an application from django 1.2 To 1.4.I have a daily task object which contains a time of day that task should be completed:In order to check if a task is still required to be completed today, I have the following code:This worked fine under 1.2, But under 1.4 I get the error:due to the lineand both comparison clauses throw this error.I have tried making timeDue timezone aware by adding pytz.UTC as an argument, but this still raises the same error.I've read some of the docs on timezones but am confused as to whether I just need to make timeDue timezone aware, or whether I need to make a fundamental change to my db and existing data.Check the thorough document for detail info.Normally, use django.utils.timezone.now to make an offset-aware current datetimeAnd django.utils.timezone.make_aware to make an offset-aware datetimeYou could then compare both offset-aware datetimes w/o trouble. Furthermore, you could convert offset-awared datetime to offset-naive datetime by stripping off timezone info, then it could be compared w/ normal datetime.datetime.now(), under utc.USE_TZ is True 'by default' (actually it's False by default, but the settings.py file generated by django-admin.py startproject set it to True), then if your DB supports timezone-aware times, values of time-related model fields would be timezone-aware. you could disable it by setting USE_TZ=False(or simply remove USE_TZ=True) in settings. 

How do you sort a list in Jinja2?

Nick Perkins

[How do you sort a list in Jinja2?](https://stackoverflow.com/questions/1959386/how-do-you-sort-a-list-in-jinja2)

I am trying to do this:But that's not right...the documentation is vague...how do you do this in Jinja2?

2009-12-24 18:56:31Z

I am trying to do this:But that's not right...the documentation is vague...how do you do this in Jinja2?As of version 2.6, Jinja2's built-in sort filter allows you to specify an attribute to sort by:See http://jinja.pocoo.org/docs/templates/#sortIf you want to sort in ascending orderIf you want to sort in descending orderUsually we sort the list before giving it to Jinja2.  There's no way to specify a key in Jinja's sort filter.However, you can always try {% for movie in movie_list|sort %}.  That's the syntax.  You don't get to provide any sort of key information for the sorting.You can also try and write a custom filter for this.  Seems silly when you can sort before giving the data to Jinja2.If movie_list is a list of objects, then you can define the various comparison methods (__lt__, __gt__, etc.) for the class of those objects.If movie_list is a list of tuples or lists, the rating must be first.  Or you'll have to do the sorting outside Jinja2.If movie_list is a list of dictionaries, then you can use dictsort, which does accept a key specification for the sorting.  Read this: http://jinja.pocoo.org/2/documentation/templates#dictsort for an example.

Python: Platform independent way to modify PATH environment variable

resi

[Python: Platform independent way to modify PATH environment variable](https://stackoverflow.com/questions/1681208/python-platform-independent-way-to-modify-path-environment-variable)

Is there a way to modify the PATH environment variable in a platform independent way using python?Something similar to os.path.join()?

2009-11-05 15:17:39Z

Is there a way to modify the PATH environment variable in a platform independent way using python?Something similar to os.path.join()?You should be able to modify os.environ.Since os.pathsep is the character to separate different paths, you should use this to append each new path:or, if there are several paths to add in a list:As you mentioned, os.path.join can also be used for each individual path you have to append in the case you have to construct them from separate parts.Please note that os.environ is not actually a dictionary.  It's a special dictionary-like object which actually sets environment variables in the current process using setenv.This means that PATH (and other environment variables) will be visible to C code run in the same process.(Since comments can't contain formatting, I have to put this in an answer, but I feel like it's an important point to make.  This is really a comment on the comment about there being no equivalent to 'export'.)The caveat to be aware of with modifying environment variables in Python, is that there is no equivalent of the "export" shell command.  There is no way of injecting changes into the current process, only child processes.

How to index into a dictionary?

Harpal

[How to index into a dictionary?](https://stackoverflow.com/questions/4326658/how-to-index-into-a-dictionary)

I have a Dictionary below:How do I index the first entry in the dictionary?colors[0] will return a KeyError for obvious reasons.

2010-12-01 16:40:16Z

I have a Dictionary below:How do I index the first entry in the dictionary?colors[0] will return a KeyError for obvious reasons.Dictionaries are unordered in Python versions up to and including Python 3.6.  If you do not care about the order of the entries and want to access the keys or values by index anyway, you can use d.keys()[i] and d.values()[i] or d.items()[i].  (Note that these methods create a list of all keys, values or items in Python 2.x.  So if you need them more then once, store the list in a variable to improve performance.)If you do care about the order of the entries, starting with Python 2.7 you can use collections.OrderedDict.  Or use a list of pairsif you don't need access by key.  (Why are your numbers strings by the way?)In Python 3.7, normal dictionaries are ordered, so you don't need to use OrderedDict anymore (but you still can ‚Äì it's basically the same type).  The CPython implementation of Python 3.6 already included that change, but since it's not part of the language specification, you can't rely on it in Python 3.6.If anybody still looking at this question, the currently accepted answer is now outdated:Since Python 3.7* the dictionaries are order-preserving, that is they now behave exactly as collections.OrderedDicts used to. Unfortunately, there is still no dedicated method to index into keys() / values() of the dictionary, so getting the first key / value in the dictionary can be done asor alternatively (this avoids instantiating the keys view into a list):If you need an n-th key, then similarly(*CPython 3.6 already included ordered dicts, but this was only an implementation detail. The language specification includes ordered dicts from 3.7 onwards.)Addressing an element of dictionary is like sitting on donkey and enjoy the ride.As rule of Python DICTIONARY is orderlessIf there isNow suppose if I go like dic[10] = "b", then it will not add like this alwaysIt may be likeOrOrOr any such combination.So thumb rule is DICTIONARY is orderless!If you need an ordered dictionary, you can use odict.actually I found a novel solution that really helped me out,  If you are especially concerned with the index of a certain value in a list or data set, you can just set the value of dictionary to that Index!:Just watch:Now through the power of hashmaps you can pull the index your entries in constant time (aka a whole lot faster)oh, that's a tough one. What you have here, basically, is two values for each item. Then you are trying to call them with a number as the key. Unfortunately, one of your values is already set as the key!Try this:Now you can call the keys by number as if they are indexed like a list. You can also reference the color and number by their position within the list.For example,Of course, you will have to come up with another way of keeping track of what location each color is in. Maybe you can have another dictionary that stores each color's key as it's value.Then, you will be able to also look up the colors key if you need to.colors[colors_key['blue']][0] will return 'blue'Something like that.And then, while you're at it, you can make a dict with the number values as keys so that you can always use them to look up your colors, you know, if you need.Then, (colors[colors_key[values[5][1]]][0]) will return 'blue'.Or you could use a list of lists.Good luck!You can't, since dict is unordered. you can use .popitem() to get an arbitrary item, but that will remove it from the dict.

How to source virtualenv activate in a Bash script

Cerin

[How to source virtualenv activate in a Bash script](https://stackoverflow.com/questions/13122137/how-to-source-virtualenv-activate-in-a-bash-script)

How do you create a Bash script to activate a Python virtualenv?I have a directory structure like:I can activate my virtualenv by:However, doing the same from a Bash script does nothing:What am I doing wrong?

2012-10-29 12:57:30Z

How do you create a Bash script to activate a Python virtualenv?I have a directory structure like:I can activate my virtualenv by:However, doing the same from a Bash script does nothing:What am I doing wrong?When you source, you're loading the activate script into your active shell.When you do it in a script, you load it into that shell which exits when your script finishes and you're back to your original, unactivated shell.Your best option would be to do it in a functionor an aliasHope this helps.You should call the bash script using source.Here is an example:On your shell just call it like that:Or as @outmind suggested: (Note that this does not work with zsh)There you go, the shell indication will be placed on your prompt.Although it doesn't add the "(.env)" prefix to the shell prompt, I found this script works as expected.e.g.Sourcing runs shell commands in your current shell.  When you source inside of a script like you are doing above, you are affecting the environment for that script, but when the script exits, the environment changes are undone, as they've effectively gone out of scope.If your intent is to run shell commands in the virtualenv, you can do that in your script after sourcing the activate script.  If your intent is to interact with a shell inside the virtualenv, then you can spawn a sub-shell inside your script which would inherit the environment.You can also do this using a subshell to better contain your usage - here's a practical example:This style is especially helpful whenWhat does sourcing the bash script for?You should use multiple commands in one line. for example:when you activate your virtual environment in one line, I think it forgets for other command lines and you can prevent this by using multiple commands in one line.

It worked for me :)Here is the script that I use often. Run it as $ source script_name

IronPython vs. Python .NET

cschol

[IronPython vs. Python .NET](https://stackoverflow.com/questions/1168914/ironpython-vs-python-net)

I want to access some .NET assemblies written in C# from Python code. A little research showed I have two choices:What are the trade-offs between both solutions?

2009-07-23 00:06:00Z

I want to access some .NET assemblies written in C# from Python code. A little research showed I have two choices:What are the trade-offs between both solutions?If you want to mainly base your code on the .NET framework, I'd highly recommend IronPython vs Python.NET.  IronPython is pretty much native .NET - so it just works great when integrating with other .NET langauges.  Python.NET is good if you want to just integrate one or two components from .NET into a standard python application.There are notable differences when using IronPython - but most of them are fairly subtle.  Python.NET uses the standard CPython runtime, so this Wiki page is a relevant discussion of the differences between the two implementations.  The largest differences occur in the cost of exceptions - so some of the standard python libraries don't perform as well in IronPython due to their implementation.While agreeing with the answers given by Reed Copsey and Alex Martelli, I'd like to point out one further difference - the Global Interpreter Lock (GIL). While IronPython doesn't have the limitations of the GIL, CPython does - so it would appear that for those applications where the GIL is a bottleneck, say in certain multicore scenarios, IronPython has an advantage over Python.NET.From the Python.NET documentation:Another issue is IDE support. CPython probably has better IDE support at present than IronPython - so this may be a factor in the choosing of one over the other.Most of scientific and numerical Python libraries that rely on CPython C-API (numpy, scipy, matplotlib, pandas, cython, etc.) are working mostly under CPython, so in that case your best bet is pythonnet (other names - Python.NET and Python for .NET).

The same is true for CPython GUI bindings such as WxWidgets, PyQt/PySide, GTK, Kivy, etc., although both pythonnet and IronPython can use WPF and WinForms.And finally IronPython does not fully support Python 3 yet.IronPython is ".NET-native" -- so it will be preferable if you want to fully integrate your Python code with .NET all the way; Python.NET works with Classic Python, so it lets you keep your Python code's "arm's length" away from .NET proper.  (Note that with this code you can actually use extensions written for CPython from your IronPython code, so that's not a discriminating condition any more).IronPython comes from Microsoft, so I would go with my gut and use that one first since you have to assume it will play nicer with other MSFT technologies.  As for 2016. In my company we used IronPython, but we were not satisfied with performances (mostly memory use - garbage collector was too slow) so we decided to switch to standard Python and integrate it with .Net using Zeroce-s ICE. IronPython, currently,  doesn't support Python 3.6 (only 2.7)from IronPython 3 "Builds of IronPython 3 are not yet provided."A funny, simple and powerful language regardless what you choose!Iron Python is basically Python 2.7 with integrated .net support it probably will never support Python 3. It loses out on C and Python libraries, however on the twist side has access to .net and can be extended with C#. So if you use C# already then Iron Python is a bonus.I mainly prefer Python for .NET, because IronPython is compiled as managed code, which can be easily decompiled (what I most hate), but with py2exe or pyinstaller you can compile Python with NET module as an unmanaged application.

How do I install an old version of Django on virtualenv?

Ram Rachum

[How do I install an old version of Django on virtualenv?](https://stackoverflow.com/questions/3220280/how-do-i-install-an-old-version-of-django-on-virtualenv)

This may sound like a stupid question, since the very purpose of virtualenv is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created easy_install script, but how do I make it install Django 1.0.7? If I do easy_install django, it will install the latest version. I tried putting the version number 1.0.7 into this command in various ways, but nothing worked.How do I do this?

2010-07-10 17:55:47Z

This may sound like a stupid question, since the very purpose of virtualenv is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created easy_install script, but how do I make it install Django 1.0.7? If I do easy_install django, it will install the latest version. I tried putting the version number 1.0.7 into this command in various ways, but nothing worked.How do I do this?There was never a Django 1.0.7. The 1.0 series only went up to 1.0.4. You can see all the releases in the tags section of the Django code repository.However to answer your question, don't use easy_install, use pip. (If it's not already installed, do easy_install pip, then never touch easy_install again). Now you can do:+1 on the previous poster's reply: use pip if you can. But, in a pinch, the easiest way is to install an older version would be to download the tarball from the downloads page or, if you have subversion installed, do an svn export of the release you want (they are all tagged here). Once you have the version of Django you want, just run the following command inside the django directory:This will install that version of Django in your virtualenv. +1 for already mentioned solutions.I just wanna add another solution.To install a specific version of Django (say 1.10.x), 

How to programmatically generate markdown output in Jupyter notebooks?

fulaphex

[How to programmatically generate markdown output in Jupyter notebooks?](https://stackoverflow.com/questions/36288670/how-to-programmatically-generate-markdown-output-in-jupyter-notebooks)

I want to write a report for classes in Jupyter notebook. I'd like to count some stuff, generate some results and include them in markdown. Can I set the output of the cell to be interpreted as markdown?

I'd like such command: print '$\phi$' to generate phi symbol, just like in markdown.

In other words, I'd like to have a template made in markdown and insert the values generated by the program written in the notebook. Recalculating the notebook should generate new results and new markdown with those new values inserted. Is that possible with this software, or do I need to replace the values by myself?

2016-03-29 15:29:52Z

I want to write a report for classes in Jupyter notebook. I'd like to count some stuff, generate some results and include them in markdown. Can I set the output of the cell to be interpreted as markdown?

I'd like such command: print '$\phi$' to generate phi symbol, just like in markdown.

In other words, I'd like to have a template made in markdown and insert the values generated by the program written in the notebook. Recalculating the notebook should generate new results and new markdown with those new values inserted. Is that possible with this software, or do I need to replace the values by myself?The functions you want are in the IPython.display module.You are basically asking for two different things:Since 2. is already covered by another answer (basically: use Latex() or Markdown() imported from IPython.display), I will focus on the first one:With the Jupyter extension Python Markdown it actually is possible to do exactly what you describe.Installation instructions can be found on the github page of nbextensions. Make sure you'll enable the python markdown extension using a jupyter command or the extension configurator. With the extension, variables are accessed via {{var-name}}. An example for such a markdown template could look like this:Naturally all variables or images a, b, i should be set in previous code. And of course you may also make use of Markdown-Latex-style expressions (like $\phi$) without the print command. This image is from the wiki of the extension, demonstrating the capability.Further info on this functionality being integrated into ipython/jupyter is discussed in the issue trackers for ipython and jupyter.

Why #egg=foo when pip-installing from git repo

Lorin Hochstein

[Why #egg=foo when pip-installing from git repo](https://stackoverflow.com/questions/11835396/why-egg-foo-when-pip-installing-from-git-repo)

When I do a "pip install -e ..." to install from a git repo, I have to specify #egg=somename or pip complains. For example:What's the significance of this "egg" string?

2012-08-06 20:24:15Z

When I do a "pip install -e ..." to install from a git repo, I have to specify #egg=somename or pip complains. For example:What's the significance of this "egg" string?per pip install -h the "egg" string is the directory that gets checked out as part of the installYou have to include #egg=Package so pip knows what to expect at that URL. See https://pip.pypa.io/en/stable/reference/pip_install/#vcs-supportmore on eggshttps://pip.pypa.io/en/stable/reference/pip_install/#vcs-support says:From this I deduce that the egg value is only used for dependency checks and therefore I think, by convention, the package name (i.e. some-pypi-package-name) should be used, not any contained folder (i.e. some_pypi_package_name)An Egg is just some bundled python code. In a git url, the egg is the project name.  VCS Support Normally we install python packages from Pypi, so you specify ONLY the package name and version (or it assumes latest version if you don't specify). Pypi then searches for which egg you want and pip installs that.  pip install celery would install the latest published egg and pip install celery[redis] would install a different egg that contains the same celery package and also installs the the latest eggs from whatever packages were listed as dependencies for redis in celery's setup.py.With git and gitlab paths, you specify /{user|group}/{repository}.git@{tag}#egg={package-name}. there is a difference between #egg=celery and #egg=celery[redis], but they will both come from the same source code. "tag" can also be a branch or commit hash in addition to an actual tag. It is assumed to be master if you do not specify.for example, git+https://github.com/celery/celery.git#egg=celery==4.3.0 would check out the master branch and install that. Even though you specified a version number, it is not taken into account in the installation. THE VERSION NUMBER IS IGNOREDWhen installing via git or other VCS urls, you will want to find the tag or hash of the version you need. For example, git+https://github.com/celery/celery.git@v4.3.0#egg=celery which will checkout the commit tagged "v4.3.0" and then install the package from that source code.  Assuming the maintainers did not egregiously mis-tag their repositories, you can get the version you want like that.

How do I find Wally with Python?

tdc

[How do I find Wally with Python?](https://stackoverflow.com/questions/8849869/how-do-i-find-wally-with-python)

Shamelessly jumping on the bandwagon :-)Inspired by How do I find Waldo with Mathematica and the followup How to find Waldo with R, as a new python user I'd love to see how this could be done. It seems that python would be better suited to this than R, and we don't have to worry about licenses as we would with Mathematica or Matlab.In an example like the one below obviously simply using stripes wouldn't work. It would be interesting if a simple rule based approach could be made to work for difficult examples such as this.I've added the [machine-learning] tag as I believe the correct answer will have to use ML techniques, such as the Restricted Boltzmann Machine (RBM) approach advocated by Gregory Klopper in the original thread. There is some RBM code available in python which might be a good place to start, but obviously training data is needed for that approach. At the 2009 IEEE International Workshop on MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP 2009) they ran a Data Analysis Competition: Where's Wally?. Training data is provided in matlab format. Note that the links on that website are dead, but the data (along with the source of an approach taken by Sean McLoone and colleagues can be found here (see SCM link). Seems like one place to start.

2012-01-13 11:28:14Z

Shamelessly jumping on the bandwagon :-)Inspired by How do I find Waldo with Mathematica and the followup How to find Waldo with R, as a new python user I'd love to see how this could be done. It seems that python would be better suited to this than R, and we don't have to worry about licenses as we would with Mathematica or Matlab.In an example like the one below obviously simply using stripes wouldn't work. It would be interesting if a simple rule based approach could be made to work for difficult examples such as this.I've added the [machine-learning] tag as I believe the correct answer will have to use ML techniques, such as the Restricted Boltzmann Machine (RBM) approach advocated by Gregory Klopper in the original thread. There is some RBM code available in python which might be a good place to start, but obviously training data is needed for that approach. At the 2009 IEEE International Workshop on MACHINE LEARNING FOR SIGNAL PROCESSING (MLSP 2009) they ran a Data Analysis Competition: Where's Wally?. Training data is provided in matlab format. Note that the links on that website are dead, but the data (along with the source of an approach taken by Sean McLoone and colleagues can be found here (see SCM link). Seems like one place to start.Here's an implementation with mahotasSplit into red, green, and blue channels. It's better to use floating point arithmetic below, so we convert at the top.w is the white channel.Build up a pattern of +1,+1,-1,-1 on the vertical axis. This is wally's shirt.Convolve with red minus white. This will give a strong response where the shirt is.Look for the maximum value and dilate it to make it visible. Now, we tone down the whole image, except the region or interest:And we get !You could try template matching, and then taking down which produced the highest resemblance, and then using machine learning to narrow it more. That is also very difficult, and with the accuracy of template matching, it may just return every face or face-like image. I am thinking you will need more than just machine learning if you hope to do this consistently.maybe you should start with breaking the problem into two smaller ones:those are still two very big problems to tackle...BTW, I would choose c++ and open CV, it seems much more suited for this.This is not impossible but very difficult because you really have no example of a successful match. There are often multiple states(in this case, more examples of find walleys drawings), you can then feed multiple pictures into an image reconization program and treat it as a hidden markov model and use something like the viterbi algorithm for inference ( http://en.wikipedia.org/wiki/Viterbi_algorithm ).Thats the way I would approach it, but assuming you have multiple images that you can give it examples of the correct answer so it can learn.  If you only have one picture, then I'm sorry there maybe another approach you need to take.I recognized that there are two main features which are almost always visible:So I would do it the following way:search for striped shirts:If there are more than one 'shirts', to say, more than one clusters of positive correlation, search for other features, like the dark brown hair:search for brown hairHere's a solution using neural networks that works nicely.The neural network is trained on several solved examples that are marked with bounding boxes indicating where Wally appears in the picture. The goal of the network is to minimize the error between the predicted box and the actual box from training/validation data.The network above uses Tensorflow Object Detection API to perform training and predictions.

How to convert Python's .isoformat() string back into datetime object [duplicate]

Alex Urcioli

[How to convert Python's .isoformat() string back into datetime object [duplicate]](https://stackoverflow.com/questions/28331512/how-to-convert-pythons-isoformat-string-back-into-datetime-object)

So in Python 3, you can generate an ISO 8601 date with .isoformat(), but you can't convert a string created by isoformat() back into a datetime object because Python's own datetime directives don't match properly. That is, %z = 0500 instead of 05:00 (which is produced by .isoformat()).For example:From Python's strptime documentation: (https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior)So, in short, Python does not even adhere to its own string formatting directives.I know datetime is already terrible in Python, but this really goes beyond unreasonable into the land of plain stupidity.Tell me this isn't true.

2015-02-04 21:10:11Z

So in Python 3, you can generate an ISO 8601 date with .isoformat(), but you can't convert a string created by isoformat() back into a datetime object because Python's own datetime directives don't match properly. That is, %z = 0500 instead of 05:00 (which is produced by .isoformat()).For example:From Python's strptime documentation: (https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior)So, in short, Python does not even adhere to its own string formatting directives.I know datetime is already terrible in Python, but this really goes beyond unreasonable into the land of plain stupidity.Tell me this isn't true.As it turns out, this is the current best "solution" to this question:Then...Try this:Usage:

Import python package from local directory into interpreter

projectshave

[Import python package from local directory into interpreter](https://stackoverflow.com/questions/1112618/import-python-package-from-local-directory-into-interpreter)

I'm developing/testing a package in my local directory. I want to import it in the interpreter (v2.5), but sys.path does not include the current directory. Right now I type in sys.path.insert(0,'.'). Is there a better way? Also, fails with this error:

2009-07-11 00:57:52Z

I'm developing/testing a package in my local directory. I want to import it in the interpreter (v2.5), but sys.path does not include the current directory. Right now I type in sys.path.insert(0,'.'). Is there a better way? Also, fails with this error:You can use relative imports only from in a module that was in turn imported as part of a package -- your script or interactive interpreter wasn't, so of course from . import (which means "import from the same package I got imported from") doesn't work. import mypackage will be fine once you ensure the parent directory of mypackage is in sys.path (how you managed to get your current directory away from sys.path I don't know -- do you have something strange in site.py, or...?)To get your current directory back into sys.path there is in fact no better way than putting it there;-).See the documentation for sys.path:http://docs.python.org/library/sys.html#sys.pathTo quote:So, there's no need to monkey with sys.path if you're starting the python interpreter from the directory containing your module.Also, to import your package, just do:Since the directory containing the package is already in sys.path, it should work fine.Keep it simple:If you want to run an unmodified python script so it imports libraries from a specific local directory you can set the PYTHONPATH environment variable - e.g. in bash:If you just want it to import from the current working directory use the . notation:A simple way to make it work is to run your script from the parent directory using python's -m flag, e.g. python -m packagename.scriptname. Obviously in this situation you need an __init__.py file to turn your directory into a package.Using sys.path should include current directory already.Try:or:however it may be not a good practice, so why not just use:Inside a package if there is setup.py, then better to install itA bit late to the party, but this is what worked for me:Apparently, if there is an empty string, Python knows that it should look in the current directory. I did not have the empty string in sys.path, which caused this error.

How does asyncio actually work?

wvxvw

[How does asyncio actually work?](https://stackoverflow.com/questions/49005651/how-does-asyncio-actually-work)

This question is motivated by my another question: How to await in cdef?There are tons of articles and blog posts on the web about asyncio, but they are all very superficial.  I couldn't find any information about how asyncio is actually implemented, and what makes I/O asynchronous.  I was trying to read the source code, but it's thousands of lines of not the highest grade C code, a lot of which deals with auxiliary objects, but most crucially, it is hard to connect between Python syntax and what C code it would translate into.Asycnio's own documentation is even less helpful.  There's no information there about how it works, only some guidelines about how to use it, which are also sometimes misleading / very poorly written.I'm familiar with Go's implementation of coroutines, and was kind of hoping that Python did the same thing.  If that was the case, the code I came up in the post linked above would have worked.  Since it didn't, I'm now trying to figure out why.  My best guess so far is as follows, please correct me where I'm wrong:In other words, here's my attempt at "desugaring" of some asyncio syntax into something more understandable:Should my guess prove correct: then I have a problem.  How does I/O actually happen in this scenario? In a separate thread?  Is the whole interpreter suspended and I/O happens outside the interpreter?  What exactly is meant by I/O?  If my python procedure called C open() procedure, and it in turn sent interrupt to kernel, relinquishing control to it, how does Python interpreter know about this and is able to continue running some other code, while kernel code does the actual I/O and until it wakes up the Python procedure which sent the interrupt originally?  How can Python interpreter in principle, be aware of this happening?

2018-02-27 09:48:11Z

This question is motivated by my another question: How to await in cdef?There are tons of articles and blog posts on the web about asyncio, but they are all very superficial.  I couldn't find any information about how asyncio is actually implemented, and what makes I/O asynchronous.  I was trying to read the source code, but it's thousands of lines of not the highest grade C code, a lot of which deals with auxiliary objects, but most crucially, it is hard to connect between Python syntax and what C code it would translate into.Asycnio's own documentation is even less helpful.  There's no information there about how it works, only some guidelines about how to use it, which are also sometimes misleading / very poorly written.I'm familiar with Go's implementation of coroutines, and was kind of hoping that Python did the same thing.  If that was the case, the code I came up in the post linked above would have worked.  Since it didn't, I'm now trying to figure out why.  My best guess so far is as follows, please correct me where I'm wrong:In other words, here's my attempt at "desugaring" of some asyncio syntax into something more understandable:Should my guess prove correct: then I have a problem.  How does I/O actually happen in this scenario? In a separate thread?  Is the whole interpreter suspended and I/O happens outside the interpreter?  What exactly is meant by I/O?  If my python procedure called C open() procedure, and it in turn sent interrupt to kernel, relinquishing control to it, how does Python interpreter know about this and is able to continue running some other code, while kernel code does the actual I/O and until it wakes up the Python procedure which sent the interrupt originally?  How can Python interpreter in principle, be aware of this happening?Before answering this question we need to understand a few base terms, skip these if you already know any of them.Generators are objects that allow us to suspend the execution of a python function. User curated generators are implement using the keyword yield. By creating a normal function containing the yield keyword, we turn that function into a generator:As you can see, calling next() on the generator causes the interpreter to load test's frame, and return the yielded value. Calling next() again, cause the frame to load again into the interpreter stack, and continue on yielding another value.By the third time next() is called, our generator was finished, and StopIteration was thrown.A less-known feature of generators, is the fact that you can communicate with them using two methods: send() and throw().Upon calling gen.send(), the value is passed as a return value from the yield keyword.gen.throw() on the other hand, allows throwing Exceptions inside generators, with the exception raised at the same spot yield was called.Returning a value from a generator, results in the value being put inside the StopIteration exception. We can later on recover the value from the exception and use it to our need.Python 3.4 came with the addition of a new keyword: yield from. What that keyword allows us to do, is pass on any next(), send() and throw() into an inner-most nested generator. If the inner generator returns a value, it is also the return value of yield from:Upon introducing the new keyword yield from in Python 3.4, we were now able to create generators inside generators that just like a tunnel, pass the data back and forth from the inner-most to the outer-most generators.  This has spawned a new meaning for generators - coroutines.Coroutines are functions that can be stopped and resumed while being run. In Python, they are defined using the async def keyword. Much like generators, they too use their own form of yield from which is await. Before async and await were introduced in Python 3.5, we created coroutines in the exact same way generators were created (with yield from instead of await).Like every iterator or generator that implement the __iter__() method, coroutines implement __await__() which allows them to continue on every time await coro is called.There's a nice sequence diagram inside the Python docs that you should check out.In asyncio, apart from coroutine functions, we have 2 important objects: tasks and futures.Futures are objects that have the __await__() method implemented, and their job is to hold a certain state and result. The state can be one of the following:The result, just like you have guessed, can either be a Python object, that will be returned, or an exception which may be raised.Another important feature of future objects, is that they contain a method called add_done_callback(). This method allows functions to be called as soon as the task is done - whether it raised an exception or finished.Task objects are special futures, which wrap around coroutines, and communicate with the inner-most and outer-most coroutines. Every time a coroutine awaits a future, the future is passed all the way back to the task (just like in yield from), and the task receives it.Next, the task binds itself to the future. It does so by calling add_done_callback() on the future. From now on, if the future will ever be done, by either being cancelled, passed an exception or passed a Python object as a result, the task's callback will be called, and it will rise back up to existence.The final burning question we must answer is - how is the IO implemented?Deep inside asyncio, we have an event loop. An event loop of tasks. The event loop's job is to call tasks every time they are ready and coordinate all that effort into one single working machine.The IO part of the event loop is built upon a single crucial function called select. Select is a blocking function, implemented by the operating system underneath, that allows waiting on sockets for incoming or outgoing data. Upon data being received it wakes up, and returns the sockets which received data, or the sockets whom are ready for writing.When you try to receive or send data over a socket through asyncio, what actually happens below is that the socket is first checked if it has any data that can be immediately read or sent. If it's .send() buffer is full, or the .recv() buffer is empty, the socket is registered to the select function (by simply adding it to one of the lists, rlist for recv and wlist for send) and the appropriate function awaits a newly created future object, tied to that socket.When all available tasks are waiting for futures, the event loop calls select and waits. When the one of the sockets has incoming data, or it's send buffer drained up, asyncio checks for the future object tied to that socket, and sets it to done.Now all the magic happens. The future is set to done, the task that added itself before with add_done_callback() rises up back to life, and calls .send() on the coroutine which resumes the inner-most coroutine (because of the await chain) and you read the newly received data from a nearby buffer it was spilled unto.Method chain again, in case of recv():In summary, asyncio uses generator capabilities, that allow pausing and resuming functions. It uses yield from capabilities that allow passing data back and forth from the inner-most generator to the outer-most. It uses all of those in order to halt function execution while it's waiting for IO to complete (by using the OS select function).And the best of all? While one function is paused, another may run and interleave with the delicate fabric, which is asyncio.Talking about async/await and asyncio is not the same thing. The first is a fundamental, low-level construct (coroutines) while the later is a library using these constructs. Conversely, there is no single ultimate answer.The following is a general description of how async/await and asyncio-like libraries work. That is, there may be other tricks on top (there are...) but they are inconsequential unless you build them yourself. The difference should be negligible unless you already know enough to not have to ask such a question.Just like subroutines (functions, procedures, ...), coroutines (generators, ...) are an abstraction of call stack and instruction pointer: there is a stack of executing code pieces, and each is at a specific instruction.The distinction of def versus async def is merely for clarity. The actual difference is return versus yield. From this, await or yield from take the difference from individual calls to entire stacks.A subroutine represents a new stack level to hold local variables, and a single traversal of its instructions to reach an end. Consider a subroutine like this:When you run it, that meansNotably, 4. means that a subroutine always starts at the same state. Everything exclusive to the function itself is lost upon completion. A function cannot be resumed, even if there are instructions after return.A coroutine is like a subroutine, but can exit without destroying its state. Consider a coroutine like this:When you run it, that meansNote the addition of 2.1 and 2.2 - a coroutine can be suspended and resumed at predefined points. This is similar to how a subroutine is suspended during calling another subroutine. The difference is that the active coroutine is not strictly bound to its calling stack. Instead, a suspended coroutine is part of a separate, isolated stack.This means that suspended coroutines can be freely stored or moved between stacks. Any call stack that has access to a coroutine can decide to resume it.So far, our coroutine only goes down the call stack with yield. A subroutine can go down and up the call stack with return and (). For completeness, coroutines also need a mechanism to go up the call stack. Consider a coroutine like this:When you run it, that means it still allocates the stack and instruction pointer like a subroutine. When it suspends, that still is like storing a subroutine.However, yield from does both. It suspends stack and instruction pointer of wrap and runs cofoo. Note that wrap stays suspended until cofoo finishes completely. Whenever cofoo suspends or something is sent, cofoo is directly connected to the calling stack.As established, yield from allows to connect two scopes across another intermediate one. When applied recursively, that means the top of the stack can be connected to the bottom of the stack.Note that root and coro_b do not know about each other. This makes coroutines much cleaner than callbacks: coroutines still built on a 1:1 relation like subroutines. Coroutines suspend and resume their entire existing execution stack up until a regular call point.Notably, root could have an arbitrary number of coroutines to resume. Yet, it can never resume more than one at the same time. Coroutines of the same root are concurrent but not parallel!The explanation has so far explicitly used the yield and yield from vocabulary of generators - the underlying functionality is the same. The new Python3.5 syntax async and await exists mainly for clarity.The async for and async with statements are needed because you would break the yield from/await chain with the bare for and with statements.By itself, a coroutine has no concept of yielding control to another coroutine. It can only yield control to the caller at the bottom of a coroutine stack. This caller can then switch to another coroutine and run it.This root node of several coroutines is commonly an event loop: on suspension, a coroutine yields an event on which it wants resume. In turn, the event loop is capable of efficiently waiting for these events to occur. This allows it to decide which coroutine to run next, or how to wait before resuming.Such a design implies that there is a set of pre-defined events that the loop understands. Several coroutines await each other, until finally an event is awaited. This event can communicate directly with the event loop by yielding control.The key is that coroutine suspension allows the event loop and events to directly communicate. The intermediate coroutine stack does not require any knowledge about which loop is running it, nor how events work.The simplest event to handle is reaching a point in time. This is a fundamental block of threaded code as well: a thread repeatedly sleeps until a condition is true.

However, a regular sleep blocks execution by itself - we want other coroutines to not be blocked. Instead, we want tell the event loop when it should resume the current coroutine stack.An event is simply a value we can identify - be it via an enum, a type or other identity. We can define this with a simple class that stores our target time. In addition to storing the event information, we can allow to await a class directly.This class only stores the event - it does not say how to actually handle it.The only special feature is __await__ - it is what the await keyword looks for. Practically, it is an iterator but not available for the regular iteration machinery.Now that we have an event, how do coroutines react to it? We should be able to express the equivalent of sleep by awaiting our event. To better see what is going on, we wait twice for half the time:We can directly instantiate and run this coroutine. Similar to a generator, using coroutine.send runs the coroutine until it yields a result.This gives us two AsyncSleep events and then a StopIteration when the coroutine is done. Notice that the only delay is from time.sleep in the loop! Each AsyncSleep only stores an offset from the current time.At this point, we have two separate mechanisms at our disposal:Notably, these two are orthogonal: neither one affects or triggers the other. As a result, we can come up with our own strategy to sleep to meet the delay of an AsyncSleep.If we have several coroutines, each can tell us when it wants to be woken up. We can then wait until the first of them wants to be resumed, then for the one after, and so on. Notably, at each point we only care about which one is next.This makes for a straightforward scheduling:A trivial implementation does not need any advanced concepts. A list allows to sort coroutines by date. Waiting is a regular time.sleep. Running coroutines works just like before with coroutine.send.Of course, this has ample room for improvement. We can use a heap for the wait queue or a dispatch table for events. We could also fetch return values from the StopIteration and assign them to the coroutine. However, the fundamental principle remains the same.The AsyncSleep event and run event loop are a fully working implementation of timed events.This cooperatively switches between each of the five coroutines, suspending each for 0.1 seconds. Even though the event loop is synchronous, it still executes the work in 0.5 seconds instead of 2.5 seconds. Each coroutine holds state and acts independently.An event loop that supports sleep is suitable for polling. However, waiting for I/O on a file handle can be done more efficiently: the operating system implements I/O and thus knows which handles are ready. Ideally, an event loop should support an explicit "ready for I/O" event.Python already has an interface to query the OS for read I/O handles. When called with handles to read or write, it returns the handles ready to read or write:For example, we can open a file for writing and wait for it to be ready:Once select returns, writeable contains our open file.Similar to the AsyncSleep request, we need to define an event for I/O. With the underlying select logic, the event must refer to a readable object - say an open file. In addition, we store how much data to read.As with AsyncSleep we mostly just store the data required for the underlying system call. This time, __await__ is capable of being resumed multiple times - until our desired amount has been read. In addition, we return the I/O result instead of just resuming.The basis for our event loop is still the run defined previously. First, we need to track the read requests. This is no longer a sorted schedule, we only map read requests to coroutines.Since select.select takes a timeout parameter, we can use it in place of time.sleep.This gives us all readable files - if there are any, we run the corresponding coroutine. If there are none, we have waited long enough for our current coroutine to run.Finally, we have to actually listen for read requests.The above was a bit of a simplification. We need to do some switching to not starve sleeping coroutines if we can always read. We need to handle having nothing to read or nothing to wait for. However, the end result still fits into 30 LOC.The AsyncSleep, AsyncRead and run implementations are now fully functional to sleep and/or read.

Same as for sleepy, we can define a helper to test reading:Running this, we can see that our I/O is interleaved with the waiting task:While I/O on files gets the concept across, it is not really suitable for a library like asyncio: the select call always returns for files, and both open and read may block indefinitely. This blocks all coroutines of an event loop - which is bad. Libraries like aiofiles use threads and synchronization to fake non-blocking I/O and events on file.However, sockets do allow for non-blocking I/O - and their inherent latency makes it much more critical. When used in an event loop, waiting for data and retrying can be wrapped without blocking anything.Similar to our AsyncRead, we can define a suspend-and-read event for sockets. Instead of taking a file, we take a socket - which must be non-blocking. Also, our __await__ uses socket.recv instead of file.read.In contrast to AsyncRead, __await__ performs truly non-blocking I/O. When data is available, it always reads. When no data is available, it always suspends. That means the event loop is only blocked while we perform useful work.As far as the event loop is concerned, nothing changes much. The event to listen for is still the same as for files - a file descriptor marked ready by select.At this point, it should be obvious that AsyncRead and AsyncRecv are the same kind of event. We could easily refactor them to be one event with an exchangeable I/O component. In effect, the event loop, coroutines and events cleanly separate a scheduler, arbitrary intermediate code and the actual I/O.In principle, what you should do at this point is replicate the logic of read as a recv for AsyncRecv. However, this is much more ugly now - you have to handle early returns when functions block inside the kernel, but yield control to you. For example, opening a connection versus opening a file is much longer:Long story short, what remains is a few dozen lines of Exception handling. The events and event loop already work at this point.Example code at githubYour coro desugaring is conceptually correct, but slightly incomplete.await doesn't suspend unconditionally, but only if it encounters a blocking call. How does it know that a call is blocking? This is decided by the code being awaited. For example, an awaitable implementation of socket read could be desugared to:In real asyncio the equivalent code modifies the state of a Future instead of returning magic values, but the concept is the same. When appropriately adapted to a generator-like object, the above code can be awaited.On the caller side, when your coroutine contains:It desugars into something close to:People familiar with generators tend to describe the above in terms of yield from which does the suspension automatically.The suspension chain continues all the way up to the event loop, which notices that the coroutine is suspended, removes it from the runnable set, and goes on to execute coroutines that are runnable, if any. If no coroutines are runnable, the loop waits in select() until either a file descriptor a coroutine is interested in becomes ready for IO. (The event loop maintains a file-descriptor-to-coroutine mapping.)In the above example, once select() tells the event loop that sock is readable, it will re-add coro to the runnable set, so it will be continued from the point of suspension.In other words:For insight on coroutine-driving event loops, I recommend this talk by Dave Beazley, where he demonstrates coding an event loop from scratch in front of live audience.It all boils down to the two main challenges that asyncio is addressing:The answer to the first point has been around for a long while and is called a select loop. In python, it is implemented in the selectors module.The second question is related to the concept of coroutine, i.e. functions that can stop their execution and be restored later on. In python, coroutines are implemented using generators and the yield from statement. That's what is hiding behind the async/await syntax.More resources in this answer.EDIT: Addressing your comment about goroutines:The closest equivalent to a goroutine in asyncio is actually not a coroutine but a task (see the difference in the documentation). In python, a coroutine (or a generator) knows nothing about the concepts of event loop or I/O. It simply is a function that can stop  its execution using yield while keeping its current state, so it can be restored later on. The yield from syntax allows for chaining them in a transparent way.Now, within an asyncio task, the coroutine at the very bottom of the chain always ends up yielding a future. This future then bubbles up to the event loop, and gets integrated into the inner machinery. When the future is set to done by some other inner callback, the event loop can restore the task by sending the future back into the coroutine chain.EDIT: Addressing some of the questions in your post:No, nothing happens in a thread. I/O is always managed by the event loop, mostly through file descriptors. However the registration of those file descriptors is usually hidden by high-level coroutines, making the dirty work for you.An I/O is any blocking call. In asyncio, all the I/O operations should go through the event loop, because as you said, the event loop has no way to be aware that a blocking call is being performed in some synchronous code. That means you're not supposed to use a synchronous open within the context of a coroutine. Instead, use a dedicated library such aiofiles which provides an asynchronous version of open.

How to maximize a plt.show() window using Python

Santiago Lovera

[How to maximize a plt.show() window using Python](https://stackoverflow.com/questions/12439588/how-to-maximize-a-plt-show-window-using-python)

Just for curiosity I would like to know how to do this in the code below. I have been searching for an answer but is useless.

2012-09-15 17:31:02Z

Just for curiosity I would like to know how to do this in the code below. I have been searching for an answer but is useless.I usually usebefore the call to plt.show(), and I get a maximized window. This works for the 'wx' backend only.EDIT:for Qt4Agg backend, see kwerenda's answer.since I am on zero reputation I can leave no other mark than a new answer

I am on a Windows (WIN7), running Python 2.7.5 & Matplotlib 1.3.1I was able to maximize Figure windows for TkAgg, QT4Agg, and wxAgg using the following lines:Hope this summary of the previous answers (and some additions) combined in a working example (at least for windows) helps.

CheersWith Qt backend (FigureManagerQT) proper command is:This makes the window take up the full screen for me, under Ubuntu 12.04 with the TkAgg backend:For me nothing of the above worked. I use the Tk backend on Ubuntu 14.04 which contains matplotlib 1.3.1.The following code creates a fullscreen plot window which is not the same as maximizing but it serves my purpose nicely:This should work (at least with TkAgg):(adopted from the above and Using Tkinter, is there a way to get the usable screen size without visibly zooming a window?)I get mng.frame.Maximize(True) AttributeError: FigureManagerTkAgg instance has no attribute 'frame' as well.Then I looked through the attributes mng has, and I found this: That worked for me.So for people who have the same trouble, you may try this.By the way, my Matplotlib version is 1.3.1.This is kind of hacky and probably not portable, only use it if you're looking for quick and dirty. If I just set the figure much bigger than the screen, it takes exactly the whole screen.In fact, in Ubuntu 16.04 with Qt4Agg, it maximizes the window (not full-screen) if it's bigger than the screen. (If you have two monitors, it just maximizes it on one of them).I found this for full screen mode on UbuntuMy best effort so far, supporting different backends:Pressing the f key (or ctrl+f in 1.2rc1) when focussed on a plot will fullscreen a plot window. Not quite maximising, but perhaps better.Other than that, to actually maximize, you will need to use GUI Toolkit specific commands (if they exist for your specific backend).HTHTry plt.figure(figsize=(6*3.13,4*3.13)) to make the plot larger.In my versions (Python 3.6, Eclipse, Windows 7), snippets given above didn't work, but with hints given by Eclipse/pydev (after typing: mng.), I found:It seems that using mng-commands is ok only for local development...The one solution that worked on Win 10 flawlessly.Try using 'Figure.set_size_inches' method, with the extra keyword argument forward=True. According to the documentation, this should resize the figure window.Whether that actually happens will depend on the operating system you are using.Ok so this is what worked for me. I did the whole showMaximize() option and it does resize your window in proportion to the size of the figure, but it does not expand and 'fit' the canvas. I solved this by:This doesn't necessarily maximize your window, but it does resize your window in proportion to the size of the figure:This might also help: http://matplotlib.1069221.n5.nabble.com/Resizing-figure-windows-td11424.htmlThe following may work with all the backends, but I tested it only on QT:Here is a function based on @Pythonio's answer. I encapsulate it into a function that automatically detects which backend is it using and do the corresponding actions.

Matplotlib discrete colorbar

bph

[Matplotlib discrete colorbar](https://stackoverflow.com/questions/14777066/matplotlib-discrete-colorbar)

I am trying to make a discrete colorbar for a scatterplot in matplotlibI have my x, y data and for each point an integer tag value which I want to be represented with a unique colour, e.g.typically tag will be an integer ranging from 0-20, but the exact range may changeso far I have just used the default settings, e.g.which gives a continuous range of colours. Ideally i would like a set of n discrete colours (n=20 in this example). Even better would be to get a tag value of 0 to produce a gray colour and 1-20 be colourful.I have found some 'cookbook' scripts but they are very complicated and I cannot think they are the right way to solve a seemingly simple problem

2013-02-08 16:29:47Z

I am trying to make a discrete colorbar for a scatterplot in matplotlibI have my x, y data and for each point an integer tag value which I want to be represented with a unique colour, e.g.typically tag will be an integer ranging from 0-20, but the exact range may changeso far I have just used the default settings, e.g.which gives a continuous range of colours. Ideally i would like a set of n discrete colours (n=20 in this example). Even better would be to get a tag value of 0 to produce a gray colour and 1-20 be colourful.I have found some 'cookbook' scripts but they are very complicated and I cannot think they are the right way to solve a seemingly simple problemYou can create a custom discrete colorbar quite easily by using a BoundaryNorm as normalizer for your scatter. The quirky bit (in my method) is making 0 showup as grey. For images i often use the cmap.set_bad() and convert my data to a numpy masked array. That would be much easier to make 0 grey, but i couldnt get this to work with the scatter or the custom cmap. As an alternative you can make your own cmap from scratch, or read-out an existing one and override just some specific entries.I personally think that with 20 different colors its a bit hard to read the specific value, but thats up to you of course.You could follow this example:which produces the following image:To set a values above or below the range of the colormap, you'll want to use the set_over and set_under methods of the colormap.  If you want to flag a particular value, mask it (i.e. create a masked array), and use the set_bad method.  (Have a look at the documentation for the base colormap class: http://matplotlib.org/api/colors_api.html#matplotlib.colors.Colormap )It sounds like you want something like this:The above answers are good, except they don't have proper tick placement on the colorbar. I like having the ticks in the middle of the color so that the number -> color mapping is more clear. You can solve this problem by changing the limits of the matshow call:I have been investigating these ideas and here is my five cents worth. It avoids calling BoundaryNorm as well as specifying norm as an argument to scatter and colorbar. However I have found no way of eliminating the rather long-winded call to matplotlib.colors.LinearSegmentedColormap.from_list.Some background is that matplotlib provides so-called qualitative colormaps, intended to use with discrete data. Set1, e.g., has 9 easily distinguishable colors, and tab20 could be used for 20 colors. With these maps it could be natural to use their first n colors to color scatter plots with n categories, as the following example does. The example also produces a colorbar with n discrete colors approprately labelled.which produces the image below. The n in the call to Set1 specifies

the first n colors of that colormap, and the last n in the call to from_list

specifies to construct a map with n colors (the default being 256). In order to set cm as the default colormap with plt.set_cmap, I found it to be necessary to give it a name and register it, viz:I think you'd want to look at colors.ListedColormap to generate your colormap, or if you just need a static colormap I've been working on an app that might help.

How do I create a multiline Python string with inline variables?

evolution

[How do I create a multiline Python string with inline variables?](https://stackoverflow.com/questions/10112614/how-do-i-create-a-multiline-python-string-with-inline-variables)

I am looking for a clean way to use variables within a multiline Python string. Say I wanted to do the following:I'm looking to see if there is something similar to $ in Perl to indicate a variable in the Python syntax.If not - what is the cleanest way to create a multiline string with variables?

2012-04-11 19:28:04Z

I am looking for a clean way to use variables within a multiline Python string. Say I wanted to do the following:I'm looking to see if there is something similar to $ in Perl to indicate a variable in the Python syntax.If not - what is the cleanest way to create a multiline string with variables?The common way is the format() function:It works fine with a multi-line format string:You can also pass a dictionary with variables:The closest thing to what you asked (in terms of syntax) are template strings. For example:I should add though that the format() function is more common because it's readily available and it does not require an import line.NOTE: The recommended way to do string formatting in Python is to use format(), as outlined in the accepted answer. I'm preserving this answer as an example of the C-style syntax that's also supported.Some reading:You can use Python 3.6's f-strings for variables inside multi-line or lengthy single-line strings. You can manually specify newline characters using \n.Alternatively, you can also create a multiline f-string with triple quotes.This is what you want:A dictionary can be passed to format(), each key name will become a variable for each associated value.

Also a list can be passed to format(), the index number of each value will be used as variables in this case.

Both solutions above will output the same:

numpy replace negative values in array

bph

[numpy replace negative values in array](https://stackoverflow.com/questions/10335090/numpy-replace-negative-values-in-array)

Can anyone advise a simple way of replacing all negative values in an array with 0?I'm having a complete block on how to do it using a numpy arraye.g.i need to returna < 0 gives:This is where I'm stuck - how to use this array to modify the original array

2012-04-26 14:03:09Z

Can anyone advise a simple way of replacing all negative values in an array with 0?I'm having a complete block on how to do it using a numpy arraye.g.i need to returna < 0 gives:This is where I'm stuck - how to use this array to modify the original arrayYou are halfway there. Try:Try numpy.clip:You can clip only the bottom half with clip(0). You can clip only the top half with clip(max=n). (This is much better than my previous suggestion, which involved passing NaN to the first parameter and using out to coerce the type.):Another interesting approach is to use where:Finally, consider aix's answer. I prefer clip for simple operations because it's self-documenting, but his answer is preferable for more complex operations.Another minimalist Python solution without using numpy:No need to define any extra functions. yields:And yet another possibility:Here's a way to do it in Python without numpy. Create a function that returns what you want and use a list comprehension, or the map function.

Why does random.shuffle return None?

alvas

[Why does random.shuffle return None?](https://stackoverflow.com/questions/17649875/why-does-random-shuffle-return-none)

Why is random.shuffle returning None in Python?How do I get the shuffled value instead of None?

2013-07-15 08:33:41Z

Why is random.shuffle returning None in Python?How do I get the shuffled value instead of None?random.shuffle() changes the x list in place.Python API methods that alter a structure in-place generally return None, not the modified data structure.If you wanted to create a new randomly-shuffled list based of an existing one, where the existing list is kept in order, you could use random.sample() with the full length of the input:You could also use sorted() with random.random() for a sorting key:but this invokes sorting (an O(NlogN) operation), while sampling to the input length only takes O(N) operations (the same process as random.shuffle() is used, swapping out random values from a shrinking pool).Demo:This method works too.According to docs:shuffle modifies the list in place.

This is nice, because copying a large list would be pure overhead if you do not need the original list anymore.According to the "explicit is better than implicit" principle of pythonic style, returning the list would be a bad idea, because then one might think it is a new one although in fact it is not.If you do need a fresh list, you will have to write something likewhich is nicely explicit.

If you need this idiom frequently, wrap it in a function shuffled (see sorted) that returns new_x.As pointed out random.shuffle replaces in place, so you wouldn't need a new list variable.Python APIs which change the structure in place itself returns None as output.Output: [1, 2, 3, 4, 5, 6, 7, 8]Output: NoneOutput: [7, 3, 2, 4, 5, 6, 1, 8]I had my aha moment with this concept like this:does not return any values. Instead, that function shuffle the variable itself.So don't tryinstead just print the variable like this.You can return the shuffled list using random.sample() as explained by  others. It works by sampling k elements from the list without replacement.

So if there are duplicate elements in your list, they will be treated uniquely.

Efficient date range overlap calculation in python?

Andreas Jung

[Efficient date range overlap calculation in python?](https://stackoverflow.com/questions/9044084/efficient-date-range-overlap-calculation-in-python)

I have two date ranges where each range is determined by a start and end date (obviously, datetime.date() instances). The two ranges can overlap or not. I need the number of days of the overlap. Of course I can pre-fill two sets with all dates within both ranges and the perform a set intersection but this is possibly inefficient...is there a better way apart from another solution using a long if-elif section covering all cases ?

2012-01-28 09:01:23Z

I have two date ranges where each range is determined by a start and end date (obviously, datetime.date() instances). The two ranges can overlap or not. I need the number of days of the overlap. Of course I can pre-fill two sets with all dates within both ranges and the perform a set intersection but this is possibly inefficient...is there a better way apart from another solution using a long if-elif section covering all cases ?Here is an example calculation:Function calls are more expensive than arithmetic operations.The fastest way of doing this involves 2 subtractions and 1 min():compared with the next best which needs 1 subtraction, 1 min() and a max():Of course with both expressions you still need to check for a positive overlap.I implemented a TimeRange class as you can see below.The get_overlapped_range first negates all the non overlapped options by a simple condition, and then calculate the overlapped range by considering all the possible options.To get the amount of days you'll need to take the TimeRange value that was returned from get_overlapped_range and divide the duration by 60*60*24.Pseudocode:Ok my solution is a bit wonky because my df uses all series - but lets say you have the following columns, 2 of which are fixed which is your "Fiscal Year".  PoP is "Period of performance" which is your variable data:Assume all of the data is in datetime format ie -Try the following equations to find the number of days overlap:You can use the datetimerange package: https://pypi.org/project/DateTimeRange/"2015-01-01T00:00:00+0900" inside the DateTimeRange() can also be datetime format, like Timestamp('2017-08-30 20:36:25').

How to do Xavier initialization on TensorFlow

Alejandro

[How to do Xavier initialization on TensorFlow](https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow)

I'm porting my Caffe network over to TensorFlow but it doesn't seem to have xavier initialization. I'm using truncated_normal but this seems to be making it a lot harder to train.

2015-11-10 22:07:54Z

I'm porting my Caffe network over to TensorFlow but it doesn't seem to have xavier initialization. I'm using truncated_normal but this seems to be making it a lot harder to train.Since version 0.8 there is a Xavier initializer, see here for the docs.You can use something like this:Just to add another example on how to define a tf.Variable initialized using Xavier and Yoshua's method:This prevented me from having nan values on my loss function due to numerical instabilities when using multiple layers with RELUs.@Aleph7, Xavier/Glorot initialization depends the number of incoming connections (fan_in), number outgoing connections (fan_out), and kind of activation function (sigmoid or tanh) of the neuron. See this: http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdfSo now, to your question. This is how I would do it in TensorFlow:Note that we should be sampling from a uniform distribution, and not the normal distribution as suggested in the other answer.Incidentally, I wrote a post yesterday for something different using TensorFlow that happens to also use Xavier initialization. If you're interested, there's also a python notebook with an end-to-end example: https://github.com/delip/blog-stuff/blob/master/tensorflow_ufp.ipynb A nice wrapper around tensorflow called prettytensor gives an implementation in the source code (copied directly from here):TF-contrib has xavier_initializer. Here is an example how to use it:In addition to this, tensorflow has other initializers:In Tensorflow 2.0 and further both tf.contrib.* and tf.get_variable() are deprecated. In order to do Xavier initialization you now have to switch to:Glorot uniform and Xavier uniform are two different names of the same initialization type. If you want to know more about how to use initializations in TF2.0 with or without Keras refer to documentation.I looked and I couldn't find anything built in. However, according to this:http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initializationXavier initialization is just sampling a (usually Gaussian) distribution where the variance is a function of the number of neurons.  tf.random_normal can do that for you, you just need to compute the stddev (i.e. the number of neurons being represented by the weight matrix you're trying to initialize).Via the kernel_initializer parameter to tf.layers.conv2d, tf.layers.conv2d_transpose, tf.layers.Dense  etce.g.https://www.tensorflow.org/api_docs/python/tf/layers/conv2dhttps://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transposehttps://www.tensorflow.org/api_docs/python/tf/layers/DenseJust in case you want to use one line as you do with:You can do:

Make more than one chart in same IPython Notebook cell

WebOrCode

[Make more than one chart in same IPython Notebook cell](https://stackoverflow.com/questions/16392921/make-more-than-one-chart-in-same-ipython-notebook-cell)

I have started my IPython Notebook with This is my code in one cellThis is working fine, it will draw two lines, but on the same chart.I would like to draw each line on a separate chart.

And it would be great if the charts would be next to each other, not one after the other.I know that I can put the second line in the next cell, and then I would get two charts. But I would like the charts close to each other, because they represent the same logical unit.

2013-05-06 06:10:36Z

I have started my IPython Notebook with This is my code in one cellThis is working fine, it will draw two lines, but on the same chart.I would like to draw each line on a separate chart.

And it would be great if the charts would be next to each other, not one after the other.I know that I can put the second line in the next cell, and then I would get two charts. But I would like the charts close to each other, because they represent the same logical unit.Make the multiple axes first and pass them to the Pandas plot function, like:It still gives you 1 figure, but with two different plots next to each other.You can also call the show() function after each plot. 

e.gAnother way, for variety. Although this is somewhat less flexible than the others. Unfortunately, the graphs appear one above the other, rather than side-by-side, which you did request in your original question. But it is very concise.If the dataframe has more than the two series, and you only want to plot those two, you'll need to replace df with df[['korisnika','osiguranika']].Something like this:Note that this will also work if you are using the seaborn package for plotting:I don't know if this is new functionality, but this will plot on separate figures:while this will plot on the same figure: (just like the code in the op)I found this question because I was using the former method and wanted them to plot on the same figure, so your question was actually my answer.

memory-efficient built-in SqlAlchemy iterator/generator?

Paul

[memory-efficient built-in SqlAlchemy iterator/generator?](https://stackoverflow.com/questions/7389759/memory-efficient-built-in-sqlalchemy-iterator-generator)

I have a ~10M record MySQL table that I interface with using SqlAlchemy.  I have found that queries on large subsets of this table will consume too much memory even though I thought I was using a built-in generator that intelligently fetched bite-sized chunks of the dataset:To avoid this, I find I have to build my own iterator that bites off in chunks:Is this normal or is there something I'm missing regarding SA built-in generators?The answer to this question seems to indicate that the memory consumption is not to be expected.

2011-09-12 14:50:36Z

I have a ~10M record MySQL table that I interface with using SqlAlchemy.  I have found that queries on large subsets of this table will consume too much memory even though I thought I was using a built-in generator that intelligently fetched bite-sized chunks of the dataset:To avoid this, I find I have to build my own iterator that bites off in chunks:Is this normal or is there something I'm missing regarding SA built-in generators?The answer to this question seems to indicate that the memory consumption is not to be expected.Most DBAPI implementations fully buffer rows as they are fetched - so usually, before the SQLAlchemy ORM even gets a hold of one result, the whole result set is in memory.But then, the way Query works is that it fully loads the given result set by default before returning to you your objects.  The rationale here regards queries that are more than simple SELECT statements. For example, in joins to other tables that may return the same object identity multiple times in one result set (common with eager loading), the full set of rows needs to be in memory so that the correct results can be returned otherwise collections and such might be only partially populated.So Query offers an option to change this behavior through yield_per().  This call will cause the Query to yield rows in batches, where you give it the batch size.  As the docs state, this is only appropriate if you aren't doing any kind of eager loading of collections so it's basically if you really know what you're doing.   Also, if the underlying DBAPI pre-buffers rows, there will still be that memory overhead so the approach only scales slightly better than not using it.I hardly ever use yield_per(); instead, I use a better version of the LIMIT approach you suggest above using window functions.   LIMIT and OFFSET have a huge problem that very large OFFSET values cause the query to get slower and slower, as an OFFSET of N causes it to page through N rows - it's like doing the same query fifty times instead of one, each time reading a larger and larger number of rows.   With a window-function approach, I pre-fetch a set of "window" values that refer to chunks of the table I want to select.  I then emit individual SELECT statements that each pull from one of those windows at a time.The window function approach is on the wiki and I use it with great success.Also note: not all databases support window functions; you need Postgresql, Oracle, or SQL Server.  IMHO using at least Postgresql is definitely worth it - if you're using a relational database, you might as well use the best.I am not a database expert, but when using SQLAlchemy as a simple Python abstraction layer (ie, not using the ORM Query object) I've came up with a satisfying solution to query a 300M-row table without exploding memory usage...Here is a dummy example:Then, I use the SQLAlchemy fetchmany() method to iterate  over the results in a infinite while loop:This method allowed me to do all kind of data aggregation without any dangerous memory overhead. NOTE the stream_results works with Postgres and the pyscopg2 adapter, but I guess it won't work with any DBAPI, nor with any database driver...There is an interesting usecase in this blog post that inspired my above method.I've been looking into efficient traversal/paging with SQLAlchemy and would like to update this answer.I think you can use the slice call to properly limit the scope of a query and you could efficiently reuse it.Example:In the spirit of Joel's answer, I use the following:Using LIMIT/OFFSET is bad, because you need to find all {OFFSET} columns before, so the larger is OFFSET - the longer request you get.

Using windowed query for me also gives bad results on large table with large amount of data (you wait first results for too long, that it's not good in my case for chunked web response).Best approach given here https://stackoverflow.com/a/27169302/450103. In my case I resolved problem simply using index on datetime field and fetching next query with datetime>=previous_datetime. Stupid, because I used that index in different cases before, but thought that for fetching all data windowed query would be better. In my case I was wrong.AFAIK, the first variant still gets all the tuples from the table (with one SQL query) but builds the ORM presentation for each entity when iterating. So it is more efficient than building a list of all entities before iterating but you still have to fetch all the (raw) data into memory.Thus, using LIMIT on huge tables sounds like a good idea to me.

Python: Maximum recursion depth exceeded

add-semi-colons

[Python: Maximum recursion depth exceeded](https://stackoverflow.com/questions/8177073/python-maximum-recursion-depth-exceeded)

I have the following recursion code, at each node I call sql query to get the nodes belong to the parent node. here is the error: Method that I call to get sql results:I actually don't have any issue with the above method but I put it anyways to give proper overview of the question.Recursion Code:Calling the recursive function Code to print the dictionary, If the recursion is too deep I should be getting the error when I call my recursion function, but when I get this error when I print the dictionary.

2011-11-18 02:30:05Z

I have the following recursion code, at each node I call sql query to get the nodes belong to the parent node. here is the error: Method that I call to get sql results:I actually don't have any issue with the above method but I put it anyways to give proper overview of the question.Recursion Code:Calling the recursive function Code to print the dictionary, If the recursion is too deep I should be getting the error when I call my recursion function, but when I get this error when I print the dictionary.You can increment the stack depth allowed - with this, deeper recursive calls will be possible, like this:... But I'd advise you to first try to optimize your code, for instance, using iteration instead of recursion.

python: urllib2 how to send cookie with urlopen request

Oleg Tarasenko

[python: urllib2 how to send cookie with urlopen request](https://stackoverflow.com/questions/3334809/python-urllib2-how-to-send-cookie-with-urlopen-request)

I am trying to use  urllib2 to open url and to send specific cookie text to the server. E.g. I want to open site Solve chess problems, with a specific cookie, e.g. search=1. How do I do it?I am trying to do the following:Thanks in advance

2010-07-26 12:36:37Z

I am trying to use  urllib2 to open url and to send specific cookie text to the server. E.g. I want to open site Solve chess problems, with a specific cookie, e.g. search=1. How do I do it?I am trying to do the following:Thanks in advanceCookie is just another HTTP header.See urllib2 examples for other ways how to add HTTP headers to your request.There are more ways how to handle cookies. Some modules like cookielib try to behave like web browser - remember what cookies did you get previously and automatically send them again in following requests.Maybe using cookielib.CookieJar can help you. For instance when posting to a page containing a form:EDIT:After Piotr's comment I'll elaborate a bit. From the docs:So whatever requests you make with your CookieJar instance, all cookies will be handled automagically. Kinda like your browser does :)I can only speak from my own experience and my 99% use-case for cookies is to receive a cookie and then need to send it with all subsequent requests in that session.

The code above handles just that, and it does so transparently.You might want to take a look at the excellent HTTP Python library called Requests. It makes every task involving HTTP a bit easier than urllib2. From Cookies section of quickstart guide:Use cookielib. The linked doc page provides examples at the end. You'll also find a tutorial here.This answer is not working since the urllib2 module has been split across several modules in Python 3. 

You need to do 

How to stop flask application without using ctrl-c

vic

[How to stop flask application without using ctrl-c](https://stackoverflow.com/questions/15562446/how-to-stop-flask-application-without-using-ctrl-c)

I want to implement a command which can stop flask application by using flask-script.

I have searched the solution for a while. Because the framework doesn't provide "app.stop()" API, I am curious about how to code this. I am working on Ubuntu 12.10 and Python 2.7.3.

2013-03-22 03:55:25Z

I want to implement a command which can stop flask application by using flask-script.

I have searched the solution for a while. Because the framework doesn't provide "app.stop()" API, I am curious about how to code this. I am working on Ubuntu 12.10 and Python 2.7.3.If you are just running the server on your desktop, you can expose an endpoint to kill the server (read more at Shutdown The Simple Server):Here is another approach that is more contained:Let me know if this helps.I did it slightly different using threadsI use it to do end to end tests for restful api, where I can send requests using the python requests library.My method can be proceeded via bash terminal/console1) run and get the process number2a) kill the process2b) kill the process if above not workingThis is a bit old thread, but if someone experimenting, learning, or testing basic flask app, started from a script that runs in the background, the quickest way to stop it is to kill the process running on the port you are running your app on.

Note: I am aware the author is looking for a way not to kill or stop the app. But this may help someone who is learning.You'll get something like this.To stop the app, kill the processAs others have pointed out, you can only use werkzeug.server.shutdown from a request handler. The only way I've found to shut down the server at another time is to send a request to yourself. For example, the /kill handler in this snippet will kill the dev server unless another request comes in during the next second:This is an old question, but googling didn't give me any insight in how to accomplish this.Because I didn't read the code here properly! (Doh!)

What it does is to raise a RuntimeError when there is no werkzeug.server.shutdown in the request.environ...So what we can do when there is no request is to raise a RuntimeErrorand catch that when app.run() returns:No need to send yourself a request.You can use method bellowIf you're working on the CLI and only have one flask app/process running (or rather, you just want want to kill any flask process running on your system), you can kill it with: kill $(pgrep -f flask)You don't have to press "CTRL-C", but you can provide an endpoint which does it for you:Now you can just call this endpoint to gracefully shutdown the server:For Windows, it is quite easy to stop/kill flask server - 

Latest 'pip' fails with„Äårequires setuptools >= 0.8 for dist-info„Äç

orome

[Latest 'pip' fails with„Äårequires setuptools >= 0.8 for dist-info„Äç](https://stackoverflow.com/questions/20905350/latest-pip-fails-with-requires-setuptools-0-8-for-dist-info)

Using the recent (1.5) version of pip, I get an error when attempting to update several packages. For example, sudo pip install -U pytz results in failure with:I don't understand this message (I have setuptools 2.1) or what to do about it.Exception information from the log for this error:

2014-01-03 14:00:42Z

Using the recent (1.5) version of pip, I get an error when attempting to update several packages. For example, sudo pip install -U pytz results in failure with:I don't understand this message (I have setuptools 2.1) or what to do about it.Exception information from the log for this error:This worked for me:Note it's usage of sudoUPDATEOn window you just need to execute pip install setuptools --no-use-wheel --upgrade as an administrator. In unix/linux, sudo command is for elevating permissions.UPDATEThis appears to have been fixed in 1.5.1.First, you should never run 'sudo pip'. If possible you should use your system package manager because it uses GPG signatures to ensure you're not running malicious code. Otherwise, try upgrading setuptools:Alternatively, try:This is of course for "global" packages. You should ideally be using virtualenvs.

How to make Android app completely in python? [closed]

Ivo

[How to make Android app completely in python? [closed]](https://stackoverflow.com/questions/49955489/how-to-make-android-app-completely-in-python)

I would like to develop a (rather simple) android app to be distributed via Play Store. I would like to do so completely in python. However, the online research hasn't quite enlightened me: most comments are either outdated (>1 year old, and I feel there might be better integration of python since then) or they talk about running python in android (e.g. here).Therefore, I'm looking for information regarding the questions:I'm quite new to app development and would highly appreciate any leads of doing this in python rather than in Jave etc., which I don't know yet.Many thanks for you help in advance.

2018-04-21 11:54:26Z

I would like to develop a (rather simple) android app to be distributed via Play Store. I would like to do so completely in python. However, the online research hasn't quite enlightened me: most comments are either outdated (>1 year old, and I feel there might be better integration of python since then) or they talk about running python in android (e.g. here).Therefore, I'm looking for information regarding the questions:I'm quite new to app development and would highly appreciate any leads of doing this in python rather than in Jave etc., which I don't know yet.Many thanks for you help in advance.To answer your first question: yes it is feasible to develop an android application in pure python, in order to achieve this I suggest you use BeeWare, which is just a suite of python tools, that work together very well and they enable you to develop platform native applications in python.checkout this video by the creator of BeeWare that perfectly explains and demonstrates it's applicationAndroid's preferred language of implementation is Java - so if you want to write an Android application in Python, you need to have a way to run your Python code on a Java Virtual Machine. This is what VOC does. VOC is a transpiler - it takes Python source code, compiles it to CPython Bytecode, and then transpiles that bytecode into Java-compatible bytecode. The end result is that your Python source code files are compiled directly to a Java .class file, which can be packaged into an Android application.VOC also allows you to access native Java objects as if they were Python objects, implement Java interfaces with Python classes, and subclass Java classes with Python classes. Using this, you can write an Android application directly against the native Android APIs.Once you've written your native Android application, you can use Briefcase to package your Python code as an Android application. Briefcase is a tool for converting a Python project into a standalone native application. You can package projects for:You can check This native Android Tic Tac Toe app written in Python, using the BeeWare suite. on GitHubin addition to the BeeWare tools, you'll need to have a JDK and Android SDK installed to test run your application.and to answer your second question: a good environment can be anything you are comfortable with be it a text editor and a command line, or an IDE, if you're looking for a good python IDE I would suggest you try Pycharm, it has  a community edition which is free, and it has a similar environment as android studio, due to to the fact that were made by the same company.I hope this has been helpfulYou could try BeeWare - as described on their website:Gives you want you want now to write Android Apps in Python, plus has the advantage that you won't need to learn yet another framework in future if you end up also wanting to do something on one of the other listed platforms.Here's the Tutorial for Android Apps.There are two primary contenders for python apps on Androidhttps://chaquo.com/chaquopy/This integrates with the Android build system, it provides a Python API for all android features. To quote the site "The complete Android API and user interface toolkit are directly at your disposal."https://pybee.org/This provides a multi target transpiler, supports many targets such as Android and iOS. It uses a generic widget toolkit (toga) that maps to the host interface calls. Both are active projects and their github accounts shows a fair amount of recent activity. Beeware Toga like all widget libraries is good for getting the basics out to multiple platforms. If you have basic designs, and a desire to expand to other platforms this should work out well for you.On the other hand, Chaquopy is a much more precise in its mapping of the python API to Android. It also allows you to mix in Java, useful if you want to use existing code from other resources. If you have strict design targets, and predominantly want to target Android this is a much better resource.Android, Python !When I saw these two keywords together in your question, Kivy is the one which came to my mind first.Before coming to native Android development in Java using Android Studio, I had tried Kivy. It just awesome. Here are a few advantage I could find out.

Simple to useWith a python basics, you won't have trouble learning it.

Good communityIt's well documented and has a great, active community.

Cross platform.You can develop thing for Android, iOS, Windows, Linux and even Raspberry Pi with this single framework.

Open source.

It is a free softwareAt least few of it's (Cross platform) competitors want you to pay a fee if you want a commercial license.

Accelerated graphics supportKivy's graphics engine build over OpenGL ES 2 makes it suitable for softwares which require fast graphics rendering such as games.

Now coming into the next part of question, you can't use Android Studio IDE for Kivy. Here is a detailed guide for setting up the development environment.

Joining pairs of elements of a list - Python

John

[Joining pairs of elements of a list - Python](https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python)

I know that a list can be joined to make one long string as in:Obviously this would output:However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be: 

2011-05-01 20:03:22Z

I know that a list can be joined to make one long string as in:Obviously this would output:However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be: You can use slice notation with steps:Same logic applies for lists too. String lenght doesn't matter, because you're simply adding two strings together.Use an iterator.  List comprehension:Generator expression:Using map, str.__add__, iternext(iterator[, default]) is available starting in Python 2.6just to be pythonic :-)in case the you want to be alarmed if the list length is odd you can try:Best of LuckWithout building temporary lists:or:Well I would do it this way as I am no good with Regs..CODEoutput:  Hope this helps :)

Copy constructor in python?

Zitrax

[Copy constructor in python?](https://stackoverflow.com/questions/1241148/copy-constructor-in-python)

Is there a copy constructor in python ? If not what would I do to achieve something similar ?The situation is that I am using a library and I have extended one of the classes there with extra functionality and I want to be able to convert the objects I get from the library to instances of my own class.

2009-08-06 20:16:48Z

Is there a copy constructor in python ? If not what would I do to achieve something similar ?The situation is that I am using a library and I have extended one of the classes there with extra functionality and I want to be able to convert the objects I get from the library to instances of my own class.I think you want the copy moduleyou can control copying in much the same way as you control pickle.In python the copy constructor can be defined using default arguments.  Lets say you want the normal constructor to run the function non_copy_constructor(self) and the copy constructor should run copy_constructor(self, orig).  Then you can do the following:A simple example of my usual implementation of a copy constructor:For your situation, I would suggest writing a class method (or it could be a static method or a separate function) that takes as an argument an instance of the library's class and returns an instance of your class with all applicable attributes copied over.Building on @Godsmith's train of thought and addressing @Zitrax's need (I think) to do the data copy for all attributes within the constructor:This ConfusionMatrix class inherits a pandas.DataFrame and adds a ton of other attributes and methods that need to be recomputed unless the other matrix data can be copied over. Searching for a solution is how I found this question.I have a similar situation differing in that the new class only needs to copy attributes. Thus using @Dunham's idea and adding some specificity to @meisterluk's suggestion, @meisterluk's "copy_constructor" method could be:The output:

Sharing a result queue among several processes

alexis

[Sharing a result queue among several processes](https://stackoverflow.com/questions/9908781/sharing-a-result-queue-among-several-processes)

The documentation for the multiprocessing module shows how to pass a queue to a process started with multiprocessing.Process. But how can I share a queue with asynchronous worker processes started with apply_async? I don't need dynamic joining or anything else, just a way for the workers to (repeatedly) report their results back to base.This fails with: 

    RuntimeError: Queue objects should only be shared between processes through inheritance.

I understand what this means, and I understand the advice to inherit rather than require pickling/unpickling (and all the special Windows restrictions). But how do I pass the queue in a way that works? I can't find an example, and I've tried several alternatives that failed in various ways. Help please?

2012-03-28 13:42:59Z

The documentation for the multiprocessing module shows how to pass a queue to a process started with multiprocessing.Process. But how can I share a queue with asynchronous worker processes started with apply_async? I don't need dynamic joining or anything else, just a way for the workers to (repeatedly) report their results back to base.This fails with: 

    RuntimeError: Queue objects should only be shared between processes through inheritance.

I understand what this means, and I understand the advice to inherit rather than require pickling/unpickling (and all the special Windows restrictions). But how do I pass the queue in a way that works? I can't find an example, and I've tried several alternatives that failed in various ways. Help please?Try using multiprocessing.Manager to manage your queue and to also make it accessible to different workers.multiprocessing.Pool already has a shared result-queue, there is no need to additionally involve a Manager.Queue. Manager.Queue is a queue.Queue (multithreading-queue) under the hood, located on a separate server-process and exposed via proxies. This adds additional overhead compared to Pool's internal queue. Contrary to relying on Pool's native result-handling, the results in the Manager.Queue also are not guaranteed to be ordered.The worker processes are not started with .apply_async(), this already happens when you instantiate Pool. What is started

when you call pool.apply_async() is a new "job". Pool's worker-processes run the multiprocessing.pool.worker-function under the hood. This function takes care of processing new "tasks" transferred over Pool's internal Pool._inqueue and of sending results back to the parent over the Pool._outqueue. Your specified func will be executed within multiprocessing.pool.worker. func only has to return something and the result will be automatically send back to the parent..apply_async() immediately (asynchronously) returns a AsyncResult object (alias for ApplyResult). You need to call .get() (is blocking) on that object to receive the actual result. Another option would be to register a callback function, which gets fired as soon as the result becomes ready.Example Output:Note: Specifying the timeout-parameter for .get() will not stop the actual processing of the task within the worker, it only unblocks the waiting parent by raising a multiprocessing.TimeoutError.

Loading a trained Keras model and continue training

Wilmar van Ommeren

[Loading a trained Keras model and continue training](https://stackoverflow.com/questions/42666046/loading-a-trained-keras-model-and-continue-training)

I was wondering if it was possible to save a partly trained Keras model and continue the training after loading the model again.The reason for this is that I will have more training data in the future and I do not want to retrain the whole model again.The functions which I am using are:Edit 1: added fully working exampleWith the first dataset after 10 epochs the loss of the last epoch will be 0.0748 and the accuracy 0.9863.After saving, deleting and reloading the model the loss and accuracy of the model trained on the second dataset will be 0.1711 and 0.9504 respectively.Is this caused by the new training data or by a completely re-trained model?

2017-03-08 08:07:57Z

I was wondering if it was possible to save a partly trained Keras model and continue the training after loading the model again.The reason for this is that I will have more training data in the future and I do not want to retrain the whole model again.The functions which I am using are:Edit 1: added fully working exampleWith the first dataset after 10 epochs the loss of the last epoch will be 0.0748 and the accuracy 0.9863.After saving, deleting and reloading the model the loss and accuracy of the model trained on the second dataset will be 0.1711 and 0.9504 respectively.Is this caused by the new training data or by a completely re-trained model?Actually - model.save saves all information need for restarting training in your case. The only thing which could be spoiled by reloading model is your optimizer state. To check that - try to save and reload model and train it on training data.The problem might be that you use a different optimizer - or different arguments to your optimizer. I just had the same issue with a custom pretrained model, using for the pretrained model, whereby the original learning rate starts at 0.0003 and during pre-training it is reduced to the min_learning rate, which is 0.000003I just copied that line over to the script which uses the pre-trained model and got really bad accuracies. Until I noticed that the last learning rate of the pretrained model was the min learning rate, i.e. 0.000003. And if I start with that learning rate, I get exactly the same accuracies to start with as the output of the pretrained model - which makes sense, as starting with a learning rate that is 100 times bigger than the last learning rate used in the pretrained model will result in a huge overshoot of GD and hence in heavily decreased accuracies. Notice that Keras sometimes has issues with loaded models, as in here.

This might explain cases in which you don't start from the same trained accuracy.All above helps, you must resume from same learning rate() as the LR when the model and weights were saved. Set it directly on the optimizer. Note that improvement from there is not guaranteed, because the model may have reached the local minimum, which may be global. There is no point to resume a model in order to search for another local minimum, unless you intent to increase the learning rate in a controlled fashion and nudge the model into a possibly better minimum not far away.You might also be hitting Concept Drift, see Should you retrain a model when new observations are available. There's also the concept of catastrophic forgetting which a bunch of academic papers discuss.  Here's one with MNIST Empirical investigation of catastrophic forgetting

What‚Äôs the point of inheritance in Python?

Stefano Borini

[What‚Äôs the point of inheritance in Python?](https://stackoverflow.com/questions/1020453/what-s-the-point-of-inheritance-in-python)

Suppose you have the following situationAs you can see, makeSpeak is a routine that accepts a generic Animal object. In this case, Animal is quite similar to a Java interface, as it contains only a pure virtual method. makeSpeak does not know the nature of the Animal it gets passed. It just sends it the signal„Äåspeak„Äçand leaves the late binding to take care of which method to call: either Cat::speak() or Dog::speak(). This means that, as far as makeSpeak is concerned, the knowledge of which subclass is actually passed is irrelevant.But what about Python? Let‚Äôs see the code for the same case in Python. Please note that I try to be as similar as possible to the C++ case for a moment:Now, in this example you see the same strategy. You use inheritance to leverage the hierarchical concept of both Dogs and Cats being Animals. 

But in Python, there‚Äôs no need for this hierarchy. This works equally wellIn Python you can send the signal„Äåspeak„Äçto any object you want. If the object is able to deal with it, it will be executed, otherwise it will raise an exception. Suppose you add a class Airplane to both codes, and submit an Airplane object to makeSpeak. In the C++ case, it won‚Äôt compile, as Airplane is not a derived class of Animal. In the Python case, it will raise an exception at runtime, which could even be an expected behavior.On the other side, suppose you add a MouthOfTruth class with a method speak(). In the C++ case, either you will have to refactor your hierarchy, or you will have to define a different makeSpeak method to accept MouthOfTruth objects, or in java you could extract the behavior into a CanSpeakIface and implement the interface for each. There are many solutions...What I‚Äôd like to point out is that I haven‚Äôt found a single reason yet to use inheritance in Python (apart of frameworks and trees of exceptions, but I guess that alternative strategies exist). you don‚Äôt need to implement a base-derived hierarchy to perform polymorphically. If you want to use inheritance to reuse implementation, you can accomplish the same through containment and delegation, with the added benefit that you can alter it at runtime, and you clearly define the interface of the contained, without risking unintended side effects.So, in the end, the question stands: what's the point of inheritance in Python?Edit: thanks for the very interesting answers. Indeed you can use it for code reuse, but I am always careful when reusing implementation. In general, I tend to do very shallow inheritance trees or no tree at all, and if a functionality is common I refactor it out as a common module routine and then call it from each object. I do see the advantage of having one single point of change (eg. instead of adding to Dog, Cat, Moose and so on, I just add to Animal, which is the basic advantage of inheritance), but you can achieve the same with a delegation chain (eg. a la JavaScript). I'm not claiming it's better though, just another way.I also found a similar post on this regard.

2009-06-19 23:28:18Z

Suppose you have the following situationAs you can see, makeSpeak is a routine that accepts a generic Animal object. In this case, Animal is quite similar to a Java interface, as it contains only a pure virtual method. makeSpeak does not know the nature of the Animal it gets passed. It just sends it the signal„Äåspeak„Äçand leaves the late binding to take care of which method to call: either Cat::speak() or Dog::speak(). This means that, as far as makeSpeak is concerned, the knowledge of which subclass is actually passed is irrelevant.But what about Python? Let‚Äôs see the code for the same case in Python. Please note that I try to be as similar as possible to the C++ case for a moment:Now, in this example you see the same strategy. You use inheritance to leverage the hierarchical concept of both Dogs and Cats being Animals. 

But in Python, there‚Äôs no need for this hierarchy. This works equally wellIn Python you can send the signal„Äåspeak„Äçto any object you want. If the object is able to deal with it, it will be executed, otherwise it will raise an exception. Suppose you add a class Airplane to both codes, and submit an Airplane object to makeSpeak. In the C++ case, it won‚Äôt compile, as Airplane is not a derived class of Animal. In the Python case, it will raise an exception at runtime, which could even be an expected behavior.On the other side, suppose you add a MouthOfTruth class with a method speak(). In the C++ case, either you will have to refactor your hierarchy, or you will have to define a different makeSpeak method to accept MouthOfTruth objects, or in java you could extract the behavior into a CanSpeakIface and implement the interface for each. There are many solutions...What I‚Äôd like to point out is that I haven‚Äôt found a single reason yet to use inheritance in Python (apart of frameworks and trees of exceptions, but I guess that alternative strategies exist). you don‚Äôt need to implement a base-derived hierarchy to perform polymorphically. If you want to use inheritance to reuse implementation, you can accomplish the same through containment and delegation, with the added benefit that you can alter it at runtime, and you clearly define the interface of the contained, without risking unintended side effects.So, in the end, the question stands: what's the point of inheritance in Python?Edit: thanks for the very interesting answers. Indeed you can use it for code reuse, but I am always careful when reusing implementation. In general, I tend to do very shallow inheritance trees or no tree at all, and if a functionality is common I refactor it out as a common module routine and then call it from each object. I do see the advantage of having one single point of change (eg. instead of adding to Dog, Cat, Moose and so on, I just add to Animal, which is the basic advantage of inheritance), but you can achieve the same with a delegation chain (eg. a la JavaScript). I'm not claiming it's better though, just another way.I also found a similar post on this regard.You are referring to the run-time duck-typing as "overriding" inheritance, however I believe inheritance has its own merits as a design and implementation approach, being an integral part of object oriented design. In my humble opinion, the question of whether you can achieve something otherwise is not very relevant, because actually you could code Python without classes, functions and more, but the question is how well-designed, robust and readable your code will be.I can give two examples for where inheritance is the right approach in my opinion, I'm sure there are more. First, if you code wisely, your makeSpeak function may want to validate that its input is indeed an Animal, and not only that "it can speak", in which case the most elegant method would be to use inheritance. Again, you can do it in other ways, but that's the beauty of object oriented design with inheritance - your code will "really" check whether the input is an "animal".Second, and clearly more straightforward, is Encapsulation - another integral part of object oriented design. This becomes relevant when the ancestor has data members and/or non-abstract methods. Take the following silly example, in which the ancestor has a function (speak_twice) that invokes a then-abstract function:Assuming "speak_twice" is an important feature, you don't want to code it in both Dog and Cat, and I'm sure you can extrapolate this example. Sure, you could implement a Python stand-alone function that will accept some duck-typed object, check whether it has a speak function and invoke it twice, but that's both non-elegant and misses point number 1 (validate it's an Animal). Even worse, and to strengthen the Encapsulation example, what if a member function in the descendant class wanted to use "speak_twice"?It gets even clearer if the ancestor class has a data member, for example "number_of_legs" that is used by non-abstract methods in the ancestor like "print_number_of_legs", but is initiated in the descendant class' constructor (e.g. Dog would initialize it with 4 whereas Snake would initialize it with 0). Again, I'm sure there are endless more examples, but basically every (large enough) software that is based on solid object oriented design will require inheritance.Inheritance in Python is all about code reuse.  Factorize common functionality into a base class, and implement different functionality in the derived classes.Inheritance in Python is more of a convenience than anything else.  I find that it's best used to provide a class with "default behavior."Indeed, there is a significant community of Python devs who argue against using inheritance at all.  Whatever you do, don't just don't overdo it.  Having an overly complicated class hierarchy is a sure way to get labeled a "Java programmer", and you just can't have that.  :-)I think the point of inheritance in Python is not to make the code compile, it is for the real reason of inheritance which is extending the class into another child class, and to override the logic in the base class. However the duck typing in Python makes the "interface" concept useless, because you can just check if the method exist before invokation with no need to use an interface to limit the class structure.I think that it is very difficult to give a meaningful, concrete answer with such abstract examples...To simplify, there are two types of inheritance: interface and implementation. If you need to inherit the implementation, then python is not so different than statically typed OO languages like C++. Inheritance of interface is where there is a big difference, with fundamental consequences for the design of your software in my experience. Languages like Python does not force you to use inheritance in that case, and avoiding inheritance is a good point in most cases, because it is very hard to fix a wrong design choice there later. That's a well known point raised in any good OOP book.There are cases where using inheritance for interfaces is advisable in Python, for example for plug-ins, etc... For those cases, Python 2.5 and below lacks a "built-in" elegant approach, and several big frameworks designed their own solutions (zope, trac, twister). Python 2.6 and above has ABC classes to solve this.It's not inheritance that duck-typing makes pointless, it's interfaces ‚Äî like the one you chose in creating an all abstract animal class.If you had used an animal class that introduce some real behavior for its descendants to make use of, then dog and cat classes that introduced some additional behavior there would be a reason for both classes.  It's only in the case of the ancestor class contributing no actual code to the descendant classes that your argument is correct.Because Python can directly know the capabilities of any object, and because those capabilities are mutable beyond the class definition, the idea of using a pure abstract interface to "tell" the program what methods can be called is somewhat pointless.  But that's not the sole, or even the main, point of inheritance.In C++/Java/etc, polymorphism is caused by inheritance. Abandon that misbegotten belief, and dynamic languages open up to you.Essentially, in Python there is no interface so much as "the understanding that certain methods are callable".  Pretty hand-wavy and academic-sounding, no? It means that because you call "speak" you clearly expect that the object should have a "speak" method.  Simple, huh?  This is very Liskov-ian in that the users of a class define its interface, a good design concept that leads you into healthier TDD.So what is left is, as another poster politely managed to avoid saying, a code sharing trick.  You could write the same behavior into each "child" class, but that would be redundant.  Easier to inherit or mix-in functionality that is invariant across the inheritance hierarchy.  Smaller, DRY-er code is better in general.  You can get around inheritance in Python and pretty much any other language.  It's all about code reuse and code simplification though.  Just a semantic trick, but after building your classes and base classes, you don't even have to know what's possible with your object to see if you can do it.  Say you have d which is a Dog that subclassed Animal.If whatever the user typed in is available, the code will run the proper method.  Using this you can create whatever combination of Mammal/Reptile/Bird hybrid monstrosity you want, and now you can make it say 'Bark!' while flying and sticking out its forked tongue and it will handle it properly!  Have fun with it!I don't see much point in inheritance.Every time I have ever used inheritance in real systems, I got burned because it led to a tangled web of dependencies, or I simply realised in time that I would be a lot better off without it. Now, I avoid it as much as possible. I simply never have a use for it.James Gosling was once asked at a press conference a question along the lines: "If you could go back and do Java differently, what would you leave out?". His response was "Classes", to which there was laughter. However, he was serious and explained that really, it was not classes that were the problem but inheritance.I kind of view it like a drug dependency - it gives you a quick fix that feels good, but in the end, it messes you up. By that I mean that it is a convenient way to reuse code, but it forces an unhealthy coupling between child and parent class. Changes to the parent may break the child. The child is dependant on the parent for certain functionality and cannot alter that functionality. Therefore the functionality provided by the child is also tied to the parent - you can only have both.Better is to provide one single client facing class for an interface which implements the interface, using the functionality of other objects which are composed at construction time. Doing this via properly designed interfaces, all coupling can be eliminated and we provide a highly composable API (This is nothing new - most programmers already do this, just not enough). Note that the implementing class must not simply expose functionality, otherwise the client should just use the composed classes directly - it must do something new by combining that functionality.There is the argument from the inheritance camp that pure delegation implementations suffer because they require lots of 'glue' methods which simply pass along values through a delegation 'chain'. However, this is simply reinventing an inheritance-like design using delegation. Programmers with too many years of exposure to inheritance-based designs are particularly vulnerable to falling into this trap, as, without realising it, they will think of how they would implement something using inheritance and then convert that to delegation.Proper separation of concerns like the above code doesn't require glue methods, as each step is actually adding value, so they are not really 'glue' methods at all (if they don't add value, the design is flawed).It boils down to this:Another small point is that op's 3'rd example, you can't call isinstance(). For example passing your 3'rd example to another object that takes and "Animal" type an calls speak on it. If you do it don't you would have to check for dog type, cat type, and so on. Not sure if instance checking is really "Pythonic", because of late binding. But then you would have to implement some way that the AnimalControl doesn't try to throw Cheeseburger types in the truck, becuase Cheeseburgers don't speak.Classes in Python are basically just ways of grouping a bunch of functions and data.. They are different to classes in C++ and such..I've mostly seen inheritance used for overriding methods of the super-class. For example, perhaps a more Python'ish use of inheritance would be..Of course cats aren't a type of dog, but I have this (third party) Dog class which works perfectly, except the speak method which I want to override - this saves re-implementing  the entire class, just so it meows. Again, while Cat isn't a type of Dog, but a cat does inherit a lot of attributes..A much better (practical) example of overriding a method or attribute is how you change the user-agent for urllib. You basically subclass urllib.FancyURLopener and change the version attribute (from the documentation):Another manner exceptions are used is for Exceptions, when inheritance is used in a more "proper" way:..you can then catch AnimalError to catch all exceptions which inherit from it, or a specific one like  AnimalBrokenLegError

How to *actually* read CSV data in TensorFlow?

Rob

[How to *actually* read CSV data in TensorFlow?](https://stackoverflow.com/questions/37091899/how-to-actually-read-csv-data-in-tensorflow)

I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd actually read CSV data into a usable example/label tensors in TensorFlow. The example from the TensorFlow tutorial on reading CSV data is pretty fragmented and only gets you part of the way to being able to train on CSV data.Here's my code that I've pieced together, based off that CSV tutorial:And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:All the code above does is print each example from the CSV file, one by one, which, while nice, is pretty darn useless for training.What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, here's a notebook I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like train_dataset and train_labels:I've tried using tf.train.shuffle_batch, like this, but it just inexplicably hangs:So to sum up, here are my questions:Edit:

As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):

2016-05-07 17:57:38Z

I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd actually read CSV data into a usable example/label tensors in TensorFlow. The example from the TensorFlow tutorial on reading CSV data is pretty fragmented and only gets you part of the way to being able to train on CSV data.Here's my code that I've pieced together, based off that CSV tutorial:And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:All the code above does is print each example from the CSV file, one by one, which, while nice, is pretty darn useless for training.What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, here's a notebook I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like train_dataset and train_labels:I've tried using tf.train.shuffle_batch, like this, but it just inexplicably hangs:So to sum up, here are my questions:Edit:

As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):I think you are mixing up imperative and graph-construction parts here. The operation tf.train.shuffle_batch creates a new queue node, and a single node can be used to process the entire dataset. So I think you are hanging because you created a bunch of shuffle_batch queues in your for loop and didn't start queue runners for them. Normal input pipeline usage looks like this:--- end of graph construction, beginning of imperative programming --To be more scalable (to avoid Python GIL), you could generate all of your data using TensorFlow pipeline. However, if performance is not critical, you can hook up a numpy array to an input pipeline by using slice_input_producer. Here's an example with some Print nodes to see what's going on (messages in Print go to stdout when node is run)You should see something like thisThe "8, 9" numbers didn't fill up the full batch, so they didn't get produced. Also tf.Print are printed to sys.stdout, so they show up in separately in Terminal for me.PS: a minimal of connecting batch to a manually initialized queue is in github issue 2193Also, for debugging purposes you might want to set timeout on your session so that your IPython notebook doesn't hang on empty queue dequeues. I use this helper function for my sessionsScalability Notes:Or you could try this, the code loads the Iris dataset into tensorflow using pandas and numpy and a simple one neuron output is printed in the session. Hope it helps for a basic understanding.... [ I havent added the way of one hot decoding labels].You can use latest tf.data API :If anyone came here searching for a simple way to read absolutely large and sharded CSV files in tf.estimator API then , please see below my codeExample usage in TF.estimator:2.0 Compatible Solution: This Answer might be provided by others in the above thread but I will provide additional links which will help the community.For more information, please refer this Tensorflow Tutorial.

When and how to use Tornado? When is it useless?

Vladimir Sidorenko

[When and how to use Tornado? When is it useless?](https://stackoverflow.com/questions/4212877/when-and-how-to-use-tornado-when-is-it-useless)

Ok, Tornado is non-blocking and quite fast and it can handle a lot of standing requests easily.But I guess it's not a silver bullet and if we just blindly run Django-based or any other site with Tornado it won't give any performance boost.I couldn't find comprehensive explanation of this, so I'm asking it here:

2010-11-18 08:29:04Z

Ok, Tornado is non-blocking and quite fast and it can handle a lot of standing requests easily.But I guess it's not a silver bullet and if we just blindly run Django-based or any other site with Tornado it won't give any performance boost.I couldn't find comprehensive explanation of this, so I'm asking it here:This distinction is a bit blurry. Only if you are serving static pages, you would use one of the fast server like lighthttpd. Other wise, most servers provides a varying complexity of framework to develop web applications. Tornado is a good web framework. Twisted is even more capable and is considered a good networking framework. It has support for lot of protocols. Tornado and Twisted are frameworks that provide support non-blocking, asynchronous web / networking application development. By it's very nature, Async / Non-Blocking I/O works great when it is I/O intensive and not computation intensive. Most web / networking applications suits well for this model. If your application demands certain computational intensive task to be done then it has to be delegated to some other service that can handle it better. While Tornado / Twisted can do the job of web server, responding to web requests.Performance is usually a characteristic of complete web application architecture. You can bring down the performance with most web frameworks, if the application is not designed properly. Think about caching, load balancing etc. Tornado and Twisted provides reasonable performance and is good for building a very performant web application. You can check out the testimonials for both twisted and tornado to see what they are capable of. I'm sorry for answering an old question, but I came across this one and wondered why it didn't have more answers. To answer Bart J's question:Well that depends on what kind of parsing you're doing and on what hardware :) Long time is a long time, so if your app takes more than say half a second to respond, it'll seem sluggish -  profile your app.The key to fast systems is great architecture, not so much the specifics as for instance which framework you're using (Twisted, Tornado, Apache+PHP). Tornado has an asynchronous processing style and that's really what a lot of it comes down to in my opinion. Node.js, Twisted and Yaws are examples of other asynchronous web servers that scale very well because of a lightweight approach and asynchronous processing style.So:Tornado is good for handling a lot of connections, since it can respond to an incoming client, dispatch a request handler and don't think about that client until the result-callback is pushed on the event queue. So for that specific quality Tornado should be used when you want to scale well when handling a lot of requests. 

The async processing facilitates functional decoupling and shared-nothing data access. That swings really well with stateless design like REST or other Service Oriented Architectures. You also don't have to deal with spawning threads or processes with the inherent overhead so much and you can save some of the locking/IPC trouble.Tornado won't make much of a difference, on the other hand, if your backend and/or data store takes a long time to process the requests. It helps to do concurrent designs and Web services in particular. The concurrent architecture makes it easier to scale your design and keep the coupling low. That's my experience with Tornado at least. 

Quicksort with Python

user2687481

[Quicksort with Python](https://stackoverflow.com/questions/18262306/quicksort-with-python)

I am totally new to python and I am trying to implement quicksort in it.

Could someone please help me complete my code?I do not know how to concatenate the three arrays and printing them. 

2013-08-15 21:37:08Z

I am totally new to python and I am trying to implement quicksort in it.

Could someone please help me complete my code?I do not know how to concatenate the three arrays and printing them. Quick sort without additional memory (in place)Usage:

There is another concise and beautiful versionLet me explain the above codes for detailsIf I search "python quicksort implementation" in Google, this question is the first result to pop up. I understand that the initial question was to "help correct the code" but there already are many answers that disregard that request: the currently second most voted one, the horrendous one-liner with the hilarious "You are fired" comment and, in general, many implementations that are not in-place (i.e. use extra memory proportional to input list size). This answer provides an in-place solution but it is for python 2.x. So, below follows my interpretation of the in-place solution from Rosetta Code which will work just fine for python 3 too:And if you are willing to forgo the in-place property, below is yet another version which better illustrates the basic ideas behind quicksort. Apart from readability, its other advantage is that it is stable (equal elements appear in the sorted list in the same order that they used to have in the unsorted list). This stability property does not hold with the less memory-hungry in-place implementation presented above.In real life, we should always use the builtin sort provided by Python. However, understanding the quicksort algorithm is instructive. My goal here is to break down the subject such that it is easily understood and replicable by the reader without having to return to reference materials.The quicksort algorithm is essentially the following:If the data are randomly distributed, selecting the first data point as the pivot is equivalent to a random selection.First, let's look at a readable example that uses comments and variable names to point to intermediate values:To restate the algorithm and code demonstrated here - we move values above the pivot to the right, and values below the pivot to the left, and then pass those partitions to same function to be further sorted.This can be golfed to 88 characters:To see how we get there, first take our readable example, remove comments and docstrings, and find the pivot in-place:Now find below and above, in-place:Now, knowing that and returns the prior element if false, else if it is true, it evaluates and returns the following element, we have:Since lambdas return a single epression, and we have simplified to a single expression (even though it is getting more unreadable) we can now use a lambda:And to reduce to our example, shorten the function and variable names to one letter, and eliminate the whitespace that isn't required.Note that this lambda, like most code golfing, is rather bad style.The prior implementation creates a lot of unnecessary extra lists. If we can do this in-place, we'll avoid wasting space.The below implementation uses the Hoare partitioning scheme, which you can read more about on wikipedia (but we have apparently removed up to 4 redundant calculations per partition() call by using while-loop semantics instead of do-while and moving the narrowing steps to the end of the outer while loop.).Not sure if I tested it thoroughly enough:This algorithm is frequently taught in computer science courses and asked for on job interviews. It helps us think about recursion and divide-and-conquer. Quicksort is not very practical in Python since our builtin timsort algorithm is quite efficient, and we have recursion limits. We would expect to sort lists in-place with list.sort or create new sorted lists with sorted - both of which take a key and reverse argument. There are many answers to this already, but I think this approach is the most clean implementation:You can of course skip storing everything in variables and return them straight away like this:functional approach:functional programming aproachI think both answers here works ok for the list provided (which answer the original question), but would breaks if an array containing non unique values is passed. So for completeness, I would just point out the small error in each and explain how to fix them.For example trying to sort the following array  [12,4,5,6,7,3,1,15,1] (Note that 1 appears twice) with Brionius algorithm .. at some point will end up with the less array empty and the equal array with a pair of values (1,1) that can not be separated in the next iteration and the len() > 1...hence you'll end up with an infinite loopYou can fix it by either returning array if less is empty or better by not calling sort in your equal array, as in zangw answer The fancier solution also breaks, but for a different cause, it is missing the return clause in the recursion line, which will cause at some point to return None and try to append it to a list ....To fix it just add a return to that linePartition - Split an array by a pivot that smaller elements move to the left and greater elemets move to the right or vice versa. A pivot can be an random element from an array. To make this algorith we need to know what is begin and end index of an array and where is a pivot. Then set two auxiliary pointers L, R.So we have an array user[...,begin,...,end,...]The start position of L and R pointers

[...,begin,next,...,end,...]

¬†¬†¬†¬†¬†R¬†¬†¬†¬†¬†¬†¬†Lwhile L < end

¬†¬†1. If a user[pivot] > user[L] then move R by one and swap user[R] with user[L]

¬†¬†2. move L by oneAfter while swap user[R] with user[pivot]Quick sort - Use the partition algorithm until every next part of the split by a pivot will have begin index greater or equals than end index.Easy implementation from grokking algorithmsThis is a version of the quicksort using Hoare partition scheme and with fewer swaps and local variablesI know many people have answered this question correctly and I enjoyed reading them. My answer is almost the same as zangw but I think the previous contributors did not do a good job of explaining visually how things actually work...so here is my attempt to help others that might visit this question/answers in the future about a simple solution for quicksort implementation.How does it work ?Here is an example along with visual to go with it ...

(pivot)9,11,2,0average: n log of n worse case: n^2 The code:items=[9,11,2,0]

print(quicksort(items))Or if you prefer a one-liner that also illustrates the Python equivalent of C/C++ varags, lambda expressions, and if expressions:A "true" in-place implementation [Algorithms 8.9, 8.11 from the Algorithm Design and Applications Book by Michael T. Goodrich and Roberto Tamassia]:The algorithm has 4 simple steps:Code for the algorithm in python:Carry on with this algorithm recursively with the left and right parts.Another quicksort implementation:For Version Python 3.x: a functional-style using operator module, primarily to improve readability. and is tested as        Here's an easy implementation:-The algorithm contains two boundaries, one having elements less than the pivot  (tracked by index "j") and the other having elements greater than the pivot (tracked by index "i"). In each iteration, a new element is processed by incrementing j.Invariant:- If the invariant is violated, ith and jth elements are swapped, and i 

is incremented. After all elements have been processed, and everything after the pivot 

has been partitioned, the pivot element is swapped with the last element 

smaller than it.The pivot element will now be in its correct place in the sequence. The 

elements before it will be less than it and the ones after it will be 

greater than it, and they will be unsorted.A "good" pivot will result in two sub-sequences of roughly the same 

size. Deterministically, a pivot element can either be selected in a 

naive manner or by computing the median of the sequence.A naive implementation of selecting a pivot will be the first or last 

element. The worst-case runtime in this case will be when the input 

sequence is already sorted or reverse sorted, as one of the subsequences 

will be empty which will cause only one element to be removed per 

recursive call.A perfectly balanced split is achieved when the pivot is the median

element of the sequence. There are an equal number of elements greater 

than it and less than it. This approach guarantees a better overall 

running time, but is much more time-consuming.A non-deterministic/random way of selecting the pivot would be to pick 

an element uniformly at random. This is a simple and lightweight 

approach that will minimize worst-case scenario and also lead to a 

roughly balanced split. This will also provide a balance between the naive approach and the median approach of selecting the pivot.I am attaching the code below! This quicksort is a great learning tool because of the Location of the pivot value. Since it is in a constant place, you can walk through it multiple times and really get a hang of how it all works. In practice it is best to randomize the pivot to avoid O(N^2) runtime.Full example with printed variables at partition step:This algorithm doesn't use recursive functions.Let N be any list of numbers with len(N) > 0. Set K = [N] and execute the following program.Note: This is a stable sorting algorithm.inlace sortwithout recursion:

How to create python bytes object from long hex string?

recursive

[How to create python bytes object from long hex string?](https://stackoverflow.com/questions/443967/how-to-create-python-bytes-object-from-long-hex-string)

I have a long sequence of hex digits in a string, such as only much longer, several kilobytes.  Is there a builtin way to convert this to a bytes object in python 2.6/3?

2009-01-14 17:42:50Z

I have a long sequence of hex digits in a string, such as only much longer, several kilobytes.  Is there a builtin way to convert this to a bytes object in python 2.6/3?Works in Python 2.7 and higher including python3:Note: There seems to be a bug with the bytearray.fromhex() function in Python 2.6. The python.org documentation states that the function accepts a string as an argument, but when applied, the following error is thrown: You can do this with the hex codec.  ie:Try the binascii moduleThats the way I did it.

Get random sample from list while maintaining ordering of items?

Yochai Timmer

[Get random sample from list while maintaining ordering of items?](https://stackoverflow.com/questions/6482889/get-random-sample-from-list-while-maintaining-ordering-of-items)

I have a sorted list, let say: (its not really just numbers, its a list of objects that are sorted with a complicated time consuming algorithm)Is there some python function that will give me N of the items, but will keep the order?Example:etc...

2011-06-26 08:12:50Z

I have a sorted list, let say: (its not really just numbers, its a list of objects that are sorted with a complicated time consuming algorithm)Is there some python function that will give me N of the items, but will keep the order?Example:etc...Following code will generate a random sample of size 4:(note: with Python 2, better use xrange instead of range)Explanationgenerates a random sample of the indices of the original list.These indices then get sorted to preserve the ordering of elements in the original list.Finally, the list comprehension pulls out the actual elements from the original list, given the sampled indices.Take a random sample without replacement of the indices, sort the indices, and take them from the original.Or more concisely:You can alternatively use a math trick and iteratively go through myList from left to right, picking numbers with dynamically-changing probability (N-numbersPicked)/(total-numbersVisited). The advantage of this approach is that it's an O(N) algorithm since it doesn't involve sorting!Proof of concept and test that probabilities are correct:Simulated with 1 trillion pseudorandom samples over the course of 5 hours:Probabilities diverge from true probabilities by less a factor of 1.0001. Running this test again resulted in a different order meaning it isn't biased towards one ordering. Running the test with fewer samples for [0,1,2,3,4], k=3 and [0,1,2,3,4,5], k=4 had similar results.edit: Not sure why people are voting up wrong comments or afraid to upvote... NO, there is nothing wrong with this method. =)(Also a useful note from user tegan in the comments: If this is python2, you will want to use xrange, as usual, if you really care about extra space.)edit: Proof: Considering the uniform distribution (without replacement) of picking a subset of k out of a population seq of size len(seq), we can consider a partition at an arbitrary point i into 'left' (0,1,...,i-1) and 'right' (i,i+1,...,len(seq)). Given that we picked numbersPicked from the left known subset, the remaining must come from the same uniform distribution on the right unknown subset, though the parameters are now different. In particular, the probability that seq[i] contains a chosen element is #remainingToChoose/#remainingToChooseFrom, or (k-numbersPicked)/(len(seq)-i), so we simulate that and recurse on the result. (This must terminate since if #remainingToChoose == #remainingToChooseFrom, then all remaining probabilities are 1.) This is similar to a probability tree that happens to be dynamically generated. Basically you can simulate a uniform probability distribution by conditioning on prior choices (as you grow the probability tree, you pick the probability of the current branch such that it is aposteriori the same as prior leaves, i.e. conditioned on prior choices; this will work because this probability is uniformly exactly N/k).edit: Timothy Shields mentions Reservoir Sampling, which is the generalization of this method when len(seq) is unknown (such as with a generator expression). Specifically the one noted as "algorithm R" is O(N) and O(1) space if done in-place; it involves taking the first N element and slowly replacing them (a hint at an inductive proof is also given). There are also useful distributed variants and miscellaneous variants of reservoir sampling to be found on the wikipedia page.edit: Here's another way to code it below in a more semantically obvious manner.)Maybe you can just generate the sample of indices and then collect the items from your list.Apparently random.sample was introduced in python 2.3so for version under that, we can use shuffle (example for 4 items):random.sample implement it.

Adding docstrings to namedtuples?

Rickard

[Adding docstrings to namedtuples?](https://stackoverflow.com/questions/1606436/adding-docstrings-to-namedtuples)

Is it possible to add a documentation string to a namedtuple in an easy manner?I triedbut that doesn't cut it. Is it possible to do in some other way?

2009-10-22 10:55:53Z

Is it possible to add a documentation string to a namedtuple in an easy manner?I triedbut that doesn't cut it. Is it possible to do in some other way?You can achieve this by creating a simple, empty wrapper class around the returned value from namedtuple.  Contents of a file I created (nt.py):Then in the Python REPL:Or you could do:If you don't like doing that by hand every time, it's trivial to write a sort-of factory function to do this:which outputs:In Python 3, no wrapper is needed, as the __doc__ attributes of types is writable.This closely corresponds to a standard class definition, where the docstring follows the header.This does not work in Python 2.AttributeError: attribute '__doc__' of 'type' objects is not writable. Came across this old question via Google while wondering the same thing.Just wanted to point out that you can tidy it up even more by calling namedtuple() right from the class declaration:Yes, in several ways.As of Python 3.6 we can use a class definition with typing.NamedTuple directly, with a docstring (and annotations!):Compared to Python 2, declaring empty __slots__ is not necessary. In Python 3.8, it isn't necessary even for subclasses.Note that declaring __slots__ cannot be non-empty!In Python 3, you can also easily alter the doc on a namedtuple:Which allows us to view the intent for them when we call help on them:This is really straightforward compared to the difficulties we have accomplishing the same thing in Python 2.In Python 2, you'll need toDeclaring __slots__ is an important part that the other answers here miss . If you don't declare __slots__ - you could add mutable ad-hoc attributes to the instances, introducing bugs.And now:Each instance will create a separate __dict__ when __dict__ is accessed (the lack of __slots__ won't otherwise impede the functionality, but the lightweightness of the tuple, immutability, and declared attributes are all important features of namedtuples). You'll also want a __repr__, if you want what is echoed on the command line to give you an equivalent object:a __repr__ like this is needed if you create the base namedtuple with a different name (like we did above with the name string argument, 'NTBase'):To test the repr, instantiate, then test for equality of a pass to eval(repr(instance))The docs also give such an example, regarding __slots__ - I'm adding my own docstring to it:This demonstrates in-place usage (like another answer here suggests), but note that the in-place usage may become confusing when you look at the method resolution order, if you're debugging, which is why I originally suggested using Base as a suffix for the base namedtuple:To prevent creation of a __dict__ when subclassing from a class that uses it, you must also declare it in the subclass. See also this answer for more caveats on using __slots__.Since Python 3.5, docstrings for namedtuple objects can be updated.From the whatsnew:In Python 3.6+ you can use:No need to use a wrapper class as suggested by the accepted answer. Simply literally add a docstring:This results in: (example using ipython3):Voil√†!You could concoct your own version of the namedtuple factory function by Raymond Hettinger and add an optional docstring argument.¬†¬†However it would be easier -- and arguably better -- to just define your own factory function using the same basic technique as in the recipe.¬†¬†Either way, you'll end up with something reusable.I created this function to quickly create a named tuple and document the tuple along with each of its parameters:You can then create a new named tuple:Then instantiate the described named tuple with your own data, ie.When executing help(MyTuple) via the python3 command line the following is shown:Alternatively, you can also specify the parameter's type via:No, you can only add doc strings to modules, classes and function (including methods)

How to round a floating point number up to a certain decimal place?

hsinxh

[How to round a floating point number up to a certain decimal place?](https://stackoverflow.com/questions/4518641/how-to-round-a-floating-point-number-up-to-a-certain-decimal-place)

Suppose I have 8.8333333333333339, and I want to convert it to 8.84. How can I accomplish this in Python?round(8.8333333333333339, 2) gives 8.83 and not 8.84. I am new to Python or programming in general.I don't want to print it as a string, and the result will be further used. For more information on the problem, please check Tim Wilson's Python Programming Tips: Loan and payment calculator.

2010-12-23 12:13:25Z

Suppose I have 8.8333333333333339, and I want to convert it to 8.84. How can I accomplish this in Python?round(8.8333333333333339, 2) gives 8.83 and not 8.84. I am new to Python or programming in general.I don't want to print it as a string, and the result will be further used. For more information on the problem, please check Tim Wilson's Python Programming Tips: Loan and payment calculator.8.833333333339 (or 8.833333333333334, the result of 106.00/12) properly rounded to two decimal places is 8.83. Mathematically it sounds like what you want is a ceiling function. The one in Python's math module is named ceil:Respectively, the floor and ceiling functions generally map a real number to the largest previous or smallest following integer which has zero decimal places ‚Äî so to use them for 2 decimal places the number is first multiplied by 102 (or 100) to shift the decimal point and is then divided by it afterwards to compensate.If you don't want to use the math module for some reason, you can use this (minimally tested) implementation I just wrote:From the sample output it appears that they rounded up the monthly payment, which is what many call the effect of the ceiling function. This means that each month a little more than 1‚ÅÑ12 of the total amount is being paid. That made the final payment a little smaller than usual ‚Äî leaving a remaining unpaid balance of only 8.76.It would have been equally valid to use normal rounding producing a monthly payment of 8.83 and a slightly higher final payment of 8.87. However, in the real world people generally don't like to have their payments go up, so rounding up each payment is the common practice ‚Äî it also returns the money to the lender more quickly.This is normal (and has nothing to do with Python) because 8.83 cannot be represented exactly as a binary float, just as 1/3 cannot be represented exactly in decimal (0.333333... ad infinitum).If you want to ensure absolute precision, you need the decimal module:You want to use the decimal module but you also need to specify the rounding mode. Here's an example:A much simpler way is to simply use the round() function. Here is an example.If you were to print out total_price right now you would getBut if you enclose it in a round() function like soThe output equalsThe round() function works by accepting two parameters. The first is the number you want to round. The second is the number of decimal places to round to.If you round 8.8333333333339 to 2 decimals, the correct answer is 8.83, not 8.84. The reason you got 8.83000000001 is because 8.83 is a number that cannot be correctly reprecented in binary, and it gives you the closest one. If you want to print it without all the zeros, do as VGE says:If you want to round, 8.84 is the incorrect answer. 8.833333333333 rounded is 8.83 not 8.84. If you want to always round up, then you can use math.ceil. Do both in a combination with string formatting, because rounding a float number itself doesn't make sense.The easiest way to do this is by using the below function, which is built in:For example:The output would be:Similarly:would give:Just for the record.  You could do it this way:There, no need for includes/importsHere is my solution for the round up/down problemUse the decimal module: http://docs.python.org/library/decimal.htmlŸéŸéŸéŸéŸéŸéI have this code:and then this code:round worked for meHere is a simple function to do this for you:Here, num is the decimal number. x is the decimal up to where you want to round a floating number.The advantage over other implementation is that it can fill zeros at the right end of the decimal to make a deciaml number up to x decimal places.Example 1:will return 10.200000000 (up to 9 decimal points)Example 2:will return10.22 (up to two decimal points)

hasattr() vs try-except block to deal with non-existent attributes

Imran

[hasattr() vs try-except block to deal with non-existent attributes](https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes)

vsWhich should be preferred and why?

2009-05-24 05:11:59Z

vsWhich should be preferred and why?hasattr internally and rapidly performs the same task as the try/except block: it's a very specific, optimized, one-task tool and thus should be preferred, when applicable, to the very general-purpose alternative.Any benches that illustrate difference in performance?timeit it's your friendThere is a third, and often better, alternative:Advantages:One thing to be careful of is if you care about the case where obj.attribute is set to None, you'll need to use a different sentinel value.I almost always use hasattr: it's the correct choice for most cases.The problematic case is when a class overrides __getattr__: hasattr will catch all exceptions instead of catching just AttributeError like you expect. In other words, the code below will print b: False even though it would be more appropriate to see a ValueError exception:The important error has thus disappeared. This has been fixed in Python 3.2 (issue9666) where hasattr now only catches AttributeError.An easy workaround is to write a utility function like this:This let's getattr deal with the situation and it can then raise the appropriate exception.I would say it depends on whether your function may accept objects without the attribute by design, e.g. if you have two callers to the function, one providing an object with the attribute and the other providing an object without it. If the only case where you'll get an object without the attribute is due to some error, I would recommend using the exceptions mechanism even though it may be slower, because I believe it is a cleaner design. Bottom line: I think it's a design and readability issue rather than an efficiency issue. If it's just one attribute you're testing, I'd say use hasattr.  However, if you're doing several accesses to attributes which may or may not exist then using a try block may save you some typing.If not having the attribute is not an error condition, the exception handling variant has a problem: it would catch also AttributeErrors that might come internally when accessing obj.attribute (for instance because attribute is a property so that accessing it calls some code).I'd suggest option 2. Option 1 has a race condition if some other thread is adding or removing the attribute.Also python has an Idiom, that EAFP ('easier to ask forgiveness than permission') is better than LBYL ('look before you leap').From a practical point of view, in most languages using a conditional will always be consderably faster than handling an exception.If you're wanting to handle the case of an attribute not existing somewhere outside of the current function, the exception is the better way to go. An indicator that you may want to be using an exception instead of a conditional is that the conditional merely sets a flag and aborts the current operation, and something elsewhere checks this flag and takes action based on that.That said, as Rax Olgud points out, communication with others is one important attribute of code, and what you want to say by saying "this is an exceptional situation" rather than "this is is something I expect to happen" may be more important.This subject was covered in the EuroPython 2016 talk Writing faster Python by Sebastian Witowski. Here's a reproduction of his slide with the performance summary. He also uses the terminology look before you leap in this discussion, worth mentioning here to tag that keyword.The first.Shorter is better. Exceptions should be exceptional.At least when it is up to just what's going on in the program, leaving out the human part of readability, etc. (which is actually most of the time more imortant than performance (at least in this case - with that performance span), as Roee Adler and others pointed out).Nevertheless looking at it from that perspective,

it then becomes a matter of choosing betweenandsince hasattr just uses the first case to determine the result.

Food for thought ;-)

Regular expression to match a dot

Yuushi

[Regular expression to match a dot](https://stackoverflow.com/questions/13989640/regular-expression-to-match-a-dot)

Was wondering what the best way is to match "test.this" from "blah blah blah test.this@gmail.com blah blah" is? Using Python.I've tried re.split(r"\b\w.\w@")

2012-12-21 11:49:34Z

Was wondering what the best way is to match "test.this" from "blah blah blah test.this@gmail.com blah blah" is? Using Python.I've tried re.split(r"\b\w.\w@")A . in regex is a metacharacter, it is used to match any character. To match a literal dot, you need to escape it, so \.In your regex you need to escape the dot "\." or use it inside a character class "[.]", as it is a meta-character in regex, which matches any character. Also, you need \w+ instead of \w to match one or more word characters.Now, if you want the test.this content, then split is not what you need. split will split your string around the test.this. For example:You can use re.findall:So, if you want to evaluate dot literaly, I think you should put it in square brackets:In javascript you have to use \. to match a dot.ExampleandThis expression,might also work OK for those specific types of input strings.If you wish to simplify/modify/explore the expression, it's been explained on the top right panel of regex101.com. If you'd like, you can also watch in this link, how it would match against some sample inputs.

Simple way to create matrix of random numbers

user2173836

[Simple way to create matrix of random numbers](https://stackoverflow.com/questions/15451958/simple-way-to-create-matrix-of-random-numbers)

I am trying to create a matrix of random numbers, but my solution is too long and looks uglythis looks ok, but in my implementation it iswhich is extremely unreadable and does not fit on one line.

2013-03-16 16:52:38Z

I am trying to create a matrix of random numbers, but my solution is too long and looks uglythis looks ok, but in my implementation it iswhich is extremely unreadable and does not fit on one line.Take a look at numpy.random.rand:You can drop the range(len()):But really, you should probably use numpy.use np.random.randint() as numpy.random.random_integers() is deprecated Looks like you are doing a Python implementation of the Coursera Machine Learning Neural Network exercise. Here's what I did for randInitializeWeights(L_in, L_out)First, create numpy array then convert it into matrix. See the code below:For random numbers out of 10. For out of 20 we have to multiply by 20.When you say "a matrix of random numbers", you can use numpy as Pavel https://stackoverflow.com/a/15451997/6169225 mentioned above, in this case I'm assuming to you it is irrelevant what distribution these (pseudo) random numbers adhere to.However, if you require a particular distribution (I imagine you are interested in the uniform distribution), numpy.random has very useful methods for you. For example, let's say you want a 3x2 matrix with a pseudo random uniform distribution bounded by [low,high]. You can do this like so:Note, you can replace uniform by any number of distributions supported by this library.Further reading: https://docs.scipy.org/doc/numpy/reference/routines.random.htmlA simple way of creating an array of random integers is:The following outputs a 2 by 3 matrix of random integers from 0 to 10:An answer using map-reduce:-

How do I convert a list into a string with spaces in Python?

user1653402

[How do I convert a list into a string with spaces in Python?](https://stackoverflow.com/questions/12309976/how-do-i-convert-a-list-into-a-string-with-spaces-in-python)

How can I convert a list into a space-separated string in Python?For example, I want to convert this list:Into the string "how are you" The spaces are important. I don't want to get howareyou as I have with my attempt so far of using

2012-09-06 23:50:35Z

How can I convert a list into a space-separated string in Python?For example, I want to convert this list:Into the string "how are you" The spaces are important. I don't want to get howareyou as I have with my attempt so far of usingyou need to join with a space not an empty string ...I'll throw this in as an alternative just for the heck of it, even though it's pretty much useless when compared to " ".join(my_list) for strings. For non-strings (such as an array of ints) this may be better:For Non String list we can do like this as wellSo in order to achieve a desired output, we should first know how the function works.The syntax for join() method as described in the python documentation is as follows:string_name.join(iterable)Things to be noted:Now, to add white spaces, we just need to replace the string_name with a " " or a ' ' both of them will work and place the iterable that we want to concatenate.So, our function will look something like this:But, what if we want to add particular number of white spaces in between our elements in the iterable ?we just need to do this:here the number will be a user input.So, for example if number=4.Then, the output of str(4*" ").join(my_list) will be how    are    you, so in between every word there are 4 white spaces.Why don't you add a space in the items of the list itself, like :

list = ["how ", "are ", "you "]

How should I declare default values for instance variables in Python?

int3

[How should I declare default values for instance variables in Python?](https://stackoverflow.com/questions/2681243/how-should-i-declare-default-values-for-instance-variables-in-python)

Should I give my class members default values like this:or like this?In this question I discovered that in both cases,is a well-defined operation.I understand that the first method will give me a class variable while the second one will not. However, if I do not require a class variable, but only need to set a default value for my instance variables, are both methods equally good? Or one of them more 'pythonic' than the other?One thing I've noticed is that in the Django tutorial, they use the second method to declare Models. Personally I think the second method is more elegant, but I'd like to know what the 'standard' way is.

2010-04-21 08:11:40Z

Should I give my class members default values like this:or like this?In this question I discovered that in both cases,is a well-defined operation.I understand that the first method will give me a class variable while the second one will not. However, if I do not require a class variable, but only need to set a default value for my instance variables, are both methods equally good? Or one of them more 'pythonic' than the other?One thing I've noticed is that in the Django tutorial, they use the second method to declare Models. Personally I think the second method is more elegant, but I'd like to know what the 'standard' way is.Extending bp's answer, I wanted to show you what he meant by immutable types.First, this is okay:However, this only works for immutable (unchangable) types. If the default value was mutable (meaning it can be replaced), this would happen instead:Note that both a and b have a shared attribute. This is often unwanted.This is the Pythonic way of defining default values for instance variables, when the type is mutable:The reason my first snippet of code works is because, with immutable types, Python creates a new instance of it whenever you want one. If you needed to add 1 to 1, Python makes a new 2 for you, because the old 1 cannot be changed. The reason is mostly for hashing, I believe.The two snippets do different things, so it's not a matter of taste but a matter of what's the right behaviour in your context. Python documentation explains the difference, but here are some examples:This binds num to the Foo instances. Change to this field is not propagated to other instances.Thus:This binds num to the Bar class. Changes are propagated!The code in exhibit B is plain wrong for this: why would you want to bind a class attribute (default value on instance creation) to the single instance?The code in exhibit A is okay.If you want to give defaults for instance variables in your constructor I would however do this:...or even:...or even: (preferrable, but if and only if you are dealing with immutable types!)This way you can do:Using class members to give default values works very well just so long as you are careful only to do it with immutable values. If you try to do it with a list or a dict that would be pretty deadly. It also works where the instance attribute is a reference to a class just so long as the default value is None.I've seen this technique used very successfully in repoze which is a framework that runs on top of Zope. The advantage here is not just that when your class is persisted to the database only the non-default attributes need to be saved, but also when you need to add a new field into the schema all the existing objects see the new field with its default value without any need to actually change the stored data.I find it also works well in more general coding, but it's a style thing. Use whatever you are happiest with.Using class members for default values of instance variables is not a good idea, and it's the first time I've seen this idea mentioned at all. It works in your example, but it may fail in a lot of cases. E.g., if the value is mutable, mutating it on an unmodified instance will alter the default:You can also declare class variables as None which will prevent propagation. This is useful when you need a well defined class and want to prevent AttributeErrors.

For example:Also if you need defaults:Of course still follow the info in the other answers about using mutable vs immutable types as the default in __init__.

Python can't find module in the same folder

Philipp_Kats

[Python can't find module in the same folder](https://stackoverflow.com/questions/24722212/python-cant-find-module-in-the-same-folder)

My python somehow can't find any modules in the same directory.

What am I doing wrong? (python2.7)So I have one directory '2014_07_13_test', with two files in it: where hello.py:and test.py:Still python gives me What's wrong?

2014-07-13 11:33:35Z

My python somehow can't find any modules in the same directory.

What am I doing wrong? (python2.7)So I have one directory '2014_07_13_test', with two files in it: where hello.py:and test.py:Still python gives me What's wrong?Your code is fine, I suspect your problem is how you are launching it.You need to launch python from your '2014_07_13_test' directory.Open up a command prompt and 'cd' into your '2014_07_13_test' directory.For instance:If you cannot 'cd' into the directory like this you can add it to sys.path In test.py:Or set/edit the PYTHONPATHAnd all should be well......well there is a slight mistake with your 'shebang' lines (the first line in both your files), there shouldn't be a space between the '#' and the '!'There is a better shebang you should use.Also you don't need the shebang line on every file... only the ones you intend to run from your shell as executable files.Change your import in test.py to:I had a similar problem, I solved it by explicitly adding the file's directory to the path list:After that, I had no problem importing from the same directory.Here is the generic solution I use. It solves the problem for importing from modules in the same folder: Put this at top of the module which gives the error "No module named xxxx"In my case, Python was unable to find it because I'd put the code inside a module with hyphens, e.g. my-module. When I changed it to my_module it worked.I ran into this issue. I had three folders in the same directory so I had to specify which folder.

        Ex: from Folder import script

Preserve case in ConfigParser?

pojo

[Preserve case in ConfigParser?](https://stackoverflow.com/questions/1611799/preserve-case-in-configparser)

I have tried to use Python's ConfigParser module to save settings. For my app it's important that I preserve the case of each name in my sections. The docs mention that passing str() to ConfigParser.optionxform() would accomplish this, but it doesn't work for me. The names are all lowercase. Am I missing something?Python pseudocode of what I get:

2009-10-23 07:03:53Z

I have tried to use Python's ConfigParser module to save settings. For my app it's important that I preserve the case of each name in my sections. The docs mention that passing str() to ConfigParser.optionxform() would accomplish this, but it doesn't work for me. The names are all lowercase. Am I missing something?Python pseudocode of what I get:The documentation is confusing. What they mean is this:I.e. override optionxform, instead of calling it; overriding can be done in a subclass or in the instance. When overriding, set it to a function (rather than the result of calling a function).I have now reported this as a bug, and it has since been fixed.For me worked to set optionxform immediately after creating the object Add to your code:I know this question is answered, but I thought some people might find this solution useful. This is a class that can easily replace the existing ConfigParser class.Edited to incorporate @OozeMeister's suggestion:Usage is the same as normal ConfigParser.This is so you avoid having to set optionxform every time you make a new ConfigParser, which is kind of tedious.Caveat:If you use defaults with ConfigParser, i.e.:and then try to make the parser case-sensitive by using this:all your options from config file(s) will keep their case, but FOO_BAZ will be converted to lowercase.To have defaults also keep their case, use subclassing like in @icedtrees answer:Now FOO_BAZ will keep it's case and you won't have InterpolationMissingOptionError.

gnuplot vs Matplotlib

Ethan Heilman

[gnuplot vs Matplotlib](https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib)

I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the 

collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?My general considerations are:How would you approach this task?

2009-05-26 16:49:02Z

I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the 

collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?My general considerations are:How would you approach this task?I know this post is old and answered but I was passing by and wanted to put my two cents. Here is my conclusion: if you have a not-so-big data set, you should use Matplotlib. It's easier and looks better. However, if you really need performance, you could use Gnuplot.  I've added some code to test it out on your machine and see for yourself if it makes a real difference (this is not a real performance benchmark but should give a first idea).The following graph represents the required time (in seconds) to:Configuration:I remember the performance gap being much wider when running on an older computer with older versions of the libraries (~30 seconds difference for a large scatter plot). Moreover, as mentionned in the comments, you can get equivalent quality of plots. But you will have to put more sweat into that to do it with Gnuplot.Here's the code to generate the graph if you want to give it a try on your machine:matplotlib has pretty good documentation, and seems to be quite stable. The plots it produces are beautiful - "publication quality" for sure. Due to the good documentation and the amount of example code available online, it's easy to learn and use, and I don't think you'll have much trouble translating gnuplot code to it. After all, matplotlib is being used by scientists to plot data and prepare reports - so it includes everything one needs.One marked advantage of matplotlib is that you can integrate it with Python GUIs (wxPython and PyQt, at least) and create GUI application with nice plots.After using GNUplot (with my own Python wrapper) for a long time (and really not liking the 80s-looking output), I just started having a look at matplotlib. I must say I like it very much, the output looks really nice and the docs are high quality and extensive (although that also goes for GNUplot). The one thing I spent ages looking for in the matplotlib docs is how to write to an image file rather than to the screen! Luckily this page explains it pretty well: http://www.dalkescientific.com/writings/diary/archive/2005/04/23/matplotlib_without_gui.htmlI have played with both, and I like Matplotlib much better in terms of Python integration, options, and quality of graphs/plots.About performance and plotting a great number of points: I compared this for a scatterplot of 500.000 points loaded from a text file and saved to a png, using gnuplot* and matplotlib.I ran it only once and the results don't look identical, but I think the idea is clear: gnuplot wins at performance.*I used gnuplot directly since the gnuplotpy demo doesn't work out-of-the-box for me. Matplotlib wins at Python integration.What Gnuplot can do Gnuplot-Py can do too. Because Gnuplot can be driven by pipe(pgnuplot).

Gnuplot-Py is just a thin layer for it. So you don't need worry about it.Why I prefer gnuplot maybe the many output format(PDF, PS and LaTex), which is very useful in papers, and the default output looks more scientific-style :)Some pro's of gnuplot (I still don't like matlibplot after years of usage):

How can I get around declaring an unused variable in a for loop?

Ramy

[How can I get around declaring an unused variable in a for loop?](https://stackoverflow.com/questions/5477134/how-can-i-get-around-declaring-an-unused-variable-in-a-for-loop)

If I have a list comprehension (for example) like this:Effectively making a new list that has an empty string for every element in a list, I never use the x. Is there a cleaner way of writing this so I don't have to declare the unused x variable?

2011-03-29 18:26:00Z

If I have a list comprehension (for example) like this:Effectively making a new list that has an empty string for every element in a list, I never use the x. Is there a cleaner way of writing this so I don't have to declare the unused x variable?_ is a standard placeholder name for ignored members in a for-loop and tuple assignment, e.g.BTW your list could be written without list comprehension (assuming you want to make a list of  immutable members like strings, integers etc.).No. As the Zen puts it: Special cases aren't special enough to break the rules. The special case being loops not using the items of the thing being iterated and the rule being that there's a "target" to unpack to.You can, however, use _ as variable name, which is usually understood as "intentionally unused" (even  PyLint etc. knows and respect this).It turns out that using dummy* (starting word is dummy) as the variable name does the same trick as _. _ is a known standard and it would be better to use meaningful variable names. So you can use dummy, dummy1, dummy_anything. By using these variable names PyLint won't complain.If you need to name your arguments (in case, for example, when writing mocks that don't use certain arguments that are referenced by name), you can add this shortcut method:and then use it like thisAdd the following comment after the for loop on the same line:#pylint: disable=unused-variableThe generator objects don't actually use the variables. So something likeshould do the trick. Note that x is not defined as a variable outside of the generator comprehension.You can also prepend a variable name with _ if you prefer giving the variable a human readable name. For example you can use _foo, _foo1, _anything and PyLint won't complain. In a for loop, it would be like:edit: Add exampleComment to How can I get around declaring an unused variable in a for loop? (Ran out of comment size)Python maintains the same reference for the object created. (irrespective of mutability),for example,You, can see both i and j, refer to the same object in memory.What happens, when we change the value of one immutable variable.you can see j now starts to point a new location, (where 2 is stored), and i still points to location where 1 is stored.

While evaluating, The value is picked from 142671248, calculated(if not already cached), and put at a new location 142671236. j is made to point to

the new location. In simpler terms a new copy made everytime an immutable variable is modified.Mutable objects act little different in this regard. When the value pointed byBoth a and b point to the same memory location.Memory location pointed by a is modified.Both a and b, still point to the same memory location. In other word, mutable variables act of the same memory location pointed by the variable, instead of making a copy of the value pointed by the variable, like in immutable variable case.

„ÄåLine contains NULL byte„Äçin CSV reader (Python)

James Roseman

[„ÄåLine contains NULL byte„Äçin CSV reader (Python)](https://stackoverflow.com/questions/7894856/line-contains-null-byte-in-csv-reader-python)

I'm trying to write a program that looks at a .CSV file (input.csv) and rewrites only the rows that begin with a certain element (corrected.csv), as listed in a text file (output.txt).This is what my program looks like right now:Unfortunately, I keep getting this error, and I have no clue what it's about.Credit to all the people here to even to get me to this point.

2011-10-25 19:39:15Z

I'm trying to write a program that looks at a .CSV file (input.csv) and rewrites only the rows that begin with a certain element (corrected.csv), as listed in a text file (output.txt).This is what my program looks like right now:Unfortunately, I keep getting this error, and I have no clue what it's about.Credit to all the people here to even to get me to this point.I've solved a similar problem with an easier solution:The key was using the codecs module to open the file with the UTF-16 encoding, there are a lot more of encodings, check the documentation.I'm guessing you have a NUL byte in input.csv.  You can test that withif you do,may get you around that.  Or it may indicate you have utf16 or something 'interesting' in the .csv file.You could just inline a generator to filter out the null values if you want to pretend they don't exist.  Of course this is assuming the null bytes are not really part of the encoding and really are some kind of erroneous artifact or bug.See the (line.replace('\0','') for line in f) below, also you'll want to probably open that file up using mode rb.If you want to replace the nulls with something you can do this:This will tell you what line is the problem.Perhaps this from daniweb would be helpful:...A tricky way:If you develop under Lunux, you can use all the power of sed:The most efficient solution for huge files.Checked for Python3, KubuntuI've recently fixed this issue and in my instance it was a file that was compressed that I was trying to read. Check the file format first. Then check that the contents are what the extension refers to. Turning my linux environment into a clean complete UTF-8 environment made the trick for me.

Try the following in your command line:This is long settled, but I ran across this answer because I was experiencing an unexpected error while reading a CSV to process as training data in Keras and TensorFlow.In my case, the issue was much simpler, and is worth being conscious of.  The data being produced into the CSV wasn't consistent, resulting in some columns being completely missing, which seems to end up throwing this error as well.The lesson: If you're seeing this error, verify that your data looks the way that you think it does!I've removed NULL bytes and solved this problem using a one-liner command from terminal.pandas.read_csv now handles the different UTF encoding when reading/writing and therefore can deal directly with null bytessee https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

hash function in Python 3.3 returns different results between sessions

redlus

[hash function in Python 3.3 returns different results between sessions](https://stackoverflow.com/questions/27522626/hash-function-in-python-3-3-returns-different-results-between-sessions)

I've implemented a BloomFilter in python 3.3, and got different results every session. Drilling down this weird behavior got me to the internal hash() function - it returns different hash values for the same string every session.Example:----- opening a new python console -----Why is this happening?

Why is this useful?

2014-12-17 09:48:17Z

I've implemented a BloomFilter in python 3.3, and got different results every session. Drilling down this weird behavior got me to the internal hash() function - it returns different hash values for the same string every session.Example:----- opening a new python console -----Why is this happening?

Why is this useful?Python uses a random hash seed to prevent attackers from tar-pitting your application by sending you keys designed to collide. See the original vulnerability disclosure. By offsetting the hash with a random seed (set once at startup) attackers can no longer predict what keys will collide.You can set a fixed seed or disable the feature by setting the PYTHONHASHSEED environment variable; the default is random but you can set it to a fixed positive integer value, with 0 disabling the feature altogether.Python versions 2.7 and 3.2 have the feature disabled by default (use the -R switch or set PYTHONHASHSEED=random to enable it); it is enabled by default in Python 3.3 and up.If you were relying on the order of keys in a Python dictionary or set, then don't. Python uses a hash table to implement these types and their order depends on the insertion and deletion history as well as the random hash seed.Also see the object.__hash__() special method documentation:If you need a stable hash implementation, you probably want to look at the hashlib module; this implements cryptographic hash functions. The pybloom project uses this approach.Since the offset consists of a prefix and a suffix (start value and final XORed value, respectively)  you cannot just store the offset, unfortunately. On the plus side, this does mean that attackers cannot easily determine the offset with timing attacks either.    Hash randomisation is turned on by default in Python 3. This is a security feature:In previous versions from 2.6.8, you could switch it on at the command line with -R, or the PYTHONHASHSEED environment option.You can switch it off by setting PYTHONHASHSEED to zero.hash() is a Python built-in function and use it to calculate a hash value for object, not for string or num.You can see the detail in this page: https://docs.python.org/3.3/library/functions.html#hash.and hash() values comes from the object's __hash__ method.

The doc says the followings:That's why your have diffent hash value for the same string in different console.What you implement is not a good way.When you want to calculate a string hash value, just use hashlibhash() is aim to get a object hash value, not a stirng.

Is it pythonic for a function to return multiple values?

Readonly

[Is it pythonic for a function to return multiple values?](https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values)

In python, you can have a function return multiple values.  Here's a contrived example:This seems very useful, but it looks like it can also be abused ("Well..function X already computes what we need as an intermediate value.  Let's have X return that value also").When should you draw the line and define a different method?  

2008-09-14 20:15:19Z

In python, you can have a function return multiple values.  Here's a contrived example:This seems very useful, but it looks like it can also be abused ("Well..function X already computes what we need as an intermediate value.  Let's have X return that value also").When should you draw the line and define a different method?  Absolutely (for the example you provided).There is a builtin function divmod() that does exactly that.There are other examples: zip, enumerate, dict.items. BTW, parentheses are not necessary most of the time.

Citation from Python Library Reference: Therefore they should return a single object. In your case this object is a tuple. Consider tuple as an ad-hoc compound data structure. There are languages where almost every single function returns multiple values (list in Lisp).Sometimes it is sufficient to return (x, y) instead of Point(x, y).With the introduction of named tuples in Python 2.6 it is preferable in many cases to return named tuples instead of plain tuples.Firstly, note that Python allows for the following (no need for the parenthesis):Regarding your question, there's no hard and fast rule either way. For simple (and usually contrived) examples, it may seem that it's always possible for a given function to have a single purpose, resulting in a single value. However, when using Python for real-world applications, you quickly run into many cases where returning multiple values is necessary, and results in cleaner code.So, I'd say do whatever makes sense, and don't try to conform to an artificial convention. Python supports multiple return values, so use it when appropriate.The example you give is actually a python builtin function, called divmod. So someone, at some point in time, thought that it was pythonic enough to include in the core functionality.To me, if it makes the code cleaner, it is pythonic. Compare these two code blocks:Yes, returning multiple values (i.e., a tuple) is definitely pythonic.  As others have pointed out, there are plenty of examples in the Python standard library, as well as in well-respected Python projects.  Two additional comments:It's definitely pythonic. The fact that you can return multiple values from a function the boilerplate you would have in a language like C where you need to define a struct for every combination of types you return somewhere.However, if you reach the point where you are returning something crazy like 10 values from a single function, you should seriously consider bundling them in a class because at that point it gets unwieldy.Returning a tuple is cool. Also note the new namedtuple

which was added in python 2.6 which may make this more palatable for you:

http://docs.python.org/dev/library/collections.html#collections.namedtupleOT: RSRE's Algol68 has the curious "/:=" operator. eg.Giving a quotient of 3, and a remainder of 16. Note: typically the value of "(x/:=y)" is discarded as quotient "x" is assigned by reference, but in RSRE's case the returned value is the remainder.c.f. Integer Arithmetic - Algol68It's fine to return multiple values using a tuple for simple functions such as divmod. If it makes the code readable, it's Pythonic.If the return value starts to become confusing, check whether the function is doing too much and split it if it is. If a big tuple is being used like an object, make it an object. Also, consider using named tuples, which will be part of the standard library in Python 2.6.I'm fairly new to Python, but the tuple technique seems very pythonic to me.  However, I've had another idea that may enhance readability.  Using a dictionary allows access to the different values by name rather than position.  For example:

Multi Index Sorting in Pandas

MattB

[Multi Index Sorting in Pandas](https://stackoverflow.com/questions/14733871/multi-index-sorting-in-pandas)

I have a dataset with multi-index columns in a pandas df that I would like to sort by values in a specific column.  I have tried using sortindex and sortlevel but haven't been able get the results I am looking for.  My dataset looks like:I want to sort all data and the index by column C in Group 1 in descending order so my results look like:Is it possible to do this sort with the structure that my data is in, or should I be swapping Group1 to the index side?

2013-02-06 16:24:31Z

I have a dataset with multi-index columns in a pandas df that I would like to sort by values in a specific column.  I have tried using sortindex and sortlevel but haven't been able get the results I am looking for.  My dataset looks like:I want to sort all data and the index by column C in Group 1 in descending order so my results look like:Is it possible to do this sort with the structure that my data is in, or should I be swapping Group1 to the index side?When sorting by a MultiIndex you need to contain the tuple describing the column inside a list*:* so as not to confuse pandas into thinking you want to sort first by Group1 then by C.Note: Originally used .sort since deprecated then removed in 0.20, in favor of .sort_values.

Why is bool a subclass of int?

ThiefMaster

[Why is bool a subclass of int?](https://stackoverflow.com/questions/8169001/why-is-bool-a-subclass-of-int)

When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where isinstance(val, int) is checked to flag the value as an integer.So I tested it in the python shell and noticed the following:But why exactly is bool a subclass of int?It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....

2011-11-17 14:43:36Z

When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where isinstance(val, int) is checked to flag the value as an integer.So I tested it in the python shell and noticed the following:But why exactly is bool a subclass of int?It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....From a comment on http://www.peterbe.com/plog/bool-is-intCredit goes to dman13 for this nice explanation.See PEP 285 -- Adding a bool type.  Relevent passage:Can also use help to check the Bool's value in Console:help(True)help(False)

Determine complete Django url configuration

Michael

