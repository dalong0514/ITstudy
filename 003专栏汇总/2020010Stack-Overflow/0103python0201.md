Credit goes to Robert William Hanks for primesfrom2to, primesfrom3to, rwh_primes, rwh_primes1, and rwh_primes2.Of the plain Python methods tested, with psyco, for n=1000000,

rwh_primes1 was the fastest tested.Of the plain Python methods tested, without psyco, for n=1000000,

rwh_primes2 was the fastest.Of all the methods tested, allowing numpy, for n=1000000,

primesfrom2to was the fastest tested.Timings were measured using the command:with {method} replaced by each of the method names.primes.py:Running the script tests that all implementations give the same result.Faster & more memory-wise pure Python code:or starting with half sieveFaster & more memory-wise numpy code:a faster variation starting with a third of a sieve:A (hard-to-code) pure-python version of the above code would be:Unfortunately pure-python don't adopt the simpler and faster numpy way of doing assignment, and calling len() inside the loop as in [False]*len(sieve[((k*k)//3)::2*k]) is too slow. So I had to improvise to correct input (& avoid more math) and do some extreme (& painful) math-magic.  Personally I think it is a shame that numpy (which is so widely used) is not part of Python standard library, and that the improvements in syntax and speed seem to be completely overlooked by Python developers.There's a pretty neat sample from the Python Cookbook here -- the fastest version proposed on that URL is:so that would giveMeasuring at the shell prompt (as I prefer to do) with this code in pri.py, I observe:so it looks like the Cookbook solution is over twice as fast.Using Sundaram's Sieve, I think I broke pure-Python's record:Comparasion:The algorithm is fast, but it has a serious flaw:You assume that numbers.pop() would return the smallest number in the set, but this is not guaranteed at all. Sets are unordered and pop() removes and returns an arbitrary element, so it cannot be used to select the next prime from the remaining numbers.For truly fastest solution with sufficiently large N would be to download a pre-calculated list of primes, store it as a tuple and do something like:If N > primes[-1] only then calculate more primes and save the new list in your code, so next time it is equally as fast.Always think outside the box.If you don't want to reinvent the wheel, you can install the symbolic maths library sympy (yes it's Python 3 compatible)And use the primerange functionIf you accept itertools but not numpy, here is an adaptation of rwh_primes2 for Python 3 that runs about twice as fast on my machine.  The only substantial change is using a bytearray instead of a list for the boolean, and using compress instead of a list comprehension to build the final list.  (I'd add this as a comment like moarningsun if I were able.)Comparisons:andIt's instructive to write your own prime finding code, but it's also useful to have a fast reliable library at hand. I wrote a wrapper around the C++ library primesieve, named it primesieve-pythonTry it pip install primesieveI'd be curious to see the speed compared.Here is two updated (pure Python 3.6) versions of one of the fastest functions,A deterministic implementation of Miller-Rabin's Primality test on the assumption that N < 9,080,191According to the article on Wikipedia (http://en.wikipedia.org/wiki/Miller–Rabin_primality_test) testing N < 9,080,191 for a = 2,3,37, and 73 is enough to decide whether N is composite or not.And I adapted the source code from the probabilistic implementation of original Miller-Rabin's test found here: http://en.literateprograms.org/Miller-Rabin_primality_test_(Python)If you have control over N, the very fastest way to list all primes is to precompute them. Seriously. Precomputing is a way overlooked optimization.Here's the code I normally use to generate primes in Python:It can't compete with the faster solutions posted here, but at least it is pure python.Thanks for posting this question. I really learnt a lot today. For the fastest code, the numpy solution is the best. For purely academic reasons, though, I'm posting my pure python version, which is a bit less than 50% faster than the cookbook version posted above. Since I make the entire list in memory, you need enough space to hold everything, but it seems to scale fairly well.And the results:A slightly different implementation of a half sieve using Numpy:http://rebrained.com/?p=458Can someone compare this with the other timings?  On my machine it seems pretty comparable to the other Numpy half-sieve.It's all written and tested. So there is no need to reinvent the wheel.gives us a record breaking 12.2 msec!If this is not fast enough, you can try PyPy:which results in:The answer with 247 up-votes lists 15.9 ms for the best solution.

Compare this!!!I tested  some unutbu's functions, i computed it with hungred millions numberThe winners are the functions that use numpy library, Note: It would also interesting make a memory utilization test :)Sample codeComplete code on my github repositoryFor Python 3Fastest prime sieve in Pure Python:I optimised Sieve of Eratosthenes for speed and memory. OutputFirst time using python, so some of the methods I use in this might seem a bit cumbersome. I just straight converted my c++ code to python and this is what I have (albeit a tad bit slowww in python)I know the competition is closed for some years. …Nonetheless this is my suggestion for a pure python prime sieve, based on omitting the multiples of 2, 3 and 5 by using appropriate steps while processing the sieve forward. Nonetheless it is actually slower for N<10^9 than @Robert William Hanks superior solutions rwh_primes2 and rwh_primes1. By using a ctypes.c_ushort sieve array above 1.5* 10^8 it is somehow adaptive to memory limits.10^6$ python -mtimeit -s"import primeSieveSpeedComp" "primeSieveSpeedComp.primeSieveSeq(1000000)"

10 loops, best of 3: 46.7 msec per loop10^7$ python -mtimeit -s"import primeSieveSpeedComp" "primeSieveSpeedComp.primeSieveSeq(10000000)"

10 loops, best of 3: 530 msec per loop10^8$ python -mtimeit -s"import primeSieveSpeedComp" "primeSieveSpeedComp.primeSieveSeq(100000000)"

10 loops, best of 3: 5.55 sec per loop10^9$ python -mtimeit -s"import primeSieveSpeedComp" "primeSieveSpeedComp.primeSieveSeq(1000000000)"

10 loops, best of 3: 61.2 sec per loopYou may copy the code below into ubuntus primeSieveSpeedComp to review this tests.Here is a numpy version of Sieve of Eratosthenes having both good complexity (lower than sorting an array of length n) and vectorization.  Compared to @unutbu times this just as fast as the packages with 46 microsecons to find all primes below a million. Timings: I've updated much of the code for Python 3 and threw it at perfplot (a project of mine) to see which is actually fastest. Turns out that, for large n, primesfrom{2,3}to take the cake:Code to reproduce the plot:My guess is that the fastest of all ways is to hard code the primes in your code.So why not just write a slow script that generates another source file that has all numbers hardwired in it, and then import that source file when you run your actual program.Of course, this works only if you know the upper bound of N at compile time, but thus is the case for (almost) all project Euler problems.  PS: I might be wrong though iff parsing the source with hard-wired primes is slower than computing them in the first place, but as far I know Python runs from compiled .pyc files so reading a binary array with all primes up to N should be bloody fast in that case.Sorry to bother but erat2() has a serious flaw in the algorithm.While searching for the next composite, we need to test odd numbers only.

q,p both are odd; then q+p is even and doesn't need to be tested, but q+2*p is always odd. This eliminates the "if even" test in the while loop condition and saves about 30% of the runtime.While we're at it: instead of the elegant 'D.pop(q,None)' get and delete method use 'if q in D: p=D[q],del D[q]' which is twice as fast! At least on my machine (P3-1Ghz).

So I suggest this implementation of this clever algorithm:The fastest method I've tried so far is based on the Python cookbook erat2 function:See this answer for an explanation of the speeding-up.I may be late to the party but will have to add my own code for this. It uses approximately n/2 in space because we don't need to store even numbers and I also make use of the bitarray python module, further draStically cutting down on memory consumption and enabling computing all primes up to 1,000,000,000This was run on a 64bit 2.4GHZ MAC OSX 10.8.3I collected several prime number sieves over time. The fastest on my computer is this:I'm slow responding to this question but it seemed like a fun exercise. I'm using numpy which might be cheating and I doubt this method is the fastest but it should be clear. It sieves a Boolean array referring to its indices only and elicits prime numbers from the indices of all True values. No modulo needed.Here is an interesting technique to generate prime numbers (yet not the most efficient) using python's list comprehensions:You can find the example and some explanations right here

Iterating through a range of dates in Python

ShawnMilo

[Iterating through a range of dates in Python](https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python)

I have the following code to do this, but how can I do it better? Right now I think it's better than nested loops, but it starts to get Perl-one-linerish when you have a generator in a list comprehension. For a start date of 2009-05-30 and an end date of 2009-06-09:

2009-06-29 20:16:02Z

I have the following code to do this, but how can I do it better? Right now I think it's better than nested loops, but it starts to get Perl-one-linerish when you have a generator in a list comprehension. For a start date of 2009-05-30 and an end date of 2009-06-09:Why are there two nested iterations? For me it produces the same list of data with only one iteration:And no list gets stored, only one generator is iterated over. Also the "if" in the generator seems to be unnecessary.After all, a linear sequence should only require one iterator, not two.Maybe the most elegant solution is using a generator function to completely hide/abstract the iteration over the range of dates:NB: For consistency with the built-in range() function this iteration stops before reaching the end_date. So for inclusive iteration use the next day, as you would with range().This might be more clear:Use the dateutil library:This python library has many more advanced features, some very useful, like relative deltas—and is implemented as a single file (module) that's easily included into a project.Pandas is great for time series in general, and has direct support for date ranges.You can then loop over the daterange to print the date:It also has lots of options to make life easier. For example if you only wanted weekdays, you would just swap in bdate_range. See http://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestampsThe power of Pandas is really its dataframes, which support vectorized operations (much like numpy) that make operations across large quantities of data very fast and easy.EDIT:

You could also completely skip the for loop and just print it directly, which is easier and more efficient:This function does more than you strictly require, by supporting negative step, etc. As long as you factor out your range logic, then you don't need the separate day_count and most importantly the code becomes easier to read as you call the function from multiple places.Why not try:This is the most human-readable solution I can think of.Show the last n days from today:Output: Numpy's arange function can be applied to dates:The use of astype is to convert from numpy.datetime64 to an array of datetime.datetime objects.I have a similar problem, but I need to iterate monthly instead of daily.This is my solutionExample #1OutputExample #2OutputCan't* believe this question has existed for 9 years without anyone suggesting a simple recursive function:Output:Edit: *Now I can believe it -- see Does Python optimize tail recursion? .  Thank you Tim.Here's code for a general date range function, similar to Ber's answer, but more flexible:You can generate a series of date between two dates using the pandas library simply and trustfullyYou can change the frequency of generating dates by setting freq as  D, M, Q, Y

(daily, monthly, quarterly, yearly

)For completeness, pandas also has a period_range function for timestamps that are out of bounds:pd.period_range(start='1/1/1626', end='1/08/1627', freq='D')What about the following for doing a range incremented by days:For a generic version:Note that .total_seconds() is only supported after python 2.7 If you are stuck with an earlier version you can write your own function:This function has some extra features:Slightly different approach to reversible steps by storing range args in a tuple. 

Print a list in reverse order with range()?

ramesh.mimit

[Print a list in reverse order with range()?](https://stackoverflow.com/questions/7286365/print-a-list-in-reverse-order-with-range)

How can you produce the following list with range() in Python?

2011-09-02 16:12:58Z

How can you produce the following list with range() in Python?use reversed() function:It's much more meaningful. Update:If you want it to be a list (as btk pointed out):Update:If you want to use only range to achieve the same result, you can use all its parameters. range(start, stop, step)For example, to generate a list [5,4,3,2,1,0], you can use the following:It may be less intuitive but as the comments mention, this is more efficient and the right usage of range for reversed list.Use the 'range' built-in function. The signature is range(start, stop, step).  This produces a sequence that yields numbers, starting with start, and ending if stop has been reached, excluding stop.In Python 3, this produces a non-list range object, which functions effectively like a read-only list (but uses way less memory, particularly for large ranges).You could userange(10)[::-1]which is the same thing asrange(9, -1, -1)and arguably more readable (if you're familiar with the commonsequence[::-1]Python idiom).For those who are interested in the "efficiency" of the options collected so far...Jaime RGP's answer led me to restart my computer after timing the somewhat "challenging" solution of Jason literally following my own suggestion (via comment). To spare the curious of you the downtime, I present here my results (worst-first):Jason's answer (maybe just an excursion into the power of list comprehension):martineau's answer (readable if you are familiar with the extended slices syntax):Michał Šrajer's answer (the accepted one, very readable):bene's answer (the very first, but very sketchy at that time):The last option is easy to remember using the range(n-1,-1,-1) notation by Val Neekman.will solve this problem. It will output 8 to 1, and -1 means a reversed listWhen you have iteration over n items and want to replace order of list returned by range(start, stop, step) you have to use third parameter of range which identifies step and set it to -1, other parameters shall be adjusted accordingly:So equivalent of range(n) in reverse order would be:Readibility aside, reversed(range(n)) seems to be faster than range(n)[::-1].Just if anyone was wondering :)The requirement in this question calls for a list of integers of size 10 in descending

order. So, let's produce a list in python.Very often asked question is whether range(9, -1, -1) better than reversed(range(10)) in Python 3? People who have worked in other languages with iterators immediately tend to think that reversed() must cache all values and then return in reverse order. Thing is that Python's reversed() operator doesn't work if the object is just an iterator. The object must have one of below two for reversed() to work:If you try to use reversed() on object that has none of above then you will get:So in short, Python's reversed() is only meant on array like objects and so it should have same performance as forward iteration.But what about range()? Isn't that a generator? In Python 3 it is generator but wrapped in a class that implements both of above. So range(100000) doesn't take up lot of memory but it still supports efficient indexing and reversing.So in summary, you can use reversed(range(10)) without any hit on performance.You can do printing of reverse numbers with range() BIF Like , Output will be 

[10,9,8,7,6,5,4,3,2,1] range() - range ( start , end , increment/decrement ) 

where start is inclusive , end is exclusive and increment can be any numbers and behaves like step i believe this can help,below is Usage:Using without [::-1] or reversed -You don't necessarily need to use the range function, you can simply do list[::-1] which should return the list in reversed order swiftly, without using any additions.Suppose you have a list call it 

a={1,2,3,4,5}

Now if you want to print the list in reverse then simply use the following code.I know you asked using range but its already answered.Is the correct form. If you useyou wont get a 0 case. For instance, say your 10 isn't a magic number and a variable you're using to lookup start from reverse. If your n case is 0, reversed(range(0)) will not execute which is wrong if you by chance have a single object in the zero index.

How to read a file without newlines?

Yotam

[How to read a file without newlines?](https://stackoverflow.com/questions/12330522/how-to-read-a-file-without-newlines)

In Python, callingresults in a list in which each element is a line in the file. It's a little stupid but still: readlines() also writes newline character to each element, something I do not wish to happen. How can I avoid it? 

2012-09-08 11:55:15Z

In Python, callingresults in a list in which each element is a line in the file. It's a little stupid but still: readlines() also writes newline character to each element, something I do not wish to happen. How can I avoid it? You can read the whole file and split lines using str.splitlines:Or you can strip the newline by hand:Note: this last solution only works if the file ends with a newline, otherwise the last line will lose a character.This assumption is true in most cases (especially for files created by text editors, which often do add an ending newline anyway).If you want to avoid this you can add a newline at the end of file:Or a simpler alternative is to strip the newline instead:Or even, although pretty unreadable:Which exploits the fact that the return value of or isn't a boolean, but the object that was evaluated true or false.The readlines method is actually equivalent to:Since readline() keeps the newline also readlines() keeps it.Note: for symmetry to readlines() the writelines() method does not add ending newlines, so f2.writelines(f.readlines()) produces an exact copy of f in f2.another example:Reading file one row at the time. Removing unwanted chars with from end of the string str.rstrip(chars)see also str.strip([chars]) and str.lstrip([chars])(python >= 2.0)I think this is the best option.Try this:To remove all leading and trailing whitespaces (inspired from Абага's Answer) -To remove all trailing whitspaces including the newline -If you just want to removes the trailing newline character and not the whitespaces -P.S. If you find it helpful then please upvote him as well.

How do I disable log messages from the Requests library?

aknuds1

[How do I disable log messages from the Requests library?](https://stackoverflow.com/questions/11029717/how-do-i-disable-log-messages-from-the-requests-library)

By default, the Requests python library writes log messages to the console, along the lines of:I'm usually not interested in these messages, and would like to disable them. What would be the best way to silence those messages or decrease Requests' verbosity?

2012-06-14 08:52:12Z

By default, the Requests python library writes log messages to the console, along the lines of:I'm usually not interested in these messages, and would like to disable them. What would be the best way to silence those messages or decrease Requests' verbosity?I found out how to configure requests's logging level, it's done via the standard logging module. I decided to configure it to not log messages unless they are at least warnings:If you wish to apply this setting for the urllib3 library (typically used by requests) too, add the following:In case you came here looking for a way to modify logging of any (possibly deeply nested) module, use logging.Logger.manager.loggerDict to get a dictionary of all of the logger objects. The returned names can then be used as the argument to logging.getLogger:Per user136036 in a comment, be aware that this method only shows you the loggers that exist at the time you run the above snippet. If, for example, a module creates a new logger when you instantiate a class, then you must put this snippet after creating the class in order to print its name.In this way all the messages of level=INFO from urllib3 won't be present in the logfile.So you can continue to use the level=INFO for your log messages...just modify this for the library you are using.Let me copy/paste the documentation section which it I wrote about week or two ago, after having a problem similar to yours:For anybody using logging.config.dictConfig you can alter the requests library log level in the dictionary like this:Setting the logger name as requests or requests.urllib3 did not work for me. I had to specify the exact logger name to change the logging level.First See which loggers you have defined, to see which ones you want to removeAnd you will see something like this:{...'urllib3.poolmanager': <logging.Logger object at 0x1070a6e10>, 'django.request': <logging.Logger object at 0x106d61290>, 'django.template': <logging.Logger object at 0x10630dcd0>, 'django.server': <logging.Logger object at 0x106dd6a50>, 'urllib3.connection': <logging.Logger object at 0x10710a350>,'urllib3.connectionpool': <logging.Logger object at 0x106e09690> ...}Then configure the level for the exact logger:If You have configuration file, You can configure it.Add urllib3 in loggers section:Add logger_urllib3 section:Kbrose's guidance on finding which logger was generating log messages was immensely useful. For my Django project, I had to sort through 120 different loggers until I found that it was the elasticsearch Python library that was causing issues for me. As per the guidance in most of the questions, I disabled it by adding this to my loggers:Posting here in case someone else is seeing the unhelpful log messages come through whenever they run an Elasticsearch query.This answer is here:  Python: how to suppress logging statements from third party libraries?You can leave the default logging level for basicConfig, and then you set the DEBUG level when you get the logger for your module.simple: just add requests.packages.urllib3.disable_warnings() after import requestsI'm not sure if the previous approaches have stopped working, but in any case, here's another way of removing the warnings:Basically, adding an environment variable in the context of the script execution.From the documentation: https://urllib3.readthedocs.org/en/latest/security.html#disabling-warnings

Creating a range of dates in Python

Thomas Browne

[Creating a range of dates in Python](https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python)

I want to create a list of dates, starting with today, and going back an arbitrary number of days, say, in my example 100 days. Is there a better way to do it than this?

2009-06-14 18:03:59Z

I want to create a list of dates, starting with today, and going back an arbitrary number of days, say, in my example 100 days. Is there a better way to do it than this?Marginally better...Pandas is great for time series in general, and has direct support for date ranges. For example pd.date_range():It also has lots of options to make life easier. For example if you only wanted weekdays, you would just swap in bdate_range.See date range documentationIn addition it fully supports pytz timezones and can smoothly span spring/autumn DST shifts.EDIT by OP:If you need actual python datetimes, as opposed to Pandas timestamps:This uses the "end" parameter to match the original question, but if you want descending dates:Get range of dates between specified start and end date (Optimized for time & space complexity):You can write a generator function that returns date objects starting from today:This generator returns dates starting from today and going backwards one day at a time. Here is how to take the first 3 dates:The advantage of this approach over a loop or list comprehension is that you can go back as many times as you want.EditA more compact version using a generator expression instead of a function:Usage:You can also use the day ordinal to make it simpler:Or as suggested in the comments you can create a list like this:yeah, reinvent the wheel....

just search the forum and you'll get something like this: From the title of this question I was expecting to find something like range(), that would let me specify two dates and create a list with all the dates in between. That way one does not need to calculate the number of days between those two dates, if one does not know it beforehand.So with the risk of being slightly off-topic, this one-liner does the job:All credits to this answer! Here's a slightly different answer building off of S.Lott's answer that gives a list of dates between two dates start and end. In the example below, from the start of 2017 to today.A bit of a late answer I know, but I just had the same problem and decided that Python's internal range function was a bit lacking in this respect so I've overridden it in a util module of mine.If there are two dates and you need the range try Based on answers I wrote for myself this:Output:The difference is that I get the 'date' object, not the 'datetime.datetime' one.Here is gist I created, from my own code, this might help. (I know the question is too old, but others can use it)https://gist.github.com/2287345(same thing below)Here's a one liner for bash scripts to get a list of weekdays, this is python 3. Easily modified for whatever, the int at the end is the number of days in the past you want. Here is a variant to provide a start (or rather, end) dateHere is a variant for arbitrary start and end dates. not that this isn't terribly efficient, but is good for putting in a for loop in a bash script:Matplotlib relatedI know this has been answered, but I'll put down my answer for historical purposes, and since I think it is straight forward.Sure it won't win anything like code-golf, but I think it is elegant.A monthly date range generator with datetime and dateutil. Simple and easy to understand:Another example that counts forwards or backwards, starting from Sandeep's answer.givesandgivesNote that the start date is included in the return, so if you want four total dates, use timedelta(days=3)From above answers i created this example for date generator

Tabs versus spaces in Python programming

Hannes Ovrén

[Tabs versus spaces in Python programming](https://stackoverflow.com/questions/119562/tabs-versus-spaces-in-python-programming)

I have always used tabs for indentation when I do Python programming. But then I came across a question here on SO where someone pointed out that most Python programmers use spaces instead of tabs to minimize editor-to-editor mistakes.How does that make a difference? Are there other reasons why one would use spaces instead of tabs for Python? Or is it simply not true?Should I switch my editor to insert spaces instead of tabs right away or keep on going like I used to?

2008-09-23 07:26:00Z

I have always used tabs for indentation when I do Python programming. But then I came across a question here on SO where someone pointed out that most Python programmers use spaces instead of tabs to minimize editor-to-editor mistakes.How does that make a difference? Are there other reasons why one would use spaces instead of tabs for Python? Or is it simply not true?Should I switch my editor to insert spaces instead of tabs right away or keep on going like I used to?Because PEP-8 tells us to use spaces.Tired of chasing after indentation typos ( 8 spaces ? no, 7 oops 9 ... ) I switched my sources to 'tabs only'.1 tab == 1 indent level, full stopThe point is: if you want to display the indentation as 4, 8 or pi / 12 character width, just change the settings in your text editor, don't mess with the code.(Personally, I use 4 char width tab... but some would prefer 3 or 8 space, or even use variable width fonts.)Thus spake the Lord: Thou shalt indent with four spaces. No more, no less. Four shall be the number of spaces thou shalt indent, and the number of thy indenting shall be four. Eight shalt thou not indent, nor either indent thou two, excepting that thou then proceed to four. Tabs are right out.  --  Georg BrandlUSE AN EDITOR THAT DISPLAYS TAB CHARACTERS (all whitespace, for that matter).  You're programming, not writing an article.I use tabs. There's no room for a one-space error in the tabs (if you can see them).  The problem IS that people use different editors, and the only common thing in the world is: tab==indent, as above.  Some bloke comes in with the tab key set to the wrong number of spaces or does it manually and makes a mess.  TABs and use a real editor.  (This isn't just contrary to the PEP, it's about C/C++ and other whitespace-agnostic languages too)./steps down from soapboxMy main reason for using tabs over spaces is the backspace key. If I'm on a line and I want to backspace-remove an indentation on just that one line I have to hit backspace 4x if it were spaces; whereas, I only need to hit it once if it's a tab.I will continue to use tabs because—like was stated before—it's easier to convert from tabs to spaces, but not the other way around.I'm thinking I want to write a simple program that converts code with spaces into code with tabs, because I freaking hate spaces. They drive me up the wall!Oh! And using the arrow keys to navigate left and right is always a pain in the ass when it's spaces.UPDATE: Sublime Text 3 now deletes a full soft tab with the backspace key; though, arrow-key navigation is still tedious.UPDATE: I now use vscode and also wrote a TabSanity extension for it to solve backspace, delete and arrow-key navigation.The most "pythonic" way is to use 4 spaces per indentation level. The Python interpreter will however recognize spaces or tabs. The only gottcha is you must never mix spaces and tabs, pick one or the other. That said, the specification recommends spaces, most developers use spaces, so unless you have a really good reason not to, I'd say go with spaces.So far as I can tell, here are the pros and cons of tabs vs spaces.Pros of tabs:Cons of tabs:There are some non-issues that are overblown by some people:I've started out using spaces to be consistent with other Python code, but to be honest it is frustrating enough that I will probably change back to tabs. A lot depends on the capabilities of your IDE, but in my experience no amount of IDE support for space indentation is as good as just using tabs.So unless you really don't like being inconsistent with most (presumably not all!) Python code, use tabs and turn on whitespace visualisation and indentation highlighting (if available). The biggest reason for me is ease of selection and the (fairly significant IMO) reduction in keystrokes. Some conventions are stupid.Update: I have discovered that there is one editor in the whole world (excluding nonsense like Vim) that properly supports spaces as indentation: Atom. It has an option called "atomic tabstops" that makes 4 spaces behave as if it were a tab in all respects (except being able to resize it). Sadly Atom is quite a slow and bloated editor, but this is a great feature and if you are forced to use spaces it might be a good option. Hopefully one day other editors will start to support it. Here's the issue for VSCode.I recently came across an article titled Python: Myths about Indentation which discusses this and related questions. The article has good reasons for recommending the use of spaces when writing Python code, but there is certainly room for disagreement.I believe it's true that most Python programmers use spaces only.Use an editor that allows you to insert spaces up to the tabstop when you press the TAB key, instead of inserting a \t character. And then forget about it.I feel very strongly that whatever the historical convention, tabs are simply a better choice and should replace spaces in every future line of Python code written. Like kicking out an incompetent tyrant. My rationale for this is: simplicty as a core value. Use two or maybe four characters for the semantic task of one? There's no justification beyond tradition, IMO.You CAN mix tabs and spaces... BUT a tab is considered to be the same indentation as 8 spaces, so unless your editor is set up to consider a tab to be 8 spaces you're asking for trouble when mixing them.The only inconvenience I experience with using spaces instead of tabs is that you cannot easily remove an indentation level; you have to remove four spaces instead of just one tab.Tabs rule.  Same argument for nested loops and you want to bring the outer loop "back" 1 level.  Tip: If you want to convert old space-riddled python code into tabs use the TabOut utility available as an executable on http://www.textpad.com/add-ons/.Editor-to-editor mistake occurs when you have mixed indentation within a file. This arises as follows: a block of code is indented with 4 spaces, and then one indentation level "in", it is indented with tabs. Now the heathen who did this (mixing tabs and spaces) had it so his tabs are also 4 spaces, so he sees no problems, and Python sees no problems.Now our victim comes along later, and he has his tabs set to 8 spaces. Now our victims thinks the code looks all whacked, and fixes it by removing one level of indentation, which now makes the code look like it is still 2 levels of indentation, but is really one level. At this point all hell breaks loose.The lesson here is that you should never, ever, mix tabs and spaces. If you keep to this, then it is easy to reindent your code into spaces or tabs, regardless of which you personally use. The best way to ensure you don't mix tabs and spaces is to always run python with -tt, which will produce an error when tabs and spaces are mixed.As for tabs and spaces, I personally use tabs so separate indentation from appearance - it is much easier to change the appearance of code when it is indented with tabs than it is with spaces. I know this runs contrary to what 99% of Python programmers do, but that is my personal preference, and it is easy in any case to convert a tabbed file to a spaced one. The reverse is not always true, since you can accidentally whack out 4 spaces in strings, etc.When I was first learning Python, I was put off a little by the idea of significant white space, as most languages to use it are inflexible. That said, I was impressed by Python's ability to understand a variety of indentation styles. When considering what style to use for a new project, I think it is important to keep two things in mind.Basically, there is a variable (which can be changed by including a comment at the top of a source file # tab-width: ) that defines the tab width. When Python encounters a tab, it increases the indentation distance to the next multiple of tab-width. Thus if a space followed by a tab is entered along the left of the file, the next multiple of tab-width is 8. If a tab by itself is entered, the same thing happens.In this way, it is safe, if your editor is configured properly, to use tabs, and even to mix tabs and spaces.  As long as you set your editor's tab stops to the same width as the Python tab-width declaration (or 8 if it is absent). It is generally a bad idea to use an editor with a tab width of other than 8 spaces, unless you specify the tab-width in the file.I have encountered projects that use a mix of tabs and spaces successfully. Basically spaces are used to indent small sections, where the fact that it is in an indented section is relatively unimportant; while tabs are used to draw the reader's attention to a large structural feature.  For example, classes begin with a tab, which simple conditional checks inside a function use two spaces.Tabs are also useful when dealing with large blocks of text indented multiple levels. When you drop out of 3-4 levels of indentation, it is far easier to line up with the proper tab than it is to line up with the proper number of spaces. If a project doesn't use the PEP 8 recommended style, it is probably best to write a style guide into a file somewhere so that the indentation pattern remains consistent and other people can read explicitly how to configure their editor to match.Also, Python 2.x has an option -t to issue warnings about mixed tabs and spaces and -tt to issue an error.  This only applied to mixed tabs and spaces inside the same scope. Python 3 assumes -tt and so far as I've found, there is no way to disable that check.Everyone has different preferences on how much code should be indented. Let's say you share code with someone and he or she has different preferences regarding indentation. If the indentations are in tabs, your friend can always just change the tab width in their editor settings. However, if the indentations are in spaces, your friend will actually have to change the source code if he or she want to set it to their preference. Then when you get your friend's changes, you may decide to change it back to your preference. In this case, you will either have to deal with the tedium of changing indentation levels back and forth, or one person must adopt the other's preferences in indentation level. If both you and your friend use tabs, the fact that you have different preferences is a non-issue, as you can each see different indentation levels while the code remains unchanged. That is why, in my opinion, tabs are better than spaces for indentation in all programming languages. I'm primarily a C++ programmer, but sometimes my projects include small amounts of Python. I use tabs to indent my C++ code. This means that I have three options here:For my projects, I generally go with option 3.Experience and PEP-8 both clearly conclude that mixing spaces and TABs is to be avoided.

If you want to mix them you have to visualize whitespace in the IDE - but then you loose the advantage of Python's indentation making scopes easily visible. Visualizing whitespace in an IDE clutters the display.If it's either TABs or spaces, then it must be spaces for a simple reason: One can switch almost all IDEs and text editors to automatically replace tabs with spaces, but the opposite is not true.Even though there are IDEs that can automatically convert leading spaces in a line to tabs, this will eventually lead to having a mixture of tabs and spaces. Consider multi line statements such as function calls with lots of parameters or doc strings. While "ascii-art" is also to be avoided it may easily happen by accident that a single space is left over after the leading tabs.Other answers brought several arguments in favor of tabs:Imho, the main point that most (if not all) answers are missing here is the interaction between teams or individuals, especially in scenarios where the list of participants is not know at the start. When code meets code either all have to use tabs or all have to use spaces. It cannot be mixed without eventually running into functionality problems. People are not perfect. Tools are not perfect. That's why imho we should not use TABs at all.No answer is complete without the link that Greg provided in his answer already: Python: Myths about IndentationThere's a scenario in which tabs simply don't work, namely: depending on the coding style you are using, you might need to indent some lines of code to one-space accuracy, i.e:In that case, using purely tabs will not work at all; using tabs for main indent and spaces for sub-indent will work but will violate the hard rule of not mixing the two.This will not be the case however when using a coding style/conventions document that avoids situations like in the above code example.The problem with using spaces instead of tabs is the file size becomes so incredibly large... For example, a 500 KB space-indented file could be reduced to 200 KB when swapping spaces for tabs which is why I always use tabs.Smaller file-size means faster loading, compiling, execution (in some cases), etc.To me, there is no point to using spaces, but if someone uses an editor which has issues with tabs, then they can replace "\t" with "  " or "    " or whatever...In addition to all the arguments already listed, I find this one fairly important (from Myths about indentation):Another argument (strongly environment-specific, though) against tabs is that they are sometimes missing on phone keyboards. This could probably be remedied by installing an alternative keyboard, where possible.An argument for tabs which no one seemed to have mentioned yet is that 1 tab is 1 character (0x09, 1 byte in the file), while 4 spaces are 4 characters (4 times 0x20, 4 bytes in the file); thus, using spaces results in a 4x waste of space.To conclude this incoherent list of arguments, I would like to cite Tim Peters answer in the Issue 7012: Tabs is better than spaces for indentation:This is PEP 8 as of July 2017:It seems this statement doesn't leave room for any other choice.But this isn't solely what PEP 8 tells us, few lines later:In the above, the first statement expresses a preference for spaces, and the second statement acknowledges the existence of code indented with tabs and this preference for some coders.So: PEP 8 is tab indentation tolerant.

It doesn't tolerate tab and space mixed for indentation though, which, since indentation itself is mandatory, is understandable.It may be worth mentioning that Google's Python coding style also follows the 4-space rule.There are other various arguments and justifications in favor of either tabs or 4-space.If you work in a company which enforce PEP 8, or regularly share your code with others who follow PEP 8, then common sense dictates 4-space. I am (was, maybe) used to tabs from C/C++. But with a properly set IDE, the difference becomes minimal.Some editors are configured by default to replace a single tab character with a set number of space characters, but some are not. If everyone uses spaces, this difference in default editor settings can be ignored.Yes, there are other valid reasons as pointed out by many answers before me. "PEP-8" says so, however, is NOT one of those reasons. This comes from the self perpetuating myth that PEP-8 is the coding standard for all Python code, when in fact it's just the coding standard for the standard set of Python libraries. Some claim that PEP-8 is widely accepted, and some claim that most Python programmers use spaces instead of tabs. I would like to ask for proofs of these claims, as the number of votes on this site CLEARLY shows that tabs are preferred by the masses. I find it quite unfortunate that you have accepted "PEP8 says so" as the answer to your question, when in fact there are many other answers that actually explains the relative advantages and disadvantages of spaces and tabs.It depends, and the answer to this final question is where I thought I could actually add some value to this thread. IMHO, regardless of the language being used, the best coding standard to use depends on the situation that you are in:So which situation do you fall under?Finally to make my stance clear, for my own solo projects, I use tabs because tabs make more sense to me, and I am more productive with tabs.I believe there is a solution to have both:In Notepad++, go to "preferences"--> "tab settings" and choose "Python" from the list on the right. Then make sure "tab size: 4", and check the box "replace [tab] by space". In this case, you can simply use the tab key to indent, but Notepad++ actually transform that to 4 spaces for you.Use spaces in place of tabs, for the sole reason that you will make more money :)Ref.: Developers Who Use Spaces Make More Money Than Those Who Use Tabs (Stack Overflow blog post). So here I am reading all the responses and wondering how I can comply with PEP-8 without the annoyance of pounding my backspace button repeatedly just to remove indentation, and I look down at my Logitech gaming keyboard with all its fancy macro buttons and a light bulb lights up in my head. I opened Logitech's software, defined a couple of macros for the buttons right next to the tab button, and the problem is solved.One button adds four spaces, and the other does backspace four times. Amazing. Just amazing. So easy to hit the buttons with my pinky, too.See, check this out:"    "<-- four spaces! With one push of a button! If I could show you the backspaces I would do that too. Go get a Logitech G105 keyboard and all your problems will go away!I'm just starting out but I find it much easier to use tabs than spaces, and do not understand the PEP-8 advocation of spaces only. Sublime Text 2 does a great job of visualizing tabs with the off-white vertical, dotted line and while there are cases of me mixing a space or two to line up elements of a list or dictionary, I have not experienced a situation where that would be detrimental thing. I love tabs but it is somehow incompatible with another rule I like: the 80 column limit.If one chooses 4 spaces tabs and inserts 10 tabs, then there is space left for 40 characters to fulfill the 80 column limit.

If another coder prefers 8 spaces tabs, the same line will appear as 120 characters long and will not appear as a valid 80 columns line!If you want to define a 80 column limit, then you have to choose a length for a tab. In this case having x spaces or a tab of length x does not really make a difference.Edit: related thread: Maintaining Maximum Line Length When Using Tabs Instead of Spaces?I think one of the main benefits to using spaces is that you remove variability in how your source code is rendered across the plethora of external tools which have to interact with the source beyond one's choice editor and whatever settings they've configured it in.As some concrete examples consider the rendering of Python docstrings in a tooltip in Visual Studio Code, or in a diff tool like Beyond Compare or WinMerge, performance or code coverage tools, etc. Basically all these various other interfacing tooling can each have different settings for how tabs are interpreted and it can be annoying and at times disorienting to find things vastly different or pushed way off screen among the toolsets you may dive in and out of.In a nutshell you define alignment in the source rather than wrangling down a uniform configuration for the suite of tools in your arsenal. Spaces are strictly interpreted in a monospace font to give reliable and consistent alignment across the full span of tooling due to the font's definition, not a third party's tab rendering implementation/configuration.Another angle to this is in copying leading tabs source to run at a terminal where the tab character can trigger an inadvertent tab completion. For example, if you copy the following Python source (tabs used as indentation),you may see something like follows (seen at Visual Studio Code's integrated terminal)...(Aside: I had wondered if this observation of consistency across tooling is a mark of a discriminating mind of a sharp developer looking to order the world that may indicate a hint at the salary difference found on Stack Overflow.)I use two space indentation and an editor (kwrite) that inserts spaces instead of tabs when I hit the tab key.

Hiding axis text in matplotlib plots

Dave

[Hiding axis text in matplotlib plots](https://stackoverflow.com/questions/2176424/hiding-axis-text-in-matplotlib-plots)

I'm trying to plot a figure without tickmarks or numbers on either of the axes (I use axes in the traditional sense, not the matplotlib nomenclature!). An issue I have come across is where matplotlib adjusts the x(y)ticklabels by subtracting a value N, then adds N at the end of the axis.This may be vague, but the following simplified example highlights the issue, with '6.18' being the offending value of N:The three things I would like to know are:

2010-02-01 11:56:52Z

I'm trying to plot a figure without tickmarks or numbers on either of the axes (I use axes in the traditional sense, not the matplotlib nomenclature!). An issue I have come across is where matplotlib adjusts the x(y)ticklabels by subtracting a value N, then adds N at the end of the axis.This may be vague, but the following simplified example highlights the issue, with '6.18' being the offending value of N:The three things I would like to know are:Instead of hiding each element, you can hide the whole axis:Or, you can set the ticks to an empty list:In this second option, you can still use plt.xlabel() and plt.ylabel() to add labels to the axes.If you want to hide just the axis text keeping the grid lines:Doing set_visible(False) or set_ticks([]) will also hide the grid lines.If you are like me and don't always retrieve the axes, ax, when plotting the figure, then a simple solution would be to do Somewhat of an old thread but, this seems to be a faster method using the latest version of matplotlib:set the major formatter for the x-axisI was not actually able to render an image without borders or axis data based on any of the code snippets here (even the one accepted at the answer). After digging through some API documentation, I landed on this code to render my imageI used the tick_params call to basically shut down any extra information that might be rendered and I have a perfect graph in my output file.I've colour coded this figure to ease the process.You can have full control over the figure using these commands, to complete the answer I've add also the control over the splines:When using the object oriented API, the Axes object has two useful methods for removing the axis text, set_xticklabels() and set_xticks().Say you create a plot using If you simply want to remove the tick labels, you could useor to remove the ticks completely, you could useThese methods are useful for specifying exactly where you want the ticks and how you want them labeled.  Passing an empty list results in no ticks, or no labels, respectively.One trick could be setting the color of tick labels as white to hide it!

How do I create a variable number of variables?

SilentGhost

[How do I create a variable number of variables?](https://stackoverflow.com/questions/1373164/how-do-i-create-a-variable-number-of-variables)

How do I accomplish variable variables in Python? Here is an elaborative manual entry, for instance: Variable variablesI have heard this is a bad idea in general though, and it is a security hole in Python. Is that true?

2009-09-03 12:37:48Z

How do I accomplish variable variables in Python? Here is an elaborative manual entry, for instance: Variable variablesI have heard this is a bad idea in general though, and it is a security hole in Python. Is that true?You can use dictionaries to accomplish this. Dictionaries are stores of keys and values. You can use variable key names to achieve the effect of variable variables without the security risk.For cases where you're thinking of doing something likea list may be more appropriate than a dict. A list represents an ordered sequence of objects, with integer indices:For ordered sequences, lists are more convenient than dicts with integer keys, because lists support iteration in index order, slicing, append, and other operations that would require awkward key management with a dict.Use the built-in getattr function to get an attribute on an object by name.  Modify the name as needed.It's not a good idea. If you are accessing a global variable you can use globals().If you want to access a variable in the local scope you can use locals(), but you cannot assign values to the returned dict.A better solution is to use getattr or store your variables in a dictionary and then access them by name.Whenever you want to use variable variables, it's probably better to use a dictionary. So instead of writingyou write This way you won't accidentally overwrite previously existing variables (which is the security aspect) and you can have different "namespaces".New coders sometimes write code like this:The coder is then left with a pile of named variables, with a coding effort of O(m * n), where m is the number of named variables and n is the number of times that group of variables needs to be accessed (including creation). The more astute beginner observes that the only difference in each of those lines is a number that changes based on a rule, and decides to use a loop. However, they get stuck on how to dynamically create those variable names, and may try something like this:They soon find that this does not work.If the program requires arbitrary variable "names," a dictionary is the best choice, as explained in other answers. However, if you're simply trying to create many variables and you don't mind referring to them with a sequence of integers, you're probably looking for a list. This is particularly true if your data are homogeneous, such as daily temperature readings, weekly quiz scores, or a grid of graphical widgets.This can be assembled as follows:This list can also be created in one line with a comprehension:The result in either case is a populated list, with the first element accessed with my_calculator.buttons[0], the next with my_calculator.buttons[1], and so on. The "base" variable name becomes the name of the list and the varying identifier is used to access it.Finally, don't forget other data structures, such as the set - this is similar to a dictionary, except that each "name" doesn't have a value attached to it. If you simply need a "bag" of objects, this can be a great choice. Instead of something like this:You will have this:Use a list for a sequence of similar objects, a set for an arbitrarily-ordered bag of objects, or a dict for a bag of names with associated values.Instead of a dictionary you can also use namedtuple from the collections module, which makes access easier.For example:If you don't want to use any object, you can still use setattr() inside your current module:The SimpleNamespace class could be used to create new attributes with setattr, or subclass SimpleNamespace and create your own function to add new attribute names (variables). Use globals()You can actually assign variables to global scope dynamically, for instance, if you want 10 variables that can be accessed on a global scope i_1, i_2 ... i_10:This will assign 'a' to all of these 10 variables, of course you can change the value dynamically as well. All of these variables can be accessed now like other globally declared variable:I'm am answering the question: How to get the value of a variable given its name in a string?

which is closed as a duplicate with a link to this question. If the variables in question are part of an object (part of a class for example) then some useful functions to achieve exactly that are hasattr, getattr, and setattr. So for example you can have:Then you can do:You have to use globals() built in method  to achieve that behaviour:The consensus is to use a dictionary for this - see the other answers. This is a good idea for most cases, however, there are many aspects arising from this:That said, I've implemented a variable variables manager-class which provides some of the above ideas. It works for python 2 and 3.You'd use the class like this:If you wish to allow overwriting of variables with the same type only:I have tried both in python 3.7.3, you can use either globals() or vars()Reference:

https://www.daniweb.com/programming/software-development/threads/111526/setting-a-string-as-a-variable-name#post548936Any set of variables can also be wrapped up in a class. 

"Variable" variables may be added to the class instance during runtime by directly accessing the built-in dictionary through __dict__ attribute. The following code defines Variables class, which adds variables (in this case attributes) to its instance during the construction. Variable names are taken from a specified list (which, for example, could have been generated by program code):

Explaining Python's '__enter__' and '__exit__'

zjm1126

[Explaining Python's '__enter__' and '__exit__'](https://stackoverflow.com/questions/1984325/explaining-pythons-enter-and-exit)

I saw this in someone's code. What does it mean?

2009-12-31 07:07:18Z

I saw this in someone's code. What does it mean?Using these magic methods (__enter__, __exit__) allows you to implement objects which can be used easily with the with statement. The idea is that it makes it easy to build code which needs some 'cleandown' code executed (think of it as a try-finally block). Some more explanation here.A useful example could be a database connection object (which then automagically closes the connection once the corresponding 'with'-statement goes out of scope):As explained above, use this object with the with statement (you may need to do from __future__ import with_statement at the top of the file if you're on Python 2.5).PEP343 -- The 'with' statement' has a nice writeup as well.If you know what context managers are then you need nothing more to understand __enter__ and __exit__ magic methods. Lets see a very simple example.In this example I am opening myfile.txt with help of open function. The try/finally block ensures that even if an unexpected exception occurs myfile.txt will be closed.Now I am opening same file with with statement:If you look at the code, I  didn't close the file & there is no try/finally block. Because with statement automatically closes  myfile.txt . You can even check it by calling print(fp.closed) attribute -- which returns True.This is because the file objects (fp in my example) returned by open function has two built-in methods __enter__ and __exit__. It is  also known as context manager. __enter__ method is called at the start of with block and __exit__  method is called at the end.  Note: with statement only works with objects that support the context mamangement protocol i.e. they have __enter__ and __exit__ methods. A class which implement both methods is known as context manager class.Now lets define our own context manager class.I hope now you have basic understanding of both __enter__ and __exit__ magic methods.I found it strangely difficult to locate the python docs for __enter__ and __exit__ methods by Googling, so to help others here is the link:https://docs.python.org/2/reference/datamodel.html#with-statement-context-managers

https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers

(detail is the same for both versions)I was hoping for a clear description of the __exit__ method arguments. This is lacking but we can deduce them...Presumably exc_type is the class of the exception.It says you should not re-raise the passed-in exception. This suggests to us that one of the arguments might be an actual Exception instance ...or maybe you're supposed to instantiate it yourself from the type and value?We can answer by looking at this article:

http://effbot.org/zone/python-with-statement.htm...so clearly value is an Exception instance.And presumably traceback is a Python traceback object.In addition to the above answers to exemplify invocation order, a simple run example

Produces the output: A reminder: when using the syntax with myclass() as mc, variable mc gets the value returned by __enter__(), in the above case None! For such use, need to define return value, such as: try adding my answers (my thought of learning) :__enter__ and [__exit__] both are methods that are invoked on entry to and exit from the body of "the with statement" (PEP 343) and implementation of both is called context manager.the with statement is intend to hiding flow control of try finally clause and make the code inscrutable.the syntax of the with statement is :which translate to (as mention in PEP 343) :try some code:and now try manually (following translate syntax):the result of the server side same as beforesorry for my bad english and my unclear explanations, thank you....This is called context manager and I just want to add that similar approaches exist for other programming languages. Comparing them could be helpful in understanding the context manager in python.

Basically, a context manager is used when we are dealing with some resources (file, network, database) that need to be initialized and at some point, tear downed (disposed). In Java 7 and above we have automatic resource management that takes the form of:Note that Session needs to implement AutoClosable or one of its (many) sub-interfaces.In C#, we have using statements for managing resources that takes the form of:In which Session should implement IDisposable. In python, the class that we use should implement __enter__ and __exit__. So it takes the form of:And as others pointed out, you can always use try/finally statement in all the languages to implement the same mechanism. This is just syntactic sugar.

Is there a way to perform「if」in python's lambda

Guy

[Is there a way to perform「if」in python's lambda](https://stackoverflow.com/questions/1585322/is-there-a-way-to-perform-if-in-pythons-lambda)

In python 2.6, I want to do:This clearly isn't the syntax. Is it possible to perform an if in lambda and if so how to do it?thanks

2009-10-18 16:28:03Z

In python 2.6, I want to do:This clearly isn't the syntax. Is it possible to perform an if in lambda and if so how to do it?thanksThe syntax you're looking for:But you can't use print or raise in a lambda.why don't you just define a function?there really is no justification to use lambda in this case.You can easily raise an exception in a lambda, if that's what you really want to do.Is this a good idea?  My instinct in general is to leave the error reporting out of lambdas; let it have a value of None and raise the error in the caller.  I don't think this is inherently evil, though--I consider the "y if x else z" syntax itself worse--just make sure you're not trying to stuff too much into a lambda body.Probably the worst python line I've written so far: If x == 2 you print, if x != 2 you raise. Lambdas in Python are fairly restrictive with regard to what you're allowed to use. Specifically, you can't have any keywords (except for operators like and, not, or, etc) in their body.So, there's no way you could use a lambda for your example (because you can't use raise), but if you're willing to concede on that… You could use:note you can use several else...if statements in your lambda definition:If you still want to print you can import future modulewhat you need exactly isnow call the function the way you needThis snippet should help you:You can also use Logical Operators to have something like a ConditionalYou can see more about Logical Operators hereFollowing sample code works for me. Not sure if it directly relates to this question, but hope it helps in some other cases.Try it:Out:An easy way to perform an if in lambda is by using list comprehension. You can't raise an exception in lambda, but this is a way in Python 3.x to do something close to your example:Another example:return 1 if M otherwise 0

How to count the occurrence of certain item in an ndarray in Python?

mflowww

[How to count the occurrence of certain item in an ndarray in Python?](https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray-in-python)

In Python, I have an ndarray y

that is printed as array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])I'm trying to count how many 0s and how many 1s are there in this array. But when I type y.count(0) or y.count(1), it says What should I do? 

2015-02-22 22:05:48Z

In Python, I have an ndarray y

that is printed as array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])I'm trying to count how many 0s and how many 1s are there in this array. But when I type y.count(0) or y.count(1), it says What should I do? Non-numpy way:Use collections.Counter;What about using numpy.count_nonzero, something like Personally, I'd go for:

(y == 0).sum() and (y == 1).sum()E.g.For your case you could also look into numpy.bincountConvert your array y to list l and then do l.count(1) and l.count(0)If you know that they are just 0 and 1:gives you the number of ones.  np.sum(1-y) gives the zeroes.For slight generality, if you want to count 0 and not zero (but possibly 2 or 3):gives the number of nonzero.But if you need something more complicated, I don't think numpy will provide a nice count option.  In that case, go to collections:This behaves like a dictIf you know exactly which number you're looking for, you can use the following;returns how many times 2 is occurred in your array.What about len(y[y==0]) and len(y[y==1]) ?y.tolist().count(val)with val 0 or 1Since a python list has a native function count, converting to list before using that function is a simple solution.Honestly I find it easiest to convert to a pandas Series or DataFrame:Or this nice one-liner suggested by Robert Muil:No one suggested to use numpy.bincount(input, minlength) with minlength = np.size(input), but it seems to be a good solution, and definitely the fastest:That's a crazy speedup between numpy.unique(x, return_counts=True) and numpy.bincount(x, minlength=np.max(x)) !Yet another simple solution might be to use numpy.count_nonzero():Don't let the name mislead you, if you use it with the boolean just like in the example, it will do the trick.To count the number of occurrences, you can use np.unique(array, return_counts=True):I'd use np.where:A general and simple answer would be:which would result into this full code as exempleNow if MyArray is in multiple dimensions and you want to count the occurence of a distribution of values in line (= pattern hereafter)You can use dictionary comprehension to create a neat one-liner. More about dictionary comprehension can be found here This will create a dictionary with the values in your ndarray as keys, and the counts of the values as the values for the keys respectively.This will work whenever you want to count occurences of a value in arrays of this format.take advantage of the methods offered by a Series:This can be done easily in the following methodSince your ndarray contains only 0 and 1, 

you can use sum() to get the occurrence of 1s 

and len()-sum() to get the occurrence of 0s.You have a special array with only 1 and 0 here. So a trick is to use which gives you the percentage of 1s in your array. Alternatively, usewill give you the absolute number of 1 and 0 in your array.It involves one more step, but a more flexible solution which would also work for 2d arrays and more complicated filters is to create a boolean mask and then use .sum() on the mask.If you don't want to use numpy or a collections module you can use a dictionary:result:Of course you can also use an if/else statement. 

I think the Counter function does almost the same thing but this is more transparant. For generic entries:Will output a count:And indices:Just copied  Seppo Enarvi's comment here which deserves to be a proper answer Try this:here I have something, through which you can count the number of occurrence of a particular number:

according to your codecount_of_zero=list(y[y==0]).count(0) print(count_of_zero)// according to the match there will be boolean values and according to True value the number 0 will be returnNumpy has a module for this. Just a small hack. Put your input array as bins. The output are 2 arrays. One with the values itself, other with the corresponding frequencies. 

Setting the correct encoding when piping stdout in Python

Joakim Lundborg

[Setting the correct encoding when piping stdout in Python](https://stackoverflow.com/questions/492483/setting-the-correct-encoding-when-piping-stdout-in-python)

When piping the output of a Python program, the Python interpreter gets confused about encoding and sets it to None. This means a program like this:will work fine when run normally, but fail with:when used in a pipe sequence.What is the best way to make this work when piping? Can I just tell it to use whatever encoding the shell/filesystem/whatever is using? The suggestions I have seen thus far is to modify your site.py directly, or hardcoding the defaultencoding using this hack:Is there a better way to make piping work?

2009-01-29 16:57:59Z

When piping the output of a Python program, the Python interpreter gets confused about encoding and sets it to None. This means a program like this:will work fine when run normally, but fail with:when used in a pipe sequence.What is the best way to make this work when piping? Can I just tell it to use whatever encoding the shell/filesystem/whatever is using? The suggestions I have seen thus far is to modify your site.py directly, or hardcoding the defaultencoding using this hack:Is there a better way to make piping work?Your code works when run in an script because Python encodes the output to whatever encoding your terminal application is using. If you are piping you must encode it yourself.A rule of thumb is: Always use Unicode internally. Decode what you receive, and encode what you send.Another didactic example is a Python program to convert between ISO-8859-1 and UTF-8, making everything uppercase in between.Setting the system default encoding is a bad idea, because some modules and libraries you use can rely on the fact it is ASCII. Don't do it.First, regarding this solution:It's not practical to explicitly print with a given encoding every time. That would be repetitive and error-prone.A better solution is to change sys.stdout at the start of your program, to encode with a selected encoding. Here is one solution I found on Python: How is sys.stdout.encoding chosen?, in particular a comment by "toka":You may want to try changing the environment variable "PYTHONIOENCODING" to "utf_8". I have written a page on my ordeal with this problem.Tl;dr of the blog post:gives youdo the job, but can't set it on python itself ...what we can do is verify if isn't setting and tell the user to set it before call script with :Update to reply to the comment: 

the problem just exist when piping to stdout .

I tested in Fedora 25 Python 2.7.13cat b.py running ./b.py running ./b.py | lessI had a similar issue last week. It was easy to fix in my IDE (PyCharm).Here was my fix:Starting from PyCharm menu bar: File -> Settings... -> Editor -> File Encodings, then set: "IDE Encoding", "Project Encoding" and "Default encoding for properties files" ALL to UTF-8 and she now works like a charm.Hope this helps!An arguable sanitized version of Craig McQueen's answer.Usage:I could "automate" it with a call to:Yes, it's possible to get an infinite loop here if this "setenv" fails.I just thought I'd mention something here which I had to spent a long time experimenting with before I finally realised what was going on. This may be so obvious to everyone here that they haven't bothered mentioning it. But it would've helped me if they had, so on that principle...!NB: I am using Jython specifically, v 2.7, so just possibly this may not apply to CPython...NB2: the first two lines of my .py file here are:The "%" (AKA "interpolation operator") string construction mechanism causes ADDITIONAL problems too... If the default encoding of the "environment" is ASCII and you try to do something likeYou will have no difficulty running in Eclipse... In a Windows CLI (DOS window) you will find that the encoding is code page 850 (my Windows 7 OS) or something similar, which can handle European accented characters at least, so it'll work.will also work.If, OTOH, you direct to a file from the CLI, the stdout encoding will be None, which will default to ASCII (on my OS anyway), which will not be able to handle either of the above prints... (dreaded encoding error).So then you might think of redirecting your stdout by usingand try running in the CLI piping to a file... Very oddly, print A above will work... But print B above will throw the encoding error! The following will however work OK:The conclusion I have come to (provisionally) is that if a string which is specified to be a Unicode string using the "u" prefix is submitted to the %-handling mechanism it appears to involve the use of the default environment encoding, regardless of whether you have set stdout to redirect!How people deal with this is a matter of choice. I would welcome a Unicode expert to say why this happens, whether I've got it wrong in some way, what the preferred solution to this, whether it also applies to CPython, whether it happens in Python 3, etc., etc.I ran into this problem in a legacy application, and it was difficult to identify where what was printed. I helped myself with this hack:On top of my script, test.py:Note that this changes ALL calls to print to use an encoding, so your console will print this:On Windows, I had this problem very often when running a Python code from an editor (like Sublime Text), but not if running it from command-line.In this case, check your editor's parameters. In the case of SublimeText, this Python.sublime-build solved it:

Is it possible to use pip to install a package from a private GitHub repository?

Adam J. Forster

[Is it possible to use pip to install a package from a private GitHub repository?](https://stackoverflow.com/questions/4830856/is-it-possible-to-use-pip-to-install-a-package-from-a-private-github-repository)

I am trying to install a Python package from a private GitHub repository. For a public repository, I can issue the following command which works fine:However, if I try this for a private repository:I get the following output:I guess this is because I am trying to access a private repository without providing any authentication. I therefore tried to use Git + ssh hoping that pip would use my SSH public key to authenticate:This gives the following output:Is what I am trying to achieve even possible? If so, how can I do it?

2011-01-28 16:52:32Z

I am trying to install a Python package from a private GitHub repository. For a public repository, I can issue the following command which works fine:However, if I try this for a private repository:I get the following output:I guess this is because I am trying to access a private repository without providing any authentication. I therefore tried to use Git + ssh hoping that pip would use my SSH public key to authenticate:This gives the following output:Is what I am trying to achieve even possible? If so, how can I do it?You can use the git+ssh URI scheme, but you must set a username:Do you see the git@ part into the URI?PS: Also read about deploy keys.PPS: In my installation, the "git+ssh" URI scheme works only with "editable" requirements:Remember: Change the : character that git remote -v prints to a / character before using the remote's address in the pip command:If you forget, you will get this error:As an additional technique, if you have the private repository cloned locally, you can do:More modernly, you can just do this (and the -e will mean you don't have to commit changes before they're reflected):You can do it directly with the HTTPS URL like this:This also works just appending that line in the requirements.txt in a Django project, for instance.It also works with Bitbucket:Pip will use your SSH keys in this case.The syntax for the requirements file is given here:https://pip.pypa.io/en/latest/reference/pip_install.html#requirements-file-formatSo for example, use:if you want the source to stick around after installation.Or justif you just want it to be installed.I found it much easier to use tokens than SSH keys. I couldn't find much good documentation on this, so I came across this solution mainly through trial and error. Further, installing from pip and setuptools have some subtle differences; but this way should work for both.GitHub don't (currently, as of August 2016) offer an easy way to get the zip / tarball of private repositories. So you need to point setuptools to tell setuptools that you're pointing to a Git repository:A couple of notes here:I figured out a way to automagically 'pip install' a GitLab private repository that requires no password prompt. This approach uses GitLab "Deploy Keys" and an SSH configuration file, so you can deploy using keys other than your personal SSH keys (in my case, for use by a 'bot). Perhaps someone kind soul can verify using GitHub.The file should show up as ~/.ssh/GitLab_Robot_Deploy_Key and ~/.ssh/GitLab_Robot_Deploy_Key.pub.Copy and paste the contents of the ~/.ssh/GitLab_Robot_Deploy_Key.pub file into the GitLab "Deploy Keys" dialog.The following command tells SSH to use your new deploy key to set up the connection. On success, you should get the message: "Welcome to GitLab, UserName!"Next, use an editor to create a ~/.ssh/config file. Add the following contents. The 'Host' value can be anything you want (just remember it, because you'll be using it later). The HostName is the URL to your GitLab instance. The IdentifyFile is path to the SSH key file you created in the first step.oxyum gave us the recipe for using pip with SSH:We just need to modify it a bit to make SSH use our new Deploy Key. We do that by pointing SSH to the Host entry in the SSH configuration file. Just replace the 'gitlab.mycorp.com' in the command to the host name we used in the SSH configuration file:The package should now install without any password prompt.Reference A 

Reference BWhen I was installing from GitHub I was able to use:But, since I had to run pip as sudo, the SSH keys were not working with GitHub any more, and "git clone" failed on "Permission denied (publickey)". Using git+https allowed me to run the command as sudo, and have GitHub ask me for my user/password.You can also install a private repository dependency via git+https://github.com/... URL by providing login credentials (login and password, or deploy token) for curl with the .netrc file:If you want to install dependencies from a requirements file within a CI server or alike, you can do this:In my case, I used GIT_USER=gitlab-ci-token and GIT_PASS=${CI_JOB_TOKEN}.This method has a clear advantage. You have a single requirements file which contains all of your dependencies.If you don't want to use SSH, you could add the username and password in the HTTPS URL.The code below assumes that you have a file called "pass" in the working directory that contains your password.oxyum's solution is OK for this answer. I just want to point out that you need to be careful if you are installing using sudo as the keys must be stored for root too (for example, /root/.ssh).Then you can typeIf you have your own library/package on GitHub, GitLab, etc., you have to add a tag to commit with a concrete version of the library, for example, v2.0, and then you can install your package:This works for me. Other solutions haven't worked for me.Just copy the remote from the original git clone command (or from git remote -v). You will get something like this:Next, you need to replace : with / next to the domain name.So install using:You may try without ssh:.... That works for me.

How can I get the named parameters from a URL using Flask?

Alex Stone

[How can I get the named parameters from a URL using Flask?](https://stackoverflow.com/questions/24892035/how-can-i-get-the-named-parameters-from-a-url-using-flask)

When the user accesses this URL running on my flask app, I want the web service to be able to handle the parameters specified after the question mark:

2014-07-22 15:49:09Z

When the user accesses this URL running on my flask app, I want the web service to be able to handle the parameters specified after the question mark:Use request.args to get parsed contents of query string:The URL parameters are available in request.args, which is a MultiDict that has a get method, with optional parameters for default value (default) and type (type) - which is a callable that converts the input value to the desired format.Examples with the code above:You can also use brackets <> on the URL of the view definition and this input will go into your view function argumentsIf you have a single argument passed in the URL you can do it as followsIn case you have multiple parameters:What you were trying to do works in case of POST requests where parameters are passed as form parameters and do not appear in the URL. In case you are actually developing a login API, it is advisable you use POST request rather than GET and expose the data to the user.In case of post request, it would work as follows:HTML snippet:Route:url: code:(Edit: removed spaces in format string)It's really simple. Let me divide this process into two simple steps. Use request.args.get(param), for example:Here is the referenced link to the code.

Threading pool similar to the multiprocessing Pool?

Martin

[Threading pool similar to the multiprocessing Pool?](https://stackoverflow.com/questions/3033952/threading-pool-similar-to-the-multiprocessing-pool)

Is there a Pool class for worker threads, similar to the multiprocessing module's Pool class?I like for example the easy way to parallelize a map functionhowever I would like to do it without the overhead of creating new processes.I know about the GIL. However, in my usecase, the function will be an IO-bound C function for which the python wrapper will release the GIL before the actual function call.Do I have to write my own threading pool?

2010-06-13 21:17:16Z

Is there a Pool class for worker threads, similar to the multiprocessing module's Pool class?I like for example the easy way to parallelize a map functionhowever I would like to do it without the overhead of creating new processes.I know about the GIL. However, in my usecase, the function will be an IO-bound C function for which the python wrapper will release the GIL before the actual function call.Do I have to write my own threading pool?I just found out that there actually  is a thread-based Pool interface in the multiprocessing module, however it is hidden somewhat and not properly documented.It can be imported viaIt is implemented using a dummy Process class wrapping a python thread.  This thread-based Process class can be found in multiprocessing.dummy which is mentioned briefly in the docs.  This dummy module supposedly provides the whole multiprocessing interface based on threads.In Python 3 you can use concurrent.futures.ThreadPoolExecutor, i.e.:See the docs for more info and examples.Yes, and it seems to have (more or less) the same API. For something very simple and lightweight (slightly modified from here):To support callbacks on task completion you can just add the callback to the task tuple.Hi to use the thread pool in Python you can use this library :and then for use, this library do like that :The threads are the number of threads that you want and tasks are a list of task that most map to the service.Here's the result I finally ended up using. It's a modified version of the classes by dgorissen above.File: threadpool.pyTo use the poolThe overhead of creating the new processes is minimal, especially when it's just 4 of them. I doubt this is a performance hot spot of your application. Keep it simple, optimize where you have to and where profiling results point to. There is no built in thread based pool. However, it can be very quick to implement a producer/consumer queue with the Queue class.From:

https://docs.python.org/2/library/queue.html

How can I use pickle to save a dict?

Chachmu

[How can I use pickle to save a dict?](https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict)

I have looked through the information that the Python docs give, but I'm still a little confused. Could somebody post sample code that would write a new file then use pickle to dump a dictionary into it?

2012-06-27 02:12:44Z

I have looked through the information that the Python docs give, but I'm still a little confused. Could somebody post sample code that would write a new file then use pickle to dump a dictionary into it?Try this:The advantage of HIGHEST_PROTOCOL is that files get smaller. This makes unpickling sometimes much faster.Important notice: The maximum file size of pickle is about 2GB.For your application, the following might be important:See also: Comparison of data serialization formatsIn case you are rather looking for a way to make configuration files, you might want to read my short article Configuration files in PythonIn general, pickling a dict will fail unless you have only simple objects in it, like strings and integers.Even a really simple dict will often fail.  It just depends on the contents.However, if you use a better serializer like dill or cloudpickle, then most dictionaries can be pickled:Or if you want to save your dict to a file...The latter example is identical to any of the other good answers posted here (which aside from neglecting the picklability of the contents of the dict are good).normally it's preferable to use the cPickle implementationSimple way to dump a Python data (e.g. dictionary) to a pickle file.If you just want to store the dict in a single file, use pickle like thatIf you want to save and restore multiple dictionaries in multiple files for 

caching and store more complex data, 

use anycache. 

It does all the other stuff you need around pickleAnycache stores the different myfunc results depending on the arguments to 

different files in cachedir and reloads them.See the documentation for any further details.I've found pickling confusing (possibly because I'm thick).  I found that this works, though:Which you can then write to a text file.  I gave up trying to use pickle as I was getting errors telling me to write integers to a .dat file.  I apologise for not using pickle.

Passing a dictionary to a function as keyword parameters

Dave Hillier

[Passing a dictionary to a function as keyword parameters](https://stackoverflow.com/questions/334655/passing-a-dictionary-to-a-function-as-keyword-parameters)

I'd like to call a function in python using a dictionary.Here is some code:This prints {'param': 'test'} but I'd like it to just print test.I'd like it to work similarly for more parameters:Is this possible?

2008-12-02 16:49:11Z

I'd like to call a function in python using a dictionary.Here is some code:This prints {'param': 'test'} but I'd like it to just print test.I'd like it to work similarly for more parameters:Is this possible?Figured it out for myself in the end. It is simple, I was just missing the ** operator to unpack the dictionarySo my example becomes:A few extra details that might be helpful to know (questions I had after reading this and went and tested):Examples:Number 1: The function can have parameters that are not included in the dictionaryNumber 2: You can not override a parameter that is already in the dictionaryNumber 3: The dictionary can not have parameters that aren't in the function.As requested in comments, a solution to Number 3 is to filter the dictionary based on the keyword arguments available in the function:Another option is to accept (and ignore) additional kwargs in your function:Notice further than you can use positional arguments and lists or tuples in effectively the same way as kwargs, here's a more advanced example incorporating both positional and keyword args:In python, this is called "unpacking", and you can find a bit about it in the tutorial. The documentation of it sucks, I agree, especially because of how fantasically useful it is.Here ya go - works just any other iterable:

Split by comma and strip whitespace in Python

Mr_Chimp

[Split by comma and strip whitespace in Python](https://stackoverflow.com/questions/4071396/split-by-comma-and-strip-whitespace-in-python)

I have some python code that splits on comma, but doesn't strip the whitespace:I would rather end up with whitespace removed like this:I am aware that I could loop through the list and strip() each item but, as this is Python, I'm guessing there's a quicker, easier and more elegant way of doing it.

2010-11-01 17:29:37Z

I have some python code that splits on comma, but doesn't strip the whitespace:I would rather end up with whitespace removed like this:I am aware that I could loop through the list and strip() each item but, as this is Python, I'm guessing there's a quicker, easier and more elegant way of doing it.Use list comprehension -- simpler, and just as easy to read as a for loop.See: Python docs on List Comprehension

A good 2 second explanation of list comprehension.Split using a regular expression. Note I made the case more general with leading spaces. The list comprehension is to remove the null strings at the front and back.This works even if ^\s+ doesn't match:Here's why you need ^\s+:See the leading spaces in blah?Clarification: above uses the Python 3 interpreter, but results are the same in Python 2.I came to add:map(str.strip, string.split(','))but saw it had already been mentioned by Jason Orendorff in a comment.Reading Glenn Maynard's comment in the same answer suggesting list comprehensions over map I started to wonder why. I assumed he meant for performance reasons, but of course he might have meant for stylistic reasons, or something else (Glenn?).So a quick (possibly flawed?) test on my box applying the three methods in a loop revealed:making map(str.strip, string.split(',')) the winner, although it seems they are all in the same ballpark.Certainly though map (with or without a lambda) should not necessarily be ruled out for performance reasons, and for me it is at least as clear as a list comprehension.Edit:Python 2.6.5 on Ubuntu 10.04Just remove the white space from the string before you split it. I know this has already been answered, but if you end doing this a lot, regular expressions may be a better way to go:The \s matches any whitespace character, and we just replace it with an empty string ''. You can find more info here: http://docs.python.org/library/re.html#re.subthis works fine for me.re (as in regular expressions) allows splitting on multiple characters at once:This doesn't work well for your example string, but works nicely for a comma-space separated list. For your example string, you can combine the re.split power to split on regex patterns to get a "split-on-this-or-that" effect.Unfortunately, that's ugly, but a filter will do the trick:Voila!map(lambda s: s.strip(), mylist) would be a little better than explicitly looping. Or for the whole thing at once: map(lambda s:s.strip(), string.split(','))Simply, comma or at least one white spaces with/without preceding/succeeding white spaces.Please try!map(lambda s: s.strip(), mylist) would be a little better than explicitly looping.

Or for the whole thing at once: That's basically everything you need.

Constructing pandas DataFrame from values in variables gives「ValueError: If using all scalar values, you must pass an index」

Nilani Algiriyage

[Constructing pandas DataFrame from values in variables gives「ValueError: If using all scalar values, you must pass an index」](https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi)

This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.I want to construct a DataFrame from this:This generates an error:  I tried this also:This gives the same error message.

2013-07-24 16:40:24Z

This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.I want to construct a DataFrame from this:This generates an error:  I tried this also:This gives the same error message.The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:or use scalar values and pass an index:You can also use pd.DataFrame.from_records which is more convenient when you already have the dictionary in hand:You can also set index, if you want, by:You need to create a pandas series first. The second step is to convert the pandas series to pandas dataframe.You can even provide a column name.You may try wrapping your dictionary in to listmy_dict = {'A':1,'B':2}pd.DataFrame([my_dict])You need to provide iterables as the values for the Pandas DataFrame columns:Maybe Series would provide all the functions you need:DataFrame can be thought of as a collection of Series hence you can :I had the same problem with numpy arrays and the solution is to flatten them:You could try:From the documentation on the 'orient' argument: If the keys of the passed dict should be the columns of the resulting DataFrame, pass ‘columns’ (default). Otherwise if the keys should be rows, pass ‘index’.If you intend to convert a dictionary of scalars, you have to include an index:Although index is not required for a dictionary of lists, the same idea can be expanded to a dictionary of lists:Of course, for the dictionary of lists, you can build the dataframe without an index:Pandas magic at work. All logic is out.The error message "ValueError: If using all scalar values, you must pass an index" Says you must pass an index.This does not necessarily mean passing an index makes pandas do what you want it to doWhen you pass an index, pandas will treat your dictionary keys as column names and the values as what the column should contain for each of the values in the index.Passing a larger index:An index is usually automatically generated by a dataframe when none is given. However, pandas does not know how many rows of 2 and 3 you want. You can however be more explicit about itThe default index is 0 based though.I would recommend always passing a dictionary of lists to the dataframe constructor when creating dataframes. It's easier to read for other developers. Pandas has a lot of caveats, don't make other developers have to experts in all of them in order to read your code.This is because a DataFrame has two intuitive dimensions - the columns and the rows. You are only specifying the columns using the dictionary keys.If you only want to specify one dimensional data, use a Series!the input does not have to be a list of records - it can be a single dictionary as well:Which seems to be equivalent to:If you have a dictionary you can turn it into a pandas data frame with the following line of code: Convert Dictionary to Data FrameGive new name to  ColumnJust pass the dict on a list:

How do I get a Cron like scheduler in Python? [closed]

jamesh

[How do I get a Cron like scheduler in Python? [closed]](https://stackoverflow.com/questions/373335/how-do-i-get-a-cron-like-scheduler-in-python)

I'm looking for a library in Python which will provide at and cron like functionality.I'd quite like have a pure Python solution, rather than relying on tools installed on the box; this way I run on machines with no cron.For those unfamiliar with cron: you can schedule tasks based upon an expression like: The cron time expression syntax is less important, but I would like to have something with this sort of flexibility. If there isn't something that does this for me out-the-box, any suggestions for the building blocks to make something like this would be gratefully received.Edit

I'm not interested in launching processes, just "jobs" also written in Python - python functions. By necessity I think this would be a different thread, but not in a different process.To this end, I'm looking for the expressivity of the cron time expression, but in Python. Cron has been around for years, but I'm trying to be as portable as possible. I cannot rely on its presence.

2008-12-17 00:56:31Z

I'm looking for a library in Python which will provide at and cron like functionality.I'd quite like have a pure Python solution, rather than relying on tools installed on the box; this way I run on machines with no cron.For those unfamiliar with cron: you can schedule tasks based upon an expression like: The cron time expression syntax is less important, but I would like to have something with this sort of flexibility. If there isn't something that does this for me out-the-box, any suggestions for the building blocks to make something like this would be gratefully received.Edit

I'm not interested in launching processes, just "jobs" also written in Python - python functions. By necessity I think this would be a different thread, but not in a different process.To this end, I'm looking for the expressivity of the cron time expression, but in Python. Cron has been around for years, but I'm trying to be as portable as possible. I cannot rely on its presence.If you're looking for something lightweight checkout schedule:Disclosure: I'm the author of that library.You could just use normal Python argument passing syntax to specify your crontab.  For example, suppose we define an Event class as below:(Note: Not thoroughly tested)Then your CronTab can be specified in normal python syntax as:This way you get the full power of Python's argument mechanics (mixing positional and keyword args, and can use symbolic names for names of weeks and months)The CronTab class would be defined as simply sleeping in minute increments, and calling check() on each event.  (There are probably some subtleties with daylight savings time / timezones to be wary of though).  Here's a quick implementation:A few things to note:  Python's weekdays / months are zero indexed (unlike cron), and that range excludes the last element, hence syntax like "1-5" becomes range(0,5) - ie [0,1,2,3,4].  If you prefer cron syntax, parsing it shouldn't be too difficult however.maybe this has come up only after the question was asked; I thought I just mention it for completeness sake: https://apscheduler.readthedocs.org/en/latest/One thing that in my searches I've seen is python's sched module which might be the kind of thing you're looking for."... Crontab module for read and writing crontab files and accessing the system cron automatically and simply using a direct API. ..."http://pypi.python.org/pypi/python-crontaband also APScheduler, a python package. Already written & debugged.http://packages.python.org/APScheduler/cronschedule.htmlMore or less same as above but concurrent using gevent :)None of the listed solutions even attempt to parse a complex cron schedule string. So, here is my version, using croniter. Basic gist:Helper routines:I have modified the script.Code on GithubI have a minor fix for the CronTab class run method suggested by Brian.The timing was out by one second leading to a one-second, hard loop at the end of each minute.There isn't a "pure python" way to do this because some other process would have to launch python in order to run your solution.  Every platform will have one or twenty different ways to launch processes and monitor their progress.  On unix platforms, cron is the old standard.  On Mac OS X there is also launchd, which combines cron-like launching with watchdog functionality that can keep your process alive if that's what you want. Once python is running, then you can use the sched module to schedule tasks.I know there are a lot of answers, but another solution could be to go with decorators. This is an example to repeat a function everyday at a specific time. The cool think about using this way is that you only need to add the Syntactic Sugar to the function you want to schedule: And the decorator will look like:Brian's solution is working quite well. However, as others have pointed out, there is a subtle bug in the run code. Also i found it overly complicated for the needs.Here is my simpler and functional alternative for the run code in case anybody needs it:Another trivial solution would be:And the class aqcron.At is:If you are looking for a distributed scheduler, you can check out https://github.com/sherinkurian/mani - it does need redis though so might not be what you are looking for. (note that i am the author)

this was built to ensure fault-tolerance by having clock run on more than one node.Method of Crontab on Server. Python file name hello.pyStep1: Create a sh file let give name s.shStep2: Open Crontab EditorStep3: Add Schedule TimeUse Crontab FormattingThis cron will run「At minute 2.」I don't know if something like that already exists. It would be easy to write your own with time, datetime and/or calendar modules, see http://docs.python.org/library/time.htmlThe only concern for a python solution is that your job needs to be always running and possibly be automatically "resurrected" after a reboot, something for which you do need to rely on system dependent solutions. You can check out PiCloud's [1] Crons [2], but do note that your jobs won't be running on your own machine. It's also a service that you'll need to pay for if you use more than 20 hours of compute time a month.[1] http://www.picloud.com[2] http://docs.picloud.com/cron.htmlI like how the pycron package solves this problem.

Python Requests throwing SSLError

TedBurrows

[Python Requests throwing SSLError](https://stackoverflow.com/questions/10667960/python-requests-throwing-sslerror)

I'm working on a simple script that involves CAS, jspring security check, redirection, etc.  I would like to use Kenneth Reitz's python requests because it's a great piece of work!  However, CAS requires getting validated via SSL so I have to get past that step first.  I don't know what Python requests is wanting?  Where is this SSL certificate supposed to reside?

2012-05-19 18:45:20Z

I'm working on a simple script that involves CAS, jspring security check, redirection, etc.  I would like to use Kenneth Reitz's python requests because it's a great piece of work!  However, CAS requires getting validated via SSL so I have to get past that step first.  I don't know what Python requests is wanting?  Where is this SSL certificate supposed to reside?The problem you are having is caused by an untrusted SSL certificate.Like @dirk mentioned in a previous comment, the quickest fix is setting verify=False:Please note that this will cause the certificate not to be verified. This will expose your application to security risks, such as man-in-the-middle attacks. Of course, apply judgment. As mentioned in the comments, this may be acceptable for quick/throwaway applications/scripts, but really should not go to production software. If just skipping the certificate check is not acceptable in your particular context, consider the following options, your best option is to set the verify parameter to a string that is the path of the .pem file of the certificate (which you should obtain by some sort of secure means).So, as of version 2.0, the verify parameter accepts the following values, with their respective semantics:Source: Requests - SSL Cert VerificationAlso take a look at the cert parameter on the same link.From requests documentation on SSL verification:If you don't want to verify your SSL certificate, make verify=False The name of CA file to use you could pass via verify:If you use verify=True then requests uses its own CA set that might not have CA that signed your server certificate.$ pip install -U requests[security]When this question was opened (2012-05) the Requests version was 0.13.1. On version 2.4.1 (2014-09) the "security" extras were introduced, using certifi package if available. Right now (2016-09) the main version is 2.11.1, that works good without verify=False. No need to use requests.get(url, verify=False), if installed with requests[security] extras.I encountered the same issue and ssl certificate verify failed issue when using aws boto3, by review boto3 code, I found the REQUESTS_CA_BUNDLE is not set, so I fixed the both issue by setting it manually:For aws-cli, I guess setting REQUESTS_CA_BUNDLE in ~/.bashrc will fix this issue (not tested because my aws-cli works without it).In case you have a library that relies on requests and you cannot modify the verify path (like with pyvmomi) then you'll have to find the cacert.pem bundled with requests and append your CA there. Here's a generic approach to find the cacert.pem location:windowslinuxbtw. @requests-devs, bundling your own cacerts with request is really, really annoying... especially the fact that you do not seem to use the system ca store first and this is not documented anywhere.updatein situations, where you're using a library and have no control over the ca-bundle location you could also explicitly set the ca-bundle location to be your host-wide ca-bundle:If you want to remove the warnings, use the code below.and verify=False with request.get or post methodI face the same problem using gspread and these commands works for me:I have found an specific approach for solving a similar issue. The idea is pointing the cacert file stored at the system and used by another ssl based applications.In Debian (I'm not sure if same in other distributions) the certificate files (.pem) are stored at /etc/ssl/certs/ So, this is the code that work for me:For guessing what pem file choose, I have browse to the url and check which Certificate Authority (CA) has generated the certificate.EDIT: if you cannot edit the code (because you are running a third app) you can try to add the pem certificate directly into /usr/local/lib/python2.7/dist-packages/requests/cacert.pem (e.g. copying it to the end of the file).If you don't bother about certificate just use verify=False.After hours of debugging I could only get this to work using the following packages:using OpenSSL 1.0.2g  1 Mar 2016Without these packages verify=False was not working.I hope this helps someone.I ran into the same issue. Turns out I hadn't installed the intermediate certificate on my server (just append it to the bottom of your certificate as seen below).https://www.digicert.com/ssl-support/pem-ssl-creation.htmMake sure you have the ca-certificates package installed:Updating the time may also resolve this:If you're using a self-signed certificate, you'll probably have to add it to your system manually.If the request calls are buried somewhere deep in the code and you do not want to install the server certificate, then, just for debug purposes only, it's possible to monkeypatch requests:Never use in production!I fought this problem for HOURS.  I tried to update requests.  Then I updated certifi.  I pointed verify to certifi.where() (The code does this by default anyways).  Nothing worked.Finally I updated my version of python to python 2.7.11.  I was on Python 2.7.5 which had some incompatibilities with the way that the certificates are verified.  Once I updated Python (and a handful of other dependencies) it started working.There is currently an issue in the requests module causing this error, present in v2.6.2 to v2.12.4 (ATOW): https://github.com/kennethreitz/requests/issues/2573Workaround for this issue is adding the following line: requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS'This is similar to @rafael-almeida 's answer, but I want to point out that as of requests 2.11+, there are not 3 values that verify can take, there are actually 4:The rest of my answer is about #4, how to use a directory containing certificates to validate:Obtain the public certificates needed and place them in a directory.  Strictly speaking, you probably "should" use an out-of-band method of obtaining the certificates, but you could also just download them using any browser.  If the server uses a certificate chain, be sure to obtain every single certificate in the chain.According to the requests documentation, the directory containing the certificates must first be processed with the "rehash" utility (openssl rehash).  (This requires openssl 1.1.1+, and not all Windows openssl implementations support rehash.  If openssl rehash won't work for you, you could try running the rehash ruby script at https://github.com/ruby/openssl/blob/master/sample/c_rehash.rb , though I haven't tried this. )I had some trouble with getting requests to recognize my certificates, but after I used the openssl x509 -outform PEM command to convert the certs to Base64 .pem format, everything worked perfectly.You can also just do lazy rehashing:As mentioned by @Rafael Almeida, the problem you are having is caused by an untrusted SSL certificate. In my case, the SSL certificate was untrusted by my server. To get around this without compromising security, I downloaded the certificate, and installed it on the server (by simply double clicking on the .crt file and then Install Certificate...).Too late to the party I guess but I wanted to paste the fix for fellow wanderers like myself! So the following worked out for me on Python 3.7.xType the following in your terminal Try running your script/requests again and see if it works (I'm sure it won't be fixed yet!). If it didn't work then try running the following command in the terminal directlyIt is not feasible to add options if requests is being called from another package. In that case adding certificates to the cacert bundle is the straight path, e.g. I had to add "StartCom Class 1 Primary Intermediate Server CA", for which I downloaded the root cert into StartComClass1.pem. given my virtualenv is named caldav, I added the certificate with:one of those might be enough, I did not checkI was having a similar or the same certification validation problem.  I read that OpenSSL versions less than 1.0.2, which requests depends upon sometimes have trouble validating strong certificates (see here).  CentOS 7 seems to use 1.0.1e which seems to have the problem.  I wasn't sure how to get around this problem on CentOS, so I decided to allow weaker 1024bit CA certificates.  I had to upgrade from Python 3.4.0 to 3.4.6In my case the reason was fairly trivial.I had known that the SSL verification had worked until a few days earlier, and was infact working on a different machine.My next step was to compare the certificate contents and size between the machine on which verification was working, and the one on which it was not. This quickly led to me determining that the Certificate on the 'incorrectly' working machine was not good, and once I replaced it with the 'good' cert, everything was fine. 

What SOAP client libraries exist for Python, and where is the documentation for them? [closed]

damon

[What SOAP client libraries exist for Python, and where is the documentation for them? [closed]](https://stackoverflow.com/questions/206154/what-soap-client-libraries-exist-for-python-and-where-is-the-documentation-for)

I've never used SOAP before and I'm sort of new to Python.  I'm doing this to get myself acquainted with both technologies.  I've installed SOAPlib and I've tried to read their Client documentation, but I don't understand it too well.  Is there anything else I can look into which is more suited for being a SOAP Client library for Python?Edit: Just in case it helps, I'm using Python 2.6.

2008-10-15 19:50:07Z

I've never used SOAP before and I'm sort of new to Python.  I'm doing this to get myself acquainted with both technologies.  I've installed SOAPlib and I've tried to read their Client documentation, but I don't understand it too well.  Is there anything else I can look into which is more suited for being a SOAP Client library for Python?Edit: Just in case it helps, I'm using Python 2.6.Update (2016):If you only need SOAP client, there is well maintained library called zeep. It supports both Python 2 and 3 :)Update:Additionally to what is mentioned above, I will refer to Python WebServices page which is always up-to-date with all actively maintained and recommended modules to SOAP and all other webservice types.Unfortunately, at the moment, I don't think there is a "best" Python SOAP library. Each of the mainstream ones available has its own pros and cons.Older libraries:"Newer" libraries:Of the above, I've only used SUDS personally, and I liked it a lot.I followed the advice of other answers to this question and gave SUDS a try. After using it "in anger" I must agree: SUDS is very nice! Highly recommended!I did run into trouble calling HTTPS-based web services from behind a proxy. At the time of this writing, this affects all Python web-service clients that use urllib2, so I'll document the solution here. The urllib2 module shipping with python 2.6.2 and below will not issue a CONNECT to the proxy for HTTPS-over-HTTP-proxy sessions. This results in a long timeout, or if you are lucky, an error that looks like:This was issue1424152 on the Python bug tracker. There are patches attached to the bug report that will fix this in Python 2.x and Python 3.x. The issue is already fixed.I had good experience with SUDS

https://fedorahosted.org/sudsUsed their TestSuite as documentation.SUDS is the way to go, no question about it.Just an FYI warning for people looking at SUDS, until this ticket is resolved, SUDS does not support the "choice" tag in WSDL:https://fedorahosted.org/suds/ticket/342see:

suds and choice tagSUDS is easy to use, but is not guaranteed to be re-entrant. If you're keeping the WSDL Client() object around in a threaded app for better performance, there's some risk involved. The solution to this risk, the clone() method, throws the unrecoverable Python 5508 bug, which seems to print but not really throw an exception. Can be confusing, but it works. It is still by far the best Python SOAP client. We released a new library: PySimpleSOAP, that provides support for simple and functional client/server. It goals are: ease of use and flexibility (no classes, autogenerated code or xml is required), WSDL introspection and generation, WS-I standard compliance, compatibility (including Java AXIS, .NET and Jboss WS). It is included into Web2Py to enable full-stack solutions (complementing other supported protocols such as XML_RPC, JSON, AMF-RPC, etc.).If someone is learning SOAP or want to investigate it, I think it is a good choice to start. I believe soaplib has deprecated its SOAP client ('sender') in favor of suds. At this point soaplib is focused on being a web framework agnostic SOAP server ('receiver'). Currently soaplib is under active development and is usually discussed in the Python SOAP mailing list:http://mail.python.org/mailman/listinfo/soapAs I suggested here I recommend you roll your own.  It's actually not that difficult and I suspect that's the reason there aren't better Python SOAP libraries out there.suds is pretty good. I tried SOAPpy but didn't get it to work in quite the way I needed whereas suds worked pretty much straight away. In my conclusion we have this:Soap client side:use only Suds-jurko (updated 2016)

suds is well maintained and updated.UPDATE 06/2017: suds-jurko library is not updated and apparently abandoned, I tested zeep library but got limitations around tokens, by now just support UsernameToken, i report a bug to create timestamp token and author update the code to fix it.Zeep start good and has good documentation , so i recently migrated my code from suds to zeep and works fine.Soap server side:We have TGWS, soaplib (pysimplesoap not tested) IMHO use and help soaplib must be the choice.Best regards,Could this help: http://users.skynet.be/pascalbotte/rcx-ws-doc/python.htm#SOAPPYI found it by searching for wsdl and python, with the rational being, that you would need a wsdl description of a SOAP server to do any useful client wrappers....We'd used SOAPpy from Python Web Services, but it seems that ZSI (same source) is replacing it.Im using SOAPpy with Python 2.5.3 in a production setting.I had to manually edit a couple of files in SOAPpy (something about header code being in the wrong place) but other than that it worked and continues to do so very reliably.

Python argparse: How to insert newline in the help text?

kennytm

[Python argparse: How to insert newline in the help text?](https://stackoverflow.com/questions/3853722/python-argparse-how-to-insert-newline-in-the-help-text)

I'm using argparse in Python 2.7 for parsing input options. One of my options is a multiple choice. I want to make a list in its help text, e.g.However, argparse strips all newlines and consecutive spaces. The result looks likeHow to insert newlines in the help text?

2010-10-04 08:40:00Z

I'm using argparse in Python 2.7 for parsing input options. One of my options is a multiple choice. I want to make a list in its help text, e.g.However, argparse strips all newlines and consecutive spaces. The result looks likeHow to insert newlines in the help text?Try using RawTextHelpFormatter:If you just want to override the one option, you should not use RawTextHelpFormatter. Instead subclass the HelpFormatter and provide a special intro for the options that should be handled "raw" (I use "R|rest of help"):And use it:Any other calls to .add_argument() where the help does not start with R| will be wrapped as normal.This is part of my improvements on argparse. The full SmartFormatter also supports adding

the defaults to all options, and raw input of the utilities description. The full version

has its own _split_lines method, so that any formatting done to e.g. version strings is preserved:Another easy way to do it is to include textwrap.For example,In this way, we can avoid the long empty space in front of each output line.I've faced similar issue (Python 2.7.6). I've tried to break down description section into several lines using RawTextHelpFormatter:And got:So RawTextHelpFormatter is not a solution. Because it prints description as it appears in source code, preserving all whitespace characters (I want to keep extra tabs in my source code for readability but I don't want to print them all. Also raw formatter doesn't wrap line when it is too long, more than 80 characters for example).Thanks to @Anton who inspired the right direction above. But that solution needs slight modification in order to format description section.Anyway, custom formatter is needed. I extended existing HelpFormatter class and overrode _fill_text method like this:Compare with the original source code coming from argparse module:In the original code the whole description is being wrapped. In custom formatter above the whole text is split into several chunks, and each of them is formatted independently.So with aid of custom formatter:the output is:I wanted to have both manual line breaks in the description text, and auto wrapping of it; but none of the suggestions here worked for me - so I ended up modifying the SmartFormatter class given in the answers here; the issues with the argparse method names not being a public API notwithstanding, here is what I have (as a file called test.py):This is how it works in 2.7 and 3.4:Starting from SmartFomatter described above, I ended to that solution:Note that strangely the formatter_class argument passed to top level parser is not inheritated by sub_parsers, one must pass it again for each created sub_parser.For this question, argparse.RawTextHelpFormatter is helpful to me.Now, I want to share how do I use the argparse.I know it may not be related to question,but these questions have been bothered me for a while.So I want to share my experience, hope that will be helpful for someone.Here we go.colorama: for change the text color: pip install coloramaWhere the class of FormatText is the following

Installing pip packages to $HOME folder

Somebody still uses you MS-DOS

[Installing pip packages to $HOME folder](https://stackoverflow.com/questions/7143077/installing-pip-packages-to-home-folder)

Is it possible? When installing pip, install the python packages inside my $HOME folder. (for example, I want to install mercurial, using pip, but inside $HOME instead of /usr/local)I'm with a mac machine and just thought about this possibility, instead of "polluting" my /usr/local, I would use my $HOME instead.PEP370 is exactly about this. Is just creating a ˜/.local and do a pip install package enough to make these packages to be installed only at my $HOME folder?

2011-08-22 04:10:25Z

Is it possible? When installing pip, install the python packages inside my $HOME folder. (for example, I want to install mercurial, using pip, but inside $HOME instead of /usr/local)I'm with a mac machine and just thought about this possibility, instead of "polluting" my /usr/local, I would use my $HOME instead.PEP370 is exactly about this. Is just creating a ˜/.local and do a pip install package enough to make these packages to be installed only at my $HOME folder?While you can use a virtualenv, you don't need to.  The trick is passing the PEP370 --user argument to the setup.py script.  With the latest version of pip, one way to do it is:This should result in the hg script being installed in $HOME/.local/bin/hg and the rest of the hg package in $HOME/.local/lib/pythonx.y/site-packages/.Note, that the above is true for Python 2.6.  There has been a bit of controversy among the Python core developers about what is the appropriate directory location on Mac OS X for PEP370-style user installations.  In Python 2.7 and 3.2, the location on Mac OS X was changed from $HOME/.local to $HOME/Library/Python.  This might change in a future release.  But, for now, on 2.7 (and 3.2, if hg were supported on Python 3), the above locations will be $HOME/Library/Python/x.y/bin/hg and $HOME/Library/Python/x.y/lib/python/site-packages.I would use virtualenv at your HOME directory.You could then also alter ~/.(login|profile|bash_profile), whichever is right for your shell to add ~/bin to your PATH and then that pip|python|easy_install would be the one used by default.You can specify the -t option (--target) to specify the destination directory. See pip install --help for detailed information. This is the command you need: for example, for installing say mxnet, in my $HOME directory, I type: 

What are the differences between numpy arrays and matrices? Which one should I use?

levesque

[What are the differences between numpy arrays and matrices? Which one should I use?](https://stackoverflow.com/questions/4151128/what-are-the-differences-between-numpy-arrays-and-matrices-which-one-should-i-u)

What are the advantages and disadvantages of each?From what I've seen, either one can work as a replacement for the other if need be, so should I bother using both or should I stick to just one of them?Will the style of the program influence my choice? I am doing some machine learning using numpy, so there are indeed lots of matrices, but also lots of vectors (arrays).

2010-11-11 03:25:09Z

What are the advantages and disadvantages of each?From what I've seen, either one can work as a replacement for the other if need be, so should I bother using both or should I stick to just one of them?Will the style of the program influence my choice? I am doing some machine learning using numpy, so there are indeed lots of matrices, but also lots of vectors (arrays).Numpy matrices are strictly 2-dimensional, while numpy arrays (ndarrays) are

N-dimensional.  Matrix objects are a subclass of ndarray, so they inherit all

the attributes and methods of ndarrays.The main advantage of numpy matrices is that they provide a convenient notation

for matrix multiplication: if a and b are matrices, then a*b is their matrix

product.On the other hand, as of Python 3.5, NumPy supports infix matrix multiplication using the @ operator, so you can achieve the same convenience of matrix multiplication with ndarrays in Python >= 3.5.Both matrix objects and ndarrays have .T to return the transpose, but matrix

objects also have .H for the conjugate transpose, and .I for the inverse.In contrast, numpy arrays consistently abide by the rule that operations are

applied element-wise (except for the new @ operator). Thus, if a and b are numpy arrays, then a*b is the array

formed by multiplying the components element-wise:To obtain the result of matrix multiplication, you use np.dot (or @ in Python >= 3.5, as shown above):The ** operator also behaves differently:Since a is a matrix, a**2 returns the matrix product a*a.

Since c is an ndarray, c**2 returns an ndarray with each component squared

element-wise.There are other technical differences between matrix objects and ndarrays

(having to do with np.ravel, item selection and sequence behavior).The main advantage of numpy arrays is that they are more general than

2-dimensional matrices. What happens when you want a 3-dimensional array? Then

you have to use an ndarray, not a matrix object. Thus, learning to use matrix

objects is more work -- you have to learn matrix object operations, and

ndarray operations.Writing a program that uses both matrices and arrays makes your life difficult

because you have to keep track of what type of object your variables are, lest

multiplication return something you don't expect.In contrast, if you stick solely with ndarrays, then you can do everything

matrix objects can do, and more, except with slightly different

functions/notation.If you are willing to give up the visual appeal of NumPy matrix product

notation (which can be achieved almost as elegantly with ndarrays in Python >= 3.5), then I think NumPy arrays are definitely the way to go.PS. Of course, you really don't have to choose one at the expense of the other,

since np.asmatrix and np.asarray allow you to convert one to the other (as

long as the array is 2-dimensional).There is a synopsis of the differences between NumPy arrays vs NumPy matrixes here.Scipy.org recommends that you use arrays:Just to add one case to unutbu's list. One of the biggest practical differences for me of numpy ndarrays compared to numpy matrices or matrix languages like matlab, is that the dimension is not preserved in reduce operations. Matrices are always 2d, while the mean of an array, for example, has one dimension less. For example demean rows of a matrix or array:with matrixwith arrayI also think that mixing arrays and matrices gives rise to many "happy" debugging hours.

However, scipy.sparse matrices are always matrices in terms of operators like multiplication.As others have mentioned, perhaps the main advantage of matrix was that it provided a convenient notation for matrix multiplication.However, in Python 3.5 there is finally a dedicated infix operator for matrix multiplication: @.With recent NumPy versions, it can be used with ndarrays:So nowadays, even more, when in doubt, you should stick to ndarray.

Why isn't Python very good for functional programming? [closed]

David Johnstone

[Why isn't Python very good for functional programming? [closed]](https://stackoverflow.com/questions/1017621/why-isnt-python-very-good-for-functional-programming)

I have always thought that functional programming can be done in Python. Thus, I was surprised that Python didn't get much of a mention in this question, and when it was mentioned, it normally wasn't very positive. However, not many reasons were given for this (lack of pattern matching and algebraic data types were mentioned). So my question is: why isn't Python very good for functional programming? Are there more reasons than its lack of pattern matching and algebraic data types? Or are these concepts so important to functional programming that a language that doesn't support them can only be classed as a second rate functional programming language? (Keep in mind that my experience with functional programming is quite limited.)

2009-06-19 12:12:06Z

I have always thought that functional programming can be done in Python. Thus, I was surprised that Python didn't get much of a mention in this question, and when it was mentioned, it normally wasn't very positive. However, not many reasons were given for this (lack of pattern matching and algebraic data types were mentioned). So my question is: why isn't Python very good for functional programming? Are there more reasons than its lack of pattern matching and algebraic data types? Or are these concepts so important to functional programming that a language that doesn't support them can only be classed as a second rate functional programming language? (Keep in mind that my experience with functional programming is quite limited.)The question you reference asks which languages promote both OO and functional programming. Python does not promote functional programming even though it works fairly well.The best argument against functional programming in Python is that imperative/OO use cases are carefully considered by Guido, while functional programming use cases are not. When I write imperative Python, it's one of the prettiest languages I know. When I write functional Python, it becomes as ugly and unpleasant as your average language that doesn't have a BDFL.Which is not to say that it's bad, just that you have to work harder than you would if you switched to a language that promotes functional programming or switched to writing OO Python.Here are the functional things I miss in Python:Guido has a good explanation of this here.  Here's the most relevant part:I pull two things out of this:Scheme doesn't have algebraic data types or pattern matching but it's certainly a functional language. Annoying things about Python from a functional programming perspective:On the other hand, python has lexical closures, Lambdas, and list comprehensions (which are really a "functional" concept whether or not Guido admits it). I do plenty of "functional-style" programming in Python, but I'd hardly say it's ideal.I would never call Python「functional」but whenever I program in Python the code invariably ends up being almost purely functional.Admittedly, that's mainly due to the extremely nice list comprehension. So I wouldn't necessarily suggest Python as a functional programming language but I would suggest functional programming for anyone using Python.Let me demonstrate with a piece of code taken from an answer to a "functional" Python question on SOPython:Haskell:The main difference here is that Haskell's standard library has useful functions for functional programming: in this case iterate, concat, and (!!)One thing that is really important for this question (and the answers) is the following:

What the hell is functional programming, and what are the most important properties of it.

I'll try to give my view of it:Functional programming is a lot like writing math on a whiteboard. When you write equations

on a whiteboard, you do not think about an execution order. There is (typically) no mutation.

You don't come back the day after and look at it, and when you make the calculations again,

you get a different result (or you may, if you've had some fresh coffee :)). Basically,

what is on the board is there, and the answer was already there when you started writing

things down, you just haven't realized what it is yet.Functional programming is a lot like that; you don't change things, you just evaluate

the equation (or in this case, "program") and figure out what the answer is. The program

is still there, unmodified. The same with the data.I would rank the following as the most important features of functional programming:

a) referential transparency - if you evaluate the same statement at some other time

   and place, but with the same variable values, it will still mean the same.

b) no side effect - no matter how long you stare at the whiteboard, the equation another

   guy is looking at at another whiteboard won't accidentally change.

c) functions are values too. which can be passed around and applied with, or to, other

   variables.

d) function composition, you can do h=g·f and thus define a new function h(..) which is

   equivalent to calling g(f(..)).This list is in my prioritized order, so referential transparency is the most important,

followed by no side effects.Now, if you go through python and check how well the language and libraries supports,

and guarantees, these aspects - then you are well on the way to answer your own question.Python is almost a functional language.  It's "functional lite".  It has extra features, so it isn't pure enough for some.  It also lacks some features, so it isn't complete enough for some.  The missing features are relatively easy to write.  Check out posts like this on FP in Python.Another reason not mentioned above is that many built-in functions and methods of built-in types modify an object but do not return the modified object. If those modified objects were returned, that would make functional code cleaner and more concise. For example, if some_list.append(some_object) returned some_list with some_object appended.In addition to other answers, one reason Python and most other multi-paradigm languages are not well suited for true functional programming is because their compilers / virtual machines / run-times do not support functional optimization.  This sort of optimization is achieved by the compiler understanding mathematical rules.  For example, many programming languages support a map function or method.  This is a fairly standard function that takes a function as one argument and a iterable as the second argument then applies that function to each element in the iterable.  Anyways it turns out that map( foo() , x ) * map( foo(), y ) is the same as map( foo(), x * y ).  The latter case is actually faster than the former because the former performs two copies where the latter performs one.Better functional languages recognize these mathematically based relationships and automatically perform the optimization.  Languages that aren't dedicated to the functional paradigm will likely not optimize.

How to rename a file using Python

zjm1126

[How to rename a file using Python](https://stackoverflow.com/questions/2491222/how-to-rename-a-file-using-python)

I want to change a.txt to b.kml.

2010-03-22 09:59:23Z

I want to change a.txt to b.kml.Use os.rename:File may be inside a directory, in that case specify the path:This will work to rename or move a file.As of Python 3.4 one can use the pathlib module to solve this.If you happen to be on an older version, you can use the backported version found hereLet's assume you are not in the root path (just to add a bit of difficulty to it) you want to rename, and have to provide a full path, we can look at this: So, you can take your path and create a Path object out of it:Just to provide some information around this object we have now, we can extract things out of it. For example, if for whatever reason we want to rename the file by modifying the filename from the_file to the_file_1, then we can get the filename part:And still hold the extension in hand as well: We can perform our modification with a simple string manipulation:Python 3.6 and greater make use of f-strings!Otherwise:And now we can perform our rename by calling the rename method on the path object we created and appending the ext to complete the proper rename structure we want:More shortly to showcase its simplicity: Python 3.6+:Versions less than Python 3.6 use the string format method instead:os.rename(old, new)This is found in the Python docs: http://docs.python.org/library/os.htmlUse os.rename. But you have to pass full path of both files to the function. If I have a file a.txt on my desktop so I will do and also I have to give full of renamed file too.You can use os.system to invoke terminal to accomplish the task:this should do it. python 3+

How to apply a function to two columns of Pandas dataframe

bigbug

[How to apply a function to two columns of Pandas dataframe](https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe)

Suppose I have a df which has columns of 'ID', 'col_1', 'col_2'. And I define a function :f = lambda x, y : my_function_expression.Now I want to apply the f to df's two columns 'col_1', 'col_2' to element-wise calculate a new column 'col_3' , somewhat like :How to do ?** Add detail sample as below ***

2012-11-11 13:48:53Z

Suppose I have a df which has columns of 'ID', 'col_1', 'col_2'. And I define a function :f = lambda x, y : my_function_expression.Now I want to apply the f to df's two columns 'col_1', 'col_2' to element-wise calculate a new column 'col_3' , somewhat like :How to do ?** Add detail sample as below ***Here's an example using apply on the dataframe, which I am calling with axis = 1. Note the difference is that instead of trying to pass two values to the function f, rewrite the function to accept a pandas Series object, and then index the Series to get the values needed. Depending on your use case, it is sometimes helpful to create a pandas group object, and then use apply on the group. There is a clean, one-line way of doing this in Pandas:This allows f to be a user-defined function with multiple input values, and uses (safe) column names rather than (unsafe) numeric indices to access the columns.Example with data (based on original question):Output of print(df):A simple solution is:A interesting question! my answer as below:Output:I changed the column name to ID,J1,J2,J3 to ensure ID < J1 < J2 < J3, so the column display in right sequence.One more brief version:The method you are looking for is Series.combine. 

However, it seems some care has to be taken around datatypes. 

In your example, you would (as I did when testing the answer) naively call However, this throws the error: My best guess is that it seems to expect the result to be of the same type as the series calling the method (df.col_1 here). However, the following works:The way you have written f it needs two inputs. If you look at the error message it says you are not providing two inputs to f, just one. The error message is correct.

The mismatch is because df[['col1','col2']] returns a single dataframe with two columns, not two separate columns.You need to change your f so that it takes a single input, keep the above data frame as input, then break it up into x,y inside the function body. Then do whatever you need and return a single value.You need this function signature because the syntax is .apply(f)

So f needs to take the single thing = dataframe and not two things which is what your current f expects.  Since you haven't provided the body of f I can't help in anymore detail - but this should provide the way out without fundamentally changing your code or using some other methods rather than applyI'm going to put in a vote for np.vectorize. It allows you to just shoot over x number of columns and not deal with the dataframe in the function, so it's great for functions you don't control or doing something like sending 2 columns and a constant into a function (i.e. col_1, col_2, 'foo').Returning a list from apply is a dangerous operation as the resulting object is not guaranteed to be either a Series or a DataFrame. And exceptions might be raised in certain cases. Let's walk through a simple example:There are three possible outcomes with returning a list from apply1) If the length of the returned list is not equal to the number of columns, then a Series of lists is returned.2) When the length of the returned list is equal to the number of

   columns then a DataFrame is returned and each column gets the

   corresponding value in the list.3) If the length of the returned list equals the number of columns for the first row but has at least one row where the list has a different number of elements than number of columns a ValueError is raised.Using apply with axis=1 is very slow. It is possible to get much better performance (especially on larger datasets) with basic iterative methods.Create larger dataframe  @Thomas answerI'm sure this isn't as fast as the solutions using Pandas or Numpy operations, but if you don't want to rewrite your function you can use map.  Using the original example data -We could pass as many arguments as we wanted into the function this way.  The output is what we wantedMy example to your questions:I suppose you don't want to change get_sublist function, and just want to use DataFrame's apply method to do the job. To get the result you want, I've wrote two help functions: get_sublist_list and unlist. As the function name suggest, first get the list of sublist, second extract that sublist from that list. Finally, We need to call apply function to apply those two functions to the df[['col_1','col_2']] DataFrame subsequently.If you don't use [] to enclose the get_sublist function, then the get_sublist_list function will return a plain list, it'll raise ValueError: could not broadcast input array from shape (3) into shape (2), as @Ted Petrou had mentioned.If you have a huge data-set, then you can use an easy but faster(execution time) way of doing this using swifter: 

How to get week number in Python?

Gerry

[How to get week number in Python?](https://stackoverflow.com/questions/2600775/how-to-get-week-number-in-python)

How to find out what week number is current year on June 16th (wk24) with Python?  

2010-04-08 14:35:52Z

How to find out what week number is current year on June 16th (wk24) with Python?  datetime.date has a isocalendar() method, which returns a tuple containing the calendar week:datetime.date.isocalendar() is an instance-method returning a tuple containing year, weeknumber and weekday in respective order for the given date instance.You can get the week number directly from datetime as string.Also you can get different "types" of the week number of the year changing the strftime parameter for:I've found out about it from here. It worked for me in Python 2.7.6I believe date.isocalendar() is going to be the answer. This article explains the math behind ISO 8601 Calendar. Check out the date.isocalendar() portion of the datetime page of the Python documentation..isocalendar() return a 3-tuple with (year, wk num, wk day). dt.isocalendar()[0] returns the year,dt.isocalendar()[1] returns the week number, dt.isocalendar()[2] returns the week day. Simple as can be.Here's another option:which prints 24.See: http://docs.python.org/library/datetime.html#strftime-and-strptime-behaviorThe ISO week suggested by others is a good one, but it might not fit your needs. It assumes each week begins with a Monday, which leads to some interesting anomalies at the beginning and end of the year.If you'd rather use a definition that says week 1 is always January 1 through January 7, regardless of the day of the week, use a derivation like this:Generally to get the current week number (starts from Sunday):For the integer value of the instantaneous week of the year try:If you are only using the isocalendar week number across the board the following should be sufficient:This retrieves the second member of the tuple returned by isocalendar for our week number.However, if you are going to be using date functions that deal in the Gregorian calendar, isocalendar alone will not work!  Take the following example:The string here says to return the Monday of the first week in 2014 as our date.  When we use isocalendar to retrieve the week number here, we would expect to get the same week number back, but we don't.  Instead we get a week number of 2.  Why?Week 1 in the Gregorian calendar is the first week containing a Monday.  Week 1 in the isocalendar is the first week containing a Thursday.  The partial week at the beginning of 2014 contains a Thursday, so this is week 1 by the isocalendar, and making date week 2.If we want to get the Gregorian week, we will need to convert from the isocalendar to the Gregorian.  Here is a simple function that does the trick.You can try %W directive as below:'%W': Week number of the year (Monday as the first day of the week) as a decimal number. All days in a new year preceding the first Monday are considered to be in week 0. (00, 01, ..., 53)isocalendar() returns incorrect year and weeknumber values for some dates:Compare with Mark Ransom's approach:I summarize the discussion to two steps: Warm up```python```1st stepTo manually generate a datetime object, we can use datetime.datetime(2017,5,3) or datetime.datetime.now().But in reality, we usually need to parse an existing string. we can use strptime function, such as datetime.strptime('2017-5-3','%Y-%m-%d') in which you have to specific the format. Detail of different format code can be found in the official documentation. Alternatively, a more convenient way is to use dateparse module. Examples are dateparser.parse('16 Jun 2010'), dateparser.parse('12/2/12') or dateparser.parse('2017-5-3')The above two approaches will return a datetime object. 2nd stepUse the obtained  datetime object to call strptime(format). For example,```python```It's very tricky to decide which format to use. A better way is to get a date object to call isocalendar(). For example,```python```In reality, you will be more likely to use date.isocalendar() to prepare a weekly report, especially in the "Christmas-New Year" shopping season. There are many systems for week numbering. The following are the most common systems simply put with code examples:A lot of answers have been given, but id like to add to them.If you need the week to display as a year/week style (ex. 1953 - week 53 of 2019, 2001 - week 1 of 2020 etc.), you can do this:It will take the current year and week, and long_week_num in the day of writing this will be:

Measuring elapsed time with the Time module

rectangletangle

[Measuring elapsed time with the Time module](https://stackoverflow.com/questions/3620943/measuring-elapsed-time-with-the-time-module)

With the Time module in python is it possible to measure elapsed time? If so, how do I do that? I need to do this so that if the cursor has been in a widget for a certain duration an event happens.  

2010-09-01 18:17:56Z

With the Time module in python is it possible to measure elapsed time? If so, how do I do that? I need to do this so that if the cursor has been in a widget for a certain duration an event happens.  You can also write simple decorator to simplify measurement of execution time of various functions:Usage:You can profile more then one function simultaneously. Then to print measurements just call the print_prof_data():time.time() will do the job.You may want to look at this question, but I don't think it will be necessary.For users that want better formatting,will print out, for 2 seconds:and for 7 minutes one second:note that the minimum time unit with gmtime is seconds. If you need microseconds consider the following:strftime documentationFor the best measure of elapsed time (since Python 3.3), use time.perf_counter().For measurements on the order of hours/days, you don't care about sub-second resolution so use time.monotonic() instead.In many implementations, these may actually be the same thing.Before 3.3, you're stuck with time.clock().New in Python 3.7 is PEP 564 -- Add new time functions with nanosecond resolution.Use of these can further eliminate rounding and floating-point errors, especially if you're measuring very short periods, or your application (or Windows machine) is long-running.Resolution starts breaking down on perf_counter() after around 100 days. So for example after a year of uptime, the shortest interval (greater than 0) it can measure will be bigger than when it started.For a longer period.would printif more than 24 hoursThat is inspired by Rutger Hofste's answer. Thank you Rutger!You need to import time and then use time.time() method to know current time. Another nice way to time things is to use the with python structure.with structure is automatically calling __enter__ and __exit__ methods which is exactly what we need to time things.Let's create a Timer class.Then, one can use the Timer class like this:The result is the following:Primes: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67,

 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151,

157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239,

241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337,

347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433,

439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499]Elapsed time to compute some prime numbers: 5.01704216003418msVadim Shender response is great. You can also use a simpler decorator like below:Here is an update to Vadim Shender's clever code with tabular output:

How to get current CPU and RAM usage in Python?

lpfavreau

[How to get current CPU and RAM usage in Python?](https://stackoverflow.com/questions/276052/how-to-get-current-cpu-and-ram-usage-in-python)

What's your preferred way of getting current system status (current CPU, RAM, free disk space, etc.) in Python? Bonus points for *nix and Windows platforms.There seems to be a few possible ways of extracting that from my search:It's not that those methods are bad but is there already a well-supported, multi-platform way of doing the same thing?

2008-11-09 16:04:50Z

What's your preferred way of getting current system status (current CPU, RAM, free disk space, etc.) in Python? Bonus points for *nix and Windows platforms.There seems to be a few possible ways of extracting that from my search:It's not that those methods are bad but is there already a well-supported, multi-platform way of doing the same thing?The psutil library will give you some system information (CPU / Memory usage) on a variety of platforms:UPDATE: Here is some example usages of psutil:Use the psutil library.  On Ubuntu 18.04, pip installed 5.5.0 (latest version) as of 1-30-2019.  Older versions may behave somewhat differently. 

 You can check your version of psutil by doing this in Python:To get some memory and CPU stats:The virtual_memory (tuple) will have the percent memory used system-wide.  This seemed to be overestimated by a few percent for me on Ubuntu 18.04.You can also get the memory used by the current Python instance:which gives the current memory use of your Python script.There are some more in-depth examples on the pypi page for psutil.Only for Linux: 

One-liner for the RAM usage with only stdlib dependency:edit: specified solution OS dependencyBelow codes, without external libraries worked for me. I tested at Python 2.7.9CPU UsageAnd Ram Usage, Total, Used and FreeHere's something I put together a while ago, it's windows only but may help you get part of what you need done.Derived from:

"for sys available mem"

http://msdn2.microsoft.com/en-us/library/aa455130.aspx"individual process information and python script examples"

http://www.microsoft.com/technet/scriptcenter/scripts/default.mspx?mfr=trueNOTE: the WMI interface/process is also available for performing similar tasks

        I'm not using it here because the current method covers my needs, but if someday it's needed to extend or improve this, then may want to investigate the WMI tools a vailable.WMI for python:http://tgolden.sc.sabren.com/python/wmi.htmlThe code:http://monkut.webfactional.com/blog/archive/2009/1/21/windows-process-memory-logging-pythonI feel like these answers were written for Python 2, and in any case nobody's made mention of the standard resource package that's available for Python 3. It provides commands for obtaining the resource limits of a given process (the calling Python process by default). This isn't the same as getting the current usage of resources by the system as a whole, but it could solve some of the same problems like e.g. "I want to make sure I only use X much RAM with this script."We chose to use usual information source for this because we could find instantaneous fluctuations in free memory and felt querying the meminfo data source was helpful.  This also helped us get a few more related parameters that were pre-parsed.CodeOutput for reference (we stripped all newlines for further analysis)"... current system status (current CPU, RAM, free disk space, etc.)"  And "*nix and Windows platforms" can be a difficult combination to achieve.The operating systems are fundamentally different in the way they manage these resources.  Indeed, they differ in core concepts like defining what counts as system and what counts as application time."Free disk space"?  What counts as "disk space?"  All partitions of all devices?  What about foreign partitions in a multi-boot environment?I don't think there's a clear enough consensus between Windows and *nix that makes this possible.  Indeed, there may not even be any consensus between the various operating systems called Windows.  Is there a single Windows API that works for both XP and Vista?This script for CPU usage:You can use psutil or psmem with subprocess

example code Reference  http://techarena51.com/index.php/how-to-install-python-3-and-flask-on-linux/https://github.com/Leo-g/python-flask-cmdBased on the cpu usage code by @Hrabal, this is what I use:I don't believe that there is a well-supported multi-platform library available. Remember that Python itself is written in C so any library is simply going to make a smart decision about which OS-specific code snippet to run, as you suggested above. 

What's the best practice using a settings file in Python? [closed]

c00kiemonster

[What's the best practice using a settings file in Python? [closed]](https://stackoverflow.com/questions/5055042/whats-the-best-practice-using-a-settings-file-in-python)

I have a command line script that I run with a lot of arguments. I have now come to a point where I have too many arguments, and I want to have some arguments in dictionary form too.So in order to simplify things I would like to run the script with a settings file instead. I don't really know what libraries to use for the parsing of the file. What's the best practice for doing this? I could of course hammer something out myself, but if there is some library for this, I'm all ears.A few 'demands':A simplified pseudo example file:

2011-02-20 03:25:06Z

I have a command line script that I run with a lot of arguments. I have now come to a point where I have too many arguments, and I want to have some arguments in dictionary form too.So in order to simplify things I would like to run the script with a settings file instead. I don't really know what libraries to use for the parsing of the file. What's the best practice for doing this? I could of course hammer something out myself, but if there is some library for this, I'm all ears.A few 'demands':A simplified pseudo example file:You can have a regular Python module, say config.py, like this:and use it like this:The sample config you provided is actually valid YAML.  In fact, YAML meets all of your demands, is implemented in a large number of languages, and is extremely human friendly.  I would highly recommend you use it.  The PyYAML project provides a nice python module, that implements YAML.  To use the yaml module is extremely simple: I Found this the most useful and easy to use

https://wiki.python.org/moin/ConfigParserExamplesYou just create a "myfile.ini" like:And retrieve the data like:Yaml and Json are the simplest and most commonly used file formats to store settings/config. PyYaml can be used to parse yaml. Json is already part of python from 2.5. Yaml is a superset of Json. Json will solve most uses cases except multi line strings where escaping is required. Yaml takes care of these cases too.

Append a dictionary to a dictionary [duplicate]

Javier Novoa C.

[Append a dictionary to a dictionary [duplicate]](https://stackoverflow.com/questions/8930915/append-a-dictionary-to-a-dictionary)

I have two existing dictionaries, and I wish to 'append' one of them to the other. By that I mean that the key,values of the other dictionary should be made into the first dictionary. For example:I think this all can be achieved through a for loop (maybe?), but is there some method of dictionaries or any other module that saves this job for me? The actual dictionaries I'm using are really big...

2012-01-19 17:55:48Z

I have two existing dictionaries, and I wish to 'append' one of them to the other. By that I mean that the key,values of the other dictionary should be made into the first dictionary. For example:I think this all can be achieved through a for loop (maybe?), but is there some method of dictionaries or any other module that saves this job for me? The actual dictionaries I'm using are really big...You can door, if you don't want orig to be modified, make a copy first:Note that if extra and orig have overlapping keys, the final value will be taken from extra. For example,The most Pythonic (and slightly faster) way to accomplish this is by:Or, depending on the problem to solve, maybe:Assuming that you do not want to change orig, you can either do a copy and update like the other answers, or you can create a new dictionary in one step by passing all items from both  dictionaries into the dict constructor:Or without itertools:Note that you only need to pass the result of items() into list() on Python 3, on 2.x dict.items() already returns a list so you can just do dict(orig.items() + extra.items()).As a more general use case, say you have a larger list of dicts that you want to combine into a single dict, you could do something like this:dict.update() looks like it will do what you want...Perhaps, though, you don't want to update your original dictionary, but work on a copy:There is the .update() method :)The answer I want to give is "use collections.ChainMap", but I just discovered that it was only added in Python 3.3: https://docs.python.org/3.3/library/collections.html#chainmap-objectsYou can try to crib the class from the 3.3 source though: http://hg.python.org/cpython/file/3.3/Lib/collections/init.py#l763Here is a less feature-full Python 2.x compatible version (same author): http://code.activestate.com/recipes/305268-chained-map-lookups/Instead of expanding/overwriting one dictionary with another using dict.merge, or creating an additional copy merging both, you create a lookup chain that searches both in order. Because it doesn't duplicate the mappings it wraps ChainMap uses very little memory, and sees later modifications to any sub-mapping. Because order matters you can also use the chain to layer defaults (i.e. user prefs > config > env).A three-liner to combine or merge two dictionaries:This creates a new dictionary dest without modifying orig and extra.Note: If a key has different values in orig and extra, then extra overrides orig.

How to query as GROUP BY in django?

simplyharsh

[How to query as GROUP BY in django?](https://stackoverflow.com/questions/629551/how-to-query-as-group-by-in-django)

I query a model:And it returns:What I want is to know the best Django way to fire

a group_by query to my database, like:Which doesn't work, of course.

I know we can do some tricks on django/db/models/query.py, but I am just curious to know how to do it without patching.

2009-03-10 10:10:30Z

I query a model:And it returns:What I want is to know the best Django way to fire

a group_by query to my database, like:Which doesn't work, of course.

I know we can do some tricks on django/db/models/query.py, but I am just curious to know how to do it without patching.If you mean to do aggregation you can use the aggregation features of the ORM:This results in a query similar toand the output would be of the formAn easy solution, but not the proper way is to use raw SQL:Another solution is to use the group_by property:You can now iterate over the results variable to retrieve your results. Note that group_by is not documented and may be changed in future version of Django.And... why do you want to use group_by? If you don't use aggregation, you can use order_by to achieve an alike result.You can also use the regroup template tag to group by attributes. From the docs:Looks like this:It also works on QuerySets I believe.source: https://docs.djangoproject.com/en/2.1/ref/templates/builtins/#regroupedit: note the regroup tag does not work as you would expect it to if your list of dictionaries is not key-sorted. It works iteratively. So sort your list (or query set) by the key of the grouper before passing it to the regroup tag.You need to do custom SQL as exemplified in this snippet:Custom SQL via subqueryOr in a custom manager as shown in the online Django docs:Adding extra Manager methodsDjango does not support free group by queries. I learned it in the very bad way. ORM is not designed to support stuff like what you want to do, without using custom SQL. You are limited to:Over a queryset qs you can call qs.query.group_by = ['field1', 'field2', ...] but it is risky if you don't know what query are you editing and have no guarantee that it will work and not break internals of the QuerySet object. Besides, it is an internal (undocumented) API you should not access directly without risking the code not being anymore compatible with future Django versions.There is module that allows you to group Django models and still work with a QuerySet in the result: https://github.com/kako-nawao/django-group-byFor example:'book/books.html'The difference to the annotate/aggregate basic Django queries is the use of the attributes of a related field, e.g. book.author.last_name.If you need the PKs of the instances that have been grouped together, add the following annotation:NOTE: ArrayAgg is a Postgres specific function, available from Django 1.9 onwards: https://docs.djangoproject.com/en/1.10/ref/contrib/postgres/aggregates/#arrayagg The document says that you can use values to group the queryset .  You can find all the books and group them by name using this code:You can watch some cheet sheet here.If I'm not mistaking you can use, whatever-query-set.group_by=['field'] first you need to import Sum

then ..

「pip install unroll」:「python setup.py egg_info」failed with error code 1

benjaminh

[「pip install unroll」:「python setup.py egg_info」failed with error code 1](https://stackoverflow.com/questions/35991403/pip-install-unroll-python-setup-py-egg-info-failed-with-error-code-1)

I'm new to Python and have been trying to install some packages with pip. But pip install unroll gives meHow can I solve this?

2016-03-14 15:20:21Z

I'm new to Python and have been trying to install some packages with pip. But pip install unroll gives meHow can I solve this?About the error codeAccording to the Python documentation:Error code 1 is defined in errno.h and means Operation not permitted.About your errorYour setuptools do not appear to be installed. Just follow the Installation Instructions from the PyPI website.If it's already installed, tryIf it's already up to date, check that the module ez_setup is not missing. If it is, thenThen try againIf it's still not working, maybe pip didn't install/upgrade setup_tools properly so you might want to tryAnd againHere's a little guide explaining a little bit how I usually install new packages on Python + Windows. It seems you're using Windows paths, so this answer will stick to that particular SO:Now, if we focus in your specific problem, where you're having a hard time installing the unroll package. It seems the fastest way to install it is doing something like this:That way it will install without any problems. To check it really works, just login into the Python installation and try import unroll, it shouldn't complain.One last note: This method works almost 99% of the time, and sometimes you'll find some pip packages which are specific to Unix or Mac OS X, in that case, when that happens I'm afraid the best way to get a Windows version is either posting some issues to the main developers or having some fun by yourself porting to Windows (typically a few hours if you're not lucky) :)It was resolved after upgrading pip:I got stuck exactly with the same error with psycopg2. It looks like I skipped a few steps while installing Python and related packages.(In your case you need to replace psycopg2 with the package you have an issue with.)It worked seamlessly.I got this same error while installing mitmproxy using pip3. The below command fixed this:Python 2.7.11 64 bit usedOther way:I had the same problem.The problem was:pyparsing 2.2 was already installed and my requirements.txt was trying to install pyparsing 2.0.1 which throw this errorContext: I was using virtualenv, and it seems the 2.2 came from my global OS Python site-packages, but even with --no-site-packages flag (now by default in last virtualenv) the 2.2 was still present. Surely because I installed Python from their website and it added Python libraries to my $PATH.Maybe a pip install --ignore-installed would have worked.Solution: as I needed to move forwards, I just removed the pyparsing==2.0.1 from my requirements.txt.I ran into the same error code when trying to install a Python module with pip.

@Hackndo noted that the documentation indicate a security issue.Based on that answer, my problem was solved by running the pip install command with sudo prefixed:I had the same issue when installing the "Twisted" library and solved it by running the following command on Ubuntu 16.04 (Xenial Xerus):I tried all of the above with no success. I then updated my Python version from 2.7.10 to 2.7.13, and it resolved the problems that I was experiencing.That means some packages in pip are old or not correctly installed.This was the easier way for me:So if you was using pip, try to use pip3 or pip2It should solve the problem.This worked for me:Upgrading Python to version 3 fixed my problem. Nothing else did.I downloaded the .whl file from http://www.lfd.uci.edu/~gohlke/pythonlibs/ and then did:Note that the version you need to use (win32/win_amd-64) depends on the version of Python and not that of Windows.I had this problem using virtualenvs (with pipenv) on my new development setup.I could only solve it by upgrading the psycopg2 version from 2.6.2 to 2.7.3. 

More information is at https://github.com/psycopg/psycopg2/issues/594I faced the same problem with the same error message but on Ubuntu 16.04  LTS (Xenial Xerus) instead:I tested all the solutions provided above and none of them worked for me. I read the full TraceBack and found out I had to create the virtual environment with Python version 2.7 instead (the default one uses Python 3.5 instead):Once I activated it, I run pip install unirest successfully.try on linux:I solved it on Centos 7 by using:Had the same problem on my Win10 PC with different packages and tried everything mentioned so far.Finally solved it by disabling Comodo Auto-Containment.Since nobody has mentioned it yet, I hope it helps someone.I had the same problem and was able to fix by doing the following.Windows Python needs Visual C++ libraries installed via the SDK to build code, such as via setuptools.extension.Extension or numpy.distutils.core.Extension. For example, building f2py modules in Windows with Python requires Visual C++ SDK as installed above. On Linux and Mac, the C++ libraries are installed with the compiler.https://www.scivision.co/python-windows-visual-c++-14-required/Following below command worked for me

How do I watch a file for changes?

Jon Cage

[How do I watch a file for changes?](https://stackoverflow.com/questions/182197/how-do-i-watch-a-file-for-changes)

I have a log file being written by another process which I want to watch for changes. Each time a change occurs I'd like to read the new data in to do some processing on it.What's the best way to do this? I was hoping there'd be some sort of hook from the PyWin32 library. I've found the win32file.FindNextChangeNotification function but have no idea how to ask it to watch a specific file.If anyone's done anything like this I'd be really grateful to hear how...[Edit] I should have mentioned that I was after a solution that doesn't require polling.[Edit] Curses! It seems this doesn't work over a mapped network drive. I'm guessing windows doesn't 'hear' any updates to the file the way it does on a local disk.

2008-10-08 11:12:55Z

I have a log file being written by another process which I want to watch for changes. Each time a change occurs I'd like to read the new data in to do some processing on it.What's the best way to do this? I was hoping there'd be some sort of hook from the PyWin32 library. I've found the win32file.FindNextChangeNotification function but have no idea how to ask it to watch a specific file.If anyone's done anything like this I'd be really grateful to hear how...[Edit] I should have mentioned that I was after a solution that doesn't require polling.[Edit] Curses! It seems this doesn't work over a mapped network drive. I'm guessing windows doesn't 'hear' any updates to the file the way it does on a local disk.Have you already looked at the documentation available on http://timgolden.me.uk/python/win32_how_do_i/watch_directory_for_changes.html? If you only need it to work under Windows the 2nd example seems to be exactly what you want (if you exchange the path of the directory with the one of the file you want to watch). Otherwise, polling will probably be the only really platform-independent option.Note: I haven't tried any of these solutions.Did you try using Watchdog?If polling is good enough for you, I'd just watch if the "modified time" file stat changes.  To read it:(Also note that the Windows native change event solution does not work in all circumstances, e.g. on network drives.)If you want a multiplatform solution, then check QFileSystemWatcher.

Here an example code (not sanitized):It should not work on windows (maybe with cygwin ?), but for unix user, you should use the "fcntl" system call. Here is an example in Python. It's mostly the same code if you need to write it in C (same function names)Check out pyinotify.inotify replaces dnotify (from an earlier answer) in newer linuxes and allows file-level rather than directory-level monitoring.Well after a bit of hacking of Tim Golden's script, I have the following which seems to work quite well:It could probably do with a load more error checking, but for simply watching a log file and doing some processing on it before spitting it out to the screen, this works well.Thanks everyone for your input - great stuff!For watching a single file with polling, and minimal dependencies, here is a fully fleshed-out example, based on answer from Deestan (above):Check my answer to a similar question. You could try the same loop in Python. This page suggests:Also see the question tail() a file with Python.Simplest solution for me is using watchdog's tool watchmedoFrom https://pypi.python.org/pypi/watchdog I now have a process that looks up the sql files in a directory and executes them if necessary. Well, since you are using Python, you can just open a file and keep reading lines from it.If the line read is not empty, you process it.You may be missing that it is ok to keep calling readline at the EOF. It will just keep returning an empty string in this case. And when something is appended to the log file, the reading will continue from where it stopped, as you need.If you are looking for a solution that uses events, or a particular library, please specify this in your question. Otherwise, I think this solution is just fine.Here is a simplified version of Kender's code that appears to do the same trick and does not import the entire file:This is another modification of Tim Goldan's script that runs on unix types and adds a simple watcher for file modification by using a dict (file=>time).usage: whateverName.py path_to_dir_to_watchAs you can see in Tim Golden's article, pointed by Horst Gutmann, WIN32 is relatively complex and watches directories, not a single file.I'd like to suggest you look into IronPython, which is a .NET python implementation.

With IronPython you can use all the .NET functionality - includingWhich handles single files with a simple Event interface.This is an example of checking a file for changes. One that may not be the best way of doing it, but it sure is a short way.Handy tool for restarting application when changes have been made to the source. I made this when playing with pygame so I can see effects take place immediately after file save.When used in pygame make sure the stuff in the 'while' loop is placed in your game loop aka update or whatever. Otherwise your application will get stuck in an infinite loop and you will not see your game updating.In case you wanted the restart code which I found on the web. Here it is. (Not relevant to the question, though it could come in handy)Have fun making electrons do what you want them to do.Here's an example geared toward watching input files that write no more than one line per second but usually a lot less.  The goal is to append the last line (most recent write) to the specified output file.  I've copied this from one of my projects and just deleted all the irrelevant lines.  You'll have to fill in or change the missing symbols. Of course, the encompassing QMainWindow class is not strictly required, ie. you can use QFileSystemWatcher alone. The best and simplest solution is to use pygtail:

   https://pypi.python.org/pypi/pygtailYou can also use a simple library called repyt, here is an example:Seems that no one has posted fswatch. It is a cross-platform file system watcher. Just install it, run it and follow the prompts.I've used it with python and golang programs and it just works.related @4Oh4 solution a smooth change for a list of files to watch;I don't know any Windows specific function. You could try getting the MD5 hash of the file every second/minute/hour (depends on how fast you need it) and compare it to the last hash. When it differs you know the file has been changed and you read out the newest lines.I'd try something like this.The loop checks if there is a new line(s) since last time file was read - if there is, it's read and passed to the functionThatAnalisesTheLine function. If not, script waits 1 second and retries the process. 

How to pretty-print a numpy.array without scientific notation and with given precision?

camillio

[How to pretty-print a numpy.array without scientific notation and with given precision?](https://stackoverflow.com/questions/2891790/how-to-pretty-print-a-numpy-array-without-scientific-notation-and-with-given-pre)

I'm curious, whether there is any way to print formatted numpy.arrays, e.g., in a way similar to this:If I want to print the numpy.array of floats, it prints several decimals, often in 'scientific' format, which is rather hard to read even for low-dimensional arrays. However, numpy.array apparently has to be printed as a string, i.e., with %s. Is there a solution for this? 

2010-05-23 12:54:29Z

I'm curious, whether there is any way to print formatted numpy.arrays, e.g., in a way similar to this:If I want to print the numpy.array of floats, it prints several decimals, often in 'scientific' format, which is rather hard to read even for low-dimensional arrays. However, numpy.array apparently has to be printed as a string, i.e., with %s. Is there a solution for this? You can use set_printoptions to set the precision of the output:And suppress suppresses the use of scientific notation for small numbers:See the docs for set_printoptions for other options.To apply print options locally, using NumPy 1.15.0 or later, you could use the numpy.printoptions context manager. 

For example, inside the with-suite precision=3 and suppress=True are set:But outside the with-suite the print options are back to default settings:If you are using an earlier version of NumPy, you can create the context manager

yourself. For example,To prevent zeros from being stripped from the end of floats:np.set_printoptions now has a formatter parameter which allows you to specify a format function for each type.which printsinstead of You can get a subset of the np.set_printoptions functionality from the np.array_str command, which applies only to a single print statement.http://docs.scipy.org/doc/numpy/reference/generated/numpy.array_str.htmlFor example:Unutbu gave a really complete answer (they got a +1 from me too), but here is a lo-tech alternative:As a function (using the format() syntax for formatting):Usage:The index of the array is accessible in the format string:FYI Numpy 1.15 (release date pending) will include a context manager for setting print options locally. This means that the following will work the same as the corresponding example in the accepted answer (by unutbu and Neil G) without having to write your own context manager. E.g., using their example:The gem that makes it all too easy to obtain the result as a string (in today's numpy versions) is hidden in denis answer:

np.array2stringYears later, another one is below. But for everyday use I justAnd here is what I use, and it's pretty uncomplicated:Was surprised to not see around method mentioned - means no messing with print options.I often want different columns to have different formats.  Here is how I print a simple 2D array using some variety in the formatting by converting (slices of) my NumPy array to a tuple:numpy.char.mod may also be useful, depending on the details of your application e.g.:numpy.char.mod('Value=%4.2f', numpy.arange(5, 10, 0.1)) will return a string array with elements "Value=5.00", "Value=5.10" etc. (as a somewhat contrived example).I find that the usual float format {:9.5f} works properly -- suppressing small-value e-notations -- when displaying a list or an array using a loop.  But that format sometimes fails to suppress its e-notation when a formatter has several items in a single print statement.  For example:My results show the bug in cases 4, 5, and 6:I have no explanation for this, and therefore I always use a loop for floating output of multiple values. I useIt's not difficult to modify it for multi-dimensional arrays.Yet another option is to use the decimal module:The numpy arrays have the method round(precision) which return a new numpy array with elements rounded accordingly.

'too many values to unpack', iterating over a dict. key=>string, value=>list

tipu

['too many values to unpack', iterating over a dict. key=>string, value=>list](https://stackoverflow.com/questions/5466618/too-many-values-to-unpack-iterating-over-a-dict-key-string-value-list)

I am getting the 'too many values to unpack' error. Any idea how I can fix this?

2011-03-29 00:32:14Z

I am getting the 'too many values to unpack' error. Any idea how I can fix this?You need to use something like iteritems.See this answer for more information on iterating through dictionaries, such as using items(), across python versions.Since Python 3 iteritems() is no longer supported. Use items() instead.For Python 3.x iteritems has been removed. Use items instead.You want to use iteritems. This returns an iterator over the dictionary, which gives you a tuple(key, value)Your problem was that you were looping over fields, which returns the keys of the dictionary.For lists, use enumerateiteritems will not work for list objectsIn Python3 iteritems() is no longer supportedSOLUTION1:Use .itemsSOLUTION2:You can use enumerate() as wellCan't be iterating directly in dictionary. So you can through converting into tuple.you are missing fields.iteritems() in your code.You could also do it other way, where you get values using keys in the dictionary.

Permanently add a directory to PYTHONPATH?

John Howard

[Permanently add a directory to PYTHONPATH?](https://stackoverflow.com/questions/3402168/permanently-add-a-directory-to-pythonpath)

Whenever I use sys.path.append, the new directory will be added. However, once I close python, the list will revert to the previous (default?) values. How do I permanently add a directory to PYTHONPATH?

2010-08-04 02:28:07Z

Whenever I use sys.path.append, the new directory will be added. However, once I close python, the list will revert to the previous (default?) values. How do I permanently add a directory to PYTHONPATH?You need to add your new directory to the environment variable PYTHONPATH, separated by a colon from previous contents thereof.  In any form of Unix, you can do that in a startup script appropriate to whatever shell you're using (.profile or whatever, depending on your favorite shell) with a command which, again, depends on the shell in question; in Windows, you can do it through the system GUI for the purpose.superuser.com may be a better place to ask further, i.e. for more details if you need specifics about how to enrich an environment variable in your chosen platform and shell, since it's not really a programming question per se.If you're using bash (on a Mac or GNU/Linux distro), add this to your ~/.bashrcInstead of manipulating PYTHONPATH you can also create a path configuration file. First find out in which directory Python searches for this information:For some reason this doesn't seem to work in Python 2.7. There you can use:Then create a .pth file in that directory containing the path you want to add (create the directory if it doesn't exist).For example:This works on WindowsThen you'll be able to see all modules within those paths from your scripts.In case anyone is still confused - if you are on a Mac, do the following:You could add the path via your pythonrc file, which defaults to ~/.pythonrc on linux. ie.You could also set the PYTHONPATH environment variable, in a global rc file, such ~/.profile on mac or linux, or via Control Panel -> System -> Advanced tab -> Environment Variables on windows.To give a bit more explanation, Python will automatically construct its search paths (as mentioned above and here) using the site.py script (typically located in sys.prefix + lib/python<version>/site-packages as well as lib/site-python). One can obtain the value of sys.prefix:The site.py script then adds a number of directories, dependent upon the platform, such as /usr/{lib,share}/python<version>/dist-packages, /usr/local/lib/python<version>/dist-packages to the search path and also searches these paths for <package>.pth config files which contain specific additional search paths. For example easy-install maintains its collection of installed packages which are added to a system specific file e.g on Ubuntu it's /usr/local/lib/python2.7/dist-packages/easy-install.pth. On a typical system there are a bunch of these .pth files around which can explain some unexpected paths in sys.path:So one can create a .pth file and put in any of these directories (including the sitedir as mentioned above). This seems to be the way most packages get added to the sys.path as opposed to using the PYTHONPATH.Note: On OSX there's a special additional search path added by site.py for 'framework builds' (but seems to work for normal command line use of python): /Library/Python/<version>/site-packages (e.g. for Python2.7: /Library/Python/2.7/site-packages/) which is where 3rd party packages are supposed to be installed (see the README in that dir). So one can add a path configuration file in there containing additional search paths e.g. create a file called /Library/Python/2.7/site-packages/pip-usr-local.pth which contains /usr/local/lib/python2.7/site-packages/ and then the system python will add that search path.For me it worked when I changed the .bash_profile file. Just changing .bashrc file worked only till I restarted the shell.For python 2.7 it should look like:at the end of the .bash_profile file.On linux you can create a symbolic link from your package to a directory of the PYTHONPATH without having to deal with the environment variables. Something like:Adding export PYTHONPATH="${PYTHONPATH}:/my/other/path" to the ~/.bashrc might not work if PYTHONPATH does not currently exist (because of the :).Adding the above to my ~/.bashrc did the trick for me on Ubuntu 16.04On MacOS, Instead of giving path to a specific library. Giving full path to the root project folder in made my day, for example: after this do: Just to add on awesomo's answer, you can also add that line into your ~/.bash_profile or ~/.profileThe add a new path to PYTHONPATH is doing in manually by:adding the path to your ~/.bashrc profile, in terminal by:paste the following to your profilethen, make sure to source your bashrc profile when ever you run your code in terminal:Hope this helps.I added permanently in Windows Vista, Python 3.5System > Control Panel > Advanced system settings > Advanced (tap) Environment Variables > System variables > (if you don't see PYTHONPATH in Variable column) (click) New > Variable name: PYTHONPATH > Variable value: Please, write the directory in the Variable value. It is details of Blue Peppers' answer.In Python 3.6.4 you can persist sys.path across python sessions like this:I strongly suggest you use virtualenv and virtualenvwrapper otherwise you will clutter your pathThe script below works on all platforms as it's pure Python. It makes use of the pathlib Path, documented here https://docs.python.org/3/library/pathlib.html, to make it work cross-platform. You run it once, restart the kernel and that's it. Inspired by https://medium.com/@arnaud.bertrand/modifying-python-s-search-path-with-pth-files-2a41a4143574.Shortest path between A <-> B is a straight line;

Favorite Django Tips & Features?

Haes

[Favorite Django Tips & Features?](https://stackoverflow.com/questions/550632/favorite-django-tips-features)

Inspired by the question series 'Hidden features of ...', I am curious to hear about your favorite Django tips or lesser known but useful features you know of.

2009-02-15 10:15:52Z

Inspired by the question series 'Hidden features of ...', I am curious to hear about your favorite Django tips or lesser known but useful features you know of.I'm just going to start with a tip from myself :)Use os.path.dirname() in settings.py to avoid hardcoded dirnames.Don't hardcode path's in your settings.py if you want to run your project in different locations. Use the following code in settings.py if your templates and static files are located within the Django project directory:Credits: I got this tip from the screencast 'Django From the Ground Up'.Install Django Command Extensions and pygraphviz and then issue the following command to get a really nice looking Django model visualization:Use django-annoying's render_to decorator instead of render_to_response.Edited to point out that returning an HttpResponse (such as a redirect) will short circuit the decorator and work just as you expect.There's a set of custom tags I use all over my site's templates. Looking for a way to autoload it (DRY, remember?), I found the following:If you put this in a module that's loaded by default (your main urlconf for instance), you'll have the tags and filters from your custom tag module available in any template, without using {% load custom_tag_module %}.The argument passed to template.add_to_builtins() can be any module path; your custom tag module doesn't have to live in a specific application. For example, it can also be a module in your project's root directory (eg. 'project.custom_tag_module').Virtualenv + Python = life saver if you are working on multiple Django projects and there is a possibility that they all don't depend on the same version of Django/an application.Don't hard-code your URLs! Use url names instead, and the reverse function to get the URL itself.When you define your URL mappings, give names to your URLs. Make sure the name is unique per URL.I usually have a consistent format "project-appplication-view", e.g. "cbx-forum-thread" for a thread view.UPDATE (shamelessly stealing ayaz's addition):This name can be used in templates with the url tag.Use django debug toolbar. For example, it allows to view all SQL queries performed while rendering view and you can also view stacktrace for any of them.Don't write your own login pages.  If you're using django.contrib.auth.The real, dirty secret is that if you're also using django.contrib.admin, and django.template.loaders.app_directories.load_template_source is in your template loaders,  you can get your templates free too!Say you have a different user model and you want to include

that in every response. Instead of doing this:Context processes give you the ability to pass any variable to your

templates. I typically put mine in 'my_project/apps/core/context.py:In your settings.py add the following line to your TEMPLATE_CONTEXT_PROCESSORSNow every time a request is made it includes the my_user key automatically.I wrote a blog post about this a few months ago so I'm just going to cut and paste:Out of the box Django gives you several signals that are

incredibly useful. You have the ability to do things pre and

post save, init, delete, or even when a request is being

processed. So lets get away from the concepts and

demonstrate how these are used. Say we’ve got a blogSo somehow you want to notify one of the many blog-pinging

services we’ve made a new post, rebuild the most recent

posts cache, and tweet about it. Well with signals you have

the ability to do all of this without having to add any

methods to the Post class.There we go, by defining that function and using the

post_init signal to connect the function to the Post model

and execute it after it has been saved.When I was starting out, I didn't know that there was a Paginator, make sure you know of its existence!!Use IPython to jump into your code at any level and debug using the power of IPython.  Once you have installed IPython just put this code in wherever you want to debug: Then, refresh the page, go to your runserver window and you will be in an interactive IPython window.I have a snippet set up in TextMate so I just type ipshell and hit tab.  I couldn't live without it.Run a development SMTP server that will just output whatever is sent to it (if you don't want to actually install SMTP on your dev server.)command line:From the django-admin documentation:If you use the Bash shell, consider installing the Django bash completion script, which lives in extras/django_bash_completion in the Django distribution. It enables tab-completion of django-admin.py and manage.py commands, so you can, for instance...The ./manage.py runserver_plus facilty which comes with django_extensions is truly awesome.  It creates an enhanced debug page that, amongst other things, uses the Werkzeug debugger to create interactive debugging consoles for each point in the stack (see screenshot).  It also provides a very useful convenience debugging method dump() for displaying information about an object/frame.To install, you can use pip:Then add 'django_extensions' to your INSTALLED_APPS tuple in settings.py and start the development server with the new extension:This will change the way you debug.I like to use the Python debugger pdb to debug Django projects.This is a helpful link for learning how to use it: http://www.ferg.org/papers/debugging_in_python.htmlWhen trying to exchange data between Django and another application, request.raw_post_data is a good friend. Use it to receive and custom-process, say, XML data.Documentation:

http://docs.djangoproject.com/en/dev/ref/request-response/Use Jinja2 alongside Django.If you find the Django template language extremely restricting (like me!) then you don't have to be stuck with it. Django is flexible, and the template language is loosely coupled to the rest of the system, so just plug-in another template language and use it to render your http responses!I use Jinja2, it's almost like a powered-up version of the django template language, it uses the same syntax, and allows you to use expressions in if statements! no more making a custom if-tags such as if_item_in_list! you can simply say %{ if item in list %}, or {% if object.field < 10 %}.But that's not all; it has many more features to ease template creation, that I can't go though all of them in here.Add assert False in your view code to dump debug information.This adds to the reply above about Django URL names and reverse URL dispatching.The URL names can also be effectively used within templates. For example, for a given URL pattern:you can have the following in templates:Since Django "views" only need to be callables that return an HttpResponse, you can easily create class-based views like those in Ruby on Rails and other frameworks.There are several ways to create class-based views, here's my favorite:You can add all sorts of other stuff like conditional request handling and authorization in your base view.Once you've got your views setup your urls.py will look something like this:Instead of using render_to_response to bind your context to a template and render it (which is what the Django docs usually show) use the generic view direct_to_template. It does the same thing that render_to_response does but it also automatically adds RequestContext to the template context, implicitly allowing context processors to be used. You can do this manually using render_to_response, but why bother? It's just another step to remember and another LOC. Besides making use of context processors, having RequestContext in your template allows you to do things like:which is very useful. In fact, +1 on generic views in general. The Django docs mostly show them as shortcuts for not even having a views.py file for simple apps, but you can also use them inside your own view functions:I don't have enough reputation to reply to the comment in question, but it's important to note that if you're going to use Jinja, it does NOT support the '-' character in template block names, while Django does. This caused me a lot of problems and wasted time trying to track down the very obscure error message it generated.The webdesign app is very useful when starting to design your website.  Once imported, you can add this to generate sample text:django.db.models.get_model does allow you to retrieve a model without importing it.James shows how handy it can be: "Django tips: Write better template tags — Iteration 4 ".Everybody knows there is a development server you can run with "manage.py runserver", but did you know that there is a development view for serving static files (CSS / JS / IMG) as well ?Newcomers are always puzzled because Django doesn't come with any way to serve static files. This is because the dev team think it is the job for a real life Web server.But when developing, you may not want to set up Apache + mod_wisgi, it's heavy. Then you can just add the following to urls.py:Your CSS / JS / IMG will be available at www.yoursite.com/site_media/.Of course, don't use it in a production environment.I learned this one from the documentation for the sorl-thumbnails app. You can use the "as" keyword in template tags to use the results of the call elsewhere in your template.For example:This is mentioned in passing in the Django templatetag documentation, but in reference to loops only. They don't call out that you can use this elsewhere (anywhere?) as well.django.views.generic.list_detail.object_list -- It provides all the logic & template variables for pagination (one of those I've-written-that-a-thousand-times-now drudgeries).  Wrapping it allows for any logic you need.  This gem has saved me many hours of debugging off-by-one errors in my "Search Results" pages and makes the view code cleaner in the process.PyCharm IDE is a nice environment to code and especially debug, with built-in support for Django.Use xml_models to create Django models that use an XML REST API backend (instead of a SQL one).  This is very useful especially when modelling third party APIs - you get all the same QuerySet syntax that you're used to.  You can install it from PyPI.XML from an API:And now in python:It can also handle relationships and collections.  We use it every day in heavily used production code, so even though it's beta it's very usable.  It also has a good set of stubs that you can use in your tests.(Disclaimer: while I'm not the author of this library, I am now a committer, having made a few minor commits)Use database migrations. Use South.

Does Python support short-circuiting?

Dinah

[Does Python support short-circuiting?](https://stackoverflow.com/questions/2580136/does-python-support-short-circuiting)

Does Python support short-circuiting in boolean expressions?

2010-04-05 18:19:55Z

Does Python support short-circuiting in boolean expressions?Yep, both and and or operators short-circuit -- see the docs.Let's first define a useful function to determine if something is executed or not. A simple function that accepts an argument, prints a message and returns the input, unchanged. One can observe the Python's short-circuiting behavior of and, or operators in the following example:Note: The following values are considered by the interpreter to mean false:Python's any() and all() functions also support short-circuiting. As shown in the docs; they evaluate each element of a sequence in-order, until finding a result that allows an early exit in the evaluation. Consider examples below to understand both. The function any() checks if any element is True. It stops executing as soon as a True is encountered and returns True. The function all() checks all elements are True and stops executing as soon as a False is encountered:Additionally, in Python Edit:

One more interesting point to note :- Logical and, or operators in Python returns an operand's value instead of a Boolean (True or False). For example:Unlike in other languages e.g. &&, || operators in C that return either 0 or 1.Examples: Similarly or operator return left most value for which bool(value) == True else right most false value (according to short-circuiting behavior), examples:  So, how is this useful? One example use given in Practical Python By Magnus Lie Hetland:

Let’s say a user is supposed to enter his or her name, but may opt to enter nothing, in which case you want to use the default value '<unknown>'. You could use an if statement, but you could also state things very succinctly:In other words, if the return value from raw_input is true (not an empty string), it is assigned to name (nothing changes); otherwise, the default '<unknown>' is assigned to name.Yes. Try the following in your python interpreter:andor

Understanding the map function

Web Master

[Understanding the map function](https://stackoverflow.com/questions/10973766/understanding-the-map-function)

Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list.What role does this play in making a Cartesian product?What effect does putting a tuple anywhere in there have? I also noticed that without the map function the output is abc and with it, it's a, b, c.I want to fully understand this function. The reference definitions is also hard to understand. Too much fancy fluff. 

2012-06-11 01:39:42Z

Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list.What role does this play in making a Cartesian product?What effect does putting a tuple anywhere in there have? I also noticed that without the map function the output is abc and with it, it's a, b, c.I want to fully understand this function. The reference definitions is also hard to understand. Too much fancy fluff. map isn't particularly pythonic. I would recommend using list comprehensions instead:is basically equivalent to:map on its own can't do a Cartesian product, because the length of its output list is always the same as its input list. You can trivially do a Cartesian product with a list comprehension though:The syntax is a little confusing -- that's basically equivalent to:map doesn't relate to a Cartesian product at all, although I imagine someone well versed in functional programming could come up with some impossible to understand way of generating a one using map. map in Python 3 is equivalent to this:and the only difference in Python 2 is that it will build up a full list of results to return all at once instead of yielding.Although Python convention usually prefers list comprehensions (or generator expressions) to achieve the same result as a call to map, particularly if you're using a lambda expression as the first argument:As an example of what you asked for in the comments on the question - "turn a string into an array", by 'array' you probably want either a tuple or a list (both of them behave a little like arrays from other languages) -A use of map here would be if you start with a list of strings instead of a single string - map can listify all of them individually:Note that map(list, a) is equivalent in Python 2, but in Python 3 you need the list call if you want to do anything other than feed it into a for loop (or a processing function such as sum that only needs an iterable, and not a sequence). But also note again that a list comprehension is usually preferred:map creates a new list by applying a function to every element of the source:n-ary map is equivalent to zipping input iterables together and then applying the transformation function on every element of that intermediate zipped list. It's not a Cartesian product:I've used zip here, but map behaviour actually differs slightly when iterables aren't the same size — as noted in its documentation, it extends iterables to contain None.Simplifying a bit, you can imagine map() doing something like this:As you can see, it takes a function and a list, and returns a new list with the result of applying the function to each of the elements in the input list. I said "simplifying a bit" because in reality map() can process more than one iterable:For the second part in the question: What role does this play in making a Cartesian product? well, map() could be used for generating the cartesian product of a list like this:... But to tell the truth, using product() is a much simpler and natural way to solve the problem:Either way, the result is the cartesian product of lst as defined above:The map() function is there to apply the same procedure to every item in an iterable data structure, like lists, generators, strings, and other stuff.Let's look at an example:

map() can iterate over every item in a list and apply a function to each item, than it will return (give you back) the new list.Imagine you have a function that takes a number, adds 1 to that number and returns it:You also have a list of numbers:if you want to increment every number in the list, you can do the following:Note: At minimum map() needs two arguments. First a function name and second something like a list.Let's see some other cool things map() can do.

map() can take multiple iterables (lists, strings, etc.) and pass an element from each iterable to a function as an argument.We have three lists:map() can make you a new list that holds the addition of elements at a specific index.Now remember map(), needs a function. This time we'll use the builtin sum() function. Running map() gives the following result:REMEMBER:

In Python 2 map(), will iterate (go through the elements of the lists) according to the longest list, and pass None to the function for the shorter lists, so your function should look for None and handle them, otherwise you will get errors. In Python 3 map() will stop after finishing with the shortest list. Also, in Python 3, map() returns an iterator, not a list.One thing that wasn't mentioned completely (although @BlooB kinda mentioned it) is that map returns a map object NOT a list. This is a big difference when it comes to time performance on initialization and iteration. Consider these two tests.As you can see initializing the map function takes almost no time at all. However iterating through the map object takes longer than simply iterating through the iterable. This means that the function passed to map() is not applied to each element until the element is reached in the iteration. If you want a list use list comprehension. If you plan to iterate through in a for loop and will break at some point, then use map.

How to keep keys/values in same order as declared?

roflwaffle

[How to keep keys/values in same order as declared?](https://stackoverflow.com/questions/1867861/how-to-keep-keys-values-in-same-order-as-declared)

I have a dictionary that I declared in a particular order and want to keep it in that order all the time. The keys/values can't really be kept in order based on their value, I just want it in the order that I declared it.So if I have the dictionary:It isn't in that order if I view it or iterate through it, is there any way to make sure Python will keep the explicit order that I declared the keys/values in?

2009-12-08 15:53:35Z

I have a dictionary that I declared in a particular order and want to keep it in that order all the time. The keys/values can't really be kept in order based on their value, I just want it in the order that I declared it.So if I have the dictionary:It isn't in that order if I view it or iterate through it, is there any way to make sure Python will keep the explicit order that I declared the keys/values in?From Python 3.6 onwards, the standard dict type maintains insertion order by default.Definingwill result in a dictionary with the keys in the order listed in the source code.This was achieved by using a simple array with integers for the sparse hash table, where those integers index into another array that stores the key-value pairs (plus the calculated hash). That latter array just happens to store the items in insertion order, and the whole combination actually uses less memory than the implementation used in Python 3.5 and before. See the original idea post by Raymond Hettinger for details.In 3.6 this  was still considered an implementation detail; see the What's New in Python 3.6 documentation:Python 3.7 elevates this implementation detail to a language specification, so  it is now mandatory that dict preserves order in all Python implementations compatible with that version or newer. See the pronouncement by the BDFL.You may still want to use the collections.OrderedDict() class in certain cases, as it offers some additional functionality on top of the standard dict type. Such as as being reversible (this extends to the view objects), and supporting reordering (via the move_to_end() method).contains If the values are True (or any other immutable object), you can also use:Rather than explaining the theoretical part, I'll give a simple example.Note that this answer applies to python versions prior to python3.7.  CPython 3.6 maintains insertion order under most circumstances as an implementation detail.  Starting from Python3.7 onward, it has been declared that implementations MUST maintain insertion order to be compliant.python dictionaries are unordered.  If you want an ordered dictionary, try collections.OrderedDict.Note that OrderedDict was introduced into the standard library in python 2.7.  If you have an older version of python, you can find recipes for ordered dictionaries on ActiveState.Dictionaries will use an order that makes searching efficient, and you cant change that,You could just use a list of objects (a 2 element tuple in a simple case, or even a class), and append items to the end. You can then use linear search to find items in it.Alternatively you could create or use a different data structure created with the intention of maintaining order.I came across this post while trying to figure out how to get OrderedDict to work. PyDev for Eclipse couldn't find OrderedDict at all, so I ended up deciding to make a tuple of my dictionary's key values as I would like them to be ordered. When I needed to output my list, I just iterated through the tuple's values and plugged the iterated 'key' from the tuple into the dictionary to retrieve my values in the order I needed them.example:It's a tad cumbersome, but I'm pressed for time and it's the workaround I came up with.note: the list of lists approach that somebody else suggested does not really make sense to me, because lists are ordered and indexed (and are also a different structure than dictionaries).You can't really do what you want with a dictionary. You already have the dictionary d = {'ac':33, 'gw':20, 'ap':102, 'za':321, 'bs':10}created. I found there was no way to keep in order once it is already created. What I did was make a json file instead with the object:I used:then used:to verify.Another alternative is to use Pandas dataframe as it guarantees the order and the index locations of the items in a dict-like structure.Generally, you can design a class that behaves like a dictionary, mainly be implementing the methods __contains__, __getitem__, __delitem__, __setitem__ and some more.  That class can have any behaviour you like, for example prividing a sorted iterator over the keys ...if you would like to have a dictionary  in a specific order, you can also create a list of lists, where the first item will be the key, and the second item will be the value

and will look like this

example I had a similar problem when developing a Django project. I couldn't use OrderedDict, because I was running an old version of python, so the solution was to use Django's SortedDict class:https://code.djangoproject.com/wiki/SortedDicte.g.,Note: This answer is originally from 2011. If you have access to Python version 2.7 or higher, then you should have access to the now standard collections.OrderedDict, of which many examples have been provided by others in this thread.You can do the same thing which i did for dictionary.Create a list and empty dictionary:

pandas: filter rows of DataFrame with operator chaining

duckworthd

[pandas: filter rows of DataFrame with operator chaining](https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining)

Most operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexingThis is unappealing as it requires I assign df to a variable before being able to filter on its values.  Is there something more like the following?

2012-08-08 17:25:37Z

Most operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexingThis is unappealing as it requires I assign df to a variable before being able to filter on its values.  Is there something more like the following?I'm not entirely sure what you want, and your last line of code does not help either, but anyway:"Chained" filtering is done by "chaining" the criteria in the boolean index.If you want to chain methods, you can add your own mask method and use that one.Filters can be chained using a Pandas query:Filters can also be combined in a single query:The answer from @lodagro is great. I would extend it by generalizing the mask function as:Then you can do stuff like:Since version 0.18.1 the .loc method accepts a callable for selection. Together with lambda functions you can create very flexible chainable filters:If all you're doing is filtering, you can also omit the .loc.I offer this for additional examples.  This is the same answer as https://stackoverflow.com/a/28159296/ I'll add other edits to make this post more useful.pandas.DataFrame.query

query was made for exactly this purpose.  Consider the dataframe dfLet's use query to filter all rows where D > BWhich we chainI had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:But I found that, if you wrap each condition in (... == True) and join the criteria with a pipe, the criteria are combined in an OR condition, satisfied whenever either of them is true: pandas provides two alternatives to Wouter Overmeire's answer which do not require any overriding. One is .loc[.] with a callable, as inthe other is .pipe(), as inMy answer is similar to the others. If you do not want to create a new function you can use what pandas has defined for you already. Use the pipe method.If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows:Usage:It's a little bit hacky but it can make things a little bit cleaner if you're continuously chopping and changing datasets according to filters.

There's also a general purpose filter adapted from Daniel Velkov above in the gen_mask function which you can use with lambda functions or otherwise if desired.File to be saved (I use masks.py):This solution is more hackish in terms of implementation, but I find it much cleaner in terms of usage, and it is certainly more general than the others proposed.https://github.com/toobaz/generic_utils/blob/master/generic_utils/pandas/where.pyYou don't need to download the entire repo: saving the file and doingshould suffice. Then you use it like this:A slightly less stupid usage example:By the way: even in the case in which you are just using boolean cols,can be much more efficient thanbecause it evaluates cond2 only where cond1 is True.DISCLAIMER: I first gave this answer elsewhere because I hadn't seen this.Just want to add a demonstration using loc to filter not only by rows but also by columns and some merits to the chained operation.The code below can filter the rows by value.By modifying it a bit you can filter the columns as well.So why do we want a chained method? The answer is that it is simple to read if you have many operations. For example,seems to work: you can nest the [] operator as well. Maybe they added it since you asked the question.If you set your columns to search as indexes, then you can use DataFrame.xs() to take a cross section. This is not as versatile as the query answers, but it might be useful in some situations.You can also leverage the numpy library for logical operations. Its pretty fast. 

Python 3 ImportError: No module named 'ConfigParser'

if __name__ is None

[Python 3 ImportError: No module named 'ConfigParser'](https://stackoverflow.com/questions/14087598/python-3-importerror-no-module-named-configparser)

I am trying to pip install the MySQL-python package, but I get an ImportError.Any ideas?

2012-12-30 01:46:32Z

I am trying to pip install the MySQL-python package, but I get an ImportError.Any ideas?In Python 3, ConfigParser has been renamed to configparser for PEP 8 compliance. It looks like the package you are installing does not support Python 3.You can instead use the mysqlclient package as a drop-in replacement for MySQL-python. It is a fork of MySQL-python with added support for Python 3.I had luck with simplyin my python3.4 virtualenv afterwhich is obviously specific to ubuntu/debian, but I just wanted to share my success :)Here is a code that should work in both Python 2.x and 3.xObviously you will need the six module, but it's almost impossible to write modules that work in both versions without six.MySQL-python is not supported on python3 instead of this you can use mysqlclient If you are on fedora/centos/Red Hat install following packageThen try to install the MYSQL-python again.

That Worked for me If you are using CentOS, then you need to useCompatibility of Python 2/3 for configparser can be solved simply by six libraryI was having the same problem. Turns out, I needed to install python3 devel on my centos. First, you need to search for the package that is compatible with your system.Then, install the package as:Then, install mysqlclient from pipDo pip3 install PyMySQL and then pip3 install mysqlclient.

Worked for meI got further with Valeres answer:I would suggest to link the file instead of copy it. It is save to update.  I linked the file to /usr/lib/python3/ directory.Try this solution which worked fine for me.Basically it's to reinstall/upgrade to latest version of mysql from brew, and then installing mysqlclient or MySQL-Python from global pip3 instead of virtualenv pip3.Then accessing the virtualenv and successfully install mysqlclient or MySQL-Python.how about checking the version of Python you are using first. I run kali linux- Rolling and I came across this problem ,when I tried running cupp.py in the terminal, after updating to python 3.6.0. After some research and trial I found that changing    ConfigParser to    configparser worked for me but then I came across another issue.config = configparser.configparser()

AttributeError: module 'configparser' has no attribute 'configparser'After a bit more research I realised that for python 3    ConfigParser is changed to    configparser but note that it has an attribute     ConfigParser().pip3 install mysql still gives the "configparser error," and I don't know why this hasn't been corrected by now.Kindly to see what is /usr/bin/python pointing toif it is pointing to python3 or higher  change to python2.7This should solve the issue.I was getting install error for all the python packages. Abe Karplus's solution & discussion gave me the hint as to what could be the problem. 

Then I recalled that I had manually changed the /usr/bin/python from python2.7 to /usr/bin/python3.5, which actually was causing the issue. Once I reverted the same. It got solved. This worked for me 

Understanding __get__ and __set__ and Python descriptors

Matt Bronson

[Understanding __get__ and __set__ and Python descriptors](https://stackoverflow.com/questions/3798835/understanding-get-and-set-and-python-descriptors)

I am trying to understand what Python's descriptors are and what they are useful for. I understand how they work, but here are my doubts. Consider the following code:

2010-09-26 16:55:42Z

I am trying to understand what Python's descriptors are and what they are useful for. I understand how they work, but here are my doubts. Consider the following code:The descriptor is how Python's property type is implemented. A descriptor simply implements __get__, __set__, etc. and is then added to another class in its definition (as you did above with the Temperature class). For example:Accessing the property you assigned the descriptor to (celsius in the above example) calls the appropriate descriptor method.instance in __get__ is the instance of the class (so above, __get__ would receive temp, while owner is the class with the descriptor (so it would be Temperature).You need to use a descriptor class to encapsulate the logic that powers it. That way, if the descriptor is used to cache some expensive operation (for example), it could store the value on itself and not its class.An article about descriptors can be found here.EDIT: As jchl pointed out in the comments, if you simply try Temperature.celsius, instance will be None.It gives you extra control over how attributes work. If you're used to getters and setters in Java, for example, then it's Python's way of doing that. One advantage is that it looks to users just like an attribute (there's no change in syntax). So you can start with an ordinary attribute and then, when you need to do something fancy, switch to a descriptor.An attribute is just a mutable value. A descriptor lets you execute arbitrary code when reading or setting (or deleting) a value. So you could imagine using it to map an attribute to a field in a database, for example – a kind of ORM.Another use might be refusing to accept a new value by throwing an exception in __set__ – effectively making the "attribute" read only.This is pretty subtle (and the reason I am writing a new answer here - I found this question while wondering the same thing and didn't find the existing answer that great).A descriptor is defined on a class, but is typically called from an instance. When it's called from an instance both instance and owner are set (and you can work out owner from instance so it seems kinda pointless). But when called from a class, only owner is set – which is why it's there.This is only needed for __get__ because it's the only one that can be called on a class. If you set the class value you set the descriptor itself. Similarly for deletion. Which is why the owner isn't needed there.Well, here's a cool trick using similar classes:(I'm using Python 3; for python 2 you need to make sure those divisions are / 5.0 and / 9.0). That gives:Now there are other, arguably better ways to achieve the same effect in python (e.g. if celsius were a property, which is the same basic mechanism but places all the source inside the Temperature class), but that shows what can be done...Descriptors are class attributes (like properties or methods) with any of the following special methods:These descriptor objects can be used as attributes on other object class definitions. (That is, they live in the __dict__ of the class object.)Descriptor objects can be used to programmatically manage the results of a dotted lookup (e.g. foo.descriptor) in a normal expression, an assignment, and even a deletion. Functions/methods, bound methods, property, classmethod, and staticmethod all use these special methods to control how they are accessed via the dotted lookup.A data descriptor, like property, can allow for lazy evaluation of attributes based on a simpler state of the object, allowing instances to use less memory than if you precomputed each possible attribute. Another data descriptor, a member_descriptor, created by __slots__, allow memory savings by allowing the class to store data in a mutable tuple-like datastructure instead of the more flexible but space-consuming __dict__.Non-data descriptors, usually instance, class, and static methods, get their implicit first arguments (usually named cls and self, respectively) from their non-data descriptor method, __get__.Most users of Python need to learn only the simple usage, and have no need to learn or understand the implementation of descriptors further.A descriptor is an object with any of the following methods (__get__, __set__, or __delete__), intended to be used via dotted-lookup as if it were a typical attribute of an instance. For an owner-object, obj_instance, with a descriptor object:obj_instance is the instance whose class contains the descriptor object's instance. self is the instance of the descriptor (probably just one for the class of the obj_instance)To define this with code, an object is a descriptor if the set of its attributes intersects with any of the required attributes:A Data Descriptor has a __set__ and/or __delete__.

A Non-Data-Descriptor has neither __set__ nor __delete__.We can see that classmethod and staticmethod are Non-Data-Descriptors:Both only have the __get__ method:Note that all functions are also Non-Data-Descriptors:However, property is a Data-Descriptor:These are important distinctions, as they affect the lookup order for a dotted lookup. The consequence of this lookup order is that Non-Data-Descriptors like functions/methods can be overridden by instances.We have learned that descriptors are objects with any of __get__, __set__, or __delete__. These descriptor objects can be used as attributes on other object class definitions. Now we will look at how they are used, using your code as an example.Here's your code, followed by your questions and answers to each:Your descriptor ensures you always have a float for this class attribute of Temperature, and that you can't use del to delete the attribute:Otherwise, your descriptors ignore the owner-class and instances of the owner, instead, storing state in the descriptor. You could just as easily share state across all instances with a simple class attribute (so long as you always set it as a float to the class and never delete it, or are comfortable with users of your code doing so):This gets you exactly the same behavior as your example (see response to question 3 below), but uses a Pythons builtin (property), and would be considered more idiomatic:instance is the instance of the owner that is calling the descriptor. The owner is the class in which the descriptor object is used to manage access to the data point. See the descriptions of the special methods that define descriptors next to the first paragraph of this answer for more descriptive variable names.Here's a demonstration:You can't delete the attribute:And you can't assign a variable that can't be converted to a float:Otherwise, what you have here is a global state for all instances, that is managed by assigning to any instance.  The expected way that most experienced Python programmers would accomplish this outcome would be to use the property decorator, which makes use of the same descriptors under the hood, but brings the behavior into the implementation of the owner class (again, as defined above):Which has the exact same expected behavior of the original piece of code:We've covered the attributes that define descriptors, the difference between data- and non-data-descriptors, builtin objects that use them, and specific questions about use.So again, how would you use the question's example? I hope you wouldn't. I hope you would start with my first suggestion (a simple class attribute) and move on to the second suggestion (the property decorator) if you feel it is necessary. Before going into the details of descriptors it may be important to know how attribute lookup in Python works. This assumes that the class has no metaclass and that it uses the default implementation of __getattribute__ (both can be used to "customize" the behavior).The best illustration of attribute lookup (in Python 3.x or for new-style classes in Python 2.x) in this case is from Understanding Python metaclasses (ionel's codelog). The image uses : as substitute for "non-customizable attribute lookup".This represents the lookup of an attribute foobar on an instance of Class:Two conditions are important here:That's where descriptors come into it:In both cases the returned value goes through __get__ called with the instance as first argument and the class as second argument.The lookup is even more complicated for class attribute lookup (see for example Class attribute lookup (in the above mentioned blog)).Let's move to your specific questions:In most cases you don't need to write descriptor classes! However you're probably a very regular end user. For example functions. Functions are descriptors, that's how functions can be used as methods with self implicitly passed as first argument.If you look up test_method on an instance you'll get back a "bound method":Similarly you could also bind a function by invoking its __get__ method manually (not really recommended, just for illustrative purposes):You can even call this "self-bound method":Note that I did not provide any arguments and the function did return the instance I had bound!Functions are Non-data descriptors!Some built-in examples of a data-descriptor would be property. Neglecting getter, setter, and deleter the property descriptor is (from Descriptor HowTo Guide "Properties"):Since it's a data descriptor it's invoked whenever you look up the "name" of the property and it simply delegates to the functions decorated with @property, @name.setter, and @name.deleter (if present).There are several other descriptors in the standard library, for example staticmethod, classmethod.The point of descriptors is easy (although you rarely need them): Abstract common code for attribute access. property is an abstraction for instance variable access, function provides an abstraction for methods, staticmethod provides an abstraction for methods that don't need instance access and classmethod provides an abstraction for methods that need class access rather than instance access (this is a bit simplified).Another example would be a class property.One fun example (using __set_name__ from Python 3.6) could also be a property that only allows a specific type:Then you can use the descriptor in a class:And playing a bit with it:Or a "lazy property":These are cases where moving the logic into a common descriptor might make sense, however one could also solve them (but maybe with repeating some code) with other means.It depends on how you look up the attribute. If you look up the attribute on an instance then:In case you look up the attribute on the class (assuming the descriptor is defined on the class):So basically the third argument is necessary if you want to customize the behavior when you do class-level look-up (because the instance is None).Your example is basically a property that only allows values that can be converted to float and that is shared between all instances of the class (and on the class - although one can only use "read" access on the class otherwise you would replace the descriptor instance):That's why descriptors generally use the second argument (instance) to store the value to avoid sharing it. However in some cases sharing a value between instances might be desired (although I cannot think of a scenario at this moment). However it makes practically no sense for a celsius property on a temperature class... except maybe as purely academic exercise.Inspired by Fluent Python by Buciano RamalhoImaging you have a class like thisWe should validate the weight and price in avoid to assign them a negative number, we can write less code if we use descriptor as a proxy as thisthen define class LineItem like this:and we can extend the Quantity class to do more common validatingI tried (with minor changes as suggested) the code from Andrew Cooke's answer. (I am running python 2.7).The code:The result:With Python prior to 3, make sure you subclass from object which will make the descriptor work correctly as the get magic does not work for old style classes.You'd see https://docs.python.org/3/howto/descriptor.html#properties

What is __main__.py?

Monika Sulik

[What is __main__.py?](https://stackoverflow.com/questions/4042905/what-is-main-py)

What is the __main__.py file for, what sort of code should I put into it, and when should I have one?

2010-10-28 12:28:20Z

What is the __main__.py file for, what sort of code should I put into it, and when should I have one?Often, a Python program is run by naming a .py file on the command line:You can also create a directory or zipfile full of code, and include a __main__.py.  Then you can simply name the directory or zipfile on the command line, and it executes the __main__.py automatically:You'll have to decide for yourself whether your application could benefit from being executed like this.Note that a __main__ module usually doesn't come from a __main__.py file. It can, but it usually doesn't. When you run a script like python my_program.py, the script will run as the __main__ module instead of the my_program module. This also happens for modules run as python -m my_module, or in several other ways.If you saw the name __main__ in an error message, that doesn't necessarily mean you should be looking for a __main__.py file.When creating a Python module, it is common to make the module execute some functionality (usually contained in a main function) when run as the entry point of the program. This is typically done with the following common idiom placed at the bottom of most Python files:You can get the same semantics for a Python package with __main__.py. This is a linux shell prompt, $, if you don't have Bash (or another Posix shell) on Windows just create these files at demo/__<init/main>__.py with contents in between the EOFs:(In a Posix/Bash shell, you can do the above without the << EOFs and ending EOFs by entering Ctrl+D, the end-of-file character, at the end of each cat command)And now:You can derive this from the documention. The documentation says:You can also package this into a single file and run it from the command line like this - but note that zipped packages can't execute sub-packages or submodules as the entry point:__main__.py is used for python programs in zip files. The __main__.py file will be executed when the zip file in run. For example, if the zip file was as such:and the contents of __main__.py was Then if we were to run python test.zip world we would get hello world out.So the __main__.py file run when python is called on a zip file.You create __main__.py in yourpackage to make it executable as:If your script is a directory or ZIP file rather than a single python file, __main__.py will be executed when the "script" is passed as an argument to the python interpreter.

How to store a dataframe using Pandas

jeffstern

[How to store a dataframe using Pandas](https://stackoverflow.com/questions/17098654/how-to-store-a-dataframe-using-pandas)

Right now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?

2013-06-13 23:05:36Z

Right now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?The easiest way is to pickle it using to_pickle:Then you can load it back using:Note: before 0.11.1 save and load were the only way to do this (they are now deprecated in favor of to_pickle and read_pickle respectively).Another popular choice is to use HDF5 (pytables) which offers very fast access times for large datasets:More advanced strategies are discussed in the cookbook.Since 0.13 there's also msgpack which may be be better for interoperability, as a faster alternative to JSON, or if you have python object/text-heavy data (see this question).Although there are already some answers I found a nice comparison in which they tried several ways to serialize Pandas DataFrames: Efficiently Store Pandas DataFrames. They compare:In their experiment, they serialize a DataFrame of 1,000,000 rows with the two columns tested separately: one with text data, the other with numbers. Their disclaimer says:The source code for the test which they refer to is available online. Since this code did not work directly I made some minor changes, which you can get here: serialize.py

 I got the following results:They also mention that with the conversion of text data to categorical data the serialization is much faster. In their test about 10 times as fast (also see the test code).Edit: The higher times for pickle than CSV can be explained by the data format used. By default pickle uses a printable ASCII representation, which generates larger data sets. As can be seen from the graph however, pickle using the newer binary data format (version 2, pickle-p2) has much lower load times.Some other references:If I understand correctly, you're already using pandas.read_csv() but would like to speed up the development process so that you don't have to load the file in every time you edit your script, is that right? I have a few recommendations:You might also be interested in this answer on stackoverflow.Pickle works good!Pandas DataFrames have the to_pickle function which is useful for saving a DataFrame:You can use feather format file. It is extremely fast.As already mentioned there are different options and file formats (HDF5, JSON, CSV, parquet, SQL) to store a data frame. However, pickle is not a first-class citizen (depending on your setup), because:Depending on your setup/usage both limitations do not apply, but I would not recommend pickle as the default persistence for pandas data frames. I prefer to use numpy files since they're fast and easy to work with.

Here's a simple benchmark for saving and loading a dataframe with 1 column of 1million points.using ipython's %%timeit magic functionthe output is to load the data back into a dataframethe output isNOT BAD!There's a problem if you save the numpy file using python 2 and then try opening using python 3 (or vice versa).https://docs.python.org/3/library/pickle.htmlThe pickle protocol formats:Protocol version 0 is the original「human-readable」protocol and is backwards compatible with earlier versions of Python.Protocol version 1 is an old binary format which is also compatible with earlier versions of Python.Protocol version 2 was introduced in Python 2.3. It provides much more efficient pickling of new-style classes. Refer to PEP 307 for information about improvements brought by protocol 2.Protocol version 3 was added in Python 3.0. It has explicit support for bytes objects and cannot be unpickled by Python 2.x. This is the default protocol, and the recommended protocol when compatibility with other Python 3 versions is required.Protocol version 4 was added in Python 3.4. It adds support for very large objects, pickling more kinds of objects, and some data format optimizations. Refer to PEP 3154 for information about improvements brought by protocol 4.Overall move has been to pyarrow/feather (deprecation warnings from pandas/msgpack).  However I have a challenge with pyarrow with transient in specification Data serialized with pyarrow 0.15.1 cannot be deserialized with 0.16.0 ARROW-7961. I'm using serialization to use redis so have to use a binary encoding.I've retested various options (using jupyter notebook)With following results for my data frame (in out jupyter variable)feather and parquet do not work for my data frame. I'm going to continue using pyarrow.  However I will supplement with pickle (no compression).  When writing to cache store pyarrow and pickle serialised forms.  When reading from cache fallback to pickle if pyarrow deserialisation fails.The above code will save the pickle fileThis two lines will open the saved pickle file

Python: defaultdict of defaultdict?

Jonathan

[Python: defaultdict of defaultdict?](https://stackoverflow.com/questions/5029934/python-defaultdict-of-defaultdict)

Is there a way to have a defaultdict(defaultdict(int)) in order to make the following code work?d needs to be built ad-hoc, depending on x.a and x.b elements.I could use:but then I wouldn't be able to use:

2011-02-17 14:04:22Z

Is there a way to have a defaultdict(defaultdict(int)) in order to make the following code work?d needs to be built ad-hoc, depending on x.a and x.b elements.I could use:but then I wouldn't be able to use:Yes like this:The argument of a defaultdict (in this case is lambda: defaultdict(int)) will be called when you try to access a key that doesn't exist. The return value of it will be set as the new value of this key, which means in our case the value of d[Key_doesnt_exist] will be defaultdict(int).If you try to access a key from this last defaultdict i.e. d[Key_doesnt_exist][Key_doesnt_exist] it will return 0, which is the return value of the argument of the last defaultdict i.e. int().The parameter to the defaultdict constructor is the function which will be called for building new elements. So let's use a lambda !Since Python 2.7, there's an even better solution using Counter:Some bonus featuresFor more information see PyMOTW - Collections - Container data types and Python Documentation - collectionsI find it slightly more elegant to use partial:Of course, this is the same as a lambda.For reference, it's possible to implement a generic nested defaultdict factory method through:The depth defines the number of nested dictionary before the type defined in default_factory is used.

For example:Others have answered correctly your question of how to get the following to work:An alternative would be to use tuples for keys:The nice thing about this approach is that it is simple and can be easily expanded.  If you need a mapping three levels deep, just use a three item tuple for the key.Previous answers have addressed how to make a two-levels or n-levels defaultdict.  In some cases you want an infinite one:Usage:

Substitute multiple whitespace with single whitespace in Python [duplicate]

creativz

[Substitute multiple whitespace with single whitespace in Python [duplicate]](https://stackoverflow.com/questions/2077897/substitute-multiple-whitespace-with-single-whitespace-in-python)

I have this string:How can I substitute the double, triple (...) whitespace chracters with a single space, so that I get:

2010-01-16 15:43:27Z

I have this string:How can I substitute the double, triple (...) whitespace chracters with a single space, so that I get:A simple possibility (if you'd rather avoid REs) isThe split and join perform the task you're explicitly asking about -- plus, they also do the extra one that you don't talk about but is seen in your example, removing trailing spaces;-).A regular expression can be used to offer more control over the whitespace characters that are combined.To match unicode whitespace:To match ASCII whitespace only:Matching only ASCII whitespace is sometimes essential for keeping control characters such as x0b, x0c, x1c, x1d, x1e, x1f.About \s:About re.ASCII: strip() will remote any leading and trailing whitespaces.For completeness, you can also use:which will work quickly on strings with relatively few spaces (faster than re in these situations). In any scenario, Alex Martelli's split/join solution performs at least as quickly (usually significantly more so).In your example, using the default values of timeit.Timer.repeat(), I get the following times:

EDIT:Just came across this post which provides a rather long comparison of the speeds of these methods.

Converting dictionary to JSON

sheetal_158

[Converting dictionary to JSON](https://stackoverflow.com/questions/26745519/converting-dictionary-to-json)

I am not able to access my data in the JSON. What am I doing wrong?

2014-11-04 21:38:47Z

I am not able to access my data in the JSON. What am I doing wrong?json.dumps() converts a dictionary to str object, not a json(dict) object! So you have to load your str into a dict to use it by using json.loads() methodSee json.dumps() as a save method and json.loads() as a retrieve method.This is the code sample which might help you understand it more:json.dumps() returns the JSON string representation of the python dict. See the docsYou can't do r['rating'] because r is a string, not a dict anymorePerhaps you meant something likeNo need to convert it in a string by using json.dumps()You can get the values directly from the dict object.Defining r as a dictionary should do the trick:

Converting Python dict to kwargs?

teaforthecat

[Converting Python dict to kwargs?](https://stackoverflow.com/questions/5710391/converting-python-dict-to-kwargs)

I want to build a query for sunburnt(solr interface) using class inheritance and therefore adding key - value pairs together. The sunburnt interface takes keyword arguments. How can I transform a dict ({'type':'Event'}) into keyword arguments (type='Event')?

2011-04-19 00:46:46Z

I want to build a query for sunburnt(solr interface) using class inheritance and therefore adding key - value pairs together. The sunburnt interface takes keyword arguments. How can I transform a dict ({'type':'Event'}) into keyword arguments (type='Event')?Use the double-star (aka double-splat?) operator:is equivalent to** operator would be helpful here. ** operator will unpack the dict elements and thus **{'type':'Event'} would be treated as type='Event'func(**{'type':'Event'}) is same as func(type='Event') i.e the dict elements would be converted to the keyword arguments. FYI* will unpack the list elements and they would be treated as positional arguments.func(*['one', 'two']) is same as func('one', 'two')Here is a complete example showing how to use the ** operator to pass values from a dictionary as keyword arguments.

Difference between filter and filter_by in SQLAlchemy

bodacydo

[Difference between filter and filter_by in SQLAlchemy](https://stackoverflow.com/questions/2128505/difference-between-filter-and-filter-by-in-sqlalchemy)

Could anyone explain the difference between filter and filter_by functions in SQLAlchemy?

Which one should I be using?

2010-01-24 19:49:46Z

Could anyone explain the difference between filter and filter_by functions in SQLAlchemy?

Which one should I be using?filter_by is used for simple queries on the column names using regular kwargs, likedb.users.filter_by(name='Joe')The same can be accomplished with filter, not using kwargs, but instead using the '==' equality operator, which has been overloaded on the db.users.name object:db.users.filter(db.users.name=='Joe')You can also write more powerful queries using filter, such as expressions like:db.users.filter(or_(db.users.name=='Ryan', db.users.country=='England'))We actually had these merged together originally, i.e. there was a "filter"-like method that accepted *args and **kwargs, where you could pass a SQL expression or keyword arguments (or both).  I actually find that a lot more convenient, but people were always confused by it, since they're usually still getting over the difference between column == expression and keyword = expression.  So we split them up.filter_by uses keyword arguments, whereas filter allows pythonic filtering arguments like filter(User.name=="john")It is a syntax sugar for faster query writing. Its implementation in pseudocode:For AND you can simply write:btw can be written as Also you can get object directly by PK via get method:When using get case its important that object can be returned without database request from identity map which can be used as cache(associated with transaction)

How to convert local time string to UTC?

Tom

[How to convert local time string to UTC?](https://stackoverflow.com/questions/79797/how-to-convert-local-time-string-to-utc)

How do I convert a datetime string in local time to a string in UTC time?I'm sure I've done this before, but can't find it and SO will hopefully help me (and others) do that in future.Clarification:  For example, if I have 2008-09-17 14:02:00 in my local timezone (+10), I'd like to generate a string with the equivalent UTC time: 2008-09-17 04:02:00.Also, from http://lucumr.pocoo.org/2011/7/15/eppur-si-muove/, note that in general this isn't possible as with DST and other issues there is no unique conversion from local time to UTC time.

2008-09-17 03:52:42Z

How do I convert a datetime string in local time to a string in UTC time?I'm sure I've done this before, but can't find it and SO will hopefully help me (and others) do that in future.Clarification:  For example, if I have 2008-09-17 14:02:00 in my local timezone (+10), I'd like to generate a string with the equivalent UTC time: 2008-09-17 04:02:00.Also, from http://lucumr.pocoo.org/2011/7/15/eppur-si-muove/, note that in general this isn't possible as with DST and other issues there is no unique conversion from local time to UTC time.First, parse the string into a naive datetime object. This is an instance of datetime.datetime with no attached timezone information. See documentation for datetime.strptime for information on parsing the date string.Use the pytz module, which comes with a full list of time zones + UTC. Figure out what the local timezone is, construct a timezone object from it, and manipulate and attach it to the naive datetime.Finally, use datetime.astimezone() method to convert the datetime to UTC.Source code, using local timezone "America/Los_Angeles", for the string "2001-2-3 10:11:12":From there, you can use the strftime() method to format the UTC datetime as needed:The datetime module's utcnow() function can be used to obtain the current UTC time.As the link mentioned above by Tom: http://lucumr.pocoo.org/2011/7/15/eppur-si-muove/  says:NOTE - If any of your data is in a region that uses DST, use pytz and take a look at John Millikin's answer.If you want to obtain the UTC time from a given string and your lucky enough to be in a region in the world that either doesn't use DST, or you have data that is only offset from UTC without DST applied:--> using local time as the basis for the offset value:--> Or, from a known offset, using datetime.timedelta():UPDATE:Since python 3.2 datetime.timezone is available.  You can generate a timezone aware datetime object with the command below:If your ready to take on timezone conversions go read this:https://medium.com/@eleroy/10-things-you-need-to-know-about-date-and-time-in-python-with-datetime-pytz-dateutil-timedelta-309bfbafb3f7Thanks @rofly, the full conversion from string to string is as follows:My summary of the time/calendar functions:time.strptime

string --> tuple (no timezone applied, so matches string)time.mktime

local time tuple --> seconds since epoch (always local time)time.gmtime

seconds since epoch --> tuple in UTCand calendar.timegm

tuple in UTC --> seconds since epochtime.localtime

seconds since epoch --> tuple in local timezoneHere's a summary of common Python time conversions.Some methods drop fractions of seconds, and are marked with (s). An explicit formula such as ts = (d - epoch) / unit can be used instead (thanks jfs).Source: taaviburns.caSource: http://feihonghsu.blogspot.com/2008/02/converting-from-local-time-to-utc.htmlExample usage from bd808: If your source is a datetime.datetime object t, call as:I'm having good luck with dateutil (which is widely recommended on SO for other related questions):(Code was derived from this answer to Convert UTC datetime string to local datetime)One more example with pytz, but includes localize(), which saved my day.I've had the most success with python-dateutil:if you prefer datetime.datetime:I did it like this:If you want to get fancy, you can turn this into a functor:Result: You can do it with:How about - if seconds is None then it converts the local time to UTC time else converts the passed in time to UTC.For getting around day-light saving, etc.None of the above answers particularly helped me. The code below works for GMT.Using http://crsmithdev.com/arrow/This library makes life easy :)I found the best answer on another question here. It only uses python built-in libraries and does not require you to input your local timezone (a requirement in my case) I'm reposting the answer here since this question pops up in google instead of the linked question depending on the search keywords.In python3:pip install python-dateutilHow about - if seconds is None then it converts the local time to UTC time else converts the passed in time to UTC.

How can I get a list of all classes within current module in Python?

mcccclean

[How can I get a list of all classes within current module in Python?](https://stackoverflow.com/questions/1796180/how-can-i-get-a-list-of-all-classes-within-current-module-in-python)

I've seen plenty of examples of people extracting all of the classes from a module, usually something like:Awesome.But I can't find out how to get all of the classes from the current module.This is probably something really obvious, but I haven't been able to find anything. Can anyone help me out?

2009-11-25 10:59:46Z

I've seen plenty of examples of people extracting all of the classes from a module, usually something like:Awesome.But I can't find out how to get all of the classes from the current module.This is probably something really obvious, but I haven't been able to find anything. Can anyone help me out?Try this:In your context:And even better:Because inspect.getmembers() takes a predicate.What about?I don't know if there's a 'proper' way to do it, but your snippet is on the right track: just add import foo to foo.py, do inspect.getmembers(foo), and it should work fine.I was able to get all I needed from the dir built in plus getattr.Though, it does come out looking like a hairball:Note that the stdlib's Python class browser module uses static source analysis, so it only works for modules that are backed by a real .py file.If you want to have all the classes, that belong to the current module, you could use this :If you use Nadia's answer and you were importing other classes on your module, that classes will be being imported too.So that's why member.__module__ == __name__ is being added to the predicate used on is_class_member. This statement checks that the class really belongs to the module.A predicate is a function (callable), that returns a boolean value.Another solution which works in Python 2 and 3:This is the line that I use to get all of the classes that have been defined in the current module (ie not imported). It's a little long according to PEP-8 but you can change it as you see fit.This gives you a list of the class names. If you want the class objects themselves just keep obj instead.This is has been more useful in my experience.I think that you can do something like this.if you need own classesI frequently find myself writing command line utilities wherein the first argument is meant to refer to one of many different classes. For example ./something.py feature command —-arguments, where Feature is a class and command is a method on that class. Here's a base class that makes this easy.The assumption is that this base class resides in a directory alongside all of its subclasses. You can then call ArgBaseClass(foo = bar).load_subclasses() which will return a dictionary. For example, if the directory looks like this:Assuming feature.py implements class Feature(ArgBaseClass), then the above invocation of load_subclasses will return { 'feature' : <Feature object> }. The same kwargs (foo = bar) will be passed into the Feature class.

Difference between numpy.array shape (R, 1) and (R,)

clwen

[Difference between numpy.array shape (R, 1) and (R,)](https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r)

In numpy, some of the operations return in shape (R, 1) but some return (R,). This will make matrix multiplication more tedious since explicit reshape is required. For example, given a matrix M, if we want to do numpy.dot(M[:,0], numpy.ones((1, R))) where R is the number of rows (of course, the same issue also occurs column-wise). We will get matrices are not aligned error since M[:,0] is in shape (R,) but numpy.ones((1, R)) is in shape (1, R).So my questions are:

2014-02-26 20:55:35Z

In numpy, some of the operations return in shape (R, 1) but some return (R,). This will make matrix multiplication more tedious since explicit reshape is required. For example, given a matrix M, if we want to do numpy.dot(M[:,0], numpy.ones((1, R))) where R is the number of rows (of course, the same issue also occurs column-wise). We will get matrices are not aligned error since M[:,0] is in shape (R,) but numpy.ones((1, R)) is in shape (1, R).So my questions are:You write, "I know literally it's list of numbers and list of lists where all list contains only a number" but that's a bit of an unhelpful way to think about it.The best way to think about NumPy arrays is that they consist of two parts, a data buffer which is just a block of raw elements, and a view which describes how to interpret the data buffer.For example, if we create an array of 12 integers:Then a consists of a data buffer, arranged something like this:and a view which describes how to interpret the data:Here the shape (12,) means the array is indexed by a single index which runs from 0 to 11. Conceptually, if we label this single index i, the array a looks like this:If we reshape an array, this doesn't change the data buffer. Instead, it creates a new view that describes a different way to interpret the data. So after:the array b has the same data buffer as a, but now it is indexed by two indices which run from 0 to 2 and 0 to 3 respectively. If we label the two indices i and j, the array b looks like this:which means that:You can see that the second index changes quickly and the first index changes slowly. If you prefer this to be the other way round, you can specify the order parameter:which results in an array indexed like this:which means that:It should now be clear what it means for an array to have a shape with one or more dimensions of size 1. After:the array d is indexed by two indices, the first of which runs from 0 to 11, and the second index is always 0:and so:A dimension of length 1 is "free" (in some sense), so there's nothing stopping you from going to town:giving an array indexed like this:and so:See the NumPy internals documentation for more details about how arrays are implemented.Since numpy.reshape just creates a new view, you shouldn't be scared about using it whenever necessary. It's the right tool to use when you want to index an array in a different way.However, in a long computation it's usually possible to arrange to construct arrays with the "right" shape in the first place, and so minimize the number of reshapes and transposes. But without seeing the actual context that led to the need for a reshape, it's hard to say what should be changed.The example in your question is:but this is not realistic. First, this expression:computes the result more simply. Second, is there really something special about column 0? Perhaps what you actually need is:The difference between (R,) and (1,R) is literally the number of indices that you need to use.  ones((1,R)) is a 2-D array that happens to have only one row.  ones(R) is a vector.  Generally if it doesn't make sense for the variable to have more than one row/column, you should be using a vector, not a matrix with a singleton dimension.For your specific case, there are a couple of options:1) Just make the second argument a vector.  The following works fine:2) If you want matlab like matrix operations, use the class matrix instead of ndarray.  All matricies are forced into being 2-D arrays, and operator * does matrix multiplication instead of element-wise (so you don't need dot).  In my experience, this is more trouble that it is worth, but it may be nice if you are used to matlab.The shape is a tuple. If there is only 1 dimension the shape will be one number and just blank after a comma. For 2+ dimensions, there will be a number after all the commas.For its base array class, 2d arrays are no more special than 1d or 3d ones.  There are some operations the preserve the dimensions, some that reduce them, other combine or even expand them.Other expressions that give the same arrayMATLAB started out with just 2D arrays.  Newer versions allow more dimensions, but retain the lower bound of 2.  But you still have to pay attention to the difference between a row matrix and column one, one with shape (1,3) v (3,1).  How often have you written [1,2,3].'?  I was going to write row vector and column vector, but with that 2d constraint, there aren't any vectors in MATLAB - at least not in the mathematical sense of vector as being 1d.Have you looked at np.atleast_2d (also _1d and _3d versions)?1) The reason not to prefer a shape of (R, 1) over (R,) is that it unnecessarily complicates things. Besides, why would it be preferable to have shape (R, 1) by default for a length-R vector instead of (1, R)? It's better to keep it simple and be explicit when you require additional dimensions.2) For your example, you are computing an outer product so you can do this without a reshape call by using np.outer:There are a lot of good answers here already. But for me it was hard to find some example, where the shape or array can break all the program.So here is the one:This will fail with error:but if we add reshape to a:this works correctly!

How do I check if a given Python string is a substring of another one? [duplicate]

snakile

[How do I check if a given Python string is a substring of another one? [duplicate]](https://stackoverflow.com/questions/5143769/how-do-i-check-if-a-given-python-string-is-a-substring-of-another-one)

I have two strings and I would like to check whether the first is a substring of the other. Does Python have such a built-in functionality?

2011-02-28 15:13:10Z

I have two strings and I would like to check whether the first is a substring of the other. Does Python have such a built-in functionality?Try using in like this:Trystring.find("substring") will help you. This function returns -1 when there is no substring.

How can I create an object and add attributes to it?

John

[How can I create an object and add attributes to it?](https://stackoverflow.com/questions/2827623/how-can-i-create-an-object-and-add-attributes-to-it)

I want to create a dynamic object (inside another object) in Python and then add attributes to it.I tried:but this didn't work.Any ideas?edit:I am setting the attributes from a for loop which loops through a list of values, e.g.In the above example I would get obj.a.attr1, obj.a.attr2, obj.a.attr3.  I used the setattr function because I didn't know how to do obj.a.NAME from a for loop.How would I set the attribute based on the value of p in the example above?

2010-05-13 14:34:03Z

I want to create a dynamic object (inside another object) in Python and then add attributes to it.I tried:but this didn't work.Any ideas?edit:I am setting the attributes from a for loop which loops through a list of values, e.g.In the above example I would get obj.a.attr1, obj.a.attr2, obj.a.attr3.  I used the setattr function because I didn't know how to do obj.a.NAME from a for loop.How would I set the attribute based on the value of p in the example above?You could use my ancient Bunch recipe, but if you don't want to make a "bunch class", a very simple one already exists in Python -- all functions can have arbitrary attributes (including lambda functions).  So, the following works:Whether the loss of clarity compared to the venerable Bunch recipe is OK, is a style decision I will of course leave up to you.The built-in object can be instantiated but can't have any attributes set on it.  (I wish it could, for this exact purpose.)  It doesn't have a __dict__ to hold the attributes.I generally just do this:When I can, I give the Object class a more meaningful name, depending on what kind of data I'm putting in it.Some people do a different thing, where they use a sub-class of dict that allows attribute access to get at the keys. (d.key instead of d['key'])Edit: For the addition to your question, using setattr is fine.  You just can't use setattr on object() instances.There is types.SimpleNamespace class in Python 3.3+:collections.namedtuple, typing.NamedTuple could be used for immutable objects. PEP 557 -- Data Classes  suggests a mutable alternative.For a richer functionality, you could try attrs package. See an example usage.There are a few ways to reach this goal.

Basically you need an object which is extendable.The mock module is basically made for that.Now you can do (not sure if it's the same answer as evilpie):Try the code below:You can also use a class object directly; it creates a namespace:as docs say:You could just use dummy-class instance.These solutions are very helpful during testing. Building on everyone else's answers I do this in Python 2.7.9 (without staticmethod I get a TypeError (unbound method...):Which objects are you using? Just tried that with a sample class and it worked fine:And I got 123 as the answer.The only situation where I see this failing is if you're trying a setattr on a builtin object.Update: From the comment this is a repetition of: Why can't you add attributes to object in python?Coming to this late in the day but here is my pennyworth with an object that just happens to hold some useful paths in an app but you can adapt it for anything where you want a sorta dict of information that you can access with getattr and dot notation (which is what I think this question is really about):This is cool because now:So this uses the function object like the above answers but uses the function to get the values (you can still use (getattr, x_path, 'repository') rather than x_path('repository') if you prefer).If we can determine and aggregate all the attributes and values together before creating the nested object, then we could create a new class that takes a dictionary argument on creation.We can also allow keyword arguments. See this post. Other way i see, this way:

How do I create test and train samples from one dataframe with pandas?

tooty44

[How do I create test and train samples from one dataframe with pandas?](https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas)

I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.Thanks!

2014-06-10 17:24:57Z

I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.Thanks!I would just use numpy's randn:And just to see this has worked:scikit learn's train_test_split is a good one.Pandas random sample will also work I would use scikit-learn's own training_test_split, and generate it from the indexThere are many ways to create a train/test and even validation samples.Case 1: classic way train_test_split without any options:Case 2: case of a very small datasets (<500 rows): in order to get results for all your lines with this cross-validation. At the end, you will have one prediction for each line of your available training set.Case 3a: Unbalanced datasets for classification purpose. Following the case 1, here is the equivalent solution:Case 3b: Unbalanced datasets for classification purpose. Following the case 2, here is the equivalent solution:Case 4: you need to create a train/test/validation sets on big data to tune hyperparameters (60% train, 20% test and 20% val).You can use below code to create test and train samples :Test size can vary depending on the percentage of data you want to put in your test and train dataset.There are many valid answers. Adding one more to the bunch.

from sklearn.cross_validation import train_test_splitYou may also consider stratified division into training and testing set. Startified division also generates training and testing set randomly but in such a way that original class proportions are preserved. This makes training and testing sets better reflect the properties of the original dataset.df[train_inds] and df[test_inds] give you the training and testing sets of your original DataFrame df.If you need to split your data with respect to the lables column in your data set you can use this:and use it:you can also pass random_state if you want to control the split randomness or use some global random seed.This is what I wrote when I needed to split a DataFrame. I considered using Andy's approach above, but didn't like that I could not control the size of the data sets exactly (i.e., it would be sometimes 79, sometimes 81, etc.).Just select range row from df like thisThere are many great answers above so I just wanna add one more example in the case that you want to specify the exact number of samples for the train and test sets by using just the numpy library.To split into more than two classes such as train, test, and validation, one can do:This will put approximately 70% of data in training, 15% in test, and 15% in validation.If your wish is to have one dataframe in and two dataframes out (not numpy arrays), this should do the trick:You can make use of df.as_matrix() function and create Numpy-array and pass it.A bit more elegant to my taste is to create a random column and then split by it, this way we can get a split that will suit our needs and will be random. You can use ~ (tilde operator) to exclude the rows sampled using df.sample(), letting pandas alone handle sampling and filtering of indexes, to obtain two sets.I think you also need to a get a copy not a slice of dataframe if you wanna add columns later.How about this?

df is my dataframe

Why does range(start, end) not include end?

MetaGuru

[Why does range(start, end) not include end?](https://stackoverflow.com/questions/4504662/why-does-rangestart-end-not-include-end)

gives youWhy not 1-11?Did they just decide to do it like that at random or does it have some value I am not seeing?

2010-12-21 22:45:46Z

gives youWhy not 1-11?Did they just decide to do it like that at random or does it have some value I am not seeing?Because it's more common to call range(0, 10) which returns [0,1,2,3,4,5,6,7,8,9] which contains 10 elements which equals len(range(0, 10)). Remember that programmers prefer 0-based indexing.Also, consider the following common code snippet:Could you see that if range() went up to exactly len(li) that this would be problematic? The programmer would need to explicitly subtract 1. This also follows the common trend of programmers preferring for(int i = 0; i < 10; i++) over for(int i = 0; i <= 9; i++).If you are calling range with a start of 1 frequently, you might want to define your own function:Exclusive ranges do have some benefits:For one thing each item in range(0,n) is a valid index for lists of length n.Also range(0,n) has a length of n, not n+1 which an inclusive range would.Although there are some useful algorithmic explanations here, I think it may help to add some simple 'real life' reasoning as to why it works this way, which I have found useful when introducing the subject to young newcomers:With something like 'range(1,10)' confusion can arise from thinking that pair of parameters represents the "start and end".It is actually start and "stop".Now, if it were the "end" value then, yes, you might expect that number would be included as the final entry in the sequence. But it is not the "end".Others mistakenly call that parameter "count" because if you only ever use 'range(n)' then it does, of course, iterate 'n' times. This logic breaks down when you add the start parameter.So the key point is to remember its name: "stop".

That means it is the point at which, when reached, iteration will stop immediately. Not after that point.So, while "start" does indeed represent the first value to be included, on reaching the "stop" value it 'breaks' rather than continuing to process 'that one as well' before stopping.One analogy that I have used in explaining this to kids is that, ironically, it is better behaved than kids! It doesn't stop after it supposed to - it stops immediately without finishing what it was doing. (They get this ;) )Another analogy - when you drive a car you don't pass a stop/yield/'give way' sign and end up with it sitting somewhere next to, or behind, your car. Technically you still haven't reached it when you do stop. It is not included in the 'things you passed on your journey'.I hope some of that helps in explaining to Pythonitos/Pythonitas!It works well in combination with zero-based indexing and len(). For example, if you have 10 items in a list x, they are numbered 0-9. range(len(x)) gives you 0-9.Of course, people will tell you it's more Pythonic to do for item in x or for index, item in enumerate(x) rather than for i in range(len(x)).Slicing works that way too: foo[1:4] is items 1-3 of foo (keeping in mind that item 1 is actually the second item due to the zero-based indexing). For consistency, they should both work the same way.I think of it as: "the first number you want, followed by the first number you don't want." If you want 1-10, the first number you don't want is 11, so it's range(1, 11).If it becomes cumbersome in a particular application, it's easy enough to write a little helper function that adds 1 to the ending index and calls range().It's also useful for splitting ranges; range(a,b) can be split into range(a, x) and range(x, b), whereas with inclusive range you would write either x-1 or x+1. While you rarely need to split ranges, you do tend to split lists quite often, which is one of the reasons slicing a list l[a:b] includes the a-th element but not the b-th. Then range having the same property makes it nicely consistent.The length of the range is the top value minus the bottom value.It's very similar to something like:in a C-style language.Also like Ruby's range:However, Ruby recognises that many times you'll want to include the terminal value and offers the alternative syntax:Basically in python range(n) iterates n times, which is of exclusive nature that is why it does not give last value when it is being printed, we can create a function which gives 

inclusive value it means it will also print last value mentioned in range.Consider the codeThe idea is that you get a list of length y-x, which you can (as you see above) iterate over. Read up on the python docs for range - they consider for-loop iteration the primary usecase.It's just more convenient to reason about in many cases.Basically, we could think of a range as an interval between start and end. If start <= end, the length of the interval between them  is end - start. If len was actually defined as the length, you'd have:However, we count the integers included in the range instead of measuring the length of the interval. To keep the above property true, we should include one of the endpoints and exclude the other.Adding the step parameter is like introducing a unit of length. In that case, you'd expectfor length. To get the count, you just use integer division.

How to manage local vs production settings in Django?

akv

[How to manage local vs production settings in Django?](https://stackoverflow.com/questions/1626326/how-to-manage-local-vs-production-settings-in-django)

What is the recommended way of handling settings for local development and the production server? Some of them (like constants, etc) can be changed/accessed in both, but some of them (like paths to static files) need to remain different, and hence should not be overwritten every time the new code is deployed.Currently, I am adding all constants to settings.py. But every time I change some constant locally, I have to copy it to the production server and edit the file for production specific changes... :( Edit: looks like there is no standard answer to this question, I've accepted the most popular method.

2009-10-26 18:00:26Z

What is the recommended way of handling settings for local development and the production server? Some of them (like constants, etc) can be changed/accessed in both, but some of them (like paths to static files) need to remain different, and hence should not be overwritten every time the new code is deployed.Currently, I am adding all constants to settings.py. But every time I change some constant locally, I have to copy it to the production server and edit the file for production specific changes... :( Edit: looks like there is no standard answer to this question, I've accepted the most popular method.In settings.py:You can override what needed in local_settings.py; it should stay out of your version control then. But since you mention copying I'm guessing you use none ;)Two Scoops of Django: Best Practices for Django 1.5 suggests using version control for your settings files and storing the files in a separate directory:The base.py file contains common settings (such as MEDIA_ROOT or ADMIN), while local.py and production.py have site-specific settings:In the base file settings/base.py:In the local development settings file settings/local.py:In the file production settings file settings/production.py:Then when you run django, you add the --settings option:The authors of the book have also put up a sample project layout template on Github.Instead of settings.py, use this layout:common.py is where most of your configuration lives.prod.py imports everything from common, and overrides whatever it needs to override:Similarly, dev.py imports everything from common.py and overrides whatever it needs to override.Finally, __init__.py is where you decide which settings to load, and it's also where you store secrets (therefore this file should not be versioned):What I like about this solution is:I use a slightly modified version of the "if DEBUG" style of settings that Harper Shelby posted.  Obviously depending on the environment (win/linux/etc.) the code might need to be tweaked a bit.I was in the past using the "if DEBUG" but I found that occasionally I needed to do testing with DEUBG set to False.  What I really wanted to distinguish if the environment was production or development, which gave me the freedom to choose the DEBUG level.I'd still consider this way of settings a work in progress. I haven't seen any one way to handling Django settings that covered all the bases and at the same time wasn't a total hassle to setup (I'm not down with the 5x settings files methods).I use a settings_local.py and a settings_production.py. After trying several options I've found that it's easy to waste time with complex solutions when simply having two settings files feels easy and fast.When you use mod_python/mod_wsgi for your Django project you need to point it to your settings file. If you point it to app/settings_local.py on your local server and app/settings_production.py on your production server then life becomes easy. Just edit the appropriate settings file and restart the server (Django development server will restart automatically).I manage my configurations with the help of django-split-settings. It is a drop-in replacement for the default settings. It is simple, yet configurable. And refactoring of your exisitng settings is not required.Here's a small example (file example/settings/__init__.py):That's it.I wrote a blog post about managing django's settings with django-split-sttings. Have a look!TL;DR: The trick is to modify os.environment before you import settings/base.py in any settings/<purpose>.py, this will greatly simplify things.Just thinking about all these intertwining files gives me a headache. 

Combining, importing (sometimes conditionally), overriding, patching of what was already set in case DEBUG setting changed later on. 

What a nightmare!Through the years I went through all different solutions. They all somewhat work, but are so painful to manage. 

WTF! Do we really need all that hassle? We started with just one settings.py file. 

Now we need a documentation just to correctly combine all these together in a correct order!I hope I finally hit the (my) sweet spot with the solution below.My strategy consists of excellent django-environ used with ini style files, 

providing os.environment defaults for local development, some minimal and short settings/<purpose>.py files that have an 

import settings/base.py AFTER the os.environment was set from an INI file. This effectively give us a kind of settings injection.The trick here is to modify os.environment before you import settings/base.py.To see the full example go do the repo: https://github.com/wooyek/django-settings-strategyA defaults for local development. A secret file, to mostly set required environment variables. 

Set them to empty values if they are not required in local development. 

We provide defaults here and not in settings/base.py to fail on any other machine if the're missing from the environment.What happens in here, is loading environment from settings/.env, then importing common settings 

from settings/base.py. After that we can override a few to ease local development.For production we should not expect an environment file, but it's easier to have one if we're testing something. 

But anyway, lest's provide few defaults inline, so settings/base.py can respond accordingly. The main point of interest here are DEBUG and ASSETS_DEBUG overrides, 

they will be applied to the python os.environ ONLY if they are MISSING from the environment and the file. These will be our production defaults, no need to put them in the environment or file, but they can be overridden if needed. Neat!These are your mostly vanilla django settings, with a few conditionals and lot's of reading them from the environment. 

Almost everything is in here, keeping all the purposed environments consistent and as similar as possible.The main differences are below (I hope these are self explanatory):The last bit shows the power here. ASSETS_DEBUG has a sensible default, 

which can be overridden in settings/production.py and even that that can be overridden by an environment setting! Yay! In effect we have a mixed hierarchy of importance:The problem with most of these solutions is that you either have your local settings applied before the common ones, or after them.So it's impossible to override things likeat the same time.One solution can be implemented using "ini"-style config files with the ConfigParser class. It supports multiple files, lazy string interpolation, default values and a lot of other goodies.

Once a number of files have been loaded, more files can be loaded and their values will override the previous ones, if any.You load one or more config files, depending on the machine address, environment variables and even values in previously loaded config files. Then you just use the parsed values to populate the settings. One strategy I have successfully used has been:As an example of something you can achieve with this, you can define a "subdomain" value per-env, which is then used in the default settings (as hostname: %(subdomain).whatever.net) to define all the necessary hostnames and cookie things django needs to work.This is as DRY I could get, most (existing) files had just 3 or 4 settings. On top of this I had to manage customer configuration, so an additional set of configuration files (with things like database names, users and passwords, assigned subdomain etc) existed, one or more per customer.One can scale this as low or as high as necessary, you just put in the config file the keys you want to configure per-environment, and once there's need for a new config, put the previous value in the default config, and override it where necessary.This system has proven reliable and works well with version control. It has been used for long time managing two separate clusters of applications (15 or more separate instances of the django site per machine), with more than 50 customers, where the clusters were changing size and members depending on the mood of the sysadmin...I am also working with Laravel and I like the implementation there. I tried to mimic it and combining it with the solution proposed by T. Stone (look above):Maybe something like this would help you.Remember that settings.py is a live code file. Assuming that you don't have DEBUG set on production (which is a best practice), you can do something like:Pretty basic, but you could, in theory, go up to any level of complexity based on just the value of DEBUG - or any other variable or code check you wanted to use.For most of my projects I use following pattern:(To run manage.py with custom settings file you simply use --settings command option: manage.py <command> --settings=settings_you_wish_to_use.py)My solution to that problem is also somewhat of a mix of some solutions already stated here:I then base all my environment-dependent settings on that one:I prefer this to having two separate settings.py files that I need to maintain as I can keep my settings structured in a single file easier than having them spread across several files. Like this, when I update a setting I don't forget to do it for both environments.Of course that every method has its disadvantages and this one is no exception. The problem here is that I can't overwrite the local_settings.py file whenever I push my changes into production, meaning I can't just copy all files blindly, but that's something I can live with.I use a variation of what jpartogi mentioned above, that I find a little shorter:Basically on each computer (development or production) I have the appropriate hostname_settings.py file that gets dynamically loaded.There is also Django Classy Settings. I personally am a big fan of it. It's built by one of the most active people on the Django IRC. You would use environment vars to set things.http://django-classy-settings.readthedocs.io/en/latest/In order to use different settings configuration on different environment, create different settings file. And in your deployment script, start the server using --settings=<my-settings.py> parameter, via which you can use different settings on different environment.Benefits of using this approach:1 - Create a new folder inside your app and name settings to it.2 - Now create a new init.py file in it and inside it write3 - Create three new files in the settings folder name local.py and production.py and base.py4 - Inside base.py copy all the content of previous settings.p folder and rename it with something different let say old_settings.py5 - In base.py change your BASE_DIR path to point to your new path of settingOld path-> BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(file)))New path -> BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(file)))now in this way the project dir can be structured and can be manageable among production and local development.I differentiate it in manage.py and created two separate settings file: local_settings.py and prod_settings.py. In manage.py I check whether the server is local server or production server. If it is a local server it would load up local_settings.py and it is a production server it would load up prod_settings.py. Basically this is how it would look like:I found it to be easier to separate the settings file into two separate file instead of doing lots of ifs inside the settings file.As an alternative to maintain different file if you wiil:

If you are using git or any other VCS to push codes from local to server, what you can do is add the settings file to .gitignore.This will allow you to have different content in both places without any problem. SO on server you can configure an independent version of settings.py and any changes made on the local wont reflect on server and vice versa.In addition, it will remove the settings.py file from github also, the big fault, which i have seen many newbies doing.I had my settings split as follows  We have 3 environments  Now obviously staging and production should have the maximum possible similar environment. So we kept prod.py for both.But there was a case where I had to identify running server is a production server. @T. Stone 's answer helped me write check as follows.  Making multiple versions of settings.py is an anti pattern for 12 Factor App methodology.

use python-decouple or django-environ instead.I think the best solution is suggested by @T. Stone, but I don't know why just don't use the DEBUG flag in Django. I Write the below code for my website:Always the simple solutions are better than complex ones.I found the responses here very helpful. (Has this been more definitively solved? The last response was a year ago.) After considering all the approaches listed, I came up with a solution that I didn't see listed here. My criteria were:I thought switching on the host machine made some sense, but then figured the real issue here is different settings for different environments, and had an aha moment. I put this code at the end of my settings.py file:This way, the app defaults to production settings, which means you are explicitly "whitelisting" your development environment. It is much safer to forget to set the environment variable locally than if it were the other way around and you forgot to set something in production and let some dev settings be used. When developing locally, either from the shell or in a .bash_profile or wherever:(Or if you're developing on Windows, set via the Control Panel or whatever its called these days... Windows always made it so obscure that you could set environment variables.)With this approach, the dev settings are all in one (standard) place, and simply override the production ones where needed. Any mucking around with development settings should be completely safe to commit to source control with no impact on production. 

Splitting on first occurrence

Acorn

[Splitting on first occurrence](https://stackoverflow.com/questions/6903557/splitting-on-first-occurrence)

What would be the best way to split a string on the first occurrence of a delimiter?For example:splitting on the first mango to get:

2011-08-01 19:45:21Z

What would be the best way to split a string on the first occurrence of a delimiter?For example:splitting on the first mango to get:From the docs:For me the better approach is that:...because if happens that occurrence is not in the string you'll get "IndexError: list index out of range".Therefore -1 will not get any harm cause number of occurrences is already set to one.You can also use str.partition:The advantage of using str.partition is that it's always gonna return a tuple in the form:So this makes unpacking the output really flexible as there's always going to be 3 elements in the resulting tuple.This will split data with the first occurrence of '.' in the string or data frame column value.

How do I create an empty array/matrix in NumPy?

Ben

[How do I create an empty array/matrix in NumPy?](https://stackoverflow.com/questions/568962/how-do-i-create-an-empty-array-matrix-in-numpy)

I can't figure out how to use an array or matrix in the way that I would normally use a list. I want to create an empty array (or matrix) and then add one column (or row) to it at a time.At the moment the only way I can find to do this is like:Whereas if it were a list, I'd do something like this:Is there a way to use that kind of notation for NumPy arrays or matrices?

2009-02-20 09:58:11Z

I can't figure out how to use an array or matrix in the way that I would normally use a list. I want to create an empty array (or matrix) and then add one column (or row) to it at a time.At the moment the only way I can find to do this is like:Whereas if it were a list, I'd do something like this:Is there a way to use that kind of notation for NumPy arrays or matrices?You have the wrong mental model for using NumPy efficiently. NumPy arrays are stored in contiguous blocks of memory. If you want to add rows or columns to an existing array, the entire array needs to be copied to a new block of memory, creating gaps for the new elements to be stored. This is very inefficient if done repeatedly to build an array.In the case of adding rows, your best bet is to create an array that is as big as your data set will eventually be, and then add data to it row-by-row:A NumPy array is a very different data structure from a list and is designed to be used in different ways.  Your use of hstack is potentially very inefficient... every time you call it, all the data in the existing array is copied into a new one. (The append function will have the same issue.)  If you want to build up your matrix one column at a time, you might be best off to keep it in a list until it is finished, and only then convert it into an array.e.g.item can be a list, an array or any iterable, as long 

as each item has the same number of elements.

In this particular case (data is some iterable holding the matrix columns) you can simply use(Also note that using list as a variable name is probably not good practice since it masks the built-in type by that name, which can lead to bugs.)EDIT:If for some reason you really do want to create an empty array, you can just use  numpy.array([]), but this is rarely useful!To create an empty multidimensional array in NumPy (e.g. a 2D array m*n to store your matrix), in case you don't know m how many rows you will append and don't care about the computational cost Stephen Simmons mentioned (namely re-buildinging the array at each append), you can squeeze to 0 the dimension to which you want to append to: X = np.empty(shape=[0, n]).This way you can use for example (here m = 5 which we assume we didn't know when creating the empty matrix, and n = 2):which will give you:I looked into this a lot because I needed to use a numpy.array as a set in one of my school projects and I needed to be initialized empty... I didn't found any relevant answer here on Stack Overflow, so I started doodling something. The result will be:Therefore you can directly initialize an np array as follows:I hope this helps.You can use the append function.  For rows:For columns:EDIT

Of course, as mentioned in other answers, unless you're doing some processing (ex. inversion) on the matrix/array EVERY time you append something to it, I would just create a list, append to it then convert it to an array.If you absolutely don't know the final size of the array, you can increment the size of the array like this:You can apply it to build any kind of array, like zeros:Depending on what you are using this for, you may need to specify the data type (see 'dtype').For example, to create a 2D array of 8-bit values (suitable for use as a monochrome image):For an RGB image, include the number of color channels in the shape: shape=(H,W,3)You may also want to consider zero-initializing with numpy.zeros instead of using numpy.empty.  See the note here.I think you want to handle most of the work with lists then use the result as a matrix. Maybe this is a way ; I think you can create empty numpy array like:This format is useful when you want to append numpy array in the loop. Here is some workaround to make numpys look more like ListsOUTPUT: array([ 2., 24.])

Getting the SQL from a Django QuerySet [duplicate]

exupero

[Getting the SQL from a Django QuerySet [duplicate]](https://stackoverflow.com/questions/3748295/getting-the-sql-from-a-django-queryset)

How do I get the SQL that Django will use on the database from a QuerySet object? I'm trying to debug some strange behavior, but I'm not sure what queries are going to the database. Thanks for your help.

2010-09-20 02:11:54Z

How do I get the SQL that Django will use on the database from a QuerySet object? I'm trying to debug some strange behavior, but I'm not sure what queries are going to the database. Thanks for your help.You print the queryset's query attribute.Easy:For example:It should also be mentioned that if you have DEBUG = True, then all of your queries are logged, and you can get them by accessing connection.queries:The django debug toolbar project uses this to present the queries on a page in a neat manner.The accepted answer did not work for me when using Django 1.4.4.  Instead of the raw query, a reference to the Query object was returned: <django.db.models.sql.query.Query object at 0x10a4acd90>. The following returned the query:This middleware will output every SQL query to your console, with color highlighting and execution time, it's been invaluable for me in optimizing some tricky requestshttp://djangosnippets.org/snippets/290/As an alternative to the other answers, django-devserver outputs SQL to the console.

Converting Epoch time into the datetime

user1667633

[Converting Epoch time into the datetime](https://stackoverflow.com/questions/12400256/converting-epoch-time-into-the-datetime)

I am getting a response from the rest is an Epoch time format likeI want to convert that epoch seconds in MySQL format time so that I could store the differences in my MySQL database.I tried:The above result is not what I am expecting. I want it be like Please suggest how can I achieve this?Also,

Why I am getting TypeError: a float is required for

2012-09-13 06:00:51Z

I am getting a response from the rest is an Epoch time format likeI want to convert that epoch seconds in MySQL format time so that I could store the differences in my MySQL database.I tried:The above result is not what I am expecting. I want it be like Please suggest how can I achieve this?Also,

Why I am getting TypeError: a float is required forTo convert your time value (float or int) to a formatted string, use:You can also use datetime:To get UTC:This is what you needPlease input a float instead of an int and that other TypeError should go away.Try this:Also in MySQL, you can FROM_UNIXTIME like:For your 2nd question, it is probably because getbbb_class.end_time is a string.  You can convert it to numeric like: float(getbbb_class.end_time)This is a little more wordy but it comes from date command in unix.First a bit of info in epoch from man gmtimeto understand how epoch should be.just ensure the arg you are passing to time.gmtime() is integer.

How to convert 2D float numpy array to 2D int numpy array?

Shan

[How to convert 2D float numpy array to 2D int numpy array?](https://stackoverflow.com/questions/10873824/how-to-convert-2d-float-numpy-array-to-2d-int-numpy-array)

How to convert real numpy array to int numpy array?

Tried using map directly to array but it did not work.

2012-06-03 20:46:28Z

How to convert real numpy array to int numpy array?

Tried using map directly to array but it did not work.Use the astype method.Some numpy functions for how to control the rounding: rint, floor,trunc, ceil. depending how  u wish to round the floats, up, down, or to the nearest int. To make one of this in to int, or one of the  other types in numpy, astype (as answered by BrenBern): you can use np.int_:If you're not sure your input is going to be a Numpy array, you can use asarray with dtype=int instead of astype:If the input array already has the correct dtype, asarray avoids the array copy while astype does not (unless you specify copy=False):

find first sequence item that matches a criterion [duplicate]

Jonathan

[find first sequence item that matches a criterion [duplicate]](https://stackoverflow.com/questions/9868653/find-first-sequence-item-that-matches-a-criterion)

What would be the most elegant and efficient way of finding/returning the first list item that matches a certain criterion?For example, if I have a list of objects and I would like to get the first object of those with attribute obj.val==5. I could of course use list comprehension, but that would incur O(n) and if n is large, it's wasteful. I could also use a loop with break once the criterion was met, but I thought there could be a more pythonic/elegant solution.

2012-03-26 08:11:32Z

What would be the most elegant and efficient way of finding/returning the first list item that matches a certain criterion?For example, if I have a list of objects and I would like to get the first object of those with attribute obj.val==5. I could of course use list comprehension, but that would incur O(n) and if n is large, it's wasteful. I could also use a loop with break once the criterion was met, but I thought there could be a more pythonic/elegant solution.If you don't have any other indexes or sorted information for your objects, then you will have to iterate until such an object is found:This is however faster than a complete list comprehension. Compare these two:The first one needs 5.75ms, the second one 58.3µs (100 times faster because the loop 100 times shorter).it'll return the object if found else it'll return "not found"

How to get current time in python and break up into year, month, day, hour, minute?

user781486

[How to get current time in python and break up into year, month, day, hour, minute?](https://stackoverflow.com/questions/30071886/how-to-get-current-time-in-python-and-break-up-into-year-month-day-hour-minu)

I would like to get the current time in Python and assign them into variables like year, month, day, hour, minute. How can this be done in Python 2.7?

2015-05-06 08:51:46Z

I would like to get the current time in Python and assign them into variables like year, month, day, hour, minute. How can this be done in Python 2.7?The datetime module is your friend:You don't need separate variables, the attributes on the returned datetime object have all you need.The datetime answer by tzaman is much cleaner, but you can do it with the original python time module:Output:Here's a one-liner that comes in just under the 80 char line max.By unpacking timetuple of datetime object, you should get what you want:For python 3 Let's see how to get and print day,month,year in python from current time:result:Three libraries for accessing and manipulating dates and times, namely datetime, arrow and pendulum, all make these items available in namedtuples whose elements are accessible either by name or index. Moreover, the items are accessible in precisely the same way. (I suppose if I were more intelligent I wouldn't be surprised.)You can use gmtimeNote: A time stamp can be passed to gmtime, default is current time as 

returned by time()See struct_timeThis is an older question, but I came up with a solution I thought others might like.timetuple() can be zipped with another array, which creates labeled tuples. Cast that to a dictionary and the resultant product can be consumed with get_current_datetime_as_dict()['year'].This has a little more overhead than some of the other solutions on here, but I've found it's so nice to be able to access named values for clartiy's sake in the code.you can use datetime module to get current Date and Time in Python 2.7Output : 

List attributes of an object

MadSc13ntist

[List attributes of an object](https://stackoverflow.com/questions/2675028/list-attributes-of-an-object)

Is there a way to grab a list of attributes that exist on instances of a class?The desired result is that "multi, str" will be output.  I want this to see the current attributes from various parts of a script.

2010-04-20 12:28:35Z

Is there a way to grab a list of attributes that exist on instances of a class?The desired result is that "multi, str" will be output.  I want this to see the current attributes from various parts of a script.You may also find pprint helpful.Then you can test what type is with type() or if is a method with callable().vars(obj) returns the attributes of an object.This of course will print any methods or attributes in the class definition. You can exclude "private" methods by changing i.startwith('__') to i.startwith('_')All previous answers are correct, you have three options for what you are asking 1.dir()2.vars()3.__dict__The inspect module provides easy ways to inspect an object: Using getmembers() you can see all attributes of your class, along with their value. To exclude private or protected attributes use .startswith('_'). To exclude methods or functions use inspect.ismethod() or inspect.isfunction(). Note that ismethod() is used on the second element of i since the first is simply a string (its name). Offtopic: Use CamelCase for class names.You can use dir(your_object) to get the attributes and getattr(your_object, your_object_attr) to get the valuesusage :This is particularly useful if your object have no __dict__. If that is not the case you can try var(your_object) alsoIt's often mentioned that to list a complete list of attributes you should use dir(). Note however that contrary to popular belief dir() does not bring out all attributes. For example you might notice that __name__ might be missing from a class's dir() listing even though you can access it from the class itself. From the doc on dir() (Python 2, Python 3):A function like the following tends to be more complete, although there's no guarantee of completeness since the list returned by dir() can be affected by many factors including implementing the __dir__() method, or customizing __getattr__() or __getattribute__() on the class or one of its parents. See provided links for more details.What do you want this for? It may be hard to get you the best answer without knowing your exact intent.There is more than one way to do it:When run, this code produces:Please see the python shell script which has been executed in sequence, here you will get the attributes of a class in string format separated by comma.I am using python 3.4In addition to these answers, I'll include a function (python 3) for spewing out virtually the entire structure of any value. It uses dir to establish the full list of property names, then uses getattr with each name. It displays the type of every member of the value, and when possible also displays the entire member:Now any of the following should give insight: As written before using obj.__dict__ can handle common cases but some classes do not have the __dict__ attribute and use __slots__ (mostly for memory efficiency).example for a more resilient way of doing this:this code's output:Note1:

Python is a dynamic language and it is always better knowing the classes you trying to get the attributes from as even this code can miss some cases.Note2:

this code outputs only instance variables meaning class variables are not provided. for example:code outputs:This code does not print the url class attribute and might omit wanted class attributes.

Sometimes we might think an attribute is an instance member but it is not and won't be shown using this example.This is the best I have:Please see the following Python shell scripting execution in sequence, it will give the solution from creation of class to extracting the field names of instances.__attr__ gives the list of attributes of an instance.

namedtuple and default values for optional keyword arguments

sasuke

[namedtuple and default values for optional keyword arguments](https://stackoverflow.com/questions/11351032/namedtuple-and-default-values-for-optional-keyword-arguments)

I'm trying to convert a longish hollow "data" class into a named tuple. My class currently looks like this:After conversion to namedtuple it looks like:But there is a problem here. My original class allowed me to pass in just a value and took care of the default by using default values for the named/keyword arguments. Something like:But this doesn't work in the case of my refactored named tuple since it expects me to pass all the fields. I can of course replace the occurrences of Node(val) to Node(val, None, None) but it isn't to my liking.So does there exist a good trick which can make my re-write successful without adding a lot of code complexity (metaprogramming) or should I just swallow the pill and go ahead with the "search and replace"? :)

2012-07-05 19:16:08Z

I'm trying to convert a longish hollow "data" class into a named tuple. My class currently looks like this:After conversion to namedtuple it looks like:But there is a problem here. My original class allowed me to pass in just a value and took care of the default by using default values for the named/keyword arguments. Something like:But this doesn't work in the case of my refactored named tuple since it expects me to pass all the fields. I can of course replace the occurrences of Node(val) to Node(val, None, None) but it isn't to my liking.So does there exist a good trick which can make my re-write successful without adding a lot of code complexity (metaprogramming) or should I just swallow the pill and go ahead with the "search and replace"? :)Use the defaults parameter.Or better yet, use the new dataclasses library, which is much nicer than namedtuple.Set Node.__new__.__defaults__ to the default values.Set Node.__new__.func_defaults to the default values.In all versions of Python, if you set fewer default values than exist in the namedtuple, the defaults are applied to the rightmost parameters. This allows you to keep some arguments as required arguments.Here's a wrapper for you, which even lets you (optionally) set the default values to something other than None. This does not support required arguments.Example:I subclassed namedtuple and overrode the __new__ method:This preserves an intuitive type hierarchy, which the creation of a factory function disguised as a class does not.Wrap it in a function.With typing.NamedTuple in Python 3.6.1+ you can provide both a default value and a type annotation to a NamedTuple field. Use typing.Any if you only need the former:Usage:Also, in case you need both default values and optional mutability, Python 3.7 is going to have data classes (PEP 557) that can in some (many?) cases replace namedtuples.

This kind of type hints is called "forward reference" ([1], [2]), and with PEP 563 Python 3.7+ is going to have a __future__ import (to be enabled by default in 4.0) that will allow to use forward references without quotes, postponing their evaluation.* AFAICT only local variable annotations are not evaluated at runtime. (source: PEP 526)This is an example straight from the docs:So, the OP's example would be:However, I like some of the other answers given here better.  I just wanted to add this for completeness. I'm not sure if there's an easy way with just the built-in namedtuple. There's a nice module called recordtype that has this functionality:Here is a more compact version inspired by justinfay's answer:In python3.7+ there's a brand new defaults= keyword argument.Example usage:Short, simple, and doesn't lead people to use isinstance improperly:A slightly extended example to initialize all missing arguments with None:Python 3.7: introduction of defaults param in namedtuple definition.Example as shown in the documentation:Read more here.You can also use this:This basically gives you the possibility to construct any named tuple with a default value and override just the parameters you need, for example:Combining approaches of @Denis and @Mark:That should support creating the tuple with positional arguments and also with mixed cases.

Test cases:but also support TypeError:I find this version easier to read:This is not as efficient as it requires creation of the object twice but you could change that by defining the default duple inside the module and just having the function do the replace line. Since you are using namedtuple as a data class, you should be aware that python 3.7 will introduce a @dataclass decorator for this very purpose -- and of course it has default values. An example from the docs:Much cleaner, readable and usable than hacking namedtuple. It is not hard to predict that usage of namedtuples will drop with the adoption of 3.7.Inspired by this answer to a different question, here is my proposed solution based on a metaclass and using super (to handle future subcalssing correctly).  It is quite similar to justinfay's answer.Then:The answer by jterrace to use recordtype is great, but the author of the library recommends to use his namedlist project, which provides both mutable (namedlist) and immutable (namedtuple) implementations.Here's a short, simple generic answer with a nice syntax for a named tuple with default arguments:Usage:Minified:Using the NamedTuple class from my Advanced Enum (aenum) library, and using the class syntax, this is quite simple:The one potential drawback is the requirement for a __doc__ string for any attribute with a default value (it's optional for simple attributes).  In use it looks like:The advantages this has over justinfay's answer:is simplicity, as well as  being metaclass based instead of exec based.Another solution:Usage:Here's a less flexible, but more concise version of Mark Lodato's wrapper: It takes the fields and defaults as a dictionary.Example:

Timeout on a function call

Teifion

[Timeout on a function call](https://stackoverflow.com/questions/492519/timeout-on-a-function-call)

I'm calling a function in Python which I know may stall and force me to restart the script. How do I call the function or what do I wrap it in so that if it takes longer than 5 seconds the script cancels it and does something else?

2009-01-29 17:08:29Z

I'm calling a function in Python which I know may stall and force me to restart the script. How do I call the function or what do I wrap it in so that if it takes longer than 5 seconds the script cancels it and does something else?You may use the signal package if you are running on UNIX:10 seconds after the call alarm.alarm(10), the handler is called. This raises an exception that you can intercept from the regular Python code.This module doesn't play well with threads (but then, who does?)Note that since we raise an exception when timeout happens, it may end up caught and ignored inside the function, for example of one such function:You can use multiprocessing.Process to do exactly that.CodeI posted a gist that solves this question/problem with a decorator and a threading.Timer. Here it is with a breakdown. It was tested with Python 2 and 3. It should also work under Unix/Linux and Windows.First the imports. These attempt to keep the code consistent regardless of the Python version:Use version independent code:Now we have imported our functionality from the standard library. Next we need a function to terminate the main() from the child thread:And here is the decorator itself:And here's the usage that directly answers your question about exiting after 5 seconds!:Demo:The second function call will not finish, instead the process should exit with a traceback!Note that sleep will not always be interrupted by a keyboard interrupt, on Python 2 on Windows, e.g.:nor is it likely to interrupt code running in extensions unless it explicitly checks for PyErr_CheckSignals(), see  Cython, Python and KeyboardInterrupt ignoredI would avoid sleeping a thread more than a second, in any case - that's an eon in processor time.To catch it and do something else, you can catch the KeyboardInterrupt.I have a different proposal which is a pure function (with the same API as the threading suggestion) and seems to work fine (based on suggestions on this thread)I ran across this thread when searching for a timeout call on unit tests.  I didn't find anything simple in the answers or 3rd party packages so I wrote the decorator below you can drop right into code:Then it's as simple as this to timeout a test or any function you like:The stopit package, found on pypi, seems to handle timeouts well.I like the @stopit.threading_timeoutable decorator, which adds a timeout parameter to the decorated function, which does what you expect, it stops the function.Check it out on pypi: https://pypi.python.org/pypi/stopitThere are a lot of suggestions, but none using concurrent.futures, which I think is the most legible way to handle this.Super simple to read and maintain.We make a pool, submit a single process and then wait up to 5 seconds before raising a TimeoutError that you could catch and handle however you needed.Native to python 3.2+ and backported to 2.7 (pip install futures).Switching between threads and processes is as simple as replacing ProcessPoolExecutor with ThreadPoolExecutor.If you want to terminate the Process on timeout I would suggest looking into Pebble.Great, easy to use and reliable PyPi project timeout-decorator (https://pypi.org/project/timeout-decorator/)installation:Usage:I am the author of wrapt_timeout_decoratorMost of the solutions presented here work wunderfully under Linux on the first glance - because we have fork() and signals() - but on windows the things look a bit different.

And when it comes to subthreads on Linux, You cant use Signals anymore.In order to spawn a process under Windows, it needs to be pickable - and many decorated functions or Class methods are not.So You need to use a better pickler like dill and multiprocess (not pickle and multiprocessing) - thats why You cant use ProcessPoolExecutor (or only with limited functionality).For the timeout itself - You need to define what timeout means - because on Windows it will take considerable (and not determinable) time to spawn the process. This can be tricky on short timeouts. Lets assume, spawning the process takes about 0.5 seconds (easily !!!). If You give a timeout of 0.2 seconds what should happen ? 

Should the function time out after 0.5 + 0.2 seconds (so let the method run for 0.2 seconds)? 

Or should the called process time out after 0.2 seconds (in that case, the decorated function will ALWAYS timeout, because in that time it is not even spawned) ? Also nested decorators can be nasty and You cant use Signals in a subthread. If You want to create a truly universal, cross-platform decorator, all this needs to be taken into consideration (and tested).Other issues are passing exceptions back to the caller, as well as logging issues (if used in the decorated function - logging to files in another process is NOT supported) I tried to cover all edge cases, You might look into the package wrapt_timeout_decorator, or at least test Your own solutions inspired by the unittests used there.@Alexis Eggermont - unfortunately I dont have enough points to comment - maybe someone else can notify You - I think I solved Your import issue.timeout-decorator don't work on windows system as , windows didn't support signal well.If you use timeout-decorator in windows system you will get the following Some suggested to use use_signals=False but didn't worked for me.Author @bitranox created the following package:Code Sample: Gives the following exception:We can use signals for the same. I think the below example will be useful for you. It is very simple compared to threads. I had a need for nestable timed interrupts (which SIGALARM can't do) that won't get blocked by time.sleep (which the thread-based approach can't do). I ended up copying and lightly modifying code from here: http://code.activestate.com/recipes/577600-queue-for-managing-multiple-sigalrm-alarms-concurr/The code itself:and a usage example:Here is a slight improvement to the given thread-based solution.The code below supports exceptions:Invoking it with a 5 second timeout:

How to merge lists into a list of tuples?

rubayeet

[How to merge lists into a list of tuples?](https://stackoverflow.com/questions/2407398/how-to-merge-lists-into-a-list-of-tuples)

What is the Pythonic approach to achieve the following?Each member of list_c is a tuple, whose first member is from list_a and the second is from list_b.

2010-03-09 07:51:46Z

What is the Pythonic approach to achieve the following?Each member of list_c is a tuple, whose first member is from list_a and the second is from list_b.In Python 2:In Python 3:In python 3.0 zip returns a zip object. You can get a list out of it by calling list(zip(a, b)).You can use map lambdaThis will also work if there lengths of original lists do not matchYoure looking for the builtin function zip.I am not sure if this a pythonic way or not but this seems simple if both lists have the same number of elements : I know this is an old question and was already answered, but for some reason, I still wanna post this alternative solution. I know it's easy to just find out which built-in function does the "magic" you need, but it doesn't hurt to know you can do it by yourself.    The output which you showed in problem statement is not the tuple but listcheck for considering you want the result as tuple out of list_a and list_b, doOne alternative without using zip:In case one wants to get not only tuples 1st with 1st, 2nd with 2nd... but all possible combinations of the 2 lists, that would be done with 

Assign output of os.system to a variable and prevent it from being displayed on the screen

John

[Assign output of os.system to a variable and prevent it from being displayed on the screen](https://stackoverflow.com/questions/3503879/assign-output-of-os-system-to-a-variable-and-prevent-it-from-being-displayed-on)

I want to assign the output of a command I run using os.system to a variable and prevent it from being output to the screen. But, in the below code ,the output is sent to the screen and the value printed for var is 0, which I guess signifies whether the command ran successfully or not. Is there any way to assign the command output to the variable and also stop it from being displayed on the screen?

2010-08-17 15:03:57Z

I want to assign the output of a command I run using os.system to a variable and prevent it from being output to the screen. But, in the below code ,the output is sent to the screen and the value printed for var is 0, which I guess signifies whether the command ran successfully or not. Is there any way to assign the command output to the variable and also stop it from being displayed on the screen?From "Equivalent of Bash Backticks in Python", which I asked a long time ago, what you may want to use is popen:From the docs for Python 3.6, Here's the corresponding code for subprocess:You might also want to look at the subprocess module, which was built to replace the whole family of Python popen-type calls.The advantage it has is that there is a ton of flexibility with how you invoke commands, where the standard in/out/error streams are connected, etc.The commands module is a reasonably high-level way to do this:status is 0, output is the contents of /etc/services.I know this has already been answered, but I wanted to share a potentially better looking way to call Popen via the use of from x import x and functions:For python 3.5+ it is recommended that you use the run function from the subprocess module. This returns a CompletedProcess object, from which you can easily obtain the output as well as return code. Since you are only interested in the output, you can write a utility wrapper like this.i do it with os.system temp file:Python 2.6 and 3 specifically say to avoid using PIPE for stdout and stderr.The correct way is 

Use of「global」keyword in Python

nik

[Use of「global」keyword in Python](https://stackoverflow.com/questions/4693120/use-of-global-keyword-in-python)

What I understand from reading the documentation is that Python has a separate namespace for functions, and if I want to use a global variable in that function, I need to use global.I'm using Python 2.7 and I tried this little testIt seems things are working fine even without global. I was able to access global variable without any problem.Am I missing anything? Also, following is from Python documentation:While formal parameters and class definition make sense to me, I'm not able to understand the restriction on for loop control target and function definition.

2011-01-14 16:11:03Z

What I understand from reading the documentation is that Python has a separate namespace for functions, and if I want to use a global variable in that function, I need to use global.I'm using Python 2.7 and I tried this little testIt seems things are working fine even without global. I was able to access global variable without any problem.Am I missing anything? Also, following is from Python documentation:While formal parameters and class definition make sense to me, I'm not able to understand the restriction on for loop control target and function definition.The keyword global is only useful to change or create global variables in a local context, although creating global variables is seldom considered a good solution.The above will give you:While if you use the global statement, the variable will become available "outside" the scope of the function, effectively becoming a global variable.So the above code will give you:In addition, due to the nature of python, you could also use global to declare functions, classes or other objects in a local context. Although I would advise against it since it causes nightmares if something goes wrong or needs debugging.While you can access global variables without the global keyword, if you want to modify them you have to use the global keyword. For example:In your case, you're just accessing the list sub.This is the difference between accessing the name and binding it within a scope.If you're just looking up a variable to read its value, you've got access to global as well as local scope.However if you assign to a variable who's name isn't in local scope, you are binding that name into this scope (and if that name also exists as a global, you'll hide that). If you want to be able to assign to the global name, you need to tell the parser to use the global name rather than bind a new local name - which is what the 'global' keyword does.Binding anywhere within a block causes the name everywhere in that block to become bound, which can cause some rather odd looking consequences (e.g. UnboundLocalError suddenly appearing in previously working code).The other answers answer your question. Another important thing to know about names in Python is that they are either local or global on a per-scope basis.Consider this, for example:You can probably guess that the value = 0 statement will be assigning to a local variable and not affect the value of the same variable declared outside the doit() function. You may be more surprised to discover that the code above won't run. The statement print value inside the function produces an UnboundLocalError.The reason is that Python has noticed that, elsewhere in the function, you assign the name value, and also value is nowhere declared global. That makes it a local variable. But when you try to print it, the local name hasn't been defined yet. Python in this case does not fall back to looking for the name as a global variable, as some other languages do. Essentially, you cannot access a global variable if you have defined a local variable of the same name anywhere in the function.Accessing a name and assigning a name are different. In your case, you are just accessing a name.If you assign to a variable within a function, that variable is assumed to be local unless you declare it global. In the absence of that, it is assumed to be global.Example:Any variable declared outside of a function is assumed to be global, it's only when declaring them from inside of functions (except constructors) that you must specify that the variable be global.This is explained well in the Python FAQhttps://docs.python.org/3/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-pythonIt means that you should not do the following:Global makes the variable "Global"This makes 'x' act like a normal variable outside the function. If you took the global out then it would give an error since it cannot print a variable inside a function.

How do I fix 'ImportError: cannot import name IncompleteRead'?

Martin Thoma

[How do I fix 'ImportError: cannot import name IncompleteRead'?](https://stackoverflow.com/questions/27341064/how-do-i-fix-importerror-cannot-import-name-incompleteread)

When I try to install anything with pip or pip3, I get:I have a Ubuntu 14.10 system.How can I fix this problem?

2014-12-07 08:44:01Z

When I try to install anything with pip or pip3, I get:I have a Ubuntu 14.10 system.How can I fix this problem?While this previous answer might be the reason, this snipped worked for me as a solution (in Ubuntu 14.04):First remove the package from the package manager:And then install the latest version by side:(thanks to @Aufziehvogel, @JunchaoGu)This problem is caused by a mismatch between your pip installation and your requests installation.As of requests version 2.4.0 requests.compat.IncompleteRead has been removed. Older versions of pip, e.g. from July 2014, still relied on IncompleteRead. In the current version of pip, the import of IncompleteRead has been removed.So the one to blame is either:You can solve this issue, by either updating pip via Ubuntu (if there is a newer version) or by installing pip aside from Ubuntu.For fixing pip3 (worked on Ubuntu 14.10):Or you can remove all requests.For example:On Ubuntu 14.04 I resolved this by using the pip installation bootstrap script, as described in the documentationThat's an OK solution for a development environment.The problem is the Python module requests. It can be fixed byIf you have this problem with Python 3, you have to write python3 instead of python.This should work for you. Follow these simple steps. First, let's remove the pip which is already installed so it won't cause any error.Open Terminal.Type: sudo apt-get remove python-pipIt removes pip that is already installed.Method-1Step: 1 sudo easy_install -U pipIt will install pip latest version.And will return its address: Installed /usr/local/lib/python2.7/dist-packages/pip-6.1.1-py2.7.eggorMethod-2Step: 1 go to this link.Step: 2 Right click >> Save as.. with name get-pip.py .Step: 3 use: cd to go to the same directory as your get-pip.py fileStep: 4 use: sudo python get-pip.pyIt will install pip latest version.orMethod-3Step: 1 use: sudo apt-get install python-pipIt will install pip latest version.Simply running easy_install -U pip resolved my problem.Check wether you have an older version of requests sitting in your ~/.local/lib/python2.7/site-packages/ and remove it if it is the case (change path to reflect your python version). This solved the issue for me. My version of pip on ubuntu suggests:In Windows, this worked from an administrative prompt:I tried with every answer below, but couldn't make it.Did this and workedAfter that I just installed virtualenv with pipI builded the virtualenv that I was working on and

the package was installed easily.

Get into the virtualenv by using source /bin/activate 

and try to install your package, for example:It worked for me, although I was using python2.7 not python3Check if have a python interpreter alive in any of the terminal windows. If so kill it and try sudo pip which worked for me.You can download recent packages manually from these pages:Then, install it by running dpkg:For CentOS I used this and it worked please use the following commands:(confirm that all those libraries have been removed)

Search and replace a line in a file in Python

pkit

[Search and replace a line in a file in Python](https://stackoverflow.com/questions/39086/search-and-replace-a-line-in-a-file-in-python)

I want to loop over the contents of a text file and do a search and replace on some lines and write the result back to the file. I could first load the whole file in memory and then write it back, but that probably is not the best way to do it.What is the best way to do this, within the following code?

2008-09-02 09:19:04Z

I want to loop over the contents of a text file and do a search and replace on some lines and write the result back to the file. I could first load the whole file in memory and then write it back, but that probably is not the best way to do it.What is the best way to do this, within the following code?I guess something like this should do it. It basically writes the content to a new file and replaces the old file with the new file:The shortest way would probably be to use the fileinput module. For example, the following adds line numbers to a file, in-place:What happens here is:fileinput has more bells and whistles. For example, it can be used to automatically operate on all files in sys.args[1:], without your having to iterate over them explicitly. Starting with Python 3.2 it also provides a convenient context manager for use in a with statement.While fileinput is great for throwaway scripts, I would be wary of using it in real code because admittedly it's not very readable or familiar. In real (production) code it's worthwhile to spend just a few more lines of code to make the process explicit and thus make the code readable.There are two options:Here's another example that was tested, and will match search & replace patterns:Example use:This should work: (inplace editing)Based on the answer by Thomas Watnedal. 

However, this does not answer the line-to-line part of the original question exactly. The function can still replace on a line-to-line basis This implementation replaces the file contents without using temporary files, as a consequence file permissions remain unchanged.Also re.sub instead of replace, allows regex replacement instead of plain text replacement only.Reading the file as a single string instead of line by line allows for multiline match and replacement.As lassevk suggests, write out the new file as you go, here is some example code:If you're wanting a generic function that replaces any text with some other text, this is likely the best way to go, particularly if you're a fan of regex's:A more pythonic way would be to use context managers like the code below:You can find the full snippet here.Create a new file, copy lines from the old to the new, and do the replacing before you write the lines to the new file.Expanding on @Kiran's answer, which I agree is more succinct and Pythonic, this adds codecs to support the reading and writing of UTF-8:Using hamishmcn's answer as a template I was able to search for a line in a file that match my regex and replacing it with empty string.fileinput is quite straightforward as mentioned on previous answers:Explanation:Can be used as follows:if you remove the indent at the like below, it will search and replace in multiple line.

See below for example.

Why is my xlabel cut off in my matplotlib plot?

Andrew

[Why is my xlabel cut off in my matplotlib plot?](https://stackoverflow.com/questions/6774086/why-is-my-xlabel-cut-off-in-my-matplotlib-plot)

I am plotting a dataset using matplotlib where I have an xlabel that is quite "tall" (it's a formula rendered in TeX that contains a fraction and is therefore has the height equivalent of a couple of lines of text).In any case, the bottom of the formula is always cut off when I draw the figures.  Changing figure size doesn't seem to help this, and I haven't been able to figure out how to shift the x-axis "up" to make room for the xlabel.  Something like that would be a reasonable temporary solution, but what would be nice would be to have a way to make matplotlib recognize automatically that the label is cut off and resize accordingly.Here's an example of what I mean:while you can see the entire ylabel, the xlabel is cut off at the bottom. In the case this is a machine-specific problem, I am running this on OSX 10.6.8 with matplotlib 1.0.0

2011-07-21 09:37:24Z

I am plotting a dataset using matplotlib where I have an xlabel that is quite "tall" (it's a formula rendered in TeX that contains a fraction and is therefore has the height equivalent of a couple of lines of text).In any case, the bottom of the formula is always cut off when I draw the figures.  Changing figure size doesn't seem to help this, and I haven't been able to figure out how to shift the x-axis "up" to make room for the xlabel.  Something like that would be a reasonable temporary solution, but what would be nice would be to have a way to make matplotlib recognize automatically that the label is cut off and resize accordingly.Here's an example of what I mean:while you can see the entire ylabel, the xlabel is cut off at the bottom. In the case this is a machine-specific problem, I am running this on OSX 10.6.8 with matplotlib 1.0.0Use:to make room for the label.Edit:Since i gave the answer, matplotlib has added the tight_layout() function.

So i suggest to use it:should make room for the xlabel.An easy option is to configure matplotlib to automatically adjust the plot size. It works perfectly for me and I'm not sure why it's not activated by default.Method 1Set this in your matplotlibrc fileSee here for more information on customizing the matplotlibrc file: http://matplotlib.org/users/customizing.htmlMethod 2Update the rcParams during runtime like thisThe advantage of using this approach is that your code will produce the same graphs on differently-configured machines.In case you want to store it to a file, you solve it using bbox_inches="tight" argument:plt.autoscale() worked for me.You can also set custom padding as defaults in your $HOME/.matplotlib/matplotlib_rc as follows.  In the example below I have modified both the bottom and left out-of-the-box padding:Putting plot.tight_layout() after all changes on the graph, just before show() or savefig() will solve the problem.

How to add an extra column to a NumPy array

Peter Smit

[How to add an extra column to a NumPy array](https://stackoverflow.com/questions/8486294/how-to-add-an-extra-column-to-a-numpy-array)

Let’s say I have a NumPy array, a:And I would like to add a column of zeros to get an array, b:How can I do this easily in NumPy?

2011-12-13 08:36:10Z

Let’s say I have a NumPy array, a:And I would like to add a column of zeros to get an array, b:How can I do this easily in NumPy?I think a more straightforward solution and faster to boot is to do the following:And timings:np.r_[ ... ] and np.c_[ ... ]

are useful alternatives to vstack and hstack,

with square brackets [] instead of round ().

A couple of examples:(The reason for square brackets [] instead of round ()

is that Python expands e.g. 1:4 in square --

the wonders of overloading.)Use numpy.append:One way, using hstack, is:I find the following most elegant:An advantage of insert is that it also allows you to insert columns (or rows) at other places inside the array. Also instead of inserting a single value you can easily insert a whole vector, for instance duplicate the last column:Which leads to:For the timing, insert might be slower than JoshAdel's solution:I was also interested in this question and compared the speed ofwhich all do the same thing for any input vector a. Timings for growing a:Note that all non-contiguous variants (in particular  stack/vstack) are eventually faster than all contiguous variants. column_stack (for its clarity and speed) appears to be a good option if you require contiguity.Code to reproduce the plot:I think:is more elegant.np.concatenate also worksAssuming M is a (100,3) ndarray and y is a (100,) ndarray append can be used as follows:The trick is to use This converts y to a (100, 1) 2D array.now givesI like JoshAdel's answer because of the focus on performance. A minor performance improvement is to avoid the overhead of initializing with zeros, only to be overwritten. This has a measurable difference when N is large, empty is used instead of zeros, and the column of zeros is written as a separate step:np.insert also serves the purpose. It inserts values, here new_col, before a given index, here idx along one axis. In other words, the newly inserted values will occupy the idx column and move what were originally there at and after idx backward.A bit late to the party, but nobody posted this answer yet, so for the sake of completeness: you can do this with list comprehensions, on a plain Python array:Numpy's np.append method takes three parameters, the first two are 2D numpy arrays and the 3rd is an axis parameter instructing along which axis to append:Prints:In my case, I had to add a column of ones to a NumPy arrayAfter

    X.shape => (97, 2)For me, the next way looks pretty intuitive and simple.There is a function specifically for this. It is called numpy.padHere is what it says in the docstring:

Wrapping a C library in Python: C, Cython or ctypes?

balpha

[Wrapping a C library in Python: C, Cython or ctypes?](https://stackoverflow.com/questions/1942298/wrapping-a-c-library-in-python-c-cython-or-ctypes)

I want to call a C library from a Python application. I don't want to wrap the whole API, only the functions and datatypes that are relevant to my case. As I see it, I have three choices:I'm not sure whether 2) or 3) is the better choice. The advantage of 3) is that ctypes is part of the standard library, and the resulting code would be pure Python – although I'm not sure how big that advantage actually is.Are there more advantages / disadvantages with either choice? Which approach do you recommend?Edit: Thanks for all your answers, they provide a good resource for anyone looking to do something similar. The decision, of course, is still to be made for the single case—there's no one "This is the right thing" sort of answer. For my own case, I'll probably go with ctypes, but I'm also looking forward to trying out Cython in some other project.With there being no single true answer, accepting one is somewhat arbitrary; I chose FogleBird's answer as it provides some good insight into ctypes and it currently also is the highest-voted answer. However, I suggest to read all the answers to get a good overview.Thanks again.

2009-12-21 20:05:51Z

I want to call a C library from a Python application. I don't want to wrap the whole API, only the functions and datatypes that are relevant to my case. As I see it, I have three choices:I'm not sure whether 2) or 3) is the better choice. The advantage of 3) is that ctypes is part of the standard library, and the resulting code would be pure Python – although I'm not sure how big that advantage actually is.Are there more advantages / disadvantages with either choice? Which approach do you recommend?Edit: Thanks for all your answers, they provide a good resource for anyone looking to do something similar. The decision, of course, is still to be made for the single case—there's no one "This is the right thing" sort of answer. For my own case, I'll probably go with ctypes, but I'm also looking forward to trying out Cython in some other project.With there being no single true answer, accepting one is somewhat arbitrary; I chose FogleBird's answer as it provides some good insight into ctypes and it currently also is the highest-voted answer. However, I suggest to read all the answers to get a good overview.Thanks again.ctypes is your best bet for getting it done quickly, and it's a pleasure to work with as you're still writing Python!I recently wrapped an FTDI driver for communicating with a USB chip using ctypes and it was great.  I had it all done and working in less than one work day. (I only implemented the functions we needed, about 15 functions).We were previously using a third-party module, PyUSB, for the same purpose.  PyUSB is an actual C/Python extension module.  But PyUSB wasn't releasing the GIL when doing blocking reads/writes, which was causing problems for us.  So I wrote our own module using ctypes, which does release the GIL when calling the native functions.One thing to note is that ctypes won't know about #define constants and stuff in the library you're using, only the functions, so you'll have to redefine those constants in your own code.Here's an example of how the code ended up looking (lots snipped out, just trying to show you the gist of it):Someone did some benchmarks on the various options.I might be more hesitant if I had to wrap a C++ library with lots of classes/templates/etc.  But ctypes works well with structs and can even callback into Python.Warning: a Cython core developer's opinion ahead.I almost always recommend Cython over ctypes. The reason is that it has a much smoother upgrade path. If you use ctypes, many things will be simple at first, and it's certainly cool to write your FFI code in plain Python, without compilation, build dependencies and all that. However, at some point, you will almost certainly find that you have to call into your C library a lot, either in a loop or in a longer series of interdependent calls, and you would like to speed that up. That's the point where you'll notice that you can't do that with ctypes. Or, when you need callback functions and you find that your Python callback code becomes a bottleneck, you'd like to speed it up and/or move it down into C as well. Again, you cannot do that with ctypes. So you have to switch languages at that point and start rewriting parts of your code, potentially reverse engineering your Python/ctypes code into plain C, thus spoiling the whole benefit of writing your code in plain Python in the first place.With Cython, OTOH, you're completely free to make the wrapping and calling code as thin or thick as you want. You can start with simple calls into your C code from regular Python code, and Cython will translate them into native C calls, without any additional calling overhead, and with an extremely low conversion overhead for Python parameters. When you notice that you need even more performance at some point where you are making too many expensive calls into your C library, you can start annotating your surrounding Python code with static types and let Cython optimise it straight down into C for you. Or, you can start rewriting parts of your C code in Cython in order to avoid calls and to specialise and tighten your loops algorithmically. And if you need a fast callback, just write a function with the appropriate signature and pass it into the C callback registry directly. Again, no overhead, and it gives you plain C calling performance. And in the much less likely case that you really cannot get your code fast enough in Cython, you can still consider rewriting the truly critical parts of it in C (or C++ or Fortran) and call it from your Cython code naturally and natively. But then, this really becomes the last resort instead of the only option.So, ctypes is nice to do simple things and to quickly get something running. However, as soon as things start to grow, you'll most likely come to the point where you notice that you'd better used Cython right from the start.Cython is a pretty cool tool in itself, well worth learning, and is surprisingly close to the Python syntax. If you do any scientific computing with Numpy, then Cython is the way to go because it integrates with Numpy for fast matrix operations.Cython is a superset of Python language. You can throw any valid Python file at it, and it will spit out a valid C program. In this case, Cython will just map the Python calls to the underlying CPython API. This results in perhaps a 50% speedup because your code is no longer interpreted.To get some optimizations, you have to start telling Cython additional facts about your code, such as type declarations. If you tell it enough, it can boil the code down to pure C. That is, a for loop in Python becomes a for loop in C. Here you will see massive speed gains. You can also link to external C programs here.Using Cython code is also incredibly easy. I thought the manual makes it sound difficult. You literally just do:and then you can import mymodule in your Python code and forget entirely that it compiles down to C. In any case, because Cython is so easy to setup and start using, I suggest trying it to see if it suits your needs. It won't be a waste if it turns out not to be the tool you're looking for.For calling a C library from a Python application there is also cffi which is a new alternative for ctypes. It brings a fresh look for FFI: I'll throw another one out there: SWIGIt's easy to learn, does a lot of things right, and supports many more languages so the time spent learning it can be pretty useful.If you use SWIG, you are creating a new python extension module, but with SWIG doing most of the heavy lifting for you.Personally, I'd write an extension module in C. Don't be intimidated by Python C extensions -- they're not hard at all to write. The documentation is very clear and helpful. When I first wrote a C extension in Python, I think it took me about an hour to figure out how to write one -- not much time at all.ctypes is great when you've already got a compiled library blob to deal with (such as OS libraries). The calling overhead is severe, however, so if you'll be making a lot of calls into the library, and you're going to be writing the C code anyway (or at least compiling it), I'd say to go for cython. It's not much more work, and it'll be much faster and more pythonic to use the resulting pyd file.I personally tend to use cython for quick speedups of python code (loops and integer comparisons are two areas where cython particularly shines), and when there is some more involved code/wrapping of other libraries involved, I'll turn to Boost.Python. Boost.Python can be finicky to set up, but once you've got it working, it makes wrapping C/C++ code straightforward.cython is also great at wrapping numpy (which I learned from the SciPy 2009 proceedings), but I haven't used numpy, so I can't comment on that.If you have already a library with a defined  API, I think ctypes is the best option, as you only have to do a little initialization and then more or less call the library the way you're used to.I think Cython or creating an extension module in C (which is not very difficult) are more useful when you need new code, e.g. calling that library and do some complex, time-consuming tasks, and then passing the result to Python.Another approach, for simple programs, is directly do a different process (compiled externally), outputting the result to standard output and call it with subprocess module. Sometimes it's the easiest approach.For example, if you make a console C program that works more or less that wayYou could call it from PythonWith a little string formating, you can take the result in any way you want. You can also capture the standard error output, so it's quite flexible.There is one issue which made me use ctypes and not cython and which is not mentioned in other answers. Using ctypes the result does not depend on compiler you are using at all. You may write a library using more or less any language which  may be compiled to native shared library. It does not matter much, which system, which language and which compiler. Cython, however, is limited by the infrastructure. E.g, if you want to use intel compiler on windows, it is much more tricky to make cython work: you should "explain" compiler to cython, recompile something with this exact compiler, etc. Which significantly limits portability.If you are targeting Windows and choose to wrap some proprietary C++ libraries, then you may soon discover that different versions of msvcrt***.dll (Visual C++ Runtime) are slightly incompatible. This means that you may not be able to use Cython since resulting wrapper.pyd is linked against msvcr90.dll (Python 2.7) or msvcr100.dll (Python 3.x). If the library that you are wrapping is linked against different version of runtime, then you're out of luck.Then to make things work you'll need to create C wrappers for C++ libraries, link that wrapper dll against the same version of msvcrt***.dll as your C++ library. And then use ctypes to load your hand-rolled wrapper dll dynamically at the runtime.So there are lots of small details, which are described in great detail in following article:"Beautiful Native Libraries (in Python)": http://lucumr.pocoo.org/2013/8/18/beautiful-native-libraries/There's also one possibility to use GObject Introspection for libraries that are using GLib.I know this is an old question but this thing comes up on google when you search stuff like ctypes vs cython, and most of the answers here are written by those who are proficient already in cython or c which might not reflect the actual time you needed to invest to learn those to implement your solution. I am a complete beginner in both. I have never touched cython before, and have very little experience on c/c++.For the last two days, I was looking for a way to delegate a performance heavy part of my code to something more low level than python. I implemented my code both in ctypes and Cython, which consisted basically of two simple functions.I had a huge string list that needed to processed. Notice list and string. 

Both types do not correspond perfectly to types in c, because python strings are by default unicode and c strings are not. Lists in python are simply NOT arrays of c.Here is my verdict. Use cython. It integrates more fluently to python, and easier to work with in general. When something goes wrong ctypes just throws you segfault, at least cython will give you compile warnings with a stack trace whenever it is possible, and you can return a valid python object easily with cython.Here is a detailed account on how much time I needed to invest in both them to implement the same function. I did very little C/C++ programming by the way:At this point, I decided to search for an alternative and decided to look into cython:For the record, I of course, did not measure the exact timings of my investment. It may very well be the case that my perception of time was a little to attentive due to mental effort required while I was dealing with ctypes. But it should convey the feel of dealing with cython and ctypes  

NameError: global name 'xrange' is not defined in Python 3

Pip

[NameError: global name 'xrange' is not defined in Python 3](https://stackoverflow.com/questions/17192158/nameerror-global-name-xrange-is-not-defined-in-python-3)

I am getting an error when running a python program:The game is from here.What causes this error?

2013-06-19 13:13:04Z

I am getting an error when running a python program:The game is from here.What causes this error?You are trying to run a Python 2 codebase with Python 3. xrange() was renamed to range() in Python 3.Run the game with Python 2 instead. Don't try to port it unless you know what you are doing, most likely there will be more problems beyond xrange() vs. range().For the record, what you are seeing is not a syntax error but a runtime exception instead.If you do know what your are doing and are actively making a Python 2 codebase compatible with Python 3, you can bridge the code by adding the global name to your module as an alias for range. (Take into account that you may have to update any existing range() use in the Python 2 codebase with list(range(...)) to ensure you still get a list object in Python 3):or replace all uses of xrange(...) with range(...) in the codebase and then use a different shim to make the Python 3 syntax compatible with Python 2:The latter is preferable for codebases that want to aim to be Python 3 compatible only in the long run, it is easier to then just use Python 3 syntax whenever possible.add xrange=range in your code :) It works to me.I solved the issue by adding this import

More infoin python 2.x, xrange is used to return a generator while range is used to return a list. In python 3.x , xrange has been removed and range returns a generator just like xrange in python 2.x. Therefore, in python 3.x you need to use range rather than xrange.Replace Python 2 xrange to Python 3 rangeRest all same. I agree with the last answer.But there is another way to solve this problem.You can download the package named future,such as pip install future.And in your .py file input this "from past.builtins import xrange".This method is for the situation that there are many xranges in your file.

How are Python's Built In Dictionaries Implemented?

ricree

[How are Python's Built In Dictionaries Implemented?](https://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented)

Does anyone know how the built in dictionary type for python is implemented?  My understanding is that it is some sort of hash table, but I haven't been able to find any sort of definitive answer.

2008-11-29 07:35:31Z

Does anyone know how the built in dictionary type for python is implemented?  My understanding is that it is some sort of hash table, but I haven't been able to find any sort of definitive answer.Here is everything about Python dicts that I was able to put together (probably more than anyone would like to know; but the answer is comprehensive). NOTE: I did the research on Python Dict implementation in response to my own question about how multiple entries in a dict can have same hash values. I posted a slightly edited version of the response here because all the research is very relevant for this question as well.Here's the short course:The ordered aspect is unofficial as of Python 3.6 (to give other implementations a chance to keep up), but official in Python 3.7.For a long time, it worked exactly like this. Python would preallocate 8 empty rows and use the hash to determine where to stick the key-value pair. For example, if the hash for the key ended in 001, it would stick it in the 1 (i.e. 2nd) index (like the example below.) Each row takes up 24 bytes on a 64 bit architecture, 12 on a 32 bit. (Note that the column headers are just labels for our purposes here - they don't actually exist in memory.)If the hash ended the same as a preexisting key's hash, this is a collision, and then it would stick the key-value pair in a different location.After 5 key-values are stored, when adding another key-value pair, the probability of hash collisions is too large, so the dictionary is doubled in size. In a 64 bit process, before the resize, we have 72 bytes empty, and after, we are wasting 240 bytes due to the 10 empty rows.This takes a lot of space, but the lookup time is fairly constant. The key comparison algorithm is to compute the hash, go to the expected location, compare the key's id - if they're the same object, they're equal. If not then compare the hash values, if they are not the same, they're not equal. Else, then we finally compare keys for equality, and if they are equal, return the value. The final comparison for equality can be quite slow, but the earlier checks usually shortcut the final comparison, making the lookups very quick.Collisions slow things down, and an attacker could theoretically use hash collisions to perform a denial of service attack, so we randomized the initialization of the hash function such that it computes different hashes for each new Python process.The wasted space described above has led us to modify the implementation of dictionaries, with an exciting new feature that dictionaries are now ordered by insertion.We start, instead, by preallocating an array for the index of the insertion.Since our first key-value pair goes in the second slot, we index like this:And our table just gets populated by insertion order:So when we do a lookup for a key, we use the hash to check the position we expect (in this case, we go straight to index 1 of the array), then go to that index in the hash-table (e.g. index 0), check that the keys are equal (using the same algorithm described earlier), and if so, return the value.We retain constant lookup time, with minor speed losses in some cases and gains in others, with the upsides that we save quite a lot of space over the pre-existing implementation and we retain insertion order. The only space wasted are the null bytes in the index array.Raymond Hettinger introduced this on python-dev in December of 2012. It finally got into CPython in Python 3.6. Ordering by insertion was considered an implementation detail for 3.6 to allow other implementations of Python a chance to catch up.Another optimization to save space is an implementation that shares keys. Thus, instead of having redundant dictionaries that take up all of that space, we have dictionaries that reuse the shared keys and keys' hashes. You can think of it like this:For a 64 bit machine, this could save up to 16 bytes per key per extra dictionary.These shared-key dicts are intended to be used for custom objects' __dict__. To get this behavior, I believe you need to finish populating your __dict__ before you instantiate your next object (see PEP 412). This means you should assign all your attributes in the __init__ or __new__, else you might not get your space savings.However, if you know all of your attributes at the time your __init__ is executed, you could also provide __slots__ for your object, and guarantee that __dict__ is not created at all (if not available in parents), or even allow __dict__ but guarantee that your foreseen attributes are stored in slots anyways. For more on __slots__, see my answer here.Python Dictionaries use Open addressing (reference inside Beautiful code)NB! Open addressing, a.k.a closed hashing should, as noted in Wikipedia, not be confused with its opposite open hashing!Open addressing means that the dict uses array slots, and when an object's primary position is taken in the dict, the object's spot is sought at a different index in the same array, using a "perturbation" scheme, where the object's hash value plays part.

pandas create new column based on values from other columns / apply a function of multiple columns, row-wise

Dave

[pandas create new column based on values from other columns / apply a function of multiple columns, row-wise](https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o)

I want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my dataframe.I've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a "1" in another ethnicity column they still are counted as Hispanic not two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(except for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. Its almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.  From the dataframe below I need to calculate a new column based on the following spec in SQL:=========================  CRITERIA  ===============================Comment: If the ERI Flag for Hispanic is True (1), the employee is classified as「Hispanic」Comment: If more than 1 non-Hispanic ERI Flag is true, return「Two or More」======================  DATAFRAME ===========================

2014-11-12 12:08:12Z

I want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my dataframe.I've tried different methods from other questions but still can't seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can't be counted as anything else.  Even if they have a "1" in another ethnicity column they still are counted as Hispanic not two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can't be counted as a unique ethnicity(except for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. Its almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.  From the dataframe below I need to calculate a new column based on the following spec in SQL:=========================  CRITERIA  ===============================Comment: If the ERI Flag for Hispanic is True (1), the employee is classified as「Hispanic」Comment: If more than 1 non-Hispanic ERI Flag is true, return「Two or More」======================  DATAFRAME ===========================OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:You may want to go over this, but it seems to do the trick - notice that the parameter going into the function is considered to be a Series object labelled "row".Next, use the apply function in pandas to apply the function - e.g.Note the axis=1 specifier, that means that the application is done at a row, rather than a column level. The results are here:If you're happy with those results, then run it again, saving the results into a new column in your original dataframe.The resultant dataframe looks like this (scroll to the right to see the new column):Since this is the first Google result for 'pandas new column from others', here's a simple example:If you get the SettingWithCopyWarning you can do it this way also:Source: https://stackoverflow.com/a/12555510/243392And if your column name includes spaces you can use syntax like this:And here's the documentation for apply, and assign.The answers above are perfectly valid, but a vectorized solution exists, in the form of numpy.select.  This allows you to define conditions, then define outputs for those conditions, much more efficiently than using apply:First, define conditions:Now, define the corresponding outputs:Finally, using numpy.select:Why should numpy.select be used over apply? Here are some performance checks:Using numpy.select gives us vastly improved performance, and the discrepancy will only increase as the data grows..apply() takes in a function as the first parameter; pass in the label_race function as so:You don't need to make a lambda function to pass in a function.  try this,O/P:use .loc instead of apply. it improves vectorization. .loc works in simple manner, mask rows based on the condition, apply values to the freeze rows. for more details visit,  .loc docs Performance metrics:Accepted Answer:My Proposed Answer:

Catching an exception while using a Python 'with' statement

grigoryvp

[Catching an exception while using a Python 'with' statement](https://stackoverflow.com/questions/713794/catching-an-exception-while-using-a-python-with-statement)

To my shame, I can't figure out how to handle exception for python 'with' statement. If I have a code:I really want to handle 'file not found exception' in order to do somehing. But I can't writeand can't writeenclosing 'with' in a try/except statement doesn't work else: exception is not raised. What can I do in order to process failure inside 'with' statement in a Pythonic way?

2009-04-03 13:00:25Z

To my shame, I can't figure out how to handle exception for python 'with' statement. If I have a code:I really want to handle 'file not found exception' in order to do somehing. But I can't writeand can't writeenclosing 'with' in a try/except statement doesn't work else: exception is not raised. What can I do in order to process failure inside 'with' statement in a Pythonic way?If you want different handling for errors from the open call vs the working code you could do:The best "Pythonic" way to do this, exploiting the with statement, is listed as Example #6 in PEP 343, which gives the background of the statement.Used as follows:The with statement has been available without the __future__ import since Python 2.6. You can get it as early as Python 2.5 (but at this point it's time to upgrade!) with:Here's the closest thing to correct that you have. You're almost there, but with doesn't have an except clause:A context manager's __exit__ method, if it returns False will reraise the error when it finishes. If it returns True, it will suppress it. The open builtin's __exit__ doesn't return True, so you just need to nest it in a try, except block:And standard boilerplate: don't use a bare except: which catches BaseException and every other possible exception and warning. Be at least as specific as Exception, and for this error, perhaps catch IOError. Only catch errors you're prepared to handle.So in this case, you'd do:Differentiating between exceptions that occur in a with statement is tricky because they can originate in different places. Exceptions can be raised from either of the following places (or functions called therein):For more details see the documentation about Context Manager Types.If we want to distinguish between these different cases, just wrapping the with into a try .. except is not sufficient. Consider the following example (using ValueError as an example but of course it could be substituted with any other exception type):Here the except will catch exceptions originating in all of the four different places and thus does not allow to distinguish between them. If we move the instantiation of the context manager object outside the with, we can distinguish between __init__ and BLOCK / __enter__ / __exit__:Effectively this just helped with the __init__ part but we can add an extra sentinel variable to check whether the body of the with started to execute (i.e. differentiating between __enter__ and the others):The tricky part is to differentiate between exceptions originating from BLOCK and __exit__ because an exception that escapes the body of the with will be passed to __exit__ which can decide how to handle it (see the docs). If however __exit__ raises itself, the original exception will be replaced by the new one. To deal with these cases we can add a general except clause in the body of the with to store any potential exception that would have otherwise escaped unnoticed and compare it with the one caught in the outermost except later on - if they are the same this means the origin was BLOCK or otherwise it was __exit__ (in case __exit__ suppresses the exception by returning a true value the outermost except will simply not be executed).PEP 343 -- The "with" Statement specifies an equivalent "non-with" version of the with statement. Here we can readily wrap the various parts with try ... except and thus differentiate between the different potential error sources:The need for such special exception handling should be quite rare and normally wrapping the whole with in a try ... except block will be sufficient. Especially if the various error sources are indicated by different (custom) exception types (the context managers need to be designed accordingly) we can readily distinguish between them. For example:

Converting NumPy array into Python List structure?

Alex Brooks

[Converting NumPy array into Python List structure?](https://stackoverflow.com/questions/1966207/converting-numpy-array-into-python-list-structure)

How do I convert a NumPy array to a Python List (for example [[1,2,3],[4,5,6]] ), and do it reasonably fast?

2009-12-27 15:29:55Z

How do I convert a NumPy array to a Python List (for example [[1,2,3],[4,5,6]] ), and do it reasonably fast?Use tolist():Note that this converts the values from whatever numpy type they may have (e.g. np.int32 or np.float32) to the "nearest compatible Python type" (in a list). If you want to preserve the numpy data types, you could call list() on your array instead, and you'll end up with a list of numpy scalars. (Thanks to Mr_and_Mrs_D for pointing that out in a comment.)The numpy .tolist method produces nested lists if the numpy array shape is 2D.if flat lists are desired, the method below works.tolist() works fine even if encountered a nested array, say a pandas DataFrame;someList = [list(map(int, input().split())) for i in range(N)]

beyond top level package error in relative import

shelper

[beyond top level package error in relative import](https://stackoverflow.com/questions/30669474/beyond-top-level-package-error-in-relative-import)

It seems there are already quite some questions here about relative import in python 3, but after going through many of them I still didn't find the answer for my issue. 

so here is the question. I have a package shown belowand I have a single line in test.py:now, I am in the folder of package, and I run I got messagebut if I am in the parent folder of package, e.g., I run:everything is fine. Now my question is: 

when I am in the folder of package, and I run the module inside the test_A sub-package as test_A.test, based on my understanding, ..A goes up only one level, which is still within the package folder, why it gives message saying beyond top-level package. What is exactly the reason that causes this error message?

2015-06-05 14:46:31Z

It seems there are already quite some questions here about relative import in python 3, but after going through many of them I still didn't find the answer for my issue. 

so here is the question. I have a package shown belowand I have a single line in test.py:now, I am in the folder of package, and I run I got messagebut if I am in the parent folder of package, e.g., I run:everything is fine. Now my question is: 

when I am in the folder of package, and I run the module inside the test_A sub-package as test_A.test, based on my understanding, ..A goes up only one level, which is still within the package folder, why it gives message saying beyond top-level package. What is exactly the reason that causes this error message?EDIT: There are better/more coherent answers to this question in other questions: Why doesn't it work? It's because python doesn't record where a package was loaded from. So when you do python -m test_A.test, it basically just discards the knowledge that test_A.test is actually stored in package (i.e. package is not considered a package). Attempting from ..A import foo is trying to access information it doesn't have any more (i.e. sibling directories of a loaded location). It's conceptually similar to allowing from ..os import path in a file in math. This would be bad because you want the packages to be distinct. If they need to use something from another package, then they should refer to them globally with from os import path and let python work out where that is with $PATH and $PYTHONPATH.When you use python -m package.test_A.test, then using from ..A import foo resolves just fine because it kept track of what's in package and you're just accessing a child directory of a loaded location.Why doesn't python consider the current working directory to be a package? NO CLUE, but gosh it would be useful.Try this.

Worked for me.Assumption:

If you are in the package directory, A and test_A are separate packages. Conclusion:

..A imports are only allowed within a package. Further notes:

Making the relative imports only available within packages is useful if you want to force that packages can be placed on any path located on sys.path.EDIT:The current working directory is usually located in sys.path. So, all files there are importable. This is behavior since Python 2 when packages did not yet exist. Making the running directory a package would allow imports of modules as "import .A" and as "import A" which then would be two different modules. Maybe this is an inconsistency to consider.None of these solutions worked for me in 3.6, with a folder structure like:My goal was to import from module1 into module2. What finally worked for me was, oddly enough:Note the single dot as opposed to the two-dot solutions mentioned so far.Edit: The following helped clarify this for me:In my case, the working directory was (unexpectedly) the root of the project.from package.A import fooI think it's clearer thanAs the most popular answer suggests, basically its because your PYTHONPATH or sys.path includes . but not your path to your package. And the relative import is relative to your current working directory, not the file where the import happens; oddly.You could fix this by first changing your relative import to absolute and then either starting it with:OR forcing the python path when called this way, because:With python -m test_A.test you're executing test_A/test.py with __name__ == '__main__' and __file__ == '/absolute/path/to/test_A/test.py'That means that in test.py you could use your absolute import semi-protected in the main case condition and also do some one-time Python path manipulation:If someone's still struggling a bit after the great answers already provided, consider checking out this:https://www.daveoncode.com/2017/03/07/how-to-solve-python-modulenotfound-no-module-named-import-error/Essential quote from the above site:It's pretty obvious that it has to be this way, thinking on it after the fact. I was trying to use the sys.path.append('..') in my tests, but ran into the issue posted by OP. By adding the import and sys.path defintion before my other imports, I was able to solve the problem.if you have an __init__.py in an upper folder, you can initialize the import as

import file/path as alias in that init file. Then you can use it on lower scripts as:

Is there a Python equivalent of the C# null-coalescing operator?

Klaus Byskov Pedersen

[Is there a Python equivalent of the C# null-coalescing operator?](https://stackoverflow.com/questions/4978738/is-there-a-python-equivalent-of-the-c-sharp-null-coalescing-operator)

In C# there's a null-coalescing operator (written as ??) that allows for easy (short) null checking during assignment:Is there a python equivalent?I know that I can do:But is there an even shorter way (where I don't need to repeat s)?

2011-02-12 15:04:49Z

In C# there's a null-coalescing operator (written as ??) that allows for easy (short) null checking during assignment:Is there a python equivalent?I know that I can do:But is there an even shorter way (where I don't need to repeat s)?Ok, it must be clarified how the or operator works. It is a boolean operator, so it works in a boolean context. If the values are not boolean, they are converted to boolean for the purposes of the operator.Note that the or operator does not return only True or False. Instead, it returns the first operand if the first operand evaluates to true, and it returns the second operand if the first operand evaluates to false.In this case, the expression x or y returns x if it is True or evaluates to true when converted to boolean. Otherwise, it returns y. For most cases, this will serve for the very same purpose of C♯'s null-coalescing operator, but keep in mind:If you use your variable s to hold something that is either a reference to the instance of a class or None (as long as your class does not define members __nonzero__() and __len__()), it is secure to use the same semantics as the null-coalescing operator.In fact, it may even be useful to have this side-effect of Python. Since you know what values evaluates to false, you can use this to trigger the default value without using None specifically (an error object, for example).In some languages this behavior is referred to as the Elvis operator.Strictly,Otherwise, s = False will become "default value", which may not be what was intended.If you want to make this shorter, try:Here's a function that will return the first argument that isn't None:reduce() might needlessly iterate over all the arguments even if the first argument is not None, so you can also use this version:I realize this is answered, but there is another option when you're dealing with objects. If you have an object that might be: You can use: Like: By adding {} as the default value, if "name" is missing, an empty object is returned and passed through to the next get. This is similar to null-safe-navigation in C#, which would be like obj?.name?.first.In addition to Juliano's answer about behavior of "or": 

it's "fast"So sometimes it's might be a useful shortcut for things likeRegarding answers by @Hugh Bothwell, @mortehu and @glglgl.Setup Dataset for testingDefine implementationsMake test functionResults on mac i7 @2.7Ghz using python 2.7Clearly the not_none function answers the OP's question correctly and handles the "falsy" problem. It is also the fastest and easiest to read. If applying the logic in many places, it is clearly the best way to go.If you have a problem where you want to find the 1st non-null value in a iterable, then @mortehu's response is the way to go. But it is a solution to a different problem than OP, although it can partially handle that case. It cannot take an iterable AND a default value. The last argument would be the default value returned, but then you wouldn't be passing in an iterable in that case as well as it isn't explicit that the last argument is a default to value. You could then do below, but I'd still use not_null for the single value use case.For those like me that stumbled here looking for a viable solution to this issue, when the variable might be undefined, the closest i got is:Note that a string is needed when checking in globals, but afterwards the actual variable is used when checking for value.More on variable existence:

How do I check if a variable exists?Addionally to @Bothwells answer (which I prefer) for single values, in order to null-checking assingment of function return values, you can use new walrus-operator (since python3.8):Thus, test function does not need to be evaluated two times (as in a = 2 if (x:= test()) is None else test())if you cannot find the name inside the dictionary, it will return the default_value, 

if the name exist then it will add any existing value with 1.hope this can helpThe two functions below I have found to be very useful when dealing with many variable testing cases. 

Cost of len() function

Imran

[Cost of len() function](https://stackoverflow.com/questions/1115313/cost-of-len-function)

What is the cost of len() function for Python built-ins? (list/tuple/string/dictionary)

2009-07-12 04:31:02Z

What is the cost of len() function for Python built-ins? (list/tuple/string/dictionary)It's O(1) (constant time, not depending of actual length of the element - very fast) on every type you've mentioned, plus set and others such as array.array.Calling len() on those data types is O(1) in CPython, the most common implementation of the Python language. Here's a link to a table that provides the algorithmic complexity of many different functions in CPython:TimeComplexity Python Wiki Page All those objects keep track of their own length. The time to extract the length is small (O(1) in big-O notation) and mostly consists of [rough description, written in Python terms, not C terms]: look up "len" in a dictionary and dispatch it to the built_in len function which will look up the object's __len__ method and call that ... all it has to do is return self.lengthThe below measurements provide evidence that len() is O(1) for oft-used data structures.  A note regarding timeit: When the -s flag is used and two strings are passed to timeit the first string is executed only once and is not timed.len is an O(1) because in your RAM, lists are stored as tables (series of contiguous addresses). To know when the table stops the computer needs two things : length and start point. That is why len() is a O(1), the computer stores the value, so it just needs to look it up.I have been thinking of len() in Python depends on the size of the list, so I always store the length in a variable if I use multiple times. But today while debugging, I noticed __len__ attribute in the list object, so len() must be just fetching it, which makes the complexity O(1). So I just googled if someone has already asked it and came across this post.

What is the purpose of meshgrid in Python / NumPy?

HonzaB

[What is the purpose of meshgrid in Python / NumPy?](https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy)

Can someone explain to me what is the purpose of meshgrid function in Numpy? I know it creates some kind of grid of coordinates for plotting, but I can't really see the direct benefit of it.I am studying "Python Machine Learning" from Sebastian Raschka, and he is using it for plotting the decision borders. See input 11 here.I have also tried this code from official documentation, but, again, the output doesn't really make sense to me.Please, if possible, also show me a lot of real-world examples.

2016-03-15 13:43:43Z

Can someone explain to me what is the purpose of meshgrid function in Numpy? I know it creates some kind of grid of coordinates for plotting, but I can't really see the direct benefit of it.I am studying "Python Machine Learning" from Sebastian Raschka, and he is using it for plotting the decision borders. See input 11 here.I have also tried this code from official documentation, but, again, the output doesn't really make sense to me.Please, if possible, also show me a lot of real-world examples.The purpose of meshgrid is to create a rectangular grid out of an array of x values and an array of y values.So, for example, if we want to create a grid where we have a point at each integer value between 0 and 4 in both the x and y directions. To create a rectangular grid, we need every combination of the x and y points.This is going to be 25 points, right? So if we wanted to create an x and y array for all of these points, we could do the following.This would result in the following x and y matrices, such that the pairing of the corresponding element in each matrix gives the x and y coordinates of a point in the grid.We can then plot these to verify that they are a grid:Obviously, this gets very tedious especially for large ranges of x and y. Instead, meshgrid can actually generate this for us: all we have to specify are the unique x and y values.Now, when we call meshgrid, we get the previous output automatically.Creation of these rectangular grids is useful for a number of tasks. In the example that you have provided in your post, it is simply a way to sample a function (sin(x**2 + y**2) / (x**2 + y**2)) over a range of values for x and y. Because this function has been sampled on a rectangular grid, the function can now be visualized as an "image".Additionally, the result can now be passed to functions which expect data on rectangular grid (i.e. contourf)Courtesy of Microsoft Excel: Actually the purpose of np.meshgrid is already mentioned in the documentation:So it's primary purpose is to create a coordinates matrices.You probably just asked yourself:The reason you need coordinate matrices with Python/NumPy is that there is no direct relation from coordinates to values, except when your coordinates start with zero and are purely positive integers. Then you can just use the indices of an array as the index.

However when that's not the case you somehow need to store coordinates alongside your data. That's where grids come in.Suppose your data is:However, each value represents a 2 kilometers wide region horizontally and 3 kilometers vertically. Suppose your origin is the upper left corner and you want arrays that represent the distance you could use:where v is:and h:So if you have two indices, let's say x and y (that's why the return value of meshgrid is usually xx or xs instead of x in this case I chose h for horizontally!) then you can get the x coordinate of the point, the y coordinate of the point and the value at that point by using:That makes it much easier to keep track of coordinates and (even more importantly) you can pass them to functions that need to know the coordinates.However, np.meshgrid itself isn't often used directly, mostly one just uses one of similar objects np.mgrid or np.ogrid.

Here np.mgrid represents the sparse=False and np.ogrid the sparse=True case (I refer to the sparse argument of np.meshgrid). Note that there is a significant difference between 

np.meshgrid and np.ogrid and np.mgrid: The first two returned values (if there are two or more) are reversed. Often this doesn't matter but you should give meaningful variable names depending on the context. For example, in case of a 2D grid and matplotlib.pyplot.imshow it makes sense to name the first returned item of np.meshgrid x and the second one y while it's 

the other way around for np.mgrid and np.ogrid.As already said the output is reversed when compared to np.meshgrid, that's why I unpacked it as yy, xx instead of xx, yy:This already looks like coordinates, specifically the x and y lines for 2D plots.Visualized:The same applies here: The output is reversed compared to np.meshgrid:Unlike ogrid these arrays contain all xx and yy coordinates in the -5 <= xx <= 5; -5 <= yy <= 5 grid. It's not only limited to 2D, these functions work for arbitrary dimensions (well, there is a maximum number of arguments given to function in Python and a maximum number of dimensions that NumPy allows):Even if these also work for 1D there are two (much more common) 1D grid creation functions:Besides the start and stop argument it also supports the step argument (even complex steps that represent the number of steps):You specifically asked about the purpose and in fact, these grids are extremely useful if you need a coordinate system.For example if you have a NumPy function that calculates the distance in two dimensions:And you want to know the distance of each point:The output would be identical if one passed in a dense grid instead of an open grid. NumPys broadcasting makes it possible!Let's visualize the result:And this is also when NumPys mgrid and ogrid become very convenient because it allows you to easily change the resolution of your grids:However, since imshow doesn't support x and y inputs one has to change the ticks by hand. It would be really convenient if it would accept the x and y coordinates, right?It's easy to write functions with NumPy that deal naturally with grids.  Furthermore, there are several functions in NumPy, SciPy, matplotlib that expect you to pass in the grid.I like images so let's explore matplotlib.pyplot.contour:Note how the coordinates are already correctly set! That wouldn't be the case if you just passed in the density.Or to give another fun example using astropy models (this time I don't care much about the coordinates, I just use them to create some grid):Although that's just "for the looks" several functions related to functional models and fitting (for example scipy.interpolate.interp2d, 

scipy.interpolate.griddata even show examples using np.mgrid) in Scipy, etc. require grids. Most of these work with open grids and dense grids, however some only work with one of them.Suppose you have a function:and you want, for example, to see what it looks like in the range 0 to 2*pi. How would you do it? There np.meshgrid comes in:and such a plot would look like:So np.meshgrid is just a convenience. In principle the same could be done by:but there you need to be aware of your dimensions (suppose you have more than two ...) and the right broadcasting. np.meshgrid does all of this for you.Also meshgrid allows you to delete coordinates together with the data if you, for example, want to do an interpolation but exclude certain values:so how would you do the interpolation now? You can give x and y to an interpolation function like scipy.interpolate.interp2d so you need a way to know which coordinates were deleted:and then you can still interpolate with the "right" coordinates (try it without the meshgrid and you will have a lot of extra code):and the original meshgrid allows you to get the interpolation on the original grid again:These are just some examples where I used the meshgrid there might be a lot more.meshgrid helps in creating a rectangular grid from two 1-D arrays of all pairs of points from the two arrays.Now, if you have defined a function f(x,y) and you wanna apply this function to all the possible combination of points from the arrays 'x' and 'y', then you can do this:Say, if your function just produces the product of two elements, then this is how a cartesian product can be achieved, efficiently for large arrays.Referred from here

Compiled vs. Interpreted Languages

chimeracoder

[Compiled vs. Interpreted Languages](https://stackoverflow.com/questions/3265357/compiled-vs-interpreted-languages)

I'm trying to get a better understanding of the difference. I've found a lot of explanations online, but they tend towards the abstract differences rather than the practical implications.Most of my programming experiences has been with CPython (dynamic, interpreted), and Java (static, compiled). However, I understand that there are other kinds of interpreted and compiled languages. Aside from the fact that executable files can be distributed from programs written in compiled languages, are there any advantages/disadvantages to each type? Oftentimes, I hear people arguing that interpreted languages can be used interactively, but I believe that compiled languages can have interactive implementations as well, correct?

2010-07-16 13:35:33Z

I'm trying to get a better understanding of the difference. I've found a lot of explanations online, but they tend towards the abstract differences rather than the practical implications.Most of my programming experiences has been with CPython (dynamic, interpreted), and Java (static, compiled). However, I understand that there are other kinds of interpreted and compiled languages. Aside from the fact that executable files can be distributed from programs written in compiled languages, are there any advantages/disadvantages to each type? Oftentimes, I hear people arguing that interpreted languages can be used interactively, but I believe that compiled languages can have interactive implementations as well, correct?A compiled language is one where the program, once compiled, is expressed in the instructions of the target machine. For example, an addition "+" operation in your source code could be translated directly to the "ADD" instruction in machine code.An interpreted language is one where the instructions are not directly executed by the target machine, but instead read and executed by some other program (which normally is written in the language of the native machine). For example, the same "+" operation would be recognised by the interpreter at run time, which would then call its own "add(a,b)" function with the appropriate arguments, which would then execute the machine code "ADD" instruction.You can do anything that you can do in an interpreted language in a compiled language and vice-versa - they are both Turing complete. Both however have advantages and disadvantages for implementation and use.I'm going to completely generalise (purists forgive me!) but, roughly, here are the advantages of compiled languages:And here are the advantages of interpreted languages:Note that modern techniques such as bytecode compilation add some extra complexity - what happens here is that the compiler targets a "virtual machine" which is not the same as the underlying hardware. These virtual machine instructions can then be compiled again at a later stage to get native code (e.g. as done by the Java JVM JIT compiler).A language itself is neither compiled nor interpreted, only a specific implementation of a language is.  Java is a perfect example.  There is a bytecode-based platform (the JVM), a native compiler (gcj) and an interpeter for a superset of Java (bsh).  So what is Java now? Bytecode-compiled, native-compiled or interpreted?  Other languages, which are compiled as well as interpreted, are Scala, Haskell or Ocaml.  Each of these languages has an interactive interpreter, as well as a compiler to byte-code or native machine code.So generally categorizing languages by "compiled" and "interpreted" doesn't make much sense.Start thinking in terms of a: blast from the past Once upon a time, long long ago, there lived in the land of computing

interpreters and compilers. All kinds of fuss ensued over the merits of

one over the other. The general opinion at that time was something along the lines of:A one or two order of magnitude difference in the runtime

performance existed between an interpreted program and a compiled program. Other distinguishing 

points, run-time mutability of the code for example, were also of some interest but the major

distinction revolved around the run-time performance issues.Today the landscape has evolved to such an extent that the compiled/interpreted distinction is 

pretty much irrelevant. Many

compiled languages call upon run-time services that are not

completely machine code based. Also, most interpreted languages are "compiled" into byte-code

before execution. Byte-code interpreters can be very efficient and rival some compiler generated

code from an execution speed point of view.The classic difference is that compilers generated native machine code, interpreters read source code and

generated machine code on the fly using some sort of run-time system. 

Today there are very few classic interpreters left - almost all of them

compile into byte-code (or some other semi-compiled state) which then runs on a virtual "machine".The extreme and simple cases:With those out of the way, let me explain that life ain't so simple any more. For instance, In the end, these days, interpreting vs. compiling is a trade-off, with time spent (once) compiling often being rewarded by better runtime performance, but an interpretative environment giving more opportunities for interaction. Compiling vs. interpreting is mostly a matter of how the work of "understanding" the program is divided up between different processes, and the line is a bit blurry these days as languages and products try to offer the best of both worlds.From http://www.quora.com/What-is-the-difference-between-compiled-and-interpreted-programming-languagesThe biggest advantage of interpreted source code over compiled source code is PORTABILITY.If your source code is compiled, you need to compile a different executable for each type of processor and/or platform that you want your program to run on (e.g. one for Windows x86, one for Windows x64, one for Linux x64, and so on). Furthermore, unless your code is completely standards compliant and does not use any platform-specific functions/libraries, you will actually need to write and maintain multiple code bases!If your source code is interpreted, you only need to write it once and it can be interpreted and executed by an appropriate interpreter on any platform! It's portable! Note that an interpreter itself is an executable program that is written and compiled for a specific platform.An advantage of compiled code is that it hides the source code from the end user (which might be intellectual property) because instead of deploying the original human-readable source code, you deploy an obscure binary executable file.A compiler and an interpreter do the same job: translating a programming language to another pgoramming language, usually closer to the hardware, often direct executable machine code.Traditionally, "compiled" means that this translation happens all in one go, is done by a developer, and the resulting executable is distributed to users. Pure example: C++.

Compilation usually takes pretty long and tries to do lots of expensive optmization so that the resulting executable runs faster. End users don't have the tools and knowledge to compile stuff themselves, and the executable often has to run on a variety of hardware, so you can't do many hardware-specific optimizations. During development, the separate compilation step means a longer feedback cycle.Traditionally, "interpreted" means that the translation happens "on the fly", when the user wants to run the program. Pure example: vanilla PHP. A naive interpreter has to parse and translate every piece of code every time it runs, which makes it very slow. It can't do complex, costly optimizations because they'd take longer than the time saved in execution. But it can fully use the capabilities of the hardware it runs on. The lack of a separrate compilation step reduces feedback time during development.But nowadays "compiled vs. interpreted" is not a black-or-white issue, there are shades in between. Naive, simple interpreters are pretty much extinct. Many languages use a two-step process where the high-level code is translated to a platform-independant bytecode (which is much faster to interpret). Then you have "just in time compilers" which compile code at most once per program run, sometimes cache results, and even intelligently decide to interpret code that's run rarely, and do powerful optimizations for code that runs a lot. During development, debuggers are capable of switching code inside a running program even for traditionally compiled languages.First, a clarification, Java is not fully static-compiled and linked in the way C++. It is compiled into bytecode, which is then interpreted by a JVM. The JVM can go and do just-in-time compilation to the native machine language, but doesn't have to do it.More to the point: I think interactivity is the main practical difference. Since everything is interpreted, you can take a small excerpt of code, parse and run it against the current state of the environment. Thus, if you had already executed code that initialized a variable, you would have access to that variable, etc. It really lends itself way to things like the functional style.Interpretation, however, costs a lot, especially when you have a large system with a lot of references and context. By definition, it is wasteful because identical code may have to be interpreted and optimized twice (although most runtimes have some caching and optimizations for that). Still, you pay a runtime cost and often need a runtime environment. You are also less likely to see complex interprocedural optimizations because at present their performance is not sufficiently interactive.Therefore, for large systems that are not going to change much, and for certain languages, it makes more sense to precompile and prelink everything, do all the optimizations that you can do. This ends up with a very lean runtime that is already optimized for the target machine. As for generating executbles, that has little to do with it, IMHO. You can often create an executable from a compiled language. But you can also create an executable from an interpreted language, except that the interpreter and runtime is already packaged in the exectuable and hidden from you. This means that you generally still pay the runtime costs (although I am sure that for some language there are ways to translate everything to a tree executable).I disagree that all languages could be made interactive. Certain languages, like C, are so tied to the machine and the entire link structure that I'm not sure you can build a meaningful fully-fledged interactive versionIt's rather difficult to give a practical answer because the difference is about the language definition itself. It's possible to build an interpreter for every compiled language, but it's not possible to build an compiler for every interpreted language. It's very much about the formal definition of a language. So that theoretical informatics stuff noboby likes at university.The Python Book © 2015 Imagine Publishing Ltd, simply distunguishes the difference by the following hint mentioned in page 10 as:Compile is the process of creating an executable program from code written in a compiled programming language. Compiling allows the computer to run and understand the program without the need of the programming software used to create it. When a program is compiled it is often compiled for a specific platform (e.g. IBM platform) that works with IBM compatible computers, but not other platforms (e.g. Apple platform).

The first compiler was developed by Grace Hopper while working on the Harvard Mark I computer. Today, most high-level languages will include their own compiler or have toolkits available that can be used to compile the program. A good example of a compiler used with Java is Eclipse and an example of a compiler used with C and C++ is the gcc command. Depending on how big the program is it should take a few seconds or minutes to compile and if no errors are encountered while being compiled an executable file is created.check this informationShort (un-precise) definition:Compiled language: Entire program is translated to machine code at once, then the machine code is run by the CPU.Interpreted language: Program is read line-by-line and as soon as a line is read the machine instructions for that line are executed by the CPU.But really, few languages these days are purely compiled or purely interpreted, it often is a mix. For a more detailed description with pictures, see this thread:What is the difference between compilation and interpretation?Or my later blog post:https://orangejuiceliberationfront.com/the-difference-between-compiler-and-interpreter/

SyntaxError: Non-ASCII character '\xa3' in file when function returns '£'

SNIFFER_dog

[SyntaxError: Non-ASCII character '\xa3' in file when function returns '£'](https://stackoverflow.com/questions/10589620/syntaxerror-non-ascii-character-xa3-in-file-when-function-returns-%c2%a3)

Say I have a function:I want to print some stuff with a pound sign in front of it and it prints an error when I try to run this program, this error message is displayed:Can anyone inform me how I can include a pound sign in my return function? I'm basically using it in a class and it's within the '__str__' part that the pound sign is included.

2012-05-14 19:12:25Z

Say I have a function:I want to print some stuff with a pound sign in front of it and it prints an error when I try to run this program, this error message is displayed:Can anyone inform me how I can include a pound sign in my return function? I'm basically using it in a class and it's within the '__str__' part that the pound sign is included.I'd recommend reading that PEP the error gives you.  The problem is that your code is trying to use the ASCII encoding, but the pound symbol is not an ASCII character.  Try using UTF-8 encoding.  You can start by putting # -*- coding: utf-8 -*- at the top of your .py file.  To get more advanced, you can also define encodings on a string by string basis in your code.  However, if you are trying to put the pound sign literal in to your code, you'll need an encoding that supports it for the entire file.Adding the following two line sat the top of my .py script worked for me (first line was necessary):First add the # -*- coding: utf-8 -*- line to the beginning of the file and then use u'foo' for all your non-ASCII unicode data:or use the magic available since Python 2.6 to make it automatic:The error message tells you exactly what's wrong. The Python interpreter needs to know the encoding of the non-ASCII character.If you want to return U+00A3 then you can saywhich represents this character in pure ASCII by way of a Unicode escape sequence.  If you want to return a byte string containing the literal byte 0xA3, that's(where in Python 2 the b is implicit; but explicit is better than implicit).The linked PEP in the error message instructs you exactly how to tell Python "this file is not pure ASCII; here's the encoding I'm using".  If the encoding is UTF-8, that would beor the Emacs-compatibleIf you don't know which encoding your editor uses to save this file, examine it with something like a hex editor and some googling.  The Stack Overflow character-encoding tag has a tag info page with more information and some troubleshooting tips.In so many words, outside of the 7-bit ASCII range (0x00-0x7F), Python can't and mustn't guess what string a sequence of bytes represents. https://tripleee.github.io/8bit#a3 shows 21 possible interpretations for the byte 0xA3 and that's only from the legacy 8-bit encodings; but it could also very well be the first byte of a multi-byte encoding. But in fact, I would guess you are actually using Latin-1, so you should haveas the first or second line of your source file.  Anyway, without knowledge of which character the byte is supposed to represent, a human would not be able to guess this, either.A caveat: coding: latin-1 will definitely remove the error message (because there are no byte sequences which are not technically permitted in this encoding), but might produce completely the wrong result when the code is interpreted if the actual encoding is something else. You really have to know the encoding of the file with complete certainty when you declare the encoding.Adding the following two lines in the script solved the issue for me.Hope it helps !You're probably trying to run Python 3 file with Python 2 interpreter. Currently (as of 2019), python command defaults to Python 2 when both versions are installed, on Windows and most Linux distributions.But in case you're indeed working on a Python 2 script, a not yet mentioned on this page solution is to resave the file in UTF-8+BOM encoding, that will add three special bytes to the start of the file, they will explicitly inform the Python interpreter (and your text editor) about the file encoding.

How to get string objects instead of Unicode from JSON?

Brutus

[How to get string objects instead of Unicode from JSON?](https://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-from-json)

I'm using Python 2 to parse JSON from ASCII encoded text files. When loading these files with either json or  simplejson, all my string values are cast to Unicode objects instead of string objects. The problem is, I have to use the data with some libraries that only accept string objects. I can't change the libraries nor update them.Is it possible to get string objects instead of Unicode ones?This question was asked a long time ago, when I was stuck with Python 2. One easy and clean solution for today is to use a recent version of Python — i.e. Python 3 and forward.

2009-06-05 16:32:17Z

I'm using Python 2 to parse JSON from ASCII encoded text files. When loading these files with either json or  simplejson, all my string values are cast to Unicode objects instead of string objects. The problem is, I have to use the data with some libraries that only accept string objects. I can't change the libraries nor update them.Is it possible to get string objects instead of Unicode ones?This question was asked a long time ago, when I was stuck with Python 2. One easy and clean solution for today is to use a recent version of Python — i.e. Python 3 and forward.Example usage:Mark Amery's function is shorter and clearer than these ones, so what's the point of them? Why would you want to use them?Purely for performance. Mark's answer decodes the JSON text fully first with unicode strings, then recurses through the entire decoded value to convert all strings to byte strings. This has a couple of undesirable effects:This answer mitigates both of those performance issues by using the object_hook parameter of json.load and json.loads. From the docs:Since dictionaries nested many levels deep in other dictionaries get passed to object_hook as they're decoded, we can byteify any strings or lists inside them at that point and avoid the need for deep recursion later.Mark's answer isn't suitable for use as an object_hook as it stands, because it recurses into nested dictionaries. We prevent that recursion in this answer with the ignore_dicts parameter to _byteify, which gets passed to it at all times except when object_hook passes it a new dict to byteify. The ignore_dicts flag tells _byteify to ignore dicts since they already been byteified.Finally, our implementations of json_load_byteified and json_loads_byteified call _byteify (with ignore_dicts=True) on the result returned from json.load or json.loads to handle the case where the JSON text being decoded doesn't have a dict at the top level.While there are some good answers here, I ended up using PyYAML to parse my JSON files, since it gives the keys and values as str type strings instead of unicode type. Because JSON is a subset of YAML it works nicely:Some things to note though:As stated, there is no conversion! If you can't be sure to only deal with ASCII values (and you can't be sure most of the time), better use a conversion function:I used the one from Mark Amery a couple of times now, it works great and is very easy to use. You can also use a similar function as an object_hook instead, as it might gain you a performance boost on big files. See the slightly more involved answer from Mirec Miskuf for that.There's no built-in option to make the json module functions return byte strings instead of unicode strings. However, this short and simple recursive function will convert any decoded JSON object from using unicode strings to UTF-8-encoded byte strings:Just call this on the output you get from a json.load or json.loads call.A couple of notes:You can use the object_hook parameter for json.loads to pass in a converter. You don't have to do the conversion after the fact. The json module will always pass the object_hook dicts only, and it will recursively pass in nested dicts, so you don't have to recurse into nested dicts yourself. I don't think I would convert unicode strings to numbers like Wells shows. If it's a unicode string, it was quoted as a string in the JSON file, so it is supposed to be a string (or the file is bad).Also, I'd try to avoid doing something like str(val) on a unicode object. You should use value.encode(encoding) with a valid encoding, depending on what your external lib expects.So, for example:That's because json has no difference between string objects and unicode objects. They're all strings in javascript.I think JSON is right to return unicode objects. In fact, I wouldn't accept anything less, since javascript strings are in fact unicode objects (i.e. JSON (javascript) strings can store any kind of unicode character) so it makes sense to create unicode objects when translating strings from JSON. Plain strings just wouldn't fit since the library would have to guess the encoding you want.It's better to use unicode string objects everywhere. So your best option is to update your libraries so they can deal with unicode objects.But if you really want bytestrings, just encode the results to the encoding of your choice:There exists an easy work-around.TL;DR - Use ast.literal_eval() instead of json.loads().  Both ast and json are in the standard library.While not a 'perfect' answer, it gets one pretty far if your plan is to ignore Unicode altogether.  In Python 2.7gives:This gets more hairy when some objects are really Unicode strings.  The full answer gets hairy quickly.Mike Brennan's answer is close, but there is no reason to re-traverse the entire structure. If you use the object_hook_pairs (Python 2.7+) parameter:With it, you get each JSON object handed to you, so you can do the decoding with no need for recursion:Notice that I never have to call the hook recursively since every object will get handed to the hook when you use the object_pairs_hook. You do have to care about lists, but as you can see, an object within a list will be properly converted, and you don't have to recurse to make it happen.EDIT: A coworker pointed out that Python2.6 doesn't have object_hook_pairs. You can still use this will Python2.6 by making a very small change. In the hook above, change:toThen use object_hook instead of object_pairs_hook:Using object_pairs_hook results in one less dictionary being instantiated for each object in the JSON object, which, if you were parsing a huge document, might be worth while.I'm afraid there's no way to achieve this automatically within the simplejson library.The scanner and decoder in simplejson are designed to produce unicode text. To do this, the library uses a function called c_scanstring (if it's available, for speed), or py_scanstring if the C version is not available. The scanstring function is called several times by nearly every routine that simplejson has for decoding a structure that might contain text. You'd have to either monkeypatch the scanstring value in simplejson.decoder, or subclass JSONDecoder and provide pretty much your own entire implementation of anything that might contain text.The reason that simplejson outputs unicode, however, is that the json spec specifically mentions that "A string is a collection of zero or more Unicode characters"... support for unicode is assumed as part of the format itself. Simplejson's scanstring implementation goes so far as to scan and interpret unicode escapes (even error-checking for malformed multi-byte charset representations), so the only way it can reliably return the value to you is as unicode.If you have an aged library that needs an str, I recommend you either laboriously search the nested data structure after parsing (which I acknowledge is what you explicitly said you wanted to avoid... sorry), or perhaps wrap your libraries in some sort of facade where you can massage the input parameters at a more granular level. The second approach might be more manageable than the first if your data structures are indeed deeply nested.As Mark (Amery) correctly notes: Using PyYaml's deserializer on a json dump works only if you have ASCII only. At least out of the box. Two quick comments on the PyYaml approach:But performance wise its of no comparison to Mark Amery's answer:Throwing some deeply nested sample dicts onto the two methods, I get this (with dt[j] = time delta of json.loads(json.dumps(m))):So deserialization including fully walking the tree and encoding, well within the order of magnitude of json's C based implementation. I find this remarkably fast and its also more robust than the yaml load at deeply nested structures. And less security error prone, looking at yaml.load.=> While I would appreciate a pointer to a C only based converter the byteify function should be the default answer. This holds especially true if your json structure is from the field, containing user input. Because then you probably need to walk anyway over your structure - independent on your desired internal data structures ('unicode sandwich' or byte strings only).Why?Unicode normalisation. For the unaware: Take a painkiller and read this.So using the byteify recursion you kill two birds with one stone: In my tests it turned out that replacing the input.encode('utf-8') with a unicodedata.normalize('NFC', input).encode('utf-8') was even faster than w/o NFC - but thats heavily dependent on the sample data I guess.The gotcha is that simplejson and json are two different modules, at least in the manner they deal with unicode. You have json in py 2.6+, and this gives you unicode values, whereas simplejson returns string objects. Just try easy_install-ing simplejson in your environment and see if that works. It did for me.Just use pickle instead of json for dump and load, like so:The output it produces is (strings and integers are handled correctly):So, I've run into the same problem. Guess what was the first Google result.Because I need to pass all data to PyGTK, unicode strings aren't very useful to me either. So I have another recursive conversion method. It's actually also needed for typesafe JSON conversion - json.dump() would bail on any non-literals, like Python objects. Doesn't convert dict indexes though.I had a JSON dict as a string. The keys and values were unicode objects like in the following example:I could use the byteify function suggested above by converting the string to a dict object using ast.literal_eval(myStringDict).Support Python2&3 using hook (from https://stackoverflow.com/a/33571117/558397)Returns:This is late to the game, but I built this recursive caster. It works for my needs and I think it's relatively complete. It may help you.Just pass it a JSON object like so:I have it as a private member of a class, but you can repurpose the method as you see fit.I rewrote Wells's _parse_json() to handle cases where the json object itself is an array (my use case).here is a recursive encoder written in C: 

https://github.com/axiros/nested_encodePerformance overhead for "average" structures around 10% compared to json.loads.using this teststructure:With Python 3.6, sometimes I still run into this problem. For example, when getting response from a REST API and loading the response text to JSON, I still get the unicode strings.

Found a simple solution using json.dumps().I ran into this problem too, and having to deal with JSON, I came up with a small loop that converts the unicode keys to strings.  (simplejson on GAE does not return string keys.)obj is the object decoded from JSON:kwargs is what I pass to the constructor of the GAE application (which does not like unicode keys in **kwargs)Not as robust as the solution from Wells, but much smaller.I've adapted the code from the answer of Mark Amery, particularly in order to get rid of isinstance for the pros of duck-typing.The encoding is done manually and ensure_ascii is disabled. The python docs for json.dump says that Disclaimer: in the doctest I used the Hungarian language. Some notable Hungarian-related character encodings are: cp852 the IBM/OEM encoding used eg. in DOS (sometimes referred as ascii, incorrectly I think, it is dependent on the codepage setting), cp1250 used eg. in Windows (sometimes referred as ansi, dependent on the locale settings), and iso-8859-2, sometimes used on http servers. The test text Tüskéshátú kígyóbűvölő is attributed to Koltai László (native personal name form) and is from wikipedia.I'd also like to highlight the answer of Jarret Hardie which references the JSON spec, quoting:In my use-case I had files with json. They are utf-8 encoded files. ensure_ascii results in properly escaped but not very readable json files, that is why I've adapted Mark Amery's answer to fit my needs.The doctest is not particularly thoughtful but I share the code in the hope that it will useful for someone.Check out this answer to a similar question like this which states thatThe u- prefix just means that you have a Unicode string.  When you really use the string, it won't appear in your data.  Don't be thrown by the printed output.For example, try this:You won't see a u.

Is it possible only to declare a variable without assigning any value in Python?

Joan Venge

[Is it possible only to declare a variable without assigning any value in Python?](https://stackoverflow.com/questions/664294/is-it-possible-only-to-declare-a-variable-without-assigning-any-value-in-python)

Is it possible to declare a variable in Python, like so?:so that it initialized to None? It seems like Python allows this, but as soon as you access it, it crashes. Is this possible? If not, why?EDIT: I want to do this for cases like this:

2009-03-19 22:21:53Z

Is it possible to declare a variable in Python, like so?:so that it initialized to None? It seems like Python allows this, but as soon as you access it, it crashes. Is this possible? If not, why?EDIT: I want to do this for cases like this:Why not just do this:Python is dynamic, so you don't need to declare things; they exist automatically in the first scope where they're assigned.  So, all you need is a regular old assignment statement as above.This is nice, because you'll never end up with an uninitialized variable.  But be careful -- this doesn't mean that you won't end up with incorrectly initialized variables.  If you init something to None, make sure that's what you really want, and assign something more meaningful if you can.I'd heartily recommend that you read Other languages have "variables" (I added it as a related link) – in two minutes you'll know that Python has "names", not "variables".In Python 3.6+ you could use Variable Annotations for this:https://www.python.org/dev/peps/pep-0526/#abstractPEP 484 introduced type hints, a.k.a. type annotations. While its main focus was function annotations, it also introduced the notion of type comments to annotate variables:PEP 526 aims at adding syntax to Python for annotating the types of variables (including class variables and instance variables), instead of expressing them through comments:It seems to be more directly in line with what you were asking "Is it possible only to declare a variable without assigning any value in Python?"I'm not sure what you're trying to do. Python is a very dynamic language; you don't usually need to declare variables until you're actually going to assign to or use them. I think what you want to do is justwhich will assign the value None to the variable foo.EDIT: What you really seem to want to do is just this:It's a little difficult to tell if that's really the right style to use from such a short code example, but it is a more "Pythonic" way to work.EDIT: below is comment by JFS (posted here to show the code)NOTE: if some_condition() raises an exception then found is unbound.

NOTE: if len(sequence) == 0 then item is unbound.The above code is not advisable. Its purpose is to illustrate how local variables work, namely whether "variable" is "defined" could be determined only at runtime in this case.

Preferable way:Or I usually initialize the variable to something that denotes the type likeor If it is going to be an object then don't initialize it until you instantiate it:Well, if you want to check if a variable is defined or not then why not check if its in the locals() or globals() arrays? Your code rewritten:If it's a local variable you are looking for then replace globals() with locals(). First of all, my response to the question you've originally askedQ: How do I discover if a variable is defined at a point in my code?A: Read up in the source file until you see a line where that variable is defined.But further, you've given a code example that there are various permutations of that are quite pythonic. You're after a way to scan a sequence for elements that match a condition, so here are some solutions:Clearly in this example you could replace the raise with a return None depending on what you wanted to achieve.If you wanted everything that matched the condition you could do this:There is another way of doing this with yield that I won't bother showing you, because it's quite complicated in the way that it works.Further, there is a one line way of achieving this:If I'm understanding your example right, you don't need to refer to 'value' in the if statement anyway. You're breaking out of the loop as soon as it could be set to anything.You look like you're trying to write C in Python.  If you want to find something in a sequence, Python has builtin functions to do that, likeIt is a good question and unfortunately bad answers as var = None is already assigning a value, and if your script runs multiple times it is overwritten with None every time. It is not the same as defining without assignment. I am still trying to figure out how to bypass this issue.If None is a valid data value then you need to the variable another way. You could use:This sentinel is suggested by Nick Coghlan.You can trick an interpreter with this ugly oneliner if None: var = None

It do nothing else but adding a variable var to local variable dictionary, not initializing it. Interpreter will throw the UnboundLocalError exception if you try to use this variable in a function afterwards. This would works for very ancient python versions too. Not simple, nor beautiful, but don't expect much from python.Is it possible to declare a variable in Python (var=None): 

Convert datetime object to a String of date only in Python

wilbev

[Convert datetime object to a String of date only in Python](https://stackoverflow.com/questions/10624937/convert-datetime-object-to-a-string-of-date-only-in-python)

I see a lot on converting a date string to an datetime object in Python, but I want to go the other way.

I've got and I would like to convert it to string like '2/23/2012'.

2012-05-16 19:00:02Z

I see a lot on converting a date string to an datetime object in Python, but I want to go the other way.

I've got and I would like to convert it to string like '2/23/2012'.You can use strftime to help you format your date.E.g.,will yield:More information about formatting see heredate and datetime objects (and time as well) support a mini-language to specify output, and there are two ways to access it:So your example could look like:orFor completeness' sake: you can also directly access the attributes of the object, but then you only get the numbers:The time taken to learn the mini-language is worth it.For reference, here are the codes used in the mini-language:Another option:You could use simple string formatting methods:type-specific formatting can be used as well:Output:It is possible to convert a datetime object into a string by working directly with the components of the datetime object.Output --> 5/23/2017You can convert datetime to string.String concatenation, str.join, can be used to build the string.If you looking for a simple way of datetime to string conversion and can omit the format. You can convert datetime object to str and then use array slicing.But note the following thing. If other solutions will rise an AttributeError when the variable is None in this case you will receive a 'None' string.If you want the time as well, just go withPrints 2019-07-11 19:36:31.118766 in console for me

How to tell if tensorflow is using gpu acceleration from inside python shell?

Tamim Addari

[How to tell if tensorflow is using gpu acceleration from inside python shell?](https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell)

I have installed tensorflow in my ubuntu 16.04 using the second answer here with ubuntu's builtin apt cuda installation.Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I import tensorflow this is the outputIs this output enough to check if tensorflow is using gpu ? 

2016-06-24 09:14:23Z

I have installed tensorflow in my ubuntu 16.04 using the second answer here with ubuntu's builtin apt cuda installation.Now my question is how can I test if tensorflow is really using gpu? I have a gtx 960m gpu. When I import tensorflow this is the outputIs this output enough to check if tensorflow is using gpu ? No, I don't think "open CUDA library" is enough to tell, because different nodes of the graph may be on different devices.To find out which device is used, you can enable log device placement like this:Check your console for this type of output.Apart from using sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) which is outlined in other answers as well as in the official TensorFlow documentation, you can try to assign a computation to the gpu and see whether you have an error.Here If you have a gpu and can use it, you will see the result. Otherwise you will see an error with a long stacktrace. In the end you will have something like this: Recently a few helpful functions appeared in TF:You can also check for available devices in the session:devices will return you something likeFollowing piece of code should give you all devices available to tensorflow.I think there is an easier way to achieve this.It usually prints like This seems easier to me rather than those verbose logs.Tensorflow 2.0As of tensorflow 2.0, Sessions are no longer used. A still functioning way to test GPU functionality is:If you get an error, you need to check your installation.This will confirm that tensorflow using GPU while training also ?CodeOutputIn addition to other answers, the following should help you to make sure that your version of tensorflow includes GPU support.This should give the list of devices available for Tensorflow (under Py-3.6):Ok, first launch an ipython shell from the terminal and import TensorFlow:Now, we can watch the GPU memory usage in a console using the following command:Since we've only imported TensorFlow but have not used any GPU yet, the usage stats will be:Notice how the GPU memory usage is very less (~ 700MB); Sometimes the GPU memory usage might even be as low as 0 MB.Now, let's load the GPU in our code. As indicated in tf documentation, do:Now, the watch stats should show an updated GPU usage memory as below:Observe now how our Python process from the ipython shell is using ~ 7 GB of the GPU memory.P.S. You can continue watching these stats as the code is running, to see how intense the GPU usage is over time.I prefer to use nvidia-smi to monitor GPU usage. if it goes up significantly when you start you program, it's a strong sign that your tensorflow is using GPU.With the recent updates of Tensorflow, you can check it as follow : This will return True if GPU is being used by Tensorflow, and return False otherwise. If you want device device_name you can type : tf.test.gpu_device_name().

Get more details from hereRun the following in Jupyter,If you've set up your environment properly, you'll get the following output in the terminal where you ran "jupyter notebook",You can see here I'm using TensorFlow with an Nvidia Quodro K620.I find just querying the gpu from the command line is easiest:if your learning is a background process the pid from 

jobs -p should match the pid from nvidia-smiPut this near the top of your jupyter notebook.  Comment out what you don't need.NOTE: With the release of TensorFlow 2.0, Keras is now included as part of the TF API.Originally answerwed here.You can check if you are currently using the GPU by running the following code:  If the output is '', it means you are using CPU only;

If the output is something like that /device:GPU:0, it means GPU works.  And use the following code to check which GPU you are using:  source hereother option is:This is the line I am using to list devices available to tf.session directly from bash:It will print available devices and tensorflow version, for example:You have some options to test whether GPU acceleration is being used by your TensorFlow installation.You can type in the following commands in three different platforms.I found below snippet is very handy to test the gpu .. If you are using TensorFlow 2.0, you can use this for loop to show the devices:if you are using tensorflow 2.x use:UPDATE FOR TENSORFLOW >= 2.1.The recommended way in which to check if TensorFlow is using GPU is the following:As of TensorFlow 2.1, tf.test.gpu_device_name() has been deprecated in favour of the aforementioned.Run this command in Jupyter or your IDE to check if Tensorflow is using a GPU or not: tf.config.list_physical_devices('GPU')The following will also return the name of your GPU devices.

Why return NotImplemented instead of raising NotImplementedError

abyx

[Why return NotImplemented instead of raising NotImplementedError](https://stackoverflow.com/questions/878943/why-return-notimplemented-instead-of-raising-notimplementederror)

Python has a singleton called NotImplemented.  Why would someone want to ever return NotImplemented instead of raising the NotImplementedError exception? Won't it just make it harder to find bugs, such as code that executes invalid methods?

2009-05-18 17:43:40Z

Python has a singleton called NotImplemented.  Why would someone want to ever return NotImplemented instead of raising the NotImplementedError exception? Won't it just make it harder to find bugs, such as code that executes invalid methods?It's because __lt__() and related comparison methods are quite commonly used indirectly in list sorts and such. Sometimes the algorithm will choose to try another way or pick a default winner. Raising an exception would break out of the sort unless caught, whereas NotImplemented doesn't get raised and can be used in further tests.http://jcalderone.livejournal.com/32837.htmlTo summarise that link:Because they have different use cases.Quoting the docs (Python 3.6):NotImplementedexception NotImplementedErrorSee the links for details.One reason is performance.  In a situation like rich comparisons, where you could be doing lots of operations in a short time, setting up and handling lots of exceptions could take a lot longer than simply returning a NotImplemented value.

Is it bad to have my virtualenv directory inside my git repository?

Lyle Pratt

[Is it bad to have my virtualenv directory inside my git repository?](https://stackoverflow.com/questions/6590688/is-it-bad-to-have-my-virtualenv-directory-inside-my-git-repository)

I'm thinking about putting the virtualenv for a Django web app I am making inside my git repository for the app. It seems like an easy way to keep deploy's simple and easy. Is there any reason why I shouldn't do this?

2011-07-06 01:42:04Z

I'm thinking about putting the virtualenv for a Django web app I am making inside my git repository for the app. It seems like an easy way to keep deploy's simple and easy. Is there any reason why I shouldn't do this?I use pip freeze to get the packages I need into a requirements.txt file and add that to my repository.  I tried to think of a way of why you would want to store the entire virtualenv, but I could not.Storing the virtualenv directory inside git will, as you noted, allow you to deploy the whole app by just doing a git clone (plus installing and configuring Apache/mod_wsgi).  One potentially significant issue with this approach is that on Linux the full path gets hard-coded in the venv's activate, django-admin.py, easy_install, and pip scripts.  This means your virtualenv won't entirely work if you want to use a different path, perhaps to run multiple virtual hosts on the same server.  I think the website may actually work with the paths wrong in those files, but you would have problems the next time you tried to run pip.The solution, already given, is to store enough information in git so that during the deploy you can create the virtualenv and do the necessary pip installs.  Typically people run pip freeze to get the list then store it in a file named requirements.txt.  It can be loaded with pip install -r requirements.txt.  RyanBrady already showed how you can string the deploy statements in a single line:  Personally, I just put these in a shell script that I run after doing the git clone or git pull.Storing the virtualenv directory also makes it a bit trickier to handle pip upgrades, as you'll have to manually add/remove and commit the files resulting from the upgrade.  With a requirements.txt file, you just change the appropriate lines in requirements.txt and re-run pip install -r requirements.txt.  As already noted, this also reduces "commit spam".I used to do the same until I started using libraries that are compiled differently depending on the environment such as PyCrypto. My PyCrypto mac wouldn't work on Cygwin wouldn't work on Ubuntu.It becomes an utter nightmare to manage the repository.Either way I found it easier to manage the pip freeze & a requirements file than having it all in git. It's cleaner too since you get to avoid the commit spam for thousands of files as those libraries get updated...  I think one of the main problems which occur is that the virtualenv might not be usable by other people. Reason is that it always uses absolute paths. So if you virtualenv was for example in /home/lyle/myenv/ it will assume the same for all other people using this repository (it must be exactly the same absolute path). You can't presume people using the same directory structure as you.Better practice is that everybody is setting up their own environment (be it with or without virtualenv) and installing libraries there. That also makes you code more usable over different platforms (Linux/Windows/Mac), also because virtualenv is installed different in each of them.I use what is basically David Sickmiller's answer with a little more automation. I create a (non-executable) file at the top level of my project named activate with the following contents:(As per David's answer, this assumes you're doing a pip freeze > requirements.txt to keep your list of requirements up to date.)The above gives the general idea; the actual activate script (documentation) that I normally use is a bit more sophisticated, offering a -q (quiet) option, using python when python3 isn't available, etc.This can then be sourced from any current working directory and will properly activate, first setting up the virtual environment if necessary. My top-level test script usually has code along these lines so that it can be run without the developer having to activate first:Sourcing ./activate, not activate, is important here because the latter will find any other activate in your path before it will find the one in the current directory.If you know which operating systems your application will be running on, I would create one virtualenv for each system and include it in my repository. Then I would make my application detect which system it is running on and use the corresponding virtualenv.The system could e.g. be identified using the platform module.In fact, this is what I do with an in-house application I have written, and to which I can quickly add a new system's virtualenv in case it is needed. This way, I do not have to rely on that pip will be able to successfully download the software my application requires. I will also not have to worry about compilation of e.g. psycopg2 which I use.If you do not know which operating system your application may run on, you are probably better off using pip freeze as suggested in other answers here.It's not a good idea to include any environment-dependent component or setting in your repos as one of the key aspects of using a repo, is perhaps, sharing it with other developers.  Here is how I would setup my development environment on a Windows PC (say, Win10). Bonus: If you want people to easily (well, almost easily) install all the libraries your software needs, you can use and put the instruction on your git so people can use the following command to download all required libraries at once. I think is that the best is to install the virtual environment in a path inside the repository folder, maybe is better inclusive to use a subdirectory dedicated to the environment (I have deleted accidentally my entire project when force installing a virtual environment in the repository root folder, good that I had the project saved in its latest version in Github).Either the automated installer, or the documentation should indicate the virtualenv path as a relative path, this way you won't run into problems when sharing the project with other people. About the packages, the packages used should be saved by pip freeze -r requirements.txt.If you just setting up development env, then use pip freeze file, caz that makes the git repo clean.Then if doing production deployment, then checkin the whole venv folder. That will make your deployment more reproducible, not need those libxxx-dev packages, and avoid the internet issues.So there are two repos. One for your main source code, which includes a requirements.txt. And a env repo, which contains the whole venv folder. 

Python subprocess/Popen with a modified environment

Oren_H

[Python subprocess/Popen with a modified environment](https://stackoverflow.com/questions/2231227/python-subprocess-popen-with-a-modified-environment)

I believe that running an external command with a slightly modified environment is a very common case. That's how I tend to do it:I've got a gut feeling that there's a better way; does it look alright?

2010-02-09 17:55:16Z

I believe that running an external command with a slightly modified environment is a very common case. That's how I tend to do it:I've got a gut feeling that there's a better way; does it look alright?I think os.environ.copy() is better if you don't intend to modify the os.environ for the current process:That depends on what the issue is. If it's to clone and modify the environment one solution could be:But that somewhat depends on that the replaced variables are valid python identifiers, which they most often are (how often do you run into environment variable names that are not alphanumeric+underscore or variables that starts with a number?).Otherwise you'll could write something like:In the very odd case (how often do you use control codes or non-ascii characters in environment variable names?) that the keys of the environment are bytes you can't (on python3) even use that construct.As you can see the techniques (especially the first) used here benefits on the keys of the environment normally is valid python identifiers, and also known in advance (at coding time), the second approach has issues. In cases where that isn't the case you should probably look for another approach.you might use my_env.get("PATH", '') instead of my_env["PATH"] in case PATH somehow not defined in the original environment, but other than that it looks fine.With Python 3.5 you could do it this way:Here we end up with a copy of os.environ and overridden PATH value.It was made possible by PEP 448 (Additional Unpacking Generalizations).Another example. If you have a default environment (i.e. os.environ), and a dict you want to override defaults with, you can express it like this:To temporarily set an environment variable without having to copy the os.envrion object etc, I do this:The env parameter accepts a dictionary. You can simply take os.environ, add a key (your desired variable) (to a copy of the dict if you must) to that and use it as a parameter to Popen. I know this has been answered for some time, but there are some points that some may want to know about using PYTHONPATH instead of PATH in their environment variable. I have outlined an explanation of running python scripts with cronjobs that deals with the modified environment in a different way (found here). Thought it would be of some good for those who, like me, needed just a bit more than this answer provided.In certain circumstances you may want to only pass down the environment variables your subprocess needs, but I think you've got the right idea in general (that's how I do it too).

Breaking out of nested loops [duplicate]

Michael Kuhn

[Breaking out of nested loops [duplicate]](https://stackoverflow.com/questions/653509/breaking-out-of-nested-loops)

Is there an easier way to break out of nested loops than throwing an exception? (In Perl, you can give labels to each loop and at least continue an outer loop.)  I.e., is there a nicer way than:

2009-03-17 09:24:17Z

Is there an easier way to break out of nested loops than throwing an exception? (In Perl, you can give labels to each loop and at least continue an outer loop.)  I.e., is there a nicer way than:It has at least been suggested, but also rejected. I don't think there is another way, short of repeating the test or re-organizing the code. It is sometimes a bit annoying.In the rejection message, Mr van Rossum mentions using return, which is really sensible and something I need to remember personally. :)The same works for deeper loops:If you're able to extract the loop code into a function, a return statement can be used to exit the outermost loop at any time.If it's hard to extract that function you could use an inner function, as @bjd2385 suggests, e.g.Use itertools.product!Here's a link to itertools.product in the python documentation: 

http://docs.python.org/library/itertools.html#itertools.productYou can also loop over an array comprehension with 2 fors in it, and break whenever you want to.Sometimes I use a boolean variable. Naive, if you want, but I find it quite flexible and comfortable to read. Testing a variable may avoid testing again complex conditions and may also collect results from several tests in inner loops.If you're going to raise an exception, you might raise a StopIteration exception.  That will at least make the intent obvious.You can also refactor your code to use a generator. But this may not be a solution for all types of nested loops.In this particular case, you can merge the loops with a modern python (3.0 and probably 2.6, too) by using itertools.product.I for myself took this as a rule of thumb, if you nest too many loops (as in, more than 2), you are usually able to extract one of the loops into a different method or merge the loops into one, as in this case.

How do I pass a string into subprocess.Popen (using the stdin argument)?

Daryl Spitzer

[How do I pass a string into subprocess.Popen (using the stdin argument)?](https://stackoverflow.com/questions/163542/how-do-i-pass-a-string-into-subprocess-popen-using-the-stdin-argument)

If I do the following:I get:Apparently a cStringIO.StringIO object doesn't quack close enough to a file duck to suit subprocess.Popen.  How do I work around this?

2008-10-02 17:25:23Z

If I do the following:I get:Apparently a cStringIO.StringIO object doesn't quack close enough to a file duck to suit subprocess.Popen.  How do I work around this?Popen.communicate() documentation:So your example could be written as follows:On the current Python 3 version, you could use subprocess.run, to pass input as a string to an external command and get its exit status, and its output as a string back in one call:I figured out this workaround:Is there a better one?I'm a bit surprised nobody suggested creating a pipe, which is in my opinion the far simplest way to pass a string to stdin of a subprocess:There's a beatiful solution if you're using Python 3.4 or better. Use the input argument instead of the stdin argument, which accepts a bytes argument:I am using python3 and found out that you need to encode your string before you can pass it into stdin:I'm afraid not.  The pipe is a low-level OS concept, so it absolutely requires a file object that is represented by an OS-level file descriptor.  Your workaround is the right one.Beware that Popen.communicate(input=s)may give you trouble ifsis too big, because apparently the parent process will buffer it before forking the child subprocess, meaning it needs "twice as much" used memory at that point (at least according to the "under the hood" explanation and linked documentation found here). In my particular case,swas a generator that was first fully expanded and only then written tostdin so the parent process was huge right before the child was spawned, 

and no memory was left to fork it:File "/opt/local/stow/python-2.7.2/lib/python2.7/subprocess.py", line 1130, in _execute_child

    self.pid = os.fork()

OSError: [Errno 12] Cannot allocate memoryOn Python 3.7+ do this:and you'll probably want to add capture_output=True to get the output of running the command as a string.On older versions of Python, replace text=True with universal_newlines=True:

What is the best way to repeatedly execute a function every x seconds?

DavidM

[What is the best way to repeatedly execute a function every x seconds?](https://stackoverflow.com/questions/474528/what-is-the-best-way-to-repeatedly-execute-a-function-every-x-seconds)

I want to repeatedly execute a function in Python every 60 seconds forever (just like an NSTimer in Objective C). This code will run as a daemon and is effectively like calling the python script every minute using a cron, but without requiring that to be set up by the user.In this question about a cron implemented in Python, the solution appears to effectively just sleep() for x seconds. I don't need such advanced functionality so perhaps something like this would workAre there any foreseeable problems with this code?

2009-01-23 21:07:05Z

I want to repeatedly execute a function in Python every 60 seconds forever (just like an NSTimer in Objective C). This code will run as a daemon and is effectively like calling the python script every minute using a cron, but without requiring that to be set up by the user.In this question about a cron implemented in Python, the solution appears to effectively just sleep() for x seconds. I don't need such advanced functionality so perhaps something like this would workAre there any foreseeable problems with this code?Use the sched module, which implements a general purpose event scheduler.Just lock your time loop to the system clock. Easy.You might want to consider Twisted which is a Python networking library that implements the Reactor Pattern.While "while True: sleep(60)" will probably work Twisted probably already implements many of the features that you will eventually need (daemonization, logging or exception handling as pointed out by bobince) and will probably be a more robust solutionIf you want a non-blocking way to execute your function periodically, instead of a blocking infinite loop I'd use a threaded timer. This way your code can keep running and perform other tasks and still have your function called every n seconds. I use this technique a lot for printing progress info on long, CPU/Disk/Network intensive tasks.Here's the code I've posted in a similar question, with start() and stop() control:Usage:Features:The easier way I believe to be:This way your code is executed, then it waits 60 seconds then it executes again, waits, execute, etc...

No need to complicate things :DIf you want to do this without blocking your remaining code, you can use this to let it run in its own thread:This solution combines several features rarely found combined in the other solutions:Here's an update to the code from MestreLion that avoids drifiting over time. The RepeatedTimer class here calls the given function every "interval" seconds as requested by the OP; the schedule doesn't depend on how long the function takes to execute. I like this solution since it doesn't have external library dependencies; this is just pure python.Sample usage (copied from MestreLion's answer):I faced a similar problem some time back. May be http://cronus.readthedocs.org might help?For v0.2, the following snippet worksThe main difference between that and cron is that an exception will kill the daemon for good. You might want to wrap with an exception catcher and logger.One possible answer:I use Tkinter after() method, which doesn't "steal the game" (like the sched module that was presented earlier), i.e. it allows other things to run in parallel:do_something1() and do_something2() can run in parallel and in whatever interval speed. Here, the 2nd one will be executed twice as fast.Note also that I have used a simple counter as a condition to terminate either function. You can use whatever other contition you like or none if you what a function to run until the program terminates (e.g. a clock). Here's an adapted version to the code from MestreLion.

In addition to the original function, this code:1) add first_interval used to fire the timer at a specific time(caller need to calculate the first_interval and pass in)2) solve a race-condition in original code. In the original code, if control thread failed to cancel the running timer("Stop the timer, and cancel the execution of the timer’s action. This will only work if the timer is still in its waiting stage." quoted from https://docs.python.org/2/library/threading.html), the timer will run endlessly.I use this to cause 60 events per hour with most events occurring at the same number of seconds after the whole minute:Depending upon actual conditions you might get ticks of length:but at the end of 60 minutes you'll have 60 ticks; and most of them will occur at the correct offset to the minute you prefer.On my system I get typical drift of < 1/20th of a second until need for correction arises.The advantage of this method is resolution of clock drift; which can cause issues if you're doing things like appending one item per tick and you expect 60 items appended per hour.  Failure to account for drift can cause secondary indications like moving averages to consider data too deep into the past resulting in faulty output.  e.g., Display current local timeI ended up using the schedule module. The API is nice.

Get a filtered list of files in a directory

mhost

[Get a filtered list of files in a directory](https://stackoverflow.com/questions/2225564/get-a-filtered-list-of-files-in-a-directory)

I am trying to get a list of files in a directory using Python, but I do not want a list of ALL the files.What I essentially want is the ability to do something like the following but using Python and not executing ls.If there is no built-in method for this, I am currently thinking of writing a for loop to iterate through the results of an os.listdir() and to append all the matching files to a new list.However, there are a lot of files in that directory and therefore I am hoping there is a more efficient method (or a built-in method).

2010-02-08 23:02:56Z

I am trying to get a list of files in a directory using Python, but I do not want a list of ALL the files.What I essentially want is the ability to do something like the following but using Python and not executing ls.If there is no built-in method for this, I am currently thinking of writing a for loop to iterate through the results of an os.listdir() and to append all the matching files to a new list.However, there are a lot of files in that directory and therefore I am hoping there is a more efficient method (or a built-in method).glob.glob('145592*.jpg')glob.glob() is definitely the way to do it (as per Ignacio). However, if you do need more complicated matching, you can do it with a list comprehension and re.match(), something like so:More flexible, but as you note, less efficient.Keep it simple:I prefer this form of list comprehensions because it reads well in English.I read the fourth line as:

  For each fn in os.listdir for my path, give me only the ones that match any one of my included extensions.It may be hard for novice python programmers to really get used to using list comprehensions for filtering, and it can have some memory overhead for very large data sets, but for listing a directory and other simple string filtering tasks, list comprehensions lead to more clean documentable code.The only thing about this design is that it doesn't protect you against making the mistake of passing a string instead of a list.  For example if you accidentally convert a string to a list and end up checking against all the characters of a string, you could end up getting a slew of false positives.But it's better to have a problem that's easy to fix than a solution that's hard to understand.Another option:https://docs.python.org/3/library/fnmatch.htmlPreliminary codeSolution 1 - use "glob"Solution 2 - use "os" + "fnmatch"Variant 2.1 - Lookup in current dirVariant 2.2 - Lookup recursiveResultSolution 3 - use "pathlib"Notes:use os.walk to  recursively list your filesThis will give you a list of jpg files with their full path. You can replace x[0]+"/"+f with f for just filenames. You can also replace f.endswith(".jpg") with whatever string condition you wish.you might also like a more high-level approach (I have implemented and packaged as findtools):can be installed withFilenames with "jpg" and "png" extensions in "path/to/images":You can use pathlib that is available in Python standard library 3.4 and above.You can define pattern and check for it. Here I have taken both start and end pattern and looking for them in the filename. FILES contains the list of all the files in a directory.You can use subprocess.check_ouput() asOf course, the string between quotes can be anything you want to execute in the shell, and store the output.

Loop backwards using indices in Python?

Joan Venge

[Loop backwards using indices in Python?](https://stackoverflow.com/questions/869885/loop-backwards-using-indices-in-python)

I am trying to loop from 100 to 0. How do I do this in Python?for i in range (100,0) doesn't work.

2009-05-15 17:17:06Z

I am trying to loop from 100 to 0. How do I do this in Python?for i in range (100,0) doesn't work.Try range(100,-1,-1), the 3rd argument being the increment to use (documented here).  ("range" options, start, stop, step are documented here)In my opinion, this is the most readable:and some slightly longer (and slower) solution:Generally in Python, you can use negative indices to start from the back:Result:Why your code didn't workYou code for i in range (100, 0) is fine, exceptthe third parameter (step) is by default +1. So you have to specify 3rd parameter to range() as -1 to step backwards.NOTE: This includes 100 & 0 in the output.There are multiple ways.Better WayFor pythonic way, check PEP 0322.This is Python3 pythonic example to print from 100 to 0 (including 100 & 0).Another solution:Result:Tip:

If you are using this method to count back indices in a list, you will want to -1 from the 'y' value, as your list indices will begin at 0.The simple answer to solve your problem could be like this:for var in range(10,-1,-1) worksShort and sweet. This was my solution when doing codeAcademy course. Prints a string in rev order. You can always do increasing range and subtract from a variable in your case 100 - i where i in range( 0, 101 ).    I tried this in one of the codeacademy exercises (reversing chars in a string without using reversed nor :: -1)I wanted to loop through a two lists backwards at the same time so I needed the negative index. This is my solution:Result:Oh okay read the question wrong, I guess it's about going backward in an array? if so, I have this:

What does Ruby have that Python doesn't, and vice versa?

John Feminella

[What does Ruby have that Python doesn't, and vice versa?](https://stackoverflow.com/questions/1113611/what-does-ruby-have-that-python-doesnt-and-vice-versa)

There is a lot of discussions of Python vs Ruby, and I all find them completely unhelpful, because they all turn around why feature X sucks in language Y, or that claim language Y doesn't have X, although in fact it does. I also know exactly why I prefer Python, but that's also subjective, and wouldn't help anybody choosing, as they might not have the same tastes in development as I do.It would therefore be interesting to list the differences, objectively. So no "Python's lambdas sucks". Instead explain what Ruby's lambdas can do that Python's can't. No subjectivity. Example code is good!Don't have several differences in one answer, please. And vote up the ones you know are correct, and down those you know are incorrect (or are subjective). Also, differences in syntax is not interesting. We know Python does with indentation what Ruby does with brackets and ends, and that @ is called self in Python.UPDATE: This is now a community wiki, so we can add the big differences here.In Ruby you have a reference to the class (self) already in the class body. In Python you don't have a reference to the class until after the class construction is finished.An example:self in this case is the class, and this code would print out "Kaka". There is no way to print out the class name or in other ways access the class from the class definition body in Python (outside method definitions).This lets you develop extensions to core classes.  Here's an example of a rails extension:Python (imagine there were no ''.startswith method):You could use it on any sequence (not just strings). In order to use it you should import it explicitly e.g., from some_module import starts_with.Ruby has first class regexps, $-variables, the awk/perl line by line input loop and other features that make it more suited to writing small shell scripts that munge text files or act as glue code for other programs.Thanks to the callcc statement. In Python you can create continuations by various techniques, but there is no support built in to the language.With the "do" statement you can create a multi-line anonymous function in Ruby, which will be passed in as an argument into the method in front of do, and called from there. In Python you would instead do this either by passing a method or with generators.Ruby:Python (Ruby blocks correspond to different constructs in Python):OrOrInterestingly, the convenience statement in Ruby for calling a block is called "yield", which in Python will create a generator.Ruby:Python:Although the principles are different, the result is strikingly similar.Python:Python has support for generators in the language. In Ruby 1.8 you can use the generator module which uses continuations to create a generator from a block. Or, you could just use a block/proc/lambda! Moreover, in Ruby 1.9 Fibers are, and can be used as, generators, and the Enumerator class is a built-in generator 4docs.python.org has this generator example:Contrast this with the above block examples.In Ruby, when you import a file with require, all the things defined in that file will end up in your global namespace. This causes namespace pollution. The solution to that is Rubys modules. But if you create a namespace with a module, then you have to use that namespace to access the contained classes.In Python, the file is a module, and you can import its contained names with from themodule import *, thereby polluting the namespace if you want. But you can also import just selected names with from themodule import aname, another or you can simply import themodule and then access the names with themodule.aname. If you want more levels in your namespace you can have packages, which are directories with modules and an __init__.py file.Docstrings are strings that are attached to modules, functions and methods and can be

introspected at runtime. This helps for creating such things as the help command and

automatic documentation.Ruby's equivalent are similar to javadocs, and located above the method instead of within it.  They can be retrieved at runtime from the files by using 1.9's Method#source_location example useRuby does not ("on purpose" -- see Ruby's website, see here how it's done in Ruby). It does reuse the module concept as a type of abstract classes.Python:Ruby:Python:Ruby:Python 2.7+:Ruby:Things similar to decorators can also be created in Ruby, and it can also be argued that they aren't as necessary as in Python.Ruby requires "end" or "}" to close all of its scopes, while Python uses white-space only.  There have been recent attempts in Ruby to allow for whitespace only indentation http://github.com/michaeledgar/seamless

2009-07-11 12:32:48Z

There is a lot of discussions of Python vs Ruby, and I all find them completely unhelpful, because they all turn around why feature X sucks in language Y, or that claim language Y doesn't have X, although in fact it does. I also know exactly why I prefer Python, but that's also subjective, and wouldn't help anybody choosing, as they might not have the same tastes in development as I do.It would therefore be interesting to list the differences, objectively. So no "Python's lambdas sucks". Instead explain what Ruby's lambdas can do that Python's can't. No subjectivity. Example code is good!Don't have several differences in one answer, please. And vote up the ones you know are correct, and down those you know are incorrect (or are subjective). Also, differences in syntax is not interesting. We know Python does with indentation what Ruby does with brackets and ends, and that @ is called self in Python.UPDATE: This is now a community wiki, so we can add the big differences here.In Ruby you have a reference to the class (self) already in the class body. In Python you don't have a reference to the class until after the class construction is finished.An example:self in this case is the class, and this code would print out "Kaka". There is no way to print out the class name or in other ways access the class from the class definition body in Python (outside method definitions).This lets you develop extensions to core classes.  Here's an example of a rails extension:Python (imagine there were no ''.startswith method):You could use it on any sequence (not just strings). In order to use it you should import it explicitly e.g., from some_module import starts_with.Ruby has first class regexps, $-variables, the awk/perl line by line input loop and other features that make it more suited to writing small shell scripts that munge text files or act as glue code for other programs.Thanks to the callcc statement. In Python you can create continuations by various techniques, but there is no support built in to the language.With the "do" statement you can create a multi-line anonymous function in Ruby, which will be passed in as an argument into the method in front of do, and called from there. In Python you would instead do this either by passing a method or with generators.Ruby:Python (Ruby blocks correspond to different constructs in Python):OrOrInterestingly, the convenience statement in Ruby for calling a block is called "yield", which in Python will create a generator.Ruby:Python:Although the principles are different, the result is strikingly similar.Python:Python has support for generators in the language. In Ruby 1.8 you can use the generator module which uses continuations to create a generator from a block. Or, you could just use a block/proc/lambda! Moreover, in Ruby 1.9 Fibers are, and can be used as, generators, and the Enumerator class is a built-in generator 4docs.python.org has this generator example:Contrast this with the above block examples.In Ruby, when you import a file with require, all the things defined in that file will end up in your global namespace. This causes namespace pollution. The solution to that is Rubys modules. But if you create a namespace with a module, then you have to use that namespace to access the contained classes.In Python, the file is a module, and you can import its contained names with from themodule import *, thereby polluting the namespace if you want. But you can also import just selected names with from themodule import aname, another or you can simply import themodule and then access the names with themodule.aname. If you want more levels in your namespace you can have packages, which are directories with modules and an __init__.py file.Docstrings are strings that are attached to modules, functions and methods and can be

introspected at runtime. This helps for creating such things as the help command and

automatic documentation.Ruby's equivalent are similar to javadocs, and located above the method instead of within it.  They can be retrieved at runtime from the files by using 1.9's Method#source_location example useRuby does not ("on purpose" -- see Ruby's website, see here how it's done in Ruby). It does reuse the module concept as a type of abstract classes.Python:Ruby:Python:Ruby:Python 2.7+:Ruby:Things similar to decorators can also be created in Ruby, and it can also be argued that they aren't as necessary as in Python.Ruby requires "end" or "}" to close all of its scopes, while Python uses white-space only.  There have been recent attempts in Ruby to allow for whitespace only indentation http://github.com/michaeledgar/seamlessRuby has the concepts of blocks, which are essentially syntactic sugar around a section of code; they are a way to create closures and pass them to another method which may or may not use the block. A block can be invoked later on through a yield statement.For example, a simple definition of an each method on Array might be something like:Then you can invoke this like so:Python has anonymous functions/closures/lambdas, but it doesn't quite have blocks since it's missing some of the useful syntactic sugar. However, there's at least one way to get it in an ad-hoc fashion. See, for example, here.Functions are first-class variables in Python.  You can declare a function, pass it around as an object, and overwrite it:This is a fundamental feature of modern scripting languages.  JavaScript and Lua do this, too.  Ruby doesn't treat functions this way; naming a function calls it.Of course, there are ways to do these things in Ruby, but they're not first-class operations.  For example, you can wrap a function with Proc.new to treat it as a variable--but then it's no longer a function; it's an object with a "call" method.Ruby functions aren't first-class objects.  Functions must be wrapped in an object to pass them around; the resulting object can't be treated like a function.  Functions can't be assigned in a first-class manner; instead, a function in its container object must be called to modify them.Ultimately all answers are going to be subjective at some level, and the answers posted so far pretty much prove that you can't point to any one feature that isn't doable in the other language in an equally nice (if not similar) way, since both languages are very concise and expressive.I like Python's syntax. However, you have to dig a bit deeper than syntax to find the true beauty of Ruby. There is zenlike beauty in Ruby's consistency. While no trivial example can possibly explain this completely, I'll try to come up with one here just to explain what I mean.Reverse the words in this string: When you think about how you would do it, you'd do the following:In Ruby, you'd do this:Exactly as you think about it, in the same sequence, one method call after another. In python, it would look more like this:It's not hard to understand, but it doesn't quite have the same flow. The subject (sentence) is buried in the middle. The operations are a mix of functions and object methods. This is a trivial example, but one discovers many different examples when really working with and understanding Ruby, especially on non-trivial tasks.Python has a "we're all adults here" mentality.  Thus, you'll find that Ruby has things like constants while Python doesn't (although Ruby's constants only raise a warning).  The Python way of thinking is that if you want to make something constant, you should put the variable names in all caps and not change it.For example, Ruby:Python:You can import only specific functions from a module in Python. In Ruby, you import the whole list of methods. You could "unimport" them in Ruby, but it's not what it's all about.EDIT:let's take this Ruby module :if you include it in your code :you'll see that both method1 and method2 have been added to your namespace. You can't import only method1. You either import them both or you don't import them at all. In Python you can import only the methods of your choosing. If this would have a name maybe it would be called selective importing?From Ruby's website:Similarities

As with Python, in Ruby,...Differences

Unlike Python, in Ruby,...What Ruby has over Python are its scripting language capabilities. Scripting language in this context meaning to be used for "glue code" in shell scripts and general text manipulation.These are mostly shared with Perl. First-class built-in regular expressions, $-Variables, useful command line options like Perl (-a, -e) etc.Together with its terse yet epxressive syntax it is perfect for these kind of tasks.Python to me is more of a dynamically typed business language that is very easy to learn and has a neat syntax. Not as "cool" as Ruby but neat.

What Python has over Ruby to me is the vast number of bindings for other libs. Bindings to Qt and other GUI libs, many game support libraries and and and. Ruby has much less. While much used bindings e.g. to Databases are of good quality I found niche libs to be better supported in Python even if for the same library there is also a Ruby binding.So, I'd say both languages have its use and it is the task that defines which one to use. Both are easy enough to learn. I use them side-by-side. Ruby for scripting and Python for stand-alone apps.I don't think "Ruby has X and Python doesn't, while Python has Y and Ruby doesn't" is the most useful way to look at it. They're quite similar languages, with many shared abilities. To a large degree, the difference is what the language makes elegant and readable. To use an example you brought up, both do theoretically have lambdas, but Python programmers tend to avoid them, and constructs made using them do not look anywhere near as readable or idiomatic as in Ruby. So in Python, a good programmer will want to take a different route to solving the problem than he would in Ruby, just because it actually is the better way to do it.I'd like to suggest a variant of the original question, "What does Ruby have that Python doesn't, and vice versa?" which admits the disappointing answer, "Well, what can you do with either Ruby or Python that can't be done in Intercal?" Nothing on that level, because Python and Ruby are both part of the vast royal family sitting on the throne of being Turing approximant.But what about this:What can be done gracefully and well in Python that can't be done in Ruby with such beauty and good engineering, or vice versa?That may be much more interesting than mere feature comparison.Python has an explicit, builtin syntax for list-comprehenions and generators whereas in Ruby you would use map and code blocks.Compareto"Variables that start with a capital letter becomes constants and can't be modified"Wrong. They can.You only get a warning if you do.Somewhat more on the infrastructure side:Shamelessly copy/pasted from: Alex Martelli answer on "What's better about Ruby than Python" thread from comp.lang.python mailing list. Some others from:http://www.ruby-lang.org/en/documentation/ruby-from-other-languages/to-ruby-from-python/(If I have misintrepreted anything or any of these have changed on the Ruby side since that page was updated, someone feel free to edit...)Strings are mutable in Ruby, not in Python (where new strings are created by "changes").Ruby has some enforced case conventions, Python does not.Python has both lists and tuples (immutable lists). Ruby has arrays corresponding to Python lists, but no immutable variant of them.In Python, you can directly access object attributes.  In Ruby, it's always via methods.In Ruby, parentheses for method calls are usually optional, but not in Python.Ruby has public, private, and protected to enforce access, instead of Python’s convention of using underscores and name mangling.Python has multiple inheritance.  Ruby has "mixins."And another very relevant link:http://c2.com/cgi/wiki?PythonVsRubyWhich, in particular, links to another good one by Alex Martelli, who's been also posting a lot of great stuff here on SO:http://groups.google.com/group/comp.lang.python/msg/028422d707512283I'm unsure of this, so I add it as an answer first.That means you can call a method either like theobject.themethod() or by TheClass.themethod(anobject).Edit: Although the difference between methods and functions is small in Python, and non-existant in Python 3, it also doesn't exist in Ruby, simply because Ruby doesn't have functions. When you define functions, you are actually defining methods on Object.But you still can't take the method of one class and call it as a function, you would have to rebind it to the object you want to call on, which is much more obstuse.I would like to mention Python descriptor API that allows one customize object-to-attribute "communication". It is also noteworthy that, in Python, one is free to implement an alternative protocol via overriding the default given through the default implementation of the __getattribute__ method.

Let me give more details about the aforementioned.

Descriptors are regular classes with __get__, __set__ and/or __delete__ methods.

When interpreter encounters something like anObj.anAttr, the following is performed:As was mentioned, this is the default behavior. One is free to change the protocol by re-implementing __getattribute__.This technique is lot more powerful than decorators.Ruby has builtin continuation support using callcc.Hence you can implement cool things like the amb-operatorAt this stage, Python still has better unicode supportPython has docstrings and ruby doesn't... Or if it doesn't, they are not accessible as easily as in python. Ps. If im wrong, pretty please, leave an example?  I have a workaround that i could monkeypatch into classes quite easily but i'd like to have docstring kinda of a feature in "native way".Ruby has a line by line loop over input files (the '-n' flag) from the commandline so it can be used like AWK. This Ruby one-liner: will count lines like the AWK one-liner:Ruby gets feature this through Perl, which took it from AWK as a way of getting sysadmins on board with Perl without having to change the way they do things.Ruby has sigils and twigils, Python doesn't.Edit: And one very important thing that I forgot (after all, the previous was just to flame a little bit :-p):Python has a JIT compiler (Psyco), a sightly lower level language for writing faster code (Pyrex) and the ability to add inline C++ code (Weave).My python's rusty, so some of these may be in python and i just don't remember/never learned in the first place, but here are the first few that I thought of:Ruby handles whitespace completely different. For starters, you don't need to indent anything (which means it doesn't matter if you use 4 spaces or 1 tab). It also does smart line continuation, so the following is valid:Basically, if you end with an operator, it figures out what is going on.Ruby has mixins which can extend instances instead of full classes:I'm not sure if this is the same as generators, but as of Ruby 1.9 ruby as enums, so Reference: http://blog.nuclearsquid.com/writings/ruby-1-9-what-s-new-what-s-changedBoth of the items listed there are supported in Ruby, although you can't skip default values like that.

You can either go in orderNote that c=5 actually assigns the variable c in the calling scope the value 5, and sets the parameter b the value 5.or you can do it with hashes, which address the second issueReference: The Pragmatic Progammer's Guide to RubyYou can have code in the class definition in both Ruby and Python. However, in Ruby you have a reference to the class (self). In Python you don't have a reference to the class, as the class isn't defined yet.An example:self in this case is the class, and this code would print out "Kaka". There is no way to print out the class name or in other ways access the class from the class definition body in Python.Syntax is not a minor thing, it has a direct impact on how we think.  It also has a direct effect on the rules we create for the systems we use.  As an example we have the order of operations because of the way we write mathematical equations or sentences.  The standard notation for mathematics allows people to read it more than one way and arrive at different answers given the same equation.  If we had used prefix or postfix notation we would have created rules to distinguish what the numbers to be manipulated were rather than only having rules for the order in which to compute values.The standard notation makes it plain what numbers we are talking about while making the order in which to compute them ambiguous.  Prefix and postfix notation make the order in which to compute plain while making the numbers ambiguous.  Python would already have multiline lambdas if it were not for the difficulties caused by the syntactic whitespace.  (Proposals do exist for pulling this kind of thing off without necessarily adding explicit block delimiters.)I find it easier to write conditions where I want something to occur if a condition is false much easier to write with the unless statement in Ruby than the semantically equivalent "if-not" construction in Ruby or other languages for example.  If most of the languages that people are using today are equal in power, how can the syntax of each language be considered a trivial thing?  After specific features like blocks and inheritance mechanisms etc.  syntax is the most important part of a language,hardly a superficial thing.What is superficial are the aesthetic qualities of beauty that we ascribe to syntax.  Aesthetics have nothing to do with how our cognition works, syntax does.Surprised to see nothing mentioned of ruby's "method missing" mechanism.  I'd give examples of the find_by_... methods in Rails, as an example of the power of that language feature.  My guess is that something similar could be implemented in Python, but to my knowledge it isn't there natively.Another difference in lambdas between Python and Ruby is demonstrated by Paul Graham's Accumulator Generator problem.  Reprinted here:In Ruby, you can do this:In Python, you'd create an object to hold the state of n:Some folks might prefer the explicit Python approach as being clearer conceptually, even if it's a bit more verbose.  You store state like you do for anything else.  You just need to wrap your head around the idea of callable objects.  But regardless of which approach one prefers aesthetically, it does show one respect in which Ruby lambdas are more powerful constructs than Python's.python has named optional argumentsAFAIK Ruby has only positioned arguments because b=2 in the function declaration is an affectation that always append.Ruby has embedded documentation:http://c2.com/cgi/wiki?PythonVsRuby

http://c2.com/cgi/wiki?SwitchedFromPythonToRuby

http://c2.com/cgi/wiki?SwitchedFromRubyToPython

http://c2.com/cgi/wiki?UsingPythonDontNeedRuby

http://c2.com/cgi/wiki?UsingRubyDontNeedPython  With Cargo you can "require libraries without cluttering your namespace".

libxml install error using pip

zjm1126

[libxml install error using pip](https://stackoverflow.com/questions/5178416/libxml-install-error-using-pip)

This is my error:What can I do?updated:the log:

2011-03-03 08:45:48Z

This is my error:What can I do?updated:the log:From the lxml documentation, assuming you are running a Debian-based distribution :For Debian based systems, it should be enough to install the known build dependencies of python-lxml or python3-lxml, e.g.This worked for me:In case, you are using Ubuntu/Lubuntu 13.04 or Ubuntu 13.10 and having problem with "/usr/bin/ld: cannot find -lz", you may need also install zlib1g-dev package:Put it all together:No you are missing the Python header files. This mostly happens on Linux when you are using the system Python (there are reasons not to do that, but that's a different question).You probably need to install some package, and it's probably called python-dev or python-devel.orOr somesuch.I solved this issue by increasing my server ram.I was running only 512 MB and when I upgraded to 1 GB I had no problem.I also installed every package manually prior to this in an attempt to fix the problem, but I'm not sure whether this is a necessary step.On Windows  I had the same error on windows when trying to manually install in Python 3.4 after it had been installed on 3.3.  I finally was able to solve it by installing the wheel and running pip from the Python34 directory.1) download wheel from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml2) cd Python34\Scripts3) pip.exe C:\Users\Home\Downloads\lxml- ......... .whljust install requirements:Now, you can install it with pip package management tool:I am using Ubuntu 14.04 and this solves the issue for meInstalling a lxml binary would do the trick. Check thisand then try upgrading python setuptoolsthis should resolve it.I was having this issue with a pip install of lxml. My CentOS instance was using python 2.6 which was throwing this error.To get around this I did the following to run with Python 2.7:pymemcompat.h:10: fatal error: Python.h: 没有那个文件或目录 Boy you should post your error log with LANG=C or people can't get the real cause from your log. The log above says: No such file or directory.That means you should install the develop package of Python. That's usually "python-dev" on Debian flavored distro, and "python-devel" on RHEL flavored distro.All the answers above assume the user has access to a privileged/root account to install the required libraries. To install it locally you will need to do the following steps. Only showed the overview since the steps can get a little involved depending on the dependencies that you might be missing 1.Download and Compile libxml2-2.9.1 & libxslt-1.1.28 (versions might change) 2.Configure each install path for both libxml and libxslt to be some local directory using configure. Ex. ./configure --prefix=/home_dir/dependencies/libxslt_path3.Run make then make install4.Download and compile lxml from sourceThis works for me, 12.04, python2.7.6I know I am late to the show,But this will help if nothing else works outSetting TMPDIR will ensure that the build can run everything that it needs to run, and setting STATIC_DEPS will tell it to pull in its own libxml2 and

libxslt2, instead of using the old versions installed on the server.Using cygwin 64 with Windows 8. I have got...I have tried everything until I realized a new cygwin toolchain has messed up python logic. cygwin install a compiler called "realgcc" that is not a real gcc. SolutionInstall gcc. Ex:The only thing helped for me was I got the same error on my Linux machine.If you go to their website that's in their documentation as well.So if you get this kind of error in Linux machine, please try out these commands,# apt install libxml2-dev libxslt-dev python-dev# pip install lxml==3.4.4I am using Ubuntu 12, and this works for me:If you have installed the libxml2 and libxslt, maybe you need to create a symbolic link between libxml2 and libxslt path to python2.6 include path. Also you can try to add INCLUDE environment argument.

Because the gcc command only search this path: -I/usr/include/python2.6.The below file worked for me on windows 

https://pypi.python.org/pypi/lxml/3.3.3#downloadsOn osx 10.10.5 and in a virtualenv, maybe you can resolve that problem like below:I work on a Windows machine. And here are some pointers for successful installation of lxml (with python 2.6 and later).Have the following installed:All are not available at a pip install.libxml2's windows binary is found here.libxslt is found here.After you are done with the above two,do : pip install lxml.Another workaround is using the stable releases from PyPI or the unofficial Windows binaries by Christoph Gohlke (found here).Using Windows 7 with Cygwin, I came across:I fixed it by installing mingw64-x86_64-libxsltFor Windows:

Strip HTML from strings in Python

directedition

[Strip HTML from strings in Python](https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python)

When printing a line in an HTML file, I'm trying to find a way to only show the contents of each HTML element and not the formatting itself. If it finds '<a href="whatever.com">some text</a>', it will only print 'some text', '<b>hello</b>' prints 'hello', etc. How would one go about doing this?

2009-04-15 18:24:26Z

When printing a line in an HTML file, I'm trying to find a way to only show the contents of each HTML element and not the formatting itself. If it finds '<a href="whatever.com">some text</a>', it will only print 'some text', '<b>hello</b>' prints 'hello', etc. How would one go about doing this?I always used this function to strip HTML tags, as it requires only the Python stdlib:On Python 2For Python 3Note: this works only for 3.1. For 3.2 or above, you need to call the parent class's init function. See Using HTMLParser in Python 3.2I haven't thought much about the cases it will miss, but you can do a simple regex:For those that don't understand regex, this searches for a string <...>, where the inner content is made of one or more (+) characters that isn't a <. The ? means that it will match the smallest string it can find. For example given <p>Hello</p>, it will match <'p> and </p> separately with the ?. Without it, it will match the entire string <..Hello..>.If non-tag < appears in html (eg. 2 < 3), it should be written as an escape sequence &... anyway so the ^< may be unnecessary.Why all of you do it the hard way?

You can use BeautifulSoup get_text() feature.Regex source: MarkupSafe.  Their version handles HTML entities too, while this quick one doesn't.It's one thing to keep people from <i>italicizing</i> things, without leaving is floating around.  But it's another to take arbitrary input and make it completely harmless.  Most of the techniques on this page will leave things like unclosed comments (<!--) and angle-brackets that aren't part of tags (blah <<<><blah) intact.  The HTMLParser version can even leave complete tags in, if they're inside an unclosed comment.What if your template is {{ firstname }} {{ lastname }}?  firstname = '<a' and lastname = 'href="http://evil.com/">' will be let through by every tag stripper on this page (except @Medeiros!), because they're not complete tags on their own.  Stripping out normal HTML tags is not enough.Django's strip_tags, an improved (see next heading) version of the top answer to this question, gives the following warning:Follow their advice!It's easy to circumvent the top answer to this question.Look at this string (source and discussion):The first time HTMLParser sees it, it can't tell that the <img...> is a tag.  It looks broken, so HTMLParser doesn't get rid of it.  It only takes out the <!-- comments -->, leaving you withThis problem was disclosed to the Django project in March, 2014.  Their old strip_tags was essentially the same as the top answer to this question.  Their new version basically runs it in a loop until running it again doesn't change the string:Of course, none of this is an issue if you always escape the result of strip_tags().Update 19 March, 2015: There was a bug in Django versions before 1.4.20, 1.6.11, 1.7.7, and 1.8c1.  These versions could enter an infinite loop in the strip_tags() function.  The fixed version is reproduced above.  More details here.My example code doesn't handle HTML entities - the Django and MarkupSafe packaged versions do.My example code is pulled from the excellent MarkupSafe library for cross-site scripting prevention.  It's convenient and fast (with C speedups to its native Python version).  It's included in Google App Engine, and used by Jinja2 (2.7 and up), Mako, Pylons, and more.  It works easily with Django templates from Django 1.7.Django's strip_tags and other html utilities from a recent version are good, but I find them less convenient than MarkupSafe.  They're pretty self-contained, you could copy what you need from this file.If you need to strip almost all tags, the Bleach library is good.  You can have it enforce rules like "my users can italicize things, but they can't make iframes."Understand the properties of your tag stripper!  Run fuzz tests on it!  Here is the code I used to do the research for this answer.sheepish note - The question itself is about printing to the console, but this is the top Google result for "python strip html from string", so that's why this answer is 99% about the web.I needed a way to strip tags and decode HTML entities to plain text. The following solution is based on Eloff's answer (which I couldn't use because it strips entities).A quick test:Result:Error handling:Security note: Do not confuse HTML stripping (converting HTML into plain text) with HTML sanitizing (converting plain text into HTML). This answer will remove HTML and decode entities into plain text – that does not make the result safe to use in a HTML context.Example: &lt;script&gt;alert("Hello");&lt;/script&gt; will be converted to <script>alert("Hello");</script>, which is 100% correct behavior, but obviously not sufficient if the resulting plain text is inserted as-is into a HTML page.The rule is not hard: Any time you insert a plain-text string into HTML output, you should always HTML escape it (using cgi.escape(s, True)), even if you "know" that it doesn't contain HTML (e.g. because you stripped HTML content).(However, the OP asked about printing the result to the console, in which case no HTML escaping is needed.)Python 3.4+ version: (with doctest!)Note that HTMLParser has improved in Python 3 (meaning less code and better error handling).There's a simple way to this:The idea is explained here: http://youtu.be/2tu9LTDujbwYou can see it working here: http://youtu.be/HPkNPcYed9M?t=35sPS - If you're interested in the class(about smart debugging with python) I give you a link: http://www.udacity.com/overview/Course/cs259/CourseRev/1. It's free! You're welcome! :)If you need to preserve HTML entities (i.e. &amp;), I added "handle_entityref" method to Eloff's answer.If you want to strip all HTML tags the easiest way I found is using BeautifulSoup:I tried the code of the accepted answer but I was getting  "RuntimeError: maximum recursion depth exceeded", which didn't happen with the above block of code.An lxml.html-based solution (lxml is a native library and therefore much faster than any pure python solution). If you need more control over what exactly is sanitized before converting to text then you might want to use the lxml Cleaner explicitly by passing the options you want in the constructor, e.g: The Beautiful Soup package does this immediately for you. Here is a simple solution that strips HTML tags and decodes HTML entities based on the amazingly fast lxml library:Here's my solution for python 3.Not sure if it is perfect, but solved my use case and seems simple. You can use either a different HTML parser (like lxml, or Beautiful Soup) -- one that offers functions to extract just text. Or, you can run a regex on your line string that strips out the tags. See Python docs for more.I have used Eloff's answer successfully for Python 3.1 [many thanks!].I upgraded to Python 3.2.3, and ran into errors. The solution, provided here thanks to the responder Thomas K, is to insert super().__init__() into the following code:... in order to make it look like this:... and it will work for Python 3.2.3.Again, thanks to Thomas K for the fix and for Eloff's original code provided above!You can write your own function:The solutions with HTML-Parser are all breakable, if they run only once:results in:what you intend to prevent. if you use a HTML-Parser, count the Tags until zero are replaced:This is a quick fix and can be even more optimized but it will work fine. This code will replace all non empty tags with "" and strips all html tags form a given input text .You can run it using ./file.py input outputA python 3 adaption of søren-løvborg's answerFor one project, I needed so strip HTML, but also css and js. Thus, I made a variation of Eloffs answer:Here's a solution similar to the currently accepted answer (https://stackoverflow.com/a/925630/95989), except that it uses the internal HTMLParser class directly (i.e. no subclassing), thereby making it significantly more terse:I'm parsing Github readmes and I find that the following really works well:And thenRemoves all markdown and html correctly.    Using BeautifulSoup, html2text or the code from @Eloff, most of the time, it remains some html elements, javascript code...So you can use a combination of these libraries and delete markdown formatting (Python 3):It works well for me but it can be enhanced, of course...Simple code!. This will remove all kind of tags and content inside of it.But it won't give full result if text contains <> symbols inside it.hext is a package that among other things can strip HTML. It is an alternative to beautifulsoup. The following was tested with hext==0.2.3.Save this in a utility module, e.g. util/hext.py:Usage examples:Usage examples with malformed HTML:This method works flawlessly for me and requires no additional installations:

Convert Python dict into a dataframe

anonuser0428

[Convert Python dict into a dataframe](https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe)

I have a Python dictionary like the following:The keys are Unicode dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.So far I have tried converting the dict into a series object but this doesn't seem to maintain the relationship between the columns:

2013-09-16 21:02:22Z

I have a Python dictionary like the following:The keys are Unicode dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)Any help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.I know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.So far I have tried converting the dict into a series object but this doesn't seem to maintain the relationship between the columns:The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):You could take the items from the dictionary (i.e. the key-value pairs):But I think it makes more sense to pass the Series constructor:When converting a dictionary into a pandas dataframe where you want the keys to be the columns of said dataframe and the values to be the row values, you can do simply put brackets around the dictionary like this:It's saved me some headaches so I hope it helps someone out there!As explained on another answer using pandas.DataFrame() directly here will not act as you think.What you can do is use pandas.DataFrame.from_dict with orient='index': Pass the items of the dictionary to the DataFrame constructor, and give the column names. After that parse the Date column to get Timestamp values.Note the difference between python 2.x and 3.x:In python 2.x:In Python 3.x: (requiring an additional 'list')p.s. in particular, I've found Row-Oriented examples helpful; since often that how records are stored externally.https://pbpython.com/pandas-list-dict.htmlPandas have built-in function for conversion of dict to data frame.For your data you can convert it like below:In my case I wanted keys and values of a dict to be columns and values of DataFrame. So the only thing that worked for me was:You can also just pass the keys and values of the dictionary to the new dataframe, like so:Accepts a dict as argument and returns a dataframe with the keys of the dict as index and values as a column.This is how it worked for me : I hope this helpsThis is what worked for me, since I wanted to have a separate index columnIf you don't encapsulate yourDict.keys() inside of list() , then you will end up with all of your keys and values being placed in every row of every column. Like this:Date  \

0   (2012-06-08, 2012-06-09, 2012-06-10, 2012-06-1...

1   (2012-06-08, 2012-06-09, 2012-06-10, 2012-06-1...

2   (2012-06-08, 2012-06-09, 2012-06-10, 2012-06-1...

3   (2012-06-08, 2012-06-09, 2012-06-10, 2012-06-1...

4   (2012-06-08, 2012-06-09, 2012-06-10, 2012-06-1...But by adding list() then the result looks like this:Date  Date_Values

0   2012-06-08          388

1   2012-06-09          388

2   2012-06-10          388

3   2012-06-11          389

4   2012-06-12          389

...I have run into this several times and have an example dictionary that I created from a function get_max_Path(),  and it returns the sample dictionary:{2: 0.3097502930247044,

 3: 0.4413177909384636,

 4: 0.5197224051562838,

 5: 0.5717654946470984,

 6: 0.6063959031223476,

 7: 0.6365209824708223,

 8: 0.655918861281035,

 9: 0.680844386645206}To convert this to a dataframe, I ran the following:df = pd.DataFrame.from_dict(get_max_path(2), orient = 'index').reset_index()Returns a simple two column dataframe with a separate index:index  0

0   2   0.309750

1   3   0.441318Just rename the columns using f.rename(columns={'index': 'Column1', 0: 'Column2'}, inplace=True)I think that you can make some changes in your data format when you create dictionary, then you can easily convert it to DataFrame:input:output:input:output: will be your DataFrameYou just need to use some text editing in somewhere like Sublime or maybe Excel.

How can I read inputs as numbers?

thefourtheye

[How can I read inputs as numbers?](https://stackoverflow.com/questions/20449427/how-can-i-read-inputs-as-numbers)

Why are x and y strings instead of ints in the below code?(Note: in Python 2.x use raw_input(). In Python 3.x use input(). raw_input() was renamed to input() in Python 3.x)

2013-12-08 03:08:15Z

Why are x and y strings instead of ints in the below code?(Note: in Python 2.x use raw_input(). In Python 3.x use input(). raw_input() was renamed to input() in Python 3.x)TLDRPython 2.xThere were two functions to get user input, called input and raw_input. The difference between them is, raw_input doesn't evaluate the data and returns as it is, in string form. But, input will evaluate whatever you entered and the result of evaluation will be returned. For example,The data 5 + 17 is evaluated and the result is 22. When it evaluates the expression 5 + 17, it detects that you are adding two numbers and so the result will also be of the same int type. So, the type conversion is done for free and 22 is returned as the result of input and stored in data variable. You can think of input as the raw_input composed with an eval call.Note: you should be careful when you are using input in Python 2.x. I explained why one should be careful when using it, in this answer.But, raw_input doesn't evaluate the input and returns as it is, as a string.Python 3.xPython 3.x's input and Python 2.x's raw_input are similar and raw_input is not available in Python 3.x. SolutionTo answer your question, since Python 3.x doesn't evaluate and convert the data type, you have to explicitly convert to ints, with int, like thisYou can accept numbers of any base and convert them directly to base-10 with the int function, like thisThe second parameter tells what is the base of the numbers entered and then internally it understands and converts it. If the entered data is wrong it will throw a ValueError.For values that can have a fractional component, the type would be float rather than int:Apart from that, your program can be changed a little bit, like thisYou can get rid of the play variable by using break and while True. In Python 3.x, raw_input was renamed to input and the Python 2.x input was removed.  This means that, just like raw_input, input in Python 3.x always returns a string object.To fix the problem, you need to explicitly make those inputs into integers by putting them in int:For multiple integer in a single line, map might be better.If the number is already known, (like 2 integers), you can useinput() (Python 3) and raw_input() (Python 2) always return strings. Convert the result to integer explicitly with int().Multiple questions require input for several integers on single line.  The best way is to input the whole string of numbers one one line and then split them to integers. Here is a Python 3 version:Convert to integers:Similarly for floating point numbers:the for loop shall run 'n' number of times . the second 'n' is the length of the array.

the last statement maps the integers to a list and takes input in space separated form .

you can also return the array at the end of for loop.I encountered a problem of taking integer input while solving a problem on CodeChef, where two integers - separated by space - should be read from one line.While int(input()) is sufficient for a single integer, I did not find a direct way to input two integers.  I tried this:Now I use num1 and  num2 as integers.  Hope this helps.While in your example, int(input(...)) does the trick in any case, python-future's builtins.input is worth consideration since that makes sure your code works for both Python 2 and 3 and disables Python2's default behaviour of input trying to be "clever" about the input data type (builtins.input basically just behaves like raw_input).

How to fix Python indentation

Shay Erlichmen

[How to fix Python indentation](https://stackoverflow.com/questions/1024435/how-to-fix-python-indentation)

I have some Python code that have inconsistent indentation. There is a lot of mixture of tabs and spaces to make the matter even worse, and even space indentation is not preserved.The code works as expected, but it's difficult to maintain.How can I fix the indentation (like HTML Tidy, but for Python) without breaking the code?

2009-06-21 18:10:18Z

I have some Python code that have inconsistent indentation. There is a lot of mixture of tabs and spaces to make the matter even worse, and even space indentation is not preserved.The code works as expected, but it's difficult to maintain.How can I fix the indentation (like HTML Tidy, but for Python) without breaking the code?Use the reindent.py script that you find in the Tools/scripts/ directory of your Python installation:Have a look at that script for detailed usage instructions.If you're using Vim, see :h retab.For example, if you simply typeall your tabs will be expanded into spaces.You may want toto make sure that any new lines will not use literal tabs.If you're not using Vim,will replace tabs with spaces, assuming tab stops every 8 characters, in file.py (with the original going to file.py.bak, just in case).  Replace the 8s with 4s if your tab stops are every 4 spaces instead.I would reach for autopep8 to do this:Note: E101 and E121 are pep8 indentation (I think you can simply pass --select=E1 to fix all indentation related issues - those starting with E1).You can apply this to your entire project using recursive flag:See also Tool to convert Python code to be PEP8 compliant.Use autopep8autopep8 automagically formats Python code to conform to the PEP 8

 nullstyle guide. It uses the pep8 utility to determine what parts of the

 nullcode needs to be formatted. autopep8 is capable of fixing most of the

 nullformatting issues that can be reported by pep8.Using Vim, it shouldn't be more involved than hitting Esc, and then typing......on the file you want to change. That will convert all tabs to four spaces. If you have inconsistent spacing as well, then that will be more difficult.There is also PythonTidy (since you said you like HTML Tidy).It can do a lot more than just clean up tabs though. If you like that type of thing, it's worth a look.On most UNIX-like systems, you can also run:from the command line, changing the number if you want to replace tabs with a number of spaces other than 4. You can easily write a shell script to do this with a bunch of files at once, retaining the original file names.If you're lazy (like me), you can also download a trial version of Wingware Python IDE, which has an auto-fix tool for messed up indentation. It works pretty well.

http://www.wingware.com/The reindent script did not work for me, due to some missing module. Anyway, I found this sed command which does the job perfect for me:Try Emacs. It has good support for indentation needed in Python. Please check this link http://python.about.com/b/2007/09/24/emacs-tips-for-python-programmers.htmTry IDLE, and use Alt + X to find indentation.I have a simple solution for this problem. You can first type ":retab" and then ":retab!", then everything would be fine

Combining two Series into a DataFrame in pandas

user7289

[Combining two Series into a DataFrame in pandas](https://stackoverflow.com/questions/18062135/combining-two-series-into-a-dataframe-in-pandas)

I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?

2013-08-05 15:37:39Z

I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?I think concat is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):Note: This extends to more than 2 Series.Why don't you just use .to_frame if both have the same indexes?>= v0.23< v0.23Pandas will automatically align these passed in series and create the joint index

They happen to be the same here. reset_index moves the index to a column.Example code:Pandas allows you to create a DataFrame from a dict with Series as the values and the column names as the keys. When it finds a Series as a value, it uses the Series index as part of the DataFrame index. This data alignment is one of the main perks of Pandas. Consequently, unless you have other needs, the freshly created DataFrame has duplicated value. In the above example, data['idx_col'] has the same data as data.index.If I may answer this.The fundamentals behind converting series to data frame is to understand that1. At conceptual level, every column in data frame is a series.2. And, every column name is a key name that maps to a series.If you keep above two concepts in mind, you can think of many ways to convert series to data frame.

One easy solution will be like this:Create two series hereCreate an empty data frame with just desired column namesPut series value inside data frame using mapping conceptCheck results nowNot sure I fully understand your question, but is this what you want to do?(index=s1.index is not even necessary here)A simplification of the solution based on join():

How to pretty print nested dictionaries?

sth

[How to pretty print nested dictionaries?](https://stackoverflow.com/questions/3229419/how-to-pretty-print-nested-dictionaries)

How can I pretty print a dictionary with depth of ~4 in Python? I tried pretty printing with pprint(), but it did not work:I simply want an indentation ("\t") for each nesting, so that I get something like this:etc. How can I do this?

2010-07-12 14:32:46Z

How can I pretty print a dictionary with depth of ~4 in Python? I tried pretty printing with pprint(), but it did not work:I simply want an indentation ("\t") for each nesting, so that I get something like this:etc. How can I do this?I'm not sure how exactly you want the formatting to look like, but you could start with a function like this:My first thought was that the JSON serializer is probably pretty good at nested dictionaries, so I'd cheat and use that:You could try YAML via PyYAML.  Its output can be fine-tuned.  I'd suggest starting with the following:print yaml.dump(data, allow_unicode=True, default_flow_style=False)The result is very readable; it can be also parsed back to Python if needed.Edit:Example:As of what have been done, I don't see any pretty printer that at least mimics the output of the python interpreter with very simple formatting so here's mine :To initialize it :It can support the addition of formatters for defined types, you simply need to make a function for that like this one and bind it to the type you want with set_formater :For historical reasons, I keep the previous pretty printer which was a function instead of a class, but they both can be used the same way, the class version simply permit much more :To use it :Compared to other versions :by this way you can print it in pretty way forexample your dictionary name is yasinAnother option with yapf:  Output:As others have posted, you can use recursion/dfs to print the nested dictionary data and call recursively if it is a dictionary; otherwise print the data.I took sth's answer and modified it slightly to fit my needs of a nested dictionaries and lists:Which then gives me output like:pout can pretty print anything you throw at it, for example (borrowing data from another answer):would result in output printed to the screen like:or you can return the formatted string output of your object:Its primary use case is for debugging so it doesn't choke on object instances or anything and it handles unicode output as you would expect, works in python 2.7 and 3.disclosure: I'm the author and maintainer of pout.Sth, i sink that's pretty ;)I wrote this simple code to print the general structure of a json object in Python. the result for the following datais very compact and looks like this:I'm a relative python newbie myself but I've been working with nested dictionaries for the past couple weeks and this is what I had came up with.You should try using a stack. Make the keys from the root dictionary into a list of a list:Going in reverse order from last to first, lookup each key in the dictionary to see if its value is (also) a dictionary. If not, print the key then delete it. However if the value for the key is a dictionary, print the key then append the keys for that value to the end of the stack, and start processing that list in the same way, repeating recursively for each new list of keys.If the value for the second key in each list were a dictionary you would have something like this after several rounds:The upside to this approach is that the indent is just \t times the length of the stack:The downside is that in order to check each key you need to hash through to the relevant sub-dictionary, though this can be handled easily with a list comprehension and a simple for loop:Be aware that this approach will require you to cleanup trailing empty lists, and to delete the last key in any list followed by an empty list (which of course may create another empty list, and so on).There are other ways to implement this approach but hopefully this gives you a basic idea of how to do it.EDIT: If you don't want to go through all that, the pprint module prints nested dictionaries in a nice format.Here's a function I wrote based on what sth's comment. It's works the same as json.dumps with indent, but I'm using tabs instead of space for indents. In Python 3.2+ you can specify indent to be a '\t' directly, but not in 2.7.Ex:Here's something that will print any sort of nested dictionary, while keeping track of the "parent" dictionaries along the way. This is a good starting point for printing according to different formats, like the one specified in OP. All you really need to do is operations around the Print blocks. Note that it looks to see if the value is 'OrderedDict()'. Depending on whether you're using something from Container datatypes Collections, you should make these sort of fail-safes so the elif block doesn't see it as an additional dictionary due to its name. As of now, an example dictionary like will printUsing the same example code, it will print the following: This isn't exactly what is requested in OP. The difference is that a parent^n is still printed, instead of being absent and replaced with white-space. To get to OP's format, you'll need to do something like the following: iteratively compare dicList with the lastDict. You can do this by making a new dictionary and copying dicList's content to it, checking if i in the copied dictionary is the same as i in lastDict, and -- if it is -- writing whitespace to that i position using the string multiplier function.From this link:I'm just returning to this question after taking sth's answer and making a small but very useful modification. This function prints all keys in the JSON tree as well as the size of leaf nodes in that tree.It's really nice when you have large JSON objects and want to figure out where the meat is. Example:This would tell you that most of the data you care about is probably inside JSON_object['key1']['key2']['value2'] because the length of that value formatted as a string is very large.Use this function:Call it like this:This is what I came up with while working on a class that needed to write a dictionary in a .txt file:Now, if we have a dictionary like this:And we do:We get:

Ignore python multiple return value

Mat

[Ignore python multiple return value](https://stackoverflow.com/questions/431866/ignore-python-multiple-return-value)

Say I have a Python function that returns multiple values in a tuple:Is there a nice way to ignore one of the results rather than just assigning to a temporary variable? Say if I was only interested in the first value, is there a better way than this:

2009-01-10 22:12:49Z

Say I have a Python function that returns multiple values in a tuple:Is there a nice way to ignore one of the results rather than just assigning to a temporary variable? Say if I was only interested in the first value, is there a better way than this:One common convention is to use a "_" as a variable name for the elements of the tuple you wish to ignore. For instance:You can use x = func()[0] to return the first value, x = func()[1] to return the second, and so on.If you want to get multiple values at a time, use something like x, y = func()[2:4].If you're using Python 3, you can you use the star before a variable (on the left side of an assignment) to have it be a list in unpacking.Remember, when you return more than one item, you're really returning a tuple. So you can do things like this:The common practice is to use the dummy variable _ (single underscore), as many have indicated here before.However, to avoid collisions with other uses of that variable name (see this response) it might be a better practice to use __ (double underscore) instead as a throwaway variable, as pointed by ncoghlan. E.g.:Three simple choices.ObviousHideousAnd there are ways to do this with a decorator.The best solution probably is to name things instead of returning meaningless tuples. Unless there is some logic behind the order of the returned items. You could even use namedtuple if you want to add extra information about what you are returning: If the things you return are often together then it may be a good idea to even define a class for it.This seems like the best choice to me:It's not cryptic or ugly (like the func()[index] method), and clearly states your purpose.This is not a direct answer to the question. Rather it answers this question: "How do I choose a specific function output from many possible options?".If you are able to write the function (ie, it is not in a library you cannot modify), then add an input argument that indicates what you want out of the function. Make it a named argument with a default value so in the "common case" you don't even have to specify it.This method gives the function "advanced warning" regarding the desired output. Consequently it can skip unneeded processing and only do the work necessary to get your desired output. Also because Python does dynamic typing, the return type can change. Notice how the example returns a scalar, a list or a tuple... whatever you like!If this is a function that you use all the time but always discard the second argument, I would argue that it is less messy to create an alias for the function without the second return value using lambda. When you have many output from a function and you don't want to call it multiple times, I think the clearest way for selecting the results would be :As a minimum working example, also demonstrating that the function is called only once :And the values are as expected :For convenience, Python list indexes can also be used :Returns : a = 3 and b = 12

How can I create a directly-executable cross-platform GUI app using Python?

Teifion

[How can I create a directly-executable cross-platform GUI app using Python?](https://stackoverflow.com/questions/2933/how-can-i-create-a-directly-executable-cross-platform-gui-app-using-python)

Python works on multiple platforms and can be used for desktop and web applications, thus I conclude that there is some way to compile it into an executable for Mac, Windows and Linux.The problem being I have no idea where to start or how to write a GUI with it, can anybody shed some light on this and point me in the right direction please?

2008-08-05 22:26:00Z

Python works on multiple platforms and can be used for desktop and web applications, thus I conclude that there is some way to compile it into an executable for Mac, Windows and Linux.The problem being I have no idea where to start or how to write a GUI with it, can anybody shed some light on this and point me in the right direction please?First you will need some GUI library with Python bindings and then (if you want) some program that will convert your python scripts into standalone executables.Cross-platform GUI libraries with Python bindings (Windows, Linux, Mac)Of course, there are many, but the most popular that I've seen in wild are:Complete list is at http://wiki.python.org/moin/GuiProgrammingSingle executable (all platforms)Single executable (Windows)Single executable (Linux)Single executable (Mac)Another system (not mentioned in the accepted answer yet) is PyInstaller, which worked for a PyQt project of mine when py2exe would not. I found it easier to use.http://www.pyinstaller.org/Pyinstaller is based on Gordon McMillan's Python Installer. Which is no longer available.An alternative tool to py2exe is bbfreeze which generates executables for windows and linux. It's newer than py2exe and handles eggs quite well. I've found it magically works better without configuration for a wide variety of applications.There's also PyGTK, which is basically a Python wrapper for the Gnome Toolkit.  I've found it easier to wrap my mind around than Tkinter, coming from pretty much no knowledge of GUI programming previously.  It works pretty well and has some good tutorials.  Unfortunately there isn't an installer for Python 2.6 for Windows yet, and may not be for a while.Since python is installed on nearly every non-Windows OS by default now, the only thing you really need to make sure of is that all of the non-standard libraries you use are installed.Having said that, it is possible to build executables that include the python interpreter, and any libraries you use.  This is likely to create a large executable, however.MacOS X even includes support in the Xcode IDE for creating full standalone GUI apps.  These can be run by any user running OS X.For the GUI itself:PyQT is pretty much the reference.Another way to develop a rapid user interface is to write a web app,

have it run locally and display the app in the browser.Plus, if you go for the Tkinter option suggested by lubos hasko

