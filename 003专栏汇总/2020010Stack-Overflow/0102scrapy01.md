搜索关键词：[scrapy]

## 01. Web-scraping JavaScript page with Python

mocopera

[Web-scraping JavaScript page with Python](https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python)

I'm trying to develop a simple web scraper. I want to extract text without the HTML code. In fact, I achieve this goal, but I have seen that in some pages where JavaScript is loaded I didn't obtain good results.For example, if some JavaScript code adds some text, I can't see it, because when I call 

    response = urllib2.urlopen(request)

I get the original text without the added one (because JavaScript is executed in the client). So, I'm looking for some ideas to solve this problem. 

2011-11-08 11:13:51Z

### 01

EDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end. 

dryscape isn't maintained anymore and the library dryscape developers recommend is Python 2 only. I have found using Selenium's python library with Phantom JS as a web driver fast enough and easy to get the work done.

Once you have installed Phantom JS, make sure the phantomjs binary is available in the current path:

```
phantomjs --version
# result:
2.1.1
```

To give an example, I created a sample page with following HTML code. (link):

```
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Javascript scraping test</title>
</head>
<body>
  <p id='intro-text'>No javascript support</p>
  <script>
     document.getElementById('intro-text').innerHTML = 'Yay! Supports javascript';
  </script> 
</body>
</html>
```

without javascript it says: No javascript support and with javascript: Yay! Supports javascript.

Scraping without JS support:

```
import requests
from bs4 import BeautifulSoup
response = requests.get(my_url)
soup = BeautifulSoup(response.text)
soup.find(id="intro-text")
# Result:
<p id="intro-text">No javascript support</p>
```

Scraping with JS support:

```
from selenium import webdriver
driver = webdriver.PhantomJS()
driver.get(my_url)
p_element = driver.find_element_by_id(id_='intro-text')
print(p_element.text)
# result:
'Yay! Supports javascript'
```

You can also use Python library dryscrape to scrape javascript driven websites. 

1『dryscrape 也是 Python 的一个库。([niklasb/dryscrape: [not actively maintained] A lightweight Python library that uses Webkit to enable easy scraping of dynamic, Javascript-heavy web pages](https://github.com/niklasb/dryscrape))』

### 02

We are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.

Therefore we need to render the javascript content before we crawl the page. As selenium is already mentioned many times in this thread (and how slow it gets sometimes was mentioned also), I will list two other possible solutions.

Solution 1: This is a very nice tutorial on how to use Scrapy to crawl javascript generated content and we are going to follow just that.What we will need:

1.Docker installed in our machine. This is a plus over other solutions until this point, as it utilizes an OS-independent platform.

2.Install Splash following the instruction listed for our corresponding OS. Quoting from splash documentation: Splash is a javascript rendering service. It’s a lightweight web browser with an HTTP API, implemented in Python 3 using Twisted and QT5. Essentially we are going to use Splash to render Javascript generated content. ([Installation — Splash 3.4 documentation](https://splash.readthedocs.io/en/latest/install.html))

3.Run the splash server: sudo docker run -p 8050:8050 scrapinghub/splash.

4.Install the scrapy-splash plugin([scrapy-plugins/scrapy-splash: Scrapy+Splash for JavaScript integration](https://github.com/scrapy-plugins/scrapy-splash#installation)): pip install scrapy-splash

5.Assuming that we already have a Scrapy project created (if not, let's make one), we will follow the guide and update the settings.py:

Then go to your scrapy project’s settings.py and set these middlewares:

```
DOWNLOADER_MIDDLEWARES = {
      'scrapy_splash.SplashCookiesMiddleware': 723,
      'scrapy_splash.SplashMiddleware': 725,
      'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
```

The URL of the Splash server(if you’re using Win or OSX this should be the URL of the docker machine: [How to get a Docker container's IP address from the host - Stack Overflow](https://stackoverflow.com/questions/17157721/how-to-get-a-docker-containers-ip-address-from-the-host)):

    SPLASH_URL = 'http://localhost:8050'

And finally you need to set these values too:

```
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```

6.Finally, we can use a SplashRequest([scrapy-plugins/scrapy-splash: Scrapy+Splash for JavaScript integration](https://github.com/scrapy-plugins/scrapy-splash#usage)):

In a normal spider you have Request objects which you can use to open URLs. If the page you want to open contains JS generated data you have to use SplashRequest(or SplashFormRequest) to render the page. Here’s a simple example:

```
class MySpider(scrapy.Spider):
    name = "jsscraper"
    start_urls = ["http://quotes.toscrape.com/js/"]

    def start_requests(self):
        for url in self.start_urls:
        yield SplashRequest(
            url=url, callback=self.parse, endpoint='render.html'
        )

    def parse(self, response):
        for q in response.css("div.quote"):
        quote = QuoteItem()
        quote["author"] = q.css(".author::text").extract_first()
        quote["quote"] = q.css(".text::text").extract_first()
        yield quote
```

SplashRequest renders the URL as html and returns the response which you can use in the callback(parse) method.

Solution 2: Let's call this experimental at the moment (May 2018)...

This solution is for Python's version 3.6 only (at the moment).

Do you know the requests module (well who doesn't)?

Now it has a web crawling little sibling: requests-HTML([psf/requests-html: Pythonic HTML Parsing for Humans™](https://github.com/psf/requests-html)): This library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible.

1.Install requests-html: pipenv install requests-html

2.Make a request to the page's url:

```
from requests_html import HTMLSession

session = HTMLSession()
r = session.get(a_page_url)
```

Render the response to get the Javascript generated bits:

r.html.render()

Finally, the module seems to offer scraping capabilities.

Alternatively, we can try the well-documented way of using BeautifulSoup with the r.html object we just rendered.

I got this error: RuntimeError: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead. – HuckIt Apr 23 '19 at 15:59

@HuckIt this seems to be a known issue: [Issue #140 · psf/requests-html](https://github.com/psf/requests-html/issues/140) – John Moutafis Oct 15 '19 at 12:22

### 03

Maybe selenium can do it.

```
from selenium import webdriver
import time

driver = webdriver.Firefox()
driver.get(url)
time.sleep(5)
htmlSource = driver.page_source
```

Selenium is really heavy for this kind of thing, that'd be unnecessarily slow and requires a browser head if you don't use PhantomJS, but this would work. – Joshua Hedges Jul 28 '17 at 16:27

@JoshuaHedges You can run other more standard browsers in headless mode. – reynoldsnlp Jan 9 at 0:55



If you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.You can also visit https://html.python-requests.org/ to learn more about this module, or if your only interested about rendering JavaScript then you can visit https://html.python-requests.org/?#javascript-support to directly learn how to use the module to render JavaScript using Python.Essentially, Once you correctly install the Requests-HTML module, the following example, which is shown on the above link, shows how you can use this module to scrape a website and render JavaScript contained within the website:I recently learnt about this from a YouTube video. Click Here! to watch the YouTube video, which demonstrates how the module works.This seems to be a good solution also, taken from a great blog postIt sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page.While you could try running javascript on the server to handle this, a simpler approach  to might be to load up the page using Firefox and use a tool like Charles or Firebug to identify exactly what that secondary URL is. Then you can just query that URL directly for the data you are interested in.Selenium is the best for scraping JS and Ajax content.Check this article for extracting data from the web using PythonThen download Chrome webdriver.Easy, right?You can also execute javascript using webdriver.or store the value in a variableI personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here's an example:Use the scrapy startproject to create your scraper and write your spider, the skeleton can be as simple as this:The real magic happens in the middlewares.py. Overwrite two methods in the downloader middleware,  __init__ and  process_request, in the following way:Dont forget to enable this middlware by uncommenting the next lines in the settings.py file:Next for dockerization. Create your Dockerfile from a lightweight image (I'm using python Alpine here), copy your project directory to it, install requirements:And finally bring it all together in docker-compose.yaml:Run docker-compose up -d. If you're doing this the first time it will take a while for it to fetch the latest selenium/standalone-chrome and the build your scraper image as well. Once it's done, you can check that your containers are running with docker ps and also check that the name of the selenium container matches that of the environment variable that we passed to our scraper container (here, it was SELENIUM_LOCATION=samplecrawler_selenium_1). Enter your scraper container with docker exec -ti YOUR_CONTAINER_NAME sh , the command for me was docker exec -ti samplecrawler_my_scraper_1 sh, cd into the right directory and run your scraper with scrapy crawl my_spider.The entire thing is on my github page and you can get it from hereYou'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).

Sometimes you'll get what you need with just one of these modules.

Sometimes you'll need two, three, or all of these modules.

Sometimes you'll need to switch off the js on your browser.

Sometimes you'll need header info in your script.

No websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.

If you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.

Just keep searching how to try what with these modules and copying and pasting your errors into the Google.A mix of BeautifulSoup and Selenium works very well for me.P.S. You can find more wait conditions hereUsing PyQt5The problem is:The solution is:

pretty straightforward - simulate browser in order to execute javascript code.





This article has detailed explanation how to solve the above mentioned problem. 

For example, you can user puppeteer library to simulate a browser.



Or you can use an API service that returns HTML of your desired page.For example, send simple GET request to:

The response will contain rendered HTML content. The service will virtually run browser environment, execute JS code and render HTML.I've been trying to find answer to this questions for two days. Many answers direct you to different issues. But serpentr's answer above is really to the point. It is the shortest, simplest solution. Just a reminder the last word "var" represents the variable name, so should be used as:

## 02. How to extract a floating number from a string [duplicate]

Ben Keating

[How to extract a floating number from a string [duplicate]](https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string)

I have a number of strings similar to Current Level: 13.4 db. and I would like to extract just the floating point number. I say floating and not decimal as it's sometimes whole. Can RegEx do this or is there a better way?

2011-01-16 02:11:39Z

I have a number of strings similar to Current Level: 13.4 db. and I would like to extract just the floating point number. I say floating and not decimal as it's sometimes whole. Can RegEx do this or is there a better way?If your float is always expressed in decimal notation something likemay suffice.A more robust version would be:If you want to validate user input, you could alternatively also check for a float by stepping to it directly:You may like to try something like this which covers all the bases, including not relying on whitespace after the number:For easy copy-pasting:Python docs has an answer that covers +/-, and exponent notationThis regular expression does not support international formats where a comma is used as the separator character between the whole and fractional part (3,14159).

In that case, replace all \. with [.,] in the above float regex.You can use the following regex to get integer and floating values from a string:Thanks

Rexas described above, works really well!

One suggestion though:will also return negative int values (like -3 in the end of this string)I think that you'll find interesting stuff in the following answer of mine that I did for a previous similar question:https://stackoverflow.com/q/5929469/551449In this answer, I proposed a  pattern that allows a regex to catch any kind of number and since I have nothing else to add to it, I think it is fairly completeAnother approach that may be more readable is simple type conversion. I've added a replacement function to cover instances where people may enter European decimals:This has disadvantages too however. If someone types in "1,000", this will be converted to 1. Also, it assumes that people will be inputting with whitespace between words. This is not the case with other languages, such as Chinese.

Passing arguments with spaces between (bash) script

John Fear

[Passing arguments with spaces between (bash) script](https://stackoverflow.com/questions/17094086/passing-arguments-with-spaces-between-bash-script)

I've got the following bash two scriptsa.sh:b.sh:The someApp binary receives $* as 2 arguments ('My' and 'Argument') instead of 1.I've tested several things:

2013-06-13 18:09:05Z

I've got the following bash two scriptsa.sh:b.sh:The someApp binary receives $* as 2 arguments ('My' and 'Argument') instead of 1.I've tested several things:$*, unquoted, expands to two words. You need to quote it so that someApp receives a single argument.It's possible that you want to use $@ instead, so that someApp would receive two arguments if you were to call b.sh asWith someApp "$*", someApp would receive a single argument My first My second. With someApp "$@", someApp would receive two arguments, My first and My second.

Scrapy Crawl URLs in Order

Jeff

[Scrapy Crawl URLs in Order](https://stackoverflow.com/questions/6566322/scrapy-crawl-urls-in-order)

So, my problem is relatively simple.  I have one spider crawling multiple sites, and I need it to return the data in the order I write it in my code.  It's posted below.The results are returned in a random order, for example it returns the 29th, then the 28th, then the 30th.  I've tried changing the scheduler order from DFO to BFO, just in case that was the problem, but that didn't change anything.

2011-07-04 00:09:17Z

So, my problem is relatively simple.  I have one spider crawling multiple sites, and I need it to return the data in the order I write it in my code.  It's posted below.The results are returned in a random order, for example it returns the 29th, then the 28th, then the 30th.  I've tried changing the scheduler order from DFO to BFO, just in case that was the problem, but that didn't change anything.start_urls defines urls which are used in start_requests method. Your parse method is called with a response for each start urls when the page is downloaded. But you cannot control loading times - the first start url might come the last to parse.A solution -- override start_requests method and add to generated requests a meta with priority key. In parse extract this priority value and add it to the item. In the pipeline do something based in this value. (I don't know why and where you need these urls to be processed in this order).Or make it kind of synchronous -- store these start urls somewhere. Put in start_urls the first of them. In parse process the first response and yield the item(s), then take next url from your storage and make a request for it with callback for parse.Scrapy Request has a priority attribute now. If you have many Request in a function and want to process a particular request first, you can set:Scrapy will process the one with priority=1 first.The google group discussion suggests using priority attribute in Request object.

Scrapy guarantees the urls are crawled in DFO by default. But it does not ensure that the urls are visited in the order they were yielded within your parse callback.Instead of yielding Request objects you want to return an array of Requests from which objects will be popped till it is empty.Can you try something like that?I doubt if it's possible to achieve what you want unless you play with scrapy internals. There are some similar discussions on scrapy google groups e.g. http://groups.google.com/group/scrapy-users/browse_thread/thread/25da0a888ac19a9/1f72594b6db059f4?lnk=gstThe solution is sequential.

This solution is similar to @wuliang I started with @Alexis de Tréglodé method but reached a problem:

The fact that your start_requests() method returns a list of URLS

return [ Request(url = start_url) for start_url in start_urls ]

is causing the output to be non-sequential (asynchronous)If the return is a single response then by creating an alternative other_urls can fulfill the requirements.  Also, other_urls can be used to add-into URLs scraped from other webpages.Off course, you can control it. 

The top secret is the method how to feed the greedy Engine/Schedulor. You requirement is just a little one. Please see I add a list named "task_urls".If you want some more complicated case, please see my project:

https://github.com/wuliang/TiebaPostGrabberThere is a much easier way to make scrapy follow the order of starts_url: you can just uncomment and change the concurrent requests in settings.py to 1.Disclaimer: haven't worked with scrapy specificallyThe scraper may be queueing and requeueing requests based on timeouts and HTTP errors, it would be a lot easier if you can get at the date from the response page?  I.e. add another hxs.select statement that grabs the date (just had a look, it is definitely in the response data), and add that to the item dict, sort items based on that.  This is probably a more robust approach, rather than relying on order of scrapes...I believe the you make will scrape the data from the site in the order it appears. Either that or scrapy is going through your start_urls in an arbitrary order. To force it to go through them in a predefined order, and mind you, this won't work if you need to crawl more sites, then you can try this: then write a parse2 that does the same thing but appends a Request for url3.html with callback=self.parse3. This is horrible coding style, but I'm just throwing it out in case you need a quick hack. Personally I like @user1460015's implementation after I managed to have my own work around solution.My solution is to use subprocess of Python to call scrapy url by url until all urls have been took care of.In my code, if user does not specify he/she wants to parse the urls sequentially, we can start the spider in a normal way.If a user specifies it needs to be done sequentially, we can do this:Note that: this implementation does not handle errors.

ReactorNotRestartable error in while loop with scrapy

k_wit

[ReactorNotRestartable error in while loop with scrapy](https://stackoverflow.com/questions/39946632/reactornotrestartable-error-in-while-loop-with-scrapy)

I get twisted.internet.error.ReactorNotRestartable error when I execute following code:For the first time it works, then I get error. I create process variable each time, so what's the problem?

2016-10-09 17:47:46Z

I get twisted.internet.error.ReactorNotRestartable error when I execute following code:For the first time it works, then I get error. I create process variable each time, so what's the problem?By default, CrawlerProcess's .start() will stop the Twisted reactor it creates when all crawlers have finished.You should call process.start(stop_after_crawl=False) if you create process in each iteration.Another option is to handle the Twisted reactor yourself and use CrawlerRunner. The docs have an example on doing that.I was able to solve this problem like this. process.start() should be called only once.Ref  http://crawl.blog/scrapy-loop/

how to handle 302 redirect in scrapy

mrki

[how to handle 302 redirect in scrapy](https://stackoverflow.com/questions/22795416/how-to-handle-302-redirect-in-scrapy)

I am receiving a 302 response from a server while scrapping a website:I want to send request to GET urls instead of being redirected. Now I found this middleware:https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/redirect.py#L31I added this redirect code to my middleware.py file and I added this into settings.py:But I am still getting redirected. Is that all I have to do in order to get this middleware working? Do I miss something?

2014-04-01 19:42:22Z

I am receiving a 302 response from a server while scrapping a website:I want to send request to GET urls instead of being redirected. Now I found this middleware:https://github.com/scrapy/scrapy/blob/master/scrapy/contrib/downloadermiddleware/redirect.py#L31I added this redirect code to my middleware.py file and I added this into settings.py:But I am still getting redirected. Is that all I have to do in order to get this middleware working? Do I miss something?Forgot about middlewares in this scenario, this will do the trick:That said, you will need to include meta parameter when you yield your request:An unexplicable 302 response, such as redirecting from a page that loads fine in a web browser to the home page or some fixed page, usually indicates a server-side measure against undesired activity.You must either reduce your crawl rate or use a smart proxy (e.g. Crawlera) or a proxy-rotation service and retry your requests when you get such a response.To retry such a response, add 'handle_httpstatus_list': [302] to the meta of the source request, and check if response.status == 302 in the callback. If it is, retry your request by yielding response.request.replace(dont_filter=True).When retrying, you should also make your code limit the maximum number of retries of any given URL. You could keep a dictionary to track retries:Depending on the scenario, you might want to move this code to a downloader middleware.DOWNLOADER_MIDDLEWARES_BASE says that RedirectMiddleware is already enabled by default, so what you did didn't matter.How? The server responds with 302 on your GET request. If you do GET on the same URL again you will be redirected again.What are you trying to achieve?If you want to not be redirected, see these questions: I had an issue with infinite loop on redirections when using HTTPCACHE_ENABLED = True. I managed to avoid the problem by setting HTTPCACHE_IGNORE_HTTP_CODES = [301,302].You can disable the RedirectMiddleware by setting REDIRECT_ENABLED to False in settings.py

How to drop a collection with pymongo?

Morton

[How to drop a collection with pymongo?](https://stackoverflow.com/questions/48923682/how-to-drop-a-collection-with-pymongo)

I use scarpy to crawl data and save it to cloud hosting mLab successfully with MongoDB.My collection name is recently and data's count is 5.

I want to crawl data again and update my collection recently, so i try to drop the collection and then insert.Here is my code pipelines.py:But when I run my spider, I see my collection recently count is 10 (It should be 5 that is what I want)

I looking for some code that how to drop collection.

It's just say db.[collection Name].drop()But its no working in my case when i try self.collection.drop() before self.collection.insert(dict(item))Anyone can give me some suggestions what is wrong with my code ? That would be appreciated. Thanks in advance.

2018-02-22 09:24:37Z

I use scarpy to crawl data and save it to cloud hosting mLab successfully with MongoDB.My collection name is recently and data's count is 5.

I want to crawl data again and update my collection recently, so i try to drop the collection and then insert.Here is my code pipelines.py:But when I run my spider, I see my collection recently count is 10 (It should be 5 that is what I want)

I looking for some code that how to drop collection.

It's just say db.[collection Name].drop()But its no working in my case when i try self.collection.drop() before self.collection.insert(dict(item))Anyone can give me some suggestions what is wrong with my code ? That would be appreciated. Thanks in advance.You need to use drop. Suppose foo is a collectionOr you can use drop_collection

correct way to nest Item data in scrapy

Granitosaurus

[correct way to nest Item data in scrapy](https://stackoverflow.com/questions/25095233/correct-way-to-nest-item-data-in-scrapy)

What is the correct way to nest Item data?For example, I want the output of a product:I have scrapy.Item of:Now, the way I do it is just to reformat the whole item in the pipeline according to new item template:and in pipeline:Is this correct way to approach this or is there a more straight-forward way to approach this without breaking the philosophy of the framework?

2014-08-02 12:53:01Z

What is the correct way to nest Item data?For example, I want the output of a product:I have scrapy.Item of:Now, the way I do it is just to reformat the whole item in the pipeline according to new item template:and in pipeline:Is this correct way to approach this or is there a more straight-forward way to approach this without breaking the philosophy of the framework?UPDATE from comments: Looks like nested loaders is the updated approach. Another comment suggests this approach will cause errors during serialization.Best way to approach this is by creating a main and a meta item class/loader.Sample usage:After that, you can easily expand your items in the future by creating more "sub-items."I think it would be more straightforward to construct the dictionary in the spider. Here are two different ways of doing it, both achieving the same result. The only possible dealbreaker here is that the processors apply on the item['meta'] field, not on the item['meta']['added_on'] and item['meta']['url'] fields.Is there a specific reason for which you want to construct it that way instead of unpacking the meta field ?

Proxy Pooling System for Scrapy to temporarily stop using slow/timing out proxies

Ryflex

[Proxy Pooling System for Scrapy to temporarily stop using slow/timing out proxies](https://stackoverflow.com/questions/48910982/proxy-pooling-system-for-scrapy-to-temporarily-stop-using-slow-timing-out-proxie)

I've been looking around trying to find a decent pooling system for Scrapy but I can't find anything that has everything I need/want.I'm looking for a solution to:Anyone know of any such solution (the main feature being the blacklisting of slow/timed out proxies...

2018-02-21 16:35:15Z

I've been looking around trying to find a decent pooling system for Scrapy but I can't find anything that has everything I need/want.I'm looking for a solution to:Anyone know of any such solution (the main feature being the blacklisting of slow/timed out proxies...As your polling rules are very specifics, you may code your own, please see the code bellow which implement some part of your rules (you have to implement some other):

Adding a wait-for-element while performing a SplashRequest in python Scrapy

NightFury13

[Adding a wait-for-element while performing a SplashRequest in python Scrapy](https://stackoverflow.com/questions/41075257/adding-a-wait-for-element-while-performing-a-splashrequest-in-python-scrapy)

I am trying to scrape a few dynamic websites using Splash for Scrapy in python. However, I see that Splash fails to wait for the complete page to load in certain cases. A brute force way to tackle this problem was to add a large wait time (eg. 5 seconds in the below snippet). However, this is extremely inefficient and still fails to load certain data (sometimes it take longer than 5 seconds to load the content). Is there some sort of a wait-for-element condition that can be put through these requests?

2016-12-10 11:58:11Z

I am trying to scrape a few dynamic websites using Splash for Scrapy in python. However, I see that Splash fails to wait for the complete page to load in certain cases. A brute force way to tackle this problem was to add a large wait time (eg. 5 seconds in the below snippet). However, this is extremely inefficient and still fails to load certain data (sometimes it take longer than 5 seconds to load the content). Is there some sort of a wait-for-element condition that can be put through these requests?Yes, you can write a Lua script to do that. Something like that:Before Splash 2.3 you can use splash:evaljs('!document.querySelector(".my-element")') instead of not splash:select('.my-element').Save this script to a variable (lua_script = """ ... """). Then you can send a request like this:See scripting tutorial and reference for more details on how to write Splash Lua scripts.I have a similar requirement, with timeouts. My solution is a slight modification of above:You can use lua script with javascript and splash:wait_for_resume (documentation).If you use without scrapy-splash plugin, attention to splash.args.url in splash:go, will be different.

python3 --version shows “NameError: name 'python3' is not defined”

Adrian Prayoga

[python3 --version shows “NameError: name 'python3' is not defined”](https://stackoverflow.com/questions/35845768/python3-version-shows-nameerror-name-python3-is-not-defined)

When we type it is supposed to show us the version of the python right?However, when I do this I get the following error:This is also the case when I tried to install the pip by using

2016-03-07 14:04:30Z

When we type it is supposed to show us the version of the python right?However, when I do this I get the following error:This is also the case when I tried to install the pip by usingpython3 is not Python syntax, it is the Python binary itself, the thing you run to get to the interactive interpreter. You are confusing the command line with the Python prompt. Open a console (Windows) or terminal (Linux, Mac), the same place you'd use dir or ls to explore your filesystem from the command line. If you are typing at a >>> or In [number]: prompt you are in the wrong place, that's the Python interpreter itself and it only takes Python syntax. If you started the Python prompt from a command line, exit at this point and go back to the command line. If you started the interpreter from IDLE or in an IDE, then you need to open a terminal or console as a separate program.Other programs that people often confuse for Python syntax; each of these is actually a program to run in your command prompt:with many more variations possible depending on what tools and libraries you have installed and what you are trying to do.If given arguments, you'll get a SyntaxError exception instead, but the underlying cause is the same:If you're using windows you can try in a Python prompt:

Scrapy process less than succesfully crawled

Bendeberia

[Scrapy process less than succesfully crawled](https://stackoverflow.com/questions/51744766/scrapy-process-less-than-succesfully-crawled)

I have 2 problems with my scraper:SpiderOutputIt succesfully crawls much (3x or 4x times more than crawls).

How can I force to process everything that was crawled? I can sacrifice the speed, but I don't want to waste what was successfully crawled 200s

2018-08-08 10:47:16Z

I have 2 problems with my scraper:SpiderOutputIt succesfully crawls much (3x or 4x times more than crawls).

How can I force to process everything that was crawled? I can sacrifice the speed, but I don't want to waste what was successfully crawled 200sThe scheduler may not have delivered all the 200 responses to the parse() method when you CloseSpider(). Log and ignore the 302s, and let the spider finish.

Does using scrapy-splash significantly affect scraping speed? [closed]

hsy

[Does using scrapy-splash significantly affect scraping speed? [closed]](https://stackoverflow.com/questions/49891688/does-using-scrapy-splash-significantly-affect-scraping-speed)

So far, I have been using just scrapy and writing custom classes to deal with websites using ajax.But if I were to use scrapy-splash, which from what I understand, scrapes the rendered html after javascript, will the speed of my crawler be affected significantly?What would be the comparison between time it takes to scrape a vanilla html page with scrapy vs javascript rendered html with scrapy-splash?And lastly, how do scrapy-splash and Selenium compare?

2018-04-18 05:17:19Z

So far, I have been using just scrapy and writing custom classes to deal with websites using ajax.But if I were to use scrapy-splash, which from what I understand, scrapes the rendered html after javascript, will the speed of my crawler be affected significantly?What would be the comparison between time it takes to scrape a vanilla html page with scrapy vs javascript rendered html with scrapy-splash?And lastly, how do scrapy-splash and Selenium compare?It depends on the amount of javascript present on the page.You must know that to render all the javascript the splash takes some time and the python application proceeds without waiting for the rendering to be complete. So sometimes splash is also not able to do it. Here,orSelenium is only used to automate web browser interaction, Scrapy is used to download HTML, process data and save it(whole web crawling framework).Talking about scraping I would recommend scrapy and if the problem is javascript.