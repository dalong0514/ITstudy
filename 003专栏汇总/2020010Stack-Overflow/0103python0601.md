I currently have a Python set of n size where n >= 0.  Is there a quick 1 or 2 lines Python solution to do it?  For example, the set will look like:The goal is to pick 2 random items from the above and it's possible that the above set can contain 0, 1 or more items.  The only way I can think of doing the above is to convert the set to a list(mutable) from where I can access 2 random unique index within the length of the set.

2009-08-11 21:12:37Z

I currently have a Python set of n size where n >= 0.  Is there a quick 1 or 2 lines Python solution to do it?  For example, the set will look like:The goal is to pick 2 random items from the above and it's possible that the above set can contain 0, 1 or more items.  The only way I can think of doing the above is to convert the set to a list(mutable) from where I can access 2 random unique index within the length of the set.Use the random module: http://docs.python.org/library/random.htmlThis samples the two values without replacement (so the two values are different).

How can I build multiple submit buttons django form?

veena

[How can I build multiple submit buttons django form?](https://stackoverflow.com/questions/866272/how-can-i-build-multiple-submit-buttons-django-form)

I have form with one input for email and two submit buttons to subscribe and unsubscribe from newsletter:I have also class form:I must write my own clean_email method and I need to know by which button was form submited. But the value of submit buttons aren't in self.cleaned_data dictionary.

Could I get values of buttons otherwise?

2009-05-14 22:50:25Z

I have form with one input for email and two submit buttons to subscribe and unsubscribe from newsletter:I have also class form:I must write my own clean_email method and I need to know by which button was form submited. But the value of submit buttons aren't in self.cleaned_data dictionary.

Could I get values of buttons otherwise?You can use self.data in the clean_email method to access the POST data before validation. It should contain a key called newsletter_sub or newsletter_unsub depending on which button was pressed.Eg:You can also do like this,CODE It's an old question now, nevertheless I had the same issue and found a solution that works for me: I wrote MultiRedirectMixin.one url to the same view! 

like so!

Python add item to the tuple

Goran

[Python add item to the tuple](https://stackoverflow.com/questions/16730339/python-add-item-to-the-tuple)

I have some object.ID-s which I try to store in the user session as tuple. When I add first one it works but tuple looks like (u'2',) but when I try to add new one using mytuple = mytuple + new.id got error can only concatenate tuple (not "unicode") to tuple. 

2013-05-24 08:04:32Z

I have some object.ID-s which I try to store in the user session as tuple. When I add first one it works but tuple looks like (u'2',) but when I try to add new one using mytuple = mytuple + new.id got error can only concatenate tuple (not "unicode") to tuple. You need to make the second element a 1-tuple, eg:Since Python 3.5 (PEP 448) you can do unpacking within a tuple, list set, and dict:From tuple to list to tuple :Or with a longer list of items to appendgives youThe point here is: List is a mutable sequence type. So you can change a given list by adding or removing elements. Tuple is an immutable sequence type. You can't change a tuple. So you have to create a new one.Tuple can only allow adding tuple to it. The best way to do it is:I tried the same scenario with the below data it all seems to be working fine.Bottom line, the easiest way to append to a tuple is to enclose the element being added with parentheses and a comma.

How do you serialize a model instance in Django?

Jason Christa

[How do you serialize a model instance in Django?](https://stackoverflow.com/questions/757022/how-do-you-serialize-a-model-instance-in-django)

There is a lot of documentation on how to serialize a Model QuerySet but how do you just serialize to JSON the fields of a Model Instance?

2009-04-16 16:47:31Z

There is a lot of documentation on how to serialize a Model QuerySet but how do you just serialize to JSON the fields of a Model Instance?You can easily use a list to wrap the required object and that's all what django serializers need to correctly serialize it, eg.:If you're dealing with a list of model instances the best you can do is using serializers.serialize(), it gonna fit your need perfectly. However, you are to face an issue with trying to serialize a single object, not a list of objects. That way, in order to get rid of different hacks, just use Django's model_to_dict (if I'm not mistaken, serializers.serialize() relies on it, too): You now just need one straight json.dumps call to serialize it to json:That's it! :)To avoid the array wrapper, remove it before you return the response:I found this interesting post on the subject too:http://timsaylor.com/convert-django-model-instances-to-dictionariesIt uses django.forms.models.model_to_dict, which looks like the perfect tool for the job.There is a good answer for this and I'm surprised it hasn't been mentioned. With a few lines you can handle dates, models, and everything else.Make a custom encoder that can handle models:Now use it when you use json.dumpsNow models, dates and everything can be serialized and it doesn't have to be in an array or serialized and unserialized. Anything you have that is custom can just be added to the default method.You can even use Django's native JsonResponse this way:It sounds like what you're asking about involves serializing the data structure of a Django model instance for interoperability.  The other posters are correct: if you wanted the serialized form to be used with a python application that can query the database via Django's api, then you would wan to serialize a queryset with one object.  If, on the other hand, what you need is a way to re-inflate the model instance somewhere else without touching the database or without using Django, then you have a little bit of work to do.Here's what I do:First, I use demjson for the conversion.  It happened to be what I found first, but it might not be the best.  My implementation depends on one of its features, but there should be similar ways with other converters.Second, implement a json_equivalent method on all models that you might need serialized.  This is a magic method for demjson, but it's probably something you're going to want to think about no matter what implementation you choose.  The idea is that you return an object that is directly convertible to json (i.e. an array or dictionary).  If you really want to do this automatically:This will not be helpful to you unless you have a completely flat data structure (no ForeignKeys, only numbers and strings in the database, etc.).  Otherwise, you should seriously think about the right way to implement this method.Third, call demjson.JSON.encode(instance) and you have what you want.If you're asking how to serialize a single object from a model and you know you're only going to get one object in the queryset (for instance, using objects.get), then use something like:which would get you something of the form:I solved this problem by adding a serialization method to my model:Here's the verbose equivalent for those averse to one-liners:_meta.fields is an ordered list of model fields which can be accessed from instances and from the model itself.Here's my solution for this, which allows you to easily customize the JSON as well as organize related recordsFirstly implement a method on the model. I call is json but you can call it whatever you like, e.g.:Then in the view I do:Use list, it will solve problemStep1:Step2:Step3:To serialize and deserialze, use the following:If you want to return the single model object as a json response to a client, you can do this simple solution:The json format will return the result as str whereas python will return the result in either list or OrderedDict.values() is what I needed to convert a model instance to JSON..values() documentation: https://docs.djangoproject.com/en/3.0/ref/models/querysets/#valuesExample usage with a model called Project.Note: I'm using Django Rest FrameworkResult from a valid id:It doesn't seem you can serialize an instance, you'd have to serialize a QuerySet of one object.I run out of the svn release of django, so this may not be in earlier versions.I return the dict of my instanceso it return something like {'field1':value,"field2":value,....}how about this way:or exclude anything you don't want.

How to display pandas DataFrame of floats using a format string for columns?

Jason S

[How to display pandas DataFrame of floats using a format string for columns?](https://stackoverflow.com/questions/20937538/how-to-display-pandas-dataframe-of-floats-using-a-format-string-for-columns)

I would like to display a pandas dataframe with a given format using print() and the IPython display(). For example:I would like to somehow coerce this into printingwithout having to modify the data itself or create a copy, just change the way it is displayed.How can I do this?

2014-01-05 18:39:43Z

I would like to display a pandas dataframe with a given format using print() and the IPython display(). For example:I would like to somehow coerce this into printingwithout having to modify the data itself or create a copy, just change the way it is displayed.How can I do this?yieldsbut this only works if you want every float to be formatted with a dollar sign.Otherwise, if you want dollar formatting for some floats only, then I think you'll have to pre-modify the dataframe (converting those floats to strings):yieldsIf you don't want to modify the dataframe, you could use a custom formatter for that column.yieldsAs of Pandas 0.17 there is now a styling system which essentially provides formatted views of a DataFrame using Python format strings:which displaysThis is a view object; the DataFrame itself does not change formatting, but updates in the DataFrame are reflected in the view:However it appears to have some limitations:which looks ok but:Similar to unutbu above, you could also use applymap as follows:I like using pandas.apply() with python format().Also, it can be easily used with multiple columns...You can also set locale to your region and set float_format to use a currency format. This will automatically set $ sign for currency in USA.summary:

Instance attribute attribute_name defined outside __init__

Steven Liao

[Instance attribute attribute_name defined outside __init__](https://stackoverflow.com/questions/19284857/instance-attribute-attribute-name-defined-outside-init)

I split up my class constructor by letting it call multiple functions, like this:While my interpreter happily runs my code, Pylint has a complaint:Instance attribute attribute_name defined outside __init__A cursory Google search is currently fruitless. Keeping all constructor logic in __init__ seems unorganized, and turning off the Pylint warning also seems hack-ish.What is a/the Pythonic way to resolve this problem?

2013-10-10 00:04:50Z

I split up my class constructor by letting it call multiple functions, like this:While my interpreter happily runs my code, Pylint has a complaint:Instance attribute attribute_name defined outside __init__A cursory Google search is currently fruitless. Keeping all constructor logic in __init__ seems unorganized, and turning off the Pylint warning also seems hack-ish.What is a/the Pythonic way to resolve this problem?The idea behind this message is for the sake of readability. We expect to find all the attributes an instance may have by reading its __init__ method.You may still want to split initialization into other methods though. In such case, you can simply assign attributes to None (with a bit of documentation) in the __init__ then call the sub-initialization methods.Just return a tuple from parse_arguments() and unpack into attributes inside __init__ as needed.Also, I would recommend that you use Exceptions in lieu of using exit(1).  You get tracebacks, your code is reusable, etc.Although the definition of instance variables outside init isn't recommended in general, there are rare cases in which it is natural. For example, when you have a parent class that defines several variables that its child classes won't use, and whose definition will make its child waste time or resources, or will be simply unaesthetic.One possible solution to this is using an init-extention function, that each child class may override, and in this function use function setattr in order to define the class-unique instance variables. May be this is not too aesthetic as well, but it eliminates the here-discussed linting warning.For each attribute you want to set via function, call the function from the init. For example, the following works for me to set the attribute ascii_txt...If you are using Python 3, you can tryAlthough not as pythonic as the accepted answer, but it should get away the Pylint alert.And if you don't concern about type and don't want to create a new object with object() use:As None will cause type not match error.

RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility

Blue482

[RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility](https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility)

I have this error for trying to load a saved SVM model. I have tried uninstalling sklearn, NumPy and SciPy, reinstalling the latest versions all-together again (using pip). I am still getting this error. Why?UPDATE: OK, by following here, andThe error has now gone, though I still have no idea why it occurred in the first place...

2016-11-28 13:17:48Z

I have this error for trying to load a saved SVM model. I have tried uninstalling sklearn, NumPy and SciPy, reinstalling the latest versions all-together again (using pip). I am still getting this error. Why?UPDATE: OK, by following here, andThe error has now gone, though I still have no idea why it occurred in the first place...According to MAINT: silence Cython warnings about changes dtype/ufunc size. - numpy/numpy:and the checks are inserted by Cython (hence are present in any module compiled with it).Long story short, these warnings should be benign in the particular case of numpy, and these messages are filtered out since numpy 1.8 (the branch this commit went onto). While scikit-learn 0.18.1 is compiled against numpy 1.6.1.To filter these warnings yourself, you can do the same as the patch does:Of course, you can just recompile all affected modules from source against your local numpy with pip install --no-binary :all:¹ instead if you have the balls tools for that.Longer story: the patch's proponent claims there should be no risk specifically with numpy, and 3rd-party packages are intentionally built against older versions:As a result, Cython's devs agreed to trust the numpy team with maintaining binary compatibility by hand, so we can probably expect that using versions with breaking ABI changes would yield a specially-crafted exception or some other explicit show-stopper.¹The previously available --no-use-wheel option has been removed since pip 10.0.0.It's the issue of new numpy version (1.15.0)You can downgrade numpy and this problem will be fixed: sudo pip uninstall numpy

 sudo pip install numpy==1.14.5This is working..if you are in an anaconda environment use:I've tried the above-mentioned ways, but nothing worked. But the issue was gone after I installed the libraries through apt install,For Python3,For Python2,Hope that helps.Just upgrade your numpy module, right now it is 1.15.4. For windowsThis error occurs because the installed packages were build agains different version of numpy.

We need to rebuild scipy and scikit-learn against the local numpy.For new pip (in my case pip 18.0) this worked:--no-binary takes a list of names of packages that you want to ignore binaries for. In this case we passed --no-binary scipy,scikit-learn which will ignore binaries for packages scipy,scikit-learn.

Didn't help meMeta-information: The recommended way to install sklearn[... do not compile from source using pip]Note that as of cython 0.29 there is a new check_size option that eliminates the warning at the source, so no work-arounds should be needed once that version percolates to the various packagesMy enviroment is Python 2.7.15I try but it does not work. It shows the error:Then I try:And it works: the useless warnings do not show.When import scipy, error info shows: RuntimeWarning: builtin.type size changed, may indicate binary incompatibility. Expected zd, got zdI solved this problem by updating python version from 2.7.2 to 2.7.13

Can I add comments to a pip requirements file?

david.libremone

[Can I add comments to a pip requirements file?](https://stackoverflow.com/questions/9159757/can-i-add-comments-to-a-pip-requirements-file)

I'd like to add comments for a few packages in a pip requirements file. (Just to explain why that package is on the list.) Can I do this?I'm imagining something like

2012-02-06 11:59:20Z

I'd like to add comments for a few packages in a pip requirements file. (Just to explain why that package is on the list.) Can I do this?I'm imagining something likeSure, you can, based on pip docs:Go ahead!You can add comments (lines beginning with #)

Python memory usage of numpy arrays

EddyTheB

[Python memory usage of numpy arrays](https://stackoverflow.com/questions/11784329/python-memory-usage-of-numpy-arrays)

I'm using python to analyse some large files and I'm running into memory issues, so I've been using sys.getsizeof() to try and keep track of the usage, but it's behaviour with numpy arrays is bizarre. Here's an example involving a map of albedos that I'm having to open:Well the data's still there, but the size of the object, a 3600x7200 pixel map, has gone from ~200 Mb to 80 bytes. I'd like to hope that my memory issues are over and just convert everything to numpy arrays, but I feel that this behaviour, if true, would in some way violate some law of information theory or thermodynamics, or something, so I'm inclined to believe that getsizeof() doesn't work with numpy arrays. Any ideas?

2012-08-02 19:19:22Z

I'm using python to analyse some large files and I'm running into memory issues, so I've been using sys.getsizeof() to try and keep track of the usage, but it's behaviour with numpy arrays is bizarre. Here's an example involving a map of albedos that I'm having to open:Well the data's still there, but the size of the object, a 3600x7200 pixel map, has gone from ~200 Mb to 80 bytes. I'd like to hope that my memory issues are over and just convert everything to numpy arrays, but I feel that this behaviour, if true, would in some way violate some law of information theory or thermodynamics, or something, so I'm inclined to believe that getsizeof() doesn't work with numpy arrays. Any ideas?You can use array.nbytes for numpy arrays, for example:The field nbytes will give you the size in bytes of all the elements of the array in a numpy.array:Notice that this does not measures "non-element attributes of the array object" so the actual size in bytes can be a few bytes larger than this.In python notebooks I often want to filter out 'dangling' numpy.ndarray's, in particular the ones that are stored in _1, _2, etc that were never really meant to stay alive.I use this code to get a listing of all of them and their size.Not sure if locals() or globals() is better here.

Currency formatting in Python

RailsSon

[Currency formatting in Python](https://stackoverflow.com/questions/320929/currency-formatting-in-python)

I am looking to format a number like 188518982.18 to £188,518,982.18 using Python.How can I do this?

2008-11-26 14:43:33Z

I am looking to format a number like 188518982.18 to £188,518,982.18 using Python.How can I do this?See the locale module.This does currency (and date) formatting.http://docs.python.org/dev/whatsnew/2.7.html#pep-0378Not quite sure why it's not mentioned more online (or on this thread), but the Babel package (and Django utilities) from the Edgewall guys is awesome for currency formatting (and lots of other i18n tasks).  It's nice because it doesn't suffer from the need to do everything globally like the core Python locale module.The example the OP gave would simply be:This is an ancient post, but I just implemented the following solution which:Code:Output:And for the original poster, obviously, just switch $ for £My locale settings seemed incomplete, so I had too look beyond this SO answer and found: http://docs.python.org/library/decimal.html#recipesOS-independentJust wanted to share here.If you are using OSX and have yet to set your locale module setting this first answer will not work you will receive the following error:To remedy this you will have to do use the following:If I were you, I would use BABEL: http://babel.pocoo.org/en/latest/index.htmlOh, that's an interesting beast.I've spent considerable time of getting that right, there are three main issues that differs from locale to locale:

 - currency symbol and direction

 - thousand separator

 - decimal pointI've written my own rather extensive implementation of this which is part of the kiwi python framework, check out the LGPL:ed source here:http://svn.async.com.br/cgi-bin/viewvc.cgi/kiwi/trunk/kiwi/currency.py?view=markupThe code is slightly Linux/Glibc specific, but shouldn't be too difficult to adopt to windows or other unixes.Once you have that installed you can do the following:Which will then give you:orDepending on the currently selected locale.The main point this post has over the other is that it will work with older versions of python. locale.currency was introduced in python 2.5."{:0,.2f}".format(float(your_numeric_value)) in Python 3 does the job; it gives out something like one of the following lines:#printing  the variable 'Total:' in a format that looks like this '9,348.237'where the '{:7,.3f}' es the number of spaces for formatting the number in this case is a million with 3 decimal points.

Then you add the '.format(zum1).  The zum1 is tha variable that has the big number for the sum of all number in my particular program.  Variable can be anything that you decide to use.I've come to look at the same thing and found python-money not really used it yet but maybe a mix of the two would be goodA lambda for calculating it inside a function, with help from @Nate's answerand then,Simple python code!

How can I generate a unique ID in Python? [duplicate]

Ryan

[How can I generate a unique ID in Python? [duplicate]](https://stackoverflow.com/questions/1210458/how-can-i-generate-a-unique-id-in-python)

I need to generate a unique ID based on a random value.

2009-07-31 02:51:44Z

I need to generate a unique ID based on a random value.Perhaps uuid.uuid4() might do the job. See uuid for more information.You might want Python's UUID functions:21.15. uuid — UUID objects according to RFC 4122eg:unique and random are mutually exclusive.  perhaps you want this?Usage:no two returned id is the same (Unique) and this is based on a randomized seed valuehere you can find an implementation :hope it helps !Maybe this work for uMaybe the uuid module?This will work very quickly but will not generate random values but monotonously increasing ones (for a given thread). It is thread safe and working with tuples may have benefit as opposed to strings (shorter if anything). If you do not need thread safety feel free remove the threading bits (in stead of threading.local, use object() and remove tid altogether). Hope that helps.

Python's os.makedirs doesn't understand「~」in my path

Johan

[Python's os.makedirs doesn't understand「~」in my path](https://stackoverflow.com/questions/2057045/pythons-os-makedirs-doesnt-understand-in-my-path)

I have a little problem with ~ in my paths.This code example creates some directories called "~/some_dir" and do not understand that I wanted to create some_dir in my home directory.Note this is on a Linux-based system.

2010-01-13 13:51:09Z

I have a little problem with ~ in my paths.This code example creates some directories called "~/some_dir" and do not understand that I wanted to create some_dir in my home directory.Note this is on a Linux-based system.You need to expand the tilde manually:The conversion of ~/some_dir to $HOME/some_dir is called tilde expansion and is a common user interface feature. The file system does not know anything about it.In Python, this feature is implemented by os.path.expanduser:That's probably because Python is not Bash and doesn't follow same conventions. You may use this:It will create a folder if not there else it wont create a folder.NOTE : it will also create folders in path (if required)This function works like mkdir -p path/to/folderYou can now do it likePlease refer to https://stackoverflow.com/a/54190233/6799074 for usage of srblib.abs_path

How do you get the magnitude of a vector in Numpy?

Nick T

[How do you get the magnitude of a vector in Numpy?](https://stackoverflow.com/questions/9171158/how-do-you-get-the-magnitude-of-a-vector-in-numpy)

In keeping with the "There's only one obvious way to do it", how do you get the magnitude of a vector (1D array) in Numpy?The above works, but I cannot believe that I must specify such a trivial and core function myself.

2012-02-07 04:48:50Z

In keeping with the "There's only one obvious way to do it", how do you get the magnitude of a vector (1D array) in Numpy?The above works, but I cannot believe that I must specify such a trivial and core function myself.The function you're after is numpy.linalg.norm. (I reckon it should be in base numpy as a property of an array -- say x.norm() -- but oh well).You can also feed in an optional ord for the nth order norm you want. Say you wanted the 1-norm:And so on.If you are worried at all about speed, you should instead use:Here are some benchmarks:EDIT: The real speed improvement comes when you have to take the norm of many vectors. Using pure numpy functions doesn't require any for loops. For example:Yet another alternative is to use the einsum function in numpy for either arrays:or vectors:There does, however, seem to be some overhead associated with calling it that may make it slower with small inputs:Fastest way I found is via inner1d. Here's how it compares to other numpy methods:inner1d is ~3x faster than linalg.norm and a hair faster than einsumuse the function norm in scipy.linalg (or numpy.linalg)You can do this concisely using the toolbelt vg. It's a light layer on top of numpy and it supports single values and stacked vectors.I created the library at my last startup, where it was motivated by uses like this: simple ideas which are far too verbose in NumPy.

Syntax behind sorted(key=lambda: …)

Christopher Markieta

[Syntax behind sorted(key=lambda: …)](https://stackoverflow.com/questions/8966538/syntax-behind-sortedkey-lambda)

I don't quite understand the syntax behind the sorted() argument:Isn't lambda arbitrary? Why is variable stated twice in what looks like a dict?

2012-01-23 02:09:18Z

I don't quite understand the syntax behind the sorted() argument:Isn't lambda arbitrary? Why is variable stated twice in what looks like a dict?key is a function that will be called to transform the collection's items before they are compared. The parameter passed to key must be something that is callable. The use of lambda creates an anonymous function (which is callable). In the case of sorted the callable only takes one parameters. Python's lambda is pretty simple. It can only do and return one thing really.The syntax of lambda is the word lambda followed by the list of parameter names then a single block of code. The parameter list and code block are delineated by colon. This is similar to other constructs in python as well such as while, for, if and so on. They are all statements that typically have a code block. Lambda is just another instance of a statement with a code block.We can compare the use of lambda with that of def to create a function.lambda just gives us a way of doing this without assigning a name. Which makes it great for using as a parameter to a function. variable is used twice here because on the left hand of the colon it is the name of a parameter and on the right hand side it is being used in the code block to compute something.I think all of the answers here cover the core of what the lambda function does in the context of sorted() quite nicely, however I still feel like a description that leads to an intuitive understanding is lacking, so here is my two cents. For the sake of completeness, I'll state the obvious up front: sorted() returns a list of sorted elements and if we want to sort in a particular way or if we want to sort a complex list of elements (e.g. nested lists or a list of tuples) we can invoke the key argument.For me, the intuitive understanding of the key argument, why it has to be callable, and the use of lambda as the (anonymous) callable function to accomplish this comes in two parts. Lambda syntax is as follows:e.g.Check it out:Base example:Example 1:Notice that my lambda function told sorted to check if (e) was even or odd before sorting. BUT WAIT! You may (or perhaps should) be wondering two things - first, why are my odds coming before my evens (since my key value seems to be telling my sorted function to prioritize evens by using the mod operator in x%2==0). Second, why are my evens out of order? 2 comes before 6 right? By analyzing this result, we'll learn something deeper about how the sorted() 'key' argument works, especially in conjunction with the anonymous lambda function. Firstly, you'll notice that while the odds come before the evens, the evens themselves are not sorted. Why is this?? Lets read the docs:We have to do a little bit of reading between the lines here, but what this tells us is that the sort function is only called once, and if we specify the key argument, then we sort by the value that key function points us to. So what does the example using a modulo return? A boolean value: True == 1, False == 0. So how does sorted deal with this key? It basically transforms the original list to a sequence of 1s and 0s.Now we're getting somewhere. What do you get when you sort the transformed list?Okay, so now we know why the odds come before the evens. But the next question is: Why does the 6 still come before the 2 in my final list? Well that's easy - its because sorting only happens once! i.e. Those 1s still represent the original list values, which are in their original positions relative to each other. Since sorting only happens once, and we don't call any kind of sort function to order the original even values from low to high, those values remain in their original order relative to one another.The final question is then this: How do I think conceptually about how the order of my boolean values get transformed back in to the original values when I print out the final sorted list?Sorted() is a built-in method that (fun fact) uses a hybrid sorting algorithm called Timsort that combines aspects of merge sort and insertion sort. It seems clear to me that when you call it, there is a mechanic that holds these values in memory and bundles them with their boolean identity (mask) determined by (...!) the lambda function. The order is determined by their boolean identity calculated from the lambda function, but keep in mind that these sublists (of one's and zeros) are not themselves sorted by their original values. Hence, the final list, while organized by Odds and Evens, is not sorted by sublist (the evens in this case are out of order). The fact that the odds are ordered is because they were already in order by coincidence in the original list. The takeaway from all this is that when lambda does that transformation, the original order of the sublists are retained. So how does this all relate back to the original question, and more importantly, our intuition on how we should implement sorted() with its key argument and lambda?That lambda function can be thought of as a pointer that points to the values we need to sort by, whether its a pointer mapping a value to its boolean transformed by the lambda function, or if its a particular element in a nested list, tuple, dict, etc., again determined by the lambda function. Lets try and predict what happens when I run the following code.My sorted call obviously says, "Please sort this list". The key argument makes that a little more specific by saying, for each element (x) in mylist, return index 1 of that element, then sort all of the elements of the original list 'mylist' by the sorted order of the list calculated by the lambda function. Since we have a list of tuples, we can return an indexed element from that tuple. So we get:Run that code, and you'll find that this is the order. Try indexing a list of integers and you'll find that the code breaks.This was a long winded explanation, but I hope this helps to 'sort' your intuition on the use of lambda functions as the key argument in sorted() and beyond.lambda is a Python keyword that is used to generate anonymous functions.The variable left of the : is a parameter name. The use of variable on the right is making use of the parameter.Means almost exactly the same as:One more example of usage sorted() function with key=lambda. Let's consider you have a list of tuples. In each tuple you have a brand, model and weight of the car and you want to sort this list of tuples by brand, model or weight. You can do it with lambda.Results:lambda is an anonymous function, not an arbitrary function.  The parameter being accepted would be the variable you're working with, and the column in which you're sorting it on.Since the usage of lambda was asked in the context of sorted(), take a look at this as well https://wiki.python.org/moin/HowTo/Sorting/#Key_FunctionsJust to rephrase, the key (Optional. A Function to execute to decide the order. Default is None) in sorted functions expects a function and you use lambda.  To define lambda, you specify the object property you want to sort and python's built-in sorted function will automatically take care of it.If you want to sort by multiple properties then assign key = lambda x: (property1, property2).To specify order-by, pass reverse= true as the third argument(Optional. A Boolean. False will sort ascending, True will sort descending. Default is False) of sorted function.Simple and not time consuming answer with an example relevant to the question asked

Follow this example:Look at the names in the list, they starts with D, B, C and A. And if you notice the ages, they are 55, 44, 33 and 22.

The first print codeResults to:sorts the name, because by key=lambda el: el["name"] we are sorting the names and the names return in alphabetical order.The second print code Result:sorts by age, and hence the list returns by ascending order of age.Try this code for better understanding.

What is the difference between json.load() and json.loads() functions

MMF

[What is the difference between json.load() and json.loads() functions](https://stackoverflow.com/questions/39719689/what-is-the-difference-between-json-load-and-json-loads-functions)

In Python, what is the difference between json.load() and json.loads()?I guess that the load() function must be used with a file object (I need thus to use a context manager) while the loads() function take the path to the file as a string. It is a bit confusing.Does the letter "s" in json.loads() stand for string?Thanks a lot for your answers!

2016-09-27 08:24:26Z

In Python, what is the difference between json.load() and json.loads()?I guess that the load() function must be used with a file object (I need thus to use a context manager) while the loads() function take the path to the file as a string. It is a bit confusing.Does the letter "s" in json.loads() stand for string?Thanks a lot for your answers!Yes, s stands for string. The json.loads function does not take the file path, but the file contents as a string. Look at the documentation at https://docs.python.org/2/library/json.html!Just going to add a simple example to what everyone has explained,json.load()json.load can deserialize a file itself i.e. it accepts a file object, for example,will output,If I use json.loads to open a file instead,I would get this error:json.loads()json.loads() deserialize string.So in order to use json.loads I will have to pass the content of the file using read() function, for example,using content.read() with json.loads() return content of the file,Output,That's because type of content.read() is string, i.e. <type 'str'>If I use json.load() with content.read(), I will get error,Gives,So, now you know json.load deserialze file and json.loads deserialize a string.Another example,sys.stdin return file object, so if i do print(json.load(sys.stdin)), I will get actual json data,If I want to use json.loads(), I would do print(json.loads(sys.stdin.read())) instead.Documentation is quite clear: https://docs.python.org/2/library/json.htmlSo load is for a file, loads for a string

How does tf.app.run() work?

Anurag Ranjan

[How does tf.app.run() work?](https://stackoverflow.com/questions/33703624/how-does-tf-app-run-work)

How does tf.app.run() work in Tensorflow translate demo? In tensorflow/models/rnn/translate/translate.py, there is a call to tf.app.run(). How is it being handled?

2015-11-14 00:03:32Z

How does tf.app.run() work in Tensorflow translate demo? In tensorflow/models/rnn/translate/translate.py, there is a call to tf.app.run(). How is it being handled?means current file is executed under a shell instead of imported as a module.As you can see through the file app.pyLet's break line by line:This ensures that the argument you pass through command line is valid,e.g. 

python my_model.py --data_dir='...' --max_iteration=10000 Actually, this feature is implemented based on python standard argparse module.The first main in right side of = is the first argument of current function run(main=None, argv=None)

. While sys.modules['__main__'] means current running file(e.g. my_model.py).So there are two cases:Last line:ensures your main(argv) or my_main_running_function(argv) function is called with parsed arguments properly.It's just a very quick wrapper that handles flag parsing and then dispatches to your own main. See the code.There is nothing special in tf.app. This is just a generic entry point script, which It has nothing to do with neural networks and it just calls the main function, passing through any arguments to it.In simple terms, the job of tf.app.run() is to first set the global flags for later usage like:and then run your custom main function with a set of arguments.For e.g. in TensorFlow NMT codebase, the very first entry point for the program execution for training/inference starts at this point (see below code)After parsing the arguments using argparse, with tf.app.run() you run the function "main" which is defined like:So, after setting the flags for global use, tf.app.run() simply runs that main function that you pass to it with argv as its parameters.P.S.: As Salvador Dali's answer says, it's just a good software engineering practice, I guess, although I'm not sure whether TensorFlow performs any optimized run of the main function than that was run using normal CPython.2.0 Compatible Answer: If you want to use tf.app.run() in Tensorflow 2.0, we should use the command, tf.compat.v1.app.run() or you can use tf_upgrade_v2 to convert 1.x code to 2.0.Google code depends on a lot on global flags being accessing in libraries/binaries/python scripts and so tf.app.run() parses out those flags to create a global state in FLAGs(or something similar) variable and then calls python main() as it should. If they didn't have this call to tf.app.run(), then users might forget to do FLAGs parsing, leading to these libraries/binaries/scripts not having access to FLAGs they need. 

Why does `if None.__eq__(「a」)` seem to evaluate to True (but not quite)?

The AI Architect

[Why does `if None.__eq__(「a」)` seem to evaluate to True (but not quite)?](https://stackoverflow.com/questions/53984116/why-does-if-none-eq-a-seem-to-evaluate-to-true-but-not-quite)

If you execute the following statement in Python 3.7, it will (from my testing) print b:However, None.__eq__("a") evaluates to NotImplemented.Naturally, "a".__eq__("a") evaluates to True, and "b".__eq__("a") evaluates to False.I initially discovered this when testing the return value of a function, but didn't return anything in the second case -- so, the function returned None.What's going on here?

2018-12-31 06:03:36Z

If you execute the following statement in Python 3.7, it will (from my testing) print b:However, None.__eq__("a") evaluates to NotImplemented.Naturally, "a".__eq__("a") evaluates to True, and "b".__eq__("a") evaluates to False.I initially discovered this when testing the return value of a function, but didn't return anything in the second case -- so, the function returned None.What's going on here?This is a great example of why the __dunder__ methods should not be used directly as they are quite often not appropriate replacements for their equivalent operators; you should use the == operator instead for equality comparisons, or in this special case, when checking for None, use is (skip to the bottom of the answer for more information). You've done Which returns NotImplemented since the types being compared are different. Consider another example where two objects with different types are being compared in this fashion, such as 1 and 'a'. Doing (1).__eq__('a') is also not correct, and will return NotImplemented. The right way to compare these two values for equality would be  What happens here isHere's a nice little MCVE using some custom classes to illustrate how this happens:Of course, that doesn't explain why the operation returns true. This is because NotImplemented is actually a truthy value:Same as,For more information on what values are considered truthy and falsy, see the docs section on Truth Value Testing, as well as this answer. It is worth noting here that NotImplemented is truthy, but it would have been a different story had the class defined a __bool__ or __len__ method that returned False or 0 respectively.If you want the functional equivalent of the == operator, use operator.eq:However, as mentioned earlier, for this specific scenario, where you are checking for None, use is:The functional equivalent of this is using operator.is_:None is a special object, and only 1 version exists in memory at any point of time. IOW, it is the sole singleton of the NoneType class (but the same object may have any number of references). The PEP8 guidelines make this explicit:In summary, for singletons like None, a reference check with is is more appropriate, although both == and is will work just fine.The result you are seeing is caused by that fact thatevaluates to NotImplemented, and NotImplemented's truth value is documented to be True:https://docs.python.org/3/library/constants.htmlIf you call the __eq()__ method manually rather than just using ==, you need to be prepared to deal with the possibility it may return NotImplemented and that its truth value is true. As you already figured None.__eq__("a") evaluates to NotImplemented however if you try something likethe result isthis mean that the truth value of NotImplemented trueTherefor the outcome of the question is obvious:None.__eq__(something) yields NotImplementedAnd bool(NotImplemented) evaluates to TrueSo if None.__eq__("a") is always TrueIt returns a NotImplemented, yeah:But if you look at this:NotImplemented is actually a truthy value, so that's why it returns b, anything that is True will pass, anything that is False wouldn't.You have to check if it is True, so be more suspicious, as you see:So you would do:And as you see, it wouldn't return anything.

Extract a part of the filepath (a directory) in Python

Thalia

[Extract a part of the filepath (a directory) in Python](https://stackoverflow.com/questions/10149263/extract-a-part-of-the-filepath-a-directory-in-python)

I need to extract the name of the parent directory of a certain path. This is what it looks like: I am modifying the content of the "file" with something that uses the directory_i_need name in it (not the path). I have created a function that will give me a list of all the files, and then...How can I do that?

2012-04-13 22:48:00Z

I need to extract the name of the parent directory of a certain path. This is what it looks like: I am modifying the content of the "file" with something that uses the directory_i_need name in it (not the path). I have created a function that will give me a list of all the files, and then...How can I do that?And you can continue doing this as many times as necessary...Edit: from os.path, you can use either os.path.split or os.path.basename:In Python 3.4 you can use the pathlib module:First, see if you have splitunc() as an available function within os.path. The first item returned should be what you want... but I am on Linux and I do not have this function when I import os and try to use it.Otherwise, one semi-ugly way that gets the job done is to use:which shows retrieving the directory just above the file, and the directory just above that.All you need is parent part if you use pathlib.Will output:Case you need all parts (already covered in other answers) use parts:Then you will get a list:Saves tone of time.This is what I did to extract the piece of the directory:Thank you for your help.This should also do the trick.You have to put the entire path as a parameter to os.path.split. See The docs. It doesn't work like string split.

Python ValueError: too many values to unpack [duplicate]

Nik

[Python ValueError: too many values to unpack [duplicate]](https://stackoverflow.com/questions/7053551/python-valueerror-too-many-values-to-unpack)

I am getting that exception from this code:The for line is the one throwing the exception. The ms are Material objects. Anybody have any ideas why?

2011-08-13 21:59:39Z

I am getting that exception from this code:The for line is the one throwing the exception. The ms are Material objects. Anybody have any ideas why?self.materials is a dict and by default you are iterating over just the keys (which are strings).Since self.materials has more than two keys*, they can't be unpacked into the tuple "k, m", hence the ValueError exception is raised.In Python 2.x, to iterate over the keys and the values (the tuple "k, m"), we use self.materials.iteritems().However, since you're throwing the key away anyway, you may as well simply iterate over the dictionary's values:In Python 3.x, prefer dict.values() (which returns a dictionary view object):example:Iterating over a dictionary object itself actually gives you an iterator over its keys. Python is trying to unpack keys, which you get from m.type + m.purity into (m, k).My crystal ball says m.type and m.purity are both strings, so your keys are also strings. Strings are iterable, so they can be unpacked; but iterating over the string gives you an iterator over its characters. So whenever m.type + m.purity is more than two characters long, you have too many values to unpack. (And whenever it's shorter, you have too few values to unpack.)To fix this, you can iterate explicitly over the items of the dict, which are the (key, value) pairs that you seem to be expecting. But if you only want the values, then just use the values.(In 2.x, itervalues, iterkeys, and iteritems are typically a better idea; the non-iter versions create a new list object containing the values/keys/items. For large dictionaries and trivial tasks within the iteration, this can be a lot slower than the iter versions which just set up an iterator.)

If function A is required only by function B should A be defined inside B? [closed]

nukl

[If function A is required only by function B should A be defined inside B? [closed]](https://stackoverflow.com/questions/4831680/if-function-a-is-required-only-by-function-b-should-a-be-defined-inside-b)

Simple example. Two methods, one called from another:In Python we can declare def inside another def.  So, if method_b is required for and called only from method_a, should I declare method_b inside method_a? like this :Or should I avoid doing this?

2011-01-28 18:23:10Z

Simple example. Two methods, one called from another:In Python we can declare def inside another def.  So, if method_b is required for and called only from method_a, should I declare method_b inside method_a? like this :Or should I avoid doing this?Is this what you were looking for? It's called a closure.You don't really gain much by doing this, in fact it slows method_a down because it'll define and recompile the other function every time it's called. Given that, it would probably be better to just prefix the function name with underscore to indicate it's a private method -- i.e. _method_b.I suppose you might want to do this if the nested function's definition varied each time for some reason, but that may indicate a flaw in your design. That said, there is a valid reason to do this to allow the nested function to use arguments that were passed to the outer function but not explicitly passed on to them, which sometimes occurs when writing function decorators, for example. It's what is being shown in the accepted answer although a decorator is not being defined or used.Update:Here's proof that nesting them is slower (using Python 3.6.1), although admittedly not by much in this trivial case:Note I added some self arguments to your sample functions to make them more like real methods (although method_b2 still isn't technically a method of the Test class). Also the nested function is actually called in that version, unlike yours.A function inside of a function is commonly used for closures. (There is a lot of contention over what exactly makes a closure a closure.)Here's an example using the built-in sum(). It defines start once and uses it from then on:In use:    Built-in python closurefunctools.partial is an example of a closure.From the python docs, it's roughly equivalent to:(Kudos to @user225312 below for the answer. I find this example easier to figure out, and hopefully will help answer @mango's comment.)Generally, no, do not define functions inside functions.Unless you have a really good reason. Which you don't. Why not?What is a really good reason to define functions inside functions?When what you actually want is a dingdang closure.It's actually fine to declare one function inside another one. This is specially useful creating decorators.However, as a rule of thumb, if the function is complex (more than 10 lines) it might be a better idea to declare it on the module level.I found this question because I wanted to pose a question why there is a performance impact if one uses nested functions. I ran tests for the following functions using Python 3.2.5 on a  Windows Notebook with a Quad Core 2.5 GHz Intel i5-2530M processorI measured the following 20 times, also for square1, square2, and square5:and got the following resultssquare0 has no nested function, square1 has one nested function, square2 has two nested functions and square5 has five nested functions.  The nested functions are only declared but not called.  So if you have defined 5 nested funtions in a function that you don't call then the execution time of the function is twice of the function without a nested function. I think should be cautious when using nested functions.The Python file for the whole test that generates this output can be found at ideone.It's just a principle about exposure APIs.Using python, It's a good idea to avoid exposure API in outer space(module or class), function is a good encapsulation place. It could be a good idea. when you ensure Even though, Abuse this technique may cause problems and implies a design flaw. Just from my exp, Maybe misunderstand your question. So in the end it is largely a question about how smart the python implementation is or is not, particularly in the case of the inner function not being a closure but simply an in function needed helper only. In clean understandable design having functions only where they are needed and not exposed elsewhere is good design whether they be embedded in a module, a class as a method, or inside another function or method.  When done well they really improve the clarity of the code.And when the inner function is a closure that can also help with clarity quite a bit even if that function is not returned out of the containing function for use elsewhere. So I would say generally do use them but be aware of the possible performance hit when you actually are concerned about performance and only remove them if you do actual profiling that shows they best be removed. Do not do premature optimization of just using "inner functions BAD" throughout all python code you write.  Please. It's perfectly OK doing it that way, but unless you need to use a closure or return the function I'd probably put in the module level. I imagine in the second code example you mean:otherwise, some_data will be the function.Having it at the module level will allow other functions to use method_b() and if you're using something like Sphinx (and autodoc) for documentation, it will allow you to document method_b as well.You also may want to consider just putting the functionality in two methods in a class if you're doing something that can be representable by an object.  This contains logic well too if that's all you're looking for.Do something like:if you were to run some_function() it would then run some_other_function() and returns 42.EDIT: I originally stated that you shouldn't define a function inside of another but it was pointed out that it is practical to do this sometimes.You can use it to avoid defining global variables. This gives you an alternative for other designs. 3 designs presenting a solution to a problem.Solution C) allows to use variables in the scope of the outer function without having the need to declare them in the inner function. Might be useful in some situations.      Function In function python

Find p-value (significance) in scikit-learn LinearRegression

elplatt

[Find p-value (significance) in scikit-learn LinearRegression](https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression)

How can I find the p-value (significance) of each coefficient?

2015-01-13 17:46:13Z

How can I find the p-value (significance) of each coefficient?This is kind of overkill but let's give it a go.  First lets use statsmodel to find out what the p-values should beand we getOk, let's reproduce this.  It is kind of overkill as we are almost reproducing a linear regression analysis using Matrix Algebra.  But what the heck.And this gives us.So we can reproduce the values from statsmodel.scikit-learn's LinearRegression doesn't calculate this information but you can easily extend the class to do it:Stolen from here. You should take a look at statsmodels for this kind of statistical analysis in Python.EDIT: Probably not the right way to do it, see commentsYou could use sklearn.feature_selection.f_regression.click here for the scikit-learn pageThe code in elyase's answer https://stackoverflow.com/a/27928411/4240413 does not actually work.  Notice that sse is a scalar, and then it tries to iterate through it.  The following code is a modified version.  Not amazingly clean, but I think it works more or less.An easy way to pull of the p-values is to use statsmodels regression:You get a series of p-values that you can manipulate (for example choose the order you want to keep by evaluating each p-value):p_value is among f statistics. if you want to get the value, simply use this few lines of code:You can use scipy for p-value. This code is from scipy documentation. There could be a mistake in @JARH's answer in the case of a multivariable regression. 

(I do not have enough reputation to comment.)In the following line:p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b],  the t-values follows a chi-squared distribution of degree len(newX)-1 instead of following a chi-squared distribution of degree len(newX)-len(newX.columns)-1.So this should be: p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-len(newX.columns)-1))) for i in ts_b](See t-values for OLS regression for more details)

python generator「send」function purpose?

Tommy

[python generator「send」function purpose?](https://stackoverflow.com/questions/19302530/python-generator-send-function-purpose)

Can someone give me an example of why the "send" function associated with Python generator function exists? I fully understand the yield function. However, the send function is confusing to me. The documentation on this method is convoluted: What does that mean? I thought value was the input to the function? The phrase "The send() method returns the next value yielded by the generator" seems to be also the exact purpose of the yield function; yield returns the next value yielded by the generator...Can someone give me an example of a generator utilizing send that accomplishes something yield cannot? 

2013-10-10 17:38:19Z

Can someone give me an example of why the "send" function associated with Python generator function exists? I fully understand the yield function. However, the send function is confusing to me. The documentation on this method is convoluted: What does that mean? I thought value was the input to the function? The phrase "The send() method returns the next value yielded by the generator" seems to be also the exact purpose of the yield function; yield returns the next value yielded by the generator...Can someone give me an example of a generator utilizing send that accomplishes something yield cannot? It's used to send values into a generator that just yielded. Here is an artificial (non-useful) explanatory example:You can't do this just with yield.As to why it's useful, one of the best use cases I've seen is Twisted's @defer.inlineCallbacks. Essentially it allows you to write a function like this:What happens is that takesTwoSeconds() returns a Deferred, which is a value promising a value will be computed later. Twisted can run the computation in another thread. When the computation is done, it passes it into the deferred, and the value then gets sent back to the doStuff() function. Thus the doStuff() can end up looking more or less like a normal procedural function, except it can be doing all sorts of computations & callbacks etc. The alternative before this functionality would be to do something like:It's a lot more convoluted and unwieldy.This function is to write coroutinesprintsSee how the control is being passed back and forth? Those are coroutines. They can be used for all kinds of cool things like asynch IO and similar.Think of it like this, with a generator and no send, it's a one way streetBut with send, it becomes a two way streetWhich opens up the door to the user customizing the generators behavior on the fly and the generator responding to the user.This may help someone. Here is a generator that is unaffected by send function. It takes in the number parameter on instantiation and is unaffected by send:Now here is how you would do the same type of function using send, so on each iteration you can change the value of number:Here is what that looks like, as you can see sending a new value for number changes the outcome:You can also put this in a for loop as such:For more help check out this great tutorial.Generators with send() allow:Here are some use cases:Let us have a recipe, which expects predefined set of inputs in some order.We may:To use it, first create the watched_attempt instance:The call to .next() is necessary to start execution of the generator.Returned value shows, our pot is currently empty.Now do few actions following what the recipe expects:As we see, the pot is finally empty.In case, one would not follow the recipe, it would fail (what could be desired outcome of watched

attempt to cook something - just learning we did not pay enough attention when given instructions.Notice, that:We may use the generator to keep track of running total of values sent to it.Any time we add a number, count of inputs and total sum is returned (valid for

the moment previous input was send into it).The output would look like:The send() method controls what the value to the left of the yield expression will be.To understand how yield differs and what value it holds, lets first quickly refresh on the order python code is evaluated.Section 6.15 Evaluation orderSo an expression a = b the right hand side is evaluated first.As the following demonstrates that a[p('left')] = p('right') the right hand side is evaluated first.What does yield do?, yield, suspends execution of the function and returns to the caller, and resumes execution at the same place it left off prior to suspending.Where exactly is execution suspended? You might have guessed it already...

the execution is suspended between the right and left side of the yield expression. So new_val = yield old_val the execution is halted at the = sign, and the value on the right (which is before suspending, and is also the value returned to the caller) may be something different then the value on the left (which is the value being assigned after resuming execution). yield yields 2 values, one to the right and another to the left.How do you control the value to the left hand side of the yield expression? via the .send() method.6.2.9. Yield expressionsThe send method implements coroutines.If you haven't encountered Coroutines they are tricky to wrap your head around because they change the way a program flows. You can read a good tutorial for more details.These confused me too.  Here is an example I made when trying to set up a generator which yields and accepts signals in alternating order (yield, accept, yield, accept)...The output is:It should first be noted that for any generator g, next(g) is exactly equivalent to g.send(None). With this in mind we can focus only on how send works and talk only about advancing the generator with send.Suppose we haveWe will imagine g as a cursor within f. After the line g = f(1), g is located at the very start of f's body (just after the colon in def f(y):). The argument 1 in g = f(1) has been captured normally and assigned to y within f's body, but the while True has not begun yet.Now, each call of g.send(z) does the following, in order:For the specific g above, this manifests as:Each time a f yields a value, execution pauses – f yields to us as a car yields to pedestrians – and is resumed by the next call to g.send, by which we yield back to it and deliver a value to it. In this regard, send and yield can be thought of as opposites of each other; they send data in opposite directions and relinquish execution to each other.This wait-capture-advance-yield-wait loop continues until f exits (or returns). Capture the value sent, continue execution until the next yield, yield the value there, wait for a send, capture the value sent, continue execution until the next yield, yield the value there, wait for a send, capture the value sent, continue execution until the next yield, yield the value there, wait for a send...If the above makes sense, then even the two below tricky-looking examples become clear.In the first, the value passed to f1 is yielded initially, and then all values sent are yielded right back. In the second, x has no value (yet) when it first come times to yield, so an UnboundLocalError is raised.Finally, two special cases: yield y and x = yield. In the first, any values sent by send are ignored, as yield y is not assigned to anything after being evaluated. The second is just equivalent to x = yield None.

Read first N lines of a file in python

Russell

[Read first N lines of a file in python](https://stackoverflow.com/questions/1767513/read-first-n-lines-of-a-file-in-python)

We have a large raw data file that we would like to trim to a specified size.

I am experienced in .net c#, however would like to do this in python to simplify things and out of interest.How would I go about getting the first N lines of a text file in python?

Will the OS being used have any effect on the implementation?

2009-11-20 00:09:32Z

We have a large raw data file that we would like to trim to a specified size.

I am experienced in .net c#, however would like to do this in python to simplify things and out of interest.How would I go about getting the first N lines of a text file in python?

Will the OS being used have any effect on the implementation?Python 2Python 3Here's another way (both Python 2 & 3)If you want to read the first lines quickly and you don't care about performance you can use .readlines() which returns list object and then slice the list.E.g. for the first 5 lines:What I do is to call the N lines using pandas. I think the performance is not the best, but for example if N=1000:There is no specific method to read number of lines exposed by file object. I guess the easiest way would be following: Based on gnibbler top voted answer (Nov 20 '09 at 0:27): this class add head() and tail() method to file object.Usage:The two most intuitive ways of doing this would be:Here is the code:The bottom line is, as long as you don't use readlines() or enumerateing the whole file into memory, you have plenty of options.most convinient way on my own:Solution based on List Comprehension

The function open() supports an iteration interface. The enumerate() covers open() and return tuples (index, item), then we check that we're inside an accepted range (if i < LINE_COUNT) and then simply print the result.Enjoy the Python. ;)For first 5 lines, simply do:If you want something that obviously (without looking up esoteric stuff in manuals) works without imports and try/except and works on a fair range of Python 2.x versions (2.2 to 2.6):Starting at Python 2.6, you can take advantage of more sophisticated functions in the IO base clase.  So the top rated answer above can be rewritten as:(You don't have to worry about your file having less than N lines since no StopIteration exception is thrown.)If you have a really big file, and assuming you want the output to be a numpy array, using np.genfromtxt will freeze your computer. This is so much better in my experience:This Method Worked for me This worked for me This works for Python 2 & 3:If you want it without with statement:

Drop all duplicate rows in Python Pandas

Jamie Bull

[Drop all duplicate rows in Python Pandas](https://stackoverflow.com/questions/23667369/drop-all-duplicate-rows-in-python-pandas)

The pandas drop_duplicates function is great for "uniquifying" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible?As an example, I would like to drop rows which match on columns A and C so this should drop rows 0 and 1.

2014-05-15 00:31:47Z

The pandas drop_duplicates function is great for "uniquifying" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible?As an example, I would like to drop rows which match on columns A and C so this should drop rows 0 and 1.This is much easier in pandas now with drop_duplicates and the keep parameter.Just want to add to Ben's answer on drop_duplicates:keep : {‘first’, ‘last’, False}, default ‘first’So setting keep to False will give you desired answer.If you want result to be stored in another dataset:orIf same dataset needs to be updated:Above examples will remove all duplicates and keep one, similar to DISTINCT * in SQLuse groupby and filterActually, drop rows 0 and 1 only requires (any observations containing matched A and C is kept.):But I suspect what you really want is this (one observation containing matched A and C is kept.):Now it is much clearer, therefore:

How to calculate a mod b in Python?

Hick

[How to calculate a mod b in Python?](https://stackoverflow.com/questions/991027/how-to-calculate-a-mod-b-in-python)

Is there a modulo function in the Python math library?Isn't 15 % 4, 3? But 15 mod 4 is 1, right?

2009-06-13 16:57:21Z

Is there a modulo function in the Python math library?Isn't 15 % 4, 3? But 15 mod 4 is 1, right?There's the % sign.  It's not just for the remainder, it is the modulo operation.you can also try divmod(x, y) which returns a tuple (x // y, x % y)The modulo gives the remainder after integer division.mod = a % bThis stores the result of a mod b in the variable mod.And you are right, 15 mod 4 is 3, which is exactly what python returns:a %= b is also valid.Why don't you use % ?I don't think you're fully grasping modulo. a % b and a mod b are just two different ways to express modulo. In this case, python uses %. No, 15 mod 4 is not 1, 15 % 4 == 15 mod 4 == 3.output:

How to enumerate a range of numbers starting at 1

JeffTaggary

[How to enumerate a range of numbers starting at 1](https://stackoverflow.com/questions/3303608/how-to-enumerate-a-range-of-numbers-starting-at-1)

I am using Python 2.5, I want an enumeration like so (starting at 1 instead of 0):I know in Python 2.6 you can do: h = enumerate(range(2000, 2005), 1) to give the above result but in python2.5 you cannot...Using python2.5:Does anyone know a way to get that desired result in python 2.5?Thanks,Jeff

2010-07-21 20:37:35Z

I am using Python 2.5, I want an enumeration like so (starting at 1 instead of 0):I know in Python 2.6 you can do: h = enumerate(range(2000, 2005), 1) to give the above result but in python2.5 you cannot...Using python2.5:Does anyone know a way to get that desired result in python 2.5?Thanks,JeffAs you already mentioned, this is straightforward to do in Python 2.6 or newer:Python 2.5 and older do not support the start parameter so instead you could create two range objects and zip them:Result:If you want to create a generator instead of a list then you can use izip instead.Just to put this here for posterity sake, in 2.6 the "start" parameter was added to enumerate like so:enumerate(sequence, start=1)Easy, just define your own function that does what you want:Simplest way to do in Python 2.5 exactly what you ask about:If you want a list, as you appear to, use zip in lieu of it.izip.(BTW, as a general rule, the best way to make a list out of a generator or any other iterable X is not [x for x in X], but rather list(X)).Now h = list(enumerate(xrange(2000, 2005), 1)) works.enumerate is trivial, and so is re-implementing it to accept a start:Note that this doesn't break code using enumerate without start argument. Alternatively, this oneliner may be more elegant and possibly faster, but breaks other uses of enumerate:The latter was pure nonsense. @Duncan got the wrapper right.Official documentation: enumerate(iterable, start=0)So you would use it like this:h = [(i + 1, x) for i, x in enumerate(xrange(2000, 2005))]Ok, I feel a bit stupid here... what's the reason not to just do it with something like 

[(a+1,b) for (a,b) in enumerate(r)] ? If you won't function, no problem either:Since this is somewhat verbose, I'd recommend writing your own function to generalize it:I don't know how these posts could possibly be made more complicated then the following:

What's the common practice for enums in Python? [duplicate]

Joan Venge

[What's the common practice for enums in Python? [duplicate]](https://stackoverflow.com/questions/702834/whats-the-common-practice-for-enums-in-python)

What's the common practice for enums in Python? I.e. how are they replicated in Python?

2009-03-31 20:14:17Z

What's the common practice for enums in Python? I.e. how are they replicated in Python?I've seen this pattern several times:You can also just use class members, though you'll have to supply your own numbering:If you're looking for something more robust (sparse values, enum-specific exception, etc.), try this recipe.I have no idea why Enums are not support natively by Python.

The best way I've found to emulate them is by overridding _ str _ and _ eq _ so you can compare them and when you use print() you get the string instead of the numerical value.Usage:You could probably use an inheritance structure although the more I played with this the dirtier I felt.

Split list into smaller lists (split in half)

corymathews

[Split list into smaller lists (split in half)](https://stackoverflow.com/questions/752308/split-list-into-smaller-lists-split-in-half)

I am looking for a way to easily split a python list in half.So that if I have an array:I would be able to get:

2009-04-15 15:44:40Z

I am looking for a way to easily split a python list in half.So that if I have an array:I would be able to get:If you want a function:A little more generic solution (you can specify the number of parts you want, not just split 'in half'):EDIT: updated post to handle odd list lengthsEDIT2: update post again based on Brians informative comments n - the predefined length of result arrays Test:result:B,C=A[:len(A)/2],A[len(A)/2:]Here is a common solution, split arr into count partIf you don't care about the order...  list[::2] gets every second element in the list starting from the 0th element.

list[1::2] gets every second element in the list starting from the 1st element.I tested, and the double slash is required to force int division in python 3. My original post was correct, although wysiwyg broke in Opera, for some reason.There is an official Python receipe for the more generalized case of splitting an array into smaller arrays of size n.This code snippet is from the python itertools doc page.Using list slicing. The syntax is basically my_list[start_index:end_index]To get the first half of the list, you slice from the first index to len(i)//2 (where // is the integer division - so 3//2 will give the floored result of1, instead of the invalid list index of1.5`):..and the swap the values around to get the second half:If you have a big list, It's better to use itertools and write a function to yield each part as needed:You can use this like:The output is:Thanks to @thefourtheye and @Bede Constantinides10 years later.. I thought - why not add another:While the answers above are more or less correct, you may run into trouble if the size of your array isn't divisible by 2, as the result of a / 2, a being odd, is a float in python 3.0, and in earlier version if you specify from __future__ import division at the beginning of your script. You are in any case better off going for integer division, i.e. a // 2, in order to get "forward" compatibility of your code.With hints from @ChristopheDThis is similar to other solutions, but a little faster.

Sphinx autodoc is not automatic enough

Cory Walker

[Sphinx autodoc is not automatic enough](https://stackoverflow.com/questions/2701998/sphinx-autodoc-is-not-automatic-enough)

I'm trying to use Sphinx to document a 5,000+ line project in Python. It has about 7 base modules. As far as I know, In order to use autodoc I need to write code like this for each file in my project:This is way too tedious because I have many files. It would be much easier if I could just specify that I wanted the 'mods' package to be documented. Sphinx could then recursively go through the package and make a page for each submodule.Is there a feature like this? If not I could write a script to make all the .rst files, but that would take up a lot of time.

2010-04-23 21:11:53Z

I'm trying to use Sphinx to document a 5,000+ line project in Python. It has about 7 base modules. As far as I know, In order to use autodoc I need to write code like this for each file in my project:This is way too tedious because I have many files. It would be much easier if I could just specify that I wanted the 'mods' package to be documented. Sphinx could then recursively go through the package and make a page for each submodule.Is there a feature like this? If not I could write a script to make all the .rst files, but that would take up a lot of time.You can check this script that I've made. I think it can help you.This script parses a directory tree looking for python modules and packages and creates ReST files appropriately to create code documentation with Sphinx. It also creates a modules index.UPDATEThis script is now part of Sphinx 1.1 as apidoc.I do not know whether Sphinx had had autosummary extension at the time original question was asked, but for now it is quite possible to set up automatic generation of that kind without using sphinx-apidoc or similar script. Below there are settings which work for one of my projects.In conclusion, there is no need to keep _autosummary directory under version control. Also, you may name it anything you want and place it anywhere in the source tree (putting it below _build will not work, though).In each package, the __init__.py file can have ..  automodule:: package.module components for each part of the package.Then you can ..  automodule:: package and it mostly does what you want.Sphinx AutoAPI does exactly this.Maybe what you're looking for is Epydoc and this Sphinx extension.

PyLint message: logging-format-interpolation

pfnuesel

[PyLint message: logging-format-interpolation](https://stackoverflow.com/questions/34619790/pylint-message-logging-format-interpolation)

For the following code:pylint produces the following warning:I know I can turn off this warning, but I'd like to understand it. I assumed using format() is the preferred way to print out statements in Python 3. Why is this not true for logger statements?

2016-01-05 19:31:32Z

For the following code:pylint produces the following warning:I know I can turn off this warning, but I'd like to understand it. I assumed using format() is the preferred way to print out statements in Python 3. Why is this not true for logger statements?It is not true for logger statement because it relies on former "%" format like string to provide lazy interpolation of this string using extra arguments given to the logger call. For instance instead of doing:you should doso  the string will only be interpolated if the message is actually emitted.You can't benefit of this functionality when using .format().Per the Optimization section of the logging docs:Maybe this time differences can help you.Following description is not the answer for your question, but it can help people.There are 3 options for logging style in the .pylintrc file: old, new, fstr.Description from .pylintrc file:for old (logging-format-style=old):for new (logging-format-style=new):Note: you can not use .format() even if you select new option.pylint still gives the same warning for this code:for fstr (logging-format-style=fstr):Personally, I prefer fstr option because of PEP-0498.In my experience a more compelling reason than optimization (for most use cases) for the lazy interpolation is that it plays nicely with log aggregators like Sentry.Consider a 'user logged in' log message. If you interpolate the user into the format string, you have as many distinct log messages as there are users. If you use lazy interpolation like this, the log aggregator can more reasonably interpret this as the same log message with a bunch of different instances.

Is there a python equivalent of Ruby's 'rvm'?

conny

[Is there a python equivalent of Ruby's 'rvm'?](https://stackoverflow.com/questions/2812471/is-there-a-python-equivalent-of-rubys-rvm)

Q: Do we have anything functionally equivalent in Python to the Ruby version manager 'rvm'? (RVM lets you easily switch completely between different versions of the ruby interpreter and different sets of gems (modules). Everything concerning download-build-install-switch  of interpreter(-s) and gems gets taken care of by invoking rvm. It is all run under your regular user account.)

2010-05-11 16:25:35Z

Q: Do we have anything functionally equivalent in Python to the Ruby version manager 'rvm'? (RVM lets you easily switch completely between different versions of the ruby interpreter and different sets of gems (modules). Everything concerning download-build-install-switch  of interpreter(-s) and gems gets taken care of by invoking rvm. It is all run under your regular user account.)Yes, it is virtualenv along with virtualenvwrapper.update: you may install both at once with virtualenv burrito.Update: the correct answer is now probably pyenv.For scientific computing, the corresponding tool is anaconda.pythonbrew has come!

http://github.com/utahta/pythonbrewpyenv: https://github.com/yyuu/pyenvI created pyenv which is a fork of Ruby's rbenv and modified for Python. Like pythonz, pyenv also supports Stackless, PyPy, and Jython.Following up on hytdsh's answer (nearly two years later)...pythonz a fork of pythonbrew that adds support for Stackless, PyPy, and Jython.If you like how rvm handles different interpreters, it may worth taking a look at pythonz. If you're strictly working with CPython, the difference is less significant.Optionally, if you're using Macports you can use python_select.

Install python_select with:Assuming python 2.6 and 2.5 have bee installed via Macports you can switch pythons like so:pyenv 1.2.15 can build and install Python for youMaybe this was not possible in earlier version, but when I tested in 1.2.15 I noticed the pyenv install option which worked:Therefore I feel that it is reasonable to call pyenv a replacement for RVM.I have given a fully detailed example of its usage at: apt-get install for different python versions

How to JSON serialize sets?

DeaconDesperado

[How to JSON serialize sets?](https://stackoverflow.com/questions/8230315/how-to-json-serialize-sets)

I have a Python set that contains objects with __hash__ and __eq__ methods in order to make certain no duplicates are included in the collection.I need to json encode this result set, but passing even an empty set to the json.dumps method raises a TypeError.I know I can create an extension to the json.JSONEncoder class that has a custom default method, but I'm not even sure where to begin in converting over the set.  Should I create a dictionary out of the set values within the default method, and then return the encoding on that?  Ideally, I'd like to make the default method able to handle all the datatypes that the original encoder chokes on (I'm using Mongo as a data source so dates seem to raise this error too)Any hint in the right direction would be appreciated.EDIT:Thanks for the answer!  Perhaps I should have been more precise.I utilized (and upvoted) the answers here to get around the limitations of the set being translated, but there are internal keys that are an issue as well.The objects in the set are complex objects that translate to __dict__, but they themselves can also contain values for their properties that could be ineligible for the basic types in the json encoder.There's a lot of different types coming into this set, and the hash basically calculates a unique id for the entity, but in the true spirit of NoSQL there's no telling exactly what the child object contains.One object might contain a date value for starts, whereas another may have some other schema that includes no keys containing "non-primitive" objects.That is why the only solution I could think of was to extend the JSONEncoder to replace the default method to turn on different cases - but I'm not sure how to go about this and the documentation is ambiguous.  In nested objects, does the value returned from default go by key, or is it just a generic include/discard that looks at the whole object?  How does that method accommodate nested values?  I've looked through previous questions and can't seem to find the best approach to case-specific encoding (which unfortunately seems like what I'm going to need to do here).

2011-11-22 16:38:01Z

I have a Python set that contains objects with __hash__ and __eq__ methods in order to make certain no duplicates are included in the collection.I need to json encode this result set, but passing even an empty set to the json.dumps method raises a TypeError.I know I can create an extension to the json.JSONEncoder class that has a custom default method, but I'm not even sure where to begin in converting over the set.  Should I create a dictionary out of the set values within the default method, and then return the encoding on that?  Ideally, I'd like to make the default method able to handle all the datatypes that the original encoder chokes on (I'm using Mongo as a data source so dates seem to raise this error too)Any hint in the right direction would be appreciated.EDIT:Thanks for the answer!  Perhaps I should have been more precise.I utilized (and upvoted) the answers here to get around the limitations of the set being translated, but there are internal keys that are an issue as well.The objects in the set are complex objects that translate to __dict__, but they themselves can also contain values for their properties that could be ineligible for the basic types in the json encoder.There's a lot of different types coming into this set, and the hash basically calculates a unique id for the entity, but in the true spirit of NoSQL there's no telling exactly what the child object contains.One object might contain a date value for starts, whereas another may have some other schema that includes no keys containing "non-primitive" objects.That is why the only solution I could think of was to extend the JSONEncoder to replace the default method to turn on different cases - but I'm not sure how to go about this and the documentation is ambiguous.  In nested objects, does the value returned from default go by key, or is it just a generic include/discard that looks at the whole object?  How does that method accommodate nested values?  I've looked through previous questions and can't seem to find the best approach to case-specific encoding (which unfortunately seems like what I'm going to need to do here).JSON notation has only a handful of native datatypes (objects, arrays, strings, numbers, booleans, and null), so anything serialized in JSON needs to be expressed as one of these types.As shown in the json module docs, this conversion can be done automatically by a JSONEncoder and JSONDecoder, but then you would be giving up some other structure you might need (if you convert sets to a list, then you lose the ability to recover regular lists; if you convert sets to a dictionary using dict.fromkeys(s) then you lose the ability to recover dictionaries).A more sophisticated solution is to build-out a custom type that can coexist with other native JSON types.  This lets you store nested structures that include lists, sets, dicts, decimals, datetime objects, etc.:Here is a sample session showing that it can handle lists, dicts, and sets:Alternatively, it may be useful to use a more general purpose serialization technique such as YAML, Twisted Jelly, or Python's pickle module.  These each support a much greater range of datatypes.You can create a custom encoder that returns a list when it encounters a set. Here's an example:You can detect other types this way too. If you need to retain that the list was actually a set, you could use a custom encoding. Something like return {'type':'set', 'list':list(obj)} might work.To illustrated nested types, consider serializing this:This raises the following error:This indicates that the encoder will take the list result returned and recursively call the serializer on its children. To add a custom serializer for multiple types, you can do this:Only dictionaries, Lists and primitive object types (int, string, bool) are available in JSON.I adapted Raymond Hettinger's solution to python 3.Here is what has changed:If you only need to encode sets, not general Python objects, and want to keep it easily human-readable, a  simplified version of Raymond Hettinger's answer can be used:You don't need to make a custom encoder class to supply the default method - it can be passed in as a keyword argument:results in [1, 2, 3] in all supported Python versions.One shortcoming of the accepted solution is that its output is very python specific. I.e. its raw json output cannot be observed by a human or loaded by another language (e.g. javascript).

example:Will get you:I can propose a solution which downgrades the set to a dict containing a list on the way out, and back to a set when loaded into python using the same encoder, therefore preserving observability and language agnosticism:Which gets you:Note that serializing a dictionary which has an element with a key "__set__" will break this mechanism. So __set__ has now become a reserved dict key. Obviously feel free to use another, more deeply obfuscated key.If you need just quick dump and don't want to implement custom encoder. You can use the following: json_string = json.dumps(data, iterable_as_array=True)This will convert all sets (and other iterables) into arrays. Just beware that those fields will stay arrays when you parse the json back. If you want to preserve the types, you need to write custom encoder.

warning about too many open figures

andreas-h

[warning about too many open figures](https://stackoverflow.com/questions/21884271/warning-about-too-many-open-figures)

In a script where I create many figures with fix, ax = plt.subplots(...), I get the warning RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (matplotlib.pyplot.figure) are retained until explicitly closed and may consume too much memory. However, I don't understand why I get this warning, because after saving the figure with fig.savefig(...), I delete it with fig.clear(); del fig. At no point in my code, I have more than one figure open at a time. Still, I get the warning about too many open figures.  What does that mean / how can I avoid getting the warning?

2014-02-19 15:00:45Z

In a script where I create many figures with fix, ax = plt.subplots(...), I get the warning RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (matplotlib.pyplot.figure) are retained until explicitly closed and may consume too much memory. However, I don't understand why I get this warning, because after saving the figure with fig.savefig(...), I delete it with fig.clear(); del fig. At no point in my code, I have more than one figure open at a time. Still, I get the warning about too many open figures.  What does that mean / how can I avoid getting the warning?Use .clf or .cla on your figure object instead of creating a new figure. From @DavidZwickerAssuming you have imported pyplot asplt.cla() clears an axis, i.e. the currently active axis in the current figure. It leaves the other axes untouched.plt.clf() clears the entire current figure with all its axes, but leaves the window opened, such that it may be reused for other plots.plt.close() closes a window, which will be the current window, if not specified otherwise. plt.close('all') will close all open figures.The reason that del fig does not work is that the pyplot state-machine keeps a reference to the figure around (as it must if it is going to know what the 'current figure' is). This means that even if you delete your ref to the figure, there is at least one live ref, hence it will never be garbage collected.Since I'm polling on the collective wisdom here for this answer, @JoeKington mentions in the comments that plt.close(fig) will remove a specific figure instance from the pylab state machine (plt._pylab_helpers.Gcf) and allow it to be garbage collected. Here's a bit more detail to expand on Hooked's answer. When I first read that answer, I missed the instruction to call clf() instead of creating a new figure. clf() on its own doesn't help if you then go and create another figure.Here's a trivial example that causes the warning:To avoid the warning, I have to pull the call to subplots() outside the loop. In order to keep seeing the rectangles, I need to switch clf() to cla(). That clears the axis without removing the axis itself.If you're generating plots in batches, you might have to use both cla() and close(). I ran into a problem where a batch could have more than 20 plots without complaining, but it would complain after 20 batches. I fixed that by using cla() after each plot, and close() after each batch.I measured the performance to see if it was worth reusing the figure within a batch, and this little sample program slowed from 41s to 49s (20% slower) when I just called close() after every plot.If you intend to knowingly keep many plots in memory, but don't want to be warned about it, you can update your options prior to generating figures.This will prevent the warning from being emitted without changing anything about the way memory is managed.The following snippet solved the issue for me:When _wrapped_figure goes out of scope the runtime calls our __del__() method with plt.close() inside. It happens even if exception fires after _wrapped_figure constructor. 

argparse: identify which subparser was used [duplicate]

user1062880

[argparse: identify which subparser was used [duplicate]](https://stackoverflow.com/questions/8250010/argparse-identify-which-subparser-was-used)

I think this must be easy but I do not get it.Assume I have the following arparse parser:How can I identify, which subparser was used?

calling:gives me an empty namespace:

2011-11-23 22:33:52Z

I think this must be easy but I do not get it.Assume I have the following arparse parser:How can I identify, which subparser was used?

calling:gives me an empty namespace:Edit: Please see quornian's answer to this question, which is better than mine and should be the accepted answer.According to the argparse documentation the result of parser.parse_args(...) will "only contain attributes for the main parser and the sub parser that was selected". Unfortunately this may not be enough information to determine which sub parser was used. The documentation recommends using the set_defaults(...) method on the sub parser to solve this problem.For example, I've added calls to set_defaults() to your code:Now if you runThe result isCheck out the add_subparsers() documentation for more information and another example.A simpler solution is to add dest to the add_subparsers call. This is buried a bit further down in the documentation:In your example replace:with:Now if you run:you will get

How to execute multi-line statements within Python's own debugger (PDB)

Mike

[How to execute multi-line statements within Python's own debugger (PDB)](https://stackoverflow.com/questions/5967241/how-to-execute-multi-line-statements-within-pythons-own-debugger-pdb)

So I am running a Python script within which I am calling Python's debugger, PDB by writing:(iPython's version of PDB, though for the matter I don't think it makes a difference; I use it for the colored output only).Now, when I get to the debugger I want to execute a multi-line statement such as an if clause or a for loop but as soon as I typeand hit the return key, I get the error message *** SyntaxError: invalid syntax (<stdin>, line 1)How can one execute multi-line statements within PDB? If not possible is there a way around this to still executing an if clause or a for loop?

2011-05-11 16:01:28Z

So I am running a Python script within which I am calling Python's debugger, PDB by writing:(iPython's version of PDB, though for the matter I don't think it makes a difference; I use it for the colored output only).Now, when I get to the debugger I want to execute a multi-line statement such as an if clause or a for loop but as soon as I typeand hit the return key, I get the error message *** SyntaxError: invalid syntax (<stdin>, line 1)How can one execute multi-line statements within PDB? If not possible is there a way around this to still executing an if clause or a for loop?You could do this while in pdb to launch a temporary interactive Python session with all the local variables available:When you're done, use Ctrl-D to return to the regular pdb prompt.Just don't hit Ctrl-C, that will terminate the entire pdb session.In python3 ipdb (and pdb) have a command called interact. It can be used to: To use it, simply enter interact at the pdb prompt. Among other things, it's useful for applying code spanning multiple lines, and also for avoiding accidental triggering of other pdb commands.My recommendation is to use IPython embedding.Inside the Python (2.7.1) interpreter or debugger (import pdb), you can execute a multi-line statement with the following syntax.Note: When I'm inside the interpreter, I have to hit return twice before the code will execute.  Inside the debugger, however, I only have to hit return once.There is the special case if you want a couple of commands be executed when hitting a break point. Then there is the debugger command commands. It allows you to enter multiple lines of commands and then end the whole sequence with the end key word. More with (pdb) help commands.I don't know if you can do this, that'd be a great feature for ipdb though. You can use list comprehensions of course, and execute simple multi-line expressions like:You could also write some functions beforehand to do whatever it is you need done that would normally take multiple lines.Write your code in a text editor, then paste it into the debugger:It is a silly solution, but also dirty quick and work. It works on Linux terminal, but I am not sure if it will work on Windows console.

Why is「import *」bad?

Software Enthusiastic

[Why is「import *」bad?](https://stackoverflow.com/questions/2386714/why-is-import-bad)

It is recommended to not to use import * in Python. Can anyone please share the reason for that, so that I can avoid it doing next time?

2010-03-05 12:40:41Z

It is recommended to not to use import * in Python. Can anyone please share the reason for that, so that I can avoid it doing next time?According to the Zen of Python:... can't argue with that, surely?You don't pass **locals() to functions, do you?Since Python lacks an "include" statement, and the self parameter is explicit, and scoping rules are quite simple, it's usually very easy to point a finger at a variable and tell where that object comes from -- without reading other modules and without any kind of IDE (which are limited in the way of introspection anyway, by the fact the language is very dynamic).The import * breaks all that.Also, it has a concrete possibility of hiding bugs.Now, if the bar module has any of the "os", "mystuff", etc... attributes, they will override the explicitly imported ones, and possibly point to very different things. Defining __all__ in bar is often wise -- this states what will implicitly be imported - but still it's hard to trace where objects come from, without reading and parsing the bar module and following its imports. A network of import * is the first thing I fix when I take ownership of a project.Don't misunderstand me: if the import * were missing, I would cry to have it. But it has to be used carefully. A good use case is to provide a facade interface over another module.

Likewise, the use of conditional import statements, or imports inside function/class namespaces, requires a bit of discipline.I think in medium-to-big projects, or small ones with several contributors, a minimum of hygiene is needed in terms of statical analysis -- running at least pyflakes or even better a properly configured pylint -- to catch several kind of bugs before they happen.Of course since this is python -- feel free to break rules, and to explore -- but be wary of projects that could grow tenfold, if the source code is missing discipline it will be a problem.It is OK to do from ... import * in an interactive session.That is because you are polluting the namespace. You will import all the functions and classes in your own namespace, which may clash with the functions you define yourself.Furthermore, I think using a qualified name is more clear for the maintenance task; you see on the code line itself where a function comes from, so you can check out the docs much more easily.In module foo:In your code:http://docs.python.org/tutorial/modules.htmlSay you have the following code in a module called foo:and then in your own module you have:You now have a difficult-to-debug module that looks like it has lxml's etree in it, but really has ElementTree instead.These are all good answers. I'm going to add that when teaching new people to code in Python, dealing with import * is very difficult. Even if you or they didn't write the code, it's still a stumbling block.I teach children (about 8 years old) to program in Python to manipulate Minecraft. I like to give them a helpful coding environment to work with (Atom Editor) and teach REPL-driven development (via bpython). In Atom I find that the hints/completion works just as effectively as bpython. Luckily, unlike some other statistical analysis tools, Atom is not fooled by import *.However, lets take this example... In this wrapper they from local_module import * a bunch modules including this list of blocks. Let's ignore the risk of namespace collisions. By doing from mcpi.block import * they make this entire list of obscure types of blocks something that you have to go look at to know what is available. If they had instead used from mcpi import block, then you could type walls = block. and then an autocomplete list would pop up.

Understood the valid points people put here. However, I do have one argument that, sometimes, "star import" may not always be a bad practice:It is a very BAD practice for two reasons:For point 1:

Let's see an example of this:Here, on seeing the code no one will get idea regarding from which module b, c and d actually belongs. On the other way, if you do it like:It is much cleaner for you, and also the new person joining your team will have better idea.For point 2: Let say both module1 and module2 have variable as b. When I do:Here the value from module1 is lost. It will be hard to debug why the code is not working even if b is declared in module1 and I have written the code expecting my code to use module1.bIf you have same variables in different modules, and you do not want to import entire module, you may even do:As a test, I created a module test.py with 2 functions A and B, which respectively print "A 1" and "B 1". After importing test.py with:. . . I can run the 2 functions as test.A() and test.B(), and "test" shows up as a module in the namespace, so if I edit test.py I can reload it with:But if I do the following:there is no reference to "test" in the namespace, so there is no way to reload it after an edit (as far as I can tell), which is a problem in an interactive session. Whereas either of the following:will add "test" or "tt" (respectively) as module names in the namespace, which will allow re-loading.If I do:the names "A" and "B" show up in the namespace as functions. If I edit test.py, and repeat the above command, the modified versions of the functions do not get reloaded.And the following command elicits an error message.If someone knows how to reload a module loaded with "from module import *", please post. Otherwise, this would be another reason to avoid the form:As suggested in the docs, you should never use import * in production code.While importing * from a module is bad, importing * from a package is even worse. Basically, from package import * imports whatever names are defined  by the package's __init__.py, but it also includes any submodules of the package that were loaded by previous import statements. Consider this example:The last statement will import the echo and surround modules into the current namespace (possibly overriding previous definitions) because they are defined in the sound.effects package when the import statement is executed.

Zip lists in Python

AJW

[Zip lists in Python](https://stackoverflow.com/questions/13704860/zip-lists-in-python)

I am trying to learn how to "zip" lists. To this end, I have a program, where at a particular point, I do the following:This gives me three lists, x1, x2, and x3, each of, say, size 20.Now, I do:However, when I do:I get 20, which is not what I expected. I expected three. I think I am doing something fundamentally wrong.

2012-12-04 14:13:00Z

I am trying to learn how to "zip" lists. To this end, I have a program, where at a particular point, I do the following:This gives me three lists, x1, x2, and x3, each of, say, size 20.Now, I do:However, when I do:I get 20, which is not what I expected. I expected three. I think I am doing something fundamentally wrong.When you zip() together three lists containing 20 elements each, the result has twenty elements. Each element is a three-tuple.See for yourself:To find out how many elements each tuple contains, you could examine the length of the first element:Of course, this won't work if the lists were empty to start with.zip takes a bunch of lists likesand "zips" them into one list whose entries are 3-tuples (ai, bi, ci). Imagine drawing a zipper horizontally from left to right.In Python 2.7 this might have worked fine:But in Python 3.4 it should be (otherwise, the result will be something like <zip object at 0x00000256124E7DC8>):zip creates a new list, filled with tuples containing elements from the iterable arguments:I expect what you try to so is create a tuple where each element is a list.Basically the zip function works on lists, tuples and dictionaries in Python.

If you are using IPython then just type zip? And check what zip() is about.If you are not using IPython then just install it: "pip install ipython"For listsThe output is [('a', 'p'), ('b', 'q'), ('c', 'r')For dictionary:The output is:Source: My Blog Post (better formatting)ExampleZero or more iterables [1] (ex. list, string, tuple, dictionary)1) Empty String: len(str)= 0 = no tuples2) Single String: len(str) == 2 tuples with len(args) == 1 element(s)1. Build a dictionary [2] out of two lists2. Print columns in a table"*" [3] is called "unpacking": f(*[arg1,arg2,arg3]) == f(arg1, arg2, arg3)zip(*args) is called「unzipping」because it has the inverse effect of zip* Code:* took numbers (1 arg) and「unpacked」its’ 2 elements into 2 argsIn Python 3 zip returns an iterator instead and needs to be passed to a list function to get the zipped tuples:Then to unzip them back just conjugate the zipped iterator:If the original form of list is needed instead of tuples:For the completeness's sake.When zipped lists' lengths are not equal.

The result list's length will become the shortest one without any error occurredI don't think zip returns a list. zip returns a generator. You have got to do list(zip(a, b)) to get a list of tuples.It's worth adding here as it is such a highly ranking question on zip. zip is great, idiomatic Python - but it doesn't scale very well at all for large lists.Instead of:Use izip. For modern processing, it stores it in L1 Cache memory and is far more performant for larger lists. Use it as simply as adding an i:

What's the difference between globals(), locals(), and vars()?

Ethan Furman

[What's the difference between globals(), locals(), and vars()?](https://stackoverflow.com/questions/7969949/whats-the-difference-between-globals-locals-and-vars)

What is the difference between globals(), locals(), and vars()?  What do they return?  Are updates to the results useful?

2011-11-01 16:56:22Z

What is the difference between globals(), locals(), and vars()?  What do they return?  Are updates to the results useful?Each of these return a dictionary:locals and vars could use some more explanation.  If locals() is called inside a function, it updates a dict with the values of the current local variable namespace (plus any closure variables) as of that moment and returns it. Multiple calls to locals() in the same stack frame return the same dict each time - it's attached to the stack frame object as its f_locals attribute. The dict's contents are updated on each locals() call and each f_locals attribute access, but only on such calls or attribute accesses. It does not automatically update when variables are assigned, and assigning entries in the dict will not assign the corresponding local variables:gives us:The first print(l) only shows an 'x' entry, because the assignment to l happens after the locals() call. The second print(l), after calling locals() again, shows an l entry, even though we didn't save the return value. The third and fourth prints show that assigning variables doesn't update l and vice versa, but after we access f_locals, local variables are copied into locals() again.Two notes:If locals() is called outside a function it returns the actual dictionary that is the current namespace.  Further changes to the namespace are reflected in the dictionary, and changes to the dictionary are reflected in the namespace:gives us:So far, everything I've said about locals() is also true for vars()... here's the difference:  vars() accepts a single object as its argument, and if you give it an object it returns the __dict__ of that object.  For a typical object, its __dict__ is where most of its attribute data is stored. This includes class variables and module globals:which gives us:Note that a function's __dict__ is its attribute namespace, not local variables. It wouldn't make sense for a function's __dict__ to store local variables, since recursion and multithreading mean there can be multiple calls to a function at the same time, each with their own locals:which gives us:Here, f calls itself recursively, so the inner and outer calls overlap. Each one sees its own local variables when it calls locals(), but both calls see the same f.__dict__, and f.__dict__ doesn't have any local variables in it.

Are list-comprehensions and functional functions faster than「for loops」?

Ericson Willians

[Are list-comprehensions and functional functions faster than「for loops」?](https://stackoverflow.com/questions/22108488/are-list-comprehensions-and-functional-functions-faster-than-for-loops)

In terms of performance in Python, is a list-comprehension, or functions like map(), filter() and reduce() faster than a for loop? Why, technically, they run in a C speed, while the for loop runs in the python virtual machine speed?.Suppose that in a game that I'm developing I need to draw complex and huge maps using for loops. This question would be definitely relevant, for if a list-comprehension, for example, is indeed faster, it would be a much better option in order to avoid lags (Despite the visual complexity of the code).

2014-03-01 00:38:59Z

In terms of performance in Python, is a list-comprehension, or functions like map(), filter() and reduce() faster than a for loop? Why, technically, they run in a C speed, while the for loop runs in the python virtual machine speed?.Suppose that in a game that I'm developing I need to draw complex and huge maps using for loops. This question would be definitely relevant, for if a list-comprehension, for example, is indeed faster, it would be a much better option in order to avoid lags (Despite the visual complexity of the code).The following are rough guidelines and educated guesses based on experience. You should timeit or profile your concrete use case to get hard numbers, and those numbers may occasionally disagree with the below.A list comprehension is usually a tiny bit faster than the precisely equivalent for loop (that actually builds a list), most likely because it doesn't have to look up the list and its append method on every iteration. However, a list comprehension still does a bytecode-level loop:Using a list comprehension in place of a loop that doesn't build a list, nonsensically accumulating a list of meaningless values and then throwing the list away, is often slower because of the overhead of creating and extending the list. List comprehensions aren't magic that is inherently faster than a good old loop.As for functional list processing functions: While these are written in C and probably outperform equivalent functions written in Python, they are not necessarily the fastest option. Some speed up is expected if the function is written in C too. But most cases using a lambda (or other Python function), the overhead of repeatedly setting up Python stack frames etc. eats up any savings. Simply doing the same work in-line, without function calls (e.g. a list comprehension instead of map or filter) is often slightly faster.Chances are, if code like this isn't already fast enough when written in good non-"optimized" Python, no amount of Python level micro optimization is going to make it fast enough and you should start thinking about dropping to C. While extensive micro optimizations can often speed up Python code considerably, there is a low (in absolute terms) limit to this. Moreover, even before you hit that ceiling, it becomes simply more cost efficient (15% speedup vs. 300% speed up with the same effort) to bite the bullet and write some C.If you check the info on python.org, you can see this summary:But you really should read the above article in details to understand the cause of the performance difference.I also strongly suggest you should time your code by using timeit.  At the end of the day, there can be a situation where, for example, you may need to break out of for loop when a condition is met. It could potentially be faster than finding out the result by calling map. You ask specifically about map(), filter() and reduce(), but I assume you want to know about functional programming in general. Having tested this myself on the problem of computing distances between all points within a set of points, functional programming (using the starmap function from the built-in itertools module) turned out to be slightly slower than for-loops (taking 1.25 times as long, in fact). Here is the sample code I used:Is the functional version faster than the procedural version?I wrote a simple script that test the speed and this is what I found out. Actually for loop was fastest in my case. That really suprised me, check out bellow (was calculating sum of squares).Adding a twist to Alphii answer, actually the for loop would be second best and about 6 times slower than mapMain changes have been to eliminate the slow sum calls, as well as the probably unnecessary int() in the last case. Putting the for loop and map in the same terms makes it quite fact, actually. Remember that lambdas are functional concepts and theoretically shouldn't have side effects, but, well, they can have side effects like adding to a.

Results in this case with Python 3.6.1, Ubuntu 14.04, Intel(R) Core(TM) i7-4770 CPU @ 3.40GHzI have managed to modify some of @alpiii's code and discovered that List comprehension is a little faster than for loop. It might be caused by int(), it is not fair between list comprehension and for loop.I modified @Alisa's code and used cProfile to show why list comprehension is faster:Here's the results:IMHO:

Calling class staticmethod within the class body?

martineau

[Calling class staticmethod within the class body?](https://stackoverflow.com/questions/12718187/calling-class-staticmethod-within-the-class-body)

When I attempt to use a static method from within the body of the class, and define the static method using the built-in staticmethod function as a decorator, like this:I get the following error:I understand why this is happening (descriptor binding), and can work around it by manually converting _stat_func() into a staticmethod after its last use, like so:So my question is:Are there better, as in cleaner or more "Pythonic", ways to accomplish this?

2012-10-03 23:12:08Z

When I attempt to use a static method from within the body of the class, and define the static method using the built-in staticmethod function as a decorator, like this:I get the following error:I understand why this is happening (descriptor binding), and can work around it by manually converting _stat_func() into a staticmethod after its last use, like so:So my question is:Are there better, as in cleaner or more "Pythonic", ways to accomplish this?staticmethod objects apparently have a __func__ attribute storing the original raw function (makes sense that they had to). So this will work:As an aside, though I suspected that a staticmethod object had some sort of attribute storing the original function, I had no idea of the specifics. In the spirit of teaching someone to fish rather than giving them a fish, this is what I did to investigate and find that out (a C&P from my Python session):Similar sorts of digging in an interactive session (dir is very helpful) can often solve these sorts of question very quickly.This is the way I prefer:I prefer this solution to Klass.stat_func, because of the DRY principle.

Reminds me of the reason why there is a new super() in Python 3 :)But I agree with the others, usually the best choice is to define a module level function.For instance with @staticmethod function, the recursion might not look very good (You would need to break DRY principle by calling Klass.stat_func inside Klass.stat_func). That's because you don't have reference to self inside static method.

With module level function, everything will look OK.This is due to staticmethod being a descriptor and requires a class-level attribute fetch to exercise the descriptor protocol and get the true callable.From the source code:But not directly from inside the class while it is being defined. But as one commenter mentioned, this is not really a "Pythonic" design at all. Just use a module level function instead. What about injecting the class attribute after the class definition?What about this solution? It does not rely on knowledge of @staticmethod decorator implementation. Inner class StaticMethod plays as a container of static initialization functions.

Exiting from python Command Line

Ank

[Exiting from python Command Line](https://stackoverflow.com/questions/9730409/exiting-from-python-command-line)

To exit from Python command line, I have to type exit(). If I type exit, it says Usually when you type exit, you would want to exit the program. Why does the interpreter give me the above error when it knows I am trying to exit the command line? Why doesn't it just exit? I know it doesn't matter and its a silly question but I am curious. 

2012-03-16 00:57:56Z

To exit from Python command line, I have to type exit(). If I type exit, it says Usually when you type exit, you would want to exit the program. Why does the interpreter give me the above error when it knows I am trying to exit the command line? Why doesn't it just exit? I know it doesn't matter and its a silly question but I am curious. In my python interpreter exit is actually a string and not a function -- 'Use Ctrl-D (i.e. EOF) to exit.'. You can check on your interpreter by entering type(exit)In active python what is happening is that exit is a function. If you do not call the function it will print out the string representation of the object. This is the default behaviour for any object returned. It's just that the designers thought people might try to type exit to exit the interpreter, so they made the string representation of the exit function a helpful message. You can check this behaviour by typing str(exit) or even print exit.This works for me, best way to come out of python prompt.When you type exit in the command line, it finds the variable with that name and calls __repr__ (or __str__) on it. Usually, you'd get a result like:But they decided to redefine that function for the exit object to display a helpful message instead. Whether or not that's a stupid behavior or not, is a subjective question, but one possible reason why it doesn't "just exit" is:Suppose you're looking at some code in a debugger, for instance, and one of the objects references the exit function. When the debugger tries to call __repr__ on that object to display that function to you, the program suddenly stops! That would be really unexpected, and the measures to counter that might complicate things further (for instance, even if you limit that behavior to the command line, what if you try to print some object that have exit as an attribute?)This message is the __str__ attribute of exitlook at these examples :123I recommend you exit the Python interpreter with Ctrl-D.  This is the old ASCII code for end-of-file or end-of-transmission.With Anaconda 4.5+ and Python 3.6+ on Windows useorIn some cases, you might have to useIf your computer doesn't have Break key then see here.Because the interpreter is not a shell where you provide commands, it's - well - an interpreter. The things that you give to it are Python code.The syntax of Python is such that exit, by itself, cannot possibly be anything other than a name for an object. Simply referring to an object can't actually do anything (except what the read-eval-print loop normally does; i.e. display a string representation of the object).You can fix that.Link PYTHONSTARTUP to a python file with the following How does this work?The python command line is a read-evaluate-print-loop, that is when you type text it will read that text, evaluate it, and eventually print the result. When you type exit() it evaluates to a callable object of type site.Quitter and calls its __call__ function which exits the system. When you type exit it evaluates to the same callable object, without calling it the object is printed which in turn calls __repr__ on the object.  We can take advantage of this by linking __repr__ to __call__ and thus get the expected behavior of exiting the system even when we type exit without parentheses.To exit from Python terminal, simply just do:Please pay attention it's a function which called as most user mix it with exit without calling, but new Pyhton terminal show a message...or as a shortcut, press:on your keyboard...If you stuck in python command line and none of above solutions worked for you,

try exit(2)"exit" is a valid variable name that can be used in your Python program. You wouldn't want to exit the interpreter when you're just trying to see the value of that variable.

Case insensitive 'in'

RadiantHex

[Case insensitive 'in'](https://stackoverflow.com/questions/3627784/case-insensitive-in)

I love using the expressionwhere USERNAMES is a list.Is there any way to match items with case insensitivity or do I need to use a custom method? Just wondering if there is a need to write extra code for this.

2010-09-02 13:56:30Z

I love using the expressionwhere USERNAMES is a list.Is there any way to match items with case insensitivity or do I need to use a custom method? Just wondering if there is a need to write extra code for this.Alternatively:Or, yes, you can make a custom method.I would make a wrapper so you can be non-invasive.  Minimally, for example...:Now, if CaseInsensitively('MICHAEL89') in whatever: should behave as required (whether the right-hand side is a list, dict, or set).  (It may require more effort to achieve similar results for string inclusion, avoid warnings in some cases involving unicode, etc).Usually (in oop at least) you shape your object to behave the way you want. name in USERNAMES is not case insensitive, so USERNAMES needs to change:The great thing about this is that it opens the path for many improvements, without having to change any code outside the class. For example, you could change the self.names to a set for faster lookups, or compute the (n.lower() for n in self.names) only once and store it on the class and so on ...str.casefold is recommended for case-insensitive string matching. @nmichaels's solution can trivially be adapted.Use either:Or:As per the docs:I think you have to write some extra code. For example:In this case we are forming a new list with all entries in USERNAMES converted to upper case and then comparing against this new list.Update As @viraptor says, it is even better to use a generator instead of map. See @Nathon's answer.Here's one way:For this to work, both string1 and string2 objects must be of type string.You could doUpdate:  played around a bit and am thinking you could get a better short-circuit type approach usingThe ifilter function is from itertools, one of my favorite modules within Python.  It's faster than a generator but only creates the next item of the list when called upon.My 5 (wrong) centsOuch, totally agree @jpp, I'll keep as an example of bad practice :( I needed this for a dictionary instead of list, Jochen solution was the most elegant for that case so I modded it a bit:now you can convert a dictionary like so USERNAMESDICT = CaseInsensitiveDict(USERNAMESDICT) and use if 'MICHAEL89' in USERNAMESDICT: 

Python: TypeError: cannot concatenate 'str' and 'int' objects [duplicate]

user1581649

[Python: TypeError: cannot concatenate 'str' and 'int' objects [duplicate]](https://stackoverflow.com/questions/11844072/python-typeerror-cannot-concatenate-str-and-int-objects)

I have this python program that adds strings to integers:I get this error:How can I add strings to integers?

2012-08-07 10:35:24Z

I have this python program that adds strings to integers:I get this error:How can I add strings to integers?There are two ways to fix the problem which is caused by the last print statement. You can assign the result of the str(c) call to c as correctly shown by @jamylak and then concatenate all of the strings, or you can replace the last print simply with this:in which case isn't necessary and can be deleted.Output of sample run:with:str(c) returns a new string representation of c, and does not mutate c itself.is probably what you are looking forIf you want to concatenate int or floats to a string you must use this:Actually, in this last line you are not changing the type of the variable c. If you doit should work.The easiest and least confusing solution:I found this on http://freecodeszone.blogspot.com/I also had the error message "TypeError: cannot concatenate 'str' and 'int' objects". It turns out that I only just forgot to add str() around a variable when printing it. Here is my code:I know, it was a stupid mistake but for beginners who are very new to python such as myself, it happens.You can convert int into str using string function:This is what i have done to get rid of this error separating variable with "," helped me.Here is the output(program exited with code: 0)Apart from other answers, one could also use format()print("a + b as integers: {}".format(c))For example -will result in output - Time elapsed - 13 hours and 32 minutesCheck out docs for more information.

Is there a decorator to simply cache function return values?

Tobias

[Is there a decorator to simply cache function return values?](https://stackoverflow.com/questions/815110/is-there-a-decorator-to-simply-cache-function-return-values)

Consider the following:I'm new, but I think the caching could be factored out into a decorator. Only I didn't find one like it ;)PS the real calculation doesn't depend on mutable values

2009-05-02 16:15:40Z

Consider the following:I'm new, but I think the caching could be factored out into a decorator. Only I didn't find one like it ;)PS the real calculation doesn't depend on mutable valuesStarting from Python 3.2 there is a built-in decorator:@functools.lru_cache(maxsize=100, typed=False)Example of an LRU cache for computing Fibonacci numbers:If you are stuck with Python 2.x, here's a list of other compatible memoization libraries:It sounds like you're not asking for a general-purpose memoization decorator (i.e., you're not interested in the general case where you want to cache return values for different argument values).  That is, you'd like to have this:while a general-purpose memoization decorator would give you this:I submit that the method-call syntax is better style, because it suggests the possibility of expensive computation while the property syntax suggests a quick lookup.[Update: The class-based memoization decorator I had linked to and quoted here previously doesn't work for methods.  I've replaced it with a decorator function.]  If you're willing to use a general-purpose memoization decorator, here's a simple one:Example usage:Another memoization decorator with a limit on the cache size can be found here.Sample uses:Python 3.8 cached_property decoratorhttps://docs.python.org/dev/library/functools.html#functools.cached_propertycached_property from Werkzeug was mentioned at: https://stackoverflow.com/a/5295190/895245 but a supposedly derived version will be merged into 3.8, which is awesome.This decorator can be seen as caching @property, or as a cleaner  @functools.lru_cache for when you don't have any arguments.The docs say:Werkzeug has a cached_property decorator (docs, source)I coded this simple decorator class to cache function responses. I find it VERY useful for my projects:The usage is straightforward:DISCLAIMER: I'm the author of kids.cache.You should check kids.cache, it provides a @cache decorator that works on python 2 and python 3. No dependencies, ~100 lines of code. It's very straightforward to use, for instance, with your code in mind, you could use it like this:ThenOr you could put the @cache decorator after the @property (same result).Using cache on a property is called lazy evaluation, kids.cache can do much more (it works on function with any arguments, properties, any type of methods, and even classes...). For advanced users, kids.cache supports cachetools which provides fancy cache stores to python 2 and python 3 (LRU, LFU, TTL, RR cache).IMPORTANT NOTE: the default cache store of kids.cache is a standard dict, which is not recommended for long running program with ever different queries as it would lead to an ever growing caching store. For this usage you can plugin other cache stores using for instance (@cache(use=cachetools.LRUCache(maxsize=2)) to decorate your function/property/class/method...)Ah, just needed to find the right name for this: "Lazy property evaluation".I do this a lot too; maybe I'll use that recipe in my code sometime.There is yet another example of a memoize decorator at Python Wiki:http://wiki.python.org/moin/PythonDecoratorLibrary#MemoizeThat example is a bit smart, because it won't cache the results if the parameters are mutable. (check that code, it's very simple and interesting!)If you are using Django Framework, it has such a property to cache a view or response of API's

using @cache_page(time) and there can be other options as well.Example:More details can be found here.Along with the Memoize Example I found the following python packages:There is fastcache, which is "C implementation of Python 3 functools.lru_cache. Provides speedup of 10-30x over standard library."Same as chosen answer, just different import:Also, it comes installed in Anaconda, unlike functools which needs to be installed.I implemented something like this, using pickle for persistance and using sha1 for short almost-certainly-unique IDs. Basically the cache hashed the code of the function and the hist of arguments to get a sha1 then looked for a file with that sha1 in the name. If it existed, it opened it and returned the result; if not, it calls the function and saves the result (optionally only saving if it took a certain amount of time to process).That said, I'd swear I found an existing module that did this and find myself here trying to find that module... The closest I can find is this, which looks about right: http://chase-seibert.github.io/blog/2011/11/23/pythondjango-disk-based-caching-decorator.htmlThe only problem I see with that is it wouldn't work well for large inputs since it hashes str(arg), which isn't unique for giant arrays.It would be nice if there were a unique_hash() protocol that had a class return a secure hash of its contents. I basically manually implemented that for the types I cared about.@lru_cache is not perfect with default function valuesmy mem decorator:and code for testing:result - only 3 times with sleep but with @lru_cache it will be 4 times, because this:will be calculated twice (bad working with defaults)Try joblib

http://pythonhosted.org/joblib/memory.htmlIf you are using Django and want to cache views, see Nikhil Kumar's answer. But if you want to cache ANY function results, you can use django-cache-utils.It reuses Django caches and provides easy to use cached decorator:

How to get the python.exe location programmatically? [duplicate]

Joan Venge

[How to get the python.exe location programmatically? [duplicate]](https://stackoverflow.com/questions/749711/how-to-get-the-python-exe-location-programmatically)

Basically I want to get a handle of the python interpreter so I can pass a script file to execute (from an external application).

2009-04-14 23:29:02Z

Basically I want to get a handle of the python interpreter so I can pass a script file to execute (from an external application).This works in Linux, perhaps in Windows too?Python 2.xPython 3.xsys.executable is not reliable if working in an embedded python environment. My suggestions is to deduce it from I think it depends on how you installed python. Note that you can have multiple installs of python, I do on my machine. However, if you install via an msi of a version of python 2.2 or above, I believe it creates a registry key like so:which gives this value on my machine:You just read the registry key to get the location.However, you can install python via an xcopy like model that you can have in an arbitrary place, and you just have to know where it is installed.

pandas groupby sort within groups

JoeDanger

[pandas groupby sort within groups](https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups)

I want to group my dataframe by two columns and then sort the aggregated results within the groups.I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:

2015-01-08 14:37:19Z

I want to group my dataframe by two columns and then sort the aggregated results within the groups.I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:What you want to do is actually again a groupby (on the result of the first groupby): sort and take the first three elements per group.Starting from the result of the first groupby:We group by the first level of the index:Then we want to sort ('order') each group and take the first three elements:However, for this, there is a shortcut function to do this, nlargest:You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group. Here's other example of taking top 3 on sorted order, and sorting within the groups:If you don't need to sum a column, then use @tvashtar's answer. If you do need to sum, then you can use @joris' answer or this one which is very similar to it.

What is Ruby equivalent of Python's `s=「hello, %s. Where is %s?」% (「John」,「Mary」)`

TIMEX

[What is Ruby equivalent of Python's `s=「hello, %s. Where is %s?」% (「John」,「Mary」)`](https://stackoverflow.com/questions/3554344/what-is-ruby-equivalent-of-pythons-s-hello-s-where-is-s-john-mar)

In Python, this idiom for string formatting is quite commonWhat is the equivalent in Ruby?

2010-08-24 07:34:52Z

In Python, this idiom for string formatting is quite commonWhat is the equivalent in Ruby?The easiest way is string interpolation.  You can inject little pieces of Ruby code directly into your strings.You can also do format strings in Ruby.Remember to use square brackets there.  Ruby doesn't have tuples, just arrays, and those use square brackets.In Ruby > 1.9 you can do this:See the docsAlmost the same way:Actually almost the same

Pandas DataFrame to List of Dictionaries

Mohamad Ibrahim

[Pandas DataFrame to List of Dictionaries](https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries)

I have the following DataFrame:which I want to translate it to list of dictionaries per row

2015-04-23 06:12:18Z

I have the following DataFrame:which I want to translate it to list of dictionaries per rowAs John Galt mentions in his answer , you should probably instead use df.to_dict('records'). It's faster than transposing manually.Use df.T.to_dict().values(), like below:Use df.to_dict('records') -- gives the output without having to transpose externally.As an extension to John Galt's answer -For the following DataFrame,If you want to get a list of dictionaries including the index values, you can do something like, Which outputs a dictionary of dictionaries where keys of the parent dictionary are index values. In this particular case, 

Python - List of unique dictionaries

Limaaf

[Python - List of unique dictionaries](https://stackoverflow.com/questions/11092511/python-list-of-unique-dictionaries)

Let's say I got a list of dictionaries:and I need to obtain a list of unique dictionaries (removing the duplicates):Can anyone help me with the most efficient way to achieve this in Python?

2012-06-18 23:30:38Z

Let's say I got a list of dictionaries:and I need to obtain a list of unique dictionaries (removing the duplicates):Can anyone help me with the most efficient way to achieve this in Python?So make a temporary dict with the key being the id. This filters out the duplicates.

The values() of the dict will be the listIn Python2.7In Python3In Python2.5/2.6The usual way to find just the common elements in a set is to use Python's set class.  Just add all the elements to the set, then convert the set to a list, and bam the duplicates are gone.The problem, of course, is that a set() can only contain hashable entries, and a dict is not hashable.If I had this problem, my solution would be to convert each dict into a string that represents the dict, then add all the strings to a set() then read out the string values as a list() and convert back to dict.A good representation of a dict in string form is JSON format.  And Python has a built-in module for JSON (called json of course).The remaining problem is that the elements in a dict are not ordered, and when Python converts the dict to a JSON string, you might get two JSON strings that represent equivalent dictionaries but are not identical strings.  The easy solution is to pass the argument sort_keys=True when you call json.dumps(). EDIT: This solution was assuming that a given dict could have any part different.  If we can assume that every dict with the same "id" value will match every other dict with the same "id" value, then this is overkill; @gnibbler's solution would be faster and easier.EDIT: Now there is a comment from André Lima explicitly saying that if the ID is a duplicate, it's safe to assume that the whole dict is a duplicate.  So this answer is overkill and I recommend @gnibbler's answer.In case the dictionaries are only uniquely identified by all items (ID is not available) you can use the answer using JSON. The following is an alternative that does not use JSON, and will work as long as all dictionary values are immutableYou can use numpy library (works for Python2.x only):To get it worked with Python 3.x (and recent versions of numpy), you need   to convert array of dicts to numpy array of strings, e.g. Here's a reasonably compact solution, though I suspect not particularly efficient (to put it mildly):Since the id is sufficient for detecting duplicates, and the id is hashable: run 'em through a dictionary that has the id as the key. The value for each key is the original dictionary.In Python 3, values() doesn't return a list; you'll need to wrap the whole right-hand-side of that expression in list(), and you can write the meat of the expression more economically as a dict comprehension:Note that the result likely will not be in the same order as the original. If that's a requirement, you could use a Collections.OrderedDict instead of a dict.As an aside, it may make a good deal of sense to just keep the data in a dictionary that uses the id as key to begin with.outputs:Expanding on John La Rooy (Python - List of unique dictionaries) answer, making it a bit more flexible:Calling Function:We can do with pandas Notice slightly different from the accept answer. drop_duplicates will check all column in pandas , if all same then the row will be dropped . For example : If we change the 2nd dict name from john to peterIn python 3.6+ (what I've tested), just use:Explanation: we're mapping the json.dumps to encode the dictionaries as json objects, which are immutable. set can then be used to produce an iterable of unique immutables. Finally, we convert back to our dictionary representation using json.loads. Note that initially, one must sort by keys to arrange the dictionaries in a unique form. This is valid for Python 3.6+ since dictionaries are ordered by default.I have summarized my favorites to try out:https://repl.it/@SmaMa/Python-List-of-unique-dictionariesA quick-and-dirty solution is just by generating a new list.I don't know if you only want the id of your dicts in the list to be unique, but if the goal is to have a set of dict where the unicity is on all keys' values.. you should use tuples key like this in your comprehension :Hope it helps you or another person having the concern....Pretty straightforward option:There are a lot of answers here, so let me add another:Heres an implementation with little memory overhead at the cost of not being as compact as the rest.output:This is the solution I found:Basically you check if the ID is present in the list, if it is, delete the dictionary, if not, append the ID to the list

Timeout function if it takes too long to finish [duplicate]

Christoffer

[Timeout function if it takes too long to finish [duplicate]](https://stackoverflow.com/questions/2281850/timeout-function-if-it-takes-too-long-to-finish)

I have a shell script that loops through a text file containing URL:s that I want to visit and take screenshots of.All this is done and simple. The script initializes a class that when run creates a screenshot of each site in the list. Some sites take a very, very long time to load, and some might not be loaded at all. So I want to wrap the screengrabber-function in a timeout script, making the function return False if it couldn't finish within 10 seconds.I'm content with the simplest solution possible, maybe setting a asynchronous timer that will return False after 10 seconds no matter what actually happens inside the function?

2010-02-17 15:21:48Z

I have a shell script that loops through a text file containing URL:s that I want to visit and take screenshots of.All this is done and simple. The script initializes a class that when run creates a screenshot of each site in the list. Some sites take a very, very long time to load, and some might not be loaded at all. So I want to wrap the screengrabber-function in a timeout script, making the function return False if it couldn't finish within 10 seconds.I'm content with the simplest solution possible, maybe setting a asynchronous timer that will return False after 10 seconds no matter what actually happens inside the function?The process for timing out an operations is described in the documentation for signal.The basic idea is to use signal handlers to set an alarm for some time interval and raise an exception once that timer expires.Note that this will only work on UNIX.Here's an implementation that creates a decorator (save the following code as timeout.py).This creates a decorator called @timeout that can be applied to any long running functions.So, in your application code, you can use the decorator like so:I rewrote David's answer using the with statement, it allows you do do this:Which will raise a TimeoutError.The code is still using signal and thus UNIX only:

How can I use Homebrew to install both Python 2 and 3 on Mac?

MostafaMV

[How can I use Homebrew to install both Python 2 and 3 on Mac?](https://stackoverflow.com/questions/18671253/how-can-i-use-homebrew-to-install-both-python-2-and-3-on-mac)

I need to be able to switch back and forth between Python 2 and 3. How do I do that using Homebrew as I don't want to mess with path and get into trouble.

Right now I have 2.7 installed through Homebrew.

2013-09-07 08:11:22Z

I need to be able to switch back and forth between Python 2 and 3. How do I do that using Homebrew as I don't want to mess with path and get into trouble.

Right now I have 2.7 installed through Homebrew.I would use pyenv You can install it:To enable pyenv in your Bash shell, you need to run:To do this automatically for Bash upon startup, add that line to your ~/.bash_profile. 1Once you have installed pyenv and activated it, you can install different versions of python and choose which one you can use. Example:You can check the versions you have installed with:And you can switch between python versions with the command:Also you can set a python version for the current directory with:You can check by running python --version:1 Homebrew used to instruct you to do this upon installation of pyenv, but the message was removed. For Zsh and other shells, the precise steps may be different.You can have both versions installed at the same time.Since 1st March 2018 the python formula will be upgraded to Python 3.x, while a new python@2 formula will be added for Python 2.7, specifically.See changes announcement here or the final doc about using Homebrew for Python here.For Python 2.x:For Python 3.x:Now, you will have both the versions installed in your machine. When you want to use version 2, use the python executable. When you want to use version 3, use the python3 executable.Currently Homebrew provides two different formulas for Python 2 and 3. brew install python installs python3, and brew install python@2 installs python2. More details in Homebrew docs:https://docs.brew.sh/Homebrew-and-PythonIf you currently have 2.x installed via Homebrew, Homebrew will give you a message such as:If you run:you should be able to do:andTo see what versions of Python 2.x and 3.x installed.Alternatively, you probably can just enter "python3" to run your most current version of python3.x and "python" or "python2" to run the latest installed 2.x version.There are ways to use both , but the simplest solution today is to use pyenv. pyenv allows easy switching between versions.

Here is what I did to set up:STEP1:Remove all pythons from your macRemove the following from ~/.bash_profileand also the following from ~/.bashrcSTEP2:Install pyenv and the python versions you needSTEP3:add pyenv init to bash_profile or bashrcSTEP4:Check what got installedSTEP5:Choose a defaultWhen a project needs older version, just go its root folder and runhttps://github.com/asdf-vm/asdf

https://github.com/tuvistavie/asdf-python

https://github.com/asdf-vm/asdf-pluginsI thought I had the same requirement - to move between Python versions - but I achieved all I needed with only Python3.6 by building from source instead of using homebrew.  git clone https://git.<theThingYouWantToInstall>Depending on the repo, check if there is MAKE file already setup for this option. I was able to just go to https://www.python.org/downloads/mac-osx/ and download the latest python. It installed along side current python in my system.Okay, I was struggling with my brew installation of Python3, because I didn't have pip3and so I did and installed the regular Python 3.6.2 from official distribution and then I had pip3 and all components were ok.

How do I create a dictionary with keys from a list and values defaulting to (say) zero? [duplicate]

blahster

[How do I create a dictionary with keys from a list and values defaulting to (say) zero? [duplicate]](https://stackoverflow.com/questions/3869487/how-do-i-create-a-dictionary-with-keys-from-a-list-and-values-defaulting-to-say)

I have a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}works but is ugly. What's a cleaner way?

2010-10-06 04:24:56Z

I have a = [1,2,3,4] and I want d = {1:0, 2:0, 3:0, 4:0}works but is ugly. What's a cleaner way?dict((el,0) for el in a) will work well. Python 2.7 and above also support dict comprehensions. That syntax is {el:0 for el in a}. a is the list, 0 is the default value. Pay attention not to set the default value to some mutable object (i.e. list or dict), because it will be one object used as value for every key in the dictionary (check here for a solution for this case). Numbers/strings are safe.In python version >= 2.7 and in python 3:In addition to Tim's answer, which is very appropriate to your specific example, it's worth mentioning collections.defaultdict, which lets you do stuff like this:For mapping [1, 2, 3, 4] as in your example, it's a fish out of water.  But depending on the reason you asked the question, this may end up being a more appropriate technique.**edit  Tim's solution is better because it uses generators see the comment to his answer.

Find and replace string values in list

Eric Herlitz

[Find and replace string values in list](https://stackoverflow.com/questions/3136689/find-and-replace-string-values-in-list)

I got this list:What I would like is to replace [br] with some fantastic value similar to &lt;br /&gt; and thus getting a new list:

2010-06-28 22:45:12Z

I got this list:What I would like is to replace [br] with some fantastic value similar to &lt;br /&gt; and thus getting a new list:These are called List Comprehensions.You can use, for example:Beside list comprehension, you can try mapIn case you're wondering about the performance of the different approaches, here are some timings:as you can see for such simple patterns the accepted list comprehension is the fastest, but look at the following:This shows that for more complicated substitutions a pre-compiled reg-exp (as in 9-10) can be (much) faster. It really depends on your problem and the shortest part of the reg-exp.An example with for loop (I prefer List Comprehensions).

Convert a Pandas DataFrame to a dictionary

Prince Bhatti

[Convert a Pandas DataFrame to a dictionary](https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary)

I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in same row be values. DataFrame:   Output should be like this:Dictionary:

2014-11-03 14:47:53Z

I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in same row be values. DataFrame:   Output should be like this:Dictionary:The to_dict() method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this.to_dict() also accepts an 'orient' argument which you'll need in order to output a list of values for each column. Otherwise, a dictionary of the form {index: value} will be returned for each column.These steps can be done with the following line:In case a different dictionary format is needed, here are examples of the possible orient arguments. Consider the following simple DataFrame:Then the options are as follows.dict - the default: column names are keys, values are dictionaries of index:data pairslist - keys are column names, values are lists of column dataseries - like 'list', but values are Seriessplit - splits columns/data/index as keys with values being column names, data values by row and index labels respectivelyrecords - each row becomes a dictionary where key is column name and value is the data in the cellindex - like 'records', but a dictionary of dictionaries with keys as index labels (rather than a list)Try to use ZipOutput: Suppose your dataframe is as follows:The results will be as follows:If you don't mind the dictionary values being tuples, you can use itertuples:For my use (node names with xy positions) I found @user4179775's answer to the most helpful / intuitive:AddendumI later returned to this issue, for other, but related, work. Here is an approach that more closely mirrors the [excellent] accepted answer.Convert Pandas dataframe to a [list], {dict}, {dict of {dict}}, ...Per accepted answer:In my case, I wanted to do the same thing but with selected columns from the Pandas dataframe, so I needed to slice the columns.  There are two approaches.(see: Convert pandas to dictionary defining the columns used fo the key values)orthat can then can be used to create a dictionary of dictionariesDataFrame.to_dict() converts DataFrame to dictionary.ExampleSee this Documentation for details

How do you create nested dict in Python?

atams

[How do you create nested dict in Python?](https://stackoverflow.com/questions/16333296/how-do-you-create-nested-dict-in-python)

I have 2 CSV files: 'Data' and 'Mapping':I know how to use dict when only 2 columns are present (1 is needed to be mapped) but I don't know how to accomplish this when 3 columns need to be mapped.Following is the code using which I tried to accomplish mapping of Device_Type:It returns Attribute Error.After some researching, I think I need to create a nested dict, but I don't have any idea how to do this.  

2013-05-02 08:14:40Z

I have 2 CSV files: 'Data' and 'Mapping':I know how to use dict when only 2 columns are present (1 is needed to be mapped) but I don't know how to accomplish this when 3 columns need to be mapped.Following is the code using which I tried to accomplish mapping of Device_Type:It returns Attribute Error.After some researching, I think I need to create a nested dict, but I don't have any idea how to do this.  A nested dict is a dictionary within a dictionary. A very simple thing.You can also use a defaultdict from the collections package to facilitate creating nested dictionaries.You can populate that however you want.I would recommend in your code something like the following:According to your comment:My suggestion would be something like this (without using defaultdict):Please note though, that for parsing csv files there is a csv module.UPDATE: For an arbitrary length of a nested dictionary, go to this answer.Use the defaultdict function from the collections. High performance: "if key not in dict" is very expensive when the data set is large.Low maintenance: make the code more readable and can be easily extended.For arbitrary levels of nestedness:It is important to remember when using defaultdict and similar nested dict modules such as nested_dict, that looking up a nonexistent key may inadvertently create a new key entry in the dict and cause a lot of havoc. Here is a Python3 example with nested_dict module:Output is:

Counting the number of True Booleans in a Python List

acs

[Counting the number of True Booleans in a Python List](https://stackoverflow.com/questions/12765833/counting-the-number-of-true-booleans-in-a-python-list)

I have a list of Booleans:and I am looking for a way to count the number of True in the list (so in the example above, I want the return to be 3.)  I have found examples of looking for the number of occurrences of specific elements, but is there a more efficient way to do it since I'm working with Booleans? I'm thinking of something analogous to all or any.

2012-10-07 03:12:41Z

I have a list of Booleans:and I am looking for a way to count the number of True in the list (so in the example above, I want the return to be 3.)  I have found examples of looking for the number of occurrences of specific elements, but is there a more efficient way to do it since I'm working with Booleans? I'm thinking of something analogous to all or any.True is equal to 1.list has a count method:This is actually more efficient than sum, as well as being more explicit about the intent, so there's no reason to use sum:If you are only concerned with the constant True, a simple sum is fine.  However, keep in mind that in Python other values evaluate as True as well.  A more robust solution would be to use the bool builtin:UPDATE: Here's another similarly robust solution that has the advantage of being more transparent:P.S. Python trivia: True could be true without being 1.  Warning: do not try this at work!Much more evil:You can use sum():Just for completeness' sake (sum is usually preferable), I wanted to mention that we can also use filter to get the truthy values. In the usual case, filter accepts a function as the first argument, but if you pass it None, it will filter for all "truthy" values. This feature is somewhat surprising, but is well documented and works in both Python 2 and 3.The difference between the versions, is that in Python 2 filter returns a list, so we can use len:But in Python 3, filter returns an iterator, so we can't use len, and if we want to avoid using sum (for any reason) we need to resort to converting the iterator to a list (which makes this much less pretty):After reading all the answers and comments on this question, I thought to do a small experiment.I generated 50,000 random booleans and called sum and count on them.Here are my results:Just to be sure, I repeated it several more times:And as you can see, count is 3 times faster than sum. So I would suggest to use count as I did in count_it.Python version: 3.6.7

CPU cores: 4

RAM size: 16 GB

OS: Ubuntu 18.04.1 LTSIt is safer to run through bool first. This is easily done:Then you will catch everything that Python considers True or False into the appropriate bucket:If you prefer, you can use a comprehension:I prefer len([b for b in boollist if b is True]) (or the generator-expression equivalent), as it's quite self-explanatory. Less 'magical' than the answer proposed by Ignacio Vazquez-Abrams.Alternatively, you can do this, which still assumes that bool is convertable to int, but makes no assumptions about the value of True:

ntrue = sum(boollist) / int(True)

Writing to an Excel spreadsheet

Jey

[Writing to an Excel spreadsheet](https://stackoverflow.com/questions/13437727/writing-to-an-excel-spreadsheet)

I am new to Python. I need to write some data from my program to a spreadsheet. I've searched online and there seem to be many packages available (xlwt, XlsXcessive, openpyxl). Others suggest to write to a .csv file (never used CSV and don't really understand what it is).The program is very simple. I have two lists (float) and three variables (strings). I don't know the lengths of the two lists and they probably won't be the same length.I want the layout to be as in the picture below:The pink column will have the values of the first list and the green column will have the values of the second list.So what's the best way to do this?P.S. I am running Windows 7 but I won't necessarily have Office installed on the computers running this program.I wrote this using all your suggestions. It gets the job done but it can be slightly improved.How do I format the cells created in the for loop (list1 values) as scientific or number?I do not want to truncate the values. The actual values used in the program would have around 10 digits after the decimal.

2012-11-18 05:20:53Z

I am new to Python. I need to write some data from my program to a spreadsheet. I've searched online and there seem to be many packages available (xlwt, XlsXcessive, openpyxl). Others suggest to write to a .csv file (never used CSV and don't really understand what it is).The program is very simple. I have two lists (float) and three variables (strings). I don't know the lengths of the two lists and they probably won't be the same length.I want the layout to be as in the picture below:The pink column will have the values of the first list and the green column will have the values of the second list.So what's the best way to do this?P.S. I am running Windows 7 but I won't necessarily have Office installed on the computers running this program.I wrote this using all your suggestions. It gets the job done but it can be slightly improved.How do I format the cells created in the for loop (list1 values) as scientific or number?I do not want to truncate the values. The actual values used in the program would have around 10 digits after the decimal.for more explanation:

    https://github.com/python-excelUse DataFrame.to_excel from pandas. Pandas allows you to represent your data in functionally rich datastructures and will let you read in excel files as well.You will first have to convert your data into a DataFrame and then save it into an excel file like so:and the excel file that comes out looks like this:Note that both lists need to be of equal length else pandas will complain. To solve this, replace all missing values with None.CSV stands for comma separated values. CSV is like a text file and can be created simply by adding the .CSV extensionfor example write this code:you can open this file with excel.I surveyed a few Excel modules for Python, and found openpyxl to be the best.The free book Automate the Boring Stuff with Python has a chapter on openpyxl with more details or you can check the Read the Docs site. You won't need Office or Excel installed in order to use openpyxl.Your program would look something like this:Try taking a look at the following libraries too:xlwings - for getting data into and out of a spreadsheet from Python, as well as manipulating workbooks and chartsExcelPython - an Excel add-in for writing user-defined functions (UDFs) and macros in Python instead of VBAOpenPyxl is quite a nice library, built to read/write Excel 2010 xlsx/xlsm files:https://openpyxl.readthedocs.io/en/stableThe other answer, referring to it is using the deperciated function (get_sheet_by_name). This is how to do it without it:The easiest way to import the exact numbers is to add a decimal after the numbers in your l1 and l2. Python interprets this decimal point as instructions from you to include the exact number. If you need to restrict it to some decimal place, you should be able to create a print command that limits the output, something simple like:Would restrict it to the tenth decimal place, assuming your data has two integers left of the decimal. You can try hfexcel Human Friendly object-oriented python library based on XlsxWriter:If your need is to modify an existing workbook, the safest way would be to use pyoo. You need to have some libraries installed and it takes a few hoops to jump through but once its set up, this would be bulletproof as you are leveraging the wide and solid API's of LibreOffice / OpenOffice.Please see my Gist on how to set up a linux system and do some basic coding using pyoo.Here is an example of the code:

Python list iterator behavior and next(iterator)

lvc

[Python list iterator behavior and next(iterator)](https://stackoverflow.com/questions/16814984/python-list-iterator-behavior-and-nextiterator)

Consider:So, advancing the iterator is, as expected, handled by mutating that same object. This being the case, I would expect:to skip every second element: the call to next should advance the iterator once, then the implicit call made by the loop should advance it a second time - and the result of this second call would be assigned to i. It doesn't. The loop prints all of the items in the list, without skipping any. My first thought was that this might happen because the loop calls iter on what it is passed, and this might give an independent iterator - this isn't the case, as we have iter(a) is a. So, why does next not appear to advance the iterator in this case?

2013-05-29 13:17:46Z

Consider:So, advancing the iterator is, as expected, handled by mutating that same object. This being the case, I would expect:to skip every second element: the call to next should advance the iterator once, then the implicit call made by the loop should advance it a second time - and the result of this second call would be assigned to i. It doesn't. The loop prints all of the items in the list, without skipping any. My first thought was that this might happen because the loop calls iter on what it is passed, and this might give an independent iterator - this isn't the case, as we have iter(a) is a. So, why does next not appear to advance the iterator in this case?What you see is the interpreter echoing back the return value of next() in addition to i being printed each iteration:So 0 is the output of print(i), 1 the return value from next(), echoed by the interactive interpreter, etc. There are just 5 iterations, each iteration resulting in 2 lines being written to the terminal.If you assign the output of next() things work as expected:or print extra information to differentiate the print() output from the interactive interpreter echo:In other words, next() is working as expected, but because it returns the next value from the iterator, echoed by the interactive interpreter, you are led to believe that the loop has its own iterator copy somehow.What is happening is that next(a) returns the next value of a, which is printed to the console because it is not affected.What you can do is affect a variable with this value:I find the existing answers a little confusing, because they only indirectly indicate the essential mystifying thing in the code example: both* the "print i" and the "next(a)" are causing their results to be printed.Since they're printing alternating elements of the original sequence, and it's unexpected that the "next(a)" statement is printing, it appears as if the "print i" statement is printing all the values.In that light, it becomes more clear that assigning the result of "next(a)" to a variable inhibits the printing of its' result, so that just the alternate values that the "i" loop variable are printed.  Similarly, making the "print" statement emit something more distinctive disambiguates it, as well.(One of the existing answers refutes the others because that answer is having the example code evaluated as a block, so that the interpreter is not reporting the intermediate values for "next(a)".)The beguiling thing in answering questions, in general, is being explicit about what is obvious once you know the answer.  It can be elusive.  Likewise critiquing answers once you understand them.  It's interesting...Something is wrong with your Python/Computer.Works like expected. Tested in Python 2.7 and in Python 3+ . Works properly in bothFor those who still do not understand.As others have already said, next increases the iterator by 1 as expected. Assigning its returned value to a variable doesn't magically changes its behaviour.It behaves the way you want if called as a function:

How to activate an Anaconda environment

pandita

[How to activate an Anaconda environment](https://stackoverflow.com/questions/20081338/how-to-activate-an-anaconda-environment)

I'm on Windows 8, using Anaconda 1.7.5 64bit.I created a new Anaconda environment withconda create -p ./test python=2.7 pipfrom C:\Pr\TEMP\venv\.This worked well (there is a folder with a new python distribution). conda tells me to type activate C:\PR\TEMP\venv\test to activate the environment, however this returns:No environment named "C:\PR\temp\venv\test" exists in C:\PR\Anaconda\envsHow can I activate the environment? What am I doing wrong?

2013-11-19 20:25:47Z

I'm on Windows 8, using Anaconda 1.7.5 64bit.I created a new Anaconda environment withconda create -p ./test python=2.7 pipfrom C:\Pr\TEMP\venv\.This worked well (there is a folder with a new python distribution). conda tells me to type activate C:\PR\TEMP\venv\test to activate the environment, however this returns:No environment named "C:\PR\temp\venv\test" exists in C:\PR\Anaconda\envsHow can I activate the environment? What am I doing wrong?If this happens you would need to set the PATH for your environment (so that it gets the right Python from the environment and Scripts\ on Windows).Imagine you have created an environment called py33 by using:Here the folders are created by default in Anaconda\envs, so you need to set the PATH as:Now it should work in the command window:The line above is the Windows equivalent to the code that normally appears in the tutorials for Mac and Linux:More info:

https://groups.google.com/a/continuum.io/forum/#!topic/anaconda/8T8i11gO39UDoes `anaconda` create a separate PYTHONPATH variable for each new environment?Use cmd instead of Powershell!

I spent 2 hours before I switched to cmd and then it worked!create Environment:see list of conda environments:activate your environment:That's all folksNote that the command for activating an environment has changed in Conda version 4.4. The recommended way of activating an environment is now conda activate myenv instead of source activate myenv. To enable the new syntax, you should modify your  .bashrc file. The line that currently reads something likeShould be changed toThis only adds the conda command to the path, but does not yet activate the base environment (which was previously called root). To do also that, add another lineafter the first command. See all the details in Anaconda's blog post from December 2017. (I think that this page is currently missing a newline between the two lines, it says .../conda.shconda activate base).(This answer is valid for Linux, but it might be relevant for Windows and Mac as well)As you can see from the error message the paths, that you specified, are wrong. Try it like this:However, when I needed to install Anaconda, I downloaded it from here and installed it to the default paths (C:\Anaconda), than I put this path to the environment variables, so now Anacondas interpreter is used as default. If you are using PyCharm, for example, you can specify the interpreter there directly.Below is how it worked for meShows new environment pathClones default root environmentDeactivating environment "d:\YourDefaultAnaconda3"...

Activating environment "d:\your\location\YourNewEnvironment"...conda environments:

#YourNewEnvironment

*  d:\your\location\YourNewEnvironmentroot                     d:\YourDefaultAnaconda3All the former answers seem to be outdated.conda activate was introduced in conda 4.4 and 4.6.ExamplesThese new sub-commands are available in "Aanconda Prompt" and "Anaconda Powershell Prompt" automatically. To use conda activate in every shell (normal cmd.exe and powershell), check expose conda command in every shell on Windows.ReferencesI've tried to activate env from Jenkins job (in bash) with

conda activate base  and it failed, so after many tries this one worked for me:  let's assume your environment name is 'demo' and you are using anaconda and want to create a virtual environment:(if you want python3)(if you want python2) After running above command you have to activate the environment by bellow command:I was having the same, a fix seems to have been made in the source.For me, using Anaconda Prompt instead of cmd or PowerShell is the key.In Anaconda Prompt, all I need to do is activate XXX

Getting one value from a tuple

BCS

[Getting one value from a tuple](https://stackoverflow.com/questions/3136059/getting-one-value-from-a-tuple)

Is there a way to get one value from a tuple in Python using expressions?I know I can do this:But that would add a few dozen lines to my function, doubling its length.

2010-06-28 20:55:35Z

Is there a way to get one value from a tuple in Python using expressions?I know I can do this:But that would add a few dozen lines to my function, doubling its length.You can writeTuples can be indexed just like lists.The main difference between tuples and lists is that tuples are immutable - you can't set the elements of a tuple to different values, or add or remove elements like you can from a list. But other than that, in most situations, they work pretty much the same.For anyone in the future looking for an answer, I would like to give a much clearer answer to the question.Hope this clears things up further for those that need it.

Why do we need tuples in Python (or any immutable data type)?

pyNewGuy

[Why do we need tuples in Python (or any immutable data type)?](https://stackoverflow.com/questions/2174124/why-do-we-need-tuples-in-python-or-any-immutable-data-type)

I've read several python tutorials (Dive Into Python, for one), and the language reference on Python.org - I don't see why the language needs tuples.Tuples have no methods compared to a list or set, and if I must convert a tuple to a set or list to be able to sort them, what's the point of using a tuple in the first place?Immutability?Why does anyone care if a variable lives at a different place in memory than when it was originally allocated? This whole business of immutability in Python seems to be over emphasized.In C/C++ if I allocate a pointer and point to some valid memory, I don't care where the address is located as long as it's not null before I use it.Whenever I reference that variable, I don't need to know if the pointer is still pointing to the original address or not. I just check for null and use it (or not).In Python, when I allocate a string (or tuple) assign it to x, then modify the string, why do I care if it's the original object? As long as the variable points to my data, that's all that matters.x still references the data I want, why does anyone need to care if its id is the same or different?

2010-02-01 01:08:15Z

I've read several python tutorials (Dive Into Python, for one), and the language reference on Python.org - I don't see why the language needs tuples.Tuples have no methods compared to a list or set, and if I must convert a tuple to a set or list to be able to sort them, what's the point of using a tuple in the first place?Immutability?Why does anyone care if a variable lives at a different place in memory than when it was originally allocated? This whole business of immutability in Python seems to be over emphasized.In C/C++ if I allocate a pointer and point to some valid memory, I don't care where the address is located as long as it's not null before I use it.Whenever I reference that variable, I don't need to know if the pointer is still pointing to the original address or not. I just check for null and use it (or not).In Python, when I allocate a string (or tuple) assign it to x, then modify the string, why do I care if it's the original object? As long as the variable points to my data, that's all that matters.x still references the data I want, why does anyone need to care if its id is the same or different?Example of optimization issue:None of the answers above point out the real issue of tuples vs lists, which many new to Python seem to not fully understand.Tuples and lists serve different purposes. Lists store homogenous data. You can and should have a list like this:The reason that is a correct use of lists is because those are all homogenous types of data, specifically, people's names. But take a list like this:That list is one person's full name, and their age. That isn't one type of data. The correct way to store that information is either in a tuple, or in an object. Lets say we have a few :The immutability and mutability of Tuples and Lists is not the main difference. A list is a list of the same kind of items: files, names, objects. Tuples are a grouping of different types of objects. They have different uses, and many Python coders abuse lists for what tuples are meant for.Please don't.Edit:I think this blog post explains why I think this better than I did: http://news.e-scribe.com/397In this particular case, there probably isn't a point. This is a non-issue, because this isn't one of the cases where you'd consider using a tuple.As you point out, tuples are immutable. The reasons for having immutable types apply to tuples:Note that a particular Python implementation may not make use of all of the above features.Dictionary keys must be immutable, otherwise changing the properties of a key-object can invalidate invariants of the underlying data structure. Tuples can thus potentially be used as keys. This is a consequence of const correctness.See also "Introducing tuples", from Dive Into Python.Sometimes we like to use objects as dictionary keysFor what it's worth, tuples recently (2.6+) grew index() and count() methodsI've always found having two completely separate types for the same basic data structure (arrays) to be an awkward design, but not a real problem in practice.  (Every language has its warts, Python included, but this isn't an important one.)These are different things.  Mutability isn't related to the place it's stored in memory; it means the stuff it points to can't change.Python objects can't change location after they're created, mutable or not.  (More accurately, the value of id() can't change--same thing, in practice.)  The internal storage of mutable objects can change, but that's a hidden implementation detail.This isn't modifying ("mutating") the variable; it's creating a new variable with the same name, and discarding the old one.  Compare to a mutating operation:As others have pointed out, this allows using arrays as keys to dictionaries, and other data structures that need immutability.Note that keys for dictionaries do not have to be completely immutable.  Only the part of it used as a key needs to be immutable; for some uses, this is an important distinction.  For example, you could have a class representing a user, which compares equality and a hash by the unique username.  You could then hang other mutable data on the class--"user is logged in", etc.  Since this doesn't affect equality or the hash, it's possible and perfectly valid to use this as a key in a dictionary.  This isn't too commonly needed in Python; I just point it out since several people have claimed that keys need to be "immutable", which is only partially correct.  I've used this many times with C++ maps and sets, though.As gnibbler offered in a comment, Guido had an opinion that is not fully accepted/appreciated:「lists are for homogeneous data, tuples are for heterogeneous data」. Of course, many of the opposers interpreted this as meaning that all elements of a list should be of the same type.I like to see it differently, not unlike others also have in the past:Note that I consider alist to be homogeneous, even if type(alist[1]) != type(alist[2]).If I can change the order of the elements and I won't have issues in my code (apart from assumptions, e.g.「it should be sorted」), then a list should be used. If not (like in the tuple blue above), then I should use a tuple.They are important since they guarantee the caller that the object they pass won't be mutated.

If you do this:The caller has no guarantee of the value of a after the call.

However, Now you as the caller or as a reader of this code know that a is the same.

You could always for this scenario make a copy of the list and pass that but now you are wasting cycles instead of using a language construct that makes more semantic sense.you can see here for some discussion on thisYour question (and follow-up comments) focus on whether the id() changes during an assignment. Focusing on this follow-on effect of the difference between immutable object replacement and mutable object modification rather than the difference itself is perhaps not the best approach.Before we continue, make sure that the behavior demonstrated below is what you expect from Python.In this case, the contents of a2 was changed, even though only a1 had a new value assigned. Contrast to the following:In this latter case, we replaced the entire list, rather than updating its contents. With immutable types such as tuples, this is the only behavior allowed.Why does this matter? Let's say you have a dict:Using a tuple, the dictionary is safe from having its keys changed "out from under it" to items which hash to a different value. This is critical to allow efficient implementation.

How to split a dos path into its components in Python

BeeBand

[How to split a dos path into its components in Python](https://stackoverflow.com/questions/3167154/how-to-split-a-dos-path-into-its-components-in-python)

I have a string variable which represents a dos path e.g:var = "d:\stuff\morestuff\furtherdown\THEFILE.txt"I want to split this string into:[ "d", "stuff", "morestuff", "furtherdown", "THEFILE.txt" ]I have tried using split() and replace() but they either only process the first backslash or they insert hex numbers into the string.I need to convert this string variable into a raw string somehow so that I can parse it.What's the best way to do this?I should also add that the contents of var i.e. the path that I'm trying to parse, is actually the return value of a command line query. It's not path data that I generate myself. Its stored in a file, and the command line tool is not going to escape the backslashes.

2010-07-02 15:41:56Z

I have a string variable which represents a dos path e.g:var = "d:\stuff\morestuff\furtherdown\THEFILE.txt"I want to split this string into:[ "d", "stuff", "morestuff", "furtherdown", "THEFILE.txt" ]I have tried using split() and replace() but they either only process the first backslash or they insert hex numbers into the string.I need to convert this string variable into a raw string somehow so that I can parse it.What's the best way to do this?I should also add that the contents of var i.e. the path that I'm trying to parse, is actually the return value of a command line query. It's not path data that I generate myself. Its stored in a file, and the command line tool is not going to escape the backslashes.I've been bitten loads of times by people writing their own path fiddling functions and getting it wrong. Spaces, slashes, backslashes, colons -- the possibilities for confusion are not endless, but mistakes are easily made anyway. So I'm a stickler for the use of os.path, and recommend it on that basis.(However, the path to virtue is not the one most easily taken, and many people when finding this are tempted to take a slippery path straight to damnation. They won't realise until one day everything falls to pieces, and they -- or, more likely, somebody else -- has to work out why everything has gone wrong, and it turns out somebody made a filename that mixes slashes and backslashes -- and some person suggests that the answer is "not to do that". Don't be any of these people. Except for the one who mixed up slashes and backslashes -- you could be them if you like.)You can get the drive and path+file like this:Get the path and the file:Getting the individual folder names is not especially convenient, but it is the sort of honest middling discomfort that heightens the pleasure of later finding something that actually works well:(This pops a "\" at the start of folders if the path was originally absolute. You could lose a bit of code if you didn't want that.)I would doFirst normalize the path string into a proper string for the OS. Then os.sep must be safe to use as a delimiter in string function split.You can simply use the most Pythonic approach (IMHO):Which will give you:The clue here is to use os.sep instead of '\\' or '/', as this makes it system independent.To remove colon from the drive letter (although I don't see any reason why you would want to do that), you can write:In Python >=3.4 this has become much simpler. You can now use pathlib.Path.parts to get all the parts of a path.Example:On a Windows install of Python 3 this will assume that you are working with Windows paths, and on *nix it will assume that you are working with posix paths. This is usually what you want, but if it isn't you can use the classes pathlib.PurePosixPath or pathlib.PureWindowsPath as needed:Edit:

There is also a backport to python 2 available: pathlib2The problem here starts with how you're creating the string in the first place.Done this way, Python is trying to special case these: \s, \m, \f, and \T.  In your case, \f is being treated as a formfeed (0x0C) while the other backslashes are handled correctly.  What you need to do is one of these:Then once you split either of these, you'll get the result you want.For a somewhat more concise solution, consider the following:I can't actually contribute a real answer to this one (as I came here hoping to find one myself), but to me the number of differing approaches and all the caveats mentioned is the surest indicator that Python's os.path module desperately needs this as a built-in function.It works for me:Sure you might need to also strip out the colon from the first component, but keeping it makes it possible to re-assemble the path.The r modifier marks the string literal as "raw"; notice how embedded backslashes are not doubled.The functional way, with a generator.In action:The stuff about about mypath.split("\\") would be better expressed as mypath.split(os.sep). sep is the path separator for your particular platform (e.g., \ for Windows, / for Unix, etc.), and the Python build knows which one to use. If you use sep, then your code will be platform agnostic.Let assume you have have a file filedata.txt with content:You can read and split the file paths:re.split() can help a little more then string.split()If you also want to support Linux and Mac paths, just add filter(None,result), so it will remove the unwanted '' from the split() since their paths starts with '/' or '//'. for example '//mount/...' or '/var/tmp/'You can recursively os.path.split the stringTesting this against some path strings, and reassembling the path with os.path.joinThe first element of the list may need to be treated differently depending on how you want to deal with drive letters, UNC paths and absolute and relative paths. Changing the last [p] to [os.path.splitdrive(p)] forces the issue by splitting the drive letter and directory root out into a tuple.Edit: I have realised that this answer is very similar to that given above by user1556435. I'm leaving my answer up as the handling of the drive component of the path is different.Just like others explained - your problem stemmed from using \, which is escape character in string literal/constant. OTOH, if you had that file path string from another source (read from file, console or returned by os function) - there wouldn't have been problem splitting on '\\' or r'\'.And just like others suggested, if you want to use \ in program literal, you have to either duplicate it \\ or the whole literal has to be prefixed by r, like so r'lite\ral' or r"lite\ral" to avoid the parser converting that \ and r to CR (carriage return) character.There is one more way though - just don't use backslash \ pathnames in your code! Since last century Windows recognizes and works fine with pathnames which use forward slash as directory separator /! Somehow not many people know that.. but it works:This by the way will make your code work on Unix, Windows and Mac... because all of them do use / as directory separator... even if you don't want to use the predefined constants of module os.I use the following as since it uses the os.path.basename function it doesn't add any slashes to the returned list. It also works with any platform's slashes: i.e window's \\ or unix's /. And furthermore, it doesn't add the \\\\ that windows uses for server paths :)So for '\\\\server\\folder1\\folder2\\folder3\\folder4'you get['server','folder1','folder2','folder3','folder4']I'm not actually sure if this fully answers the question, but I had a fun time writing this little function that keeps a stack, sticks to os.path-based manipulations, and returns the list/stack of items.Below line of code can handle:path = re.split(r'[///\]', path)use ntpath.split()One recursive for the fun.Not the most elegant answer, but should work everywhere:

What is the most efficient string concatenation method in python?

mshsayem

[What is the most efficient string concatenation method in python?](https://stackoverflow.com/questions/1316887/what-is-the-most-efficient-string-concatenation-method-in-python)

Is there any efficient mass string concatenation method in Python (like StringBuilder in C# or StringBuffer in Java)? I found following methods here:But what do you experts use or suggest, and why? [A related question here]

2009-08-22 19:53:41Z

Is there any efficient mass string concatenation method in Python (like StringBuilder in C# or StringBuffer in Java)? I found following methods here:But what do you experts use or suggest, and why? [A related question here]You may be interested in this: An optimization anecdote by Guido.  Although it is worth remembering also that this is an old article and it predates the existence of things like ''.join (although I guess string.joinfields is more-or-less the same)On the strength of that, the array module may be fastest if you can shoehorn your problem into it.  But ''.join is probably fast enough and has the benefit of being idiomatic and thus easier for other python programmers to understand.Finally, the golden rule of optimization: don't optimize unless you know you need to, and measure rather than guessing.You can measure different methods using the timeit module. That can tell you which is fastest, instead of random strangers on the internet making guesses.''.join(sequenceofstrings) is what usually works best -- simplest and fastest.Python 3.6 changed the game for string concatenation of known components with Literal String Interpolation.Given the test case from mkoistinen's answer, having stringsThe contenders are Thus currently the shortest and the most beautiful code possible is also fastest.In alpha versions of Python 3.6 the implementation of f'' strings was the slowest possible - actually the generated byte code is pretty much equivalent to the ''.join() case with unnecessary calls to str.__format__ which without arguments would just return self unchanged. These inefficiencies were addressed before 3.6 final.The speed can be contrasted with the fastest method for Python 2, which is + concatenation on my computer; and that takes 0.203 µs with 8-bit strings, and 0.259 µs if the strings are all Unicode.It depends on what you're doing.After Python 2.5, string concatenation with the + operator is pretty fast. If you're just concatenating a couple of values, using the + operator works best:However, if you're putting together a string in a loop, you're better off using the list joining method:...but notice that you have to be putting together a relatively high number of strings before the difference becomes noticeable.As per John Fouhy's answer, don't optimize unless you have to, but if you're here and asking this question, it may be precisely because you have to. In my case, I needed assemble some URLs from string variables... fast. I noticed no one (so far) seems to be considering the string format method, so I thought I'd try that and, mostly for mild interest, I thought I'd toss the string interpolation operator in there for good measuer. To be honest, I didn't think either of these would stack up to a direct '+' operation or a ''.join(). But guess what? On my Python 2.7.5 system, the string interpolation operator rules them all and string.format() is the worst performer:The results:If I use a shorter domain and shorter path, interpolation still wins out. The difference is more pronounced, though, with longer strings.Now that I had a nice test script, I also tested under Python 2.6, 3.3 and 3.4, here's the results. In Python 2.6, the plus operator is the fastest! On Python 3, join wins out. Note: these tests are very repeatable on my system. So, 'plus' is always faster on 2.6, 'intp' is always faster on 2.7 and 'join' is always faster on Python 3.x.Lesson learned:tl;dr:it pretty much depends on the relative sizes of the new string after every new concatenation.

With the + operator, for every concatenation a new string is made. If the intermediary strings are relatively long, the + becomes increasingly slower because the new intermediary string is being stored.Consider this case:Results1 0.004931926727292 0.0005090236663823 0.000422000885014 0.000482797622681In the case of 1&2, we add a large string, and join() performs about 10 times faster.

In case 3&4, we add a small string, and '+' performs slightly fasterI ran into a situation where I needed to have an appendable string of unknown size.  These are the benchmark results (python 2.7.3):This seems to show that '+=' is the fastest.  The results from the skymind link are a bit out of date.  (I realize that the second example is not complete, the final list would need to be joined.  This does show, however, that simply preparing the list takes longer than the string concat.)One Year later, let's test mkoistinen's answer with python 3.4.3:Nothing changed. Join is still the fastest method. With intp being arguably the best choice in terms of readability you might want to use intp nevertheless.Inspired by @JasonBaker's benchmarks, here's a simple one comparing 10 "abcdefghijklmnopqrstuvxyz" strings, showing that .join() is faster; even with this tiny increase in variables:For a small set of short strings (i.e. 2 or 3 strings of no more than a few characters), plus is still way faster. Using mkoistinen's wonderful script in Python 2 and 3:So when your code is doing a huge number of separate small concatenations, plus is the preferred way if speed is crucial.Probably "new f-strings in Python 3.6" is the most efficient way of concatenating strings.Using %sUsing .format    Using fSource: https://realpython.com/python-f-strings/

Automatic creation date for Django model form objects?

Roger

[Automatic creation date for Django model form objects?](https://stackoverflow.com/questions/3429878/automatic-creation-date-for-django-model-form-objects)

What's the best way to set a creation date for an object automatically, and also a field that will record when the object was last updated?models.py:views.py:I get the error:Do I have to manually set this value myself? I thought that was the point of the parameters passed to DateTimeField (or are they just defaults, and since I've set editable=False they don't get displayed on the form, hence don't get submitted in the request, and therefore don't get put into the form?).What's the best way of doing this? An __init__ method?

2010-08-07 09:21:28Z

What's the best way to set a creation date for an object automatically, and also a field that will record when the object was last updated?models.py:views.py:I get the error:Do I have to manually set this value myself? I thought that was the point of the parameters passed to DateTimeField (or are they just defaults, and since I've set editable=False they don't get displayed on the form, hence don't get submitted in the request, and therefore don't get put into the form?).What's the best way of doing this? An __init__ method?You can use the auto_now and auto_now_add options for updated_at and created_at respectively.Well, the above answer is correct, auto_now_add and auto_now would do it, but it would be better to make an abstract class and use it in any model where you require created_at and updated_at fields.Now anywhere you want to use it you can do a simple inherit and you can use timestamp in any model you make like.In this way, you can leverage object-oriented reusability, in Django DRY(don't repeat yourself)

Can you give a Django app a verbose name for use throughout the admin?

rmh

[Can you give a Django app a verbose name for use throughout the admin?](https://stackoverflow.com/questions/612372/can-you-give-a-django-app-a-verbose-name-for-use-throughout-the-admin)

In the same way that you can give fields and models verbose names that appear in the Django admin, can you give an app a custom name?

2009-03-04 20:51:03Z

In the same way that you can give fields and models verbose names that appear in the Django admin, can you give an app a custom name?Django 1.8+Per the 1.8 docs (and current docs),Example:Then alter your AppConfig as listed below.Django 1.7As stated by rhunwicks' comment to OP, this is now possible out of the box since Django 1.7Taken from the docs:then set the default_app_config variable to YourAppConfigPrior to Django 1.7You can give your application a custom name by defining app_label in your model definition. But as django builds the admin page it will hash models by their app_label, so if you want them to appear in one application, you have to define this name in all models of your application.As stated by rhunwicks' comment to OP, this is now possible out of the box since Django 1.7Taken from the docs:then set the default_app_config variable to YourAppConfigIf you have more than one model in the app just create a model with the Meta information and create subclasses of that class for all your models.Give them a verbose_name property.Don't get your hopes up. You will also need to copy the index view from django.contrib.admin.sites into your own ProjectAdminSite view and include it in your own custom admin instance:then tweak the copied view so that it uses your verbose_name property as the label for the app.I did it by adding something a bit like this to the copied view:While you are tweaking the index view why not add an 'order' property too.Well I started an app called todo and have now decided I want it to be named Tasks. The problem is that I already have data within my table so my work around was as follows. Placed into the models.py:Hope it helps.For Django 1.4 (not yet released, but trunk is pretty stable), you can use the following method. It relies on the fact that AdminSite now returns a TemplateResponse, which you can alter before it is rendered.Here, we do a small bit of monkey patching to insert our behaviour, which can be avoided if you use a custom AdminSite subclass.This fixes the index and the app_index views. It doesn't fix the bread crumbs in all other admin views.First you need to create a apps.py file like this on your appfolder:To load this AppConfig subclass by default:Is the best way to do. tested on Django 1.7For the person who had problems with the SpanishThis code enable the utf-8 compatibility on python2 scriptsNo, but you can copy admin template and define app name there.I'm using django-admin-tools for that.There is a hack that can be done that does not require any migrations.

Taken from Ionel's blog and credit goes to him: http://blog.ionelmc.ro/2011/06/24/custom-app-names-in-the-django-admin/There is also a ticket for this that should be fixed in Django 1.7  https://code.djangoproject.com/ticket/3591"""Suppose you have a model like this:You have verbose_name, however you want to customise app_label too for different display in admin. Unfortunatelly having some arbitrary string (with spaces) doesn't work and it's not for display anyway.Turns out that the admin uses app_label. title () for display so we can make a little hack: str subclass with overriden title method:Now we can have the model like this:and the admin will show "The stuff box" as the app name."""If you already have existing tables using the old app name, and you don't want to migrate them, then just set the app_label on a proxy of the original model.Then you just have to change this in your admin.py:Be aware that the url will be /admin/NewAPPname/mynewmodel/ so you might just want to make sure that the class name for the new model looks as close to the old model as possible.Well, this works for me. In the app.py use this:In setting.py add the name of App and the class name present in app.py file in App folder]The following plug-and-play piece of code works perfectly since Django 1.7. All you have to do is copy the below code in the __init__.py file of the specific app and change the VERBOSE_APP_NAME parameter.If you use this for multiple apps, you should factor out the get_current_app_name function to a helper file.

Concurrent.futures vs Multiprocessing in Python 3

GIS-Jonathan

[Concurrent.futures vs Multiprocessing in Python 3](https://stackoverflow.com/questions/20776189/concurrent-futures-vs-multiprocessing-in-python-3)

Python 3.2 introduced Concurrent Futures, which appear to be some advanced combination of the older threading and multiprocessing modules.What are the advantages and disadvantages of using this for CPU bound tasks over the older multiprocessing module?This article suggests they're much easier to work with - is that the case?

2013-12-25 19:41:55Z

Python 3.2 introduced Concurrent Futures, which appear to be some advanced combination of the older threading and multiprocessing modules.What are the advantages and disadvantages of using this for CPU bound tasks over the older multiprocessing module?This article suggests they're much easier to work with - is that the case?I wouldn't call concurrent.futures more "advanced" - it's a simpler interface that works very much the same regardless of whether you use multiple threads or multiple processes as the underlying parallelization gimmick.So, like virtually all instances of "simpler interface", much the same tradeoffs are involved:  it has a shallower learning curve, in large part just because there's so much less available to be learned; but, because it offers fewer options, it may eventually frustrate you in ways the richer interfaces won't.So far as CPU-bound tasks go, that's waaaay too under-specified to say much meaningful.  For CPU-bound tasks under CPython, you need multiple processes rather than multiple threads to have any chance of getting a speedup.  But how much (if any) of a speedup you get depends on the details of your hardware, your OS, and especially on how much inter-process communication your specific tasks require.  Under the covers, all inter-process parallelization gimmicks rely on the same OS primitives - the high-level API you use to get at those isn't a primary factor in bottom-line speed.Edit: exampleHere's the final code shown in the article you referenced, but I'm adding an import statement needed to make it work:Here's exactly the same thing using multiprocessing instead:Note that the ability to use multiprocessing.Pool objects as context managers was added in Python 3.3.Which one is easier to work with?  LOL ;-)  They're essentially identical.One difference is that Pool supports so many different ways of doing things that you may not realize how easy it can be until you've climbed quite a way up the learning curve.Again, all those different ways are both a strength and a weakness.  They're a strength because the flexibility may be required in some situations.  They're a weakness because of "preferably only one obvious way to do it".  A project sticking exclusively (if possible) to concurrent.futures will probably be easier to maintain over the long run, due to the lack of gratuitous novelty in how its minimalistic API can be used.

How to run an .ipynb Jupyter Notebook from terminal?

Vincent

[How to run an .ipynb Jupyter Notebook from terminal?](https://stackoverflow.com/questions/35545402/how-to-run-an-ipynb-jupyter-notebook-from-terminal)

I have some code in a .ipynb file and got it to the point where I don't really need the "interactive" feature of IPython Notebook. I would like to just run it straight from a Mac Terminal Command Line.Basically, if this were just a .py file, I believe I could just do python filename.py from the command line. Is there something similar for a .ipynb file?

2016-02-22 03:35:28Z

I have some code in a .ipynb file and got it to the point where I don't really need the "interactive" feature of IPython Notebook. I would like to just run it straight from a Mac Terminal Command Line.Basically, if this were just a .py file, I believe I could just do python filename.py from the command line. Is there something similar for a .ipynb file?From the command line you can convert a notebook to python with this command:https://github.com/jupyter/nbconvertYou may have to install the python mistune package:nbconvert allows you to run notebooks with the --execute flag:If you want to run a notebook and produce a new notebook, you can add --to notebook:Or if you want to replace the existing notebook with the new output:Since that's a really long command, you can use an alias:You can export all your code from .ipynb and save it as a .py script. Then you can run the script in your terminal.Hope it helps.For new version instead of:You can use jupyter instend of ipython:In your Terminal run ipython: then locate your script and put there:Update with quoted comment by author for better visibility:Install runipy library that allows running your code on terminalAfter just compiler your code:You can try cronjob as well. All information is hereIn my case, the command that best suited me was:Why? This command does not create extra files (just like a .py file) and the output of the cells is overwritten everytime the notebook is executed.If you run:

How do I print bold text in Python?

Jia-Luo

[How do I print bold text in Python?](https://stackoverflow.com/questions/8924173/how-do-i-print-bold-text-in-python)

How do I print bold text in Python?For example:What should I do so that the text「hello」is displayed in bold?

2012-01-19 10:06:01Z

How do I print bold text in Python?For example:What should I do so that the text「hello」is displayed in bold?Use this:And to change back to normal:This page is a good reference for printing in colors and font-weights. Go to the section that says 'Set graphics mode:'And note this won't work on all operating systems but you don't need any modules. and then try thisand that's it for me In straight-up computer programming, there is no such thing as "printing bold text". Let's back up a bit and understand that your text is a string of bytes and bytes are just bundles of bits. To the computer, here's your "hello" text, in binary.Each one or zero is a bit. Every eight bits is a byte. Every byte is, in a string like that in Python 2.x, one letter/number/punctuation item (called a character). So for example:The computer translates those bits into letters, but in a traditional string (called an ASCII string), there is nothing to indicate bold text. In a Unicode string, which works a little differently, the computer can support international language characters, like Chinese ones, but again, there's nothing to say that some text is bold and some text is not. There's also no explicit font, text size, etc.In the case of printing HTML, you're still outputting a string. But the computer program reading that string (a web browser) is programmed to interpret text like this is <b>bold</b> as "this is bold" when it converts your string of letters into pixels on the screen. If all text were WYSIWYG, the need for HTML itself would be mitigated -- you would just select text in your editor and bold it instead of typing out the HTML.Other programs use different systems -- a lot of answers explained a completely different system for printing bold text on terminals. I'm glad you found out how to do what you want to do, but at some point, you'll want to understand how strings and memory work.This depends if you're using linux/unix:The word text should be bold.Check out colorama.  It doesn't necessarily help with bolding... but you can do colorized output on both Windows and Linux, and control the brightness:There is a very useful module for formatting text (bold, underline, colors..) in Python. It uses curses lib but it's very straight-forward to use.An example:I wrote a simple module named colors.py to make this a little more pythonic:\033[1m is the unicode for bold in the terminal

\033[0m is the unicode for end the edited text and back default text formate!!!!!if you do not use \033[0m than all upcoming text of the terminal will become bold!!!!!!!!!Assuming that you really mean "print" on a real printing terminal:Just send that to your stdout.Install the termcolor moduleand then try this for colored textor this for bold text:In Python 3 you can alternatively use cprint as a drop-in replacement for the built-in print, with the optional second parameter for colors or the attrs parameter for bold (and other attributes such as underline) in addition to the normal named print arguments such as file or end.Some terminals allow to print colored text. Some colors look like if they are "bold". Try:The sequence '\033[1;37m' makes some terminals to start printing in "bright white" that may look a bit like bolded white. '\033[0;0m' will turn it off.In python 3 you could use colorama - simple_colors:

(Simple Colours page: https://pypi.org/project/simple-colors/ - go to the heading 'Usage'.) Before you do what is below, make sure you pip install simple_colours.

Run certain code every n seconds [duplicate]

John Howard

[Run certain code every n seconds [duplicate]](https://stackoverflow.com/questions/3393612/run-certain-code-every-n-seconds)

Is there a way to, for example, print Hello World! every n seconds?

For example, the program would go through whatever code I had, then once it had been 5 seconds (with time.sleep()) it would execute that code. I would be using this to update a file though, not print Hello World.For example:

2010-08-03 04:40:04Z

Is there a way to, for example, print Hello World! every n seconds?

For example, the program would go through whatever code I had, then once it had been 5 seconds (with time.sleep()) it would execute that code. I would be using this to update a file though, not print Hello World.For example:https://docs.python.org/3/library/threading.html#timer-objectsMy humble take on the subject, a generalization of Alex Martelli's answer, with start() and stop() control:Usage:Features:Save yourself a schizophrenic episode and use the Advanced Python scheduler:

    http://pythonhosted.org/APSchedulerThe code is so simple:That'll run as a function. The while True: makes it run forever. You can always take it out of the function if you need.Here is a simple example compatible with APScheduler 3.00+:Alternatively, you can use the following. Unlike many of the alternatives, this timer will execute the desired code every n seconds exactly (irrespective of the time it takes for the code to execute). So this is a great option if you cannot afford any drift.Here's a version that doesn't create a new thread every n seconds:The event is used to stop the repetitions:See Improve current implementation of a setInterval pythonYou can start a separate thread whose sole duty is to count for 5 seconds, update the file, repeat. You wouldn't want this separate thread to interfere with your main thread.

Adding Python Path on Windows 7

rogerklutz

[Adding Python Path on Windows 7](https://stackoverflow.com/questions/6318156/adding-python-path-on-windows-7)

I've been trying to add the Python path to the command line on Windows 7, yet no matter the method I try, nothing seems to work. I've used the set command, I've tried adding it through the Edit Environment variables prompt etc.Further more if I run the set command on the command line it lists thisYet it still doesn't recognize the Python command.Reading the documentation, and various other sources hasn't seemed to help.Edit: Just to clarify further, I've appended the path of the Python executable to PATH in edit environment prompt. Doesn't seem to work.

2011-06-11 19:46:56Z

I've been trying to add the Python path to the command line on Windows 7, yet no matter the method I try, nothing seems to work. I've used the set command, I've tried adding it through the Edit Environment variables prompt etc.Further more if I run the set command on the command line it lists thisYet it still doesn't recognize the Python command.Reading the documentation, and various other sources hasn't seemed to help.Edit: Just to clarify further, I've appended the path of the Python executable to PATH in edit environment prompt. Doesn't seem to work.When setting Environmental Variables in Windows, I have gone wrong on many, many occasions. 

I thought I should share a few of my past mistakes here hoping that it might help someone.

(These apply to all Environmental Variables, not just when setting Python Path)Watch out for these possible mistakes:Hope this helps someone.  Open cmd.exe with administrator privileges (right click on app). Then type:Remember to end with a semi-colon and don't include a trailing slash.I've had a problem with this for a LONG time. I added it to my path in every way I could think of but here's what finally worked for me:IDK why this works but it did for me.then try typing "python" into your command line and it should work!Edit:Lately I've been using this program which seems to work pretty well. There's also this one which looks pretty good too, although I've never tried it.Try adding this python.bat file to System32 folder and the command line will now run python when you type in python python.batSource:https://github.com/KartikTalwar/dotfiles/blob/master/bat/python.batYou can set the path from the current cmd window using the PATH = command. That will only add it for the current cmd instance. if you want to add it permanently, you should add it to system variables. (Computer > Advanced System Settings > Environment Variables)You would goto your cmd instance, and put in PATH=C:/Python27/;%PATH%.Make sure you don't add a space before the new directory.Good:

old;old;old;newBad:

old;old;old; newPython comes with a small utility that does just this.  From the command line run:Make sure you close the command window (with exit or the close button) and open it again.The following program will add the python executable path and the subdir Scripts (which is where e.g. pip and easy_install are installed) to your environment. It finds the path to the python executable from the registry key binding the .py extension. It will remove old python paths in your environment. Works with XP (and probably Vista) as well.

It only uses modules that come with the basic windows installer.I know this post is old but I'd like to add that the solutions assume admin privs. If you don't have those you can:Go to control panel, type path (this is Windows 7 now so that's in the Search box) and click "Edit Environment variables for your account". You'll now see the Environment Variable dialog with "User variables" on the top and "System variables" below.You can, as a user, click the top "New" button and add:Variable name: PATH

Variable value: C:\Python27(no spaces anywhere) and click OK. Once your command prompt is restarted, any PATH in the User variables is appended to the end of the System Path. It doesn't replace the PATH in any other way.If you want a specific full path set up, you're better off creating a batch file like this little one:Call it "compiler.bat" or whatever and double click to start it. Or link to it. Or pin it etc...You need to make changes in your system variable

-- Right click on "My computer"

-- Click "Properties"

-- Click "Advanced system settings" in the side panel

-- Click on Environment Variable -- You will two sections of user variable and system variable

-- Under system variable section search for the variable 'Path' click on edit and add

  "C:\Python27;" (without quotes) save it

-- Now open command line type 'path' hit enter you will see path variable has been modified

-- Now type python --version you will see the python versionAnd it is doneFor anyone trying to achieve this with Python 3.3+, the Windows installer now includes an option to add python.exe to the system search path. Read more in the docs.Working with Windows environment variables is always a horrible experience. Recently, I found an amazing tool called Rapid Environment Editor, which gives an awesomely simple GUI for managing them.If you use chocolatey, you can install it using choco install rapidee. Otherwise, take a look at http://www.rapidee.com/en/download

Re-reading this, it sounds like a paid shill, but I swear I'm not! It's just been one of the most useful utilities in my toolkit for a while and I'm surprised no one seems to know about it.If Python was installed with another program, such as ArcGIS 10.1 in my case, then you also must include any extra folders that path to the python.exe in your Environment Variables.So my Environment Variables looks like this:System variables > Path > add ;C:\Python27\ArcGIS10.1This question is pretty old, but I just ran into a similar problem and my particular solution wasn't listed here:Make sure you don't have a folder in your PATH that doesn't exist.In my case, I had a bunch of default folders (Windows, Powershell, Sql Server, etc) and then a custom C:\bin that I typically use, and then various other tweaks like c:\python17, etc. It turns out that the cmd processor was finding that c:\bin didn't exist and then stopped processing the rest of the variable.Also, I don't know that I ever would have noticed this without PATH manager. It nicely highlighted the fact that that item was invalid.I just installed Python 3.3 on Windows 7 using the option "add python to PATH".In PATH variable, the installer automatically added a final backslash: C:\Python33\

and so it did not work on command prompt (i tried closing/opening the prompt several times)I removed the final backslash and then it worked: C:\Python33Thanks Ram Narasimhan for your tip #4 !I organized my python environment variable like this under Win7 64-bit using cmd.I set the variable PYTHONPATH via environment variable menue of windows and added %PYTHONPATH% to the PATH variable:...;%PYTHONPATH%The cmd shell expands the variable correctly to this:Do not forget to restart cmd shell after changing PATH.write that on your Command Prompt:Replace %path% by the Path of your Python Folder Example:If you have got frustrated by setting the path for the python just download the new version of python uninstall the older version of the python and while installing the new version it will ask whether to set path mark that and installits the best way

Given a string of a million numbers, return all repeating 3 digit numbers

its.david

[Given a string of a million numbers, return all repeating 3 digit numbers](https://stackoverflow.com/questions/47581326/given-a-string-of-a-million-numbers-return-all-repeating-3-digit-numbers)

I had an interview with a hedge fund company in New York a few months ago and unfortunately, I did not get the internship offer as a data/software engineer. (They also asked the solution to be in Python.)I pretty much screwed up on the first interview problem...For example: if the string was: 123412345123456 then the function/program would return:They did not give me the solution after I failed the interview, but they did tell me that the time complexity for the solution was constant of 1000 since all the possible outcomes are between:000 --> 999Now that I'm thinking about it, I don't think it's possible to come up with a constant time algorithm. Is it? 

2017-11-30 19:37:45Z

I had an interview with a hedge fund company in New York a few months ago and unfortunately, I did not get the internship offer as a data/software engineer. (They also asked the solution to be in Python.)I pretty much screwed up on the first interview problem...For example: if the string was: 123412345123456 then the function/program would return:They did not give me the solution after I failed the interview, but they did tell me that the time complexity for the solution was constant of 1000 since all the possible outcomes are between:000 --> 999Now that I'm thinking about it, I don't think it's possible to come up with a constant time algorithm. Is it? You got off lightly, you probably don't want to be working for a hedge fund where the quants don't understand basic algorithms :-)There is no way to process an arbitrarily-sized data structure in O(1) if, as in this case, you need to visit every element at least once. The best you can hope for is O(n) in this case, where n is the length of the string.It appears to me you could have impressed them in a number of ways.First, by informing them that it's not possible to do it in O(1), unless you use the "suspect" reasoning given above.Second, by showing your elite skills by providing Pythonic code such as:This outputs:though you could, of course, modify the output format to anything you desire.And, finally, by telling them there's almost certainly no problem with an O(n) solution, since the code above delivers results for a one-million-digit string in well under half a second. It seems to scale quite linearly as well, since a 10,000,000-character string takes 3.5 seconds and a 100,000,000-character one takes 36 seconds.And, if they need better than that, there are ways to parallelise this sort of stuff that can greatly speed it up.Not within a single Python interpreter of course, due to the GIL, but you could split the string into something like (overlap indicated by vv is required to allow proper processing of the boundary areas):You can farm these out to separate workers and combine the results afterwards.The splitting of input and combining of output are likely to swamp any saving with small strings (and possibly even million-digit strings) but, for much larger data sets, it may well make a difference. My usual mantra of "measure, don't guess" applies here, of course.This mantra also applies to other possibilities, such as bypassing Python altogether and using a different language which may be faster.For example, the following C code, running on the same hardware as the earlier Python code, handles a hundred million digits in 0.6 seconds, roughly the same amount of time as the Python code processed one million. In other words, much faster:Constant time isn't possible. All 1 million digits need to be looked at at least once, so that is a time complexity of O(n), where n = 1 million in this case.For a simple O(n) solution, create an array of size 1000 that represents the number of occurrences of each possible 3 digit number. Advance 1 digit at a time, first index == 0, last index == 999997, and increment array[3 digit number] to create a histogram (count of occurrences for each possible 3 digit number). Then output the content of the array with counts > 1.The simple O(n) solution would be to count each 3-digit number:This would search through all 1 million digits 1000 times.Traversing the digits only once:Timing shows that iterating only once over the index is twice as fast as using count.A million is small for the answer I give below. Expecting only that you have to be able to run the solution in the interview, without a pause, then The following works in less than two seconds and gives the required result:Hopefully the interviewer would be looking for use of the standard libraries collections.Counter class.I wrote a blog post on this with more explanation.Here is a NumPy implementation of the "consensus" O(n) algorithm: walk through all triplets and bin as you go. The binning is done by upon encountering say "385", adding one to bin[3, 8, 5] which is an O(1) operation. Bins are arranged in a 10x10x10 cube. As the binning is fully vectorized there is no loop in the code.Unsurprisingly, NumPy is a bit faster than @Daniel's pure Python solution on large data sets. Sample output:I would solve the problem as follows:Applied to your example string, this yields:This solution runs in O(n) for n being the length of the provided string, and is, I guess, the best you can get.As per my understanding, you cannot have the solution in a constant time. It will take at least one pass over the million digit number (assuming its a string). You can have a 3-digit rolling iteration over the digits of the million length number and increase the value of hash key by 1 if it already exists or create a new hash key (initialized by value 1) if it doesn't exists already in the dictionary.The code will look something like this:You can filter down to the keys which have item value greater than 1.As mentioned in another answer, you cannot do this algorithm in constant time, because you must look at at least n digits.  Linear time is the fastest you can get.However, the algorithm can be done in O(1) space.  You only need to store the counts of each 3 digit number, so you need an array of 1000 entries.  You can then stream the number in.My guess is that either the interviewer misspoke when they gave you the solution, or you misheard "constant time" when they said "constant space."Here's my answer:The array lookup method is very fast (even faster than @paul-panzer's numpy method!).  Of course, it cheats since it isn't technicailly finished after it completes, because it's returning a generator.  It also doesn't have to check every iteration if the value already exists, which is likely to help a lot.Image as answer:Looks like a sliding window.Here is my solution:With a bit of creativity in for loop(and additional lookup list with True/False/None for example) you should be able to get rid of last line, as you only want to create keys in dict that we visited once up to that point.

Hope it helps :)-Telling from the perspective of C.

-You can have an int 3-d array results[10][10][10];

-Go from 0th location to n-4th location, where n being the size of the string array.

-On each location, check the current, next and next's next.

-Increment the cntr as resutls[current][next][next's next]++;

-Print the values of -It is O(n) time, there is no comparisons involved.

-You can run some parallel stuff here by partitioning the array and calculating the matches around the partitions.

Using headers with the Python requests library's get method

Breedly

[Using headers with the Python requests library's get method](https://stackoverflow.com/questions/6260457/using-headers-with-the-python-requests-librarys-get-method)

So I recently stumbled upon this great library for handling HTTP requests in Python; found here http://docs.python-requests.org/en/latest/index.html.I love working with it, but I can't figure out how to add headers to my get requests. Help?

2011-06-07 04:03:15Z

So I recently stumbled upon this great library for handling HTTP requests in Python; found here http://docs.python-requests.org/en/latest/index.html.I love working with it, but I can't figure out how to add headers to my get requests. Help?According to the api, the headers can all be passed in using requests.get:Seems pretty straightforward, according to the docs on the page you linked (emphasis mine).This answer taught me that you can set headers for an entire session:Bonus: Sessions also handle cookies.

Convert Python program to C/C++ code? [closed]

CrazyFlyingCloseline

[Convert Python program to C/C++ code? [closed]](https://stackoverflow.com/questions/4650243/convert-python-program-to-c-c-code)

is it possible to convert a Python program to C/C++?I need to implement a couple of algorithms, and I'm not sure if the performance gap is big enough to justify all the pain I'd go through when doing it in C/C++ (which I'm not good at). I thought about writing one simple algorithm and benchmark it against such a converted solution. If that alone is significantly faster than the Python version, then I'll have no other choice than doing it in C/C++.

2011-01-10 18:46:12Z

is it possible to convert a Python program to C/C++?I need to implement a couple of algorithms, and I'm not sure if the performance gap is big enough to justify all the pain I'd go through when doing it in C/C++ (which I'm not good at). I thought about writing one simple algorithm and benchmark it against such a converted solution. If that alone is significantly faster than the Python version, then I'll have no other choice than doing it in C/C++.Yes. Look at Cython. It does just that: Converts Python to C for speedups."invest" isn't the right word here.This quote is important.Shed Skin is "a (restricted) Python-to-C++ compiler".Just came across this new tool in hacker news.From their page - "Nuitka is a good replacement for the Python interpreter and compiles every construct that CPython 2.6, 2.7, 3.2 and 3.3 offer. It translates the Python into a C++ program that then uses "libpython" to execute in the same way as CPython does, in a very compatible way."Another option - to convert to C++ besides Shed Skin - is Pythran.To quote High Performance Python by Micha Gorelick and Ian Ozsvald:I know this is an older thread but I wanted to give what I think to be helpful information.I personally use PyPy which is really easy to install using pip. I interchangeably use Python/PyPy interpreter, you don't need to change your code at all and I've found it to be roughly 40x faster than the standard python interpreter (Either Python 2x or 3x). I use pyCharm Community Edition to manage my code and I love it. I like writing code in python as I think it lets you focus more on the task than the language, which is a huge plus for me. And if you need it to be even faster, you can always compile to a binary for Windows, Linux, or Mac (not straight forward but possible with other tools). From my experience, I get about 3.5x speedup over PyPy when compiling, meaning 140x faster than python. PyPy is available for Python 3x and 2x code and again if you use an IDE like PyCharm you can interchange between say PyPy, Cython, and Python very easily (takes a little of initial learning and setup though). Some people may argue with me on this one, but I find PyPy to be faster than Cython. But they're both great choices though.Edit: I'd like to make another quick note about compiling: when you compile, the resulting binary is much bigger than your python script as it builds all dependencies into it, etc. But then you get a few distinct benefits: speed!, now the app will work on any machine (depending on which OS you compiled for, if not all. lol) without Python or libraries, it also obfuscates your code and is technically 'production' ready (to a degree). Some compilers also generate C code, which I haven't really looked at or seen if it's useful or just gibberish. Good luck.Hope that helps.I realize that an answer on a quite new solution is missing. If Numpy is used in the code, I would advice to try Pythran:http://pythran.readthedocs.io/For the functions I tried, Pythran gives extremely good results. The resulting functions are as fast as well written Fortran code (or only slightly slower) and a little bit faster than the (quite optimized) Cython solution.The advantage compared to Cython is that you just have to use Pythran on the Python function optimized for Numpy, meaning that you do not have to expand the loops and add types for all variables in the loop. Pythran takes its time to analyse the code so it understands the operations on numpy.ndarray.It is also a huge advantage compared to Numba or other projects based on just-in-time compilation for which (to my knowledge), you have to expand the loops to be really efficient. And then the code with the loops becomes very very inefficient using only CPython and Numpy...A drawback of Pythran: no classes! But since only the functions that really need to be optimized have to be compiled, it is not very annoying.Another point: Pythran supports well (and very easily) OpenMP parallelism. But I don't think mpi4py is supported...http://code.google.com/p/py2c/ looks like a possibility - they also mention on their site: Cython, Shedskin and RPython and confirm that they are converting Python code to pure C/C++ which is much faster than C/C++ riddled with Python API calls. Note: I haven’t tried it but I am going to.. 

Pandas: sum DataFrame rows for given columns

Colonel Beauvel

[Pandas: sum DataFrame rows for given columns](https://stackoverflow.com/questions/25748683/pandas-sum-dataframe-rows-for-given-columns)

I have the following DataFrame:I would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.Going across forums, I thought something like this would work:But no!I would like to realize the operation having the list of columns ['a','b','d'] and df as inputs.

2014-09-09 15:36:03Z

I have the following DataFrame:I would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.Going across forums, I thought something like this would work:But no!I would like to realize the operation having the list of columns ['a','b','d'] and df as inputs.You can just sum and set param axis=1 to sum the rows, this will ignore none numeric columns:If you want to just sum specific columns then you can create a list of the columns and remove the ones you are not interested in:If you have just a few columns to sum, you can write: This creates new column e with the values:For longer lists of columns, EdChum's answer is preferred.This is a simpler way using iloc to select which columns to sum:Produces:I can't find a way to combine a range and specific columns that works e.g. something like:Create a list of column names you want to add up.If you want the sum for certain rows, specify the rows using ':'Following syntax helped me when I have columns in sequence You can simply pass your dataframe into the following function: Example: I have a dataframe (awards_frame) as follows: ...and I want to create a new column that shows the sum of awards for each row:Usage: I simply pass my awards_frame into the function, also specifying the name of the new column, and a list of column names that are to be summed: Result: 

Best way to create a simple python web service [closed]

Jeremy Cantrell

[Best way to create a simple python web service [closed]](https://stackoverflow.com/questions/415192/best-way-to-create-a-simple-python-web-service)

I've been using python for years, but I have little experience with python web programming. I'd like to create a very simple web service that exposes some functionality from an existing python script for use within my company. It will likely return the results in csv. What's the quickest way to get something up? If it affects your suggestion, I will likely be adding more functionality to this, down the road.

2009-01-06 02:17:54Z

I've been using python for years, but I have little experience with python web programming. I'd like to create a very simple web service that exposes some functionality from an existing python script for use within my company. It will likely return the results in csv. What's the quickest way to get something up? If it affects your suggestion, I will likely be adding more functionality to this, down the road.Have a look at werkzeug. Werkzeug started as a simple collection of various utilities for WSGI applications and has become one of the most advanced WSGI utility modules. It includes a powerful debugger, full featured request and response objects, HTTP utilities to handle entity tags, cache control headers, HTTP dates, cookie handling, file uploads, a powerful URL routing system and a bunch of community contributed addon modules.It includes lots of cool tools to work with http and has the advantage that you can use it with wsgi in different environments (cgi, fcgi, apache/mod_wsgi or with a plain simple python server for debugging). web.py is probably the simplest web framework out there.  "Bare" CGI is simpler, but you're completely on your own when it comes to making a service that actually does something."Hello, World!" according to web.py isn't much longer than an bare CGI version, but it adds URL mapping, HTTP command distinction, and query parameter parsing for free:The simplest way to get a Python script online is to use CGI:Put that code in a script that lives in your web server CGI directory, make it executable, and run it. The cgi module has a number of useful utilities when you need to accept parameters from the user.Raw CGI is kind of a pain, Django is kind of heavyweight.  There are a number of simpler, lighter frameworks about, e.g. CherryPy.  It's worth looking around a bit.Look at the WSGI reference implementation.  You already have it in your Python libraries.  It's quite simple.If you mean with "Web Service" something accessed by other Programms  SimpleXMLRPCServer might be right for you. It is included with every Python install since Version 2.2.For Simple human accessible things I usually use Pythons SimpleHTTPServer which also comes with every install. Obviously you also could access SimpleHTTPServer by client programs.Life is simple if you get a good web framework.  Web services in Django are easy.  Define your model, write view functions that return your CSV documents.  Skip the templates.If you mean "web service" in SOAP/WSDL sense, you might want to look at Generating a WSDL using Python and SOAPpymaybe Twisted

http://twistedmatrix.com/trac/

Why does Python print unicode characters when the default encoding is ASCII?

Michael Ekoka

[Why does Python print unicode characters when the default encoding is ASCII?](https://stackoverflow.com/questions/2596714/why-does-python-print-unicode-characters-when-the-default-encoding-is-ascii)

From the Python 2.6 shell:I expected to have either some gibberish or an Error after the print statement, since the "é" character isn't part of ASCII and I haven't specified an encoding. I guess I don't understand what ASCII being the default encoding means.EDITI moved the edit to the Answers section and accepted it as suggested.

2010-04-08 00:03:08Z

From the Python 2.6 shell:I expected to have either some gibberish or an Error after the print statement, since the "é" character isn't part of ASCII and I haven't specified an encoding. I guess I don't understand what ASCII being the default encoding means.EDITI moved the edit to the Answers section and accepted it as suggested.Thanks to bits and pieces from various replies, I think we can stitch up an explanation. By trying to print an unicode string, u'\xe9', Python implicitly try to encode that string using the encoding scheme currently stored in sys.stdout.encoding. Python actually picks up this setting from the environment it's been initiated from. If it can't find a proper encoding from the environment, only then does it revert to its default, ASCII.For example, I use a bash shell which encoding defaults to UTF-8. If I start Python from it, it picks up and use that setting:Let's for a moment exit the Python shell and set bash's environment with some bogus encoding:Then start the python shell again and verify that it does indeed revert to its default ascii encoding.Bingo! If you now try to output some unicode character outside of ascii you should get a nice error messageLets exit Python and discard the bash shell. We'll now observe what happens after Python outputs strings. For this we'll first start a bash shell within a graphic terminal (I use Gnome Terminal) and we'll set the terminal to decode output with ISO-8859-1 aka latin-1 (graphic terminals usually have an option to Set Character Encoding in one of their dropdown menus). Note that this doesn't change the actual shell environment's encoding, it only changes the way the terminal itself will decode output it's given, a bit like a web browser does. You can therefore change the terminal's encoding, independantly from the shell's environment. Let's then start Python from the shell and verify that sys.stdout.encoding is set to the shell environment's encoding (UTF-8 for me):(1) python outputs binary string as is, terminal receives it and tries to match its value with latin-1 character map. In latin-1, 0xe9 or 233 yields the character "é" and so that's what the terminal displays.(2) python attempts to implicitly encode the Unicode string with whatever scheme is currently set in sys.stdout.encoding, in this instance it's "UTF-8". After UTF-8 encoding, the resulting binary string is '\xc3\xa9' (see later explanation). Terminal receives the stream as such and tries to decode 0xc3a9 using latin-1, but latin-1 goes from 0 to 255 and so, only decodes streams 1 byte at a time. 0xc3a9 is 2 bytes long, latin-1 decoder therefore interprets it as 0xc3 (195) and 0xa9 (169) and that yields 2 characters: Ã and ©.(3) python encodes unicode code point u'\xe9' (233) with the latin-1 scheme. Turns out latin-1 code points range is 0-255 and points to the exact same character as Unicode within that range. Therefore, Unicode code points in that range will yield the same value when encoded in latin-1. So u'\xe9' (233) encoded in latin-1 will also yields the binary string '\xe9'. Terminal receives that value and tries to match it on the latin-1 character map. Just like case (1), it yields "é" and that's what's displayed. Let's now change the terminal's encoding settings to UTF-8 from the dropdown menu (like you would change your web browser's encoding settings). No need to stop Python or restart the shell. The terminal's encoding now matches Python's. Let's try printing again:(4) python outputs a binary string as is. Terminal attempts to decode that stream with UTF-8. But UTF-8 doesn't understand the value 0xe9 (see later explanation) and is therefore unable to convert it to a unicode code point. No code point found, no character printed. (5) python attempts to implicitly encode the Unicode string with whatever's in sys.stdout.encoding. Still "UTF-8". The resulting binary string is '\xc3\xa9'. Terminal receives the stream and attempts to decode 0xc3a9 also using UTF-8. It yields back code value 0xe9 (233), which on the Unicode character map points to the symbol "é". Terminal displays "é".(6) python encodes unicode string with latin-1, it yields a binary string with the same value '\xe9'. Again, for the terminal this is pretty much the same as case (4).Conclusions:

- Python outputs non-unicode strings as raw data, without considering its default encoding. The terminal just happens to display them if its current encoding matches the data. 

- Python outputs Unicode strings after encoding them using the scheme specified in sys.stdout.encoding. 

- Python gets that setting from the shell's environment.

- the terminal displays output according to its own encoding settings.

- the terminal's encoding is independant from the shell's.More details on unicode, UTF-8 and latin-1:Unicode is basically a table of characters where some keys (code points) have been conventionally assigned to point to some symbols. e.g. by convention it's been decided that key 0xe9 (233) is the value pointing to the symbol 'é'. ASCII and Unicode use the same code points from 0 to 127, as do latin-1 and Unicode from 0 to 255. That is, 0x41 points to 'A' in ASCII, latin-1 and Unicode, 0xc8 points to 'Ü' in latin-1 and Unicode, 0xe9 points to 'é' in latin-1 and Unicode. When working with electronic devices, Unicode code points need an efficient way to be represented electronically. That's what encoding schemes are about. Various Unicode encoding schemes exist (utf7, UTF-8, UTF-16, UTF-32). The most intuitive and straight forward encoding approach would be to simply use a code point's value in the Unicode map as its value for its electronic form, but Unicode currently has over a million code points, which means that some of them require 3 bytes to be expressed. To work efficiently with text, a 1 to 1 mapping would be rather impractical, since it would require that all code points be stored in exactly the same amount of space, with a minimum of 3 bytes per character, regardless of their actual need.Most encoding schemes have shortcomings regarding space requirement, the most economic ones don't cover all unicode code points, for example ascii only covers the first 128, while latin-1 covers the first 256. Others that try to be more comprehensive end up also being wasteful, since they require more bytes than necessary, even for common "cheap" characters. UTF-16 for instance, uses a minimum of 2 bytes per character, including those in the ascii range ('B' which is 65, still requires 2 bytes of storage in UTF-16). UTF-32 is even more wasteful as it stores all characters in 4 bytes. UTF-8 happens to have cleverly resolved the dilemma, with a scheme able to store code points with a variable amount of byte spaces. As part of its encoding strategy, UTF-8 laces code points with flag bits that indicate (presumably to decoders) their space requirements and their boundaries.UTF-8 encoding of unicode code points in the ascii range (0-127):e.g. Unicode code point for 'B' is '0x42' or 0100 0010 in binary (as we said, it's the same in ASCII). After encoding in UTF-8 it becomes:UTF-8 encoding of Unicode code points above 127 (non-ascii):e.g. 'é' Unicode code point is 0xe9 (233).When UTF-8 encodes this value, it determines that the value is larger than 127 and less than 2048, therefore should be encoded in 2 bytes:The 0xe9 Unicode code points after UTF-8 encoding becomes 0xc3a9. Which is exactly how the terminal receives it. If your terminal is set to decode strings using latin-1 (one of the non-unicode legacy encodings), you'll see Ã©, because it just so happens that 0xc3 in latin-1 points to Ã and 0xa9 to ©.When Unicode characters are printed to stdout, sys.stdout.encoding is used.  A non-Unicode character is assumed to be in sys.stdout.encoding and is just sent to the terminal.  On my system (Python 2):sys.getdefaultencoding() is only used when Python doesn't have another option.Note that Python 3.6 or later ignores encodings on Windows and uses Unicode APIs to write Unicode to the terminal.  No UnicodeEncodeError warnings and the correct character is displayed if the font supports it.  Even if the font doesn't support it the characters can still be cut-n-pasted from the terminal to an application with a supporting font and it will be correct.  Upgrade!The Python REPL tries to pick up what encoding to use from your environment. If it finds something sane then it all Just Works. It's when it can't figure out what's going on that it bugs out.You have specified an encoding by entering an explicit Unicode string.  Compare the results of not using the u prefix.In the case of \xe9 then Python assumes your default encoding (Ascii), thus printing ... something blank.It works for me:    As per Python default/implicit string encodings and conversions :

Specifying and saving a figure with exact size in pixels

Amelio Vazquez-Reina

[Specifying and saving a figure with exact size in pixels](https://stackoverflow.com/questions/13714454/specifying-and-saving-a-figure-with-exact-size-in-pixels)

Say I have an image of size 3841 x 7195 pixels. I would like to save the contents of the figure to disk, resulting in an image of the exact size I specify in pixels.No axis, no titles. Just the image. I don't personally care about DPIs, as I only want to specify the size the image takes in the screen in disk in pixels.I have read other threads, and they all seem to do conversions to inches and then specify the dimensions of the figure in inches and adjust dpi's in some way. I would like to avoid dealing with the potential loss of accuracy that could result from pixel-to-inches conversions.I have tried with:with no luck (Python complains that width and height must each be below 32768 (?))From everything I have seen, matplotlib requires the figure size to be specified in inches and dpi, but I am only interested in the pixels the figure takes in disk. How can I do this?To clarify: I am looking for a way to do this with matplotlib, and not with other image-saving libraries.

2012-12-05 00:34:49Z

Say I have an image of size 3841 x 7195 pixels. I would like to save the contents of the figure to disk, resulting in an image of the exact size I specify in pixels.No axis, no titles. Just the image. I don't personally care about DPIs, as I only want to specify the size the image takes in the screen in disk in pixels.I have read other threads, and they all seem to do conversions to inches and then specify the dimensions of the figure in inches and adjust dpi's in some way. I would like to avoid dealing with the potential loss of accuracy that could result from pixel-to-inches conversions.I have tried with:with no luck (Python complains that width and height must each be below 32768 (?))From everything I have seen, matplotlib requires the figure size to be specified in inches and dpi, but I am only interested in the pixels the figure takes in disk. How can I do this?To clarify: I am looking for a way to do this with matplotlib, and not with other image-saving libraries.Matplotlib doesn't work with pixels directly, but rather physical sizes and DPI. If you want to display a figure with a certain pixel size, you need to know the DPI of your monitor. For example this link will detect that for you.If you have an image of 3841x7195 pixels it is unlikely that you monitor will be that large, so you won't be able to show a figure of that size (matplotlib requires the figure to fit in the screen, if you ask for a size too large it will shrink to the screen size). Let's imagine you want an 800x800 pixel image just for an example. Here's how to show an 800x800 pixel image in my monitor (my_dpi=96):So you basically just divide the dimensions in inches by your DPI. If you want to save a figure of a specific size, then it is a different matter. Screen DPIs are not so important anymore (unless you ask for a figure that won't fit in the screen). Using the same example of the 800x800 pixel figure, we can save it in different resolutions using the dpi keyword of savefig. To save it in the same resolution as the screen just use the same dpi:To to save it as an 8000x8000 pixel image, use a dpi 10 times larger:Note that the setting of the DPI is not supported by all backends. Here, the PNG backend is used, but the pdf and ps backends will implement the size differently. Also, changing the DPI and sizes will also affect things like fontsize. A larger DPI will keep the same relative sizes of fonts and elements, but if you want smaller fonts for a larger figure you need to increase the physical size instead of the DPI. Getting back to your example, if you want to save a image with 3841 x 7195 pixels, you could do the following:Note that I used the figure dpi of 100 to fit in most screens, but saved with dpi=1000 to achieve the required resolution. In my system this produces a png with 3840x7190 pixels -- it seems that the DPI saved is always 0.02 pixels/inch smaller than the selected value, which will have a (small) effect on large image sizes. Some more discussion of this here.This worked for me, based on your code, generating a 93Mb png image with color noise and the desired dimensions:I am using the last PIP versions of the Python 2.7 libraries in Linux Mint 13.Hope that helps!Based on the accepted response by tiago, here is a small generic function that exports a numpy array to an image having the same resolution as the array:As said in the previous reply by tiago, the screen DPI needs to be found first, which can be done here for instance: http://dpi.lvI've added an additional argument resize_fact in the function which which you can export the image to 50% (0.5) of the original resolution, for instance.

When should I use ugettext_lazy?

Dzejkob

[When should I use ugettext_lazy?](https://stackoverflow.com/questions/4160770/when-should-i-use-ugettext-lazy)

I have a question about using ugettext and ugettext_lazy for translations.

I learned that in models I should use ugettext_lazy, while in views ugettext.

But are there any other places, where I should use ugettext_lazy too? What about form definitions?

Are there any performance diffrences between them?Edit:

And one more thing. Sometimes, instead of ugettext_lazy, ugettext_noop is used. As documentation says, ugettext_noop strings are only marked for translation and translated at the latest possible momment before displaying them to the user, but I'm little confused here, isn't that similar to what ugettext_lazy do? It's still hard for me to decide, which should I use in my models and forms.

2010-11-12 01:06:04Z

I have a question about using ugettext and ugettext_lazy for translations.

I learned that in models I should use ugettext_lazy, while in views ugettext.

But are there any other places, where I should use ugettext_lazy too? What about form definitions?

Are there any performance diffrences between them?Edit:

And one more thing. Sometimes, instead of ugettext_lazy, ugettext_noop is used. As documentation says, ugettext_noop strings are only marked for translation and translated at the latest possible momment before displaying them to the user, but I'm little confused here, isn't that similar to what ugettext_lazy do? It's still hard for me to decide, which should I use in my models and forms.In definitions like forms or models you should use ugettext_lazy because the code of this definitions is only executed once (mostly on django's startup); ugettext_lazy translates the strings in a lazy fashion, which means, eg. every time you access the name of an attribute on a model the string will be newly translated-which totally makes sense because you might be looking at this model in different languages since django was started!In views and similar function calls you can use ugettext without problems, because everytime the view is called ugettext will be newly executed, so you will always get the right translation fitting the request!As Bryce pointed out in his answer, this function marks a string as extractable for translation but does return the untranslated string. This is useful for using the string in two places – translated and untranslated. See the following example:An excellent use of _noop, is when you want to log a message in English for the developers, but present the translated string to a viewer.  An example of this is at http://blog.bessas.me/posts/using-gettext-in-django/The lazy version returns a proxy object instead of a string and in some situation it would not work as expected. For example:would fail because very last line would try serialize lst object into JSON and instead of a string for "client" it would have a proxy object. The proxy object is not serializeable into json.

What is choice_set in this Django app tutorial?

Peter Mortensen

[What is choice_set in this Django app tutorial?](https://stackoverflow.com/questions/2048777/what-is-choice-set-in-this-django-app-tutorial)

There is this line in the Django tutorial, Writing your first Django app, part 1:How is choice_set called into existence and what is it?I suppose the choice part is the lowercase version of the model Choice used in the tutorial, but what is choice_set? Can you elaborate?UPDATE: Based on Ben's answer, I located this documentation: Following relationships "backward".

2010-01-12 12:15:07Z

There is this line in the Django tutorial, Writing your first Django app, part 1:How is choice_set called into existence and what is it?I suppose the choice part is the lowercase version of the model Choice used in the tutorial, but what is choice_set? Can you elaborate?UPDATE: Based on Ben's answer, I located this documentation: Following relationships "backward".You created a foreign key on Choice which relates each one to a Question.So, each Choice explicitly has a question field, which you declared in the model.Django's ORM follows the relationship backwards from Question too, automatically generating a field on each instance called foo_set where Foo is the model with a ForeignKey field to that model.choice_set is a RelatedManager which can create querysets of Choice objects which relate to the Question instance, e.g. q.choice_set.all()If you don't like the foo_set naming which Django chooses automatically, or if you have more than one foreign key to the same model and need to distinguish them, you can choose your own overriding name using the related_name argument to ForeignKey.

TensorFlow, why was python the chosen language?

Ollegn

[TensorFlow, why was python the chosen language?](https://stackoverflow.com/questions/35677724/tensorflow-why-was-python-the-chosen-language)

I recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted?What are the advantages of using Python over a language like C++ for machine learning?

2016-02-28 01:40:08Z

I recently started studying deep learning and other ML techniques, and I started searching for frameworks that simplify the process of build a net and training it, then I found TensorFlow, having little experience in the field, for me, it seems that speed is a big factor for making a big ML system even more if working with deep learning, so why python was chosen by Google to make TensorFlow? Wouldn't it be better to make it over an language that can be compiled and not interpreted?What are the advantages of using Python over a language like C++ for machine learning?The most important thing to realize about TensorFlow is that, for the most part, the core is not written in Python:  It's written in a combination of highly-optimized C++ and CUDA (Nvidia's language for programming GPUs).  Much of that happens, in turn, by using Eigen (a high-performance C++ and CUDA numerical library) and NVidia's cuDNN (a very optimized DNN library for NVidia GPUs, for functions such as convolutions).The model for TensorFlow is that the programmer uses "some language" (most likely Python!) to express the model.  This model, written in the TensorFlow constructs such as:is not actually executed when the Python is run.  Instead, what's actually created is a dataflow graph that says to take particular inputs, apply particular operations, supply the results as the inputs to other operations, and so on.  This model is executed by fast C++ code, and for the most part, the data going between operations is never copied back to the Python code.Then the programmer "drives" the execution of this model by pulling on nodes -- for training, usually in Python, and for serving, sometimes in Python and sometimes in raw C++:This one Python (or C++ function call) uses either an in-process call to C++ or an RPC for the distributed version to call into the C++ TensorFlow server to tell it to execute, and then copies back the results.So, with that said, let's re-phrase the question:  Why did TensorFlow choose  Python as the first well-supported language for expressing and controlling the training of models?The answer to that is simple:  Python is probably the most comfortable language for a large range of data scientists and machine learning experts that's also that easy to integrate and have control a C++ backend, while also being general, widely-used both inside and outside of Google, and open source.  Given that with the basic model of TensorFlow, the performance of Python isn't that important, it was a natural fit.  It's also a huge plus that NumPy makes it easy to do pre-processing in Python -- also with high performance -- before feeding it in to TensorFlow for the truly CPU-heavy things.There's also a bunch of complexity in expressing the model that isn't used when executing it -- shape inference (e.g., if you do matmul(A, B), what is the shape of the resulting data?) and automatic gradient computation.  It turns out to have been nice to be able to express those in Python, though I think in the long term they'll probably move to the C++ backend to make adding other languages easier.(The hope, of course, is to support other languages in the future for creating and expressing models.  It's already quite straightforward to run inference using several other languages -- C++ works now, someone from Facebook contributed Go bindings that we're reviewing now, etc.)TF is not written in python. It is written in C++ (and uses high-performant numerical libraries and CUDA code) and you can check this by looking at their github. So the core is written not in python but TF provide an interface to many other languages (python, C++, Java, Go)If you come from a data analysis world, you can think about it like numpy (not written in python, but provides an interface to Python) or if you are a web-developer - think about it as a database (PostgreSQL, MySQL, which can be invoked from Java, Python, PHP)Python frontend (the language in which people write models in TF) is the most popular due to many reasons. In my opinion the main reason is historical: majority of ML users already use it (another popular choice is R) so if you will not provide an interface to python, your library is most probably doomed to obscurity.But being written in python does not mean that your model is executed in python. On the contrary, if you written your model in the right way Python is never executed during the evaluation of the TF graph (except of tf.py_func(), which exists for debugging and should be avoided in real model exactly because it is executed on Python's side).This is different from for example numpy. For example if you do np.linalg.eig(np.matmul(A, np.transpose(A)) (which is eig(AA')), the operation will compute transpose in some fast language (C++ or fortran), return it to python, take it from python together with A, and compute a multiplication in some fast language and return it to python, then compute eigenvalues and return it to python. So nonetheless expensive operations like matmul and eig are calculated efficiently, you still lose time by moving the results to python back and force. TF does not do it, once you defined the graph your tensors flow not in python but in C++/CUDA/something else.Python allows you to create extension modules using C and C++, interfacing with native code, and still getting the advantages that Python gives you.TensorFlow uses Python, yes, but it also contains large amounts of C++.This allows a simpler interface for experimentation with less human-thought overhead with Python, and add performance by programming the most important parts in C++.The latest ratio you can check from here shows inside TensorFlow C++ takes ~50% of code, and Python takes ~40% of code.Both C++ and Python are the official languages at Google so there is no wonder why this is so. If I would have to provide fast regression where C++ and Python are present... C++ is inside the computational algebra, and Python is used for everything else including for the testing. Knowing how ubiquitous the testing is today it is no wonder why Python code contributes that much to TF.

pandas get column average/mean

PepperoniPizza

[pandas get column average/mean](https://stackoverflow.com/questions/31037298/pandas-get-column-average-mean)

I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column weightThe following returns several values, not one:So does this:

2015-06-24 21:22:11Z

I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column weightThe following returns several values, not one:So does this:If you only want the mean of the weight column, select the column (which is a Series) and call .mean():Try df.mean(axis=0) , axis=0 argument calculates the column wise mean of the dataframe so the result will be axis=1 is row wise mean so you are getting multiple values.Do try to give print (df.describe()) a shot. I hope it will be very helpful to get an overall description of your dataframe.you can use    you will get basic statistics of the dataframe and to get mean of specific column you can use    You can also access a column using the dot notation (also called attribute access) and then calculate its mean:Mean for each column in  df :and if you want average of all columns:You can use either of the two statements below

pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available

Santosh Kumar G

[pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available](https://stackoverflow.com/questions/45954528/pip-is-configured-with-locations-that-require-tls-ssl-however-the-ssl-module-in)

I am using Python3.6, when I try to install "modules" using pip3, I am facing the below mentioned issue "pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available" 

2017-08-30 07:31:39Z

I am using Python3.6, when I try to install "modules" using pip3, I am facing the below mentioned issue "pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available" For Windows 10

if you want use pip in normal cmd, not only in Anaconda prompt. you need add 3 environment paths.

like the followings:most people only add D:\Anaconda3\Scripts;I had the same problem on Mac OS(Mojave) and solved the problem as mentioned on this link - Openssl issue.Update:

Keep in mind, that I had to use --ignore-dependencies flag, because  other packages installed that depend on OpenSSL. Additional if the problem is caused after using pyenv, you can fix it by using:For Debian users, the following may be of use:Then cd to the folder with the Python 3.X library source code and run:For future Oracle Linux users trying to solve this, below is what worked for me.

First install missing libs:Then cd to your Python3.X library and run: For centos 7:Install openssl:now goto python directory were we extracted the python tar,run below commandsThis will fix the problem in centos...For Windows 10,windows 7

If pip install is not working on CMD prompt, run it using Anaconda prompt - it works. https://github.com/pypa/virtualenv/issues/1139For OpenSUSE in the same manner, but a few changes of listed above packages:Then cd to Python sources dir andorAnd perhapsshould be executed for OpenSUSE users. See Python 3.7 install not working on openSUSE Leap 42.3Just try installing through Anaconda promptCurrently there is same issue in Anaconda prompt (Anaconda3) on Windows 10. Here is workaround: https://github.com/ContinuumIO/anaconda-issues/issues/10576Worked for me.Use this to enable ssl for pip.

Let me know if someone encounters issues.I ran into this issue with Visual Studio Code installing pylint from the VS Code prompt.

I was able to overcome the issue by opening the Anaconda installation directory and running Then VS Code was happy, but that did not fix the issue as running pretty much gave the same error so it seems that VS Code is unable to access the python modules.Note that VS Code picks up the first python env it see when installed, the bottom left of the screen indicates which env is being used. Clicking on that area allows to set the environment. So even if you ran the pip install for an environment VS Code could be looking at a different one.Best approach was to make sure that VS code had the correct python environment selected and that same environment is in the system PATH (under System Properties --> Advanced --> Environmental Variables)Under the Path Variable, Edit and browse to the specific Anaconda directory that you want VSCode to use and add to PATH, I needed to Add the following:Your Anaconda installation directory may differ.

One note is that Windows does not have the PATH variable take effect until you restart the terminal. In this case close and re-op VS code. If using a Terminal or PS Shell then close and reopen and check Path to make sure it is included.As Tokci said, it also works for Windows 7."Go with the mouse to the Windows Icon (lower left) and start typing "Anaconda". There should show up some matching entries. Select "Anaconda Prompt". A new command window, named "Anaconda Prompt" will open."Then pip works.The following also helped to import xgboost:

https://www.youtube.com/watch?v=05djBSOs1FAIf someone is using Arch Linux OS, I solved the TLS/SSL problem by running this:Then I could use pip to install the package I needed:Go to Anaconda prompt and type (if you have python 3.x installed on your engine) :i was having the same issue and this solved my problem. later after doing this you can import pymysql in power shell or any other prompt. Encountered this issue while installing python 3.8 from source on ubuntu. The steps needed to install it successfully alongside the default python 3.7 are summarised below :The install instruction for zlib1g-dev and build-essential is redundant, as ubuntu desktop already has these, but was necessary for some of Amazon's EC2 instances. python 3.8.0 is the current release just now, but should be replaced with the latest available.These instructions are best for keeping python 3.7 as the default for python3, and running python 3.8 in a virtual environment.I ran into this problem! I accidentally installed the 32-bit version of Miniconda3. Make sure you choose the 64 bit version!This worked for me:python version and package manager might differ.I tried:And After that, it works fine for me in Windows 10.This will install Ansible on macOS without uninstalling openssl, installing other packages, or doing anything else unsafe or "extra", simply use the python module for pip and Ansible will install.This is the problem with your default ssl setting. You gotta download the Python3 and add the path to your system. If you use Pycharm, set the interpreter to your python3 path. Then you can normally use the pip3 without error.Fixed this without having to change anything related to TSL/SSL. I was trying to see if the same thing was happening to pip, and saw that pip was broken. Did some digging and realized it's probably caused by Homebrew deleted python@2 on February 1st, 2020. Running brew uninstall python@2 to delete python2 installed by Homebrew.  Destroyed the virtual env created using python3 and created a new one. pip3 installing works fine again. 

In Python, how do I create a string of n characters in one line of code?

Thierry Lam

[In Python, how do I create a string of n characters in one line of code?](https://stackoverflow.com/questions/1424005/in-python-how-do-i-create-a-string-of-n-characters-in-one-line-of-code)

I need to generate a string with n characters in Python.  Is there a one line answer to achieve this with the existing Python library?  For instance, I need a string of 10 letters:

2009-09-14 21:26:10Z

I need to generate a string with n characters in Python.  Is there a one line answer to achieve this with the existing Python library?  For instance, I need a string of 10 letters:To simply repeat the same letter 10 times:And if you want something more complex, like n random lowercase letters, it's still only one line of code (not counting the import statements and defining n):The first ten lowercase letters are string.lowercase[:10] (if you have imported the standard library module string previously, of course;-).Other ways to "make a string of 10 characters": 'x'*10 (all the ten characters will be lowercase xs;-), ''.join(chr(ord('a')+i) for i in xrange(10)) (the first ten lowercase letters again), etc, etc;-).if you just want any letters:if you want consecutive letters (up to 26):Why "one line"? You can fit anything onto one line.Assuming you want them to start with 'a', and increment by one character each time (with wrapping > 26), here's a line:This might be a little off the question, but for those interested in the randomness of the generated string, my answer would be:See these answers and random.py's source for more insight.If you can use repeated letters, you can use the * operator:

Hash Map in Python

Kiran Bhat

[Hash Map in Python](https://stackoverflow.com/questions/8703496/hash-map-in-python)

I want to implement a HashMap in Python.  I want to ask a user for an input. depending on his input I am retrieving some information from the HashMap. If the user enters a key of the HashMap,  I would like to retrieve the corresponding value.How do I implement this functionality in Python?

2012-01-02 17:17:10Z

I want to implement a HashMap in Python.  I want to ask a user for an input. depending on his input I am retrieving some information from the HashMap. If the user enters a key of the HashMap,  I would like to retrieve the corresponding value.How do I implement this functionality in Python?Python dictionary is a built-in type that supports key-value pairs.as well as using the dict keyword:or:All you wanted (at the time the question was originally asked) was a hint. Here's a hint: In Python, you can use dictionaries.It's built-in for Python. See dictionaries.Based on your example:You could then access it like so:Also worth mentioning: it can use any non-mutable data type as a key. That is, it can use a tuple, boolean, or string as a key.And to retrieve values:OrThat's using number as keys, put quotes around the numbers to use strings as keys.Hash maps are built-in in Python, they're called dictionaries:Usage:See the documentation for more information, e.g. built-in methods and so on. They're great, and very common in Python programs (unsurprisingly).Here is the implementation of the Hash Map using python

For the simplicity hash map is of a fixed size 16.

This can be changed easily.

Rehashing is out of scope of this code.Output:Python Counter is also a good option in this case:This returns a dict with the count of each element in the list:In python you would use a dictionary.It is a very important type in python and often used.You can create one easily byDictionaries have many methods:You can not influence the order of a dict.

TypeError: 'NoneType' object is not iterable in Python

l--''''''---------''''''''''''

[TypeError: 'NoneType' object is not iterable in Python](https://stackoverflow.com/questions/3887381/typeerror-nonetype-object-is-not-iterable-in-python)

What does error TypeError: 'NoneType' object is not iterable mean?I am getting it on this Python code:

2010-10-08 02:56:26Z

What does error TypeError: 'NoneType' object is not iterable mean?I am getting it on this Python code:It means the value of data is None.In python2, NoneType is the type of None.  In Python3 NoneType is the class of None, for example:Guido says only use is to check for None because is is more robust to identity checking.  Don't use equality operations because those can spit bubble-up implementationitis of their own.  Python's Coding Style Guidelines - PEP-008Python's interpreter converted your code to pyc bytecode.  The Python virtual machine processed the bytecode, it encountered a looping construct which said iterate over a variable containing None.  The operation was performed by invoking the __iter__ method on the None. None has no __iter__ method defined, so Python's virtual machine tells you what it sees: that NoneType has no __iter__ method. This is why Python's duck-typing ideology is considered bad.  The programmer does something completely reasonable with a variable and at runtime it gets contaminated by None, the python virtual machine attempts to soldier on, and pukes up a bunch of unrelated nonsense all over the carpet.  Java or C++ doesn't have these problems because such a program wouldn't be allowed to compile since you haven't defined what to do when None occurs.  Python gives the programmer lots of rope to hang himself by allowing you to do lots of things that should cannot be expected to work under exceptional circumstances.  Python is a yes-man, saying yes-sir when it out to be stopping you from harming yourself, like Java and C++ does.Code: for row in data:

Error message: TypeError: 'NoneType' object is not iterableWhich object is it complaining about? Choice of two, row and data.

In for row in data, which needs to be iterable? Only data.What's the problem with data? Its type is NoneType. Only None has type NoneType. So data is None.You can verify this in an IDE, or by inserting e.g. print "data is", repr(data) before the for statement, and re-running.Think about what you need to do next: 

How should "no data" be represented? Do we write an empty file? Do we raise an exception or log a warning or keep silent?Another thing that can produce this error is when you are setting something equal to the return from a function, but forgot to actually return anything.Example:This is a little bit of a tough error to spot because the error can also be produced if the row variable happens to be None on one of the iterations. The way to spot it is that the trace fails on the last line and not inside the function.If your only returning one variable from a function, I am not sure if the error would be produced... I suspect error "'NoneType' object is not iterable in Python" in this case is actually implying "Hey, I'm trying to iterate over the return values to assign them to these three variables in order but I'm only getting None to iterate over"It means that the data variable is passing None (which is type NoneType), its equivalent for nothing. So it can't be iterable as a list, as you are trying to do.You're calling write_file with arguments like this:But you haven't defined 'foo' correctly, or you have a typo in your code so that it's creating a new empty variable and passing it in.For me it was a case of having my Groovy hat on instead of the Python 3 one.Forgot the return keyword at the end of a def function. Had not been coding Python 3 in earnest for a couple of months. Was thinking last statement evaluated in routine was being returned per the Groovy way.Took a few iterations, looking at the stack trace, inserting try: ... except TypeError: ... block debugging/stepping thru code to figure out what was wrong. The solution for the message certainly did not make the error jump out at me.

How to do parallel programming in Python?

ilovecp3

[How to do parallel programming in Python?](https://stackoverflow.com/questions/20548628/how-to-do-parallel-programming-in-python)

For C++, we can use OpenMP to do parallel programming; however, OpenMP will not work for Python. What should I do if I want to parallel some parts of my python program?The structure of the code may be considered as:Where solve1 and solve2 are two independent function. How to run this kind of code in parallel instead of in sequence in order to reduce the running time?

Hope someone can help me. Thanks very much in advance.

The code is:Where setinner and setouter are two independent functions. That's where I want to parallel...

2013-12-12 16:19:17Z

For C++, we can use OpenMP to do parallel programming; however, OpenMP will not work for Python. What should I do if I want to parallel some parts of my python program?The structure of the code may be considered as:Where solve1 and solve2 are two independent function. How to run this kind of code in parallel instead of in sequence in order to reduce the running time?

Hope someone can help me. Thanks very much in advance.

The code is:Where setinner and setouter are two independent functions. That's where I want to parallel...You can use the multiprocessing module. For this case I might use a processing pool:This will spawn processes that can do generic work for you. Since we did not pass processes, it will spawn one process for each CPU core on your machine. Each CPU core can execute one process simultaneously.If you want to map a list to a single function you would do this:Don't use threads because the GIL locks any operations on python objects. This can be done very elegantly with Ray.To parallelize your example, you'd need to define your functions with the @ray.remote decorator, and then invoke them with .remote.There are a number of advantages of this over the multiprocessing module.Note that Ray is a framework I've been helping develop.CPython uses the Global Interpreter Lock which makes parallel programing a bit more interesting than C++This topic has several useful examples and descriptions of the challenge:Python Global Interpreter Lock (GIL) workaround on multi-core systems using taskset on Linux?The solution, as others have said, is to use multiple processes. Which framework is more appropriate, however, depends on many factors. In addition to the ones already mentioned, there is also charm4py and mpi4py (I am the developer of charm4py).There is a more efficient way to implement the above example than using the worker pool abstraction. The main loop sends the same parameters (including the complete graph G) over and over to workers in each of the 1000 iterations. Since at least one worker will reside on a different process, this involves copying and sending the arguments to the other process(es). This could be very costly depending on the size of the objects. Instead, it makes sense to have workers store state and simply send the updated information.For example, in charm4py this can be done like this:Note that for this example we really only need one worker. The main loop could execute one of the functions, and have the worker execute the other. But my code helps to illustrate a couple of things:In some cases, it's possible to automatically parallelize loops using Numba, though it only works with a small subset of Python:Unfortunately, it seems that Numba only works with Numpy arrays, but not with other Python objects. In theory, it might also be possible to compile Python to C++ and then automatically parallelize it using the Intel C++ compiler, though I haven't tried this yet.

Python3 integer division [duplicate]

Megatron

[Python3 integer division [duplicate]](https://stackoverflow.com/questions/19507808/python3-integer-division)

In Python3 vs Python2.6, I've noticed that I can divide two integers and get a float. How do you get the Python2.6 behaviour back? Is there a different method to get int/int = int?

2013-10-22 02:01:33Z

In Python3 vs Python2.6, I've noticed that I can divide two integers and get a float. How do you get the Python2.6 behaviour back? Is there a different method to get int/int = int?Try this:

How are POST and GET variables handled in Python?

Click Upvote

[How are POST and GET variables handled in Python?](https://stackoverflow.com/questions/464040/how-are-post-and-get-variables-handled-in-python)

In PHP you can just use $_POST for POST and $_GET for GET (Query string) variables. What's the equivalent in Python?

2009-01-21 03:59:03Z

In PHP you can just use $_POST for POST and $_GET for GET (Query string) variables. What's the equivalent in Python?suppose you're posting a html form with this:If using raw cgi:If using Django, Pylons, Flask or Pyramid: Using Turbogears, Cherrypy:Web.py:Werkzeug:If using Cherrypy or Turbogears, you can also define your handler function taking a parameter directly:Google App Engine:So you really will have to choose one of those frameworks.I know this is an old question. Yet it's surprising that no good answer was given.First of all the question is completely valid without mentioning the framework. The CONTEXT is a PHP language equivalence. Although there are many ways to get the query string parameters in Python, the framework variables are just conveniently populated. In PHP, $_GET and $_POST are also convenience variables. They are parsed from QUERY_URI and php://input respectively.In Python, these functions would be os.getenv('QUERY_STRING') and sys.stdin.read(). Remember to import os and sys modules.We have to be careful with the word "CGI" here, especially when talking about two languages and their commonalities when interfacing with a web server. 1. CGI, as a protocol, defines the data transport mechanism in the HTTP protocol. 2. Python can be configured to run as a CGI-script in Apache. 3. The CGI module in Python offers some convenience functions.Since the HTTP protocol is language-independent, and that Apache's CGI extension is also language-independent, getting the GET and POST parameters should bear only syntax differences across languages.Here's the Python routine to populate a GET dictionary:and for POST:You can now access the fields as following:I must also point out that the CGI module doesn't work well. Consider this HTTP request:Using CGI.FieldStorage().getvalue('user_id') will cause a null pointer exception because the module blindly checks the POST data, ignoring the fact that a POST request can carry GET parameters too. I've found nosklo's answer very extensive and useful! For those, like myself, who might find accessing the raw request data directly also useful, I would like to add the way to do that:They are stored in the CGI fieldstorage object.It somewhat depends on what you use as a CGI framework, but they are available in dictionaries accessible to the program. I'd point you to the docs, but I'm not getting through to python.org right now.  But this note on mail.python.org will give you a first pointer.  Look at the CGI and URLLIB Python libs for more.UpdateOkay, that link busted.  Here's the basic wsgi refPython is only a language, to get GET and POST data, you need a web framework or toolkit written in Python.  Django is one, as Charlie points out, the cgi and urllib standard modules are others.  Also available are Turbogears, Pylons, CherryPy, web.py, mod_python, fastcgi, etc, etc.In Django, your view functions receive a request argument which has request.GET and request.POST.  Other frameworks will do it differently.

What are all possible pos tags of NLTK?

OrangeTux

[What are all possible pos tags of NLTK?](https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk)

How do I find a list with all possible pos tags used by the Natural Language Toolkit (nltk)?

2013-03-13 14:59:09Z

How do I find a list with all possible pos tags used by the Natural Language Toolkit (nltk)?The book has a note how to find help on tag sets, e.g.:Others are probably similar. (Note: Maybe you first have to download tagsets from the download helper's Models section for this)To save some folks some time, here is a list I extracted from a small corpus.  I do not know if it is complete, but it should have most (if not all) of the help definitions from upenn_tagset...CC: conjunction, coordinatingCD: numeral, cardinalDT: determinerEX: existential thereIN: preposition or conjunction, subordinatingJJ: adjective or numeral, ordinalJJR: adjective, comparativeJJS: adjective, superlativeLS: list item markerMD: modal auxiliaryNN: noun, common, singular or massNNP: noun, proper, singularNNS: noun, common, pluralPDT: pre-determinerPOS: genitive markerPRP: pronoun, personalPRP$: pronoun, possessiveRB: adverbRBR: adverb, comparativeRBS: adverb, superlativeRP: particleTO: "to" as preposition or infinitive markerUH: interjectionVB: verb, base formVBD: verb, past tenseVBG: verb, present participle or gerundVBN: verb, past participleVBP: verb, present tense, not 3rd person singularVBZ: verb, present tense, 3rd person singularWDT: WH-determinerWP: WH-pronounWRB: Wh-adverbThe tag set depends on the corpus that was used to train the tagger. 

The default tagger of nltk.pos_tag() uses the Penn Treebank Tag Set. In NLTK 2, you could check which tagger is the default tagger as follows: That means that it's a Maximum Entropy tagger trained on the Treebank corpus. nltk.tag._POS_TAGGER does not exist anymore in NLTK 3 but the documentation states that the off-the-shelf tagger still uses the Penn Treebank tagset. The below can be useful to access a dict keyed by abbreviations:The reference is available at the official siteCopy and pasting from there:You can download the list here: ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz. It includes confusing parts of speech, capitalization, and other conventions. Also, wikipedia has an interesting section similar to this. Section: Part-of-speech tags used. Just run this verbatim. nltk.tag._POS_TAGGER won't work. It will give AttributeError: module 'nltk.tag' has no attribute '_POS_TAGGER'. It's not available in NLTK 3 anymore. ['LS', 'TO', 'VBN', "''", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']Based on Doug Shore's method but make it more copy-paste friendly 

How do I suppress scientific notation in Python?

Matt

[How do I suppress scientific notation in Python?](https://stackoverflow.com/questions/658763/how-do-i-suppress-scientific-notation-in-python)

Here's my code:My quotient displays as 1.00000e-05Is there any way to suppress scientific notation and make it display as

0.00001? I'm going to use the result as a string.

2009-03-18 15:27:21Z

Here's my code:My quotient displays as 1.00000e-05Is there any way to suppress scientific notation and make it display as

0.00001? I'm going to use the result as a string.but you need to manage precision yourself. e.g.,will display zeros only.

details are in the docsOr for Python 3 the equivalent old formatting or the newer style formattingUsing the newer version ''.format (also remember to specify how many digit after the . you wish to display, this depends on how small is the floating number). See this example:as shown above, default is 6 digits! This is not helpful for our case example, so instead we could use something like this:Starting in Python 3.6, this can be simplified with the new formatted string literal, as follows:With newer versions of Python (2.6 and later), you can use ''.format() to accomplish what @SilentGhost suggested:Another option, if you are using pandas and would like to suppress scientific notation for all floats, is to adjust the pandas options.Most of the answers above require you to specify precision. But what if you want to display floats like this, with no unnecessary zeros: numpy has an answer: np.format_float_positionalThis will work for any exponent:This is using Captain Cucumber's answer, but with 2 additions.1) allowing the function to get non scientific notation numbers and just return them as is (so you can throw a lot of input that some of the numbers are 0.00003123 vs  3.123e-05 and still have function work.2) added support for negative numbers. (in original function,  a negative number would end up like 0.0000-108904 from -1.08904e-05)In addition to SG's answer, you can also use the Decimal module:Since this is the top result on Google, I will post here after failing to find a solution for my problem. If you are looking to format the display value of a float object and have it remain a float - not a string, you can use this solution:Create a new class that modifies the way that float values are displayed.You can modify the precision yourself by changing the integer values in {:f}If it is a string then use the built in float on it to do the conversion for instance:

print( "%.5f" % float("1.43572e-03"))

answer:0.00143572Using 3.6.4, I was having a similar problem that randomly, a number in the output file would be formatted with scientific notation when using this:All that I had to do to fix it was to add 'f':As of 3.6 (probably works with slightly older 3.x as well), this is my solution:The purpose of the precision calculation is to ensure we have enough precision to keep out of scientific notation (default precision is still 6).The dec_precision argument adds additional precision to use for decimal points. Since this makes use of the n format, no insignificant zeros will be added (unlike f formats). n also will take care of rendering already-round integers without a decimal.n does require float input, thus the cast.

Is there a「do … until」in Python? [duplicate]

Matt Joiner

[Is there a「do … until」in Python? [duplicate]](https://stackoverflow.com/questions/1662161/is-there-a-do-until-in-python)

Is there ain Python, or a nice way to implement such a looping construct?

2009-11-02 16:04:26Z

Is there ain Python, or a nice way to implement such a looping construct?There is no do-while loop in Python.This is a similar construct, taken from the link above.There's no prepackaged "do-while", but the general Python way to implement peculiar looping constructs is through generators and other iterators, e.g.:so, for example:executes one leg, as desired, even though the predicate's already false at the start.It's normally better to encapsulate more of the looping logic into your generator (or other iterator) -- for example, if you often have cases where one variable increases, one decreases, and you need a do/while loop comparing them, you could code:which you can use like:It's up to you how much loop-related logic you want to put inside your generator (or other iterator) and how much you want to have outside of it (just like for any other use of a function, class, or other mechanism you can use to refactor code out of your main stream of execution), but, generally speaking, I like to see the generator used in a for loop that has little (ideally none) "loop control logic" (code related to updating state variables for the next loop leg and/or making tests about whether you should be looping again or not).I prefer to use a looping variable, as it tends to read a bit nicer than just "while 1:", and no ugly-looking break statement:No there isn't. Instead use a while loop such as:

Max retries exceeded with URL in requests

user3446000

[Max retries exceeded with URL in requests](https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url-in-requests)

I'm trying to get the content of App Store > Business:When I try the range with (0,2) it works, but when I put the range in 100s it shows this error:

2014-04-11 12:56:36Z

I'm trying to get the content of App Store > Business:When I try the range with (0,2) it works, but when I put the range in 100s it shows this error:What happened here is that itunes server refuses your connection (you're sending too many requests from same ip address in short period of time) error trace is misleading it should be something like "No connection could be made because the target machine actively refused it".There is an issue at about python.requests lib at Github, check it out hereTo overcome this issue (not so much an issue as it is misleading debug trace) you should catch connection related exceptions like so:Another way to overcome this problem is if you use enough time gap to send requests to server this can be achieved by sleep(timeinsec) function in python (don't forget to import sleep)All in all requests is awesome python lib, hope that solves your problem.Just use requests' features:This will GET the URL and retry 3 times in case of requests.exceptions.ConnectionError. backoff_factor will help to apply delays between attempts to avoid to fail again in case of periodic request quota.Take a look at requests.packages.urllib3.util.retry.Retry, it has many options to simplify retries.Just do this,Paste the following code in place of page = requests.get(url):You're welcome :)pip install pyopenssl seemed to solve it for me.https://github.com/requests/requests/issues/4246I got similar problem but the following code worked for me."verify=False" disables SSL verification. Try and catch can be added as usual.It is always good to implement exception handling. It does not only help to avoid unexpected exit of script but can also help to log errors and info notification. When using Python requests I prefer to catch exceptions like this:Here renewIPadress() is a user define function which can change the IP address if it get blocked. You can go without this function.Specifying the proxy in a corporate environment solved it for me.The full error is:i wasn't able to make it work on windows even after installing pyopenssl and trying various python versions (while it worked fine on mac), so i switched to urllib and it works on python 3.6 (from python .org) and 3.7 (anaconda)When I was writing a selenium browser test script, I encountered this error when calling driver.quit() before a usage of a JS api call.Remember that quiting webdriver is last thing to do!Adding my own experience for those who are experiencing this in the future. My specific error wasIt turns out that this was actually because I had reach the maximum number of open files on my system. It had nothing to do with failed connections, or even a DNS error as indicated.Add headers for this request.

Changing default encoding of Python?

Ali Nadalizadeh

[Changing default encoding of Python?](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python)

I have many "can't encode" and "can't decode" problems with Python when I run my applications from the console. But in the Eclipse PyDev IDE, the default character encoding is set to UTF-8, and I'm fine.I searched around for setting the default encoding, and people say that Python deletes the sys.setdefaultencoding function on startup, and we can not use it.So what's the best solution for it?

2010-02-16 20:46:28Z

I have many "can't encode" and "can't decode" problems with Python when I run my applications from the console. But in the Eclipse PyDev IDE, the default character encoding is set to UTF-8, and I'm fine.I searched around for setting the default encoding, and people say that Python deletes the sys.setdefaultencoding function on startup, and we can not use it.So what's the best solution for it?Here is a simpler method (hack) that gives you back the setdefaultencoding() function that was deleted from sys:(Note for Python 3.4+: reload() is in the importlib library.)This is not a safe thing to do, though: this is obviously a hack, since sys.setdefaultencoding() is purposely removed from sys when Python starts. Reenabling it and changing the default encoding can break code that relies on ASCII being the default (this code can be third-party, which would generally make fixing it impossible or dangerous).If you get this error when you try to pipe/redirect output of your scriptUnicodeEncodeError: 'ascii' codec can't encode characters in position 0-5: ordinal not in range(128)Just export PYTHONIOENCODING in console and then run your code.export PYTHONIOENCODING=utf8A) To control sys.getdefaultencoding() output:asciiThenand utf-16-beYou could put your sitecustomize.py higher in your PYTHONPATH.Also  you might like to try reload(sys).setdefaultencoding by @EOL B) To control stdin.encoding and stdout.encoding you want to set PYTHONIOENCODING:ascii asciiThen utf-16-be utf-16-beFinally: you can use A) or B) or both!Starting with PyDev 3.4.1, the default encoding is not being changed anymore. 

See this ticket for details.For earlier versions a solution is to make sure PyDev does not run with UTF-8 as the default encoding. Under Eclipse, run dialog settings ("run configurations", if I remember correctly); you can choose the default encoding on the common tab. Change it to US-ASCII if you want to have these errors 'early' (in other words: in your PyDev environment). Also see an original blog post for this workaround.Regarding python2 (and python2 only), some of the former answers rely on using the following hack:It is discouraged to use it (check this or this)In my case, it come with a side-effect: I'm using ipython notebooks, and once I run the code the ´print´ function no longer works. I guess there would be solution to it, but still I think using the hack should not be the correct option.After trying many options, the one that worked for me was using the same code in the sitecustomize.py, where that piece of code is meant to be. After evaluating that module, the setdefaultencoding function is removed from sys.So the solution is to append to file /usr/lib/python2.7/sitecustomize.py the code:When I use virtualenvwrapper the file I edit is ~/.virtualenvs/venv-name/lib/python2.7/sitecustomize.py.And when I use with python notebooks and conda, it is ~/anaconda2/lib/python2.7/sitecustomize.pyThere is an insightful blog post about it.See https://anonbadger.wordpress.com/2015/06/16/why-sys-setdefaultencoding-will-break-code/.I paraphrase its content below.In python 2 which was not as strongly typed regarding the encoding of strings you could perform operations on differently encoded strings, and succeed. E.g. the following would return True.That would hold for every (normal, unprefixed) string that was encoded in sys.getdefaultencoding(), which defaulted to ascii, but not others.The default encoding was meant to be changed system-wide in site.py, but not somewhere else. The hacks (also presented here) to set it in user modules were just that: hacks, not the solution.Python 3 did changed the system encoding to default to utf-8 (when LC_CTYPE is unicode-aware), but the fundamental problem was solved with the requirement to explicitly encode "byte"strings whenever they are used with unicode strings.First: reload(sys) and setting some random default encoding just regarding the need of an output terminal stream is bad practice. reload often changes things in sys which have been put in place depending on the environment - e.g.  sys.stdin/stdout streams, sys.excepthook, etc. The best solution I know for solving the encode problem of print'ing unicode strings and beyond-ascii str's (e.g. from literals) on sys.stdout is: to take care of a sys.stdout (file-like object) which is capable and optionally tolerant regarding the needs:Here an example:The only good reason to change the global default encoding (to UTF-8 only) I think is regarding an application source code decision - and not because of I/O stream encodings issues: For writing beyond-ascii string literals into code without being forced to always use u'string' style unicode escaping. This can be done rather consistently (despite what anonbadger's article says) by taking care of a Python 2 or Python 2 + 3 source code basis which uses ascii or UTF-8 plain string literals consistently - as far as those strings potentially undergo silent unicode conversion and move between modules or potentially go to stdout. For that, prefer "# encoding: utf-8" or ascii (no declaration). Change or drop libraries which still rely in a very dumb way fatally on ascii default encoding errors beyond chr #127 (which is rare today).And do like this at application start (and/or via sitecustomize.py) in addition to the SmartStdout scheme above - without using reload(sys):This way string literals and most operations (except character iteration) work comfortable without thinking about unicode conversion as if there would be Python3 only. 

File I/O of course always need special care regarding encodings - as it is in Python3.Note: plains strings then are implicitely converted from utf-8 to unicode in SmartStdout before being converted to the output stream enconding.Here is the approach I used to produce code that was compatible with both python2 and python3 and always produced utf8 output.  I found this answer elsewhere, but I can't remember the source.This approach works by replacing sys.stdout with something that isn't quite file-like (but still only using things in the standard library). This may well cause problems for your underlying libraries, but in the simple case where you have good control over how sys.stdout out is used through your framework this can be a reasonable approach.This fixed the issue for me.This is a quick hack for anyone who is (1) On a Windows platform (2) running Python 2.7 and (3) annoyed because a nice piece of software (i.e., not written by you so not immediately a candidate for encode/decode printing maneuvers) won't display the "pretty unicode characters" in the IDLE environment (Pythonwin prints unicode fine), For example, the neat First Order Logic symbols that Stephan Boyer uses in the output from his pedagogic prover at First Order Logic Prover.I didn't like the idea of forcing a sys reload and I couldn't get the system to cooperate with setting environment variables like PYTHONIOENCODING (tried direct Windows environment variable and also dropping that in a sitecustomize.py in site-packages as a one liner ='utf-8').So, if you are willing to hack your way to success, go to your IDLE directory, typically:

"C:\Python27\Lib\idlelib"

Locate the file IOBinding.py. Make a copy of that file and store it somewhere else so you can revert to original behavior when you choose. Open the file in the idlelib with an editor (e.g., IDLE). Go to this code area:In other words, comment out the original code line following the 'try' that was making the encoding variable equal to locale.getdefaultlocale (because that will give you cp1252 which you don't want) and instead brute force it to 'utf-8' (by adding the line 'encoding = 'utf-8' as shown). I believe this only affects IDLE display to stdout and not the encoding used for file names etc. (that is obtained in the filesystemencoding prior). If you have a problem with any other code you run in IDLE later, just replace the IOBinding.py file with the original unmodified file.You could change the encoding of your entire operating system. On Ubuntu you can do this with

Python string prints as [u'String']

gnuchu

[Python string prints as [u'String']](https://stackoverflow.com/questions/599625/python-string-prints-as-ustring)

This will surely be an easy one but it is really bugging me. I have a script that reads in a webpage and uses Beautiful Soup to parse it. From the soup I extract all the links as my final goal is to print out the link.contents.All of the text that I am parsing is ASCII. I know that Python treats strings as unicode, and I am sure this is very handy, just of no use in my wee script. Every time I go to print out a variable that holds 'String' I get [u'String'] printed to the screen. Is there a simple way of getting this back into just ascii or should I write a regex to strip it?

2009-03-01 10:48:45Z

This will surely be an easy one but it is really bugging me. I have a script that reads in a webpage and uses Beautiful Soup to parse it. From the soup I extract all the links as my final goal is to print out the link.contents.All of the text that I am parsing is ASCII. I know that Python treats strings as unicode, and I am sure this is very handy, just of no use in my wee script. Every time I go to print out a variable that holds 'String' I get [u'String'] printed to the screen. Is there a simple way of getting this back into just ascii or should I write a regex to strip it?[u'ABC'] would be a one-element list of unicode strings. Beautiful Soup always produces Unicode. So you need to convert the list to a single unicode string, and then convert that to ASCII.I don't know exaxtly how you got the one-element lists; the contents member would be a list of strings and tags, which is apparently not what you have. Assuming that you really always get a list with a single element, and that your test is really only ASCII you would use this:However, please double-check that your data is really ASCII. This is pretty rare. Much more likely it's latin-1 or utf-8.Or you ask Beautiful Soup what the original encoding was and get it back in this encoding: You probably have a list containing one unicode string. The repr of this is [u'String'].You can convert this to a list of byte strings using any variation of the following:will print If accessing/printing single element lists (e.g., sequentially or filtered):pass the output to str() function and it will remove the convert the unicode output.

also by printing the output it will remove the u'' tags from it. Use dir or type on the 'string' to find out what it is. I suspect that it's one of BeautifulSoup's tag objects, that prints like a string, but really isn't one. Otherwise, its inside a list and you need to convert each string separately.In any case, why are you objecting to using Unicode? Any specific reason?Do you really mean u'String'? In any event, can't you just do str(string) to get a string rather than a unicode-string? (This should be different for Python 3, for which all strings are unicode.)[u'String'] is a text representation of a list that contains a Unicode string on Python 2.If you run print(some_list) then it is equivalent to

print'[%s]' % ', '.join(map(repr, some_list)) i.e., to create a text representation of a Python object with the type list, repr() function is called for each item.Don't confuse a Python object and its text representation—repr('a') != 'a' and even the text representation of the text representation differs: repr(repr('a')) != repr('a').repr(obj) returns a string that contains a printable representation of an object. Its purpose is to be an unambiguous representation of an object that can be useful for debugging, in a REPL. Often eval(repr(obj)) == obj.To avoid calling repr(), you could print list items directly (if they are all Unicode strings) e.g.: print ",".join(some_list)—it prints a comma separated list of the strings: StringDo not encode a Unicode string to bytes using a hardcoded character encoding, print Unicode directly instead. Otherwise, the code may fail because the encoding can't represent all the characters e.g., if you try to use 'ascii' encoding with non-ascii characters. Or the code silently produces mojibake (corrupted data is passed further in a pipeline) if the environment uses an encoding that is incompatible with the hardcoded encoding.encode("latin-1") helped me in my case:Maybe i dont understand , why cant you just get the element.text and then convert it before using it ?

for instance (dont know why you would do this but...)

find all label elements of the web page and iterate between them until you find one called MyTextConvert the string from i and do whatever you wanted to do ... 

maybe im missing something in the original message ? 

or was this what you were looking for ?

Python, Unicode, and the Windows console

James Sulak

[Python, Unicode, and the Windows console](https://stackoverflow.com/questions/5419/python-unicode-and-the-windows-console)

When I try to print a Unicode string in a Windows console, I get a UnicodeEncodeError: 'charmap' codec can't encode character .... error.  I assume this is because the Windows console does not accept Unicode-only characters. What's the best way around this? Is there any way I can make Python automatically print a ? instead of failing in this situation?Edit:  I'm using Python 2.5.Note: @LasseV.Karlsen answer with the checkmark is sort of outdated (from 2008). Please use the solutions/answers/suggestions below with care!!@JFSebastian answer is more relevant as of today (6 Jan 2016).

2008-08-07 22:26:58Z

When I try to print a Unicode string in a Windows console, I get a UnicodeEncodeError: 'charmap' codec can't encode character .... error.  I assume this is because the Windows console does not accept Unicode-only characters. What's the best way around this? Is there any way I can make Python automatically print a ? instead of failing in this situation?Edit:  I'm using Python 2.5.Note: @LasseV.Karlsen answer with the checkmark is sort of outdated (from 2008). Please use the solutions/answers/suggestions below with care!!@JFSebastian answer is more relevant as of today (6 Jan 2016).Note: This answer is sort of outdated (from 2008). Please use the solution below with care!!Here is a page that details the problem and a solution (search the page for the text Wrapping sys.stdout into an instance):PrintFails - Python WikiHere's a code excerpt from that page:There's some more information on that page, well worth a read.Update: Python 3.6 implements PEP 528: Change Windows console encoding to UTF-8: the default console on Windows will now accept all Unicode characters. Internally, it uses the same Unicode API as the win-unicode-console package mentioned below. print(unicode_string) should just work now.The error means that Unicode characters that you are trying to print can't be represented using the current (chcp) console character encoding. The codepage is often 8-bit encoding such as cp437 that can represent only ~0x100 characters from ~1M Unicode characters:Windows console does accept Unicode characters and it can even display them (BMP only) if the corresponding font is configured. WriteConsoleW() API should be used as suggested in @Daira Hopwood's answer. It can be called transparently i.e., you don't need to and should not modify your scripts if you use win-unicode-console package:See What's the deal with Python 3.4, Unicode, different languages and Windows?If it is enough to replace all unencodable characters with ? in your case then you could set PYTHONIOENCODING envvar:In Python 3.6+, the encoding specified by PYTHONIOENCODING envvar is ignored for interactive console buffers unless PYTHONLEGACYWINDOWSIOENCODING envvar is set to a non-empty string. Despite the other plausible-sounding answers that suggest changing the code page to 65001, that does not work. (Also, changing the default encoding using sys.setdefaultencoding is not a good idea.)See this question for details and code that does work.If you're not interested in getting a reliable representation of the bad character(s) you might use something like this (working with python >= 2.6, including 3.x):The bad character(s) in the string will be converted in a representation which is printable by the Windows console.The below code will make Python output to console as UTF-8 even on Windows. The console will display the characters well on Windows 7 but on Windows XP it will not display them well, but at least it will work and most important you will have a consistent output from your script on all platforms. You'll be able to redirect the output to a file.Below code was tested with Python 2.6 on Windows.Like Giampaolo Rodolà's answer, but even more dirty: I really, really intend to spend a long time (soon) understanding the whole subject of encodings and how they apply to Windoze consoles, For the moment I just wanted sthg which would mean my program would NOT CRASH, and which I understood ... and also which didn't involve importing too many exotic modules (in particular I'm using Jython, so half the time a Python module turns out not in fact to be available).NB "pr" is shorter to type than "print" (and quite a bit shorter to type than "safeprint")...!Just enter this code in command line before executing python script:For Python 2 try:For Python 3 try:Or try win-unicode-console:The cause of your problem is NOT the Win console not willing to accept Unicode (as it does this since I guess Win2k by default). It is the default system encoding. Try this code and see what it gives you:if it says ascii, there's your cause ;-)

You have to create a file called sitecustomize.py and put it under python path (I put it under /usr/lib/python2.5/site-packages, but that is differen on Win - it is c:\python\lib\site-packages or something), with the following contents:and perhaps you might want to specify the encoding in your files as well:Edit: more info can be found in excellent the Dive into Python bookTL;DR:I ran into this myself, working on a Twitch chat (IRC) bot. (Python 2.7 latest)I wanted to parse chat messages in order to respond...but also print them safely to the console in a human-readable format:This corrected the issue of the bot throwing UnicodeEncodeError: 'charmap' errors and replaced the unicode characters with ?.Kind of related on the answer by J. F. Sebastian, but more direct.If you are having this problem when printing to the console/terminal, then do this:Python 3.6 windows7: There is several way to launch a python you could use the python console (which has a python logo on it) or the windows console (it's written cmd.exe on it). I could not print utf8 characters in the windows console. Printing utf-8 characters throw me this error:After trying and failing to understand the answer above I discovered it was only a setting problem. Right click on the top of the cmd console windows, on the tab font chose lucida console.James Sulak asked,Other solutions recommend we attempt to modify the Windows environment or replace Python's print() function.  The answer below comes closer to fulfilling Sulak's request.Under Windows 7, Python 3.5 can be made to print Unicode without throwing a UnicodeEncodeError as follows:    In place of:

   print(text)

    substitute:

    print(str(text).encode('utf-8'))Instead of throwing an exception, Python now displays unprintable Unicode characters as \xNN hex codes, e.g.:  Halmalo n\xe2\x80\x99\xc3\xa9tait plus qu\xe2\x80\x99un point noirInstead of  Halmalo n’était plus qu’un point noirGranted, the latter is preferable ceteris paribus, but otherwise the former is completely accurate for diagnostic messages.  Because it displays Unicode as literal byte values the former may also assist in diagnosing encode/decode problems.Note: The str() call above is needed because otherwise encode() causes Python to reject a Unicode character as a tuple of numbers.

How to mock an import

Jonathan

[How to mock an import](https://stackoverflow.com/questions/8658043/how-to-mock-an-import)

Module A includes import B at its top. However under test conditions I'd like to mock B in A (mock A.B) and completely refrain from importing B. In fact, B isn't installed in the test environment on purpose.A is the unit under test. I have to import A with all its functionality. B is the module I need to mock. But how can I mock B within A and stop A from importing the real B, if the first thing A does is import B?(The reason B isn't installed is that I use pypy for quick testing and unfortunately B isn't compatible with pypy yet.)How could this be done?

2011-12-28 15:51:32Z

Module A includes import B at its top. However under test conditions I'd like to mock B in A (mock A.B) and completely refrain from importing B. In fact, B isn't installed in the test environment on purpose.A is the unit under test. I have to import A with all its functionality. B is the module I need to mock. But how can I mock B within A and stop A from importing the real B, if the first thing A does is import B?(The reason B isn't installed is that I use pypy for quick testing and unfortunately B isn't compatible with pypy yet.)How could this be done?You can assign to sys.modules['B'] before importing A to get what you want:test.py:A.py:Note B.py does not exist, but when running test.py no error is returned and print(A.B.__name__) prints mock_B. You still have to create a mock_B.py where you mock B's actual functions/variables/etc. Or you can just assign a Mock() directly:test.py:The builtin __import__ can be mocked with the 'mock' library for more control:Say A looks like:A.a() returns b_mock.func() which can be mocked also.Note for Python 3:

As stated in the changelog for 3.0, __builtin__ is now named builtins:The code in this answer works fine if you replace __builtin__ by builtins for Python 3.Easy, just mock the library in sys.modules before it gets imported:and then, so long as A doesn't rely on specific types of data being returned from B's objects:should just work.This works even if you have submodules, but you'll want to mock each module. Say you have this:To mock, simply do the below before the module that contains the above is imported: (My experience: I had a dependency that works on one platform, Windows, but didn't work on Linux, where we run our daily tests. 

So I needed to mock the dependency for our tests. Luckily it was a black box, so I didn't need to set up a lot of interaction.)Addendum: Actually, I needed to simulate a side-effect that took some time. So I needed an object's method to sleep for a second. That would work like this:And then the code takes some time to run, just like the real method.I realize I'm a bit late to the party here, but here's a somewhat insane way to automate this with the mock library:(here's an example usage)The reason this is so ridiculously complicated is when an import occurs python basically does this (take for example from herp.derp import foo)There are some downsides to this hacked together solution: If something else relies on other stuff in the module path this kind of screws it over.  Also this only works for stuff that is being imported inline such asorI found fine way to mock the imports in Python. It's Eric's Zaadi solution found here which I just use inside my Django application.I've got class SeatInterface which is interface to Seat model class.

So inside my seat_interface module I have such an import:I wanted to create isolated tests for SeatInterface class with mocked Seat class as FakeSeat. The problem was - how tu run tests offline, where Django application is down. I had below error:The solution was:And then test magically runs OK :)If you do an import ModuleB you are really calling the builtin method __import__ as:You could overwrite this method by importing the __builtin__ module and make a wrapper around the __builtin__.__import__method. Or you could play with the NullImporter hook from the imp module. Catching the exception and Mock your module/class in the except-block.Pointer to the relevant docs:docs.python.org: __import__Accessing Import internals with the imp ModuleI hope this helps. Be HIGHLY adviced that you step into the more arcane perimeters of python programming and that a) solid understanding what you really want to achieve and b)thorough understanding of the implications is important.Aaron Hall's answer works for me.

Just want to mention one important thing,if in A.py you dofrom B.C.D import Ethen in test.py you must mock every module along the path, otherwise you get ImportError

Why does the expression 0 < 0 == 0 return False in Python?

Marcelo Santos

[Why does the expression 0 < 0 == 0 return False in Python?](https://stackoverflow.com/questions/6074018/why-does-the-expression-0-0-0-return-false-in-python)

Looking into Queue.py in Python 2.6, I found this construct that I found a bit strange:If maxsize is 0 the queue is never full.My question is how does it work for this case? How 0 < 0 == 0 is considered False?

2011-05-20 15:19:08Z

Looking into Queue.py in Python 2.6, I found this construct that I found a bit strange:If maxsize is 0 the queue is never full.My question is how does it work for this case? How 0 < 0 == 0 is considered False?I believe Python has special case handling for sequences of relational operators to make range comparisons easy to express. It's much nicer to be able to say 0 < x <= 5 than to say (0 < x) and (x <= 5).These are called chained comparisons. And that's a link to the documentation for them.With the other cases you talk about, the parenthesis force one relational operator to be applied before the other, and so they are no longer chained comparisons. And since True and False have values as integers you get the answers you do out of the parenthesized versions.Becauseis False. You can chain together comparison operators and they are automatically expanded out into the pairwise comparisons.EDIT -- clarification about True and False in PythonIn Python True and False are just instances of bool, which is a subclass of int. In other words, True really is just 1.The point of this is that you can use the result of a boolean comparison exactly like an integer. This leads to confusing things likeBut these will only happen if you parenthesise the comparisons so that they are evaluated first. Otherwise Python will expand out the comparison operators.The strange behavior your experiencing comes from pythons ability to chain conditions. Since it finds 0 is not less than 0, it decides the entire expression evaluates to false. As soon as you break this apart into seperate conditions, you're changing the functionality. It initially is essentially testing that a < b && b == c for your original statement of a < b == c.Another example:This is a chained comparison. It returns true if each pairwise comparison in turn is true. It is the equivalent to (0 < 0) and (0 == 0)This is equivalent to 0 < True which evaluates to True.This is equivalent to False == 0 which evaluates to True.Equivalent to 0 < True which, as above, evaluates to True.Looking at the disassembly (the bytes codes) it is obvious why 0 < 0 == 0 is False.Here is an analysis of this expression:Notice lines 0-8: These lines check if 0 < 0 which obviously returns False onto the python stack.Now notice line 11: JUMP_IF_FALSE_OR_POP 23

This means that if 0 < 0 returns False perform a jump to line 23.Now, 0 < 0 is False, so the jump is taken, which leaves the stack with a False which is the return value for the whole expression 0 < 0 == 0, even though the == 0 part isn't even checked.So, to conclude, the answer is like said in other answers to this question.

0 < 0 == 0 has a special meaning. The compiler evaluates this to two terms: 0 < 0 and 0 == 0. As with any complex boolean expressions with and between them, if the first fails then the second one isn't even checked.Hopes this enlightens things up a bit, and I really hope that the method I used to analyse this unexpected behavior will encourage others to try the same in the future.As other's mentioned x comparison_operator y comparison_operator z is syntactical sugar for (x comparison_operator y) and (y comparison_operator z) with the bonus that y is only evaluated once.So your expression 0 < 0 == 0 is really (0 < 0) and (0 == 0), which evaluates to False and True which is just False.maybe this excerpt from the docs can help:These were comparisons but since you are chaining comparisons you should know that:Here it is, in all its glory.I'm thinking Python is doing it's weird between magic. Same as 1 < 2 < 3 means 2 is between 1 and 3.In this case, I think it's doing [middle 0] is greater than [left 0] and equal to [right 0]. Middle 0 is not greater than left 0, so it evaluates to false.

What's the difference between a Python「property」and「attribute」?

Carson

[What's the difference between a Python「property」and「attribute」?](https://stackoverflow.com/questions/7374748/whats-the-difference-between-a-python-property-and-attribute)

I am generally confused about the difference between a "property" and an "attribute", and can't find a great resource to concisely detail the differences. 

2011-09-10 21:15:34Z

I am generally confused about the difference between a "property" and an "attribute", and can't find a great resource to concisely detail the differences. Properties are a special kind of attribute.  Basically, when Python encounters the following code:it looks up eggs in spam, and then examines eggs to see if it has a __get__, __set__, or __delete__ method — if it does, it's a property.  If it is a property, instead of just returning the eggs object (as it would for any other attribute) it will call the __get__ method (since we were doing lookup) and return whatever that method returns.More information about Python's data model and descriptors.With a property you have complete control on its getter, setter and deleter methods, which you don't have (if not using caveats) with an attribute.In general speaking terms a property and an attribute are the same thing. However, there is a property decorator in Python which provides getter/setter access to an attribute (or other data).The property allows you to get and set values like you would normal attributes, but underneath there is a method being called translating it into a getter and setter for you.  It's really just a convenience to cut down on the boilerplate of calling getters and setters.Lets say for example, you had a class that held some x and y coordinates for something you needed.  To set them you might want to do something like:That is much easier to look at and think about than writing:The problem is, what if one day your class changes such that you need to offset your x and y by some value?  Now you would need to go in and change your class definition and all of the code that calls it, which could be really time consuming and error prone.  The property allows you to use the former syntax while giving you the flexibility of change of the latter.In Python, you can define getters, setters, and delete methods with the property function.  If you just want the read property, there is also a @property decorator you can add above your method.http://docs.python.org/library/functions.html#propertyI learnt 2 differences from  site of Bernd Klein, in summary:1. Property is a more convenient way to do data encapsulation.ex: If your have a public attribute lenght of Object, later on, your project requires you to encapsulate it, i.e: change it to private and provide getter and setter => you have to change many of the codes you wrote before:If you use @property and @lenght.setter => you don't need to change those old codes2. A property can encapsulate multiple attributesIn this example, __physic_health and __mental_health are private and can not be accessed directly from out side, the only way outside class interact with them is throught property conditionThere is also one not obvious difference that i use to cache or refresh data , often we have a function connected to class attribute. For instance i need to read file once and keep content assigned to the attribute so the value is cached:Output:We accessed the attribute twice but our function was fired only once. Changing the above example to use property will cause attribute's value refresh each time you access it:Output:

Access nested dictionary items via a list of keys?

kolergy

[Access nested dictionary items via a list of keys?](https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys)

I have a complex dictionary structure which I would like to access via a list of keys to address the correct item.or    I have made the following code which works but I'm sure there is a better and more efficient way to do this if anyone has an idea.

2013-02-04 18:04:35Z

I have a complex dictionary structure which I would like to access via a list of keys to address the correct item.or    I have made the following code which works but I'm sure there is a better and more efficient way to do this if anyone has an idea.Use reduce() to traverse the dictionary:and reuse getFromDict to find the location to store the value for setInDict():All but the last element in mapList is needed to find the 'parent' dictionary to add the value to, then use the last element to set the value to the right key.Demo:Note that the Python PEP8 style guide prescribes snake_case names for functions. The above works equally well for lists or a mix of dictionaries and lists, so the names should really be get_by_path() and set_by_path():So why not use the suggested method from kolergy's question for getting a value:And the code from @eafit's answer for setting a value:Both work straight in python 2 and 3Using reduce is clever, but the OP's set method may have issues if the parent keys do not pre-exist in the nested dictionary. Since this is the first SO post I saw for this subject in my google search, I would like to make it slightly better.The set method in ( Setting a value in a nested python dictionary given a list of indices and value ) seems more robust to missing parental keys. To copy it over:Also, it can be convenient to have a method that traverses the key tree and get all the absolute key paths, for which I have created:One use of it is to convert the nested tree to a pandas DataFrame, using the following code (assuming that all leafs in the nested dictionary have the same depth).This library may be helpful: https://github.com/akesterson/dpath-pythonHow about using recursive functions?To get a value:And to set a value:Pure Python style, without any import:OutputAn alternative way if you don't want to raise errors if one of the keys is absent (so that your main code can run without interruption):In this case, if any of the input keys is not present, None is returned, which can be used as a check in your main code to perform an alternative task.Instead of taking a performance hit each time you want to look up a value, how about you flatten the dictionary once then simply look up the key like b:v:yThis way you can simply look up items using flat_dict['b:v:y'] which will give you 1.And instead of traversing the dictionary on each lookup, you may be able to speed this up by flattening the dictionary and saving the output so that a lookup from cold start would mean loading up the flattened dictionary and simply performing a key/value lookup with no traversal.Solved this with recursion:Using your example:How about check and then set dict element without processing all indexes twice?Solution:Example workflow:TestVery late to the party, but posting in case this may help someone in the future. For my use case, the following function worked the best. Works to pull any data type out of dictionarydict is the dictionary containing our valuelist is a list of "steps" towards our valueI suggest you to use python-benedict to access nested items using keypath.Install it using pip:Then:Here the full documentation:

https://github.com/fabiocaccamo/python-benedictIf you also want the ability to work with arbitrary json including nested lists and dicts, and nicely handle invalid lookup paths, here's my solution:a method for concatenating strings:It's satisfying to see these answers for having two static methods for setting & getting nested attributes. These solutions are way better than using nested trees https://gist.github.com/hrldcpr/2012250Here's my implementation.Usage:To set nested attribute call sattr(my_dict, 1, 2, 3, 5) is equal to my_dict[1][2][3][4]=5To get a nested attribute call gattr(my_dict, 1, 2)Extending @DomTomCat and others' approach, these functional (ie, return modified data via deepcopy without affecting the input) setter and mapper works for nested dict and list.  setter:mapper:You can make use of the eval function in python.ExplanationFor your example query: maplist = ["b", "v", "y"]nestq will be "nest['b']['v']['y']" where nest is the nested dictionary.The eval builtin function executes the given string. However, it is important to be careful about possible vulnerabilities that arise from use of eval function. Discussion can be found here:In the nested_parse() function, I have made sure that no __builtins__ globals are available and only local variable that is available is the nest dictionary. 

python dataframe pandas drop column using int

user1802143

[python dataframe pandas drop column using int](https://stackoverflow.com/questions/20297317/python-dataframe-pandas-drop-column-using-int)

I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?

2013-11-30 06:27:11Z

I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?You can delete column on i index like this:It could work strange, if you have duplicate names in columns, so to do this you can rename column you want to delete column by new name. Or you can reassign DataFrame like this:Drop multiple columns like this:inplace=True is used to make the changes in the dataframe itself without doing the column dropping on a copy of the data frame. If you need to keep your original intact, use:If there are multiple columns with identical names, the solutions given here so far will remove all of the columns,  which may not be what one is looking for. This may be the case if one is trying to remove duplicate columns except one instance. The example below clarifies this situation:As you can see, both Xs columns were dropped.

Alternative solution:As you can see, this truly removed only the 0th column (first 'x').You need to identify the columns based on their position in dataframe. For example, if you want to drop (del) column number 2,3 and 5, it will be,if you really want to do it with integers (but why?), then you could build a dictionary.then df = df.drop(col_dict[0], 1) will work as desirededit: you can put it in a function that does that for you, though this way it creates the dictionary every time you call itIf you have two columns with the same name. One simple way is to manually rename the columns like this:-Then you can drop via column index as you requested, like this:-df.column[1] will drop index 1.Remember axis 1 = columns and axis 0 = rows.You can use the following line to drop the first two columns (or any column you don't need):ReferenceSince there can be multiple columns with same name , we should first rename the columns.

Here is code for the solution.

Python Matplotlib figure title overlaps axes label when using twiny

Magic_Matt_Man

[Python Matplotlib figure title overlaps axes label when using twiny](https://stackoverflow.com/questions/12750355/python-matplotlib-figure-title-overlaps-axes-label-when-using-twiny)

I am trying to plot two separate quantities on the same graph using twiny as follows:and the data is presented fine, but I am having the problem that the figure title is overlapping with the axes labels on the secondary x axis so that it's barely legible (I wanted to post a picture example here, but I don't have a high enough rep yet).I'd like to know if there's a straightforward way to just shift the title directly up a few tens of pixels, so that the chart looks prettier.

2012-10-05 16:26:49Z

I am trying to plot two separate quantities on the same graph using twiny as follows:and the data is presented fine, but I am having the problem that the figure title is overlapping with the axes labels on the secondary x axis so that it's barely legible (I wanted to post a picture example here, but I don't have a high enough rep yet).I'd like to know if there's a straightforward way to just shift the title directly up a few tens of pixels, so that the chart looks prettier.I'm not sure whether it is a new feature in later versions of matplotlib, but at least for 1.3.1, this is simply:This also works for plt.suptitle(), but not (yet) for plt.xlabel(), etc.Forget using plt.title and place the text directly with plt.text. An over-exaggerated example is given below:If you put '\n' right after your title string, the plot is drawn just below the title. That might be a fast solution too.I was having an issue with the x-label overlapping a subplot title; this worked for me:reference:Just use plt.tight_layout() before plt.show(). It works well. 

Convert floating point number to a certain precision, and then copy to string

pauliwago

[Convert floating point number to a certain precision, and then copy to string](https://stackoverflow.com/questions/15263597/convert-floating-point-number-to-a-certain-precision-and-then-copy-to-string)

I have a floating point number, say 135.12345678910. I want to concatenate that value to a string, but only want 135.123456789. With print, I can easily do this by doing something like:with numvar being my original number. Is there an easy way to do this?

2013-03-07 05:14:46Z

I have a floating point number, say 135.12345678910. I want to concatenate that value to a string, but only want 135.123456789. With print, I can easily do this by doing something like:with numvar being my original number. Is there an easy way to do this?With Python < 3 (e.g. 2.6 [see comments] or 2.7), there are two ways to do so.But note that for Python versions above 3 (e.g. 3.2 or 3.3), option two is preferred.For more information on option two, I suggest this link on string formatting from the Python documentation.And for more information on option one, this link will suffice and has info on the various flags.Python 3.6 (officially released in December of 2016), added the f string literal, see more information here, which extends the str.format method (use of curly braces such that f"{numvar:.9f}" solves the original problem), that is,solves the problem. Check out @Or-Duan's answer for more info, but this method is fast.Using round:Just to make it clear, you can use f-string formatting. This has almost the same syntax as the format method, but make it a bit nicer.Example:More reading about the new f string:It's not print that does the formatting, It's a property of strings, so you can just useIn case the precision is not known until runtime, this other formatting option is useful:To set precision with 9 digits, get:Return precision with 2 digits:Return precision with 2 digits and float converted value:The str function has a bug. Please try the following. You will see '0,196553' but the right output is '0,196554'. Because the str function's default value is ROUND_HALF_UP.

Common use-cases for pickle in Python

satoru

[Common use-cases for pickle in Python](https://stackoverflow.com/questions/3438675/common-use-cases-for-pickle-in-python)

I've looked at the pickle documentation, but I don't understand where pickle is useful.What are some common use-cases for pickle?

2010-08-09 09:15:46Z

I've looked at the pickle documentation, but I don't understand where pickle is useful.What are some common use-cases for pickle?Some uses that I have come across:1) saving a program's state data to disk so that it can carry on where it left off when restarted (persistence)2) sending python data over a TCP connection in a multi-core or distributed system (marshalling)3) storing python objects in a database4) converting an arbitrary python object to a string so that it can be used as a dictionary key (e.g. for caching & memoization).There are some issues with the last one - two identical objects can be pickled and result in different strings - or even the same object pickled twice can have different representations.  This is because the pickle can include reference count information.To emphasise @lunaryorn's comment - you should never unpickle a string from an untrusted source, since a carefully crafted pickle could execute arbitrary code on your system.  For example see https://blog.nelhage.com/2011/03/exploiting-pickle/Minimal roundtrip example..Edit: but as for the question of real-world examples of pickling, perhaps the most advanced use of pickling (you'd have to dig quite deep into the source) is ZODB:

http://svn.zope.org/Otherwise, PyPI mentions several:

http://pypi.python.org/pypi?:action=search&term=pickle&submit=searchI have personally seen several examples of pickled objects being sent over the network as an easy to use network transfer protocol.I have used it in one of my projects. If the app was terminated during it's working (it did a lengthy task and processed lots of data), I needed to save the whole data structure and reload it after the app was run again. I used cPickle for this, as speed was a crucial thing and the size of data was really big. Pickling is absolutely necessary for distributed and parallel computing.Say you wanted to do a parallel map-reduce with multiprocessing (or across cluster nodes with pyina), then you need to make sure the function you want to have mapped across the parallel resources will pickle.  If it doesn't pickle, you can't send it to the other resources on another process, computer, etc.  Also see here for a good example.To do this, I use dill, which can serialize almost anything in python. Dill also has some good tools for helping you understand what is causing your pickling to fail when your code fails.And, yes, people use picking to save the state of a calculation, or your ipython session, or whatever.For the beginner (as is the case with me) it's really hard to understand why use pickle in the first place when reading the official documentation. It's maybe because the docs imply that you already know the whole purpose of serialization. Only after reading the general description of serialization have I understood the reason for this module and its common use cases. Also broad explanations of serialization disregarding a particular programming language may help:

https://stackoverflow.com/a/14482962/4383472, What is serialization?,

https://stackoverflow.com/a/3984483/4383472Pickle is like "Save As.." and "Open.." for your data structures and classes. Let's say I want to save my data structures so that it is persistent between program runs.Saving:Loading:Now I don't have to build myStuff from scratch all over again, and I can just pick(le) up from where I left off. To add a real-world example: The Sphinx documentation tool for Python uses pickle to cache parsed documents and cross-references between documents, to speed up subsequent builds of the documentation.I can tell you the uses I use it for and have seen it used for:Those are the ones I use it for at leastI use pickling during web scrapping one of website at that time I want to store more than 8000k urls and want to process them as fast as possible so I use pickling because its output quality is very high.you can easily reach to url and where you stop even job directory key word also fetch url details very fast for resuming the process.

How do I create a namespace package in Python?

joeforker

[How do I create a namespace package in Python?](https://stackoverflow.com/questions/1675734/how-do-i-create-a-namespace-package-in-python)

In Python, a namespace package allows you to spread Python code among several projects. This is useful when you want to release related libraries as separate downloads. For example, with the directories Package-1 and Package-2 in PYTHONPATH,the end-user can import namespace.module1 and import namespace.module2.What's the best way to define a namespace package so more than one Python product can define modules in that namespace?

2009-11-04 18:27:10Z

In Python, a namespace package allows you to spread Python code among several projects. This is useful when you want to release related libraries as separate downloads. For example, with the directories Package-1 and Package-2 in PYTHONPATH,the end-user can import namespace.module1 and import namespace.module2.What's the best way to define a namespace package so more than one Python product can define modules in that namespace?TL;DR:On Python 3.3 you don't have to do anything, just don't put any __init__.py in your namespace package directories and it will just work. On pre-3.3, choose the pkgutil.extend_path() solution over the pkg_resources.declare_namespace() one, because it's future-proof and already compatible with implicit namespace packages.Python 3.3 introduces implicit namespace packages, see PEP 420.This means there are now three types of object that can be created by an import foo:Packages are modules too, but here I mean "non-package module" when I say "module".First it scans sys.path for a module or regular package. If it succeeds, it stops searching and creates and initalizes the module or package. If it found no module or regular package, but it found at least one directory, it creates and initializes a namespace package.Modules and regular packages have __file__ set to the .py file they were created from. Regular and namespace packages have __path__set to the directory or directories they were created from.When you do import foo.bar, the above search happens first for foo, then if a package was found, the search for bar is done with foo.__path__as the search path instead of sys.path. If foo.bar is found, foo and foo.bar are created and initialized.So how do regular packages and namespace packages mix? Normally they don't, but the old pkgutil explicit namespace package method has been extended to include implicit namespace packages.If you have an existing regular package that has an __init__.py like this:... the legacy behavior is to add any other regular packages on the searched path to its __path__. But in Python 3.3, it also adds namespace packages.So you can have the following directory structure:... and as long as the two __init__.py have the extend_path lines (and path1, path2 and path3 are in your sys.path) import package.foo, import package.bar and import package.baz will all work.pkg_resources.declare_namespace(__name__) has not been updated to include implicit namespace packages.There's a standard module, called pkgutil, with which you

can 'append' modules to a given namespace.With the directory structure you've provided:You should put those two lines in both Package-1/namespace/__init__.py and Package-2/namespace/__init__.py (*):(* since -unless you state a dependency between them- you don't know which of them will be recognized first - see PEP 420 for more information)As the documentation says:From now on, you should be able to distribute those two packages independently.This section should be pretty self-explanatory.In short, put the namespace code in __init__.py, update setup.py to declare a namespace, and you are free to go.This is an old question, but someone recently commented on my blog that my posting about namespace packages was still relevant, so thought I would link to it here as it provides a practical example of how to make it go:https://web.archive.org/web/20150425043954/http://cdent.tumblr.com/post/216241761/python-namespace-packages-for-tiddlywebThat links to this article for the main guts of what's going on:http://www.siafoo.net/article/77#multiple-distributions-one-virtual-packageThe __import__("pkg_resources").declare_namespace(__name__) trick is pretty much drives the management of plugins in TiddlyWeb and thus far seems to be working out.You have your Python namespace concepts back to front, it is not possible in python to put packages into modules. Packages contain modules not the other way around.A Python package is simply a folder containing a __init__.py file. A module is any other file in a package (or directly on the PYTHONPATH) that has a .py extension. So in your example you have two packages but no modules defined. If you consider that a package is a file system folder and a module is file then you see why packages contain modules and not the other way around.So in your example assuming Package-1 and Package-2 are folders on the file system that you have put on the Python path you can have the following:You now have one package namespace with two modules module1 and module2. and unless you have a good reason you should probably put the modules in the folder and have only that on the python path like below:

Is there a better way to iterate over two lists, getting one element from each list for each iteration? [duplicate]

chongman

[Is there a better way to iterate over two lists, getting one element from each list for each iteration? [duplicate]](https://stackoverflow.com/questions/1919044/is-there-a-better-way-to-iterate-over-two-lists-getting-one-element-from-each-l)

I have a list of Latitudes and one of Longitudes and need to iterate over the latitude and longitude pairs.Is it better to:(Note that B is incorrect. This gives me all the pairs, equivalent to itertools.product())Any thoughts on the relative merits of each, or which is more pythonic?

2009-12-17 02:00:55Z

I have a list of Latitudes and one of Longitudes and need to iterate over the latitude and longitude pairs.Is it better to:(Note that B is incorrect. This gives me all the pairs, equivalent to itertools.product())Any thoughts on the relative merits of each, or which is more pythonic?This is as pythonic as you can get:Another way to do this would be to by using map.One difference in using map compared to zip is, with zip the length of new list is

same as the length of shortest list.

For example:Using map on same data:Good to see lots of love for zip in the answers here. However it should be noted that if you are using a python version before 3.0, the itertools module in the standard library contains an izip function which returns an iterable, which is more appropriate in this case (especially if your list of latt/longs is quite long).In python 3 and later zip behaves like izip.in case your Latitude and Longitude lists are large and lazily loaded:or if you want to avoid the for-loopIterating through elements of two lists simultaneously is known as zipping, and python provides a built in function for it, which is documented here.[Example is taken from pydocs]In your case, it will be simply:This post helped me with zip(). I know I'm a few years late, but I still want to contribute.  This is in Python 3.Note: in python 2.x, zip() returns a list of tuples; in Python 3.x, zip() returns an iterator.

itertools.izip() in python 2.x == zip() in python 3.xSince it looks like you're building a list of tuples, the following code is the most pythonic way of trying to accomplish what you are doing.Or, alternatively, you can use list comprehensions (or list comps) should you need more complicated operations.  List comprehensions also run about as fast as map(), give or take a few nanoseconds, and are becoming the new norm for what is considered Pythonic versus map().

Convert decimal to binary in python [duplicate]

Paul

[Convert decimal to binary in python [duplicate]](https://stackoverflow.com/questions/3528146/convert-decimal-to-binary-in-python)

Is there any module or function in python I can use to convert a decimal number to its binary equivalent?

I am able to convert binary to decimal using int('[binary_value]',2), so any way to do the reverse without writing the code to do it myself?

2010-08-20 04:13:12Z

Is there any module or function in python I can use to convert a decimal number to its binary equivalent?

I am able to convert binary to decimal using int('[binary_value]',2), so any way to do the reverse without writing the code to do it myself?all numbers are stored in binary. if you want a textual representation of a given number in binary, use bin(i)Without the 0b in front:Starting with Python 3.6 you can also use formatted string literal or f-string, --- PEP:  It's that easy.You can also use a function from the numpy modulewhich can also handle leading zeros:I agree with @aaronasterling's answer. However, if you want a non-binary string that you can cast into an int, then you can use the canonical algorithm:For the sake of completion: if you want to convert fixed point representation to its binary equivalent you can perform the following operations:You can read the explanation here. 

Can't install PIL after Mac OS X 10.9

Vincent Audebert

[Can't install PIL after Mac OS X 10.9](https://stackoverflow.com/questions/19532125/cant-install-pil-after-mac-os-x-10-9)

I've just updated my Mac OS to 10.9 and I discovered that some (all?) of my Python modules are not here anymore, especially the Image one.So I try to execute sudo pip install pil, but I get this error:My Xcode is up-to-date and I don't have any idea. Is it possible that PIL is not yet 10.9 compatible ?

2013-10-23 03:14:19Z

I've just updated my Mac OS to 10.9 and I discovered that some (all?) of my Python modules are not here anymore, especially the Image one.So I try to execute sudo pip install pil, but I get this error:My Xcode is up-to-date and I don't have any idea. Is it possible that PIL is not yet 10.9 compatible ?Following worked for me:UPDATE:But there is more correct solution below, provided by Will.open your terminal and execute: xcode-select --installhelps for me!

os x 10.9but! after pip install ...and finally i fix it by running: then reinstall pillowWorks for me ( OS X Yosemite 10.10.2 - Python 2.7.9 ) :Try this to check it:Here is what I did, some steps may not be necessary just for PIL but I needed libpng and others anyways:1) Run xcode install, use this command or download updates from the app store:1b) Add the Command Line Tools optional tool, in Mountain Lion this was an option on the xcode Download page, but now you have to register with your apple id and download from: https://developer.apple.com/downloads/Look for Command Line Tools (OS X Mavericks) for Xcode2) Install everything needed for python (using brew), I believe you can use port as well:Unlink/ relink if needed i.e. if upgrading.3) Install Pip and required modules:4) Finally this works with no errors:UPDATE 11/04/14: PIL repo no longer receives updates or support so Pillow should be used. The below is now deprecated so stick with Pillow.UPDATE (OLD) : The same thing applies when installing Pillow (PIL fork) and should be mentioned as its quickly becoming a replacement in most cases of PIL. Instead of installing pip in step 4, run this instead:Hope this helps someone!installing command line tools fixed the issue for meyou have to install them separately as they are not part of the packages in xcode now:https://developer.apple.com/downloads/index.action?=command%20line%20tools#Non of those worked for me.. I kept receiving:So I found a work around with the following solution:This way I was able to install.I had a similar problem: Installing pillow failed with clang: error: unknown argument: '-mno-fused-madd' [-Wunused-command-line-argument-hard-error-in-future], installing command line tools failed with Can't install the software because it is not currently available from the Software Update server., and even after installing the command line tools manually, the compilation of PIL failed.This happens cause clang under the newest version of xcode doesn't warn on unknown compiler flags, but rather stop the compilation with a hard error. To fix this, just run export ARCHFLAGS="-Wno-error=unused-command-line-argument-hard-error-in-future" on the terminal before trying to compile (installing pil). Simply run

pip install pil --allow-external pil --allow-unverified pil

This my steps on mac os 10.9.1You could use Homebrew to do the install  http://brew.sh    Make sure you have Command Line Tools installed on your xcode. Then execute:I was having the following errorThe solution to this was to symlink freetype2 to freetype and this solved the problem.I didn't want to install XCode (I don't use it) and I'm loath to fiddle with Application directory.  I've cribbed from the many answers in this post and the following two steps work for me with 10.9.5:It did appear to me strange that I had to use easy_install to install pip.  But pip didn't want to work for me before that (re-)install.Found the solution ... You've to symlink X11 like this ln -s /opt/X11/include/X11 /usr/local/include/X11 and then sudo pip install pil should work.Reusing @DmitryDemidenko's answer that is how it worked for me:and thenExecute the bellow command lines. Works like a charm on Mac OS 10.9.5easy_install pip sudo pip install setuptools --no-use-wheel --upgradesudo pip install PillowBest, 

TheoThat's what I did:First upgrade to Xcode 5 (I am running 10.9). Then, execute the following commands in a terminal:A more complete solution requires the installation of the Xquartz X11 subsystem that has been built outside of Apple for several years now. Here are the steps I used to get it all workingHad the same issue after update to 10.9 and fixed the error "command 'cc' failed with exit status 1" with this: As the accepted answer is the right one with xcode-select --install but some people (including me) may encounter Can't install the software because it is not currently available from the Software Update server

If you are using beta software (as I am using Yosemite now and had the same problem) you NEED to get the CLT separately since it is NOT included in XCode (even xcode beta)

Head over to developers.apple.com and get CLT tools for your OS ;)P.S. You don't need XQuartz for PIL or Pillow to workMy machine which was recently upgraded from OS 10.8 -> 10.9 got stuck in a loop between xcrun and lipo.Rename /usr/bin/lipo to /usr/bin/lipo_brokenRefer to this thread for further information on how to resolve:xcrun/lipo freezes with OS X Mavericks and XCode 4.xInstall Pillow instead:Try this:worked for me. I'm running Python 2.7.9 on Yosemite.import PIL now works for me.Installing PIL (Imaging.1.1.7) on Mac OSC 10.10 Yosemite.  I tried numerous fixes recommended here but ran into trouble with each one.  I finally solved this problem by editing the setup.py file such that:TCL_ROOT = "/opt/X11/include"which passes the appropriate include path for X11 in the compilation of _imagingtk.c, which was causing the problem for me.  Worked immediately after change.I've moved from pyenv to virtualenv and this fixed my problem.

Should I use scipy.pi, numpy.pi, or math.pi?

Douglas B. Staple

[Should I use scipy.pi, numpy.pi, or math.pi?](https://stackoverflow.com/questions/12645547/should-i-use-scipy-pi-numpy-pi-or-math-pi)

In a project using SciPy and NumPy, should I use scipy.pi, numpy.pi, or math.pi?

2012-09-28 18:36:58Z

In a project using SciPy and NumPy, should I use scipy.pi, numpy.pi, or math.pi?So it doesn't matter, they are all the same value.The only reason all three modules provide a pi value is so if you are using just one of the three modules, you can conveniently have access to pi without having to import another module. They're not providing different values for pi.One thing to note is that not all libraries will use the same meaning for pi, of course, so it never hurts to know what you're using. For example, the symbolic math library Sympy's representation of pi is not the same as math and numpy:

How do I keep track of pip-installed packages in an Anaconda (Conda) environment?

gromiczek

[How do I keep track of pip-installed packages in an Anaconda (Conda) environment?](https://stackoverflow.com/questions/18640305/how-do-i-keep-track-of-pip-installed-packages-in-an-anaconda-conda-environment)

I've installed and have been using the Anaconda Python distribution, and I have started using the Anaconda (Conda) environment. I can use the standard conda install... command to put packages from the distribution into my environments, but to use anything outside (i.e. Flask-WTF, flask-sqlalchemy, and alembic) I need to use pip install in the active environment. However, when I look at the contents of the environment, either in the directory, or using conda list these pip installed packages don't show up. Using pip freeze and pip list just lists every package I've ever installed. Is there a way to keep track of what is in each of my Anaconda envs (both pip and conda installed)? 

2013-09-05 15:25:41Z

I've installed and have been using the Anaconda Python distribution, and I have started using the Anaconda (Conda) environment. I can use the standard conda install... command to put packages from the distribution into my environments, but to use anything outside (i.e. Flask-WTF, flask-sqlalchemy, and alembic) I need to use pip install in the active environment. However, when I look at the contents of the environment, either in the directory, or using conda list these pip installed packages don't show up. Using pip freeze and pip list just lists every package I've ever installed. Is there a way to keep track of what is in each of my Anaconda envs (both pip and conda installed)? conda-env now does this automatically (if pip was installed with conda).You can see how this works by using the export tool used for migrating an environment:The file will list both conda packages and pip packages:If you're looking to follow through with exporting the environment, move environment.yml to the new host machine and run:conda will only keep track of the packages it installed. And pip will give you the packages that were either installed using the pip installer itself or they used setuptools in their setup.py so conda build generated the egg information. So you have basically three options.I would personally recommend the third option since it's very easy to build conda packages. There is a git repository of example recipes on the continuum's github account. But it usually boils down to:or just:Also when you have built them once, you can upload them to https://binstar.org/ and just install from there.Then you'll have everything managed using conda.There is a branch of conda (new-pypi-install) that adds better integration with pip and PyPI.   In particular conda list will also show pip installed packages and conda install will first try to find a conda package and failing that will use pip to install the package. This branch is scheduled to be merged later this week so that version 2.1 of conda will have better pip-integration with conda.I followed @Viktor Kerkez's answer and have had mixed success. I found that sometimes this recipe of would look like everything worked but I could not successfully import PACKAGE. Recently I asked about this on the Anaconda user group and heard from @Travis Oliphant himself on the best way to use conda to build and manage packages that do not ship with Anaconda. You can read this thread here, but I'll describe the approach below to hopefully make the answers to the OP's question more complete...Example: I am going to install the excellent prettyplotlib package on Windows using conda 2.2.5.1a) conda build --build-recipe prettyplotlibYou'll see the build messages all look good until the final TEST section of the build. I saw this error1b) Go into /conda-recipes/prettyplotlib and edit the meta.yaml file. Presently, the packages being set up like in step 1a result in yaml files that have an error in the test section. For example, here is how mine looked for prettyplotlibEdit this section to remove the blank line preceded by the - and also remove the redundant prettyplotlib line. At the time of this writing I have found that I need to edit most meta.yaml files like this for external packages I am installing with conda, meaning that there is a blank import line causing the error along with a redundant import of the given package.1c) Rerun the command from 1a, which should complete with out error this time. At the end of the build you'll be asked if you want to upload the build to binstar. I entered No and then saw this message:That tar.bz2 file is the build that you now need to actually install.2) conda install C:\Anaconda\conda-bld\win-64\prettyplotlib-0.1.3-py27_0.tar.bz2Following these steps I have successfully used conda to install a number of packages that do not come with Anaconda. Previously, I had installed some of these using pip, so I did pip uninstall PACKAGE prior to installing PACKAGE with conda. Using conda, I can now manage (almost) all of my packages with a single approach rather than having a mix of stuff installed with conda, pip, easy_install, and python setup.py install.For context, I think this recent blog post by @Travis Oliphant will be helpful for people like me who do not appreciate everything that goes into robust Python packaging but certainly appreciate when stuff "just works". conda seems like a great way forward...This is why I wrote Picky: http://picky.readthedocs.io/It's a python package that tracks packages installed with either pip or conda in either virtualenvs and conda envs.I think what's missing here is that when you do:to install a local package with a setup.py,

it installs a package that is visible to all the conda envs that use

the same version of python. Note I am using the conda version of pip!e.g., if I'm using python2.7 it puts the local package here:/usr/local/anaconda/lib/python2.7/site-packagesIf I then later create a new conda env with python=2.7 (= the default):And then do:However, if I do:So in this case, conda does not know about the pip package, but the package is available to python.However, If I instead install the local package (again using pip) after I've created (and activated) the new conda env, now conda sees it:So I think the interaction between conda and pip has some issues - ie, using pip to install a local package from within one conda env makes that package available (but not seen via conda list) to all other conda envs of the same python version.conda env export lists all conda and pip packages in an environment. conda-env must be installed in the conda root (conda install -c conda conda-env).To write an environment.yml file describing the current environment:References:I usually prefix the 'bin/pip' folder for the specific environment you want to install the package before the 'pip' command. For instance, if you would like to install pymc3 in the environment py34, you should use this command:You basically just need to find the right path to your environment 'bin/pip' folder and put it before the install command.My which pip shows the following path:So, whatever package I install using pip install <package-name> will have to be reflected in the list of packages when the list is exported using:But, I don't. So, instead I used the following command as suggested by several others:Now, I can see all the packages in my all-packages.yml file.You can start by installing the below given command in the conda environment:conda install pipFollowed by installing all pip packages you need in the environment.After installing all the conda and pip packages to export the environment use:conda env export -n <env-name> > environment.ymlThis will create the required file in the folder

Iterating over a numpy array

Ram Rachum

[Iterating over a numpy array](https://stackoverflow.com/questions/6967463/iterating-over-a-numpy-array)

Is there a less verbose alternative to this:I came up with this:Which saves one indentation, but is still pretty ugly.I'm hoping for something that looks like this pseudocode:Does anything like that exist?

2011-08-06 14:27:03Z

Is there a less verbose alternative to this:I came up with this:Which saves one indentation, but is still pretty ugly.I'm hoping for something that looks like this pseudocode:Does anything like that exist?I think you're looking for the ndenumerate.Regarding the performance. It is a bit slower than a list comprehension. If you are worried about the performance you could optimise a bit further by looking at the implementation of ndenumerate, which does 2 things, converting to an array and  looping. If you know you have an array, you can call the .coords attribute of the flat iterator. If you only need the indices, you could try numpy.ndindex:see nditer

Python Request Post with param data

slysid

[Python Request Post with param data](https://stackoverflow.com/questions/15900338/python-request-post-with-param-data)

This is the raw request for an API call:This request returns a success (2xx) response.Now I am trying to post this request using requests:Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response.

2013-04-09 11:12:56Z

This is the raw request for an API call:This request returns a success (2xx) response.Now I am trying to post this request using requests:Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response.params is for GET-style URL parameters, data is for POST-style body information. It is perfectly legal to provide both types of information in a request, and your request does so too, but you encoded the URL parameters into the URL already.Your raw post contains JSON data though. requests can handle JSON encoding for you, and it'll set the correct Content-Header too; all you need to do is pass in the Python object to be encoded as JSON into the json keyword argument.You could split out the URL parameters as well:then post your data with:The json keyword is new in requests version 2.4.2; if you still have to use an older version, encode the JSON manually using the json module and post the encoded result as the data key; you will have to explicitly set the Content-Type header in that case:Set data to this: Assign the response to a value and test the attributes of it. These should tell you something useful.

How to loop over grouped Pandas dataframe?

Tjorriemorrie

[How to loop over grouped Pandas dataframe?](https://stackoverflow.com/questions/27405483/how-to-loop-over-grouped-pandas-dataframe)

DataFrame:Code:I'm trying to just loop over the aggregated data, but I get the error:@EdChum, here's the expected output:The output is not the problem, I wish to loop over every group.

2014-12-10 16:01:40Z

DataFrame:Code:I'm trying to just loop over the aggregated data, but I get the error:@EdChum, here's the expected output:The output is not the problem, I wish to loop over every group.df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)) does already return a dataframe, so you cannot loop over the groups anymore.In general:Here is an example of iterating over a pd.DataFrame grouped by the column atable. For an sample usecase, "create" statements for an SQL database are generated within the for loop:You can iterate over the index values if your dataframe has already been created.

String formatting named parameters?

mpen

[String formatting named parameters?](https://stackoverflow.com/questions/2451821/string-formatting-named-parameters)

I know it's a really simple question, but I have no idea how to google it.how can I doSo that my_url is used twice? I assume I have to "name" the %s and then use a dict in the params, but I'm not sure of the proper syntax?just FYI, I'm aware I can just use my_url twice in the params, but that's not the point :)

2010-03-16 02:50:31Z

I know it's a really simple question, but I have no idea how to google it.how can I doSo that my_url is used twice? I assume I have to "name" the %s and then use a dict in the params, but I'm not sure of the proper syntax?just FYI, I'm aware I can just use my_url twice in the params, but that's not the point :)In Python 2.6+ and Python 3, you might choose to use the newer string formatting method.which saves you from repeating the argument, orif you want named parameters.which is strictly positional, and only comes with the caveat that format() arguments follow Python rules where unnamed args must come first, followed by named arguments, followed by *args (a sequence like list or tuple) and then *kwargs (a dict keyed with strings if you know what's good for you).

The interpolation points are determined first by substituting the named values at their labels, and then positional from what's left.

So, you can also do this...But not this...Python 3.6 introduces literal string formatting, so that you can format the named parameters without any repeating any of your named parameters outside the string:This will evaluate my_url, so if it's not defined you will get a NameError.  In fact, instead of my_url, you can write an arbitrary Python expression, as long as it evaluates to a string (because of the :s formatting code).  If you want a string representation for the result of an expression that might not be a string, replace :s by !s, just like with regular, pre-literal string formatting.For details on literal string formatting, see PEP 498, where it was first introduced.You will be addicted to syntax.Also C# 6.0, EcmaScript developers has also familier this syntax.For building HTML pages, you want to use a templating engine, not simple string interpolation.As well as the dictionary way, it may be useful to know the following format:print '<a href="%s">%s</a>' % (my_url, my_url)Here it's a tad redundant, and the dictionary way is certainly less error prone when modifying the code, but it's still possible to use tuples for multiple insertions. The first %s is substituted for the first element in the tuple, the second %s is substituted for the second element in the tuple, and so on for each element in the tuple.

Function for Factorial in Python

Nir Levy

[Function for Factorial in Python](https://stackoverflow.com/questions/5136447/function-for-factorial-in-python)

How do I go about computing a factorial of an integer in Python?

2011-02-27 22:22:46Z

How do I go about computing a factorial of an integer in Python?Easiest way is to use math.factorial (available in Python 2.6 and above):If you want/have to write it yourself, you can use an iterative approach:or a recursive approach:Note that the factorial function is only defined for positive integers so you should also check that n >= 0 and that isinstance(n, int). If it's not, raise a ValueError or a TypeError respectively. math.factorial will take care of this for you.On Python 2.6 and up, try:Not really necessary since this is such an old thread. But I did here is another way to compute the factorial of an integer using a while loop. The shortest and probably the fastest solution is:You can also build your own solution. Generally you have two approaches. The one that suits me best is:(it works also for bigger numbers, when the result becomes long)The second way of achieving the same is:If you are using Python2.5 or older tryfor newer Python, there is factorial in the math module as given in other answers hereUsing a for-loop, counting backwards from n:Using the stack is convenient(like recursive call), but it comes at a cost: storing detailed information can take up a lot of memory.If the stack is high, it means that the computer stores a lot of information about function calls.The method only takes up constant memory(like iteration).Here is my tryOne line, fast and large numbers also works:I know this has been answered but here is another method with a reverse range list comprehension, making the range easier to read and more compact:You can see a full version of the code within this gist: https://gist.github.com/sadmicrowave/d4fbefc124eb69027d7a3131526e8c06Another way to do it is to use np.prod shown below:Factorial of a positive integer n, denoted by n!, is the product of all positive integers less than or equal to n.Formula : n! = n * (n-1) * (n-2) * (n-3) * (n-4) * ....... * 1There are several methods to find the factorial in python by using builtin funtion/library etc. Here I created user-defined function with reference of basic definition of factorial.We can also implement the factorial function using recursive technique as shown below. But this method is only efficient for small integer values. Because in recursion, the function is call repeatedly & require a memory space to maintain the stack which is not an efficient or optimised approach for large integer values to find the factorial.

Executing periodic actions in Python [duplicate]

Bruce

[Executing periodic actions in Python [duplicate]](https://stackoverflow.com/questions/8600161/executing-periodic-actions-in-python)

I am working on Windows. I want to execute a function foo() every 10 seconds.How do I do this?

2011-12-22 06:27:02Z

I am working on Windows. I want to execute a function foo() every 10 seconds.How do I do this?At the end of foo(), create a Timer which calls foo() itself after 10 seconds.

Because, Timer create a new thread to call foo().

 You can do other stuff without being blocked.Simply sleeping for 10 seconds or using threading.Timer(10,foo) will result in start time drift. (You may not care about this, or it may be a significant source of problems depending on your exact situation.) There can be two causes for this - inaccuracies in the wake up time of your thread or execution time for your function. You can see some results at the end of this post, but first an example of how to fix it. You need to track when your function should next be called as opposed to when it actually got called and account for the difference. Here's a version that drifts slightly:Its output looks like this:You can see that the sub-second count is constantly increasing and thus, the start time is "drifting".This is code that correctly accounts for drift:Its output looks like this:Here you can see that there is no longer any increase in the sub-second times. If your events are occurring really frequently you may want to run the timer in a single thread, rather than starting a new thread for each event. While accounting for drift this would look like:However your application will not exit normally, you'll need to kill the timer thread. If you want to exit normally when your application is done, without manually killing the thread, you should useSurprised to not find a solution using a generator for timing. I just designed this one for my own purposes.This solution: single threaded, no object instantiation each period, uses generator for times, rock solid on timing down to precision of the time module (unlike several of the solutions I've tried from stack exchange).Note: for Python 2.x, replace next(g) below with g.next().Results in, for example: Note that this example includes a simulation of the cpu doing something else for .3 seconds each period. If you changed it to be random each time it wouldn't matter. The max in the yield line serves to protect sleep from negative numbers in case the function being called takes longer than the period specified. In that case it would execute immediately and make up the lost time in the timing of the next execution.Perhaps the sched module will meet your needs.Alternatively, consider using a Timer object.Here's a nice implementation using the Thread class: http://g-off.net/software/a-python-repeatable-threadingtimer-classthe code below is a little more quick and dirty:This will insert a 10 second sleep in between every call to foo(), which is approximately what you asked for should the call complete quickly.  To do other things while your foo() is being called in a background threadYou can execute your task in a different thread. threading.Timer will let you execute a given callback once after some time has elapsed, if you want to execute your task, for example, as long as the callback returns True (this is actually what glib.timeout_add provides, but you might not have it installed in windows) or until you cancel it, you can use this code:Example output:Note: The callback isn't executed every interval execution. Interval is the time the thread waits between the callback finished the last time and the next time is called.Here's a simple single threaded sleep based version that drifts, but tries to auto-correct when it detects drift.NOTE: This will only work if the following 3 reasonable assumptions are met:-If you meant to run foo() inside a python script every 10 seconds, you can do something on these lines.

How to open every file in a folder?

B.Mr.W.

[How to open every file in a folder?](https://stackoverflow.com/questions/18262293/how-to-open-every-file-in-a-folder)

I have a python script parse.py, which in the script open a file, say file1, and then do something maybe print out the total number of characters. Right now, I am using stdout to direct the result to my output file - outputHowever, I don't want to do this file by file manually, is there a way to take care of every single file automatically? LikeThen the problem is how could I read the file name from standardin? 

or there are already some built-in functions to do the ls and those kind of work easily?Thanks!

2013-08-15 21:36:23Z

I have a python script parse.py, which in the script open a file, say file1, and then do something maybe print out the total number of characters. Right now, I am using stdout to direct the result to my output file - outputHowever, I don't want to do this file by file manually, is there a way to take care of every single file automatically? LikeThen the problem is how could I read the file name from standardin? 

or there are already some built-in functions to do the ls and those kind of work easily?Thanks!OsYou can list all files in the current directory using os.listdir:GlobOr you can list only some files, depending on the file pattern using the glob module:It doesn't have to be the current directory you can list them in any path you want:Pipe

Or you can even use the pipe as you specified using fileinputAnd then use it with piping:you should try using os.walkI was looking for this answer:you can choose as well '*.txt' or other ends of your filenameYou can actually just use os module to do both:Now you have not only listed all the files in a folder but also have them (optionally) sorted by starting name, file type and others. Just now iterate over each list and do your stuff.The code below reads for any text files available in the directory which contains the script we are running. Then it opens every text file and stores the words of the text line into a list. After store the words we print each word line by line 

django-debug-toolbar not showing up

AlexBrand

[django-debug-toolbar not showing up](https://stackoverflow.com/questions/10517765/django-debug-toolbar-not-showing-up)

I looked at other questions and can't figure it out... I did the following to install django-debug-toolbar:3 Added INTERNAL_IPS:4 Added debug_toolbar to installed appsI am not getting any errors or anything, and the toolbar doesn't show up on any page, not even admin.I even added the directory of the debug_toolbar templates to my TEMPLATE_DIRS

2012-05-09 14:05:43Z

I looked at other questions and can't figure it out... I did the following to install django-debug-toolbar:3 Added INTERNAL_IPS:4 Added debug_toolbar to installed appsI am not getting any errors or anything, and the toolbar doesn't show up on any page, not even admin.I even added the directory of the debug_toolbar templates to my TEMPLATE_DIRSStupid question, but you didn't mention it, so... What is DEBUG set to? It won't load unless it's True.If it's still not working, try adding '127.0.0.1' to INTERNAL_IPS as well.UPDATEThis is a last-ditch-effort move, you shouldn't have to do this, but it will clearly show if there's merely some configuration issue or whether there's some larger issue.Add the following to settings.py:That will effectively remove all checks by debug toolbar to determine if it should or should not load itself; it will always just load. Only leave that in for testing purposes, if you forget and launch with it, all your visitors will get to see your debug toolbar too.For explicit configuration, also see the official install docs here.EDIT(6/17/2015):Apparently the syntax for the nuclear option has changed. It's now in its own dictionary:Their tests use this dictionary.Debug toolbar wants the ip address in request.META['REMOTE_ADDR'] to be set in the INTERNAL_IPS setting. Throw in a print statement in one of your views like such:And then load that page. Make sure that IP is in your INTERNAL_IPS setting in settings.py. Normally I'd think you would be able to determine the address easily by looking at your computer's ip address, but in my case I'm running the server in a Virtual Box with port forwarding...and who knows what happened. Despite not seeing it anywhere in ifconfig on the VB or my own OS, the IP that showed up in the REMOTE_ADDR key was what did the trick of activating the toolbar.If everything else is fine, it could also be that your template lacks an explicit closing <body> tag—The current stable version 0.11.0 requires the following things to be true for the toolbar to be shown: Settings file:Template files:Static files:If you are serving static content make sure you collect the css, js and html by doing: Note on upcoming versions of django-debug-toolbarNewer, development versions have added defaults for settings points 2, 3 and 4 which makes life a bit simpler, however, as with any development version it has bugs. I found that the latest version from git resulted in an ImproperlyConfigured error when running through nginx/uwsgi. Either way, if you want to install the latest version from github run:You can also clone a specific commit by doing: I tried everything, from setting DEBUG = True, to settings INTERNAL_IPS to my client's IP address, and even configuring Django Debug Toolbar manually (note that recent versions make all configurations automatically, such as adding the middleware and URLs). Nothing worked in a remote development server (though it did work locally).

The ONLY thing that worked was configuring the toolbar as follows:This replaces the default method that decides if the toolbar should be shown, and always returns true.I have the toolbar working just perfect. With this configurations:I hope it helpsIf you're developing with a Django server in a Docker container with docker, the instructions for enabling the toolbar don't work.  The reason is related to the fact that the actual address that you would need to add to INTERNAL_IPS is going to be something dynamic, like 172.24.0.1. 

Rather than trying to dynamically set the value of INTERNAL_IPS, the straightforward solution is to replace the function that enables the toolbar, in your settings.py, for example:

This should also work for other dynamic routing situations, like vagrant.

Here are some more details for the curious.  The code in django_debug_tool that determines whether to show the toolbar examines the value of REMOTE_ADDR like this:so if you don't actually know the value of REMOTE_ADDR due to your dynamic docker routing, the toolbar will not work.  You can use the docker network command to see the dynamic IP values, for example docker network inspect my_docker_network_nameAdd 10.0.2.2 to your INTERNAL_IPS on Windows, it is used with vagrant internallyINTERNAL_IPS = (

    '10.0.2.2',

)This should work.I had the same problem and finally resolved it after some googling. In INTERNAL_IPS, you need to have the client's IP address.Another thing that can cause the toolbar to remain hidden is if it cannot find the required static files.  The debug_toolbar templates use the {{ STATIC_URL }} template tag, so make sure there is a folder in your static files called debug toolbar.The collectstatic management command should take care of this on most installations.I tried the configuration from pydanny's cookiecutter-django and it worked for me:I just modified it by adding 'debug_toolbar.apps.DebugToolbarConfig' instead of 'debug_toolbar' as mentioned in the official django-debug-toolbar docs, as I'm using Django 1.7.An addition to previous answers:if the toolbar doesn't show up, but it loads in the html (check your site html in a browser, scroll down)the issue can be that debug toolbar static files are not found (you can also see this in your site's access logs then, e.g. 404 errors for /static/debug_toolbar/js/toolbar.js)It can be fixed the following way then (examples for nginx and apache):nginx config:apache config:Or:more on collectstatic here: https://docs.djangoproject.com/en/dev/ref/contrib/staticfiles/#collectstaticOr manualy move debug_toolbar folder of debug_toolbar static files to your set static files folderIn my case, it was another problem that hasn't been mentioned here yet: I had GZipMiddleware in my list of middlewares.As the automatic configuration of debug toolbar puts the debug toolbar's middleware at the top, it only gets the "see" the gzipped HTML, to which it can't add the toolbar.I removed GZipMiddleware in my development settings. Setting up the debug toolbar's configuration manually and placing the middleware after GZip's should also work.django 1.8.5:I had to add the following to the project url.py file to get the debug toolbar display. After that debug tool bar is displayed.django 1.10: and higher:Also don't forget to include the debug_toolbar to your middleware.

The Debug Toolbar is mostly implemented in a middleware. Enable it in your settings module as follows:

(django newer versions)Old-style middleware:(need to have _CLASSES keywork in the Middleware)This wasn't the case for this specific author but I just have been struggling with the Debug Toolbar not showing and after doing everything they pointed out, I found out it was a problem with MIDDLEWARE order. So putting the middleware early in the list could work. Mine is first:MIDDLEWARE_CLASSES = (

    'debug_toolbar.middleware.DebugToolbarMiddleware',

    'django.middleware.common.CommonMiddleware',

    'django.contrib.sessions.middleware.SessionMiddleware',

    'django.middleware.csrf.CsrfViewMiddleware',

    'django.contrib.auth.middleware.AuthenticationMiddleware',

    'django.contrib.messages.middleware.MessageMiddleware',

    'dynpages.middleware.DynpageFallbackMiddleware',

    'utils.middleware.UserThread',

)In my case I just needed to remove the python compiled files (*.pyc)you have to make sure there is a closing  tag in your templates. My problem is that there is no regular html tags in my templates, I just display content in plain text. I solved  it by inheriting every html file from base.html, which has a  tag.For me this was as simple as typing 127.0.0.1:8000 into the address bar, rather than localhost:8000 which apparently was not matching the INTERNAL_IPS.I got the same problem, I solved it by looking at the Apache's error log.

I got the apache running on mac os x with mod_wsgi

The debug_toolbar's tamplete folder wasn't being loadLog sample:I just add this line to my VirtualHost file:I had the same problem using Vagrant. I solved this problem by adding ::ffff:192.168.33.1 to the INTERNAL_IPS as below example. Remembering that 192.168.33.10 is the IP in my private network in Vagrantfile.I had this problem and had to install the debug toolbar from source.Version 1.4 has a problem where it's hidden if you use PureCSS and apparently other CSS frameworks.This is the commit which fixes that.The docs explain how to install from source.For anyone who is using Pycharm 5 - template debug is not working there in some versions. Fixed in 5.0.4, affected vesions - 5.0.1, 5.0.2

Check out issue Spend A LOT time to find that out. Maybe will help someoneIn the code I was working on, multiple small requests were made during handling of main request (it's very specific use case). They were requests handled by the same Django's thread. Django debug toolbar (DjDT) doesn't expect this behaviour and includes DjDT's toolbars to the first response and then it removes its state for the thread. So when main request was sent back to the browser, DjDT was not included in the response.Lessons learned: DjDT saves it's state per thread. It removes state for a thread after the first response.What got me is an outdated browser!Noticed that it loads some stylesheets from debug toolbar and guessed it might be a front-end issue.One stupid thing got me.. that if you use apache wsgi, remember to touch the .wsgi file to force your code recompile. just waste 20 minutes of my time to debug the stupid error :(

How can I save my secret keys and password securely in my version control system?

Chris W.

[How can I save my secret keys and password securely in my version control system?](https://stackoverflow.com/questions/11575398/how-can-i-save-my-secret-keys-and-password-securely-in-my-version-control-system)

I keep important settings like the hostnames and ports of development and production servers in my version control system. But I know that it's bad practice to keep secrets (like private keys and database passwords) in a VCS repository.But passwords--like any other setting--seem like they should be versioned. So what is the proper way to keep passwords version controlled?I imagine it would involve keeping the secrets in their own "secrets settings" file and having that file encrypted and version controlled. But what technologies? And how to do this properly? Is there a better way entirely to go about it?I ask the question generally, but in my specific instance I would like to store secret keys and passwords for a Django/Python site using git and github.Also, an ideal solution would do something magical when I push/pull with git--e.g., if the encrypted passwords file changes a script is run which asks for a password and decrypts it into place.

2012-07-20 08:11:06Z

I keep important settings like the hostnames and ports of development and production servers in my version control system. But I know that it's bad practice to keep secrets (like private keys and database passwords) in a VCS repository.But passwords--like any other setting--seem like they should be versioned. So what is the proper way to keep passwords version controlled?I imagine it would involve keeping the secrets in their own "secrets settings" file and having that file encrypted and version controlled. But what technologies? And how to do this properly? Is there a better way entirely to go about it?I ask the question generally, but in my specific instance I would like to store secret keys and passwords for a Django/Python site using git and github.Also, an ideal solution would do something magical when I push/pull with git--e.g., if the encrypted passwords file changes a script is run which asks for a password and decrypts it into place.You're exactly right to want to encrypt your sensitive settings file while still maintaining the file in version control. As you mention, the best solution would be one in which Git will transparently encrypt certain sensitive files when you push them so that locally (i.e. on any machine which has your certificate) you can use the settings file, but Git or Dropbox or whoever is storing your files under VC does not have the ability to read the information in plaintext.Tutorial on Transparent Encryption/Decryption during Push/PullThis gist https://gist.github.com/873637 shows a tutorial on how to use the Git's smudge/clean filter driver with openssl to transparently encrypt pushed files. You just need to do some initial setup.Summary of How it WorksYou'll basically be creating a .gitencrypt folder containing 3 bash scripts,which are used by Git for decryption, encryption, and supporting Git diff. A master passphrase and salt (fixed!) is defined inside these scripts and you MUST ensure that .gitencrypt is never actually pushed.

Example clean_filter_openssl script:Similar for smudge_filter_open_ssl and diff_filter_oepnssl. See Gist.Your repo with sensitive information should have a .gitattribute file (unencrypted and included in repo) which references the .gitencrypt directory (which contains everything Git needs to encrypt/decrypt the project transparently) and which is present on your local machine..gitattribute contents:Finally, you will also need to add the following content to your .git/config fileNow, when you push the repository containing your sensitive information to a remote repository, the files will be transparently encrypted. When you pull from a local machine which has the .gitencrypt directory (containing your passphrase), the files will be transparently decrypted.NotesI should note that this tutorial does not describe a way to only encrypt your sensitive settings file. This will transparently encrypt the entire repository that is pushed to the remote VC host and decrypt the entire repository so it is entirely decrypted locally. To achieve the behavior you want, you could place sensitive files for one or many projects in one sensitive_settings_repo. You could investigate how this transparent encryption technique works with Git submodules http://git-scm.com/book/en/Git-Tools-Submodules if you really need the sensitive files to be in the same repository.The use of a fixed passphrase could theoretically lead to brute-force vulnerabilities if attackers had access to many encrypted repos/files. IMO, the probability of this is very low. As a note at the bottom of this tutorial mentions, not using a fixed passphrase will result in local versions of a repo on different machines always showing that changes have occurred with 'git status'.Heroku pushes the use of environment variables for settings and secret keys:With Foreman and .env files Heroku provide an enviable toolchain to export, import and synchronise environment variables.Personally, I believe it's wrong to save secret keys alongside code. It's fundamentally inconsistent with source control, because the keys are for services extrinsic to the the code. The one boon would be that a developer can clone HEAD and run the application without any setup. However, suppose a developer checks out a historic revision of the code. Their copy will include last year's database password, so the application will fail against today's database.With the Heroku method above, a developer can checkout last year's app, configure it with today's keys, and run it successfully against today's database. The cleanest way in my opinion is to use environment variables. You won't have to deal with .dist files for example, and the project state on the production environment would be the same as your local machine's. I recommend reading The Twelve-Factor App's config chapter, the others too if you're interested.   An option would be to put project-bound credentials into an encrypted container (TrueCrypt or Keepass) and push it.Update as answer from my comment below:Interesting question btw. I just found this: github.com/shadowhand/git-encrypt which looks very promising for automatic encryptionI suggest using configuration files for that and to not version them.You can however version examples of the files.I don't see any problem of sharing development settings. By definition it should contain no valuable data.BlackBox was recently released by StackExchange and while I have yet to use it, it seems to exactly address the problems and support the features requested in this question.From the description on https://github.com/StackExchange/blackbox:Since asking this question I have settled on a solution, which I use when developing small application with a small team of people.git-cryptgit-crypt uses GPG to transparently encrypt files when their names match certain patterns. For intance, if you add to your .gitattributes file......then a file like config.secret.json will always be pushed to remote repos with encryption, but remain unencrypted on your local file system.If I want to add a new GPG key (a person) to your repo which can decrypt the protected files then run git-crypt add-gpg-user <gpg_user_key>. This creates a new commit. The new user will be able to decrypt subsequent commits.No, just don't, even if it's your private repo and you never intend to share it, don't.You should create a local_settings.py put it on VCS ignore and in your settings.py do something likeIf your secrets settings are that versatile, I am eager to say you're doing something wrongEDIT: I assume you want to keep track of your previous passwords versions - say, for a script that would prevent password reusing etc.I think GnuPG is the best way to go - it's already used in one git-related project (git-annex) to encrypt repository contents stored on cloud services. GnuPG (gnu pgp) provides a very strong key-based encryption.Now if your 'mypassword' file did not change then encrypting it will result with same ciphertext and it won't be added to the index (no redundancy). Slightest modification of mypassword results in radically different ciphertext and mypassword.gpg in staging area differs a lot from the one in repository, thus will be added to the commit. Even if the attacker gets a hold of your gpg key he still needs to bruteforce the password. If the attacker gets an access to remote repository with ciphertext he can compare a bunch of ciphertexts, but their number won't be sufficient to give him any non-negligible advantage.Later on you can use .gitattributes to provide an on-the-fly decryption for quit git diff of your password.Also you can have separate keys for different types of passwords etc.Usually, i seperate password as a config file. and make them dist.And when i run main.py, put the real password in default.cfg that copied.ps. when you work with git or hg. you can ignore *.cfg files to make .gitignore or .hgignoreProvide a way to override the configThis is the best way to manage a set of sane defaults for the config you checkin without requiring the config be complete, or contain things like hostnames and credentials.  There are a few ways to override default configs.Environment variables (as others have already mentioned) are one way of doing it.The best way is to look for an external config file that overrides the default config values.  This allows you to manage the external configs via a configuration management system like Chef, Puppet or Cfengine.  Configuration management is the standard answer for the management of configs separate from the codebase so you don't have to do a release to update the config on a single host or a group of hosts.FYI: Encrypting creds is not always a best practice, especially in a place with limited resources.  It may be the case that encrypting creds will gain you no additional risk mitigation and simply add an unnecessary layer of complexity.  Make sure you do the proper analysis before making a decision.Encrypt the passwords file, using for example GPG. Add the keys on your local machine and on your server. Decrypt the file and put it outside your repo folders.I use a passwords.conf, located in my homefolder. On every deploy this file gets updated. No, private keys and passwords do not fall under revision control. There is no reason to  burden everyone with read access to your repository with knowing sensitive service credentials used in production, when most likely not all of them should have access to those services.Starting with Django 1.4, your Django projects now ship with a project.wsgi module that defines the application object and it's a perfect place to start enforcing the use of a project.local settings module that contains site-specific configurations. This settings module is ignored from revision control, but it's presence is required when running your project instance as a WSGI application, typical for production environments. This is how it should look like:Now you can have a local.py module who's owner and group can be configured so that only authorized personnel and the Django processes can read the file's contents.If you need VCS for your secrets you should at least keep them in a second repository seperated from you actual code. So you can give your team members access to the source code repository and they won't see your credentials. Furthermore host this repository somewhere else (eg. on your own server with an encrypted filesystem, not on github) and for checking it out to the production system you could use something like git-submodule.Another approach could be to completely avoid saving secrets in version control systems and instead use a tool like vault from hashicorp, a secret storage with key rolling and auditing, with an API and embedded encryption.This is what I do:You could use EncFS if your system provides that. Thus you could keep your encrypted data as a subfolder of your repository, while providing your application a decrypted view to the data mounted aside. As the encryption is transparent, no special operations are needed on pull or push. It would however need to mount the EncFS folders, which could be done by your application based on an password stored elsewhere outside the versioned folders (eg. environment variables).

Keyboard Interrupts with python's multiprocessing Pool

Fragsworth

[Keyboard Interrupts with python's multiprocessing Pool](https://stackoverflow.com/questions/1408356/keyboard-interrupts-with-pythons-multiprocessing-pool)

How can I handle KeyboardInterrupt events with python's multiprocessing Pools? Here is a simple example:When running the code above, the KeyboardInterrupt gets raised when I press ^C, but the process simply hangs at that point and I have to kill it externally.I want to be able to press ^C at any time and cause all of the processes to exit gracefully.

2009-09-10 23:59:35Z

How can I handle KeyboardInterrupt events with python's multiprocessing Pools? Here is a simple example:When running the code above, the KeyboardInterrupt gets raised when I press ^C, but the process simply hangs at that point and I have to kill it externally.I want to be able to press ^C at any time and cause all of the processes to exit gracefully.This is a Python bug.  When waiting for a condition in threading.Condition.wait(), KeyboardInterrupt is never sent.  Repro:The KeyboardInterrupt exception won't be delivered until wait() returns, and it never returns, so the interrupt never happens.  KeyboardInterrupt should almost certainly interrupt a condition wait.Note that this doesn't happen if a timeout is specified; cond.wait(1) will receive the interrupt immediately.  So, a workaround is to specify a timeout.  To do that, replacewithor similar.From what I have recently found, the best solution is to set up the worker processes to ignore  SIGINT altogether, and confine all the cleanup code to the parent process.  This fixes the problem for both idle and busy worker processes, and requires no error handling code in your child processes.Explanation and full example code can be found at http://noswap.com/blog/python-multiprocessing-keyboardinterrupt/ and http://github.com/jreese/multiprocessing-keyboardinterrupt respectively.For some reasons, only exceptions inherited from the base Exception class are handled normally. As a workaround, you may re-raise your KeyboardInterrupt as an Exception instance:Normally you would get the following output:So if you hit ^C, you will get:Usually this simple structure works for Ctrl-C on Pool :As was stated in few similar posts:Capture keyboardinterrupt in Python without try-exceptIt seems there are two issues that make exceptions while multiprocessing annoying.  The first (noted by Glenn) is that you need to use map_async with a timeout instead of map in order to get an immediate response (i.e., don't finish processing the entire list).  The second (noted by Andrey) is that multiprocessing doesn't catch exceptions that don't inherit from Exception (e.g., SystemExit).  So here's my solution that deals with both of these:The voted answer does not tackle the core issue but a similar side effect.Jesse Noller, the author of the multiprocessing library, explains how to correctly deal with CTRL+C when using multiprocessing.Pool in a old blog post.I found, for the time being, the best solution is to not use the multiprocessing.pool feature but rather roll your own pool functionality. I provided an example demonstrating the error with apply_async as well as an example showing how to avoid using the pool functionality altogether.http://www.bryceboe.com/2010/08/26/python-multiprocessing-and-keyboardinterrupt/I'm a newbie in Python. I was looking everywhere for answer and stumble upon this and a few other blogs and youtube videos. I have tried to copy paste the author's code above and reproduce it on my python 2.7.13 in windows 7 64- bit. It's close to what I wanna achieve.I made my child processes to ignore the ControlC and make the parent process terminate. Looks like bypassing the child process does avoid this problem for me.The part starting at pool.terminate() never seems to execute.You can try using the apply_async method of a Pool object, like this:Output:An advantage of this method is that results processed before interruption will be returned in the results dictionary:Strangely enough it looks like you have to handle the KeyboardInterrupt in the children as well.  I would have expected this to work as written... try changing slowly_square to:That should work as you expected.

Logical operators for boolean indexing in Pandas

user2988577

[Logical operators for boolean indexing in Pandas](https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas)

I'm working with boolean index in Pandas.

The question is why the statement:works fine whereasexits with error?Example:

2014-01-28 20:04:04Z

I'm working with boolean index in Pandas.

The question is why the statement:works fine whereasexits with error?Example:When you sayYou are implicitly asking Python to convert (a['x']==1) and (a['y']==10) to boolean values. NumPy arrays (of length greater than 1) and Pandas objects such as Series do not have a boolean value -- in other words, they raise when used as a boolean value. That's because its unclear when it should be True or False. Some users might assume they are True if they have non-zero length, like a Python list. Others might desire for it to be True only if all its elements are True. Others might want it to be True if any of its elements are True. Because there are so many conflicting expectations, the designers of NumPy and Pandas refuse to guess, and instead raise a ValueError.Instead, you must be explicit, by calling the empty(), all() or any() method to indicate which behavior you desire.In this case, however, it looks like you do not want boolean evaluation, you want element-wise logical-and. That is what the & binary operator performs:returns a boolean array. By the way, as alexpmil notes, 

the parentheses are mandatory since & has a higher operator precedence than ==.

Without the parentheses, a['x']==1 & a['y']==10 would be evaluated as a['x'] == (1 & a['y']) == 10 which would in turn be equivalent to the chained comparison (a['x'] == (1 & a['y'])) and ((1 & a['y']) == 10). That is an expression of the form Series and Series.

The use of and with two Series would again trigger the same ValueError as above. That's why the parentheses are mandatory.Python's and, or and not logical operators are designed to work with scalars. So Pandas had to do one better and override the bitwise operators to achieve vectorized (element-wise) version of this functionality. So the following in python (exp1 and exp2 are expressions which evaluate to a boolean result)... ...will translate to...for pandas.If in the process of performing logical operation you get a ValueError, then you need to use parentheses for grouping:For example,And so on.Boolean Indexing: A common operation is to compute boolean masks through logical conditions to filter the data. Pandas provides three operators: & for logical AND, | for logical OR, and ~ for logical NOT. Consider the following setup:For df above, say you'd like to return all rows where A < 5 and B > 5. This is done by computing masks for each condition separately, and ANDing them. Overloaded Bitwise & Operator

Before continuing, please take note of this particular excerpt of the docs, which stateSo, with this in mind, element wise logical AND can be implemented with the bitwise operator &: And the subsequent filtering step is simply,The parentheses are used to override the default precedence order of bitwise operators, which have higher precedence over the conditional operators < and >. See the section of Operator Precedence in the python docs. If you do not use parentheses, the expression is evaluated incorrectly. For example, if you accidentally attempt something such as It is parsed as Which becomes, Which becomes (see the python docs on chained operator comparison),Which becomes, Which throwsSo, don't make that mistake!1Avoiding Parentheses Grouping

The fix is actually quite simple. Most operators have a corresponding bound method for DataFrames. If the individual masks are built up using functions instead of conditional operators, you will no longer need to group by parens to specify evaluation order:See the section on Flexible Comparisons.. To summarise, we haveAnother option for avoiding parentheses is to use DataFrame.query (or eval):I have extensively documented query and eval in Dynamic Expression Evaluation in pandas using pd.eval().operator.and_

Allows you to perform this operation in a functional manner. Internally calls Series.__and__ which corresponds to the bitwise operator.You won't usually need this, but it is useful to know.Generalizing: np.logical_and (and logical_and.reduce)

Another alternative is using  np.logical_and, which also does not need parentheses grouping:np.logical_and is a ufunc (Universal Functions), and most ufuncs have a reduce method. This means it is easier to generalise with logical_and if you have multiple masks to AND. For example, to AND masks m1 and m2 and m3 with &, you would have to do However, an easier option is This is powerful, because it lets you build on top of this with more complex logic (for example, dynamically generating masks in a list comprehension and adding all of them):1 - I know I'm harping on this point, but please bear with me. This is a very, very common beginner's mistake, and must be explained very thoroughly. For the df above, say you'd like to return all rows where A == 3 or B == 7.Overloaded Bitwise |    If you haven't yet, please also read the section on Logical AND above, all caveats apply here.Alternatively, this operation can be specified with operator.or_

Calls Series.__or__ under the hood.np.logical_or

For two conditions, use logical_or:For multiple masks, use logical_or.reduce:Given a mask, such as If you need to invert every boolean value (so that the end result is [False, False, True]), then you can use any of the methods below.Bitwise ~   Again, expressions need to be parenthesised.This internally calls But don't use it directly.operator.inv

Internally calls __invert__ on the Series.np.logical_not

This is the numpy variant.Note, np.logical_and can be substituted for np.bitwise_and, logical_or with bitwise_or, and logical_not with invert.It's important to realize that you cannot use any of the Python logical operators (and, or or not) on pandas.Series or pandas.DataFrames (similarly you cannot use them on numpy.arrays with more than one element). The reason why you cannot use those is because they implicitly call bool on their operands which throws an Exception because these data structures decided that the boolean of an array is ambiguous:I did cover this more extensively  in my answer to the "Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()" Q+A.However NumPy provides element-wise operating equivalents to these operators as functions that can be used on numpy.array, pandas.Series, pandas.DataFrame, or any other (conforming) numpy.array subclass:So, essentially, one should use (assuming df1 and df2 are pandas DataFrames):However in case you have boolean NumPy array, pandas Series, or pandas DataFrames you could also use the element-wise bitwise functions (for booleans they are - or at least should be - indistinguishable from the logical functions):Typically the operators are used. However when combined with comparison operators one has to remember to wrap the comparison in parenthesis because the bitwise operators have a higher precedence than the comparison operators:This may be irritating because the Python logical operators have a lower precendence than the comparison operators so you normally write a < 10 and b > 10 (where a and b are for example simple integers) and don't need the parenthesis.It is really important to stress that bit and logical operations are only equivalent for boolean NumPy arrays (and boolean Series & DataFrames). If these don't contain booleans then the operations will give different results. I'll include examples using NumPy arrays but the results will be similar for the pandas data structures:And since NumPy (and similarly pandas) does different things for boolean (Boolean or「mask」index arrays) and integer (Index arrays) indices the results of indexing will be also be different:Where the logical operator does not work for NumPy arrays, pandas Series, and pandas DataFrames. The others work on these data structures (and plain Python objects) and work element-wise.

However be careful with the bitwise invert on plain Python bools because the bool will be interpreted as integers in this context (for example ~False returns -1 and ~True returns -2).

Can you define aliases for imported modules in Python?

Jordan Parmer

[Can you define aliases for imported modules in Python?](https://stackoverflow.com/questions/706595/can-you-define-aliases-for-imported-modules-in-python)

In Python, is it possible to define an alias for an imported module?For instance:...so that is has an alias of 'short_name'.

2009-04-01 17:33:02Z

In Python, is it possible to define an alias for an imported module?For instance:...so that is has an alias of 'short_name'.also works forCheck hereorIf you've done:you can also give it an alias by:There's no reason to do it this way in code, but I sometimes find it useful in the interactive interpreter.Yes, modules can be imported under an alias name.

using as keyword.

See

How to document a method with parameter(s)?

David Andreoletti

[How to document a method with parameter(s)?](https://stackoverflow.com/questions/9195455/how-to-document-a-method-with-parameters)

How to document methods with parameters using Python's documentation strings?EDIT: 

PEP 257 gives this example:Is this the convention used by most Python developers ?I was expecting something a little bit more formal such as Environment: Python 2.7.1

2012-02-08 14:41:52Z

How to document methods with parameters using Python's documentation strings?EDIT: 

PEP 257 gives this example:Is this the convention used by most Python developers ?I was expecting something a little bit more formal such as Environment: Python 2.7.1Based on my experience, the numpy docstring conventions (PEP257 superset) are the most widely-spread followed conventions that are also supported by tools, such as Sphinx. One example:Since docstrings are free-form, it really depends on what you use to parse code to generate API documentation.I would recommend getting familiar with the Sphinx markup, since it is widely used and is becoming the de-facto standard for documenting Python projects, in part because of the excellent readthedocs.org service. To paraphrase an example from the Sphinx documentation as a Python snippet:This markup supports cross-referencing between documents and more. Note that the Sphinx documentation uses (e.g.) :py:attr: whereas you can just use :attr: when documenting from the source code.Naturally, there are other tools to document APIs. There's the more classic Doxygen which uses \param commands but those are not specifically designed to document Python code like Sphinx is.Note that there is a similar question with a similar answer in here...Conventions:Tools:Update: Since Python 3.5 you can use type hints which is a compact, machine-readable syntax:The main advantage of this syntax is that it is defined by the language and that it's unambiguous, so tools like PyCharm can easily take advantage from it.python doc strings are free-form, you can document it in any way you like.Examples:Now, there are some conventions, but python doesn't enforce any of them. Some projects have their own conventions. Some tools to work with docstrings also follow specific conventions.If you plan to use Sphinx to document your code, it is capable of producing nicely formatted HTML docs for your parameters with their 'signatures' feature.  http://sphinx-doc.org/domains.html#signaturesThe mainstream is, as other answers here already pointed out, probably going with the Sphinx way so that you can use Sphinx to generate those fancy documents later.That being said, I personally go with inline comment style occasionally.One more example here, with some tiny details documented inline:The benefits (as @mark-horvath already pointed out in another comment) are:Now, some may think this style looks "ugly". But I would say "ugly" is a subjective word. A more neutual way is to say, this style is not mainstream so it may look less familiar to you, thus less comfortable. Again, "comfortable" is also a subjective word. But the point is, all the benefits described above are objective. You can not achieve them if you follow the standard way.Hopefully some day in the future, there will be a doc generator tool which can also consume such inline style. That will drive the adoption.PS: This answer is derived from my own preference of using inline comments whenever I see fit. I use the same inline style to document a dictionary too.Building upon the type-hints answer (https://stackoverflow.com/a/9195565/2418922), which provides a better structured way to document types of parameters, there exist also a structured manner to document both type and descriptions of parameters:example adopted from: https://pypi.org/project/autocommand/Docstrings are only useful within interactive environments, e.g. the Python shell. When documenting objects that are not going to be used interactively (e.g. internal objects, framework callbacks), you might as well use regular comments. Here’s a style I use for hanging indented comments off items, each on their own line, so you know that the comment is applying to:You can’t do this sort of thing with docstrings.

How to break a line of chained methods in Python?

Juliusz Gonera

[How to break a line of chained methods in Python?](https://stackoverflow.com/questions/4768941/how-to-break-a-line-of-chained-methods-in-python)

I have a line of the following code (don't blame for naming conventions, they are not mine):I don't like how it looks like (not too readable) but I don't have any better idea to limit lines to 79 characters in this situation. Is there a better way of breaking it (preferably without backslashes)?

2011-01-22 16:14:20Z

I have a line of the following code (don't blame for naming conventions, they are not mine):I don't like how it looks like (not too readable) but I don't have any better idea to limit lines to 79 characters in this situation. Is there a better way of breaking it (preferably without backslashes)?You could use additional parenthesis:This is a case where a line continuation character is preferred to open parentheses.  The need for this style becomes more obvious as method names get longer and as methods start taking arguments:PEP 8 is intend to be interpreted with a measure of common-sense and an eye for both the practical and the beautiful.  Happily violate any PEP 8 guideline that results in ugly or hard to read code.That being said, if you frequently find yourself at odds with PEP 8, it may be a sign that there are readability issues that transcend your choice of whitespace :-)My personal choice would be:Just store the intermediate result/object and invoke the next method on it,

e.g.It's a bit of a different solution than provided by others but a favorite of mine since it leads to nifty metaprogramming sometimes.This is a nice technique for building searches.  Go through a list of conditionals to mine from your complex query form (or string-based deductions about what the user is looking for), then just explode the dictionary into the filter.According to Python Language Reference

You can use a backslash.

Or simply break it. If a bracket is not paired, python will not treat that as a line. And under such circumstance, the indentation of following lines doesn't matter.You seems using SQLAlchemy, if it is true, sqlalchemy.orm.query.Query.filter_by() method takes multiple keyword arguments, so you could write like:But it would be better:I like to indent the arguments by two blocks, and the statement by one block, like these:

How to create an object for a Django model with a many to many field?

Sai Krishna

[How to create an object for a Django model with a many to many field?](https://stackoverflow.com/questions/6996176/how-to-create-an-object-for-a-django-model-with-a-many-to-many-field)

My model:I want to save both user1 and user2 in that model:I know that's wrong, but I'm sure you get what I want to do. How would you do it ?

2011-08-09 12:26:46Z

My model:I want to save both user1 and user2 in that model:I know that's wrong, but I'm sure you get what I want to do. How would you do it ?You cannot create m2m relations from unsaved objects. If you have the pks, try this:Update: After reading the saverio's answer, I decided to investigate the issue a bit more in depth. Here are my findings.This was my original suggestion. It works, but isn't optimal. (Note: I'm using Bars and a Foo instead of Users and a Sample, but you get the idea).It generates a whopping total of 7 queries:I'm sure we can do better. You can pass multiple objects to the add() method:As we can see, passing multiple objects saves one SELECT:I wasn't aware that you can also assign a list of objects:Unfortunately, that creates one additional SELECT:Let's try to assign a list of pks, as saverio suggested:As we don't fetch the two Bars, we save two SELECT statements, resulting in a total of 5:And the winner is:Passing pks to add() gives us a total of 4 queries:For future visitors, you can create an object and all of its m2m objects in 2 queries using the new bulk_create in django 1.4. Note that this is only usable if you don't require any pre or post-processing on the data with save() methods or signals. What you insert is exactly what will be in the DBYou can do this without specifying a "through" model on the field. For completeness, the example below creates a blank Users model to mimic what the original poster was asking.Now, in a shell or other code, create 2 users, create a sample object, and bulk add the users to that sample object.Django 1.9

A quick example:RelatedObjectManagers are different "attributes" than fields in a Model. The simplest way to achieve what you are looking for isThat's the same as assigning a User list, without the additional queries and the model building.If the number of queries is what bothers you (instead of simplicity), then the optimal solution requires three queries:This will work because we already know that the 'users' list is empty, so we can create mindlessly.You could replace the set of related objects in this way (new in Django 1.9):

Find an element in a list of tuples

Bruce

[Find an element in a list of tuples](https://stackoverflow.com/questions/2191699/find-an-element-in-a-list-of-tuples)

I have a list 'a'I need to find all the tuples for a particular number. say for 1 it will beHow do I do that?

2010-02-03 12:08:58Z

I have a list 'a'I need to find all the tuples for a particular number. say for 1 it will beHow do I do that?If you just want the first number to match you can do it like this:If you are just searching for tuples with 1 in them:There is actually a clever way to do this that is useful for any list of tuples where the size of each tuple is 2: you can convert your list into a single dictionary.For example,Read up on List ComprehensionsAlso read up up generator functions and the yield statement.Using filter function:[(1, 2), (1, 4)]Or takewhile, ( addition to this, example of more values is shown ):if unsorted, like:The filter function can also provide an interesting solution:which searches the tuples in list for any occurrence of 1. If the search is limited to the first element, the solution can be modified into: if you want to search tuple for any number which is present in tuple then you can useYou can also use if i==j[0] or i==j[index] if you want to search a number in particular index 

How do I get the different parts of a Flask request's url?

Dogukan Tufekci

[How do I get the different parts of a Flask request's url?](https://stackoverflow.com/questions/15974730/how-do-i-get-the-different-parts-of-a-flask-requests-url)

I want to detect if the request came from the localhost:5000 or foo.herokuapp.com host and what path was requested.  How do I get this information about a Flask request?

2013-04-12 15:02:05Z

I want to detect if the request came from the localhost:5000 or foo.herokuapp.com host and what path was requested.  How do I get this information about a Flask request?You can examine the url through several Request fields:You can easily extract the host part with the appropriate splits.another example:request:then:you should try:It suppose to work always, even on localhost (just did it).If you are using Python, I would suggest by exploring the request object:dir(request)Since the object support the method dict:request.__dict__It can be printed or saved. I use it to log 404 codes in Flask:

How do I calculate square root in Python?

Merlin

[How do I calculate square root in Python?](https://stackoverflow.com/questions/9595135/how-do-i-calculate-square-root-in-python)

Why does Python give the "wrong" answer?   Yes, I know import math and use sqrt. But I'm looking for an answer to the above.

2012-03-07 02:48:51Z

Why does Python give the "wrong" answer?   Yes, I know import math and use sqrt. But I'm looking for an answer to the above.sqrt=x**(1/2) is doing integer division. 1/2 == 0.So you're computing x(1/2) in the first instance, x(0) in the second.So it's not wrong, it's the right answer to a different question.You have to write: sqrt = x**(1/2.0), otherwise an integer division is performed and the expression 1/2 returns 0.This behavior is "normal" in Python 2.x, whereas in Python 3.x 1/2 evaluates to 0.5. If you want your Python 2.x code to behave like 3.x w.r.t. division write from __future__ import division - then 1/2 will evaluate to 0.5 and for backwards compatibility, 1//2 will evaluate to 0.And for the record, the preferred way to calculate a square root is this:It is a trivial addition to the answer chain. However since the Subject is very common google hit, this deserves to be added, I believe./ performs an integer division in Python 2:If one of the numbers is a float, it works as expected:What you're seeing is integer division. To get floating point division by default, Or, you could convert 1 or 2 of 1/2 into a floating point value.This might be a little late to answer but most simple and accurate way to compute square root is newton's method.You have a number which you want to compute its square root (num) and you have a guess of its square root (estimate). Estimate can be any number bigger than 0, but a number that makes sense shortens the recursive call depth significantly.This line computes a more accurate estimate with those 2 parameters. You can pass new_estimate value to the function and compute another new_estimate which is more accurate than the previous one or you can make a recursive function definition like this.For example we need to find 30's square root. We know that the result is between 5 and 6.    number is 30 and estimate is 5. The result from each recursive calls are:The last result is the most accurate computation of the square root of number. It is the same value as the built-in function math.sqrt().Perhaps a simple way to remember: add a dot after the numerator (or denominator)You can use NumPy to calculate square roots of arrays:I hope the below mentioned code will answer your question.

Python: What does the slash mean in help() output?

Joschua

[Python: What does the slash mean in help() output?](https://stackoverflow.com/questions/24735311/python-what-does-the-slash-mean-in-help-output)

What does the / mean in Python 3.4's help output for range before the closing parenthesis?

2014-07-14 11:15:32Z

