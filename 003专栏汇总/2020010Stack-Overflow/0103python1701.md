matplotlib-somehow-optimized-iteration vs simple-list-iterationLet's make N independent comparisons (N pairs of point and polygon)?Result:Matplotlib is still much better, but not 100 times better.

Now let's try much simpler polygon...result:I will just leave it here, just rewrote the code above using numpy, maybe somebody finds it useful:Wrapped ray_tracing into Tested on 100000 points, results:

Convert Bytes to Floating Point Numbers in Python

Cristian

[Convert Bytes to Floating Point Numbers in Python](https://stackoverflow.com/questions/5415/convert-bytes-to-floating-point-numbers-in-python)

I have a binary file that I have to parse and I'm using Python. Is there a way to take 4 bytes and convert it to a single precision floating point number?

2008-08-07 22:24:27Z

I have a binary file that I have to parse and I'm using Python. Is there a way to take 4 bytes and convert it to a single precision floating point number?Just a little addition, if you want a float number as output from the unpack method instead of a tuple just write If you have more floats then just write

regexes: How to access multiple matches of a group? [duplicate]

Tom Scrace

[regexes: How to access multiple matches of a group? [duplicate]](https://stackoverflow.com/questions/5060659/regexes-how-to-access-multiple-matches-of-a-group)

I am putting together a fairly complex regular expression. One part of the expression matches strings such as '+a', '-57' etc. A + or a - followed by any number of letters or numbers. I want to match 0 or more strings matching this pattern.This is the expression I came up with:If I were to search the string '-56+a' using this pattern I would expect to get two matches:+a and -56However, I only get the last match returned:Looking at the python docs I see that:So, my question is: how do you access multiple group matches?

2011-02-20 22:52:15Z

I am putting together a fairly complex regular expression. One part of the expression matches strings such as '+a', '-57' etc. A + or a - followed by any number of letters or numbers. I want to match 0 or more strings matching this pattern.This is the expression I came up with:If I were to search the string '-56+a' using this pattern I would expect to get two matches:+a and -56However, I only get the last match returned:Looking at the python docs I see that:So, my question is: how do you access multiple group matches?Drop the * from your regex (so it matches exactly one instance of your pattern). Then use either re.findall(...) or re.finditer (see here) to return all matches. Update:It sounds like you're essentially building a recursive descent parser. For relatively simple parsing tasks, it is quite common and entirely reasonable to do that by hand. If you're interested in a library solution (in case your parsing task may become more complicated later on, for example), have a look at pyparsing.The regex module fixes this, by adding a .captures method:

Python db-api: fetchone vs fetchmany vs fetchall

Alex Q

[Python db-api: fetchone vs fetchmany vs fetchall](https://stackoverflow.com/questions/5189997/python-db-api-fetchone-vs-fetchmany-vs-fetchall)

I just had a discussion today with some coworkers about python's db-api fetchone vs fetchmany vs fetchall.I'm sure the use case for each of these is dependent on the implementation of the db-api that I'm using, but in general what are the use cases for fetchone vs fetchmany vs fetchall?In other words are the following equivalent? or is there one of these that is preferred over the others? and if so in which situations?

2011-03-04 05:18:39Z

I just had a discussion today with some coworkers about python's db-api fetchone vs fetchmany vs fetchall.I'm sure the use case for each of these is dependent on the implementation of the db-api that I'm using, but in general what are the use cases for fetchone vs fetchmany vs fetchall?In other words are the following equivalent? or is there one of these that is preferred over the others? and if so in which situations?I think it indeed depends on the implementation, but you can get an idea of the differences by looking into MySQLdb sources. Depending on the options, mysqldb fetch* keep the current set of rows in memory or server side, so fetchmany vs fetchone has some flexibility here to know what to keep in (python's) memory and what to keep db server side.PEP 249 does not give much detail, so I guess this is to optimize things depending on the database while exact semantics are implementation-defined.fetchone()Fetch the next row of a query result set, returning a single tuple, or None when no more data is available:A ProgrammingError is raised if the previous call to execute*() did not produce any result set or no call was issued yet.fetchmany([size=cursor.arraysize])Fetch the next set of rows of a query result, returning a list of tuples. An empty list is returned when no more rows are available.The number of rows to fetch per call is specified by the parameter. If it is not given, the cursor’s arraysize determines the number of rows to be fetched. The method should try to fetch as many rows as indicated by the size parameter. If this is not possible due to the specified number of rows not being available, fewer rows may be returned:A ProgrammingError is raised if the previous call to execute*() did not produce any result set or no call was issued yet.Note there are performance considerations involved with the size parameter. For optimal performance, it is usually best to use the arraysize attribute. If the size parameter is used, then it is best for it to retain the same value from one fetchmany() call to the next.List itemfetchall()Fetch all (remaining) rows of a query result, returning them as a list of tuples. An empty list is returned if there is no more record to fetch.A ProgrammingError is raised if the previous call to execute*() did not produce any result set or no call was issued yet.These are implementation specific. Will get all the results from the table. This will work better when size of the table is small. If the table size is bigger, fetchall will fail in those cases. Will use most of the memory.Will cause some issues will can occur if the queries is done on network. fetchmany will get only required number of results. You can yield the results and process. Simple Snippet of implementation of fetchmany.

Python: self.__class__ vs. type(self) [duplicate]

Martin Schulze

[Python: self.__class__ vs. type(self) [duplicate]](https://stackoverflow.com/questions/10386166/python-self-class-vs-typeself)

I'm wondering if there is a difference betweenand?Is there a reason to prefer one or the other? (In my use case I want to use it to determine the logger name, but I guess this doesn't matter)

2012-04-30 15:22:50Z

I'm wondering if there is a difference betweenand?Is there a reason to prefer one or the other? (In my use case I want to use it to determine the logger name, but I guess this doesn't matter)So those two are the same. I would use self.__class__ since it's more obvious what it is.However, type(t) won't work for old-style classes since the type of an instance of an old-style class is instance while the type of a new-style class instance is its class:As far as I am aware, the latter is just a nicer way of doing the former. It's actually not that unusual in Python, consider repr(x), which just calls x.__repr__() or len(x), which just calls x.__len__(). Python prefers to use built-ins for common functions that you are likely to use over a range of classes, and generally implements these by calling __x__() methods.

Sort cProfile output by percall when profiling a Python script

Brandon O'Rourke

[Sort cProfile output by percall when profiling a Python script](https://stackoverflow.com/questions/10326936/sort-cprofile-output-by-percall-when-profiling-a-python-script)

I'm using python -m cProfile -s calls myscript.pypython -m cProfile -s percall myscript.py does not work.The Python documentation says "Look in the Stats documentation for valid sort values.": http://docs.python.org/library/profile.html#module-cProfile, which I cannot find.

2012-04-26 03:45:33Z

I'm using python -m cProfile -s calls myscript.pypython -m cProfile -s percall myscript.py does not work.The Python documentation says "Look in the Stats documentation for valid sort values.": http://docs.python.org/library/profile.html#module-cProfile, which I cannot find.-s only uses the keys found under sort_stats.Here's an example

Python imports for tests using nose - what is best practice for imports of modules above current package

leonigmig

[Python imports for tests using nose - what is best practice for imports of modules above current package](https://stackoverflow.com/questions/6670275/python-imports-for-tests-using-nose-what-is-best-practice-for-imports-of-modul)

This is a question which is asked frequently in different forms, and often obtains "lol you're not doing it properly" responses. Pretty sure that's because there's a common sense scenario people (including me) are trying to use as an implementation, and the solution is not obvious (if you've not done it before).Would accept an answer which "lets the fly out of the bottle".GivenWhere tests_sut.py starts:Running nosetests in the root dir leads to:Avenues traveled:a) do a relative using b) add root of project to PYTHONPATHc) use the to add the .. path before the imports at the start of each test module.d) just remember to do a on the project to install the modules into the site-packages before running tests.So the requirement is to have tests located beneath the test package root which have access to the project. Each of the above don't feel "natural" to me, have proved problematic or seem like too much hard work!In java this works, but basically by dint of your build tool / IDE placing all your classes on the classpath. Perhaps the issue is I'm expecting "magic" from Python? Have noted in the Flask webframework tests, option d) seems to be preferred.In any case, statements below recommending a preferred solution would remove the feeling of "unnaturalness" in my own.

2011-07-12 19:57:39Z

This is a question which is asked frequently in different forms, and often obtains "lol you're not doing it properly" responses. Pretty sure that's because there's a common sense scenario people (including me) are trying to use as an implementation, and the solution is not obvious (if you've not done it before).Would accept an answer which "lets the fly out of the bottle".GivenWhere tests_sut.py starts:Running nosetests in the root dir leads to:Avenues traveled:a) do a relative using b) add root of project to PYTHONPATHc) use the to add the .. path before the imports at the start of each test module.d) just remember to do a on the project to install the modules into the site-packages before running tests.So the requirement is to have tests located beneath the test package root which have access to the project. Each of the above don't feel "natural" to me, have proved problematic or seem like too much hard work!In java this works, but basically by dint of your build tool / IDE placing all your classes on the classpath. Perhaps the issue is I'm expecting "magic" from Python? Have noted in the Flask webframework tests, option d) seems to be preferred.In any case, statements below recommending a preferred solution would remove the feeling of "unnaturalness" in my own.You have answered your question pretty well already..

D (install to system location) is preferred for distributable code. I usually use C (modify sys.path) because I don't want system-wide installs of my hundreds of custom libs. In theory A (relative import) seems nicer, but there are cases where it fails. 

B (PYTHONPATH) is right out, really only for testing purposes in my opinion.That pretty much sums up all of the options. The option you prefer (Python magically knows where to look) is really not a workable solution because it can lead to unpredictable results, such as automagically finding libraries from unrelated projects. In my opinion, the best thing to do is put this at the entry point(s) to your program:I had the same problem and found an answer in a related question work for me. I know there is a answer checked and I still think it's a good reason to share other alternatives :)There is a nose-pathmunge giving you a control to set sys.path while invoking nosestests.

How do I get a decimal value when using the division operator in Python?

Ray Vega

[How do I get a decimal value when using the division operator in Python?](https://stackoverflow.com/questions/117250/how-do-i-get-a-decimal-value-when-using-the-division-operator-in-python)

For example, the standard division symbol '/' rounds to zero:However, I want it to return 0.04. What do I use?

2008-09-22 20:06:16Z

For example, the standard division symbol '/' rounds to zero:However, I want it to return 0.04. What do I use?There are three options:which is the same behavior as the C, C++, Java etc, or You can also activate this behavior by passing the argument -Qnew to the Python interpreter:The second option will be the default in Python 3.0. If you want to have the old integer division, you have to use the // operator. Edit: added section about -Qnew, thanks to ΤΖΩΤΖΙΟΥ!Other answers suggest how to get a floating-point value.  While this wlil be close to what you want, it won't be exact:If you actually want a decimal value, do this:That will give you an object that properly knows that 4 / 100 in base 10 is "0.04".  Floating-point numbers are actually in base 2, i.e. binary, not decimal.Make one or both of the terms a floating point number, like so:Alternatively, turn on the feature that will be default in Python 3.0, 'true division', that does what you want. At the top of your module or script, do:You need to tell Python to use floating point values, not integers. You can do that simply by using a decimal point yourself in the inputs:You might want to look at Python's decimal package, also.  This will provide nice decimal results.Try 4.0/100A simple route 4 / 100.0 or4.0 / 100Here we have two possible cases given below    You could also try adding a ".0" at the end of the number.4.0/100.0You cant get a decimal value by dividing one integer with another, you'll allways get an integer that way (result truncated to integer). You need at least one value to be a decimal number.Add the following function in your code with its callback.This function works on the basis of "Euclid Division Algorithm". This function is very useful if you don't want to import any external header files in your project.Syntex : divide([divident], [divisor], [decimal place(optional))Code : divide(1, 7, 10) OR divide(1, 7)Comment below for any queries.It's only dropping the fractional part after decimal.

Have you tried :  4.0 / 100Import division from future library like this:  

User-friendly time format in Python?

flybywire

[User-friendly time format in Python?](https://stackoverflow.com/questions/1551382/user-friendly-time-format-in-python)

Python: I need to show file modification times in the "1 day ago", "two hours ago", format.Is there something ready to do that? It should be in English.

2009-10-11 18:28:45Z

Python: I need to show file modification times in the "1 day ago", "two hours ago", format.Is there something ready to do that? It should be in English.The code was originally published on a blog post "Python Pretty Date function" (http://evaisse.com/post/93417709/python-pretty-date-function)It is reproduced here as the blog account has been suspended and the page is no longer available.If you happen to be using Django, then new in version 1.4 is the naturaltime template filter.To use it, first add 'django.contrib.humanize' to your INSTALLED_APPS setting in settings.py, and {% load humanize %} into the template you're using the filter in.Then, in your template, if you have a datetime variable my_date, you can print its distance from the present by using {{ my_date|naturaltime }}, which will be rendered as something like 4 minutes ago.Other new things in Django 1.4.Documentation for naturaltime and other filters in the django.contrib.humanize set.In looking for the same thing with the additional requirement that it handle future dates, I found this: 

http://pypi.python.org/pypi/py-pretty/1Example code (from site):You can also do that with arrow packageFrom github page:The answer Jed Smith linked to is good, and I used it for a year or so, but I think it could be improved in a few ways:Here's what I came up with:Notice how every tuple in intervals is easy to interpret and check: a 'minute' is 60 seconds; an 'hour' is 60 minutes; etc.  The only fudge is setting weeks_per_month to its average value; given the application, that should be fine.  (And note that it's clear at a glance that the last three constants multiply out to 365.242, the number of days per year.)One downside to my function is that it doesn't do anything outside the "## units" pattern: "Yesterday", "just now", etc. are right out.  Then again, the original poster didn't ask for these fancy terms, so I prefer my function for its succinctness and the readability of its numerical constants. :)The ago package provides this.  Call human on a datetime object to get a human readable description of the difference. There is humanize package:It supports localization l10n, internationalization i18n:Using datetime objects with tzinfo:I have written a detailed blog post for the solution on http://sunilarora.org/17329071

I am posting a quick snippet here as well.This is the gist of @sunil 's postYou can download and install from below link. It should be more helpful for you. It has been providing user friendly message from second to year.It's well tested.https://github.com/nareshchaudhary37/timestamp_contentBelow steps to install into your virtual env.Here is an updated answer based on Jed Smith's implementation that properly hands both offset-naive and offset-aware datetimes. You can also give a default timezones. Python 3.5+.

Python: How to remove empty lists from a list? [duplicate]

SandyBr

[Python: How to remove empty lists from a list? [duplicate]](https://stackoverflow.com/questions/4842956/python-how-to-remove-empty-lists-from-a-list)

I have a list with empty lists in it:How can I remove the empty lists so that I get:I tried list.remove('') but that doesn't work.

2011-01-30 12:49:03Z

I have a list with empty lists in it:How can I remove the empty lists so that I get:I tried list.remove('') but that doesn't work.TryIf you want to get rid of everything that is "falsy", e.g. empty strings, empty tuples, zeros, you could also useYou can use filter() instead of a list comprehension:If None is used as first argument to filter(), it filters out every value in the given list, which is False in a boolean context.  This includes empty lists.It might be slightly faster than the list comprehension, because it only executes a single function in Python, the rest is done in C.Calling filter with None will filter out all falsey values from the list (which an empty list is)A few options:sample session:I found this question because I wanted to do the same as the OP.

I would like to add the following observation:The iterative way (user225312, Sven Marnach):Will return a list object in python3 and python2 . Instead the filter way (lunaryorn, Imran) will differently behave over versions:It will return a filter object in python3 and a list in python2 (see this question found at the same time). This is a slight difference but it must be take in account when developing compatible scripts.This does not make any assumption about performances of those solutions. Anyway the filter object can be reverted to a list using:output:Output:Adding to the answers above, Say you have a list of lists of the form:and you want to take out the empty entries from each list as well as the empty lists you can do:Your new list will look like

What's the shortest way to count the number of items in a generator/iterator?

Fred Foo

[What's the shortest way to count the number of items in a generator/iterator?](https://stackoverflow.com/questions/5384570/whats-the-shortest-way-to-count-the-number-of-items-in-a-generator-iterator)

If I want the number of items in an iterable without caring about the elements themselves, what would be the pythonic way to get that? Right now, I would definebut I understand lambda is close to being considered harmful, and lambda _: 1 certainly isn't pretty.(The use case of this is counting the number of lines in a text file matching a regex, i.e. grep -c.)

2011-03-21 22:35:56Z

If I want the number of items in an iterable without caring about the elements themselves, what would be the pythonic way to get that? Right now, I would definebut I understand lambda is close to being considered harmful, and lambda _: 1 certainly isn't pretty.(The use case of this is counting the number of lines in a text file matching a regex, i.e. grep -c.)The usual way isMethod that's meaningfully faster than sum(1 for i in it) when the iterable may be long (and not meaningfully slower when the iterable is short), while maintaining fixed memory overhead behavior (unlike len(list(it))) to avoid swap thrashing and reallocation overhead for larger inputs:Like len(list(it)) it performs the loop in C code on CPython (deque, count and zip are all implemented in C); avoiding byte code execution per loop is usually the key to performance in CPython.It's surprisingly difficult to come up with fair test cases for comparing performance (list cheats using __length_hint__ which isn't likely to be available for arbitrary input iterables, itertools functions that don't provide __length_hint__ often have special operating modes that work faster when the value returned on each loop is released freed before the next value is requested, which deque with maxlen=0 will do). The test case I used was to create a generator function that would take an input and return a C level generator that lacked special itertools return container optimizations or __length_hint__, using Python 3.3's yield from:Then using ipython %timeit magic (substituting different constants for 100):When the input isn't large enough that len(list(it)) would cause memory issues, on a Linux box running Python 3.5 x64, my solution takes about 50% longer than def ilen(it): return len(list(it)), regardless of input length.For the smallest of inputs, the setup costs to call deque/zip/count/next means it takes infinitesimally longer this way than def ilen(it): sum(1 for x in it) (about 200 ns more on my machine for a length 0 input, which is a 33% increase over the simple sum approach), but for longer inputs, it runs in about half the time per additional element; for length 5 inputs, the cost is equivalent, and somewhere in the length 50-100 range, the initial overhead is unnoticeable compared to the real work; the sum approach takes roughly twice as long.Basically, if memory use matters or inputs don't have bounded size and you care about speed more than brevity, use this solution. If inputs are bounded and smallish, len(list(it)) is probably best, and if they're unbounded, but simplicity/brevity counts, you'd use sum(1 for x in it).A short way is:Note that if you are generating a lot of elements (say, tens of thousands or more), then putting them in a list may become a performance issue. However, this is a simple expression of the idea where the performance isn't going to matter for most cases.more_itertools is a third-party library that implements an ilen tool.  pip install more_itertoolsI like the cardinality package for this, it is very lightweight and tries to use the fastest possible implementation available depending on the iterable. Usage:These would be my choices either one or another:

Find the date for the first Monday after a given a date

ghickman

[Find the date for the first Monday after a given a date](https://stackoverflow.com/questions/6558535/find-the-date-for-the-first-monday-after-a-given-a-date)

Given a particular date, say 2011-07-02, how can I find the date of the next Monday (or any weekday day for that matter) after that date?

2011-07-02 17:32:37Z

Given a particular date, say 2011-07-02, how can I find the date of the next Monday (or any weekday day for that matter) after that date?Here's a succinct and generic alternative to the slightly weighty answers above.Tryusing, that the next monday is 7 days after the a monday, 6 days after a tuesday, and so on, and also using, that Python's datetime type reports monday as 0, ..., sunday as 6.You can start adding one day to date object and stop when it's monday.Another simple elegant solution is to use pandas offsets.

I find it very helpful and robust when playing with dates.

- If you want the first Sunday just modify the frequency to freq='W-SUN'.

- If you want a couple of next Sundays, change the offsets.Day(days).

- Using pandas offsets allow you to ignore holidays, work only with Business Days and more.

You can also apply this method easily on a whole DataFrame using apply method.This is example of calculations within ring mod 7.will print:As you see it's correctly give you next monday, tuesday, wednesday, thursday friday and saturday. And it also understood that 2018-04-15 is a sunday and returned current sunday instead of next one. I'm sure you'll find this answer extremely helpful after 7 years ;-)Another alternative uses rrulerrule docs: https://dateutil.readthedocs.io/en/stable/rrule.htmlThis will give the first next Monday after given date:2011-07-04

2015-09-07

2015-09-07via list comprehension?

Adding padding to a tkinter widget only on one side

Jack S.

[Adding padding to a tkinter widget only on one side](https://stackoverflow.com/questions/4174575/adding-padding-to-a-tkinter-widget-only-on-one-side)

How can I add padding to a tkinter window, without tkinter centering the widget?

I tried:andI want 30px padding only on the top of the label.

2010-11-13 20:37:11Z

How can I add padding to a tkinter window, without tkinter centering the widget?

I tried:andI want 30px padding only on the top of the label.The padding options padx and pady of the grid and pack methods can take a 2-tuple that represent the left/right and top/bottom padding. Here's an example:

Python/Django: how to assert that unit test result contains a certain string?

user798719

[Python/Django: how to assert that unit test result contains a certain string?](https://stackoverflow.com/questions/17536916/python-django-how-to-assert-that-unit-test-result-contains-a-certain-string)

In a python unit test (actually Django), what is the correct assert statement that will tell me if my test result contains a string of my choosing?I want to make sure that my result contains at least the json object (or string) that I specified as the second argument above

2013-07-08 22:16:52Z

In a python unit test (actually Django), what is the correct assert statement that will tell me if my test result contains a string of my choosing?I want to make sure that my result contains at least the json object (or string) that I specified as the second argument aboveYou can modify it to work with json.Use self.assertContains only for HttpResponse objects. For other objects, use self.assertIn.To assert if a string is or is not a substring of another, you should use assertIn and assertNotIn:These are new since Python 2.7 and Python 3.1You can write assertion about expected part of string in another string with a simple assertTrue + in python keyword :Build a JSON object using json.dumps().Then compare them using assertEqual(result, your_json_dict)As mentioned by Ed I, assertIn is probably the simplest answer to finding one string in another. However, the question states:Therefore I would use multiple assertions so that helpful messages are received on failure - tests will have to be understood and maintained in the future, potentially by someone that didn't write them originally. Therefore assuming we're inside a django.test.TestCase:Which gives helpful messages as follows:I found myself in a similar problem and I used the attribute rendered_content, so I wroteassertTrue('string' in response.rendered_content) and similarlyassertFalse('string' in response.rendered_content) if I want to test that a string is not renderedBut it also worked the early suggested method,AssertContains(response, 'html string as rendered')So I'd say that the first one is more straightforward.

I hope it will help.

How to check if an element of a list is a list (in Python)?

rishimaharaj

[How to check if an element of a list is a list (in Python)?](https://stackoverflow.com/questions/9759930/how-to-check-if-an-element-of-a-list-is-a-list-in-python)

If we have the following list:If I need to find out if an element in the list is itself a list, what can I replace aValidList in the following code with?Is there a special import to use?  Is there a best way of checking if a variable/element is a list?

2012-03-18 16:22:47Z

If we have the following list:If I need to find out if an element in the list is itself a list, what can I replace aValidList in the following code with?Is there a special import to use?  Is there a best way of checking if a variable/element is a list?Use isinstance:If you want to check that an object is a list or a tuple, pass several classes to isinstance:You might ask "why not just use type(x) == list?" You shouldn't do that, because then you won't support things that look like lists. And part of the Python mentality is duck typing:In other words, you shouldn't require that the objects are lists, just that they have the methods you will need. The collections module provides a bunch of abstract base classes, which are a bit like Java interfaces. Any type that is an instance of collections.Sequence, for example, will support indexing.Expression you are looking for may be: Testing:Probably, more intuitive way would be like thisyou can simply write:

Test if an attribute is present in a tag in BeautifulSoup

LB40

[Test if an attribute is present in a tag in BeautifulSoup](https://stackoverflow.com/questions/5015483/test-if-an-attribute-is-present-in-a-tag-in-beautifulsoup)

I would like to get all the <script> tags in a document and then process each one based on the presence (or absence) of certain attributes.E.g., for each <script> tag, if the attribute for is present do something; else if the attribute bar is present do something else.Here is what I am doing currently:But this way I filter all the <script> tags with the for attribute... but I lost the other ones (those without the for attribute).

2011-02-16 10:50:45Z

I would like to get all the <script> tags in a document and then process each one based on the presence (or absence) of certain attributes.E.g., for each <script> tag, if the attribute for is present do something; else if the attribute bar is present do something else.Here is what I am doing currently:But this way I filter all the <script> tags with the for attribute... but I lost the other ones (those without the for attribute).If i understand well, you just want all the script tags, and then check for some attributes in them?For future reference, has_key has been deprecated is beautifulsoup 4.  Now you need to use has_attrYou don't need any lambdas to filter by attribute, you can simply use some_attribute=True in find or find_all.Here are more examples with other approaches as well:You can also use regular expressions with find or find_all:If you only need to get tag(s) with attribute(s), you can use lambda:ORThought it might be useful.you can check if some attribute are presentBy using the pprint module you can examine the contents of an element.Using this on a bs4 element will print something similar to this:To access an attribute - lets say the class list - use the following:You can filter elements using this approach:

Recovering features names of explained_variance_ratio_ in PCA with sklearn

mazieres

[Recovering features names of explained_variance_ratio_ in PCA with sklearn](https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn)

I'm trying to recover from a PCA done with scikit-learn, which features are selected as relevant.A classic example with IRIS dataset.This returnsHow can I recover which two features allow these two explained variance among the dataset ?

Said diferently, how can i get the index of this features in iris.feature_names ?Thanks in advance for your help.

2014-04-10 09:43:18Z

I'm trying to recover from a PCA done with scikit-learn, which features are selected as relevant.A classic example with IRIS dataset.This returnsHow can I recover which two features allow these two explained variance among the dataset ?

Said diferently, how can i get the index of this features in iris.feature_names ?Thanks in advance for your help.This information is included in the pca attribute: components_. As described in the documentation, pca.components_ outputs an array of [n_components, n_features], so to get how components are linearly related with the different features you have to:Note: each coefficient represents the correlation between a particular pair of component and feature IMPORTANT: As a side comment, note the PCA sign does not affect its interpretation since the sign does not affect the variance contained in each component. Only the relative signs of features forming the PCA dimension are important. In fact, if you run the PCA code again, you might get the PCA dimensions with the signs inverted. For an intuition about this, think about a vector and its negative in 3-D space - both are essentially representing the same direction in space.

Check this post for further reference.Edit: as others have commented, you may get same values from .components_ attribute.Each principal component is a linear combination of the original variables:where X_is are the original variables, and Beta_is are the corresponding weights or so called coefficients.To obtain the weights, you may simply pass identity matrix to the transform method:Each column of the coef matrix above shows the weights in the linear combination which obtains corresponding principal component:For example, above shows that the second principal component (PC-2) is mostly aligned with sepal width, which has the highest weight of 0.926 in absolute value;Since the data were normalized, you can confirm that the principal components have variance 1.0 which is equivalent to each coefficient vector having norm 1.0:One may also confirm that the principal components can be calculated as the dot product of the above coefficients and the original variables:Note that we need to use numpy.allclose instead of regular equality operator, because of floating point precision error.The way this question is phrased reminds me of a misunderstanding of Principle Component Analysis when I was first trying to figure it out. I’d like to go through it here in the hope that others won’t spend as much time on a road-to-nowhere as I did before the penny finally dropped.The notion of「recovering」feature names suggests that PCA identifies those features that are most important in a dataset. That’s not strictly true.PCA, as I understand it, identifies the features with the greatest variance in a dataset, and can then use this quality of the dataset to create a smaller dataset with a minimal loss of descriptive power. The advantages of a smaller dataset is that it requires less processing power and should have less noise in the data. But the features of greatest variance are not the "best" or "most important" features of a dataset, insofar as such concepts can be said to exist at all.To bring that theory into the practicalities of @Rafa’s sample code above:

consider the following:

In this case, post_pca_array has the same 150 rows of data as data_scaled, but data_scaled’s four columns have been reduced from four to two.The critical point here is that the two columns – or components, to be terminologically consistent – of post_pca_array are not the two「best」columns of data_scaled. They are two new columns, determined by the algorithm behind sklearn.decomposition’s PCA module. The second column, PC-2 in @Rafa’s example, is informed by sepal_width more than any other column, but the values in PC-2 and data_scaled['sepal_width'] are not the same.As such, while it’s interesting to find out how much each column in original data contributed to the components of a post-PCA dataset, the notion of「recovering」column names is a little misleading, and certainly misled me for a long time. The only situation where there would be a match between post-PCA and original columns would be if the number of principle components were set at the same number as columns in the original. However, there would be no point in using the same number of columns because the data would not have changed. You would only have gone there to come back again, as it were.Given your fitted estimator pca, the components are to be found in pca.components_, which represent the directions of highest variance in the dataset.Get the most important feature name on the PCs:This prints:So on the PC1 the feature named e is the most important and on PC2 the d.

How to strip comma in Python string

msampaio

[How to strip comma in Python string](https://stackoverflow.com/questions/16233593/how-to-strip-comma-in-python-string)

How can I strip the comma from a Python string such as Foo, bar? I tried 'Foo, bar'.strip(','), but it didn't work.

2013-04-26 09:53:49Z

How can I strip the comma from a Python string such as Foo, bar? I tried 'Foo, bar'.strip(','), but it didn't work.You want to replace it, not strip it:Use replace method of strings not strip:An example:unicode('foo,bar').translate(dict([[ord(char), u''] for char in u',']))This will strip all commas from the text and left justify it. 

How to convert XML to JSON in Python? [duplicate]

Geuis

[How to convert XML to JSON in Python? [duplicate]](https://stackoverflow.com/questions/471946/how-to-convert-xml-to-json-in-python)

I'm doing some work on App Engine and I need to convert an XML document being retrieved from a remote server into an equivalent JSON object.I'm using xml.dom.minidom to parse the XML data being returned by urlfetch. I'm also trying to use django.utils.simplejson to convert the parsed XML document into JSON. I'm completely at a loss as to how to hook the two together. Below is the code I'm tinkering with:

2009-01-23 05:12:30Z

I'm doing some work on App Engine and I need to convert an XML document being retrieved from a remote server into an equivalent JSON object.I'm using xml.dom.minidom to parse the XML data being returned by urlfetch. I'm also trying to use django.utils.simplejson to convert the parsed XML document into JSON. I'm completely at a loss as to how to hook the two together. Below is the code I'm tinkering with:Soviut's advice for lxml objectify is good. With a specially subclassed simplejson, you can turn an lxml objectify result into json.See the docstring for example of usage, essentially you pass the result of lxml objectify to the encode method of an instance of objectJSONEncoderNote that Koen's point is very valid here, the solution above only works for simply nested xml and doesn't include the name of root elements. This could be fixed.I've included this class in a gist here: http://gist.github.com/345559xmltodict (full disclosure: I wrote it) can help you convert your XML to a dict+list+string structure, following this "standard". It is Expat-based, so it's very fast and doesn't need to load the whole XML tree in memory.Once you have that data structure, you can serialize it to JSON:I think the XML format can be so diverse that it's impossible to write a code that could do this without a very strict defined XML format. Here is what I mean:Would becomeBut what would this be:See what I mean?Edit: just found this article: http://www.xml.com/pub/a/2006/05/31/converting-between-xml-and-json.htmlJacob Smullyan wrote a utility called pesterfish which uses effbot's ElementTree to convert XML to JSON.One possibility would be to use Objectify or ElementTree from the lxml module.  An older version ElementTree is also available in the python xml.etree module as well.  Either of these will get your xml converted to Python objects which you can then use simplejson to serialize the object to JSON.While this may seem like a painful intermediate step, it starts making more sense when you're dealing with both XML and normal Python objects.In general, you want to go from XML to regular objects of your language (since there are usually reasonable tools to do this, and it's the harder conversion). And then from Plain Old Object produce JSON -- there are tools for this, too, and it's a quite simple serialization (since JSON is "Object Notation", natural fit for serializing objects).

I assume Python has its set of tools.I wrote a small command-line based Python script based on pesterfesh that does exactly this:https://github.com/hay/xml2json

Find the indexes of all regex matches?

xitrium

[Find the indexes of all regex matches?](https://stackoverflow.com/questions/3519565/find-the-indexes-of-all-regex-matches)

I'm parsing strings that could have any number of quoted strings inside them (I'm parsing code, and trying to avoid PLY).  I want to find out if a substring is quoted, and I have the substrings index.  My initial thought was to use re to find all the matches and then figure out the range of indexes they represent.It seems like I should use re with a regex like \"[^\"]+\"|'[^']+' (I'm avoiding dealing with triple quoted and such strings at the moment).  When I use findall() I get a list of the matching strings, which is somewhat nice, but I need indexes.My substring might be as simple as c, and I need to figure out if this particular c is actually quoted or not.

2010-08-19 07:14:56Z

I'm parsing strings that could have any number of quoted strings inside them (I'm parsing code, and trying to avoid PLY).  I want to find out if a substring is quoted, and I have the substrings index.  My initial thought was to use re to find all the matches and then figure out the range of indexes they represent.It seems like I should use re with a regex like \"[^\"]+\"|'[^']+' (I'm avoiding dealing with triple quoted and such strings at the moment).  When I use findall() I get a list of the matching strings, which is somewhat nice, but I need indexes.My substring might be as simple as c, and I need to figure out if this particular c is actually quoted or not.This is what you want: (source)You can then get the start and end positions from the MatchObjects.e.g.

How to downcase the first character of a string?

Natim

[How to downcase the first character of a string?](https://stackoverflow.com/questions/3840843/how-to-downcase-the-first-character-of-a-string)

There is a function to capitalize a string, I would like to be able to change the first character of a string to be sure it will be lowercase.How can I do that in Python?

2010-10-01 15:52:28Z

There is a function to capitalize a string, I would like to be able to change the first character of a string to be sure it will be lowercase.How can I do that in Python?One-liner which handles empty strings and None:Interestingly, none of these answers does exactly the opposite of capitalize().  For example, capitalize('abC') returns Abc rather than AbC.  If you want the opposite of capitalize(), you need something like:Simplest way: Update See this answer (by @RichieHindle) for a more foolproof solution, including handling empty strings. That answer doesn't handle None though, so here is my take:No need to handle special cases (and I think the symmetry is more Pythonic): I'd write it this way:This has the (relative) merit that it will throw an error if you inadvertently pass it something that isn't a string, like None or an empty list.This duplicate post lead me here. If you've a list of strings like the one shown belowThen, to convert the first letter of all items in the list, you can useOutput

python zipfile module doesn't seem to be compressing my files

Ramy

[python zipfile module doesn't seem to be compressing my files](https://stackoverflow.com/questions/4166447/python-zipfile-module-doesnt-seem-to-be-compressing-my-files)

I made a little helper function:The problem is that all my files are NOT being COMPRESSED! The files are the same size and, effectively, just the extension is being change to ".zip" (from ".xls" in this case).I'm running python 2.5 on winXP sp2.

2010-11-12 15:56:53Z

I made a little helper function:The problem is that all my files are NOT being COMPRESSED! The files are the same size and, effectively, just the extension is being change to ".zip" (from ".xls" in this case).I'm running python 2.5 on winXP sp2.This is because ZipFile requires you to specify the compression method. If you don't specify it, it assumes the compression method to be zipfile.ZIP_STORED, which only stores the files without compressing them. You need to specify the method to be zipfile.ZIP_DEFLATED. You will need to have the zlib module installed for this (it is usually installed by default).There is a really easy way to compress  zip format,Use in shutil.make_archive library.For example: Can see more extensive documentation at: Here

AttributeError: can't set attribute in python

Pratyush Dhanuka

[AttributeError: can't set attribute in python](https://stackoverflow.com/questions/22562425/attributeerror-cant-set-attribute-in-python)

Here is my codeIn the last line I cant set the items[node.ind].v value to node.v as I want, and am getting the error I don't know what's wrong but it must be something based on syntax as using statements like node.v+=1 is also showing same error. I'm new to Python, so please suggest a way to make the above change possible.

2014-03-21 15:06:27Z

Here is my codeIn the last line I cant set the items[node.ind].v value to node.v as I want, and am getting the error I don't know what's wrong but it must be something based on syntax as using statements like node.v+=1 is also showing same error. I'm new to Python, so please suggest a way to make the above change possible.(Note: Don't be discouraged to use this solution because of the leading underscore in the function _replace. Specifically for namedtuple some functions have leading underscore which is not for indicating they are meant to be "private")namedtuples are immutable, just like standard tuples. You have two choices:The former would look like:And the latter:Edit: if you want the latter, Ignacio's answer does the same thing more neatly using baked-in functionality.

Only add to a dict if a condition is met

user1814016

[Only add to a dict if a condition is met](https://stackoverflow.com/questions/14263872/only-add-to-a-dict-if-a-condition-is-met)

I am using urllib.urlencode to build web POST parameters, however there are a few values I only want to be added if a value other than None exists for them.That works fine, however if I make the orange variable optional, how can I prevent it from being added to the parameters? Something like this (pseudocode):I hope this was clear enough, does anyone know how to solve this?

2013-01-10 17:34:39Z

I am using urllib.urlencode to build web POST parameters, however there are a few values I only want to be added if a value other than None exists for them.That works fine, however if I make the orange variable optional, how can I prevent it from being added to the parameters? Something like this (pseudocode):I hope this was clear enough, does anyone know how to solve this?You'll have to add the key separately, after the creating the initial dict:Python has no syntax to define a key as conditional; you could use a dict comprehension if you already had everything in a sequence:but that's not very readable.To piggyback on sqreept's answer, here's a subclass of dict that behaves as desired:This will allow values of existing keys to be changed to None, but assigning None to a key that does not exist is a no-op. If you wanted setting an item to None to remove it from the dictionary if it already exists, you could do this:Values of None can get in if you pass them in during construction. If you want to avoid that, add an __init__ method to filter them out:You could also make it generic by writing it so you can pass in the desired condition when creating the dictionary:Pretty old question but here is an alternative using the fact that updating a dict with an empty dict does nothing.You can clear None after the assignment:I did this. Hope this help.Expected output : {'orange': 10, 'apple': 23}Although, if orange = None , then there will be a single entry for None:None. For example consider this : Expected Output : {None: None, 'apple': 23}Another valid answer is that you can create you own dict-like container that doesn't store None values.yields:I really like the neat trick in the answer here: https://stackoverflow.com/a/50311983/3124256But, it has some pitfalls:To avoid this, you can do the following:Expected Output : {'apple': 23}Note: the None: None entry ensures two things: If you aren't worried about these things, you can leave it out and wrap the del in a try...except (or check if the None key is present before deling).  To address number 2 alternatively, you could also put the conditional check on the value (in addition to the key).There is a counter-intuitive but reliable hack, to reuse the other prop name you want to exclude it.In this case, the latter 'apple' will override the previous 'apple' effectively removing it. Note that the conditional expressions should go above the real ones.

anaconda/conda - install a specific package version

s5s

[anaconda/conda - install a specific package version](https://stackoverflow.com/questions/38411942/anaconda-conda-install-a-specific-package-version)

I want to install the 'rope' package in my current active environment using conda. Currently, the following 'rope' versions are available:I would like to install the following one:I've tried all sorts of permutations of 'conda install' which I'm not going to list here because none of them are correct.I am also not sure what the py35_0 is (I'm assuming this is the version of the python against which the package was built?) and I also don't know what 'defaults' means?

2016-07-16 13:57:22Z

I want to install the 'rope' package in my current active environment using conda. Currently, the following 'rope' versions are available:I would like to install the following one:I've tried all sorts of permutations of 'conda install' which I'm not going to list here because none of them are correct.I am also not sure what the py35_0 is (I'm assuming this is the version of the python against which the package was built?) and I also don't know what 'defaults' means?There is no version 1.3.0 for rope. 1.3.0 refers to the package cached-property. The highest available version of rope is 0.9.4.You can install different versions with conda install package=version. But in this case there is only one version of rope so you don't need that.The reason you see the cached-property in this listing is because it contains the string "rope": "cached-p rope erty"py35_0 means that you need python version 3.5 for this specific version. If you only have python3.4 and the package is only for version 3.5 you cannot install it with conda.I am not quite sure on the defaults either. It should be an indication that this package is inside the default conda channel.To install a specific package:eg:For version >=, >, <, <= using a single or double quoteswhere option -y, --yes  Do not ask for confirmation.

Tested on conda 4.7.12

Getting Python's unittest results in a tearDown() method

Joey Robert

[Getting Python's unittest results in a tearDown() method](https://stackoverflow.com/questions/4414234/getting-pythons-unittest-results-in-a-teardown-method)

Is it possible to get the results of a test (i.e. whether all assertions have passed) in a tearDown() method? I'm running Selenium scripts, and I'd like to do some reporting from inside tearDown(), however I don't know if this is possible.

2010-12-10 23:37:36Z

Is it possible to get the results of a test (i.e. whether all assertions have passed) in a tearDown() method? I'm running Selenium scripts, and I'd like to do some reporting from inside tearDown(), however I don't know if this is possible.CAVEAT: I have no way of double checking the following theory at the moment, being away from a dev box. So this may be a shot in the dark.Perhaps you could check the return value of sys.exc_info() inside your tearDown() method, if it returns (None, None, None), you know the test case succeeded. Otherwise, you could use returned tuple to interrogate the exception object.See sys.exc_info documentation.Another more explicit approach is to write a method decorator that you could slap onto all your test case methods that require this special handling. This decorator can intercept assertion exceptions and based on that modify some state in self allowing your tearDown method to learn what's up.If you take a look at the implementation of unittest.TestCase.run, you can see that all test results are collected in the result object (typically a unittest.TestResult instance) passed as argument. No result status is left in the unittest.TestCase object.So there isn't much you can do in the unittest.TestCase.tearDown method unless you mercilessly break the elegant decoupling of test cases and test results with something like this:EDIT: This works for Python 2.6 - 3.3,

(modified for new Python bellow).This solution is for Python versions 2.7 to 3.7 (the highest current version), without any decorators or other modification in any code before tearDown. Everything works according to the builtin classification of results. Also skipped tests or expectedFailure are recognized correctly. It evaluates the result of the current test, not a summary of all tests passed so far. Compatible also with pytest.Comments: Only one or zero exceptions (error or failure) need be reported because not more can be expected before tearDown. The package unittest expects that a second exception can be raised by tearDown. Therefore the lists errors and failures can contain only one or zero elements together before tearDown. Lines after "demo" comment are reporting a short result.Demo output: (not important)

Comparision to other solutions - (with respect to commit history of Python source repository):  Explained by Python source repository

= Lib/unittest/case.py =

Python v 2.7 - 3.3

Python v. 3.4 - 3.6Note (by reading Python commit messages): A reason why test results are so much decoupled from tests is memory leaks prevention. Every exception info can access to frames of the failed process state including all local variables. If a frame is assigned to a local variable in a code block that could also fail, then a cross memory refence could be easily created. It is not terrible, thanks to garbage collector, but the free memory can became fragmented more quickly than if the memory would be released correctly. This is a reason why exception information and traceback are converted very soon to strings and why temporary objects like self._outcome are encapsulated and are set to None in a finally block in order to memory leaks are prevented.If you are using Python2 you can use the method _resultForDoCleanups. This method return a TextTestResult object:<unittest.runner.TextTestResult run=1 errors=0 failures=0>You can use this object to check the result of your tests:If you are using Python3 you can use _outcomeForDoCleanups:Following on from amatellanes' answer, if you're on Python3.4, you can't use _outcomeForDoCleanups.  Here's what I managed to hack together:yucky, but it seems to work.It depends what kind of reporting you'd like to produce. In case you'd like to do some actions on failure (such as generating a screenshots), instead of using tearDown(), you may achieve that by overriding failureException.For example:Here's a solution for those of us who are uncomfortable using solutions that rely on unittest internals:First, we create a decorator that will set a flag on the TestCase instance to determine whether or not the test case failed or passed:This decorator is actually pretty simple.  It relies on the fact that unittest detects failed tests via Exceptions.  As far as I'm aware, the only special exception that needs to be handled is unittest.SkipTest (which does not indicate a test failure).  All other exceptions indicate test failures so we mark them as such when they bubble up to us.We can now use this decorator directly:It's going to get really annoying writing this decorator all the time.  Is there a way we can simplify?  Yes there is!*  We can write a metaclass to handle applying the decorator for us:Now we apply this to our base TestCase subclass and we're all set:There are likely a number of cases that this doesn't handle properly.  For example, it does not correctly detect failed subtests or expected failures.  I'd be interested in other failure modes of this, so if you find a case that I'm not handling properly, let me know in the comments and I'll look into it.*If there wasn't an easier way, I wouldn't have made _tag_error a private function ;-)Python 2.7.You can also get result after unittest.main():or use suite:Name of current test can be retrieved with unittest.TestCase.id() method. So in tearDown you can check self.id().  Example shows how to: Tested example here works with @scoffey 's nice example. example of result:Inspired by scoffey’s answer, I decided to take mercilessnes to the next level, and have come up with the following.It works in both vanilla unittest, and also when run via nosetests, and also works in Python versions 2.7, 3.2, 3.3, and 3.4 (I did not specifically test 3.0, 3.1, or 3.5, as I don’t have these installed at the moment, but if I read the source code correctly, it should work in 3.5 as well):When run with unittest:When run with nosetests:I started with this:However, this only works in Python 2. In Python 3, up to and including 3.3, the control flow appears to have changed a bit: Python 3’s unittest package processes results after calling each test’s tearDown() method… this behavior can be confirmed if we simply add an extra line (or six) to our test class:Then just re-run the tests:…and you will see that you get this as a result:Now, compare the above to Python 2’s output:Since Python 3 processes errors/failures after the test is torn down, we can’t readily infer the result of a test using result.errors or result.failures in every case. (I think it probably makes more sense architecturally to process a test’s results after tearing it down, however, it does make the perfectly valid use-case of following a different end-of-test procedure depending on a test’s pass/fail status a bit harder to meet…)Therefore, instead of relying on the overall result object, instead we can reference _outcomeForDoCleanups as others have already mentioned, which contains the result object for the currently running test, and has the necessary errors and failrues attributes, which we can use to infer a test’s status by the time tearDown() has been called:This adds support for the early versions of Python 3.As of Python 3.4, however, this private member variable no longer exists, and instead, a new (albeit also private) method was added: _feedErrorsToResult.This means that for versions 3.4 (and later), if the need is great enough, one can — very hackishly — force one’s way in to make it all work again like it did in version 2……provided, of course, all consumers of this class remember to super(…, self).tearDown() in their respective tearDown methods…Disclaimer: Purely educational, don’t try this at home, etc. etc. etc. I’m not particularly proud of this solution, but it seems to work well enough for the time being, and is the best I could hack up after fiddling for an hour or two on a Saturday afternoon…Tested for python 3.7 - sample code for getting info of failing assertion but can give idea how to deal with errors:In few words, this gives True if all test run so far exited with no errors or failures:Explore _outcome's properties to access more detailed possibilities.

How to download a file via FTP with Python ftplib

Intekhab Khan

[How to download a file via FTP with Python ftplib](https://stackoverflow.com/questions/11573817/how-to-download-a-file-via-ftp-with-python-ftplib)

I have the following code which easily connects to the FTP server and opens a zip file. I want to download that file into the local system. How to do that?

2012-07-20 06:15:58Z

I have the following code which easily connects to the FTP server and opens a zip file. I want to download that file into the local system. How to do that?Of course it would we be wise to handle possible errors.The ftplib module in the Python standard library can be compared to assembler. Use a high level library like: https://pypi.python.org/pypi/ftputilThis is a Python code that is working fine for me. Comments are in Spanish but the app is easy to understandPlease note if you are downloading from the FTP to your local, you will need to use the following: Otherwise, the script will at your local file storage rather than the FTP.I spent a few hours making the mistake myself.Script below:

How can I concatenate str and int objects?

Zero Piraeus

[How can I concatenate str and int objects?](https://stackoverflow.com/questions/25675943/how-can-i-concatenate-str-and-int-objects)

If I try to do the following:I get the following error in Python 3.x:... and a similar error in Python 2.x:How can I get around this problem?

2014-09-04 22:28:29Z

If I try to do the following:I get the following error in Python 3.x:... and a similar error in Python 2.x:How can I get around this problem?The problem here is that the + operator has (at least) two different meanings in Python: for numeric types, it means "add the numbers together":... and for sequence types, it means "concatenate the sequences":As a rule, Python doesn't implicitly convert objects from one type to another1 in order to make operations "make sense", because that would be confusing: for instance, you might think that '3' + 5 should mean '35', but someone else might think it should mean 8 or even '8'.Similarly, Python won't let you concatenate two different types of sequence:Because of this, you need to do the conversion explicitly, whether what you want is concatenation or addition:However, there is a better way. Depending on which version of Python you use, there are three different kinds of string formatting available2, which not only allow you to avoid multiple + operations:... but also allow you to control how values are displayed:Whether you use % interpolation, str.format(), or f-strings is up to you: % interpolation has been around the longest (and is familiar to people with a background in C), str.format() is often more powerful, and f-strings are more powerful still (but available only in Python 3.6 and later).Another alternative is to use the fact that if you give print multiple positional arguments, it will join their string representations together using the sep keyword argument (which defaults to ' '):... but that's usually not as flexible as using Python's built-in string formatting abilities.1 Although it makes an exception for numeric types, where most people would agree on the 'right' thing to do:2 Actually four ... but template strings are rarely used and somewhat awkward.TL;DRA bit more verbal explanation:

Although there is anything not covered from the excellent @Zero Piraeus answer above, I will try to "minify" it a bit: 

You cannot concatenate a string and a number (of any kind) in python because those objects have different definitions of the plus(+) operator which are not compatible with each other (In the str case + is used for concatenation, in the number case it is used to add two numbers together).

So in order to solve this "misunderstanding" between objects:Have fun and do read the @Zero Piraeus answer it surely worth your time!Python 2.xPython 3.6+ReferenceAnother alternative is using str.format() method to concatenate an int into a String.    In your case: if you have 

converting json to string in python

BAE

[converting json to string in python](https://stackoverflow.com/questions/34600003/converting-json-to-string-in-python)

I did not explain my questions clearly at beginning.

Try to use str() and json.dumps() when converting json to string in python. My question is:My expected output: "{'jsonKey': 'jsonValue','title': 'hello world''}"My expected output: "{'jsonKey': 'jsonValue','title': 'hello world\"'}"It is not necessary to change the output string to json (dict) again for me.How to do this?

2016-01-04 21:16:35Z

I did not explain my questions clearly at beginning.

Try to use str() and json.dumps() when converting json to string in python. My question is:My expected output: "{'jsonKey': 'jsonValue','title': 'hello world''}"My expected output: "{'jsonKey': 'jsonValue','title': 'hello world\"'}"It is not necessary to change the output string to json (dict) again for me.How to do this?json.dumps() is much more than just making a string out of a Python object, it would always produce a valid JSON string (assuming everything inside the object is serializable) following the Type Conversion Table.For instance, if one of the values is None, the str() would produce an invalid JSON which cannot be loaded: But the dumps() would convert None into null making a valid JSON string that can be loaded:There are other differences. For instance, {'time': datetime.now()} cannot be serialized to JSON, but can be converted to string. You should use one of these tools depending on the purpose (i.e. will the result later be decoded).

Using python's mock patch.object to change the return value of a method called within another method

mdoc-2011

[Using python's mock patch.object to change the return value of a method called within another method](https://stackoverflow.com/questions/18191275/using-pythons-mock-patch-object-to-change-the-return-value-of-a-method-called-w)

Is it possible to mock a return value of a function called within another function I am trying to test? I would like the mocked method (which will be called in many methods I'm testing) to returned my specified variables each time it is called. For example: In the unit test, I would like to use mock to change the return value of uses_some_other_method() so that any time it is called in Foo, it will return what I defined in @patch.object(...)

2013-08-12 15:46:28Z

Is it possible to mock a return value of a function called within another function I am trying to test? I would like the mocked method (which will be called in many methods I'm testing) to returned my specified variables each time it is called. For example: In the unit test, I would like to use mock to change the return value of uses_some_other_method() so that any time it is called in Foo, it will return what I defined in @patch.object(...)There are two ways you can do this; with patch and with patch.objectPatch assumes that you are not directly importing the object but that it is being used by the object you are testing as in the followingIf you are directly importing the module to be tested, you can use patch.object as follows:In both cases some_fn will be 'un-mocked' after the test function is complete.Edit:

In order to mock multiple functions, just add more decorators to the function and add arguments to take in the extra parametersNote that the closer the decorator is to the function definition, the earlier it is in the parameter list.This can be done with something like this:Here's a source that you can read: Patching in the wrong placeLet me clarify what you're talking about: you want to test Foo in a testcase, which calls external method uses_some_other_method. Instead of calling the actual method, you want to mock the return value.Okay, suppose the above code is in foo.py, uses_some_other_method is defined in module bar.py. Here is the unittest:If you want to change the return value every time you passed in different arguements, mock provides side_effect .

heapq with custom compare predicate

vsekhar

[heapq with custom compare predicate](https://stackoverflow.com/questions/8875706/heapq-with-custom-compare-predicate)

I am trying to build a heap with a custom sort predicate. Since the values going into it are of 'user-defined' type, I cannot modify their built-in comparison predicate.Is there a way to do something like:Or even better, I could wrap the heapq functions in my own container so I don't need to keep passing the predicate.

2012-01-16 04:16:16Z

I am trying to build a heap with a custom sort predicate. Since the values going into it are of 'user-defined' type, I cannot modify their built-in comparison predicate.Is there a way to do something like:Or even better, I could wrap the heapq functions in my own container so I don't need to keep passing the predicate.According to the heapq documentation, the way to customize the heap order is to have each element on the heap to be a tuple, with the first tuple element being one that accepts  normal Python comparisons.The functions in the heapq module are a bit cumbersome (since they are not object-oriented), and always require our heap object (a heapified list) to be explicitly passed as the first parameter. We can kill two birds with one stone by creating a very simple wrapper class that will allow us to specify a key function, and present the heap as an object.The class below keeps an internal list, where each element is a tuple, the first member of which is a key, calculated at element insertion time using the key parameter, passed at Heap instantiation:The heapq documentation suggests that heap elements could be tuples in which the first element is the priority and defines the sort order.More pertinent to your question, however, is that the documentation includes a discussion with sample code of how one could implement their own heapq wrapper functions to deal with the problems of sort stability and elements with equal priority (among other issues).In a nutshell, their solution is to have each element in the heapq be a triple with the priority, an entry count and the element to be inserted. The entry count ensures that elements with the same priority a sorted in the order they were added to the heapq.Define a class, in which override the __lt__() function. See example below (works in Python 3.7):The limitation with both answers is that they don't allow ties to be treated as ties.  In the first, ties are broken by comparing items, in the second by comparing input order.  It is faster to just let ties be ties, and if there are a lot of them it could make a big difference.  Based on the above and on the docs, it is not clear if this can be achieved in heapq.  It does seem strange that heapq does not accept a key, while functions derived from it in the same module do.

P.S.:

If you follow the link in the first comment ("possible duplicate...") there is another suggestion of defining le which seems like a solution.

what's the meaning of %r in python

photon

[what's the meaning of %r in python](https://stackoverflow.com/questions/2354329/whats-the-meaning-of-r-in-python)

what's the meaning of %r in the following statement?I think I've heard of %s, %d, and %f but never heard of this.

2010-03-01 07:13:09Z

what's the meaning of %r in the following statement?I think I've heard of %s, %d, and %f but never heard of this.Background:In Python, there are two builtin functions for turning an object into a string: str vs. repr. str is supposed to be a friendly, human readable string. repr is supposed to include detailed information about an object's contents (sometimes, they'll return the same thing, such as for integers). By convention, if there's a Python expression that will eval to another object that's ==, repr will return such an expression e.g.If returning an expression doesn't make sense for an object, repr should return a string that's surrounded by < and > symbols e.g. <blah>.To answer your original question:%s <-> str

%r <-> reprIn addition:You can control the way an instance of your own classes convert to strings by implementing __str__ and __repr__ methods.It calls repr() on the object and inserts the resulting string.It prints the replacement as a string with repr().Adding to the replies given above, '%r' can be useful in a scenario where you have a list with heterogeneous data type.

Let's say, we have a list = [1, 'apple' , 2 , 'r','banana']

Obviously in this case using '%d' or '%s' would cause an error. Instead, we can use '%r' to print all these values.The difference between %r and %s is, %r calls the repr() method and %s calls the str() method. Both of these are built-in Python functions.The repr() method returns a printable representation of the given object.

The str() method returns the "informal" or nicely printable representation of a given object.In simple language, what the str() method does is print the result in a way which the end user would like to see:The repr() method would print or show what an object actually looks like:%r calls repr() on the object, and inserts the resulting string returned by __repr__.The string returned by __repr__ should be unambiguous and, if possible, match the source code necessary to recreate the object being represented. A quick example:So, %s calls the __str()__ method of the selected object and replaces itself with the return value,%r calls the __repr()__ method of the selected object and replaces itself with the return value.See String Formatting Operations in the docs. Notice that %s and %d etc, might work differently to how you expect if you are used to the way they work in another language such as C.In particular, %s also works well for ints and floats unless you have special formatting requirements where %d or %f will give you more control.I read in "Learning Python the Hard Way", the author said that 

Which JSON module can I use in Python 2.5?

moinudin

[Which JSON module can I use in Python 2.5?](https://stackoverflow.com/questions/791561/which-json-module-can-i-use-in-python-2-5)

I would like to use Python's JSON module. It was only introduced in Python 2.6 and I'm stuck with 2.5 for now. Is the particular JSON module provided with Python 2.6 available as a separate module that can be used with 2.5?

2009-04-26 20:34:15Z

I would like to use Python's JSON module. It was only introduced in Python 2.6 and I'm stuck with 2.5 for now. Is the particular JSON module provided with Python 2.6 available as a separate module that can be used with 2.5?You can use simplejson.As shown by the answer form pkoch you can use the following import statement to  get a json library depending on the installed python version:To Wells and others:Here's how:

try:

    import json

except ImportError:

    import simplejson as json 

I wrote the cjson 1.0.6 patch and my advice is don't use cjson -- there are other problems with cjson in how it handles unicode etc.  I don't think the speed of cjson is worth dealing with the bugs -- encoding/decoding json is usually a very small bit of the time needed to process a typical web request...json in python 2.6+ is basically simplejson brought into the standard library I believe...I prefer cjson since it's much faster: http://www.vazor.com/cjson.htmljson is a built-in module, you don't need to install it with pip.I am programming in Python 2.5 as well and wanted a suitable library. Here is how I did it.donwloaded the simplejson egg file called  simplejson-2.0.6-py2.5-linux-i686.egg from

http://pypi.python.org/simple/simplejson/installed it using the command :sudo python ./ez_setup.py ./simplejson-2.0.6-py2.5-linux-i686.eggThen imported the json library into the script file by doing :

How slow is Python's string concatenation vs. str.join?

Wayne Werner

[How slow is Python's string concatenation vs. str.join?](https://stackoverflow.com/questions/3055477/how-slow-is-pythons-string-concatenation-vs-str-join)

As a result of the comments in my answer on this thread, I wanted to know what the speed difference is between the += operator and ''.join()So what is the speed comparison between the two?

2010-06-16 16:58:00Z

As a result of the comments in my answer on this thread, I wanted to know what the speed difference is between the += operator and ''.join()So what is the speed comparison between the two?From: Efficient String ConcatenationMethod 1: Method 4: Now I realise they are not strictly representative, and the 4th method appends to a list before iterating through and joining each item, but it's a fair indication.String join is significantly faster then concatenation.Why? Strings are immutable and can't be changed in place. To alter one, a new representation needs to be created (a concatenation of the two).My original code was wrong, it appears that + concatenation is usually faster (especially with newer versions of Python on newer hardware)The times are as follows:Python 3.3 on Windows 7, Core i7Python 2.7 on Windows 7, Core i7On Linux Mint, Python 2.7, some slower processorAnd here is the code:The existing answers are very well-written and researched, but here's another answer for the Python 3.6 era, since now we have literal string interpolation (AKA, f-strings):Test performed using CPython 3.6.5 on a 2012 Retina MacBook Pro with an Intel Core i7 at 2.3 GHz.This is by no means any formal benchmark, but it looks like using f-strings is roughly as performant as using += concatenation; any improved metrics or suggestions are, of course, welcome.I rewrote the last answer, could jou please share your opinion on the way i tested?NOTE: This example is written in Python 3.5, where range() acts like the former xrange()The output i got:Personally i prefer ''.join([]) over the 'Plusser way' because it's cleaner and more readable. This is what silly programs are designed to test :)Use plusOutput of:Now with join....Output Of:So on python 2.6 on windows, I would say + is about 18 times faster than join :)

Get a sub-set of a Python dictionary

Oli

[Get a sub-set of a Python dictionary](https://stackoverflow.com/questions/3953371/get-a-sub-set-of-a-python-dictionary)

I have a dictionary:I need to pass a sub-set of that dictionary to third-party code. It only wants a dictionary containing keys ['key1', 'key2', 'key99'] and if it gets another key (eg 'key3'), it explodes in a nasty mess. The code in question is out of my control so I'm left in a position where I have to clean my dictionary.What's the best, way to limit a dictionary to a set of keys?Given the example dictionary and allowed keys above, I want:

2010-10-17 13:25:25Z

I have a dictionary:I need to pass a sub-set of that dictionary to third-party code. It only wants a dictionary containing keys ['key1', 'key2', 'key99'] and if it gets another key (eg 'key3'), it explodes in a nasty mess. The code in question is out of my control so I'm left in a position where I have to clean my dictionary.What's the best, way to limit a dictionary to a set of keys?Given the example dictionary and allowed keys above, I want:In Python3 (or Python2.7 or later) you can do it with a dict-comprehension too:In modern Python (2.7+,3.0+), use a dictionary comprehension:My way to do this is.UpdateI have to admit though, the above implementation doesn't handle the case when the length of ks is 0 or 1.  The following code handles the situation and it is no longer an one-liner.An other solution without if in dict comprehension.With a complex class Myclass being a subclass of collections.UserDict. To select a subset of it, i.e keeping all its container properties, it's convenient to define a method, e.g. named sub like so:It is then used as Myclass.sub([key1, key2 ...])

How to check if a deque is empty in Python?

Ilya Smagin

[How to check if a deque is empty in Python?](https://stackoverflow.com/questions/5652278/how-to-check-if-a-deque-is-empty-in-python)

How to check if a deque is empty in Python?

2011-04-13 16:11:42Z

How to check if a deque is empty in Python?If d is your deque, useThis will implicitly convert d to a bool, which yields True if the deque contains any items and False if it is empty.There are two main ways:1) Containers can be used as booleans (with false indicating the container is empty):2) Containers in Python also have a __len__ method to indicate their size.Here are a few patterns:The latter technique isn't as fast or succinct as the others, but it does have the virtue of being explicit for readers who may not know about the boolean value of containers.Other ways are possible. For example, indexing with d[0] raises an IndexError for an empty sequence.  I've seen this used a few times.Hope this helps :-)          

How can I use seaborn without changing the matplotlib defaults?

Hello lad

[How can I use seaborn without changing the matplotlib defaults?](https://stackoverflow.com/questions/25393936/how-can-i-use-seaborn-without-changing-the-matplotlib-defaults)

I am trying to use seaborn, because of its distplot function. But I prefer the default matplotlib settings. When I import seaborn, it changes automatically the appearance of my figure.How can I use seaborn functions without changing the look of the plots?

2014-08-19 22:22:05Z

I am trying to use seaborn, because of its distplot function. But I prefer the default matplotlib settings. When I import seaborn, it changes automatically the appearance of my figure.How can I use seaborn functions without changing the look of the plots?Version 0.8 (july 2017) changed this behaviour. From https://seaborn.pydata.org/whatsnew.html: For older versions, Import seaborn like this:and then you should be able to use sns.distplot but maintain the default matplotlib styling + your personal rc configuration.According to documentation reset_orig  restore all RC params to original settings:

Python argparse - Add argument to multiple subparsers

rahmu

[Python argparse - Add argument to multiple subparsers](https://stackoverflow.com/questions/7498595/python-argparse-add-argument-to-multiple-subparsers)

My script defines one main parser and multiple subparsers. I want to apply the -p argument to some subparsers. So far the code looks like this: As you can see the add_arument ("-p") is repeated twice. I actually have a lot more subparsers. Is there a way to loop through the existing subparsers in order to avoid repetition?For the record, I am using Python 2.7

2011-09-21 10:59:45Z

My script defines one main parser and multiple subparsers. I want to apply the -p argument to some subparsers. So far the code looks like this: As you can see the add_arument ("-p") is repeated twice. I actually have a lot more subparsers. Is there a way to loop through the existing subparsers in order to avoid repetition?For the record, I am using Python 2.7This can be achieved by defining a parent parser containing the common option(s):This produces help messages of the format:Output:Output:However, if you run your program, you will not encounter an error if you do not specify an action (i.e. create or update). If you desire this behavior, modify your code as follows.This fix was brought up in this SO question which refers to an issue tracking a pull request.The accepted answer is correct; the proper way is to use parent parsers. However, the example code IMO was not really solving the problem. Let me add my few cents to provide a more suitable example.The main difference with accepted answer is the explicit possibility to have some root-level arguments (like --verbose) as well as shared arguments only for some subparsers (-p only for the create and update subparsers but not for others)This is the top-level help message (note that -p parameter is not shown here, which is exactly what you would expect—because it is specific to some subparsers):And the help message for the create action:You can also loop over the subparsers and add the same option to all of them.You can loop over your subparsers in the following way.

Note that by using subparsers.choices that you avoid needing to hard-code all of the subparsers.

Why is ''.join() faster than += in Python?

Rodney Wells

[Why is ''.join() faster than += in Python?](https://stackoverflow.com/questions/39312099/why-is-join-faster-than-in-python)

I'm able to find a bevy of information online (on Stack Overflow and otherwise) about how it's a very inefficient and bad practice to use + or += for concatenation in Python.I can't seem to find WHY += is so inefficient. Outside of a mention here that "it's been optimized for 20% improvement in certain cases" (still not clear what those cases are), I can't find any additional information.What is happening on a more technical level that makes ''.join() superior to other Python concatenation methods?

2016-09-03 23:11:19Z

I'm able to find a bevy of information online (on Stack Overflow and otherwise) about how it's a very inefficient and bad practice to use + or += for concatenation in Python.I can't seem to find WHY += is so inefficient. Outside of a mention here that "it's been optimized for 20% improvement in certain cases" (still not clear what those cases are), I can't find any additional information.What is happening on a more technical level that makes ''.join() superior to other Python concatenation methods?Let's say you have this code to build up a string from three strings:In this case, Python first needs to allocate and create 'foobar' before it can allocate and create 'foobarbaz'.So for each += that gets called, the entire contents of the string and whatever is getting added to it need to be copied into an entirely new memory buffer.  In other words, if you have N strings to be joined, you need to allocate approximately N temporary strings and the first substring gets copied ~N times. The last substring only gets copied once, but on average, each substring gets copied ~N/2 times.With .join, Python can play a number of tricks since the intermediate strings do not need to be created. CPython figures out how much memory it needs up front and then allocates a correctly-sized buffer. Finally, it then copies each piece into the new buffer which means that each piece is only copied once.There are other viable approaches which could lead to better performance for += in some cases. E.g. if the internal string representation is actually a rope or if the runtime is actually smart enough to somehow figure out that the temporary strings are of no use to the program and optimize them away.However, CPython certainly does not do these optimizations reliably (though it may for a few corner cases) and since it is the most common implementation in use, many best-practices are based on what works well for CPython. Having a standardized set of norms also makes it easier for other implementations to focus their optimization efforts as well.I think this behaviour is best explained in Lua's string buffer chapter.To rewrite that explanation in context of Python, let's start with an innocent code snippet (a derivative of the one at Lua's docs):Assume that each l is 20 bytes and the s has already been parsed to a size of 50 KB. When Python concatenates s + l it creates a new string with 50,020 bytes and copies 50 KB from s into this new string. That is, for each new line, the program moves 50 KB of memory, and growing. After reading 100 new lines (only 2 KB), the snippet has already moved more than 5 MB of memory. To make things worse, after the assignmentthe old string is now garbage. After two loop cycles, there are two old strings making a total of more than 100 KB of garbage. So, the language compiler decides to run its garbage collector and frees those 100 KB. The problem is that this will happen every two cycles and the program will run its garbage collector two thousand times before reading the whole list. Even with all this work, its memory usage will be a large multiple of the list's size.And, at the end:Python strings are also immutable objects.

What are the different use cases of joblib versus pickle?

msunbot

[What are the different use cases of joblib versus pickle?](https://stackoverflow.com/questions/12615525/what-are-the-different-use-cases-of-joblib-versus-pickle)

Background: I'm just getting started with scikit-learn, and read at the bottom of the page about joblib, versus pickle. I read this Q&A on Pickle, 

Common use-cases for pickle in Python and wonder if the community here can share the differences between joblib and pickle?  When should one use one over another? 

2012-09-27 06:39:45Z

Background: I'm just getting started with scikit-learn, and read at the bottom of the page about joblib, versus pickle. I read this Q&A on Pickle, 

Common use-cases for pickle in Python and wonder if the community here can share the differences between joblib and pickle?  When should one use one over another? joblib is usually significantly faster on large numpy arrays because it has a special handling for the array buffers of the numpy datastructure. To find about the implementation details you can have a look at the source code. It can also compress that data on the fly while pickling using zlib or lz4.joblib also makes it possible to memory map the data buffer of an uncompressed joblib-pickled numpy array when loading it which makes it possible to share memory between processes.Note that if you don't pickle large numpy arrays, then regular pickle can be significantly faster, especially on large collections of small python objects (e.g. a large dict of str objects) because the pickle module of the standard library is implemented in C while joblib is pure python.Note that once PEP 574 (Pickle protocol 5) is merged (hopefully for Python 3.8), it will be much more efficient to pickle large numpy arrays using the standard library.joblib might still be useful to load objects that have nested numpy arrays in memory mapped mode with mmap_mode="r" though.Thanks to Gunjan for giving us this script! I modified it for Python3 resultsI came across same question, so i tried this one (with Python 2.7) as i need to load a large pickle fileOutput for this isAccording to this joblib works better than cPickle and Pickle module from these 3 modules. Thanks

Why does next raise a 'StopIteration', but 'for' do a normal return?

Martijn Pieters

[Why does next raise a 'StopIteration', but 'for' do a normal return?](https://stackoverflow.com/questions/14413969/why-does-next-raise-a-stopiteration-but-for-do-a-normal-return)

In this piece of code, why does using for result in no StopIteration

or is the for loop trapping all exceptions and then silently exiting?

In which case, why do we have the extraneous return?? Or is the

raise StopIteration caused by: return None?Assuming StopIteration is being triggered by: return None.

When is GeneratorExit generated?If I manually do a:In which case why don't I see a traceback?

2013-01-19 11:44:06Z

In this piece of code, why does using for result in no StopIteration

or is the for loop trapping all exceptions and then silently exiting?

In which case, why do we have the extraneous return?? Or is the

raise StopIteration caused by: return None?Assuming StopIteration is being triggered by: return None.

When is GeneratorExit generated?If I manually do a:In which case why don't I see a traceback?The for loop listens for StopIteration explicitly.The purpose of the for statement is to loop over the sequence provided by an iterator and the exception is used to signal that the iterator is now done; for doesn't catch other exceptions raised by the object being iterated over, just that one.That's because StopIteration is the normal, expected signal to tell whomever is iterating that there is nothing more to be produced.A generator function is a special kind of iterator; it indeed raises StopIteration when the function is done (i.e. when it returns, so yes, return None raises StopIteration). It is a requirement of iterators; they must raise StopIteration when they are done; in fact, once a StopIteration has been raised, attempting to get another element from them (through next(), or calling the .next() (py 2) or .__next__() (py 3) method on the iterator) must always raise StopIteration again.GeneratorExit is an exception to communicate in the other direction. You are explicitly closing a generator with a yield expression, and the way Python communicates that closure to the generator is by raising GeneratorExit inside of that function. You explicitly catch that exception inside of countdown, its purpose is to let a generator clean up resources as needed when closing.A GeneratorExit is not propagated to the caller; see the generator.close() documentation.

How do you use subprocess.check_output() in Python?

JOHANNES_NYÅTT

[How do you use subprocess.check_output() in Python?](https://stackoverflow.com/questions/14078117/how-do-you-use-subprocess-check-output-in-python)

I have found documentation about subprocess.check_output() but I cannot find one with  arguments and the documentation is not very in depth. I am using Python 3 (but am trying to run a Python 2 file through Python 3)I am trying to run this command:

python py2.py -i test.txt-i is a positional argument for argparse, test.txt is what the -i is, py2.py is the file to runI have tried a lot of (non working) variations including:

py2output = subprocess.check_output([str('python py2.py '),'-i', 'test.txt'])py2output = subprocess.check_output([str('python'),'py2.py','-i', test.txt'])

2012-12-29 02:11:40Z

I have found documentation about subprocess.check_output() but I cannot find one with  arguments and the documentation is not very in depth. I am using Python 3 (but am trying to run a Python 2 file through Python 3)I am trying to run this command:

python py2.py -i test.txt-i is a positional argument for argparse, test.txt is what the -i is, py2.py is the file to runI have tried a lot of (non working) variations including:

py2output = subprocess.check_output([str('python py2.py '),'-i', 'test.txt'])py2output = subprocess.check_output([str('python'),'py2.py','-i', test.txt'])The right answer (using Python 2.7 and later, since check_output() was introduced then) is:To demonstrate, here are my two programs:py2.py:py3.py:Running it:Here's what's wrong with each of your versions:First, str('python py2.py') is exactly the same thing as 'python py2.py'—you're taking a str, and calling str to convert it to an str. This makes the code harder to read, longer, and even slower, without adding any benefit.More seriously, python py2.py can't be a single argument, unless you're actually trying to run a program named, say, /usr/bin/python\ py2.py. Which you're not; you're trying to run, say, /usr/bin/python with first argument py2.py. So, you need to make them separate elements in the list.Your second version fixes that, but you're missing the ' before test.txt'. This should give you a SyntaxError, probably saying EOL while scanning string literal.Meanwhile, I'm not sure how you found documentation but couldn't find any examples with arguments. The very first example is:That calls the "echo" command with an additional argument, "Hello World!".Also:I'm pretty sure -i is not a positional argument, but an optional argument. Otherwise, the second half of the sentence makes no sense.Adding on to the one mentioned by @abarnerta better one is to catch the exception this stderr= subprocess.STDOUT is for making sure you dont get the filenotfound error in stderr- which cant be usually caught in filenotfoundexception, else you would end up gettingInfact a better solution to this might be to check, whether the file/scripts exist and then to run the file/scriptSince Python 3.5, subprocess.run() is recommended over subprocess.check_output():Since Python 3.7, instead of the above, you can use capture_output=true parameter to capture stdout and stderr:Also, you may want to use universal_newlines=True or its equivalent since Python 3.7 text=True to work with text instead of binary:See subprocess.run() documentation for more information.

dictionary update sequence element #0 has length 3; 2 is required

rindra

[dictionary update sequence element #0 has length 3; 2 is required](https://stackoverflow.com/questions/14302248/dictionary-update-sequence-element-0-has-length-3-2-is-required)

I want to add lines to the object account.bank.statement.line through other object But I get following error: Here is my code:I changed it on that but then I ran into this error: Code: 

2013-01-13 09:27:20Z

I want to add lines to the object account.bank.statement.line through other object But I get following error: Here is my code:I changed it on that but then I ran into this error: Code: This error raised up because you trying to update dict object by using a wrong sequence (list or tuple) structure.cash_id.create(cr, uid, lines,context=None) trying to convert lines into dict object:Remove the second zero from this tuple to properly convert it into a dict object.To test it your self, try this into python shell:I was getting this error when I was updating the dictionary with the wrong syntax:Try with these:instead ofOne of the fast ways to create a dict from equal-length tuples:

Python remove set from set

cod3monk3y

[Python remove set from set](https://stackoverflow.com/questions/9056833/python-remove-set-from-set)

According to my interpretation of Python 2.7.2 documentation for Built-In Types 5.7 Set Types, it should be possible to remove the elements of set A from set B by passing A to set.remove(elem) or set.discard(elem)From the documentation for 2.7.2:I interpret this to mean that I can pass a set to remove(elem) or discard(elem) and all those elements will be removed from the target set. I would use this to do something weird like remove all vowels from a string or remove all common words from a word-frequency histogram. Here's the test code: Which I expect to return:I know I can accomplish this with a.difference(b) which returns a new set; or with a set.difference_update(other); or with set operators a -= b, which modify the set in-place. So is this a bug in the documentation? Can set.remove(elem) actually not take a set as an argument? Or does the documentation refer to sets of sets? Given that difference_update accomplishes my interpretation, I'm guess the case is the latter.Is that unclear enough?EDIT

After 3 years of additional (some professional) python work, and being recently drawn back to this question, I realize now what I was actually trying to do could be accomplished with:which is what I was originally trying to get.EDIT

After 4 more years of python development... I realize this operation can be expressed more cleanly using set literals and the - operator; and that it is more complete to show that set difference is non-commutative.

2012-01-29 21:10:35Z

According to my interpretation of Python 2.7.2 documentation for Built-In Types 5.7 Set Types, it should be possible to remove the elements of set A from set B by passing A to set.remove(elem) or set.discard(elem)From the documentation for 2.7.2:I interpret this to mean that I can pass a set to remove(elem) or discard(elem) and all those elements will be removed from the target set. I would use this to do something weird like remove all vowels from a string or remove all common words from a word-frequency histogram. Here's the test code: Which I expect to return:I know I can accomplish this with a.difference(b) which returns a new set; or with a set.difference_update(other); or with set operators a -= b, which modify the set in-place. So is this a bug in the documentation? Can set.remove(elem) actually not take a set as an argument? Or does the documentation refer to sets of sets? Given that difference_update accomplishes my interpretation, I'm guess the case is the latter.Is that unclear enough?EDIT

After 3 years of additional (some professional) python work, and being recently drawn back to this question, I realize now what I was actually trying to do could be accomplished with:which is what I was originally trying to get.EDIT

After 4 more years of python development... I realize this operation can be expressed more cleanly using set literals and the - operator; and that it is more complete to show that set difference is non-commutative.You already answered the question. It refers to sets of sets (actually sets containing frozensets).The paragraph you are referring to begins with:which means that b in a.remove(b) can be a set, and then continues with:which means that if b is a set, a.remove(b) will scan a for a frozenset equivalent to b and remove it (or throw a KeyError if it doesn't exist).You can't have sets of sets in Python as a set is mutable.  Instead, you can have sets of frozensets.  On the other hand, you can call __contains__(), remove(), and discard() with a set.  See this example:So the answer to your question is that the documentation is referring to sets of frozensets. I'm looking at the built-in help for various versions of python (for mac). Here are the results.The documentation you refer to, in full, actually says:This seems to be a footnote, that suggests the argument may be a set, but unless it finds a matching frozen set within the set, it will not be removed. The mention about the set being modified is so it can be hashed to look for a a matching frozen set.I think the documentation is referring to sets of (frozen)sets, yes.Took me a while to find the solution in the OP's post. So adding it here for others to find more easily. 

Assert that a method was called with one argument out of several

user1427661

[Assert that a method was called with one argument out of several](https://stackoverflow.com/questions/21611559/assert-that-a-method-was-called-with-one-argument-out-of-several)

I'm mocking out a call to requests.post using the Mock library: The the call involves multiple arguments: the URL, a payload, some auth stuff, etc. I want to assert that requests.post is called with a particular URL, but I don't care about the other arguments. When I try this: the test fails, as it expects it to be called with only that argument. Is there any way to check if a single argument is used somewhere in the function call without having to pass in the other arguments? Or, even better, is there a way to assert a specific URL and then abstract data types for the other arguments (i.e., data should be a dictionary, auth should be an instance of HTTPBasicAuth, etc.)? 

2014-02-06 18:55:47Z

I'm mocking out a call to requests.post using the Mock library: The the call involves multiple arguments: the URL, a payload, some auth stuff, etc. I want to assert that requests.post is called with a particular URL, but I don't care about the other arguments. When I try this: the test fails, as it expects it to be called with only that argument. Is there any way to check if a single argument is used somewhere in the function call without having to pass in the other arguments? Or, even better, is there a way to assert a specific URL and then abstract data types for the other arguments (i.e., data should be a dictionary, auth should be an instance of HTTPBasicAuth, etc.)? As far as I know Mock doesn't provide a way to achieve what you want via assert_called_with. You could access the call_args and call_args_list members and perform the assertions manually.However the is a simple (and dirty) way of achieving almost what you want. You have to implement a class whose __eq__ method always returns True:Using it as:As you can see it only checks for the arg. You have to create subclasses of int, otherwise the comparisons wont work1. However you still have to provide all the arguments. If you have many arguments you might shorten your code using tuple-unpacking:Except for this I cannot think of a way to avoid passing all parameters to assert_called_with and work it as you intend.The above solution can be extended to check for types of other arguments. For example:however this doesn't allow arguments that can be, for example, both an int or a str. Allowing multiple arguments to Any and using multiple-inheritance wont help. We can solve this using abc.ABCMetaExample:1 I used the name Any for the function since it is "used as a class" in the code. Also any is a built-in...You can also use the ANY helper to always match arguments you don't know or aren't checking for.More on the ANY helper: 

https://docs.python.org/3/library/unittest.mock.html#anySo for instance you could match the argument 'session' to anything like so:see: calls-as-tuplesIf there are too many parameters being passed and only one of them is to be checked, doing something like {'slug': 'foo', 'field1': ANY, 'field2': ANY, 'field3': ANY, ' . . . } can be clumsy.

I took the following approach to achieve this:In simple words, this returns a tuple with all positional arguments and dictionary with all named arguments passed to the function call, so now you can check anything you want.

Furthermore, if you wanted to check the datatype of a few fields

Also, if you're passing arguments (instead of keyword arguments), you can access them via args like thisYou can use : assert_any_call(args) 

https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_any_callrequests.post.assert_any_call(requests_arguments)

Sort a list of Class Instances Python [duplicate]

Ned Batchelder

[Sort a list of Class Instances Python [duplicate]](https://stackoverflow.com/questions/4010322/sort-a-list-of-class-instances-python)

I have a list of class instances - among other attributes the class has score attribute. How can I sort the items in  ascending order based on this parameter?EDIT: The list in python has something called sort. Could I use this here? How do I direct this function to use my score attribute?

2010-10-24 20:35:38Z

I have a list of class instances - among other attributes the class has score attribute. How can I sort the items in  ascending order based on this parameter?EDIT: The list in python has something called sort. Could I use this here? How do I direct this function to use my score attribute?if you want to sort x in-place, you can also:In addition to the solution you accepted, you could also implement the special __lt__() ("less than") method on the class. The sort() method (and the sorted() function) will then be able to compare the objects, and thereby sort them. This works best when you will only ever sort them on this attribute, however.

Break string into list of characters in Python

FlexedCookie

[Break string into list of characters in Python](https://stackoverflow.com/questions/9833392/break-string-into-list-of-characters-in-python)

Essentially I want to suck a line of text from a file, assign the characters to a list, and create a list of all the separate characters in a list -- a list of lists.At the moment, I've tried this:That's all I've got. I don't quite know how to extract the single characters and assign them to a new list.The line I get from the file will be something like:I want to turn it into this list, with each single character on its own:

2012-03-23 02:32:20Z

Essentially I want to suck a line of text from a file, assign the characters to a list, and create a list of all the separate characters in a list -- a list of lists.At the moment, I've tried this:That's all I've got. I don't quite know how to extract the single characters and assign them to a new list.The line I get from the file will be something like:I want to turn it into this list, with each single character on its own:Strings are iterable (just like a list).I'm interpreting that you really want something like:ororchars would contain all of the characters in the file.You can do this using list:Be aware that any spaces in the line will be included in this list, to the best of my knowledge. I'm a bit late it seems to be, but...So to add the string hello to a list as individual characters, try this:However, it is easier to do this:Or use a fancy list comprehension, which are supposed to be "computationally more efficient", when working with very very large files/listsBtw: The answer that was accepted does not account for the whitespaces...An easy way is using function「map()」.In python many things are iterable including files and strings.

Iterating over a filehandler gives you a list of all the lines in that file.

Iterating over a string gives you a list of all the characters in that string.or if you want a one liner..Edit: as agf mentions you can use itertools.chain.from_iterableHis method is better, unless you want the ability to specify which lines to grab

list(itertools.chain.from_iterable(open(filename, 'rU)))This does however require one to be familiar with itertools, and as a result looses some readablityIf you only want to iterate over the chars, and don't care about storing a list, then I would use the nested for loops. This method is also the most readable.Version 3.5 onwards allows the use of PEP 448 - Extended Unpacking Generalizations:This is a specification of the language syntax, so it is faster than calling list:Because strings are (immutable) sequences they can be unpacked similar to lists:When running map(lambda x: x, multiLine) this is clearly more efficient, but in fact it returns a map object instead of a list.Turning the map object into a list will take longer than the unpacking method.

MySQL: Get column name or alias from query

panofish

[MySQL: Get column name or alias from query](https://stackoverflow.com/questions/5010042/mysql-get-column-name-or-alias-from-query)

I'm not asking for the SHOW COLUMNS command.I want to create an application that works similarly to heidisql, where you can specify an SQL query and when executed, returns a result set with rows and columns representing your query result. The column names in the result set should match your selected columns as defined in your SQL query.In my Python program (using MySQLdb) my query returns only the row and column results, but not the column names.  In the following example the column names would be ext, totalsize, and filecount.  The SQL would eventually be external from the program. The only way I can figure to make this work, is to write my own SQL parser logic to extract the selected column names.Is there an easy way to get the column names for the provided SQL?

Next I'll need to know how many columns does the query return?

2011-02-15 22:01:17Z

I'm not asking for the SHOW COLUMNS command.I want to create an application that works similarly to heidisql, where you can specify an SQL query and when executed, returns a result set with rows and columns representing your query result. The column names in the result set should match your selected columns as defined in your SQL query.In my Python program (using MySQLdb) my query returns only the row and column results, but not the column names.  In the following example the column names would be ext, totalsize, and filecount.  The SQL would eventually be external from the program. The only way I can figure to make this work, is to write my own SQL parser logic to extract the selected column names.Is there an easy way to get the column names for the provided SQL?

Next I'll need to know how many columns does the query return?cursor.description will give you a tuple of tuples where [0] for each is the column header.This is the same as thefreeman but more in pythonic way using list and dictionary comprehension Similar to @James answer, a more pythonic way can be:You can get a single column with map over the result:or filter results:or accumulate values for filtered columns:I think this should do what you need (builds on the answer above) .  I am sure theres a more pythony way to write it, but you should get the general idea.You could also use MySQLdb.cursors.DictCursor. This turns your result set into a python list of python dictionaries, although it uses a special cursor, thus technically less portable than the accepted answer. Not sure about speed. Here's the edited original code that uses this.Standard dictionary functions apply, for example, len(row[0]) to count the number of columns for the first row, list(row[0]) for a list of column names (for the first row), etc. Hope this helps!Looks like MySQLdb doesn't actually provide a translation for that API call.  The relevant C API call is mysql_fetch_fields, and there is no MySQLdb translation for thatThis is only an add-on to the accepted answer:where dotdict is:This will allow you to access much easier the values by column names.

Suppose you have a user table with columns name and email:Try:mysql connector version:You can also do this to just get the field titles:

Python 2.x gotchas and landmines

David

[Python 2.x gotchas and landmines](https://stackoverflow.com/questions/530530/python-2-x-gotchas-and-landmines)

The purpose of my question is to strengthen my knowledge base with Python and get a better picture of it, which includes knowing its faults and surprises.  To keep things specific, I'm only interested in the CPython interpreter.I'm looking for something similar to what learned from my PHP landmines

question where some of the answers were well known to me but a couple were borderline horrifying.Update:

   Apparently one maybe two people are upset that I asked a question that's already partially answered outside of Stack Overflow.  As some sort of compromise here's the URL

http://www.ferg.org/projects/python_gotchas.htmlNote that one or two answers here already are original from what was written on the site referenced above.

2009-02-09 23:25:00Z

The purpose of my question is to strengthen my knowledge base with Python and get a better picture of it, which includes knowing its faults and surprises.  To keep things specific, I'm only interested in the CPython interpreter.I'm looking for something similar to what learned from my PHP landmines

question where some of the answers were well known to me but a couple were borderline horrifying.Update:

   Apparently one maybe two people are upset that I asked a question that's already partially answered outside of Stack Overflow.  As some sort of compromise here's the URL

http://www.ferg.org/projects/python_gotchas.htmlNote that one or two answers here already are original from what was written on the site referenced above.Expressions in default arguments are calculated when the function is defined, not when it’s called. Example: consider defaulting an argument to the current time:The when argument doesn't change. It is evaluated when you define the function. It won't change until the application is re-started.Strategy: you won't trip over this if you default arguments to None and then do something useful when you see it:Exercise: to make sure you've understood: why is this happening?You should be aware of how class variables are handled in Python. Consider the following class hierarchy:Now, check the output of the following code:Surprised? You won't be if you remember that class variables are internally handled as dictionaries of a class object. For read operations, if a variable name is not found in the dictionary of current class, the parent classes are searched for it. So, the following code again, but with explanations:Same goes for handling class variables in class instances (treat this example as a continuation of the one above):Loops and lambdas (or any closure, really): variables are bound by nameA work around is either creating a separate function or passing the args by name:Dynamic binding makes typos in your variable names surprisingly hard to find. It's easy to spend half an hour fixing a trivial bug.EDIT: an example...One of the biggest surprises I ever had with Python is this one:This works as one might expect, except for raising a TypeError after updating the first entry of the tuple!  So a will be ([42, 43, 44],) after executing the += statement, but there will be an exception anyway.  If you try this on the other handyou won't get an error.reason this doesn't work is because IndexError is the type of exception you're catching, and ValueError is the name of the variable you're assigning the exception to.Correct code to catch multiple exceptions is:There was a lot of discussion on hidden language features a while back: hidden-features-of-python. Where some pitfalls were mentioned (and some of the good stuff too). Also you might want to check out Python Warts.But for me, integer division's a gotcha:You probably wanted:If you really want this (C-like) behaviour, you should write:As that will work with floats too (and it will work when you eventually go to Python 3):GvR explains how integer division came to work how it does on the history of Python.List slicing has caused me a lot of grief. I actually consider the following behavior a bug.Define a list xAccess index 2:As you expect.Slice the list from index 2 and to the end of the list:As you expect.Access index 7:Again, as you expect.However, try to slice the list from index 7 until the end of the list:??? The remedy is to put a lot of tests when using list slicing. I wish I'd just get an error instead. Much easier to debug.Not including an __init__.py in your packages.  That one still gets me sometimes.The only gotcha/surprise I've dealt with is with CPython's GIL.  If for whatever reason you expect python threads in CPython to run concurrently... well they're not and this is pretty well documented by the Python crowd and even Guido himself.A long but thorough explanation of CPython threading and some of the things going on under the hood and why true concurrency with CPython isn't possible.

http://jessenoller.com/2009/02/01/python-threads-and-the-global-interpreter-lock/James Dumay eloquently reminded me of another Python gotcha: Not all of Python's「included batteries」are wonderful. James’ specific example was the HTTP libraries: httplib, urllib, urllib2, urlparse, mimetools, and ftplib. Some of the functionality is duplicated, and some of the functionality you'd expect is completely absent, e.g. redirect handling. Frankly, it's horrible. If I ever have to grab something via HTTP these days, I use the urlgrabber module forked from the Yum project.Floats are not printed at full precision by default (without repr):repr prints too many digits:Unintentionally mixing oldstyle and newstyle classes can cause seemingly mysterious errors.Say you have a simple class hierarchy consisting of superclass A and subclass B. When B is instantiated, A's constructor must be called first. The code below correctly does this:But if you forget to make A a newstyle class and define it like this:you get this traceback:Two other questions relating to this issue are 489269 and 770134results in an UnboundLocalError, because local names are detected statically.  A different example would beYou cannot use locals()['x'] = whatever  to change local variable values as you might expect.This actually burnt me in an answer here on SO, since I had tested it outside a function and got the change I wanted.  Afterwards, I found it mentioned and contrasted to the case of globals() in "Dive Into Python."  See example 8.12.  (Though it does not note that the change via locals() will work at the top level as I show above.)This caught me out today and wasted an hour of my time debugging:Explanation: Python list problemOne creates a new list while the other modifies in placeUsing class variables when you want instance variables. Most of the time this doesn't cause problems, but if it's a mutable value it causes surprises.But:You almost always want instance variables, which require you to assign inside __init__:Python 2 has some surprising behaviour with comparisons:What's going on? repr() to the rescue:If you assign to a variable inside a function, Python assumes that the variable is defined inside that function:Use global x (or nonlocal x in Python 3) to declare you want to set a variable defined outside your function.The values of range(end_val) are not only strictly smaller than end_val, but strictly smaller than int(end_val). For a float argument to range, this might be an unexpected result: Due to 'truthiness' this makes sense:but you might not expect it to go the other way:This can be a gotcha if you're converting strings to numeric and your data has True/False values.If you create a list of list this way: Then this creates an array with all elements pointing to the same object ! This might create a real confusion. Consider this:then if you print arrThe proper way of initializing the array is for example with a list comprehension:

Short rot13 function - Python [closed]

svenwltr

[Short rot13 function - Python [closed]](https://stackoverflow.com/questions/3269686/short-rot13-function-python)

I am searching for a short and cool rot13 function in Python ;-)

I've written this function:Can anyone make it better? E.g supporting uppercase characters.

2010-07-17 00:33:29Z

I am searching for a short and cool rot13 function in Python ;-)

I've written this function:Can anyone make it better? E.g supporting uppercase characters.Here's a maketrans/translate solutionIt's very simple:This works on Python 2 (but not Python 3):The maketrans and translate functions in the string module are handy for this type of thing.  Of course, the encode method in Amber's response is even handier for this specific case.Here's a general solution:From the module this.py (import this).As of Python 3.1, string.translate and string.maketrans no longer exist. However, these methods can be used with bytes instead.Thus, an up-to-date solution directly inspired from Paul Rubel's one, is:Conversion from string to bytes and vice-versa can be done with the encode and decode built-in functions.Try this:In python-3 the str-codec that @amber mentioned has moved to codecs standard-library:The following function rot(s, n) encodes a string s with ROT-n encoding for any integer n, with n defaulting to 13. Both upper- and lowercase letters are supported. Values of n over 26 or negative values are handled appropriately, e.g., shifting by 27 positions is equal to shifting by one position. Decoding is done with invrot(s, n).A one-liner to rot13 a string S:For arbitrary values, something like this works for 2.xThis works for uppercase and lowercase. I don't know how elegant you deem it to be. You can support uppercase letters on the original code posted by Mr. Walter by alternating the upper case and lower case letters. If you notice the index of the uppercase letters are all even numbers while the index of the lower case letters are odd.  This odd-even pattern allows us to safely add the amount needed without having to worry about the case.  The reason you add 26 is because the string has doubled in letters due to the upper case letters. However, the shift is still 13 spaces on the alphabet. The full code: OUTPUT (Tested with python 2.7):Another solution with shifting. Maybe this code helps other people to understand rot13 better.

Haven't tested it completely.Interesting exercise ;-) i think i have the best solution because: Python 2 & 3 (probably Python 1):I found this post when I started wondering about the easiest way to implement

rot13 into Python myself.  My goals were:This meets all three of those requirements.  That being said, I'm sure it's not winning any code golf competitions.This works by creating a lookup table and simply returning the original character for any character not found in the lookup table.I got to worrying about someone wanting to use this to encrypt an arbitrarily-large file (say, a few gigabytes of text).  I don't know why they'd want to do this, but what if they did?  So I rewrote it as a generator.  Again, this has been tested in both Python 2.7.6 and 3.3.I couldn't leave this question here with out a single statement using the modulo operator.This is not pythonic nor good practice, but it works!You can also use this alsoShort solution:

TypeError: 'str' object is not callable (Python)

P'sao

[TypeError: 'str' object is not callable (Python)](https://stackoverflow.com/questions/6039605/typeerror-str-object-is-not-callable-python)

Code:The first Dict(z, 'w') works and then the second time around it comes up with an error:Does anyone know why this is?@Greg Hewgill: I've already tried that and I get the error:

2011-05-18 03:41:01Z

Code:The first Dict(z, 'w') works and then the second time around it comes up with an error:Does anyone know why this is?@Greg Hewgill: I've already tried that and I get the error:This is the problem:You are redefining what str() means. str is the built-in Python name of the string type, and you don't want to change it. Use a different name for the local variable, and remove the global statement.While not in your code, another hard-to-spot error is when the % character is missing in an attempt of string formatting:but it should be:The missing % would result in the same TypeError: 'str' object is not callable.In my case I had a class that had a method and a string property of the same name, I was trying to call the method but was getting the string property.You can get this error if you have variable str and trying to call str() function.It is important to note (in case you came here by Google) that "TypeError: 'str' object is not callable" means only that a variable that was declared as String-type earlier is attempted to be used as a function (e.g. by adding parantheses in the end.)You can get the exact same error message also, if you use any other built-in method as variable name.Another case of this: Messing with the __repr__ function of an object where a format() call fails non-transparently.In our case, we used a @property decorator on the __repr__ and passed that object to a format().  The @property decorator causes the __repr__ object to be turned into a string, which then results in the str object is not callable error.Check your input parameters, and make sure you don't have one named type. If so then you will have a clash and get this error.An issue I just had was accidentally calling a stringYou can concatenate string by just putting them next to each other like so however because of the open brace in the first example it thought I was trying to call "Foo" it could be also you are trying to index in the wrong way:I had yet another issue with the same error!Turns out I had created a property on a model, but was stupidly calling that property with parentheses. Hope this helps someone!I had the same error. In my case wasn`t because of a variable named str. But because i named a function with a str parameter and the variable the same. I run it in a loop. The first time it run ok. The second time i got this error. Renaming the variable to a name different from the function name fixed this. So I think it´s because Python once associate a function name in a scope, the second time tries to associate the left part ( same_name =) as a call to the function and detects that the str parameter is not present, so it's missing, then it throws that error.Whenever that happens, just issue the following ( it was also posted above)That should fix it.Even I faced issue with the above code  as we are shadowing str() function.Solution is:In my case, I had a Class with a method in it. The method did not have 'self' as the first parameter and the error was being thrown when I made a call to the method. Once I added 'self,' to the method's parameter list, it was fine.

ImportError: No module named 'Queue'

Ali Faki

[ImportError: No module named 'Queue'](https://stackoverflow.com/questions/33432426/importerror-no-module-named-queue)

I am trying to import requests module, but I got this error

my python version is 3.4 running on ubuntu 14.04

2015-10-30 09:17:19Z

I am trying to import requests module, but I got this error

my python version is 3.4 running on ubuntu 14.04import queue is lowercase q in Python 3.Change Q to q and it will be fine.(See code in  https://stackoverflow.com/a/29688081/632951 for smart switching.)Queue is in the multiprocessing module so:I solve the problem my issue was I had file named queue.py in the same directoryIt's because of the Python version. In Python 3 it's  import Queue as queue; on the contrary in Python 2.x it's import queue. If you want it for both environments you may use something below as mentioned here In my case it should be:from multiprocessing import JoinableQueueSince in python2, Queue has methods like .task_done(), but in python3 multiprocessing.Queue doesn't have this method, and multiprocessing.JoinableQueue does.I run into the same problem and learn that queue module defines classes and exceptions, that defines the public methods (Queue Objects).Ex. You need install Queuelib either via the Python Package Index (PyPI) or from source.To install using pip:-To install using easy_install:-If you have downloaded a source tarball you can install it by running the following (as root):-

How to implement the ReLU function in Numpy

Andoni Zubizarreta

[How to implement the ReLU function in Numpy](https://stackoverflow.com/questions/32109319/how-to-implement-the-relu-function-in-numpy)

I want to make a simple neural network and I wish to use the ReLU function. Can someone give me a clue of how can I implement the function using numpy.

Thanks for your time!

2015-08-20 03:58:56Z

I want to make a simple neural network and I wish to use the ReLU function. Can someone give me a clue of how can I implement the function using numpy.

Thanks for your time!There are a couple of ways.If timing the results with the following code:We get:So the multiplication seems to be the fastest.If you don't mind x being modified, use np.maximum(x, 0, x). This was pointed out by Daniel S. It is much faster and because people might overlook it, I'll repost it as an answer. Here's the comparison:I found a faster method for ReLU with numpy. You can use the fancy index feature of numpy as well.fancy index:20.3 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)Here is my benchmark:You can do it in much easier way:Richard Möhn's comparison  is not fair.

As Andrea Di Biagio's comment, the in-place method np.maximum(x, 0, x) will modify x at the first loop.

So here is my benchmark:  Timing it:  Get the results:  In-place maximum method is only a bit faster than the maximum method, and it may because it omits the variable assignment for 'out'. And it's still slower than the multiplication method.

And since you're implementing the ReLU func. You may have to save the 'x' for backprop through relu. E.g.:  So i recommend you to use multiplication method.If we have 3 parameters (t0, a0, a1) for Relu, that is we want to implementWe can use the following code:X there is a matrix.numpy didn't have the function of relu, but you define it by yourself as follow:for example:This is more precise implementation:

Understanding min_df and max_df in scikit CountVectorizer

moeabdol

[Understanding min_df and max_df in scikit CountVectorizer](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer)

I have five text files that I input to a CountVectorizer. When specifying min_df and max_df to the CountVectorizer instance what does the min/max document frequency exactly means? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (5 txt files)?How is it different when min_df and max_df are provided as integers or as floats?The documentation doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of min_df and/or max_df. Could someone provide an explanation or example demonstrating min_df or max_df.

2014-12-29 23:57:13Z

I have five text files that I input to a CountVectorizer. When specifying min_df and max_df to the CountVectorizer instance what does the min/max document frequency exactly means? Is it the frequency of a word in its particular text file or is it the frequency of the word in the entire overall corpus (5 txt files)?How is it different when min_df and max_df are provided as integers or as floats?The documentation doesn't seem to provide a thorough explanation nor does it supply an example to demonstrate the use of min_df and/or max_df. Could someone provide an explanation or example demonstrating min_df or max_df.max_df is used for removing terms that appear too frequently, also known as "corpus-specific stop words". For example:The default max_df is 1.0, which means "ignore terms that appear in more than 100% of the documents". Thus, the default setting does not ignore any terms.min_df is used for removing terms that appear too infrequently. For example:The default min_df is 1, which means "ignore terms that appear in less than 1 document". Thus, the default setting does not ignore any terms.As per the CountVectorizer documentation here.When using a float in the range [0.0, 1.0] they refer to the document frequency. That is the percentage of documents that contain the term.When using an int it refers to absolute number of documents that hold this term.Consider the example where you have 5 text files (or documents). If you set max_df = 0.6 then that would translate to 0.6*5=3 documents. If you set max_df = 2 then that would simply translate to 2 documents.The source code example below is copied from Github here and shows how the max_doc_count is constructed from the max_df. The code for min_df is similar and can be found on the GH page.The defaults for min_df and max_df are 1 and 1.0, respectively. This basically says "If my term is found in only 1 document, then it's ignored. Similarly if it's found in all documents (100% or 1.0) then it's ignored."max_df and min_df are both used internally to calculate max_doc_count and min_doc_count, the maximum and minimum number of documents that a term must be found in. This is then passed to self._limit_features as the keyword arguments high and low respectively, the docstring for self._limit_features isI would add this point also for understanding min_df and max_df in tf-idf better.If you go with the default values, meaning considering all terms, you have generated definitely more tokens. So your clustering process (or any other thing you want to do with those terms later) will take a longer time. BUT the quality of your clustering should NOT be reduced. One might think that allowing all terms (e.g. too frequent terms or stop-words) to be present might lower the quality but in tf-idf it doesn't. Because tf-idf measurement instinctively will give a low score to those terms, effectively making them not influential (as they appear in many documents).So to sum it up, pruning the terms via min_df and max_df is to improve the performance, not the quality of clusters (as an example).And the crucial point is that if you set the min and max mistakenly, you would lose some important terms and thus lower the quality. So if you are unsure about the right threshold (it depends on your documents set), or if you are sure about your machine's processing capabilities, leave the min, max parameters unchanged.The defaults for min_df and max_df are 1 and 1.0, respectively. These defaults really don't do anything at all.  That being said, I believe the currently accepted answer by @Ffisegydd answer isn't quite correct.For example, run this using the defaults, to see that when min_df=1 and max_df=1.0, then 1) all tokens that appear in at least one document are used  (e.g., all tokens!)2) all tokens that appear in all documents are used (we'll test with one candidate: everywhere).  We get:  All tokens are kept. There are no stopwords.  Further messing around with the arguments will clarify other configurations.  For fun and insight, I'd also recommend playing around with stop_words = 'english' and seeing that, peculiarly, all the words except 'seven' are removed! Including `everywhere'.The goal of MIN_DF is to ignore words that have very few occurrences to be considered meaningful. For example, in your text you may have names of people that may appear in only 1 or two documents. In some applications, this may qualify as noise and could be eliminated from further analysis. Similarly, you can ignore words that are too common with MAX_DF.Instead of using a minimum/maximum term frequency (total occurrences of a word) to eliminate words, MIN_DF and MAX_DF look at how many documents contained a term, better known as document frequency. The threshold values can be an absolute value (e.g. 1, 2, 3, 4) or a value representing proportion of documents (e.g. 0.25 meaning, ignore words that have appeared in 25% of the documents) .See some usage examples here.

Python Argument Binders

Dustin Getz

[Python Argument Binders](https://stackoverflow.com/questions/277922/python-argument-binders)

How can I bind arguments to a Python method to store a nullary functor for later invocation?  Similar to C++'s boost::bind.For example:

2008-11-10 14:01:22Z

How can I bind arguments to a Python method to store a nullary functor for later invocation?  Similar to C++'s boost::bind.For example:functools.partial returns a callable wrapping a function with some or all of the arguments frozen.The above usage is equivalent to the following lambda.I'm not overly familiar with boost::bind, but the partial function from functools may be a good start:If functools.partial is not available then it can be easily emulated:Orlambdas allow you to create a new unnamed function with less arguments and call the function!https://docs.python.org/2/library/functools.html#functools.partialif you are planning to use named argument binding in the function call this is also applicable:Please note that if you bind arguments from the left you need to call the arguments by name. If you bind from the right it works as expected.This would work, too:Functors can be defined this way in Python.  They're callable objects.  The "binding" merely sets argument values.You can do things like

How can I make a for-loop pyramid more concise in Python? [duplicate]

Yuxiang Wang

[How can I make a for-loop pyramid more concise in Python? [duplicate]](https://stackoverflow.com/questions/28382433/how-can-i-make-a-for-loop-pyramid-more-concise-in-python)

In solid mechanics, I often use Python and write code that looks like the following:I do this really often that I start to wonder whether there is a more concise way to do this. The drawback of the current code is: if I comply with PEP8, then I cannot exceed the 79-character-limit per line, and there is not too much space left, especially if this is again in a function of a class.

2015-02-07 13:03:32Z

In solid mechanics, I often use Python and write code that looks like the following:I do this really often that I start to wonder whether there is a more concise way to do this. The drawback of the current code is: if I comply with PEP8, then I cannot exceed the 79-character-limit per line, and there is not too much space left, especially if this is again in a function of a class.Based on what you want to do, you can use the itertools module to minimize the for loops (or zip).In this case itertools.product would create what you have done with the 4 loops:And in your code you can do :Edit:As @ PeterE says in comment product() can be used even if the ranges have differing length :The idea to use itertools.product is a good one. Here's a more general approach that will support ranges of varying sizes.It won't be more concise as it will cost you a generator function, but at least you won't be bothered by PEP8 :(in python 2.x you should use xrange instead of range in the generator function)EDIT:Above method should be fine when the depth of the pyramid is known. But you can also make a generic generator that way without any external module :(I used (l, k, j, i) because in above generator, first index varies first)This is equivalent:If you're doing this all the time, consider using a general generator for it:

Get time zone information of the system in Python?

Josh Gibson

[Get time zone information of the system in Python?](https://stackoverflow.com/questions/1111056/get-time-zone-information-of-the-system-in-python)

I want to get the default timezone (PST) of my system from Python.  What's the best way to do that?  I'd like to avoid forking another process.

2009-07-10 17:59:28Z

I want to get the default timezone (PST) of my system from Python.  What's the best way to do that?  I'd like to avoid forking another process.Check out the Python Time Module.Pacific Standard TimeThis should work:time.tzname returns a tuple of two strings: The first is the name of the local non-DST timezone, the second is the name of the local DST timezone.Example return: ('MST', 'MDT')Gives a UTC offset like in ThomasH's answer, but takes daylight savings into account.The value of time.timezone or time.altzone is in seconds West of UTC (with areas East of UTC getting a negative value). This is the opposite to how we'd actually like it, hence the * -1.time.localtime().tm_isdst will be zero if daylight savings is currently not in effect (although this may not be correct if an area has recently changed their daylight savings law).EDIT: marr75 is correct, I've edited the answer accordingly.I found this to work well:For me this was able to differentiate between daylight savings and not.Reference with more detail: https://stackoverflow.com/a/39079819/4549682The code snippets for calculating offset are incorrect, see http://bugs.python.org/issue7229.The correct way to handle this is:This is in all likelihood, not the exact question that the OP asked, but there are two incorrect snippets on the page and time bugs suck to track down and fix.If you prefer UTC offsets over strings:

Python Glob without the whole path - only the filename

Tom Zych

[Python Glob without the whole path - only the filename](https://stackoverflow.com/questions/7336096/python-glob-without-the-whole-path-only-the-filename)

Is there a way I can use glob on a directory, to get files with a specific extension, but only the filename itself, not the whole path?

2011-09-07 15:02:31Z

Is there a way I can use glob on a directory, to get files with a specific extension, but only the filename itself, not the whole path?Use os.path.basename(path) to get the filename.This might help someone:names = [os.path.basename(x) for x in glob.glob('/your_path')]Use glob in combination with os.path.basename.I keep rewriting the solution for relative globbing (esp. when I need to add items to a zipfile) - this is what it usually ends up looking like.os.path.basename works for me.Here is Code example:Output :Returns an iterable with all the file names and extensions.

Passing values in Python [duplicate]

Joan Venge

[Passing values in Python [duplicate]](https://stackoverflow.com/questions/534375/passing-values-in-python)

When you pass a collection like list, array to another function in python, does it make a copy of it, or is it just a pointer?

2009-02-10 21:51:43Z

When you pass a collection like list, array to another function in python, does it make a copy of it, or is it just a pointer?Python passes references-to-objects by value.Thing is, the whole reference/value concept won't fit into python. Python has no "value" of a variable. Python has only objects and names that refer to objects.So when you call a function and put a "name" inside the parenthesis, like this:The actual object that myname is pointing is passed, not the name myname itself. Inside the function another name (x) is given to refer to the same object passed. You can modify the object inside the function if it is mutable, but you can't change what the outside name is pointing to. Just the same that happens when you do Therefore I can answer your question with:  it is "pass by value" but all values are just references to objects.Answers here have been helpful, but I find the need to exhibit this fine distinction which I haven't seen covered, which I've proven to myself with the subsequent CL experiment:'num' does not change here because it is an immutable Number object [supports my point 1.]:'list[0]' here is an immutable Number object also.So how did 'list[0]', being an immutable Number object, change (supports my point 2.) while the above example's Number object 'num' did not?  The immutable Number object 'list[0]' is contained within the mutable list object 'list',  while 'num' from the 1st example is just a non-contianed Number object.Although well-intended, I feel @Stephen Pape top-rated answer (quoted below), and some other similar ones, were not totally correct (and that motivated me to write this answer):My 2nd code experiment above shows a Number object ('list[0]') being altered within a method, and then the original instance outside the function changed.A reference is passed, but if the parameter is an immutable object, modifying it within the method will create a new instance.The object is passed.  Not a copy, but a reference to the underlying object.I would also recommend looking at the copy module:Python documentation for copyIt will help you to understand the underlying issues and how to use it to perform your own deep copy.By reference:Please let me give a humble examplewhich produces the following result28329344 var1

28331264 var2

28329344 x

28329344 a

28331264 b

After a = b

28331264 a

after b = x

28329344 b

after return

28329344 var1

28331264 var2

['1', '2', '3', '4']

['20', '6', '7', '8', '9']Mapping to the memory addresses

28329344                 28331264 

var1                     var2

a                        b

x

After a=b

                         a

After b=x

b

After a[0] = '20'

                         [0] = '20'

After return

['1','2','3','4']        ['20', '6', '7', '8', '9']

Remove nodes from graph or reset entire default graph

Mohammed AlQuraishi

[Remove nodes from graph or reset entire default graph](https://stackoverflow.com/questions/33765336/remove-nodes-from-graph-or-reset-entire-default-graph)

When working with the default global graph, is it possible to remove nodes after they've been added, or alternatively to reset the default graph to empty? When working with TF interactively in IPython, I find myself having to restart the kernel repeatedly. I would like to be able to experiment with graphs more easily if possible.

2015-11-17 19:24:07Z

When working with the default global graph, is it possible to remove nodes after they've been added, or alternatively to reset the default graph to empty? When working with TF interactively in IPython, I find myself having to restart the kernel repeatedly. I would like to be able to experiment with graphs more easily if possible.Update 11/2/2016tf.reset_default_graph()Old stuffThere's reset_default_graph, but not part of public API (I think it should be, does someone wants to file an issue on GitHub?)My work-around to reset things is this:By default, a session is constructed around the default graph.

To avoid leaving dead nodes in the session, you need to either control the default graph or use an explicit graph.EDIT: For recent versions of tensorflow (1.0+), the correct function is g.as_default.IPython / Jupyter notebook cells keep state between runs of a cell.Create a custom graph:Once ran, the graph is cleaned up.Tensorflow 2.0 Compatible Answer: In Tensorflow Version >= 2.0, the Command to Reset Entire Default Graph, when run in Graph Mode is tf.compat.v1.reset_default_graph.NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a tf.compat.v1.Session or tf.compat.v1.InteractiveSession is active will result in undefined behavior. Using any previously created tf.Operation or tf.Tensor objects after calling this function will result in undefined behavior. Raises: AssertionError: If this function is called within a nested graph.Not sure if I faced the very same problem, butat the beginning of the cell in which the model (Keras, in my case) was constructed and trained helped to "cut the clutter" so only the current graph remains in the TensorBoard visualization after repeated runs of the same cell.Environment: TensorFlow 2.0 (tensorflow-gpu==2.0.0b1) in Colab with built-in TensorBoard (using the %load_ext tensorboard trick).

Changing file permission in Python

Abul Hasnat

[Changing file permission in Python](https://stackoverflow.com/questions/16249440/changing-file-permission-in-python)

I am trying to change permission of a file access:I want to make it read-only:Is there any other way make a file read-only?

2013-04-27 07:17:12Z

I am trying to change permission of a file access:I want to make it read-only:Is there any other way make a file read-only?statos.chmod(path, 0444) is the Python command for changing file permissions in Python 2.x. For a combined Python 2 and Python 3 solution, change 0444 to 0o444.You could always use Python to call the chmod command using subprocess. I think this will only work on Linux though.Simply include permissions integer in octal (works for both python 2 and python3):All the current answers clobber the non-writing permissions: they make the file readable-but-not-executable for everybody.  Granted, this is because the initial question asked for 444 permissions -- but we can do better!Here's a solution that leaves all the individual "read" and "execute" bits untouched.  I wrote verbose code to make it easy to understand; you can make it more terse if you like.Why does this work?As John La Rooy pointed out,stat.S_IWUSR basically means "the bitmask for the user's write permissions".  We want to set the corresponding permission bit to 0.  To do that, we need the exact opposite bitmask (i.e., one with a 0 in that location, and 1's everywhere else).  The ~ operator, which flips all the bits, gives us exactly that.  If we apply this to any variable via the "bitwise and" operator (&), it will zero out the corresponding bit.We need to repeat this logic with the "group" and "other" permission bits, too.  Here we can save some time by just &'ing them all together (forming the NO_WRITING bit constant).The last step is to get the current file's permissions, and actually perform the bitwise-and operation.No need to remember flags. Remember that you can always do:subprocess.call(["chmod", "a-w", "file/path])Not portable but easy to write and remember:Refer man chmod for additional options and more detailed explanation.FYI here is a function to convert a permission string with 9 characters (e.g. 'rwsr-x-wt') to a mask that can be used with os.chmod().Note that setting SUID/SGID/SVTX bits will automatically set the corresponding execute bit. Without this, the resulting permission would be invalid (ST characters). Just add 0 before the permission number: 

For example - we want to give all permissions - 777 

Syntax: os.chmod("file_name" , permission)Python version 3.7 does not support this syntax. It requires '0o' prefix for octal literals - this is the comment I have got in PyCharmSo for python 3.7, it will be 

ImportError: No module named tensorflow

Abhishek Gangwar

[ImportError: No module named tensorflow](https://stackoverflow.com/questions/42244198/importerror-no-module-named-tensorflow)

Please help me with this errorI have installed the tensorflow module on my server and below is it's informationBut when I try to import tensorflow I get following errorMy python version is as followingI have tried the solutions given in 

sol1Sol2I do not have sudo access to the server 

I can only use pip to install any module

2017-02-15 08:30:59Z

Please help me with this errorI have installed the tensorflow module on my server and below is it's informationBut when I try to import tensorflow I get following errorMy python version is as followingI have tried the solutions given in 

sol1Sol2I do not have sudo access to the server 

I can only use pip to install any moduleTry installing tensorflow again with the whatever version you want and with option --ignore-installed like:I solved same issue using this command.I had a more basic problem when I received this error.The "Validate your installation" instructions say to type: pythonHowever, I have both 2.7 and 3.6 installed.  Because I used pip3 to install tensorflow, I needed to type: python3Using the correct version, I could import the "tensorflow" module.Check if Tensorflow was installed successfully using:If you get something likeYou may try adding the path of your tensorflow location by:For Anaconda3, simply install in Anaconda Navigator:

Try installing tensorflow in the user site - This installation only works for you.You may need this since first one may not work.python3 -m pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0-py3-none-any.whl

with python2pip show tensorflow  to check installpython test.py  to run testwith python3pip3 show tensorflow to check installpython3 test.py to run testtest.pyOr, if you haven't install tensorflow yet, try the offical documentyou might wanna try this:I was trying to install tensorflow GPU for a Windows 7 with pip3 for python3.5.x. 

Instead of doing pip3 install --upgrade tensorflow I just did pip install tensorflow and after it completed i was finally able to import tensorflow in python3.5.x.Instead of using the doc's command (conda create -n tensorflow pip python=2.7 # or python=3.3, etc.) which wanted to install python2.7 in the conda environment, and kept erroring out saying the module can't be found when following the installation validation steps, I used conda create -n tensorflow pip python=3 to make sure python3 was installed in the environment. Doing this, I only had to type python instead of python3 when validating the installation and the error went away.Try Anaconda install steps from TensorFlow docs.Activate the virtualenv environment by issuing one of the following commands:$ source ~/tensorflow/bin/activate # bash, sh, ksh, or zsh

$ source ~/tensorflow/bin/activate.csh  # csh or tcshHope this helpThis Worked for me:In my case, I install 32 Bit Python so I cannot install Tensorflow, After uninstall 32 Bit Python and install 64 Bit Python, I can install tensorflow successfully.After reinstall Python 64 bit, you need to check your python install folder path is properly set in Windows Environment Path.You can check Python version by typing python in cmd.For me, if I didthen I got the error the OP reports when using a 3rd party library calling tensorflow.However, when I substituted either tensorflow-cpu or tensorflow-gpu (depending upon which one is appropriate for you) then the code was suddenly able to find tensorflow.I had exactly the same problem.

It's because default python is in version 2

You need to link to the version 3.I ran into the same issue. I simply updated my command to begin with python3 instead of python and it worked perfectly.

Checking File Permissions in Linux with Python

Jon Phenow

[Checking File Permissions in Linux with Python](https://stackoverflow.com/questions/1861836/checking-file-permissions-in-linux-with-python)

I'm writing a script to check permissions of files in user's directories and if they're not acceptable I'll be warning them, but I want to check permissions of not just the logged in user, but also group and others. How can i do this? It seems to me that os.access() in Python can only check the permissions for the user running the script.

2009-12-07 18:10:31Z

I'm writing a script to check permissions of files in user's directories and if they're not acceptable I'll be warning them, but I want to check permissions of not just the logged in user, but also group and others. How can i do this? It seems to me that os.access() in Python can only check the permissions for the user running the script.You're right that os.access, like the underlying access syscall, checks for a specific user (real rather than effective IDs, to help out with suid situations).os.stat is the right way to get more general info about a file, including permissions per user, group, and others.  The st_mode attribute of the object that os.stat returns has the permission bits for the file.To help interpret those bits, you may want to use the stat module.  Specifically, you'll want the bitmasks defined here, and you'll use the & operator (bit-and) to use them to mask out the relevant bits in that st_mode attribute -- for example, if you just need a True/False check on whether a certain file is group-readable, one approach is:Take care: the os.stat call can be somewhat costly, so make sure to extract all info you care about with a single call, rather than keep repeating calls for each bit of interest;-).Just to help other people like me who came here for something a bit different : See this for more details : https://stackoverflow.com/a/5337329/1814774You can check file permissions via os.stat(path) in conjunction with the stat module for interpreting the results.Use os.access() with flags os.R_OK, os.W_OK, and os.X_OK.Edit: Check out this related question if you are testing directory permissions on Windows.os.stat and the associated bit masks for the mode.

How do I manage third-party Python libraries with Google App Engine? (virtualenv? pip?)

a paid nerd

[How do I manage third-party Python libraries with Google App Engine? (virtualenv? pip?)](https://stackoverflow.com/questions/4863557/how-do-i-manage-third-party-python-libraries-with-google-app-engine-virtualenv)

What's the best strategy for managing third-party Python libraries with Google App Engine?Say I want to use Flask, a webapp framework. A blog entry says to do this, which doesn't seem right:There must be a better way to manage third-party code, especially if I want to track versions, test upgrades or if two libraries share a subdirectory. I know that Python can import modules from zipfiles and that pip can work with a wonderful REQUIREMENTS file, and I've seen that pip has a zip command for use with GAE.(Note: There's a handful of similar questions — 1, 2, 3, 4, 5 — but they're case-specific and don't really answer my question.)

2011-02-01 14:10:49Z

What's the best strategy for managing third-party Python libraries with Google App Engine?Say I want to use Flask, a webapp framework. A blog entry says to do this, which doesn't seem right:There must be a better way to manage third-party code, especially if I want to track versions, test upgrades or if two libraries share a subdirectory. I know that Python can import modules from zipfiles and that pip can work with a wonderful REQUIREMENTS file, and I've seen that pip has a zip command for use with GAE.(Note: There's a handful of similar questions — 1, 2, 3, 4, 5 — but they're case-specific and don't really answer my question.)What about simply:Create/edit <your_app_directory>/appengine_config.py:Google updated their sample to appengine_config.py, like:Note: Even though their example has .gitignore ignoring lib/ directory you still need to keep that directory under source control if you use git-push deployment method.Here's how I do it:The project directory is the top level directory where the virtualenv sits. I get the virtualenv using the following commands:The src directory is where all your code goes. When you deploy your code to GAE, *only* deploy those in the src directory and nothing else. The appcfg.py will resolve the symlinks and copy the library files to GAE for you.I don't install my libraries as zip files mainly for convenience in case I need to read the source code, which I happen to do a lot just out of curiosity. However, if you really want to zip the libraries, put the following code snippet into your main.pyAfter this you can import your zipped up packages as usual.One thing to watch out for is setuptools' pkg_resources.py. I copied that directly into my src directory so my other symlinked packages can use it. Watch out for anything that uses entry_points. In my case I'm using Toscawidgets2 and I had to dig into the source code to manually wire up the pieces. It can become annoying if you had a lot of libraries that rely on entry_point.I prefer buildout.You set up dependencies in setup.py in your project or buildout.cfg, pin the versions in buildout.cfg, and specify which packages are not available on GAE and should be included in packages.zip. rod.recipe.appengine will copy required packages into packages.zip, and as long as you insert packages.zip into the sys.path, they can be imported anywhere.You can also use forks from github if the package you need is not on pypiand all of these settings and dependencies are versioned in git.Instead of wondering which copy of Flask is currently included in your source tree and perhaps copied into your version control (or requiring new developers to manually unpack and upgrade), you simply check the version in buildout.cfg. If you want a new version, change buildout.cfg and rerun buildout.You can also use it to insert variables into config file templates, like setting the appspot id and version in app.yaml if you have staging server with staging.cfg and so on.I recently created a tool for this called gaenv. It follows a requirements.txt format, but doesn't install it, you can install with pip install -r requirements.txt then run the command line tool gaenv.This creates symlinks automatically, you could install gaenv in your virtualenv too and run the binary from there.

Here is a blog post about it: http://blog.altlimit.com/2013/06/google-app-engine-virtualenv-tool-that.htmlalso on githubhttps://github.com/faisalraja/gaenvWernight's solution is the closest to current practice in the official Flask example app, which I've already improved by changing the sys.path.insert() call to site.addsitedir() in order to allow for namespace packages by processing their attendant .pth files (which are important for frameworks like Pyramid).So far so good, but that appends the directory to the path, and so loses the opportunity to override the included libraries (like WebOb and requests) with newer versions.What is needed then in appengine_config.py (and I am trying to get this change accepted into the official repos as well) is the following:The final version of this code may end up hidden away in a vendor.py module and called like insertsitedir(index, path) or some other variation, as you can see in the discussion attending this pull request, but the logic is more or less how it will work regardless, to allow a simple pip install -r requirements.txt -t lib/ to work for all packages including namespace ones, and to still allow overriding the included libraries with new versions, as I have so far been unable to find a simpler alternative.Note: this answer is specific for Flask on Google App Engine.See the flask-appengine-template project for an example of how to get Flask extensions to work on App Engine.

https://github.com/kamalgill/flask-appengine-templateDrop the extension into the namespace package folder at src/packages/flaskext and you're all set.

https://github.com/kamalgill/flask-appengine-template/tree/master/src/lib/flaskextNon-Flask packages can be dropped into the src/packages folder as zip files, eggs, or unzipped packages, as the project template includes the sys.path.insert() snippet posted above.

Increment a Python floating point value by the smallest possible amount

James

[Increment a Python floating point value by the smallest possible amount](https://stackoverflow.com/questions/6063755/increment-a-python-floating-point-value-by-the-smallest-possible-amount)

I'm using floating point values as dictionary keys.Occasionally, very occasionally (and perhaps never, but not certainly never), there will be collisions. I would like to resolve these by incrementing the floating point value by as small an amount as possible. How can I do this?In C, I would twiddle the bits of the mantissa to achieve this, but I assume that isn't possible in Python.

2011-05-19 19:16:52Z

I'm using floating point values as dictionary keys.Occasionally, very occasionally (and perhaps never, but not certainly never), there will be collisions. I would like to resolve these by incrementing the floating point value by as small an amount as possible. How can I do this?In C, I would twiddle the bits of the mantissa to achieve this, but I assume that isn't possible in Python.You are not crazy and you should be able to do this. It is a current shortcoming of the Python math library, sadly, both in Python 2.X and Python3000. There should be a math.nextafter(x,y) in Python but there isn't. It would be trivial to add since most C compilers have the functions. The nextafter(x,y) functions return the next discretely different representable floating-point value following x in the direction of y. The nextafter() functions are guaranteed to work on the platform or to return a sensible value to indicate that the next value is not possible. The nextafter() functions are part of POSIX and ISO C99 standards and is _nextafter() in Visual C. C99 compliant standard math libraries, Visual C, C++, Boost and Java all implement the IEEE recommended nextafter() functions or methods. (I do not honestly know if .NET has nextafter(). Microsoft does not care much about C99 or POSIX.)Since Python seems to be heading in the direction of supporting most C99 math functions and behaviors for the math module, the exclusion of nextafter() is curious. Luckily there are easy workarounds. None of the bit twiddling functions here fully or correctly deal with the edge cases, such as values going though 0.0, negative 0.0, subnormals, infinities, negative values, over or underflows, etc. Here is a reference implementation of nextafter() in C to give an idea of how to do the correct bit twiddling if that is your direction. There are two solid work arounds to get nextafter() or other excluded POSIX math functions in Python:Use Numpy:Link directly to the system math DLL:And if you really really want a pure Python solution:Or, use Mark Dickinson's excellent solution Obviously the Numpy solution is the easiest. First, this "respond to a collision" is a pretty bad idea.If they collide, the values in the dictionary should have been lists of items with a common key, not individual items.Your "hash probing" algorithm will have to loop through more than one "tiny increments" to resolve collisions.And sequential hash probes are known to be inefficient.Read this: http://en.wikipedia.org/wiki/Quadratic_probingSecond, use math.frexp and sys.float_info.epsilon to fiddle with mantissa and exponent separately.I recommend against assuming that floats (or timestamps) will be unique if at all possible.  Use a counting iterator, database sequence or other service to issue unique identifiers.Instead of incrementing the value, just use a tuple for the colliding key. If you need to keep them in order, every key should be a tuple, not just the duplicates.A better answer (now I'm just doing this for fun...), motivated by twiddling the bits. Handling the carry and overflows between parts of the number of negative values is somewhat tricky.Forgetting about why we would want to increment a floating point value for a moment, I would have to say I think Autopulated's own answer is probably correct.But for the problem domain, I share the misgivings of most of the responders to the idea of using floats as dictionary keys.  If the objection to using Decimal (as proposed in the main comments) is that it is a "heavyweight" solution, I suggest a do-it-yourself compromise:  Figure out what the practical resolution is on the timestamps, pick a number of digits to adequately cover it, then multiply all the timestamps by the necessary amount so that you can use integers as the keys.  If you can afford an extra digit or two beyond the timer precision, then you can be even more confident that there will be no or fewer collisions, and that if there are collisions, you can just add 1 (instead of some rigamarole to find the next floating point value).Interesting problem. The amount you need to add obviously depends on the magnitude of the colliding value, so that a normalized add will affect only the least significant bits.It's not necessary to determine the smallest value that can be added. All you need to do is approximate it.  The FPU format provides 52 mantissa bits plus a hidden bit for 53 bits of precision. No physical constant is known to anywhere near this level of precision. No sensor is able measure anything near it. So you don't have a hard problem.In most cases, for key k, you would be able to add k/253, because of that 52-bit fraction plus the hidden bit.But it's not necessary to risk triggering library bugs or exploring rounding issues by shooting for the very last bit or anything near it.1. Possibly more than once until it doesn't collide any more, at least to foil any diabolical unit test authors.Instead of resolving the collisions by changing the key, how about collecting the collisions?  IE: becomeswould that work?Instead of modifying your float timestamp, use a tuple for every key as Mark Ransom suggests where the tuple (x,y) is composed of x=your_unmodified_time_stamp and y=(extremely unlikely to be a same value twice). So: While 2.1 (random int from a large range) there works great for ethernet, I would use 2.2 (serializer) or 2.3 (UUID). Easy, fast, bulletproof. For 2.2 and 2.3 you don't even need collision detection (you might want to still have it for 2.1 as ethernet does.)The advantage of 2.2 is that you can also tell, and sort, data elements that have the same float time stamp. Then just extract x from the tuple for any sorting type operations and the tuple itself is a collision free key for the hash / dictionary. EditI guess example code will help:Output:Here it part of it.  This is dirty and slow, but maybe that is how you like it.  It is missing several corner cases, but maybe this gets someone else close.The idea is to get the hex string of a floating point number.  That gives you a string with the mantissa and exponent bits to twiddle.  The twiddling is a pain since you have to do all it manually and keep converting to/from strings.  Anyway, you add(subtract) 1 to(from) the last digit for positive(negative) numbers.  Make sure you carry through to the exponent if you overflow.  Negative numbers are a little more tricky to make you don't waste any bits.I think you mean "by as small an amount possible to avoid a hash collision", since for example the next-highest-float may already be a key! =)That said you probably don't want to be using timestamps as keys.After Looking at Autopopulated's answer I came up with a slightly different answer:Disclaimer: I'm really not as great at maths as I think I am ;)  Please verify this is correct before using it. Also I'm not sure about performancesome notes:It turns out that this is actually quite complicated (maybe why seven people have answered without actually providing an answer yet...).I think this is the right solution, it certainly seems to handle 0 and positive values correctly:

Making an API call in Python with an API that requires a bearer token

user4657

[Making an API call in Python with an API that requires a bearer token](https://stackoverflow.com/questions/29931671/making-an-api-call-in-python-with-an-api-that-requires-a-bearer-token)

Looking for some help with integrating a JSON API call into a Python program.I am looking to integrate the following API into a Python .py program to allow it to be called and the response to be printed.The API guidance states that a bearer token must be generated to allow calls to the API, which I have done successfully. However I am unsure of the syntax to include this token as bearer token authentication in Python API request.I can successfully complete the above request using cURL with a token included. I have tried "urllib" and "requests"  routes but to no avail.Full API details: IBM X-Force Exchange API Documentation - IP Reputation

2015-04-28 23:25:32Z

Looking for some help with integrating a JSON API call into a Python program.I am looking to integrate the following API into a Python .py program to allow it to be called and the response to be printed.The API guidance states that a bearer token must be generated to allow calls to the API, which I have done successfully. However I am unsure of the syntax to include this token as bearer token authentication in Python API request.I can successfully complete the above request using cURL with a token included. I have tried "urllib" and "requests"  routes but to no avail.Full API details: IBM X-Force Exchange API Documentation - IP ReputationIt just means it expects that as a key in your header dataThe token has to be placed in an Authorization header according to the following format:Authorization: Bearer [Token_Value]If you are using requests module, an alternative option is to write an auth class, as discussed in "New Forms of Authentication":and then can you send requests like thiswhich allows you to use the same auth argument just like basic auth, and may help you in certain situations.

Converting a list to a string [duplicate]

PARIJAT

[Converting a list to a string [duplicate]](https://stackoverflow.com/questions/2906092/converting-a-list-to-a-string)

I have extracted some data from a file and want to write it to a second file. But my program is returning the error:This appears to be happening because write() wants a string but it is receiving a list.So, with respect to this code, how can I convert the list buffer to a string so that I can save the contents of buffer to file2?

2010-05-25 15:29:33Z

I have extracted some data from a file and want to write it to a second file. But my program is returning the error:This appears to be happening because write() wants a string but it is receiving a list.So, with respect to this code, how can I convert the list buffer to a string so that I can save the contents of buffer to file2?Try str.join:Documentation says:Explanation:

str(anything) will convert any python object into its string representation. Similar to the output you get if you do print(anything), but as a string.NOTE: This probably isn't what OP wants, as it has no control on how the elements of buffer are concatenated -- it will put , between each one -- but it may be useful to someone else.will give "'a','b','c'" as outputMethod 1:Method 2:Method 3:From the official Python Programming FAQ for Python 3.6.4:

Handling urllib2's timeout? - Python

RadiantHex

[Handling urllib2's timeout? - Python](https://stackoverflow.com/questions/2712524/handling-urllib2s-timeout-python)

I'm using the timeout parameter within the urllib2's urlopen.How do I tell Python that if the timeout expires a custom error should be raised?Any ideas?

2010-04-26 10:03:37Z

I'm using the timeout parameter within the urllib2's urlopen.How do I tell Python that if the timeout expires a custom error should be raised?Any ideas?There are very few cases where you want to use except:. Doing this captures any exception, which can be hard to debug, and it captures exceptions including SystemExit and KeyboardInterupt, which can make your program annoying to use..At the very simplest, you would catch urllib2.URLError:The following should capture the specific error raised when the connection times out:In Python 2.7.3:

setup.py and adding file to /bin/

Aaron Yodaiken

[setup.py and adding file to /bin/](https://stackoverflow.com/questions/4840182/setup-py-and-adding-file-to-bin)

I can't figure out how to make setup.py add a scrip to the the user's /bin or /usr/bin or whatever.E.g., I'd like to add a myscript.py to /usr/bin so that the user can call myscript.py from any directory.

2011-01-29 23:41:21Z

I can't figure out how to make setup.py add a scrip to the the user's /bin or /usr/bin or whatever.E.g., I'd like to add a myscript.py to /usr/bin so that the user can call myscript.py from any directory.The Python documentation explains it under the installing scripts section.Consider using console_scripts:Where main_func_name is a main function in your main module.

command-name is a name under which it will be saved in /usr/local/bin/ (usually)There are two ways in order to get a working command line tool from setuptools and PyPI infrastructure:If you're willing to build and install the entire python package, this is how I would go about it:setup(name='myproject',author='',author_email='',scripts=['bin/myscript.py'])

In TensorFlow, what is tf.identity used for?

rd11

[In TensorFlow, what is tf.identity used for?](https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for)

I've seen tf.identity used in a few places, such as the official CIFAR-10 tutorial and the batch-normalization implementation on stackoverflow, but I don't see why it's necessary.What's it used for? Can anyone give a use case or two?One proposed answer is that it can be used for transfer between the CPU and GPU. This is not clear to me. Extension to the question, based on this: loss = tower_loss(scope) is under the GPU block, which suggests to me that all operators defined in tower_loss are mapped to the GPU. Then, at the end of tower_loss, we see total_loss = tf.identity(total_loss) before it's returned. Why? What would be the flaw with not using tf.identity here?

2016-01-19 13:01:41Z

I've seen tf.identity used in a few places, such as the official CIFAR-10 tutorial and the batch-normalization implementation on stackoverflow, but I don't see why it's necessary.What's it used for? Can anyone give a use case or two?One proposed answer is that it can be used for transfer between the CPU and GPU. This is not clear to me. Extension to the question, based on this: loss = tower_loss(scope) is under the GPU block, which suggests to me that all operators defined in tower_loss are mapped to the GPU. Then, at the end of tower_loss, we see total_loss = tf.identity(total_loss) before it's returned. Why? What would be the flaw with not using tf.identity here?After some stumbling I think I've noticed a single use case that fits all the examples I've seen. If there are other use cases, please elaborate with an example.Use case:Suppose you'd like to run an operator every time a particular Variable is evaluated. For example, say you'd like to add one to x every time the variable y is evaluated. It might seem like this will work:It doesn't: it'll print 0, 0, 0, 0, 0. Instead, it seems that we need to add a new node to the graph within the control_dependencies block. So we use this trick:This works: it prints 1, 2, 3, 4, 5.If in the CIFAR-10 tutorial we dropped tf.identity, then loss_averages_op would never run.tf.identity is useful when you want to explicitly transport tensor between devices (like, from GPU to a CPU).

The op adds send/recv nodes to the graph, which make a copy when the devices of the input and the output are different.A default behavior is that the send/recv nodes are added implicitly when the operation happens on a different device but you can imagine some situations (especially in a multi-threaded/distributed settings) when it might be useful to fetch the value of the variable multiple times within a single execution of the session.run. tf.identity allows for more control with regard to when the value should be read from the source device. Possibly a more appropriate name for this op would be read.Also, please note that in the implementation of tf.Variable link, the identity op is added in the constructor, which makes sure that all the accesses to the variable copy the data from the source only once. Multiple copies can be expensive in cases when the variable lives on a GPU but it is read by multiple CPU ops (or the other way around). Users can change the behavior with multiple calls to tf.identity when desired.EDIT: Updated answer after the question was edited.In addition, tf.identity can be used used as a dummy node to update a reference to the tensor. This is useful with various control flow ops. In the CIFAR case we want to enforce that the ExponentialMovingAverageOp will update relevant variables before retrieving the value of the loss. This can be implemented as:Here, the tf.identity doesn't do anything useful aside of marking the total_loss tensor to be ran after evaluating loss_averages_op.In addition to the above, I simply use it when I need to assign a name to ops that do not have a name argument, just like when initializing a state in RNN's:I came across another use case that is not completely covered by the other answers.Most of the time when constructing a convolutional layer, you just want the activations returned so you can feed those into the next layer. Sometimes, however - for example when building an auto-encoder - you want the pre-activation values.In this situation an elegant solution is to pass tf.identity as the activation function, effectively not activating the layer.I found another application of tf.identity in Tensorboard.

If you use tf.shuffle_batch, it returns multiple tensors at once, so you see messy picture when visualizing the graph, you can't split tensor creation pipeline from actiual input tensors: messyBut with tf.identity you can create duplicate nodes, which don't affect computation flow: niceWhen our input data is serialized in bytes, and we want to extract features from this dataset. We can do so in key-value format and then get a placeholder for it. Its benefits are more realised when there are multiple features and each feature has to be read in different format.In distribution training, we should use tf.identity or the workers will hang at waiting for initialization of the chief worker:For details, without identity, the chief worker would treat some variables as local variables inappropriately and the other workers wait for an initialization operation that can not endI see this kind of hack to check assert:Also it's used just to give a name:

Python Date Comparisons

Ryan White

[Python Date Comparisons](https://stackoverflow.com/questions/130618/python-date-comparisons)

I would like to find out if a particular python datetime object is older than X hours or minutes. I am trying to do something similar to:  This generates a type error. What is the proper way to do date time comparison in python? I already looked at WorkingWithTime which is close but not exactly what I want. I assume I just want the datetime object represented in seconds so that I can do a normal int comparison. Please post lists of datetime best practices.

2008-09-24 23:34:31Z

I would like to find out if a particular python datetime object is older than X hours or minutes. I am trying to do something similar to:  This generates a type error. What is the proper way to do date time comparison in python? I already looked at WorkingWithTime which is close but not exactly what I want. I assume I just want the datetime object represented in seconds so that I can do a normal int comparison. Please post lists of datetime best practices.Use the datetime.timedelta class:Your example could be written as:orCompare the difference to a timedelta that you create:Alternative:Assuming self.timestamp is an datetime instanceYou can use a combination of the 'days' and 'seconds' attributes of the returned object to figure out the answer, like this:Use abs() in the answer if you always want a positive number of seconds.To discover how many seconds into the past a timestamp is, you can use it like this:You can subtract two datetime objects to find the difference between them.

You can use datetime.fromtimestamp to parse a POSIX time stamp.Like so:

Python Lambda in a loop

FunkySayu

[Python Lambda in a loop](https://stackoverflow.com/questions/19837486/python-lambda-in-a-loop)

Considering the following code snippet :I expect to create a dictionary of two function as following :but it looks like the two lambda function generated are exactly the same :I really don't understand why. Do you have any suggestions ?

2013-11-07 13:42:43Z

Considering the following code snippet :I expect to create a dictionary of two function as following :but it looks like the two lambda function generated are exactly the same :I really don't understand why. Do you have any suggestions ?You need to bind d for each function created. One way to do that is to pass it as a parameter with a default value:Now the d inside the function uses the parameter, even though it has the same name, and the default value for that is evaluated when the function is created. To help you see this:Remember how default values work, such as for mutable objects like lists and dicts, because you are binding an object.This idiom of parameters with default values is common enough, but may fail if you introspect function parameters and determine what to do based on their presence.  You can avoid the parameter with another closure:This is due to the point at which d is being bound. The lambda functions all point at the variable d rather than the current value of it, so when you update d in the next iteration, this update is seen across all your functions.For a simpler example:You can get around this by adding an additional function, like so:You can also fix the scoping inside the lambda expression However in general this is not good practice as you have changed the signature of your function.Alternatively, instead of lambda, you can use functools.partial which, in my opinion, has a cleaner syntax. Instead of:it will be:Or, here is another simple example:I met the same problem. The selected solution helped me a lot, but I consider necessary to add a precision to make functional the code of the question: define the lambda function outside of the loop. By the way, default value is not necessary.

Pandas Plotting with Multi-Index

Reustonium

[Pandas Plotting with Multi-Index](https://stackoverflow.com/questions/25386870/pandas-plotting-with-multi-index)

After performing a groupby.sum() on a DataFrame I'm having some trouble trying to create my intended plot.How can I create a subplot (kind='bar') for each Code, where the x-axis is the Month and the bars are ColA and ColB?

2014-08-19 15:05:48Z

After performing a groupby.sum() on a DataFrame I'm having some trouble trying to create my intended plot.How can I create a subplot (kind='bar') for each Code, where the x-axis is the Month and the bars are ColA and ColB?I found the unstack(level) method to work perfectly, which has the added benefit of not needing a priori knowledge about how many Codes there are.Using the following DataFrame ...... you can plot the following (using cross-section):One for A, one for B and one for C, x-axis: 'Month', the bars are ColA and ColB.

Maybe this is what you are looking for.

Easiest way to rm -rf in Python

Josh Gibson

[Easiest way to rm -rf in Python](https://stackoverflow.com/questions/814167/easiest-way-to-rm-rf-in-python)

What is the easiest way to do the equivalent of rm -rf in Python?

2009-05-02 04:52:24Z

What is the easiest way to do the equivalent of rm -rf in Python?While useful, rmtree isn't equivalent: it errors out if you try to remove a single file, which rm -f does not (see example below).To get around this, you'll need to check whether your path is a file or a directory, and act accordingly. Something like this should do the trick:Note: this function will not handle character or block devices (that would require using the stat module).Example in difference of between rm -f and Python's shutils.rmtreeEdit: handle symlinks; note limitations as per @pevik's commentSlightly improved Gabriel Grant's version. This works also on symlinks to directories.

Note: function does not handle Un*x character and block devices (it would require to use stat module).A workaround for Windows where it blocks deletion of file is to truncate the file:source: https://stackoverflow.com/a/2769090/6345724shutil.rmtree() is right answer, but just look at another useful function - os.walk()Just do this:

Count number of non-NaN entries in every column of Dataframe

cryp

[Count number of non-NaN entries in every column of Dataframe](https://stackoverflow.com/questions/29971075/count-number-of-non-nan-entries-in-every-column-of-dataframe)

I have a really big DataFrame and I was wondering if there was short (one or two liner) way to get the a count of non-NaN entries in a DataFrame. I don't want to do this one column at a time as I have close to 1000 columns. Output:

2015-04-30 14:57:40Z

I have a really big DataFrame and I was wondering if there was short (one or two liner) way to get the a count of non-NaN entries in a DataFrame. I don't want to do this one column at a time as I have close to 1000 columns. Output:The count() method returns the number of non-NaN values in each column:Similarly, count(axis=1) returns the number of non-NaN values in each row.If you want to sum the total count values which are not NAN, one can do;

As a Java programmer learning Python, what should I look out for? [closed]

froadie

[As a Java programmer learning Python, what should I look out for? [closed]](https://stackoverflow.com/questions/2339371/as-a-java-programmer-learning-python-what-should-i-look-out-for)

Much of my programming background is in Java, and I'm still doing most of my programming in Java. However, I'm starting to learn Python for some side projects at work, and I'd like to learn it as independent of my Java background as possible - i.e. I don't want to just program Java in Python. What are some things I should look out for?A quick example - when looking through the Python tutorial, I came across the fact that defaulted mutable parameters of a function (such as a list) are persisted (remembered from call to call). This was counter-intuitive to me as a Java programmer and hard to get my head around. (See here and here if you don't understand the example.)Someone also provided me with this list, which I found helpful, but short. Anyone have any other examples of how a Java programmer might tend to misuse Python...? Or things a Java programmer would falsely assume or have trouble understanding?Edit: Ok, a brief overview of the reasons addressed by the article I linked to to prevent duplicates in the answers (as suggested by Bill the Lizard). (Please let me know if I make a mistake in phrasing, I've only just started with Python so I may not understand all the concepts fully. And a disclaimer - these are going to be very brief, so if you don't understand what it's getting at check out the link.)(And if you find this question at all interesting, check out the link anyway. :) It's quite good.)

2010-02-26 03:48:47Z

Much of my programming background is in Java, and I'm still doing most of my programming in Java. However, I'm starting to learn Python for some side projects at work, and I'd like to learn it as independent of my Java background as possible - i.e. I don't want to just program Java in Python. What are some things I should look out for?A quick example - when looking through the Python tutorial, I came across the fact that defaulted mutable parameters of a function (such as a list) are persisted (remembered from call to call). This was counter-intuitive to me as a Java programmer and hard to get my head around. (See here and here if you don't understand the example.)Someone also provided me with this list, which I found helpful, but short. Anyone have any other examples of how a Java programmer might tend to misuse Python...? Or things a Java programmer would falsely assume or have trouble understanding?Edit: Ok, a brief overview of the reasons addressed by the article I linked to to prevent duplicates in the answers (as suggested by Bill the Lizard). (Please let me know if I make a mistake in phrasing, I've only just started with Python so I may not understand all the concepts fully. And a disclaimer - these are going to be very brief, so if you don't understand what it's getting at check out the link.)(And if you find this question at all interesting, check out the link anyway. :) It's quite good.)Just do this:The referenced article has some good advice that can easily be misquoted and misunderstood.  And some bad advice.Leave Java behind.   Start fresh.  "do not trust your [Java-based] instincts".  Saying things are "counter-intuitive" is a bad habit in any programming discipline.  When learning a new language, start fresh, and drop your habits.  Your intuition must be wrong.  Languages are different.  Otherwise, they'd be the same language with different syntax, and there'd be simple translators.  Because there are not simple translators, there's no simple mapping.  That means that intuition is unhelpful and dangerous.One thing you might be used to in Java that you won't find in Python is strict privacy.  This is not so much something to look out for as it is something not to look for (I am embarrassed by how long I searched for a Python equivalent to 'private' when I started out!).  Instead, Python has much more transparency and easier introspection than Java.  This falls under what is sometimes described as the "we're all consenting adults here" philosophy.  There are a few conventions and language mechanisms to help prevent accidental use of "unpublic" methods and so forth, but the whole mindset of information hiding is virtually absent in Python.The biggest one I can think of is not understanding or not fully utilizing duck typing.  In Java you're required to specify very explicit and detailed type information upfront.  In Python typing is both dynamic and largely implicit.  The philosophy is that you should be thinking about your program at a higher level than nominal types.  For example, in Python, you don't use inheritance to model substitutability.  Substitutability comes by default as a result of duck typing.  Inheritance is only a programmer convenience for reusing implementation.Similarly, the Pythonic idiom is "beg forgiveness, don't ask permission".  Explicit typing is considered evil.  Don't check whether a parameter is a certain type upfront.  Just try to do whatever you need to do with the parameter.  If it doesn't conform to the proper interface, it will throw a very clear exception and you will be able to find the problem very quickly.  If someone passes a parameter of a type that was nominally unexpected but has the same interface as what you expected, then you've gained flexibility for free.The most important thing, from a Java POV, is that it's perfectly ok to not make classes for everything.  There are many situations where a procedural approach is simpler and shorter.The next most important thing is that you will have to get over the notion that the type of an object controls what it may do; rather, the code controls what objects must be able to support at runtime (this is by virtue of duck-typing).  Oh, and use native lists and dicts (not customized descendants) as far as possible.The way exceptions are treated in Python is different from 

how they are treated in Java. While in Java the advice

is to use exceptions only for exceptional conditions this is not

so with Python. In Python things like Iterator makes use of exception mechanism to signal that there are no more items.But such a design is not considered as good practice in Java.As Alex Martelli puts in his book Python in a Nutshell

the exception mechanism with other languages (and applicable to Java) 

is LBYL (Look Before You Leap) : 

is to check in advance, before attempting an operation, for all circumstances that might make the operation invalid. Where as with Python the approach is EAFP (it's easier to Ask for forgiveness than permission)A corrollary to "Don't use classes for everything": callbacks. The Java way for doing callbacks relies on passing objects that implement the callback interface (for example ActionListener with its actionPerformed() method). Nothing of this sort is necessary in Python, you can directly pass methods or even locally defined functions:Or even lambdas:

Is there a way to instantiate a class without calling __init__?

Woltan

[Is there a way to instantiate a class without calling __init__?](https://stackoverflow.com/questions/6383914/is-there-a-way-to-instantiate-a-class-without-calling-init)

Is there a way to circumvent the constructor __init__ of a class in python?Example:Now I would like to create an instance of A. It could look like this, however this syntax is not correct.EDIT:An even more complex example:Suppose I have an object C, which purpose it is to store one single parameter and do some computations with it. The parameter, however, is not passed as such but it is embedded in a huge parameter file. It could look something like this:Now I would like to dump and load an instance of that object C. However, when I load this object, I only have the single variable self._Parameter and I cannot call the constructor, because it is expecting the parameter file. In other words, it is not possible to create an instance without passing the parameter file. In my "real" case, however, it is not a parameter file but some huge junk of data I certainly not want to carry around in memory or even store it to disc.And since I want to return an instance of C from the method Load I do somehow have to call the constructor.OLD EDIT:A more complex example, which explains why I am asking the question:As you can see, since data is not stored in a class variable I cannot pass it to __init__. Of course I could simply store it, but what if the data is a huge object, which I do not want to carry around in memory all the time or even save it to disc?

2011-06-17 09:37:22Z

Is there a way to circumvent the constructor __init__ of a class in python?Example:Now I would like to create an instance of A. It could look like this, however this syntax is not correct.EDIT:An even more complex example:Suppose I have an object C, which purpose it is to store one single parameter and do some computations with it. The parameter, however, is not passed as such but it is embedded in a huge parameter file. It could look something like this:Now I would like to dump and load an instance of that object C. However, when I load this object, I only have the single variable self._Parameter and I cannot call the constructor, because it is expecting the parameter file. In other words, it is not possible to create an instance without passing the parameter file. In my "real" case, however, it is not a parameter file but some huge junk of data I certainly not want to carry around in memory or even store it to disc.And since I want to return an instance of C from the method Load I do somehow have to call the constructor.OLD EDIT:A more complex example, which explains why I am asking the question:As you can see, since data is not stored in a class variable I cannot pass it to __init__. Of course I could simply store it, but what if the data is a huge object, which I do not want to carry around in memory all the time or even save it to disc?You can circumvent __init__ by calling __new__ directly. Then you can create a object of the given type and call an alternative method for __init__. This is something that pickle would do. However, first I'd like to stress very much that it is something that you shouldn't do and whatever you're trying to achieve, there are better ways to do it, some of which have been mentioned in the other answers. In particular, it's a bad idea to skip calling __init__. When objects are created, more or less this happens:You could skip the second step.Here's why you shouldn't do this: The purpose of __init__ is to initialize the object, fill in all the fields and ensure that the __init__ methods of the parent classes are also called. With pickle it is an exception because it tries to store all the data associated with the object (including any fields/instance variables that are set for the object), and so anything that was set by __init__ the previous time would be restored by pickle, there's no need to call it again. If you skip __init__ and use an alternative initializer, you'd have a sort of a code duplication - there would be two places where the instance variables are filled in, and it's easy to miss one of them in one of the initializers or accidentally make the two fill the fields act differently. This gives the possibility of subtle bugs that aren't that trivial to trace (you'd have to know which initializer was called), and the code will be more difficult to maintain. Not to mention that you'd be in an even bigger mess if you're using inheritance - the problems will go up the inheritance chain, because you'd have to use this alternative initializer everywhere up the chain.Also by doing so you'd be more or less overriding Python's instance creation and making your own. Python already does that for you pretty well, no need to go reinventing it and it will confuse people using your code.Here's what to best do instead: Use a single __init__ method that is to be called for all possible instantiations of the class that initializes all instance variables properly. For different modes of initialization use either of the two approaches:If __init__ has an optional step (e.g. like processing that data argument, although you'd have to be more specific), you can either make it an optional argument or make a normal method that does the processing... or both.Use classmethod decorator for your Load method:So you can do:Edit:Anyway, if you still want to omit __init__ method, try __new__:Taking your question literally I would use meta classes :This can be useful e.g. for copying constructors without polluting the parameter list.

But to do this properly would be more work and care than my proposed hack.Not really.  The purpose of __init__ is to instantiate an object, and by default it really doesn't do anything.  If the __init__ method is not doing what you want, and it's not your own code to change, you can choose to switch it out though.  For example, taking your class A, we could do the following to avoid calling that __init__ method:This will dynamically switch out which __init__ method from the class, replacing it with an empty call.  Note that this is probably NOT a good thing to do, as it does not call the super class's __init__ method.You could also subclass it to create your own class that does everything the same, except overriding the __init__ method to do what you want it to (perhaps nothing).Perhaps, however, you simply wish to call the method from the class without instantiating an object.  If that is the case, you should look into the @classmethod and @staticmethod decorators.  They allow for just that type of behavior.In your code you have put the @staticmethod decorator, which does not take a self argument.  Perhaps what may be better for the purpose would a @classmethod, which might look more like this:UPDATE: Rosh's Excellent answer pointed out that you CAN avoid calling __init__ by implementing __new__, which I was actually unaware of (although it makes perfect sense).  Thanks Rosh!I was reading the Python cookbook and there's a section talking about this: the example is given using __new__ to bypass __init__()However I think this only works in Python3. Below is running under 2.7As I said in my comment you could change your __init__ method so that it allows creation without giving any values to its parameters:would become:or:

How to force deletion of a python object?

wim

[How to force deletion of a python object?](https://stackoverflow.com/questions/6772481/how-to-force-deletion-of-a-python-object)

I am curious about the details of __del__ in python, when and why it should be used and what it shouldn't be used for.  I've learned the hard way that it is not really like what one would naively expected from a destructor, in that it is not the opposite of __new__ / __init__.  I saw in the documentation that it is not guaranteed __del__() methods are called for objects that still exist when the interpreter exits.  

2011-07-21 07:09:58Z

I am curious about the details of __del__ in python, when and why it should be used and what it shouldn't be used for.  I've learned the hard way that it is not really like what one would naively expected from a destructor, in that it is not the opposite of __new__ / __init__.  I saw in the documentation that it is not guaranteed __del__() methods are called for objects that still exist when the interpreter exits.  The way to close resources are context managers, aka the with statement:output:2) Python's objects get deleted when their reference count is 0. In your example the del foo removes the last reference so __del__ is called instantly. The GC has no part in this.output:The gc has nothing to do with deleting your and most other objects. It's there to clean up when simple reference counting does not work, because of self-references or circular references:output:3) Lets see:gives:Objects are created with __new__ then passed to __init__ as self. After a exception in __init__, the object will typically not have a name (ie the f = part isn't run) so their ref count is 0. This means that the object is deleted normally and __del__ is called.In general, to make sure something happens no matter what, you usefinally blocks will be run whether or not there is an error in the try block, and whether or not there is an error in any error handling that takes place in except blocks. If you don't handle an exception that is raised, it will still be raised after the finally block is excecuted.The general way to make sure a file is closed is to use a "context manager".http://docs.python.org/reference/datamodel.html#context-managersThis will automatically close f.For your question #2, bar gets closed on immediately when it's reference count reaches zero, so on del foo if there are no other references.Objects are NOT created by __init__, they're created by __new__.http://docs.python.org/reference/datamodel.html#object.newWhen you do foo = Foo() two things are actually happening, first a new object is being created, __new__, then it is being initialized, __init__. So there is no way you could possibly call del foo before both those steps have taken place. However, if there is an error in __init__, __del__ will still be called because the object was actually already created in __new__.Edit: Corrected when deletion happens if a reference count decreases to zero.Perhaps you are looking for a context manager?

Where is the __builtin__ module in Python3? Why was it renamed?

Derek Litz

[Where is the __builtin__ module in Python3? Why was it renamed?](https://stackoverflow.com/questions/9047745/where-is-the-builtin-module-in-python3-why-was-it-renamed)

I was curious about the __builtin__ module and how it's used, but I can't find it in Python3!  Why was it moved?Python 2.7Python 3.2

2012-01-28 19:07:01Z

I was curious about the __builtin__ module and how it's used, but I can't find it in Python3!  Why was it moved?Python 2.7Python 3.2The __builtin__ module was renamed to builtins in Python3.This change solves 2 sources of confusion for the average Python developer.This confusion mainly arises because of the violation of pep8 convention.  Also, the lack of pluralization on the module hinders communication as well.  Both of these are greatly illustrated by the lengths Guido must go to explain the following from http://mail.python.org/pipermail/python-ideas/2009-March/003821.html:For example,Python2.7Python3.2Related resources:Other name changes - http://docs.pythonsprints.com/python3_porting/py-porting.html#name-changesFor a succinct explanation of how __builtins__ is used in name resolution - __builtin__ module in Python

(Python) Use a library locally instead of installing it

Trevor Hickey

[(Python) Use a library locally instead of installing it](https://stackoverflow.com/questions/9059699/python-use-a-library-locally-instead-of-installing-it)

Script:

I've written a script in python that occasionally sends tweets to twitter

It only uses one library called: tweepy

after installing the library it works, great.  Problem:

I would like to host the script on a server where I do not have privileges to install anything

It would be great if I can just include it locally from the folder I've got it in.

As of right now, all I need to include at the top of my file is:  the tweepy folder (DOES have a __init__.py file which I believe is important.  Question:

How can I use this library without installing it?

basically I want to replace: import tweepy with import local_folder/tweepy/*  this might just be python common sense, but I'm stuck!

2012-01-30 05:21:37Z

Script:

I've written a script in python that occasionally sends tweets to twitter

It only uses one library called: tweepy

after installing the library it works, great.  Problem:

I would like to host the script on a server where I do not have privileges to install anything

It would be great if I can just include it locally from the folder I've got it in.

As of right now, all I need to include at the top of my file is:  the tweepy folder (DOES have a __init__.py file which I believe is important.  Question:

How can I use this library without installing it?

basically I want to replace: import tweepy with import local_folder/tweepy/*  this might just be python common sense, but I'm stuck!EDIT: This answer is outdated. You should be using VirtualEnv. If you are allergic to third-party software for some reason (in which case, why are you installing libraries?), there is something called venv, that is literally built into python3, so there is no excuse not to use some kind of virtualization. (Most people active in the community prefer VirtualEnv, however. See https://stackoverflow.com/a/41573588/410889.)VirtualEnv installs a local python interpreter, with a local packages folder and everything. In addition to this entirely solving the issue of administrative privileges, the most important feature of VirtualEnv is that it allows you to keep your environments separate. If you have one project that needs Foo version 2.3 and another that needs Foo version 1.5, you can't have them share the same environment; you have to keep their environments separate with VirtualEnv.There are a few possibilities:If you already know how to install Python modules, the default distutils setup already includes a per-user installation option. Just run python setup.py install --user instead of python setup.py install. This is the easiest, since this does not necessitate the addition of any source code.You could also run the script with the directory of tweepy as the current working directory.You could add an environment variable named PYTHONPATH to whatever environment (e.g., the shell) you use to run your script, and make it contain the path to tweepy.If all else fails, and you really do want to edit your source code, you'll need to edit sys.path. sys.path is a list of locations where Python will look for code.In your code, write:you should install a virtual python environment so you don't have to ever worry about having admin privileges, and you can install whatever you want.Also if you want to use it frequently like in script . 

Easy way to export "PYTHONPATH" in bashrc/zshrc file and give path to the directory containing your code .For example:This way you don't need to do sys.path everytime you restart .Happy CodingSimple and clean solution:answer above with 'append' doesn't work with packages which are installed as well

How to use Python's「easy_install」on Windows … it's not so easy

Nick

[How to use Python's「easy_install」on Windows … it's not so easy](https://stackoverflow.com/questions/4016151/how-to-use-pythons-easy-install-on-windows-its-not-so-easy)

After installing Python 2.7 on Windows XP, then manually setting the %PATH% to python.exe (why won't the python installer do this?), then installing setuptools 0.6c11 (why doesn't the python installer do this?), then manually setting the %PATH% to easy_install.exe (why doesn't the installer do this?), I finally tried to install a python package with easy_install, but easy_install failed when it couldn't install the pywin32 package, which is a dependency. How can I make easy_install work properly on Windows XP? The failure follows:

2010-10-25 15:31:20Z

After installing Python 2.7 on Windows XP, then manually setting the %PATH% to python.exe (why won't the python installer do this?), then installing setuptools 0.6c11 (why doesn't the python installer do this?), then manually setting the %PATH% to easy_install.exe (why doesn't the installer do this?), I finally tried to install a python package with easy_install, but easy_install failed when it couldn't install the pywin32 package, which is a dependency. How can I make easy_install work properly on Windows XP? The failure follows:One problem is that easy_install is set up to download and install .egg files or source distributions (contained within .tgz, .tar, .tar.gz, .tar.bz2, or .zip files). It doesn't know how to deal with the PyWin32 extensions because they are put within a separate installer executable. You will need to download the appropriate PyWin32 installer file (for Python 2.7) and run it yourself. When you run easy_install again (provided you have it installed right, like in Sergio's instructions), you should see that your winpexpect package has been installed correctly.Since it's Windows and open source we are talking about, it can often be a messy combination of install methods to get things working properly. However, easy_install is still better than hand-editing configuration files, for sure.If you are using windows 7 64-bit version, then the solution is found here: http://pypi.python.org/pypi/setuptoolsnamely, you need to download a python script, run it, and then easy_install will work normally from commandline. P.S. I agree with the original poster saying that this should work out of the box. I also agree with the OP that all these things should come with Python already set. I guess we will have to deal with it until that day comes. Here is a solution that actually worked for me :installing easy_install faster and easierI hope it helps you or anyone with the same problem!Copy the below script "ez_setup.py" from the below URLhttps://bootstrap.pypa.io/ez_setup.pyAnd copy it into your Python locationRun the commandThis will install the easy_install under Scripts directoryRun easy install from the Scripts directory >For one thing, it says you already have that module installed. If you need to upgrade it, you should do something like this:easy_install -U packageNameOf course, easy_install doesn't work very well if the package has some C headers that need to be compiled and you don't have the right version of Visual Studio installed. You might try using pip or distribute instead of easy_install and see if they work better.If you are using Anaconda's Python distribution,you can install it through pippip install setuptoolsand then execute it as a modulepython -m easy_install

Why can't you add attributes to object in python? [duplicate]

quano

[Why can't you add attributes to object in python? [duplicate]](https://stackoverflow.com/questions/1285269/why-cant-you-add-attributes-to-object-in-python)

(Written in Python shell)Why doesn't object allow you to add attributes to it?

2009-08-16 20:26:53Z

(Written in Python shell)Why doesn't object allow you to add attributes to it?Notice that an object instance has no __dict__ attribute:An example to illustrate this behavior in a derived class:Quoting from the docs on slots:EDIT: To answer ThomasH from the comments, OP's test class is an "old-style" class.  Try:and you'll notice there is a __dict__ instance.  The object class may not have a __slots__ defined, but the result is the same: lack of a __dict__, which is what prevents dynamic assignment of an attribute. I've reorganized my answer to make this clearer (move the second paragraph to the top).Good question, my guess is that it has to do with the fact that object is a built-in/extension type.IIRC, this has to do with the presence of a __dict__ attribute or, more correctly, setattr() blowing up when the object doesn't have a __dict__ attribute.

IndexError: too many indices for array

Chris

[IndexError: too many indices for array](https://stackoverflow.com/questions/28036812/indexerror-too-many-indices-for-array)

I know there is a ton of these threads but all of them are for very simple cases like 3x3 matrices and things of that sort and the solutions do not even begin to apply to my situation.  So I'm trying to graph G versus l1 (that's not an eleven, but an L1). The data is in the file that I loaded from an excel file. The excel file is 14x250 so there are 14 arguments, each with 250 data points.  I had another user (shout out to Hugh Bothwell!) help me with an error in my code, but now another error has surfaced.So here is the code in question:After running the entire program, I recieve the error message:and before I ran into that problem, I had another involving the line a few below the one the above error message refers to:I understand the first error, but am just having problems fixing it.  The second error is confusing for me though.  My boss is really breathing down my neck so any help would be GREATLY appreciated!

2015-01-20 02:28:24Z

I know there is a ton of these threads but all of them are for very simple cases like 3x3 matrices and things of that sort and the solutions do not even begin to apply to my situation.  So I'm trying to graph G versus l1 (that's not an eleven, but an L1). The data is in the file that I loaded from an excel file. The excel file is 14x250 so there are 14 arguments, each with 250 data points.  I had another user (shout out to Hugh Bothwell!) help me with an error in my code, but now another error has surfaced.So here is the code in question:After running the entire program, I recieve the error message:and before I ran into that problem, I had another involving the line a few below the one the above error message refers to:I understand the first error, but am just having problems fixing it.  The second error is confusing for me though.  My boss is really breathing down my neck so any help would be GREATLY appreciated!I think the problem is given in the error message, although it is not very easy to spot:'Too many indices' means you've given too many index values.  You've given 2 values as you're expecting data to be a 2D array.  Numpy is complaining because data is not 2D (it's either 1D or None).This is a bit of a guess - I wonder if one of the filenames you pass to loadfile() points to an empty file, or a badly formatted one?  If so, you might get an array returned that is either 1D, or even empty (np.array(None) does not throw an Error, so you would never know...).  If you want to guard against this failure, you can insert some error checking into your loadfile function.I highly recommend in your for loop inserting:This will work in Python 2.x or 3.x and might reveal the source of the issue.  You might well find it is only one value of your outputs_l1 list (i.e. one file) that is giving the issue.The message that you are getting is not for the default Exception of Python:For a fresh python list, IndexError is thrown only on index not being in range (even docs say so). If we try passing multiple items to list, or some other value, we get the TypeError:However, here, you seem to be using matplotlib that internally uses numpy for handling arrays. On digging deeper through the codebase for numpy, we see:where, the unpack method will throw an error if it the size of the index is greater than that of the results. So, Unlike Python which raises a TypeError on incorrect Indexes, Numpy raises the IndexError because it supports multidimensional arrays.

set pythonpath before import statements

DKG

[set pythonpath before import statements](https://stackoverflow.com/questions/15109548/set-pythonpath-before-import-statements)

My code is:but the scriptlib is in some other directory, so I will have to include that directory in environment variable "PYTHONPATH".Is there anyway in which I can first add the scriptlib directory in environment variable "PYTHONPATH" before import statement getting executed like :If so, is the value only for that command prompt or is it global ?Thanks in advance

2013-02-27 10:20:21Z

My code is:but the scriptlib is in some other directory, so I will have to include that directory in environment variable "PYTHONPATH".Is there anyway in which I can first add the scriptlib directory in environment variable "PYTHONPATH" before import statement getting executed like :If so, is the value only for that command prompt or is it global ?Thanks in advanceThis will add a path to your Python process / instance (i.e. the running executable). The path will not be modified for any other Python processes. Another running Python program will not have its path modified, and if you exit your program and run again the path will not include what you added before. What are you are doing is generally correct.set.py:loop.pyrun: python loop.py &This will run loop.py, connected to your STDOUT, and it will continue to run in the background. You can then run python set.py. Each has a different set of environment variables. Observe that the output from loop.py does not change because set.py does not change loop.py's environment.A note on importingPython imports are dynamic, like the rest of the language. There is no static linking going on. The import is an executable line, just like sys.path.append....As also noted in the docs here.

Go to Python X.X/Lib and add these lines to the site.py there,This changes your sys.path so that on every load, it will have that value in it..  As stated here about site.py,  For other possible methods of adding some path to sys.path see these docs

Parsing XML in Python using ElementTree example

Casey

[Parsing XML in Python using ElementTree example](https://stackoverflow.com/questions/1786476/parsing-xml-in-python-using-elementtree-example)

I'm having a hard time finding a good, basic example of how to parse XML in python using Element Tree. From what I can find, this appears to be the easiest library to use for parsing XML. Here is a sample of the XML I'm working with:I am able to do what I need, using a hard-coded method. But I need my code to be a bit more dynamic. Here is what worked:Here are a couple of things I've tried, none of them worked, reporting that they couldn't find timeSeries (or anything else I tried):Basically, I want to load the xml file, search for the timeSeries tag, and iterate through the value tags, returning the dateTime and the value of the tag itself; everything I'm doing in the above example, but not hard coding the sections of xml I'm interested in. Can anyone point me to some examples, or give me some suggestions on how to work through this?Thanks for all the help. Using both of the below suggestions worked on the sample file I provided, however, they didn't work on the full file. Here is the error I get from the real file when I use Ed Carrel's method:I figured there was something in the real file it didn't like, so I incremently removed things until it worked. Here are the lines that I changed:Removing the attributes that have 'xsi:...' fixed the problem. Is the 'xsi:...' not valid XML? It will be hard for me to remove these programmatically. Any suggested work arounds?Here is the full XML file: http://www.sendspace.com/file/lofcptWhen I originally asked this question, I was unaware of namespaces in XML. Now that I know what's going on, I don't have to remove the "xsi" attributes, which are the namespace declarations. I just include them in my xpath searches. See this page for more info on namespaces in lxml.

2009-11-23 22:24:01Z

I'm having a hard time finding a good, basic example of how to parse XML in python using Element Tree. From what I can find, this appears to be the easiest library to use for parsing XML. Here is a sample of the XML I'm working with:I am able to do what I need, using a hard-coded method. But I need my code to be a bit more dynamic. Here is what worked:Here are a couple of things I've tried, none of them worked, reporting that they couldn't find timeSeries (or anything else I tried):Basically, I want to load the xml file, search for the timeSeries tag, and iterate through the value tags, returning the dateTime and the value of the tag itself; everything I'm doing in the above example, but not hard coding the sections of xml I'm interested in. Can anyone point me to some examples, or give me some suggestions on how to work through this?Thanks for all the help. Using both of the below suggestions worked on the sample file I provided, however, they didn't work on the full file. Here is the error I get from the real file when I use Ed Carrel's method:I figured there was something in the real file it didn't like, so I incremently removed things until it worked. Here are the lines that I changed:Removing the attributes that have 'xsi:...' fixed the problem. Is the 'xsi:...' not valid XML? It will be hard for me to remove these programmatically. Any suggested work arounds?Here is the full XML file: http://www.sendspace.com/file/lofcptWhen I originally asked this question, I was unaware of namespaces in XML. Now that I know what's going on, I don't have to remove the "xsi" attributes, which are the namespace declarations. I just include them in my xpath searches. See this page for more info on namespaces in lxml.So I have ElementTree 1.2.6 on my box now, and ran the following code against the XML chunk you posted: and got the following back:It appears to have found the timeSeries element without needing to use numerical indices.What would be useful now is knowing what you mean when you say "it doesn't work." Since it works for me given the same input, it is unlikely that ElementTree is broken in some obvious way. Update your question with any error messages, backtraces, or anything you can provide to help us help you.If I understand your question correctly:or if you prefer (and if there is only one occurrence of timeSeries/values:The findall() method returns a list of all matching elements, whereas find() returns only the first matching element. The first example loops over all the found elements, the second loops over the child elements of the values element, in this case leading to the same result.I don't see where the problem with not finding timeSeries comes from however. Maybe you just forgot the getroot() call? (note that you don't really need it because you can work from the elementtree itself too, if you change the path expression to for example /timeSeriesResponse/timeSeries/values or //timeSeries/values) 

How can I remove the ANSI escape sequences from a string in python

SpartaSixZero

[How can I remove the ANSI escape sequences from a string in python](https://stackoverflow.com/questions/14693701/how-can-i-remove-the-ansi-escape-sequences-from-a-string-in-python)

This is my string:I was using code to retrieve the output from a SSH command and I want my string to only contain 'examplefile.zip'What I can use to remove the extra escape sequences?

2013-02-04 19:07:04Z

This is my string:I was using code to retrieve the output from a SSH command and I want my string to only contain 'examplefile.zip'What I can use to remove the extra escape sequences?Delete them with a regular expression:or, without the VERBOSE flag, in condensed form:Demo:The above regular expression covers all 7-bit ANSI C1 escape sequences, but not the 8-bit C1 escape sequence openers. The latter are never used in today's UTF-8 world where the same range of bytes have a different meaning.If you do need to cover the 8-bit codes too (and are then, presumably, working with bytes values) then the regular expression becomes a bytes pattern like this:which can be condensed down toFor more information, see:The example you gave contains 4 CSI (Control Sequence Introducer) codes, as marked by the \x1B[ or ESC [ opening bytes, and each contains a SGR (Select Graphic Rendition) code, because they each end in m. The parameters (separated by ; semicolons) in between those tell your terminal what graphic rendition attributes to use. So for each \x1B[....m sequence, the 3 codes that are used are:However, there is more to ANSI than just CSI SGR codes. With CSI alone you can also control the cursor, clear lines or the whole display, or scroll (provided the terminal supports this of course). And beyond CSI, there are codes to select alternative fonts (SS2 and SS3), to send 'private messages' (think passwords), to communicate with the terminal (DCS), the OS (OSC), or the application itself (APC, a way for applications to piggy-back custom control codes on to the communication stream), and further codes to help define strings (SOS, Start of String, ST String Terminator) or to reset everything back to a base state (RIS). The above regexes cover all of these.Note that the above regex only removes the ANSI C1 codes, however, and not any additional data that those codes may be marking up (such as the strings sent between an OSC opener and the terminating ST code). Removing those would require additional work outside the scope of this answer.The accepted answer to this question only considers color and font effects.  There are a lot of sequences that do not end in 'm', such as cursor positioning, erasing, and scroll regions.The complete regexp for Control Sequences (aka ANSI Escape Sequences) isRefer to ECMA-48 Section 5.4 and ANSI escape codeBased on Martijn Pieters♦'s answer with Jeff's regexp.If you want to run it by yourself, use python3 (better unicode support, blablabla). Here is how the test file should be:The suggested regex didn't do the trick for me so I created one of my own.

The following is a python regex that I created based on the spec found hereI tested my regex on the following snippet (basically a copy paste from the ascii-table.com page)Hopefully this will help others :)If it helps future Stack Overflowers, I was using the crayons library to give my Python output a bit more visual impact, which is advantageous as it works on both Windows and Linux platforms.  However I was both displaying onscreen as well as appending to log files, and the escape sequences were impacting legibility of the log files, so wanted to strip them out.  However the escape sequences inserted by crayons produced an error:The solution was to cast the parameter to a string, so only a tiny modification to the commonly accepted answer was needed:if you want to remove the \r\n bit, you can pass the string through this function (written by sarnold):Careful though, this will lump together the text in front and behind the escape sequences. So, using Martijn's filtered string 'ls\r\nexamplefile.zip\r\n', you will get lsexamplefile.zip. Note the ls in front of the desired filename.I would use the stripEscape function first to remove the escape sequences, then pass the output to Martijn's regular expression, which would avoid concatenating the unwanted bit.

Deserialize a json string to an object in python

John La Rooy

[Deserialize a json string to an object in python](https://stackoverflow.com/questions/15476983/deserialize-a-json-string-to-an-object-in-python)

I have the following string I Want to deserialize to a object of class I am using python 2.6 and 2.7

2013-03-18 12:34:38Z

I have the following string I Want to deserialize to a object of class I am using python 2.6 and 2.7To elaborate on Sami's answer:From the docs:My objection to the solution is that while it does the job and is concise, the Payload class becomes totally generic - it doesn't document its fields. For example, if the Payload message had an unexpected format, instead of throwing a key not found error when the Payload was created, no error would be generated until the payload was used.If you are embracing the type hints in Python 3.6, you can do it like this:Which then allows you do instantiate typed objects like this:This syntax requires Python 3.6 though and does not cover all cases - for example, support for typing.Any... But at least it does not pollute the classes that need to be deserialized with extra init/tojson methods.If you want to save lines of code and leave the most flexible solution, we can deserialize the json string to a dynamic object:

>>>> p.action  

output: u'print' >>>> p.method

output: u'onData' I prefer to add some checking of the fields, e.g. so you can catch errors like when you get invalid json, or not the json you were expecting, so I used namedtuples:this will let give you nice errors when the json you are parsing does not match the thing you want it to parseif you want to parse nested relations, e.g. '{"parent":{"child":{"name":"henry"}}}'

you can still use the namedtuples, and even a more reusable functiongiving  youI thought I lose all my hairs for solving this 'challenge'. I faced following problems:I found a library called jsonpickle which is has proven to be really useful. Installation:Here is a code example with writing nested objects to file:Output:Website: http://jsonpickle.github.io/Hope it will save your time (and hairs).You can specialize an encoder for object creation: http://docs.python.org/2/library/json.htmlIn recent versions of python, you can use marshmallow-dataclass:Another way is to simply pass the json string as a dict to the constructor of your object. For example your object is:And the following two lines of python code will construct it:Basically, we first create a generic json object from the json string. Then, we pass the generic json object as a dict to the constructor of the Payload class. The constructor of Payload class interprets the dict as keyword arguments and sets all the appropriate fields.While Alex's answer points us to a good technique, the implementation that he gave runs into a problem when we have nested objects.with the below code:payload.more_info will also be treated as an instance of payload which will lead to parsing errors.From the official docs:Hence, I would prefer to propose the following solution instead:There are different methods to deserialize json string to an object. All above methods are acceptable but I suggest using a library to prevent duplicate key issues or serializing/deserializing of nested objects.Pykson, is a JSON Serializer and Deserializer for Python which can help you achieve. Simply define Payload class model as JsonObject then use Pykson to convert json string to object.

sscanf in Python

Matt Joiner

[sscanf in Python](https://stackoverflow.com/questions/2175080/sscanf-in-python)

I'm looking for an equivalent to sscanf() in Python. I want to parse /proc/net/* files, in C I could do something like this:I thought at first to use str.split, however it doesn't split on the given characters, but the sep string as a whole:Which should be returning 17, as explained above.Is there a Python equivalent to sscanf (not RE), or a string splitting function in the standard library that splits on any of a range of characters that I'm not aware of?

2010-02-01 06:38:38Z

I'm looking for an equivalent to sscanf() in Python. I want to parse /proc/net/* files, in C I could do something like this:I thought at first to use str.split, however it doesn't split on the given characters, but the sep string as a whole:Which should be returning 17, as explained above.Is there a Python equivalent to sscanf (not RE), or a string splitting function in the standard library that splits on any of a range of characters that I'm not aware of?Python doesn't have an sscanf equivalent built-in, and most of the time it actually makes a whole lot more sense to parse the input by working with the string directly, using regexps, or using a parsing tool. Probably mostly useful for translating C, people have implemented sscanf, such as in this module: http://hkn.eecs.berkeley.edu/~dyoo/python/scanf/In this particular case if you just want to split the data based on multiple split characters, re.split is really the right tool.There is also the parse module.parse() is designed to be the opposite of format() (the newer string formatting function in Python 2.6 and higher).When I'm in a C mood, I usually use zip and list comprehensions for scanf-like behavior.  Like this:Note that for more complex format strings, you do need to use regular expressions:Note also that you need conversion functions for all types you want to convert. For example, above I used something like:You can split on a range of characters using the re module.You can parse with module re using named groups. It won't parse the substrings to their actual datatypes (e.g. int) but it's very convenient when parsing strings.Given this sample line from /proc/net/tcp:An example mimicking your sscanf example with the variable could be:There is an ActiveState recipe which implements a basic scanf

http://code.activestate.com/recipes/502213-simple-scanf-implementation/Update: The Python documentation for its regex module, re, includes a section on simulating scanf, which I found more useful than any of the answers above.https://docs.python.org/2/library/re.html#simulating-scanfyou can turn the ":" to space, and do the split.egno regex needed (for this case)Upvoted orip's answer. I think it is sound advice to use re module. The Kodos application is helpful when approaching a complex regexp task with Python. http://kodos.sourceforge.net/home.htmlIf the separators are ':', you can split on ':', and then use x.strip() on the strings to get rid of any leading or trailing whitespace. int() will ignore the spaces.There is a Python 2 implementation by odiak.You could install pandas and use pandas.read_fwf for fixed width format files.  Example using /proc/net/arp:By default it tries to figure out the format automagically, but there are options you can give for more explicit instructions (see documentation).  There are also other IO routines in pandas that are powerful for other file formats.There is an example in the official python docs about how to use sscanf from libc:

Download Returned Zip file from URL

user1229108

[Download Returned Zip file from URL](https://stackoverflow.com/questions/9419162/download-returned-zip-file-from-url)

If I have a URL that, when submitted in a web browser, pops up a dialog box to save a zip file, how would I go about catching and downloading this zip file in Python?

2012-02-23 18:42:35Z

If I have a URL that, when submitted in a web browser, pops up a dialog box to save a zip file, how would I go about catching and downloading this zip file in Python?Most people recommend using requests if it is available, and the requests documentation recommends this for downloading and saving raw data from a url:Since the answer asks about downloading and saving the zip file, I haven't gone into details regarding reading the zip file. See one of the many answers below for possibilities.If for some reason you don't have access to requests, you can use urllib.request instead. It may not be quite as robust as the above.Finally, if you are using Python 2 still, you can use urllib2.urlopen.As far as I can tell, the proper way to do this is:of course you'd want to check that the GET was successful with r.ok.For python 3+, sub the StringIO module with the io module and use BytesIO instead of StringIO: Here are release notes that mention this change. With the help of this blog post, I've got it working with just requests. The point of the weird stream thing is so we don't need to call content on large requests, which would require it to all be processed at once, clogging the memory. The stream avoids this by iterating through the data one chunk at a time.Here's what I got to work in Python 3:Either use urllib2.urlopen, or you could try using the excellent Requests module and avoid urllib2 headaches:I came here searching how to save a .bzip2 file. Let me paste the code for others who might come looking for this.I just wanted to save the file as is.Thanks to @yoavram for the above solution,

 my url path linked to a zipped folder, and encounter an error of BADZipfile 

 (file is not a zip file), and it was strange if I tried several times it 

 retrieve the url and unzipped it all of sudden so I amend the solution a little 

 bit. using the is_zipfile method as per here 

Best way to check function arguments in Python [closed]

carmellose

[Best way to check function arguments in Python [closed]](https://stackoverflow.com/questions/19684434/best-way-to-check-function-arguments-in-python)

I'm looking for an efficient way to check variables of a python function. For example, I'd like to check arguments type and value. Is there a module for this? Or should I use something like decorators, or any specific idiom?

2013-10-30 14:03:34Z

I'm looking for an efficient way to check variables of a python function. For example, I'd like to check arguments type and value. Is there a module for this? Or should I use something like decorators, or any specific idiom?The most Pythonic idiom is to clearly document what the function expects and then just try to use whatever gets passed to your function and either let exceptions propagate or just catch attribute errors and raise a TypeError instead. Type-checking should be avoided as much as possible as it goes against duck-typing. Value testing can be OK – depending on the context.The only place where validation really makes sense is at system or subsystem entry point, such as web forms, command line arguments, etc. Everywhere else, as long as your functions are properly documented, it's the caller's responsibility to pass appropriate arguments.In this elongated answer, we implement a Python 3.x-specific type checking decorator based on PEP 484-style type hints in less than 275 lines of pure-Python (most of which is explanatory docstrings and comments) – heavily optimized for industrial-strength real-world use complete with a py.test-driven test suite exercising all possible edge cases.Feast on the unexpected awesome of bear typing:As this example suggests, bear typing explicitly supports type checking of parameters and return values annotated as either simple types or tuples of such types. Golly!O.K., that's actually unimpressive. @beartype resembles every other Python 3.x-specific type checking decorator based on PEP 484-style type hints in less than 275 lines of pure-Python. So what's the rub, bub?Bear typing is dramatically more efficient in both space and time than all existing implementations of type checking in Python to the best of my limited domain knowledge. (More on that later.)Efficiency usually doesn't matter in Python, however. If it did, you wouldn't be using Python. Does type checking actually deviate from the well-established norm of avoiding premature optimization in Python? Yes. Yes, it does.Consider profiling, which adds unavoidable overhead to each profiled metric of interest (e.g., function calls, lines). To ensure accurate results, this overhead is mitigated by leveraging optimized C extensions (e.g., the _lsprof C extension leveraged by the cProfile module) rather than unoptimized pure-Python (e.g., the profile module). Efficiency really does matter when profiling.Type checking is no different. Type checking adds overhead to each function call type checked by your application – ideally, all of them. To prevent well-meaning (but sadly small-minded) coworkers from removing the type checking you silently added after last Friday's caffeine-addled allnighter to your geriatric legacy Django web app, type checking must be fast. So fast that no one notices it's there when you add it without telling anyone. I do this all the time! Stop reading this if you are a coworker.If even ludicrous speed isn't enough for your gluttonous application, however, bear typing may be globally disabled by enabling Python optimizations (e.g., by passing the -O option to the Python interpreter):Just because. Welcome to bear typing.Bear typing is bare-metal type checking – that is, type checking as close to the manual approach of type checking in Python as feasible. Bear typing is intended to impose no performance penalties, compatibility constraints, or third-party dependencies (over and above that imposed by the manual approach, anyway). Bear typing may be seamlessly integrated into existing codebases and test suites without modification.Everyone's probably familiar with the manual approach. You manually assert each parameter passed to and/or return value returned from every function in your codebase. What boilerplate could be simpler or more banal? We've all seen it a hundred times a googleplex times, and vomited a little in our mouths everytime we did. Repetition gets old fast. DRY, yo.Get your vomit bags ready. For brevity, let's assume a simplified easy_spirit_bear() function accepting only a single str parameter. Here's what the manual approach looks like:Python 101, right? Many of us passed that class.Bear typing extracts the type checking manually performed by the above approach into a dynamically defined wrapper function automatically performing the same checks – with the added benefit of raising granular TypeError rather than ambiguous AssertionError exceptions. Here's what the automated approach looks like:It's long-winded. But it's also basically* as fast as the manual approach. * Squinting suggested.Note the complete lack of function inspection or iteration in the wrapper function, which contains a similar number of tests as the original function – albeit with the additional (maybe negligible) costs of testing whether and how the parameters to be type checked are passed to the current function call. You can't win every battle.Can such wrapper functions actually be reliably generated to type check arbitrary functions in less than 275 lines of pure Python? Snake Plisskin says, "True story. Got a smoke?"And, yes. I may have a neckbeard.Bear beats duck. Duck may fly, but bear may throw salmon at duck. In Canada, nature can surprise you.Next question.Existing solutions do not perform bare-metal type checking – at least, none I've grepped across. They all iteratively reinspect the signature of the type-checked function on each function call. While negligible for a single call, reinspection overhead is usually non-negligible when aggregated over all calls. Really, really non-negligible.It's not simply efficiency concerns, however. Existing solutions also often fail to account for common edge cases. This includes most if not all toy decorators provided as stackoverflow answers here and elsewhere. Classic failures include:Bear typing succeeds where non-bears fail. All one, all bear!Bear typing shifts the space and time costs of inspecting function signatures from function call time to function definition time – that is, from the wrapper function returned by the @beartype decorator into the decorator itself. Since the decorator is only called once per function definition, this optimization yields glee for all.Bear typing is an attempt to have your type checking cake and eat it, too. To do so, @beartype:Shall we? Let's dive into the deep end.And leycec said, Let the  @beartype bring forth type checking fastly: and it was so.Nothing is perfect. Even bear typing.Bear typing does not type check unpassed parameters assigned default values. In theory, it could. But not in 275 lines or less and certainly not as a stackoverflow answer.The safe (...probably totally unsafe) assumption is that function implementers claim they knew what they were doing when they defined default values. Since default values are typically constants (...they'd better be!), rechecking the types of constants that never change on each function call assigned one or more default values would contravene the fundamental tenet of bear typing: "Don't repeat yourself over and oooover and oooo-oooover again."Show me wrong and I will shower you with upvotes.PEP 484 ("Type Hints") formalized the use of function annotations first introduced by PEP 3107 ("Function Annotations"). Python 3.5 superficially supports this formalization with a new top-level typing module, a standard API for composing arbitrarily complex types from simpler types (e.g., Callable[[Arg1Type, Arg2Type], ReturnType], a type describing a function accepting two arguments of type Arg1Type and Arg2Type and returning a value of type ReturnType).Bear typing supports none of them. In theory, it could. But not in 275 lines or less and certainly not as a stackoverflow answer.Bear typing does, however, support unions of types in the same way that the isinstance() builtin supports unions of types: as tuples. This superficially corresponds to the typing.Union type – with the obvious caveat that typing.Union supports arbitrarily complex types, while tuples accepted by @beartype support only simple classes. In my defense, 275 lines.Here's the gist of it. Get it, gist? I'll stop now.As with the @beartype decorator itself, these py.test tests may be seamlessly integrated into existing test suites without modification. Precious, isn't it?Now the mandatory neckbeard rant nobody asked for.Python 3.5 provides no actual support for using PEP 484 types. wat?It's true: no type checking, no type inference, no type nuthin'. Instead, developers are expected to routinely run their entire codebases through heavyweight third-party CPython interpreter wrappers implementing a facsimile of such support (e.g., mypy). Of course, these wrappers impose:I ask Guido: "Why? Why bother inventing an abstract API if you weren't willing to pony up a concrete API actually doing something with that abstraction?" Why leave the fate of a million Pythonistas to the arthritic hand of the free open-source marketplace? Why create yet another techno-problem that could have been trivially solved with a 275-line decorator in the official Python stdlib?I have no Python and I must scream.Edit: as of 2019 there is more support for using type annotations and static checking in Python; check out the typing module and mypy. The 2013 answer follows:Type checking is generally not Pythonic. In Python, it is more usual to use duck typing. Example:In you code, assume that the argument (in your example a) walks like an int and quacks like an int. For instance:This means that not only does your function work with integers, it also works with floats and any user defined class with the __add__ method defined, so less (sometimes nothing) has to be done if you, or someone else, want to extend your function to work with something else. However, in some cases you might need an int, so then you could do something like this:and the function still works for any a that defines the __int__ method.In answer to your other questions, I think it is best (as other answers have said to either do this:orSome type checking decorators I made:One way is to use assert:You can use Type Enforcement accept/returns decorators from

PythonDecoratorLibrary

It's very easy and readable:There are different ways to check what a variable is in Python. So, to list a few:hasattr leans more towards duck-typing, and something that is usually more pythonic but that term is up opinionated.Just as a note, assert statements are usually used in testing, otherwise, just use if/else statements.Normally, you do something like this:I did quite a bit of investigation on that topic recently since I was not satisfied with the many libraries I found out there.I ended up developing a library to address this, it is named valid8. As explained in the documentation, it is for value validation mostly (although it comes bundled with simple type validation functions too), and you might wish to associate it with a PEP484-based type checker such as enforce or pytypes.This is how you would perform validation with valid8 alone (and mini_lambda actually, to define the validation logic - but it is not mandatory) in your case:And this is the same example leveraging PEP484 type hints and delegating type checking to enforce:This checks the type of input arguments upon calling the function:Also check with second=9 (it must give assertion error)If you want to check **kwargs, *args as well as normal arguments in one go, you can use the locals() function as the first statement in your function definition to get a dictionary of the arguments.Then use type() to examine the arguments, for example whilst iterating over the dict.Demo:more about locals()If you want to do the validation for several functions you can add the logic inside a decorator like this:and use it:Hope this helps!This is not the solution to you, but if you want to restrict the function calls to some specific parameter types then you must use the PROATOR { The Python Function prototype validator }. you can refer the following link. https://github.com/mohit-thakur-721/proator 

Setting Different Bar color in matplotlib Python [duplicate]

Santiago Munez

[Setting Different Bar color in matplotlib Python [duplicate]](https://stackoverflow.com/questions/18973404/setting-different-bar-color-in-matplotlib-python)

Supposely, I have the bar chart as below:Any ideas on how to set different colors for each carrier? As for example, AK would be Red, GA would be Green, etc?I am using Pandas and matplotlib in PythonFor the suggestions above, how do exactly we could enumerate ax.get_children() and check if the object type is rectangle? So if the object is rectangle, we would assign different random color?

2013-09-24 05:13:17Z

Supposely, I have the bar chart as below:Any ideas on how to set different colors for each carrier? As for example, AK would be Red, GA would be Green, etc?I am using Pandas and matplotlib in PythonFor the suggestions above, how do exactly we could enumerate ax.get_children() and check if the object type is rectangle? So if the object is rectangle, we would assign different random color?Simple, just use .set_colorFor your new question, not much harder either, just need to find the bar from your axis, an example:If you have a complex plot and want to identify the bars first, add those:I assume you are using Series.plot() to plot your data.  If you look at the docs for Series.plot() here:http://pandas.pydata.org/pandas-docs/dev/generated/pandas.Series.plot.htmlthere is no color parameter listed where you might be able to set the colors for your bar graph.However, the Series.plot() docs state the following at the end of the parameter list: What that means is that when you specify the kind argument for Series.plot() as bar, Series.plot() will actually call matplotlib.pyplot.bar(), and matplotlib.pyplot.bar() will be sent all the extra keyword arguments that you specify at the end of the argument list for Series.plot().  If you examine the docs for the matplotlib.pyplot.bar() method here:http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar..it also accepts keyword arguments at the end of it's parameter list, and if you peruse the list of recognized parameter names, one of them is color, which can be a sequence specifying the different colors for your bar graph.  Putting it all together, if you specify the color keyword argument at the end of your Series.plot() argument list, the keyword argument will be relayed to the matplotlib.pyplot.bar() method.  Here is the proof:  Note that if there are more bars than colors in your sequence, the colors will repeat.Update pandas 0.17.0@7stud's answer for the newest pandas version would require to just call instead ofThe plotting functions have become members of the Series, DataFrame objects and in fact calling pd.Series.plot with a color argument gives an error

What is Python's heapq module?

minerals

[What is Python's heapq module?](https://stackoverflow.com/questions/19979518/what-is-pythons-heapq-module)

I tried "heapq" and arrived at the conclusion that my expectations differ from what I see on the screen. I need somebody to explain how it works and where it can be useful.From the book Python Module of the Week under paragraph 2.2 Sorting it is writtenHere is what I do and get.So, as you see the "heap" list is not sorted at all, in fact the more you add and remove the items the more cluttered it becomes. Pushed values take unexplainable positions.

What is going on? 

2013-11-14 13:54:16Z

I tried "heapq" and arrived at the conclusion that my expectations differ from what I see on the screen. I need somebody to explain how it works and where it can be useful.From the book Python Module of the Week under paragraph 2.2 Sorting it is writtenHere is what I do and get.So, as you see the "heap" list is not sorted at all, in fact the more you add and remove the items the more cluttered it becomes. Pushed values take unexplainable positions.

What is going on? The heapq module maintains the heap invariant, which is not the same thing as maintaining the actual list object in sorted order.Quoting from the heapq documentation:This means that it is very efficient to find the smallest element (just take heap[0]), which is great for a priority queue. After that, the next 2 values will be larger (or equal) than the 1st, and the next 4 after that are going to be larger than their 'parent' node, then the next 8 are larger, etc.You can read more about the theory behind the datastructure in the Theory section of the documentation. You can also watch this lecture from the MIT OpenCourseWare Introduction to Algorithms course, which explains the algorithm in general terms.A heap can be turned back into a sorted list very efficiently:by just popping the next element from the heap. Using sorted(heap) should be faster still, however, as the TimSort algorithm used by Python’s sort will take advantage of the partial ordering already present in a heap.You'd use a heap if you are only interested in the smallest value, or the first n smallest values, especially if you are interested in those values on an ongoing basis; adding new items and removing the smallest is very efficient indeed, more so than resorting the list each time you added a value.Your book is wrong! As you demonstrate, a heap is not a sorted list (though a sorted list is a heap). What is a heap? To quote Skiena's Algorithm Design ManualCompared to a sorted list, a heap obeys a weaker condition the heap invariant. Before defining it, first think why relaxing the condition might be useful. The answer is the weaker condition is easier to maintain. You can do less with a heap, but you can do it faster.A heap has three operations:Crucially Insert is O(log n) which beats O(n) for a sorted list.What is the heap invariant? "A binary tree where parents dominate their children". That is, "p ≤ c for all children c of p". Skiena illustrates with pictures and goes on to demonstrate the algorithm for inserting elements while maintaining the invariant. If you think a while, you can invent them yourself. (Hint: they are known as bubble up and bubble down)The good news is that batteries-included Python implements everything for you, in the heapq module. It doesn't define a heap type (which I think would be easier to use), but provides them as helper functions on list.Moral: If you write an algorithm using a sorted list but only ever inspect and remove from one end, then you can make the algorithm more efficient by using a heap.For a problem in which a heap data structure is useful, read https://projecteuler.net/problem=500There is some misunderstanding of the heap data structure implementation. The heapq module is actually a variant of the binary heap implementation, where heap elements are stored in a list, as described here: https://en.wikipedia.org/wiki/Binary_heap#Heap_implementationQuoting Wikipedia:This image below should help you to feel the difference between tree and list representation of the heap and (note, that this is a max heap, which is the inverse of the usual min-heap!):In general, heap data structure is different from a sorted list in that it sacrifices some information about whether any particular element is bigger or smaller than any other. Heap only can tell, that this particular element is less, than it's parent and bigger, than it's children. The less information a data structure stores, the less time/memory it takes to modify it. Compare the complexity of some operations between a heap and a sorted array:I know this is an older question, but the OP just missed the answer, with diagrams and an explanation of why the sort order looks off when listed in a liner fashion.(so I am not going into the optimization, efficiency, etc. I am answering the visual ordering, structure of the OP quesion)He was at pymotw.com but if he had only gotten to:

https://pymotw.com/2/heapq/" A min-heap requires that the parent be less than or equal to its children"So think tree, think pyramid. This isn't a bad link at all either

https://medium.com/basecs/learning-to-love-heaps-cef2b273a238So each parent has a two-child policy. And the kids can only have two child elements as well. The beauty of it is that the kids will always be either less than or equal to (heap-max) to their parents  or more than or equal to  their parents (heap min). heap-max or heap-min (that causes confusion) refer to the top-most element or if linear, heap[0]. Whether that represents the max value as a start or min value as a start. I'm going to leave the math out as much as possible.So (numbers are indices)heap[0] has two kids. heap[1] and heap[2]. heap[1] kids would be heap[3] and heap[4]heap[2] kids would be heap[5] and heap[6]heap[3] kids would be heap[7] and heap[8]heap[4] kids would be heap[9] and heap[10]and so on. so, the question,[2, 3, 5, 7, 4, 11, 6, 10, 8, 9] <<< Why is 11 put between 4 and 6?because value 11 stored at index 5. And index 5 is a child of index 2 which has the value of 3. The value 4 (index 4) and is the child of index 1It is ordered from smallest, it just doesn't LOOK it when examined in a linear fashion.so.... this again. And it is true.

"A min-heap requires that the parent be less than or equal to its children"Make yourself crazy and pencil it out for max.... it will be true still.(ever write one of these things and just wait to get squashed by some post doctoral?)so let's pop off the first element and do like a normal list or queueLet's stop.index 1 has a value of 5. index 3, it child's value is 4 and is smaller.... the rule is broken. The heap is reordered to maintain the relationships. so it will basically, never look sorted and it won't look anything like the prior iteration of itself before popping off the value.There are ways to reorder the node, and that second article talks bout them. I just wanted to answer the question specifically.

Getting error ImportMismatchError while running py.test

A J

[Getting error ImportMismatchError while running py.test](https://stackoverflow.com/questions/44067609/getting-error-importmismatcherror-while-running-py-test)

When I am running tests locally its working fine, but after creating the docker and running inside the container I am getting below error./apis - its the WORKDIR in Dockerfile.

2017-05-19 10:26:04Z

When I am running tests locally its working fine, but after creating the docker and running inside the container I am getting below error./apis - its the WORKDIR in Dockerfile.I have fixed it by removing all __pycache__ pkg under test/ directory, the issue was when I was creating docker image its picking all my __pycache__ and *.pyc files too, at the time when test are running its using my local machine path instead of the path in docker container.Conclusion: Clear your *.pyc and __pycache__ files before creating a docker image.You can use the .dockerignore file to exclude all __pycache__ folders from being sent to the docker image context:.dockerignore file, excludes __pycache__ folders and *.pyc files from all sub/folders:Delete all the .pyc files. You can do this by 

find . -name \*.pyc -deleteI am using Python 3.6. In my case, I was getting ImportMismatchError in modules with the same name under different packages, e.g., A/B/main.py and C/D/main.py. Python 3 does not require __init__.py file in source folders, but adding __init__.py under A/B and C/D solved the issue.You can set environment variable PY_IGNORE_IMPORTMISMATCH=1 to skip this errros. It should be fine in simple cases like running tests inside and outside docker container.Found __pycache__ files in coverage/fullcoverage/ which are hidden in jupyter notebook etc.simply navigate to the folder and use rm -r __pyache__/ . This will take care of your pytest. 

How can I tell if a python variable is a string or a list?

Graeme Perrow

[How can I tell if a python variable is a string or a list?](https://stackoverflow.com/questions/836387/how-can-i-tell-if-a-python-variable-is-a-string-or-a-list)

I have a routine that takes a list of strings as a parameter, but I'd like to support passing in a single string and converting it to a list of one string. For example:How can my function tell whether a string or a list has been passed in? I know there is a type function, but is there a "more pythonic" way?

2009-05-07 18:53:26Z

I have a routine that takes a list of strings as a parameter, but I'd like to support passing in a single string and converting it to a list of one string. For example:How can my function tell whether a string or a list has been passed in? I know there is a type function, but is there a "more pythonic" way?Well, there's nothing unpythonic about checking type.  Having said that, if you're willing to put a small burden on the caller:I'd argue this is more pythonic in that "explicit is better than implicit".  Here there is at least a recognition on the part of the caller when the input is already in list form.Personally, I don't really like this sort of behavior -- it interferes with duck typing. One could argue that it doesn't obey the "Explicit is better than implicit" mantra. Why not use the varargs syntax:I would say the most Python'y way is to make the user always pass a list, even if there is only one item in it. It makes it really obvious func() can take a list of filesAs Dave suggested, you could use the func(*files) syntax, but I never liked this feature, and it seems more explicit ("explicit is better than implicit") to simply require a list. It's also turning your special-case (calling func with a single file) into the default case, because now you have to use extra syntax to call func with a list..If you do want to make a special-case for an argument being a string, use the isinstance() builtin, and compare to basestring (which both str() and unicode() are derived from) for example:Really, I suggest simply requiring a list, even with only one file (after all, it only requires two extra characters!)If you have more control over the caller, then one of the other answers is better. I don't have that luxury in my case so I settled on the following solution (with caveats):This approach will work for cases where you are dealing with a know set of list-like types that meet the above criteria. Some sequence types will be missed though.Varargs was confusing for me, so I tested it out in Python to clear it up for myself.First of all the PEP for varargs is here.Here is sample program, based on the two answers from Dave and David Berger, followed by the output, just for clarification.And the resulting output;Hope this is helpful to somebody else.

Why aren't Python sets hashable?

Dan Burton

[Why aren't Python sets hashable?](https://stackoverflow.com/questions/6310867/why-arent-python-sets-hashable)

I stumbled across a blog post detailing how to implement a powerset function in Python. So I went about trying my own way of doing it, and discovered that Python apparently cannot have a set of sets, since set is not hashable. This is irksome, since the definition of a powerset is that it is a set of sets, and I wanted to implement it using actual set operations.Is there a good reason Python sets are not hashable?

2011-06-10 18:56:59Z

I stumbled across a blog post detailing how to implement a powerset function in Python. So I went about trying my own way of doing it, and discovered that Python apparently cannot have a set of sets, since set is not hashable. This is irksome, since the definition of a powerset is that it is a set of sets, and I wanted to implement it using actual set operations.Is there a good reason Python sets are not hashable?Generally, only immutable objects are hashable in Python.  The immutable variant of set() -- frozenset() -- is hashable.Because they're mutable.If they were hashable, a hash could silently become "invalid", and that would pretty much make hashing pointless.From the Python docs:In case this helps... if you really need to convert unhashable things into hashable equivalents for some reason you might do something like this:My HashableDict implementation is the simplest and least rigorous example from here.  If you need a more advanced HashableDict that supports pickling and other things, check the many other implementations.  In my version above I wanted to preserve the original dict class, thus preserving the order of OrderedDicts.  I also use AttrDict from here for attribute-like access.My example above is not in any way authoritative, just my solution to a similar problem where I needed to store some things in a set and needed to "hashify" them first.

Python interpreter error, x takes no arguments (1 given)

Linus

[Python interpreter error, x takes no arguments (1 given)](https://stackoverflow.com/questions/4445405/python-interpreter-error-x-takes-no-arguments-1-given)

I'm writing a small piece of python as a homework assignment, and I'm not getting it to run! I don't have that much Python-experience, but I know quite a lot of Java. 

I'm trying to implement a Particle Swarm Optimization algorithm, and here's what I have: Now, I see no reason why this shouldn't work. 

However, when I run it, I get this error:"TypeError: updateVelocity() takes no arguments (1 given)"I don't understand! I'm not giving it any arguments!Thanks for the help,Linus

2010-12-14 23:40:01Z

I'm writing a small piece of python as a homework assignment, and I'm not getting it to run! I don't have that much Python-experience, but I know quite a lot of Java. 

I'm trying to implement a Particle Swarm Optimization algorithm, and here's what I have: Now, I see no reason why this shouldn't work. 

However, when I run it, I get this error:"TypeError: updateVelocity() takes no arguments (1 given)"I don't understand! I'm not giving it any arguments!Thanks for the help,LinusPython implicitly passes the object to method calls, but you need to explicitly declare the parameter for it. This is customarily named self:Make sure, that all of your class methods (updateVelocity, updatePosition, ...) take at least one positional argument, which is canonically named self and refers to the current instance of the class.When you call particle.updateVelocity(), the called method implicitly gets an argument: the instance, here particle as first parameter. Your updateVelocity() method is missing the explicit self parameter in its definition. Should be something like this:Your other methods (except for __init__) have the same problem.I have been puzzled a lot with this problem, since I am relively new in Python. I cannot apply the solution to the code given by the questioned, since it's not self executable. So I bring a very simple code:As you can see, the solution consists in using two (dummy) arguments, even if they are not used either by the function itself or in calling it! It sounds crazy, but I believe there must be a reason for it (hidden from the novice!).I have tried a lot of other ways ('self' included). It's the only one that works (for me, at least). 

Python warnings.warn() vs. logging.warning()

hekevintran

[Python warnings.warn() vs. logging.warning()](https://stackoverflow.com/questions/9595009/python-warnings-warn-vs-logging-warning)

What is the difference between warnings.warn() and logging.warn() in terms of what they do and how they should be used?

2012-03-07 02:30:18Z

What is the difference between warnings.warn() and logging.warn() in terms of what they do and how they should be used?One raises an exception which can be caught or ignored as desired, and the other optionally adds an entry to the log based on the current logging level. One should be used when one is warning about various things in code, and the other should be used when logging.I agree with the other answer -- logging is for logging and warning is for warning -- but I'd like to add more detail.Here is a tutorial-style HOWTO taking you through the steps in using the logging module. 

https://docs.python.org/3/howto/logging.htmlIt directly answers your question:logging.warning just logs something at the WARNING level, in the same way that logging.info logs at the INFO level and logging.error logs at the ERROR level. It has no special behaviour.warnings.warn emits a Warning, which may be printed to stderr, ignored completely, or thrown like a normal Exception (potentially crashing your application) depending upon the precise Warning subclass emitted and how you've configured your Warnings Filter. By default, warnings will be printed to stderr or ignored.Warnings emitted by warnings.warn are often useful to know about, but easy to miss (especially if you're running a Python program in a background process and not capturing stderr). For that reason, it can be helpful to have them logged. Python provides a built-in integration between the logging module and the warnings module to let you do this; just call logging.captureWarnings(True) at the start of your script and all warnings emitted by the warnings module will automatically be logged at level WARNING.Besides the canonical explanation in official documentationIt is also worth noting that, by default warnings.warn("same message") will show up only once. That is a major noticeable difference. Quoted from official doc

Python: load variables in a dict into namespace

dzhelil

[Python: load variables in a dict into namespace](https://stackoverflow.com/questions/2597278/python-load-variables-in-a-dict-into-namespace)

I want to use a bunch of local variables defined in a function, outside of the function. So I am passing x=locals() in the return value. How can I load all the variables defined in that dictionary into the namespace outside the function, so that instead of accessing the value using x['variable'], I could simply use variable.

2010-04-08 02:50:31Z

I want to use a bunch of local variables defined in a function, outside of the function. So I am passing x=locals() in the return value. How can I load all the variables defined in that dictionary into the namespace outside the function, so that instead of accessing the value using x['variable'], I could simply use variable.Consider the Bunch alternative:so if you have a dictionary d and want to access (read) its values with the syntax x.foo instead of the clumsier d['foo'], just dothis works both inside and outside functions -- and it's enormously cleaner and safer than injecting d into globals()!  Remember the last line from the Zen of Python...:Rather than create your own object, you can use argparse.Namespace:To do the inverse:This is perfectly valid case to import variables in

one local space into another local space as long as

one is aware of what he/she is doing.

I have seen such code many times being used in useful ways.

Just need to be careful not to pollute common global space.You can do the following:Importing variables into a local namespace is a valid problem and often utilized in templating frameworks.Return all local variables from a function:Then import as follows:There's Always this option, I don't know that it is the best method out there, but it sure does work.  Assuming type(x) = dictUsed following snippet (PY2) to make recursive namespace from my dict(yaml) configs:

Python sum, why not strings? [closed]

Muhammad Alkarouri

[Python sum, why not strings? [closed]](https://stackoverflow.com/questions/3525359/python-sum-why-not-strings)

Python has a built in function sum, which is effectively equivalent to:for all types of parameters except strings. It works for numbers and lists, for example:Why were strings specially left out?I seem to remember discussions in the Python list for the reason, so an explanation or a link to a thread explaining it would be fine.Edit: I am aware that the standard way is to do "".join. My question is why the option of using sum for strings was banned, and no banning was there for, say, lists.Edit 2: Although I believe this is not needed given all the good answers I got, the question is: Why does sum work on an iterable containing numbers or an iterable containing lists but not an iterable containing strings?

2010-08-19 19:13:30Z

Python has a built in function sum, which is effectively equivalent to:for all types of parameters except strings. It works for numbers and lists, for example:Why were strings specially left out?I seem to remember discussions in the Python list for the reason, so an explanation or a link to a thread explaining it would be fine.Edit: I am aware that the standard way is to do "".join. My question is why the option of using sum for strings was banned, and no banning was there for, say, lists.Edit 2: Although I believe this is not needed given all the good answers I got, the question is: Why does sum work on an iterable containing numbers or an iterable containing lists but not an iterable containing strings?Python tries to discourage you from "summing" strings. You're supposed to join them:It's a lot faster, and uses much less memory.A quick benchmark:Edit (to answer OP's edit): As to why strings were apparently "singled out", I believe it's simply a matter of optimizing for a common case, as well as of enforcing best practice: you can join strings much faster with ''.join, so explicitly forbidding strings on sum will point this out to newbies.BTW, this restriction has been in place "forever", i.e., since the sum was added as a built-in function (rev. 32347)You can in fact use sum(..) to concatenate strings, if you use the appropriate starting object! Of course, if you go this far you have already understood enough to use "".join(..) anyway..Here's the source: http://svn.python.org/view/python/trunk/Python/bltinmodule.c?revision=81029&view=markupIn the builtin_sum function we have this bit of code:So.. that's your answer.It's explicitly checked in the code and rejected.From the docs:By making sum refuse to operate on strings, Python has encouraged you to use the correct method.Short answer: Efficiency.Long answer: The sum function has to create an object for each partial sum.Assume that the amount of time required to create an object is directly proportional to the size of its data.  Let N denote the number of elements in the sequence to sum.doubles are always the same size, which makes sum's running time O(1)×N = O(N).int (formerly known as long) is arbitary-length.  Let M denote the absolute value of the largest sequence element.  Then sum's worst-case running time is lg(M) + lg(2M) + lg(3M) + ... + lg(NM) = N×lg(M) + lg(N!) = O(N log N).For str (where M = the length of the longest string), the worst-case running time is M + 2M + 3M + ... + NM = M×(1 + 2 + ... + N) = O(N²).Thus, summing strings would be much slower than summing numbers.str.join does not allocate any intermediate objects.  It preallocates a buffer large enough to hold the joined strings, and copies the string data.  It runs in O(N) time, much faster than sum.The Reason Why@dan04 has an excellent explanation for the costs of using sum on large lists of strings.The missing piece as to why str is not allowed for sum is that many, many people were trying to use sum for strings, and not many use sum for lists and tuples and other O(n**2) data structures. The trap is that sum works just fine for short lists of strings, but then gets put in production where the lists can be huge, and the performance slows to a crawl. This was such a common trap that the decision was made to ignore duck-typing in this instance, and not allow strings to be used with sum.Edit: Moved the parts about immutability to history.Basically, its a question of preallocation. When you use a statement such as and expect it to work similar to a reduce statement, the code generated looks something likeIn each of these steps a new string is created, which for one might give some copying overhead as the strings are getting longer and longer. But that’s maybe not the point here. What’s more important, is that every new string on each line must be allocated to it’s specific size (which. I don’t know it it must allocate in every iteration of the reduce statement, there might be some obvious heuristics to use and Python might allocate a bit more here and there for reuse – but at several points the new string will be large enough that this won’t help anymore and Python must allocate again, which is rather expensive.A dedicated method like join, however has the job to figure out the real size of the string before it starts and would therefore in theory only allocate once, at the beginning and then just fill that new string, which is much cheaper than the other solution.I dont know why, but this works!

Python: How to sort a list of dictionaries by several values?

Joko

[Python: How to sort a list of dictionaries by several values?](https://stackoverflow.com/questions/16082954/python-how-to-sort-a-list-of-dictionaries-by-several-values)

I want to sort a list at first by a value and then by a second value. Is there an easy way to do this? Here is a small example:This command is for sorting this list by 'name':But how I can sort this list by a second value? Like 'age' in this example.I want a sorting like this (first sort by 'name' and then sort by 'age'):Thanks!

2013-04-18 12:27:04Z

I want to sort a list at first by a value and then by a second value. Is there an easy way to do this? Here is a small example:This command is for sorting this list by 'name':But how I can sort this list by a second value? Like 'age' in this example.I want a sorting like this (first sort by 'name' and then sort by 'age'):Thanks!This sorts by a tuple of the two attributes, the following is equivalent and much faster/cleaner:From the comments: @BakuriuTo summarize: itemgetter keeps the execution fully on the C level, so it's as fast as possible.Here is the alternative general solution - it sorts elements of dict by keys and values.

The advantage of it - no need to specify keys, and it would still work if some keys are missing in some of dictionaries.

Python Requests requests.exceptions.SSLError: [Errno 8] _ssl.c:504: EOF occurred in violation of protocol

jasonamyers

[Python Requests requests.exceptions.SSLError: [Errno 8] _ssl.c:504: EOF occurred in violation of protocol](https://stackoverflow.com/questions/14102416/python-requests-requests-exceptions-sslerror-errno-8-ssl-c504-eof-occurred)

I'm on Ubuntu 12.10 with OpenSSL 1.0.1c, python 2.7.3, Requests 1.0.3 and 1.0.4 (tried both), and when attempting to connect to the website in the url variable with the following code.It throws the following error:Attempting the connection with openssl returns the following:If I force it to use tls1 it works (output truncated):I've seen numerous bug reports for this; however, I've not found a way to get around it using the python requests library.  Any assistance would be greatly appreciated.

2012-12-31 13:51:58Z

I'm on Ubuntu 12.10 with OpenSSL 1.0.1c, python 2.7.3, Requests 1.0.3 and 1.0.4 (tried both), and when attempting to connect to the website in the url variable with the following code.It throws the following error:Attempting the connection with openssl returns the following:If I force it to use tls1 it works (output truncated):I've seen numerous bug reports for this; however, I've not found a way to get around it using the python requests library.  Any assistance would be greatly appreciated.Reposting this here for others from the requests issue page:Requests' does not support doing this before version 1. Subsequent to version 1, you are expected to subclass the HTTPAdapter, like so:When you've done that, you can do this:Any request through that session object will then use TLSv1.Setting verify=False only skips verifying the server certificate, but will not help to resolve SSL protocol errors.This issue is likely due to SSLv2 being disabled on the web server, but Python 2.x tries to establish a connection with PROTOCOL_SSLv23 by default. This happens at https://github.com/python/cpython/blob/360aa60b2a36f5f6e9e20325efd8d472f7559b1e/Lib/ssl.py#L1057You can monkey-patch ssl.wrap_socket() in the ssl module by overriding the ssl_version keyword parameter. The following code can be used as-is. Put this at the start of your program before making any requests. Installing the "security" package extras for requests solved for me:This is a known bug, you can work it around with a hack:Open up site-packages/requests/packages/urllib3/connectionpool.py (or otherwise just make a local copy of requests inside your own project), and change the block that says:to:Otherwise, I suppose there's an override somewhere which is less hacky, but I couldn't find one with a few glances.NOTE: On a sidenote, requests from PIP (1.0.4) on a MacOS just works with the URL you provided.I encountered this error, and the fix appears to be turning off SNI, which Python 2.7 does not support:http://bugs.python.org/issue5639urllib3 on python 2.7 SNI error on Google App EngineTo people that can't get above fixes working.Had to change file ssl.py to fix it.

Look for function create_default_context and change line:toMaybe someone can create easier solution without editing ssl.py?I had the same issue:I had fiddler running, I stopped fiddler capture and did not see this error. Could be because of fiddler.Unfortunately the accepted answer did not work for me. As a temporary workaround you could also use verify=False when connecting to the secure website.From Python Requests throwing up SSLErrorI was having similar issue and I think if we simply ignore the ssl verification will work like charm as it worked for me. So connecting to server with https scheme but directing them not to verify the certificate.Using requests. Just mention verify=False instead of NoneHoping this will work for those who needs :).I have had this error when connecting to a RabbitMQ MQTT server via TLS. I'm pretty sure the server is broken but anyway it worked with OpenSSL 1.0.1, but not OpenSSL 1.0.2.You can check your version in Python using this:I'm not sure how to downgrade OpenSSL within Python (it seems to be statically linked on Windows at least), other than using an older version of Python.

visual studio code disable auto wrap long line

Kymo Wang

[visual studio code disable auto wrap long line](https://stackoverflow.com/questions/47406741/visual-studio-code-disable-auto-wrap-long-line)

I use vs code to write python with pylint. When I press ctrl+S(save), the editor wrap a long line into multiple short lines. How to disable the action or configure wrap column count to 120 (default is 80)? I  have tried "python.linting.pylintArgs": ["--max-line-length=120"] and "editor.wordWrapColumn": 120, but didn't work.

2017-11-21 06:40:14Z

I use vs code to write python with pylint. When I press ctrl+S(save), the editor wrap a long line into multiple short lines. How to disable the action or configure wrap column count to 120 (default is 80)? I  have tried "python.linting.pylintArgs": ["--max-line-length=120"] and "editor.wordWrapColumn": 120, but didn't work.Check your Python formatting provider.I guess in your case it is not PyLint who keeps wrapping the long lines, but autopep8.Try setting --max-line-length for autopep8 instead.When using custom arguments, each top-level element of an argument string that's separated by space on the command line must be a separate item in the args list. For example:For proper formatting of these Python settings you can check 

Formatter-specific settings here: https://code.visualstudio.com/docs/python/editing#_formatterspecific-settingsAlso check the answers here:

https://stackoverflow.com/a/54031007/1130803

What is 'print' in Python?

Mike

[What is 'print' in Python?](https://stackoverflow.com/questions/7020417/what-is-print-in-python)

I understand what print does, but of what "type" is that language element? I think it's a function, but why does this fail?Isn't print a function? Shouldn't it print something like this?

2011-08-11 03:12:23Z

I understand what print does, but of what "type" is that language element? I think it's a function, but why does this fail?Isn't print a function? Shouldn't it print something like this?In 2.7 and down, print is a statement. In python 3, print is a function. To use the print function in Python 2.6 or 2.7, you can do See this section from the Python Language Reference, as well as PEP 3105 for why it changed.In Python 3, print() is a built-in function (object)Before this, print was a statement. Demonstration...print is a mistake that has been rectified in Python 3. In Python 3 it is a function. In Python 1.x and 2.x it is not a function, it is a special form like if or while, but unlike those two it is not a control structure.So, I guess the most accurate thing to call it is a statement.In Python all statements (except assignment) are expressed with reserved words, not addressible objects.  That is why you cannot simply print print and you get a SyntaxError for trying.  It's a reserved word, not an object.Confusingly, you can have a variable named print.  You can't address it in the normal way, but you can setattr(locals(), 'print', somevalue) and then print locals()['print'].Other reserved words that might be desirable as variable names but are nonetheless verboten:In Python 2, print is a statement, which is a whole different kind of thing from a variable or function. Statements are not Python objects that can be passed to type(); they're just part of the language itself, even more so than built-in functions. For example, you could do sum = 5 (even though you shouldn't), but you can't do print = 5 or if = 7 because print and if are statements.In Python 3, the print statement was replaced with the print() function. So if you do type(print), it'll return <class 'builtin_function_or_method'>.BONUS:In Python 2.6+, you can put from __future__ import print_function at the top of your script (as the first line of code), and the print statement will be replaced with the print() function.

How to pass a list from Python, by Jinja2 to JavaScript

user1843766

[How to pass a list from Python, by Jinja2 to JavaScript](https://stackoverflow.com/questions/15321431/how-to-pass-a-list-from-python-by-jinja2-to-javascript)

Let's say I have a Python variable:and I pass it to Jinja by rendering HTML, and I also have a function in JavaScript called somefunction(variable). I am trying to pass each item of list_of_items. I tried something like this:Is it possible to pass a list from Python to JavaScript or should I pass each item from list one by one in a loop? How can I do this?

2013-03-10 10:54:07Z

Let's say I have a Python variable:and I pass it to Jinja by rendering HTML, and I also have a function in JavaScript called somefunction(variable). I am trying to pass each item of list_of_items. I tried something like this:Is it possible to pass a list from Python to JavaScript or should I pass each item from list one by one in a loop? How can I do this?To pass some context data to javascript code, you have to serialize it in a way it will be "understood" by javascript (namely JSON). You also need to mark it as safe using the safe Jinja filter, to prevent your data from being htmlescaped.You can achieve this by doing something like that:So, to achieve exactly what you want (loop over a list of items, and pass them to a javascript function), you'd need to serialize every item in your list separately. Your code would then look like this:In my example, I use Flask, I don't know what framework you're using, but you got the idea, you just have to make it fit the framework you use.NEVER EVER DO THIS WITH USER-SUPPLIED DATA, ONLY DO THIS WITH TRUSTED DATA!Otherwise, you would expose your application to XSS vulnerabilities!I had a similar problem using Flask, but I did not have to resort to JSON. I just passed a list letters = ['a','b','c'] with render_template('show_entries.html', letters=letters), and set in my javascript code. Jinja2 replaced {{ letters }} with ['a','b','c'], which javascript interpreted as an array of strings.You can do this with Jinja's tojson filter, whichFor example, in your Python, write:... or, in the context of a Flask endpoint:Then in your template, write this:(Note that the onclick attribute is single-quoted. This is necessary since |tojson escapes ' characters but not " characters in its output, meaning that it can be safely used in single-quoted HTML attributes but not double-quoted ones.)Or, to use list_of_items in an inline script instead of an HTML attribute, write this:DON'T use json.dumps to JSON-encode variables in your Python code and pass the resulting JSON text to your template. This will produce incorrect output for some string values, and will expose you to XSS if you're trying to encode user-provided values. This is because Python's built-in json.dumps doesn't escape characters like < and > (which need escaping to safely template values into inline <script>s, as noted at https://html.spec.whatwg.org/multipage/scripting.html#restrictions-for-contents-of-script-elements) or single quotes (which need escaping to safely template values into single-quoted HTML attributes).If you're using Flask, note that Flask injects a custom tojson filter instead of using Jinja's version. However, everything written above still applies. The two versions behave almost identically; Flask's just allows for some app-specific configuration that isn't available in Jinja's version.I can suggest you a javascript oriented approach which makes it easy to work with javascript files in your project.Create a javascript section in your jinja template file and place all variables you want to use in your javascript files in a window object:Start.htmlJinja will replace values and our appConfig object will be reachable from our other script files:App.jsI have seperated javascript code from html documents with this way which is easier to manage and seo friendly.To add up on the selected answer, I have been testing a new option that is working too using jinja2 and flask:The template:the output of the rendered template:The safe could be added but as well like {{ data | tojson | safe }} but it is working without too.Make some invisible HTML tags like <label>, <p>, <input> etc. and name its id, and the class name is a pattern so that you can retrieve it later.Let you have two lists maintenance_next[] and maintenance_block_time[] of the same length, and you want to pass these two list's data to javascript using the flask. So you take some invisible label tag and set its tag name is a pattern of list's index and set its class name as value at index.Now you can retrieve the data in javascript using some javascript operation like below -

Assign variable in while loop condition in Python?

user541686

[Assign variable in while loop condition in Python?](https://stackoverflow.com/questions/6631128/assign-variable-in-while-loop-condition-in-python)

I just came across this piece of codeand thought, there must be a better way to do this, than using an infinite loop with break.So I tried:and, obviously, got an error.Is there any way to avoid using a break in that situation?Ideally, you'd want to avoid saying readline twice... IMHO, repeating is even worse than just a break, especially if the statement is complex.

2011-07-08 22:16:05Z

I just came across this piece of codeand thought, there must be a better way to do this, than using an infinite loop with break.So I tried:and, obviously, got an error.Is there any way to avoid using a break in that situation?Ideally, you'd want to avoid saying readline twice... IMHO, repeating is even worse than just a break, especially if the statement is complex.If you aren't doing anything fancier with data, like reading more lines later on, there's always:Try this one, works for files opened with open('filename')Starting Python 3.8, and the introduction of assignment expressions (PEP 572) (:= operator), it's now possible to capture the condition value (data.readline()) of the while loop as a variable (line) in order to re-use it within the body of the loop:This isn't much better, but this is the way I usually do it.  Python doesn't return the value upon variable assignment like other languages (e.g., Java).Like,? It large depends on the semantics of the data object's readline semantics. If data is a file object, that'll work.Will iterate over each line in the file, rather than using a while. It is a much more common idiom for the task of reading a file in my experience (in Python).In fact, data does not have to be a file but merely provide an iterator.You could do:If data has a function that returns an iterator instead of readline (say data.iterate), you could simply do:If data is a file, as stated in other answers, using for line in file will work fine. If data is not a file, and a random data reading object, then you should implement it as an iterator, implementing __iter__ and next methods. The next method should to the reading, check if there is more data, and if not, raise StopIteration. If you do this, you can continue using the for line in data idiom.According to the FAQ from Python's documentation, iterating over the input with for construct or running an infinite while True loop and using break statement to terminate it, are preferred and idiomatic ways of iteration.As of python 3.8 (which implements PEP-572) this code is now valid:

Python equivalent of npm or rubygems

Dillon Benson

[Python equivalent of npm or rubygems](https://stackoverflow.com/questions/13537901/python-equivalent-of-npm-or-rubygems)

I've been looking around for a package manager that can be used with python. I want to list project dependencies in a file. For example ruby uses Gemfile where you can use bundle install.

How can I achieve this in python?

2012-11-24 03:10:41Z

I've been looking around for a package manager that can be used with python. I want to list project dependencies in a file. For example ruby uses Gemfile where you can use bundle install.

How can I achieve this in python?The pip tool is becoming the standard in equivalent of Ruby's gems. Like distribute, pip uses the PyPI package repository (by default) for resolving and downloading dependencies. pip can install dependencies from a file listing project dependencies (called requirements.txt by convention):You can "freeze" the current packages on the Python path using pip as well:When used in combination with the virtualenv package, you can reliably create project Python environments with a project's required dependencies.(I know it's an old question, and it already has an answer but for anyone coming here looking for a different answer like me.)I've found a very good equivalent for npm, It's called pipenv. It handles both virtualenv and pip requirements at the same time so it's more like npm.then you can make a new virtualenv with third version of python, as well as making a pipfile that will be filled with your projects requirement and other stuff:using your created virtualenv:installing a new python package:running your .py file is like:you can find it's doc here. Python uses pip for a package manager. The pip install command has a -r <file> option to install packages from the specified requirements file.Install command:Example requirements.txt contents:See the Requirements Parsing section of the docs for a full description of the format: https://pip.pypa.io/en/stable/user_guide/#requirements-filesThis is how I restrict pip's scope to the current project. It feels like the opposite if you're coming from NodeJS's npm or PHP's composer where you explicitly specify global installations with -g or --global.If you don't already have virtualenv installed, then install it globally with:Each Python project should have its own virtualenv installation. It's easy to set one up, just cd to your project's root and:Activate virtualenv:Now, any interaction with pip is contained within your project.Run pip install package_name==version for each of your dependencies. They are installed in ./env/lib/python3.x/site-packages/When you want to save your project's dependencies to a file, run:You actually don't need -l or --local if you're in an activated project-specific virtualenv (which you should be).Now, when you want to install your dependencies from requirements.txt, set up your virtualenv, and run:That's all.This is an old question but things are constantly evolving.Further to the other answer about pipenv. There is also a python package manger called poetry. There is a detailed comparison between pipenv and poerty here: Feature comparison between npm, pip, pipenv and poetry package managers. It also links the features to common npm features.

Convert RGB color to English color name, like 'green' with Python

MikeVaughan

[Convert RGB color to English color name, like 'green' with Python](https://stackoverflow.com/questions/9694165/convert-rgb-color-to-english-color-name-like-green-with-python)

I want to convert a color tuple to a color name, like 'yellow' or 'blue'Is there a simple way in python to do this?

2012-03-14 00:14:19Z

I want to convert a color tuple to a color name, like 'yellow' or 'blue'Is there a simple way in python to do this?It looks like webcolors will allow you to do this:it is vice-versa-able:However webcolors raises an exception if it can't find a match for the requested colour. I've written a little fix that delivers the closest matching name for the requested RGB colour. It matches by Euclidian distance in the RGB space.Output:There is a program called pynche which can change RGB to colour name in English for Python.You can try to use the method ColorDB.nearest() in ColorDB.py which can do what you want.You can find more information about this method here : ColorDB PyncheFor those who, like me, want a more familiar colour name, you can use the CSS 2.1 colour names, also provided by webcolors:Just use fraxel's excellent answer and code for getting the closest colour, adapted to CSS 2.1:A solution to your problem consists in mapping the RGB values to the HSL color space.Once you have the color in the HSL color space you can use the H (hue) component to map it the color. Note that color is a somewhat subjective concept, so you would have to define which ranges of H corresponds to a given color.

「UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.」when plotting figure with pyplot on Pycharm

johnwolf1987

[「UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.」when plotting figure with pyplot on Pycharm](https://stackoverflow.com/questions/56656777/userwarning-matplotlib-is-currently-using-agg-which-is-a-non-gui-backend-so)

I am trying to plot a simple graph using pyplot, e.g.:but the figure does not appear and I get the following message:I saw in several places that one had to change the configuration of matplotlib using the following:I did this, but then got an error message because it cannot find a module:Then, I tried to install "tkinter" using pip install tkinter (inside the virtual environment), but it does not find it:I should also mention that I am running all this on Pycharm Community Edition IDE using a virtual environment, and that my operating system is Linux/Ubuntu 18.04.I would like to know how I can solve this problem in order to be able to display the graph.

2019-06-18 20:43:38Z

I am trying to plot a simple graph using pyplot, e.g.:but the figure does not appear and I get the following message:I saw in several places that one had to change the configuration of matplotlib using the following:I did this, but then got an error message because it cannot find a module:Then, I tried to install "tkinter" using pip install tkinter (inside the virtual environment), but it does not find it:I should also mention that I am running all this on Pycharm Community Edition IDE using a virtual environment, and that my operating system is Linux/Ubuntu 18.04.I would like to know how I can solve this problem in order to be able to display the graph.I found a solution to my problem (thanks to the help of ImportanceOfBeingErnest).All I had to do was to install tkinter through the Linux bash terminal using the following command:instead of installing it with pip or directly in the virtual environment in Pycharm.In my case, the error message was implying that I was working in a headless console. So plt.show() could not work. What worked was calling plt.savefig:I found the answer on a github repository.If you use Arch Linux (distributions like Manjaro or Antegros) simply type:sudo pacman -S tkAnd all will work perfectly!Try import tkinter because pycharm already installed tkinter for you, I looked Install tkinter for Python You can maybe try:as a tkinter-installing wayI've tried your way, it seems no error to run at my computer, it successfully shows the figure. maybe because pycharm have tkinter as a system package, so u don't need to install it. But if u can't find tkinter inside, you can go to Tkdocs to see the way of installing tkinter, as it mentions, tkinter is a core package for python.I too had this issue in PyCharm. This issue is because you don't have tkinter module in your machine.To install follow the steps given below (select your appropriate os)For ubuntu users orFor Centos usersorFor Windows, use pip to install tkAfter installing tkinter restart your Pycharm and run your code, it will work

difference between find and index

SohamC

[difference between find and index](https://stackoverflow.com/questions/22190064/difference-between-find-and-index)

I am new to python and cannot quite understand the difference between find and index. They always return the same result.

Thanks!!

2014-03-05 06:17:05Z

I am new to python and cannot quite understand the difference between find and index. They always return the same result.

Thanks!!str.find returns -1 when it does not find the substring.While str.index raises ValueError:Both the functions behave the same way if a sub-string is found. Also find is only available for strings where as index is available for lists, tuples and strings@falsetru provided an explanation about the difference between functions, I did a performance test between them.

Number of regex matches

dutt

[Number of regex matches](https://stackoverflow.com/questions/3895646/number-of-regex-matches)

I'm using the finditer function in the re module to match some things and everything is working. Now I need to find out how many matches I've got. Is it possible without looping through the iterator twice? (one to find out the count and then the real iteration)Some code:Everything works, I just need to get the number of matches before the loop.

2010-10-09 03:57:11Z

I'm using the finditer function in the re module to match some things and everything is working. Now I need to find out how many matches I've got. Is it possible without looping through the iterator twice? (one to find out the count and then the real iteration)Some code:Everything works, I just need to get the number of matches before the loop.If you know you will want all the matches, you could use the re.findall function. It will return a list of all the matches. Then you can just do len(result) for the number of matches.If you always need to know the length, and you just need the content of the match rather than the other info, you might as well use re.findall.  Otherwise, if you only need the length sometimes, you can use e.g.to store the iteration of the matches in a reusable tuple.  Then just do len(matches).Another option, if you just need to know the total count after doing whatever with the match objects, is to usewhich will return an (index, match) pair for each of the original matches.  So then you can just store the first element of each tuple in some variable.But if you need the length first of all, and you need match objects as opposed to just the strings, you should just doIf you find you need to stick with finditer(), you can simply use a counter while you iterate through the iterator.Example:If you need the features of finditer() (not matching to overlapping instances), use this method.I know this is a little old, but this but here is a concise function for counting regex patterns.For those moments when you really want to avoid building lists:Sometimes you might need to operate on huge strings. This might help.

Importing a function from a class in another file?

Capkutay

[Importing a function from a class in another file?](https://stackoverflow.com/questions/6757192/importing-a-function-from-a-class-in-another-file)

I'm writing a Python program for fun but got stuck trying to import a function from a class in another file. Here is my code:I want to return a function called run() from a class in another file. When I import the file, it first runs the class with run() in it, then proceeds to run the original code. Why does this happen? Here is the code from comm_system:

2011-07-20 04:57:54Z

I'm writing a Python program for fun but got stuck trying to import a function from a class in another file. Here is my code:I want to return a function called run() from a class in another file. When I import the file, it first runs the class with run() in it, then proceeds to run the original code. Why does this happen? Here is the code from comm_system:Change the end of comm system to:It's those lines being always run that are causing it to be run when imported as well as when executed.FILENAME is w/o the suffix If, like me, you want to make a function pack or something that people can download then it's very simple. Just write your function in a python file and save it as the name you want IN YOUR PYTHON DIRECTORY. Now, in your script where you want to use this, you type:Note - the parts in capital letters are where you type the file name and function name.Now you just use your function however it was meant to be.Example:FUNCTION SCRIPT - saved at C:\Python27 as function_choose.pySCRIPT USING FUNCTION - saved whereverOUTPUT WILL BE DOG, CAT, OR CHICKENHoped this helped, now you can create function packs for download!--------------------------------This is for Python 2.7-------------------------------------First you need to make sure if both of your files are in the same working directory. Next, you can import the whole file. For example,or you can import the entire class and entire functions from the file. For example,Finally, you need to create an instance of the class from the original file and call the instance objects.It would really help if you'd include the code that's not working (from the 'other' file), but I suspect you could do what you want with a healthy dose of the 'eval' function.For example:The chooser returns the name of the function to execute, eval then turns a string into actual code to be executed in-place, and the parentheses finish off the function call.You can use the below syntax - 

Python open() gives IOError: Errno 2 No such file or directory

Santiago

[Python open() gives IOError: Errno 2 No such file or directory](https://stackoverflow.com/questions/12201928/python-open-gives-ioerror-errno-2-no-such-file-or-directory)

For some reason my code is having trouble opening a simple file:This is the code:And the error is: 

2012-08-30 17:01:39Z

For some reason my code is having trouble opening a simple file:This is the code:And the error is: Let me clarify how Python finds files: If you try to do open('sortedLists.yaml'), Python will see that you are passing it a relative path, so it will search for the file inside the current working directory. Calling os.chdir will change the current working directory.Example: Let's say file.txt is found in C:\Folder.To open it, you can do:or The file may be existing but may have a different path. Try writing the absolute path for the file.Try os.listdir() function to check that atleast python sees the file. Try it like this:Most likely, the problem is that you're using a relative file path to open the file, but the current working directory isn't set to what you think it is.It's a common misconception that relative paths are relative to the location of the python script, but this is untrue. Relative file paths are always relative to the current working directory, and the current working directory doesn't have to be the location of your python script.You have three options:Other common mistakes that could cause a "file not found" error include:Possibly, you closed the 'file1'.

Just use 'w' flag, that create new file:(see also https://docs.python.org/3/library/functions.html?highlight=open#open)

What is the difference between static files and media files in Django?

Silver Light

[What is the difference between static files and media files in Django?](https://stackoverflow.com/questions/5016589/what-is-the-difference-between-static-files-and-media-files-in-django)

I'm moving to Django 1.3 and find this separation of media and static files a bit confusing. Here is how default settings.py looks like:What should I put into MEDIA_ROOT and a STATIC_ROOT? Should those be separate directories? What is the difference?

2011-02-16 12:45:11Z

I'm moving to Django 1.3 and find this separation of media and static files a bit confusing. Here is how default settings.py looks like:What should I put into MEDIA_ROOT and a STATIC_ROOT? Should those be separate directories? What is the difference?Static files are meant for javascript/images etc, but media files are for user-uploaded content.As Uku Loskit said, static files are for things like your applications' css files, javascript files, images, etc. Media files are typically user or admin uploadable files.Normally you will want MEDIA_ROOT and STATIC_ROOT to be separate directories. Keep in mind that STATIC_ROOT is where the management command collectstatic will place all the static files it finds. In production, you then configure your webserver to serve the files out of STATIC_ROOT when given a request that starts with STATIC_URL. If you are using the Django devserver for development, it will automatically serve static files.The staticfiles application thus disentangles user uploaded media from application media, thus making deployment, backups, and version control easier. Prior to the staticfiles app, it was common for developers to have the media files mixed in with static application assets.The 1.3 docs for staticfiles have been steadily improving; for more details, look at the how-to.

Generating HTML documents in python

shoes

[Generating HTML documents in python](https://stackoverflow.com/questions/6748559/generating-html-documents-in-python)

In python, what is the most elegant way to generate HTML documents.  I currently manually append all of the tags to a giant string, and write that to a file.  Is there a more elegant way of doing this?

2011-07-19 14:12:17Z

In python, what is the most elegant way to generate HTML documents.  I currently manually append all of the tags to a giant string, and write that to a file.  Is there a more elegant way of doing this?I find yattag to be the most elegant way of doing this.It reads like html, with the added benefit that you don't have to close tags.I would suggest using one of the many template languages available for python, for example the one built into Django  (you don't have to use the rest of Django to use its templating engine) - a google query should give you plenty of other alternative template implementations.I find that learning a template library helps in so many ways - whenever you need to generate an e-mail, HTML page, text file or similar, you just write a template, load it with your template library, then let the template code create the finished product. Here's some simple code to get you started:It's even simpler if you have templates on disk - check out the render_to_string function for django 1.7 that can load templates from disk from a predefined list of search paths, fill with data from a dictory and render to a string - all in one function call. (removed from django 1.8 on, see Engine.from_string for comparable action)If you're building HTML documents than I highly suggest using a template system (like jinja2) as others have suggested. If you're in need of some low level generation of html bits (perhaps as an input to one of your templates), then the xml.etree package is a standard python package and might fit the bill nicely. Prints the following:I would recommend using xml.dom to do this.http://docs.python.org/library/xml.dom.htmlRead this manual page, it has methods for building up XML (and therefore XHTML).  It makes all XML tasks far easier, including adding child nodes, document types, adding attributes, creating texts nodes.  This should be able to assist you in the vast majority of things you will do to create HTML.It is also very useful for analysing and processing existing xml documents.Hope this helpsP.SHere is a tutorial that should help you with applying the syntaxhttp://www.postneo.com/projects/pyxml/I am using the code snippet known as throw_out_your_templates for some of my own projects:https://github.com/tavisrudd/throw_out_your_templateshttps://bitbucket.org/tavisrudd/throw-out-your-templates/srcUnfortunately, there is no pypi package for it and it's not part of any distribution as this is only meant as a proof-of-concept. I was also not able to find somebody who took the code and started maintaining it as an actual project. Nevertheless, I think it is worth a try even if it means that you have to ship your own copy of throw_out_your_templates.py with your code.Similar to the suggestion to use yattag by John Smith Optional, this module does not require you to learn any templating language and also makes sure that you never forget to close tags or quote special characters. Everything stays written in Python. Here is an example of how to use it:Yes, you are looking for file .writelinesA sequence is generally a list or array. So put all your lines into a list or array.

And toss them to the function below.Make sure to remove any new line constants from your strings just to be safePython Documentation ( search for file.writelines )file.writelines(sequence)

    Write a sequence of strings to the file. The sequence can be any iterable object producing strings, typically a list of strings. There is no return value. (The name is intended to match readlines(); writelines() does not add line separators.)

How to set opacity of background colour of graph wit Matplotlib

Joe Kington

[How to set opacity of background colour of graph wit Matplotlib](https://stackoverflow.com/questions/4581504/how-to-set-opacity-of-background-colour-of-graph-wit-matplotlib)

I've been playing around with Matplotlib and I can't figure out how to change the background colour of the graph, or how to make the background completely transparent.

2011-01-03 01:26:23Z

I've been playing around with Matplotlib and I can't figure out how to change the background colour of the graph, or how to make the background completely transparent.If you just want the entire background for both the figure and the axes to be transparent, you can simply specify transparent=True when saving the figure with fig.savefig. e.g.:If you want more fine-grained control, you can simply set the facecolor and/or alpha values for the figure and axes background patch.  (To make a patch completely transparent, we can either set the alpha to 0, or set the facecolor to 'none' (as a string, not the object None!))e.g.:

Embed (create) an interactive Python shell inside a Python program

zJay

[Embed (create) an interactive Python shell inside a Python program](https://stackoverflow.com/questions/5597836/embed-create-an-interactive-python-shell-inside-a-python-program)

Is it possible to start an interactive Python shell inside a Python program?I want to use such an interactive Python shell (which is running inside my program's execution) to inspect some program-internal variables.

2011-04-08 16:08:46Z

Is it possible to start an interactive Python shell inside a Python program?I want to use such an interactive Python shell (which is running inside my program's execution) to inspect some program-internal variables.The code module provides an interactive console:In ipython 0.13+ you need to do this:I've had this code for a long time, I hope you can put it to use.To inspect/use variables, just put them into the current namespace. As an example, I can access var1 and var2 from the command line.However if you wanted to strictly debug your application, I'd highly suggest using an IDE or pdb(python debugger).Using IPython you just have to call:Another trick (besides the ones already suggested) is opening an interactive shell and importing your (perhaps modified) python script. Upon importing, most of the variables, functions, classes and so on (depending on how the whole thing is prepared) are available, and you could even create objects interactively from command line. So, if you have a test.py file, you could open Idle or other shell, and type import test (if it is in current working directory).

How can I release memory after creating matplotlib figures

sequoia

[How can I release memory after creating matplotlib figures](https://stackoverflow.com/questions/7101404/how-can-i-release-memory-after-creating-matplotlib-figures)

I have several matlpotlib functions rolled into some django-celery tasks.Every time the tasks are called more RAM is dedicated to python. Before too long, python is taking up all of the RAM.QUESTION: How can I release this memory?UPDATE 2 - A Second Solution:I asked a similar question specifically about the memory locked up when matplotlib errors, but I got a good answer to this question .clf(), .close(), and gc.collect() aren't needed if you use multiprocess to run the plotting function in a separate process whose memory will automatically be freed once the process ends.Matplotlib errors result in a memory leak. How can I free up that memory?UPDATE - The Solution:These stackoverflow posts suggested that I can release the memory used by matplotlib objects with the following commands:.clf(): Matplotlib runs out of memory when plotting in a loop.close(): Python matplotlib: memory not being released when specifying figure sizeHere is the example I used to test the solution:

2011-08-18 01:28:22Z

I have several matlpotlib functions rolled into some django-celery tasks.Every time the tasks are called more RAM is dedicated to python. Before too long, python is taking up all of the RAM.QUESTION: How can I release this memory?UPDATE 2 - A Second Solution:I asked a similar question specifically about the memory locked up when matplotlib errors, but I got a good answer to this question .clf(), .close(), and gc.collect() aren't needed if you use multiprocess to run the plotting function in a separate process whose memory will automatically be freed once the process ends.Matplotlib errors result in a memory leak. How can I free up that memory?UPDATE - The Solution:These stackoverflow posts suggested that I can release the memory used by matplotlib objects with the following commands:.clf(): Matplotlib runs out of memory when plotting in a loop.close(): Python matplotlib: memory not being released when specifying figure sizeHere is the example I used to test the solution:Did you try to run you task function several times (in a for) to be sure that not your function is leaking no matter of celery?

Make sure that django.settings.DEBUG is set False( The connection object holds all queries in memmory when DEBUG=True).

Datetime current year and month in Python

user3778327

[Datetime current year and month in Python](https://stackoverflow.com/questions/28189442/datetime-current-year-and-month-in-python)

I must have the current year and month in datetime.I use this:Is there maybe another way?

2015-01-28 10:06:54Z

I must have the current year and month in datetime.I use this:Is there maybe another way?Use:I assume you want the first of the month.Try this solution:Use:Reference: 8.1.7. strftime() and strptime() BehaviorReference: strftime() and strptime() BehaviorThe above things are useful for any date parsing, not only now or today. It can be useful for any date parsing.And so on...You can always use a sub-string method:This will get you the current month and year in integer format. If you want them to be strings you simply have to remove the " int " precedence while assigning values to the variables curr_year and curr_month.You can write the accepted answer as a one-liner using date.replace:

Converting a String to a List of Words?

rectangletangle

[Converting a String to a List of Words?](https://stackoverflow.com/questions/6181763/converting-a-string-to-a-list-of-words)

I'm trying to convert a string to a list of words using python. I want to take something like the following:Then convert to something like this :Notice the omission of punctuation and spaces. What would be the fastest way of going about this? 

2011-05-31 00:09:24Z

I'm trying to convert a string to a list of words using python. I want to take something like the following:Then convert to something like this :Notice the omission of punctuation and spaces. What would be the fastest way of going about this? Try this:How it works: From the docs :Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. If the pattern isn’t found, string is returned unchanged. repl can be a string or a function.so in our case : pattern is any non-alphanumeric character.[\w] means any alphanumeric character and is equal to the character set 

[a-zA-Z0-9_]a to z, A to Z , 0 to 9 and underscore. so we match any non-alphanumeric character and replace it with a space . and then we split() it which splits string by space  and converts it to a list so 'hello-world' becomes 'hello world'with re.sub and then ['hello' , 'world']after split()let me know if any doubts come up.I think this is the simplest way for anyone else stumbling on this post given the late response:To do this properly is quite complex. For your research, it is known as word tokenization. You should look at NLTK if you want to see what others have done, rather than starting from scratch:The most simple way:Using string.punctuation for completeness:This handles newlines as well.Well, you could useNote that both string and list are names of builtin types, so you probably don't want to use those as your variable names.Inspired by @mtrw's answer, but improved to strip out punctuation at word boundaries only:A regular expression for words would give you the most control.  You would want to carefully consider how to deal with words with dashes or apostrophes, like "I'm".Personally, I think this is slightly cleaner than the answers providedThis is from my attempt on a coding challenge that can't use regex,The role of apostrophe seems interesting.You can try and do this:This way you eliminate every special char outside of the alphabet:I'm not sure if this is fast or optimal or even the right way to program.

Split a large pandas dataframe

Nilani Algiriyage

[Split a large pandas dataframe](https://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe)

I have a large dataframe with 423244 lines. I want to split this in to 4. I tried the following code which gave an error? ValueError: array split does not result in an equal divisionHow to split this dataframe in to 4 groups?

2013-06-26 09:01:24Z

I have a large dataframe with 423244 lines. I want to split this in to 4. I tried the following code which gave an error? ValueError: array split does not result in an equal divisionHow to split this dataframe in to 4 groups?Use np.array_split:I wanted to do the same, and I had first problems with the split finction, then problems with installing pandas 0.15.2, so I went back to my old version, and wrote a little function that works very well. I hope this can help!Be aware that np.array_split(df, 3) splits the dataframe into 3 sub-dataframes, while splitDataFrameIntoSmaller(df, chunkSize = 3) splits the dataframe every chunkSize rows.Example:You get 3 sub-dataframes:With:You get 4 sub-dataframes:Hope I'm right, hope this is usefull.Caution:np.array_split doesn't work with numpy-1.9.0. I checked out: It works with 1.8.1. Error:I guess now we can use plain iloc with range for this.You can use groupby, assuming you have an integer enumerated index:Note: groupby returns a tuple in which the 2nd element is the dataframe, thus the slightly complicated extraction.

How should I verify a log message when testing Python code under nose?

jkp

[How should I verify a log message when testing Python code under nose?](https://stackoverflow.com/questions/899067/how-should-i-verify-a-log-message-when-testing-python-code-under-nose)

I'm trying to write a simple unit test that will verify that, under a certain condition, a class in my application will log an error via the standard logging API.  I can't work out what the cleanest way to test this situation is.I know that nose already captures logging output through it's logging plugin, but this seems to be intended as a reporting and debugging aid for failed tests.  The two ways to do this I can see are:If I go for the former approach, I'd like to know what the cleanest way to reset the global state to what it was before I mocked out the logging module.Looking forward to your hints and tips on this one...

2009-05-22 17:42:46Z

I'm trying to write a simple unit test that will verify that, under a certain condition, a class in my application will log an error via the standard logging API.  I can't work out what the cleanest way to test this situation is.I know that nose already captures logging output through it's logging plugin, but this seems to be intended as a reporting and debugging aid for failed tests.  The two ways to do this I can see are:If I go for the former approach, I'd like to know what the cleanest way to reset the global state to what it was before I mocked out the logging module.Looking forward to your hints and tips on this one...I used to mock loggers, but in this situation I found best to use logging handlers, so I wrote this one based on the document suggested by jkp(now dead, but cached on Internet Archive)From python 3.4 on, the standard unittest library offers a new test assertion context manager, assertLogs. From the docs:Fortunately this is not something that you have to write yourself; the testfixtures package provides a context manager that captures all logging output that occurs in the body of the with statement. You can find the package here:http://pypi.python.org/pypi/testfixturesAnd here are its docs about how to test logging:http://testfixtures.readthedocs.org/en/latest/logging.htmlUPDATE: No longer any need for the answer below. Use the built-in Python way instead!This answer extends the work done in https://stackoverflow.com/a/1049375/1286628. The handler is largely the same (the constructor is more idiomatic, using super). Further, I add a demonstration of how to use the handler with the standard library's unittest.Then you can use the handler in a standard-library unittest.TestCase like so:Brandon's answer:snippet:Note: the above does not conflict with calling nosetests and getting the output of logCapture plugin of the toolAs a follow up to Reef's answer, I took a liberty of coding up an example using pymox.

It introduces some extra helper functions that make it easier to stub functions and methods.You should use mocking, as someday You might want to change Your logger to a, say, database one. You won't be happy if it'll try to connect to the database during nosetests.Mocking will continue to work even if standard output will be suppressed.I have used pyMox's stubs. Remember to unset the stubs after the test.If you define a helper method like this:Then you can write test code like this:Found one answer since I posted this.  Not bad.The ExpectLog class implemented in tornado is a great utility:http://tornado.readthedocs.org/en/latest/_modules/tornado/testing.html#ExpectLogKeying off @Reef's answer, I did tried the code below. It works well for me both for Python 2.7 (if you install mock) and for Python 3.4.

Fuzzy String Comparison

jacksonstephenc

[Fuzzy String Comparison](https://stackoverflow.com/questions/10383044/fuzzy-string-comparison)

What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. I am unsure which operation to use to allow me to complete this in Python 3. I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines

// Should score high point but not 1Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines

// Should score lower than text 20Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.

// Should score lower than text 21 but NOT 0Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.

// Should score a 0!

2012-04-30 11:37:20Z

What I am striving to complete is a program which reads in a file and will compare each sentence according to the original sentence. The sentence which is a perfect match to the original will receive a score of 1 and a sentence which is the total opposite will receive a 0. All other fuzzy sentences will receive a grade in between 1 and 0. I am unsure which operation to use to allow me to complete this in Python 3. I have included the sample text in which the Text 1 is the original and the other preceding strings are the comparisons.  Text 1: It was a dark and stormy night. I was all alone sitting on a red chair. I was not completely alone as I had three cats.Text 20: It was a murky and stormy night. I was all alone sitting on a crimson chair. I was not completely alone as I had three felines

// Should score high point but not 1Text 21: It was a murky and tempestuous night. I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines

// Should score lower than text 20Text 22: I was all alone sitting on a crimson cathedra. I was not completely alone as I had three felines. It was a murky and tempestuous night.

// Should score lower than text 21 but NOT 0Text 24: It was a dark and stormy night. I was not alone. I was not sitting on a red chair. I had three cats.

// Should score a 0!There is a package called fuzzywuzzy. Install via pip:Simple usage:The package is built on top of difflib. Why not just use that, you ask? Apart from being a bit simpler, it has a number of different matching methods (like token order insensitivity, partial string matching) which make it more powerful in practice. The process.extract functions are especially useful: find the best matching strings and ratios from a set. From their readme:There is a module in the standard library (called difflib) that can compare strings and return a score based on their similarity. The SequenceMatcher class should do what you are after.EDIT: Small example from python prompt:HTH!fuzzyset is much faster than fuzzywuzzy (difflib) for both indexing and searching.Warning: Be careful not to mix unicode and bytes in your fuzzyset.The task is called Paraphrase Identification which is an active area of research in Natural Language Processing. I have linked several state of the art papers many of which you can find open source code on GitHub for.Note that all the answered question assume that there is some string/surface similarity between the two sentences while in reality two sentences with little string similarity can be semantically similar.If you're interested in that kind of similarity you can use Skip-Thoughts.

Install the software according to the GitHub guides and go to paraphrase detection section in readme:This converts your sentences (X_sentences) to vectors. Later you can find the similarity of two vectors by:where we are assuming vector[0] and vector1 are the corresponding vector to X_sentences[0], X_sentences1 which you wanted to find their scores.There are other models to convert a sentence to a vector which you can find here.Once you convert your sentences into vectors the similarity is just a matter of finding the Cosine similarity between those vectors.

How to make Django serve static files with Gunicorn?

alix

[How to make Django serve static files with Gunicorn?](https://stackoverflow.com/questions/12800862/how-to-make-django-serve-static-files-with-gunicorn)

I want to run my django project under gunicorn on localhost. I installed and integrated gunicorn. When I run:It works but there are no any static files (css and js)I disabled debug and template_debug in settings.py (made them false), but it is still same.  Am I missing something?I call statics like:

2012-10-09 13:13:16Z

I want to run my django project under gunicorn on localhost. I installed and integrated gunicorn. When I run:It works but there are no any static files (css and js)I disabled debug and template_debug in settings.py (made them false), but it is still same.  Am I missing something?I call statics like:When in development mode and when you are using some other server for local development add this to your url.pyMore info hereWhen in production you never, ever put gunicorn in front. Instead you use

a server like nginx which dispatches requests to a pool of gunicorn workers and also serves the static files.See hereWhitenoisePost v4.0http://whitenoise.evans.io/en/stable/changelog.html#v4-0Pre v4.0Heroku recommends this method at: https://devcenter.heroku.com/articles/django-assets:Install with:wsgi.py:Tested on Django 1.9.The gunicorn should be used to serve the python "application" itself, while the static files are served by a static file server ( such as Nginx ).There is a good guide here: http://honza.ca/2011/05/deploying-django-with-nginx-and-gunicornThis is an excerpt from one of my configurations:Some notes:In closing: while it is possible to serve static files from gunicorn ( by enabling a debug-only static file serving view ), that is considered bad practice in production. I've used this for my development environment (which uses gunicorn):And then run gunicorn myapp.wsgi. This works similar to @rantanplan's answer, however, it does not run any middleware when running static files.Since Django 1.3 there is django/conf/urls/static.py that handle static files in the DEBUG mode:Read more https://docs.djangoproject.com/en/2.0/howto/static-files/#serving-static-files-during-development

Concatenating two range function results

MAG

[Concatenating two range function results](https://stackoverflow.com/questions/14099872/concatenating-two-range-function-results)

Does range function allows concatenation ? Like i want to make a range(30) & concatenate it with range(2000, 5002).  So my concatenated range will be 0, 1, 2, ... 29, 2000, 2001,  ... 5001 Code like this does not work on my latest python (ver: 3.3.0)

2012-12-31 09:34:12Z

Does range function allows concatenation ? Like i want to make a range(30) & concatenate it with range(2000, 5002).  So my concatenated range will be 0, 1, 2, ... 29, 2000, 2001,  ... 5001 Code like this does not work on my latest python (ver: 3.3.0)You can use itertools.chain for this:It works for arbitrary iterables. Note that there's a difference in behavior of range() between Python 2 and 3 that you should know about: in Python 2 range returns a list, and in Python3 an iterator, which is memory-efficient, but not always desirable.Lists can be concatenated with +, iterators cannot.I like the most simple solutions that are possible (including efficiency). It is not always clear whether the solution is such. Anyway, the range() in Python 3 is a generator. You can wrap it to any construct that does iteration. The list() is capable of construction of a list value from any iterable. The + operator for lists does concatenation. I am using smaller values in the example:This is what range(5) + range(10, 20) exactly did in Python 2.5 -- because range() returned a list.In Python 3, it is only useful if you really want to construct the list. Otherwise, I recommend the Lev Levitsky's solution with itertools.chain. The documentation also shows the very straightforward implementation:The solution by Inbar Rose is fine and functionally equivalent. Anyway, my +1 goes to Lev Levitsky and to his argument about using the standard libraries. From The Zen of Python...In my opinion, the itertools.chain is more readable. But what really is important...... it is about 3 times faster.Can be done using list-comprehension.Works for your request, but it is a long answer so I will not post it here.note: can be made into a generator for increased performance:or even into a generator variable.With the help of the extend method, we can concatenate two lists.You can use iterable unpacking in lists (see PEP 448: Additional Unpacking Generalizations). If you need a list,This preserves order and does not remove duplicates. Or, you might want a tuple, ... or a set,It also happens to be faster than calling itertools.chain.The benefit of chain, however, is that you can pass an arbitrary list of ranges.OTOH, unpacking generalisations haven't been "generalised" to arbitrary sequences, so you will still need to unpack the individual ranges yourself.range() in Python 2.x returns a list:xrange() in Python 2.x returns an iterator:And in Python 3 range() also returns an iterator:So it is clear that you can not concatenate iterators other by using chain() as the other guy pointed out.I came to this question because I was trying to concatenate an unknown number of ranges, that might overlap, and didn't want repeated values in the final iterator. My solution was to use set and the union operator like so:You can use list function around range function to make a list

LIKE THIS

Suppress calls to print (python)

dmlicht

[Suppress calls to print (python)](https://stackoverflow.com/questions/8391411/suppress-calls-to-print-python)

Is there a way to stop a function from calling print?I am using the pygame.joystick module for a game I am working on.I created a pygame.joystick.Joystick object and in the actual loop of the game call its member function get_button to check for user input. The function does everything I need it to do, but the problem is that it also calls print, which slows down the game considerably. Can I block this call to print?

2011-12-05 20:26:54Z

Is there a way to stop a function from calling print?I am using the pygame.joystick module for a game I am working on.I created a pygame.joystick.Joystick object and in the actual loop of the game call its member function get_button to check for user input. The function does everything I need it to do, but the problem is that it also calls print, which slows down the game considerably. Can I block this call to print?Python lets you overwrite standard output (stdout) with any file object.  This should work cross platform and write to the null device.If you don't want that one function to print, call blockPrint() before it, and enablePrint() when you want it to continue.  If you want to disable all printing, start blocking at the top of the file.Based on @FakeRainBrigand solution I'm suggesting a safer solution:Then you can use it like this:This is much safer because you can not forget to re-enable stdout, which is especially critical when handling exceptions.If you forgot the finally clause, none of your print calls would print anything anymore. Using the with statement, that can not happen.It is not safe to use sys.stdout = None, because someone could call methods like sys.stdout.write()As @Alexander Chzhen suggested, using a context manager would be safer than calling a pair of state-changing functions.However, you don't need to reimplement the context manager - it's already in the standard library. You can redirect stdout (the file object that print uses) with contextlib.redirect_stdout, and also stderr with contextlib.redirect_stderr.No, there is not, especially that majority of PyGame is written in C.But if this function calls print, then it's PyGame bug, and you should just report it.I have had the same problem, and I did not come to another solution but to redirect the output of the program (I don't know exactly whether the spamming happens on stdout or stderr) to /dev/null nirvana. Indeed, it's open source, but I wasn't passionate enough to dive into the pygame sources - and the build process - to somehow stop the debug spam. EDIT : The pygame.joystick module has calls to printf in all functions that return the actual values to Python:Unfortunately you would need to comment these out and recompile the whole thing. Maybe the provided setup.py would make this easier than I thought. You could try this...If you want to block print calls made by a particular function, there is a neater solution using decorators. Define the following decorator:Then just place @blockPrinting before any function. For example:A completely different approach would be redirecting at the command line.  If you're on Windows, this means a batch script.  On Linux, bash.  Unless you're dealing with multiple processes, this should work.  For Windows users this could be the shortcuts you're creating (start menu / desktop).The module I used printed to stderr. So the solution in that case would be:Based on @Alexander Chzhen solution, I present here the way to apply it on a function with an option to suppress printing or not.I hope I can add my solution below answer of Alexander as a comment, but I don`t have enough (50) reputations to do so.

change figure size and figure format in matplotlib

golay

[change figure size and figure format in matplotlib](https://stackoverflow.com/questions/17109608/change-figure-size-and-figure-format-in-matplotlib)

I  want to obtain fig1 exactly of 4 by 3 inch sized, and in tiff format correcting the program below:Any help?

2013-06-14 13:27:28Z

I  want to obtain fig1 exactly of 4 by 3 inch sized, and in tiff format correcting the program below:Any help?You can set the figure size if you explicitly create the figure withTo change the format of the saved figure just change the extension in the file name. However I don't know if any of matplotlib backends support tiffYou can change the size of the plot by adding thisThe first part (setting the output size explictly) isn't too hard:But after a quick google search on matplotlib + tiff, I'm not convinced that matplotlib can make tiff plots.  There is some mention of the GDK backend being able to do it.One option would be to convert the output with a tool like imagemagick's convert.(Another option is to wait around here until a real matplotlib expert shows up and proves me wrong ;-)If you need to change the figure size after you have created it, use the methodswhere value_height and value_width are in inches. For me this is the most practical way.

Python equivalent of a given wget command

Soviero

[Python equivalent of a given wget command](https://stackoverflow.com/questions/24346872/python-equivalent-of-a-given-wget-command)

I'm trying to create a Python function that does the same thing as this wget command:-c - Continue from where you left off if the download is interrupted.--read-timeout=5 - If there is no new data coming in for over 5 seconds, give up and try again.  Given -c this mean it will try again from where it left off.--tries=0 - Retry forever.Those three arguments used in tandem results in a download that cannot fail.I want to duplicate those features in my Python script, but I don't know where to begin...

2014-06-21 23:46:48Z

I'm trying to create a Python function that does the same thing as this wget command:-c - Continue from where you left off if the download is interrupted.--read-timeout=5 - If there is no new data coming in for over 5 seconds, give up and try again.  Given -c this mean it will try again from where it left off.--tries=0 - Retry forever.Those three arguments used in tandem results in a download that cannot fail.I want to duplicate those features in my Python script, but I don't know where to begin...urllib.request should work. 

Just set it up in a while(not done) loop, check if a localfile already exists, if it does send a GET with a RANGE header, specifying how far you got in downloading the localfile. 

Be sure to use read() to append to the localfile until an error occurs.This is also potentially a duplicate of Python urllib2 resume download doesn't work when network reconnectsThere is also a nice Python module named wget that is pretty easy to use. Found here.This demonstrates the simplicity of the design:Enjoy.However, if wget doesn't work (I've had trouble with certain PDF files), try this solution.Edit: You can also use the out parameter to use a custom output directory instead of current working directory.I had to do something like this on a version of linux that didn't have the right options compiled into wget. This example is for downloading the memory analysis tool 'guppy'. I'm not sure if it's important or not, but I kept the target file's name the same as the url target name...Here's what I came up with:That's the one-liner, here's it a little more readable:This worked for downloading a tarball. I was able to extract the package and download it after downloading.EDIT:To address a question, here is an implementation with a progress bar printed to STDOUT. There is probably a more portable way to do this without the clint package, but this was tested on my machine and works fine:A solution that I often find simpler and more robust is to simply execute a terminal command within python. In your case:For Windows and Python 3.x, my two cents contribution about renaming the file on download :Truely working command line example : Note : 'C:\\PathToMyDownloadFolder\\NewFileName.extension' is not mandatory. By default, the file is not renamed, and the download folder is your local path.Let me Improve a example with threads in case you want download many files.easy as py:TensorFlow makes life easier. file path gives us the location of downloaded file.

How do I unescape HTML entities in a string in Python 3.1? [duplicate]

VolatileRig

[How do I unescape HTML entities in a string in Python 3.1? [duplicate]](https://stackoverflow.com/questions/2360598/how-do-i-unescape-html-entities-in-a-string-in-python-3-1)

I have looked all around and only found solutions for python 2.6 and earlier, NOTHING on how to do this in python 3.X. (I only have access to Win7 box.)I HAVE to be able to do this in 3.1 and preferably without external libraries. Currently, I have httplib2 installed and access to command-prompt curl (that's how I'm getting the source code for pages). Unfortunately, curl does not decode html entities, as far as I know, I couldn't find a command to decode it in the documentation.YES, I've tried to get Beautiful Soup to work, MANY TIMES without success in 3.X. If you could provide EXPLICIT instructions on how to get it to work in python 3 in MS Windows environment, I would be very grateful.So, to be clear, I need to turn strings like this: Suzy &amp; John into a string like this: "Suzy & John".

2010-03-02 02:50:19Z

I have looked all around and only found solutions for python 2.6 and earlier, NOTHING on how to do this in python 3.X. (I only have access to Win7 box.)I HAVE to be able to do this in 3.1 and preferably without external libraries. Currently, I have httplib2 installed and access to command-prompt curl (that's how I'm getting the source code for pages). Unfortunately, curl does not decode html entities, as far as I know, I couldn't find a command to decode it in the documentation.YES, I've tried to get Beautiful Soup to work, MANY TIMES without success in 3.X. If you could provide EXPLICIT instructions on how to get it to work in python 3 in MS Windows environment, I would be very grateful.So, to be clear, I need to turn strings like this: Suzy &amp; John into a string like this: "Suzy & John".You could use the function html.unescape:In Python3.4+ (thanks to J.F. Sebastian for the update):In Python3.3 or older:In Python2:You can use xml.sax.saxutils.unescape for this purpose. This module is included in the Python standard library, and is portable between Python 2.x and Python 3.x.Apparently I don't have a high enough reputation to do anything but post this. unutbu's answer does not unescape quotations. The only thing that I found that did was this function:Which I got from this page.Python 3.x has html.entities tooIn my case I have a html string escaped in as3 escape function. After a hour of googling haven't found anything useful so I wrote this recusrive function to serve for my needs. Here it is,Edit-1 Added functionality to handle unicode characters.I am not sure if this is a built in library or not but it looks like what you need and supports 3.1.From: http://docs.python.org/3.1/library/xml.sax.utils.html?highlight=html%20unescapexml.sax.saxutils.unescape(data, entities={})

Unescape '&', '<', and '>' in a string of data.

