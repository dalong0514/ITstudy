days, seconds, microseconds, milliseconds, minutes, hours, weeks. Also timedelta instance has total_seconds() method that:In the form closest to your original:If you need to know the number of seconds from 1970-01-01 rather than a native Python datetime, use this instead:Python has naming conventions that are at odds with what you might be used to in Javascript, see PEP 8. Also, a function that simply returns the result of another function is rather silly; if it's just a matter of making it more accessible, you can create another name for a function by simply assigning it. The first example above could be replaced with:Simple, standard library only, for modern python. Gives timezone-aware datetime, unlike datetime.utcnow(). datetimes without timezones are accidents waiting to happen.From datetime.datetime you already can export to timestamps with method strftime. Following your function example:If you want microseconds, you need to change the export string and cast to float like: return float(now.strftime("%s.%f"))    In Python 3 using lambda expression, try out;and call the function normally;

How to capture stdout output from a Python function call?

Nico Schlömer

[How to capture stdout output from a Python function call?](https://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call)

I'm using a Python library that does something to an objectand changes it. While doing so, it prints some statistics to stdout, and I'd like to get a grip on this information. The proper solution would be to change do_something() to return the relevant information,but it will be a while before the devs of do_something() get to this issue. As a workaround, I thought about parsing whatever do_something() writes to stdout.How can I capture stdout output between two points in the code, e.g.,?

2013-05-15 17:13:30Z

I'm using a Python library that does something to an objectand changes it. While doing so, it prints some statistics to stdout, and I'd like to get a grip on this information. The proper solution would be to change do_something() to return the relevant information,but it will be a while before the devs of do_something() get to this issue. As a workaround, I thought about parsing whatever do_something() writes to stdout.How can I capture stdout output between two points in the code, e.g.,?Try this context manager:Usage:output is now a list containing the lines printed by the function call.Advanced usage:What may not be obvious is that this can be done more than once and the results concatenated:Output:Update: They added redirect_stdout() to contextlib in Python 3.4 (along with redirect_stderr()). So you could use io.StringIO with that to achieve a similar result (though Capturing being a list as well as a context manager is arguably more convenient).In python >= 3.4, contextlib contains a redirect_stdout decorator. It can be used to answer your question like so:From the docs:

Python str vs unicode types

Caumons

[Python str vs unicode types](https://stackoverflow.com/questions/18034272/python-str-vs-unicode-types)

Working with Python 2.7, I'm wondering what real advantage there is in using the type unicode instead of str, as both of them seem to be able to hold Unicode strings. Is there any special reason apart from being able to set Unicode codes in unicode strings using the escape char \?:Executing a module with:Results in: á, áEDIT:More testing using Python shell:So, the unicode string seems to be encoded using latin1 instead of utf-8 and the raw string is encoded using utf-8? I'm even more confused now! :S

2013-08-03 15:15:25Z

Working with Python 2.7, I'm wondering what real advantage there is in using the type unicode instead of str, as both of them seem to be able to hold Unicode strings. Is there any special reason apart from being able to set Unicode codes in unicode strings using the escape char \?:Executing a module with:Results in: á, áEDIT:More testing using Python shell:So, the unicode string seems to be encoded using latin1 instead of utf-8 and the raw string is encoded using utf-8? I'm even more confused now! :Sunicode is meant to handle text. Text is a sequence of code points which may be bigger than a single byte. Text can be encoded in a specific encoding to represent the text as raw bytes(e.g. utf-8, latin-1...).Note that unicode is not encoded! The internal representation used by python is an implementation detail, and you shouldn't care about it as long as it is able to represent the code points you want.On the contrary str in Python 2 is a plain sequence of bytes. It does not represent text!You can think of unicode as a general representation of some text, which can be encoded in many different ways into a sequence of binary data represented via str.Note: In Python 3, unicode was renamed to str and there is a new bytes type for a plain sequence of bytes.Some differences that you can see:Note that using str you have a lower-level control on the single bytes of a specific encoding representation, while using unicode you can only control at the code-point level. For example you can do:What before was valid UTF-8, isn't anymore. Using a unicode string you cannot operate in such a way that the resulting string isn't valid unicode text.

You can remove a code point, replace a code point with a different code point etc. but you cannot mess with the internal representation.Unicode and encodings are completely different, unrelated things.Assigns a numeric ID to each character:So, Unicode assigns the number 0x41 to A, 0xE1 to á, and 0x414 to Д.Even the little arrow → I used has its Unicode number, it's 0x2192. And even emojis have their Unicode numbers, 😂 is 0x1F602.You can look up the Unicode numbers of all characters in this table. In particular, you can find the first three characters above here, the arrow here, and the emoji here.These numbers assigned to all characters by Unicode are called code points.The purpose of all this is to provide a means to unambiguously refer to a each character. For example, if I'm talking about 😂, instead of saying "you know, this laughing emoji with tears", I can just say, Unicode code point 0x1F602. Easier, right?Note that Unicode code points are usually formatted with a leading U+, then the hexadecimal numeric value padded to at least 4 digits. So, the above examples would be U+0041, U+00E1, U+0414, U+2192, U+1F602.Unicode code points range from U+0000 to U+10FFFF. That is 1,114,112 numbers. 2048 of these numbers are used for surrogates, thus, there remain 1,112,064. This means, Unicode can assign a unique ID (code point) to 1,112,064 distinct characters. Not all of these code points are assigned to a character yet, and Unicode is extended continuously (for example, when new emojis are introduced).The important thing to remember is that all Unicode does is to assign a numerical ID, called code point, to each character for easy and unambiguous reference.Map characters to bit patterns.These bit patterns are used to represent the characters in computer memory or on disk.There are many different encodings that cover different subsets of characters. In the English-speaking world, the most common encodings are the following:Maps 128 characters (code points U+0000 to U+007F) to bit patterns of length 7.Example:You can see all the mappings in this table.Maps 191 characters (code points U+0020 to U+007E and U+00A0 to U+00FF) to bit patterns of length 8.Example:You can see all the mappings in this table.Maps 1,112,064 characters (all existing Unicode code points) to bit patterns of either length 8, 16, 24, or 32 bits (that is, 1, 2, 3, or 4 bytes).Example:The way UTF-8 encodes characters to bit strings is very well described here.Looking at the above examples, it becomes clear how Unicode is useful.For example, if I'm Latin-1 and I want to explain my encoding of á, I don't need to say:But I can just say:And if I'm UTF-8, I can say:And it's unambiguously clear to everybody which character we mean.It's true that sometimes the bit pattern of an encoding, if you interpret it as a binary number, is the same as the Unicode code point of this character.For example:Of course, this has been arranged like this on purpose for convenience. But you should look at it as a pure coincidence. The bit pattern used to represent a character in memory is not tied in any way to the Unicode code point of this character.Nobody even says that you have to interpret a bit string like 11100001 as a binary number. Just look at it as the sequence of bits that Latin-1 uses to encode the character á.The encoding used by your Python interpreter is UTF-8.Here's what's going on in your examples:The following encodes the character á in UTF-8. This results in the bit string 11000011 10100001, which is saved in the variable a.When you look at the value of a, its content 11000011 10100001 is formatted as the hex number 0xC3 0xA1 and output as '\xc3\xa1':The following saves the Unicode code point of á, which is U+00E1, in the variable ua (we don't know which data format Python uses internally to represent the code point U+00E1 in memory, and it's unimportant to us):When you look at the value of ua, Python tells you that it contains the code point U+00E1:The following encodes Unicode code point U+00E1 (representing character á) with UTF-8, which results in the bit pattern 11000011 10100001. Again, for output this bit pattern is represented as the hex number 0xC3 0xA1:The following encodes Unicode code point U+00E1 (representing character á) with Latin-1, which results in the bit pattern 11100001. For output, this bit pattern is represented as the hex number 0xE1, which by coincidence is the same as the initial code point U+00E1:There's no relation between the Unicode object ua and the Latin-1 encoding. That the code point of á is U+00E1 and the Latin-1 encoding of á is 0xE1 (if you interpret the bit pattern of the encoding as a binary number) is a pure coincidence. Your terminal happens to be configured to UTF-8.The fact that printing a works is a coincidence; you are writing raw UTF-8 bytes to the terminal. a is a value of length two, containing two bytes, hex values C3 and A1, while ua is a unicode value of length one, containing a codepoint U+00E1.This difference in length is one major reason to use Unicode values; you cannot easily measure the number of text characters in a byte string; the len() of a byte string tells you how many bytes were used, not how many characters were encoded.You can see the difference when you encode the unicode value to different output encodings:Note that the first 256 codepoints of the Unicode standard match the Latin 1 standard, so the U+00E1 codepoint is encoded to Latin 1 as a byte with hex value E1.Furthermore, Python uses escape codes in representations of unicode and byte strings alike, and low code points that are not printable ASCII are represented using \x.. escape values as well. This is why a Unicode string with a code point between 128 and 255 looks just like the Latin 1 encoding. If you have a unicode string with codepoints beyond U+00FF a different escape sequence, \u.... is used instead, with a four-digit hex value.It looks like you don't yet fully understand what the difference is between Unicode and an encoding. Please do read the following articles before you continue:When you define a as unicode, the chars a and á are equal. Otherwise á counts as two chars. Try len(a) and len(au). In addition to that, you may need to have the encoding when you work with other environments. For example if you use md5, you get different values for a and ua

Visibility of global variables in imported modules

Nubarke

[Visibility of global variables in imported modules](https://stackoverflow.com/questions/15959534/visibility-of-global-variables-in-imported-modules)

I've run into a bit of a wall importing modules in a Python script. I'll do my best to describe the error, why I run into it, and why I'm tying this particular approach to solve my problem (which I will describe in a second):Let's suppose I have a module in which I've defined some utility functions/classes, which refer to entities defined in the namespace into which this auxiliary module will be imported (let "a" be such an entity):module1:And then I have the main program, where "a" is defined, into which I want to import those utilities:Executing the program will trigger the following error:Similar questions have been asked in the past (two days ago, d'uh) and several solutions have been suggested, however I don't really think these fit my requirements. Here's my particular context:I'm trying to make a Python program which connects to a MySQL database server and displays/modifies data with a GUI. For cleanliness sake, I've defined the bunch of auxiliary/utility MySQL-related functions in a separate file. However they all have a common variable, which I had originally defined inside the utilities module, and which is the cursor object from MySQLdb module.

I later realised that the cursor object (which is used to communicate with the db server) should be defined in the main module, so that both the main module and anything that is imported into it can access that object.End result would be something like this:utilities_module.py:And my main module:program.py:And then, as soon as I try to call any of the utilities functions, it triggers the aforementioned "global name not defined" error.A particular suggestion was to have a "from program import cur" statement in the utilities file, such as this:utilities_module.py:program.py:But that's cyclic import or something like that and, bottom line, it crashes too. So my question is:How in hell can I make the "cur" object, defined in the main module, visible to those auxiliary functions which are imported into it?Thanks for your time and my deepest apologies if the solution has been posted elsewhere. I just can't find the answer myself and I've got no more tricks in my book.

2013-04-11 21:53:11Z

I've run into a bit of a wall importing modules in a Python script. I'll do my best to describe the error, why I run into it, and why I'm tying this particular approach to solve my problem (which I will describe in a second):Let's suppose I have a module in which I've defined some utility functions/classes, which refer to entities defined in the namespace into which this auxiliary module will be imported (let "a" be such an entity):module1:And then I have the main program, where "a" is defined, into which I want to import those utilities:Executing the program will trigger the following error:Similar questions have been asked in the past (two days ago, d'uh) and several solutions have been suggested, however I don't really think these fit my requirements. Here's my particular context:I'm trying to make a Python program which connects to a MySQL database server and displays/modifies data with a GUI. For cleanliness sake, I've defined the bunch of auxiliary/utility MySQL-related functions in a separate file. However they all have a common variable, which I had originally defined inside the utilities module, and which is the cursor object from MySQLdb module.

I later realised that the cursor object (which is used to communicate with the db server) should be defined in the main module, so that both the main module and anything that is imported into it can access that object.End result would be something like this:utilities_module.py:And my main module:program.py:And then, as soon as I try to call any of the utilities functions, it triggers the aforementioned "global name not defined" error.A particular suggestion was to have a "from program import cur" statement in the utilities file, such as this:utilities_module.py:program.py:But that's cyclic import or something like that and, bottom line, it crashes too. So my question is:How in hell can I make the "cur" object, defined in the main module, visible to those auxiliary functions which are imported into it?Thanks for your time and my deepest apologies if the solution has been posted elsewhere. I just can't find the answer myself and I've got no more tricks in my book.Globals in Python are global to a module, not across all modules. (Many people are confused by this, because in, say, C, a global is the same across all implementation files unless you explicitly make it static.)There are different ways to solve this, depending on your actual use case.Before even going down this path, ask yourself whether this really needs to be global. Maybe you really want a class, with f as an instance method, rather than just a free function? Then you could do something like this:If you really do want a global, but it's just there to be used by module1, set it in that module.On the other hand, if a is shared by a whole lot of modules, put it somewhere else, and have everyone import it:… and, in module1.py:Don't use a from import unless the variable is intended to be a constant. from shared_stuff import a would create a new a variable initialized to whatever shared_stuff.a referred to at the time of the import, and this new a variable would not be affected by assignments to shared_stuff.a.Or, in the rare case that you really do need it to be truly global everywhere, like a builtin, add it to the builtin module. The exact details differ between Python 2.x and 3.x. In 3.x, it works like this:As a workaround, you could consider setting environment variables in the outer layer, like this.  main.py:mymodule.py:As an extra precaution, handle the case when MYVAL is not defined inside the module. A function uses the globals of the module it's defined in. Instead of setting a = 3, for example, you should be setting module1.a = 3. So, if you want cur available as a global in utilities_module, set utilities_module.cur.A better solution: don't use globals. Pass the variables you need into the functions that need it, or create a class to bundle all the data together, and pass it when initializing the instance.This post is just an observation for Python behaviour I encountered. Maybe the advices you read above don't work for you if you made the same thing I did below.Namely, I have a module which contains global/shared variables (as suggested above):Then I had the main module which imports the shared stuff with:and some other modules that actually populated these arrays. These are called by the main module. When exiting these other modules I can clearly see that the arrays are populated. But when reading them back in the main module, they were empty. This was rather strange for me (well, I am new to Python). However, when I change the way I import the sharedstuff.py in the main module to:it worked (the arrays were populated).Just sayin'The easiest solution to this particular problem would have been to add another function within the module that would have stored the cursor in a variable global to the module. Then all the other functions could use it as well.module1:main program:Since globals are module specific, you can add the following function to all imported modules, and then use it to:Then all you need to pass on current globals is: Since I haven't seen it in the answers above, I thought I would add my simple workaround, which is just to add a global_dict argument to the function requiring the calling module's globals, and then pass the dict into the function when calling; e.g:The OOP way of doing this would be to make your module a class instead of a set of unbound methods. Then you could use __init__ or a setter method to set the variables from the caller for use in the module methods.

Iterating through a JSON object

myusuf3

[Iterating through a JSON object](https://stackoverflow.com/questions/2733813/iterating-through-a-json-object)

I am trying to iterate through a JSON object to import data, i.e. title and link. I can't seem to get to the content that is past the :. JSON: I tried using a dictionary:This code only prints the information before :.

(ignore the Justin Bieber track :))

2010-04-28 23:30:21Z

I am trying to iterate through a JSON object to import data, i.e. title and link. I can't seem to get to the content that is past the :. JSON: I tried using a dictionary:This code only prints the information before :.

(ignore the Justin Bieber track :))Your loading of the JSON data is a little fragile. Instead of:you should really just do:You shouldn't think of what you get as a "JSON object". What you have is a list. The list contains two dicts. The dicts contain various key/value pairs, all strings. When you do json_object[0], you're asking for the first dict in the list. When you iterate over that, with for song in json_object[0]:, you iterate over the keys of the dict. Because that's what you get when you iterate over the dict. If you want to access the value associated with the key in that dict, you would use, for example, json_object[0][song].None of this is specific to JSON. It's just basic Python types, with their basic operations as covered in any tutorial.I believe you probably meant:NB: use song.items instead of song.iteritems for Python 3.This question has been out here a long time, but I wanted to contribute how I usually iterate through a JSON object.  In the example below, I've shown a hard-coded string that contains the JSON, but the JSON string could just as easily have come from a web service or a file.After deserializing the JSON, you have a python object. Use the regular object methods.In this case you have a list made of dictionaries:etc.I would solve this problem more like thisFor Python 3, you have to decode the data you get back from the web server. For instance I decode the data as utf8 then deal with it:If you don't decode you will get bytes vs string errors in Python 3.for iterating through JSON you can use this:

AttributeError: 'module' object has no attribute 'tests'

Chris

[AttributeError: 'module' object has no attribute 'tests'](https://stackoverflow.com/questions/25575073/attributeerror-module-object-has-no-attribute-tests)

I'm running this command:and it causes this error:Below is my directory structure. I've also added app1 to my installed apps config.Directory structure:

2014-08-29 19:34:59Z

I'm running this command:and it causes this error:Below is my directory structure. I've also added app1 to my installed apps config.Directory structure:I finally figured it out working on another problem. The problem was that my test couldn't find an import. It looks like you get the above error if your test fails to import. This makes sense because the test suite can't import a broken test. At least I think this is what is going on because I fixed the import within my test file and sure enough it started working.To validate your test case just try import the test case file in python console. Example:Use:./manage.py shellfollowed by import myapp.teststo find the nature of the import error.For my case, I need to create an empty __init__.py in my app/tests folderSteve Bradshaw's example above works for import errors (thanks Steve).Other type of errors (e.g. ValueError) may also cause to see what these errors areI had the same error as Chris.  I had deleted an old model, then run tests.py, but another file (views.py) was still trying to import the deleted model.When I took out the now-obsolete import statement, problem solved.Make sure that all modules that you are using in your script are not broken. By this I mean check spelling in your import statements. You can test yours modules by executing imports statements in djano's interactive console.According to django document When you run your tests, the default behavior of the test utility is to find all the test cases (that is, subclasses of unittest.TestCase) in any file whose name begins with test, automatically build a test suite out of those test cases, and run that suite.so try this : python manage.py test tests.pyGot the same error, but checked all the reasons list here, did not fix my problem. Finally figure it out that, the reason is that the name of one method that imported but not used yet is not correct. Though it is a stupid error, it happens.I resolved the error "AttributeError: module 'utils' has no attribute 'name_of_my_function' " by fixing a circular import reference. My files manage.py and utils.py each had an import statement pointing at each other.I had the same error. It turned out to be because I named my module common.py, yet there already was some other common.py module. All I had to do was to rename my module.I had a similar error while writing a unittest.TestCase.

When I re-typed the same method definition as-is, it seemed to work !The only change I noticed on PyCharm  was the 'override' icon pop-up the 2nd time, as the setup(self) method needs to override the original method defined in TestCase. 

Non-alphanumeric list order from os.listdir()

marshall.ward

[Non-alphanumeric list order from os.listdir()](https://stackoverflow.com/questions/4813061/non-alphanumeric-list-order-from-os-listdir)

I often use python to process directories of data. Recently, I have noticed that the default order of the lists has changed to something almost nonsensical. For example, if I am in a current directory containing the following subdirectories: run01, run02, ... run19, run20, and then I generate a list from the following command:then I usually get a list in this order:and so on. The order used to be alphanumeric. But this new order has remained with me for a while now.What is determining the (displayed) order of these lists?

2011-01-27 05:30:29Z

I often use python to process directories of data. Recently, I have noticed that the default order of the lists has changed to something almost nonsensical. For example, if I am in a current directory containing the following subdirectories: run01, run02, ... run19, run20, and then I generate a list from the following command:then I usually get a list in this order:and so on. The order used to be alphanumeric. But this new order has remained with me for a while now.What is determining the (displayed) order of these lists?I think the order has to do with the way the files are indexed on your FileSystem.

If you really want to make it adhere to some order you can always sort the list after getting the files.You can use the builtin sorted function to sort the strings however you want.  Based on what you describe, Alternatively, you can use the .sort method of a list:I think should do the trick.Note that the order that os.listdir gets the filenames is probably completely dependent on your filesystem.Per the documentation:Order cannot be relied upon and is an artifact of the filesystem.To sort the result, use sorted(os.listdir(path)).Python for whatever reason does not come with a built-in way to have natural sorting (meaning 1, 2, 10 instead of 1, 10, 2), so you have to write it yourself:You can now use this function to sort a list:PROBLEMS:

In case you use the above function to sort strings (for example folder names) and want them sorted like Windows Explorer does, it will not work properly in some edge cases.

This sorting function will return incorrect results on Windows, if you have folder names with certain 'special' characters in them. For example this function will sort 1, !1, !a, a, whereas Windows Explorer would sort !1, 1, !a, a.  So if you want to sort exactly like Windows Explorer does in Python you have to use the Windows built-in function StrCmpLogicalW via ctypes (this of course won't work on Unix):This function is slightly slower than sorted_alphanumeric().Bonus: winsort can also sort full paths on Windows.Alternatively, especially if you use Unix, you can use the natsort library (pip install natsort) to sort by full paths in a correct way (meaning subfolders at the correct position).You can use it like this to sort full paths:Don't use it for normal sorting of just folder names (or strings in general), as it's quite a bit slower than then sorted_alphanumeric() function above.

natsorted library will give you incorrect results if you expect Windows Explorer sorting, so use winsort() for that.It's probably just the order that C's readdir() returns.  Try running this C program:The build line should be something like gcc -o foo foo.c.P.S. Just ran this and your Python code, and they both gave me sorted output, so I can't reproduce what you're seeing.I think by default the order is determined with the ASCII value. The solution to this problem is thisI found "sort" does not always do what I expected. eg, I have a directory as below, and the "sort" give me a very strange result:It seems it compares the first character first, if that is the biggest, it would be the last one.As In case of mine requirement I have the case like row_163.pkl here os.path.splitext('row_163.pkl') will break it into ('row_163', '.pkl') so need to split it based on '_' also.but in case of your requirement you can do something likewhere and also for directory retrieving you can do sorted(os.listdir(path))and for the case of like 'run01.txt' or 'run01.csv' you can do like this From the documentation:This means that the order is probably OS/filesystem dependent, has no particularly meaningful order, and is therefore not guaranteed to be anything in particular. As many answers mentioned: if preferred, the retrieved list can be sorted.Cheers :)Elliot's answer solves it perfectly but because it is a comment, it goes unnoticed so with the aim of helping someone, I am reiterating it as a solution.Use natsort library: Install the library with the following command for Ubuntu and other Debian versions Python 2Python 3Details of how to use this library is found hereThe proposed combination of os.listdir and sorted commands generates the same result as ls -l command under Linux. The following example verifies this assumption:So, for someone who wants to reproduce the result of the well-known ls -l command in his Python code, sorted( os.listdir( DIR ) ) works pretty well.

'module' has no attribute 'urlencode'

Croll

['module' has no attribute 'urlencode'](https://stackoverflow.com/questions/28906859/module-has-no-attribute-urlencode)

When I try to follow the Python Wiki's example related to URL encoding:An error is raised on the second line:What am I missing?

2015-03-06 20:13:04Z

When I try to follow the Python Wiki's example related to URL encoding:An error is raised on the second line:What am I missing?urllib has been split up in Python 3.  The urllib.urlencode() function is now urllib.parse.urlencode(), the urllib.urlopen() function is now urllib.request.urlopen().You use the Python 2 docs but write your program in Python 3.

SQLAlchemy ORM conversion to pandas DataFrame

Jared

[SQLAlchemy ORM conversion to pandas DataFrame](https://stackoverflow.com/questions/29525808/sqlalchemy-orm-conversion-to-pandas-dataframe)

This topic hasn't been addressed in a while, here or elsewhere. Is there a solution converting a SQLAlchemy <Query object> to a pandas DataFrame?Pandas has the capability to use pandas.read_sql but this requires use of raw SQL. I have two reasons for wanting to avoid it: 1) I already have everything using the ORM (a good reason in and of itself) and 2) I'm using python lists as part of the query (eg: .db.session.query(Item).filter(Item.symbol.in_(add_symbols) where Item is my model class and add_symbols is a list). This is the equivalent of SQL SELECT ... from ... WHERE ... IN. Is anything possible?

2015-04-08 21:36:34Z

This topic hasn't been addressed in a while, here or elsewhere. Is there a solution converting a SQLAlchemy <Query object> to a pandas DataFrame?Pandas has the capability to use pandas.read_sql but this requires use of raw SQL. I have two reasons for wanting to avoid it: 1) I already have everything using the ORM (a good reason in and of itself) and 2) I'm using python lists as part of the query (eg: .db.session.query(Item).filter(Item.symbol.in_(add_symbols) where Item is my model class and add_symbols is a list). This is the equivalent of SQL SELECT ... from ... WHERE ... IN. Is anything possible?Below should work in most cases:See pandas.read_sql documentation for more information on the parameters.Just to make this more clear for novice pandas programmers, here is a concrete example,Here we select a complaint from complaints table (sqlalchemy model is Complaint) with id = 2The selected solution didn't work for me, as I kept getting the error I found the following worked:If you want to compile a query with parameters and dialect specific arguments, use something like this:

Batch Renaming of Files in a Directory

Nate

[Batch Renaming of Files in a Directory](https://stackoverflow.com/questions/225735/batch-renaming-of-files-in-a-directory)

Is there an easy way to rename a group of files already contained in a directory, using Python?Example:   I have a directory full of *.doc files and I want to rename them in a consistent way.

2008-10-22 13:33:23Z

Is there an easy way to rename a group of files already contained in a directory, using Python?Example:   I have a directory full of *.doc files and I want to rename them in a consistent way.Such renaming is quite easy, for example with os and glob modules:You could then use it in your example like this:The above example will convert all *.doc files in c:\temp\xx dir to new(%s).doc, where %s is the previous base name of the file (without extension).I prefer writing small one liners for each replace I have to do instead of making a more generic and complex code. E.g.:This replaces all underscores with hyphens in any non-hidden file in the current directoryIf you don't mind using regular expressions, then this function would give you much power in renaming files:So in your example, you could do (assuming it's the current directory where the files are):but you could also roll back to the initial filenames:and more.I have this to simply rename all files in subfolders of folderI am replacing all occurences of old_str with any case by new_str.Try: http://www.mattweber.org/2007/03/04/python-script-renamepy/The program's source code is also available.I've written a python script on my own. It takes as arguments the path of the directory in which the files are present and the naming pattern that you want to use. However, it renames by attaching an incremental number (1, 2, 3 and so on) to the naming pattern you give.Hope this works for you.I had a similar problem, but I wanted to append text to the beginning of the file name of all files in a directory and used a similar method. See example below:as to me in my directory I have multiple subdir, each subdir has lots of images I want to change all the subdir images to 1.jpg ~ n.jpg(mys own answer)https://stackoverflow.com/a/45734381/6329006If you would like to modify file names in an editor (such as vim), the click library comes with the command click.edit(), which can be used to receive user input from an editor. Here is an example of how it can be used to refactor files in a directory.I wrote a command line application that uses the same technique, but that reduces the volatility of this script, and comes with more options, such as recursive refactoring. Here is the link to the github page. This is useful if you like command line applications, and are interested in making some quick edits to file names. (My application is similar to the "bulkrename" command found in ranger).

Assignment inside lambda expression in Python

Cat

[Assignment inside lambda expression in Python](https://stackoverflow.com/questions/6282042/assignment-inside-lambda-expression-in-python)

I have a list of objects and I want to remove all objects that are empty except for one, using filter and a lambda expression.For example if the input is:...then the output should be:Is there a way to add an assignment to a lambda expression? For example:

2011-06-08 16:23:36Z

I have a list of objects and I want to remove all objects that are empty except for one, using filter and a lambda expression.For example if the input is:...then the output should be:Is there a way to add an assignment to a lambda expression? For example:The assignment expression operator := added in Python 3.8 supports assignment inside of lambda expressions. This operator can only appear within a parenthesized (...), bracketed [...], or braced {...} expression for syntactic reasons. For example, we will be able to write the following:In Python 2, it was possible to perform local assignments as a side effect of list comprehensions.However, it's not possible to use either of these in your example because your variable flag is in an outer scope, not the lambda's scope. This doesn't have to do with lambda, it's the general behaviour in Python 2. Python 3 lets you get around this with the nonlocal keyword inside of defs, but nonlocal can't be used inside lambdas.There's a workaround (see below), but while we're on the topic...In some cases you can use this to do everything inside of a lambda:Please don't....back to your original example: though you can't perform assignments to the flag variable in the outer scope, you can use functions to modify the previously-assigned value.For example, flag could be an object whose .value we set  using setattr:If we wanted to fit the above theme, we could use a list comprehension instead of setattr:But really, in serious code you should always use a regular function definition instead of a lambda if you're going to be doing outer assignment.You cannot really maintain state in a filter/lambda expression (unless abusing the global namespace). You can however achieve something similar using the accumulated result being passed around in a reduce() expression:You can, of course, tweak the condition a bit. In this case it filters out duplicates, but you can also use a.count(""), for example, to only restrict empty strings.Needless to say, you can do this but you really shouldn't. :)Lastly, you can do anything in pure Python lambda: http://vanderwijk.info/blog/pure-lambda-calculus-python/There's no need to use a lambda, when you can remove all the null ones, and put one back if the input size changes:Normal assignment (=) is not possible inside a lambda expression, although it is possible to perform various tricks with setattr and friends.  Solving your problem, however, is actually quite simple:which will give youAs you can see, it's keeping the first blank instance instead of the last.  If you need the last instead, reverse the list going in to filter, and reverse the list coming out of filter:which will give youOne thing to be aware of: in order for this to work with arbitrary objects, those objects must properly implement __eq__ and __hash__ as explained here.UPDATE:or using filter and lambda:Previous AnswerOK, are you stuck on using filter and lambda?It seems like this would be better served with a dictionary comprehension,I think the reason that Python doesn't allow assignment in a lambda is similar to why it doesn't allow assignment in a comprehension and that's got something to do with the fact that these things are evaluated on the C side and thus can give us an increase in speed. At least that's my impression after reading one of Guido's essays.My guess is this would also go against the philosophy of having one right way of doing any one thing in Python.If instead of flag = True we can do an import instead, then I think this meets the criteria:Or maybe the filter is better written as:Or, just for a simple boolean, without any imports:TL;DR: When using functional idioms it's better to write functional codeAs many people have pointed out, in Python lambdas assignment is not allowed.  In general when using functional idioms your better off thinking in a functional manner which means wherever possible no side effects and no assignments.Here is functional solution which uses a lambda.  I've assigned the lambda to fn for clarity (and because it got a little long-ish).You can also make this deal with iterators rather than lists by changing things around a little.  You also have some different imports.You can always reoganize the code to reduce the length of the statements.  The pythonic way to track state during iteration is with generators. The itertools way is quite hard to understand IMHO and trying to hack lambdas to do this is plain silly. I'd try:Overall, readability trumps compactness every time.No, you cannot put an assignment inside a lambda because of its own definition. If you work using functional programming, then you must assume that your values are not mutable.One solution would be the following code:If you need a lambda to remember state between calls, I would recommend either a function declared in the local namespace or a class with an overloaded __call__. Now that all my cautions against what you are trying to do is out of the way, we can get to an actual answer to your query.If you really need to have your lambda to have some memory between calls, you can define it like:Then you just need to pass f to filter(). If you really need to, you can get back the value of flag with the following:Alternatively, you can modify the global namespace by modifying the result of globals(). Unfortunately, you cannot modify the local namespace in the same way as modifying the result of locals() doesn't affect the local namespace.You can use a bind function to use a pseudo multi-statement lambda.  Then you can use a wrapper class for a Flag to enable assignment.first , you dont need to use a local assigment for your job, just check the above answersecond, its simple to use locals() and globals() to got the variables table and then change the valuecheck this sample code:if you need to change the add a global variable to your environ, try to replace locals() with globals()python's list comp is cool but most of the triditional project dont accept this(like flask :[)hope it could help

How to install MySQLdb (Python data access library to MySQL) on Mac OS X?

mblackwell8

[How to install MySQLdb (Python data access library to MySQL) on Mac OS X?](https://stackoverflow.com/questions/1448429/how-to-install-mysqldb-python-data-access-library-to-mysql-on-mac-os-x)

I'm a Python newbie, but I've just spent a day working out how to get MySQLdb working properly, and the universe according to google includes numerous references to what a PITA it is, and an inordinate number of guides that seem to be outdated.  Given that this site is intended to address these sorts of problems, and I know that I'm going to need a reference to the solution in future, I'm going to ask the question, provide my answer and see what else floats to the surface.So the question is how to get MySQLdb working on Mac OS X?

2009-09-19 12:04:41Z

I'm a Python newbie, but I've just spent a day working out how to get MySQLdb working properly, and the universe according to google includes numerous references to what a PITA it is, and an inordinate number of guides that seem to be outdated.  Given that this site is intended to address these sorts of problems, and I know that I'm going to need a reference to the solution in future, I'm going to ask the question, provide my answer and see what else floats to the surface.So the question is how to get MySQLdb working on Mac OS X?Update for those using Python3:

You can simply use conda install mysqlclient to install the libraries required to use MySQLdb as it currently exists. The following SO question was a helpful clue: Python 3 ImportError: No module named 'ConfigParser' . Installing mysqlclient will install mysqlclient, mysql-connector, and llvmdev (at least, it installed these 3 libraries on my machine).Here is the tale of my rambling experience with this problem.  Would love to see it edited or generalised if you have better experience of the issue... apply a bit of that SO magic.Note: Comments in next paragraph applied to Snow Leopard, but not to Lion, which appears to require 64-bit MySQLFirst off, the author (still?) of MySQLdb says here that one of the most pernicious problems is that OS X comes installed with a 32 bit version of Python, but most average joes (myself included) probably jump to install the 64 bit version of MySQL.  Bad move... remove the 64 bit version if you have installed it (instructions on this fiddly task are available on SO here), then download and install the 32 bit version (package here)There are numerous step-by-steps on how to build and install the MySQLdb libraries.  They often have subtle differences.  This seemed the most popular to me, and provided the working solution.  I've reproduced it with a couple of edits belowStep 0:

Before I start, I assume that you have MySQL, Python, and GCC installed on the mac.Step 1:

Download the latest MySQL for Python adapter from SourceForge.Step 2:

Extract your downloaded package:Step 3:

Inside the folder, clean the package:COUPLE OF EXTRA STEPS, (from this comment)Step 3b: 

Remove everything under your MySQL-python-1.2.2/build/* directory -- don't trust the "python setup.py clean" to do it for youStep 3c: 

Remove the egg under Users/$USER/.python-eggsStep 4: 

Originally required editing _mysql.c, but is now NO LONGER NECESSARY.  MySQLdb community seem to have fixed this bug now.Step 5:

Create a symbolic link under lib to point to a sub-directory called mysql. This is where it looks for during compilation.Step 6:

Edit the setup_posix.py and change the followingmysql_config.path = "mysql_config"tomysql_config.path = "/usr/local/mysql/bin/mysql_config"Step 7:

In the same directory, rebuild your package (ignore the warnings that comes with it)Step 8:

Install the package and you are done.Step 9:

Test if it's working. It works if you can import MySQLdb.>>> import MySQLdbStep 10:

If upon trying to import you receive an error complaining that Library not loaded: libmysqlclient.18.dylib ending with: Reason: image not found you need to create one additional symlink which is:You should then be able to import MySQLdb without any errors.One final hiccup though is that if you start Python from the build directory you will get this error:/Library/Python/2.5/site-packages/MySQL_python-1.2.3c1-py2.5-macosx-10.5-i386.egg/_mysql.py:3: UserWarning: Module _mysql was already imported from /Library/Python/2.5/site-packages/MySQL_python-1.2.3c1-py2.5-macosx-10.5-i386.egg/_mysql.pyc, but XXXX/MySQL-python-1.2.3c1 is being added to sys.pathThis is pretty easy to Google, but to save you the trouble you will end up here (or maybe not... not a particularly future-proof URL) and figure out that you need to cd .. out of build directory and the error should disappear.As I wrote at the top, I'd love to see this answer generalised, as there are numerous other specific experiences of this horrible problem out there. Edit away, or provide your own, better answer.A quick and easy way for Mac OS X 10.8 (Mountain Lion), 10.9 (Mavericks), 10.10 (Yosemite), and 10.11 (El Capitan):I assume you have XCode, its command line tools, Python, and MySQL installed.It worked like a charm for me. I hope it helps.If you encounter an error regarding a missing library: Library not loaded: libmysqlclient.18.dylib then you have to symlink it to /usr/lib like so:Install mysql and python via Macports The porters have done all the difficult work.should install what you need. (see Stack overflow for comments re mysql server)If you only need to connect to mysql and not run a server then the first line is sufficient.Macports now (early 2013) will provide binary downloads for common combinations of OS a executable architecture, for others (and if you request it) it will build from source.In general macports (or fink) help when there are complex libraries etc that need to be installed.Python only code and if simple C dependencies can be set up via setuptools etc, but it begins to get complex if you mix the two.Install pip:Install brew:Install mysql:Install MySQLdbIf you have compilation problems, try editing the ~/.profile file like in one of the answers here.Here's another step I had to go through, after receiving an error on completing Step 9:Reference: Thanks! http://ageekstory.blogspot.com/2011_04_01_archive.htmlAs stated on Installing MySQL-python on mac :Then test it :Just had this problem (again!) after getting a new Lion box.Best solution I've found (still not 100% optimal, but working):you can get it by downloading XCode/Dev Tools from Apple - this is a big download - ... but instead I recommend this github which has what you need (and does not have XCode): https://github.com/kennethreitz/osx-gcc-installerI downloaded their prebuilt PKG for lion, https://github.com/downloads/kennethreitz/osx-gcc-installer/GCC-10.7-v2.pkgNOTE THAT:Permanently add the DYLD_LIBRARY_PATH!  You can add it to your bash_profile or similar.

This was the missing step for me, without which my system continued to insist on various errors finding _mysql.so and so on.export DYLD_LIBRARY_PATH = /usr/local/mysql/lib/@richard-boardman, just noticed your soft link solution, which may in effect be doing the same thing my PATH solution does...folks, whatever works best for you.Best reference:

http://activeintelligence.org/blog/archive/mysql-python-aka-mysqldb-on-osx-lion/You could try using the pure-python pymysql:(Or use pip if you have it installed.) Then, add this before you import MySQLdb in your code:Or simple try:If it gives a error like below:, then just run this If you are using 64 bit MySQL, using ARCHFLAGS to specify your cpu architecture while building mysql-python libraries would do the trick:should fix the issue for you as the system is not able to find the mysql_config file.On macos Sierra this work for me, where python is managed by anaconda:anaconda search -t conda mysql-pythonanaconda show CEFCA/mysql-pythonconda install --channel https://conda.anaconda.org/CEFCA mysql-pythonThe to use with SQLAlchemy:This answer is an update, circa November 2019. None of the popular pip installs will give you a working setup on MacOS 10.13 (and likely other versions as well). Here is a simple way that I got things working:If you need help installing brew, see this site: https://brew.sh/I ran into this issue and found that mysqlclient needs to know where to find openssl, and OSX hides this by default. Locate openssl with brew info openssl, and then add the path to your openssl bin to your PATH:I recommend adding that to your .zshrc or .bashrc so you don't need to worry about it in the future. Then, with that variable exported (which may meed closing and re-opening your bash session), add two more env variables:Then, finally, run:and it should install just fine.Source: https://medium.com/@shandou/pipenv-install-mysqlclient-on-macosx-7c253b0112f2

Issue with virtualenv - cannot activate

user1157538

[Issue with virtualenv - cannot activate](https://stackoverflow.com/questions/8921188/issue-with-virtualenv-cannot-activate)

I created a virtualenv around my project, but when I try to activate it I cannot.

It might just be syntax or folder location, but I am stumped right now.You can see below, I create the virtualenv and call it venv. Everything looks good, then I try to activate it by running source venv/bin/activateI'm thinking it might just have to do with my system path, but not sure what to point it to (I do know how to edit the path). I'm on python 7 / windows os, virtual env 2.2.x

2012-01-19 04:54:37Z

I created a virtualenv around my project, but when I try to activate it I cannot.

It might just be syntax or folder location, but I am stumped right now.You can see below, I create the virtualenv and call it venv. Everything looks good, then I try to activate it by running source venv/bin/activateI'm thinking it might just have to do with my system path, but not sure what to point it to (I do know how to edit the path). I'm on python 7 / windows os, virtual env 2.2.xsource is a shell command designed for users running on Linux (or any Posix, but whatever, not Windows).On Windows, virtualenv creates a batch file, so you should run venv\Scripts\activate instead (per the virtualenv documentation on the activate script).Edit:

The trick here for Windows is not specifying the BAT extension:PS C:\DEV\aProject\env\Scripts> & .\activate

(env) PS C:\DEV\aProject\env\Scripts>I had the same problem. I was using Python 2, Windows 10 and Git Bash. Turns out in Git Bash you need to use:Your prompt will change to indicate that you are now operating within the virtual environment. It will look something like this (venv)user@host:~/venv$.And your venv is activated now.For windows, type "C:\Users\Sid\venv\FirstProject\Scripts\activate" in the terminal without quotes. Simply give the location of your Scripts folder in your project. So, the command will be location_of_the_Scripts_Folder\activate.I was also facing the same issue in my Windows 10 machine.

What steps i tried were:Go to andconda terminal

Step 1Step 2or Step 3You can check it via spider tool in anaconda by typing import tensorflow as tfI have a hell of a time using virtualenv on windows with git bash, I usually end up specifying the python binary explicitly. If my environment is in say .env I'll call python via ./.env/Scripts/python.exe …, or in a shebang line #!./.env/Scripts/python.exe; Both assuming your working directory contains your virtualenv (.env).You can run the source command on cygwin terminalIf you see the 5 folders (Include,Lib,Scripts,tcl,pip-selfcheck) after using the virtualenv yourenvname command, change directory to Scripts folder in the cmd itself and simply use "activate" command.If you are using windows OS then in Gitbash terminal use the following command $source venv/Scripts/activate. This will help you to enter the virtual environment.A small reminder, but I had my slashes the wrong way on Win10 cmd. According to python documentation the activate command is: C:\> <venv>\Scripts\activate.bat

When you're browsing directories it's e.g. cd .env/ScriptsSo to create my venv I used python -m venv --copies .env and to activate .env\Scripts\activate.batsource command is officially for Unix operating systems family and you can't use it on windows basically. instead, you can use venv\Scripts\activate command to activate your virtual environment.If you’re using Windows, use the command "venv\Scripts\activate" (without the word source) to activate the virtual environment. If you’re using PowerShell, you might need to capitalize Activate.open the folder with any gitbash console. 

for example using visualCode and Gitbash console program:

1)Install Gitbash for windows2) using VisualCode IDE, right click over the project  open in terminal console option 3) on window console in Visualcode, looking for a Select->default shell and change it for Gitbash 4)now your project is open with bash console and right path, put source ./Scripts/activatebtw : . with  blank space = source

Single Line Nested For Loops

Asher Garland

[Single Line Nested For Loops](https://stackoverflow.com/questions/17006641/single-line-nested-for-loops)

Wrote this function in python that transposes a matrix:In the process I realized I don't fully understand how single line nested for loops execute. Please help me understand by answering the following questions:Given,Additional information is appreciated as well.

2013-06-09 05:08:31Z

Wrote this function in python that transposes a matrix:In the process I realized I don't fully understand how single line nested for loops execute. Please help me understand by answering the following questions:Given,Additional information is appreciated as well.The best source of information is the official Python tutorial on list comprehensions.  List comprehensions are nearly the same as for loops (certainly any list comprehension can be written as a for-loop) but they are often faster than using a for loop.Look at this longer list comprehension from the tutorial (the if part filters the comprehension, only parts that pass the if statement are passed into the final part of the list comprehension (here (x,y)):It's exactly the same as this nested for loop (and, as the tutorial says, note how the order of for and if are the same).The major difference between a list comprehension and a for loop is that the final part of the for loop (where you do something) comes at the beginning rather than at the end.On to your questions:An iterable. Any object that can generate a (finite) set of elements. These include any container, lists, sets, generators, etc.They are assigned in exactly the same order as they are generated from each list, as if they were in a nested for loop (for your first comprehension you'd get 1 element for i, then every value from j, 2nd element into i, then every value from j, etc.)Yes, already shown above.Sure, but it's not a great idea. Here, for example, gives you a list of lists of characters:You might be interested in itertools.product, which returns an iterable yielding tuples  of values from all the iterables you pass it. That is, itertools.product(A, B) yields all values of the form (a, b), where the a values come from A and the b values come from B. For example:This prints:Notice how the final argument passed to itertools.product is the "inner" one. Generally, itertools.product(a0, a1, ... an) is equal to [(i0, i1, ... in) for in in an for in-1 in an-1 ... for i0 in a0]First of all, your first code doesn't use a for loop per se, but a list comprehension.Below code for best examples for nested loops, while using two for loops please remember the  output of the first loop is input for the second loop. 

Loop termination also important while using the nested loops  

Open files in 'rt' and 'wt' modes

alecxe

[Open files in 'rt' and 'wt' modes](https://stackoverflow.com/questions/23051062/open-files-in-rt-and-wt-modes)

Several times here on SO I've seen people using rt and wt modes for reading and writing files.For example:I don't see the modes documented, but since open() doesn't throw an error - looks like it's pretty much legal to use.What is it for and is there any difference between using wt vs w and  rt vs r?

2014-04-14 02:33:02Z

Several times here on SO I've seen people using rt and wt modes for reading and writing files.For example:I don't see the modes documented, but since open() doesn't throw an error - looks like it's pretty much legal to use.What is it for and is there any difference between using wt vs w and  rt vs r?t refers to the text mode.  There is no difference between r and rt or w and wt since text mode is the default.Documented here:The default mode is 'r' (open for reading text, synonym of 'rt').The t indicates text mode, meaning that \n characters will be translated to the host OS line endings when writing to a file, and back again when reading. The flag is basically just noise, since text mode is the default.Other than U, those mode flags come directly from the standard C library's fopen() function, a fact that is documented in the sixth paragraph of the python2 documentation for open().As far as I know, t is not and has never been part of the C standard, so although many implementations of the C library accept it anyway, there's no guarantee that they all will, and therefore no guarantee that it will work on every build of python. That explains why the python2 docs didn't list it, and why it generally worked anyway. The python3 docs make it official.The 'r' is for reading, 'w' for writing and 'a' is for appending.The 't' represents text mode as apposed to binary mode.Edit: Are you sure you saw rt and not rb?These functions generally wrap the fopen function which is described here:http://www.cplusplus.com/reference/cstdio/fopen/As you can see it mentions the use of b to open the file in binary mode.The document link you provided also makes reference to this b mode:Appending 'b' is useful even on systems that don’t treat binary and text files differently, where it serves as documentation.t indicates for text modehttps://docs.python.org/release/3.1.5/library/functions.html#openon linux, there's no difference between text mode and binary mode,

however, in windows, they converts \n to \r\n when text mode.http://www.cygwin.com/cygwin-ug-net/using-textbinary.html

How to parse/read a YAML file into a Python object? [duplicate]

Jamal Khan

[How to parse/read a YAML file into a Python object? [duplicate]](https://stackoverflow.com/questions/6866600/how-to-parse-read-a-yaml-file-into-a-python-object)

How to parse/read a YAML file into a Python object?For example, this YAML:To this Python class:I am using PyYAML by the way.

2011-07-28 22:36:01Z

How to parse/read a YAML file into a Python object?For example, this YAML:To this Python class:I am using PyYAML by the way.If your YAML file looks like this:And you've installed PyYAML like this:And the Python code looks like this:The variable dataMap now contains a dictionary with the tree data. If you print dataMap using PrettyPrint, you will get something like:So, now we have seen how to get data into our Python program. Saving data is just as easy:You have a dictionary, and now you have to convert it to a Python object:Then you can use:and follow "Convert Python dict to object".For more information you can look at pyyaml.org and this.From http://pyyaml.org/wiki/PyYAMLDocumentation:add_path_resolver(tag, path, kind) adds a path-based implicit tag resolver. A path is a list of keys that form a path to a node in the representation graph. Paths elements can be string values, integers, or None. The kind of a node can be str, list, dict, or None.Here is one way to test which YAML implementation the user has selected on the virtualenv (or the system) and then define load_yaml_file appropriately:

How to do math in a Django template?

Tommy

[How to do math in a Django template?](https://stackoverflow.com/questions/6285327/how-to-do-math-in-a-django-template)

I want to do this:So for example, the output would be 20 if {{ object.article.rating_score }} equaled 80.How do I do this at the template level? I don't have access to the Python code.

2011-06-08 21:10:51Z

I want to do this:So for example, the output would be 20 if {{ object.article.rating_score }} equaled 80.How do I do this at the template level? I don't have access to the Python code.You can use the add filter:Use django-mathfilters. In addition to the built-in add filter, it provides filters to subtract, multiply, divide, and take the absolute value.For the specific example above, you would use {{ 100|sub:object.article.rating_score }}.Generally it is recommended you do this calculation in your view. Otherwise, you could use the add filter.

ValueError : I/O operation on closed file

GobSmack

[ValueError : I/O operation on closed file](https://stackoverflow.com/questions/18952716/valueerror-i-o-operation-on-closed-file)

Here, p is a dictionary, w and c both are strings.When I try to write in the file it reports error:Help me, I'm really new to python. I'm working with Python 2.7.3 

Thank you in advance. 

2013-09-23 06:08:21Z

Here, p is a dictionary, w and c both are strings.When I try to write in the file it reports error:Help me, I'm really new to python. I'm working with Python 2.7.3 

Thank you in advance. Indent correctly; for statement should be inside with block:Outside the with block, the file is closed.Same error can raise by mixing: tabs + spaces.

Is there a benefit to defining a class inside another class in Python?

dF.

[Is there a benefit to defining a class inside another class in Python?](https://stackoverflow.com/questions/78799/is-there-a-benefit-to-defining-a-class-inside-another-class-in-python)

What I'm talking about here are nested classes. Essentially, I have two classes that I'm modeling. A DownloadManager class and a DownloadThread class. The obvious OOP concept here is composition. However, composition doesn't necessarily mean nesting, right?I have code that looks something like this:But now I'm wondering if there's a situation where nesting would be better. Something like:

2008-09-17 01:12:59Z

What I'm talking about here are nested classes. Essentially, I have two classes that I'm modeling. A DownloadManager class and a DownloadThread class. The obvious OOP concept here is composition. However, composition doesn't necessarily mean nesting, right?I have code that looks something like this:But now I'm wondering if there's a situation where nesting would be better. Something like:You might want to do this when the "inner" class is a one-off, which will never be used outside the definition of the outer class. For example to use a metaclass, it's sometimes handy to doinstead of defining a metaclass separately, if you're only using it once.The only other time I've used nested classes like that, I used the outer class only as a namespace to group a bunch of closely related classes together:Then from another module, you can import Group and refer to these as Group.cls1, Group.cls2 etc. However one might argue that you can accomplish exactly the same (perhaps in a less confusing way) by using a module.I don't know Python, but your question seems very general. Ignore me if it's specific to Python.Class nesting is all about scope. If you think that one class will only make sense in the context of another one, then the former is probably a good candidate to become a nested class.It is a common pattern make helper classes as private, nested classes.There is really no benefit to doing this, except if you are dealing with metaclasses.the class: suite really isn't what you think it is. It is a weird scope, and it does strange things. It really doesn't even make a class! It is just a way of collecting some variables - the name of the class, the bases, a little dictionary of attributes, and a metaclass.The name, the dictionary and the bases are all passed to the function that is the metaclass, and then it is assigned to the variable 'name' in the scope where the class: suite was.What you can gain by messing with metaclasses, and indeed by nesting classes within your stock standard classes, is harder to read code, harder to understand code, and odd errors that are terribly difficult to understand without being intimately familiar with why the 'class' scope is entirely different to any other python scope.You could be using a class as class generator. Like (in some off the cuff code :)I feel like when you need this functionality it will be very clear to you. If you don't need to be doing something similar than it probably isn't a good use case.There is another usage for nested class, when one wants to construct inherited classes whose enhanced functionalities are encapsulated in a specific nested class.See this example:Note that in the constructor of foo, the line self.a = self.bar() will construct a foo.bar when the object being constructed is actually a foo object, and a foo2.bar object when the object being constructed is actually a foo2 object. If the class bar was defined outside of class foo instead, as well as its inherited version (which would be called bar2 for example), then defining the new class foo2 would be much more painful, because the constuctor of foo2 would need to have its first line replaced by self.a = bar2(), which implies re-writing the whole constructor. No, composition does not mean nesting.

It would make sense to have a nested class if you want to hide it more in the namespace of the outer class.Anyway, I don't see any practical use for nesting in your case. It would make the code harder to read (understand) and it would also increase the indentation which would make the lines shorter and more prone to splitting.

Is it ok having both Anacondas 2.7 and 3.5 installed in the same time?

GileBrt

[Is it ok having both Anacondas 2.7 and 3.5 installed in the same time?](https://stackoverflow.com/questions/37442494/is-it-ok-having-both-anacondas-2-7-and-3-5-installed-in-the-same-time)

I am using currently Anaconda with Python 2.7, but I will need to use Python 3.5. Is it ok to have them installed both in the same time? Should I expect some problems?

I am on a 64-bit Win8.

2016-05-25 16:08:38Z

I am using currently Anaconda with Python 2.7, but I will need to use Python 3.5. Is it ok to have them installed both in the same time? Should I expect some problems?

I am on a 64-bit Win8.My understanding is you don't need to install Anaconda again to start using a different version of python. Instead, conda has the ability to separately manage python 2 and 3 environments.I use both depending on who in my department I am helping (Some people prefer 2.7, others 3.5). Anyway, I use Anaconda and my default installation is 3.5. I use environments for other versions of python, packages, etc.. So for example, when I wanted to start using python 2.7 I ran:This creates a new environment named Python27 and installs Python version 2.7. You can add arguments to that line for installing other packages by default or just start from scratch. The environment will automatically activate, to deactivate simply type deactivate (windows) or source deactivate (linux, osx) in the command line. To activate in the future type activate Python27 (windows) or source activate Python27 (linux, osx). I would recommend reading the documentation for Managing Environments in Anaconda, if you choose to take that route.UpdateAs of conda version 4.6 you can now use conda activate and conda deactivate. The use of source is now deprecated and will eventually be removed.Yes you can.You don't have to download both Anaconda. Only you need to download one of the version of Anaconda and need activate other version of Anaconda python.If you have Python 3, you can set up a Python 2 kernel like this;If you have Python 2, Then you will be able to see both version of Python!If you are using Anaconda Spyder then you should swap version here:If you are using Jupiter then check here:Note: If your Jupiter or Anaconda already open after installation you need to restart again. Then you will be able to see.Yes, It should be alright to have both versions installed. It's actually pretty much expected nowadays. A lot of stuff is written in 2.7, but 3.5 is becoming the norm. I would recommend updating all your python to 3.5 ASAP, though.I have python 2.7.13 and 3.6.2 both installed.  Install Anaconda for python 3 first and then you can use conda syntax to get 2.7.  My install used:

conda create -n py27 python=2.7.13 anacondaAnaconda is made for the purpose you are asking. It is also an environment manager. It separates out environments. It was made because stable and legacy packages were not supported with newer/unstable versions of host languages; therefore a software was required that could separate and manage these versions on the same machine without the need to reinstall or uninstall individual host programming languages/environments.You can find creation/deletion of environments in the Anaconda documentation. Hope this helped.

How to Implement a Binary Tree?

Bruce

[How to Implement a Binary Tree?](https://stackoverflow.com/questions/2598437/how-to-implement-a-binary-tree)

Which is the best data structure that can be used to implement Binary Tree in Python?

2010-04-08 08:23:42Z

Which is the best data structure that can be used to implement Binary Tree in Python?Here is my simple recursive implementation of binary search tree. Read more about it Here:-This is a very simple implementation of a binary tree. This is a nice tutorial with questions in betweenSimple implementation of BST in Python A very quick 'n dirty way of implementing a binary tree using lists.

Not the most efficient, nor does it handle nil values all too well.

But it's very transparent (at least to me):Constructing a tree from an iterable:Traversing a tree:I can't help but notice that most answers here are implementing a Binary Search Tree. Binary Search Tree != Binary Tree.Now, to answer the OP's question, I am including a full implementation of a Binary Tree in Python. The underlying data structure storing each BinaryTreeNode is a dictionary, given it offers optimal O(1) lookups. I've also implemented depth-first and breadth-first traversals. These are very common operations performed on trees.you don't need to have two classes[What you need for interviews] A Node class is the sufficient bare minimum to represent a binary tree. (While other answers are mostly correct, they are not required for a binary tree no need to extend object class, no need to be a BST, no need to import deque). Here is an example of a tree: In this example n1 is the root of the tree having n2, n3 as its children.A Node-based class of connected nodes is a standard approach.  These can be hard to visualize.Motivated from an essay on Python Patterns - Implementing Graphs, consider a simple dictionary:GivenA binary treeCodeMake a dictionary of unique nodes:DetailsTree-based functions often include the following common operations:Try implementing all of these operations.

Here we demonstrate one of these functions - a BFS traversal:ExampleThis is a breadth-first search (level-order) algorithm applied to a dict of nodes and children.  See also this in-depth tutorial on trees.InsightSomething great about traversals in general, we can easily alter the latter iterative approach to depth-first search (DFS) by simply replacing the queue with a stack (a.k.a LIFO Queue).  This simply means we dequeue from the same side that we enqueue.  DFS allows us to search each branch.How?  Since we are using a deque, we can emulate a stack by changing node = q.popleft() to node = q.pop() (right).  The result is a right-favored, pre-ordered DFS: ['a', 'c', 'f', 'b', 'e', 'd'].A little more "Pythonic" ?I know many good solutions have already been posted but I usually have a different approach for binary trees: going with some Node class and implementing it directly is more readable but when you have a lot of nodes it can become very greedy regarding memory, so I suggest adding one layer of complexity and storing the nodes in a python list, and then simulating a tree behavior using only the list.You can still define a Node class to finally represent the nodes in the tree when needed, but keeping them in a simple form [value, left, right] in a list will use half the memory or less!Here is a quick example of a Binary Search Tree class storing the nodes in an array. It provides basic fonctions such as add, remove, find...I've added a parent attribute so that you can remove any node and maintain the BST structure.Sorry for the readability, especially for the "remove" function. Basically, when a node is removed, we pop the tree array and replace it with the last element (except if we wanted to remove the last node). To maintain the BST structure, the removed node is replaced with the max of its left children or the min of its right children and some operations have to be done in order to keep the indexes valid but it's fast enough.I used this technique for more advanced stuff to build some big words dictionaries with an internal radix trie and I was able to divide memory consumption by 7-8 (you can see an example here: https://gist.github.com/fbparis/b3ddd5673b603b42c880974b23db7cda)This implementation supports insert, find and delete operations without destroy the structure of the tree. This is not a banlanced tree.A good implementation of binary search tree, taken from here:I want to show a variation of @apadana's method, which is more useful when there is a considerable number of nodes:Binary Tree in PythonHere is a simple solution which can be used to build a binary tree using a recursive approach to display the tree in order traversal has been used in the below code.Code taken from : Binary Tree in Python

What exactly is Python multiprocessing Module's .join() Method Doing?

MikeiLL

[What exactly is Python multiprocessing Module's .join() Method Doing?](https://stackoverflow.com/questions/25391025/what-exactly-is-python-multiprocessing-modules-join-method-doing)

Learning about Python Multiprocessing (from a PMOTW article) and would love some clarification on what exactly the join() method is doing.In an old tutorial from 2008 it states that without the p.join() call in the code below, "the child process will sit idle and not terminate, becoming a zombie you must manually kill".I added a printout of the PID as well as a time.sleep to test and as far as I can tell, the process terminates on its own:within 20 seconds:after 20 seconds:Behavior is the same with p.join() added back at end of the file. Python Module of the Week offers a very readable explanation of the module; "To wait until a process has completed its work and exited, use the join() method.", but it seems like at least OS X was doing that anyway. Am also wondering about the name of the method. Is the .join() method concatenating anything here? Is it concatenating a process with it's end? Or does it just share a name with Python's native .join() method?

2014-08-19 18:59:28Z

Learning about Python Multiprocessing (from a PMOTW article) and would love some clarification on what exactly the join() method is doing.In an old tutorial from 2008 it states that without the p.join() call in the code below, "the child process will sit idle and not terminate, becoming a zombie you must manually kill".I added a printout of the PID as well as a time.sleep to test and as far as I can tell, the process terminates on its own:within 20 seconds:after 20 seconds:Behavior is the same with p.join() added back at end of the file. Python Module of the Week offers a very readable explanation of the module; "To wait until a process has completed its work and exited, use the join() method.", but it seems like at least OS X was doing that anyway. Am also wondering about the name of the method. Is the .join() method concatenating anything here? Is it concatenating a process with it's end? Or does it just share a name with Python's native .join() method?The join() method, when used with threading or multiprocessing, is not related to str.join() - it's not actually concatenating anything together. Rather, it just means "wait for this [thread/process] to complete". The name join is used because the multiprocessing module's API is meant to look as similar to the threading module's API, and the threading module uses join for its Thread object. Using the term join to mean "wait for a thread to complete" is common across many programming languages, so Python just adopted it as well.Now, the reason you see the 20 second delay both with and without the call to join() is because by default, when the main process is ready to exit, it will implicitly call join() on all running multiprocessing.Process instances. This isn't as clearly stated in the multiprocessing docs as it should be, but it is mentioned in the Programming Guidelines section:You can override this behavior by setting the daemon flag on the Process to True prior to starting the process:If you do that, the child process will be terminated as soon as the main process completes:Without the join(), the main process can complete before the child process does. I'm not sure under what circumstances that leads to zombieism. The main purpose of join() is to ensure that a child process has completed before the main process does anything that depends on the work of the child process.The etymology of join() is that it's the opposite of fork, which is the common term in Unix-family operating systems for creating child processes. A single process "forks" into several, then "joins" back into one.I'm not going to explain in detail what join does, but here's the etymology and the intuition behind it, which should help you remember its meaning more easily.The idea is that execution "forks" into multiple processes of which one is the master, the rest workers (or "slaves"). When the workers are done, they "join" the master so that serial execution may be resumed.The join method causes the master process to wait for a worker to join it. The method might better have been called "wait", since that's the actual behavior it causes in the master (and that's what it's called in POSIX, although POSIX threads call it "join" as well). The joining only occurs as an effect of the threads cooperating properly, it's not something the master does.The names "fork" and "join" have been used with this meaning in multiprocessing since 1963.join() is used to wait for the worker processes to exit. One must call close() or terminate() before using join().Like @Russell mentioned join is like the opposite of fork (which Spawns sub-processes). For join to run you have to run close() which will prevent any more tasks from being submitted to the pool and exit once all tasks complete. Alternatively, running terminate() will just exit by stopping all worker processes immediately."the child process will sit idle and not terminate, becoming a zombie you must manually kill" this is possible when the main (parent) process exits but the child process is still running and once completed it has no parent process to return its exit status to.

How can I increment a char?

Tom R

[How can I increment a char?](https://stackoverflow.com/questions/2156892/how-can-i-increment-a-char)

I'm new to Python, coming from Java and C. How can I increment a char? In Java or C, chars and ints are practically interchangeable, and in certain loops, it's very useful to me to be able to do increment chars, and index arrays by chars. How can I do this in Python? It's bad enough not having a traditional for(;;) looper - is there any way I can achieve what I want to achieve without having to rethink my entire strategy?

2010-01-28 18:28:20Z

I'm new to Python, coming from Java and C. How can I increment a char? In Java or C, chars and ints are practically interchangeable, and in certain loops, it's very useful to me to be able to do increment chars, and index arrays by chars. How can I do this in Python? It's bad enough not having a traditional for(;;) looper - is there any way I can achieve what I want to achieve without having to rethink my entire strategy?In Python 2.x, just use the ord and chr functions:Python 3.x makes this more organized and interesting, due to its clear distinction between bytes and unicode. By default, a "string" is unicode, so the above works (ord receives Unicode chars and chr produces them).But if you're interested in bytes (such as for processing some binary data stream), things are even simpler:"bad enough not having a traditional for(;;) looper"??  What?  Are you trying to do Or perhaps you're using string.uppercase or string.letters?Python doesn't have for(;;) because there are often better ways to do it.  It also doesn't have character math because it's not necessary, either.I came from PHP, where you can increment char (A to B, Z to AA, AA to AB etc.) using ++ operator. I made a simple function which does the same in Python. You can also change list of chars to whatever (lowercase, uppercase, etc.) is your need.There is a way to increase character using ascii_letters from string package which ascii_letters is a string that contains all English alphabet, uppercase and lowercase:Also it can be done manually;output: 

Google Colaboratory: misleading information about its GPU (only 5% RAM available to some users)

stason

[Google Colaboratory: misleading information about its GPU (only 5% RAM available to some users)](https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available)

update: this question is related to Google Colab's "Notebook settings: Hardware accelerator: GPU". This question was written before the "TPU" option was added.Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run fast.ai lesson on it for it to never complete - quickly running out of memory. I started investigating of why.The bottom line is that「free Tesla K80」is not "free" for all - for some only a small slice of it is "free". I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM.Clearly 0.5GB GPU RAM is insufficient for most ML/DL work.If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook):Executing it in a jupyter notebook before running any other code gives me:The lucky users who get access to the full card will see:Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil?Can you confirm that you get similar results if you run this code on Google Colab notebook?If my calculations are correct, is there any way to get more of that GPU RAM on the free box?update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing!note: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still).

2018-02-12 15:44:14Z

update: this question is related to Google Colab's "Notebook settings: Hardware accelerator: GPU". This question was written before the "TPU" option was added.Reading multiple excited announcements about Google Colaboratory providing free Tesla K80 GPU, I tried to run fast.ai lesson on it for it to never complete - quickly running out of memory. I started investigating of why.The bottom line is that「free Tesla K80」is not "free" for all - for some only a small slice of it is "free". I connect to Google Colab from West Coast Canada and I get only 0.5GB of what supposed to be a 24GB GPU RAM. Other users get access to 11GB of GPU RAM.Clearly 0.5GB GPU RAM is insufficient for most ML/DL work.If you're not sure what you get, here is little debug function I scraped together (only works with the GPU setting of the notebook):Executing it in a jupyter notebook before running any other code gives me:The lucky users who get access to the full card will see:Do you see any flaw in my calculation of the GPU RAM availability, borrowed from GPUtil?Can you confirm that you get similar results if you run this code on Google Colab notebook?If my calculations are correct, is there any way to get more of that GPU RAM on the free box?update: I'm not sure why some of us get 1/20th of what other users get. e.g. the person who helped me to debug this is from India and he gets the whole thing!note: please don't send any more suggestions on how to kill the potentially stuck/runaway/parallel notebooks that might be consuming parts of the GPU. No matter how you slice it, if you are in the same boat as I and were to run the debug code you'd see that you still get a total of 5% of GPU RAM (as of this update still).So to prevent another dozen of answers suggesting invalid in the context of this thread suggestion to !kill -9 -1, let's close this thread:The answer is simple:As of this writing Google simply gives only 5% of GPU to some of us, whereas 100% to the others. Period.dec-2019 update: The problem still exists - this question's upvotes continue still.mar-2019 update: A year later a Google employee @AmiF commented on the state of things, stating that the problem doesn't exist, and anybody who seems to have this problem needs to simply reset their runtime to recover memory. Yet, the upvotes continue, which to me this tells that the problem still exists, despite @AmiF's suggestion to the contrary.dec-2018 update: I have a theory that Google may have a blacklist of certain accounts, or perhaps browser fingerprints, when its robots detect a non-standard behavior. It could be a total coincidence, but for quite some time I had an issue with Google Re-captcha on any website that happened to require it, where I'd have to go through dozens of puzzles before I'd be allowed through, often taking me 10+ min to accomplish. This lasted for many months. All of a sudden as of this month I get no puzzles at all and any google re-captcha gets resolved with just a single mouse click, as it used to be almost a year ago. And why I'm telling this story? Well, because at the same time I was given 100% of the GPU RAM on Colab. That's why my suspicion is that if you are on a theoretical Google black list then you aren't being trusted to be given a lot of resources for free. I wonder if any of you find the same correlation between the limited GPU access and the Re-captcha nightmare. As I said, it could be totally a coincidence as well.Last night I ran your snippet and got exactly what you got:but today:I think the most probable reason is the GPUs are shared among VMs, so each time you restart the runtime you have chance to switch the GPU, and there is also probability you switch to one that is being used by other users.UPDATED:

It turns out that I can use GPU normally even when the GPU RAM Free is 504 MB, which I thought as the cause of ResourceExhaustedError I got last night. If you execute a cell that just has

!kill -9 -1

in it, that'll cause all of your runtime's state (including memory, filesystem, and GPU) to be wiped clean and restarted.  Wait 30-60s and press the CONNECT button at the top-right to reconnect.Misleading description on the part of Google. I got too excited about it too, I guess. Set everything up, loaded the data, and now I am not able to do anything with it due to having only 500Mb memory allocated to my Notebook.Find the Python3 pid and kill the pid. Please see the below imageNote: kill only python3(pid=130) not jupyter python(122).Restart Jupyter IPython Kernel:Im not sure if this blacklisting is true! Its rather possible, that the cores are shared among users. I ran also the test, and my results are the following:Gen RAM Free: 12.9 GB  | Proc size: 142.8 MB

GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MBIt seems im getting also full core. However i ran it a few times, and i got the same result. Maybe i will repeat this check a few times during the day to see if there is any change.just give a heavy task to google colab, it will ask us to change to 25 gb of ram. example run this code twice:then click on get more ram :)

I believe if we have multiple notebooks open. Just closing it doesn't actually stop the process. I haven't figured out how to stop it. But I used top to find PID of the python3 that was running longest and using most of the memory and I killed it. Everything back to normal now.

cv2.imshow command doesn't work properly in opencv-python

top.eng

[cv2.imshow command doesn't work properly in opencv-python](https://stackoverflow.com/questions/21810452/cv2-imshow-command-doesnt-work-properly-in-opencv-python)

I'm using opencv 2.4.2, python 2.7

The following simple code created a window of the correct name, but its content is just blank and doesn't show the image:does anyone knows about this issue?

2014-02-16 11:24:54Z

I'm using opencv 2.4.2, python 2.7

The following simple code created a window of the correct name, but its content is just blank and doesn't show the image:does anyone knows about this issue?imshow() only works with waitKey():(The whole message-loop necessary for updating the window is hidden in there.)I found the answer that worked for me here:

http://txt.arboreus.com/2012/07/11/highgui-opencv-window-from-ipython.htmlYou must use cv2.waitKey(0) after cv2.imshow("window",img). Only then will it work.If you are running inside a Python console, do this:Then if you press Enter on the image, it will successfully close the image and you can proceed running other commands.I faced the same issue. I tried to read an image from IDLE and tried to display it using cv2.imshow(), but the display window freezes and shows pythonw.exe is not responding when trying to close the window.The post below gives a possible explanation for why this is happeningpythonw.exe is not responding"Basically, don't do this from IDLE. Write a script and run it from the shell or the script directly if in windows, by naming it with a .pyw extension and double clicking it. There is apparently a conflict between IDLE's own event loop and the ones from GUI toolkits."When I used imshow() in a script and execute it rather than running it directly over IDLE, it worked. add cv2.waitKey(0) in the end.For me waitKey() with number greater than 0 workedYou've got all the necessary pieces somewhere in this thread:works fine for me in IDLE.If you have not made this working, you better putinto one file and run it.If you choose to use "cv2.waitKey(0)", be sure that you have written "cv2.waitKey(0)" instead of "cv2.waitkey(0)", because that lowercase "k" might freeze your program too.I also had a -215 error. I thought imshow was the issue, but when I changed imread to read in a non-existent file I got no error there. So I put the image file in the working folder and added cv2.waitKey(0) and it worked.I had the same 215 error, which I was able to overcome by giving the full path to the image, as in, C:\Folder1\Folder2\filename.extThis error is produced because the image is not found. So it's not an error of imshow function.

Changing user agent on urllib2.urlopen

Paolo Bergantino

[Changing user agent on urllib2.urlopen](https://stackoverflow.com/questions/802134/changing-user-agent-on-urllib2-urlopen)

How can I download a webpage with a user agent other than the default one on urllib2.urlopen?

2009-04-29 12:32:36Z

How can I download a webpage with a user agent other than the default one on urllib2.urlopen?Setting the User-Agent from everyone's favorite Dive Into Python.The short story: You can use Request.add_header to do this.You can also pass the headers as a dictionary when creating the Request itself, as the docs note:I answered a similar question a couple weeks ago.There is example code in that question, but basically you can do something like this: (Note the capitalization of User-Agent as of RFC 2616, section 14.43.)Or, a bit shorter:For python 3, urllib is split into 3 modules...All these should work in theory, but (with Python 2.7.2 on Windows at least) any time you send a custom User-agent header, urllib2 doesn't send that header.  If you don't try to send a User-agent header, it sends the default Python / urllib2 None of these methods seem to work for adding User-agent but they work for other headers:For urllib you can use:Another solution in urllib2 and Python 2.7:Try this :there are two properties of urllib.URLopener() namely:

addheaders = [('User-Agent', 'Python-urllib/1.17'), ('Accept', '*/*')] and

version = 'Python-urllib/1.17'.

To fool the website you need to changes both of these values to an accepted User-Agent. for e.g.

Chrome browser : 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.149 Safari/537.36'

Google bot : 'Googlebot/2.1'

like this  changing just one property does not work because the website marks it as a suspicious request.

Python : List of dict, if exists increment a dict value, if not append a new dict

Natim

[Python : List of dict, if exists increment a dict value, if not append a new dict](https://stackoverflow.com/questions/1692388/python-list-of-dict-if-exists-increment-a-dict-value-if-not-append-a-new-dic)

I would like do something like that.How can I do ? I don't know if I should take the tuple to edit it or figure out the tuple indices?Any help ?

2009-11-07 08:07:05Z

I would like do something like that.How can I do ? I don't know if I should take the tuple to edit it or figure out the tuple indices?Any help ?That is a very strange way to organize things.  If you stored in a dictionary, this is easy:This code for updating a dictionary of counts is a common "pattern" in Python.  It is so common that there is a special data structure, defaultdict, created just to make this even easier:If you access the defaultdict using a key, and the key is not already in the defaultdict, the key is automatically added with a default value.  The defaultdict takes the callable you passed in, and calls it to get the default value.  In this case, we passed in class int; when Python calls int() it returns a zero value.  So, the first time you reference a URL, its count is initialized to zero, and then you add one to the count.But a dictionary full of counts is also a common pattern, so Python provides a ready-to-use class: containers.Counter  You just create a Counter instance by calling the class, passing in any iterable; it builds a dictionary where the keys are values from the iterable, and the values are counts of how many times the key appeared in the iterable.  The above example then becomes:If you really need to do it the way you showed, the easiest and fastest way would be to use any one of these three examples, and then build the one you need.If you are using Python 2.7 or newer you can do it in a one-liner:Using the default works, but so does:using .get, you can get a default return if it doesn't exist. By default it's None, but in the case I sent you, it would be 0.Use defaultdict:This always works fine for me:To do it exactly your way? You could use the for...else structureBut it is quite inelegant. Do you really have to store the visited urls as a LIST?

If you sort it as a dict, indexed by url string, for example, it would be way cleaner:A few things to note in that second example:And of course I'm assuming that dict(url='http://www.google.fr', nbr=1) is a simplification of your own data structure, because otherwise, urls could simply be:Which can get very elegant with the defaultdict stance:Except for the first time, each time a word is seen the if statement's test fails. If you are counting a large number of words, many will probably occur multiple times. In a situation where the initialization of a value is only going to occur once and the augmentation of that value will occur many times it is cheaper to use a try statement:you can read more about this:  https://wiki.python.org/moin/PythonSpeed/PerformanceTips

Bitwise operation and usage

3zzy

[Bitwise operation and usage](https://stackoverflow.com/questions/1746613/bitwise-operation-and-usage)

Consider this code:I can understand the arithmetic operators in Python (and other languages), but I never understood 'bitwise' operators quite well. In the above example (from a Python book), I understand the left-shift but not the other two.Also, what are bitwise operators actually used for? I'd appreciate some examples.

2009-11-17 04:37:05Z

Consider this code:I can understand the arithmetic operators in Python (and other languages), but I never understood 'bitwise' operators quite well. In the above example (from a Python book), I understand the left-shift but not the other two.Also, what are bitwise operators actually used for? I'd appreciate some examples.Bitwise operators are operators that work on multi-bit values, but conceptually one bit at a time.These can often be best shown as truth tables.  Input possibilities are on the top and left, the resultant bit is one of the four (two in the case of NOT since it only has one input) values shown at the intersection of the inputs.One example is if you only want the lower 4 bits of an integer, you AND it with 15 (binary 1111) so:The zero bits in 15 in that case effectively act as a filter, forcing the bits in the result to be zero as well.In addition, >> and << are often included as bitwise operators, and they "shift" a value respectively right and left by a certain number of bits, throwing away bits that roll of the end you're shifting towards, and feeding in zero bits at the other end.So, for example:Note that the left shift in Python is unusual in that it's not using a fixed width where bits are discarded - while many languages use a fixed width based on the data type, Python simply expands the width to cater for extra bits. In order to get the discarding behaviour in Python, you can follow a left shift with a bitwise and such as in an 8-bit value shifting left four bits:With that in mind, another example of bitwise operators is if you have two 4-bit values that you want to pack into an 8-bit one, you can use all three of your operators (left-shift, and and or):If val1 is 7 and val2 is 4:One typical usage:| is used to set a certain bit to 1& is used to test or clear a certain bit Take the case of your list for example:x | 2 is used to set bit 1 of x to 1 x & 1 is used to test if bit 0 of x is 1 or 0    One of the most common uses of bitwise operations is for parsing hexadecimal colours.For example, here's a Python function that accepts a String like #FF09BE and returns a tuple of its Red, Green and Blue values.I know that there are more efficient ways to acheive this, but I believe that this is a really concise example illustrating both shifts and bitwise boolean operations.I think that the second part of the question:  Has been only partially addressed. These are my two cents on that matter. Bitwise operations in programming languages play a fundamental role when dealing with a lot of applications. Almost all low-level computing must be done using this kind of operations. In all applications that need to send data between two nodes, such as:In the lower level layer of communication, the data is usually sent in what is called frames. Frames are just strings of bytes that are sent through a physical channel. This frames usually contain the actual data plus some other fields (coded in bytes) that are part of what is called the header. The header usually contains bytes that encode some information related to the status of the communication (e.g, with flags (bits)), frame counters, correction and error detection codes, etc. To get the transmitted data in a frame, and to build the frames to send data, you will need for sure bitwise operations.In general, when dealing with that kind of applications, an API is available so you don't have to deal with all those details. For example, all modern programming languages provide libraries for socket connections, so you don't actually need to build the TCP/IP communication frames. But think about the good people that programmed those APIs for you, they had to deal with frame construction for sure; using all kinds of bitwise operations to go back and forth from the low-level to the higher-level communication.As a concrete example, imagine some one gives you a file that contains raw data that was captured directly by telecommunication hardware. In this case, in order to find the frames,  you will need to read the raw bytes in the file and try to find some kind of synchronization words, by scanning the data bit by bit. After identifying the synchronization words, you will need to get the actual frames, and SHIFT them if necessary (and that is just the start of the story) to get the actual data that is being transmitted.Another very different low level family of application is when you need to control hardware using some (kind of ancient) ports, such as parallel and serial ports. This ports are controlled by setting some bytes, and each bit of that bytes has a specific meaning, in terms of instructions, for that port (see for instance http://en.wikipedia.org/wiki/Parallel_port). If you want to build software that does something with that hardware you will need bitwise operations to translate the instructions you want to execute to the bytes that the port understand.For example, if you have some physical buttons connected to the parallel port to control some other device, this is a line of code that you can find in the soft application: Hope this contributes.I hope this clarifies those two:Think of 0 as false and 1 as true.  Then bitwise and(&) and or(|) work just like regular and and or except they do all of the bits in the value at once.  Typically you will see them used for flags if you have 30 options that can be set (say as draw styles on a window) you don't want to have to pass in 30 separate boolean values to set or unset each one so you use | to combine options into a single value and then you use & to check if each option is set.  This style of flag passing is heavily used by OpenGL.  Since each bit is a separate flag you get flag values on powers of two(aka numbers that have only one bit set) 1(2^0) 2(2^1) 4(2^2) 8(2^3) the power of two tells you which bit is set if the flag is on.Also note 2 = 10 so x|2 is 110(6) not 111(7) If none of the bits overlap(which is true in this case) | acts like addition.I didn't see it mentioned above but you will also see some people use left and right shift for  arithmetic operations.  A left shift by x is equivalent to multiplying by 2^x (as long as it doesn't overflow) and a right shift is equivalent to dividing by 2^x.Recently I've seen people using x << 1 and x >> 1 for doubling and halving, although I'm not sure if they are just trying to be clever or if there really is a distinct advantage over the normal operators.SetsSets can be combined using mathematical operations.Try It Yourself:Result:This example will show you the operations for all four 2 bit values:Here is one example of usage:Another common use-case is manipulating/testing file permissions.  See the Python stat module: http://docs.python.org/library/stat.html.For example, to compare a file's permissions to a desired permission set, you could do something like:I cast the results as booleans, because I only care about the truth or falsehood, but it would be a worthwhile exercise to print out the bin() values for each one.Bit representations of integers are often used in scientific computing to represent arrays of true-false information because a bitwise operation is much faster than iterating through an array of booleans. (Higher level languages may use the idea of a bit array.)A nice and fairly simple example of this is the general solution to the game of Nim. Take a look at the Python code on the Wikipedia page. It makes heavy use of bitwise exclusive or, ^.There may be a better way to find where an array element is between two values, but as this example shows, the & works here, whereas and does not.i didnt see it mentioned, This example will show you the (-) decimal operation for 2 bit values: A-B (only if A contains B)this operation is needed when we hold an verb in our program that represent bits. sometimes we need to add bits (like above) and sometimes we need to remove bits (if the verb contains then)with python:

7 & ~4 = 3  (remove from 7 the bits that represent 4)with python:

1 & ~4 = 1  (remove from 1 the bits that represent 4 - in this case 1 is not 'contains' 4)..Whilst manipulating bits of an integer is useful, often for network protocols, which may be specified down to the bit, one can require manipulation of longer byte sequences (which aren't easily converted into one integer). In this case it is useful to employ the bitstring library which allows for bitwise operations on data - e.g. one can import the string 'ABCDEFGHIJKLMNOPQ' as a string or as hex and bit shift it (or perform other bitwise operations):the following bitwise operators: &, |, ^, and ~ return values (based on their input) in the same way logic gates affect signals. You could use them to emulate circuits.To flip bits (i.e. 1's complement/invert) you can do the following:Since value ExORed with all 1s results into inversion, 

for a given bit width you can use ExOR to invert them.

Python Script execute commands in Terminal

Ali

[Python Script execute commands in Terminal](https://stackoverflow.com/questions/3730964/python-script-execute-commands-in-terminal)

I read this somewhere a while ago but cant seem to find it. I am trying to find a command that  will execute commands in the terminal and then output the result.For example: the script will be:It will out the result of running that command in the terminal

2010-09-16 21:28:28Z

I read this somewhere a while ago but cant seem to find it. I am trying to find a command that  will execute commands in the terminal and then output the result.For example: the script will be:It will out the result of running that command in the terminalThere are several ways to do this:A simple way is using the os module:More complex things can be achieved with the subprocess module:

for example:I prefer usage of subprocess module:Reason is that if you want to pass some variable in the script this gives very easy way for example take the following part of the codeIn fact any question on subprocess will be a good readYou should also look into commands.getstatusoutputThis returns a tuple of length 2..

The first is the return integer ( 0 - when the commands is successful )

second is the whole output as will be shown in the terminal.For ls The os.popen() is pretty simply to use, but it has been deprecated since Python 2.6.

You should use the subprocess module instead.Read here: reading a os.popen(command) into a stringThis should work. I do not know how to print the output into the python Shell. You could import the 'os' module and use it like this : In a jupyter notebook you can use the magic function ! To execute this as a .py script you would need to use ipython execute script 

What is the id( ) function used for?

Thanakron Tandavas

[What is the id( ) function used for?](https://stackoverflow.com/questions/15667189/what-is-the-id-function-used-for)

I read the Python 2 docs and noticed the id() function:So, I experimented by using id() with a list:What is the integer returned from the function? Is it synonymous to memory addresses in C? If so, why doesn't the integer correspond to the size of the data type?When is id() used in practice?

2013-03-27 18:58:15Z

I read the Python 2 docs and noticed the id() function:So, I experimented by using id() with a list:What is the integer returned from the function? Is it synonymous to memory addresses in C? If so, why doesn't the integer correspond to the size of the data type?When is id() used in practice?Your post asks several questions:It is "an integer (or long integer) which is guaranteed to be unique and constant for this object during its lifetime." (Python Standard Library - Built-in Functions) A unique number. Nothing more, and nothing less. Think of it as a social-security number or employee id number for Python objects.Conceptually, yes, in that they are both guaranteed to be unique in their universe during their lifetime. And in one particular implementation of Python, it actually is the memory address of the corresponding C object.Because a list is not an array, and a list element is a reference, not an object.Hardly ever. id() (or its equivalent) is used in the is operator. That's the identity of the location of the object in memory...This example might help you understand the concept a little more.These all point to the same location in memory, which is why their values are the same. In the example, 1 is only stored once, and anything else pointing to 1 will reference that memory location.id() does return the address of the object being referenced (in CPython), but your confusion comes from the fact that python lists are very different from C arrays. In a python list, every element is a reference. So what you are doing is much more similar to this C code:In other words, you are printing the address from the reference and not an address relative to where your list is stored.In my case, I have found the id() function handy for creating opaque handles to return to C code when calling python from C. Doing that, you can easily use a dictionary to look up the object from its handle and it's guaranteed to be unique.Rob's answer (most voted above) is correct. I would like to add that in some situations using IDs is useful as it allows for comparison of objects and finding which objects refer to your objects.The later usually helps you for example to debug strange bugs where mutable objects are passed as parameter to say classes and are assigned to local vars in a class. Mutating those objects will mutate vars in a class. This manifests itself in strange behavior where multiple things change at the same time.Recently I had this problem with a Python/Tkinter app where editing text in one text entry field changed the text in another as I typed :)Here is an example on how you might use function id() to trace where those references are. By all means this is not a solution covering all possible cases, but you get the idea. Again IDs are used in the background and user does not see them:OUTPUT:Underscores in variable names are used to prevent name colisions. Functions use "fromwhere" argument so that you can let them know where to start searching for references. This argument is filled by a function that lists all names in a given namespace. Globals() is one such function.I am starting out with python and I use id when I use the interactive shell to see whether my variables are assigned to the same thing or if they just look the same.Every value is an id, which is a unique number related to where it is stored in the memory of the computer.If you're using python 3.4.1 then you get a different answer to your question.returns:The integers -5 to 256 have a constant id, and on finding it multiple times its id does not change, unlike all other numbers before or after it that have different id's every time you find it.

The numbers from -5 to 256 have id's in increasing order and differ by 16.The number returned by id() function is a unique id given to each item stored in memory and it is analogy wise the same as the memory location in C.The answer is pretty much never. IDs are mainly used internally to Python.The average Python programmer will probably never need to use id() in their code.It is the address of the object in memory, exactly as the doc says. However, it has metadata attached to it, properties of the object and location in the memory is needed to store the metadata. So, when you create your variable called list, you also create metadata for the list and its elements.So, unless you an absolute guru in the language you can't determine the id of the next element of your list based on the previous element, because you don't know what the language allocates along with the elements.I have an idea to use value of id() in logging.

It's cheap to get and it's quite short.

In my case I use tornado and id() would like to have an anchor to group messages scattered and mixed over file by web socket.The is operator uses it to check whether two objects are identical (as opposed to equal). The actual value that is returned from id() is pretty much never used for anything because it doesn't really have a meaning, and it's platform-dependent.I'm a little bit late and i will talk about Python3. To understand what id() is and how it (and Python) works, consider next example:You need to think about everything on the right side as objects. Every time you make assignment - you create new object and that means new id. In the middle you can see a "wild" object which is created only for function - id(1000). So, it's lifetime is only for that line of code. If you check the next line - you see that when we create new variable x, it has the same id as that wild object. Pretty much it works like memory address.Be carefull (concerning the answer just below)...That's only true because 123 is between -5 and 256...As of in python 3 id is assigned to a value not a variable. This means that if you create two functions as below, all the three id's are the same.

python-pandas and databases like mysql

user1320615

[python-pandas and databases like mysql](https://stackoverflow.com/questions/10065051/python-pandas-and-databases-like-mysql)

The documentation for Pandas has numerous examples of best practices for working with data stored in various formats.However, I am unable to find any good examples for working with databases like MySQL for example.Can anyone point me to links or give some code snippets of how to convert query results using mysql-python to data frames in Pandas efficiently ?

2012-04-08 18:01:13Z

The documentation for Pandas has numerous examples of best practices for working with data stored in various formats.However, I am unable to find any good examples for working with databases like MySQL for example.Can anyone point me to links or give some code snippets of how to convert query results using mysql-python to data frames in Pandas efficiently ?As Wes says, io/sql's read_sql will do it, once you've gotten a database connection using a DBI compatible library.  We can look at two short examples using the MySQLdb and cx_Oracle libraries to connect to Oracle and MySQL and query their data dictionaries. Here is the example for cx_Oracle:And here is the equivalent example for MySQLdb:For recent readers of this question: pandas have the following warning in their docs for version 14.0:And:This makes many of the answers here outdated. You should use sqlalchemy:For the record, here is an example using a sqlite database:I prefer to create queries with SQLAlchemy, and then make a DataFrame from it. SQLAlchemy makes it easier to combine SQL conditions Pythonically if you intend to mix and match things over and over.MySQL example:The same syntax works for Ms SQL server using podbc also. And this is how you connect to PostgreSQL using psycopg2 driver (install with "apt-get install python-psycopg2" if you're on Debian Linux derivative OS).For Sybase the following works (with http://python-sybase.sourceforge.net)pandas.io.sql.frame_query is deprecated. Use pandas.read_sql instead.That works just fine and using pandas.io.sql frame_works (with the deprecation warning). Database used is the sample database from mysql tutorial.This should work just fine.This helped for me for connecting to AWS MYSQL(RDS) from python 3.x based lambda function and loading into a pandas DataFrameFor Postgres users

Numpy how to iterate over columns of array?

User

[Numpy how to iterate over columns of array?](https://stackoverflow.com/questions/10148818/numpy-how-to-iterate-over-columns-of-array)

Suppose I have and m x n array.  I want to pass each column of this array to a function to perform some operation on the entire column.  How do I iterate over the columns of the array?For example, I have a 4 x 3 array likewhere column would be "1,2,3,4" in the first iteration, "99,14,12,43" in the second, and "2,5,7,1" in the third.

2012-04-13 21:55:04Z

Suppose I have and m x n array.  I want to pass each column of this array to a function to perform some operation on the entire column.  How do I iterate over the columns of the array?For example, I have a 4 x 3 array likewhere column would be "1,2,3,4" in the first iteration, "99,14,12,43" in the second, and "2,5,7,1" in the third.Just iterate over the transposed of your array:This should give you a startFor a three dimensional array you could try:See the docs on how array.transpose works. Basically you are specifying which dimension to shift. In this case we are shifting the second dimension (e.g. columns) to the first dimension.You can also use unzip to iterate through the columnsFor example you want to find a mean of each column in matrix. Let's create the following matrixThe function for mean isTo do what is needed and store result in colon vector 'results'The results are: 

array([4.33333333, 5.        , 5.66666667, 4.        ])

Set attributes from dictionary in python

OscarRyz

[Set attributes from dictionary in python](https://stackoverflow.com/questions/2466191/set-attributes-from-dictionary-in-python)

Is it possible to create an object from a dictionary in python in such a way that each key is an attribute of that object?Something like this:I think it would be pretty much the inverse of this question: Python dictionary from an object's fields

2010-03-17 21:54:59Z

Is it possible to create an object from a dictionary in python in such a way that each key is an attribute of that object?Something like this:I think it would be pretty much the inverse of this question: Python dictionary from an object's fieldsSure, something like this:UpdateAs Brent Nash suggests, you can make this more flexible by allowing keyword arguments as well:Then you can call it like this:or like this:or even like this:Setting attributes in this way is almost certainly not the best way to solve a problem. Either:Another solution that is basically equivalent to case 1 is to use a collections.namedtuple. See van's answer for how to implement that.You can access the attributes of an object with __dict__, and call the update method on it:Why not just use attribute names as keys to a dictionary?You can initialize with named arguments, a list of tuples, or a dictionary, or

individual attribute assignments, e.g.:Alternatively, instead of raising the attribute error, you can return None for unknown values.  (A trick used in the web2py storage class)I think that answer using settattr are the way to go if you really need to support dict.  But if Employee object is just a structure which you can access with dot syntax (.name) instead of dict syntax (['name']), you can use namedtuple like this:You do have _asdict() method to access all properties as dictionary, but you cannot add additional attributes later, only during the construction.say for exampleif you want to set the attributes at oncesimilar to using a dict, you could just use kwargs like so:

Python name mangling

Paul Manta

[Python name mangling](https://stackoverflow.com/questions/7456807/python-name-mangling)

In other languages, a general guideline that helps produce better code is always make everything as hidden as possible. If in doubt about whether a variable should be private or protected, it's better to go with private.Does the same hold true for Python? Should I use two leading underscores on everything at first, and only make them less hidden (only one underscore) as I need them?If the convention is to use only one underscore, I'd also like to know the rationale.Here's a comment I left on JBernardo's answer. It explains why I asked this question and also why I'd like to know why Python is different from the other languages:

2011-09-17 18:07:49Z

In other languages, a general guideline that helps produce better code is always make everything as hidden as possible. If in doubt about whether a variable should be private or protected, it's better to go with private.Does the same hold true for Python? Should I use two leading underscores on everything at first, and only make them less hidden (only one underscore) as I need them?If the convention is to use only one underscore, I'd also like to know the rationale.Here's a comment I left on JBernardo's answer. It explains why I asked this question and also why I'd like to know why Python is different from the other languages:When in doubt, leave it "public" - I mean, do not add anything to obscure the name of your attribute. If you have a class with some internal value, do not bother about it. Instead of writing:write this by default:This is for sure a controversial way of doing things. Python newbies just hate it and even some old Python guys despise this default - but it is the default anyway, so I really recommend you to follow it, even if you feel uncomfortable.If you really want to send the message "Can't touch this!" to your users, the usual way is to precede the variable with one underscore. This is just a convention, but people understand it and take double care when dealing with such stuff:This can be useful, too, for avoiding conflict between property names and attribute names:What about the double underscore? Well, the double underscore magic is used mainly to avoid accidental overloading of methods and name conflicts with superclasses' attributes. It can be quite useful if you write a class that is expected to be extended many times.If you want to use it for other purposes, you can, but it is neither usual nor recommended.EDIT: Why is this so? Well, the usual Python style does not emphasize making things private - on the contrary! There are a lot of reasons for that - most of them controversial... Let us see some of them.Most OO languages today use the opposite approach: what should not be used should not be visible, so attributes should be private. Theoretically, this would yield more manageable, less coupled classes, because no one would change values inside the objects recklessly.However, it is not so simple. For example, Java classes do have a lot attributes and getters that just get the values and setters that just set the values. You need, let us say, seven lines of code to declare a single attribute - which a Python programmer would say is needlessly complex. Also, in practice, you just write this whole lot of code to get one public field, since you can change its value using the getters and setters.So why to follow this private-by-default policy? Just make your attributes public by default. Of course, this is problematic in Java, because if you decide to add some validation to your attribute, it would require you to change allin your code to, let us say,setAge() being:So in Java (and other languages), the default is to use getters and setters anyway, because they can be annoying to write but can spare you a lot of time if you find yourself in the situation I've described.However, you do not need to do it in Python, since Python has properties. If you have this class:and then you decide to validate ages, you do not need to change the person.age = age pieces of your code. Just add a property (as shown below)If you can do it and still use  person.age = age, why would you add private fields and getters and setters?(Also, see Python is not Java and this article about the harms of using getters and setters.).Even in languages where there are private attributes, you can access them through some kind of reflection/introspection library. And people do it a lot, in frameworks and for solving urgent needs. The problem is that introspection libraries are just a hard way of doing what you could do with public attributes.Since Python is a very dynamic language, it is just counterproductive to add this burden to your classes.For a Pythonista, encapsulation is not the inability of seeing the internals of classes, but the possibility of avoiding looking at it. What I mean is, encapsulation is the property of a component which allows it to be used without the user being concerned about the internal details. If you can use a component without bothering yourself about its implementation, then it is encapsulated (in the opinion of a Python programmer).Now, if you wrote your class in such a way you can use it without having to think about implementation details, there is no problem if you want to look inside the class for some reason. The point is: your API should be good and the rest is details.Well, this is not controversial: he said so, actually. (Look for "open kimono.")Yes, there are some reasons, but no critical reason. This is mostly a cultural aspect of programming in Python. Frankly, it could be the other way, too - but it is not. Also, you could just as easily ask the other way around: why do some languages use private attributes by default? For the same main reason as for the Python practice: because it is the culture of these languages, and each choice has advantages and disadvantages.Since there already is this culture, you are well advised to follow it. Otherwise, you will get annoyed by Python programmers telling you to remove the __ from your code when you ask a question in Stack Overflow :)Name mangling is invoked when you are in a class definition and use __any_name or __any_name_, that is, two (or more) leading underscores and at most one trailing underscore. And now:The ostensible use is to prevent subclassers from using an attribute that the class uses. A potential value is in avoiding name collisions with subclassers who want to override behavior, so that the parent class functionality keeps working as expected. However, the example in the Python documentation is not Liskov substitutable, and no examples come to mind where I have found this useful. The downsides are that it increases cognitive load for reading and understanding a code base, and especially so when debugging where you see the double underscore name in the source and a mangled name in the debugger.My personal approach is to intentionally avoid it. I work on a very large code base. The rare uses of it stick out like a sore thumb and do not seem justified. You do need to be aware of it so you know it when you see it.PEP 8, the Python standard library style guide, currently says (abridged):If you prepend two underscores (without ending double-underscores) in a class definition, the name will be mangled, and an underscore followed by the class name will be prepended on the object:Note that names will only get mangled when the class definition is parsed:Also, those new to Python sometimes have trouble understanding what's going on when they can't manually access a name they see defined in a class definition. This is not a strong reason against it, but it's something to consider if you have a learning audience.When my intention is for users to keep their hands off an attribute, I tend to only use the one underscore, but that's because in my mental model, subclassers would have access to the name (which they always have, as they can easily spot the mangled name anyways). If I were reviewing code that uses the __ prefix, I would ask why they're invoking name mangling, and if they couldn't do just as well with a single underscore, keeping in mind that if subclassers choose the same names for the class and class attribute there will be a name collision in spite of this.I wouldn't say that practice produces better code. Visibility modifiers only distract you from the task at hand, and as a side effect force your interface to be used as you intended. Generally speaking, enforcing visibility prevents programmers from messing things up if they haven't read the documentation properly.A far better solution is the route that Python encourages: Your classes and variables should be well documented, and their behaviour clear. The source should be available. This is far more extensible and reliable way to write code.My strategy in Python is this:Above all, it should be clear what everything does. Document it if someone else will be using it. Document it if you want it to be useful in a year's time.As a side note, you should actually be going with protected in those other languages: You never know your class might be inherited later and for what it might be used. Best to only protect those variables that you are certain cannot or should not be used by foreign code.You shouldn't start with private data and make it public as necessary. Rather, you should start by figuring out the interface of your object. I.e. you should start by figuring out what the world sees (the public stuff) and then figure out what private stuff is necessary for that to happen.Other language make difficult to make private that which once was public. I.e. I'll break lots of code if I make my variable private or protected. But with properties in python this isn't the case. Rather, I can maintain the same interface even with rearranging the internal data.The difference between _ and __ is that python actually makes an attempt to enforce the latter. Of course, it doesn't try really hard but it does make it difficult. Having _ merely tells other programmers what the intention is, they are free to ignore at their peril. But ignoring that rule is sometimes helpful. Examples include debugging, temporary hacks, and working with third party code that wasn't intended to be used the way you use it. There are already a lot of good answers to this, but I'm going to offer another one. This is also partially a response to people who keep saying that double underscore isn't private (it really is).If you look at Java/C#, both of them have private/protected/public. All of these are compile-time constructs. They are only enforced at the time of compilation. If you were to use reflection in Java/C#, you could easily access private method.Now every time you call a function in Python, you are inherently using reflection. These pieces of code are the same in Python.The "dot" syntax is only syntactic sugar for the latter piece of code. Mostly because using getattr is already ugly with only one function call. It just gets worse from there.So with that, there can't be a Java/C# version of private, as Python doesn't compile the code. Java and C# can't check if a function is private or public at runtime, as that information is gone (and it has no knowledge of where the function is being called from).Now with that information, the name mangling of the double underscore makes the most sense for achieving "private-ness". Now when a function is called from the 'self' instance and it notices that it starts with '__', it just performs the name mangling right there. It's just more syntactic sugar. That syntactic sugar allows the equivalent of 'private' in a language that only uses reflection for data member access.Disclaimer: I have never heard anybody from the Python development say anything like this. The real reason for the lack of "private" is cultural, but you'll also notice that most scripting/interpreted languages have no private. A strictly enforceable private is not practical at anything except for compile time.First: Why do you want to hide your data? Why is that so important?Most of the time you don't really want to do it but you do because others are doing.If you really really really don't want people using something, add one underscore in front of it. That's it... Pythonistas know that things with one underscore is not guaranteed to work every time and may change without you knowing.That's the way we live and we're okay with that.Using two underscores will make your class so bad to subclass that even you will not want to work that way.The chosen answer does a good job of explaining how properties remove the need for private attributes, but I would also add that functions at the module level remove the need for private methods.If you turn a method into a function at the module level, you remove the opportunity for subclasses to override it. Moving some functionality to the module level is more Pythonic than trying to hide methods with name mangling.Following code snippet will explain all different cases :printing all valid attributes of Test ObjectHere, you can see that name of __a has been changed to _Test__a to prevent this variable to be overridden by any of the subclass. This concept is known as "Name Mangling" in python.

You can access this like this :Similarly, in case of _a, the variable is just to notify the developer that it should be used as internal variable of that class, the python interpreter  won't do anything even if you access it, but it is not a good practise. a variable can be accesses from anywhere it's like a public class variable.Hope the answer helped you :)At first glance it should be the same as for other languages (under "other" I mean Java or C++), but it isn't.In Java you made private all variables that shouldn't be accessible outside.  In the same time in Python you can't achieve this since there is no "privateness" (as one of Python principles says - "We're all adults").  So double underscore means only "Guys, do not use this field directly".  The same meaning has singe underscore, which in the same time doesn't cause any headache when you have to inherit from considered class (just an example of possible problem caused by double underscore).So, I'd recommend you to use single underscore by default for "private" members."If in doubt about whether a variable should be private or protected, it's better to go with private." - yes, same holds in Python.Some answers here say about 'conventions', but don't give the links to those conventions. The authoritative guide for Python, PEP 8 states explicitly:The distinction between public and private, and name mangling in Python have been considered in other answers. From the same link,

Character reading from file in Python

Graviton

[Character reading from file in Python](https://stackoverflow.com/questions/147741/character-reading-from-file-in-python)

In a text file, there is a string "I don't like this".However, when I read it into a string, it becomes "I don\xe2\x80\x98t like this". I understand that \u2018 is the unicode representation of "'". I use command to do the reading.Now, is it possible to read the string in such a way that when it is read into the string, it is "I don't like this", instead of "I don\xe2\x80\x98t like this like this"?Second edit: I have seen some people use mapping to solve this problem, but really, is there no built-in conversion that does this kind of ANSI to unicode ( and vice versa) conversion?

2008-09-29 06:47:47Z

In a text file, there is a string "I don't like this".However, when I read it into a string, it becomes "I don\xe2\x80\x98t like this". I understand that \u2018 is the unicode representation of "'". I use command to do the reading.Now, is it possible to read the string in such a way that when it is read into the string, it is "I don't like this", instead of "I don\xe2\x80\x98t like this like this"?Second edit: I have seen some people use mapping to solve this problem, but really, is there no built-in conversion that does this kind of ANSI to unicode ( and vice versa) conversion?Ref: http://docs.python.org/howto/unicodeReading Unicode from a file is therefore simple:It's also possible to open files in update mode, allowing both reading and writing:EDIT: I'm assuming that your intended goal is just to be able to read the file properly into a string in Python. If you're trying to convert to an ASCII string from Unicode, then there's really no direct way to do so, since the Unicode characters won't necessarily exist in ASCII.If you're trying to convert to an ASCII string, try one of the following: There are a few points to consider.A \u2018 character may appear only as a fragment of representation of a unicode string in Python, e.g. if you write:Now if you simply want to print the unicode string prettily, just use unicode's encode method:To make sure that every line from any file would be read as unicode, you'd better use the codecs.open function instead of just open, which allows you to specify file's encoding:But it really is "I don\u2018t like this" and not "I don't like this". The character u'\u2018' is a completely different character than "'" (and, visually, should correspond more to '`').If you're trying to convert encoded unicode into plain ASCII, you could perhaps keep a mapping of unicode punctuation that you would like to translate into ASCII.There are an awful lot of punctuation characters in unicode, however, but I suppose you can count on only a few of them actually being used by whatever application is creating the documents you're reading.Leaving aside the fact that your text file is broken (U+2018 is a left quotation mark, not an apostrophe): iconv can be used to transliterate unicode characters to ascii.You'll have to google for "iconvcodec", since the module seems not to be supported anymore and I can't find a canonical home page for it.Alternatively you can use the iconv command line utility to clean up your file:There is a possibility that somehow you have a non-unicode string with unicode escape characters, e.g.:This actually happened to me once before. You can use a unicode_escape codec to decode the string to unicode and then encode it to any format you want:It is also possible to read an encoded text file using the python 3 read method:With this variation, there is no need to import any additional librariesThis is Pythons way do show you unicode encoded strings. But i think you should be able to print the string on the screen or write it into a new file without any problems.Actually, U+2018 is the Unicode representation of the special character ‘ . If you want, you can convert instances of that character to U+0027 with this code:In addition, what are you using to write the file? f1.read() should return a string that looks like this:If it's returning this string, the file is being written incorrectly:

ImportError: No module named six

asadullah07

[ImportError: No module named six](https://stackoverflow.com/questions/13967428/importerror-no-module-named-six)

I'm trying to build OpenERP project, done with dependencies. It's giving this error nowCould someone guide what's wrong and how it can be fixed???

2012-12-20 07:45:24Z

I'm trying to build OpenERP project, done with dependencies. It's giving this error nowCould someone guide what's wrong and how it can be fixed???You probably don't have the six Python module installed. You can find it on pypi.To install it:(if you have pip installed, use pip install six instead)If pip "says" six is installed but you're still getting:ImportError: No module named six.movestry re-installing six (worked for me):On Ubuntu and Debiandoes the trick.Use sudo apt-get install python-six if you get an error saying "permission denied".For Mac OS X:Source: 1233 thumbs up on this commenton Ubuntu Bionic (18.04), six is already install for python2 and python3 but I have the error launching Wammu.

@3ygun solution worked for me to solvewhen launching WammuIf it's occurred for python3 program, six come withand if you don't have pip3:with sudo under Ubuntu!

Which is more preferable to use: lambda functions or nested functions ('def')?

Ray Vega

[Which is more preferable to use: lambda functions or nested functions ('def')?](https://stackoverflow.com/questions/134626/which-is-more-preferable-to-use-lambda-functions-or-nested-functions-def)

I mostly use lambda functions but sometimes use nested functions that seem to provide the same behavior. Here are some trivial examples where they functionally do the same thing if either were found within another function:Lambda functionNested functionAre there advantages to using one over the other? (Performance? Readability? Limitations? Consistency? etc.) Does it even matter? If it doesn't then does that violate the Pythonic principle: 

2008-09-25 17:15:03Z

I mostly use lambda functions but sometimes use nested functions that seem to provide the same behavior. Here are some trivial examples where they functionally do the same thing if either were found within another function:Lambda functionNested functionAre there advantages to using one over the other? (Performance? Readability? Limitations? Consistency? etc.) Does it even matter? If it doesn't then does that violate the Pythonic principle: If you need to assign the lambda to a name, use a def instead. defs are just syntactic sugar for an assignment, so the result is the same, and they are a lot more flexible and readable.lambdas can be used for use once, throw away functions which won't have a name.However, this use case is very rare. You rarely need to pass around unnamed function objects.The builtins map() and filter() need function objects, but list comprehensions and generator expressions are generally more readable than those functions and can cover all use cases, without the need of lambdas. For the cases you really need a small function object, you should use the operator module functions, like operator.add instead of lambda x, y: x + yIf you still need some lambda not covered, you might consider writing a def, just to be more readable. If the function is more complex than the ones at operator module, a def is probably better. So, real world good lambda use cases are very rare.Practically speaking, to me there are two differences:The first is about what they do and what they return:Hence, if you need to call a function that takes a function object, the only way to do that in one line of python code is with a lambda.  There's no equivalent with def.In some frameworks this is actually quite common; for example, I use Twisted a lot, and so doing something likeis quite common, and more concise with lambdas.The second difference is about what the actual function is allowed to do.For example,works as expected, whileis a SyntaxError.Of course, there are workarounds - substitute print with sys.stdout.write, or import with __import__.  But usually you're better off going with a function in that case.In this interview,  Guido van Rossum says he wishes he hadn't let 'lambda' into Python:IMHO, Iambdas can be convenient sometimes, but usually are convenient at the expense of readibility. Can you tell me what this does:I wrote it, and it took me a minute to figure it out. This is from Project Euler - i won't say which problem because i hate spoilers, but it runs in 0.124 seconds :)For n=1000 here's some timeit's of calling a function vs a lambda:I agree with nosklo's advice: if you need to give the function a name, use def.  I reserve lambda functions for cases where I'm just passing a brief snippet of code to another function, e.g.:There is one advantage to using a lambda over a regular function: they are created in an expression.There are several drawbacks:They are also both the same type of object. For those reasons, I generally prefer to create functions with the def keyword instead of with lambdas.A lambda results in the same type of object as a regular functionSince lambdas are functions, they're first-class objects.Both lambdas and functions:But lambdas are, by default, missing some things that functions get via full function definition syntax.Lambdas are anonymous functions, after all, so they don't know their own name.Thus lambda's can't be looked up programmatically in their namespace.This limits certain things. For example, foo can be looked up with serialized code, while l cannot:We can lookup foo just fine - because it knows its own name:Basically, lambdas are not documented. Let's rewrite foo to be better documented:Now, foo has documentation:Whereas, we don't have the same mechanism to give the same information to lambdas:But we can hack them on:But there's probably some error messing up the output of help, though.Lambdas can't return complex statements, only expressions.Expressions can admittedly be rather complex, and if you try very hard you can probably accomplish the same with a lambda, but the added complexity is more of a detriment to writing clear code. We use Python for clarity and maintainability. Overuse of lambdas can work against that.This is the only possible upside. Since you can create a lambda with an expression, you can create it inside of a function call. Creating a function inside a function call avoids the (inexpensive) name lookup versus one created elsewhere. However, since Python is strictly evaluated, there is no other performance gain to doing so aside from avoiding the name lookup.For a very simple expression, I might choose a lambda.I also tend to use lambdas when doing interactive Python, to avoid multiple lines when one will do. I use the following sort of code format when I want to pass in an argument to a constructor when calling timeit.repeat:And now:I believe the slight time difference above can be attributed to the name lookup in return_nullary_function - note that it is very negligible.Lambdas are good for informal situations where you want to minimize lines of code in favor of making a singular point.Lambdas are bad for more formal situations where you need clarity for editors of code who will come later, especially in cases where they are non-trivial.We know we are supposed to give our objects good names. How can we do so when the object has no name?For all of these reasons, I generally prefer to create functions with def instead of with lambda.Performance:Creating a function with lambda is slightly faster than creating it with def. The difference is due to def creating a name entry in the locals table. The resulting function has the same execution speed.Readability:Lambda functions are somewhat less readable for most Python users, but also much more concise in some circumstances. Consider converting from using non-functional to functional routine:As you can see, the lambda version is shorter and "easier" in the sense that you only need to add lambda v: to the original non-functional version to convert to the functional version. It's also a lot more concise. But remember, a lot of Python users will be confused by the lambda syntax, so what you lose in length and real complexity might be gained back in confusion from fellow coders.Limitations:Consistency:Python mostly avoids functional programming conventions in favor of procedural and simpler objective semantics. The lambda operator stands in direct contrast to this bias. Moreover, as an alternative to the already prevalent def, the lambda function adds diversity to your syntax. Some would consider that less consistent.Pre-existing functions:As noted by others, many uses of lambda in the field can be replaced by members of the operator or other modules. For instance:Using the pre-existing function can make code more readable in many cases.The Pythonic principle:「There should be one—and preferably only one—obvious way to do it」That's similar to the single source of truth doctrine. Unfortunately, the single-obvious-way-to-do-it principle has always been more an wistful aspiration for Python, rather than a true guiding principal. Consider the very-powerful array comprehensions in Python. They are functionally equivalent to the map and filter functions:lambda and def are the same.It's a matter of opinion, but I would say that anything in the Python language intended for general use which doesn't obviously break anything is "Pythonic" enough.While agreeing with the other answers, sometimes it's more readable. Here's an example where lambda comes in handy, in a use case I keep encountering of an N dimensional defaultdict.Here's an example:I find it more readable than creating a def for the second dimension. This is even more significant for higher dimensions.The primary use of lambda has always been for simple callback functions, and for map, reduce, filter, which require a function as an argument. With list comprehensions becoming the norm, and the added allowed if as in:it's hard to imagine a real case for the use of lambda in daily use. As a result, I'd say, avoid lambda and create nested functions.An important limitation of lambdas is that they cannot contain anything besides an expression.  It's nearly impossible for a lambda expression to produce anything besides trivial side effects, since it cannot have anywhere near as rich a body as a def'ed function.That being said, Lua influenced my programming style toward the extensive use of anonymous functions, and I litter my code with them.  On top of that, I tend to think about map/reduce as abstract operators in ways I don't consider list comprehensions or generators, almost as If I'm deferring an implementation decision explicitly by using those operators.  Edit: This is a pretty old question, and my opinions on the matter have changed, somewhat.First off, I am strongly biased against assigning a lambda expression to a variable; as python has a special syntax just for that (hint, def).  In addition to that, many of the uses for lambda, even when they don't get a name, have predefined (and more efficient) implementations.  For instance, the example in question can be abbreviated to just (1).__add__, without the need to wrap it in a lambda or def.  Many other common uses can be satisfied with some combination of the operator, itertools and functools modules.Considering a simple example,If you are just going to assign the lambda to a variable in the local scope, you may as well use def because it is more readable and can be expanded more easily in the future:orOne use for lambdas I have found... is in debug messages.Since lambdas can be lazily evaluated you can have code like this:instead of possibly expensive:which processes the format string even if the debug call does not produce output because of current logging level.Of course for it to work as described the logging module in use must support lambdas as "lazy parameters" (as my logging module does).The same idea may be applied to any other case of lazy evaluation for on demand content value creation.For example this custom ternary operator:instead of:with lambdas only the expression selected by the condition will be evaluated, without lambdas both will be evaluated.Of course you could simply use functions instead of lambdas, but for short expressions lambdas are (c)leaner.I agree with nosklo. By the way, even with a use once, throw away function, most of the time you just want to use something from the operator module.E.G : You have a function with this signature : myFunction(data, callback function).You want to pass a function that add 2 elements.Using lambda :The pythonic way :Or course this is a simple example, but there is a lot of stuff the operator module provides, including the items setters  / getters for list and dict. Really cool.A major difference is that you can not use def functions inline, which is in my opinion the most convenient use case for a lambda function. For example when sorting a list of objects:I would therefore suggest keeping the use of lambdas to this kind of trivial operations, which also do not really benefit from the automatic documentation provided by naming the function. lambda is useful for generating new functions:

Elegant setup of Python logging in Django

Parand

[Elegant setup of Python logging in Django](https://stackoverflow.com/questions/1598823/elegant-setup-of-python-logging-in-django)

I have yet to find a way of setting up Python logging with Django that I'm happy with. My requirements are fairly simple:My current setup is to use a logging.conf file and setup logging in each module I log from. It doesn't feel right. Do you have a logging setup that you like? Please detail it: how do you setup the configuration (do you use logging.conf or set it up in code), where/when do you initiate the loggers, and how do you get access to them in your modules, etc.

2009-10-21 05:07:01Z

I have yet to find a way of setting up Python logging with Django that I'm happy with. My requirements are fairly simple:My current setup is to use a logging.conf file and setup logging in each module I log from. It doesn't feel right. Do you have a logging setup that you like? Please detail it: how do you setup the configuration (do you use logging.conf or set it up in code), where/when do you initiate the loggers, and how do you get access to them in your modules, etc.The best way I've found so far is to initialize logging setup in settings.py - nowhere else. You can either use a configuration file or do it programmatically step-by-step - it just depends on your requirements. The key thing is that I usually add the handlers I want to the root logger, using levels and sometimes logging.Filters to get the events I want to the appropriate files, console, syslogs etc. You can of course add handlers to any other loggers too, but there isn't commonly a need for this in my experience.In each module, I define a logger usingand use that for logging events in the module (and, if I want to differentiate further) use a logger which is a child of the logger created above.If my app is going to be potentially used in a site which doesn't configure logging in settings.py, I define a NullHandler somewhere as follows:and ensure that an instance of it is added to all loggers created in the modules in my apps which use logging. (Note: NullHandler is already in the logging package for Python 3.1, and will be in Python 2.7.) So:This is done to ensure that your modules play nicely in a site which doesn't configure logging in settings.py, and that you don't get any annoying "No handlers could be found for logger X.Y.Z" messages (which are warnings about potentially misconfigured logging).Doing it this way meets your stated requirements:Update: Note that as of version 1.3, Django now incorporates support for logging.I know this is a solved answer already, but as per django >= 1.3 there's a new logging setting.Moving from old to new is not automatic, so I thought i'll write it down here.And of course checkout the django doc for some more.This is the basic conf, created by default with django-admin createproject v1.3 - mileage might change with latest django versions:This structure is based upon the standard Python logging dictConfig, that dictates the following blocks:I usually do at least this:Which translates into:editSee request exceptions are now always logged and Ticket #16288: I updated the above sample conf to explicitly include the correct filter for mail_admins so that, by default, emails are not sent when debug is True.You should add a filter:and apply it to the mail_admins handler:Otherwise the django.core.handers.base.handle_uncaught_exception doesn't pass errors to the 'django.request' logger if settings.DEBUG is True.If you don't do this in Django 1.5 you'll get a but things will still work correctly BOTH in django 1.4 and django 1.5.** end edit **That conf is strongly inspired by the sample conf in the django doc, but adding the log file part.I often also do the following:Then in my python code I always add a NullHandler in case no logging conf is defined whatsoever. This avoid warnings for no Handler specified. Especially useful for libs that are not necessarily called only in Django (ref)[...]Hope this helps!We initialize logging in the top-level urls.py by using a logging.ini file.The location of the logging.ini is provided in settings.py, but that's all.Each module then doesTo distinguish testing, development and production instances, we have different logging.ini files.  For the most part, we have a "console log" that goes to stderr with Errors only.  We have an "application log" that uses a regular rolling log file that goes to a logs directory.I am currently using a logging system, which I created myself. It uses CSV format for logging.django-csvlogThis project still doesn't have full documentation, but I am working on it.

How do you set your pythonpath in an already-created virtualenv?

TIMEX

[How do you set your pythonpath in an already-created virtualenv?](https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv)

What file do I edit, and how? I created a virtual environment.

2011-01-21 09:24:23Z

What file do I edit, and how? I created a virtual environment.EDIT #2The right answer is @arogachev's one.If you want to change the PYTHONPATH used in a virtualenv, you can add the following line to your virtualenv's bin/activate file:This way, the new PYTHONPATH will be set each time you use this virtualenv.EDIT: (to answer @RamRachum's comment)To have it restored to its original value on deactivate, you could addbefore the previously mentioned line, and add the following line to your bin/postdeactivate script.The comment by @s29 should be an answer:One way to add a directory to the virtual environment is to install virtualenvwrapper (which is useful for many things) and then doIf you want to remove these path edit the file myenvhomedir/lib/python2.7/site-packages/_virtualenv_path_extensions.pthDocumentation on virtualenvwrapper can be found at http://virtualenvwrapper.readthedocs.org/en/latest/Specific documentation on this feature can be found at

http://virtualenvwrapper.readthedocs.org/en/latest/command_ref.html?highlight=add2virtualenvYou can create a .pth file that contains the directory to search for, and place it in the site-packages directory. E.g.:The effect is the same as adding /some/library/path to sys.path, and remain local to the virtualenv setup.I modified my activate script to source the file .virtualenvrc, if it exists in the current directory, and to save/restore PYTHONPATH on activate/deactivate.You can find the patched activate script here.. It's a drop-in replacement for the activate script created by virtualenv 1.11.6.Then I added something like this to my .virtualenvrc:It's already answered here -> Is my virtual environment (python) causing my PYTHONPATH to break?Add "export PYTHONPATH=/usr/local/lib/python2.0" this to ~/.bashrc file and source it by typing "source ~/.bashrc" OR ". ~/.bashrc".1) Go to the Control panel

2) Double click System

3) Go to the Advanced tab

4) Click on Environment VariablesIn the System Variables window, check if you have a variable named PYTHONPATH. If you have one already, check that it points to the right directories. If you don't have one already,  click the New button and create it.Alternatively, you can also do below your code:-

What's the difference of ContentType and MimeType

Frangossauro

[What's the difference of ContentType and MimeType](https://stackoverflow.com/questions/3452381/whats-the-difference-of-contenttype-and-mimetype)

As far as I know, they are absolute equal. However, browsing some django docs, I've

found this piece of code:HttpResponse.__init__(content='', mimetype=None, status=200, content_type='text/html')which surprise me the two getting along each other. The official docs was able to solve the issue in a pratical manner:However, I don't find it elucidating enough. Why we use 2 different naming for (almost the same) thing? Is "Content-Type" just a name used in browser requests, and with very little use outside it?What's the main difference between the each one, and when is right to call something mimetype as opposed to content-type ? Am I being pitty and grammar nazi?

2010-08-10 18:54:58Z

As far as I know, they are absolute equal. However, browsing some django docs, I've

found this piece of code:HttpResponse.__init__(content='', mimetype=None, status=200, content_type='text/html')which surprise me the two getting along each other. The official docs was able to solve the issue in a pratical manner:However, I don't find it elucidating enough. Why we use 2 different naming for (almost the same) thing? Is "Content-Type" just a name used in browser requests, and with very little use outside it?What's the main difference between the each one, and when is right to call something mimetype as opposed to content-type ? Am I being pitty and grammar nazi?The reason isn't only backward compatibility, and I'm afraid the usually excellent Django documentation is a bit hand-wavy about it. MIME (it's really worth reading at least the Wikipedia entry) has its origin in extending internet mail, and specifically SMTP. From there, the MIME and MIME-inspired extension design has found its way into a lot of other protocols (such as HTTP here), and is still being used when new kinds of metadata or data need to be transmitted in an existing protocol. There are dozens of RFCs that discuss MIME used for a plethora of purposes.Specifically, Content-Type: is one among several MIME headers. "Mimetype" does indeed sound obsolete, but a reference to MIME itself isn't. Call that part backward-compatibility, if you will.[BTW, this is purely a terminology problem which has nothing whatsoever to do with grammar. Filing every usage question under "grammar" is a pet peeve of mine. Grrrr.]I've always viewed contentType to be a superset of mimeType.  The only difference being the optional character set encoding.  If the contentType does not include an optional character set encoding then it is identical to a mimeType.  Otherwise, the mimeType is the data prior to the character set encoding sequence.E.G. text/html; charset=UTF-8text/html is the mimeType

; is the additional parameters indicator

charset=UTF-8 is the character set encoding parameterE.G. application/mswordapplication/msword is the mimeType

It cannot have a character set encoding as it describes a well formed octet-stream not comprising characters directly.If you want to know the details see ticket 3526.Quote:Backwards compatibility, based on your quote from the documentation.

PEP 8, why no spaces around '=' in keyword argument or a default parameter value?

soulcheck

[PEP 8, why no spaces around '=' in keyword argument or a default parameter value?](https://stackoverflow.com/questions/8853063/pep-8-why-no-spaces-around-in-keyword-argument-or-a-default-parameter-value)

Why does PEP 8 recommend not having spaces around = in a keyword argument or a default parameter value?Is this inconsistent with recommending spaces around every other occurrence of = in Python code?How is:better than:Any links to discussion/explanation by Python's BDFL will be appreciated.Mind, this question is more about kwargs than default values, i just used the phrasing from PEP 8.I'm not soliciting opinions. I'm asking for reasons behind this decision. It's more like asking why would I use { on the same line as if statement in a C program, not whether I should use it or not.

2012-01-13 15:37:34Z

Why does PEP 8 recommend not having spaces around = in a keyword argument or a default parameter value?Is this inconsistent with recommending spaces around every other occurrence of = in Python code?How is:better than:Any links to discussion/explanation by Python's BDFL will be appreciated.Mind, this question is more about kwargs than default values, i just used the phrasing from PEP 8.I'm not soliciting opinions. I'm asking for reasons behind this decision. It's more like asking why would I use { on the same line as if statement in a C program, not whether I should use it or not.I guess that it is because a keyword argument is essentially different than a variable assignment.For example, there is plenty of code like this:As you see, it makes completely sense to assign a variable to a keyword argument named exactly the same, so it improves readability to see them without spaces. It is easier to recognize that we are using keyword arguments and not assigning a variable to itself.Also, parameters tend to go in the same line whereas assignments usually are each one in their own line, so saving space is likely to be an important matter there.I wouldn't use very_long_variable_name as a default argument. So consider this:over this:Also, it doesn't make much sense to use variables as default values. Perhaps some constant variables (which aren't really constants) and in that case I would use names that are all caps, descriptive yet short as possible. So no another_very_...IMO leaving out the spaces for args provides cleaner visual grouping of the arg/value pairs; it looks less cluttered.There are pros and cons.I very much dislike how PEP8 compliant code reads. I don't buy into the argument that very_long_variable_name=another_very_long_variable_name can ever be more human readable than

very_long_variable_name = another_very_long_variable_name.

This is not how people read. It's an additional cognitive load, particularly in the absence of syntax highlighting.There is a significant benefit, however. If the spacing rules are adhered to, it makes searching for parameters exclusively using tools much more effective.I think there are several reasons for this, although I might just be rationalizing:For me it makes code more readable and is thus a good convention. I think the key difference in terms of style between variable assignments and function keyword assignments is that there should only be a single = on a line for the former, whereas generally there are multiple =s on a line for the latter.If there were no other considerations, we would prefer foo = 42 to foo=42, because the latter is not how equals signs are typically formatted, and because the former nicely visually separates the variable and value with whitespace.But when there are multiple assignments on one line, we prefer f(foo=42, bar=43, baz=44) to f(foo = 42, bar = 43, baz = 44), because the former visually separates the several assignments with whitespace, whereas the latter does not, making it a bit harder to see where the keyword/value pairs are.Here's another way of putting it: there is a consistency behind the convention. That consistency is this: the "highest level of separation" is made visually clearer via spaces. Any lower levels of separation are not (because it would be confused with the whitespace separating the higher level). For variable assignment, the highest level of separation is between variable and value. For function keyword assignment, the highest level of separation is between the individual assignments themselves.

Why is parenthesis in print voluntary in Python 2.7?

Hubro

[Why is parenthesis in print voluntary in Python 2.7?](https://stackoverflow.com/questions/6182964/why-is-parenthesis-in-print-voluntary-in-python-2-7)

In Python 2.7 both the following will do the sameHowever the following will notIn Python 3.x parenthesis on print is mandatory, essentially making it a function, but in 2.7 both will work with differing results. What else should I know about print in Python 2.7?

2011-05-31 04:18:27Z

In Python 2.7 both the following will do the sameHowever the following will notIn Python 3.x parenthesis on print is mandatory, essentially making it a function, but in 2.7 both will work with differing results. What else should I know about print in Python 2.7?In Python 2.x print is actually a special statement and not a function*.This is also why it can't be used like: lambda x: print xNote that (expr) does not create a Tuple (it results in expr), but , does. This likely results in the confusion between print (x) and print (x, y) in Python 2.7However, since print is a special syntax statement/grammar construct in Python 2.x then, without the parenthesis, it treats the ,'s in a special manner - and does not create a Tuple. This special treatment of the print statement enables it to act differently if there is a trailing , or not.Happy coding.*This print behavior in Python 2 can be changed to that of Python 3:It's all very simple and has nothing to do with forward or backward compatibility.The general form for the print statement in all Python versions before version 3 is:(Each expression in turn is evaluated, converted to a string and displayed with a space between them.)But remember that putting parentheses around an expression is still the same expression.So you can also write this as:This has nothing to do with calling a function.Here we have interesting side effect when it comes to UTF-8.The last print is tuple with hexadecimal byte values.Basically in Python before Python 3, print was a special statement that printed all the strings if got as arguments.  So print "foo","bar" simply meant "print 'foo' followed by 'bar'".  The problem with that was it was tempting to act as if print were a function, and the Python grammar is ambiguous on that, since (a,b) is a tuple containing a and b but foo(a,b) is a call to a function of two arguments.So they made the incompatible change for 3 to make programs less ambiguous and more regular.(Actually, I think 2.7 behaves as 2.6 did on this, but I'm not certain.)

Thread Safety in Python's dictionary

nmat

[Thread Safety in Python's dictionary](https://stackoverflow.com/questions/6953351/thread-safety-in-pythons-dictionary)

I have a class which holds a dictionaryAnd I am running 4 threads (one for each restaurant) that call the method OrderBook.addOrder. Here is the function ran by each thread:Is this safe, or do I have to use a lock before calling addOrder?

2011-08-05 08:16:37Z

I have a class which holds a dictionaryAnd I am running 4 threads (one for each restaurant) that call the method OrderBook.addOrder. Here is the function ran by each thread:Is this safe, or do I have to use a lock before calling addOrder?Python's built-in structures are thread-safe for single operations, but it can sometimes be hard to see where a statement really becomes multiple operations.Your code should be safe.  Keep in mind: a lock here will add almost no overhead, and will give you peace of mind.http://effbot.org/pyfaq/what-kinds-of-global-value-mutation-are-thread-safe.htm  has more details.Yes, built-in types are inherently thread-safe:

http://docs.python.org/glossary.html#term-global-interpreter-lock It is worth noting that Google's style guide advises against relying on dict atomicity, as I have explained in further detail at: Is Python variable assignment atomic?And I agree with this one: there is already the GIL in CPython, so the performance hit of using a Lock will be negligible. Much more costly will be the hours spent bug hunting in a complex codebase when those CPython implementation details change one day.

os.walk without digging into directories below

Setori

[os.walk without digging into directories below](https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below)

How do I limit os.walk to only return files in the directory I provide it?

2008-10-23 10:03:59Z

How do I limit os.walk to only return files in the directory I provide it?Use the walklevel function.It works just like os.walk, but you can pass it a level parameter that indicates how deep the recursion will go.Don't use os.walk.Example:I think the solution is actually very simple.use to only do first iteration of the for loop, there must be a more elegant way.The first time you call os.walk, it returns tulips for the current directory, then on next loop the contents of the next directory.  Take original script and just add a break.The suggestion to use listdir is a good one.  The direct answer to your question in Python 2 is root, dirs, files = os.walk(dir_name).next().The equivalent Python 3 syntax is root, dirs, files = next(os.walk(dir_name))You could use os.listdir() which returns a list of names (for both files and directories) in a given directory. If you need to distinguish between files and directories, call os.stat() on each name.If you have more complex requirements than just the top directory (eg ignore VCS dirs etc), you can also modify the list of directories to prevent os.walk recursing through them.ie:Note - be careful to mutate the list, rather than just rebind it.  Obviously os.walk doesn't know about the external rebinding.The same idea with listdir, but shorter:Felt like throwing my 2 pence in.In Python 3, I was able to do this:Since Python 3.5 you can use os.scandir instead of os.listdir. Instead of strings you get an iterator of DirEntry objects in return. From the docs:You can access the name of the object via DirEntry.name which is then equivalent to the output of os.listdirYou could also do the following:This is how I solved itThere is a catch when using listdir.  The os.path.isdir(identifier) must be an absolute path.  To pick subdirectories you do:The alternative is to change to the directory to do the testing without the os.path.join().You can use this snippetcreate a list of excludes, use fnmatch to skip the directory structure and do the processsame as for 'includes':Why not simply use a range and os.walk combined with the zip? Is not the best solution, but would work too.For example like this:Works for me on python 3.Also: A break is simpler too btw. (Look at the answer from @Pieter)A slight change to Alex's answer, but using __next__():print(next(os.walk('d:/'))[2])

or

   print(os.walk('d:/').__next__()[2])with the [2] being the file in root, dirs, file mentioned in other answersroot folder changes for every directory os.walk finds. I solver that checking if root == directory

Converting a column within pandas dataframe from int to string

Malfet

[Converting a column within pandas dataframe from int to string](https://stackoverflow.com/questions/17950374/converting-a-column-within-pandas-dataframe-from-int-to-string)

I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first the columns within the dataframe. To do that I have to convert an int column to str. 

I've tried to do as follows:or but in both cases it's not working and I'm getting an error saying "cannot concatenate 'str' and 'int' objects". Concatenating two str columns is working perfectly fine.

2013-07-30 14:53:03Z

I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first the columns within the dataframe. To do that I have to convert an int column to str. 

I've tried to do as follows:or but in both cases it's not working and I'm getting an error saying "cannot concatenate 'str' and 'int' objects". Concatenating two str columns is working perfectly fine.Convert a seriesDon't forget to assign the result back:Convert the whole frameChange data type of DataFrame column:To int:df.column_name = df.column_name.astype(np.int64)To str:df.column_name = df.column_name.astype(str)Warning: Both solutions given ( astype() and apply() ) do not preserve NULL values in either the nan or the None form.I believe this is fixed by the implementation of to_string()Use the following code:

How to pass a user defined argument in scrapy spider

L Lawliet

[How to pass a user defined argument in scrapy spider](https://stackoverflow.com/questions/15611605/how-to-pass-a-user-defined-argument-in-scrapy-spider)

I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?I read about a parameter -a somewhere but have no idea how to use it.

2013-03-25 09:35:13Z

I am trying to pass a user defined argument to a scrapy's spider. Can anyone suggest on how to do that?I read about a parameter -a somewhere but have no idea how to use it.Spider arguments are passed in the crawl command using the -a option. For example:Spiders can access arguments as attributes:Taken from the Scrapy doc: http://doc.scrapy.org/en/latest/topics/spiders.html#spider-argumentsUpdate 2013: Add second argumentUpdate 2015: Adjust wordingUpdate 2016: Use newer base class and add super, thanks @BirlaUpdate 2017: Use Python3 superUpdate 2018: As @eLRuLL points out, spiders can access arguments as attributesPrevious answers were correct, but you don't have to declare the constructor (__init__) every time you want to code a scrapy's spider, you could just specify the parameters as before:and in your spider code you can just use them as spider arguments:And it just works.To pass arguments with crawl commandTo pass arguments to run on scrapyd replace -a with -dThe spider will receive arguments in its constructor.Scrapy puts all the arguments as spider attributes and you can skip the init method completely. Beware use getattr method for getting those attributes so your code does not break.Spider arguments are passed while running the crawl command using the -a option. For example if i want to pass a domain name as argument to my spider then i will do this-And receive arguments in spider's constructors:...it will work :)Alternatively we can use ScrapyD which expose an API where we can pass the start_url and spider name. ScrapyD has api's to stop/start/status/list the spiders.scrapyd-deploy will deploy the spider in the form of egg into the daemon and even it maintains the version of the spider. While starting the spider you can mention which version of spider to use.curl http://localhost:6800/schedule.json -d project=default -d spider=testspider -d start_urls="https://www.anyurl...|https://www.anyurl2"Added advantage is you can build your own UI to accept the url and other params from the user and schedule a task using the above scrapyd schedule APIRefer scrapyd API documentation for more details

Quick and easy file dialog in Python?

Buttons840

[Quick and easy file dialog in Python?](https://stackoverflow.com/questions/9319317/quick-and-easy-file-dialog-in-python)

I have a simple script which parses a file and loads it's contents to a database. I don't need a UI, but right now I'm prompting the user for the file to parse using raw_input which is most unfriendly, especially because the user can't copy/paste the path.  I would like a quick and easy way to present a file selection dialog to the user, they can select the file, and then it's loaded to the database.  (In my use case, if they happened to chose the wrong file, it would fail parsing, and wouldn't be a problem even if it was loaded to the database.)This code is close to what I want, but it leaves an annoying empty frame open (which isn't able to be closed, probably because I haven't registered a close event handler).I don't have to use tkInter, but since it's in the Python standard library it's a good candidate for quickest and easiest solution.Whats a quick and easy way to prompt for a file or filename in a script without any other UI?

2012-02-16 21:24:41Z

I have a simple script which parses a file and loads it's contents to a database. I don't need a UI, but right now I'm prompting the user for the file to parse using raw_input which is most unfriendly, especially because the user can't copy/paste the path.  I would like a quick and easy way to present a file selection dialog to the user, they can select the file, and then it's loaded to the database.  (In my use case, if they happened to chose the wrong file, it would fail parsing, and wouldn't be a problem even if it was loaded to the database.)This code is close to what I want, but it leaves an annoying empty frame open (which isn't able to be closed, probably because I haven't registered a close event handler).I don't have to use tkInter, but since it's in the Python standard library it's a good candidate for quickest and easiest solution.Whats a quick and easy way to prompt for a file or filename in a script without any other UI?Tkinter is the easiest way if you don't want to have any other dependencies.

To show only the dialog without any other GUI elements, you have to hide the root window using the withdraw method:Python 2 variant:You can use easygui:To install easygui, you can use pip:It is a single pure Python module (easygui.py) that uses tkinter.Try with wxPython:If you don't need the UI or expect the program to run in a CLI, you could parse the filepath as an argument. This would allow you to use the autocomplete feature of your CLI to quickly find the file you need.This would probably only be handy if the script is non-interactive besides the filepath input.Check out EasyGUI, a very easy to use module that should do the job - http://easygui.sourceforge.net/You would use the fileopenbox function detailed on this api documentation page - https://easygui.readthedocs.io/en/latest/api.htmlpywin32 provides access to the GetOpenFileName win32 function. From the exampleUsing tkinter (python 2) or Tkinter (python 3) it's indeed possible to display file open dialog (See other answers here). Please notice however that user interface of that dialog is outdated and does not corresponds to newer file open dialogs available in Windows 10. Moreover - if you're looking on way to embedd python support into your own application - you will find out soon that tkinter library is not open source code and even more - it is commercial library.(For example search for "activetcl pricing" will lead you to this web page: https://reviews.financesonline.com/p/activetcl/)So tkinter library will cost money for any application wanting to embedd python.I by myself managed to find pythonnet library:(MIT License)Using following command it's possible to install pythonnet:And here you can find out working example for using open file dialog:https://stackoverflow.com/a/50446803/2338477Let me copy an example also here:If you also miss more complex user interface - see Demo folder

in pythonnet git.I'm not sure about portability to other OS's, haven't tried, but .net 5 is planned to be ported to multiple OS's (Search ".net 5 platforms", https://devblogs.microsoft.com/dotnet/introducing-net-5/ ) - so this technology is also future proof.

Python if-else short-hand [duplicate]

learner

[Python if-else short-hand [duplicate]](https://stackoverflow.com/questions/14461905/python-if-else-short-hand)

I want to do the following in python:Clearly, when either i or j hits a limit, the code will break out of the loop. I need the values of i and j outside of the loop.Must I really doOr does anyone know of a shorter way?Besides the above, can I get Python to support something similar to

2013-01-22 15:20:00Z

I want to do the following in python:Clearly, when either i or j hits a limit, the code will break out of the loop. I need the values of i and j outside of the loop.Must I really doOr does anyone know of a shorter way?Besides the above, can I get Python to support something similar toThe most readable way isbut you can use and and or, too:The "Zen of Python" says that "readability counts", though, so go for the first way.Also, the and-or trick will fail if you put a variable instead of 10 and it evaluates to False.However, if more than the assignment depends on this condition, it will be more readable to write it as you have:unless you put i and j in a container. But if you show us why you need it, it may well turn out that you don't.Try this:This is a sample of execution:

How to replace (or strip) an extension from a filename in Python?

ereOn

[How to replace (or strip) an extension from a filename in Python?](https://stackoverflow.com/questions/3548673/how-to-replace-or-strip-an-extension-from-a-filename-in-python)

Is there a built-in function in Python that would replace (or remove, whatever) the extension of a filename (if it has one) ?Example:In my example: /home/user/somefile.txt would become /home/user/somefile.jpgI don't know if it matters, but I need this for a SCons module I'm writing. (So perhaps there is some SCons specific function I can use ?)I'd like something clean. Doing a simple string replacement of all occurrences of .txt within the string is obviously not clean. (This would fail if my filename is somefile.txt.txt.txt)

2010-08-23 14:49:25Z

Is there a built-in function in Python that would replace (or remove, whatever) the extension of a filename (if it has one) ?Example:In my example: /home/user/somefile.txt would become /home/user/somefile.jpgI don't know if it matters, but I need this for a SCons module I'm writing. (So perhaps there is some SCons specific function I can use ?)I'd like something clean. Doing a simple string replacement of all occurrences of .txt within the string is obviously not clean. (This would fail if my filename is somefile.txt.txt.txt)Try os.path.splitext it should do what you want.Expanding on AnaPana's answer, how to remove an extension using pathlib (Python >= 3.4):As @jethro said, splitext is the neat way to do it. But in this case, it's pretty easy to split it yourself, since the extension must be the part of the filename coming after the final period:The rsplit tells Python to perform the string splits starting from the right of the string, and the 1 says to perform at most one split (so that e.g. 'foo.bar.baz' -> [ 'foo.bar', 'baz' ]). Since rsplit will always return a non-empty array, we may safely index 0 into it to get the filename minus the extension.I prefer the following one-liner approach using str.rsplit():Example:For Python >= 3.4:Another way to do is to use the str.rpartition(sep) method. For example:In the case where you have multiple extensions this one-liner using pathlib and str.replace works a treat:If you also want a pathlib object output then you can obviously wrap the line in Path()

How to round the minute of a datetime object python

Lucas Manco

[How to round the minute of a datetime object python](https://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object-python)

I have a datetime object produced using strptime(). What I need to do is round the minute to the closest 10th minute. What I have been doing up to this point was taking the minute value and using round() on it.However, as with the above example, it gives an invalid time when the minute value is greater than 56. i.e.: 3:60What is a better way to do this? Does datetime support this? 

2010-08-12 00:45:21Z

I have a datetime object produced using strptime(). What I need to do is round the minute to the closest 10th minute. What I have been doing up to this point was taking the minute value and using round() on it.However, as with the above example, it gives an invalid time when the minute value is greater than 56. i.e.: 3:60What is a better way to do this? Does datetime support this? This will get the 'floor' of a datetime object stored in tm rounded to the 10 minute mark before tm.If you want classic rounding to the nearest 10 minute mark, do this:or this:General function to round a datetime at any time lapse in seconds:Samples with 1 hour rounding & 30 minutes rounding:From the best answer I modified to an adapted version using only datetime objects, this avoids having to do the conversion to seconds and makes the calling code more readable:Samples with 1 hour rounding & 15 minutes rounding:I used Stijn Nevens code (thank you Stijn) and have a little add-on to share. Rounding up, down and rounding to nearest.update 2019-03-09 = comment Spinxz incorporated; thank you.update 2019-12-27 = comment Bart incorporated; thank you.Tested for date_delta of "X hours" or "X minutes" or "X seconds".if you don't want to use condition, you can use modulo operator:UPDATEdid you want something like this?.. if you want result as string. for obtaining datetime result, it's better to use timedelta - see other responses ;)Pandas has a datetime round feature, but as with most things in Pandas it needs to be in Series format.Docs - Change the frequency string as needed.Here is a simpler generalized solution without floating point precision issues and external library dependencies:In your case:i'm using this.  it has the advantage of working with tz aware datetimes.it has the disadvantage of only working for timeslices less than an hour.Based on Stijn Nevens and modified for Django use to round current time to the nearest 15 minute.if you need full date and time just remove the .strftime('%H:%M:%S')Not the best for speed when the exception is caught, however this would work.TimingsA two line intuitive solution to round to a given time unit, here seconds, for a datetime object t:If you wish to round to a different unit simply alter format_str. This approach does not round to arbitrary time amounts as above methods, but is a nicely Pythonic way to round to a given hour, minute or second.Other solution:Hope this helps!The shortest way I know

how to change default python version?

Zhu Shengqi

[how to change default python version?](https://stackoverflow.com/questions/5846167/how-to-change-default-python-version)

I have installed python 3.2 in my mac. After I run /Applications/Python 3.2/Update Shell Profile.command, it's confusing that when I type python -V in Terminal it says that Python 2.6.1, how can I change the default python version?

2011-05-01 03:10:52Z

I have installed python 3.2 in my mac. After I run /Applications/Python 3.2/Update Shell Profile.command, it's confusing that when I type python -V in Terminal it says that Python 2.6.1, how can I change the default python version?This is probably desirable for backwards compatibility.Python3 breaks backwards compatibility, and programs invoking 'python' probably expect python2. You probably have many programs and scripts which you are not even aware of which expect python=python2, and changing this would break those programs and scripts.The answer you are probably looking for is You should not change this.You could, however, make a custom alias in your shell. The way you do so depends on the shell, but perhaps you could do alias py=python3If you are confused about how to start the latest version of python, it is at least the case on Linux that python3 leaves your python2 installation intact (due to the above compatibility reasons); thus you can start python3 with the python3 command.On Mac OS X using the python.org installer as you apparently have, you need to invoke Python 3 with python3, not python.  That is currently reserved for Python 2 versions.  You could also use python3.2 to specifically invoke that version.If you also installed a Python 2 from python.org, it would have a similar framework bin directory with no overlapping file names (except for 2to3).Check the location of python 3Write alias in bash_profileReload bash_profileConfirm python commandOld question, but alternatively:Change the "default" Python by putting it ahead of the system Python on your path, for instance:Do right thing, do thing right!--->Zero Open your terminal,--Firstly input python -V, It likely shows:-Secondly input python3 -V, It likely shows:--Thirdly input where  python or which python, It likely shows:---Fourthly input where python3 or which python3, It likely shows:--Fifthly add the following line at the bottom of your PATH environment variable file in ~/.profile file or ~/.bash_profile under Bash or ~/.zshrc under zsh.OR-Sixthly input source ~/.bash_profile under Bash or source ~/.zshrc under zsh.--Seventhly Quit the terminal.---Eighthly Open your terminal, and input python -V, It likely shows: I had done successfully try it.Others, the ~/.bash_profile under zsh is not that ~/.bash_profile. The PATH environment variable under zsh instead ~/.profile (or ~/.bash_file) via ~/.zshrc.Help you guys!According to a quick google search, this update only applies to the current shell you have open. It can probably be fixed by typing python3, as mac and linux are similar enough for things like this to coincide. Link to the result of google search.Also, as ninjagecko stated, most programs have not been updated to 3.x yet, so having the default python as 3.x would break many python scripts used in applications.I am using OS X 10.7.5 and Python 3.4.2. If you type python3 and what you want to run it will run it using python 3. For example 

pyhton3 test001.py. That ran a test program I made called test001. I hope this helps.Navigate to:My Computer -> Properties -> Advanced -> Environment Variables -> System VariablesSuppose you had already having python 2.7 added in path variable and you want to change default path to python 3.xthen add path of python3.5.x folder before python2.7 path.open cmd: type "python --version"python version will be changed to python 3.5.xThis will break scripts, but is exactly the way to change python. You should also rewrite the scripts to not assume python is 2.x. This will work regardless of the place where you call system or exec.

How can I add the sqlite3 module to Python?

Jin-Dominique

[How can I add the sqlite3 module to Python?](https://stackoverflow.com/questions/19530974/how-can-i-add-the-sqlite3-module-to-python)

Can someone tell me how to install the sqlite3 module alongside the most recent version of Python?

I am using a Macbook, and on the command line, I tried:but an error pops up.

2013-10-23 01:01:51Z

Can someone tell me how to install the sqlite3 module alongside the most recent version of Python?

I am using a Macbook, and on the command line, I tried:but an error pops up.You don't need to install sqlite3 module. It is included in the standard library (since Python 2.5).I have python 2.7.3 and this solved my problem:For Python version 3:Normally, it is included. However, as @ngn999 said, if your python has been built from source manually, you'll have to add it. Here is an example of a script that will setup an encapsulated version (virtual environment) of Python3 in your user directory with an encapsulated version of sqlite3.Why do this? You might want a modular python environment that you can completely destroy and rebuild without affecting your operating system–for an independent development environment. In this case, the solution is to install sqlite3 modularly too.

Combining two lists and removing duplicates, without removing duplicates in original list

Lee Olayvar

[Combining two lists and removing duplicates, without removing duplicates in original list](https://stackoverflow.com/questions/1319338/combining-two-lists-and-removing-duplicates-without-removing-duplicates-in-orig)

I have two lists that i need to combine where the second list has any duplicates of the first list ignored. .. A bit hard to explain, so let me show an example of what the code looks like, and what i want as a result.You'll notice that the result has the first list, including its two "2" values, but the fact that second_list also has an additional 2 and 5 value is not added to the first list.Normally for something like this i would use sets, but a set on first_list would purge the duplicate values it already has. So i'm simply wondering what the best/fastest way to achieve this desired combination.Thanks.

2009-08-23 19:27:58Z

I have two lists that i need to combine where the second list has any duplicates of the first list ignored. .. A bit hard to explain, so let me show an example of what the code looks like, and what i want as a result.You'll notice that the result has the first list, including its two "2" values, but the fact that second_list also has an additional 2 and 5 value is not added to the first list.Normally for something like this i would use sets, but a set on first_list would purge the duplicate values it already has. So i'm simply wondering what the best/fastest way to achieve this desired combination.Thanks.You need to append to the first list those elements of the second list that aren't in the first - sets are the easiest way of determining which elements they are, like this:Or if you prefer one-liners 8-)You can use sets:You can bring this down to one single line of code if you use numpy:Simplest to me is:Based on the recipe : This might help   The union function merges the second list into first, with out duplicating an element of a, if it's already in a. Similar to set union operator. This function does not change b. If a=[1,2,3] b=[2,3,4]. After union(a,b) makes a=[1,2,3,4] and b=[2,3,4]You can also combine RichieHindle's and Ned Batchelder's responses for an average-case O(m+n) algorithm that preserves order:Note that x in s has a worst-case complexity of O(m), so the worst-case complexity of this code is still O(m*n).Output: [1, 2, 3, 4][1, 2, 2, 5, 7, 9]simply like belove : 

How to extract a floating number from a string [duplicate]

Ben Keating

[How to extract a floating number from a string [duplicate]](https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string)

I have a number of strings similar to Current Level: 13.4 db. and I would like to extract just the floating point number. I say floating and not decimal as it's sometimes whole. Can RegEx do this or is there a better way?

2011-01-16 02:11:39Z

I have a number of strings similar to Current Level: 13.4 db. and I would like to extract just the floating point number. I say floating and not decimal as it's sometimes whole. Can RegEx do this or is there a better way?If your float is always expressed in decimal notation something likemay suffice.A more robust version would be:If you want to validate user input, you could alternatively also check for a float by stepping to it directly:You may like to try something like this which covers all the bases, including not relying on whitespace after the number:For easy copy-pasting:Python docs has an answer that covers +/-, and exponent notationThis regular expression does not support international formats where a comma is used as the separator character between the whole and fractional part (3,14159).

In that case, replace all \. with [.,] in the above float regex.You can use the following regex to get integer and floating values from a string:Thanks

Rexas described above, works really well!

One suggestion though:will also return negative int values (like -3 in the end of this string)I think that you'll find interesting stuff in the following answer of mine that I did for a previous similar question:https://stackoverflow.com/q/5929469/551449In this answer, I proposed a  pattern that allows a regex to catch any kind of number and since I have nothing else to add to it, I think it is fairly completeAnother approach that may be more readable is simple type conversion. I've added a replacement function to cover instances where people may enter European decimals:This has disadvantages too however. If someone types in "1,000", this will be converted to 1. Also, it assumes that people will be inputting with whitespace between words. This is not the case with other languages, such as Chinese.

Determining Whether a Directory is Writeable

illuminatedtiger

[Determining Whether a Directory is Writeable](https://stackoverflow.com/questions/2113427/determining-whether-a-directory-is-writeable)

What would be the best way in Python to determine whether a directory is writeable for the user executing the script? Since this will likely involve using the os module I should mention I'm running it under a *nix environment. 

2010-01-21 22:24:39Z

What would be the best way in Python to determine whether a directory is writeable for the user executing the script? Since this will likely involve using the os module I should mention I'm running it under a *nix environment. Although what Christophe suggested is a more Pythonic solution, the os module does have the os.access function to check access:os.access('/path/to/folder', os.W_OK) # W_OK is for writing, R_OK for reading, etc.It may seem strange to suggest this, but a common Python idiom is Following that idiom, one might say:Try writing to the directory in question, and catch the error if you don't have the permission to do so.My solution using the tempfile module:Update:

After testing the code again on Windows I see that there is indeed an issue when using tempfile there, see issue22107: tempfile module misinterprets access denied error on Windows.

In the case of a non-writable directory, the code hangs for several seconds and finally throws an IOError: [Errno 17] No usable temporary file name found. Maybe this is what user2171842 was observing?

Unfortunately the issue is not resolved for now so to handle this, the error needs to be catched as well:The delay is of course still present in these cases then.Stumbled across this thread searching for examples for someone. First result on Google, congrats!People talk about the Pythonic way of doing it in this thread, but no simple code examples? Here you go, for anyone else who stumbles in:This attempts to open a filehandle for writing, and exits with an error if the file specified cannot be written to: This is far easier to read, and is a much better way of doing it rather than doing prechecks on the file path or the directory, as it avoids race conditions; cases where the file becomes unwriteable between the time you run the precheck, and when you actually attempt to write to the file. If you only care about the file perms, os.access(path, os.W_OK) should do what you ask for. If you instead want to know whether you can write to the directory, open() a test file for writing (it shouldn't exist beforehand), catch and examine any IOError, and clean up the test file afterwards.More generally, to avoid TOCTOU attacks (only a problem if your script runs with elevated privileges -- suid or cgi or so), you shouldn't really trust these ahead-of-time tests, but drop privs, do the open(), and expect the IOError.Check the mode bits: Here is something I created based on ChristopheD's answer:more info about access can be find it hereI ran into this same need while adding an argument via argparse. The built in type=FileType('w') wouldn't work for me as I was looking for a directory. I ended up writing my own method to solve my problem. Here is the result with argparse snippet.That results in the following:I went back and added print opts.dir to the end, and everything appears to be functioning as desired.If you need to check the permission of another user (yes, I realize this contradicts the question, but may come in handy for someone), you can do it through the pwd module, and the directory's mode bits.Disclaimer - does not work on Windows, as it doesn't use the POSIX permissions model (and the pwd module is not available there), e.g. - solution only for *nix systems.Note that a directory has to have all the 3 bits set - Read, Write and eXecute.

Ok, R is not an absolute must, but w/o it you cannot list the entries in the directory (so you have to know their names). Execute on the other hand is absolutely needed - w/o it the user cannot read the file's inodes; so even having W, without X files cannot be created or modified. More detailed explanation at this link.Finally, the modes are available in the stat module, their descriptions are in inode(7) man.Sample code how to check:

Using an SSH keyfile with Fabric

Yuval Adam

[Using an SSH keyfile with Fabric](https://stackoverflow.com/questions/5327465/using-an-ssh-keyfile-with-fabric)

How do you configure fabric to connect to remote hosts using SSH keyfiles (for example, Amazon EC2 instances)?

2011-03-16 15:20:01Z

How do you configure fabric to connect to remote hosts using SSH keyfiles (for example, Amazon EC2 instances)?Also worth mentioning here that you can use the command line args for this:Finding a simple fabfile with a working example of SSH keyfile usage isn't easy for some reason. I wrote a blog post about it (with a matching gist).Basically, the usage goes something like this:The important part is setting the env.key_filename environment variable, so that the Paramiko configuration can look for it when connecting.Another cool feature available as of Fabric 1.4 - Fabric now supports SSH configs.If you already have all the SSH connection parameters in your ~/.ssh/config file, Fabric will natively support it, all you need to do is add:at the beginning of your fabfile.For me, the following didn't work:or However, the following did:orFor fabric2 in fabfile use the following:and run it with:UPDATE:

For multiple hosts (one host will do also) you can use this:and run it with fab or fab2:I had to do this today, my .py file was as simple as possible, like the one posted in the answer of @YuvalAdam but still I kept getting prompted for a password...Looking at the paramiko (the library used by fabric for ssh) log, I found the line:I updated paramiko with:And now it's working. As stated above, Fabric will support .ssh/config file settings after a fashion, but using a pem file for ec2 seems to be problematic.   IOW a properly setup .ssh/config file will work from the command line via 'ssh servername' and fail to work with 'fab sometask' when env.host=['servername'].This was overcome by specifying the env.key_filename='keyfile' in my fabfile.py and duplicating the IdentityFile entry already in my .ssh/config.   This could be either Fabric or paramiko, which in my case was Fabric 1.5.3 and Paramiko 1.9.0.

Python element-wise tuple operations like sum

Rodrigo

[Python element-wise tuple operations like sum](https://stackoverflow.com/questions/497885/python-element-wise-tuple-operations-like-sum)

Is there anyway to get tuple operations in Python to work like this:instead of:I know it works like that because the __add__ and __mul__ methods are defined to work like that. So the only way would be to redefine them?

2009-01-31 00:51:48Z

Is there anyway to get tuple operations in Python to work like this:instead of:I know it works like that because the __add__ and __mul__ methods are defined to work like that. So the only way would be to redefine them?Using all built-ins..This solution doesn't require an import:Sort of combined the first two answers, with a tweak to ironfroggy's code so that it returns a tuple:Note: using self.__class__ instead of stuple to ease subclassing.gives array([4,4,4]).See http://www.scipy.org/Tentative_NumPy_TutorialGenerator comprehension could be used instead of map. Built-in map function is not obsolete but it's less readable for most people than list/generator/dict comprehension, so I'd recommend not to use map function in general.simple solution without class definition that returns tupleAll generator solution. Not sure on performance (itertools is fast, though)Yes. But you can't redefine built-in types. You have to subclass them:even simpler and without using map, you can do thatI currently subclass the "tuple" class to overload +,- and *. I find it makes the code beautiful and writing the code easier.In case someone need to average a list of tuples: 

Representing graphs (data structure) in Python

shad0w_wa1k3r

[Representing graphs (data structure) in Python](https://stackoverflow.com/questions/19472530/representing-graphs-data-structure-in-python)

How can one neatly represent a graph in Python? (Starting from scratch i.e. no libraries!)What data structure (e.g. dicts/tuples/dict(tuples)) will be fast but also memory efficient?One must be able to do various graph operations on it.

As pointed out, the various graph representations might help. How does one go about implementing them in Python?As for the libraries, this question has quite good answers.

2013-10-20 00:13:02Z

How can one neatly represent a graph in Python? (Starting from scratch i.e. no libraries!)What data structure (e.g. dicts/tuples/dict(tuples)) will be fast but also memory efficient?One must be able to do various graph operations on it.

As pointed out, the various graph representations might help. How does one go about implementing them in Python?As for the libraries, this question has quite good answers.Even though this is a somewhat old question, I thought I'd give a practical answer for anyone stumbling across this.Let's say you get your input data for your connections as a list of tuples like so:The data structure I've found to be most useful and efficient for graphs in Python is a dict of sets. This will be the underlying structure for our Graph class. You also have to know if these connections are arcs (directed, connect one way) or edges (undirected, connect both ways). We'll handle that by adding a directed parameter to the Graph.__init__ method. We'll also add some other helpful methods.I'll leave it as an "exercise for the reader" to create a find_shortest_path and other methods.Let's see this in action though...NetworkX is an awesome Python graph library. You'll be hard pressed to find something you need that it doesn't already do.And it's open source so you can see how they implemented their algorithms. You can also add additional algorithms.https://github.com/networkx/networkx/tree/master/networkx/algorithmsFirst, the choice of classical list vs. matrix representations depends on the purpose (on what do you want to do with the representation). The well-known problems and algorithms are related to the choice. The choice of the abstract representation kind of dictates how it should be implemented. Second, the question is whether the vertices and edges should be expressed only in terms of existence, or whether they carry some extra information.From Python built-in data types point-of-view, any value contained elsewhere is expressed as a (hidden) reference to the target object. If it is a variable (i.e. named reference), then the name and the reference is always stored in (an internal) dictionary. If you do not need names, then the reference can be stored in your own container -- here probably Python list will always be used for the list as abstraction.Python list is implemented as a dynamic array of references, Python tuple is implemented as static array of references with constant content (the value of references cannot be changed). Because of that they can be easily indexed. This way, the list can be used also for implementation of matrices.Another way to represent matrices are the arrays implemented by the standard module array -- more constrained with respect to the stored type, homogeneous value. The elements store the value directly. (The list stores the references to the value objects instead). This way, it is more memory efficient and also the access to the value is faster.Sometimes, you may find useful even more restricted representation like bytearray.There are two excellent graph libraries

NetworkX and igraph. You can find both library source codes on GitHub. You can always see how the functions are written. But I prefer NetworkX because its easy to understand.

See their codes to know how they make the functions. You will get multiple ideas and then can choose how you want to make a graph using data structures.

Reading a binary file with python

Brian

[Reading a binary file with python](https://stackoverflow.com/questions/8710456/reading-a-binary-file-with-python)

I find particularly difficult reading binary file with Python. Can you give me a hand?

I need to read this file, which in Fortran 90 is easily read byIn detail, the file format is:How can I read this with Python? I tried everything but it never worked. Is there any chance I might use a f90 program in python, reading this binary file and then save the data that I need to use?

2012-01-03 09:57:51Z

I find particularly difficult reading binary file with Python. Can you give me a hand?

I need to read this file, which in Fortran 90 is easily read byIn detail, the file format is:How can I read this with Python? I tried everything but it never worked. Is there any chance I might use a f90 program in python, reading this binary file and then save the data that I need to use?Read the binary file content like this:then "unpack" binary data using struct.unpack:The start bytes: struct.unpack("iiiii", fileContent[:20])The body: ignore the heading bytes and the trailing byte (= 24); The remaining part forms the body, to know the number of bytes in the body do an integer division by 4; The obtained quotient is multiplied by the string 'i' to create the correct format for the unpack method:The end byte: struct.unpack("i", fileContent[-4:])In general, I would recommend that you look into using Python's struct module for this. It's standard with Python, and it should be easy to translate your question's specification into a formatting string suitable for struct.unpack().Do note that if there's "invisible" padding between/around the fields, you will need to figure that out and include it in the unpack() call, or you will read the wrong bits.Reading the contents of the file in order to have something to unpack is pretty trivial:This unpacks the first two fields, assuming they start at the very beginning of the file (no padding or extraneous data), and also assuming native byte-order (the @ symbol). The Is in the formatting string mean "unsigned integer, 32 bits".You could use numpy.fromfile, which can read data from both text and binary files. You would first construct a data type, which represents your file format, using numpy.dtype, and then read this type from file using numpy.fromfile.To read a binary file to a bytes object:To create an int from bytes 0-3 of the data:To unpack multiple ints from the data:

List of tuples to dictionary

Sarah Vessels

[List of tuples to dictionary](https://stackoverflow.com/questions/6522446/list-of-tuples-to-dictionary)

Here's how I'm currently converting a list of tuples to dictionary in Python:Is there a better way?  It seems like there should be a one-liner to do this.

2011-06-29 14:35:26Z

Here's how I'm currently converting a list of tuples to dictionary in Python:Is there a better way?  It seems like there should be a one-liner to do this.Just call dict() on the list of tuples directlyThe dict constructor accepts input exactly as you have it (key/value tuples).From the documentation:With dict comprehension:  

Convert Python dictionary to JSON array

HyperDevil

[Convert Python dictionary to JSON array](https://stackoverflow.com/questions/14661051/convert-python-dictionary-to-json-array)

Currently I have this dictionary, printed using pprint:  When I do this:  I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte

2013-02-02 10:44:40Z

Currently I have this dictionary, printed using pprint:  When I do this:  I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byteIf you are fine with non-printable symbols in your json, then add ensure_ascii=False to dumps call.ensure_ascii=False really only defers the issue to the decoding stage:Ultimately you can't store raw bytes in a JSON document, so you'll want to use some means of unambiguously encoding a sequence of arbitrary bytes as an ASCII string - such as base64.If you use Python 2, don't forget to add the UTF-8 file encoding comment on the first line of your script.This will fix some Unicode problems and make your life easier.One possible solution that I use is to use python3.  It seems to solve many utf issues.Sorry for the late answer, but it may help people in the future.For example,

Django Rest Framework - Could not resolve URL for hyperlinked relationship using view name「user-detail」

bpipat

[Django Rest Framework - Could not resolve URL for hyperlinked relationship using view name「user-detail」](https://stackoverflow.com/questions/20550598/django-rest-framework-could-not-resolve-url-for-hyperlinked-relationship-using)

I am building a project in Django Rest Framework where users can login to view their wine cellar.

My ModelViewSets were working just fine and all of a sudden I get this frustrating error:The traceback shows:I have a custom email user model and the bottle model in models.py is:My serializers:My views:and finally the url:I don't have a user detail view and I don't see where this issue could come from. Any ideas?Thanks

2013-12-12 17:49:04Z

I am building a project in Django Rest Framework where users can login to view their wine cellar.

My ModelViewSets were working just fine and all of a sudden I get this frustrating error:The traceback shows:I have a custom email user model and the bottle model in models.py is:My serializers:My views:and finally the url:I don't have a user detail view and I don't see where this issue could come from. Any ideas?ThanksBecause it's a HyperlinkedModelSerializer your serializer is trying to resolve the URL for the related User on your Bottle.

As you don't have the user detail view it can't do this. Hence the exception.I came across this error too and solved it as follows:The reason is I forgot giving "**-detail" (view_name, e.g.: user-detail) a namespace. So, Django Rest Framework could not find that view.There is one app in my project, suppose that my project name is myproject, and the app name is myapp.There is two urls.py file, one is myproject/urls.py and the other is myapp/urls.py. I give the app a namespace in myproject/urls.py, just like:I registered the rest framework routers in myapp/urls.py, and then got this error.My solution was to provide url with namespace explicitly:And it solved my problem.Maybe someone can have a look at this :  http://www.django-rest-framework.org/api-guide/routers/If using namespacing with hyperlinked serializers you'll also need to ensure that any view_name parameters on the serializers correctly reflect the namespace. For example:you'd need to include a parameter such as view_name='api:user-detail' for serializer fields hyperlinked to the user detail view.This code should work, too.Another nasty mistake that causes this error is having the base_name unnecessarily defined in your urls.py.  For example:This will cause the error noted above.  Get that base_name outta there and get back to a working API. The code below would fix the error.  Hooray!However, you probably didn't just arbitrarily add the base_name, you might have done it because you defined a custom def get_queryset() for the View and so Django mandates that you add the base_name.  In this case you'll need to explicitly define the 'url' as a HyperlinkedIdentityField for the serializer in question.  Notice we are defining this HyperlinkedIdentityField ON THE SERIALIZER of the view that is throwing the error.  If my error were "Could not resolve URL for hyperlinked relationship using view name "study-detail". You may have failed to include the related model in your API, or incorrectly configured the lookup_field attribute on this field."  I could fix this with the following code. My ModelViewSet (the custom get_queryset is why I had to add the base_name to the router.register() in the first place): My router registration for this ModelViewSet in urls.py:AND HERE'S WHERE THE MONEY IS!  Then I could solve it like so:Yep.  You have to explicitly define this HyperlinkedIdentityField on itself for it to work. And you need to make sure that the view_name defined on the HyperlinkedIdentityField is the same as you defined on the base_name in urls.py with a '-detail' added after it.I ran into this error after adding namespace to my urland adding app_name to my urls.py I resolved this by specifying NamespaceVersioning for my rest framework api in settings.py of my projectSame Error, but different reason:I define a custom user model, nothing new field:This is my view function:Since I didn't give queryset directly in UserViewSet, I have to set base_name when I register this viewset. This is where my error message caused by urls.py file:You need a base_name same as your model name - customuser.If you're extending the GenericViewSet and ListModelMixin classes, and have the same error when adding the url field in the list view, it's because you're not defining the detail view. Be sure you're extending the RetrieveModelMixin mixin:Today, I got the same error and below changes rescue me.Change to:I ran into the same error while I was following the DRF quickstart guide

http://www.django-rest-framework.org/tutorial/quickstart/ and then attempting to browse to /users.  I've done this setup many times before without problems.My solution was not in the code but in replacing the database.The difference between this install and the others before was when I created the local database.This time I ran myimmediately after runningInstead of the exact order listed in the guide.I suspected something wasn't properly created in the DB. I didn't care about my dev db so I deleted it and ran the ./manage.py migrate command once more, created a super user, browsed to /users and the error was gone.Something was problematic with the order of operations in which I configured DRF and the db.If you are using sqlite and are able to test changing to a fresh DB then it's worth an attempt before you go dissecting all of your code.Bottle = serializers.PrimaryKeyRelatedField(read_only=True)read_only allows you to represent the field without having to link it to another view of the model.I got that error on DRF 3.7.7 when a slug value was empty (equals to '') in the database. I ran into this same issue and resolved it by adding generics.RetrieveAPIView as a base class to my viewset.I was stuck in this error for almost 2 hours:ImproperlyConfigured at /api_users/users/1/

Could not resolve URL for hyperlinked relationship using view name "users-detail". You may have failed to include the related model in your API, or incorrectly configured the lookup_field attribute on this field.When I finally get the solution  but I don't understand why, so my code is:but in my main URLs, it was:So to finally I resolve the problem erasing namespace:And I finally resolve my problem, so any one can let me know why, bests.If you omit the fields 'id' and 'url' from your serializer you won't have any problem. You can access to the posts by using the id that is returned in the json object anyways, which it makes it even easier to implement your frontend.I had the same problem , I think you should check yourget_absolute_url object model's method input value (**kwargs) title. 

and use exact field name in lookup_fieldIt appears that HyperlinkedModelSerializer do not agree with having a path namespace. In my application I made two changes.In the imported urls fileHope this helps.

How should I organize Python source code? [closed]

Andres Jaan Tack

[How should I organize Python source code? [closed]](https://stackoverflow.com/questions/1849311/how-should-i-organize-python-source-code)

I'm getting started with Python (it's high time I give it a shot), and I'm looking for some best practices.My first project is a queue which runs command-line experiments in multiple threads. I'm starting to get a very long main.py file, and I'd like to break it up. In general, I'm looking for: How do python programmers organize multiple source files? Is there a particular structure that works for you?My specific questions include:I can probably draw some of my own conclusions here by trial and error, but I'd rather start from something good.

2009-12-04 20:00:52Z

I'm getting started with Python (it's high time I give it a shot), and I'm looking for some best practices.My first project is a queue which runs command-line experiments in multiple threads. I'm starting to get a very long main.py file, and I'd like to break it up. In general, I'm looking for: How do python programmers organize multiple source files? Is there a particular structure that works for you?My specific questions include:I can probably draw some of my own conclusions here by trial and error, but I'd rather start from something good.The article Eric pointed to is awesome because it covers details of organising large Python code bases.If you've landed here from Google and are trying to find out how to split one large source file into multiple, more manageable, files I'll summarise the process briefly.Assume you currently have everything in a file called main.py:Conceptually what this does is to create a new module called utils in another source file.  You can then import it wherever it's needed.The way you should organise your code and tests is exactly the same you would for any OO language. Answers from the way I do it. It may not be right but works for me

How can I distribute python programs?

Georg Schölly

[How can I distribute python programs?](https://stackoverflow.com/questions/1558385/how-can-i-distribute-python-programs)

My application looks like this:The program is started with main.py. Is there a good way to create a 'final' application out of it? I'm thinking of something like py2exe/py2app, but without copying the python interpreter / modules into the application where one has only one executable.I had a look at distutils, but this looks like it installs a program into the Python directory, which isn't usual on non-linux platforms.At the moment I just copy the whole source folder onto the target machine and create an alias to main.pyw on windows. Some inconveniences:How does one create a nice automated distribution?

2009-10-13 05:48:20Z

My application looks like this:The program is started with main.py. Is there a good way to create a 'final' application out of it? I'm thinking of something like py2exe/py2app, but without copying the python interpreter / modules into the application where one has only one executable.I had a look at distutils, but this looks like it installs a program into the Python directory, which isn't usual on non-linux platforms.At the moment I just copy the whole source folder onto the target machine and create an alias to main.pyw on windows. Some inconveniences:How does one create a nice automated distribution?The normal way of distributing Python applications is with distutils. It's made both for distributing library type python modules, and python applications, although I don't know how it works on Windows. You would on Windows have to install Python separately if you use distutils, in any case.I'd probably recommend that you distribute it with disutils for Linux, and Py2exe or something similar for Windows. For OS X I don't know. If it's an end user application you would probably want an disk image type of thing, I don't know how to do that. But read this post for more information on the user experience of it. For an application made for programmers you are probably OK with a distutils type install on OS X too.I highly recommend Pyinstaller, which supports all major platforms pretty seamlessly.  Like py2exe and py2app, it produces a standard executable on Windows and an app bundle on OS X, but has the benefit of also doing a fantastic job of auto-resolving common dependencies and including them without extra configuration tweaks.Also note that if you're deploying Python 2.6 to Windows, you should apply this patch to Pyinstaller trunk.You indicated that you don't need an installer, but Inno Setup is an easy to use and quick to setup choice for the Windows platform.Fredrik Lundh's squeeze.py can create a single file that does not contain the Python interpreter, but instead contains bytecode.  With the right arguments, you can include other files, modules, etc. in the result file.  I used it successfully in one project.  The resulting program ran on OS X, Linux and Windows without any problem!PS: Each machine needs to have a Python interpreter which is compatible with the bytecode generated by squeeze.py.  You can generate different bytecode versions for different versions of Python, if need be (just run squeeze.py with the right version of Python).I think it’s also worth mentioning PEX (considering more the attention this question received and less the question itself). According to its own description:I stumbled upon it when I read an overview of packaging for python. They posted this nice picture there:

To summarize: If you can afford relying on python being installed on the target machine, use PEX to produce a self-containing »executable« which probably will have smaller file size than an executable produced by PyInstaller, for example.I may be mistaken, but doesn't IronPython have a built in compiler for windows?http://www.ironpython.net[EDIT]Try out Cx_Freeze, By far the best .py to .exe (plus a few .dlls) compiler I've ever used.http://cx-freeze.sourceforge.net/ If you are distributing on windows, use an installer to install all the relevant files/interpeter whatever is needed. Distribute a setup.exe. That is the best way on windows. Otherwise users will complain.The most convenient* cross-platform way of distributing python desktop applications is to rely on cross-platform conda package manager. There are several tools that use it:* The most convenient for the developer. Enough convenient for the end-user.

Standard deviation of a list

physics_for_all

[Standard deviation of a list](https://stackoverflow.com/questions/15389768/standard-deviation-of-a-list)

I want to find mean and standard deviation of 1st, 2nd,... digits of several (Z) lists. For example, I haveNow I want to take the mean and std of *_Rank[0], the mean and std of *_Rank[1], etc.

(ie: mean and std of the 1st digit from all the (A..Z)_rank lists;

the mean and std of the 2nd digit from all the (A..Z)_rank lists;

the mean and std of the 3rd digit...; etc). 

2013-03-13 15:38:20Z

I want to find mean and standard deviation of 1st, 2nd,... digits of several (Z) lists. For example, I haveNow I want to take the mean and std of *_Rank[0], the mean and std of *_Rank[1], etc.

(ie: mean and std of the 1st digit from all the (A..Z)_rank lists;

the mean and std of the 2nd digit from all the (A..Z)_rank lists;

the mean and std of the 3rd digit...; etc). Since Python 3.4 / PEP450 there is a statistics module in the standard library, which has a method stdev for calculating the standard deviation of iterables like yours:I would put A_Rank et al into a 2D NumPy array, and then use numpy.mean() and numpy.std() to compute the means and the standard deviations:Here's some pure-Python code you can use to calculate the mean and standard deviation.All code below is based on the statistics module in Python 3.4+.Note: for improved accuracy when summing floats, the statistics module uses a custom function _sum rather than the built-in sum which I've used in its place.Now we have for example:In Python 2.7.1, you may calculate standard deviation using numpy.std() for:It calculates sample std rather than population std.In python 2.7 you can use NumPy's numpy.std() gives the population standard deviation.In Python 3.4 statistics.stdev() returns the sample standard deviation. The pstdv() function is the same as numpy.std().pure python code:Using python, here are few methods:The other answers cover how to do std dev in python sufficiently, but no one explains how to do the bizarre traversal you've described.I'm going to assume A-Z is the entire population. If not see Ome's answer on how to inference from a sample.So to get the standard deviation/mean of the first digit of every list you would need something like this:To shorten the code and generalize this to any nth digit use the following function I generated for you:Now you can simply get the stdd and mean of all the nth places from A-Z like this:

Getting「Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?」when installing lxml through pip

SaeX

[Getting「Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?」when installing lxml through pip](https://stackoverflow.com/questions/33785755/getting-could-not-find-function-xmlcheckversion-in-library-libxml2-is-libxml2)

I'm getting an error Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed? when trying to install lxml through pip.I don't find any libxml2 dev packages to install via pip.Using Python 2.7.10 on x86 in a virtualenv under Windows 10.

2015-11-18 16:49:22Z

I'm getting an error Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed? when trying to install lxml through pip.I don't find any libxml2 dev packages to install via pip.Using Python 2.7.10 on x86 in a virtualenv under Windows 10.I had this issue and realised that whilst I did have libxml2 installed, I didn't have the necessary development libraries required by the python package. Installing them solved the problem:Install lxml from http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml for your python version. It's a precompiled WHL with required modules/dependencies. The site lists several packages, when e.g. using Win32 Python 2.7, use lxml-3.6.1-cp27-cp27m-win32.whl.Download the file, and then install with Try to use:

easy_install lxml

That works for me, win10, python 2.7.On Mac OS X El Capitan I had to run these two commands to fix this error:Which ended up installing lxml-3.5.0When you run the xcode-select command you may have to sign a EULA (so have an X-Term handy for the UI if you're doing this on a headless machine).In case anyone else has the same issue as this on worked for me.run this command instead, must have VS C++ compiler installed firsthttps://blogs.msdn.microsoft.com/pythonengineering/2016/04/11/unable-to-find-vcvarsall-bat/It works for me with Python 3.5.2 and Windows 7I tried install a lib that depends lxml and nothing works. I see a message when build was started: "Building without Cython", so after install cython with apt-get install cython, lxml was installed.It is not strange for me that none of the solutions above came up, but I saw how the igd installation removed the new version and installed the old one, for the solution I downloaded this archive:https://pypi.org/project/igd/#filesand changed the recommended version of the new version: 'lxml==4.3.0' in setup.py

It works!

How to dynamically compose an OR query filter in Django?

Jack Ha

[How to dynamically compose an OR query filter in Django?](https://stackoverflow.com/questions/852414/how-to-dynamically-compose-an-or-query-filter-in-django)

From an example you can see a multiple OR query filter:For example, this results in:However, I want to create this query filter from a list. How to do that?e.g. [1, 2, 3] -> Article.objects.filter(Q(pk=1) | Q(pk=2) | Q(pk=3))

2009-05-12 12:08:51Z

From an example you can see a multiple OR query filter:For example, this results in:However, I want to create this query filter from a list. How to do that?e.g. [1, 2, 3] -> Article.objects.filter(Q(pk=1) | Q(pk=2) | Q(pk=3))You could chain your queries as follows:To build more complex queries there is also the option to use built in Q() object's constants Q.OR and Q.AND together with the add() method like so:A shorter way of writing Dave Webb's answer using python's reduce function:Maybe it's better to use sql IN statement.See queryset api reference.If you really need to make queries with dynamic logic, you can do something like this (ugly + not tested):See the docs:Note that this method only works for primary key lookups, but that seems to be what you're trying to do.So what you want is:In case we want to programmatically set what db field we want to query: Solution which use reduce and or_ operators to filter by multiply fields.   p.s. f is a new format strings literal. It was introduced in python 3.6You can use the |= operator to programmatically update a query using Q objects.This one is for dynamic pk list:Another option I wasn't aware of until recently - QuerySet also overrides the &, |, ~, etc, operators. The other answers that OR Q objects are a better solution to this question, but for the sake of interest/argument, you can do:str(q.query) will return one query with all the filters in the WHERE clause.easy..

    from django.db.models import Q

    import you model

    args = (Q(visibility=1)|(Q(visibility=0)&Q(user=self.user)))  #Tuple

    parameters={}  #dic 

    order = 'create_at' 

    limit = 10For loop:Reduce: Both of these are equivalent to Article.objects.filter(pk__in=values)It's important to consider what you want when values is empty. Many answers with Q() as a starting value will return everything. Q(pk__in=[]) is a better starting value. It's an always-failing Q object that's handled nicely by the optimizer (even for complex equations).If you want to return everything when values is empty, you should AND with ~Q(pk__in=[]) to ensure that behaviour:It's important to remember that Q() is nothing, not an always-succeeding Q object. Any operation involving it will just drop it completely.

Convert row to column header for Pandas DataFrame,

E.K.

[Convert row to column header for Pandas DataFrame,](https://stackoverflow.com/questions/26147180/convert-row-to-column-header-for-pandas-dataframe)

The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?I want to do something like:

2014-10-01 17:33:44Z

The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?I want to do something like:Set the column labels to equal the values in the 2nd row (index location 1):If the index has unique labels, you can drop the 2nd row using:If the index is not unique, you could use:Using df.drop(df.index[1]) removes all rows with the same label as the second row. Because non-unique indexes can lead to stumbling blocks (or potential bugs) like this, it's often better to take care that the index is unique (even though Pandas does not require it).This works (pandas v'0.19.2'):It would be easier to recreate the data frame.

This would also interpret the columns types from scratch.You can specify the row index in the read_csv or read_html constructors via the header parameter which represents Row number(s) to use as the column names, and the start of the data. This has the advantage of automatically dropping all the preceding rows which supposedly are junk.

Pretty-Print JSON Data to a File using Python

Zelbinian

[Pretty-Print JSON Data to a File using Python](https://stackoverflow.com/questions/9170288/pretty-print-json-data-to-a-file-using-python)

A project for class involves parsing Twitter JSON data. I'm getting the data and setting it to the file without much trouble, but it's all in one line. This is fine for the data manipulation I'm trying to do, but the file is ridiculously hard to read and I can't examine it very well, making the code writing for the data manipulation part very difficult. Does anyone know how to do that from within Python (i.e. not using the command line tool, which I can't get to work)? Here's my code so far:Note I appreciate people pointing me to simplejson documentation and such, but as I have stated, I have already looked at that and continue to need assistance. A truly helpful reply will be more detailed and explanatory than the examples found there. ThanksAlso:

Trying this in the windows command line:results in this:I'd give you the data I'm using, but it's very large and you've already seen the code I used to make the file.

2012-02-07 02:37:47Z

A project for class involves parsing Twitter JSON data. I'm getting the data and setting it to the file without much trouble, but it's all in one line. This is fine for the data manipulation I'm trying to do, but the file is ridiculously hard to read and I can't examine it very well, making the code writing for the data manipulation part very difficult. Does anyone know how to do that from within Python (i.e. not using the command line tool, which I can't get to work)? Here's my code so far:Note I appreciate people pointing me to simplejson documentation and such, but as I have stated, I have already looked at that and continue to need assistance. A truly helpful reply will be more detailed and explanatory than the examples found there. ThanksAlso:

Trying this in the windows command line:results in this:I'd give you the data I'm using, but it's very large and you've already seen the code I used to make the file.You can parse the JSON, then output it again with indents like this:See http://docs.python.org/library/json.html for more info.You don't need json.dumps() if you don't want to parse the string later, just simply use json.dump(). It's faster too.You can use json module of python to pretty print.So, in your case If you already have existing JSON files which you want to pretty format you could use this:If you are generating new *.json or modifying existing josn file the use "indent" parameter for pretty view json format.You could redirect a file to python and open using the tool and to read it use more. The sample code will be,

Django URL Redirect

RickD

[Django URL Redirect](https://stackoverflow.com/questions/14959217/django-url-redirect)

How can I redirect traffic that doesn't match any of my other URLs back to the home page?urls.py:As it stands, the last entry sends all "other" traffic to the home page but I want to redirect via either an HTTP 301 or 302.

2013-02-19 13:54:10Z

How can I redirect traffic that doesn't match any of my other URLs back to the home page?urls.py:As it stands, the last entry sends all "other" traffic to the home page but I want to redirect via either an HTTP 301 or 302.You can try the Class Based View called RedirectViewNotice how as url in the <url_to_home_view> you need to actually specify the url.permanent=False will return HTTP 302, while permanent=True will return HTTP 301.Alternatively you can use django.shortcuts.redirectIn Django 1.8, this is how I did mine.Instead of using url, you can use the pattern_name, which is a bit un-DRY, and will ensure you change your url, you don't have to change the redirect too.If you are stuck on django 1.2 like I am and RedirectView doesn't exist, another route-centric way to add the redirect mapping is using:You can also re-route everything on a match. This is useful when changing the folder of an app but wanting to preserve bookmarks:This is preferable to django.shortcuts.redirect if you are only trying to modify your url routing and do not have access to .htaccess, etc (I'm on Appengine and app.yaml doesn't allow url redirection at that level like an .htaccess).Another way of doing it is using HttpResponsePermanentRedirect like so:In view.pyIn the url.pyThe other methods work fine, but you can also use the good old django.shortcut.redirect.The code below was taken from this answer.In Django 2.x:

What is the Python equivalent of Matlab's tic and toc functions?

Alex

[What is the Python equivalent of Matlab's tic and toc functions?](https://stackoverflow.com/questions/5849800/what-is-the-python-equivalent-of-matlabs-tic-and-toc-functions)

What is the Python equivalent of Matlab's tic and toc functions?

2011-05-01 16:46:34Z

What is the Python equivalent of Matlab's tic and toc functions?Apart from timeit which ThiefMaster mentioned, a simple way to do it is just (after importing time):I have a helper class I like to use:It can be used as a context manager:Sometimes I find this technique more convenient than timeit - it all depends on what you want to measure.I had the same question when I migrated to python from Matlab.  With the help of this thread I was able to construct an exact analog of the Matlab tic() and toc() functions.  Simply insert the following code at the top of your script.That's it! Now we are ready to fully use tic() and toc() just as in Matlab.  For exampleActually, this is more versatile than the built-in Matlab functions.  Here, you could create another instance of the TicTocGenerator to keep track of multiple operations, or just to time things differently. For instance, while timing a script, we can now time each piece of the script seperately, as well as the entire script. (I will provide a concrete example)Now you should be able to time two separate things: In the following example, we time the total script and parts of a script separately.Actually, you do not even need to use tic() each time. If you have a series of commands that you want to time, then you can writeI hope that this is helpful.The absolute best analog of tic and toc would be to simply define them in python.Then you can use them as:Usually, IPython's %time, %timeit, %prun and %lprun (if one has line_profiler installed) satisfy my profiling needs quite well. However, a use case for tic-toc-like functionality arose when I tried to profile calculations that were interactively driven, i.e., by the user's mouse motion in a GUI. I felt like spamming tics and tocs in the sources while testing interactively would be the fastest way to reveal the bottlenecks. I went with Eli Bendersky's Timer class, but wasn't fully happy, since it required me to change the indentation of my code, which can be inconvenient in some editors and confuses the version control system. Moreover, there may be the need to measure the time between points in different functions, which wouldn't work with the with statement. After trying lots of Python cleverness, here is the simple solution that I found worked best:Since this works by pushing the starting times on a stack, it will work correctly for multiple levels of tics and tocs. It also allows one to change the format string of the toc statement to display additional information, which I liked about Eli's Timer class.For some reason I got concerned with the overhead of a pure Python implementation, so I tested  a C extension module as well:This is for MacOSX, and I have omitted code to check if lvl is out of bounds for brevity. While tictoc.res() yields a resolution of about 50 nanoseconds on my system, I found that the jitter of measuring any Python statement is easily in the microsecond range (and much more when used from IPython). At this point, the overhead of the Python implementation becomes negligible, so that it can be used with the same confidence as the C implementation. I found that the usefulness of the tic-toc-approach is practically limited to code blocks that take more than 10 microseconds to execute. Below that, averaging strategies like in timeit are required to get a faithful measurement. Just in case someone is interested. Based on all the other answers I wrote a tictoc class that has the best of them all.The link on github is here.You can also use pip to get it.As of how to use it:Without creating any object you can time your code as follow.Or by creating an object you can do de same.You can also call the tic toc explicitply as shown bellow.If you want to time multiple levels of your code, you can also do it by setting 'indentation' to True.The class has 3 arguments: name,method, and indentation.The method argument can be either int, str, or your method choice. If it's a string, the valid values are time, perf_counter, and process_time. If it's an integer, the valid values are 0, 1, and 2.If python version >= 3.7:

- time_ns or 3: time.time_ns

- perf_counter_ns or 4: time.perf_counter_ns

- process_time_ns or 5: time.process_time_nsIn case you prefere to use other method you just do (using as example time.clock:The class is the following:I have just created a module [tictoc.py] for achieving nested tic tocs, which is what Matlab does.And it works this way:I hope it helps.Have a look at the timeit module.

It's not really equivalent but if the code you want to time is inside a function you can easily use it.This can also be done using a wrapper. Very general way of keeping time.The wrapper in this example code wraps any function and prints the amount of time needed to execute the function:I changed @Eli Bendersky's answer a little bit to use the ctor __init__() and dtor __del__() to do the timing, so that it can be used more conveniently without indenting the original code:To use, simple put Timer("blahblah") at the beginning of some local scope. Elapsed time will be printed at the end of the scope:It prints out:Updating Eli's answer to Python 3:Just like Eli's, it can be used as a context manager:Output: I have also updated it to print the units of time reported (seconds) and trim the number of digits as suggested by Can, and with the option of also appending to a log file. You must import datetime to use the logging feature:Building on Stefan and antonimmo's answers, I ended up puttingin a utils.py module, and I use it with aThis way(here toc does not print the elapsed time, but returns it.)In the code:Disclaimer: I'm the author of this library.

pip throws TypeError: parse() got an unexpected keyword argument 'transport_encoding' when trying to install new packages

thewayup

[pip throws TypeError: parse() got an unexpected keyword argument 'transport_encoding' when trying to install new packages](https://stackoverflow.com/questions/46499808/pip-throws-typeerror-parse-got-an-unexpected-keyword-argument-transport-enco)

I am using the latest version of Anaconda3. I just installed it and I am trying to download some packages. I am using the Anaconda Prompt. While trying to use pip to do anything (including upgrading existing packages) I get the following traceback.Any ideas? (this problem only started after I installed tensorflow) Thanks. 

2017-09-30 05:19:06Z

I am using the latest version of Anaconda3. I just installed it and I am trying to download some packages. I am using the Anaconda Prompt. While trying to use pip to do anything (including upgrading existing packages) I get the following traceback.Any ideas? (this problem only started after I installed tensorflow) Thanks. I had the same problem and what worked for me was updating pip with conda:conda install pipIt changed my pip from  9.0.1-py36hadba87b_3 to 9.0.1-py36h226ae91_4 and solved issue.download https://github.com/html5lib/html5lib-python/tree/master/html5lib and overwrite all the files within html5lib folder in your tensorflow environment 

"envs\tensorflow\Lib\site-packages\html5lib"

Then you should be able to run any "pip install" commands after thatI ran into the same problem while installing keras (after I installed tensorflow 1.3 using pip) on the latest version of Anaconda 3. I was able to fix the problem by installing keras using conda conda install -c conda-forge keras I was getting this exact error installing SerpentAI. All I did to fix it was run activate serpent in conda prompt and then I ran the command again. Not sure if it's applicable to your situation, but they seem close enough that it might.EDIT - if the above didn't work, comment out this line:

That worked perfectly for me.

(this took a helpful member of our community 8 hours to debug)This worked for me:worked for mehere's the html5lib bug on github from: https://stackoverflow.com/a/39087283Here was the fix for me:Contents of dir:Run:pip3 works fine after. Was loading the old 0.999 version.

How to check if a file is a valid image file?

Sujoy

[How to check if a file is a valid image file?](https://stackoverflow.com/questions/889333/how-to-check-if-a-file-is-a-valid-image-file)

I am currently using PIL.However, while this sufficiently covers most cases, some image files like, xcf, svg and psd are not being detected. Psd files throws an OverflowError exception.Is there someway I could include them as well?

2009-05-20 17:55:16Z

I am currently using PIL.However, while this sufficiently covers most cases, some image files like, xcf, svg and psd are not being detected. Psd files throws an OverflowError exception.Is there someway I could include them as well?A lot of times the first couple chars will be a magic number for various file formats.  You could check for this in addition to your exception checking above. I have just found the builtin imghdr module. From python documentation:This is how it works:Using a module is much better than reimplementing similar functionalityIn addition to what Brian is suggesting you could use PIL's verify method to check if the file is broken.UpdateI also implemented the following solution in my Python script here on GitHub.I also verified that damaged files (jpg) frequently are not 'broken' images i.e, a damaged picture file sometimes remains a legit picture file, the original image is lost or altered but you are still able to load it with no errors. But, file truncation cause always errors. End UpdateYou can use Python Pillow(PIL) module, with most image formats, to check if a file is a valid and intact image file.In the case you aim at detecting also broken images, @Nadia Alramli correctly suggests the im.verify() method, but this does not detect all the possible image defects, e.g., im.verify does not detect truncated images (that most viewers often load with a greyed area).Pillow is able to detect these type of defects too, but you have to apply image manipulation or image decode/recode in or to trigger the check. Finally I suggest to use this code:In case of image defects this code will raise an exception.

Please consider that im.verify is about 100 times faster than performing the image manipulation (and I think that flip is one of the cheaper transformations).

With this code you are going to verify a set of images at about 10 MBytes/sec with standard Pillow or 40 MBytes/sec with Pillow-SIMD module (modern 2.5Ghz x86_64 CPU).For the other formats psd,xcf,.. you can use Imagemagick wrapper Wand, the code is as follows:But, from my experiments Wand does not detect truncated images, I think it loads lacking parts as greyed area without prompting. I red that Imagemagick has an external command identify that could make the job, but I have not found a way to invoke that function programmatically and I have not tested this route.I suggest to always perform a preliminary check, check the filesize to not be zero (or very small), is a very cheap idea:Additionally to the PIL image check you can also add file name extension check like this:Note that this only checks if the file name has a valid image extension, it does not actually open the image to see if it's a valid image, that's why you need to use additionally PIL or one of the libraries suggested in the other answers.On Linux, you could use python-magic (http://pypi.python.org/pypi/python-magic/0.1) which uses libmagic to identify file formats.AFAIK, libmagic looks into the file and tries to tell you more about it than just the format, like bitmap dimensions, format version etc.. So you might see this as a superficial test for "validity".For other definitions of "valid" you might have to write your own tests.You could use the Python bindings to libmagic, python-magic and then check the mime types. This won't tell you if the files are corrupted or intact but it should be able to determine what type of image it is.Well, I do not know about the insides of psd, but I, sure, know that, as a matter of fact, svg is not an image file per se, -- it is based on xml, so it is, essentially, a plain text file.Would checking the file extensions be acceptable or are you trying to confirm the data itself represents an image file?If you can check the file extension a regular expression or a simple comparison could satisfy the requirement.

How to upload a file to directory in S3 bucket using boto

Dheeraj Gundra

[How to upload a file to directory in S3 bucket using boto](https://stackoverflow.com/questions/15085864/how-to-upload-a-file-to-directory-in-s3-bucket-using-boto)

I want to copy a file in s3 bucket using python.Ex : I have bucket name = test. And in the bucket, I have 2 folders name "dump" & "input". Now I want to copy a file from local directory to S3 "dump" folder using python... Can anyone help me?

2013-02-26 09:47:53Z

I want to copy a file in s3 bucket using python.Ex : I have bucket name = test. And in the bucket, I have 2 folders name "dump" & "input". Now I want to copy a file from local directory to S3 "dump" folder using python... Can anyone help me?Try this...[UPDATE]

I am not a pythonist, so thanks for the heads up about the import statements.

Also, I'd not recommend placing credentials inside your own source code. If you are running this inside AWS use IAM Credentials with Instance Profiles (http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html), and to keep the same behaviour in your Dev/Test environment, use something like Hologram from AdRoll (https://github.com/AdRoll/hologram)No need to make it that complicated:I used this and it is very simple to implementhttps://www.smore.com/labs/tinys3/This will also work:Upload file to s3 within a session with credentials.This is a three liner. Just follow the instructions on the boto3 documentation.Some important arguments are:

Parameters:

For upload folder example as following code and S3 folder picture

PS: For more reference URLUsing boto3For more:-

https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html

how to convert an RGB image to numpy array?

Shan

[how to convert an RGB image to numpy array?](https://stackoverflow.com/questions/7762948/how-to-convert-an-rgb-image-to-numpy-array)

I have an RGB image. I want to convert it to numpy array. I did the followingIt creates an array with no shape. I assume it is a iplimage object.

2011-10-14 04:13:07Z

I have an RGB image. I want to convert it to numpy array. I did the followingIt creates an array with no shape. I assume it is a iplimage object.You can use newer OpenCV python interface (if I'm not mistaken it is available since OpenCV 2.2). It natively uses numpy arrays:result:PIL (Python Imaging Library) and Numpy work well together.I use the following functions.The 'Image.fromarray' is a little ugly because I clip incoming data to [0,255], convert to bytes, then create a grayscale image. I mostly work in gray.An RGB image would be something like:You can also use matplotlib for this.output:

<class 'numpy.ndarray'>As of today, your best bet is to use:You'll see img will be a numpy array of type:Late answer, but I've come to prefer the imageio module to the other alternativesSimilar to cv2.imread(), it produces a numpy array by default, but in RGB form.You need to use cv.LoadImageM instead of cv.LoadImage:When using the answer from David Poole I get a SystemError with gray scale PNGs and maybe other files. My solution is:Actually img.getdata() would work for all files, but it's slower, so I use it only when the other method fails.I also adopted imageio, but I found the following machinery useful for pre- and post-processing:The rationale is that I am using numpy for image processing, not just image displaying. For this purpose, uint8s are awkward, so I convert to floating point values ranging from 0 to 1.When saving images, I noticed I had to cut the out-of-range values myself, or else I ended up with a really gray output. (The gray output was the result of imageio compressing the full range, which was outside of [0, 256), to values that were inside the range.)There were a couple other oddities, too, which I mentioned in the comments.load the image by using following syntax:-

Remove and Replace Printed items [duplicate]

Alex 

[Remove and Replace Printed items [duplicate]](https://stackoverflow.com/questions/5290994/remove-and-replace-printed-items)

I was wondering if it was possible to remove items you have printed in Python - not from the Python GUI, but from the command prompt.

e.g.so it printsBut, my problem is I want this all on one line, and for it it remove it self when something else comes along. So instead of printing "Loading", "Loading.", "Loading... I want it to print "Loading.", then it removes what is on the line and replaces it with "Loading.." and then removes "Loading.." and replaces it (on the same line) with "Loading...". It's kind of hard to describe.p.s I have tried to use the Backspace character but it doesn't seem to work ("\b")Thanks

2011-03-13 17:15:54Z

I was wondering if it was possible to remove items you have printed in Python - not from the Python GUI, but from the command prompt.

e.g.so it printsBut, my problem is I want this all on one line, and for it it remove it self when something else comes along. So instead of printing "Loading", "Loading.", "Loading... I want it to print "Loading.", then it removes what is on the line and replaces it with "Loading.." and then removes "Loading.." and replaces it (on the same line) with "Loading...". It's kind of hard to describe.p.s I have tried to use the Backspace character but it doesn't seem to work ("\b")ThanksJust use CR to go to beginning of the line.One way is to use ANSI escape sequences:Also sometimes useful (for example if you print something shorter than before):Note that you might have to run sys.stdout.flush() right after sys.stdout.write('\r'+b) depending on which console you are doing the printing to have the results printed when requested without any buffering.

How to drop rows from pandas data frame that contains a particular string in a particular column? [duplicate]

London guy

[How to drop rows from pandas data frame that contains a particular string in a particular column? [duplicate]](https://stackoverflow.com/questions/28679930/how-to-drop-rows-from-pandas-data-frame-that-contains-a-particular-string-in-a-p)

I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.For example, I want to drop all rows which have the string "XYZ" as a substring in the column C of the data frame.Can this be implemented in an efficient way using .drop() method?

2015-02-23 17:43:01Z

I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.For example, I want to drop all rows which have the string "XYZ" as a substring in the column C of the data frame.Can this be implemented in an efficient way using .drop() method?pandas has vectorized string operations, so you can just filter out the rows that contain the string you don't want:If your string constraint is not just one string you can drop those corresponding rows with:The above will drop all rows containing elements of your listThis will only work if you want to compare exact strings.

It will not work in case you want to check if the column string contains any of the strings in the list.The right way to compare with a list would be : Slight modification to the code. Having na=False will skip empty values. Otherwise you can get an error TypeError: bad operand type for unary ~: floatSource: TypeError: bad operand type for unary ~: floatReference: https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/The below code will give you list of all the rows:- To store the values from the above code into a dataframe :-if you do not want to delete all NaN, use

Check to see if python script is running

Josh Hunt

[Check to see if python script is running](https://stackoverflow.com/questions/788411/check-to-see-if-python-script-is-running)

I have a python daemon running as a part of my web app/ How can I quickly check (using python) if my daemon is running and, if not, launch it?I want to do it that way to fix any crashes of the daemon, and so the script does not have to be run manually, it will automatically run as soon as it is called and then stay running.How can i check (using python) if my script is running?

2009-04-25 06:59:10Z

I have a python daemon running as a part of my web app/ How can I quickly check (using python) if my daemon is running and, if not, launch it?I want to do it that way to fix any crashes of the daemon, and so the script does not have to be run manually, it will automatically run as soon as it is called and then stay running.How can i check (using python) if my script is running?Drop a pidfile somewhere (e.g. /tmp). Then you can check to see if the process is running by checking to see if the PID in the file exists. Don't forget to delete the file when you shut down cleanly, and check for it when you start up.Then you can check to see if the process is running by checking to see if the contents of /tmp/mydaemon.pid are an existing process. Monit (mentioned above) can do this for you, or you can write a simple shell script to check it for you using the return code from ps.For extra credit, you can use the atexit module to ensure that your program cleans up its pidfile under any circumstances (when killed, exceptions raised, etc.).A technique that is handy on a Linux system is using domain sockets:It is atomic and avoids the problem of having lock files lying around if your process gets sent a SIGKILLYou can read in the documentation for socket.close that sockets are automatically closed when garbage collected. The pid library can do exactly this.It will also automatically handle the case where the pidfile exists but the process is not running.There are very good packages for restarting processes on UNIX.  One that has a great tutorial about building and configuring it is monit.   With some tweaking you can have a rock solid proven technology keeping up your daemon.Of course the example from Dan will not work as it should be.Indeed, if the script crash, rise an exception, or does not clean pid file, the script will be run multiple times.I suggest the following based from another website: This is to check if there is already a lock file existingThis is part of code where we put a PID file in the lock fileThis code will check the value of pid compared to existing running process., avoiding double execution.I hope it will help.My solution is to check for the process and command line arguments

Tested on windows and ubuntu linuxThere are a myriad of options. One method is using system calls or python libraries that perform such calls for you. The other is simply to spawn out a process like:and parse the output. Many people choose this approach, it isn't necessarily a bad approach in my view.Came across this old question looking for solution myself.Use psutil:I'm a big fan of Supervisor for managing daemons.  It's written in Python, so there are plenty of examples of how to interact with or extend it from Python.  For your purposes the XML-RPC process control API should work nicely.Try this other versionRather than developing your own PID file solution (which has more subtleties and corner cases than you might think), have a look at supervisord -- this is a process control system that makes it easy to wrap job control and daemon behaviors around an existing Python script.The other answers are great for things like cron jobs, but if you're running a daemon you should monitor it with something like daemontools.if yor debug script in pycharm always exittry this:

Here is more useful code (with checking if exactly python executes the script):Here is string:returns 0 if "grep"  is successful, and the process "python" is currently running with the name of your script as a parameter .A simple example if you only are looking for a process name exist or not:Consider the following example to solve your problem:I suggest this script because it can be executed one time only.Using bash to look for a process with the current script's name. No extra file.To test, addThis is what I use in Linux to avoid starting a script if already running:This approach works good without any dependency on an external module. 

How to pip or easy_install tkinter on Windows

Dirk Calloway

[How to pip or easy_install tkinter on Windows](https://stackoverflow.com/questions/20044559/how-to-pip-or-easy-install-tkinter-on-windows)

My Idle is throwing errors that and says tkinter can't be imported.Is there a simple way to install tkinter via pip or easy_install?There seem to be a lot of package names flying around for this...This and other assorted variations with tkinter-pypy aren't working.I'm on Windows with Python 2.7 and can't apt-get.Thanks.

2013-11-18 09:43:31Z

My Idle is throwing errors that and says tkinter can't be imported.Is there a simple way to install tkinter via pip or easy_install?There seem to be a lot of package names flying around for this...This and other assorted variations with tkinter-pypy aren't working.I'm on Windows with Python 2.7 and can't apt-get.Thanks.Well I can see two solutions here:1) Follow the Docs-Tkinter install for Python (for Windows): Tkinter (and, since Python 3.1, ttk) are included with all standard Python distributions. It is important that you use a version of Python supporting Tk 8.5 or greater, and ttk. We recommend installing the "ActivePython" distribution from ActiveState, which includes everything you'll need.In your web browser, go to Activestate.com, and follow along the links to download the Community Edition of ActivePython for Windows. Make sure you're downloading a 3.1 or newer version, not a 2.x version.Run the installer, and follow along. You'll end up with a fresh install of ActivePython, located in, e.g. C:\python32. From a Windows command prompt, or the Start Menu's "Run..." command, you should then be able to run a Python shell via:This should give you the Python command prompt. From the prompt, enter these two commands:This should pop up a small window; the first line at the top of the window should say "This is Tcl/Tk version 8.5"; make sure it is not 8.4!2) Uninstall 64-bit Python and install 32 bit Python.The Tkinter library is built-in with every Python installation. And since you are on Windows, I believe you installed Python through the binaries on their website?If so, Then most probably you are typing the command wrong. It should be:Note the capital T at the beginning of Tkinter.For Python 3,If you are using virtualenv, it is fine to install tkinter using sudo apt-get install python-tk(python2), sudo apt-get install python3-tk(python3), and and it will work fine in the virtual environmentWhen you install python for Windows, use the standard option or install everything it asks. I got the error because I deselected tcl.When installing make sure that under Tcl/Tk you select Will be installed on hard drive.  If it is installing with a cross at the left then Tkinter will not be installed.The same goes for Python 3:Had the same problem in Linux. This solved it. (I'm on Debian 9 derived Bunsen Helium)I had the similar problem with Win-8 and python-3.4 32 bit , I got it resolved by downloading same version from python.org . Next step will be to hit the repair button and Install the Tk/tkinter Package or Just hit the repair.

Now  should get Python34/Lib/tkinter Module present.

The import tkinter should work  ..I solved the same problem using these two commands 100%In python Tkinter was default package ,repair at the time we can select the  Tcl/Tk , in c directory the tkinter stored in C:\Python27\DLLs_tkinter.pyd on that place , reinstall otherwise directly put the 

(_tkinter.pyd)file into DLLsenter image description hereI'm posting as the top answer requotes the documentation which I didn't find  useful.tkinter comes packaged with python install on windows IFF you select it during the install window. The solution is to repair the installation (via uninstall GUI is fine), and select to install tk this time. You may need to point at or redownload the binary in this process. Downloading directly from activestate did not work for me. This is a common problem people have on windows as it's easy to not want to install TCL/TK if you don't know what it is, but Matplotlib etc require it. on windows terminal , run command 'pip install tk'** Easiest Way And Screenshot of installing**if your using python 3.4.1 just write this line from tkinter import * this will put  everything in the module into the default namespace of your program. in fact instead of referring to say a button like tkinter.Button you just type Button

Is there a WebSocket client implemented for Python? [closed]

diegueus9

[Is there a WebSocket client implemented for Python? [closed]](https://stackoverflow.com/questions/3142705/is-there-a-websocket-client-implemented-for-python)

I found this project: http://code.google.com/p/standalonewebsocketserver/ for a WebSocket server, but I need to implement a WebSocket client in python, more exactly I need to receive some commands from XMPP in my WebSocket server.

2010-06-29 16:28:54Z

I found this project: http://code.google.com/p/standalonewebsocketserver/ for a WebSocket server, but I need to implement a WebSocket client in python, more exactly I need to receive some commands from XMPP in my WebSocket server.http://pypi.python.org/pypi/websocket-client/Ridiculously easy to use.Sample client code:Sample server code:Autobahn has a good websocket client implementation for Python as well as some good examples.  I tested the following with a Tornado WebSocket server and it worked.Since I have been doing a bit of research in that field lately (Jan, '12), the most promising client is actually : WebSocket for Python. It support a normal socket that you can call like this :The client can be Threaded or based on IOLoop from Tornado project. This will allow you to create a multi concurrent connection client. Useful if you want to run stress tests.The client also exposes the onmessage, opened and closed methods. (WebSocket style).web2py has comet_messaging.py, which uses Tornado for websockets look at an example here: http://vimeo.com/18399381 and here vimeo . com / 18232653

Detect if a NumPy array contains at least one non-numeric value?

Salim Fadhley

[Detect if a NumPy array contains at least one non-numeric value?](https://stackoverflow.com/questions/911871/detect-if-a-numpy-array-contains-at-least-one-non-numeric-value)

I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only return a numeric value). The number of dimensions of the input array is not known in advance - the function should give the correct value regardless of ndim. As an extra complication the input could be a single float or numpy.float64 or even something oddball like a zero-dimensional array. The obvious way to solve this is to write a recursive function which iterates over every iterable object in the array until it finds a non-iterabe. It will apply the numpy.isnan() function over every non-iterable object. If at least one non-numeric value is found then the function will return False immediately. Otherwise if all the values in the iterable are numeric it will eventually return True. That works just fine, but it's pretty slow and I expect that NumPy has a much better way to do it. What is an alternative that is faster and more numpyish?Here's my mockup:

2009-05-26 17:43:47Z

I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only return a numeric value). The number of dimensions of the input array is not known in advance - the function should give the correct value regardless of ndim. As an extra complication the input could be a single float or numpy.float64 or even something oddball like a zero-dimensional array. The obvious way to solve this is to write a recursive function which iterates over every iterable object in the array until it finds a non-iterabe. It will apply the numpy.isnan() function over every non-iterable object. If at least one non-numeric value is found then the function will return False immediately. Otherwise if all the values in the iterable are numeric it will eventually return True. That works just fine, but it's pretty slow and I expect that NumPy has a much better way to do it. What is an alternative that is faster and more numpyish?Here's my mockup:This should be faster than iterating and will work regardless of shape.Edit: 30x faster:Results:Bonus: it works fine for non-array NumPy types:If infinity is a possible value, I would use numpy.isfiniteIf the above evaluates to True, then myarray contains no, numpy.nan, numpy.inf or -numpy.inf values.numpy.nan will be OK with numpy.inf values, for example:With numpy 1.3 or svn you can do thisThe treatment of nans in comparisons was not consistent in earlier versions.Pfft! Microseconds!

Never solve a problem in microseconds that can be solved in nanoseconds.Note that the accepted answer:A better solution is to return True immediately when NAN is found:and works for n-dimensions:Compare this to the numpy native solution:The early-exit method is 3 orders or magnitude speedup (in some cases).

Not too shabby for a simple annotation.(np.where(np.isnan(A)))[0].shape[0] will be greater than 0 if A contains at least one element of nan, A could be an n x m matrix.Example:

One-liner to check whether an iterator yields at least one element?

Bastien Léonard

[One-liner to check whether an iterator yields at least one element?](https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element)

Currently I'm doing this:But I would like an expression that I can place inside a simple if statement.

Is there anything built-in which would make this code look less clumsy?any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not.

I only need it to check the first item.Someone asks what I'm trying to do.

I have written a function which executes an SQL query and yields its results.

Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.

2010-06-24 22:06:33Z

Currently I'm doing this:But I would like an expression that I can place inside a simple if statement.

Is there anything built-in which would make this code look less clumsy?any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not.

I only need it to check the first item.Someone asks what I'm trying to do.

I have written a function which executes an SQL query and yields its results.

Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.any won't go beyond the first element if it's True. In case the iterator yields something false-ish you can write any(True for _ in iterator).In Python 2.6+, if name sentinel is bound to a value which the iterator can't possibly yield,If you have no idea of what the iterator might possibly yield, make your own sentinel (e.g. at the top of your module) withOtherwise, you could use, in the sentinel role, any value which you "know" (based on application considerations) that the iterator can't possibly yield.This isn't really cleaner, but it shows a way to package it in a function losslessly:This isn't really pythonic, and for particular cases, there are probably better (but less general) solutions, like the next default. This isn't general because None can be a valid element in many iterables. you can use:but it's a bit nonexplanatory for the code readerThe best way to do that is with a peekable from more_itertools.Just beware if you kept refs to the old iterator, that iterator will get advanced. You have to use the new peekable iterator from then on. Really, though, peekable expects to be the only bit of code modifying that old iterator, so you shouldn't be keeping refs to the old iterator lying around anyway.What about:__length_hint__ estimates the length of list(it) - it's private method, though:This is an overkill iterator wrapper that generally allows to check whether there's a next item (via conversion to boolean).  Of course pretty inefficient.Output:A little late, but... You could turn the iterator into a list and then work with that list:

Python Regex instantly replace groups

Martin Ender

[Python Regex instantly replace groups](https://stackoverflow.com/questions/14007545/python-regex-instantly-replace-groups)

Is there any way to directly replace all groups using regex syntax?The normal way:But I want to achieve something like this:I want to build the new string instantaneously from the groups the Regex just captured.

2012-12-22 23:45:57Z

Is there any way to directly replace all groups using regex syntax?The normal way:But I want to achieve something like this:I want to build the new string instantaneously from the groups the Regex just captured.Have a look at re.sub:This is Python's regex substitution (replace) function. The replacement string can be filled with so-called backreferences (backslash, group number) which are replaced with what was matched by the groups. Groups are counted the same as by the group(...) function, i.e. starting from 1, from left to right, by opening parentheses.  The accepted answer is perfect. I would add that group reference is probably better achieved by using this syntax:for the replacement string. This way, you work around syntax limitations where a group may be followed by a digit. Again, this is all present in the doc, nothing new, just sometimes difficult to spot at first sight.

How to create abstract properties in python abstract classes

Boris Gorelik

[How to create abstract properties in python abstract classes](https://stackoverflow.com/questions/5960337/how-to-create-abstract-properties-in-python-abstract-classes)

In the following code, I create a base abstract class Base. I want all the classes that inherit from Base to provide the name property, so I made this property an @abstractmethod.Then I created a subclass of Base, called Base_1, which is meant to supply some functionality, but still remain abstract. There is no name property in Base_1, but nevertheless python instatinates an object of that class without an error. How does one create abstract properties?

2011-05-11 06:49:24Z

In the following code, I create a base abstract class Base. I want all the classes that inherit from Base to provide the name property, so I made this property an @abstractmethod.Then I created a subclass of Base, called Base_1, which is meant to supply some functionality, but still remain abstract. There is no name property in Base_1, but nevertheless python instatinates an object of that class without an error. How does one create abstract properties?Since Python 3.3 a bug was fixed meaning the property() decorator is now correctly identified as abstract when applied to an abstract method.Note: Order matters, you have to use @property before @abstractmethodPython 3.3+: (python docs):Python 2: (python docs)Until Python 3.3, you cannot nest @abstractmethod and @property.Use @abstractproperty to create abstract properties (docs).The code now raises the correct exception:Based on James answer above and use it as a decorator

Differences between numpy.random and random.random in Python

Laura

[Differences between numpy.random and random.random in Python](https://stackoverflow.com/questions/7029993/differences-between-numpy-random-and-random-random-in-python)

I have a big script in Python. I inspired myself in other people's code so I ended up using the numpy.random module for some things (for example for creating an array of random numbers taken from a binomial distribution) and in other places I use the module random.random.Can someone please tell me the major differences between the two?

Looking at the doc webpage for each of the two it seems to me that numpy.random just has more methods, but I am unclear about how the generation of the random numbers is different.The reason why I am asking is because I need to seed my main program for debugging purposes. But it doesn't work unless I use the same random number generator in all the modules that I am importing, is this correct?Also, I read here, in another post, a discussion about NOT using numpy.random.seed(), but I didn't really understand why this was such a bad idea. I would really appreciate if someone explain me why this is the case.

2011-08-11 17:05:54Z

I have a big script in Python. I inspired myself in other people's code so I ended up using the numpy.random module for some things (for example for creating an array of random numbers taken from a binomial distribution) and in other places I use the module random.random.Can someone please tell me the major differences between the two?

Looking at the doc webpage for each of the two it seems to me that numpy.random just has more methods, but I am unclear about how the generation of the random numbers is different.The reason why I am asking is because I need to seed my main program for debugging purposes. But it doesn't work unless I use the same random number generator in all the modules that I am importing, is this correct?Also, I read here, in another post, a discussion about NOT using numpy.random.seed(), but I didn't really understand why this was such a bad idea. I would really appreciate if someone explain me why this is the case.You have made many correct observations already!Unless you'd like to seed both of the random generators, it's probably simpler in the long run to choose one generator or the other. For numpy.random.seed(), the main difficulty is that it is not thread-safe - that is, it's not safe to use if you have many different threads of execution, because it's not guaranteed to work if two different threads are executing the function at the same time. If you're not using threads, and if you can reasonably expect that you won't need to rewrite your program this way in the future, numpy.random.seed() should be fine.  If there's any reason to suspect that you may need threads in the future, it's much safer in the long run to do as suggested, and to make a local instance of the numpy.random.Random class. As far as I can tell, random.random.seed() is thread-safe (or at least, I haven't found any evidence to the contrary).The numpy.random library contains a few extra probability distributions commonly used in scientific research, as well as a couple of convenience functions for generating arrays of random data. The random.random library is a little more lightweight, and should be fine if you're not doing scientific research or other kinds of work in statistics.Otherwise, they both use the Mersenne twister sequence to generate their random numbers, and they're both completely deterministic - that is, if you know a few key bits of information, it's possible to predict with absolute certainty what number will come next. For this reason, neither numpy.random nor random.random is suitable for any serious cryptographic uses. But because the sequence is so very very long, both are fine for generating random numbers in cases where you aren't worried about people trying to reverse-engineer your data. This is also the reason for the necessity to seed the random value - if you start in the same place each time, you'll always get the same sequence of random numbers!As a side note, if you do need cryptographic level randomness, you should use the secrets module, or something like Crypto.Random if you're using a Python version earlier than Python 3.6.From Python for Data Analysis, the module numpy.random supplements the Python random with functions for efficiently generating whole arrays of sample values from many kinds of probability distributions.By contrast, Python's built-in random module only samples one value at a time, while numpy.random can generate very large sample faster. Using IPython magic function %timeit one can see which module performs faster:The source of the seed and the distribution profile used are going to affect the outputs - if you are looking for cryptgraphic randomness, seeding from os.urandom() will get nearly real random bytes from device chatter (ie ethernet or disk) (ie /dev/random on BSD)this will avoid you giving a seed and so generating determinisitic random numbers.  However the random calls then allow you to fit the numbers to a distribution (what I call scientific random ness - eventually all you want is a bell curve distribution of random numbers, numpy is best at delviering this.SO yes, stick with one generator, but decide what random you want - random, but defitniely from a distrubtuion curve, or as random as you can get without a quantum device.

What kinds of patterns could I enforce on the code to make it easier to translate to another programming language? [closed]

NullUserException

[What kinds of patterns could I enforce on the code to make it easier to translate to another programming language? [closed]](https://stackoverflow.com/questions/3455456/what-kinds-of-patterns-could-i-enforce-on-the-code-to-make-it-easier-to-translat)

I am setting out to do a side project that has the goal of translating code from one programming language to another. The languages I am starting with are PHP and Python (Python to PHP should be easier to start with), but ideally I would be able to add other languages with (relative) ease. The plan is:Then I believe I can start outputting code. I don't need a perfect translation. I'll still have to review the generated code and fix problems. Ideally the translator should flag problematic translations.Before you ask "What the hell is the point of this?" The answer is... It'll be an interesting learning experience. If you have any insights on how to make this less daunting, please let me know.I am more interested in knowing what kinds of patterns I could enforce on the code to make it easier to translate (ie: IoC, SOA ?) the code than how to do the translation.

2010-08-11 05:14:45Z

I am setting out to do a side project that has the goal of translating code from one programming language to another. The languages I am starting with are PHP and Python (Python to PHP should be easier to start with), but ideally I would be able to add other languages with (relative) ease. The plan is:Then I believe I can start outputting code. I don't need a perfect translation. I'll still have to review the generated code and fix problems. Ideally the translator should flag problematic translations.Before you ask "What the hell is the point of this?" The answer is... It'll be an interesting learning experience. If you have any insights on how to make this less daunting, please let me know.I am more interested in knowing what kinds of patterns I could enforce on the code to make it easier to translate (ie: IoC, SOA ?) the code than how to do the translation.I've been building tools (DMS Software Reengineering Toolkit) to do general purpose program manipulation (with language translation being a special case) since 1995, supported by a strong team of computer scientists.  DMS provides generic parsing, AST building, symbol tables, control and data flow analysis, application of translation rules, regeneration of source text with comments, etc., all parameterized by explicit definitions of computer languages.The amount of machinery you need to do this well is vast (especially if you want to be able to do this for multiple languages in a general way), and then you need reliable parsers for languages with unreliable definitions (PHP is perfect example of this).There's nothing wrong with you thinking about building a language-to-language translator or attempting it, but I think you'll find this a much bigger task for real languages than you expect.  We have some 100 man-years invested in just DMS, and another 6-12 months in each "reliable" language definition (including the one we painfully built for PHP), much more for nasty languages such as C++.  It will be a "hell of a learning experience"; it has been for us.  (You might find the technical Papers section at the above website interesting to jump start that learning).People often attempt to build some kind of generalized machinery by starting with some piece of technology with which they are familiar, that does a part of the job. (Python ASTs are great example).  The good news, is that part of the job is done.  The bad news is that machinery has a zillion assumptions built into it, most of which you won't discover until you try to wrestle it into doing something else.  At that point you find out the machinery is wired to do what it originally does, and will really, really resist your attempt to make it do something else. (I suspect trying to get the Python AST to model PHP is going to be a lot of fun).The reason I started to build DMS originally was to build foundations that had very few such assumptions built in.   It has some that give us headaches.  So far, no black holes. (The hardest part of my job over the last 15 years is to try to prevent such assumptions from creeping in).Lots of folks also make the mistake of assuming that if they can parse (and perhaps get an AST), they are well on the way to doing something complicated.  One of the hard lessons is that you need symbol tables and flow analysis to do good program analysis or transformation.   ASTs are necessary but not sufficient.  This is the reason that Aho&Ullman's compiler book doesn't stop at chapter 2.  (The OP has this right in that he is planning to build additional machinery beyond the AST).  For more on this topic, see Life After Parsing.The remark about "I don't need a perfect translation" is troublesome.  What weak translators do is convert the "easy" 80% of the code, leaving the hard 20% to do by hand.  If the application you intend to convert are pretty small, and you only intend to convert it once well, then that 20% is OK. If you want to convert many applications (or even the same one with minor changes over time), this is not nice.  If you attempt to convert 100K SLOC then 20% is 20,000 original lines of code that are hard to translate, understand and modify in the context of another 80,000 lines of translated program you already don't understand.  That takes a huge amount of effort.  At the million line level, this is simply impossible in practice.  (Amazingly there are people that distrust automated tools and insist on translating million line systems by hand; that's even harder and they normally find out painfully with long time delays, high costs and often outright failure.)What you have to shoot for to translate large-scale systems is high nineties percentage  conversion rates, or it is likely that you can't complete the manual part of the translation activity.Another key consideration is size of code to be translated.  It takes a lot of energy to build a working, robust translator, even with good tools.  While it seems sexy and cool to build a translator instead of simply doing a manual conversion, for small code bases (e.g., up to about 100K SLOC in our experience) the economics simply don't justify it. Nobody likes this answer, but if you really have to translate just 10K SLOC of code, you are probably better off just biting the bullet and doing it.  And yes, that's painful.I consider our tools to be extremely good (but then, I'm pretty biased).   And it is still very hard to build a good translator; it takes us about 1.5-2 man-years and we know how to use our tools.  The difference is that with this much machinery, we succeed considerably more often than we fail.My answer will address the specific task of parsing Python in order to translate it to another language, and not the higher-level aspects which Ira addressed well in his answer.In short: do not use the parser module, there's an easier way.The ast module, available since Python 2.6 is much more suitable for your needs, since it gives you a ready-made AST to work with. I've written an article on this last year, but in short, use the parse method of ast to parse Python source code into an AST. The parser module will give you a parse tree, not an AST. Be wary of the difference. Now, since Python's ASTs are quite detailed, given an AST the front-end job isn't terribly hard. I suppose you can have a simple prototype for some parts of the functionality ready quite quickly. However, getting to a complete solution will take more time, mainly because the semantics of the languages are different. A simple subset of the language (functions, basic types and so on) can be readily translated, but once you get into the more complex layers, you'll need heavy machinery to emulate one language's core in another. For example consider Python's generators and list comprehensions which don't exist in PHP (to my best knowledge, which is admittedly poor when PHP is involved).To give you one final tip, consider the 2to3 tool created by the Python devs to translate Python 2 code to Python 3 code. Front-end-wise, it has most of the elements you need to translate Python to something. However, since the cores of Python 2 and 3 are similar, no emulation machinery is required there.Writing a translator isn't impossible, especially considering that Joel's Intern did it over a summer.If you want to do one language, it's easy. If you want to do more, it's a little more difficult, but not too much. The hardest part is that, while any turing complete language can do what another turing complete language does, built-in data types can change what a language does phenomenally.For instance:takes a lot of C++ code to duplicate (ok, well you can do it fairly short with some looping constructs, but still).That's a bit of an aside, I guess. Have you ever written a tokenizer/parser based on a language grammar? You'll probably want to learn how to do that if you haven't, because that's the main part of this project. What I would do is come up with a basic Turing complete syntax - something fairly similar to Python  bytecode. Then you create a lexer/parser that takes a language grammar (perhaps using BNF), and based on the grammar, compiles the language into your intermediate language. Then what you'll want to do is do the reverse - create a parser from your language into target languages based on the grammar.The most obvious problem I see is that at first you'll probably create horribly inefficient code, especially in more powerful* languages like Python.But if you do it this way then you'll probably be able to figure out ways to optimize the output as you go along. To summarize:*by powerful I mean that this takes 4 lines:Show me another language that can do something like that in 4 lines, and I'll show you a language that's as powerful as Python.There are a couple answers telling you not to bother. Well, how helpful is that? You want to learn? You can learn. This is compilation. It just so happens that your target language isn't machine code, but another high-level language. This is done all the time.There's a relatively easy way to get started. First, go get http://sourceforge.net/projects/lime-php/ (if you want to work in PHP) or some such and go through the example code. Next, you can write a lexical analyzer using a sequence of regular expressions and feed tokens to the parser you generate. Your semantic actions can either output code directly in another language or build up some data structure (think objects, man) that you can massage and traverse to generate output code.You're lucky with PHP and Python because in many respects they are the same language as each other, but with different syntax. The hard part is getting over the semantic differences between the grammar forms and data structures. For example, Python has lists and dictionaries, while PHP only has assoc arrays.The "learner" approach is to build something that works OK for a restricted subset of the language (such as only print statements, simple math, and variable assignment), and then progressively remove limitations. That's basically what the "big" guys in the field all did.Oh, and since you don't have static types in Python, it might be best to write and rely on PHP functions like "python_add" which adds numbers, strings, or objects according to the way Python does it.Obviously, this can get much bigger if you let it.I will second @EliBendersky point of view regarding using ast.parse instead of parser (which I did not know about before). I also warmly recommend you to review his blog. I used ast.parse to do Python->JavaScript translator (@https://bitbucket.org/amirouche/pythonium). I've come up with Pythonium design by somewhat reviewing other implementations and trying them on my own. I forked Pythonium from https://github.com/PythonJS/PythonJS which I also started, It's actually a complete rewrite . The overall design is inspired from PyPy and http://www.hpl.hp.com/techreports/Compaq-DEC/WRL-89-1.pdf paper.Everything I tried, from beginning to the best solution, even if it looks like Pythonium marketing it really isn't (don't hesitate to tell me if something doesn't seem correct to the netiquette):I never actually succeed at running Pyjamas #fail and never tried to read the code #fail again. But in my mind PyJamas was doing API->API tranlation (or framework to framework) and not Python to JavaScript translation. The JavaScript framework consume data that is already in the page or data from the server. Python code is only "plumbing". After that I discovered  that pyjamas was actually a real python->js translator. Still I think it's possible to do API->API (or framework->framework) translation and that's basicly what I do in Pythonium but at lower level. Probably Pyjamas use the same algorithm as Pythonium...Then I discovered brython fully written in Javascript like Skulpt, no need for compilation and lot of fluff... but written in JavaScript.Since the initial line written in the course of this project, I knew about PyPy, even the JavaScript backend for PyPy. Yep, you can, if you find it, directly generate a Python interpreter in JavaScript from PyPy. People say, it was a disaster. I read no where why. But I think the reason is that the intermediate language they use to implement the interpreter, RPython, is a subset of Python tailored to be translated to C (and maybe asm). Ira Baxter says you always make assumptions when you build something and probably you fine tune it to be the best at what it's meant to do in the case of PyPy: Python->C translation. Those assumptions might not be relevant in another context worse they can infere overhead otherwise said direct translation will most likely always be better.Having the interpreter written in Python sounded like a (very) good idea. But I was more interested in a compiler for performance reasons also it's actually more easy to compile Python to JavaScript than interpret it.I started PythonJS with the idea of putting together a subset of Python that I could easily translate to JavaScript. At first I didn't even bother to implement OO system because of past experience. The subset of Python that I achieved to translate to JavaScript are:This seems like a lot but actually very narrow compared to full blown semantic of Python. It's really JavaScript with a Python syntax. The generated JS is perfect ie. there is no overhead, it can not be improved in terms of performance by further editing it. If you can improve the generated code, you can do it from the Python source file too. Also, the compiler did not rely on any JS tricks that you can find in .js written by http://superherojs.com/, so it's very readable.The direct descendant of this part of PythonJS is the Pythonium Veloce mode. The full implementation can be found @ https://bitbucket.org/amirouche/pythonium/src/33898da731ee2d768ced392f1c369afd746c25d7/pythonium/veloce/veloce.py?at=master 793 SLOC + around 100 SLOC of shared code with the other translator.An adapted version of pystones.py can be translated in Veloce mode cf. https://bitbucket.org/amirouche/pythonium/src/33898da731ee2d768ced392f1c369afd746c25d7/pystone/?at=masterAfter having setup basic Python->JavaScript translation I choosed another path to translate full Python to JavaScript. The way of glib doing object oriented class based code except the target language is JS so you have access to arrays, map-like objects and many other tricks and all that part was written in Python. IIRC there is no javascript code written by in Pythonium translator. Getting single inheritance is not difficult here are the difficult parts making Pythonium fully compliant with Python:This part is factored in https://bitbucket.org/amirouche/pythonium/src/33898da731ee2d768ced392f1c369afd746c25d7/pythonium/compliant/runtime.py?at=master It's written in Python compatible with Python Veloce.The actual compliant translator https://bitbucket.org/amirouche/pythonium/src/33898da731ee2d768ced392f1c369afd746c25d7/pythonium/compliant/compliant.py?at=master doesn't generate JavaScript code directly and most importantly doesn't do ast->ast transformation. I tried the ast->ast thing and ast even if nicer than cst is not nice to work with even with ast.NodeTransformer and more importantly I don't need to do ast->ast.Doing python ast to python ast in my case at least would maybe be a performance improvement since I sometime inspect the content of a block before generating the code associated with it, for instance:So I don't really visit each node once for each phase of the translation.The overall process can be described as:Python builtins are written in Python code (!), IIRC there is a few restrictions related to bootstraping types, but you have access to everything that can translate Pythonium in compliant mode. Have a look at https://bitbucket.org/amirouche/pythonium/src/33898da731ee2d768ced392f1c369afd746c25d7/pythonium/compliant/builtins/?at=masterReading JS code generated from pythonium compliant can be understood but source maps will greatly help.The valuable advice I can give you in the light of this experience are kind old farts:With Python Veloce mode only, I'm very happy! But along the way I discovered that what I was really looking for was liberating me and others from Javascript but more importantly being able to create in a comfortable way. This lead me to Scheme, DSL, Models and eventually domain specific models (cf. http://dsmforum.org/).About what Ira Baxter response:The estimations are not helpful at all. I took me more or less 6 month of free time for both PythonJS and Pythonium. So I can expect more from full time 6 month. I think we all know what 100 man-year in an enterprise context can mean and not mean at all...When someone says something is hard or more often impossible, I answer that "it only takes time to find a solution for a problem that is impossible" otherwise said nothing is impossible except if it's proven impossible in this case a math proof...If it's not proven impossible then it leaves room for imagination:andorIt's not just optimistic thinking. When I started Python->Javascript everybody was saying it was impossible. PyPy impossible. Metaclasses too hard. etc... I think that the only revolution that brings PyPy over Scheme->C paper (which is 25 years old) is some automatic JIT generation (based hints written in the RPython interpreter I think).Most people that say that a thing is "hard" or "impossible" don't provide the reasons. C++ is hard to parse? I know that, still they are (free)  C++ parser. Evil is in the detail? I know that. Saying it's impossible alone is not helpful, It's even worse than "not helpful" it's discouraging, and some people mean to discourage others. I heard about this question via https://stackoverflow.com/questions/22621164/how-to-automatically-generate-a-parser-code-to-code-translator-from-a-corpus.What would be perfection for you? That's how you define next goal and maybe reach the overall goal.I see no patterns that can not be translated from one language to another language at least in a less than perfect way. Since language to language translation is possible, you'd better aim for this first. Since, I think according to http://en.wikipedia.org/wiki/Graph_isomorphism_problem, translation between two computer languages is a tree or DAG isomorphism. Even if we already know that they are both turing complete, so...Framework->Framework which I better visualize as API->API translation might still be something that you might keep in mind as a way to improve the generated code. E.g: Prolog as very specific syntax but still you can do Prolog like computation by describing the same graph in Python... If I was to implement a Prolog to Python translator I wouldn't implement unification in Python but in a C library and come up with a "Python syntax" that is very readable for a Pythonist. In the end, syntax is only "painting" for which we give a meaning (that's why I started scheme). Evil is in the detail of the language and I'm not talking about the syntax. The concepts that are used in the language getattribute hook (you can live without it) but required VM features like tail-recursion optimisation can be difficult to deal with. You don't care if the initial program doesn't use tail recursion and even if there is no tail recursion in the target language you can emulate it using greenlets/event loop. For target and source languages, look for:From this will emerge:You will also probably be able to know what will be translated to fast and slow code.There is also the question of the stdlib or any library but there is no clear answer, it depends of your goals.Idiomatic code or readable generated code have also solutions...Targeting a platform like PHP is much more easy than targeting browsers since you can provide C-implementation of slow and/or critical path.Given you first project is translating Python to PHP, at least for the PHP3 subset I know of, customising veloce.py is your best bet. If you can implement veloce.py for PHP then probably you will be able to run the compliant mode... Also if you can translate PHP to the subset of PHP you can generate with php_veloce.py it means that you can translate PHP to the subset of Python that veloce.py can consume which would mean that you can translate PHP to Javascript. Just saying...You can also have a look at those libraries:Also you might be interested by this blog post (and comments): https://www.rfk.id.au/blog/entry/pypy-js-poc-jit/You could take a look at the Vala compiler, which translates Vala (a C#-like language) into C.

Suppress/ print without b' prefix for bytes in Python 3

sdaau

[Suppress/ print without b' prefix for bytes in Python 3](https://stackoverflow.com/questions/16748083/suppress-print-without-b-prefix-for-bytes-in-python-3)

Just posting this so I can search for it later, as it always seems to stump me:As question: how to print a binary (bytes) string in Python 3, without the b' prefix?

2013-05-25 09:14:03Z

Just posting this so I can search for it later, as it always seems to stump me:As question: how to print a binary (bytes) string in Python 3, without the b' prefix?Use decode:If the bytes use an appropriate character encoding already; you could print them directly:orIf the data is in an UTF-8 compatible format, you can convert the bytes to a string.Optionally convert to hex first, if the data is not already UTF-8 compatible. E.g. when the data are actual raw bytes.If we take a look at the source for bytes.__repr__, it looks as if the b'' is baked into the method.The most obvious workaround is to manually slice off the b'' from the resulting repr():

How to convert an OrderedDict into a regular dict in python3

Ben A.

[How to convert an OrderedDict into a regular dict in python3](https://stackoverflow.com/questions/20166749/how-to-convert-an-ordereddict-into-a-regular-dict-in-python3)

I am struggling with the following problem:

I want to convert an OrderedDict like this:into a regular dict like this:because I have to store it as string in a database. After the conversion the order is not important anymore, so I can spare the ordered feature anyway.Thanks for any hint or solutions,Ben

2013-11-23 19:21:04Z

I am struggling with the following problem:

I want to convert an OrderedDict like this:into a regular dict like this:because I have to store it as string in a database. After the conversion the order is not important anymore, so I can spare the ordered feature anyway.Thanks for any hint or solutions,BenHowever, to store it in a database it'd be much better to convert it to a format such as JSON or Pickle. With Pickle you even preserve the order!Even though this is a year old question, I would like to say that using dict will not help if you have an ordered dict within the ordered dict. The simplest way that could convert those recursive ordered dict will beIt is easy to convert your OrderedDict to a regular Dict like this:If you have to store it as a string in your database, using JSON is the way to go.  That is also quite simple, and you don't even have to worry about converting to a regular dict:Or dump the data directly to a file:If you are looking for a recursive version without using the json module:Here is what seems simplest and works in python 3.7I think a workaround for the nested OrderedDict problem would be to utilise:The resulting data structure will be a pure dict  and not a dict with OrderedDict values.If somehow you want a simple, yet different solution, you can use the {**dict} syntax:Its simple way 

python pandas dataframe to dictionary

perigee

[python pandas dataframe to dictionary](https://stackoverflow.com/questions/18695605/python-pandas-dataframe-to-dictionary)

I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance. Dataframe:

2013-09-09 09:49:56Z

I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance. Dataframe:See the docs for to_dict. You can use it like this:And if you have only one column, to avoid the column name is also a level in the dict (actually, in this case you use the Series.to_dict()):If you want a simple way to preserve duplicates, you could use groupby:The answers by joris in this thread and by punchagan in the duplicated thread are very elegant, however they will not give correct results if the column used for the keys contains any duplicated value. For example:If you have duplicated entries and do not want to lose them, you can use this ugly but working code:in some versions  the code below might not workso make it explicitNote i used id_ because the word id is reserved wordYou can use 'dict comprehension'Simplest solution:Example:If you have multiple values, like val1, val2, val3,etc and u want them as lists, then use the below code:Another (slightly shorter) solution for not losing duplicate entries:You need a list as a dictionary value. This code will do the trick.

How to beautify JSON in Python?

Khwaishien

[How to beautify JSON in Python?](https://stackoverflow.com/questions/9105031/how-to-beautify-json-in-python)

Can someone suggest how I can beautify JSON in Python or through the command line?The only online based JSON beautifier which could do it was: http://jsonviewer.stack.hu/.I need to use it from within Python, however.This is my dataset:

2012-02-01 23:34:14Z

Can someone suggest how I can beautify JSON in Python or through the command line?The only online based JSON beautifier which could do it was: http://jsonviewer.stack.hu/.I need to use it from within Python, however.This is my dataset:From the command-line:which outputs:Programmtically, the Python manual describes pretty-printing JSON:Use the indent argument of the dumps function in the json module.From the docs:A minimal in-python solution that colors json data supplied via the command line:Inspired by pjson mentioned above. This code needs pygments to be installed.Output example:Try underscore-cli:It's a pretty nifty tool that can elegantly do a lot of manipulation of structured data, execute js snippets, fill templates, etc.  It's ridiculously well documented, polished, and ready for serious use.  And I wrote it.  :)The cli command I've used with python for this is:You should be able to find more info here:http://docs.python.org/library/json.htmlIt looks like jsbeautifier open sourced their tools and packaged them as Python and JS libs, and as CLI tools. It doesn't look like they call out to a web service, but I didn't check too closely. See the github repo with install instructions.From their docs for Python CLI and library usage:To beautify using python:Beautified output goes to stdout.To use jsbeautifier as a library is simple:...or, to specify some options:If you want to pass a string instead of a filename, and you are using bash, then you can use process substitution like so:This will pretty print JSON that's on the clipboard in OSX. Just Copy it then call the alias from a Bash prompt.You could pipe the output to jq. If you python script contains something likethen you can fire:I didn't like the output of json.dumps(...) -> For my taste way too much newlines. And I didn't want to use a command line tool or install something.

I finally found Pythons pprint (= pretty print)!Output of json.dumps(json_dict, indent=4)Usage of pprint:Result of pprint.pformat(...) or pprint.pprint(...):Use the python tool libraryCommand line: python -mjson.toolIn code: http://docs.python.org/library/json.htmlFirst install pygmentsthen echo '<some json>' | python -m json.tool | pygmentize -l jsonYour data is poorly formed. The value fields in particular have numerous spaces and new lines. Automated formatters won't work on this, as they will not modify the actual data. As you generate the data for output, filter it as needed to avoid the spaces.With jsonlint (like xmllint):

One line ftp server in python

zio

[One line ftp server in python](https://stackoverflow.com/questions/4994638/one-line-ftp-server-in-python)

Is it possible to have a one line command in python to do a simple ftp server? I'd like to be able to do this as quick and temporary way to transfer files to a linux box without having to install a ftp server. Preferably a way using built in python libraries so there's nothing extra to install.

2011-02-14 16:34:27Z

Is it possible to have a one line command in python to do a simple ftp server? I'd like to be able to do this as quick and temporary way to transfer files to a linux box without having to install a ftp server. Preferably a way using built in python libraries so there's nothing extra to install.Obligatory Twisted example:And probably useful:Check out pyftpdlib from Giampaolo Rodola.  It is one of the very best ftp servers out there for python.  It's used in google's chromium (their browser) and bazaar (a version control system).  It is the most complete implementation on Python for RFC-959 (aka: FTP server implementation spec).From the commandline:Alternatively 'my_server.py':There's more examples on the website if you want something more complicated.To get a list of command line options:Note, if you want to override or use a standard ftp port, you'll need admin privileges (e.g. sudo).Why don't you instead use a one-line HTTP server?will serve the contents of the current working directory over HTTP on port 8000.If you use Python 3, you should instead writeSee the SimpleHTTPServer module docs for 2.x and the http.server docs for 3.x.By the way, in both cases the port parameter is optional.The answers above were all assuming your Python distribution would have some third-party libraries in order to achieve the "one liner python ftpd" goal, but that is not the case of what @zio was asking. Also, SimpleHTTPServer involves web broswer for downloading files, it's not quick enough.  Python can't do ftpd by itself, but you can use netcat, nc:  nc is basically a built-in tool from any UNIX-like systems (even embedded systems), so it's perfect for "quick and temporary way to transfer files".Step 1, on the receiver side, run:  this will listen on port 12345, waiting for data.  Step 2, on the sender side:  You can also put pv in the middle to monitor the progress of transferring: After the transferring is finished, both sides of nc will quit automatically,  and job done.For pyftpdlib users. I found this on the pyftpdlib website. This creates anonymous ftp with write access to your filesystem so please use with due care. More features are available under the hood for better security so just go look:Might be helpful for those that tried using the deprecated method above. sudo python -m pyftpdlib.ftpserverInstall:Then the code:Get deeper:http://twistedmatrix.com/documents/current/core/examples/The simpler solution will be to user pyftpd library. This library allows you to spin Python FTP server in one line. It doesn’t come installed by default though, but we can install it using simple apt command now from the directory you want to serve just run the pythod moduleI dont know about a one-line FTP server, but if you doIt'll run an HTTP server on 0.0.0.0:8000, serving files out of the current directory. If you're looking for a way to quickly get files off a linux box with a web browser, you cant beat it.Good list of tools athttp://www.willdonnelly.net/blog/file-transfer/I've used woof myself on a number of occasions. Very nice.

How to join components of a path when you are constructing a URL in Python

amjoconn

[How to join components of a path when you are constructing a URL in Python](https://stackoverflow.com/questions/1793261/how-to-join-components-of-a-path-when-you-are-constructing-a-url-in-python)

For example, I want to join a prefix path to resource paths like /js/foo.js.I want the resulting path to be relative to the root of the server.  In the above example if the prefix was "media" I would want the result to be /media/js/foo.js.os.path.join does this really well, but how it joins paths is OS dependent.  In this case I know I am targeting the web, not the local file system.Is there a best alternative when you are working with paths you know will be used in URLs?  Will os.path.join work well enough?  Should I just roll my own?

2009-11-24 22:06:07Z

For example, I want to join a prefix path to resource paths like /js/foo.js.I want the resulting path to be relative to the root of the server.  In the above example if the prefix was "media" I would want the result to be /media/js/foo.js.os.path.join does this really well, but how it joins paths is OS dependent.  In this case I know I am targeting the web, not the local file system.Is there a best alternative when you are working with paths you know will be used in URLs?  Will os.path.join work well enough?  Should I just roll my own?Since, from the comments the OP posted, it seems he doesn't want to preserve "absolute URLs" in the join (which is one of the key jobs of urlparse.urljoin;-), I'd recommend avoiding that.  os.path.join would also be bad, for exactly the same reason.So, I'd use something like '/'.join(s.strip('/') for s in pieces) (if the leading / must also be ignored -- if the leading piece must be special-cased, that's also feasible of course;-).Python2But beware,as well asPython3The reason you get different results from /js/foo.js and js/foo.js is because the former begins with a slash which signifies that it already begins at the website root.Like you say, os.path.join joins paths based on the current os. posixpath is the underlying module that is used on posix systems under the namespace os.path:So you can just import and use posixpath.join instead for urls, which is available and will work on any platform.Edit: @Pete's suggestion is a good one, you can alias the import for increased readabilityEdit: I think this is made clearer, or at least helped me understand, if you look into the source of os.py (the code here is from Python 2.7.11, plus I've trimmed some bits). There's conditional imports in os.py that picks which path module to use in the namespace os.path. All the underlying modules (posixpath, ntpath, os2emxpath, riscospath) that may be imported in os.py, aliased as path, are there and exist to be used on all systems. os.py is just picking one of the modules to use in the namespace os.path at run time based on the current OS.This does the job nicely:The basejoin function in the urllib package might be what you're looking for.Edit: I didn't notice before, but urllib.basejoin seems to map directly to urlparse.urljoin, making the latter preferred.Using furl,  pip install furl it will be:I know this is a bit more than the OP asked for, However I had the pieces to the following url, and was looking for a simple way to join them:Doing some looking around:So in addition to the path joining which has already been answered in the other answers, To get what I was looking for I did the following:According to the documentation it takes EXACTLY a 5 part tuple.With the following tuple format:To improve slightly over Alex Martelli's response, the following will not only cleanup extra slashes but also preserve trailing (ending) slashes, which can sometimes be useful :It's not as easy to read though, and won't cleanup multiple extra trailing slashes.Rune Kaagaard provided a great and compact solution that worked for me, I expanded on it a little:This allows all arguments to be joined regardless of trailing and ending slashes while preserving the last slash if present.I found things not to like about all the above solutions, so I came up with my own. This version makes sure parts are joined with a single slash and leaves leading and trailing slashes alone. No pip install, no urllib.parse.urljoin weirdness.Using furl and regex (python 3)

How to use PyCharm to debug Scrapy projects

William Kinaan

[How to use PyCharm to debug Scrapy projects](https://stackoverflow.com/questions/21788939/how-to-use-pycharm-to-debug-scrapy-projects)

I am working on Scrapy 0.20 with Python 2.7. I found PyCharm has a good Python debugger. I want to test my Scrapy spiders using it. Anyone knows how to do that please?Actually I tried to run the spider as a scrip. As a result, I built that scrip. Then, I tried to add my Scrapy project to PyCharm as a model like this:But I don't know what else I have to do

2014-02-14 20:27:14Z

I am working on Scrapy 0.20 with Python 2.7. I found PyCharm has a good Python debugger. I want to test my Scrapy spiders using it. Anyone knows how to do that please?Actually I tried to run the spider as a scrip. As a result, I built that scrip. Then, I tried to add my Scrapy project to PyCharm as a model like this:But I don't know what else I have to doThe scrapy command is a python script which means you can start it from inside PyCharm.When you examine the scrapy binary (which scrapy) you will notice that this is actually a python script:This means that a command like 

scrapy crawl IcecatCrawler can also be executed like this: python /Library/Python/2.7/site-packages/scrapy/cmdline.py crawl IcecatCrawlerTry to find the scrapy.cmdline package.

In my case the location was here: /Library/Python/2.7/site-packages/scrapy/cmdline.pyCreate a run/debug configuration inside PyCharm with that script as script. Fill the script parameters with the scrapy command and spider. In this case crawl IcecatCrawler. Like this:

Put your breakpoints anywhere in your crawling code and it should work™.You just need to do this.Create a Python file on crawler folder on your project. I used main.py.Inside your main.py put this code below.And you need to create a "Run Configuration" to run your main.py.Doing this, if you put a breakpoint at your code it will stop there.As of 2018.1 this became a lot easier. You can now select Module name in your project's Run/Debug Configuration. Set this to scrapy.cmdline and the Working directory to the root dir of the scrapy project (the one with settings.py in it).Like so:Now you can add breakpoints to debug your code.I am running scrapy in a virtualenv with Python 3.5.0 and setting the "script" parameter to /path_to_project_env/env/bin/scrapy solved the issue for me.intellij idea also work.create main.py:show below:To add a bit to the accepted answer, after almost an hour I found I had to select the correct Run Configuration from the dropdown list (near the center of the icon toolbar), then click the Debug button in order to get it to work. Hope this helps! I am also using PyCharm, but I am not using its built-in debugging features.For debugging I am using ipdb. I set up a keyboard shortcut to insert import ipdb; ipdb.set_trace() on any line I want the break point to happen.Then I can type n to execute the next statement, s to step in a function, type any object name to see its value, alter execution environment, type c to continue execution...This is very flexible, works in environments other than PyCharm, where you don't control the execution environment.Just type in your virtual environment pip install ipdb and place import ipdb; ipdb.set_trace() on a line where you want the execution to pause.According to the documentation https://doc.scrapy.org/en/latest/topics/practices.htmlI use this simple script:Extending @Rodrigo's version of the answer I added this script and now I can set spider name from configuration instead of changing in the string.

Format floats with standard json module

Manuel Ceron

[Format floats with standard json module](https://stackoverflow.com/questions/1447287/format-floats-with-standard-json-module)

I am using the standard json module in python 2.6 to serialize a list of floats. However, I'm getting results like this:I want the floats to be formated with only two decimal digits. The output should look like this:I have tried defining my own JSON Encoder class:This works for a sole float object:But fails for nested objects:I don't want to have external dependencies, so I prefer to stick with the standard json module.How can I achieve this?

2009-09-19 00:08:37Z

I am using the standard json module in python 2.6 to serialize a list of floats. However, I'm getting results like this:I want the floats to be formated with only two decimal digits. The output should look like this:I have tried defining my own JSON Encoder class:This works for a sole float object:But fails for nested objects:I don't want to have external dependencies, so I prefer to stick with the standard json module.How can I achieve this?Unfortunately, I believe you have to do this by monkey-patching (which, to my opinion, indicates a design defect in the standard library json package). E.g., this code:emits:as you desire. Obviously, there should be an architected way to override FLOAT_REPR so that EVERY representation of a float is under your control if you wish it to be; but unfortunately that's not how the json package was designed:-(.emitsNo monkeypatching necessary.If you're using Python 2.7, a simple solution is to simply round your floats explicitly to the desired precision.This works because Python 2.7 made float rounding more consistent. Unfortunately this does not work in Python 2.6: The solutions mentioned above are workarounds for 2.6, but none are entirely adequate. Monkey patching json.encoder.FLOAT_REPR does not work if your Python runtime uses a C version of the JSON module. The PrettyFloat class in Tom Wuttke's answer works, but only if %g encoding works globally for your application. The %.15g is a bit magic, it works because float precision is 17 significant digits and %g does not print trailing zeroes.I spent some time trying to make a PrettyFloat that allowed customization of precision for each number. Ie, a syntax likeIt's not easy to get this right. Inheriting from float is awkward. Inheriting from Object and using a JSONEncoder subclass with its own default() method should work, except the json module seems to assume all custom types should be serialized as strings. Ie: you end up with the Javascript string "0.33" in the output, not the number 0.33. There may be a way yet to make this work, but it's harder than it looks.Really unfortunate that dumps doesn't allow you to do anything to floats. However loads does. So if you don't mind the extra CPU load, you could throw it through the encoder/decoder/encoder and get the right result:If you're stuck with Python 2.5 or earlier versions: The monkey-patch trick does not seem to work with the original simplejson module if the C speedups are installed:Here's a solution that worked for me in Python 3 and does not require monkey patching:Output is:It copies the data but with rounded floats.You can do what you need to do, but it isn't documented:Alex Martelli's solution will work for single threaded apps, but may not work for multi-threaded apps that need to control the number of decimal places per thread. Here is a solution that should work in multi threaded apps:You can merely set encoder.thread_local.decimal_places to the number of decimal places you want, and the next call to json.dumps() in that thread will use that number of decimal placesIf you need to do this in python 2.7 without overriding the global json.encoder.FLOAT_REPR, here's one way.Then, in python 2.7:In python 2.6, it doesn't quite work as Matthew Schinckel points out below:Pros:Cons:When importing the standard json module, it is enough to change the default encoder FLOAT_REPR. There isn't really the need to import or create Encoder instances.Sometimes is also very useful to output as json the best representation python can guess with str. This will make sure signifficant digits are not ignored.I agree with @Nelson that inheriting from float is awkward, but perhaps a solution that only touches the __repr__ function might be forgiveable. I ended up using the decimal package for this to reformat floats when needed. The upside is that this works in all contexts where repr() is being called, so also when simply printing lists to stdout for example. Also, the precision is runtime configurable, after the data has been created. Downside is of course that your data needs to be converted to this special float class (as unfortunately you cannot seem to monkey patch float.__repr__). For that I provide a brief conversion function.The code:Usage example:

How to install PIP on Python 3.6?

bradley plater

[How to install PIP on Python 3.6?](https://stackoverflow.com/questions/43304612/how-to-install-pip-on-python-3-6)

I'm trying to Install PIP for python 3.6 and I've looked over YouTube for tutorials but all of them seem to be out of date and none of them have seemed to work. Any information would be helpful so I can carry on with my project.

2017-04-09 08:24:17Z

I'm trying to Install PIP for python 3.6 and I've looked over YouTube for tutorials but all of them seem to be out of date and none of them have seemed to work. Any information would be helpful so I can carry on with my project.pip is bundled with Python > 3.4On Unix-like systems use:On a Windows system use:(On Windows you may need to run the command prompt as administrator to be able to write into python installation directory)Download python 3.6It is possible that pip does not get installed by default. One potential fix is to open cmd and type:and thenactually i had nothing in my scripts folder idk why 

but these steps  worked for me.If pip doesn't come with your installation of python 3.6, this may work:then you can python -m install pip is included in Python installation. If you can't call pip.exe try calling python -m pip [args] from cmdHow did you install Python, and are you using a virtual environment?

As you specifically mentioned Python 3.6, and you have not marked this as answered I will make a guess that you might have installed it using sudo add-apt-repository ppa:jonathonf/python-3.6, as this is a common way to install Python 3.6 on a Unix OS that doesn't have a native 3.6 package.  If this is the case the correct way to install pip is as follows ....Step 1) Make a Virtual Environment with Python 3.6 ...Step 2) Activate your virtual environemnt ...Step 3) Install pip into your environemnt ...I Used these commands in Centos 7to check the pip version:There are situations when your pip doesn't get downloaded along with python installation. Even your whole script folder can be empty.You can do so manually as well.Just head to Command Prompt and type python -m ensurepip --default-pip Press Enter. Make sure that value of path variable is updated.This will do the TrickSince python 3.4 pip is included in the installation of python.There is an issue with downloading and installing Python 3.6. Unchecking pip in the installation prevents the issue. So pip is not given in every installation. I have in this moment install the bs4 with python 3.6.3 on Windows. with the syntax like the user post below:I just successfully installed a package for excel. After installing the python 3.6, you have to download the desired package, then install. For eg,I just successfully installed a package for excel. After installing the python 3.6, you have to download the desired package, then install. 

For eg,Yes, Python3.6 installs PIP but it is unreachable as it is installed.  There is no code which will invoke it as it is!  I have been at it for more than a week and I read every single advice given, without success!  Finally, I tested going to the pip directory and upgrading the installed version:That command created a PIP which now works properly from any location on my FreeBSD server!  Clearly, the tools installed simply do not work, as installed with Python3.6. The only way to run them is to invoke Python and the desired python files as you can see in the command issued.  Once the update is called, however, the new PIP works globally without having to invoke Python3.6...  This what worked for me on Amazon Linuxsudo yum install python36.x86_64 python36-tools.x86_64$ python3 --version

Python 3.6.8$ pip -V

pip 9.0.3 from /usr/lib/python2.7/dist-packages (python 2.7)]$ sudo python3.6 -m pip install --upgrade pip

Collecting pip

  Downloading https://files.pythonhosted.org/packages/d8/f3/413bab4ff08e1fc4828dfc59996d721917df8e8583ea85385d51125dceff/pip-19.0.3-py2.py3-none-any.whl (1.4MB)

    100% |████████████████████████████████| 1.4MB 969kB/s

Installing collected packages: pip

  Found existing installation: pip 9.0.3

    Uninstalling pip-9.0.3:

      Successfully uninstalled pip-9.0.3

Successfully installed pip-19.0.3$ pip -V

pip 19.0.3 from /usr/local/lib/python3.6/site-packages/pip (python 3.6)For python3 it should be pip3on ubuntu 

Python: Convert timedelta to int in a dataframe

Asaf Hanish

[Python: Convert timedelta to int in a dataframe](https://stackoverflow.com/questions/25646200/python-convert-timedelta-to-int-in-a-dataframe)

I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do something more manual?timedelta column                day integer column

2014-09-03 13:53:18Z

I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do something more manual?timedelta column                day integer columnUse the dt.days attribute. Access this attribute via:You can also get the seconds and microseconds attributes in the same way.You could do this, where td is your series of timedeltas.  The division converts the nanosecond deltas into day deltas, and the conversion to int drops to whole days.Timedelta objects have read-only instance attributes .days, .seconds, and .microseconds.If the question isn't just "how to access an integer form of the timedelta?" but "how to convert the timedelta column in the dataframe to an int?" the answer might be a little different. In addition to the .dt.days accessor you need either df.astype or pd.to_numericEither of these options should help:or

Input and output numpy arrays to h5py

lovespeed

[Input and output numpy arrays to h5py](https://stackoverflow.com/questions/20928136/input-and-output-numpy-arrays-to-h5py)

I have a Python code whose output is a  sized matrix, whose entries are all of the type float. If I save it with the extension .dat the file size is of the order of 500 MB. I read that using h5py reduces the file size considerably. So, let's say I have the 2D numpy array named A. How do I save it to an h5py file?

Also, how do I read the same file and put it as a numpy array in a different code, as I need to do manipulations with the array?

2014-01-04 23:41:12Z

I have a Python code whose output is a  sized matrix, whose entries are all of the type float. If I save it with the extension .dat the file size is of the order of 500 MB. I read that using h5py reduces the file size considerably. So, let's say I have the 2D numpy array named A. How do I save it to an h5py file?

Also, how do I read the same file and put it as a numpy array in a different code, as I need to do manipulations with the array?h5py provides a model of datasets and groups. The former is basically arrays and the latter you can think of as directories. Each is named. You should look at the documentation for the API and examples:http://docs.h5py.org/en/latest/quick.htmlA simple example where you are creating all of the data upfront and just want to save it to an hdf5 file would look something like:You can then load that data back in using:

'Definitely check out the docs:http://docs.h5py.orgWriting to hdf5 file depends either on h5py or pytables (each has a different python API that sits on top of the hdf5 file specification). You should also take a look at other simple binary formats provided by numpy natively such as np.save, np.savez etc:http://docs.scipy.org/doc/numpy/reference/routines.io.htmlA cleaner way to handle file open/close and avoid memory leaks:Prep:    Write:    Read:    The with statement of python takes care of closing all the handles and cleans the gc memory.

Bundling data files with PyInstaller (--onefile)

arboreal shark

[Bundling data files with PyInstaller (--onefile)](https://stackoverflow.com/questions/7674790/bundling-data-files-with-pyinstaller-onefile)

I'm trying to build a one-file EXE with PyInstaller which is to include an image and an icon. I cannot for the life of me get it to work with --onefile.If I do --onedir it works all works very well.

When I use --onefile, it can't find the referenced additional files (when running the compiled EXE). It finds the DLLs and everything else fine, just not the two images.I've looked in the temp-dir generated when running the EXE (\Temp\_MEI95642\ for example) and the files are indeed in there. When I drop the EXE in that temp-directory it finds them. Very perplexing.This is what I've added to the .spec fileI should add that I have tried not putting them in subfolders as well, didn't make a difference.Edit: Marked newer answer as correct due to PyInstaller update.

2011-10-06 13:20:34Z

I'm trying to build a one-file EXE with PyInstaller which is to include an image and an icon. I cannot for the life of me get it to work with --onefile.If I do --onedir it works all works very well.

When I use --onefile, it can't find the referenced additional files (when running the compiled EXE). It finds the DLLs and everything else fine, just not the two images.I've looked in the temp-dir generated when running the EXE (\Temp\_MEI95642\ for example) and the files are indeed in there. When I drop the EXE in that temp-directory it finds them. Very perplexing.This is what I've added to the .spec fileI should add that I have tried not putting them in subfolders as well, didn't make a difference.Edit: Marked newer answer as correct due to PyInstaller update.Newer versions of PyInstaller do not set the env variable anymore, so Shish's excellent answer will not work. Now the path gets set as sys._MEIPASS:pyinstaller unpacks your data into a temporary folder, and stores this directory path in the _MEIPASS2 environment variable. To get the _MEIPASS2 dir in packed-mode and use the local directory in unpacked (development) mode, I use this:Output:All of the other answers use the current working directory in the case where the application is not PyInstalled (i.e. sys._MEIPASS is not set). That is wrong, as it prevents you from running your application from a directory other than the one where your script is.A better solution:Perhaps i missed a step or did something wrong but the methods which are above, didn't bundle data files with PyInstaller into one exe file. Let me share the steps what i have done.Conclusion: There's still more than one file in the dist folder.Note: I'm using Python 3.5.EDIT: Finally it works with Jonathan Reinhart's method.After the 6. step your one file is ready to use.Instead for rewriting all my path code as suggested, I changed the working directory:Just add those two lines at the beginning of your code, you can leave the rest as is.Slight modification to the accepted answer.The most common complaint/question I've seen wrt PyInstaller is "my code can't find a data file which I definitely included in the bundle, where is it?", and it isn't easy to see what/where your code is searching because the extracted code is in a temp location and is removed when it exits. Add this bit of code to see what's included in your onefile and where it is, using @Jonathon Reinhart's resource_path()I found the existing answers confusing, and took a long time to work out where the problem is. Here's a compilation of everything I found.When I run my app, I get an error Failed to execute script foo (if foo.py is the main file). To troubleshoot this, don't run PyInstaller with --noconsole (or edit main.spec to change console=False => console=True). With this, run the executable from a command-line, and you'll see the failure.The first thing to check is that it's packaging up your extra files correctly. You should add tuples like ('x', 'x') if you want the folder x to be included.After it crashes, don't click OK. If you're on Windows, you can use Search Everything. Look for one of your files (eg. sword.png). You should find the temporary path where it unpacked the files (eg. C:\Users\ashes999\AppData\Local\Temp\_MEI157682\images\sword.png). You can browse this directory and make sure it included everything. If you can't find it this way, look for something like main.exe.manifest (Windows) or python35.dll (if you're using Python 3.5).If the installer includes everything, the next likely problem is file I/O: your Python code is looking in the executable's directory, instead of the temp directory, for files.To fix that, any of the answers on this question work. Personally, I found a mixture of them all to work: change directory conditionally first thing in your main entry-point file, and everything else works as-is:

if hasattr(sys, '_MEIPASS'):

    os.chdir(sys._MEIPASS)

If you are still trying to put files relative to your executable instead of in the temp directory, you need to copy it yourself.  This is how I ended up getting it done.https://stackoverflow.com/a/59415662/999943You add a step in the spec file that does a filesystem copy to the DISTPATH variable.Hope that helps.

Select Pandas rows based on list index

user2806761

[Select Pandas rows based on list index](https://stackoverflow.com/questions/19155718/select-pandas-rows-based-on-list-index)

I have a dataframe df :Then I want to select rows with certain sequence numbers which indicated in a list, suppose here is [1,3], then left:How or what function can do that ?

2013-10-03 09:36:32Z

I have a dataframe df :Then I want to select rows with certain sequence numbers which indicated in a list, suppose here is [1,3], then left:How or what function can do that ?should do the trick!

When I index with data frames I always use the .ix() method. Its so much easier and more flexible...UPDATE

This is no longer the accepted method for indexing. The ix method is  deprecated. Use .iloc for integer based indexing and .loc for label based indexing. you can also use iloc:Another way (although it is a longer code) but it is faster than the above codes. Check it using %timeit function:PS: You figure out the reasonFor large datasets, it is memory efficient to read only selected rows via the skiprows parameter.ExampleThis will now return a DataFrame from a file that skips all rows except 1 and 3.DetailsFrom the docs:This feature works in version pandas 0.20.0+.  See also the corresponding issue and a related post.

Python extending with - using super() Python 3 vs Python 2

Oz123

[Python extending with - using super() Python 3 vs Python 2](https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2)

Originally I wanted to ask this question, but then I found it was already thought of before... Googling around I found this example of extending configparser. The following works with Python 3:But not with Python 2:Then I read a little bit on Python New Class vs. Old Class styles (e.g. here.

And now I am wondering, I can do:But, shouldn't I call init? Is this in Python 2 the equivalent:

2012-05-07 13:28:42Z

Originally I wanted to ask this question, but then I found it was already thought of before... Googling around I found this example of extending configparser. The following works with Python 3:But not with Python 2:Then I read a little bit on Python New Class vs. Old Class styles (e.g. here.

And now I am wondering, I can do:But, shouldn't I call init? Is this in Python 2 the equivalent:In a single inheritance case (when you subclass one class only), your new class inherits methods of the base class. This includes __init__. So if you don't define it in your class, you will get the one from the base.Things start being complicated if you introduce multiple inheritance (subclassing more than one class at a time). This is because if more than one base class has __init__, your class will inherit the first one only.In such cases, you should really use super if you can, I'll explain why. But not always you can. The problem is that all your base classes must also use it (and their base classes as well -- the whole tree).If that is the case, then this will also work correctly (in Python 3 but you could rework it into Python 2 -- it also has super):Notice how both base classes use super even though they don't have their own base classes.What super does is: it calls the method from the next class in MRO (method resolution order). The MRO for C is: (C, A, B, object). You can print C.__mro__ to see it.So, C inherits __init__ from A and super in A.__init__ calls B.__init__ (B follows A in MRO).So by doing nothing in C, you end up calling both, which is what you want.Now if you were not using super, you would end up inheriting A.__init__ (as before) but this time there's nothing that would call B.__init__ for you.To fix that you have to define C.__init__:The problem with that is that in more complicated MI trees, __init__ methods of some classes may end up being called more than once whereas super/MRO guarantee that they're called just once.In short, they are equivalent. 

Let's have a history view:(1) at first, the function looks like this.(2) to make code more abstract (and more portable). A common method to get Super-Class is invented like:And init function can be:However  requiring an explicit passing of both the class and instance break the DRY (Don't Repeat Yourself) rule a bit. (3) in V3. It is more smart, is enough in most case. You can refer to http://www.python.org/dev/peps/pep-3135/Just to have a simple and complete example for Python 3, which most people seem to be using now.givesAnother python3 implementation that involves the use of Abstract classes with super(). You should remember that   has the same effect as  Remember there's a hidden 'self' in super(), So the same object passes on to the superclass init method and the attributes are added to the object that called it. 

Hence super()gets translated to  Person and then if you include the hidden self, you get the above code frag.  

How do I add a path to PYTHONPATH in virtualenv

Flavien

[How do I add a path to PYTHONPATH in virtualenv](https://stackoverflow.com/questions/10738919/how-do-i-add-a-path-to-pythonpath-in-virtualenv)

I am trying to add a path to the PYTHONPATH environment variable, that would be only visible from a particular virtualenv environment. I tried SET PYTHONPATH=... under a virtualenv command prompt, but that sets the variable for the whole environment.How do I achieve that?

2012-05-24 13:57:03Z

I am trying to add a path to the PYTHONPATH environment variable, that would be only visible from a particular virtualenv environment. I tried SET PYTHONPATH=... under a virtualenv command prompt, but that sets the variable for the whole environment.How do I achieve that?You can usually avoid having to do anything with PYTHONPATH by using .pth files. Just put a file with a .pth extension (any basename works) in your virtualenv's site-packages folder, e.g. lib\python2.7\site-packages, with the absolute path to the directory containing your package as its only contents.If you're using virtualenv, you should probably also be using virtualenvwrapper, in which case you can use the add2virtualenv command to add paths to the Python path for the current virtualenv:add2virtualenv directory1 directory2 …  You can also try to put symlink to one of your virtualenv.eg.

1) activate your virtualenv

2) run python

3) import sys and check sys.path

4) you will find python search path there. Choose one of those (eg. site-packages) 

5) go there and create symlink to your package like:

ln -s path-to-your-package name-with-which-you'll-be-importingThat way you should be able to import it even without activating your virtualenv. Simply try: path-to-your-virtualenv-folder/bin/python

and import your package.If you are using virtualenvwrapper, console will display That's it, and you should be good to goI strongly suggest you use virtualenv and virtualenvwrapper to avoid cluttering path

Is there a function in python to split a word into a list?

gath

[Is there a function in python to split a word into a list?](https://stackoverflow.com/questions/113655/is-there-a-function-in-python-to-split-a-word-into-a-list)

Is there a function in python to split a word into a list of single letters? e.g:to get

2008-09-22 07:40:50Z

Is there a function in python to split a word into a list of single letters? e.g:to getThe easiest way is probably just to use list(), but there is at least one other option as well:They should both give you what you need:As stated, the first is likely the most preferable for your example but there are use cases that may make the latter quite handy for more complex stuff, such as if you want to apply some arbitrary function to the items, such as with:The list function will do thisAbuse of the rules, same result:

    (x for x in 'Word to split')Actually an iterator, not a list. But it's likely you won't really care.def count():

    list = 'oixfjhibokxnjfklmhjpxesriktglanwekgfvnk'

Importing a CSV file into a sqlite3 database table using Python

Hossein

[Importing a CSV file into a sqlite3 database table using Python](https://stackoverflow.com/questions/2887878/importing-a-csv-file-into-a-sqlite3-database-table-using-python)

I have a CSV file and I want to bulk-import this file into my sqlite3 database using Python. the command is ".import .....". but it seems that it cannot work like this. Can anyone give me an example of how to do it in sqlite3? I am using windows just in case.

Thanks

2010-05-22 11:25:58Z

I have a CSV file and I want to bulk-import this file into my sqlite3 database using Python. the command is ".import .....". but it seems that it cannot work like this. Can anyone give me an example of how to do it in sqlite3? I am using windows just in case.

ThanksCreating an sqlite connection to a file on disk is left as an exercise for the reader ... but there is now a two-liner made possible by the pandas libraryMy 2 cents (more generic):The .import command is a feature of the sqlite3 command-line tool. To do it in Python, you should simply load the data using whatever facilities Python has, such as the csv module, and inserting the data as per usual.This way, you also have control over what types are inserted, rather than relying on sqlite3's seemingly undocumented behaviour.Many thanks for bernie's answer!  Had to tweak it a bit - here's what worked for me:My text file (PC.txt) looks like this:Based on Guy L solution (Love it) but can handle escaped fields.You can do this using blaze & odo efficientlyOdo will store the csv file to data.db (sqlite database) under the schema dataOr you use odo directly, without blaze. Either ways is fine. Read this documentationYou're right that .import is the way to go, but that's a command from the SQLite3.exe shell. A lot of the top answers to this question involve native python loops, but if your files are large (mine are 10^6 to 10^7 records), you want to avoid reading everything into pandas or using a native python list comprehension/loop (though I did not time them for comparison).For large files, I believe the best option is to create the empty table in advance using sqlite3.execute("CREATE TABLE..."), strip the headers from your CSV files, and then use subprocess.run() to execute sqlite's import statement.  Since the last part is I believe the most pertinent, I will start with that.Explanation

From the command line, the command you're looking for is sqlite3 my.db -cmd ".mode csv" ".import file.csv table".  subprocess.run() runs a command line process.  The argument to subprocess.run() is a sequence of strings which are interpreted as a command followed by all of it's arguments.Not really the main point of the question, but here's what I used.  Again, I didn't want to read the whole files into memory at any point:If the CSV file must be imported as part of a python program, then for simplicity and efficiency, you could use os.system along the lines suggested by the following:The point is that by specifying the filename of the database, the data will automatically be saved, assuming there are no errors reading it.in the interest of simplicity, you could use the sqlite3 command line tool from the Makefile of your project.make test.sql3 then creates the sqlite database from an existing test.csv file, with a single table "test". you can then make test.dump to verify the contents.I've found that it can be necessary to break up the transfer of data from the csv to the database in chunks as to not run out of memory. This can be done like this:

Reference list item by index within Django template?

user456584

[Reference list item by index within Django template?](https://stackoverflow.com/questions/4651172/reference-list-item-by-index-within-django-template)

This may be simple, but I looked around and couldn't find an answer.  What's the best way to reference a single item in a list from a Django template?In other words how do I do the equivalent of {{ data[0] }} within the template language?Thanks.

2011-01-10 20:27:43Z

This may be simple, but I looked around and couldn't find an answer.  What's the best way to reference a single item in a list from a Django template?In other words how do I do the equivalent of {{ data[0] }} within the template language?Thanks.It looks like {{ data.0 }}. See Variables and lookups.A better way: custom template filter: https://docs.djangoproject.com/en/dev/howto/custom-template-tags/such as get my_list[x] in templates:in templatetemplatetags/index.pyif my_list = [['a','b','c'], ['d','e','f']], you can use {{ my_list|index:x|index:y }} in template to get my_list[x][y]It works fine with "for"Tested and works well ^_^{{ data.0 }} should work.Let's say you wrote data.obj django tries data.obj and data.obj(). If they don't work it tries data["obj"]. In your case data[0] can be written as {{ data.0 }}. But I recommend you to pull data[0] in the view and send it as separate variable.@jennifer06262016, you can definitely add another filter to return the objects inside a django Queryset.In that case, you would type something like this {{ Queryset|index:x|get_item }} into your template to access some dictionary object. It works for me.

How do I access the request object or any other variable in a form's clean() method?

nubela

[How do I access the request object or any other variable in a form's clean() method?](https://stackoverflow.com/questions/1057252/how-do-i-access-the-request-object-or-any-other-variable-in-a-forms-clean-met)

I am trying to request.user for a form's clean method, but how can I access the request object? Can I modify the clean method to allow variables input?

2009-06-29 08:49:45Z

I am trying to request.user for a form's clean method, but how can I access the request object? Can I modify the clean method to allow variables input?The answer by Ber - storing it in threadlocals - is a very bad idea. There's absolutely no reason to do it this way.A much better way is to override the form's __init__ method to take an extra keyword argument, request. This stores the request in the form, where it's required, and from where you can access it in your clean method.and in your view:UPDATED 10/25/2011: I'm now using this with a metaclass instead of method, as Django 1.3 displays some weirdness otherwise.Then override MyCustomForm.__init__ as follows:You can then access the request object from any method of ModelForm with self.request.For what it's worth, if you're using Class Based Views, instead of function based views, override get_form_kwargs in your editing view. Example code for a custom CreateView:The above view code will make request available as one of the keyword arguments to the form's __init__ constructor function. Therefore in your ModelForm do:The usual aproach is to store the request object in a thread-local reference using a middleware. Then you can access this from anywhere in you app, including the Form.clean() method.Changing the signature of the Form.clean() method means you have you own, modified version of Django, which may not be what you want.Thank middleware count look something like this:Register this middleware as described in the Django docsFor Django admin, in Django 1.8I ran into this particular problem when customizing the admin. I wanted a certain field to be validated based on the particular admin's credentials.Since I did not want to modify the view to pass the request as an argument to the form, the following is what I did:You can't always use this method (and its probably bad practice), but if you are only using the form in one view you could scope it inside the view method itself.The answer by Daniel Roseman is still the best. However, I would use the first positional argument for the request instead of the keyword argument for a few reasons:Lastly, I would use a more unique name to avoid overriding an existing variable. Thus, My modified answer looks like:fresh cheese from cheesebaker@pypi: django-requestproviderI have another answer to this question as per your requirement you want to access the user into the clean method of the form.

You can Try this.

View.pyforms.pyNow you can access the self.instance in any clean method in form.py

Python PIL: how to write PNG image to string

maxp

[Python PIL: how to write PNG image to string](https://stackoverflow.com/questions/646286/python-pil-how-to-write-png-image-to-string)

I have generated an image using PIL. How can I save it to a string in memory?

The Image.save() method requires a file.I'd like to have several such images stored in dictionary.

2009-03-14 17:15:17Z

I have generated an image using PIL. How can I save it to a string in memory?

The Image.save() method requires a file.I'd like to have several such images stored in dictionary.You can use the BytesIO class to get a wrapper around strings that behaves like a file. The BytesIO object provides the same interface as a file, but saves the contents just in memory:You have to explicitly specify the output format with the format parameter, otherwise PIL will raise an error when trying to automatically detect it.If you loaded the image from a file it has a format parameter that contains the original file format, so in this case you can use format=image.format.In old Python 2 versions before introduction of the io module you would have used the StringIO module instead.For Python3 it is required to use BytesIO: Read more: http://fadeit.dk/blog/post/python3-flask-pil-in-memory-imagesth's solution didn't work for me

because in ...It was trying to detect the format from the extension in the filename , which doesn't exist in StringIO case  You can bypass the format detection by setting the format yourself in a parameter  save() can take a file-like object as well as a path, so you can use an in-memory buffer like a StringIO:With modern (as of mid-2017 Python 3.5 and Pillow 4.0):StringIO no longer seems to work as it used to. The BytesIO class is the proper way to handle this. Pillow's save function expects a string as the first argument, and surprisingly doesn't see StringIO as such. The following is similar to older StringIO solutions, but with BytesIO in its place.When you say "I'd like to have number of such images stored in dictionary", it's not clear if this is an in-memory structure or not.You don't need to do any of this to meek an image in memory.  Just keep the image object in your dictionary.If you're going to write your dictionary to a file, you might want to look at  im.tostring() method and the Image.fromstring() functionhttp://effbot.org/imagingbook/image.htmThe "format" (.jpeg, .png, etc.) only matters on disk when you are exchanging the files.  If you're not exchanging files, format doesn't matter.

Set order of columns in pandas dataframe

durbachit

[Set order of columns in pandas dataframe](https://stackoverflow.com/questions/41968732/set-order-of-columns-in-pandas-dataframe)

Is there a way to reorder columns in pandas dataframe based on my personal preference (i.e. not alphabetically or numerically sorted, but more like following certain conventions)?Simple example:produces this:But instead, I would like this:(Please, provide a generic solution rather than specific to this case. Many thanks.)

2017-01-31 22:34:03Z

Is there a way to reorder columns in pandas dataframe based on my personal preference (i.e. not alphabetically or numerically sorted, but more like following certain conventions)?Simple example:produces this:But instead, I would like this:(Please, provide a generic solution rather than specific to this case. Many thanks.)Just select the order yourself by typing in the column names. Note the double brackets:You can use this:You could also do something like df = df[['x', 'y', 'a', 'b']]  Also, you can get the list of columns with:The output will produce something like this:Which is then easy to rearrange manually.Here is a solution I use very often. When you have a large data set with tons of columns, you definitely do not want to manually rearrange all the columns. What you can and, most likely, want to do is to just order the first a few columns that you frequently use, and let all other columns just be themselves. This is a common approach in R. df %>%select(one, two, three, everything())So you can first manually type the columns that you want to order and to be positioned before all the other columns in a list cols_to_order. Then you construct a list for new columns by combining the rest of the columns:After this, you can use the new_columns as other solutions suggested. Construct it with a list instead of a dictionaryYou can also use OrderedDict:Add the 'columns' parameter:Try indexing (so you want a generic solution not only for this, so index order can be just what you want):Now:Is:I find this to be the most straightforward and working:

I want to exception handle 'list index out of range.'

H.Choi

[I want to exception handle 'list index out of range.'](https://stackoverflow.com/questions/11902458/i-want-to-exception-handle-list-index-out-of-range)

I am using BeautifulSoup and parsing some HTMLs.I'm getting a certain data from each HTML (using for loop) and adding that data to a certain list.The problem is, some of the HTMLs have different format (and they don't have the data that I want in them).So, I was trying to use exception handling and add value null to the list (I should do this since the sequence of data is important.)For instance, I have a code like:and some of the links don't have any <dd class='title'>, so what I want to do is add string null to the list instead.The error appears: What I have done tried is to add some lines like this:But it doesn't work out. It still shows error:What should I do about this? Should I use exception handling? or is there any easier way?Any suggestions? Any help would be really great!

2012-08-10 13:15:45Z

I am using BeautifulSoup and parsing some HTMLs.I'm getting a certain data from each HTML (using for loop) and adding that data to a certain list.The problem is, some of the HTMLs have different format (and they don't have the data that I want in them).So, I was trying to use exception handling and add value null to the list (I should do this since the sequence of data is important.)For instance, I have a code like:and some of the links don't have any <dd class='title'>, so what I want to do is add string null to the list instead.The error appears: What I have done tried is to add some lines like this:But it doesn't work out. It still shows error:What should I do about this? Should I use exception handling? or is there any easier way?Any suggestions? Any help would be really great!Handling the exception is the way to go:Of course you could also check the len() of dlist; but handling the exception is more intuitive.You have two options; either handle the exception or test the length:orUse the first if there often is no second item, the second if there sometimes is no second item.A ternary will suffice. change:tothis is a shorter way of expressingTaking reference of ThiefMaster♦ sometimes we get an error with value given as '\n' or null and perform for that required to handle ValueError:Handling the exception is the way to gofor any one interested in a shorter way:But for best performance, I suggest using False instead of 'null', then a one line test will suffice:

How can you get the SSH return code using Paramiko?

Beyonder

[How can you get the SSH return code using Paramiko?](https://stackoverflow.com/questions/3562403/how-can-you-get-the-ssh-return-code-using-paramiko)

Is there any way to get the command return code?It's hard to parse all stdout/stderr and know whether the command finished successfully or not.

2010-08-25 02:14:06Z

Is there any way to get the command return code?It's hard to parse all stdout/stderr and know whether the command finished successfully or not.SSHClient is a simple wrapper class around the more lower-level functionality in Paramiko.  The API documentation lists a recv_exit_status() method on the Channel class.A very simple demonstration script:A much easier example that doesn't involve invoking the "lower level" channel class directly (i.e. - NOT using the client.get_transport().open_session() command):Thanks for JanC, I added some modification for the example and tested in Python3, it really useful for me.In my case, output buffering was the problem. Because of buffering, the outputs from the application doesn't come out non-blocking way. You can find the answer about how to print output without buffering in here: Disable output buffering. For short, just run python with -u option like this:> python -u script.py

Replace string within file contents

Joey

[Replace string within file contents](https://stackoverflow.com/questions/4128144/replace-string-within-file-contents)

How can I open a file, Stud.txt, and then replace any occurences of "A" with "Orange"?

2010-11-08 21:16:35Z

How can I open a file, Stud.txt, and then replace any occurences of "A" with "Orange"?If you'd like to replace the strings in the same file, you probably have to read its contents into a local variable, close it, and re-open it for writing:I am using the with statement in this example, which closes the file after the with block is terminated - either normally when the last command finishes executing, or by an exception.It is worth mentioning that if the filenames were different, we could have done this more elegantly with a single with statement.Something likeIf you are on linux and just want to replace the word dog with catyou can do:text.txt:Linux Command:Output:Original Post: https://askubuntu.com/questions/20414/find-and-replace-text-within-a-file-using-commandseasiest way is to do it with regular expressions, assuming that you want to iterate over each line in the file (where 'A' would be stored) you do...

How I can I lazily read multiple JSON values from a file/stream in Python?

Jeremy Banks

[How I can I lazily read multiple JSON values from a file/stream in Python?](https://stackoverflow.com/questions/6886283/how-i-can-i-lazily-read-multiple-json-values-from-a-file-stream-in-python)

I'd like to read multiple JSON objects from a file/stream in Python, one at a time. Unfortunately json.load() just .read()s until end-of-file; there doesn't seem to be any way to use it to read a single object or to lazily iterate over the objects.Is there any way to do this? Using the standard library would be ideal, but if there's a third-party library I'd use that instead.At the moment I'm putting each object on a separate line and using json.loads(f.readline()), but I would really prefer not to need to do this.

2011-07-30 22:12:57Z

I'd like to read multiple JSON objects from a file/stream in Python, one at a time. Unfortunately json.load() just .read()s until end-of-file; there doesn't seem to be any way to use it to read a single object or to lazily iterate over the objects.Is there any way to do this? Using the standard library would be ideal, but if there's a third-party library I'd use that instead.At the moment I'm putting each object on a separate line and using json.loads(f.readline()), but I would really prefer not to need to do this.Here's a much, much simpler solution.  The secret is to try, fail, and use the information in the exception to parse correctly.  The only limitation is the file must be seekable.Edit:  just noticed that this will only work for Python >=3.5.  For earlier, failures return a ValueError, and you have to parse out the position from the string, e.g.JSON generally isn't very good for this sort of incremental use; there's no standard way to serialise multiple objects so that they can easily be loaded one at a time, without parsing the whole lot.The object per line solution that you're using is seen elsewhere too. Scrapy calls it 'JSON lines':You can do it slightly more Pythonically:I think this is about the best way - it doesn't rely on any third party libraries, and it's easy to understand what's going on. I've used it in some of my own code as well.A little late maybe, but I had this exact problem (well, more or less). My standard solution for these problems is usually to just do a regex split on some well-known root object, but in my case it was impossible. The only feasible way to do this generically is to implement a proper tokenizer.After not finding a generic-enough and reasonably well-performing solution, I ended doing this myself, writing the splitstream module. It is a pre-tokenizer that understands JSON and XML and splits a continuous stream into multiple chunks for parsing (it leaves the actual parsing up to you though). To get some kind of performance out of it, it is written as a C module.Example:Sure you can do this. You just have to take to raw_decode directly. This implementation loads the whole file into memory and operates on that string (much as json.load does); if you have large files you can modify it to only read from the file as necessary without much difficulty.Usage: just as you requested, it's a generator.This is a pretty nasty problem actually because you have to stream in lines, but pattern match across multiple lines against braces, but also pattern match json. It's a sort of json-preparse followed by a json parse. Json is, in comparison to other formats, easy to parse so it's not always necessary to go for a parsing library, nevertheless, how to should we solve these conflicting issues?Generators to the rescue!The beauty of generators for a problem like this is you can stack them on top of each other gradually abstracting away the difficulty of the problem whilst maintaining laziness. I also considered using the mechanism for passing back values into a generator (send()) but fortunately found I didn't need to use that.To solve the first of the problems you need some sort of streamingfinditer, as a streaming version of re.finditer. My attempt at this below pulls in lines as needed (uncomment the debug statement to see) whilst still returning matches. I actually then modified it slightly to yield non-matched lines as well as matches (marked as 0 or 1 in the first part of the yielded tuple).With that, it's then possible to match up until braces, account each time for whether the braces are balanced, and then return either simple or compound objects as appropriate.This returns tuples as follows:Basically that's the nasty part done. We now just have to do the final level of parsing as we see fit. For example we can use Jeremy Roman's iterload function (Thanks!) to do parsing for a single line:Test it:I get these results (and if you turn on that debug line, you'll see it pulls in the lines as needed):This won't work for all situations. Due to the implementation of the json library, it is impossible to work entirely correctly without reimplementing the parser yourself.I believe a better way of doing it would be to use a state machine. Below is a sample code that I worked out by converting a NodeJS code on below link to Python 3 (used nonlocal keyword only available in Python 3, code won't work on Python 2)Edit-1: Updated and made code compatible with Python 2Edit-2: Updated and added a Python3 only version as wellhttps://gist.github.com/creationix/5992451The output of the same isI'd like to provide a solution. The key thought is to "try" to decode: if it fails, give it more feed, otherwise use the offset information to prepare next decoding.However the current json module can't tolerate SPACE in head of string to be decoded, so I have to strip them off.=========================

I have tested for several txt files, and it works fine.

(in1.txt)(in2.txt)(in.txt, your initial)(output for Benedict's testcase)I used @wuilang's elegant solution. The simple approach -- read a byte, try to decode, read a byte, try to decode, ... -- worked, but unfortunately it was very slow.In my case, I was trying to read "pretty-printed" JSON objects of the same object type from a file. This allowed me to optimize the approach; I could read the file line-by-line, only decoding when I found a line that contained exactly "}":If you happen to be working with one-per-line compact JSON that escapes newlines in string literals, then you can safely simplify this approach even more:Obviously, these simple approaches only work for very specific kinds of JSON. However, if these assumptions hold, these solutions work correctly and quickly.Here's mine:If you use a json.JSONDecoder instance you can use raw_decode member function. It returns a tuple of python representation of the JSON value and an index to where the parsing stopped. This makes it easy to slice (or seek in a stream object) the remaining JSON values. I'm not so happy about the extra while loop to skip over the white space between the different JSON values in the input but it gets the job done in my opinion.The next version is much shorter and eats the part of the string that is already parsed. It seems that for some reason a second call json.JSONDecoder.raw_decode() seems to fail when the first character in the string is a whitespace, that is also the reason why I skip over the whitespace in the whileloop above ...In the documentation about the json.JSONDecoder class the method raw_decode https://docs.python.org/3/library/json.html#encoders-and-decoders contains the following:And this extraneous data can easily be another JSON value. In other words the method might be written with this purpose in mind.With the input.txt using the upper function I obtain the example output as presented in the original question.You can use https://pypi.org/project/json-stream-parser/ for exactly that purpose.output

Frequency table for a single variable

Abe

[Frequency table for a single variable](https://stackoverflow.com/questions/12207326/frequency-table-for-a-single-variable)

One last newbie pandas question for the day:  How do I generate a table for a single Series?For example:Lots of googling has led me to Series.describe() and pandas.crosstabs, but neither of these does quite what I need: one variable, counts by categories.  Oh, and it'd be nice if it worked for different data types: strings, ints, etc.

2012-08-31 00:10:12Z

One last newbie pandas question for the day:  How do I generate a table for a single Series?For example:Lots of googling has led me to Series.describe() and pandas.crosstabs, but neither of these does quite what I need: one variable, counts by categories.  Oh, and it'd be nice if it worked for different data types: strings, ints, etc.Maybe .value_counts()?You can use list comprehension on a dataframe to count frequencies of the columns as suchBreakdown:The answer provided by @DSM is simple and straightforward, but I thought I'd add my own input to this question. If you look at the code for pandas.value_counts, you'll see that there is a lot going on.If you need to calculate the frequency of many series, this could take a while. A faster implementation would be to use numpy.unique with return_counts = TrueHere is an example:Notice here that the item returned is a pandas.SeriesIn comparison, numpy.unique returns a tuple with two items, the unique values and the counts.You can then combine these into a dictionary:And then into a pandas.Series

argparse store false if unspecified

siamii

[argparse store false if unspecified](https://stackoverflow.com/questions/8203622/argparse-store-false-if-unspecified)

How can I store false if -auto is unspecified? I can faintly remember that this way, it stores None if unspecified

2011-11-20 18:20:18Z

How can I store false if -auto is unspecified? I can faintly remember that this way, it stores None if unspecifiedThe store_true option automatically creates a default value of False.Likewise, store_false will default to True when the command-line argument is not present.The source for this behavior is succinct and clear:  http://hg.python.org/cpython/file/2.7/Lib/argparse.py#l861The argparse docs aren't clear on the subject, so I'll update them now: http://hg.python.org/cpython/rev/49677cc6d83aWithrunningyieldsSo it appears to be storing False by default.Raymond Hettinger answers OP's question already.However, my group has experienced readability issues using "store_false". Especially when new members join our group. This is because it is most intuitive way to think is that when a user specifies an argument, the value corresponding to that argument will be True or 1.For example, if the code is -The code reader may likely expect the logging statement to be off when the value in stop_logging is true. But code such as the following will lead to the opposite of the desired behavior - On the other hand, if the interface is defined as the following, then the "if-statement" works and is more intuitive to read - store_false will actually default to 0 by default (you can test to verify). To change what it defaults to, just add default=True to your declaration.So in this case:

parser.add_argument('-auto', action='store_true', default=True)

Detect & Record Audio in Python

cryo

[Detect & Record Audio in Python](https://stackoverflow.com/questions/892199/detect-record-audio-in-python)

I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.Just can't quite get my head around it, can anyone get me started with a basic example.

2009-05-21 10:05:32Z

I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.Just can't quite get my head around it, can anyone get me started with a basic example.As a follow up to Nick Fortescue's answer, here's a more complete example of how to record from the microphone and process the resulting data:I believe the WAVE module does not support recording, just processing existing files. You might want to look at PyAudio for actually recording.

WAV is about the world's simplest file format. In paInt16 you just get a signed integer representing a level, and closer to 0 is quieter. I can't remember if WAV files are high byte first or low byte, but something like this ought to work (sorry, I'm not really a python programmer:PyAudio code for recording kept for reference:Thanks to cryo for improved version that I based my tested code below:I think this will help.It is a simple script which will check if there is a silence or not.If silence is detected it will not record otherwise it will record.The pyaudio website has many examples that are pretty short and clear:

http://people.csail.mit.edu/hubert/pyaudio/Update 14th of December 2019 - Main example from the above linked website from 2017:You might want to look at csounds, also.  It has several API's, including Python.  It might be able to interact with an A-D interface and gather sound samples.

Public free web services for testing soap client [closed]

bhadra

[Public free web services for testing soap client [closed]](https://stackoverflow.com/questions/311654/public-free-web-services-for-testing-soap-client)

Are there any publicly available SOAP 1.2/WSDL 2.0 compliant free web services for testing a Python based soap client library (e.g. Zolera SOAP Infrastructure)? So far, it appears to me that Google Web API may be the only option.Otherwise, how can one test a SOAP 1.2 compliant client library?

2008-11-22 19:06:39Z

Are there any publicly available SOAP 1.2/WSDL 2.0 compliant free web services for testing a Python based soap client library (e.g. Zolera SOAP Infrastructure)? So far, it appears to me that Google Web API may be the only option.Otherwise, how can one test a SOAP 1.2 compliant client library?There is a bunch on here:http://www.webservicex.net/WS/wscatlist.aspxJust google for "Free WebService" or "Open WebService" and you'll find tons of open SOAP endpoints.Remember, you can get a WSDL from any ASMX endpoint by adding ?WSDL to the url.

TensorFlow saving into/loading a graph from a file

Technicolor

[TensorFlow saving into/loading a graph from a file](https://stackoverflow.com/questions/38947658/tensorflow-saving-into-loading-a-graph-from-a-file)

From what I've gathered so far, there are several different ways of dumping a TensorFlow graph into a file and then loading it into another program, but I haven't been able to find clear examples/information on how they work. What I already know is this:However, I haven't been able to clear up several questions regarding these different methods:In short, what I'm looking for is a method to save both a graph (as in, the various operations and such) and its weights/variables into a file, which can then be used to load the graph and weights into another program, for use (not necessarily continuing/retraining).Documentation about this topic isn't very straightforward, so any answers/information would be greatly appreciated.

2016-08-14 23:53:27Z

From what I've gathered so far, there are several different ways of dumping a TensorFlow graph into a file and then loading it into another program, but I haven't been able to find clear examples/information on how they work. What I already know is this:However, I haven't been able to clear up several questions regarding these different methods:In short, what I'm looking for is a method to save both a graph (as in, the various operations and such) and its weights/variables into a file, which can then be used to load the graph and weights into another program, for use (not necessarily continuing/retraining).Documentation about this topic isn't very straightforward, so any answers/information would be greatly appreciated.There are many ways to approach the problem of saving a model in TensorFlow, which can make it a bit confusing. Taking each of your sub-questions in turn:You can try the following code:

When to create a new app (with startapp) in Django?

Håkan

[When to create a new app (with startapp) in Django?](https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django)

I've googled around for this, but I still have trouble relating to what Django defines as "apps". Should I create a new app for each piece of functionality in a site, even though it uses models from the main project? Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the "main project" or other apps?

2008-09-15 16:03:22Z

I've googled around for this, but I still have trouble relating to what Django defines as "apps". Should I create a new app for each piece of functionality in a site, even though it uses models from the main project? Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the "main project" or other apps?James Bennett has a wonderful set of slides on how to organize reusable apps in Django.I prefer to think of Django applications as reusable modules or components than as "applications". This helps me encapsulate and decouple certain features from one another, improving re-usability should I decide to share a particular "app" with the community at large, and maintainability.My general approach is to bucket up specific features or feature sets into "apps" as though I were going to release them publicly. The hard part here is figuring out how big each bucket is. A good trick I use is to imagine how my apps would be used if they were released publicly. This often encourages me to shrink the buckets and more clearly define its "purpose".Here is the updated presentation on 6 September 2008.DjangoCon 2008: Reusable Apps @7:53Slide: Reusable_apps.pdfI tend to create new applications for each logically separate set of models. e.g.:The rule I follow is it should be a new app if I want to reuse the functionality in a different project.If it needs deep understanding of the models in your project, it's probably more cohesive to stick it with the models.The two best answers to this question I've found around the web are:Both sources agree that you should create a separate app in the following situations:An 'app' could be many different things, it all really comes down to taste. For example, let's say you are building a blog. Your app could be the entire blog, or you could have an 'admin' app, a 'site' app for all of the public views, an 'rss' app, a 'services' app so developers can interface with the blog in their own ways, etc.I personally would make the blog itself the app, and break out the functionality within it. The blog could then be reused rather easily in other websites.The nice thing about Django is that it will recognize any models.py file within any level of your directory tree as a file containing Django models. So breaking your functionality out into smaller 'sub apps' within an 'app' itself won't make anything more difficult.

Ignore .pyc files in git repository

enfix

[Ignore .pyc files in git repository](https://stackoverflow.com/questions/5551269/ignore-pyc-files-in-git-repository)

How can I ignore .pyc files in git?If I put it in .gitignore it doesn't work. I need them to be untracked and not checked for commits.

2011-04-05 11:45:31Z

How can I ignore .pyc files in git?If I put it in .gitignore it doesn't work. I need them to be untracked and not checked for commits.Put it in .gitignore. But from the gitignore(5) man page:So, either specify the full path to the appropriate *.pyc entry, or put it in a .gitignore file in any of the directories leading from the repository root (inclusive).You should add a line with:to the .gitignore file in the root folder of your git repository tree right after repository initialization.As ralphtheninja said, if you forgot to to do it beforehand, if you just add the line to the .gitignore file, all previously committed .pyc files will still be tracked, so you'll need to remove them from the repository.If you are on a Linux system (or "parents&sons" like a MacOSX), you can quickly do it with just this one line command that you need to execute from the root of the repository:This just means: After *.pyc files deletion from git as tracked files, commit this change to the repository, and then you can finally add the *.pyc line to the .gitignore file.(adapted from http://yuji.wordpress.com/2010/10/29/git-remove-all-pyc/)You have probably added them to the repository before putting *.pyc in .gitignore.

First remove them from the repository.i try to use the sentence of a prior post and don't work recursively, then read some help and get this line:p.d. is necessary to add *.pyc in .gitignore file to maintain git cleanEnjoy.Thanks @Enrico for the answer. Note if you're using virtualenv you will have several more .pyc files within the directory you're currently in, which will be captured by his find command.For example:I suppose it's harmless to remove all the files, but if you only want to remove the .pyc files in your main directory, then just dofind "*.pyc" -exec git rm -f "{}" \;This will remove just the app.pyc file from the git repository.

Change one value based on another value in pandas

Parseltongue

[Change one value based on another value in pandas](https://stackoverflow.com/questions/19226488/change-one-value-based-on-another-value-in-pandas)

I'm trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the data.Let's say I want to iterate over all values in the column head 'ID.' If that ID matches a specific number, then I want to change two corresponding values FirstName and LastName.In Stata it looks like this:So this replaces all values in FirstName that correspond with values of ID == 103 to Matt.  In PANDAS, I'm trying something like thisNot sure where to go from here.  Any ideas?

2013-10-07 13:42:28Z

I'm trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the data.Let's say I want to iterate over all values in the column head 'ID.' If that ID matches a specific number, then I want to change two corresponding values FirstName and LastName.In Stata it looks like this:So this replaces all values in FirstName that correspond with values of ID == 103 to Matt.  In PANDAS, I'm trying something like thisNot sure where to go from here.  Any ideas?One option is to use Python's slicing and indexing features to logically evaluate the places where your condition holds and overwrite the data there.Assuming you can load your data directly into pandas with pandas.read_csv then the following code might be helpful for you.As mentioned in the comments, you can also do the assignment to both columns in one shot:Note that you'll need pandas version 0.11 or newer to make use of loc for overwrite assignment operations.Another way to do it is to use what is called chained assignment. The behavior of this is less stable and so it is not considered the best solution (it is explicitly discouraged in the docs), but it is useful to know about:You can use map, it can map vales from a dictonairy or even a custom function.Suppose this is your df:Create the dicts:And map:The result will be:Or use a custom function:This question might still be visited often enough that it's worth offering an addendum to Mr Kassies' answer. The dict built-in class can be sub-classed so that a default is returned for 'missing' keys. This mechanism works well for pandas. But see below.In this way it's possible to avoid key errors.The same thing can be done more simply in the following way. The use of the 'default' argument for the get method of a dict object makes it unnecessary to subclass a dict.The original question addresses a specific narrow use case. For those who need more generic answers here are some examples:Given the dataframe below:

Below we are adding a new description column as a concatenation of other columns by using the + operation which is overridden for series. Fancy string formatting, f-strings etc won't work here since the + applies to scalars and not 'primitive' values:We get 1 years for the cat (instead of 1 year) which we will be fixing below using conditionals.Here we are replacing the original animal column with values from other columns, and using np.where to set a conditional substring based on the value of age:A more flexible approach is to call .apply() on an entire dataframe rather than on a single column:In the code above the transform_row(r) function takes a Series object representing a given row (indicated by axis=1, the default value of axis=0 will provide a Series object for each column). This simplifies processing since we can access the actual 'primitive' values in the row using the column names and have visibility of other cells in the given row/column.

'DataFrame' object has no attribute 'sort'

Shi Jie Tio

['DataFrame' object has no attribute 'sort'](https://stackoverflow.com/questions/44123874/dataframe-object-has-no-attribute-sort)

I face some problem here, in my python package I have install numpy, but I still have this error 'DataFrame' object has no attribute 'sort'Anyone can give me some idea..This is my code :

2017-05-23 00:14:45Z

I face some problem here, in my python package I have install numpy, but I still have this error 'DataFrame' object has no attribute 'sort'Anyone can give me some idea..This is my code :sort() was deprecated for DataFrames in favor of either:sort() was  deprecated (but still available) in Pandas with release 0.17 (2015-10-09) with the introduction of sort_values() and sort_index(). It was removed from Pandas with release 0.20 (2017-05-05).sort has been replaced in v0.20 by DataFrame.sort_values and DataFrame.sort_index. Aside from this, we also have argsort.Here are some common use cases in sorting, and how to solve them using the sorting functions in the current API. First, the setup.For example, to sort df by column "A", use sort_values with a single column name:If you need a fresh RangeIndex, use DataFrame.reset_index.For example, to sort by both col "A" and "B" in df, you can pass a list to sort_values:You can do this using sort_index:Here are some comparable methods with their performance:For example,This "sorting" problem is actually a simple indexing problem. Just passing integer labels to iloc will do.

Does python have an equivalent to Java Class.forName()?

Jason

[Does python have an equivalent to Java Class.forName()?](https://stackoverflow.com/questions/452969/does-python-have-an-equivalent-to-java-class-forname)

I have the need to take a string argument and create an object of the class named in that string in Python.  In Java, I would use Class.forName().newInstance().  Is there an equivalent in Python?Thanks for the responses.  To answer those who want to know what I'm doing: I want to use a command line argument as the class name, and instantiate it.  I'm actually programming in Jython and instantiating Java classes, hence the Java-ness of the question.  getattr() works great.  Thanks much.

2009-01-17 08:10:48Z

I have the need to take a string argument and create an object of the class named in that string in Python.  In Java, I would use Class.forName().newInstance().  Is there an equivalent in Python?Thanks for the responses.  To answer those who want to know what I'm doing: I want to use a command line argument as the class name, and instantiate it.  I'm actually programming in Jython and instantiating Java classes, hence the Java-ness of the question.  getattr() works great.  Thanks much.Reflection in python is a lot easier and far more flexible than it is in Java.I recommend reading this tutorialThere's no direct function (that I know of) which takes a fully qualified class name and returns the class, however you have all the pieces needed to build that, and you can connect them together.One bit of advice though: don't try to program in Java style when you're in python.If you can explain what is it that you're trying to do, maybe we can help you find a more pythonic way of doing it.Here's a function that does what you want:You can use the return value of this function as if it were the class itself.Here's a usage example:How does that work?We're using __import__ to import the module that holds the class, which required that we first extract the module name from the fully qualified name. Then we import the module:In this case, m will only refer to the top level module, For example, if your class lives in foo.baz module, then m will be the module foo

We can easily obtain a reference to foo.baz using getattr( m, 'baz' )To get from the top level module to the class, have to recursively use gettatr on the parts of the class nameSay for example, if you class name is foo.baz.bar.Model then we do this:This is what's happening in this loop:At the end of the loop, m will be a reference to the class. This means that m is actually the class itslef, you can do for instance:Assuming the class is in your scope:Otherwise:Edit:  Note, you can't give a name like 'foo.bar' to getattr.  You'll need to split it by . and call getattr() on each piece left-to-right.  This will handle that:Yet another implementation.It seems you're approaching this from the middle instead of the beginning. What are you really trying to do? Finding the class associated with a given string is a means to an end.If you clarify your problem, which might require your own mental refactoring, a better solution may present itself.For instance: Are you trying to load a saved object based on its type name and a set of parameters? Python spells this unpickling and you should look at the pickle module. And even though the unpickling process does exactly what you describe, you don't have to worry about how it works internally:This is found in the python standard library, as unittest.TestLoader.loadTestsFromName. Unfortunately the method goes on to do additional test-related activities, but this first ha looks re-usable. I've edited it to remove the test-related functionality:I needed to get objects for all existing classes in my_package. So I import all necessary classes into my_package's __init__.py.So my directory structure is like this:And my __init__.py looks like this:Then I create a function like this:Where module_name = 'my_package'inspect doc: https://docs.python.org/3/library/inspect.html#inspect.getmembers

Why do I get「Pickle - EOFError: Ran out of input」reading an empty file?

Magix

[Why do I get「Pickle - EOFError: Ran out of input」reading an empty file?](https://stackoverflow.com/questions/24791987/why-do-i-get-pickle-eoferror-ran-out-of-input-reading-an-empty-file)

I am getting an interesting error while trying to use Unpickler.load(), here is the source code:Here is the traceback:The file I am trying to read is empty.

How can I avoid getting this error, and get an empty variable instead?

2014-07-16 22:35:42Z

I am getting an interesting error while trying to use Unpickler.load(), here is the source code:Here is the traceback:The file I am trying to read is empty.

How can I avoid getting this error, and get an empty variable instead?I would check that the file is not empty first:Also open(target, 'a').close() is doing nothing in your code and you don't need to use ;.Most of the answers here have dealt with how to mange EOFError exceptions, which is really handy if you're unsure about whether the pickled object is empty or not.However, if you're surprised that the pickle file is empty, it could be because you opened the filename through 'wb' or some other mode that could have over-written the file.for example:This will over-write the pickled file. You might have done this by mistake before using:And then got the EOFError because the previous block of code over-wrote the cd.pkl file. When working in Jupyter, or in the console (Spyder) I usually write a wrapper over the reading/writing code, and call the wrapper subsequently. This avoids common read-write mistakes, and saves a bit of time if you're going to be reading the same file multiple times through your travails  As you see, that's actually a natural error ..A typical construct for reading from an Unpickler object would be like this ..EOFError is simply raised, because it was reading an empty file, it just meant End of File ..It is very likely that the pickled file is empty. It is surprisingly easy to overwrite a pickle file if you're copying and pasting code. For example the following writes a pickle file:And if you copied this code to reopen it, but forgot to change 'wb' to 'rb' then you would overwrite the file:The correct syntax is You can catch that exception and return whatever you want from there. Note that the mode of opening files is 'a' or some other have alphabet 'a' will also make error because of the overwritting.

redis-py : What's the difference between StrictRedis() and Redis()?

ABS

[redis-py : What's the difference between StrictRedis() and Redis()?](https://stackoverflow.com/questions/19021765/redis-py-whats-the-difference-between-strictredis-and-redis)

I want to use redis-py for caching some data, but I can't find a suitable explanation of the difference between redis.StrictRedis() and redis.Redis(). Are they equivalent?In addition, I can't find any clear documentation about redis.StrictRedis()'s arguments in Redis Python Docs.

Any idea?

2013-09-26 07:17:52Z

I want to use redis-py for caching some data, but I can't find a suitable explanation of the difference between redis.StrictRedis() and redis.Redis(). Are they equivalent?In addition, I can't find any clear documentation about redis.StrictRedis()'s arguments in Redis Python Docs.

Any idea?This seems pretty clear:andDo you need backwards compatibility? Use Redis. Don't care? Use StrictRedis.2017-03-31Here are the specifics of the backwards compatibility, from the github.com link cited:It's an old question but for anyone who reaches this question after google search:from redis-py readme (link):Here is the line from redis-py code which defines StrictRedis (link):

Convert generator object to list for debugging [duplicate]

Seanny123

[Convert generator object to list for debugging [duplicate]](https://stackoverflow.com/questions/24130745/convert-generator-object-to-list-for-debugging)

When I'm debugging in Python using IPython, I sometimes hit a break-point and I want to examine a variable that is currently a generator. The simplest way I can think of doing this is converting it to a list, but I'm not clear on what's an easy way of doing this in one line in ipdb, since I'm so new to Python.

2014-06-09 23:41:38Z

When I'm debugging in Python using IPython, I sometimes hit a break-point and I want to examine a variable that is currently a generator. The simplest way I can think of doing this is converting it to a list, but I'm not clear on what's an easy way of doing this in one line in ipdb, since I'm so new to Python.Simply call list on the generator.Be aware that this affects the generator which will not return any further items.You also cannot directly call list in IPython, as it conflicts with a command for listing lines of code.Tested on this file:which when run:There are debugger commands p and pp that will print and prettyprint any expression following them.So you could use it as follows:There is also an exec command, called by prefixing your expression with !, which forces debugger to take your expression as Python one.For more details see help p, help pp and help exec when in debugger.

How can I check if an ip is in a network in Python?

Staale

[How can I check if an ip is in a network in Python?](https://stackoverflow.com/questions/819355/how-can-i-check-if-an-ip-is-in-a-network-in-python)

Given an ip address (say 192.168.0.1), how do I check if it's in a network (say 192.168.0.0/24) in Python?Are there general tools in Python for ip address manipulation? Stuff like host lookups, ip adddress to int, network address with netmask to int and so on? Hopefully in the standard Python library for 2.5.

2009-05-04 08:59:20Z

Given an ip address (say 192.168.0.1), how do I check if it's in a network (say 192.168.0.0/24) in Python?Are there general tools in Python for ip address manipulation? Stuff like host lookups, ip adddress to int, network address with netmask to int and so on? Hopefully in the standard Python library for 2.5.This article shows you can do it with socket and struct modules without too much extra effort.  I added a little to the article as follows:This outputs:If you just want a single function that takes strings it would look like this:I like to use netaddr for that:As arno_v pointed out in the comments, new version of netaddr does it like this:Using ipaddress (in the stdlib since 3.3, at PyPi for 2.6/2.7):If you want to evaluate a lot of IP addresses this way, you'll probably want to calculate the netmask upfront, likeThen, for each address, calculate the binary representation with one ofFinally, you can simply check:For python3This code is working for me on Linux x86.  I haven't really given any thought to endianess issues, but I have tested it against the "ipaddr" module using over 200K IP addresses tested against 8 different network strings, and the results of ipaddr are the same as this code.Example:I tried Dave Webb's solution but hit some problems:Most fundamentally - a match should be checked by ANDing the IP address with the mask, then checking the result matched the Network address exactly. Not ANDing the IP address with the Network address as was done.I also noticed that just ignoring the Endian behaviour assuming that consistency will save you will only work for masks on octet boundaries (/24, /16). In order to get other masks (/23, /21) working correctly I added a "greater than" to the struct commands and changed the code for creating the binary mask to start with all "1" and shift left by (32-mask). Finally, I added a simple check that the network address is valid for the mask and just print a warning if it is not.Here's the result:I'm not a fan of using modules when they are not needed.  This job only requires simple math, so here is my simple function to do the job:Then to use it:That's it, this is much faster than the solutions above with the included modules.The accepted answer doesn't work ... which is making me angry. Mask is backwards and doesn't work with any bits that are not a simple 8 bit block (eg /24). I adapted the answer, and it works nicely.here is a function that returns a dotted binary string to help visualize the masking.. kind of like ipcalc output.eg:Not in the Standard library for 2.5, but ipaddr makes this very easy. I believe it is in 3.3 under the name ipaddress. Marc's code is nearly correct. A complete version of the code is -Obviously from the same sources as above...A very Important note is that the first code has a small glitch - The IP address 255.255.255.255 also shows up as a Valid IP for any subnet. I had a heck of time getting this code to work and thanks to Marc for the correct answer.Relying on the "struct" module can cause problems with endian-ness and type sizes, and just isn't needed. Nor is socket.inet_aton(). Python works very well with dotted-quad IP addresses:I need to do IP matching on each socket accept() call, against a whole set of allowable source networks, so I precompute masks and networks, as integers:Then I can quickly see if a given IP is within one of those networks:No module imports needed, and the code is very fast at matching.Here is the usage for this method:Basically you provide an ip address as the first argument and a list of cidrs as the second argument.  A list of hits are returned.previous solution have a bug in ip & net == net. Correct ip lookup is ip & netmask = netbugfixed code:The choosen answer has a bug.Following is the correct code:Note: ipaddr & netmask == netaddr & netmask instead of ipaddr & netmask == netmask. I also replace ((2L<<int(bits)-1) - 1) with ((1L << int(bits)) - 1), as the latter seems more understandable.Here is a class I wrote for longest prefix matching:And here is a test program:Thank you for your script!

I have work quite a long on it to make everything working... So I'm sharing it hereSo my new addressInNetwork function looks-like:And now, answer is right!!I hope that it will help other people, saving time for them!Relating to all of the above, I think socket.inet_aton() returns bytes in network order, so the correct way to unpack them is probablyThere is an API that's called SubnetTree available in python that do this job very well. 

This is a simple example : This is the link$ python check-subnet.py

False

True

FalseI don't know of anything in the standard library, but PySubnetTree is a Python library that will do subnet matching.Using Python3 ipaddress:You can think of an IP Address as a Network with the largest possible netmask (/32 for IPv4, /128 for IPv6)Checking whether 192.168.0.1 is in 192.168.0.0/16 is essentially the same as checking whether 192.168.0.1/32 is a subnet of 192.168.0.0/16From various sources above, and from my own research, this is how I got subnet and address calculation working.  These pieces are enough to solve the question and other related questions.Here is my codeIf you do not want to import other modules you could go with:I tried one subset of proposed solutions in these answers.. with no success, I finally adapted and fixed the proposed code and wrote my fixed function. I tested it and works at least on little endian architectures--e.g.x86-- if anyone likes to try on a big endian architecture, please give me feedback.IP2Int code comes from this post, the other method is a fully (for my test cases) working fix of previous proposals in this question. The code:Hope useful,Here is the solution using netaddr package

Matplotlib connect scatterplot points with line - Python

brno792

[Matplotlib connect scatterplot points with line - Python](https://stackoverflow.com/questions/20130227/matplotlib-connect-scatterplot-points-with-line-python)

