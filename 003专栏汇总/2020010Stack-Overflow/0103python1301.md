[Determine complete Django url configuration](https://stackoverflow.com/questions/1828187/determine-complete-django-url-configuration)

Is there a way to get the complete django url configuration?For example Django's debugging 404 page does not show included url configs, so this is not the complete configuration.Answer: Thanks to Alasdair, here is an example script:

2009-12-01 19:05:15Z

Is there a way to get the complete django url configuration?For example Django's debugging 404 page does not show included url configs, so this is not the complete configuration.Answer: Thanks to Alasdair, here is an example script:Django is Python, so introspection is your friend.In the shell, import urls. By looping through urls.urlpatterns, and drilling down through as many layers of included url configurations as possible, you can build the complete url configuration. The list urls.urlpatterns contains RegexURLPattern and RegexURLResolver objects.For a RegexURLPattern object p you can display the regular expression withFor a RegexURLResolver object q, which represents an included url configuration, you can display the first part of the regular expression withThen usewhich will return a further list of RegexURLResolver and RegexURLPattern objects.Django extensions provides a utility to do this as a manage.py command.Then add django_extensions to your INSTALLED_APPS in settings.py. then from the console just type the followingAt the risk of adding a "me too" answer, I am posting a modified version of the above submitted script that gives you a view listing all the URLs in the project, somewhat prettified and sorted alphabetically, and the views that they call.  More of a developer tool than a production page.  and the template:If you wanted to get real fancy you could render the list with input boxes for any of the regexes that take variables to pass to the view (again as a developer tool rather than production page).  This question is a bit old, but I ran into the same problem and I thought I would discuss my solution. A given Django project obviously needs a means of knowing about all its URLs and needs to be able to do a couple things:Django accomplishes this mostly through an object called a RegexURLResolver.You can get your hands on one of these objects the following way:Then, you can simply print out your urls the following way:That said, Alasdair's solution is perfectly fine and has some advantages, as it prints out some what more nicely than this method. But knowing about and getting your hands on a RegexURLResolver object is something nice to know about, especially if you are interested in Django internals.I have submitted a package (django-showurls) that adds this functionality to any Django project, it's a simple new management command that integrates well with manage.py:You can install it through pip:And then add it to your installed apps in your Django project settings.py file:And you're ready to go.More info here -

https://github.com/Niklas9/django-showurlsThe easiest way to get a complete list of registered URLs is to install contrib.admindocs then check the "Views" section. Very easy to set up, and also gives you fully browsable docs on all of your template tags, models, etc.Are you looking for the urls evaluated or not evaluated as shown in the DEBUG mode? For evaluated, django.contrib.sitemaps can help you there, otherwise it might involve some reverse engineering with Django's code.When I tried the other answers here, I got this error:It looks like the problem comes from using django.contrib.admin.autodiscover() in my urls.py, so I can either comment that out, or load Django properly before dumping the URL's. Of course if I want to see the admin URL's in the mapping, I can't comment them out.The way I found was to create a custom management command that dumps the urls.If you are running Django in debug mode (have DEBUG = True in your settings) and then type a non-existent URL you will get an error page listing the complete URL configuration.

Catch Ctrl+C / SIGINT and exit multiprocesses gracefully in python [duplicate]

zenpoy

[Catch Ctrl+C / SIGINT and exit multiprocesses gracefully in python [duplicate]](https://stackoverflow.com/questions/11312525/catch-ctrlc-sigint-and-exit-multiprocesses-gracefully-in-python)

How do I catch a Ctrl+C in multiprocess python program and exit all processes gracefully, I need the solution to work both on unix and windows. I've tried the following:And it's kind of working, but I don't think it's the right solution.

2012-07-03 13:56:48Z

How do I catch a Ctrl+C in multiprocess python program and exit all processes gracefully, I need the solution to work both on unix and windows. I've tried the following:And it's kind of working, but I don't think it's the right solution.The previously accepted solution has race conditions and it does not work with map and async functions.The correct way to handle Ctrl+C/SIGINT with multiprocessing.Pool is to:Putting it together:As @YakovShklarov noted, there is a window of time between ignoring the signal and unignoring it in the parent process, during which the signal can be lost. Using pthread_sigmask instead to temporarily block the delivery of the signal in the parent process would prevent the signal from being lost, however, it is not available in Python-2.The solution is based on this link and this link and it solved the problem, I had to moved to Pool though:Just handle KeyboardInterrupt-SystemExit exceptions in your worker process:

Get pandas.read_csv to read empty values as empty string instead of nan

BrenBarn

[Get pandas.read_csv to read empty values as empty string instead of nan](https://stackoverflow.com/questions/10867028/get-pandas-read-csv-to-read-empty-values-as-empty-string-instead-of-nan)

I'm using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string "nan" is a possible value, as is an empty string.  I managed to get pandas to read "nan" as a string, but I can't figure out how to get it not to read an empty value as NaN.  Here's sample data and outputIt correctly reads "nan" as the string "nan', but still reads the empty cells as NaN.  I tried passing in str in the converters argument to read_csv (with converters={'One': str})), but it still reads the empty cells as NaN.I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?

2012-06-03 00:38:37Z

I'm using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string "nan" is a possible value, as is an empty string.  I managed to get pandas to read "nan" as a string, but I can't figure out how to get it not to read an empty value as NaN.  Here's sample data and outputIt correctly reads "nan" as the string "nan', but still reads the empty cells as NaN.  I tried passing in str in the converters argument to read_csv (with converters={'One': str})), but it still reads the empty cells as NaN.I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?I added a ticket to add an option of some sort here:https://github.com/pydata/pandas/issues/1450In the meantime, result.fillna('') should do what you wantEDIT: in the development version (to be 0.8.0 final) if you specify an empty list of na_values, empty strings will stay empty strings in the resultI was still confused after reading the other answers and comments. But the answer now seems simpler, so here you go.Since Pandas version 0.9 (from 2012), you can read your csv with empty cells interpreted as empty strings by simply setting keep_default_na=False:This issue is more clearly explained inThat was fixed on on Aug 19, 2012 for Pandas version 0.9 inWe have a simple argument in Pandas read_csv for this:Use:Pandas documentation clearly explains how the above argument works.Link

How to print from Flask @app.route to python console

Robert Filter

[How to print from Flask @app.route to python console](https://stackoverflow.com/questions/32550487/how-to-print-from-flask-app-route-to-python-console)

I would like to simply print a "hello world" to the python console after /button is called by the user.This is my naive approach:Background: I would like to execute other python commands from flask (not shell). "print" should be the easiest case.

I believe I have not understood a basic twist here.

Thanks in advance!

2015-09-13 14:07:28Z

I would like to simply print a "hello world" to the python console after /button is called by the user.This is my naive approach:Background: I would like to execute other python commands from flask (not shell). "print" should be the easiest case.

I believe I have not understood a basic twist here.

Thanks in advance!It seems like you have it worked out, but for others looking for this answer, an easy way to do this is by printing to stderr. You can do that like this:Flask will display things printed to stderr in the console. For other ways of printing to stderr, see this stackoverflow postWe can also use logging to print data on the console.Example:I think the core issue with Flask is that stdout gets buffered. I was able to print with print('Hi', flush=True). You can also disable buffering by setting the PYTHONUNBUFFERED environment variable.

Unit testing with django-celery?

Jason Webb

[Unit testing with django-celery?](https://stackoverflow.com/questions/4055860/unit-testing-with-django-celery)

I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:Overall any hints or tips for testing with celery would be very helpful.

2010-10-29 20:57:31Z

I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:Overall any hints or tips for testing with celery would be very helpful.Try setting:(Thanks to asksol's comment.)I like to use the override_settings decorator on tests which need celery results to complete.If you want to apply this to all tests you can use the celery test runner as described at http://docs.celeryproject.org/en/2.5/django/unit-testing.html which basically sets these same settings except (BROKER_BACKEND = 'memory'). In settings:Look at the source for CeleryTestSuiteRunner and it's pretty clear what's happening.Here's an excerpt from my testing base class that stubs out the apply_async method and records to the calls to it (which includes Task.delay.)  It's a little gross, but it's managed to fit my needs over the past few months I've been using it.Here's an "off the top of the head" example of how you'd use it in your test cases:mymodule.pytest_mymodule.pysince I still see this come up in search results, settings override with worked for me as per Celery DocsFor everyone getting here in 2019: checkout this article covering different strategies, including calling tasks synchronously.This is what I didInside myapp.tasks.py I have:Inside myapp.test_tasks.py I have:

How can I set the 'backend' in matplotlib in Python?

user504909

[How can I set the 'backend' in matplotlib in Python?](https://stackoverflow.com/questions/4930524/how-can-i-set-the-backend-in-matplotlib-in-python)

I am new user of matplotlib, my platform is Ubuntu 10.04 Python 2.6.5This is my codeThe error is:Here is the error:

2011-02-08 07:17:14Z

I am new user of matplotlib, my platform is Ubuntu 10.04 Python 2.6.5This is my codeThe error is:Here is the error:AGG backend is for writing to file, not for rendering in a window. See the backend FAQ at the matplotlib web site.For the second error, maybe your matplotlib distribution is not compiled with GTK support, or you miss the PyGTK package. Try to install it.Do you call the show() method inside a terminal or application that has access to a graphical environment?Try other GUI backends, in this order:FYI, I found I needed to put matplotlib.use('Agg') first in Python import order. For what I was doing (unit testing needed to be headless) that meant puttingat the top of my master test script. I didn't have to touch any other files.This can also be set in the configuration file matplotlibrc (as explained in the error message), for instance:That way, the backend does not need to be hardcoded if the code is shared with other people.

For more information, check the documentation.The errors you posted are unrelated. The first one is due to you selecting a backend that is not meant for interactive use, i.e. agg. You can still use (and should use) those for the generation of plots in scripts that don't require user interaction.If you want an interactive lab-environment, as in Matlab/Pylab, you'd obviously import a backend supporting gui usage, such as Qt4Agg (needs Qt and AGG), GTKAgg (GTK an AGG) or WXAgg (wxWidgets and Agg).I'd start by trying to use WXAgg, apart from that it really depends on how you installed Python and matplotlib (source, package etc.)I hit this when trying to compile python, numpy, scipy, matplotlib in my own VIRTUAL_ENVBefore installing matplotlib you have to build and install:

pygobject

pycairo

pygtkAnd then do it with matplotlib:

Before building matplotlib check with 'python ./setup.py build --help' if 'gtkagg' backend is enabled. Then build and install Before export PKG_CONFIG_PATH=$VIRTUAL_ENV/lib/pkgconfigBefore starting python, you can do in bashYou can also try viewing the graph in a browser.Use the following:

subsampling every nth entry in a numpy array

Rich Williams

[subsampling every nth entry in a numpy array](https://stackoverflow.com/questions/25876640/subsampling-every-nth-entry-in-a-numpy-array)

I am a beginner with numpy, and I am trying to extract some data from a long numpy array. What I need to do is start from a defined position in my array, and then subsample every nth data point from that position, until the end of my array. basically if I hadI want to subsample this to start at a[1] and then sample every fourth point from there, to produce something like

2014-09-16 19:11:25Z

I am a beginner with numpy, and I am trying to extract some data from a long numpy array. What I need to do is start from a defined position in my array, and then subsample every nth data point from that position, until the end of my array. basically if I hadI want to subsample this to start at a[1] and then sample every fourth point from there, to produce something likeYou can use numpy's slicing, simply start:stop:step.This creates a view of the the original data, so it's constant time. It'll also reflect changes to the original array and keep the whole original array in memory:so if either of the above things are a problem, you can make a copy explicitly:This isn't constant time, but the result isn't tied to the original array. The copy also contiguous in memory, which can make some operations on it faster.

Modifying a Python dict while iterating over it

NPE

[Modifying a Python dict while iterating over it](https://stackoverflow.com/questions/6777485/modifying-a-python-dict-while-iterating-over-it)

Let's say we have a Python dictionary d, and we're iterating over it like so:(f and g are just some black-box transformations.)In other words, we try to add/remove items to d while iterating over it using iteritems.Is this well defined? Could you provide some references to support your answer?(It's pretty obvious how to fix this if it's broken, so this isn't the angle I am after.)

2011-07-21 14:15:58Z

Let's say we have a Python dictionary d, and we're iterating over it like so:(f and g are just some black-box transformations.)In other words, we try to add/remove items to d while iterating over it using iteritems.Is this well defined? Could you provide some references to support your answer?(It's pretty obvious how to fix this if it's broken, so this isn't the angle I am after.)It is explicitly mentioned on the Python doc page (for Python 2.7) thatSimilarly for Python 3.The same holds for iter(d), d.iterkeys() and d.itervalues(), and I'll go as far as saying that it does for for k, v in d.items(): (I can't remember exactly what for does, but I would not be surprised if the implementation called iter(d)).Alex Martelli weighs in on this here.It may not be safe to change the container (e.g. dict) while looping over the container.

So del d[f(k)] may not be safe. As you know, the workaround is to use d.items() (to loop over an independent copy of the container) instead of d.iteritems() (which uses the same underlying container).It is okay to modify the value at an existing index of the dict, but inserting values at new indices (e.g. d[g(k)]=v) may not work.You cannot do that, at least with d.iteritems(). I tried it, and Python fails withIf you instead use d.items(), then it works.In Python 3, d.items() is a view into the dictionary, like d.iteritems() in Python 2. To do this in Python 3, instead use d.copy().items(). This will similarly allow us to iterate over a copy of the dictionary in order to avoid modifying the data structure we are iterating over.I have a large dictionary containing Numpy arrays, so the dict.copy().keys() thing suggested by @murgatroid99 was not feasible (though it worked).  Instead, I just converted the keys_view to a list and it worked fine (in Python 3.4):I realize this doesn't dive into the philosophical realm of Python's inner workings like the answers above, but it does provide a practical solution to the stated problem.The following code shows that this is not well defined:The first example calls g(k), and throws an exception (dictionary changed size during iteration).The second example calls h(k) and throws no exception, but outputs:Which, looking at the code, seems wrong - I would have expected something like:I got the same problem  and I used following procedure to solve this issue.Python List can be iterate even if you modify during iterating over it.

so for following code it will print 1's infinitely.So using list and dict collaboratively you can solve this problem.Today I had a similar use-case, but instead of simply materializing the keys on the dictionary at the beginning of the loop, I wanted changes to the dict to affect the iteration of the dict, which was an ordered dict.I ended up building the following routine, which can also be found in jaraco.itertools:The docstring illustrates the usage. This function could be used in place of d.iteritems() above to have the desired effect.Python 3 you should just:or use:You should never modify original dictionary, it leads to confusion as well as potential bugs or RunTimeErrors. Unless you just append to the dictionary with new key names.

What is the difference between drawing plots using plot, axes or figure in matplotlib?

hashcode55

[What is the difference between drawing plots using plot, axes or figure in matplotlib?](https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl)

I'm kind of confused what is going at the backend when I draw plots in matplotlib, tbh, I'm not clear with the hierarchy of plot, axes and figure. I read the documentation and it was helpful but I'm still confused...The below code draws the same plot in three different ways - Now my question is -

2016-06-22 14:04:10Z

I'm kind of confused what is going at the backend when I draw plots in matplotlib, tbh, I'm not clear with the hierarchy of plot, axes and figure. I read the documentation and it was helpful but I'm still confused...The below code draws the same plot in three different ways - Now my question is -Method 1This lets you plot just one figure with (x,y) coordinates. If you just want to get one graphic, you can use this way.Method 2This lets you plot one or several figure(s) in the same window. As you write it, you will plot just one figure, but you can make something like this:You will plot 4 figures which are named ax1, ax2, ax3 and ax4 each one but on the same window. This window will be just divided in 4 parts with my example.Method 3I didn't use it, but you can find documentation.Example: 

Other example:Matplotlib is strongly object oriented and its principal objects are the figure and the axes (I find the name axes a bit misleading, but probably it's just me).You can think of the figure as a canvas, of which you typically specify the dimensions and possibly e.g., the background color etc etc.  You use the canvas, the figure, essentially in two ways, placing other objects on it (mostly axes, but also text labels etc) and saving its contents with savefig.You can think of an axes as a sort of Swiss Army knife, a handy object that offers a tool (e.g. .plot, .scatter, .hist etc) for everything, mostly.  You can place one, two, ... many axes inside a figure using one of many different methods.The plt procedural interface was originally developed to mimic the MATLAB™ interface but is not really different from the object oriented interface, even if you don't make a direct reference to the main objects (i.e., a figure and an axes) these objects are automatically instantiated and each plt method is, essentially, translated to a call of one of the methods of the underlying fundamental objects: e.g., a plt.plot() is a hidden_axes.plot and a plt.savefig is a hidden_figure.savefig.In every moment you can have an handle on these hidden objects using plt.gcf and plt.gca, and this is sometimes necessary when one of the object methods has not been ported  to a method in the plt namespace.I'd like to add that the plt namespace contains also a number of convenience methods to instantiate, in different ways, figure and axes.Here you use only the plt interface, you can only use a single axes in each figure, but this is what you want when you are doing an exploration of your data,

a quick recipe that gets the work done...Here you use a convenience method in the plt namespace to give a name (and a handle) to your axes object, but btw there is also an hidden figure.  You can later use the axes object to plot, to make an histogram etc, all things that you can do with the plt interface, but you can also access all its attributes and modify them with greater freedom.Here you start instantiating a figure using a convenience method in the plt namespace and later you use only the object oriented interface.It is possible to bypass the plt convenience method (matplotlib.figure.Figure) but you then have to tweak the figure for a better interactive experience (after all, it's a convenience method).I suggest bare plt.plot, plt.scatter in the context of an interactive session, possibly using IPython with its %matplotlib magic command, and also in the context of an exploratory Jupyter notebook.On the other hand the object oriented approach, plus a few plt

convenience methods, is the way to goThere is a large gray area between these extremes and if you ask me what to do I'd just say "It depends"...

When should I use @classmethod and when def method(self)?

marue

[When should I use @classmethod and when def method(self)?](https://stackoverflow.com/questions/10586787/when-should-i-use-classmethod-and-when-def-methodself)

While integrating a Django app I have not used before, I found two different ways used to define functions in classes. The author seems to use them both very intentionally. The first one is one I myself use a lot:The other one is one I do not use, mostly because I do not understand when to use it, and what for:In the Python docs the classmethod decorator is explained with this sentence:So I guess cls refers to Dummy itself (the class, not the instance). I do not exactly understand why this exists, because I could always do this:Is this just for the sake of clarity, or did I miss the most important part: spooky and fascinating things that couldn't be done without it?

2012-05-14 15:54:17Z

While integrating a Django app I have not used before, I found two different ways used to define functions in classes. The author seems to use them both very intentionally. The first one is one I myself use a lot:The other one is one I do not use, mostly because I do not understand when to use it, and what for:In the Python docs the classmethod decorator is explained with this sentence:So I guess cls refers to Dummy itself (the class, not the instance). I do not exactly understand why this exists, because I could always do this:Is this just for the sake of clarity, or did I miss the most important part: spooky and fascinating things that couldn't be done without it?Your guess is correct - you understand how classmethods work.The why is that these methods can be called both on an instance OR on the class (in both cases, the class object will be passed as the first argument):On the use of these on instances: There are at least two main uses for calling a classmethod on an instance:The difference with staticmethod: There is another way of defining methods that don't access instance data, called staticmethod. That creates a method which does not receive an implicit first argument at all; accordingly it won't be passed any information about the instance or class on which it was called. The main use I've found for it is to adapt an existing function (which doesn't expect to receive a self) to be a method on a class (or object).Basically, you should use a @classmethod when you realize that the definition of the method will not be changed or overriden.An additional : teorically, class methods are faster then object methods, because don't need to be instantiated and need less memory.If you add decorator @classmethod, That means you are going to make that method as static method of java or C++. ( static method is a general term I guess ;) )

Python also has @staticmethod. and difference between classmethod and staticmethod is whether you can

access to class or static variable using argument or classname itself.all those classes increase cls.cls_var by 1 and print it.And every classes using same name on same scope or instances constructed with these class is going to share those methods.

There's only one TestMethod.cls_var 

and also there's only one TestMethod.class_method() , TestMethod.static_method()And important question. why these method would be needed.classmethod or staticmethod is useful when you make that class as a factory

or when you have to initialize your class only once. like open file once, and using feed method to read the file line by line.

Override a method at instance level

pistacchio

[Override a method at instance level](https://stackoverflow.com/questions/394770/override-a-method-at-instance-level)

Is there a way in Python to override a class method at instance level?

For example:

2008-12-27 07:12:07Z

Is there a way in Python to override a class method at instance level?

For example:Please do not do this as shown.   You code becomes unreadable when you monkeypatch an instance to be different from the class.You cannot debug monkeypatched code.When you find a bug in boby and print type(boby), you'll see that (a) it's a Dog, but (b) for some obscure reason it doesn't bark correctly.  This is a nightmare.  Do not do it.Please do this instead.Yes, it's possible:You need to use MethodType from types module. Purpose of MethodType is overwrite instance level methods (so that self can be available in overwritten method).see below example.You can use the boby variable inside the function if you need. Since you are overriding the method just for this one instance object, this way is simpler and has exactly the same effect as using self.To explain @codelogic's excellent answer, I propose a more explicit approach. This is the same technique that the . operator goes thorough to bind a class method when you access it as an instance attribute, except that your method will actually be a function defined outside of a class.Working with @codelogic's code, the only difference is in how the method is bound. I am using the fact that functions and methods are non-data descriptors in Python, and invoking the __get__ method. Note particularly that both the original and the replacement have identical signatures, meaning that you can write the replacement as a full class method, accessing all the instance attributes via self.By assigning the bound method to an instance attribute, you have created a nearly complete simulation of overriding a method. One handy feature that is missing is access to the no-arg version of super, since you are not in a class definition. Another thing is that the __name__ attribute of your bound method will not take the name of the function it is overriding, as it would in class definition, but you can still set it manually. The third difference is that your manually-bound method is a plain attribute reference that just happens to be a function. The . operator does nothing but fetch that reference. When invoking a regular method from an instance on the other hand, the binding process creates a new bound method every time.The only reason that this works, by the way, is that instance attributes override non-data descriptors. Data descriptors have __set__ methods, which methods (fortunately for you) do not. Data descriptors in the class actually take priority over any instance attributes. That is why you can assign to a property: their __set__ method gets invoked when you try to make an assignment. I personally like to take this a step further and hide the actual value of the underlying attribute in the instance's __dict__, where it is inaccessible by normal means exactly because the property shadows it.You should also keep in mind that this is pointless for magic (double underscore) methods. Magic methods can of course be overridden in this way, but the operations that use them only look at the type. For example, you can set __contains__ to something special in your instance, but calling x in instance would disregard that and use type(instance).__contains__(instance, x) instead. This applies to all magic methods specified in the Python data model.Since functions are first class objects in Python you can pass them while initializing your class object or override it anytime for a given class instance:and the results are Since no one is mentioning functools.partial here:Though I liked the inheritance idea from S. Lott and agree with the 'type(a)' thing, but since functions too have accessible attributes, I think the it can be managed this way:and the output is :Dear this is not overriding you are just calling the same function twice with the object. Basically overriding is related to more than one class. when same signature method exist in different classes then which function your are calling this decide the object who calls this. Overriding is possible in python when you make more than one classes are writes the same functions and one thing more to share that overloading is not allowed in pythonI found this to be the most accurate answer to the original questionhttps://stackoverflow.com/a/10829381/7640677

JavaScript function similar to Python range()

clwen

[JavaScript function similar to Python range()](https://stackoverflow.com/questions/8273047/javascript-function-similar-to-python-range)

Is there a function in JavaScript similar to Python's range()?I think there should be a better way than to write the following lines every time:

2011-11-25 18:33:19Z

Is there a function in JavaScript similar to Python's range()?I think there should be a better way than to write the following lines every time:No, there is none, but you can make one.Trying to emulate how it works in Python, I would create function similar to this:See this jsfiddle for a proof.It works in the following way:and its Python counterpart works exactly the same way (at least in the mentioned cases):So if you need a function to work similarly to Python's range(), you can use above mentioned solution.For a very simple range in ES6:From bigOmega's comment, this can be shortened using Spread syntax:2018: this answer keeps getting upvotes, so here's an update. The code below is obsolete, but luckily ES6 standardized generators and the yield keyword, and they are universally supported across platforms. An example of the lazy range() using yield can be found here.  In addition to what's already said, Javascript 1.7+ provides support for iterators and generators which can be used to create a lazy, memory-efficient version of range, simlar to xrange in Python2: Fusing together both answers from @Tadeck and @georg, I came up with this:To use it in a for loop you need the ES6/JS1.7 for-of loop:A port of the range function from Python 2 is provided by the underscore.js and lodash utility libraries (along with many other useful tools). Examples copied from the underscore docs:Can be achieved by attaching an iterator to the Number prototypeTaken from Kyle Simpson's course Rethinking Asynchronous JavaScriptHere you go.This will write (or overwrite) the value of each index with the index number.If you don't provide a number, it will use the current length of the Array.Use it like this:Here's a small extension for one of the answers in case you need to specify both starting and ending position of the range:The following is a natural adaption of Python's range() function to JavaScript:It supports any argument which can be compared to 0 and stop and can be incremented by step. It behaves identical to the Python version when used with numbers not exceeding Number.MAX_SAFE_INTEGER.Please note the following corner cases:In contrast to @Tadeck's, @Volv's and @janka102's answer which return [], undefined or enter an infinite loop when step evaluates to 0 or NaN, this generator function throws an exception similar to Python's behavior.For getting an array of size x, here's an one-liner without using any libraryworks as Further refined with ES6 default parameters.You may use underscore library. It contains dozens of useful functions for working with arrays and many more.pythonic mimics the Python range behaviour best it can using JS' generators (yield), supporting both the range(stop) and range(start, stop, step) use cases. In addition, pythonic's range function returns a custom built Generator object that supports map and filter, so one could do fancy one-liners like:Install using npm:Disclosure I'm author and maintainer of PythonicAll of the solutions here are referring to Python 2's range (probably because of the code example you gave).  However in Python 3, the range() method returns an iterator.  JavaScript also has iterators and they're more space efficient than generating the whole array and storing it in memory.So the more accurate representation of Python 3's range(n) function is Array(n).keys().For example:One more example (which has already been covered in the other answers). Converting the iterator to an array (ES6):Here is another es6 implementation of the range MDN recommends this approach: Sequence generator (range)No, there is none, but you can make one.I'm partial to Python3 behavior of range. You will find below JavaScript's implementation of Python's range():Assuming you need a simple range with a single step:    elserefer to here for more.As answered before: no, there's not. But you can make your own.

I believe this is an interesting approach for ES6. It works very similar to Python 2.7 range(), but it's much more dynamic.This function can behave in three different ways (just like Python's range()):These examples:Will give you the following output:Note that instead of iterating over the array with the in operator (like python), you have to use of. Thus the i variable assumes the value, and not the index, of the array's element.Still no built-in function that is equivalent to range(), but with the most recent version - ES2015 - you can build your own implementation. Here's a limited version of it. Limited because it doesn't take into account the step parameter. Just min, max.This is accomplished by the Array.from method able to build an array from any object that has a length property. So passing in a simple object with just the length property will create an ArrayIterator that will yield length number of objects.

Python: Removing list element while iterating over list [duplicate]

Scrontch

[Python: Removing list element while iterating over list [duplicate]](https://stackoverflow.com/questions/6022764/python-removing-list-element-while-iterating-over-list)

I'm iterating over a list of elements in Python, do some action on it, and then remove them if they meet certain criteria.What should I use in place of remove_element?

I have seen similar questions asked, but notice the presence of the do_action part that is to be executed for all elements and thus eliminates the solution of using filters.

2011-05-16 20:12:40Z

I'm iterating over a list of elements in Python, do some action on it, and then remove them if they meet certain criteria.What should I use in place of remove_element?

I have seen similar questions asked, but notice the presence of the do_action part that is to be executed for all elements and thus eliminates the solution of using filters.You could always iterate over a copy of the list, leaving you free to modify the original:To meet these criteria: modify original list in situ, no list copies, only one pass, works, a traditional solution is to iterate backwards:Bonus: Doesn't do len(somelist) on each iteration. Works on any version of Python (at least as far back as 1.5.2) ... s/xrange/range/ for 3.X.Update: If you want to iterate forwards, it's possible, just trickier and uglier:List comp:If you really need to do it in one pass without copying the listYou can still use filter, moving to an outside function the element modification (iterating just once)Another way of doing so is:So, you delete the elements side by side while checkingYou can make a generator that returns everything that isn't removed:Why not rewrite it to be See this question for how to remove from the list, though it looks like you've already seen that

Remove items from a list while iteratingAnother option is to do this if you really want to keep this the sameNot exactly in-place, but some idea to do it:You can extend the function to call you filter in the condition.

Slow Requests on Local Flask Server

Meroon

[Slow Requests on Local Flask Server](https://stackoverflow.com/questions/11150343/slow-requests-on-local-flask-server)

Just starting to play around with Flask on a local server and I'm noticing the request/response times are way slower than I feel they should be.Just a simple server like the following takes close to 5 seconds to respond.Any ideas? Or is this just how the local server is?

2012-06-22 04:48:28Z

Just starting to play around with Flask on a local server and I'm noticing the request/response times are way slower than I feel they should be.Just a simple server like the following takes close to 5 seconds to respond.Any ideas? Or is this just how the local server is?Ok I figured it out. It appears to be an issue with Werkzeug and os's that support ipv6.From the Werkzeug site http://werkzeug.pocoo.org/docs/serving/:So the fix is to disable ipv6 from the localhost by commenting out the following line from my hosts file:Once I do this the latency problems go away.I'm really digging Flask and I'm glad that it's not a problem with the framework. I knew it couldn't be.Add "threaded=True" as an argument to app.run(), as suggested here:

http://arusahni.net/blog/2013/10/flask-multithreading.htmlFor example: app.run(host="0.0.0.0", port=8080, threaded=True)The ipv6-disabling solution did not work for me, but this did. The solution from @sajid-siddiqi is technically correct, but keep in mind that the built-in WSGI server in Werkzeug (which is packaged into Flask and what it uses for app.run()) is only single-threaded.Install a WSGI server to be able to handle multi-threaded behavior. I did a bunch of research on various WSGI server performances. Your needs may vary, but if all you're using is Flask, then I would recommend one of the following webservers.For Python 2.x: geventYou can install gevent through pip with the command pip install gevent. Instructions on how to modify your code accordingly are here: http://flask.pocoo.org/docs/0.10/deploying/wsgi-standalone/#geventFor Python 3.x: meinheldgevent is better, but it still hasn't been updated to use python3 (see this thread for updates: https://github.com/gevent/gevent/issues/38). From all the benchmarks I've looked at that involve real-world testing, meinheld seems to be the most straightforward, simplistic WSGI server. (You could also take a look at uWSGI if you don't mind some more configuration.)You can also install meinheld through pip3 with the command pip3 install meinheld. From there, look at the sample provided in the meinheld source to integrate Flask: https://github.com/mopemope/meinheld/blob/master/example/flask_sample.py*NOTE: From my use of PyCharm, the line from meinheld import server highlights as an error, but the server will run, so you can ignore the error.I don't quite have the reputation to comment, so I'll add this as a "solution".

My problem was solved by "threaded=True", but I want to give some background to distinguish my problem from others for which this may not do it.My best guess is that Chrome was trying to keep the session open and Flask was blocking the subsequent requests. As soon as the connection from Chrome was stopped or reset, everything else was processed.In my case, threading fixed it. Of course, I'm now going through some of the links others have provided to make sure that it's not going to cause any other issues.threaded=True works for me, but finally I figured out that the issue is due to foxyproxy on firefox. Since when the flask app is running on localhost, slow response happens ifslow response won't happen ifThe only solution I found is to disable foxyproxy, tried to add localhost to proxy blacklist and tweak settings but none of them worked.I used Miheko's response to solve my issue. ::1     localhost was already commented out on my hosts file, and setting Threaded=true didn't work for me. Every REST request was taking 1 second to process instead of being instant.I'm using python 3.6, and I got flask to be fast and responsive to REST requests by making flask use gevent as its WSGI.To use gevent, install it with pip install geventAfterwards, I used the https://gist.github.com/viksit/b6733fe1afdf5bb84a40#file-async_flask-py-L41 to set flask to use gevent.Incase the link goes down, here's the important parts of the script:Instead of calling http://localhost:port/endpoint call http://127.0.0.1:port/endpoint. 

This removed the initial 500ms delay for me.I got this error when running on hosts other than localhost as well, so for some, different underlying problems may exhibit the same symptoms.I switched most of the things I've been using to Tornado, and anecdotally it's helped an amount. I've had a few slow page loads, but things seem generally more responsive. Also, very anecdotal, but I seem to notice that Flask alone will slow down over time, but Flask + Tornado less so. I imagine using Apache and mod_wsgi would make things even better, but Tornado's really simple to set up (see http://flask.pocoo.org/docs/deploying/others/).(Also, a related question: Flask app occasionally hanging)I had a different solution here. I just deleted all .pyc from the server's directory and started it again.

By the way, localhost was already commented out in my hosts file (Windows 8).The server was freezing the whole time and now it works fine again.

How to write UTF-8 in a CSV file

Martin

[How to write UTF-8 in a CSV file](https://stackoverflow.com/questions/18766955/how-to-write-utf-8-in-a-csv-file)

I am trying to create a text file in csv format out of a PyQt4 QTableWidget. I want to write the text with a UTF-8 encoding because it contains special characters. I use following code:It works until the cell contains a special character. I tried also withBut it also stops when a special character appears. I have no idea what I am doing wrong.

2013-09-12 14:25:27Z

I am trying to create a text file in csv format out of a PyQt4 QTableWidget. I want to write the text with a UTF-8 encoding because it contains special characters. I use following code:It works until the cell contains a special character. I tried also withBut it also stops when a special character appears. I have no idea what I am doing wrong.From your shell run:And (unlike the original question) presuming you're using Python's built in csv module, turn 

import csv into 

import unicodecsv as csv in your code.It's very simple for Python 3.x (docs).For Python 2.x, look here.Use this package, it just works: https://github.com/jdunck/python-unicodecsv. For me the UnicodeWriter class from Python 2 CSV module documentation didn't really work as it breaks the csv.writer.write_row() interface.For example:

works, while:

will throw AttributeError: 'int' object has no attribute 'encode'.As UnicodeWriter obviously expects all column values to be strings, we can convert the values ourselves and just use the default CSV module:Or we can even monkey-patch csv_writer to add a write_utf8_row function - the exercise is left to the reader.The examples in the Python documentation show how to write Unicode CSV files: http://docs.python.org/2/library/csv.html#examples(can't copy the code here because it's protected by copyright)For python2 you can use this code before csv_writer.writerows(rows)

 This code will NOT convert integers to utf-8 stringsA very simple hack is to use the json import instead of csv.  For example instead of csv.writer just do the following:Basically, given the list of fields in correct order, the json formatted string is identical to a csv line except for [ and ] at the start and end respectively. And json seems to be robust to utf-8 in python 2.*

Why don't my south migrations work?

TIMEX

[Why don't my south migrations work?](https://stackoverflow.com/questions/4840102/why-dont-my-south-migrations-work)

First, I create my database.I add "south" to installed Apps.  Then, I go to this tutorial:  http://south.aeracode.org/docs/tutorial/part1.htmlThe tutorial tells me to do this:Great, now I migrate.But it gives me this error...So I use Google (which never works. hence my 870 questions asked on Stackoverflow), and I get this page: http://groups.google.com/group/south-users/browse_thread/thread/d4c83f821dd2ca1cAlright, so I follow that instructionsBut when I run syncdb, Django creates a bunch of tables. Yes, it creates the south_migrationhistory table, but it also creates my app's tables.Cool....now it tells me to migrate these.  So, I do this:Alright, so fine. I'll add wall to initial migrations.Then I migrate:You know what? It gives me this BS:Sorry, this is really pissing me off. Can someone help ? thanks.How do I get South to work and sync correctly with everything? The only thing I can think of is remove my app from INSTALLED_APPS, then run syncdb, then add it back on.That is SO SILLY.

2011-01-29 23:23:13Z

First, I create my database.I add "south" to installed Apps.  Then, I go to this tutorial:  http://south.aeracode.org/docs/tutorial/part1.htmlThe tutorial tells me to do this:Great, now I migrate.But it gives me this error...So I use Google (which never works. hence my 870 questions asked on Stackoverflow), and I get this page: http://groups.google.com/group/south-users/browse_thread/thread/d4c83f821dd2ca1cAlright, so I follow that instructionsBut when I run syncdb, Django creates a bunch of tables. Yes, it creates the south_migrationhistory table, but it also creates my app's tables.Cool....now it tells me to migrate these.  So, I do this:Alright, so fine. I'll add wall to initial migrations.Then I migrate:You know what? It gives me this BS:Sorry, this is really pissing me off. Can someone help ? thanks.How do I get South to work and sync correctly with everything? The only thing I can think of is remove my app from INSTALLED_APPS, then run syncdb, then add it back on.That is SO SILLY.South allows you to create migrations when you first start out with a new app and the tables haven't been added to the database yet, as well as creating migrations for legacy apps that already have tables in the database. The key is to know when to do what. Your first mistake was when you deleted your migrations, as soon as you did that, and then ran syncdb, Django didn't know that you wanted south to manage that app anymore, so it created the tables for you. When you created your initial migrations and then ran migrate, south was trying to create tables that django already created, and thus your error.At this point you have two options.Now that you are setup with south, you can start using south to manage model changes to those apps. The most common command to run is python manage.py schemamigration app_name migration_name --auto that will look at the last migration you ran and it will find the changes and build out a migration file for you. Then you just need to run python manage.py migrate and it alter your database for you.Hope that helps.This is how I get things working.References:http://garmoncheg.blogspot.com/2011/08/django-how-and-why-to-use-migrations.html

http://www.djangopro.com/2011/01/django-database-migration-tool-south-explained/The tutorial you're using states:Assuming that your post accurately details the steps you've taken, following that link seems to show that you missed a step before setting up your new app. As you are following a tutorial for setting up migrations on a new application, the order is:I.e., you should've already run syncdb before you added in the models for your new app. Your solution of removing your app from INSTALLED_APPS should work, but it's worth noting that it's really only a "silly" work-around, as you missed a step earlier on. Had syncdb been run before you created the models for that app, you wouldn't have to use the work-around.Just for future ref. If South is giving you any problems:This seems obvious, but I'd highly recommend reading the docs.Even after reading the answers to this question I struggled to understand how to use South effectively.That all changed of course the day I read the docs and you should too, South is simpler to use than you might think.http://south.aeracode.org/docs/about.htmlhttp://south.aeracode.org/docs/tutorial/index.htmlhttp://south.aeracode.org/docs/convertinganapp.html#converting-an-appI also found this useful:http://www.djangopro.com/2011/01/django-database-migration-tool-south-explained/And make sure you read Jeff Atwood's Coding Horror articles on database version control.I have used that fix with South troubles in the past. Not a pretty solution but very effective ;)But the main problem is that your order isn't correct. You should have run syncdb before the tutorial. Than it works properly.

Python NameError: name 'include' is not defined [closed]

eloiletagant

[Python NameError: name 'include' is not defined [closed]](https://stackoverflow.com/questions/34471102/python-nameerror-name-include-is-not-defined)

I'm currently developing a website with the framework django (I'm very beginner) but I have a problem with python : since I have created my templates, I can't run server anymore for this reason (Stacktrace points to a line in urls.py):Where can I import include from?

2015-12-26 11:54:37Z

I'm currently developing a website with the framework django (I'm very beginner) but I have a problem with python : since I have created my templates, I can't run server anymore for this reason (Stacktrace points to a line in urls.py):Where can I import include from?Guessing on the basis of whatever little information provided in the question, i think you might have forget to add the following import in your urls.py file.

How to reload modules in django shell?

Mad Wombat

[How to reload modules in django shell?](https://stackoverflow.com/questions/3772260/how-to-reload-modules-in-django-shell)

I am working with Django and use Django shell all the time. The annoying part is that while the Django server reloads on code changes, the shell does not, so every time I make a change to a method I am testing, I need to quit the shell and restart it, re-import all the modules I need, reinitialize all the variables I need etc. While iPython history saves a lot of typing on this, this is still a pain. Is there a way to make django shell auto-reload, the same way django development server does?I know about reload(), but I import a lot of models and generally use from app.models import * syntax, so reload() is not much help.

2010-09-22 18:02:49Z

I am working with Django and use Django shell all the time. The annoying part is that while the Django server reloads on code changes, the shell does not, so every time I make a change to a method I am testing, I need to quit the shell and restart it, re-import all the modules I need, reinitialize all the variables I need etc. While iPython history saves a lot of typing on this, this is still a pain. Is there a way to make django shell auto-reload, the same way django development server does?I know about reload(), but I import a lot of models and generally use from app.models import * syntax, so reload() is not much help.I recommend using the django-extensions project like stated above by dongweiming. But instead of just 'shell_plus' management command, use:This will open a IPython notebook on your web browser. Write your code there in a cell, your imports etc. and run it. When you change your modules, just click the notebook menu item 'Kernel->Restart'There you go, your code is now using your modified modules.I'd suggest use IPython autoreload extension.And from now all imported modules would be refreshed before evaluate.Works also if something was imported before %load_ext autoreload command.There is possible also prevent some imports from refreshing with %aimport command and 3 autoreload strategies:This generally works good for my use, but there are some cavetas:look at the manage.py shell_plus command provided by the django-extensions project. It will load all your model files on shell startup. and autoreload your any modify but do not need exit, you can direct call thereMy solution to it is I write the code and save to a file and then use:So I can make the change, save and run that command again till I fix whatever I'm trying to fix.It seems that the general consensus on this topic, is that python reload() sucks and there is no good way to do this.Reload() doesn't work in Django shell without some tricks. You can check this thread na and my answer specifically:How do you reload a Django model module using the interactive interpreter via "manage.py shell"?My solution for this inconvenient follows. I am using IPython.For Python 3.x, 'reload' must be imported using:Hope it helps. Of course it is for debug purposes.Cheers.Use shell_plus with an ipython config.  This will enable autoreload before shell_plus automatically imports anything.Edit your ipython profile (~/.ipython/profile_default/ipython_config.py):Open a shell - note that you do not need to include --ipython:Now anything defined in SHELL_PLUS_PRE_IMPORTS or SHELL_PLUS_POST_IMPORTS (docs) will autoreload!Note that if your shell is at a debugger (ex pdb.set_trace()) when you save a file it can interfere with the reload.Instead of running commands from the Django shell, you can set up a management command like so and rerun that each time.Not exactly what you want, but I now tend to build myself management commands for testing and fiddling with things.In the command you can set up a bunch of locals the way you want and afterwards drop into an interactive shell.No reload, but an easy and less annoying way to interactively test django functionality.

How to create a user in Django?

Amr M. AbdulRahman

[How to create a user in Django?](https://stackoverflow.com/questions/10372877/how-to-create-a-user-in-django)

I'm trying to create a new User in a Django project by the following code, but the highlighted line fires an exception.Any help?

2012-04-29 14:27:13Z

I'm trying to create a new User in a Django project by the following code, but the highlighted line fires an exception.Any help?The correct way to create a user in Django is to use the create_user function. This will handle the hashing of the password, etc.. Have you confirmed that you are passing actual values and not None?Bulk user creation with set_passwordI you are creating several test users, bulk_create is much faster, but we can't use create_user with it.set_password is another way to generate the hashed passwords:Question specific about password hashing: How to use Bcrypt to encrypt passwords in DjangoTested in Django 1.9.If you creat user normally, you will not be able to login as password creation method may b different

You can use default signup form for thatYou can extend that alsoThen in view use this class

Read streaming input from subprocess.communicate()

Heinrich Schmetterling

[Read streaming input from subprocess.communicate()](https://stackoverflow.com/questions/2715847/read-streaming-input-from-subprocess-communicate)

I'm using Python's subprocess.communicate() to read stdout from a process that runs for about a minute. How can I print out each line of that process's stdout in a streaming fashion, so that I can see the output as it's generated, but still block on the process terminating before continuing? subprocess.communicate() appears to give all the output at once.

2010-04-26 18:23:18Z

I'm using Python's subprocess.communicate() to read stdout from a process that runs for about a minute. How can I print out each line of that process's stdout in a streaming fashion, so that I can see the output as it's generated, but still block on the process terminating before continuing? subprocess.communicate() appears to give all the output at once.Please note, I think J.F. Sebastian's method (below) is better.Here is an simple example (with no checking for errors): If ls ends too fast, then the while loop may end before you've read all the data.You can catch the remainder in stdout this way:To get subprocess' output line by line as soon as the subprocess flushes its stdout buffer:iter() is used to read lines as soon as they are written to workaround the read-ahead bug in Python 2.If subprocess' stdout uses a block buffering instead of a line buffering in non-interactive mode (that leads to a delay in the output until the child's buffer is full or flushed explicitly by the child) then you could try to force an unbuffered output using  pexpect, pty modules or unbuffer, stdbuf, script utilities, see Q: Why not just use a pipe (popen())?Here's Python 3 code:Note: Unlike Python 2 that outputs subprocess' bytestrings as is; Python 3 uses text mode (cmd's output is decoded using locale.getpreferredencoding(False) encoding).I believe the simplest way to collect output from a process in a streaming fashion is like this:The readline() or read() function should only return an empty string on EOF, after the process has terminated - otherwise it will block if there is nothing to read (readline() includes the newline, so on empty lines, it returns "\n"). This avoids the need for an awkward final communicate() call after the loop.On files with very long lines read() may be preferable to reduce maximum memory usage - the number passed to it is arbitrary, but excluding it results in reading the entire pipe output at once which is probably not desirable.If you want a non-blocking approach, don't use process.communicate(). If you set the subprocess.Popen() argument stdout to PIPE, you can read from process.stdout and check if the process still runs using process.poll().If you're simply trying to pass the output through in realtime, it's hard to get simpler than this:See the docs for subprocess.check_call().If you need to process the output, sure, loop on it. But if you don't, just keep it simple.Edit: J.F. Sebastian points out both that the defaults for the stdout and stderr parameters pass through to sys.stdout and sys.stderr, and that this will fail if sys.stdout and sys.stderr have been replaced (say, for capturing output in tests).

Python Pandas User Warning: Sorting because non-concatenation axis is not aligned

MishD

[Python Pandas User Warning: Sorting because non-concatenation axis is not aligned](https://stackoverflow.com/questions/50501787/python-pandas-user-warning-sorting-because-non-concatenation-axis-is-not-aligne)

I'm doing some code practice and applying merging of data frames while doing this 

getting user warning On these lines of code: Can you please help to get the solution of this warning.

2018-05-24 05:43:33Z

I'm doing some code practice and applying merging of data frames while doing this 

getting user warning On these lines of code: Can you please help to get the solution of this warning.tl;dr:concat and append currently sort the non-concatenation index (e.g. columns if you're adding rows) if the columns don't match. In pandas 0.23 this started generating a warning; pass the parameter sort=True to silence it. In the future the default will change to not sort, so it's best to specify either sort=True or False now, or better yet ensure that your non-concatenation indices match.The warning is new in pandas 0.23.0:In a future version of pandas pandas.concat() and DataFrame.append() will no longer sort the non-concatenation axis when it is not already aligned. The current behavior is the same as the previous (sorting), but now a warning is issued when sort is not specified and the non-concatenation axis is not aligned,

link.More information from linked very old github issue, comment by smcinerney :After some time the parameter sort was implemented in pandas.concat and DataFrame.append:So if both DataFrames have the same columns in the same order, there is no warning and no sorting:But if the DataFrames have different columns, or the same columns in a different order, pandas returns a warning if no parameter sort is explicitly set (sort=None is the default value):If the DataFrames have different columns, but the first columns are aligned - they will be correctly assigned to each other (columns a and b from df1 with a and b from df2 in the example below) because they exist in both. For other columns that exist in one but not both DataFrames, missing values are created.Lastly, if you pass sort=True, columns are sorted alphanumerically. If sort=False and the second DafaFrame has columns that are not in the first, they are appended to the end with no sorting:In your code:jezrael's answer is good, but did not answer a question I had: Will getting the "sort" flag wrong mess up my data in any way? The answer is apparently "no", you are fine either way.

Linear regression with matplotlib / numpy

DSM

[Linear regression with matplotlib / numpy](https://stackoverflow.com/questions/6148207/linear-regression-with-matplotlib-numpy)

I'm trying to generate a linear regression on a scatter plot I have generated, however my data is in list format, and all of the examples I can find of using polyfit require using arange. arange doesn't accept lists though. I have searched high and low about how to convert a list to an array and nothing seems clear. Am I missing something?Following on, how best can I use my list of integers as inputs to the polyfit?here is the polyfit example I am following:

2011-05-27 05:32:22Z

I'm trying to generate a linear regression on a scatter plot I have generated, however my data is in list format, and all of the examples I can find of using polyfit require using arange. arange doesn't accept lists though. I have searched high and low about how to convert a list to an array and nothing seems clear. Am I missing something?Following on, how best can I use my list of integers as inputs to the polyfit?here is the polyfit example I am following:arange generates lists (well, numpy arrays); type help(np.arange) for the details.  You don't need to call it on existing lists.I should add that I tend to use poly1d here rather than write out "m*x+b" and the higher-order equivalents, so my version of your code would look something like this:This code:gives out a list with the following:SourceUSe this ..Another quick and dirty answer is that you can just convert your list to an array using: 

Find length of 2D array Python

Ronaldinho Learn Coding

[Find length of 2D array Python](https://stackoverflow.com/questions/10713004/find-length-of-2d-array-python)

How do I find how many rows and columns are in a 2d array?

For example,should be displayed as 3 rows and 2 columns.

2012-05-23 03:19:40Z

How do I find how many rows and columns are in a 2d array?

For example,should be displayed as 3 rows and 2 columns.Like this:Assuming that all the sublists have the same length (that is, it's not a jagged array).You can use numpy.shape.Result:First value in the tuple is number rows = 3; second value in the tuple is number of columns = 2.In addition, correct way to count total item number would be:Assuming input[row][col],You can also use np.size(a,1), 1 here is the axis and this will give you the number of columnsassuming input[row][col]

Subprocess changing directory

ducin

[Subprocess changing directory](https://stackoverflow.com/questions/21406887/subprocess-changing-directory)

I want to execute a script inside a subdirectory/superdirectory (I need to be inside this sub/super-directory first). I can't get subprocess to enter my subdirectory:Python throws OSError and I don't know why. It doesn't matter whether I try to go into an existing subdir or go one directory up (as above) - I always end up with the same error.

2014-01-28 13:28:30Z

I want to execute a script inside a subdirectory/superdirectory (I need to be inside this sub/super-directory first). I can't get subprocess to enter my subdirectory:Python throws OSError and I don't know why. It doesn't matter whether I try to go into an existing subdir or go one directory up (as above) - I always end up with the same error.What your code tries to do is call a program named cd ... What you want is call a command named cd.But cd is a shell internal. So you can only call it asBut it is pointless to do so. As no process can change another process's working directory (again, at least on a UNIX-like OS, but as well on Windows), this call will have the subshell change its dir and exit immediately.What you want can be achieved with os.chdir() or with the subprocess named parameter cwd which changes the working directory immediately before executing a subprocess.For example, to execute ls in the root directory, you either can door simplyTo run your_command as a subprocess in a different directory, pass cwd parameter, as suggested in @wim's answer:A child process can't change its parent's working directory (normally). Running cd .. in a child shell process using subprocess won't change your parent Python script's working directory i.e., the code example in @glglgl's answer is wrong. cd is a shell builtin (not a separate executable), it can change the directory only in the same process.You want to use an absolute path to the executable, and use the cwd kwarg of Popen to set the working directory.  See the docs.subprocess.call and other methods in the subprocess module have a cwd parameter.This parameter determines the working directory where you want to execute your process.So you can do something like this:Check out docs subprocess.popen-constructorAnother option based on this answer: https://stackoverflow.com/a/29269316/451710This allows you to execute multiple commands (e.g cd) in the same process.I guess these days you would do:If you want to have cd functionality (assuming shell=True) and still want to change the directory in terms of the Python script, this code will allow 'cd' commands to work.

glob exclude pattern

Anastasios Andronidis

[glob exclude pattern](https://stackoverflow.com/questions/20638040/glob-exclude-pattern)

I have a directory with a bunch of files inside: eee2314, asd3442 ... and eph.I want to exclude all files that start with eph with the glob function.How can I do it?

2013-12-17 15:26:28Z

I have a directory with a bunch of files inside: eee2314, asd3442 ... and eph.I want to exclude all files that start with eph with the glob function.How can I do it?The pattern rules for glob are not regular expressions. Instead, they follow standard Unix path expansion rules. There are only a few special characters: two different wild-cards, and character ranges are supported [from glob].  So you can exclude some files with patterns.

For example to exclude manifests files (files starting with _) with glob, you can use:  You can't exclude patterns with the glob function, globs only allow for inclusion patterns. Globbing syntax is very limited (even a [!..] character class must match a character, so it is an inclusion pattern for every character that is not in the class).You'll have to do your own filtering; a list comprehension usually works nicely here:You can deduct sets:Late to the game but you could alternatively just apply a python filter to the result of a glob:or replacing the lambda with an appropriate regex search, etc...EDIT: I just realized that if you're using full paths the startswith won't work, so you'd need a regex Compare with glob, I recommend pathlib, filter one pattern is very simple.and if you want to filter more complex pattern, you can define a function to do that, just like:use that code, you can filter all files that start with eph or start with epi.How about skipping the particular file while iterating over all the files in the folder!

Below code would skip all excel files that start with 'eph'This way you can use more complex regex patterns to include/exclude a particular set of files in a folder. More generally, to exclude files that don't comply with some shell regexp, you could use module fnmatch:    The above will first generate a list from a given path and next pop out the files that won't satisfy the regular expression with the desired constraint.As mentioned by the accepted answer, you can't exclude patterns with glob, so the following is a method to filter your glob result.The accepted answer is probably the best pythonic way to do things but if you think list comprehensions look a bit ugly and want to make your code maximally numpythonic anyway (like I did) then you can do this (but note that this is probably less efficient than the list comprehension method): (In my case, I had some image frames, bias frames, and flat frames all in one directory and I just wanted the image frames)You can use the below method:

What's the best way to initialize a dict of dicts in Python? [duplicate]

mike

[What's the best way to initialize a dict of dicts in Python? [duplicate]](https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python)

A lot of times in Perl, I'll do something like this:How would I translate this to Python? So far I have:Is there a better way?

2009-03-16 19:22:51Z

A lot of times in Perl, I'll do something like this:How would I translate this to Python? So far I have:Is there a better way?Testing:Output:If the amount of nesting you need is fixed, collections.defaultdict is wonderful.e.g. nesting two deep:If you want to go another level of nesting, you'll need to do something like:edit: MizardX points out that we can get full genericity with a simple function:Now we can do:Is there a reason it needs to be a dict of dicts?  If there's no compelling reason for that particular structure, you could simply index the dict with a tuple:The parentheses are required if you initialize the dict with a tuple key, but you can omit them when setting/getting values using []:I guess the literal translation would be:Calling:gives you 1.That looks a little gross to me, though.(I'm no perl guy, though, so I'm guessing at what your perl does)

How do I get the current date and current time only respectively in Django?

Houman

[How do I get the current date and current time only respectively in Django?](https://stackoverflow.com/questions/12030187/how-do-i-get-the-current-date-and-current-time-only-respectively-in-django)

I came across an interesting situation when using this class:Django decides to use DATETIME_INPUT_FORMATS defined within the formats.py file.

Which makes sense, because I am passing in a datetime.now() to both fields.I think I could make Django to use DATE_INPUT_FORMATS and TIME_INPUT_FORMATS respectively, if I passed in only the current date and current time in.Something like this:But this obviously throws an exception as now doesn't exist like that.  Is there a different way to achieve this?

2012-08-19 21:33:51Z

I came across an interesting situation when using this class:Django decides to use DATETIME_INPUT_FORMATS defined within the formats.py file.

Which makes sense, because I am passing in a datetime.now() to both fields.I think I could make Django to use DATE_INPUT_FORMATS and TIME_INPUT_FORMATS respectively, if I passed in only the current date and current time in.Something like this:But this obviously throws an exception as now doesn't exist like that.  Is there a different way to achieve this?For the date, you can use datetime.date.today() or datetime.datetime.now().date().For the time, you can use datetime.datetime.now().time().However, why have separate fields for these in the first place? Why not use a single DateTimeField?You can always define helper functions on the model that return the .date() or .time() later if you only want one or the other.For the time  Current Date and time  Current date onlyCurrent year onlyCurrent month only Current day only 

Is there a way to get the current ref count of an object in Python?

tehvan

[Is there a way to get the current ref count of an object in Python?](https://stackoverflow.com/questions/510406/is-there-a-way-to-get-the-current-ref-count-of-an-object-in-python)

Is there a way to get the current ref count of an object in Python?

2009-02-04 07:32:51Z

Is there a way to get the current ref count of an object in Python?According to the Python documentation, the sys module contains a function:Generally 1 higher than you might expect, because of object arg temp reference. Using the gc module, the interface to the garbage collector guts, you can call gc.get_referrers(foo) to get a list of everything referring to foo.Hence, len(gc.get_referrers(foo)) will give you the length of that list: the number of referrers, which is what you're after.See also the gc module documentation.There is gc.get_referrers() and sys.getrefcount(). But, It is kind of hard to see how sys.getrefcount(X) could serve the purpose of traditional reference counting. Consider:Then function(SomeObject) delivers '7', 

sub_function(SomeObject) delivers '5', 

sub_sub_function(SomeObject) delivers '3', and 

sys.getrefcount(SomeObject) delivers '2'.In other words: If you use sys.getrefcount() you must be aware of the function call depth. For gc.get_referrers() one might have to filter the list of referrers. I would propose to do manual reference counting for purposes such as「isolation on change」, i.e.「clone if referenced elsewhere」.ctypes takes address of the variable as an argument. 

The advantage of using ctypes over sys.getRefCount is that you need not subtract 1 from the result.

Python: call a function from string name [duplicate]

İlker Dağlı

[Python: call a function from string name [duplicate]](https://stackoverflow.com/questions/7936572/python-call-a-function-from-string-name)

I have a str object for example: menu = 'install'. I want to run install method from this string. For example when I call menu(some, arguments) it will call install(some, arguments). Is there any way to do that ?

2011-10-29 02:17:48Z

I have a str object for example: menu = 'install'. I want to run install method from this string. For example when I call menu(some, arguments) it will call install(some, arguments). Is there any way to do that ?If it's in a class, you can use getattr:or if it's a function:You can use a dictionary too.Why cant we just use eval()?New methodAnd then you call the method as belowThis gives the output as 

RuntimeWarning: invalid value encountered in divide

Bogdan Osyka

[RuntimeWarning: invalid value encountered in divide](https://stackoverflow.com/questions/14861891/runtimewarning-invalid-value-encountered-in-divide)

I have to make a program using Euler's method for the "ball in a spring" modelI keep getting this error:I can't figure it out, what is wrong with the code?

2013-02-13 19:54:53Z

I have to make a program using Euler's method for the "ball in a spring" modelI keep getting this error:I can't figure it out, what is wrong with the code?I think your code is trying to "divide by zero" or "divide by NaN". If you are aware of that and don't want it to bother you, then you can try:For more details see:Python indexing starts at 0 (rather than 1), so your assignment "r[1,:] = r0" defines the second (i.e. index 1) element of r and leaves the first (index 0) element as a pair of zeros. The first value of i in your for loop is 0, so rr gets the square root of the dot product of the first entry in r with itself (which is 0), and the division by rr in the subsequent line throws the error.To prevent division by zero you could pre-initialize the output 'out' where the div0 error happens, eg np.where does not cut it since the complete line is evaluated regardless of condition.example with pre-initialization:You are dividing by rr which may be 0.0. Check if rr is zero and do something reasonable other than using it in the denominator. 

Display all dataframe columns in a Jupyter Python Notebook

Michail N

[Display all dataframe columns in a Jupyter Python Notebook](https://stackoverflow.com/questions/47022070/display-all-dataframe-columns-in-a-jupyter-python-notebook)

I want to show all columns in a dataframe in a Jupyter Notebook. Jupyter shows some of the columns and adds dots to the last columns like in the following picture:How can I display all columns? 

2017-10-30 18:34:36Z

I want to show all columns in a dataframe in a Jupyter Notebook. Jupyter shows some of the columns and adds dots to the last columns like in the following picture:How can I display all columns? Try the display max_columns setting as follows:OrEdit: Pandas 0.11.0 backwardsThis is deprecated but in versions of Pandas older than 0.11.0 the max_columns setting is specified as follows:I know this question is a little old but the following worked for me in a Jupyter Notebook running pandas 0.22.0 and Python 3:You can do the same for the rows too:This saves importing IPython, and there are more options in the pandas.set_option documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.htmlMaybe because I have an older version of pandas but on Jupyter notebook this work for meI recommend setting the display options inside a context manager so that it only affects a single output. If you also want to print a "pretty" html-version, I would define a function and display the dataframe df using force_show_all(df):As others have mentioned, be cautious to only call this on a reasonably-sized dataframe.

Python - manually install package using virtualenv

Jamison

[Python - manually install package using virtualenv](https://stackoverflow.com/questions/5979513/python-manually-install-package-using-virtualenv)

I have a python program I want to install into my virtualenv - it's a zip package that I need to unzip and then run a setup.py program - but my question is more regarding how to get these unzipped files into my virtualenv so that the package gets installed into the virtualenv's site-packages folder?I can also install from inside my virtualenv using pip install <package name>, but for some reason, the package that PIP downloads is out of date.So - can someone tell me a few easy steps for installing a package manually?So far I have the basic commands to load up the Virtualenv:So - does it matter where I unzip the python package/program to - or should I be logged in to the virtualenv first before unzipping? After I load up the virtualenv and I'm inside using it with the 'workon test' command, will any python package I install, regardless of the directory I find it, install itself into the proper virtualenv's site-packages folder?Option 1 is to unzip the python program into /home/username/tmp - then log into my virtualenv, navigate to that folder and run the setup.py program - assuming that the virtualenv will transfer all relevant files to it's own site-packages folder.OR scenario 2 is to unzip the files directly into site-packages, and run it from there (after logging in to the virtualenv), etcThank you for helping a Python clutz with this! 

2011-05-12 14:26:37Z

I have a python program I want to install into my virtualenv - it's a zip package that I need to unzip and then run a setup.py program - but my question is more regarding how to get these unzipped files into my virtualenv so that the package gets installed into the virtualenv's site-packages folder?I can also install from inside my virtualenv using pip install <package name>, but for some reason, the package that PIP downloads is out of date.So - can someone tell me a few easy steps for installing a package manually?So far I have the basic commands to load up the Virtualenv:So - does it matter where I unzip the python package/program to - or should I be logged in to the virtualenv first before unzipping? After I load up the virtualenv and I'm inside using it with the 'workon test' command, will any python package I install, regardless of the directory I find it, install itself into the proper virtualenv's site-packages folder?Option 1 is to unzip the python program into /home/username/tmp - then log into my virtualenv, navigate to that folder and run the setup.py program - assuming that the virtualenv will transfer all relevant files to it's own site-packages folder.OR scenario 2 is to unzip the files directly into site-packages, and run it from there (after logging in to the virtualenv), etcThank you for helping a Python clutz with this! I typically would extract the program to a temporary folder, then from that folder, run the setup.py using the direct path to the virtualenv python instance.  eg if your virtualenv is in /home/username/virtualpy, use this (from your temporary folder)This should install it to your virtualenv site package folder.well when you switch to the virtual environment. you should type which python and if it returns the path where your virtual environment exists then its okay you can directly run this command.but if it gives the global level path which is not your virtualenv's path then you should try usingIf a package won't install from repository, try under venv by use sudo.

As example for python pathos package;PACKAGE_DIR=/some/package/directory/path

export VENV=$(pipenv --venv) && export BASE_DIR=$(pwd) && cd $PACKAGE_DIR && $VENV/bin/python setup.py install && cd $BASE_DIR

imploding a list for use in a python MySQLDB IN clause

mluebke

[imploding a list for use in a python MySQLDB IN clause](https://stackoverflow.com/questions/589284/imploding-a-list-for-use-in-a-python-mysqldb-in-clause)

I know how to map a list to a string:And I know that I can use the following to get that string into an IN clause:What I need is to accomplish the same thing SAFELY (avoiding SQL injection) using MySQLDB. In the above example because foostring is not passed as an argument to execute, it is vulnerable. I also have to quote and escape outside of the mysql library. (There is a related SO question, but the answers listed there either do not work for MySQLDB or are vulnerable to SQL injection.)

2009-02-26 05:59:25Z

I know how to map a list to a string:And I know that I can use the following to get that string into an IN clause:What I need is to accomplish the same thing SAFELY (avoiding SQL injection) using MySQLDB. In the above example because foostring is not passed as an argument to execute, it is vulnerable. I also have to quote and escape outside of the mysql library. (There is a related SO question, but the answers listed there either do not work for MySQLDB or are vulnerable to SQL injection.)Use the list_of_ids directly:That way you avoid having to quote yourself, and avoid all kinds of sql injection.Note that the data (list_of_ids) is going directly to mysql's driver, as a parameter (not in the query text) so there is no injection. You can leave any chars you want in the string, no need to remove or quote chars.If you use Django 2.0 or 2.1 and Python 3.6, this is the right way:ref: https://stackoverflow.com/a/23891759/2803344

     https://docs.djangoproject.com/en/2.1/topics/db/sql/#passing-parameters-into-rawThough this question is quite old, thought it would be better to leave a response in case someone else was looking for what I wantedAccepted answer gets messy when we have a lot of the params or if we want to use named parametersAfter some trials Tested with python2.7, pymysql==0.7.11Another simple solution using list comprehension:Pain-less MySQLdb execute('...WHERE name1 = %s AND name2 IN (%s)', value1, values2)This could work for some use-cases if you don't wish to be concerned with the method in which you have to pass arguments to complete the query string and would like to invoke just cursror.execute(query).Another way could be: Very simple: Just use the below formationrules_id = ["9","10"]sql1 = "SELECT * FROM attendance_rules_staff WHERE id in("+", ".join(map(str, rules_id))+")"", ".join(map(str, rules_id)) 

python: is it possible to attach a console into a running process

Bin Chen

[python: is it possible to attach a console into a running process](https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process)

I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.It's better the process is running without being affected(of course performance can down a little bit)

2010-11-12 11:04:00Z

I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.It's better the process is running without being affected(of course performance can down a little bit)If you have access to the program's source-code, you can add this functionality relatively easily. See Recipe 576515: Debugging a running python process by interrupting and providing an interactive prompt (Python)To quote:Another implementation of roughly the same concept is provided by rconsole.  From the documentation:This will interrupt your process (unless you start it in a thread), but you can use the code module to start a Python console:This will block until the user exits the interactive console by executing exit().The code module is available in at least Python v2.6, probably others.I tend to use this approach in combination with signals for my Linux work (for Windows, see below). I slap this at the top of my Python scripts:And then trigger it from a shell with kill -SIGUSR2 <PID>, where <PID> is the process ID. The process then stops whatever it is doing and presents a console:Generally from there I'll load the server-side component of a remote debugger like the excellent WinPDB.Windows is not a POSIX-compliant OS, and so does not provide the same signals as Linux. However, Python v2.2 and above expose a Windows-specific signal SIGBREAK (triggered by pressing CTRL+Pause/Break). This does not interfere with normal CTRL+C (SIGINT) operation, and so is a handy alternative.Therefore a portable, but slightly ugly, version of the above is:Advantages of this approach:Here's the code I use in my production environment which will load the server-side of WinPDB (if available) and fall back to opening a Python console.Use pyrasite-shell. I can't believe it works so well, but it does. "Give it a pid, get a shell".This launches the python shell with access to the globals() and locals() variables of that running python process, and other wonderful things.Only tested this personally on Ubuntu but seems to cater for OSX too.Adapted from this answer.Note: The line switching off the ptrace_scope property is only necessary for kernels/systems that have been built with CONFIG_SECURITY_YAMA on. Take care messing with ptrace_scope in sensitive environments because it could introduce certain security vulnerabilities. See here for details.Why not simply using the pdb module? It allows you to stop a script, inspect elements values, and execute the code line by line. And since it is built upon the Python interpreter, it also provides the features provided by the classic interpreter. To use it, just put these 2 lines in your code, where you wish to stop and inspect it:Another possibility, without adding stuff to the python scripts, is described here:https://wiki.python.org/moin/DebuggingWithGdbUnfortunately, this solution also requires some forethought, at least to the extent that you need to be using a version of python with debugging symbols in it.Using PyCharm, I was getting a failure to connect to process in Ubuntu.  The fix for this is to disable YAMA.  For more info see askubuntu

pandas: complex filter on rows of DataFrame

duckworthd

[pandas: complex filter on rows of DataFrame](https://stackoverflow.com/questions/11418192/pandas-complex-filter-on-rows-of-dataframe)

I would like to filter rows by a function of each row, e.g.Or for another more complex, contrived example,How can I do so?

2012-07-10 16:56:37Z

I would like to filter rows by a function of each row, e.g.Or for another more complex, contrived example,How can I do so?You can do this using DataFrame.apply, which applies a function along a given axis,Suppose I had a DataFrame as follows:I can use sin and DataFrame.prod to create a boolean mask:Then use the mask to select from the DataFrame:Specify reduce=True to handle empty DataFrames as well.https://crosscompute.com/n/jAbsB6OIm6oCCJX9PBIbY5FECFKCClyV/-/apply-custom-filter-on-rows-of-dataframeI canot comment on duckworthd's answer, but it is not perfectly working. It crashes when the dataframe is empty: Outputs: To me it looks like a bug in pandas, since { } is definitively a valid set of boolean values. For a solution refer to Roy Hyunjin Han's answer.The best approach I've found is, instead of using reduce=True to avoid errors for empty df (since this arg is deprecated anyway), just check that df size > 0 before applying the filter:

How to config nltk data directory from code?

Juanjo Conti

[How to config nltk data directory from code?](https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code)

How to config nltk data directory from code?

2010-08-19 13:42:58Z

How to config nltk data directory from code?Just change items of nltk.data.path, it's a simple list.From the code, http://www.nltk.org/_modules/nltk/data.html: Then within the code:To modify the path, simply append to the list of possible paths:Or in windows:I use append, exampleInstead of adding nltk.data.path.append('your/path/to/nltk_data') to every script, NLTK accepts NLTK_DATA environment variable. (code link)Open ~/.bashrc (or ~/.profile) with text editor (e.g. nano, vim, gedit), and add following line:  Execute source to load environmental variable  Open python and execute following linesYour can see your nltk data path already in there.Reference: @alvations's answer on

nltk/nltk #1997 For those using uwsgi: I was having trouble because I wanted a uwsgi app (running as a different user than myself) to have access to nltk data that I had previously downloaded. What worked for me was adding the following line to myapp_uwsgi.ini:This sets the environment variable NLTK_DATA, as suggested by @schemacs.

You may need to restart your uwsgi process after making this change. Another solution is to get ahead of it. try 

    import nltk 

    nltk.download()  When the window box pops up asking if you want to download the corpus , you can specify there which directory it is to be downloaded to. 

list() uses slightly more memory than list comprehension

vishes_shell

[list() uses slightly more memory than list comprehension](https://stackoverflow.com/questions/40018398/list-uses-slightly-more-memory-than-list-comprehension)

So i was playing with list objects and found little strange thing that if list is created with list() it uses more memory, than list comprehension? I'm using Python 3.5.2From the docs:But it seems that using list() it uses more memory.And as much list is bigger, the gap increases.Why this happens?UPDATE #1Test with Python 3.6.0b2:UPDATE #2Test with Python 2.7.12:

2016-10-13 10:25:25Z

So i was playing with list objects and found little strange thing that if list is created with list() it uses more memory, than list comprehension? I'm using Python 3.5.2From the docs:But it seems that using list() it uses more memory.And as much list is bigger, the gap increases.Why this happens?UPDATE #1Test with Python 3.6.0b2:UPDATE #2Test with Python 2.7.12:I think you're seeing over-allocation patterns this is a sample from the source:Printing the sizes of list comprehensions of lengths 0-88 you can see the pattern matches:Results (format is (list length, (old total size, new total size))):The over-allocation is done for performance reasons allowing lists to grow without allocating more memory with every growth (better amortized performance).A probable reason for the difference with using list comprehension, is that list comprehension can not deterministically calculate the size of the generated list, but list() can. This means comprehensions will continuously grow the list as it fills it using over-allocation until finally filling it.It is possible that is will not grow the over-allocation buffer with unused allocated nodes once its done (in fact, in most cases it wont, that would defeat the over-allocation purpose).list(), however, can add some buffer no matter the list size since it knows the final list size in advance.Another backing evidence, also from the source, is that we see list comprehensions invoking LIST_APPEND, which indicates usage of list.resize, which in turn indicates consuming the pre-allocation buffer without knowing how much of it will be filled. This is consistent with the behavior you're seeing.To conclude, list() will pre-allocate more nodes as a function of the list sizeList comprehension does not know the list size so it uses append operations as it grows, depleting the pre-allocation buffer:Thanks everyone for helping me to understand that awesome Python.I don't want to make question that massive(that why i'm posting answer), just want to show and share my thoughts.As @ReutSharabani noted correctly: "list() deterministically determines list size". You can see it from that graph.When you append or using list comprehension you always have some sort of boundaries that extends when you reach some point. And with list() you have almost the same boundaries, but they are floating.UPDATESo thanks to @ReutSharabani, @tavo, @SvenFestersenTo sum up: list() preallocates memory depend on list size, list comprehension cannot do that(it requests more memory when it needed, like .append()). That's why list() store more memory.One more graph, that show list() preallocate memory. So green line shows list(range(830)) appending element by element and for a while memory not changing.UPDATE 2As @Barmar noted in comments below, list() must me faster than list comprehension, so i ran timeit() with number=1000 for length of list from 4**0 to 4**10 and the results are

Memory error when using pandas read_csv

Anne

[Memory error when using pandas read_csv](https://stackoverflow.com/questions/17557074/memory-error-when-using-pandas-read-csv)

I am trying to do something fairly simple, reading a large csv file into a pandas dataframe.The code either fails with a MemoryError, or just never finishes.Mem usage in the task manager stopped at 506 Mb and after 5 minutes of no change and no CPU activity in the process I stopped it.I am using pandas version 0.11.0.I am aware that there used to be a memory problem with the file parser, but according to http://wesmckinney.com/blog/?p=543 this should have been fixed.The file I am trying to read is 366 Mb, the code above works if I cut the file down to something short (25 Mb).It has also happened that I get a pop up telling me that it can't write to address 0x1e0baf93... Stacktrace:A bit of background - I am trying to convince people that Python can do the same as R. For this I am trying to replicate an R script that doesR not only manages to read the above file just fine, it even reads several of these files in a for loop (and then does some stuff with the data). If Python does have a problem with files of that size I might be fighting a loosing battle...

2013-07-09 19:57:49Z

I am trying to do something fairly simple, reading a large csv file into a pandas dataframe.The code either fails with a MemoryError, or just never finishes.Mem usage in the task manager stopped at 506 Mb and after 5 minutes of no change and no CPU activity in the process I stopped it.I am using pandas version 0.11.0.I am aware that there used to be a memory problem with the file parser, but according to http://wesmckinney.com/blog/?p=543 this should have been fixed.The file I am trying to read is 366 Mb, the code above works if I cut the file down to something short (25 Mb).It has also happened that I get a pop up telling me that it can't write to address 0x1e0baf93... Stacktrace:A bit of background - I am trying to convince people that Python can do the same as R. For this I am trying to replicate an R script that doesR not only manages to read the above file just fine, it even reads several of these files in a for loop (and then does some stuff with the data). If Python does have a problem with files of that size I might be fighting a loosing battle...Memory errors happens a lot with python when using the 32bit version in Windows. This is because 32bit processes only gets 2GB of memory to play with by default.If you are not using 32bit python in windows but are looking to improve on your memory efficiency while reading csv files, there is a trick.The pandas.read_csv function takes an option called dtype. This lets pandas know what types exist inside your csv data.By default, pandas will try to guess what dtypes your csv file has. This is a very heavy operation because while it is determining the dtype, it has to keep all raw data as objects (strings) in memory.Let's say your csv looks like this:This example is of course no problem to read into memory, but it's just an example.If pandas were to read the above csv file without any dtype option, the age would be stored as strings in memory until pandas has read enough lines of the csv file to make a qualified guess.I think the default in pandas is to read 1,000,000 rows before guessing the dtype.By specifying dtype={'age':int} as an option to the .read_csv() will let pandas know that age should be interpreted as a number. This saves you lots of memory.However, if your csv file would be corrupted, like this:Then specifying dtype={'age':int} will break the .read_csv() command, because it cannot cast "40+" to int. So sanitize your data carefully!Here you can see how the memory usage of a pandas dataframe is a lot higher when floats are kept as strings:I had the same memory problem with a simple read of a tab delimited text file around 1 GB in size (over 5.5 million records) and this solved the memory problem：Spyder 3.2.3

Python 2.7.13 64bitsI use Pandas on my Linux box and faced many memory leaks that only got resolved after upgrading Pandas to the latest version after cloning it from github.There is no error for Pandas 0.12.0 and NumPy 1.8.0.I have managed to create a big DataFrame and save it to a csv file and then successfully read it. Please see the example here. The size of the file is 554 Mb (It even worked for 1.1 Gb file, took longer, to generate 1.1Gb file use frequency of 30 seconds). Though I have 4Gb of RAM available. My suggestion is try updating Pandas. Other thing that could be useful is try running your script from command line, because for R you are not using Visual Studio (this already was suggested in the comments to your question), hence it has more resources available. I encountered this issue as well when I was running in a virtual machine, or somewere else where the memory is stricktly limited. It has nothing to do with pandas or numpy or csv, but will always happen if you try using more memory as you are alowed to use, not even only in python. The only chance you have is what you already tried, try to chomp down the big thing into smaller pieces which fit into memory. If you ever asked yourself what MapReduce is all about, you found out by yourself...MapReduce would try to distribute the chunks over many machines, you would try to process the chunke on one machine one after another.What you found out with the concatenation of the chunk files might be an issue indeed, maybe there are some copy needed in this operation...but in the end this maybe saves you in your current situation but if your csv gets a little bit larger you might run against that wall again...It also could be, that pandas is so smart, that it actually only loads the individual data chunks into memory if you do something with it, like concatenating to a big df?Several things you can try:I tried chunksize while reading big CSV fileThe read is now the list. We can iterate the reader and write/append to the new csv or can perform any operationAlthough this is a workaround not so much as a fix, I'd try converting that CSV to JSON (should be trivial) and using read_json method instead - I've been writing and reading  sizable JSON/dataframes (100s of MB) in Pandas this way without any problem at all.

how to tell a variable is iterable but not a string

priestc

[how to tell a variable is iterable but not a string](https://stackoverflow.com/questions/1055360/how-to-tell-a-variable-is-iterable-but-not-a-string)

I have a function that take an argument which can be either a single item or a double item:so that:The problem is that string is technically iterable, so I can't just catch the ValueError when trying arg[1]. I don't want to use isinstance(), because that's not good practice (or so I'm told).

2009-06-28 17:45:28Z

I have a function that take an argument which can be either a single item or a double item:so that:The problem is that string is technically iterable, so I can't just catch the ValueError when trying arg[1]. I don't want to use isinstance(), because that's not good practice (or so I'm told).Use isinstance (I don't see why it's bad practice)Note the use of StringTypes.  It ensures that we don't forget about some obscure type of string.On the upside, this also works for derived string classes.Also, you might want to have a look at this previous question.Cheers.NB: behavior changed in Python 3 as StringTypes and basestring are no longer defined. Depending on your needs, you can replace them in isinstance by str, or a subset tuple of (str, bytes, unicode), e.g. for Cython users.

As @Theron Luhn mentionned, you can also use six.As of 2017, here is a portable solution that works with all versions of Python:Since Python 2.6, with the introduction of abstract base classes, isinstance (used on ABCs, not concrete classes) is now considered perfectly acceptable.  Specifically:This is an exact copy (changing only the class name) of Iterable as defined in _abcoll.py (an implementation detail of collections.py)... the reason this works as you wish, while collections.Iterable doesn't, is that the latter goes the extra mile to ensure strings are considered iterable, by calling Iterable.register(str) explicitly just after this class statement.Of course it's easy to augment __subclasshook__ by returning False before the any call for other classes you want to specifically exclude from your definition.In any case, after you have imported this new module as myiter, isinstance('ciao', myiter.NonStringIterable) will be False, and isinstance([1,2,3], myiter.NonStringIterable)will be True, just as you request -- and in Python 2.6 and later this is considered the proper way to embody such checks... define an abstract base class and check isinstance on it.I realise this is an old post but thought it was worth adding my approach for Internet posterity. The function below seems to work for me under most circumstances with both Python 2 and 3:This checks for a non-string iterable by (mis)using the built-in hasattr which will raise a TypeError when its second argument is not a string or unicode string.By combining previous replies, I'm using:Not 100% fools proof, but if an object is not an iterable you still can let it pass and fall back to duck typing.Edit: Python3types.StringTypes == (str, unicode). The Phython3 equivalent is:huh, don't get it... what's wrong with going ?... NB elgehelge puts this in a comment here, saying "look at my more detailed answer" but I couldn't find his/her detailed answerlaterIn view of David Charles' comment about Python3, what about:?  Apparently "basestring" is no longer a type in Python3:https://docs.python.org/3.0/whatsnew/3.0.htmlAs you point out correctly, a single string is a character sequence.So the thing you really want to do is to find out what kind of sequence arg is by using isinstance or type(a)==str.If you want to realize a function that takes a variable amount of parameters, you should do it like this:function("ff") and function("ff", "ff") will work.I can't see a scenario where an isiterable() function like yours is needed. It isn't isinstance() that is bad style but situations where you need to use isinstance().

Python class definition syntax

Falmarri

[Python class definition syntax](https://stackoverflow.com/questions/4109552/python-class-definition-syntax)

Is there a difference betweenandI just realized that a couple of my classes are defined as the former and they work just fine. Do the empty parenthesis make any difference?

2010-11-05 19:59:26Z

Is there a difference betweenandI just realized that a couple of my classes are defined as the former and they work just fine. Do the empty parenthesis make any difference?The latter is a syntax error on older versions of Python. In Python 2.x you should derive from object whenever possible though, since several useful features are only available with new-style classes (deriving from object is optional in Python 3.x, since new-style classes are the default there).While it might not be syntactically incorrect to use the empty parentheses in a class definition, parentheses after a class definition are used to indicate inheritance, e.g:In Python, the preferred syntax for a class declaration without any base classes is simply:Don't use parentheses unless you are subclassing other classes.The docs on the matter should give you a better understanding of how to declare and use classes in Python.

requirements.txt vs setup.py

lucy

[requirements.txt vs setup.py](https://stackoverflow.com/questions/43658870/requirements-txt-vs-setup-py)

I started working with Python. I've added requirements.txt and setup.py to my project. But, I am still confused about the purpose of both files. I have read that setup.py is designed for redistributable things and that requirements.txt is designed for non-redistributable things.  But I am not certain this is accurate.How are those two files truly intended to be used?

2017-04-27 13:10:30Z

I started working with Python. I've added requirements.txt and setup.py to my project. But, I am still confused about the purpose of both files. I have read that setup.py is designed for redistributable things and that requirements.txt is designed for non-redistributable things.  But I am not certain this is accurate.How are those two files truly intended to be used?requirements.txtThis helps you to set up your development environment. Programs like pip can be used to install all packages listed in the file in one fell swoop. After that you can start developing your python script. Especially useful if you plan to have others contribute to the development or use virtual environments.

This is how you use it:setup.pyThis allows you to create packages that you can redistribute. This script is meant to install your package on the end user's system, not to prepare the development environment as pip install -r requirements.txt does.

See this answer for more details on setup.py.The dependencies of your project are listed in both files.The short answer is that requirements.txt is for listing package requirements only. setup.py on the other hand is more like an installation script. If you don't plan on installing the python code, typically you would only need requirements.txt. The file setup.py describes, in addition to the package dependencies, the set of files and modules that should be packaged (or compiled, in the case of native modules (i.e., written in C)), and metadata to add to the python package listings (e.g. package name, package version, package description, author, ...).Because both files list dependencies, this can lead to a bit of duplication. Read below for details.requirements.txtThis file lists python package requirements. It is a plain text file (optionally with comments) that lists the package dependencies of your python project (one per line). It does not describe the way in which your python package is installed. You would generally consume the requirements file with pip install -r requirements.txt.The filename of the text file is arbitrary, but is often requirements.txt by convention. When exploring source code repositories of other python packages, you might stumble on other names, such as dev-dependencies.txt or dependencies-dev.txt. Those serve the same purpose as dependencies.txt but generally list additional dependencies of interest to developers of the particular package, namely for testing the source code (e.g. pytest, pylint, etc.) before release. Users of the package generally wouldn't need the entire set of developer dependencies to run the package. If multiplerequirements-X.txt variants are present, then usually one will list runtime dependencies, and the other build-time, or test dependencies. Some projects also cascade their requirements file, i.e. when one requirements file includes another file (example). Doing so can reduce repetition.setup.pyThis is a python script which uses the setuptools module to define a python package (name, files included, package metadata, and installation). It will, like requirements.txt, also list runtime dependencies of the package. Setuptools is the de-facto way to build and install python packages, but it has its shortcomings, which over time have sprouted the development of new "meta-package managers", like pip. Example shortcomings of setuptools are its inability to install multiple versions of the same package, and lack of an uninstall command.When a python user does pip install ./pkgdir_my_module (or pip install my-module), pip will run setup.py in the given directory (or module). Similarly, any module which has a setup.py can be pip-installed, e.g. by running pip install . from the same folder.Do I really need both?Short answer is no, but it's nice to have both. They achieve different purposes, but they can both be used to list your dependencies.There is one trick you may consider to avoid duplicating your list of dependencies between requirements.txt and setup.py. If you have written a fully working setup.py for your package already, and your dependencies are mostly external, you could consider having a simple requirements.txt with only the following:The -e is a special pip install option which installs the given package in editable mode. When pip -r requirements.txt is run on this file, pip will install your dependencies via the list in ./setup.py. The editable option will place a symlink in your install directory (instead of an egg or archived copy). It allows developers to edit code in place from the repository without reinstalling.You can also take advantage of what's called "setuptools extras" when you have both files in your package repository. You can define optional packages in setup.py under a custom category, and install those packages from just that category with pip:and then, in the requirements file:This would keep all your dependency lists inside setup.py. Note: You would normally execute pip and setup.py from a sandbox, such as those created with the program virtualenv. This will avoid installing python packages outside the context of your project's development environment.For the sake of completeness, here is how I see it in 3 different angles.This is the precise description quoted from the official documentation (emphasis mine):But it might still not easy to be understood, so in next section, there come 2 factual examples to demonstrate how the 2 approaches are supposed to be used, differently.

Interactively validating Entry widget content in tkinter

Malcolm

[Interactively validating Entry widget content in tkinter](https://stackoverflow.com/questions/4140437/interactively-validating-entry-widget-content-in-tkinter)

What is the recommended technique for interactively validating content in a tkinter Entry widget?I've read the posts about using validate=True and validatecommand=command, and it appears that these features are limited by the fact that they get cleared if the validatecommand command updates the Entry widget's value.Given this behavior, should we bind on the KeyPress, Cut, and Paste events and monitor/update our Entry widget's value through these events? (And other related events that I might have missed?)Or should we forget interactive validation altogether and only validate on FocusOut events?

2010-11-10 01:24:48Z

What is the recommended technique for interactively validating content in a tkinter Entry widget?I've read the posts about using validate=True and validatecommand=command, and it appears that these features are limited by the fact that they get cleared if the validatecommand command updates the Entry widget's value.Given this behavior, should we bind on the KeyPress, Cut, and Paste events and monitor/update our Entry widget's value through these events? (And other related events that I might have missed?)Or should we forget interactive validation altogether and only validate on FocusOut events?The correct answer is, use the validatecommand attribute of the widget. Unfortunately this feature is severely under-documented in the Tkinter world, though it is quite sufficiently documented in the Tk world. Even though it's not documented well, it has everything you need to do validation without resorting to bindings or tracing variables, or modifying the widget from within the validation procedure.The trick is to know that you can have Tkinter pass in special values to your validate command. These values give you all the information you need to know to decide on whether the data is valid or not: the value prior to the edit, the value after the edit if the edit is valid, and several other bits of information. To use these, though, you need to do a little voodoo to get this information passed to your validate command.Note: it's important that the validation command returns either True or False. Anything else will cause the validation to be turned off for the widget.Here's an example that only allows lowercase (and prints all those funky values):For more information about what happens under the hood when you call the register method, see Input validation tkinterAfter studying and experimenting with Bryan's code, I produced a minimal version of input validation.  The following code will put up an Entry box and only accept numeric digits.Perhaps I should add that I am still learning Python and I will gladly accept any and all comments/suggestions.Use a Tkinter.StringVar to track the value of the Entry widget.  You can validate the value of the StringVar by setting a trace on it.Here's a short working program that accepts only valid floats in the Entry widget.While studying Bryan Oakley's answer, something told me that a far more general solution could be developed. The following example introduces a mode enumeration, a type dictionary, and a setup function for validation purposes. See line 48 for example usage and a demonstration of its simplicity.Bryan's answer is correct, however no one mentioned the 'invalidcommand' attribute of the tkinter widget.A good explanation is here:

http://infohost.nmt.edu/tcc/help/pubs/tkinter/web/entry-validation.htmlText copy/pasted in case of broken linkThe Entry widget also supports an invalidcommand option that specifies a callback function that is called whenever the validatecommand returns False. This command may modify the text in the widget by using the .set() method on the widget's associated textvariable. Setting up this option works the same as setting up the validatecommand. You must use the .register() method to wrap your Python function; this method returns the name of the wrapped function as a string. Then you will pass as the value of the invalidcommand option either that string, or as the first element of a tuple containing substitution codes.Note:

There is only one thing that I cannot figure out how to do: If you add validation to an entry, and the user selects a portion of the text and types a new value, there is no way to capture the original value and reset the entry. Here's an exampleHere is a simple way to validate the entry value, which allows user to enter digits only:PS: This example can be very useful for creating an app like calc.Responding to orionrobert's problem of dealing with simple validation upon substitutions of text through selection, instead of separate deletions or insertions:A substitution of selected text is processed as a deletion followed by an insertion. This may lead to problems, for example, when the deletion should move the cursor to the left, while a substitution should move the cursor to the right. Fortunately, these two processes are executed immediately after one another.

Hence, we can differentiate between a deletion by itself and a deletion directly followed by an insertion due to a substitution because the latter has no idle time between deletion and insertion. This is exploited using a substitutionFlag and a Widget.after_idle().

after_idle() executes the lambda-function at the end of the event queue:Of course, after a substitution, while validating the deletion part, one still won’t know whether an insert will follow.

Luckily however, with:

.set(), 

.icursor(), 

.index(SEL_FIRST), 

.index(SEL_LAST), 

.index(INSERT), 

we can achieve most desired behavior retrospectively (since the combination of our new substitutionFlag with an insertion is a new unique and final event.

Printing list elements on separated lines in Python

Larry

[Printing list elements on separated lines in Python](https://stackoverflow.com/questions/6167731/printing-list-elements-on-separated-lines-in-python)

I am trying to print out Python path folders using this:The output is like this:How do I print them into separate lines so I can parse them properly?It should be like this:

2011-05-29 12:32:10Z

I am trying to print out Python path folders using this:The output is like this:How do I print them into separate lines so I can parse them properly?It should be like this:(The outer parentheses are included for Python 3 compatibility and are usually omitted in Python 2.)Use the print function (Python 3.x) or import it (Python 2.6+):Another good option for handling this kind of option is the pprint module, which (among other things) pretty prints long lists with one element per line:Sven Marnach's answer is pretty much it, but has one generality issue... It will fail if the list being printed doesn't just contain strings.So, the more general answer to "How to print out a list with elements separated by newlines"...Then again, the print function approach JBernardo points out is superior.  If you can, using the print function instead of the print statement is almost always a good idea.Use the splat operator(*)By default, * operator prints list separated by space. Use sep operator to specify the  delimitersys.path returns the list of pathsrefA list of strings that specifies the search path for modules. Initialized from the environment variable PYTHONPATH, plus an installation-dependent default.As initialized upon program startup, the first item of this list, path[0], is the directory containing the script that was used to invoke the Python interpreter. If the script directory is not available (e.g. if the interpreter is invoked interactively or if the script is read from standard input), path[0] is the empty string, which directs Python to search modules in the current directory first. Notice that the script directory is inserted before the entries inserted as a result of PYTHONPATH.or you can print only first path byprint(dir[0])

Shortest Sudoku Solver in Python - How does it work?

Ville Salonen

[Shortest Sudoku Solver in Python - How does it work?](https://stackoverflow.com/questions/201461/shortest-sudoku-solver-in-python-how-does-it-work)

I was playing around with my own Sudoku solver and was looking for some pointers to good and fast design when I came across this:My own implementation solves Sudokus the same way I solve them in my head but how does this cryptic algorithm work?http://scottkirkwood.blogspot.com/2006/07/shortest-sudoku-solver-in-python.html

2008-10-14 14:46:15Z

I was playing around with my own Sudoku solver and was looking for some pointers to good and fast design when I came across this:My own implementation solves Sudokus the same way I solve them in my head but how does this cryptic algorithm work?http://scottkirkwood.blogspot.com/2006/07/shortest-sudoku-solver-in-python.htmlWell, you can make things a little easier by fixing up the syntax:Cleaning up a little:Okay, so this script expects a command-line argument, and calls the function r on it.  If there are no zeros in that string, r exits and prints out its argument.  I guess this means that zeros correspond to open spaces, and a puzzle with no zeros is solved.  Then there's that nasty recursive expression.The loop is interesting: for m in'%d'%5**18Why 5**18? It turns out that '%d'%5**18 evaluates to '3814697265625'.  This is a string that has each digit 1-9 at least once, so maybe it's trying to place each of them.  In fact, it looks like this is what r(a[:i]+m+a[i+1:]) is doing: recursively calling r, with the first blank filled in by a digit from that string.  But this only happens if the earlier expression is false.  Let's look at that:m in [(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3) or a[j] for j in range(81)]So the placement is done only if m is not in that monster list.  Each element is either a number (if the first expression is nonzero) or a character (if the first expression is zero).  m is ruled out as a possible substitution if it appears as a character, which can only happen if the first expression is zero.  When is the expression zero?It has three parts that are multiplied:If any of these three parts is zero, the entire expression is zero.  In other words, if i and j share a row, column, or 3x3 block, then the value of j can't be used as a candidate for the blank at i.  Aha!Note that if none of the placements work out, r will return and back up to the point where something else can be chosen, so it's a basic depth first algorithm.Not using any heuristics, it's not particularly efficient.  I took this puzzle from Wikipedia (http://en.wikipedia.org/wiki/Sudoku):Addendum: How I would rewrite it as a maintenance programmer (this version has about a 93x speedup :)unobfuscating it:So, we just need to work out the inner list expression. I know it collects the digits set in the line -- otherwise, the code around it makes no sense. However, I have no real clue how it does that (and Im too tired to work out that binary fancyness right now, sorry)r(a) is a recursive function which attempts to fill in a 0 in the board in each step.i=a.find('0');~i or exit(a) is the on-success termination.  If no more 0 values exist in the board, we're done.m is the current value we will try to fill the 0 with.m

in[(i-j)%9*(i/9^j/9)*(i/27^j/27|i%9/3^j%9/3)or a[j]for

j in range(81)] evaluates to truthy if it is obivously incorrect to put m in the current 0.  Let's nickname it "is_bad".  This is the most tricky bit. :)is_bad or r(a[:i]+m+a[i+1:] is a conditional recursive step.  It will recursively try to evaluate the next 0  in the board iff the current solution candidate appears to be sane.for m in '%d'%5**18 enumerates all the numbers from 1 to 9 (inefficiently).A lot of the short sudoku solvers just recursively try every possible legal number left until they have successfully filled the cells.  I haven't taken this apart, but just skimming it, it looks like that's what it does.The code doesn't actually work. You can test it yourself. Here is a sample unsolved sudoku puzzle:807000003602080000000200900040005001000798000200100070004003000000040108300000506You can use this website (http://www.sudokuwiki.org/sudoku.htm), click on import puzzle and simply copy the above string. The output of the python program is:

817311213622482322131224934443535441555798655266156777774663869988847188399979596which does not correspond to the solution. In fact you can already see a contradiction, two 1s in the first row. 

Sorting related items in a Django template

Dale Wilson

[Sorting related items in a Django template](https://stackoverflow.com/questions/6540032/sorting-related-items-in-a-django-template)

Is it possible to sort a set of related items in a DJango template?That is: this code (with HTML tags omitted for clarity):displays almost exactly want I want.  The only thing I want to change is I the list of attendees to be sorted by last name.  I've tried saying something like this:Alas, the above syntax doesn't work (it produces an empty list) and neither does any other variation I have thought of (lot's of syntax errors reported, but no joy).   I could, of course, produce some kind of array of sorted attendee lists in my view, but that is an ugly and fragile (and did I mention ugly) solution.Needless to say, but I'll say it anyway, I have perused the on-line docs and searched Stack Overflow and the archives of django-user without finding anything helpful (ah, if only a query set were a dictionary dictsort would do the job, but it's not and it doesn't)==============================================Edited to add additional thoughts

after accepting Tawmas's answer.Tawmas addressed the issue exactly as I presented it -- although the solution was not what I expected.  As a result I learned a useful technique that can be used in other situations as well. Tom's answer proposed an approach I had already mentioned in my OP and tentatively rejected as being "ugly".  The "ugly" was a gut reaction, and I wanted to clarify what was wrong with it.  In doing so I realized that the reason it was an ugly approach was because I was hung up on the idea of passing a query set to the template to be rendered.   If I relax that requirement, there is an un-ugly approach that should work.  I haven't tried this yet, but suppose that rather than passing the queryset, the view code iterated through the query set producing a list of Events, then decorated each Event with a query set for the corresponding attendees which WAS sorted (or filtered, or whatever) in the desired way.   Something like so:Now the template becomes:The downside is the view has to "actualize" all of the events at once which could be a problem if there were large numbers of events.  Of course one could add pagination, but that complicates the view considerably.  The upside is the "prepare the data to be displayed" code is in the view where it belongs letting the template focus on formatting the data provided by the view for display. This is right and proper.So my plan is to use Tawmas' technique for large tables and the above technique for small

tables, with the definition of large and small left to the reader (grin.)

2011-06-30 19:25:39Z

Is it possible to sort a set of related items in a DJango template?That is: this code (with HTML tags omitted for clarity):displays almost exactly want I want.  The only thing I want to change is I the list of attendees to be sorted by last name.  I've tried saying something like this:Alas, the above syntax doesn't work (it produces an empty list) and neither does any other variation I have thought of (lot's of syntax errors reported, but no joy).   I could, of course, produce some kind of array of sorted attendee lists in my view, but that is an ugly and fragile (and did I mention ugly) solution.Needless to say, but I'll say it anyway, I have perused the on-line docs and searched Stack Overflow and the archives of django-user without finding anything helpful (ah, if only a query set were a dictionary dictsort would do the job, but it's not and it doesn't)==============================================Edited to add additional thoughts

after accepting Tawmas's answer.Tawmas addressed the issue exactly as I presented it -- although the solution was not what I expected.  As a result I learned a useful technique that can be used in other situations as well. Tom's answer proposed an approach I had already mentioned in my OP and tentatively rejected as being "ugly".  The "ugly" was a gut reaction, and I wanted to clarify what was wrong with it.  In doing so I realized that the reason it was an ugly approach was because I was hung up on the idea of passing a query set to the template to be rendered.   If I relax that requirement, there is an un-ugly approach that should work.  I haven't tried this yet, but suppose that rather than passing the queryset, the view code iterated through the query set producing a list of Events, then decorated each Event with a query set for the corresponding attendees which WAS sorted (or filtered, or whatever) in the desired way.   Something like so:Now the template becomes:The downside is the view has to "actualize" all of the events at once which could be a problem if there were large numbers of events.  Of course one could add pagination, but that complicates the view considerably.  The upside is the "prepare the data to be displayed" code is in the view where it belongs letting the template focus on formatting the data provided by the view for display. This is right and proper.So my plan is to use Tawmas' technique for large tables and the above technique for small

tables, with the definition of large and small left to the reader (grin.)You need to specify the ordering in the attendee model, like this. For example (assuming your model class is named Attendee):See the manual for further reference.EDIT. Another solution is to add a property to your Event model, that you can access from your template:You could define more of these as you need them...You can use template filter dictsort https://docs.djangoproject.com/en/dev/ref/templates/builtins/#std:templatefilter-dictsortThis should work:One solution is to make a custom templatag:use like this:regroup should be able to do what you want, but is there a reason you can't order them the way you want back in the view?

How to find the cumulative sum of numbers in a list?

user2259323

[How to find the cumulative sum of numbers in a list?](https://stackoverflow.com/questions/15889131/how-to-find-the-cumulative-sum-of-numbers-in-a-list)

I want to sum up the numbers like [4, 4+6, 4+6+12] in order to get the list t = [4, 10, 22].I tried the following:

2013-04-08 21:15:02Z

I want to sum up the numbers like [4, 4+6, 4+6+12] in order to get the list t = [4, 10, 22].I tried the following:If you're doing much numerical work with arrays like this, I'd suggest numpy, which comes with a cumulative sum function cumsum:Numpy is often faster than pure python for this kind of thing, see in comparison to @Ashwini's accumu:But of course if it's the only place you'll use numpy, it might not be worth having a dependence on it.In Python 2 you can define your own generator function like this:And in Python 3.2+ you can use itertools.accumulate():Behold:Will output (as expected):I did a bench-mark of the top two answers with Python 3.4 and I found itertools.accumulate is faster than numpy.cumsum under many circumstances, often much faster. However, as you can see from the comments, this may not always be the case, and it's difficult to exhaustively explore all options. (Feel free to add a comment or edit this post if you have further benchmark results of interest.)Some timings...For short lists accumulate is about 4 times faster:For longer lists accumulate is about 3 times faster:If the numpy array is not cast to list, accumulate is still about 2 times faster:If you put the imports outside of the two functions and still return a numpy array, accumulate is still nearly 2 times faster:Try this:

 accumulate function, along with operator add performs the running addition.Assignment expressions from PEP 572 (expected for Python 3.8) offer yet another way to solve this:You can calculate the cumulative sum list in linear time with a simple for loop:The standard library's itertools.accumulate may be a faster alternative (since it's implemented in C):If You want a pythonic way without numpy working in 2.7 this would be my way of doing itnow let's try it and test it against all other implementationsRunning this code givesFirst, you want a running list of subsequences:Then you just call sum on each subsequence:(This isn't the most efficient way to do it, because you're adding all of the prefixes repeatedly. But that probably won't matter for most use cases, and it's easier to understand if you don't have to think of the running totals.)If you're using Python 3.2 or newer, you can use itertools.accumulate to do it for you:And if you're using 3.1 or earlier, you can just copy the "equivalent to" source straight out of the docs (except for changing next(it) to it.next() for 2.5 and earlier).In Python3, To find the cumulative sum of a list where the ith element

is the sum of the first i+1 elements from the original list, you may do:OR you may use list comprehension:Output Try this:This is slighlty faster than the generator method above by @Ashwini for small listsFor larger lists, the generator is the way to go for sure. . . Somewhat hacky, but seems to work:I did think that the inner function would be able to modify the y declared in the outer lexical scope, but that didn't work, so we play some nasty hacks with structure modification instead. It is probably more elegant to use a generator.Without having to use Numpy, you can loop directly over the array and accumulate the sum along the way. For example:Results in: A pure python oneliner for cumulative sum: This is a recursive version inspired by recursive cumulative sums. Some explanations:Test:And simular for cumulative product:Test:If you are looking for a more efficient solution (bigger lists?) a generator could be a good call (or just use numpy if you really care about perf).This would be Haskell-style:

What is the difference between contiguous and non-contiguous arrays?

jdeng

[What is the difference between contiguous and non-contiguous arrays?](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays)

In the numpy manual about the reshape() function, it says My questions are:Thanks for your answer!

2014-11-18 15:45:08Z

In the numpy manual about the reshape() function, it says My questions are:Thanks for your answer!A contiguous array is just an array stored in an unbroken block of memory: to access the next value in the array, we just move to the next memory address.Consider the 2D array arr = np.arange(12).reshape(3,4). It looks like this:In the computer's memory, the values of arr are stored like this:This means arr is a C contiguous array because the rows are stored as contiguous blocks of memory. The next memory address holds the next row value on that row. If we want to move down a column, we just need to jump over three blocks (e.g. to jump from 0 to 4 means we skip over 1,2 and 3).Transposing the array with arr.T means that C contiguity is lost because adjacent row entries are no longer in adjacent memory addresses. However, arr.T is Fortran contiguous since the columns are in contiguous blocks of memory:Performance-wise, accessing memory addresses which are next to each other is very often faster than accessing addresses which are more "spread out" (fetching a value from RAM could entail a number of neighbouring addresses being fetched and cached for the CPU.) This means that operations over contiguous arrays will often be quicker.As a consequence of C contiguous memory layout, row-wise operations are usually faster than column-wise operations. For example, you'll typically find that is slightly faster than:Similarly, operations on columns will be slightly faster for Fortran contiguous arrays.Finally, why can't we flatten the Fortran contiguous array by assigning a new shape?In order for this to be possible NumPy would have to put the rows of arr.T together like this:(Setting the shape attribute directly assumes C order - i.e. NumPy tries to perform the operation row-wise.)This is impossible to do. For any axis, NumPy needs to have a constant stride length (the number of bytes to move) to get to the next element of the array. Flattening arr.T in this way would require skipping forwards and backwards in memory to retrieve consecutive values of the array.If we wrote arr2.reshape(12) instead, NumPy would copy the values of arr2 into a new block of memory (since it can't return a view on to the original data for this shape).Maybe this example with 12 different array values will help:The C order values are in the order that they were generated in.  The transposed ones are notYou can get 1d views of boththe shape of x can also be changed.But the shape of the transpose cannot be changed.  The data is still in the 0,1,2,3,4... order, which can't be accessed accessed as 0,4,8... in a 1d array.But a copy of x1 can be changed:Looking at strides might also help.  A strides is how far (in bytes) it has to step to get to the next value.  For a 2d array, there will be be 2 stride values:To get to the next row, step 16 bytes, next column only 4.Transpose just switches the order of the strides.  The next row is only 4 bytes- i.e. the next number.    Changing the shape also changes the strides - just step through the buffer 4 bytes at a time.Even though x2 looks just like x1, it has its own data buffer, with the values in a different order.  The next column is now 4 bytes over, while the next row is 12 (3*4).And as with x, changing the shape to 1d reduces the strides to (4,).  For x1, with data in the 0,1,2,... order, there isn't a 1d stride that would give 0,4,8....__array_interface__ is another useful way of displaying array information:The x1 data buffer address will be same as for x, with which it shares the data.  x2 has a different buffer address.You could also experiment with adding a order='F' parameter to the copy and reshape commands.

How can I check if a date is the same day as datetime.today()?

madprops

[How can I check if a date is the same day as datetime.today()?](https://stackoverflow.com/questions/6407362/how-can-i-check-if-a-date-is-the-same-day-as-datetime-today)

This condition always evaluates to True even if it's the same day because it is comparing time.How can I check if a date is the same day as datetime.today()?

2011-06-20 06:01:25Z

This condition always evaluates to True even if it's the same day because it is comparing time.How can I check if a date is the same day as datetime.today()?If you want to just compare dates,Or, obviously,If you want to check that they're the same date.The documentation is usually helpful. It is also usually the first google result for python thing_i_have_a_question_about. Unless your question is about a function/module named "snake".Basically, the datetime module has three types for storing a point in time:Note that timedelta has the following format:So you are able to check diff in days, seconds, msec, minutes and so on depending on what you really need:In your case when you need to check that two dates are exactly the same you can use timedelta(0):You can set the hours, minutes, seconds and microseconds to whatever you likebut trutheality's answer is probably best when they are all to be zero and you can just compare the .date()s of the timesMaybe it is faster though if you have to compare hundreds of datetimes because you only need to do the replace() once vs hundreds of calls to date()One should compare using .date(), but I leave this method as an example in case one wanted to, for example, compare things by month or by minute, etc.

How to tell if string starts with a number with Python?

Illusionist

[How to tell if string starts with a number with Python?](https://stackoverflow.com/questions/5577501/how-to-tell-if-string-starts-with-a-number-with-python)

I have a string that starts with a number (from 0-9)

I know I can "or" 10 test cases using startswith() but there is probably a neater solutionso instead of writingIs there a cleverer/more efficient way?

2011-04-07 07:33:48Z

I have a string that starts with a number (from 0-9)

I know I can "or" 10 test cases using startswith() but there is probably a neater solutionso instead of writingIs there a cleverer/more efficient way?Python's string library has isdigit() method:sometimes, you can use regexYour code won't work; you need or instead of ||.Tryoror, if you are really crazy about startswith,This piece of code:prints out the following:You can also use try...except:Here are my "answers" (trying to be unique here, I don't actually recommend either for this particular case :-)Using ord() and the special a <= b <= c form:(This a <= b <= c, like a < b < c, is a special Python construct and it's kind of neat: compare 1 < 2 < 3 (true) and 1 < 3 < 2 (false) and (1 < 3) < 2 (true). This isn't how it works in most other languages.)Using a regular expression:Surprising that after such a long time there is still the best answer missing.The downside of the other answers is using [0] to select the first character, but as noted, this breaks on the empty string.Using the following circumvents this problem, and, in my opinion, gives the prettiest and most readable syntax of the options we have. It also does not import/bother with regex either):You could use regular expressions.You can detect digits using:The [0-9] par matches any digit, and yourstring[:1] matches the first character of your stringUse Regular Expressions, if you are going to somehow extend method's functionality.Try this:

Nested Function in Python

Hosam Aly

[Nested Function in Python](https://stackoverflow.com/questions/1589058/nested-function-in-python)

What benefit or implications could we get with Python code like this:I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?

2009-10-19 14:40:51Z

What benefit or implications could we get with Python code like this:I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?Normally you do it to make closures:Inner functions can access variables from the enclosing scope (in this case, the local variable x).  If you're not accessing any variables from the enclosing scope, they're really just ordinary functions with a different scope.Aside from function generators, where internal function creation is almost the definition of a function generator, the reason I create nested functions is to improve readability.  If I have a tiny function that will only be invoked by the outer function, then I inline the definition so you don't have to skip around to determine what that function is doing.  I can always move the inner method outside of the encapsulating method if I find a need to reuse the function at a later date.Toy example:One potential benefit of using inner methods is that it allows you to use outer method local variables without passing them as arguments.can be written as follows, which arguably reads betterI can't image any good reason for code like that.Maybe there was a reason for the inner function in older revisions, like other Ops. For example, this makes slightly more sense:but then the inner function should be ("private") class methods instead:The idea behind local methods is similar to local variables: don't pollute the larger name space.  Obviously the benefits are limited since most languages don't also provide such functionality directly.Are you sure the code was exactly like this? The normal reason for doing something like this is for creating a partial - a function with baked-in parameters. Calling the outer function returns a callable that needs no parameters, and so therefore can be stored and used somewhere it is impossible to pass parameters. However, the code you've posted won't do that - it calls the function immediately and returns the result, rather than the callable. It might be useful to post the actual code you saw.

Pandas aggregate count distinct

dave

[Pandas aggregate count distinct](https://stackoverflow.com/questions/18554920/pandas-aggregate-count-distinct)

Let's say I have a log of user activity and I want to generate a report of total duration and the number of unique users per day.Aggregating duration is pretty straightforward:What I'd like to do is sum the duration and count distincts at the same time, but I can't seem to find an equivalent for count_distinct:This works, but surely there's a better way, no?I'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function, but I don't have a lot of exposure to the various libraries at my disposal. Also, it seems that the groupby object already knows this information, so wouldn't I just be duplicating effort?

2013-09-01 03:25:36Z

Let's say I have a log of user activity and I want to generate a report of total duration and the number of unique users per day.Aggregating duration is pretty straightforward:What I'd like to do is sum the duration and count distincts at the same time, but I can't seem to find an equivalent for count_distinct:This works, but surely there's a better way, no?I'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function, but I don't have a lot of exposure to the various libraries at my disposal. Also, it seems that the groupby object already knows this information, so wouldn't I just be duplicating effort?How about either of:'nunique' is an option for .agg() since pandas 0.20.0, so:Just adding to the answers already given, the solution using the string "nunique" seems much faster, tested here on ~21M rows dataframe, then grouped to ~2M 

Appending to list in Python dictionary [duplicate]

Michael Murphy

[Appending to list in Python dictionary [duplicate]](https://stackoverflow.com/questions/26367812/appending-to-list-in-python-dictionary)

Is there a more elegant way to write this code? What I am doing: I have keys and dates. There can be a number of dates assigned to a key and so I am creating a dictionary of lists of dates to represent this. The following code works fine, but I was hoping for a more elegant and Pythonic method. I was expecting the below to work, but I keep getting a NoneType has no attribute append error. This probably has something to do with the fact thatbut why?

2014-10-14 18:29:55Z

Is there a more elegant way to write this code? What I am doing: I have keys and dates. There can be a number of dates assigned to a key and so I am creating a dictionary of lists of dates to represent this. The following code works fine, but I was hoping for a more elegant and Pythonic method. I was expecting the below to work, but I keep getting a NoneType has no attribute append error. This probably has something to do with the fact thatbut why?list.append returns None, since it is an in-place operation and you are assigning it back to dates_dict[key]. So, the next time when you do dates_dict.get(key, []).append you are actually doing None.append. That is why it is failing. Instead, you can simply doBut, we have collections.defaultdict for this purpose only. You can do something like thisThis will create a new list object, if the key is not found in the dictionary.Note: Since the defaultdict will create a new list if the key is not found in the dictionary, this will have unintented side-effects. For example, if you simply want to retrieve a value for the key, which is not there, it will create a new list and return it.Use collections.defaultdict:dates_dict[key] = dates_dict.get(key, []).append(date) sets   dates_dict[key] to None as list.append returns None.You should use collections.defaultdict

Filtering a list of strings based on contents

Chris.Jie

[Filtering a list of strings based on contents](https://stackoverflow.com/questions/2152898/filtering-a-list-of-strings-based-on-contents)

Given the list ['a','ab','abc','bac'], I want to compute a list with strings that have 'ab' in them. I.e. the result is ['ab','abc']. How can this be done in Python?

2010-01-28 07:17:41Z

Given the list ['a','ab','abc','bac'], I want to compute a list with strings that have 'ab' in them. I.e. the result is ['ab','abc']. How can this be done in Python?This simple filtering can be achieved in many ways with Python. The best approach is to use "list comprehensions" as follows:Another way is to use the filter function:Tried this out quickly in the interactive shell:Why does this work? Because the in operator is defined for strings to mean: "is substring of".Also, you might want to consider writing out the loop as opposed to using the list comprehension syntax used above:

Install particular version with easy_install

dr jerry

[Install particular version with easy_install](https://stackoverflow.com/questions/3833011/install-particular-version-with-easy-install)

I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.What is the best way to fix this and how can I force easy_install to install a particular version?(Mac os x 10.6)

2010-09-30 17:05:03Z

I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.What is the best way to fix this and how can I force easy_install to install a particular version?(Mac os x 10.6)I believe the way to specify a version would be like this:I (and most other Python users I suspect) stopped using easy_install and started using pip some time ago, so a solution in those terms is:(pip has several benefits, including an uninstall command)From the easy_install documentation:easy_install PackageName==1.2.3You should do something like this:

Python requests library how to pass Authorization header with single token

Ian Stapleton Cordasco

[Python requests library how to pass Authorization header with single token](https://stackoverflow.com/questions/19069701/python-requests-library-how-to-pass-authorization-header-with-single-token)

I have a request URI and a token. If I use:etc., I get a 200 and view the corresponding JSON data.

So, I installed requests and when I attempt to access this resource I get a 403 probably because I do not know the correct syntax to pass that token. Can anyone help me figure it out?

This is what I have:I already tried:But none of these work.

2013-09-28 17:25:29Z

I have a request URI and a token. If I use:etc., I get a 200 and view the corresponding JSON data.

So, I installed requests and when I attempt to access this resource I get a 403 probably because I do not know the correct syntax to pass that token. Can anyone help me figure it out?

This is what I have:I already tried:But none of these work.In python:is equivalent toAnd requests interpretsAs you wanting requests to use Basic Authentication and craft an authorization header like so:Which is the base64 representation of 'TOK:<MY_TOKEN>'To pass your own header you pass in a dictionary like so:I was looking for something similar and came across this. It looks like in the first option you mentioned"auth" takes two parameters: username and password, so the actual statement should beIn my case, there was no password, so I left the second parameter in auth field empty as shown below:Hope this helps somebody :)This worked for me:You can also set headers for the entire session:Requests natively supports basic auth only with user-pass params, not with tokens.You could, if you wanted, add the following class to have requests support token based basic authentication:Then, to use it run the following request :An alternative would be to formulate a custom header instead, just as was suggested by other users here.i founded here, its ok with me for linkedin:

https://auth0.com/docs/flows/guides/auth-code/call-api-auth-code

so my code with with linkedin login here:You can try something like thisThis worked for me:  My code uses user generated token.

How do I update a Mongo document after inserting it?

TIMEX

[How do I update a Mongo document after inserting it?](https://stackoverflow.com/questions/4372797/how-do-i-update-a-mongo-document-after-inserting-it)

Let's say I insert the document.Now, let's say I want to add a field and update it. How do I do that? This doesn't seem to work.....

2010-12-07 02:13:57Z

Let's say I insert the document.Now, let's say I want to add a field and update it. How do I do that? This doesn't seem to work.....In pymongo you can update with:

mycollection.update({'_id':mongo_id}, {"$set": post}, upsert=False)

Upsert parameter will insert instead of updating if the post is not found in the database.

Documentation is available at mongodb site.UPDATE For version > 3 use update_one instead of update:mycollection.update_one({'_id':mongo_id}, {"$set": post}, upsert=False)should work splendidly for you. If there is no document of id mongo_id, it will fail, unless you also use upsert=True. This returns the old document by default. To get the new one, pass return_document=ReturnDocument.AFTER. All parameters are described in the API.The method was introduced for MongoDB 3.0. It was extended for 3.2, 3.4, and 3.6.I will use collection.save(the_changed_dict) this way. I've just tested this, and it still works for me. The following is quoted directly from pymongo doc.:save(to_save[, manipulate=True[, safe=False[, **kwargs]]])This is an old question, but I stumbled onto this when looking for the answer so I wanted to give the update to the answer for reference.The methods save and update are deprecated.in the OPs particular case, it's better to use replace_one.According to the latest documentation about PyMongo titled Insert a Document (insert is deprecated) and following defensive approach, you should insert and update as follows:

Convert pandas data frame to series

user1357015

[Convert pandas data frame to series](https://stackoverflow.com/questions/33246771/convert-pandas-data-frame-to-series)

I'm somewhat new to pandas. I have a pandas data frame that is 1 row by 23 columns.I want to convert this into a series? I'm wondering what the most pythonic way to do this is?I've tried pd.Series(myResults) but it complains ValueError: cannot copy sequence with size 23 to array axis with dimension 1. It's not smart enough to realize it's still a "vector" in math terms. Thanks!

2015-10-20 21:05:48Z

I'm somewhat new to pandas. I have a pandas data frame that is 1 row by 23 columns.I want to convert this into a series? I'm wondering what the most pythonic way to do this is?I've tried pd.Series(myResults) but it complains ValueError: cannot copy sequence with size 23 to array axis with dimension 1. It's not smart enough to realize it's still a "vector" in math terms. Thanks!Say rather that it's smart enough to recognize a difference in dimensionality. :-)I think the simplest thing you can do is select that row positionally using iloc, which gives you a Series with the columns as the new index and the values as the values:You can transpose the single-row dataframe (which still results in a dataframe) and then squeeze the results into a series (the inverse of to_frame).Note: To accommodate the point raised by @IanS (even though it is not in the OP's question), test for the dataframe's size.  I am assuming that df is a dataframe, but the edge cases are an empty dataframe, a dataframe of shape (1, 1), and a dataframe with more than one row in which case the use should implement their desired functionality.This can also be simplified along the lines of the answer provided by @themachinist.You can retrieve the series through slicing your dataframe using one of these two methods:http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iloc.html

http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.htmlAnother way -Suppose myResult is the dataFrame that contains your data in the form of 1 col and 23 rowsIn similar fashion, you can get series from Dataframe with multiple columns.This gives a dataframe with index as column name of data and all data are present in "values" column

Python logging not outputting anything

murgatroid99

[Python logging not outputting anything](https://stackoverflow.com/questions/7016056/python-logging-not-outputting-anything)

In a python script I am writing, I am trying to log events using the logging module. I have the following code to configure my logger:When I try to run logging.debug("Some string"), I get no output to the console, even though this page in the docs says that logging.debug should have the root logger output the message. Why is my program not outputting anything, and how can I fix it?

2011-08-10 18:45:29Z

In a python script I am writing, I am trying to log events using the logging module. I have the following code to configure my logger:When I try to run logging.debug("Some string"), I get no output to the console, even though this page in the docs says that logging.debug should have the root logger output the message. Why is my program not outputting anything, and how can I fix it?The default logging level is warning.

Since you haven't changed the level, the root logger's level is still warning.

That means that it will ignore any logging with a level that is lower than warning, including debug loggings.This is explained in the tutorial:The 'info' line doesn't print anything, because the level is higher than info.To change the level, just set it in the root logger:In other words, it's not enough to define a handler with level=DEBUG, the actual logging level must also be DEBUG in order to get it to output anything.Many years later there seems to still be a usability problem with the Python logger. Here's some explanations with examples:A common source of confusion comes from a badly initialised root logger. Consider this:Output:Depending on your runtime environment and logging levels, the first log line (before basic config) might not show up anywhere.Maybe try this? It seems the problem is solved after remove all the handlers in my case.For anyone here that wants a super-simple answer: just set the level you want displayed. At the top of all my scripts I just put:Then to display anything at or above that level:It is a hierarchical set of five levels so that logs will display at the level you set, or higher. So if you want to display an error you could use logging.error("The plumbus is broken").The levels, in increasing order of severity, are DEBUG, INFO, WARNING, ERROR, and CRITICAL. The default setting is WARNING. This is a good article containing this information expressed better than my answer:

https://www.digitalocean.com/community/tutorials/how-to-use-logging-in-python-3

Is there a built-in product() in Python? [duplicate]

George Burrows

[Is there a built-in product() in Python? [duplicate]](https://stackoverflow.com/questions/7948291/is-there-a-built-in-product-in-python)

I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().Is the only way I could find the product of items in a list by importing the mul() operator?

2011-10-30 22:18:05Z

I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().Is the only way I could find the product of items in a list by importing the mul() operator?Yes, that's right.  Guido rejected the idea for a built-in prod() function because he thought it was rarely needed.In Python 3.8, prod() was added to the math module:As you suggested, it is not hard to make your own using reduce() and operator.mul():In Python 3, the reduce() function was moved to the functools module, so you would need to add:As a side note, the primary motivating use case for prod() is to compute factorials.  We already have support for that in the math module:If your data consists of floats, you can compute a product using sum() with exponents and logarithms:There is no product in Python, but you can define it asOr, if you have NumPy, use numpy.product.

Since the reduce() function has been moved to the module functools python 3.0, you have to take a different approach.You can use functools.reduce() to access the function:Or, if you want to follow the spirit of the python-team (which removed reduce() because they think for would be more readable), do it with a loop:numpy has many really cool functions for lists!

How to choose cross-entropy loss in TensorFlow?

Maxim

[How to choose cross-entropy loss in TensorFlow?](https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow)

Classification problems, such as logistic regression or multinomial

logistic regression, optimize a cross-entropy loss.

Normally, the cross-entropy layer follows the softmax layer,

which produces probability distribution.In tensorflow, there are at least a dozen of different cross-entropy loss functions:Which one works only for binary classification and which are suitable for multi-class problems? When should you use sigmoid instead of softmax? How are sparse functions different from others and why is it only softmax?Related (more math-oriented) discussion: What are the differences between all these cross-entropy losses in Keras and TensorFlow?. 

2017-10-31 11:59:49Z

Classification problems, such as logistic regression or multinomial

logistic regression, optimize a cross-entropy loss.

Normally, the cross-entropy layer follows the softmax layer,

which produces probability distribution.In tensorflow, there are at least a dozen of different cross-entropy loss functions:Which one works only for binary classification and which are suitable for multi-class problems? When should you use sigmoid instead of softmax? How are sparse functions different from others and why is it only softmax?Related (more math-oriented) discussion: What are the differences between all these cross-entropy losses in Keras and TensorFlow?. As stated earlier, sigmoid loss function is for binary classification.

But tensorflow functions are more general and allow to do

multi-label classification, when the classes are independent.

In other words, tf.nn.sigmoid_cross_entropy_with_logits solves N

binary classifications at once.The labels must be one-hot encoded or can contain soft class probabilities.tf.losses.sigmoid_cross_entropy in addition allows to set the in-batch weights,

i.e. make some examples more important than others.

tf.nn.weighted_cross_entropy_with_logits allows to set class weights

(remember, the classification is binary), i.e. make positive errors larger than

negative errors. This is useful when the training data is unbalanced.These loss functions should be used for multinomial mutually exclusive classification,

i.e. pick one out of N classes. Also applicable when N = 2.The labels must be one-hot encoded or can contain soft class probabilities:

a particular example can belong to class A with 50% probability and class B

with 50% probability. Note that strictly speaking it doesn't mean that

it belongs to both classes, but one can interpret the probabilities this way.Just like in sigmoid family, tf.losses.softmax_cross_entropy allows

to set the in-batch weights, i.e. make some examples more important than others.

As far as I know, as of tensorflow 1.3, there's no built-in way to set class weights.[UPD] In tensorflow 1.5, v2 version was introduced and the original softmax_cross_entropy_with_logits loss got deprecated. The only difference between them is that in a newer version, backpropagation  happens into both logits and labels (here's a discussion why this may be useful).Like ordinary softmax above, these loss functions should be used for

multinomial mutually exclusive classification, i.e. pick one out of N classes.

The difference is in labels encoding: the classes are specified as integers (class index),

not one-hot vectors. Obviously, this doesn't allow soft classes, but it

can save some memory when there are thousands or millions of classes.

However, note that logits argument must still contain logits per each class,

thus it consumes at least [batch_size, classes] memory.Like above, tf.losses version has a weights argument which allows

to set the in-batch weights.These functions provide another alternative for dealing with huge number of classes.

Instead of computing and comparing an exact probability distribution, they compute

a loss estimate from a random sample.The arguments weights and biases specify a separate fully-connected layer that

is used to compute the logits for a chosen sample.Like above, labels are not one-hot encoded, but have the shape [batch_size, num_true].Sampled functions are only suitable for training. In test time, it's recommended to

use a standard softmax loss (either sparse or one-hot) to get an actual distribution.Another alternative loss is tf.nn.nce_loss, which performs noise-contrastive estimation (if you're interested, see this very detailed discussion). I've included this function to the softmax family, because NCE guarantees approximation to softmax in the limit.However, for version 1.5, softmax_cross_entropy_with_logits_v2 must be used instead, while using its argument with the argument key=..., for example

Where is pip cache folder

Arash Hatami

[Where is pip cache folder](https://stackoverflow.com/questions/34578168/where-is-pip-cache-folder)

Where is Python pip cache folder? I had error during install and now reinstall packages using cache filesWhere is that directory? I want backup them for install in the future. Is it possible ?For example I have this one : I searched google for this directory but anything I saw, is learn to how install from a folder, I want to find default cache directory.And another question, these cache files will stay in that directory or will remove soon ?

2016-01-03 15:26:50Z

Where is Python pip cache folder? I had error during install and now reinstall packages using cache filesWhere is that directory? I want backup them for install in the future. Is it possible ?For example I have this one : I searched google for this directory but anything I saw, is learn to how install from a folder, I want to find default cache directory.And another question, these cache files will stay in that directory or will remove soon ?Because this question ranks, and the accepted answer doesn't quite match the question title:The location of the cache directory can be changed via the command line option --cache-dir.Pythonic and cross-platform way:Under the hood, it normalizes paths, manages different locations for exotic and ordinary operating systems and platforms, performs Windows registry lookup.It may worth mentioning, if you have different Python versions installed, 2.x'es and 3.x'es, they all do share the same cache location.You can backup the associated wheel rather than attempting to perform a backup of the cache folder.Download the wheel for csselect of version 0.9.1 into /tmp/wheelhouse:Install the downloaded wheel:

Python void return type annotation

Tregoreg

[Python void return type annotation](https://stackoverflow.com/questions/36797282/python-void-return-type-annotation)

In python 3.x, it is common to use return type annotation of a function, such as:What is the correct annotation for the "void" type?I'm considering 3 options:Option 2. seems the most logical to me, but I've already seen some instances of 1.

2016-04-22 15:02:45Z

In python 3.x, it is common to use return type annotation of a function, such as:What is the correct annotation for the "void" type?I'm considering 3 options:Option 2. seems the most logical to me, but I've already seen some instances of 1.This is straight from PEP 484 -- Type Hints documentation:And, as you can see most of the examples use None as return type.TLDR: The idiomatic equivalent of a void return type annotation is -> None.Type hinting in Python does not strictly require actual types. For example, annotations may use strings of type names: Union[str, int], Union[str, 'int'], 'Union[str, int]' and various variants are equivalent.Similarly, the type annotation None is considered to mean "is of NoneType". This can be used not just for return types, though you will see it most often there:This also applies to generic types. For example, you can use None in Generator[int, None, None] to indicate a generator does not take or return values.Even though PEP 484 suggests that None means type(None), you should  not use the latter form explicitly. The type hinting specification does not include any form of type(...). This is technically a runtime expression, and its support is entirely up to the type checker. The mypy project is considering to remove support for type(None) and remove it from 484 as well.The static equivalent of "the type of X" is Type[X]. This is valid for None as well:However, it is basically the equivalent of Type[True] and similar. It eliminates one special case (use of a value instead of a type) with another special case (use of a type derived from a value). The idiomatic special case is to just use None.Omitting the return type does not mean that there is no return value. As per PEP 484:This means the value is considered dynamically typed and statically supports any operation. That is practically the opposite meaning of void.

Weird Try-Except-Else-Finally behavior with Return statements

Kyle Owens

[Weird Try-Except-Else-Finally behavior with Return statements](https://stackoverflow.com/questions/11164144/weird-try-except-else-finally-behavior-with-return-statements)

This is some code that is behaving peculiarly. This is a simplified version of the behavior that I've written. This will still demonstrate the weird behavior and I had some specific questions on why this is occurring.I'm using Python 2.6.6 on Windows 7.Results:

2012-06-22 21:07:35Z

This is some code that is behaving peculiarly. This is a simplified version of the behavior that I've written. This will still demonstrate the weird behavior and I had some specific questions on why this is occurring.I'm using Python 2.6.6 on Windows 7.Results:Because finally statements are guaranteed to be executed (well, presuming no power outage or anything outside of Python's control). This means that before the function can return, it must run the finally block, which returns a different value.The Python docs state:This means that when you try to return, the finally block is called, returning it's value, rather than the one that you would have had.The execution order is:So, any return in the finally block will end the steps in advance.

Upload files in Google App Engine

Graviton

[Upload files in Google App Engine](https://stackoverflow.com/questions/81451/upload-files-in-google-app-engine)

I am planning to create a web app that allows users to downgrade their visual studio project files. However, It seems Google App Engine accepts files uploading and flat file storing on the Google Server through db.TextProperty and db.BlobProperty.I'll be glad anyone can provide code sample (both the client and the server side) on how this can be done.

2008-09-17 09:28:53Z

I am planning to create a web app that allows users to downgrade their visual studio project files. However, It seems Google App Engine accepts files uploading and flat file storing on the Google Server through db.TextProperty and db.BlobProperty.I'll be glad anyone can provide code sample (both the client and the server side) on how this can be done.Here is a complete, working file.  I pulled the original from the Google site and modified it to make it slightly more real world.A few things to notice:Good Luck!In fact, this question is answered in the App Egnine documentation. See an example on Uploading User Images.HTML code, inside <form></form>:Python code:There is a thread in Google Groups about it:Uploading FilesWith a lot of useful code, that discussion helped me very much in uploading files.Google has released a service for storing large files. Have a look at blobstore API documentation. If your files are > 1MB, you should use it.I try it today, It works as following:my sdk version is 1.3.xhtml page:Server Code:If your still having a problem, check you are using enctype in the form tagNo:Yes:You can not store files as there is not a traditional file system.  You can only store them in their own DataStore (in a field defined as a BlobProperty)There is an example in the previous link:Personally I found the tutorial described here useful when using the Java run time with GAE. For some reason, when I tried to upload a file usingI found that my HttpServlet class for some reason wouldn't accept the form with the 'enctype' attribute. Removing it works, however, this means I can't upload any files.There's no flat file storing in Google App Engine.  Everything has to go in to the Datastore which is a bit like a relational database but not quite.You could store the files as TextProperty or BlobProperty attributes.There is a 1MB limit on DataStore entries which may or may not be a problem.I have observed some strange behavior when uploading files on App Engine. When you submit the following form:And then you extract the img from the request like this:The img_contents variable is a str() in Google Chrome, but it's unicode in Firefox. And as you now, the db.Blob() constructor takes a string and will throw an error if you pass in a unicode string. Does anyone know how this can be fixed?Also, what I find absolutely strange is that when I copy and paste the Guestbook application (with avatars), it works perfectly. I do everything exactly the same way in my code, but it just won't work. I'm very close to pulling my hair out.There is a way of using flat file system( Atleast in usage perspective)There is this Google App Engine Virtual FileSystem project. that is implemented with the help of datastore and memcache APIs to emulate an ordinary filesystem. Using this library you can use in you project a similar filesystem access(read and write).

How do I access the child classes of an object in django without knowing the name of the child class?

Gabriel Hurley

[How do I access the child classes of an object in django without knowing the name of the child class?](https://stackoverflow.com/questions/929029/how-do-i-access-the-child-classes-of-an-object-in-django-without-knowing-the-nam)

In Django, when you have a parent class and multiple child classes that inherit from it you would normally access a child through parentclass.childclass1_set or parentclass.childclass2_set, but what if I don't know the name of the specific child class I want?Is there a way to get the related objects in the parent->child direction without knowing the child class name?

2009-05-30 04:52:56Z

In Django, when you have a parent class and multiple child classes that inherit from it you would normally access a child through parentclass.childclass1_set or parentclass.childclass2_set, but what if I don't know the name of the specific child class I want?Is there a way to get the related objects in the parent->child direction without knowing the child class name?(Update: For Django 1.2 and newer, which can follow select_related queries across reverse OneToOneField relations (and thus down inheritance hierarchies), there's a better technique available which doesn't require the added real_type field on the parent model. It's available as InheritanceManager in the django-model-utils project.)The usual way to do this is to add a ForeignKey to ContentType on the Parent model which stores the content type of the proper "leaf" class.  Without this, you may have to do quite a number of queries on child tables to find the instance, depending how large your inheritance tree is.  Here's how I did it in one project:This is implemented as an abstract base class to make it reusable; you could also put these methods and the FK directly onto the parent class in your particular inheritance hierarchy.This solution won't work if you aren't able to modify the parent model.  In that case you're pretty much stuck checking all the subclasses manually.In Python, given a ("new-style") class X, you can get its (direct) subclasses with X.__subclasses__(), which returns a list of class objects. (If you want "further descendants", you'll also have to call __subclasses__ on each of the direct subclasses, etc etc -- if you need help on how to do that effectively in Python, just ask!).Once you have somehow identified a child class of interest (maybe all of them, if you want instances of all child subclasses, etc), getattr(parentclass,'%s_set' % childclass.__name__) should help (if the child class's name is 'foo', this is just like accessing parentclass.foo_set -- no more, no less). Again, if you need clarification or examples, please ask!Carl's solution is a good one, here's one way to do it manually if there are multiple related child classes:It uses a function out of _meta, which is not  guaranteed to be stable as django evolves, but it does the trick and can be used on-the-fly if need be.It turns out that what I really needed was this:Model inheritance with content type and inheritance-aware managerThat has worked perfectly for me. Thanks to everyone else, though. I learned a lot just reading your answers!You can use django-polymorphic for that.It allows to automatically cast derived classes back to their actual type. It also provides Django admin support, more efficient SQL query handling, and proxy model, inlines and formset support.The basic principle seems to be reinvented many times (including Wagtail's .specific, or the examples outlined in this post). It takes more effort however, to make sure it doesn't result in an N-query issue, or integrate nicely with the admin, formsets/inlines or third party apps.Here's my solution, again it uses _meta so isn't guaranteed to be stable.You can achieve this looking for all the fields in the parent that are an instance of django.db.models.fields.related.RelatedManager. From your example it seems that the child classes you are talking about are not subclasses. Right?An alternative approach using proxies can be found in this blog post. Like the other solutions, it has its benefits and liabilities, which are very well put in the end of the post.

Python in Browser: How to choose between Brython, PyPy.js, Skulpt and Transcrypt?

P i

[Python in Browser: How to choose between Brython, PyPy.js, Skulpt and Transcrypt?](https://stackoverflow.com/questions/30155551/python-in-browser-how-to-choose-between-brython-pypy-js-skulpt-and-transcrypt)

I'm very excited to see that it is now possible to code Python in the browser. These are the main candidates (please add any I may have overlooked):But how to choose between them? The only obvious difference I can see is that Skulpt is based on Python 2, whereas Brython is based on Python 3.Please note: This is not a request for recommendations or opinions. I'm seeking objective facts that would inform an educated choice.

2015-05-10 19:08:35Z

I'm very excited to see that it is now possible to code Python in the browser. These are the main candidates (please add any I may have overlooked):But how to choose between them? The only obvious difference I can see is that Skulpt is based on Python 2, whereas Brython is based on Python 3.Please note: This is not a request for recommendations or opinions. I'm seeking objective facts that would inform an educated choice.Running Python in the Browser is a really good and up-to-date(as of 2019) article that compares Brython, Skulpt, PyPy.js, Transcrypt, Pyodide, Batavia. I highly recommend reading it.A good summary can be seen in the following pictures.Here's some info on Brython vs Transcrypt (July 2016, since Transcrypt was added as an option on this question by the OP), gleaned by starting off a project with Brython a few months ago and moving to Transcrypt (completed moving last week). I like Brython and Transcrypt and can see uses for both of them. For people that are new to this, Brython and Transcrypt both 'transpile' python input to javascript (Edit: maybe it's better to view Brython as a 'a Python implementation for the browser' because it doesn't produce standalone javascript). Both require Python 3 syntax. Brython includes a substantial number of Python standard libraries and some of it's own for dealing with web related things, whereas Transcrypt avoids that for the most part and suggests using Javascript libraries instead.Brython (Github) can do the conversion in the browser. So you write in python and the brython.js engine converts it to javascript on the fly when the page is loaded. This is really convenient, and is much faster than you might think. However, the brython.js engine that you need to include in your pages is about 500Kb. Also, there's the matter of importing standard libraries, which Brython handles by fetching separate .js files with XHR requests. Some libs are already compiled into brython.js, so not every import will pull in new files, but if you use many imports, things can get slow. However, there are ways around this. What i did was to check the network tab in browser dev tools to see what files were being pulled in when the page was loaded, then delete all the files my project wasn't using in a copy of the Brython src folder, and run the script included with Brython (i think it's at Brython/www/scripts/make_VFS.py) that compiles all of the available libs into one file called py_VFS.js that you need to also link to from your html. Normally, it will make one huge 2MB+ file, but if you delete the things you aren't using, it can be quite tiny. Doing it this way, means you only need to pull in brython.js, py_VFS.js and your python code, and no additional XHR requests will be needed.Transcrypt (Github)on the other hand, is distributed as a python 3 package that you can use manually, or hook into your toolchain, to compile python to javascript in advance. So with Transcrypt, you write in python, run transcrypt against the python and it spits out javascript that you can link to in your project. It is more like a traditional compiler also in that it offers some control over the output. For example, you can choose to compile to ES6 or ES5, or ask it to output sourcemaps (that during debugging let's the browser take you directly to the corresponding python code, insead of the generated javascript code.) Transcrypt's javascript output is pretty terse (or put another way, it's pretty and terse). In my case 150kB of python is converted to 165kB of unminified ES5 javascript. By way of comparison, the Brython version of my project used about 800Kb after conversion. However, getting the benefits of Transcrypts terseness, requires reading the docs a bit (really just a bit). For example, with Transcrypt, Python's 'truthiness' for data structures like dict, set and list isn't enabled by default and globally enabling it is discouraged because of potential performance issues related to typechecking. For clarity: Under CPython, an empty dict, set or list has truth value False, whereas in Javascript it's considered 'true'.. Example:There are at least three ways to address this:Right, so my project was getting bigger and i wanted to pre-compile for a performance gain but found it hard to do so with Brython (though it's technically possible, an easy way being to use the online editor and click the javascript button to see the output). I did that and linked to the generated javascript from project.html but it didn't work for some reason. Also, I find it hard to understand error messages from Brython so i didn't know where to start after this step failed. Also, the big size of the outputted code and the size of the brython engine was beginning to bug me. So i decided to have a closer look at Transcrypt, which had at first seemed to be higher grade because i prefer dumbed down instructions that tell me how to get started immediately (these have since been added).The main thing getting it set up after installing Python3.5 was:Main issues moving acrossI have rather simple needs, so your mileage may vary.6) Also, you can't iterate dicts by default using 'for i in dict:', without enabling that (cmd line -i or __pragma__('iconv'), but you can avoid having to enable it by just using the keys() member e.g.:To summariseHope that helps someone see which of these might be good for their particular project.I've used and committed to skulpt as well as pypyjs. And they are all three very different that any comparison is moot if you ask me. It depends on what you are looking for which will make the most sense.pypyjs is huge it's a 12MB javascript file that contains the entire pypy virtual machine. So if you want python implementation completeness this is your baby. It has a javascript bridge that works really good but it's not a viable option for writing your javascript website code in python. It will however let you import compiler.It's built with emscripten and is faster then CPython, in running the pystone benchmark. I gave a short talk about pypyjs here are the slides.Is a teaching tool (or it has evolved into that over time), it compiles your python into a state machine very closely emulating the cpython compiler. At it's core it's a handwritten implementation of the python compiler in javascript. It allows for asynchronous execution which lets you do:Without locking up the browser.Skulpt is the only one that supports asynchronous continuations, it lets you pause the execution of python while it's resolving some asynchronous thing to happen. Making this work:Skulpt runs at about a tenth of the speed of CPython, when comparing pystone.I know least about this one maybe @olemis-lang can expand this one. But next to the obvious difference that Brython is py3 and the others py2. Brython is also a transpiler. Brython doesn't run the pystone benchmark because time.clock isn't implemented, because officially it's a hardware function.https://brythonista.wordpress.com/2015/03/28/comparing-the-speed-of-cpython-brython-skulpt-and-pypy-js/ This page benchmarks the three candidates.  Brython emerges as a clear winner.Despite the 'help' explaining that S.O. is not good for this kind of question, it appears that a concise answer is in this case possible.Maybe people are being too hasty?First of all I'm a Brython committer . Nevertheless I'll try to be as impartial as possible for the sake of doing an objective assessment .The last time I used it Skulpt did not support features like generator expressions . Brython and PyPy.js do so , so at the feature level IMHO the later are superior .Brython (at this time) is still work in progress . Some modules cannot be imported (e.g. xml.ElementTree ) . Nevertheless this situation is starting

to change since we are working towards running the whole CPython test suite in spite of achieving full compatibility with standards (at least when it makes sense) .Brython also supports .vfs.js to speed up module imports .PyPy.js has a number of characteristics that follow straightforward from the fact of it being powered by PyPy (JIT compilation , well tested , ...) but I'm not sure of whether it is suitable for running in the browser . This might change as the project evolves .TODO: I'll try to complement my answer with reliable benchmarks .Not mentioned here is RapydScript or RapydScript-NG. They produce very efficient JavaScript code, which is used in GlowScript VPython (glowscript.org). I used to use the original RapydScript of Alex Tsepkov (https://github.com/atsepkov/RapydScript) but recently switched to RapydScript-NG of Kovid Goyal (https://github.com/kovidgoyal/rapydscript-ng). I recently ran the pystone benchmark on CPython, RapydScript, and Brython, and you can see the results here:https://groups.google.com/forum/?fromgroups&hl=en#!topic/brython/20hAC9L3ayESince nobody has mentioned it I thought it was worth it to mention Batavia which implements the Python virtual machine for running precompiled Python bytecode.I just tried it and, while it's an interesting concept, it is still in early stages as there is little documentation.In the end it will depend on what you are trying to do. I chose Transcrypt after having a look because it was more pragmatic and better performant, also more recently released/maintained.This is an updated conference which compares all available options in the market right now:https://www.youtube.com/watch?v=2XSeNQyPlTYThe speaker is Russell Keith-Magee, who is a well known developer in the area.

Heroku truncates HTTP responses?

Nitzan Shaked

[Heroku truncates HTTP responses?](https://stackoverflow.com/questions/15411498/heroku-truncates-http-responses)

I am running a Flask/Gunicorn Python app on a Heroku Cedar dyno. The app returns JSON responses to its clients (it's an API server, really).Once in a while clients get 0-byte responses. It's not me returning them, however. Here is a snippet of my app's log:The first line above is me starting to handle the request. The second line is me returning a value (to Flask -- it's a Flask "Response" object).The third line is Gnicorn's, where you can see the Gunicorn got the 200 status and 22 bytes HTTP body ("200 22").However, the client got 0 bytes. Here is the Heroku router log:Why does Gunicorn return 22 bytes, but Heroku sees 0, and indeed passes back 0 bytes to the client? Is this a Heroku bug?

2013-03-14 14:04:20Z

I am running a Flask/Gunicorn Python app on a Heroku Cedar dyno. The app returns JSON responses to its clients (it's an API server, really).Once in a while clients get 0-byte responses. It's not me returning them, however. Here is a snippet of my app's log:The first line above is me starting to handle the request. The second line is me returning a value (to Flask -- it's a Flask "Response" object).The third line is Gnicorn's, where you can see the Gunicorn got the 200 status and 22 bytes HTTP body ("200 22").However, the client got 0 bytes. Here is the Heroku router log:Why does Gunicorn return 22 bytes, but Heroku sees 0, and indeed passes back 0 bytes to the client? Is this a Heroku bug?I know I may be considered a little off the wall here but there is another option.We know that from time to time there is a bug that happens on transit.We know that there is not much we can do right now to stop the problem. If you are only providing the API then stop reading however if you write the client too, keep going.The error is a known case, and known cause. The result of an empty return value means that something went wrong. However the value is available and was fetched, calculated, whatever... My instinct as a developer would be to treat an empty result as an HTTP error and request the data be resent. You could then track the resend requests and see how often this happens.I would suggest (although you strike me as the kind of developer to think of this too) that you count the requests and set a sane value for responding "network error" to the user. My instinct would be to retry right away and then to wait a little while before retrying some more.From what you describe the first retry would probably pick up the data properly. Of course this could mean keeping older requests hanging about in cache for a few minutes or running the request a second time depending on what seemed most appropriate.This would also route around any number of other point-to-point networking errors and leave the app far more robust even in the face of connectivity problems.I know our instinct as developers is to fix the known fault but sometimes it is better to work towards a system that is able to operate despite faults. That said it never hurts to log errors and problems and try to fix them anyway.

Python: How can I know which exceptions might be thrown from a method call

GabiMe

[Python: How can I know which exceptions might be thrown from a method call](https://stackoverflow.com/questions/1591319/python-how-can-i-know-which-exceptions-might-be-thrown-from-a-method-call)

Is there a way knowing (at coding time) which exceptions to expect when executing python code? 

I end up catching the base Exception class 90% of the time since I don't know which exception type might be thrown(and don't tell me to read the documentation. many times an exception can be propagated from the deep. and many times the documentation is not updated or correct). Is there some kind of tool to check this ? (like by reading the python code and libs)?

2009-10-19 21:43:36Z

Is there a way knowing (at coding time) which exceptions to expect when executing python code? 

I end up catching the base Exception class 90% of the time since I don't know which exception type might be thrown(and don't tell me to read the documentation. many times an exception can be propagated from the deep. and many times the documentation is not updated or correct). Is there some kind of tool to check this ? (like by reading the python code and libs)?I guess a solution could be only imprecise because of lack of static typing rules.I'm not aware of some tool that checks exceptions, but you could come up with your own tool matching your needs (a good chance to play a little with static analysis).As a first attempt, you could write a function that builds an AST, finds all Raise nodes, and then tries to figure out common patterns of raising exceptions (e. g. calling a constructor directly)Let x be the following program:Build the AST using the compiler package:Then define a Raise visitor class:And walk the AST collecting Raise nodes:You may continue by resolving symbols using compiler symbol tables, analyzing data dependencies, etc. Or you may just deduce, that CallFunc(Name('IOError'), ...) "should definitely mean raising IOError", which is quite OK for quick practical results :)You should only catch exceptions that you will handle.Catching all exceptions by their concrete types is nonsense.  You should catch specific exceptions you can and will handle.  For other exceptions, you may write a generic catch that catches "base Exception", logs it (use str() function) and terminates your program (or does something else that's appropriate in a crashy situation).If you really gonna handle all exceptions and are sure none of them are fatal (for example, if you're running the code in some kind of a sandboxed environment), then your approach of catching generic BaseException fits your aims.You might be also interested in language exception reference, not a reference for the library you're using.If the library reference is really poor and it doesn't re-throw its own exceptions when catching system ones, the only useful approach is to run tests (maybe add it to test suite, because if something is undocumented, it may change!).  Delete a file crucial for your code and check what exception is being thrown.  Supply too much data and check what error it yields.You will have to run tests anyway, since, even if the method of getting the exceptions by source code existed, it wouldn't give you any idea how you should handle any of those.  Maybe you should be showing error message "File needful.txt is not found!" when you catch IndexError?  Only test can tell.The correct tool to solve this problem is unittests. If you are having exceptions raised by real code that the unittests do not raise, then you need more unittests.Consider thisduck can be any objectObviously you can have an AttributeError if duck has no quack, a TypeError if duck has a quack but it is not callable. You have no idea what duck.quack() might raise though, maybe even a DuckError or somethingNow supposing you have code like thisIf it raises an IndexError you don't know whether it has come from arr[i] or from deep inside the database function. usually it doesn't matter so much where the exception occurred, rather that something went wrong and what you wanted to happen didn't happen.A handy technique is to catch and maybe reraise the exception like thisNoone explained so far, why you can't have a full, 100% correct list of exceptions, so I thought it's worth commenting on. One of the reasons is a first-class function. Let's say that you have a function like this:Now apl can raise any exception that f raises. While there are not many functions like that in the core library, anything that uses list comprehension with custom filters, map, reduce, etc. are affected.The documentation and the source analysers are the only "serious" sources of information here. Just keep in mind what they cannot do.I ran into this when using socket, I wanted to find out all the error conditions I would run in to (so rather than trying to create errors and figure out what socket does I just wanted a concise list). Ultimately I ended up grep'ing "/usr/lib64/python2.4/test/test_socket.py" for "raise":Which is a pretty concise list of errors. Now of course this only works on a case by case basis and depends on the tests being accurate (which they usually are). Otherwise you need to pretty much catch all exceptions, log them and dissect them and figure out how to handle them (which with unit testing wouldn't be to difficult).normally, you'd need to catch exception only around a few lines of code. You wouldn't want to put your whole main function into the try except clause. for every few line you always should now (or be able easily to check) what kind of exception might be raised.docs have an exhaustive list of built-in exceptions. don't try to except those exception that you're not expecting, they might be handled/expected in the calling code.edit: what might be thrown depends on obviously on what you're doing! accessing random element of a sequence: IndexError, random element of a dict: KeyError, etc.Just try to run those few lines in IDLE and cause an exception. But unittest would be a better solution, naturally.There are two ways that I found informative. The first one, run the instructions in iPython, which will display the exception type.In the second way we settle for catching too much and improving on it. Include a try expression in your code and catch except Exception as err. Print sufficient data to know what exception was thrown. As exceptions are thrown improve your code by adding a more precise except clause. When you feel that you have cached all relevant exceptions remove the all inclusive one. A good thing to do anyway because it swallows programming errors.

Python packages and egg-info directories

Jeremy Cantrell

[Python packages and egg-info directories](https://stackoverflow.com/questions/256417/python-packages-and-egg-info-directories)

Can someone explain how egg-info directories are tied to their respective modules? For example, I have the following:I'm assuming the egg-info directory is to make the corresponding module visible to setuptools (easy_install), right? If so, how does setuptools tie the egg-info directory to the module directory?Assuming that I'm on the right track, and for the sake of example... If I wanted to make an existing package of mine visible to setuptools, could I just symlink the module directory and the egg-info directory to the site-packages directory? I would have just tried this myself, but I'm not sure how to test if the package is visible to setuptools. Bonus points if you can also tell me how to test this :)The main reason I'm trying to understand all this is because I would like to symlink some of my modules into site-packages so that I can make changes to them and have the changes visible to the scripts that use them without having to reinstall the egg from PyPI after each change.

2008-11-02 02:26:20Z

Can someone explain how egg-info directories are tied to their respective modules? For example, I have the following:I'm assuming the egg-info directory is to make the corresponding module visible to setuptools (easy_install), right? If so, how does setuptools tie the egg-info directory to the module directory?Assuming that I'm on the right track, and for the sake of example... If I wanted to make an existing package of mine visible to setuptools, could I just symlink the module directory and the egg-info directory to the site-packages directory? I would have just tried this myself, but I'm not sure how to test if the package is visible to setuptools. Bonus points if you can also tell me how to test this :)The main reason I'm trying to understand all this is because I would like to symlink some of my modules into site-packages so that I can make changes to them and have the changes visible to the scripts that use them without having to reinstall the egg from PyPI after each change.The .egg-info directories get only created if --single-version-externally-managed was used to install the egg. "Normally", installing an egg would create a single directory (or zip file), containing both the code and the metadata. pkg_resources (which is the library that reads the metadata) has a function require which can be used to request a specific version of the package. For "old-style", regular imports, easy_install hacks a .pth file to get the egg directory onto sys.path. For --single-version-externally-managed, this hacking is not necessary, because there will only be a single version installed (by the system's pacakging infrastructure, e.g. rpm or dpkg). The egg-info is still included, for applications that use require (or any of the other pkg_resources binding mechanisms).If you want to install a package by hard-linking, I recommend to use "setup.py develop". This is a command from setuptools which doesn't actually install the egg, but makes it available site-wide. To do so, it creates an egg-link file so that pkg_resources can find it, and it manipulates a .pth file, so that regular import can find it.

Cannot display HTML string

Hoa Vu

[Cannot display HTML string](https://stackoverflow.com/questions/29208984/cannot-display-html-string)

I am struggling with display string HTML in Android WebView.

On the server side, I downloaded a web page and escape HTML characters and quotes (I used Python):On the Android client side: strings are unescaped by:However webview just display them as literal strings. Here are the result:

Edit: I add original string returned from server side:

2015-03-23 11:22:07Z

I am struggling with display string HTML in Android WebView.

On the server side, I downloaded a web page and escape HTML characters and quotes (I used Python):On the Android client side: strings are unescaped by:However webview just display them as literal strings. Here are the result:

Edit: I add original string returned from server side:I have modified the code here:Try this code,Try this:

MySQL parameterized queries

Specto

[MySQL parameterized queries](https://stackoverflow.com/questions/775296/mysql-parameterized-queries)

I am having a hard time using the MySQLdb module to insert information into my database.  I need to insert 6 variables into the table.  Can someone help me with the syntax here?

2009-04-22 00:43:21Z

I am having a hard time using the MySQLdb module to insert information into my database.  I need to insert 6 variables into the table.  Can someone help me with the syntax here?Beware of using string interpolation for SQL queries, since it won't escape the input parameters correctly and will leave your application open to SQL injection vulnerabilities. The difference might seem trivial, but in reality it's huge.It adds to the confusion that the modifiers used to bind parameters in a SQL statement varies between different DB API implementations and that the mysql client library uses printf style syntax instead of the more commonly accepted '?' marker (used by eg. python-sqlite).You have a few options available. You'll want to get comfortable with python's string iterpolation. Which is a term you might have more success searching for in the future when  you want to know stuff like this.Better for queries:Considering you probably have all of your data in an object or dictionary already, the second format will suit you better. Also it sucks to have to count "%s" appearances in a string when you have to come back and update this method in a year :)The linked docs give the following example:So you just need to adapt this to your own code - example:(If SongLength is numeric, you may need to use %d instead of %s).Actually, even if your variable (SongLength) is numeric, you will still have to format it with %s in order to bind the parameter correctly.  If you try to use %d, you will get an error.  Here's a small excerpt from this link http://mysql-python.sourceforge.net/MySQLdb.html:To perform a query, you first need a cursor, and then you can execute queries on it:In this example, max_price=5 Why, then, use %s in the string? Because MySQLdb will convert it to a SQL literal value, which is the string '5'. When it's finished, the query will actually say, "...WHERE price < 5".As an alternative to the chosen answer, and with the same safe semantics of Marcel's, here is a compact way of using a Python dictionary to specify the values. It has the benefit of being easy to modify as you add or remove columns to insert:Where meta is the dictionary holding the values to insert. Update can be done in the same way:The first solution works well. I want to add one small detail here. Make sure the variable you are trying to replace/update it will has to be a type str. My mysql type is decimal but I had to make the parameter variable as str to be able to execute the query. Here is another way to do it. It's documented on the MySQL official website.

https://dev.mysql.com/doc/connector-python/en/connector-python-api-mysqlcursor-execute.htmlIn the spirit, it's using the same mechanic of @Trey Stout's answer. However, I find this one prettier and more readable.And to better illustrate any need for variables:NB: note the escape being done.

Plotting categorical data with pandas and matplotlib

Ivan

[Plotting categorical data with pandas and matplotlib](https://stackoverflow.com/questions/31029560/plotting-categorical-data-with-pandas-and-matplotlib)

I have a data frame with categorical data:I want to generate some graphs, like pie charts and histograms based on the categories. Is it possible without creating dummy numeric variables? Something like

2015-06-24 14:37:16Z

I have a data frame with categorical data:I want to generate some graphs, like pie charts and histograms based on the categories. Is it possible without creating dummy numeric variables? Something likeYou can simply use value_counts on the series:like this : You might find useful mosaic plot from statsmodels. Which can also give statistical highlighting for the variances.But beware of the 0 sized cell - they will cause problems with labels.See this answer for detailsYou could also use countplot from seaborn.  This package builds on pandas to create a high level plotting interface. It gives you good styling and correct axis labels for free.It also supports coloring the bars in the right color with a little trickTo plot multiple categorical features as bar charts on the same plot, I would suggest:

Save / load scipy sparse csr_matrix in portable data format

Henry Thornton

[Save / load scipy sparse csr_matrix in portable data format](https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format)

How do you save/load a scipy sparse csr_matrix in a portable format?  The scipy sparse matrix is created on Python 3 (Windows 64-bit) to run on Python 2 (Linux 64-bit).  Initially, I used pickle (with protocol=2 and fix_imports=True) but this didn't work going from Python 3.2.2 (Windows 64-bit) to Python 2.7.2 (Windows 32-bit) and got the error:Next, tried numpy.save and numpy.load as well as scipy.io.mmwrite() and scipy.io.mmread() and none of these methods worked either.

2012-01-21 18:20:42Z

How do you save/load a scipy sparse csr_matrix in a portable format?  The scipy sparse matrix is created on Python 3 (Windows 64-bit) to run on Python 2 (Linux 64-bit).  Initially, I used pickle (with protocol=2 and fix_imports=True) but this didn't work going from Python 3.2.2 (Windows 64-bit) to Python 2.7.2 (Windows 32-bit) and got the error:Next, tried numpy.save and numpy.load as well as scipy.io.mmwrite() and scipy.io.mmread() and none of these methods worked either.edit: SciPy 1.19 now has scipy.sparse.save_npz and scipy.sparse.load_npz.For both functions, the file argument may also be a file-like object (i.e. the result of open) instead of a filename.Got an answer from the Scipy user group:So for example:Though you write, scipy.io.mmwrite and scipy.io.mmread don't work for you, I just want to add how they work. This question is the no. 1 Google hit, so I myself started with np.savez and pickle.dump before switching to the simple and obvious scipy-functions. They work for me and shouldn't be overseen by those who didn't tried them yet.Here is performance comparison of the three most upvoted answers using Jupyter notebook. The input is a 1M x 100K random sparse matrix with density 0.001, containing 100M non-zero values:(note that the format has been changed from csr to coo).Note: cPickle does not work with very large objects (see this answer).

In my experience, it didn't work for a 2.7M x 50k matrix with 270M non-zero values.

np.savez solution worked well.(based on this simple test for CSR matrices)

cPickle is the fastest method, but it doesn't work with very large matrices, np.savez is only slightly slower, while io.mmwrite is much slower, produces bigger file and restores to the wrong format. So np.savez is the winner here.Now you can use scipy.sparse.save_npz :

https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.htmlAssuming you have scipy on both machines, you can just use pickle.However, be sure to specify a binary protocol when pickling numpy arrays.  Otherwise you'll wind up with a huge file.At any rate, you should be able to do this:You can then load it with:As of scipy 0.19.0, you can save and load sparse matrices this way:EDIT Apparently it is simple enough to:Which will yield a ((i, j), value) tuples, which are easy to serialize and deserialize. Not sure how it compares performance-wise with the code below for csr_matrix, but it's definitely simpler. I'm leaving the original answer below as I hope it's informative.Adding my two cents: for me, npz is not portable as I can't use it to export my matrix easily to non-Python clients (e.g. PostgreSQL -- glad to be corrected). So I would have liked to get CSV output for the sparse matrix (much like you would get it you print() the sparse matrix). How to achieve this depends on the representation of the sparse matrix. For a CSR matrix, the following code spits out CSV output. You can adapt for other representations.It's about 2 times slower than save_npz in the current implementation, from what I've tested.This is what I used to save a lil_matrix.I must say I found NumPy's np.load(..) to be very slow. This is my current solution, I feel runs much faster:I was asked to send the matrix in a simple and generic format: I ended up with this:This works for me:The trick was to call .tolist() to convert the shape 0 object array to the original object.

How can I find the union of two Django querysets?

Paul D. Waite

[How can I find the union of two Django querysets?](https://stackoverflow.com/questions/4411049/how-can-i-find-the-union-of-two-django-querysets)

I’ve got a Django model with two custom manager methods. Each returns a different subset of the model’s objects, based on a different property of the object.Is there any way to get a queryset, or just a list of objects, that’s the union of the querysets returned by each manager method?

2010-12-10 16:32:44Z

I’ve got a Django model with two custom manager methods. Each returns a different subset of the model’s objects, based on a different property of the object.Is there any way to get a queryset, or just a list of objects, that’s the union of the querysets returned by each manager method?This works and looks a bit cleaner:If you don't want duplicates, then you will need to append .distinct():Starting from version 1.11, django querysets have a builtin union method.See my blog post on this for more examples.I would suggest using 'query1.union(query2)' instead of 'query1 | query2';

I got different results from the above two methods and the former one is what I expected.

The following is what I had come across:result:

how to delete files from amazon s3 bucket?

Suhail

[how to delete files from amazon s3 bucket?](https://stackoverflow.com/questions/3140779/how-to-delete-files-from-amazon-s3-bucket)

I need to write code in python that will delete the required file from an Amazon s3 bucket. I am able to connect to the Amazon s3 bucket, and also to save files, but how can I delete a file?

2010-06-29 12:54:07Z

I need to write code in python that will delete the required file from an Amazon s3 bucket. I am able to connect to the Amazon s3 bucket, and also to save files, but how can I delete a file?found one more way to do it using the boto:Using boto3 (currently version 1.4.4) use S3.Object.delete().Using the Python boto3 SDK (and assuming credentials are setup for AWS), the following will delete a specified object in a bucket:I'm surprised there isn't this easy way : key.delete() :Via which interface? Using the REST interface, you just send a delete:Via the SOAP interface:If you're using a Python library like boto, it should expose a "delete" feature, like delete_key().For now I have resolved the issue by using the Linux utility s3cmd. I used it like this in Python:It's worked for me try it.you can do it using aws cli : https://aws.amazon.com/cli/ and some unix command.this aws cli commands should work:if you want to include sub-folders you should add the flag --recursiveor with unix commands:explanation:Try to look for an updated method, since Boto3 might change from time to time. I used my_bucket.delete_objects():if you are trying to delete file using your own local host console then you can try running this python script assuming that you have have already assigned your access id and secret key in the system

Splitting dataframe into multiple dataframes

Martin Petri Bagger

[Splitting dataframe into multiple dataframes](https://stackoverflow.com/questions/19790790/splitting-dataframe-into-multiple-dataframes)

I have a very large dataframe (around 1 million rows) with data from an experiment (60 respondents).

I would like to split the dataframe into 60 dataframes (a dataframe for each participant). In the dataframe (called = data) there is a variable called 'name' which is the unique code for each participant.I have tried the following, but nothing happens (or the does not stop within an hour). What I intend to do is to split the dataframe (data) into smaller dataframes and append these to a list (datalist):I do not get an error message, the script just seems to run forever!Is there a smart way to do it?

2013-11-05 14:01:13Z

I have a very large dataframe (around 1 million rows) with data from an experiment (60 respondents).

I would like to split the dataframe into 60 dataframes (a dataframe for each participant). In the dataframe (called = data) there is a variable called 'name' which is the unique code for each participant.I have tried the following, but nothing happens (or the does not stop within an hour). What I intend to do is to split the dataframe (data) into smaller dataframes and append these to a list (datalist):I do not get an error message, the script just seems to run forever!Is there a smart way to do it?Firstly your approach is inefficient because the appending to the list on a row by basis will be slow as it has to periodically grow the list when there is insufficient space for the new entry, list comprehensions are better in this respect as the size is determined up front and allocated once.However, I think fundamentally your approach is a little wasteful as you have a dataframe already so why create a new one for each of these users?I would sort the dataframe by column 'name', set the index to be this and if required not drop the column.Then generate a list of all the unique entries and then you can perform a lookup using these entries and crucially if you only querying the data, use the selection critieria to return a view on the dataframe without incurring a costly data copy.So:sort is now deprecated, you need to use sort_values now:Can I ask why not just do it by slicing the data frame. Something likeHey presto you have a dictionary of data frames just as (I think) you want them. Need to access one? Just enterHope that helpsYou can convert groupby object to tuples and then to dict:It is not recommended, but possible create DataFrames by groups:Groupby can helps you:Then you can work with each group like with a dataframe for each participant. And DataFrameGroupBy object methods such as (apply, transform, aggregate, head, first, last) return a DataFrame object.Or you can make list from grouped and get all DataFrame's by index:l_grouped[0][1] - DataFrame for first group with first name.Easy:In addition to Gusev Slava's answer, you might want to use groupby's groups:This will yield a dictionary with the keys you have grouped by, pointing to the corresponding partitions. The advantage is that the keys are maintained and don't vanish in the list index.Here's a groupby way (and you could do an arbitrary apply rather than sum)Sum is cythonized that's why this is so fastThe method based on list comprehension and groupby- Which stores all the split dataframe in list variable and can be accessed using the index.ExampleYou can use the groupby command, if you already have some labels for your data. Here's a detailed example:Let's say we want to partition a pd series using some labels into a list of chunks

For example, in_series is:And its corresponding label_series is:Run which returns out_list a list of two pd.Series:Note that you can use some parameters from in_series itself to group the series, e.g., in_series.index.dayI had similar problem. I had a time series of daily sales for 10 different stores and 50 different items. I needed to split the original dataframe in 500 dataframes (10stores*50stores) to apply Machine Learning models to each of them and I couldn't do it manually.This is the head of the dataframe:I have created two lists;

one for the names of dataframes

and one for the couple of array [item_number, store_number].And once the two lists are ready you can loop on them to create the dataframes you want:In this way I have created 500 dataframes.Hope this will be helpful!

subprocess.check_output() doesn't seem to exist (Python 2.6.5)

robintw

[subprocess.check_output() doesn't seem to exist (Python 2.6.5)](https://stackoverflow.com/questions/4814970/subprocess-check-output-doesnt-seem-to-exist-python-2-6-5)

I've been reading the Python documentation about the subprocess module (see here) and it talks about a subprocess.check_output() command which seems to be exactly what I need.However, when I try and use it I get an error that it doesn't exist, and when I run dir(subprocess) it is not listed.I am running Python 2.6.5, and the code I have used is below:Does anyone have any idea why this is happening?

2011-01-27 10:05:43Z

I've been reading the Python documentation about the subprocess module (see here) and it talks about a subprocess.check_output() command which seems to be exactly what I need.However, when I try and use it I get an error that it doesn't exist, and when I run dir(subprocess) it is not listed.I am running Python 2.6.5, and the code I have used is below:Does anyone have any idea why this is happening?It was introduced in 2.7 See the docs.Use subprocess.Popen if you want the output:IF it's used heavily in the code you want to run but that code doesn't have to be maintained long-term (or you need a quick fix irrespective of potential maintenance headaches in the future) then you could duck punch (aka monkey patch) it in wherever subprocess is imported...Just lift the code from 2.7 and insert it thusly...Minor fidgeting may be required.Do bear in mind though the onus is on you to maintain dirty little backports like this. If bugs are discovered and corrected in the latest python then you a) have to notice that and b) update your version if you want to stay secure. Also, overriding & defining internal functions yourself is the next guy's worst nightmare, especially when the next guy is YOU several years down the line and you've forgot all about the grody hacks you did last time! In summary: it's very rarely a good idea.Thanks to the monkey patch suggestion (and my attempts failing - but we were consuming CalledProcessError output, so needed to monkey patch that) found a working 2.6 patch here:

http://pydoc.net/Python/pep8radius/0.9.0/pep8radius.shell/

Is it possible to break a long function name across multiple lines?

byxor

[Is it possible to break a long function name across multiple lines?](https://stackoverflow.com/questions/40955302/is-it-possible-to-break-a-long-function-name-across-multiple-lines)

Our development team uses a PEP8 linter which requires a maximum line length of 80 characters.When I'm writing unit tests in python, I like to have descriptive method names to describe what each test does. However this often leads to me exceeding the character limit.Here is an example of a function that is too long...Is there a way in Python to split a long function declaration across multiple lines?For example...Or will I have to bite the bullet and shorten it myself?

2016-12-04 04:12:10Z

Our development team uses a PEP8 linter which requires a maximum line length of 80 characters.When I'm writing unit tests in python, I like to have descriptive method names to describe what each test does. However this often leads to me exceeding the character limit.Here is an example of a function that is too long...Is there a way in Python to split a long function declaration across multiple lines?For example...Or will I have to bite the bullet and shorten it myself?No, this is not possible.In most cases such a long name would be undesirable from the standpoint of readability and usability of the function, though your use case for test names seems pretty reasonable.The lexical rules of Python do not allow a single token (in this case an identifier) to be split across multiple lines. The logical line continuation character (\ at the end of a line) can join multiple physical lines into a single logical line, but cannot join a single token across multiple lines.You could also write a decorator that mutates .__name__ for the method.Then you could write:relying on the fact that Python concatenates source-adjacent string literals.Per the answer to this question:How to disable a pep8 error in a specific file?, use the # nopep8 or # noqa trailing comment to disable PEP-8 for a long line. It's important to know when to break the rules. Of course, the Zen of Python would tell you that "Special cases aren't special enough to break the rules."We can applying decorator to the class instead of method since unittest get methods name from dir(class).The decorator decorate_method will go through class methods and rename method's name based on func_mapping dictionary.Thought of this after seeing decorator answer from @Sean Vieira , +1 from metest run with unittest as below did show the full long descriptive function name, thinks it might works for your case though it may not sounds so elegant

and readable from the implementationSort of a context-specific approach to the problem. The test case you've presented actually looks very much like a Natural Language format of describing the necessary steps for a test case to take. See if using the behave Behavior Driver development style framework would make more sense here. Your "feature" might look like (see how the given, when, then reflect what you had):  There is also relevant pyspecs package, sample usage from a recent answer on a related topic:The need for this kind of names may hint at other smells.ClientConnectionTest sounds pretty broad (and not at all like a testable unit), and is likely a large class with plenty of tests inside that could be refocused. Like this:"Test" is not useful in the name because it's implied.With all the code you've given me, my final advice is: refactor your test code, then revisit your problem (if it's still there).The shorter function name solution has a lot of merit.  Think about what is really needed in your actual function name and what is supplied already.Surely you already know it's a test when you run it?  Do you really need to use underscores? are words like 'that' really required for the name to be understood? would camel case be just as readable? how about the first example below as a rewriting of the above (character count = 79):

  Accepting a convention to use abbreviations for a small collection of common words is even more effective, e.g. Connection = Conn, Error = Err.  When using abbreviations you have to be mindful of the context and only use them when there is no possiblity of confusion - Second example below.

  If you accept that there's no actual need to mention the client as the test subject in the method name as that information is in the class name then the third example may be appropriate. (54) characters.ClientEventListenerReceivesConnectionRefusedErrorWithoutServer(self):ClientEventListenerReceivesConnRefusedErrWithoutServer(self):EventListenerReceiveConnRefusedErrWithoutServer(self):I'd also agree with the the suggestion from B Rad C "use descriptive name as the msg kwarg arg in in a self.assert" You should only be interested in seeing output from failed tests when the testsuite is run.  Verification that you have all the necessary tests written shouldn't depend on having the method names so detailed.P.S. I'd probably also remove 'WithoutServer' as superfluous as well.  Shouldn't the client event handler receive the event in the case that the server isn't contactable for any reason? (although tbh I'd think that it would be better that if they client can't connect to a server it receives some sort of 'connection unavailable' , connection refused suggests that the server can be found but refuses the connection itself.)

How to git commit nothing without an error?

kojiro

[How to git commit nothing without an error?](https://stackoverflow.com/questions/8123674/how-to-git-commit-nothing-without-an-error)

I'm trying to write a fabric script that does a git commit; however, if there is nothing to commit, git exits with a status of 1. The deploy script takes that as unsuccessful, and quits. I do want to detect actual failures-to-commit, so I can't just give fabric a blanket ignore for git commit failures. How can I allow empty-commit failures to be ignored so that deploy can continue, but still catch errors caused when a real commit fails?

2011-11-14 15:13:49Z

I'm trying to write a fabric script that does a git commit; however, if there is nothing to commit, git exits with a status of 1. The deploy script takes that as unsuccessful, and quits. I do want to detect actual failures-to-commit, so I can't just give fabric a blanket ignore for git commit failures. How can I allow empty-commit failures to be ignored so that deploy can continue, but still catch errors caused when a real commit fails?Catch this condition beforehand by checking the exit code of git diff?For example (in shell):EDIT: Fixed git diff command according to Holger's comment.From the git commit man page:This causes fabric to ignore the failure. Has the advantage of not creating empty commits.You can wrap it in a additional layer of with hide('warnings'): to totally suppress output, otherwise you'll get a note in the fabric output that the commit failed (but the fabfile continues to execute).  try/catch baby!

Argmax of numpy array returning non-flat indices

Andreas Mueller

[Argmax of numpy array returning non-flat indices](https://stackoverflow.com/questions/9482550/argmax-of-numpy-array-returning-non-flat-indices)

I'm trying to get the indices of the maximum element in a Numpy array.

This can be done using numpy.argmax. My problem is, that I would like to find the biggest element in the whole array and get the indices of that.numpy.argmax can be either applied along one axis, which is not what I want, or on the flattened array, which is kind of what I want.My problem is that using numpy.argmax with axis=None returns the flat index when I want the multi-dimensional index.I could use divmod to get a non-flat index but this feels ugly. Is there any better way of doing this?

2012-02-28 13:12:04Z

I'm trying to get the indices of the maximum element in a Numpy array.

This can be done using numpy.argmax. My problem is, that I would like to find the biggest element in the whole array and get the indices of that.numpy.argmax can be either applied along one axis, which is not what I want, or on the flattened array, which is kind of what I want.My problem is that using numpy.argmax with axis=None returns the flat index when I want the multi-dimensional index.I could use divmod to get a non-flat index but this feels ugly. Is there any better way of doing this?You could use numpy.unravel_index() on the result of numpy.argmax():returns coordinates of the maximum element(s), but has to parse the array twice.This, comparing to argmax, returns coordinates of all elements equal to the maximum. argmax returns just one of them (np.ones(5).argmax() returns 0).To get the non-flat index of all occurrences of the maximum value, you can modify eumiro's answer slightly by using argwhere instead of where:

Pandas sum by groupby, but exclude certain columns

user308827

[Pandas sum by groupby, but exclude certain columns](https://stackoverflow.com/questions/32751229/pandas-sum-by-groupby-but-exclude-certain-columns)

What is the best way to do a groupby on a Pandas dataframe, but exclude some columns from that groupby? e.g. I have the following dataframe:I want to groupby the column Country and Item_Code and only compute the sum of the rows falling under the columns Y1961, Y1962 and Y1963. The resulting dataframe should look like this:Right now I am doing this:However this adds up the values in the Item_Code column as well. Is there any way I can specify which columns to include in the sum() operation and which ones to exclude?

2015-09-23 23:45:53Z

What is the best way to do a groupby on a Pandas dataframe, but exclude some columns from that groupby? e.g. I have the following dataframe:I want to groupby the column Country and Item_Code and only compute the sum of the rows falling under the columns Y1961, Y1962 and Y1963. The resulting dataframe should look like this:Right now I am doing this:However this adds up the values in the Item_Code column as well. Is there any way I can specify which columns to include in the sum() operation and which ones to exclude?You can select the columns of a groupby:Note that the list passed must be a subset of the columns otherwise you'll see a KeyError.The agg function will do this for you.  Pass the columns and function as a dict with column, output:This will display only the group by columns, and the specified aggregate columns.  In this example I included two agg functions applied to 'Y1962'.To get exactly what you hoped to see, included the other columns in the group by, and apply sums to the Y variables in the frame:If you are looking for a more generalized way to apply to many columns, what you can do is to build a list of column names and pass it as the index of the grouped dataframe. In your case, for example:

Nested For Loops Using List Comprehension

John Howard

[Nested For Loops Using List Comprehension](https://stackoverflow.com/questions/3633140/nested-for-loops-using-list-comprehension)

If I had two strings, 'abc' and 'def', I could get all combinations of them using two for loops:However, I would like to be able to do this using list comprehension. I've tried many ways, but have never managed to get it. Does anyone know how to do this?

2010-09-03 04:57:45Z

If I had two strings, 'abc' and 'def', I could get all combinations of them using two for loops:However, I would like to be able to do this using list comprehension. I've tried many ways, but have never managed to get it. Does anyone know how to do this?orif you want tuples.Like in the question, for j... is the outer loop, for k... is the inner loop.Essentially, you can have as many independent 'for x in y' clauses as you want in a list comprehension just by sticking one after the other.Since this is essentially a Cartesian product, you can also use itertools.product. I think it's clearer, especially when you have more input iterables.Try recursion too:Gives you the 8 combinations:

many-to-many in list display django

Mdjon26

[many-to-many in list display django](https://stackoverflow.com/questions/18108521/many-to-many-in-list-display-django)

I have that code. Unfortunately, the error comes in admin.py with the ManyToManyFieldThe error says:However, it compiles when I take 'product' out of list_display. So how can I display 'product' in list_display without giving it errors?edit: Maybe a better question would be how do you display a ManyToManyField in list_display?

2013-08-07 16:18:22Z

I have that code. Unfortunately, the error comes in admin.py with the ManyToManyFieldThe error says:However, it compiles when I take 'product' out of list_display. So how can I display 'product' in list_display without giving it errors?edit: Maybe a better question would be how do you display a ManyToManyField in list_display?You may not be able to do it directly. From the documentation of list_displayYou can do something like this:OR define a model method, and use thatand in the admin list_displayThis way you can do it, kindly checkout the following snippet: And in your admin.py module call method as follows:

What happens when you assign the value of one variable to another variable in Python?

Ruslan Mushkaev

[What happens when you assign the value of one variable to another variable in Python?](https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py)

This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.Here is how I understand them currently:Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?

2017-07-12 09:15:48Z

This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.Here is how I understand them currently:Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?As a C++ developer you can think of Python variables as pointers.Thus when you write spam = 100, this means that you "assign the pointer", which was previously pointing to the object 42, to point to the object 100.Earlier on, cheese was assigned to point to the same object as spam pointed to, which happened to be 42 at that time. Since you have not modified cheese, it still points to 42.Immutability has nothing to do with it in this case, since pointer assignment does not change anything about the object being pointed to.The way I see it there are different views of a language.From the language lawyer perspective python variables always "point at" an object. However unlike Java and C++ the behvaiour of == <= >= etc depends on the runtime type of the objects that the variables point at. Furthermore in python memory management is handled by the language.From a practical programmer perspective we can treat the fact that integers, strings, tuples etc are immutable* objects rather than straight values as an irrelevent detail. The exception is when storing large ammounts of numeric data we may want to use types that can store the values directly (e.g. numpy arrays) rather than types that will end up with an array full of references to tiny objects.From an implementers perspective most languages have some sort of as-if rule such that if the specified behaviours are correct the implementation is correct regardless of how things are actually done under the hood.So yes your explanation is correct from a language lawyer perspective. Your book is correct from a practical programmer perspective. What an implementation actually does depends on the implementation. In cpython integers are real objects though small value integers are taken from a cache pool rather than created anew. I'm not sure what the other implementations (e.g. pypy and jython) do.* note the distinction between mutable and immutable objects here. With a mutable object we have to be careful about treating it "like a value" because some other code might mutate it. With an immutable object we have no such concerns.It is correct you can more or less thing of variables as pointers. However example code would help greatly with explaining how this actually is working.First, we will heavily utilize the id function:It's likely this will return different absolute values on your machine.Consider this example:You can see that:when we change the value of foo, it is assigned to a different id:An interesting observation too is that integers implicitly have this functionality up to 256:However beyond 256 this is no longer true:however assigning a to b will indeed keep the id the same as shown before:Python is neither pass-by-reference or pass-by-value. Python variables are not pointers, they are not references, they are not values. Python variables are names.Think of it as "pass-by-alias" if you need the same phrase type, or possibly "pass-by-object", because you can mutate the same object from any variable that indicates it, if it's mutable, but reassignment of a variable (alias) only changes that one variable.A Python variable's name is a key in the global (or local) namespace, which is effectively a dictionary. The underlying value is some object in memory. Assignment gives a name to that object. Assignment of one variable to another variable means both variables are names for the same object. Re-assignment of one variable changes what object is named by that variable without changing the other variable. You've moved the tag but not changed the previous object or any other tags on it.In the underlying C code of the CPython implementation, every Python object is a PyObject*, so you can think of it as working like C if you only ever had pointers to data (no pointers-to-pointers, no directly-passed values).When you run spam = 100 python create one more object in the memory but not change existing. so you still have pointer cheese to 42 and spam to 100What is happening in spam = 100 line is replacement of previous value (pointer to object of type int with value 42) with another pointer to another object (type int, value 100)As @DeepSpace mentioned in the comments, Ned Batchelder does a great job demystifying variables (names) and assignments to values in a blog, from which he delivered a talk at PyCon 2015, Facts and Myths about Python names and values.  It can be insightful for Pythonistas at any level of mastery.In Python, a variable holds the reference to the object.  An object is a chunk of allocated memory that holds a value and a header. Object's header contains its type and a reference counter that denotes the amount of times this object is referenced in the source code so that Garbage Collection can identify whether an object can be collected.Now when you assign values to a variable, Python actually assigns references which are pointers to memory locations allocated to objects:Now when you assign objects of different type to the same variable, you actually change the reference so that it points to a different object (i.e. different memory location). By the time you assign a different reference (and thus object) to a variable, the Garbage Collector will immediately reclaim the space allocated to the previous object, assuming that it is not being referenced by any other variable in the source code:Coming to your example now, spam holds the reference to object(type=int, value=42, refCounter=1):Now cheese will also hold the reference to object(type=int, value=42, refCounter=2)Now spam holds a reference to a different object(type=int, value=100, refCounter=1)But cheese will keep pointing to object(type=int, value=42, refCounter=1)When you store spam = 42 , it creates an object in the memory. Then you assign cheese = spam , It assigns the object referenced by spam to cheese. And finally, when you change spam = 100, it changes only spam object. So cheese = 42.  numpy.copy() function page has an explanation https://docs.scipy.org/doc/numpy/reference/generated/numpy.copy.htmlThe example it gives is as follows:Create an array x, with a reference y and a copy z:Note that, when we modify x, y changes, but not z:

Differences between node.js and Tornado [closed]

coffee-grinder

[Differences between node.js and Tornado [closed]](https://stackoverflow.com/questions/5561701/differences-between-node-js-and-tornado)

Besides the fact that node.js is written in JS and Tornado in Python, what are some of the differences between the two?  They're both non-blocking asynchronous web servers, right?  Why choose one over the other besides the language?

2011-04-06 05:06:40Z

Besides the fact that node.js is written in JS and Tornado in Python, what are some of the differences between the two?  They're both non-blocking asynchronous web servers, right?  Why choose one over the other besides the language?The main advantage of node.js is that all its libraries are async so you don't have to worry much about blocking. There are async libraries for mysql, postgres, redis, etc. All is async by default.Python have a library for anything - but most of these libraries are not asynchronous. In order to take advantage of tornado (and not to block the process) special libraries for are necessary (e.g. you can't just 'pip install redis' and use it, you'll need something like brukva), and there are much less tornado libraries than node.js libraries. There is no async mysql tornado driver available at the moment, for example (or at least I'm not aware of it).But you can still use many python libraries with tornado (ones that doesn't do i/o), and tornado community is raising and filling the gaps. It is easier to write an app using node.js than using tornado in my experience. I personally switched to tornado from node.js because it fits into existing infrastructure of my python project better (integration between django site serving html pages and tornado server providing realtime features was quite painless). As Rich Bradshaw points out Node.js is written in JS, which means you can keep the front end and the back end in the same language and possibly share some codebase. To me that is a huge potential benefit of Node.js.

Node also comes with more asynchronous libraries out of the box it seems.V8 should make JS faster than Python at least that's what benchmarks seem to suggest, but it may not matter much, because both Node.js and Tornado (and most other web frameworks for that matter) use wrappers for native libraries. A lot of the Python standard library is written in C or can be replaced by a faster alternative, which mitigates potential differences even more.Web services are usually I/O bound, so that means we're spending the time waiting for the data store and not processing the data. That makes the synthetic speed difference between JS and Python irrelevant in many applications.node.js uses V8 which compiles into assembly code, tornado doesn't do that yet.Other than that (which doesn't actually seem to make much difference to the speed), it's the ecosystem. Do you prefer the event model of JS, or the way Python works? Are you happier using Python or JS libraries?Nodejs also has a seamless integration / implementation of websockets called Socket.io. It handles browsers supporting sockets - events and also has backward polling compatibility for older browsers. It is quite quick on development requiring a notification framework or some similar event based programming.I would suggested you go with NodeJS, if there is no personal pref to python. I like Python a lot, but for async I choose Tornado over node, and later had to struggle finding way to do a thing, or libraries with async support (like Cassandra has async in tests, but nowhere could I find way to use cqlengine with async. Had to choose Mongo since I already surpassed the deadline).

In terms of performance and async, Node far better than tornado.

How can I install from a git subdirectory with pip?

J. Martinot-Lagarde

[How can I install from a git subdirectory with pip?](https://stackoverflow.com/questions/13566200/how-can-i-install-from-a-git-subdirectory-with-pip)

I have a git repository with many folders, one of them being a python module installable with pip, like this:Right now I have to do the following to install:Is it possible to install the module directly with pip without explicitly cloning ?I tried:But I get:

2012-11-26 13:49:53Z

I have a git repository with many folders, one of them being a python module installable with pip, like this:Right now I have to do the following to install:Is it possible to install the module directly with pip without explicitly cloning ?I tried:But I get:There is a pull request regarding this feature, and it seems to have been merged to develop branch a month ago. The syntax is the following:We probably have to wait for a while until it gets merged to master and is distributed.UPDATE: This is now available and documented at https://pip.readthedocs.io/en/stable/reference/pip_install/#vcs-support as follows:Note: On Windows, you must place the URL in double quotes, or you'll get an error "'subdirectory' is not recognized as an internal or external command". E.g., use: It's been already stated in one of the comments under the correct answer, but just to highlight this issue: when executing this from Linux command line, you must escape the &-character since ampersand is telling the command line to run a command in background:Notice the backslash before the ampersand. The escaping behaviour might depend on the Linux distro; I'm not an expert.

If you ignore this, you might run into a cryptic error like the following:

Getting started with the Python debugger, pdb [closed]

Matthew Rankin

[Getting started with the Python debugger, pdb [closed]](https://stackoverflow.com/questions/4228637/getting-started-with-the-python-debugger-pdb)

I want to add pdb—the Python debugger—to my toolbox. What's the best way to get started?

2010-11-19 19:28:39Z

I want to add pdb—the Python debugger—to my toolbox. What's the best way to get started?Here's a list of resources to get started with the Python debugger:Synopsis:Now run your script:

Where do I find the bashrc file on Mac?

pencilVester

[Where do I find the bashrc file on Mac?](https://stackoverflow.com/questions/19662713/where-do-i-find-the-bashrc-file-on-mac)

Hello I am following this page.. I'm installing Python onto my mac so that I can set up a Django / Eclipse development environment.

However I am not too sure how to go about executing this step:Where do I find the bashrc file on my mac and where do I find the homebrew directory?I am running a macbook pro with OS 10.8.5.

2013-10-29 15:36:13Z

Hello I am following this page.. I'm installing Python onto my mac so that I can set up a Django / Eclipse development environment.

However I am not too sure how to go about executing this step:Where do I find the bashrc file on my mac and where do I find the homebrew directory?I am running a macbook pro with OS 10.8.5.The .bashrc file is in your home directory.So from command line do:This will show all the hidden files in your home directory. "cd" will get you home and ls -a will "list all".In general when you see ~/ the tilda slash refers to your home directory. So ~/.bashrc is your home directory with the .bashrc file.And the standard path to homebrew is in /usr/local/ so if you:you should see the homebrew directory (/usr/local/homebrew). SourceYes sometimes you may have to create this file and the typical format of a .bashrc file is:If you create your own .bashrc file make sure that the following line is in your ~/.bash_profileI would think you should add it to ~/.bash_profile instead of .bashrc, (creating .bash_profile if it doesn't exist.) Then you don't have to add the extra step of checking for ~/.bashrc in your .bash_profileAre you comfortable working and editing in a terminal? Just in case, ~/ means your home directory, so if you open a new terminal window that is where you will be "located". And the dot at the front makes the file invisible to normal ls command, unless you put -a or specify the file name.Check this answer for more detail. ~/.bashrc is already a path to .bashrc.If you do echo ~ you'll see that it's a path to your home directory.Homebrew directory is /usr/local/bin. Homebrew is installed inside it and everything installed by homebrew will be installed there.For example, if you do brew install python Homebrew will put Python binary in /usr/local/bin.Finally, to add Homebrew directory to your path you can run echo "export PATH=/usr/local/lib:$PATH" >> ~/.bashrc. It will create .bashrc file if it doesn't exist and then append the needed line to the end. You can check the result by running tail ~/.bashrc.Open Terminal and execute commands given below.subl denotes Sublime editor. You can replace subl with vi to open bashrc file in default editor. This will workout only if you have bashrc file, created earlier.On some system, instead of the .bashrc file, you can edit your profils' specific by editing:

random.choice from set? python

jamyn

[random.choice from set? python](https://stackoverflow.com/questions/15837729/random-choice-from-set-python)

I'm working on an AI portion of a guessing game. I want the AI to select a random letter from this list. I'm doing it as a set so I can easily remove letters from the list as they are guessed in the game and are therefore no longer available to be guessed again.it says set object isn't indexable. How can I work around this?

2013-04-05 15:26:03Z

I'm working on an AI portion of a guessing game. I want the AI to select a random letter from this list. I'm doing it as a set so I can easily remove letters from the list as they are guessed in the game and are therefore no longer available to be guessed again.it says set object isn't indexable. How can I work around this?Documentation: https://docs.python.org/3/library/random.html#random.sampleYou should use random.choice(tuple(myset)), because it's faster and arguably cleaner looking than random.sample.  I wrote the following to test:From the numbers it seems that random.sample takes 7% longer.

Is there any way to use pythonappend with SWIG's new builtin feature?

Mike

[Is there any way to use pythonappend with SWIG's new builtin feature?](https://stackoverflow.com/questions/9270052/is-there-any-way-to-use-pythonappend-with-swigs-new-builtin-feature)

I have a little project that works beautifully with SWIG.  In particular, some of my functions return std::vectors, which get translated to tuples in Python.  Now, I do a lot of numerics, so I just have SWIG convert these to numpy arrays after they're returned from the c++ code.  To do this, I use something like the following in SWIG.(Actually, there are several functions named Data, some of which return floats, which is why I check that val is actually a tuple.)  This works just beautifully.But, I'd also like to use the -builtin flag that's now available.  Calls to these Data functions are rare and mostly interactive, so their slowness is not a problem, but there are other slow loops that speed up significantly with the builtin option.The problem is that when I use that flag, the pythonappend feature is silently ignored.  Now, Data just returns a tuple again.  Is there any way I could still return numpy arrays?  I tried using typemaps, but it turned into a giant mess.Borealid has answered the question very nicely.  Just for completeness, I include a couple related but subtly different typemaps that I need because I return by const reference and I use vectors of vectors (don't start!).  These are different enough that I wouldn't want anyone else stumbling around trying to figure out the minor differences.Though not quite what I was looking for, similar problems may also be solved using @MONK's approach (explained here).

2012-02-14 00:19:20Z

I have a little project that works beautifully with SWIG.  In particular, some of my functions return std::vectors, which get translated to tuples in Python.  Now, I do a lot of numerics, so I just have SWIG convert these to numpy arrays after they're returned from the c++ code.  To do this, I use something like the following in SWIG.(Actually, there are several functions named Data, some of which return floats, which is why I check that val is actually a tuple.)  This works just beautifully.But, I'd also like to use the -builtin flag that's now available.  Calls to these Data functions are rare and mostly interactive, so their slowness is not a problem, but there are other slow loops that speed up significantly with the builtin option.The problem is that when I use that flag, the pythonappend feature is silently ignored.  Now, Data just returns a tuple again.  Is there any way I could still return numpy arrays?  I tried using typemaps, but it turned into a giant mess.Borealid has answered the question very nicely.  Just for completeness, I include a couple related but subtly different typemaps that I need because I return by const reference and I use vectors of vectors (don't start!).  These are different enough that I wouldn't want anyone else stumbling around trying to figure out the minor differences.Though not quite what I was looking for, similar problems may also be solved using @MONK's approach (explained here).I agree with you that using typemap gets a little messy, but it is the right way to accomplish this task. You are also right that the SWIG documentation does not directly say that %pythonappend is incompatible with -builtin, but it is strongly implied: %pythonappend adds to the Python proxy class, and the Python proxy class does not exist at all in conjunction with the -builtin flag.Before, what you were doing was having SWIG convert the C++ std::vector objects into Python tuples, and then passing those tuples back down to numpy - where they were converted again.What you really want to do is convert them once, at the C level.Here's some code which will turn all std::vector<int> objects into NumPy integer arrays:This uses the C-level numpy functions to construct and return an array. In order, it:This code should be placed before you %import the headers which contain the functions returning std::vector<int>. Other than that restriction, it's entirely self-contained, so it shouldn't add too much subjective "mess" to your codebase.If you need other vector types, you can just change the NPY_INT and all the int* and int bits, otherwise duplicating the function above.

How to convert column with dtype as object to string in Pandas Dataframe [duplicate]

user3546523

[How to convert column with dtype as object to string in Pandas Dataframe [duplicate]](https://stackoverflow.com/questions/33957720/how-to-convert-column-with-dtype-as-object-to-string-in-pandas-dataframe)

When I read a csv file to pandas dataframe, each column is cast to its own datatypes. I have a column that was converted to an object. I want to perform string operations for this column such as splitting the values and creating a list. But no such operation is possible because its dtype is object. Can anyone please let me know the way to convert all the items of a column to strings instead of objects?I tried several ways but nothing worked. I used astype, str(), to_string etc.

2015-11-27 12:43:50Z

When I read a csv file to pandas dataframe, each column is cast to its own datatypes. I have a column that was converted to an object. I want to perform string operations for this column such as splitting the values and creating a list. But no such operation is possible because its dtype is object. Can anyone please let me know the way to convert all the items of a column to strings instead of objects?I tried several ways but nothing worked. I used astype, str(), to_string etc.since strings data types have variable length, it is by default stored as object dtype. If you want to store them as string type, you can do something like this.or alternativelyDid you try assigning it back to the column?Referring to this question, the pandas dataframe stores the pointers to the strings and hence it is of type 

'object'. As per the docs ,You could try:   Not answering the question directly, but it might help someone else.I have a column called Volume, having both - (invalid/NaN) and numbers formatted with ,Casting to string is required for it to apply to str.replacepandas.Series.str.replace

pandas.to_numericYou could try using df['column'].str. and then use any string function. Pandas documentation includes those like splitPlease use df.to_string()Reference linkhttp://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.to_string.html

Securely storing passwords for use in python script [duplicate]

user1598386

[Securely storing passwords for use in python script [duplicate]](https://stackoverflow.com/questions/12042724/securely-storing-passwords-for-use-in-python-script)

I am looking for a way to securely store passwords which I intend to use in some Python scripting. I will be logging into different things and I don't want to store the passwords as plaintext in the script itself.Instead I was wondering if there is anything which is able to securely store those passwords and then retrieve them using something like a master password which I could enter to the script at the beginning.

2012-08-20 18:05:03Z

I am looking for a way to securely store passwords which I intend to use in some Python scripting. I will be logging into different things and I don't want to store the passwords as plaintext in the script itself.Instead I was wondering if there is anything which is able to securely store those passwords and then retrieve them using something like a master password which I could enter to the script at the beginning.Know the master key yourself. Don't hard code it. Use py-bcrypt (bcrypt), powerful hashing technique to generate a password yourself.Basically you can do this (an idea...)save salt and hashed password somewhere so whenever you need to use the password, you are reading the encrypted password, and test against the raw password you are entering again.This is basically how login should work these days.I typically have a secrets.py that is stored separately from my other python scripts and is not under version control. Then whenever required, you can do from secrets import <required_pwd_var>. This way you can rely on the operating systems in-built file security system without re-inventing your own.Using Base64 encoding/decoding is also another way to obfuscate the password though not completely secureMore here - Hiding a password in a python script (insecure obfuscation only)the secure way is encrypt your sensitive data by AES and the encryption key is derivation by password-based key derivation function (PBE), the master password used to encrypt/decrypt the encrypt key for AES.You can use pbkdf2make sure to store the salt/iv/passphrase , and decrypt using same salt/iv/passphaseWeblogic used similar approach to protect passwords in config files

Add column in dataframe from list

mane

[Add column in dataframe from list](https://stackoverflow.com/questions/26666919/add-column-in-dataframe-from-list)

I have a dataframe with some columns like this:The possible range of values in A are only from 0 to 7. Also, I have a list of 8 elements like this:If the element in column A is n, I need to insert the n th element from the List in a new column, say 'D'.How can I do this in one go without looping over the whole dataframe? The resulting dataframe would look like this:Note: The dataframe is huge and iteration is the last option option. But I can also arrange the elements in 'List' in any other data structure like dict if necessary.

2014-10-31 03:00:30Z

I have a dataframe with some columns like this:The possible range of values in A are only from 0 to 7. Also, I have a list of 8 elements like this:If the element in column A is n, I need to insert the n th element from the List in a new column, say 'D'.How can I do this in one go without looping over the whole dataframe? The resulting dataframe would look like this:Note: The dataframe is huge and iteration is the last option option. But I can also arrange the elements in 'List' in any other data structure like dict if necessary.IIUC, if you make your (unfortunately named) List into an ndarray, you can simply index into it naturally.Here I built a new m, but if you use m = np.asarray(List), the same thing should work: the values in df.A will pick out the appropriate elements of m.Note that if you're using an old version of numpy, you might have to use m[df.A.values] instead-- in the past, numpy didn't play well with others, and some refactoring in pandas caused some headaches.  Things have improved now.Just assign the list directly:Alternative

Convert the list to a series or array and then assign:orA solution improving on the great one from @sparrow.Let df, be your dataset, and mylist the list with the values you want to add to the dataframe.Let's suppose you want to call your new column simply, new_columnFirst make the list into a Series:Then use the insert function to add the column. This function has the advantage to let you choose in which position you want to place the column.

In the following example we will position the new column in the first position from left (by setting loc=0)First let's create the dataframe you had, I'll ignore columns B and C as they are not relevant.And the mapping that you desire:Done!Output:Old question; but I always try to use fastest code!I had a huge list with 69 millions of uint64. np.array() was fastest for me.

Random is barely random at all?

orokusaki

[Random is barely random at all?](https://stackoverflow.com/questions/2145510/random-is-barely-random-at-all)

I did this to test the randomness of randint:I tried about 10 times more and the best result I got was 121 iterations before a repeater. Is this the best sort of result you can get from the standard library?

2010-01-27 08:50:51Z

I did this to test the randomness of randint:I tried about 10 times more and the best result I got was 121 iterations before a repeater. Is this the best sort of result you can get from the standard library?There are a couple of issues at play in the OP's problem.  One is the birthday paradox as mentioned above and the second is the nature of what you are generating, which does not inherently guarantee that a given number will not be repeated.  The Birthday Paradox applies where given value can occur more than once during the period of the generator - and therefore duplicates can happen within a sample of values.  The effect of the Birthday Paradox is that the real likelihood of getting such duplicates is quite significant and the average period between them is smaller than one might otherwise have thought.  This dissonance between the perceived and actual probabilities makes the Birthday Paradox a good example of a cognitive bias, where a naive intuitive estimate is likely to be wildly wrong.A quick primer on Pseudo Random Number Generators (PRNGs)The first part of your problem is that you are taking the exposed value of a random number generator and converting it to a much smaller number, so the space of possible values is reduced.  Although some pseudo-random number generators do not repeat values during their period this transformation changes the domain to a much smaller one.  The smaller domain invalidates the 'no repeats' condition so you can expect a significant likelihood of repeats.   Some algorithms, such as the linear congruential PRNG (A'=AX|M) do guarantee uniqueness for the entire period.  In an LCG the generated value contains the entire state of the accumulator and no additional state is held.  The generator is deterministic and cannot repeat a number within the period - any given accumulator value can imply only one possible successive value.  Therefore, each value can only occur once within the period of the generator.  However, the period of such a PRNG is relatively small - about 2^30 for typical implementations of the LCG algorithm - and cannot possibly be larger than the number of distinct values.Not all PRNG algorithms share this characteristic; some can repeat a given value within the period.  In the OP's problem, the Mersenne Twister algorithm (used in Python's random module) has a very long period - much greater than 2^32.   Unlike a Linear Congruential PRNG, the result is not purely a function of the previous output value as the accumulator contains additional state.  With 32-bit integer output and a period of ~2^19937, it cannot possibly provide a such a guarantee.  The Mersenne Twister is a popular algorithm for PRNGs because it has good statistical and geometric properties and a very long period - desirable characteristics for a PRNG used on simulation models.   The 32-bit integer produced by an MT19337 PRNG cannot possibly represent enough discrete values to avoid repeating during such a large period.  In this case, duplicate values are likely to occur and inevitable with a large enough sample.  The Birthday Paradox in a nutshellThis problem is originally defined as the probability of any two people in the room sharing the same birthday.  The key point is that any two people in the room could share a birthday.  People tend to naively misinterpret the problem as the probability of someone in the room sharing a birthday with a specific individual, which is the source of the cognitive bias that often causes people to underestimate the probability.  This is the incorrect assumption - there is no requirement for the match to be to a specific individual and any two individuals could match.  The probability of a match occurring between any two individuals is much higher than the probability of a match to a specific individual as the match does not have to be to a specific date.  Rather, you only have to find two individuals that share the same birthday.  From this graph (which can be found on the Wikipedia page on the subject), we can see that we only need 23 people in the room for there to be a 50% chance of finding two that match in this way.From the Wikipedia entry on the subject we can get a nice summary.  In the OP's problem, we have 4,500 possible 'birthdays', rather than 365.  For a given number of random values generated (equating to 'people') we want to know the probability of any two identical values appearing within the sequence.  Computing the likely effect of the Birthday Paradox on the OP's problemFor a sequence of 100 numbers, we have 

 pairs (see Understanding the Problem) that could potentially match (i.e. the first could match with the second, third etc., the second could match the third, fourth etc. and so on), so the number of combinations that could potentially match is rather more than just 100.  From Calculating the Probability we get an expression of 

.  The following snippet of Python code below does a naive evaluation of the probability of a matching pair occurring.  This produces a sensible looking result of p=0.669 for a match occurring within 100 numbers sampled from a population of 4500 possible values. (Maybe someone could verify this and post a comment if it's wrong).  From this we can see that the lengths of runs between matching numbers observed by the OP seem to be quite reasonable.Footnote: using shuffling to get a unique sequence of pseudo-random numbers See this answer below from S. Mark for a means of getting a guaranteed unique set of random numbers.  The technique the poster refers to takes an array of numbers (which you supply, so you can make them unique) and shuffles them into a random order.  Drawing the numbers in sequence from the shuffled array will give you a sequence of pseudo-random numbers that are guaranteed not to repeat.Footnote: Cryptographically Secure PRNGs The MT algorithm is not cryptographically secure as it is relatively easy to infer the internal state of the generator by observing a sequence of numbers.  Other algorithms such as Blum Blum Shub are used for cryptographic applications but may be unsuitable for simulation or general random number applications.  Cryptographically secure PRNGs may be expensive (perhaps requiring bignum calculations) or may not have good geometric properties.  In the case of this type of algorithm, the primary requirement is that it should be computationally infeasible to infer the internal state of the generator by observing a sequence of values.Before blaming Python, you should really brush up some probability & statistics theory. Start by reading about the birthday paradoxBy the way, the random module in Python uses the Mersenne twister PRNG, which is considered very good, has an enormous period and was extensively tested. So rest assured you're in good hands.If you don't want repetative one, generate sequential array and use random.shuffleAs an answer to the answer of Nimbuz:http://xkcd.com/221/True randomness definitely includes repetition of values before the whole set of possible values is exhausted. It would not be random otherwise, as you would be able to predict for how long a value would not be repeated.If you ever rolled dice, you surely got 3 sixes in row quite often...Python's random implementation is actually quite state of the art: That's not a repeater. A repeater is when you repeat the same sequence. Not just one number.You are generating 4500 random numbers from a range 500 <= x <= 5000. You then check to see for each number whether it has been generated before. The birthday problem tells us what the probability is for two of those numbers to match given n tries out of a range d.You can also invert the formula to calculate how many numbers you have to generate until the chance of generating a duplicate is more than 50%. In this case you have a >50% chance of finding a duplicate number after 79 iterations.You have defined a random space of 4501 values  (500-5000), and you are iterating 4500 times.  You are basically guaranteed to get a collision in the test you wrote.To think about it another way:So by the time you get to 45/4500, that insert has a 1% chance of being a duplicate, and that probability keeps increasing with each subsequent insert.To create a test that truly tests the abilities of the random function, increase the universe of possible random values (eg: 500-500000)  You may, or may not get a dupe.  But you'll get far more iterations on average.For anyone else with this problem, I used uuid.uuid4() and it works like a charm.There is the birthday paradox. Taking this into account you realize that what you are saying is that finding "764, 3875, 4290, 4378, 764" or something like that is not very random because a number in that sequence repeats. The true way to do it is to compare sequences to each other. I wrote a python script to do this.

adding noise to a signal in python

user1551817

[adding noise to a signal in python](https://stackoverflow.com/questions/14058340/adding-noise-to-a-signal-in-python)

I want to add some random noise to some 100 bin signal that I am simulating in Python - to make it more realistic.On a basic level, my first thought was to go bin by bin and just generate a random number between a certain range and add or subtract this from the signal.I was hoping (as this is python) that there might a more intelligent way to do this via numpy or something. (I suppose that ideally a number drawn from a gaussian distribution and added to each bin would be better also.)Thank you in advance of any replies.I'm just at the stage of planning my code, so I don't have anything to show. I was just thinking that there might be a more sophisticated way of generating the noise.In terms out output, if I had 10 bins with the following values:Bin 1: 1

Bin 2: 4

Bin 3: 9

Bin 4: 16

Bin 5: 25

Bin 6: 25

Bin 7: 16

Bin 8: 9

Bin 9: 4

Bin 10: 1I just wondered if there was a pre-defined function that could add noise to give me something like:Bin 1: 1.13

Bin 2: 4.21

Bin 3: 8.79

Bin 4: 16.08

Bin 5: 24.97

Bin 6: 25.14

Bin 7: 16.22

Bin 8: 8.90

Bin 9: 4.02

Bin 10: 0.91If not, I will just go bin-by-bin and add a number selected from a gaussian distribution to each one.Thank you.It's actually a signal from a radio telescope that I am simulating. I want to be able to eventually choose the signal to noise ratio of my simulation.

2012-12-27 17:03:19Z

I want to add some random noise to some 100 bin signal that I am simulating in Python - to make it more realistic.On a basic level, my first thought was to go bin by bin and just generate a random number between a certain range and add or subtract this from the signal.I was hoping (as this is python) that there might a more intelligent way to do this via numpy or something. (I suppose that ideally a number drawn from a gaussian distribution and added to each bin would be better also.)Thank you in advance of any replies.I'm just at the stage of planning my code, so I don't have anything to show. I was just thinking that there might be a more sophisticated way of generating the noise.In terms out output, if I had 10 bins with the following values:Bin 1: 1

Bin 2: 4

Bin 3: 9

Bin 4: 16

Bin 5: 25

Bin 6: 25

Bin 7: 16

Bin 8: 9

Bin 9: 4

Bin 10: 1I just wondered if there was a pre-defined function that could add noise to give me something like:Bin 1: 1.13

Bin 2: 4.21

Bin 3: 8.79

Bin 4: 16.08

Bin 5: 24.97

Bin 6: 25.14

Bin 7: 16.22

Bin 8: 8.90

Bin 9: 4.02

Bin 10: 0.91If not, I will just go bin-by-bin and add a number selected from a gaussian distribution to each one.Thank you.It's actually a signal from a radio telescope that I am simulating. I want to be able to eventually choose the signal to noise ratio of my simulation.You can generate a noise array, and add it to your signal... And for those who - like me - are very early in their numpy learning curve, For those trying to make the connection between SNR and a normal random variable generated by numpy: [1] , where it's important to keep in mind that P is average power.Or in dB:

[2] In this case, we already have a signal and we want to generate noise to give us a desired SNR.While noise can come in different flavors depending on what you are modeling, a good start (especially for this radio telescope example) is Additive White Gaussian Noise (AWGN). As stated in the previous answers, to model AWGN you need to add a zero-mean gaussian random variable to your original signal. The variance of that random variable will affect the average noise power.For a Gaussian random variable X, the average power , also known as the second moment, is

[3]  So for white noise,  and the average power is then equal to the variance .When modeling this in python, you can either

1. Calculate variance based on a desired SNR and a set of existing measurements, which would work if you expect your measurements to have fairly consistent amplitude values.

2. Alternatively, you could set noise power to a known level to match something like receiver noise. Receiver noise could be measured by pointing the telescope into free space and calculating average power. Either way, it's important to make sure that you add noise to your signal and take averages in the linear space and not in dB units.Here's some code to generate a signal and plot voltage, power in Watts, and power in dB:Here's an example for adding AWGN based on a desired SNR:  And here's an example for adding AWGN based on a known noise power:  For those who want to add noise to a multi-dimensional dataset loaded within a pandas dataframe or even a numpy ndarray, here's an example: Awesome answers above. I recently had a need to generate simulated data and this is what I landed up using. Sharing in-case helpful to others as well,

Simple URL GET/POST function in Python

TIMEX

[Simple URL GET/POST function in Python](https://stackoverflow.com/questions/4476373/simple-url-get-post-function-in-python)

I can't seem to Google it, but I want a function that does this:Accept 3 arguments (or more, whatever):Return me the results, and the response code.Is there a snippet that does this?

2010-12-18 03:02:47Z

I can't seem to Google it, but I want a function that does this:Accept 3 arguments (or more, whatever):Return me the results, and the response code.Is there a snippet that does this?requestshttps://github.com/kennethreitz/requests/Here's a few common ways to use it:httplib2https://github.com/jcgregorio/httplib2Even easier: via the requests module.To send data that is not form-encoded, send it serialised as a string (example taken from the documentation):You could use this to wrap urllib2:That will return a Request object that has result data and response codes.[Update]Some of these answers are old. Today I would use the requests module like the answer by robaple.I know you asked for GET and POST but I will provide CRUD since others may need this just in case: (this was tested in Python 3.7)

Finding all possible permutations of a given string in python

Nihar Sarangi

[Finding all possible permutations of a given string in python](https://stackoverflow.com/questions/8306654/finding-all-possible-permutations-of-a-given-string-in-python)

I have a string. I want to generate all permutations from that string, by changing the order of characters in it. For example, say:what I want is a list like this,Currently I am iterating on the list cast of the string, picking 2 letters randomly and transposing them to form a new string, and adding it to set cast of l. Based on the length of the string, I am calculating the number of permutations possible and continuing iterations till set size reaches the limit.

There must be a better way to do this. 

2011-11-29 06:12:34Z

I have a string. I want to generate all permutations from that string, by changing the order of characters in it. For example, say:what I want is a list like this,Currently I am iterating on the list cast of the string, picking 2 letters randomly and transposing them to form a new string, and adding it to set cast of l. Based on the length of the string, I am calculating the number of permutations possible and continuing iterations till set size reaches the limit.

There must be a better way to do this. The itertools module has a useful method called permutations(). The documentation says:You'll have to join your permuted letters as strings though.If you find yourself troubled by duplicates, try fitting your data into a structure with no duplicates like a set:Thanks to @pst for pointing out that this is not what we'd traditionally think of as a type cast, but more of a call to the set() constructor.You can get all N! permutations without much codeHere is another way of doing the permutation of string with minimal code.

We basically create a loop and then we keep swapping two characters at a time,

Inside the loop we'll have the recursion. Notice,we only print when indexers reaches the length of our string.

Example:

ABC

i for our starting point and our recursion param

j for our loop  here is a visual help how it works from left to right top to bottom (is the order of permutation)the code :Stack Overflow users have already posted some strong solutions but I wanted to show yet another solution. This one I find to be more intuitiveThe idea is that for a given string: we can recurse by the algorithm (pseudo-code):I hope it helps someone!Here's a simple function to return unique permutations:Here is another approach different from what @Adriano and @illerucis posted. This has a better runtime, you can check that yourself by measuring the time:For an arbitrary string "dadffddxcf" it took 1.1336 sec for the permutation library, 9.125 sec for this implementation and 16.357 secs for @Adriano's and @illerucis' version. Of course you can still optimize it.itertools.permutations is good, but it doesn't deal nicely with sequences that contain repeated elements. That's because internally it permutes the sequence indices and is oblivious to the sequence item values.Sure, it's possible to filter the output of itertools.permutations through a set to eliminate the duplicates, but it still wastes time generating those duplicates, and if there are several repeated elements in the base sequence there will be lots of duplicates. Also, using a collection to hold the results wastes RAM, negating the benefit of using an iterator in the first place.Fortunately, there are more efficient approaches. The code below uses the algorithm of the 14th century Indian mathematician Narayana Pandita, which can be found in the Wikipedia article on Permutation. This ancient algorithm is still one of the fastest known ways to generate permutations in order, and it is quite robust, in that it properly handles permutations that contain repeated elements.outputOf course, if you want to collect the yielded strings into a list you can door in recent Python versions:why do you not simple do:you get no duplicate as you can see :See itertools.combinations or itertools.permutations.Here's a slightly improved version of illerucis's code for returning a list of all permutations of a string s with distinct characters (not necessarily in lexicographic sort order), without using itertools:Here's a really simple generator version:I think it's not so bad!This is one way to generate permutations with recursion, you can understand the code easily by taking strings 'a','ab' & 'abc' as input.You get all N! permutations with this, without duplicates.Everyone loves the smell of their own code.  Just sharing the one I find the simplest:This program does not eliminate the duplicates, but I think it is one of the most efficient approaches:Simpler solution using permutations.Yet another initiative and recursive solution. The idea is to select a letter as a pivot and then create a word.Output:Here's a simple and straightforward recursive implementation;

Plot width settings in ipython notebook

pt12lol

[Plot width settings in ipython notebook](https://stackoverflow.com/questions/29589119/plot-width-settings-in-ipython-notebook)

I've got the following plots:It would look nicer if they have the same width. Do you have any idea how to do it in ipython notebook when I am using %matplotlib inline?UPDATE:To generate both figures I am using the following functions:

2015-04-12 11:48:45Z

I've got the following plots:It would look nicer if they have the same width. Do you have any idea how to do it in ipython notebook when I am using %matplotlib inline?UPDATE:To generate both figures I am using the following functions:If you use %pylab inline you can (on a new line) insert the following command:This will set all figures in your document (unless otherwise specified) to be of the size (10, 6), where the first entry is the width and the second is the height.See this SO post for more details. https://stackoverflow.com/a/17231361/1419668This is way I did it:You can define your own sizes.If you're not in an ipython notebook (like the OP), you can also just declare the size when you declare the figure:

Assert that a method was called in a Python unit test

Mark Heath

[Assert that a method was called in a Python unit test](https://stackoverflow.com/questions/3829742/assert-that-a-method-was-called-in-a-python-unit-test)

Suppose I have the following code in a Python unit test:Is there an easy way to assert that a particular method (in my case aw.Clear()) was called during the second line of the test? e.g. is there something like this:

2010-09-30 10:32:55Z

Suppose I have the following code in a Python unit test:Is there an easy way to assert that a particular method (in my case aw.Clear()) was called during the second line of the test? e.g. is there something like this:I use Mock (which is now unittest.mock on py3.3+) for this:For your case, it could look like this:Mock supports quite a few useful features, including ways to patch an object or module, as well as checking that the right thing was called, etc etc. Caveat emptor! (Buyer beware!)If you mistype assert_called_with (to assert_called_once or assert_called_wiht) your test may still run, as Mock will think this is a mocked function and happily go along, unless you use autospec=true. For more info read assert_called_once: Threat or Menace.Yes if you are using Python 3.3+. You can use the built-in unittest.mock to assert method called. For Python 2.6+ use the rolling backport Mock, which is the same thing.Here is a quick example in your case:I'm not aware of anything built-in.  It's pretty simple to implement:This requires that the object itself won't modify self.b, which is almost always true.Yes, I can give you the outline but my Python is a bit rusty and I'm too busy to explain in detail.Basically, you need to put a proxy in the method that will call the original, eg:This StackOverflow answer about callable may help you understand the above.In more detail:Although the answer was accepted, due to the interesting discussion with Glenn and having a few minutes free, I wanted to enlarge on my answer:You can mock out aw.Clear, either manually or using a testing framework like pymox. Manually, you'd do it using something like this:Using pymox, you'd do it like this:

How to allow users to change their own passwords in Django?

Hulk

[How to allow users to change their own passwords in Django?](https://stackoverflow.com/questions/1873806/how-to-allow-users-to-change-their-own-passwords-in-django)

Can any one point me to code where users can change their own passwords in Django?

2009-12-09 13:15:50Z

Can any one point me to code where users can change their own passwords in Django?How to change Django passwords See the Changing passwords sectionYou can also use the simple manage.py command:manage.py changepassword *username*Just enter the new password twice.from the Changing passwords section in the docs.If you have the django.contrib.admin in your INSTALLED_APPS, you can visit: example.com/path-to-admin/password_change/ which will have a form to confirm your old password and enter the new password twice.You can also just use the django.contrib.auth.views.password_change view in your URLconf. It uses a default form and template; supplying your own is optional.Its without need to go to shell enter passwd and reenter passwdUsing shell urls.py:Template:Documented at: https://docs.djangoproject.com/en/1.9/topics/auth/default/#using-the-viewsThis tutorial shows how to do it with function based views:View file:Url file:And finally, the template:Once the url pattern is added as shown in Ciro Santilli's answer, a quick way to allow users to change passwords is to give them "staff access" for the admin functions.  If you don't add them to any groups or give them special permissions, they can still change their password by going to the example.com/admin page.  The staff access lets them go to the page even if it is blank; in the upper right corner they can click "change password" and use the admin funtionality.This is the command i used, just in case you are having problem in that throw AttributeError: Manager isn't available; 'auth.User' has been swapped for 'users.User'.Very similar to @Ciro's answer, but more specific to the original question (without adding all the authentication views):just add to urlpatterns in urls.py:Note that post_change_redirect specifies the url to redirect after the password is changed.Then, just add to your template:

How could I use batch normalization in TensorFlow?

Shawn Lee

[How could I use batch normalization in TensorFlow?](https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow)

I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.I did not find a method called MovingMoments either.

2015-11-27 03:17:52Z

I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.I did not find a method called MovingMoments either.Update July 2016  The easiest way to use batch normalization in TensorFlow is through the higher-level interfaces provided in either contrib/layers, tflearn, or slim.Previous answer if you want to DIY:

The documentation string for this has improved since the release - see the docs comment in the master branch instead of the one you found.  It clarifies, in particular, that it's the output from tf.nn.moments.You can see a very simple example of its use in the batch_norm test code.  For a more real-world use example, I've included below the helper class and use notes that I scribbled up for my own use (no warranty provided!):Note that I called it a ConvolutionalBatchNormalizer because it pins the use of tf.nn.moments to sum across axes 0, 1, and 2, whereas for non-convolutional use you might only want axis 0.Feedback appreciated if you use it.As of TensorFlow 1.0 (February 2017) there's also the high-level tf.layers.batch_normalization API included in TensorFlow itself.It's super simple to use:...except that it adds extra ops to the graph (for updating its mean and variance variables) in such a way that they won't be dependencies of your training op. You can either just run the ops separately:or add the update ops as dependencies of your training op manually, then just run your training op as normal:The following works fine for me, it does not require invoking EMA-apply outside.Example:There is also an "official" batch normalization layer coded by the developers. They don't have very good docs on how to use it but here is how to use it (according to me):to actually use it you need to create a placeholder for train_phase that indicates if you are in training or inference phase (as in train_phase = tf.placeholder(tf.bool, name='phase_train')). Its value can be filled during inference or training with a tf.session as in:or during training:I'm pretty sure this is correct according to the discussion in github.Seems there is another useful link:http://r2rt.com/implementing-batch-normalization-in-tensorflow.htmlYou can simply use the build-in batch_norm layer:where prev is the output of your previous layer (can be both fully-connected or a convolutional layer) and is_train is a boolean placeholder. Just use batch_norm as the input to the next layer, then.Since someone recently edited this, I'd like to clarify that this is no longer an issue.This answer does not seem correct  When phase_train is set to false, it still updates the ema mean and variance. This can be verified with the following code snippet.Using TensorFlow built-in batch_norm layer, below is the code to load data, build a network with one hidden ReLU layer and L2 normalization and introduce batch normalization for both hidden and out layer. This runs fine and trains fine. Just FYI this example is mostly built upon the data and code from Udacity DeepLearning course.

P.S. Yes, parts of it were discussed one way or another in answers earlier but I decided to gather in one code snippet everything so that you have example of whole network training process with Batch Normalization and its evaluationSo a simple example of the use of this batchnorm class:

How to prevent my site page to be loaded via 3rd party site frame of iFrame

abovesun

[How to prevent my site page to be loaded via 3rd party site frame of iFrame](https://stackoverflow.com/questions/2896623/how-to-prevent-my-site-page-to-be-loaded-via-3rd-party-site-frame-of-iframe)

How can I find out that my page is embedded as a frame to other site during page loading? I guess referrer request header can't help me here? Thanks. 

2010-05-24 11:32:53Z

How can I find out that my page is embedded as a frame to other site during page loading? I guess referrer request header can't help me here? Thanks. You cannot check it from the server's side, but you can use javascript to detect it after the page has loaded. Compare top and self, if they're not identical, you are in a frame.Additionally, some modern browsers respect the X-FRAME-OPTIONS header, that can have two values:Users include Google's Picasa, that cannot be embedded in a frame.Browsers that support the header, with the minimum version:Stackoverflow includes some JS to test it (master.js). This is the relevant part of it:But keep in mind that JS can be disabled.For modern browsers, you can use CSP (Content Security Policy), which is a standard. The following header will prevent the document from loading in a frame anywhere:(IE 11 needs the X- prefix, though). You can also change 'none' to the origin on which framing is allowed, such as your own site.To cover the older browsers, this is best used together with @Maerlyn's answer.you can prevent loading you page in an iframe with javascriptthis code change address of container of your page's iframe to your page address and force container to show your page.Or you can block a specific domain if you don't mind your content in some locations but don't want it on a certain site.  For example, if offendingdomain.com was embedding your content, you could do this:This would check the parent document's location and see if it's the offendingdomain.com that is embedding your content.  This script will then send that iframe to a certain famous youtube video as punishment.  In effect they just Rick-Rolled themselves.  Use javascript to check if it was loaded on iframe by placing the following script at the end of your php file and redirect to a page that displays warning or notice that your page should not be loaded using iframe.

How to have same text in two links with restructured text?

luispedro

[How to have same text in two links with restructured text?](https://stackoverflow.com/questions/5464627/how-to-have-same-text-in-two-links-with-restructured-text)

Here is what I would like to do:To obtain:The context is a list of publications, where I want them all to have a link marked "DOI" at the end.However, this seems to fails with:The exact error seems to depend on the version of docutils that I use, but they've all failed.Is there a way to generate multiple links with the same text in restructured text?

2011-03-28 20:20:34Z

Here is what I would like to do:To obtain:The context is a list of publications, where I want them all to have a link marked "DOI" at the end.However, this seems to fails with:The exact error seems to depend on the version of docutils that I use, but they've all failed.Is there a way to generate multiple links with the same text in restructured text?The warningoccurs when you use the same text for two different links in "Named hyperlink references":To circumvent it, use anonymous hyperlink references with double underscores:This works without a warning on docutils 0.8.1.I think you'll want to use anonymous hyperlinks:Keep in mind that the order they're referred to in the document is important.  More information can be found here.Seems like you need a newline and two underscores.This is what I do:to obtain:

Convert binary to ASCII and vice versa

sbrichards

[Convert binary to ASCII and vice versa](https://stackoverflow.com/questions/7396849/convert-binary-to-ascii-and-vice-versa)

Using this code to take a string and convert it to binary:this outputs:Which, if I put it into this site (on the right hand site) I get my message of hello back. I'm wondering what method it uses. I know I could splice apart the string of binary into 8's and then match it to the corresponding value to bin(ord(character)) or some other way. Really looking for something simpler.

2011-09-13 04:34:14Z

Using this code to take a string and convert it to binary:this outputs:Which, if I put it into this site (on the right hand site) I get my message of hello back. I'm wondering what method it uses. I know I could splice apart the string of binary into 8's and then match it to the corresponding value to bin(ord(character)) or some other way. Really looking for something simpler.For ASCII characters in the range [ -~] on Python 2:In reverse:In Python 3.2+:In reverse:Here is a pure python method for simple strings, left here for posterity.I'm not sure how you think you can do it other than character-by-character -- it's inherently a character-by-character operation. There is certainly code out there to do this for you, but there is no "simpler" way than doing it character-by-character.First, you need to strip the 0b prefix, and left-zero-pad the string so it's length is divisible by 8, to make dividing the bitstring up into characters easy:Then you divide the string up into blocks of eight binary digits, convert them to ASCII characters, and join them back into a string:If you actually want to treat it as a number, you still have to account for the fact that the leftmost character will be at most seven digits long if you want to go left-to-right instead of right-to-left.This is my way to solve your task:if you don'y want to import any files you can use this:to convert a text file to binary.and you can use this to convert it back to string:hope that is helpful, i've used this with some custom encryption to send over TCP. Are you looking for the code to do it or understanding the algorithm?Does this do what you need? Specifically a2b_uu and b2a_uu? There are LOTS of other options in there in case those aren't what you want.(NOTE: Not a Python guy but this seemed like an obvious answer)This is a spruced up version of J.F. Sebastian's. Thanks for the snippets though J.F. Sebastian.

How to know the version of pip itself

doniyor

[How to know the version of pip itself](https://stackoverflow.com/questions/26378344/how-to-know-the-version-of-pip-itself)

Which shell command gives me the actual version of pip I am using? pip gives with pip show all version of modules that are installed but excludes itself.

2014-10-15 08:54:01Z

Which shell command gives me the actual version of pip I am using? pip gives with pip show all version of modules that are installed but excludes itself.You can do this:or:Just for completeness:pip -Vpip --versionpip list and inside the list you'll find also pip with its version.For windows:shows the version at the end of the help file.On RHEL "pip -V" works : For windows just type:Many people use both 2.X and 3.X python. You can use pip -V to show default pip version.

If you have many python versions, and you want to install some packages through different pip, I advise this way:First, open a command prompt After type a bellow commands. check a version itself Easily :Form Windows:For Windows machine go to command prompt and type.Any of the following should workor or check two thingsandbecause the default pip may be anyone of this so it is always better to check both.However note, if you are using macos catelina which has the zsh (z shell) it might give you a whole bunch of things, so the best option is to try install the version or start as -- pip3 

How to get Tensorflow tensor dimensions (shape) as int values?

stackoverflowuser2010

[How to get Tensorflow tensor dimensions (shape) as int values?](https://stackoverflow.com/questions/40666316/how-to-get-tensorflow-tensor-dimensions-shape-as-int-values)

Suppose I have a Tensorflow tensor. How do I get the dimensions (shape) of the tensor as integer values? I know there are two methods, tensor.get_shape() and tf.shape(tensor), but I can't get the shape values as integer int32 values.For example, below I've created a 2-D tensor, and I need to get the number of rows and columns as int32 so that I can call reshape() to create a tensor of shape (num_rows * num_cols, 1). However, the method tensor.get_shape() returns values as Dimension type, not int32.

2016-11-17 22:37:28Z

Suppose I have a Tensorflow tensor. How do I get the dimensions (shape) of the tensor as integer values? I know there are two methods, tensor.get_shape() and tf.shape(tensor), but I can't get the shape values as integer int32 values.For example, below I've created a 2-D tensor, and I need to get the number of rows and columns as int32 so that I can call reshape() to create a tensor of shape (num_rows * num_cols, 1). However, the method tensor.get_shape() returns values as Dimension type, not int32.To get the shape as a list of ints, do tensor.get_shape().as_list().To complete your tf.shape() call, try tensor2 = tf.reshape(tensor, tf.TensorShape([num_rows*num_cols, 1])). Or you can directly do tensor2 = tf.reshape(tensor, tf.TensorShape([-1, 1])) where its first dimension can be inferred.Another way to solve this is like this:This will return the int value of the Dimension object.for a 2-D tensor, you can get the number of rows and columns as int32 using the following code:2.0 Compatible Answer: In Tensorflow 2.x (2.1), you can get the dimensions (shape) of the tensor as integer values, as shown in the Code below:Method 1 (using tf.shape):Method 2 (using tf.get_shape()):In later versions (tested with TensorFlow 1.14) there's a more numpy-like way to get the shape of a tensor. You can use tensor.shape to get the shape of the tensor.

How to have logarithmic bins in a Python histogram

Brian

[How to have logarithmic bins in a Python histogram](https://stackoverflow.com/questions/6855710/how-to-have-logarithmic-bins-in-a-python-histogram)

As far as I know the option Log=True in the histogram function only refers to the y-axis.I need the bins to be equally spaced in log10. Is there something that can do this?

2011-07-28 07:55:35Z

As far as I know the option Log=True in the histogram function only refers to the y-axis.I need the bins to be equally spaced in log10. Is there something that can do this?use logspace() to create a geometric sequence, and pass it to bins parameter. And set the scale of xaxis to log scale.The most direct way is to just compute the log10 of the limits, compute linearly spaced bins, and then convert back by raising to the power of 10, as below:The following code indicates how you can use bins='auto' with the log scale.In addition to what was stated, performing this on pandas dataframes works as well:I would caution, that there may be an issue with normalizing the bins. Each bin is larger than the previous one, and therefore must be divided by it's size to normalize the frequencies before plotting, and it seems that neither my solution, nor HYRY's solution accounts for this.Source: https://arxiv.org/pdf/cond-mat/0412004.pdf

Remove NaN from pandas series

user1802143

[Remove NaN from pandas series](https://stackoverflow.com/questions/20235401/remove-nan-from-pandas-series)

Is there a way to remove a NaN values from a panda series? I have a series that may or may not have some NaN values in it, and I'd like to return a copy of the series with all the NaNs removed.

2013-11-27 06:26:28Z

Is there a way to remove a NaN values from a panda series? I have a series that may or may not have some NaN values in it, and I'd like to return a copy of the series with all the NaNs removed.update or even better approach as @DSM suggested in comments, using pandas.Series.dropna():A small usage of np.nan ! = np.nanMore Info If you have a pandas serie with NaN, and want to remove it (without loosing index):

requirements.txt depending on python version

aquavitae

[requirements.txt depending on python version](https://stackoverflow.com/questions/19559247/requirements-txt-depending-on-python-version)

I'm trying to port a python2 package to python3 (not my own) using six so that it's compatible with both.  However one of the packages listed in requirements.txt is now included in the python3 stdlib and the pypi version doesn't work in python3 so I want to conditionally exclude it.  Doing this in setup.py is easy, I can just do something like:But I would like requirements.txt to reflect the correct list too.  I can't find anything on this in the pip documentation. so does anyone know how to do it, or if it is even possible?

2013-10-24 07:09:41Z

I'm trying to port a python2 package to python3 (not my own) using six so that it's compatible with both.  However one of the packages listed in requirements.txt is now included in the python3 stdlib and the pypi version doesn't work in python3 so I want to conditionally exclude it.  Doing this in setup.py is easy, I can just do something like:But I would like requirements.txt to reflect the correct list too.  I can't find anything on this in the pip documentation. so does anyone know how to do it, or if it is even possible?You can use the environment markers to achieve this in requirements.txt since pip 6.0:It is supported by setuptools too by declaring extra requirements in setup.py:See also requirement specifiers.

And Strings for the string versions of corresponding Python commands.You can create multiple requirements files, put those common packages in a common file, and include them in another pip requirements file with -r file_pathpython2.txt:python3.txt:pip install -r requirements/python2.txt

Parsing non-zero padded timestamps in Python

Pythontology

[Parsing non-zero padded timestamps in Python](https://stackoverflow.com/questions/25279993/parsing-non-zero-padded-timestamps-in-python)

I want to get datetimes from timestamps like the following :3/1/2014 9:55 with datetime.strptime, or something equivalent.The month, day of month, and hour is not zero padded, but there doesn't seem to be a formatting directive listed here that is able to parse this automatically.What's the best approach to do so?  Thanks!

2014-08-13 07:01:07Z

I want to get datetimes from timestamps like the following :3/1/2014 9:55 with datetime.strptime, or something equivalent.The month, day of month, and hour is not zero padded, but there doesn't seem to be a formatting directive listed here that is able to parse this automatically.What's the best approach to do so?  Thanks!strptime is able to parse non-padded values. The fact that they are noted as being padded in the formatting codes table applies to strftime's output. So you can just usestrptime isdo not require 0-padded values. See example belowJust in case this answer helps someone else -- I came here thinking I had a problem with zero padding, but it was actually to do with 12:00 vs 00:00 and the %I formatter.The %I formatter is meant to match 12-hour-clock hours, optionally zero-padded.  But depending on your data source, you might get data that says that midnight or midday is actually zero, eg:What strptime actually wanted was a 12, not a zero:But we don't always control our data sources!  My solution for this edge case was to catch the exception, try parsing it with a %H, with a quick check that we are in the edge case we think we are in.The non-pattern way is use dateutil.parse module, it lets to parse the common date formats, even if you don't know what it is using currently

Ex:You can see the strftime document here，but in fact they aren't all working well in all platforms，for instance，%-d，%-m don't work on win7 by python 2.7，so you can accomplish like this

psycopg2: AttributeError: 'module' object has no attribute 'extras'

n1000

[psycopg2: AttributeError: 'module' object has no attribute 'extras'](https://stackoverflow.com/questions/30940167/psycopg2-attributeerror-module-object-has-no-attribute-extras)

In my code I use the DictCursor from psycopg2.extras like thisHowever, all of the sudden I get the following error when I load the cursor:Maybe something is dorked in my installation but I have no clue where to start looking. I made some updates with pip, but as far as I know no dependencies of psycopg2.

2015-06-19 14:04:22Z

In my code I use the DictCursor from psycopg2.extras like thisHowever, all of the sudden I get the following error when I load the cursor:Maybe something is dorked in my installation but I have no clue where to start looking. I made some updates with pip, but as far as I know no dependencies of psycopg2.You need to explicitly import psycopg2.extras:As of July 2018, the import psycopg2.extras doesn't work for me.

The following works for me:pip install psycopg2-binaryand later:

Python - Join with newline

TTT

[Python - Join with newline](https://stackoverflow.com/questions/14560863/python-join-with-newline)

In the Python console, when I type:Gives:Though I'd expect to see such an output:What am I missing here?

2013-01-28 11:18:20Z

In the Python console, when I type:Gives:Though I'd expect to see such an output:What am I missing here?The console is printing the representation, not the string itself.If you prefix with print, you'll get what you expect.See this question for details about the difference between a string and the string's representation. Super-simplified, the representation is what you'd type in source code to get that string.You forgot to print the result. What you get is the P in RE(P)L and not the actual printed result.In Py2.x you should so something like and in Py3.X, print is a function, so you should doNow that was the short answer. Your Python Interpreter, which is actually a REPL, always displays the representation of the string rather than the actual displayed output. Representation is what you would get with the repr statementYou need to print to get that output.

You should doYou have to print it:When you print it with this print 'I\nwould\nexpect\nmultiple\nlines' you would get:The \n is a new line character specially used for marking END-OF-TEXT. It signifies the end of the line or text. This characteristics is shared by many languages like C, C++ etc.

Continuing in Python's unittest when an assertion fails

Bruce Christensen

[Continuing in Python's unittest when an assertion fails](https://stackoverflow.com/questions/4732827/continuing-in-pythons-unittest-when-an-assertion-fails)

EDIT: switched to a better example, and clarified why this is a real problem.I'd like to write unit tests in Python that continue executing when an assertion fails, so that I can see multiple failures in a single test. For example:Here, the purpose of the test is to ensure that Car's __init__ sets its fields correctly. I could break it up into four methods (and that's often a great idea), but in this case I think it's more readable to keep it as a single method that tests a single concept ("the object is initialized correctly").If we assume that it's best here to not break up the method, then I have a new problem: I can't see all of the errors at once. When I fix the model error and re-run the test, then the wheel_count error appears. It would save me time to see both errors when I first run the test.For comparison, Google's C++ unit testing framework distinguishes between between non-fatal EXPECT_* assertions and fatal ASSERT_* assertions:Is there a way to get EXPECT_*-like behavior in Python's unittest? If not in unittest, then is there another Python unit test framework that does support this behavior?Incidentally, I was curious about how many real-life tests might benefit from non-fatal assertions, so I looked at some code examples (edited 2014-08-19 to use searchcode instead of Google Code Search, RIP). Out of 10 randomly selected results from the first page, all contained tests that made multiple independent assertions in the same test method. All would benefit from non-fatal assertions.

2011-01-19 07:38:08Z

EDIT: switched to a better example, and clarified why this is a real problem.I'd like to write unit tests in Python that continue executing when an assertion fails, so that I can see multiple failures in a single test. For example:Here, the purpose of the test is to ensure that Car's __init__ sets its fields correctly. I could break it up into four methods (and that's often a great idea), but in this case I think it's more readable to keep it as a single method that tests a single concept ("the object is initialized correctly").If we assume that it's best here to not break up the method, then I have a new problem: I can't see all of the errors at once. When I fix the model error and re-run the test, then the wheel_count error appears. It would save me time to see both errors when I first run the test.For comparison, Google's C++ unit testing framework distinguishes between between non-fatal EXPECT_* assertions and fatal ASSERT_* assertions:Is there a way to get EXPECT_*-like behavior in Python's unittest? If not in unittest, then is there another Python unit test framework that does support this behavior?Incidentally, I was curious about how many real-life tests might benefit from non-fatal assertions, so I looked at some code examples (edited 2014-08-19 to use searchcode instead of Google Code Search, RIP). Out of 10 randomly selected results from the first page, all contained tests that made multiple independent assertions in the same test method. All would benefit from non-fatal assertions.What you'll probably want to do is derive unittest.TestCase since that's the class that throws when an assertion fails.  You will have to re-architect your TestCase to not throw (maybe keep a list of failures instead). Re-architecting stuff can cause other issues that you would have to resolve.  For example you may end up needing to derive TestSuite to make changes in support of the changes made to your TestCase.Another way to have non-fatal assertions is to capture the assertion exception and store the exceptions in a list. Then assert that that list is empty as part of the tearDown.One option is assert on all the values at once as a tuple.For example:The output from this tests would be:This shows that both the model and the wheel count are incorrect.It is considered an anti-pattern to have multiple asserts in a single unit test. A single unit test is expected to test only one thing. Perhaps you are testing too much. Consider splitting this test up into multiple tests. This way you can name each test properly.Sometimes however, it is okay to check multiple things at the same time. For instance when you are asserting properties of the same object. In that case you are in fact asserting whether that object is correct. A way to do this is to write a custom helper method that knows how to assert on that object. You can write that method in such a way that it shows all failing properties or for instance shows the complete state of the expected object and the complete state of the actual object when an assert fails.Do each assert in a separate method.I liked the approach by @Anthony-Batchelor, to capture the AssertionError exception. But a slight variation to this approach using decorators and also a way to report the tests cases with pass/fail.Output from console:expect is very useful in gtest.

This is python way in gist, and code:There is a soft assertion package in PyPI called softest that will handle your requirements. It works by collecting the failures, combining exception and stack trace data, and reporting it all as part of the usual unittest output.For instance, this code:...produces this console output:NOTE: I created and maintain softest.I don't think there is a way to do this with PyUnit and wouldn't want to see PyUnit extended in this way.I prefer to stick to one assertion per test function (or more specifically asserting one concept per test) and would rewrite test_addition() as four separate test functions. This would give more useful information on failure, viz:If you decide that this approach isn't for you, you may find this answer helpful.It looks like you are testing two concepts with your updated question and I would split these into two unit tests. The first being that the parameters are being stored on the creation of a new object. This would have two assertions, one for make and one for model. If the first fails, the that clearly needs to be fixed, whether the second passes or fails is irrelevant at this juncture.The second concept is more questionable... You're testing whether some default values are initialised. Why? It would be more useful to test these values at the point that they are actually used (and if they are not used, then why are they there?). Both of these tests fail, and both should. When I am unit-testing, I am far more interested in failure than I am in success as that is where I need to concentrate.I got a problem with @Anthony Batchelor answer, because it forces-me to use try...catch inside my Unit Tests. Then, I encapsulated the try...catch logic in an override of the TestCase.assertEqual method. The following hack remove the try...catch blocks from the Unit Tests code:Result output:More alternative solutions for the correct stacktrace capture could be posted on How to correctly override TestCase.assertEqual(), producing the right stacktrace?I realize this question was asked literally years ago, but there are now (at least) two Python packages that allow you to do this.One is softest: https://pypi.org/project/softest/The other is Python-Delayed-Assert: https://github.com/pr4bh4sh/python-delayed-assertI haven't used either, but they look pretty similar to me.Since Python 3.4 you can also use subtests:(msg parameter is used to more easily determine which test failed.)Output:

More Pythonic Way to Run a Process X Times

Lionel

[More Pythonic Way to Run a Process X Times](https://stackoverflow.com/questions/4264634/more-pythonic-way-to-run-a-process-x-times)

Which is more pythonic?While loop:For loop:Edit: not duplicate because this has answers to determine which is clearer, vs. how to run a range without 'i' -- even though that ended up being the most elegant

2010-11-24 08:10:24Z

Which is more pythonic?While loop:For loop:Edit: not duplicate because this has answers to determine which is clearer, vs. how to run a range without 'i' -- even though that ended up being the most elegantPersonally:if you don't need i. If you use Python < 3 and you want to repeat the loop a lot of times, use xrange as there is no need to generate the whole list beforehand.The for loop is definitely more pythonic, as it uses Python's higher level built in functionality to convey what you're doing both more clearly and concisely. The overhead of range vs xrange, and assigning an unused i variable, stem from the absence of a statement like Verilog's repeat statement. The main reason to stick to the for range solution is that other ways are more complex. For instance:Using repeat instead of range here is less clear because it's not as well known a function, and more complex because you need to import it. The main style guides if you need a reference are PEP 20 - The Zen of Python and PEP 8 - Style Guide for Python Code. We also note that the for range version is an explicit example used in both the language reference and tutorial, although in that case the value is used. It does mean the form is bound to be more familiar than the while expansion of a C-style for loop. If you are after the side effects that happen within the loop, I'd personally go for the range() approach.If you care about the result of whatever functions you call within the loop, I'd go for a list comprehension or map approach. Something like this:How about?or in a more ugly way:or if you want to catch the last time through:where:There is not a really pythonic way of repeating something. 

However, it is a better way:If you need to pass the index then:Consider that it returns the results as a collection.

So, if you need to collect the results it can help.

What's with the integer cache maintained by the interpreter?

felix021

[What's with the integer cache maintained by the interpreter?](https://stackoverflow.com/questions/15171695/whats-with-the-integer-cache-maintained-by-the-interpreter)

After dive into Python's source code, I find out that it maintains an array of PyInt_Objects ranging from int(-5) to int(256) (@src/Objects/intobject.c)A little experiment proves it:But if I run those code together in a py file (or join them with semi-colons), the result is different:I'm curious why they are still the same object, so I digg deeper into the syntax tree and compiler, I came up with a calling hierarchy listed below:Then I added some debug code in PyInt_FromLong and before/after PyAST_FromNode, and executed a test.py:the output looks like:It means that during the cst to ast transform, two different PyInt_Objects are created (actually it's performed in the ast_for_atom() function), but they are later merged.I find it hard to comprehend the source in PyAST_Compile and PyEval_EvalCode, so I'm here to ask for help, I'll be appreciative if some one gives a hint?

2013-03-02 06:50:35Z

After dive into Python's source code, I find out that it maintains an array of PyInt_Objects ranging from int(-5) to int(256) (@src/Objects/intobject.c)A little experiment proves it:But if I run those code together in a py file (or join them with semi-colons), the result is different:I'm curious why they are still the same object, so I digg deeper into the syntax tree and compiler, I came up with a calling hierarchy listed below:Then I added some debug code in PyInt_FromLong and before/after PyAST_FromNode, and executed a test.py:the output looks like:It means that during the cst to ast transform, two different PyInt_Objects are created (actually it's performed in the ast_for_atom() function), but they are later merged.I find it hard to comprehend the source in PyAST_Compile and PyEval_EvalCode, so I'm here to ask for help, I'll be appreciative if some one gives a hint?Python caches integers in the range [-5, 256], so it is expected that integers in that range are also identical.What you see is the Python compiler optimizing identical literals when part of the same text.When typing in the Python shell each line is a completely different statement, parsed in a different moment, thus:But if you put the same code into a file:This happens whenever the parser has a chance to analyze where the literals are used, for example when defining a function in the interactive interpreter:Note how the compiled code contains a single constant for the 257.In conclusion, the Python bytecode compiler is not able to perform massive optimizations (like static types languages), but it does more than you think. One of these things is to analyze usage of literals and avoid duplicating them.Note that this does not have to do with the cache, because it works also for floats, which do not have a cache:For more complex literals, like tuples, it "doesn't work":But the literals inside the tuple are shared:Regarding why you see that two PyInt_Object are created, I'd guess that this is done to avoid literal comparison. for example, the number 257 can be expressed by multiple literals:The parser has two choices:Probably the Python parser uses the second approach, which avoids rewriting the conversion code and also it's easier to extend (for example it works with floats as well).Reading the Python/ast.c file, the function that parses all numbers is parsenumber, which calls PyOS_strtoul to obtain the integer value (for intgers) and eventually calls PyLong_FromString:As you can see here the parser does not check whether it already found an integer with the given value and so this explains why you see that two int objects are created,

and this also means that my guess was correct: the parser first creates the constants and only afterward optimizes the bytecode to use the same object for equal constants.The code that does this check must be somewhere in Python/compile.c or Python/peephole.c, since these are the files that transform the AST into bytecode.In particular, the compiler_add_o function seems the one that does it. There is this comment in compiler_lambda:So it seems like compiler_add_o is used to insert constants for functions/lambdas etc.

The compiler_add_o function stores the constants into a dict object, and from this immediately follows that equal constants will fall in the same slot, resulting in a single constant in the final bytecode.

What is the fastest (to access) struct-like object in Python?

DNS

[What is the fastest (to access) struct-like object in Python?](https://stackoverflow.com/questions/2646157/what-is-the-fastest-to-access-struct-like-object-in-python)

I'm optimizing some code whose main bottleneck is running through and accessing a very large list of struct-like objects.  Currently I'm using namedtuples, for readability.  But some quick benchmarking using 'timeit' shows that this is really the wrong way to go where performance is a factor:Named tuple with a, b, c:Class using __slots__, with a, b, c:Dictionary with keys a, b, c:Tuple with three values, using a constant key:List with three values, using a constant key:Tuple with three values, using a local key:List with three values, using a local key:First of all, is there anything about these little timeit tests that would render them invalid?  I ran each several times, to make sure no random system event had thrown them off, and the results were almost identical.It would appear that dictionaries offer the best balance between performance and readability, with classes coming in second.  This is unfortunate, since, for my purposes, I also need the object to be sequence-like; hence my choice of namedtuple.Lists are substantially faster, but constant keys are unmaintainable; I'd have to create a bunch of index-constants, i.e. KEY_1 = 1, KEY_2 = 2, etc. which is also not ideal.Am I stuck with these choices, or is there an alternative that I've missed?

2010-04-15 14:29:29Z

I'm optimizing some code whose main bottleneck is running through and accessing a very large list of struct-like objects.  Currently I'm using namedtuples, for readability.  But some quick benchmarking using 'timeit' shows that this is really the wrong way to go where performance is a factor:Named tuple with a, b, c:Class using __slots__, with a, b, c:Dictionary with keys a, b, c:Tuple with three values, using a constant key:List with three values, using a constant key:Tuple with three values, using a local key:List with three values, using a local key:First of all, is there anything about these little timeit tests that would render them invalid?  I ran each several times, to make sure no random system event had thrown them off, and the results were almost identical.It would appear that dictionaries offer the best balance between performance and readability, with classes coming in second.  This is unfortunate, since, for my purposes, I also need the object to be sequence-like; hence my choice of namedtuple.Lists are substantially faster, but constant keys are unmaintainable; I'd have to create a bunch of index-constants, i.e. KEY_1 = 1, KEY_2 = 2, etc. which is also not ideal.Am I stuck with these choices, or is there an alternative that I've missed?One thing to bear in mind is that namedtuples are optimised for access as tuples.  If you change your accessor to be a[2] instead of a.c, you'll see similar performance to the tuples.  The reason is that the name accessors are effectively translating into calls to self[idx], so pay both the indexing and the name lookup price.If your usage pattern is such that access by name is common, but access as tuple isn't, you could write a quick equivalent to namedtuple that does things the opposite way: defers index lookups to access by-name.  However, you'll pay the price on the index lookups then.  Eg here's a quick implementation:But the timings are very bad when __getitem__ must be called:ie, the same performance as a __slots__ class for attribute access (unsurprisingly - that's what it is), but huge penalties due to the double lookup in index-based accesses.  (Noteworthy is that __slots__ doesn't actually help much speed-wise.  It saves memory, but the access time is about the same without them.)One third option would be to duplicate the data, eg. subclass from list and store the values both in the attributes and listdata.  However you don't actually get list-equivalent performance.  There's a big speed hit just in having subclassed (bringing in checks for pure-python overloads).  Thus struct[0] still takes around 0.5s (compared with 0.18 for raw list) in this case, and you do double the memory usage, so this may not be worth it.This question is fairly old (internet-time), so I thought I'd try duplicating your test today, both with regular CPython (2.7.6), and with pypy (2.2.1) and see how the various methods compared.  (I also added in an indexed lookup for the named tuple.)This is a bit of a micro-benchmark, so YMMV, but pypy seemed to speed up named tuple access by a factor of 30 vs CPython (whereas dictionary access was only sped up by a factor of 3).Python Results:PyPy Results:A couple points and ideas:1) You're timing accessing the same index many times in a row.  Your actual program probably uses random or linear access, which will have different behavior.  In particular, there will be more CPU cache misses.  You might get slightly different results using your actual program.2) OrderedDictionary is written as a wrapper around dict, ergo it will be slower than dict.  That's a non-solution.3) Have you tried both new-style and old-style classes?  (new-style classes inherit from object; old-style classes do not)4) Have you tried using psyco or Unladen Swallow?5) Does your inner loop to modify the data or just access it?  It might be possible to transform the data into the most efficient possible form before entering the loop, but use the most convenient form elsewhere in the program.This problem may be obsolete soon. CPython dev has evidently made significant improvements to the performance of accessing named tuple values by attribute name. The changes are scheduled for release in Python 3.8, near the end of Oct. 2019. See: https://bugs.python.org/issue32492 and https://github.com/python/cpython/pull/10495.I would be tempted to either (a) invent some kind of workload specific caching, and offload the storage and retrieval of my data to a memcachedb-like process, to improve scalability rather than performance alone or (b) rewrite as a C extension, with native data storage.  An ordered-dictionary type perhaps.You could start with this:

http://www.xs4all.nl/~anthon/Python/ordereddict/You can make your classes sequence like by adding __iter__, and __getitem__ methods, to make them sequence like (indexable and iterable.)Would an OrderedDict work?  There are several implementations available, and it is included in the Python31 collections module.

Python Virtualenv - No module named virtualenvwrapper.hook_loader

Thomas Kremmel

[Python Virtualenv - No module named virtualenvwrapper.hook_loader](https://stackoverflow.com/questions/11507186/python-virtualenv-no-module-named-virtualenvwrapper-hook-loader)

I'm running Mac OS 10.6.8. and wanted to install in addition to python 2.6 also python 2.7 and use python 2.7 in a new virtualenv. I executed the following steps:I downloaded python 2.7 and installed it:Then I run the command to setup a new virtualenv using python2.7:My .bash_profile looks like the following:Now when I open the console I get the following error message. I also found in a different post that I should upgrade virtualenvwrapper. That did not help.Any help would be appreciated.

2012-07-16 15:11:22Z

I'm running Mac OS 10.6.8. and wanted to install in addition to python 2.6 also python 2.7 and use python 2.7 in a new virtualenv. I executed the following steps:I downloaded python 2.7 and installed it:Then I run the command to setup a new virtualenv using python2.7:My .bash_profile looks like the following:Now when I open the console I get the following error message. I also found in a different post that I should upgrade virtualenvwrapper. That did not help.Any help would be appreciated.The issue was solved following the steps below:Re-arrange the export command in order that it is placed before the virtualenv commands in my .bash_profile file:Re-Install setuptools, easy install and PIP. This is obviously needed in order that they work properly with the new python version:Also, if you have macports, make sure /opt/local/Library/Frameworks/Python.framework/Versions/2.7/bin is listed before /Library/Frameworks/Python.framework/Versions/2.7/bin and /usr/local/bin in PATH. Then set the following in you .profile:In my case, adding this line into my .zshrc file did the trick,This happened to me and I solved it by re-installing pip. What had happend was that which pip gave /usr/bin/pip as a result, while which python gave /usr/local/bin/python. The path for pip should be /usr/local/bin/pip. This probably broke when I updated my Python installation.If you follow the pip documentation you can easily reinstall pip for your current working Python setup. You need to:This solved the problem for me.For anyone using Ubuntu 18.04 and Python 3+, this did the trick for me: There are a number of things that can cause this error.  If your environment isThen, for whatever reason, the virtual environment is created without the virtualenvwrapper library.  You can solve it by simply installing it again, but this time from within the virtlualenvI just had to make sure that /usr/local/bin/python existed.For me it was a simple:I get the same error . Found out I had old version of pip . I fixed the error by simply upgrading the pip .I just installed python 3.5,tried the virtualenvwrapper and then had this problem. I came to realize that python3.5 was installed in /usr/local/bin/python3.5 and NOT /usr/bin/python3.5. So, I revised my .bash_profile script to look like the following and everything seems to work now I'm enough of a novice to not be sure how that 'local' in the path to python3.5 is going to affect me in the long run but, for now, it works.I had this problem after uninstalling the virtualenvwrapper package. When I logged in to any user (or su to a different one), I would get:The solution was to delete the /etc/bash_completion.d/virtualenvwrapper file.Edit:Do not delete the above file or it won't be recreated if you reinstall virtualenvwrapper. Instead what you need to do is purge the virtualenvwrapper package when you uninstall it. Like this on Debian:Try to uninstall your virtualenv and virtualenvwrapper and install it again using pip in version 2.7 (I think).I encountered the same error and I just did this and solved my problem.I using UEven though there is an accepted answer I thought I would put what fixed it for me.Firstly I installed Python and had just upgraded it via Homebrew. I am also using ZSH so if some bits don't quite match your output then that might be why.By running brew info python and looking through the output I found the following nice bit of information:So I added this to my terminal startup script as shown and the error n longer displays.Note: I inserted this into another part of my PATH and the error persisted on start up.Ran into a similar issue after installing Conda/Anaconda project. This question was quite helpful in resolving my issue on MAC.Resolving the issue had my .zshrc relevant portion looking like this:This is depended on where I have conda installed and you'll have to figure that in your own case. To get the specifics for your given environment depending on where you've installed anaconda you can use the following:DON'T FORGET TO UNINSTALL AND INSTALL virtualenv and virtualenvwrapper as highlighted in other answers.Just bumped into this issue on a Centos 7.4. None of the above answers suited my case. After quite a bit of digging around I pinpointed this to  too-strict file permissions in python libs (I guess python installation on Centos differs a bit from other POSIX systems).So, if everything else fails you might want to check that your python libs are readable by the user you're trying to run virtualenvwrapper with.In particular check:

/usr/lib/python3.6

/usr/lib64/python3.6

(amend the paths for different python versions).If you see that group and others lack read and execute permissions in there then add them:

sudo chmod og+rx -R /usr/lib/python3.6

sudo chmod og+rx -R /usr/lib64/python3.6

Note:

I'm not sure whether this works against a Centos security policy but it's probably safe as long as you don't give write persmisions.In my situation (OS X 10.13.6), this did itI had the same problem as this one and spent so much time on configuring out what was wrong. And I finally found out what was wrong.First I looked for where virtualenvwrapper folder exists.

In my case /usr/local/lib/python3.7/site-packages.

Inside the folder is hook_loader.py that caused the error. Next, I used python script.I couldn't find the /usr/local/lib/python3.7/site-packages directory so,

at last I wrote,to .bashrc file. Done.Meaning of PYTHON PATHAs you can see in above link, PYTHONPATH augment the default search path for modules. 

Django: 'current_tags' is not a valid tag library

snakile

[Django: 'current_tags' is not a valid tag library](https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library)

I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:The problem is with a line in an html file:This exact same code works on his system with no errors. What could that be?

2011-03-30 23:45:28Z

I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:The problem is with a line in an html file:This exact same code works on his system with no errors. What could that be?I would suggest the following:If everything else fails, check this link:

http://www.b-list.org/weblog/2007/dec/04/magic-tags/I had this problem and fixed it by adding a blank __init__.py file in my appname/templatetags/ directory.Possibilities are many:Restart the server has solved the issue for me. They must have mentioned it in the documentation.I was getting the same error but for a different reason so I'll tell you (in case someone else comes the same problem).I had everything right but I had my custom tag inside a folder named template_tags and after a long search I found out that it had to be templatetags, and now it works. So check the folder name is exactly templatetags.suppose you have the following structure:-- Application_Name-------templatetags--------------init.py--------------templates_extras.py-------init.py-------settings.py-- manage.pyYou have to make sure of the following:"custom tags" is not a valid tag library error, more often is occurred because the custom tags are not loaded into the app.place an empty init.py inside the same folder where your "custom template tag" is placed and run the below code on the terminal to load the custom template tagsPlease ensure your templatetags folder is initialized with python, if you are in doubt, just take bakup of all files,Remove all files,

Inside templatetags folder create init.py file only,

then restart your server,Now your folder is under Python, then do your stuff.This works for me...For me, it was the mistake of putting quotes around the library name in load tag.WRONG: {% load 'library_name' %}CORRECT: {% load library_name %}Refer to other answers too. I solved a couple of those problems too before landing here.For others facing this . Suppose your App name is MyApp and your tag folder name is templatetags then in settings.py you should have :Both your django app and your tag folder which is under your app package are needed there.And in your template file :Also app_filters.py be like :check all above steps and you may find the issue.Make sure the load statement is correct. It should be the name of the file, not the name of the app. For instance, if you have this app:Then you should put this in your template:Of course, you should check the other answers too.After you have created the template tag and it should be within the 'templatetags' package within an app installed in the settings.INSTALLED_APPS, make sure you restart your dev-server.Maybe someone will find this useful: somehow I had named the directory 'templatetags ' instead of 'templatetags', that is -- with a space at the end. Took hours to finally realize.All of the advice listed here didn't help me. So in my specific case the problem was that the templatetag had to be loaded from a third-party app, and I manually copied source folder with that app into src folder in my virtualenv. Then I ran python setup.py install inside that folder. After that django could not load this module. Then I removed the source and installed folder of this app and installed it using pip install -r requirements.txt after adding a relevant line into requirements.txt file. It was downloaded into the src folder, installed and everything began working properly. Hope this helps someone.In my case

I have created library instance using tag variable instead of register variable But it should beIn my case the problem was,

I was using {% load filter_method_name %}I had to change to {% filename %}In my case it was - I am using I forgot to create that template and right away it started working. I know above answers are more related to most of the issues - but hope maybe someone find it useful. It should have gotten to me:but it did not and this worked. 

Common pitfalls in Python [duplicate]

uolot

[Common pitfalls in Python [duplicate]](https://stackoverflow.com/questions/1011431/common-pitfalls-in-python)

Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now "Table of Contents" gets longer and longer after each invocation of "generate pdf". :)What else should I add to my list of things to MUST avoid?

2009-06-18 08:23:47Z

Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now "Table of Contents" gets longer and longer after each invocation of "generate pdf". :)What else should I add to my list of things to MUST avoid?Don't use index to loop over a sequenceDon't :Do :For will automate most iteration operations for you.Use enumerate if you really need both the index and the element.Be careful when using "==" to check against True or False Do not check if you can, just do it and handle the errorPythonistas usually say "It's easier to ask for forgiveness than permission".Don't :Do :Or even better with python 2.6+ / 3:It is much better because it's much more generical. You can apply "try / except" to almost anything. You don't need to care about what to do to prevent it, just about the error you are risking.Do not check against type Python is dynamically typed, therefore checking for type makes you lose flexibility. Instead, use duck typing by checking behavior. E.G, you expect a string in a function, then use str() to convert any object in a string. You expect a list, use list() to convert any iterable in a list.Don't :Do :Using the last way, foo will accept any object. Bar will accept strings, tuples, sets, lists and much more. Cheap DRY :-)Don't mix spaces and tabsJust don't. You would cry.Use object as first parentThis is tricky, but it will bite you as your program grows. There are old and new classes in Python 2.x. The old ones are, well, old. They lack some features, and can have awkward behavior with inheritance. To be usable, any of your class must be of the "new style". To do so, make it inherit from "object" :Don't :Do :In Python 3.x all classes are new style so you can declare class Father: is fine.Don't initialize class attributes outside the __init__ methodPeople coming from other languages find it tempting because that what you do the job in Java or PHP. You write the class name, then list your attributes and give them a default value. It seems to work in Python, however, this doesn't work the way you think.Doing that will setup class attributes (static attributes), then when you will try to get the object attribute, it will gives you its value unless it's empty. In that case it will return the class attributes. It implies two big hazards :Don't (unless you want static) :Do : When you need a population of arrays you might be tempted to type something like this:And sure enough it will give you what you expect when you look at itBut don't expect the elements of your population to be seperate objects:Unless this is what you need...It is worth mentioning a workaround:Python Language Gotchas -- things that fail in very obscure waysPython Design GotchasMutating a default argument:The default value is evaluated only once, and not every time the function is called. Repeated calls to foo() would return ['baz'], ['baz', 'baz'], ['baz', 'baz', 'baz'], ...If you want to mutate bar do something like this:Or, if you like arguments to be final:I don't know whether this is a common mistake, but while Python doesn't have increment and decrement operators, double signs are allowed, soandis syntactically correct code, but doesn't do anything "useful" or that you may be expecting.Rolling your own code before looking in the standard library. For example, writing this:When you could just use this:Examples of frequently overlooked modules (besides itertools) include:Avoid using keywords as your own identifiers. Also, it's always good to not use from somemodule import *.If you're coming from C++, realize that variables declared in a class definition are static.  You can initialize nonstatic members in the init method.Example:Surprised that nobody said this:Really, it's a killer. Believe me. In particular, if it runs.Not using functional tools.  This isn't just a mistake from a style standpoint, it's a mistake from a speed standpoint because a lot of the functional tools are optimized in C.This is the most common example:The correct way to do it:The just as correct way to do it:And if you only need the processed items available one at a time, rather than all at once, you can save memory and improve speed by using the iterable equivalentsCode Like a Pythonista: Idiomatic PythonNormal copying (assigning) is done by reference, so filling a container by adapting the same object and inserting, ends up with a container with references to the last added object.Use copy.deepcopy instead.Importing re and using the full regular expression approach to string matching/transformation, when perfectly good string methods exist for every common operation (e.g. capitalisation, simple matching/searching).Last link is the original one, this SO question is an duplicate.Using the %s formatter in error messages. In almost every circumstance, %r should be used.For example, imagine code like this:Printed this error:It's impossible to tell if the person variable is the string "wolever", the unicode string u"wolever" or an instance of the Person class (which has __str__ defined as def __str__(self): return self.name). Whereas, if %r was used, there would be three different error messages:Would produce the much more helpful errors:Another good reason for this is that paths are a whole lot easier to copy/paste. Imagine:If path is some path/with 'strange' "characters", the error message will be:Which is hard to visually parse and hard to copy/paste into a shell.Whereas, if %r is used, the error would be:Easy to visually parse, easy to copy-paste, all around better.A bad habit I had to train myself out of was using X and Y or Z for inline logic.Unless you can 100% always guarantee that Y will be a true value, even when your code changes in 18 months time, you set yourself up for some unexpected behaviour.Thankfully, in later versions you can use Y if X else Z.I would stop using deprecated methods in 2.6, so that your app or script will be ready and easier to convert to Python 3.I've started learning Python as well and one of the bigest mistakes I made is constantly using C++/C# indexed "for" loop. Python have for(i ; i < length ; i++) type loop and for a good reason - most of the time there are better ways to do there same thing.Example:

I had a method that iterated over a list and returned the indexes of selected items:Instead Python has list comprehension that solves the same problem in a more elegant and easy to read way:++n and --n may not work as expected by people coming from C or Java background.++n is positive of a positive number, which is simply n.--n is negative of a negative number, which is simply n.Beautiful is better than ugly.

Explicit is better than implicit.

Simple is better than complex.

Complex is better than complicated.

Flat is better than nested.

Sparse is better than dense.

Readability counts.

Special cases aren't special enough to break the rules.

Although practicality beats purity.

Errors should never pass silently.

Unless explicitly silenced.

In the face of ambiguity, refuse the temptation to guess.

There should be one-- and preferably only one --obvious way to do it.

Although that way may not be obvious at first unless you're Dutch.

Now is better than never.

Although never is often better than right now.

If the implementation is hard to explain, it's a bad idea.

If the implementation is easy to explain, it may be a good idea.

Namespaces are one honking great idea -- let's do more of those!  Write ugly code.

Write implicit code.

Write complex code.

Write nested code.

Write dense code.

Write unreadable code.

Write special cases.

Strive for purity.

Ignore errors and exceptions.

Write optimal code before releasing.

Every implementation needs a flowchart.

Don't use namespaces.  Never assume that having a multi-threaded Python application and a SMP capable machine (for instance one equipped with a multi-core CPU) will give you the benefit of introducing true parallelism into your application. Most likely it will not because of GIL (Global Interpreter Lock) which synchronizes your application on the byte-code interpreter level. There are some workarounds like taking advantage of SMP by putting the concurrent code in  C API calls or using multiple processes (instead of threads) via wrappers (for instance like the one available at http://www.parallelpython.org) but if one needs true multi-threading in Python one should look at things like Jython, IronPython etc. (GIL is a feature of the CPython interpreter so other implementations are not affected).According to Python 3000 FAQ (available at Artima) the above still stands even for the latest Python versions.Some personal opinions, but I find it best NOT to: Somewhat related to the default mutable argument, how one checks for the "missing" case results in differences when an empty list is passed:Here's the output:You've mentioned default arguments... One that's almost as bad as mutable default arguments: default values which aren't None.Consider a function which will cook some food:Because it specifies a default value for breakfast, it is impossible for some other function to say "cook your default breakfast" without a special-case:However, this could be avoided if cook used None as a default value:A good example of this is Django bug #6988. Django's caching module had a "save to cache" function which looked like this:But, for the memcached backend, a timeout of 0 means "never timeout"… Which, as you can see, would be impossible to specify.Don't modify a list while iterating over it.One common suggestion to work around this problem is to iterate over the list in reverse:But even better is to use a list comprehension to build a new list to replace the old:Common pitfall: default arguments are evaluated once:prints:i.e. you always get the same list.The very first mistake before you even start: don't be afraid of whitespace.When you show someone a piece of Python code, they are impressed until you tell them that they have to indent correctly. For some reason, most people feel that a language shouldn't force a certain style on them while all of them will indent the code nonetheless.Python won't warn you in any way that on the second assignment you misspelled the variable name and created a new one.Creating a local module with the same name as one from the stdlib.  This is almost always done by accident (as reported in this question), but usually results in cryptic error messages.This is something that I see a surprising amount in production code and it makes me cringe. Was the exception the one you want suppress, or is it more serious? But there are more subtle cases. That can make you pull your hair out trying to figure out.The problem is foo or baz could be the culprits too. I think this can be more insidious in that this is idiomatic python where you are checking your types for proper methods. But each method call has chance to return something unexpected and suppress bugs that should be raising exceptions.Knowing what exceptions a method can throw are not always obvious. For example, urllib and urllib2 use socket which has its own exceptions that percolate up and rear their ugly head when you least expect it.Exception handling is a productivity boon in handling errors over system level languages like C. But I have found suppressing exceptions improperly can create truly mysterious debugging sessions and take away a major advantage interpreted languages provide.   

pip installation /usr/local/opt/python/bin/python2.7: bad interpreter: No such file or directory

Mona Jalal

[pip installation /usr/local/opt/python/bin/python2.7: bad interpreter: No such file or directory](https://stackoverflow.com/questions/31768128/pip-installation-usr-local-opt-python-bin-python2-7-bad-interpreter-no-such-f)

I don't know what's the deal but I am stuck following some stackoverflow solutions which gets nowhere. Can you please help me on this?

2015-08-02 02:49:20Z

I don't know what's the deal but I am stuck following some stackoverflow solutions which gets nowhere. Can you please help me on this?I had used home-brew to install 2.7 on OS X 10.10 and the new install was missing the sym links. I ranas mentioned in How to symlink python in Homebrew? and it solved the problem.I'm guessing you have two python installs, or two pip installs, one of which has been partially removed.Why do you use sudo? Ideally you should be able to install and run everything from your user account instead of using root. If you mix root and your local account together you are more likely to run into permissions issues (e.g. see the warning it gives about "parent directory is not owned by the current user").What do you get if you run this?This will show you which python binary pip is trying to use. If it's pointing /usr/local/opt/python/bin/python2.7, then try running this:If this says "No such file or directory", then pip is trying to use a python binary that has been removed.Next, try this:To see the path of the python binary that's actually working.Since it looks like pip was successfully installed somewhere, it could be that /usr/local/bin/pip is part of an older installation of pip that's higher up on the PATH. To test that, you may try moving the non-functioning pip binary out of the way like this (might require sudo):Then try running your pip --version command again. Hopefully it picks up the correct version and runs successfully.Only solution in OSX and its variant.For this error:The source of this problem is a bad python path hardcoded in pip (which means it won't be fixed by e.g. changing your $PATH). That path is no longer hardcoded in the lastest version of pip, so a solution which should work is:But of course, this command uses pip, so it fails with the same error.The way to bootstrap yourself out of this mess:For me, I found this issue by first having the identical issue from virtualenv:The solution here is to runIf running that command gives the same error from pip, see above.I made the same error using sudo for my installation. (oops)This brought everything back to normal.Because I had both python 2 and 3 installed on Mac OSX I was having all sorts of errors.I used which to find the location of my python2.7 file (/usr/local/bin/python2.7)Then I symlinked my real python2.7 install location with the one the script expected:I had similar issue. Basically pip was looking in a wrong path (old installation path) or python. The following solution worked for me:In case it helps anyone, the solution mentioned in this other question worked for me when pip stopped working today after upgrading it: Pip broken after upgradingIt seems that it's an issue when a previously cached location changes, so you can refresh the cache with this command:To simplify to operation, we can use the below command to reinstall version 2:

