cannot import name patterns

Autokilled

[cannot import name patterns](https://stackoverflow.com/questions/8074955/cannot-import-name-patterns)

Before I wrote in urls.py, my code... everything worked perfectly. Now I have problems - can't go to my site. "cannot import name patterns"My urls.py is:They said what error is somewhere here.

2011-11-10 04:18:43Z

Before I wrote in urls.py, my code... everything worked perfectly. Now I have problems - can't go to my site. "cannot import name patterns"My urls.py is:They said what error is somewhere here.You don't need those imports. The only thing you need in your urls.py (to start) is:NOTE: This solution was intended for Django <1.6. This was actually the code generated by Django itself. For newer version, see Jacob Hume's answer.As of Django 1.10, the patterns module has been removed (it had been deprecated since 1.8).Luckily, it should be a simple edit to remove the offending code, since the urlpatterns should now be stored in a plain-old list:Yes:I met this problem too.patterns module is not supported.. mine worked with this.This is the code which worked for me. My django version is 1.10.4 finalPattern module in not available from django 1.8. So you need to remove pattern from your import and do something similar to the following:I Resolved it by cloning my project directly into Eclipse from GIT, Initially I was cloning it at specific location on file system then importing it as existing project into Eclipse.Seems you are using outdated version of django..

Simply update django and try again..

Following command will update your django version..pip install --upgrade django

How to convert int to Enum in python?

User

[How to convert int to Enum in python?](https://stackoverflow.com/questions/23951641/how-to-convert-int-to-enum-in-python)

Using the new Enum feature (via backport enum34) with python 2.7.6.Given the following definition, how can I convert an int to the corresponding Enum value?I know I can hand craft a series of if-statements to do the conversion but is there an easy pythonic way to convert? Basically, I'd like a function ConvertIntToFruit(int) that returns an enum value.My use case is I have a csv file of records where I'm reading each record into an object.  One of the file fields is an integer field that represents an enumeration.  As I'm populating the object I'd like to convert that integer field from the file into the corresponding Enum value in the object.

2014-05-30 09:50:36Z

Using the new Enum feature (via backport enum34) with python 2.7.6.Given the following definition, how can I convert an int to the corresponding Enum value?I know I can hand craft a series of if-statements to do the conversion but is there an easy pythonic way to convert? Basically, I'd like a function ConvertIntToFruit(int) that returns an enum value.My use case is I have a csv file of records where I'm reading each record into an object.  One of the file fields is an integer field that represents an enumeration.  As I'm populating the object I'd like to convert that integer field from the file into the corresponding Enum value in the object.You 'call' the Enum class:to turn 5 into Fruit.Orange:From the Programmatic access to enumeration members and their attributes section of the documentation:In a related note: to map a string value containing the name of an enum member, use subscription:I think it is in simple words is to convert the int value into Enum by calling EnumType(int_value), after that access the name of the Enum object:Or as a function:I wanted something similar so that I could access either part of the value pair from a single reference.  The vanilla version:The failure throws an exception as would be expected.A more robust version:When not used correctly this avoids creating an exception and, instead, passes back a fault indication.  A more Pythonic way to do this would be to pass back "None" but my particular application uses the text directly.

How to transform numpy.matrix or array to scipy sparse matrix

Flake

[How to transform numpy.matrix or array to scipy sparse matrix](https://stackoverflow.com/questions/7922487/how-to-transform-numpy-matrix-or-array-to-scipy-sparse-matrix)

For SciPy sparse matrix, one can use todense() or toarray() to transform to NumPy matrix or array. What are the functions to do the inverse?I searched, but got no idea what keywords should be the right hit.

2011-10-27 21:14:08Z

For SciPy sparse matrix, one can use todense() or toarray() to transform to NumPy matrix or array. What are the functions to do the inverse?I searched, but got no idea what keywords should be the right hit.You can pass a numpy array or matrix as an argument when initializing a sparse matrix. For a CSR matrix, for example, you can do the following.There are several sparse matrix classes in scipy. Any of them can do the conversion.See http://docs.scipy.org/doc/scipy/reference/sparse.html#usage-information .As for the inverse, the function is inv(A), but I won't recommend using it, since for huge matrices it is very computationally costly and unstable. Instead, you should use an approximation to the inverse, or if you want to solve Ax = b you don't really need A-1.

How can I print a Python file's docstring when executing it?

thias

[How can I print a Python file's docstring when executing it?](https://stackoverflow.com/questions/7791574/how-can-i-print-a-python-files-docstring-when-executing-it)

I have a Python script with a docstring. When the parsing of the command-line arguments does not succeed, I want to print the docstring for the user's information.Is there any way to do this?

2011-10-17 09:09:03Z

I have a Python script with a docstring. When the parsing of the command-line arguments does not succeed, I want to print the docstring for the user's information.Is there any way to do this?The docstring is stored in the module's __doc__ global.By the way, this goes for any module: import sys; print(sys.__doc__). Docstrings of functions and classes are also in their __doc__ attribute.Here is an alternative that does not hardcode the script's filename, but instead uses sys.argv[0] to print it. Using %(scriptName)s instead of %s improves readability of the code.Argument parsing should always be done with argparse.You can display the __doc__ string by passing it to the description parameter of Argparse:If you call this mysuperscript.py and execute it you get:I got such problem, walked over the web, and fortunately found the answer, learned  sys module, and created a script in Python, here is itby typing ./yourscriptname.py --help or python3 yourscriptname.py --help it will show your docstring

How to enable a virtualenv in a systemd service unit?

guettli

[How to enable a virtualenv in a systemd service unit?](https://stackoverflow.com/questions/37211115/how-to-enable-a-virtualenv-in-a-systemd-service-unit)

I want to "activate" a virtualenv in a systemd service file.I would like avoid to have a shell process between the systemd process and the python interpreter.My current solution looks like this:/etc/sysconfig/fooservice.envBut I am having trouble. I get ImportErrors since some enties in sys.path are missing.

2016-05-13 13:18:13Z

I want to "activate" a virtualenv in a systemd service file.I would like avoid to have a shell process between the systemd process and the python interpreter.My current solution looks like this:/etc/sysconfig/fooservice.envBut I am having trouble. I get ImportErrors since some enties in sys.path are missing.The virtualenv is "baked into the Python interpreter in the virtualenv". This means you can launch python or console_scripts directly in that virtualenv and don't need to activate the virtualenv first or manage PATH yourself.:orand remove the EnvironmentFile entry.To verify that it is indeed correct you can check sys.path by runningand comparing the output toWhile the path for libraries is indeed baked into the python interpreter of the virtualenv, I've had issues with python tools that were using binaries installed in that virtualenv. For instance, my apache airflow service wouldn't work because it couldn't find the gunicorn binary. To work around this, here's my ExecStart instruction, with an Environment instruction (which sets an environment variable for the service alone).ExecStartexplicitly uses the python interpreter of the virtualenv. I'm also adding a PATH variable, which adds the binary folder of the virtualenv before the system PATH. That way, I get the desired python libraries as well as binaries. Note that I'm using ansible to build this service, ergo the curly braces of jinja2.I'm not using virtualenv but pyenv: here is it just to use the real .pyenv path in the shebang and make sure it is in the PATHEx: pyenv activate flask-prod for user mortenb which is running in prod Then to my flask scripts starting in systemd *.service I add the following shebang:

Logging within py.test tests

superselector

[Logging within py.test tests](https://stackoverflow.com/questions/4673373/logging-within-py-test-tests)

I would like to put some logging statements within test function to examine some state variables.I have the following code snippet:I get the following output:Notice that only the logging messages from the '__name__ == __main__' block get transmitted to the console.Is there a way to force pytest to emit logging to console from test methods as well?

2011-01-12 19:53:52Z

I would like to put some logging statements within test function to examine some state variables.I have the following code snippet:I get the following output:Notice that only the logging messages from the '__name__ == __main__' block get transmitted to the console.Is there a way to force pytest to emit logging to console from test methods as well?Works for me, here's the output I get: [snip -> example was incorrect]Edit: It seems that you have to pass the -s option to py.test so it won't capture stdout. Here (py.test not installed), it was enough to use  python pytest.py -s pyt.py.For your code, all you need is to pass -s in args to main:See the py.test documentation on capturing output.Since version 3.3, pytest supports live logging, meaning that all the log records emitted in tests will be printed to the terminal immediately. The feature is documented under Live Logs section. Live logging is disabled by default; to enable it, set log_cli = 1 in the pytest.ini config1. Live logging supports emitting to terminal and file; the relevant options allow records customizing:Note: log_cli flag can't be passed from command line and must be set in pytest.ini. All the other options can be both passed from command line or set in the config file. As pointed out by Kévin Barré in this comment, overriding ini options from command line can be done via the -o/--override option. So instead of declaring log_cli in pytest.ini, you can simply call:Simple test file used for demonstrating:As you can see, no extra configuration needed; pytest will setup the logger automatically, based on options specified in pytest.ini or passed from command line.Configuration in pytest.ini:Running the test:Configuration in pytest.ini:Test run:1 Although you can configure pytest in setup.cfg under the [tool:pytest] section, don't be tempted to do that when you want to provide custom live logging format. Other tools reading setup.cfg might treat stuff like %(message)s as string interpolation and fail. Use pytest.ini to avoid errors.

Pythonically add header to a csv file

albus_c

[Pythonically add header to a csv file](https://stackoverflow.com/questions/20347766/pythonically-add-header-to-a-csv-file)

I wrote a Python script merging two csv files, and now I want to add a header to the final csv. I tried following the suggestions reported here and I got the following error: expected string, float found. What is the most pythonic way to fix this?Here is the code I am using:

2013-12-03 09:52:26Z

I wrote a Python script merging two csv files, and now I want to add a header to the final csv. I tried following the suggestions reported here and I got the following error: expected string, float found. What is the most pythonic way to fix this?Here is the code I am using:The DictWriter() class expects dictionaries for each row. If all you wanted to do was write an initial header, use a regular csv.writer() and pass in a simple row for the header:The alternative would be to generate dictionaries when copying across your data:You just add one additional row before you execute the loop. This row contains 

your CSV file header name.This worked for me. 

In python, why use logging instead of print?

Sam Odio

[In python, why use logging instead of print?](https://stackoverflow.com/questions/6918493/in-python-why-use-logging-instead-of-print)

For simple debugging in a complex project is there a reason to use the python logger instead of print?  What about other use-cases?  Is there an accepted best use-case for each (especially when you're only looking for stdout)?I've always heard that this is a "best practice" but I haven't been able to figure out why.

2011-08-02 20:54:41Z

For simple debugging in a complex project is there a reason to use the python logger instead of print?  What about other use-cases?  Is there an accepted best use-case for each (especially when you're only looking for stdout)?I've always heard that this is a "best practice" but I haven't been able to figure out why.The logging package has a lot of useful features:Print doesn't have any of these.Also, if your project is meant to be imported by other python tools, it's bad practice for your package to print things to stdout, since the user likely won't know where the print messages are coming from.  With logging, users of your package can choose whether or not they want to propogate logging messages from your tool or not.One of the biggest advantages of proper logging is that you can categorize messages and turn them on or off depending on what you need. For example, it might be useful to turn on debugging level messages for a certain part of the project, but tone it down for other parts, so as not to be taken over by information overload and to easily concentrate on the task for which you need logging.Also, logs are configurable. You can easily filter them, send them to files, format them, add timestamps, and any other things you might need on a global basis. Print statements are not easily managed.Print statements are sort of the worst of both worlds, combining the negative aspects of an online debugger with diagnostic instrumentation.  You have to modify the program but you don't get more, useful code from it.An online debugger allows you to inspect the state of a running program;  But the nice thing about a real debugger is that you don't have to modify the source; neither before nor after the debugging session;  You just load the program into the debugger, tell the debugger where you want to look, and you're all set.Instrumenting the application might take some work up front, modifying the source code in some way, but the resulting diagnostic output can have enormous amounts of detail, and can be turned on or off to a very specific degree.  The python logging module can show not just the message logged, but also the file and function that called it, a traceback if there was one, the actual time that the message was emitted, and so on.  More than that; diagnostic instrumentation need never be removed;  It's just as valid and useful when the program is finished and in production as it was the day it was added; but it can have it's output stuck in a log file where it's not likely to annoy anyone, or the log level can be turned down to keep all but the most urgent messages out.anticipating the need or use for a debugger is really no harder than using ipython while you're testing, and becoming familiar with the commands it uses to control the built in pdb debugger.  When you find yourself thinking that a print statement might be easier than using pdb (as it often is), You'll find that using a logger pulls your program in a much easier to work on state than if you use and later remove print statements.I have my editor configured to highlight print statements as syntax errors, and logging statements as comments, since that's about how I regard them.If you use logging then the person responsible for deployment can configure the logger to send it to a custom location, with custom information. If you only print, then that's all they get.Logging essentially creates a searchable plain text database of print outputs with other meta data (timestamp, loglevel, line number, process etc.).This is pure gold, I can run egrep over the log file after the python script has run.

I can tune my egrep pattern search to pick exactly what I am interested in and ignore the rest. This reduction of cognitive load and freedom to pick my egrep pattern later on by trial and error is the key benefit for me. Now throw in other cool things that print can't do (sending to socket, setting debug levels, logrotate, adding meta data etc.), you have every reason to prefer logging over plain print statements.I tend to use print statements because it's lazy and easy, adding logging needs some boiler plate code, hey we have yasnippets (emacs) and ultisnips (vim) and other templating tools, so why give up logging for plain print statements!?

Is there a need for range(len(a))?

Hyperboreus

[Is there a need for range(len(a))?](https://stackoverflow.com/questions/19184335/is-there-a-need-for-rangelena)

One frequently finds expressions of this type in python questions on SO. Either for just accessing all items of the iterableWhich is just a clumbersome way of writing:Or for assigning to elements of the iterable:Which should be the same as:Or for filtering over the indices:Which could be expressed like this:Or when you just need the length of the list, and not its content:Which could be:In python we have enumerate, slicing, filter, sorted, etc... As python for constructs are intended to iterate over iterables and not only ranges of integers, are there real-world use-cases where you need in range(len(a))?

2013-10-04 14:52:49Z

One frequently finds expressions of this type in python questions on SO. Either for just accessing all items of the iterableWhich is just a clumbersome way of writing:Or for assigning to elements of the iterable:Which should be the same as:Or for filtering over the indices:Which could be expressed like this:Or when you just need the length of the list, and not its content:Which could be:In python we have enumerate, slicing, filter, sorted, etc... As python for constructs are intended to iterate over iterables and not only ranges of integers, are there real-world use-cases where you need in range(len(a))?If you need to work with indices of a sequence, then yes - you use it... eg for the equivalent of numpy.argsort...:What if you need to access two elements of the list simultaneously?You can use this, but it's probably less clear:Personally I'm not 100% happy with either!Short answer: mathematically speaking, no, in practical terms, yes, for example for Intentional Programming.Technically, the answer would be "no, it's not needed" because it's expressible using other constructs. But in practice, I use for i in range(len(a) (or for _ in range(len(a)) if I don't need the index) to make it explicit that I want to iterate as many times as there are items in a sequence without needing to use the items in the sequence for anything.So: "Is there a need?"? — yes, I need it to express the meaning/intent of the code for readability purposes.See also: https://en.wikipedia.org/wiki/Intentional_programmingAnd obviously, if there is no collection that is associated with the iteration at all, for ... in range(len(N)) is the only option, so as to not resort to i = 0; while i < N; i += 1 ...Going by the comments as well as personal experience, I say no, there is no need for range(len(a)).  Everything you can do with range(len(a)) can be done in another (usually far more efficient) way.You gave many examples in your post, so I won't repeat them here.  Instead, I will give an example for those who say "What if I want just the length of a, not the items?".  This is one of the only times you might consider using range(len(a)).  However, even this can be done like so:Clements answer (as shown by Allik) can also be reworked to remove range(len(a)):So, in conclusion, range(len(a)) is not needed.   Its only upside is readability (its intention is clear).  But that is just preference and code style.Sometimes matplotlib requires range(len(y)), e.g., while y=array([1,2,5,6]), plot(y) works fine, scatter(y) does not.  One has to write scatter(range(len(y)),y).  (Personally, I think this is a bug in scatter; plot and its friends scatter and stem should use the same calling sequences as much as possible.)It's nice to have when you need to use the index for some kind of manipulation and having the current element doesn't suffice. Take for instance a binary tree that's stored in an array. If you have a method that asks you to return a list of tuples that contains each nodes direct children then you need the index.Of course if the element you're working on is an object, you can just call a get children method. But yea, you only really need the index if you're doing some sort of manipulation.I have an use case I don't believe any of your examples cover.I'm relatively new to  python though so  happy to  learn a more elegant approach.If you have to iterate over the first len(a) items of an object b (that is larger than a), you should probably use range(len(a)):Sometimes, you really don't care about the collection itself. For instance, creating a simple model fit line to compare an "approximation" with the raw data:In this case, the values of the Fibonacci sequence itself were irrelevant. All we needed here was the size of the input sequence we were comparing with.Very simple example:I can't think of a solution that does not use the range-len composition quickly.But probably instead this should be done with try .. except to stay pythonic i guess..My code is:It is a binary adder but I don't think the range len or the inside can be replaced to make it smaller/better.

SQL Alchemy ORM returning a single column, how to avoid common post processing

Derek Litz

[SQL Alchemy ORM returning a single column, how to avoid common post processing](https://stackoverflow.com/questions/9486180/sql-alchemy-orm-returning-a-single-column-how-to-avoid-common-post-processing)

I'm using SQL Alchemy's ORM and I find when I return a single column I get the results like so:With a set like this I find that I have to do this often:This isn't that "bad" because my result sets are usually small, but if they weren't this could add significant overhead. The biggest thing is I feel it clutters the source, and missing this step is a pretty common error I run into.Is there any way to avoid this extra step?A related aside: This behaviour of the orm seems inconvenient in this case, but another case where my result set was, [(id, value)] it ends up like this:I then can just do:This one has the advantage of making sense as a useful step after returning the results.Is this really a problem or am I just being a nitpick and the post processing after getting the result set makes sense for both cases? I'm sure we can think of some other common post processing operations to make the result set more usable in the application code.  Is there high performance and convenient solutions across the board or is post processing unavoidable, and merely required for varying application usages?When my application can actually take advantage of the objects that are returned by SQL Alchemy's ORM it seems extremely helpful, but in cases where I can't or don't, not so much.  Is this just a common problem of ORMs in general?  Am I better off not using the ORM layer in cases like this?I suppose I should show an example of the actual orm queries I'm talking about:orOf course, in a real query there'd normally be some filters, etc.

2012-02-28 16:51:25Z

I'm using SQL Alchemy's ORM and I find when I return a single column I get the results like so:With a set like this I find that I have to do this often:This isn't that "bad" because my result sets are usually small, but if they weren't this could add significant overhead. The biggest thing is I feel it clutters the source, and missing this step is a pretty common error I run into.Is there any way to avoid this extra step?A related aside: This behaviour of the orm seems inconvenient in this case, but another case where my result set was, [(id, value)] it ends up like this:I then can just do:This one has the advantage of making sense as a useful step after returning the results.Is this really a problem or am I just being a nitpick and the post processing after getting the result set makes sense for both cases? I'm sure we can think of some other common post processing operations to make the result set more usable in the application code.  Is there high performance and convenient solutions across the board or is post processing unavoidable, and merely required for varying application usages?When my application can actually take advantage of the objects that are returned by SQL Alchemy's ORM it seems extremely helpful, but in cases where I can't or don't, not so much.  Is this just a common problem of ORMs in general?  Am I better off not using the ORM layer in cases like this?I suppose I should show an example of the actual orm queries I'm talking about:orOf course, in a real query there'd normally be some filters, etc.Python's zip combined with the * inline expansion operator is a pretty handy solution to this:Then you only have to [0] index in once. For such a short list your comprehension is faster:However for longer lists zip should be faster:So it's up to you to determine which is better for your situation.One way to decrease the clutter in the source is to iterate like this:Although this solution is one character longer than using the [] operator, I think it's easier on the eyes. For even less clutter, remove the parenthesis. This makes it harder when reading the code, to notice that you're actually handling tuples, though:I struggled with this too until I realized it's just like any other query:I found the following more readable, also includes the answer for the dict (in Python 2.7):For the single value, borrowing from another answer:Compare with the built-in zip solution, adapted to the list:which in my timeits provides only about 4% speed improvements.My solution looks like this ;) NOTE: py3 only.Wow, guys, why strain? There are method steeper way, faster and more elegant)Speed:But if more elements in list - use only zip. Zip more speed.

How to dynamically change base class of instances at runtime?

Adam Parkin

[How to dynamically change base class of instances at runtime?](https://stackoverflow.com/questions/9539052/how-to-dynamically-change-base-class-of-instances-at-runtime)

This article has a snippet showing usage of __bases__ to dynamically change the inheritance hierarchy of some Python code, by adding a class to an existing classes collection of classes from which it inherits.  Ok, that's hard to read, code is probably clearer:That is, Person doesn't inherit from Friendly at the source level, but rather this inheritance relation is added dynamically at runtime by modification of the __bases__attribute of the Person class.  However, if you change Friendly and Person to be new style classes (by inheriting from object), you get the following error:A bit of Googling on this seems to indicate some incompatibilities between new-style and old style classes in regards to changing the inheritance hierarchy at runtime.  Specifically: "New-style class objects don't support assignment to their bases attribute".My question, is it possible to make the above Friendly/Person example work using new-style classes in Python 2.7+, possibly by use of the __mro__ attribute?Disclaimer: I fully realise that this is obscure code.  I fully realize that in real production code tricks like this tend to border on unreadable, this is purely a thought experiment, and for funzies to learn something about how Python deals with issues related to multiple inheritance.

2012-03-02 19:16:23Z

This article has a snippet showing usage of __bases__ to dynamically change the inheritance hierarchy of some Python code, by adding a class to an existing classes collection of classes from which it inherits.  Ok, that's hard to read, code is probably clearer:That is, Person doesn't inherit from Friendly at the source level, but rather this inheritance relation is added dynamically at runtime by modification of the __bases__attribute of the Person class.  However, if you change Friendly and Person to be new style classes (by inheriting from object), you get the following error:A bit of Googling on this seems to indicate some incompatibilities between new-style and old style classes in regards to changing the inheritance hierarchy at runtime.  Specifically: "New-style class objects don't support assignment to their bases attribute".My question, is it possible to make the above Friendly/Person example work using new-style classes in Python 2.7+, possibly by use of the __mro__ attribute?Disclaimer: I fully realise that this is obscure code.  I fully realize that in real production code tricks like this tend to border on unreadable, this is purely a thought experiment, and for funzies to learn something about how Python deals with issues related to multiple inheritance.Ok, again, this is not something you should normally do, this is for informational purposes only.  Where Python looks for a method on an instance object is determined by the __mro__ attribute of the class which defines that object (the M ethod R esolution O rder attribute).  Thus, if we could modify the __mro__ of Person, we'd get the desired behaviour.  Something like:The problem is that __mro__ is a readonly attribute, and thus setattr won't work.  Maybe if you're a Python guru there's a way around that, but clearly I fall short of guru status as I cannot think of one.A possible workaround is to simply redefine the class:What this doesn't do is modify any previously created Person instances to have the hello() method.  For example (just modifying main()):If the details of the type call aren't clear, then read e-satis' excellent answer on 'What is a metaclass in Python?'.I've been struggling with this too, and was intrigued by your solution, but Python 3 takes it away from us:I actually have a legitimate need for a decorator that replaces the (single) superclass of the decorated class. It would require too lengthy a description to include here (I tried, but couldn't get it to a reasonably length and limited complexity -- it came up in the context of the use by many Python applications of an Python-based enterprise server where different applications needed slightly different variations of some of the code.)The discussion on this page and others like it provided hints that the problem of assigning to __bases__ only occurs for classes with no superclass defined (i.e., whose only superclass is object). I was able to solve this problem (for both Python 2.7 and 3.2) by defining the classes whose superclass I needed to replace as being subclasses of a trivial class:I can not vouch for the consequences, but that this code does what you want at py2.7.2.We know that this is possible. Cool. But we'll never use it!I needed a solution for this which:Here's what I came up with:Used like this within the application:Use like this from within unit test code:Right of the bat, all the caveats of messing with class hierarchy dynamically are in effect. But if it has to be done then, apparently, there is a hack that get's around the "deallocator differs from 'object" issue when modifying the __bases__ attribute for the new style classes.You can define  a class objectWhich derives a class from the built-in metaclass type.

That's it, now your new style classes can modify the __bases__ without any problem.In my tests this actually worked very well as all existing (before changing the inheritance) instances of it and its derived classes felt the effect of the change including their mro getting updated.The above answers are good if you need to change an existing class at runtime. However, if you are just looking to create a new class that inherits by some other class, there is a much cleaner solution. I got this idea from https://stackoverflow.com/a/21060094/3533440, but I think the example below better illustrates a legitimate use case. Correct me if I'm wrong, but this strategy seems very readable to me, and I would use it in production code. This is very similar to functors in OCaml.

How to construct a defaultdict from a dictionary?

Karthick

[How to construct a defaultdict from a dictionary?](https://stackoverflow.com/questions/7539115/how-to-construct-a-defaultdict-from-a-dictionary)

If I have d=dict(zip(range(1,10),range(50,61))) how can I build a collections.defaultdict out of the dict? The only argument defaultdict seems to take is the factory function, will I have to initialize and then go through the original d and update the defaultdict?

2011-09-24 12:47:55Z

If I have d=dict(zip(range(1,10),range(50,61))) how can I build a collections.defaultdict out of the dict? The only argument defaultdict seems to take is the factory function, will I have to initialize and then go through the original d and update the defaultdict?Read the docs:Or given a dictionary d: 

Mesh grid functions in Python (meshgrid mgrid ogrid ndgrid)

scls

[Mesh grid functions in Python (meshgrid mgrid ogrid ndgrid)](https://stackoverflow.com/questions/12402045/mesh-grid-functions-in-python-meshgrid-mgrid-ogrid-ndgrid)

I'm looking for a clear comparison of meshgrid-like functions. Unfortunately I don't find it!Numpy http://docs.scipy.org/doc/numpy/reference/ providesScitools http://hplgit.github.io/scitools/doc/api/html/index.html providesIdeally a table summarizing all this would be perfect!

2012-09-13 08:12:27Z

I'm looking for a clear comparison of meshgrid-like functions. Unfortunately I don't find it!Numpy http://docs.scipy.org/doc/numpy/reference/ providesScitools http://hplgit.github.io/scitools/doc/api/html/index.html providesIdeally a table summarizing all this would be perfect!numpy.meshgrid is modelled after Matlab's meshgrid command. It is used to vectorise functions of two variables, so that you can writeSo ZZ contains all the combinations of x and y put into the function. When you think about it, meshgrid is a bit superfluous for numpy arrays, as they broadcast. This means you can doand get the same result.mgrid and ogrid are helper classes which use index notation so that you can create XX and YY in the previous examples directly, without having to use something like linspace. The order in which the output are generated is reversed.I am not familiar with the scitools stuff, but ndgrid seems equivalent to meshgrid, while BoxGrid is actually a whole class to help with this kind of generation.np.mgrid and np.meshgrid() do the same thing but the first and the second axis are swapped:yields False. Just swap the first two dimensions:yields True. 

How to get exception message in Python properly

FrozenHeart

[How to get exception message in Python properly](https://stackoverflow.com/questions/33239308/how-to-get-exception-message-in-python-properly)

What is the best way to get exceptions' messages from components of standard library in Python?I noticed that in some cases you can get it via message field like this:but in some cases (for example, in case of socket errors) you have to do something like this:I wondered is there any standard way to cover most of these situations?

2015-10-20 14:23:45Z

What is the best way to get exceptions' messages from components of standard library in Python?I noticed that in some cases you can get it via message field like this:but in some cases (for example, in case of socket errors) you have to do something like this:I wondered is there any standard way to cover most of these situations?If you look at the documentation for the built-in errors, you'll see that most Exception classes assign their first argument as a message attribute. Not all of them do though.Notably,EnvironmentError (with subclasses IOError and OSError) has a first argument of errno, second of strerror. There is no message... strerror is roughly analogous to what would normally be a message.More generally, subclasses of Exception can do whatever they want. They may or may not have a message attribute. Future built-in Exceptions may not have a message attribute. Any Exception subclass imported from third-party libraries or user code may not have a message attribute.I think the proper way of handling this is to identify the specific Exception subclasses you want to catch, and then catch only those instead of everything with an except Exception, then utilize whatever attributes that specific subclass defines however you want.If you must print something, I think that printing the caught Exception itself is most likely to do what you want, whether it has a message attribute or not.You could also check for the message attribute if you wanted, like this, but I wouldn't really suggest it as it just seems messy:To improve on the answer provided by @artofwarfare, here is what I consider a neater way to check for the message attribute and print it or print the Exception object as a fallback.The call to repr is optional, but I find it necessary in some use cases.Update #1:Following the comment by @MadPhysicist, here's a proof of why the call to repr might be necessary. Try running the following code in your interpreter:Update #2:Here is a demo with specifics for Python 2.7 and 3.5: https://gist.github.com/takwas/3b7a6edddef783f2abddffda1439f533I had the same problem. I think the best solution is to use log.exception, which will automatically print out stack trace and error message, such as:

How can I check if a string contains ANY letters from the alphabet?

Justin Papez

[How can I check if a string contains ANY letters from the alphabet?](https://stackoverflow.com/questions/9072844/how-can-i-check-if-a-string-contains-any-letters-from-the-alphabet)

What is best pure Python implementation to check if a string contains ANY letters from the alphabet?Where string_1 would return False for having no letters of the alphabet in it and string_2 would return True for having letter.

2012-01-31 00:29:55Z

What is best pure Python implementation to check if a string contains ANY letters from the alphabet?Where string_1 would return False for having no letters of the alphabet in it and string_2 would return True for having letter.Regex should be a fast approach:How about:You can use islower() on your string to see if it contains some lowercase letters (amongst other characters). or it with isupper() to also check if contains some uppercase letters:below: letters in the string: test yields truebelow: no letters in the string: test yields false.Not to be mixed up with isalpha() which returns True only if all characters are letters, which isn't what you want.Note that Barm's answer completes mine nicely, since mine doesn't handle the mixed case well.I liked the answer provided by @jean-françois-fabre, but it is incomplete.

His approach will work, but only if the text contains purely lower- or uppercase letters:The better approach is to first upper- or lowercase your string and then check.You can use regular expression like this:I tested each of the above methods for finding if any alphabets are contained in a given string and found out average processing time per string on a standard computer.

~250 ns for~3 µs for~6 µs for~850 ns forOpposite to as alleged, importing re takes negligible time, and searching with re takes just about half time as compared to iterating isalpha() even for a relatively small string.

Hence for larger strings and greater counts, re would be significantly more efficient.

But converting string to a case and checking case (i.e. any of upper().isupper() or lower().islower() ) wins here. In every loop it is significantly faster than re.search() and it doesn't even require any additional imports.You can also do this in additionAlso note that if variable val returns None. That means the search did not find a match 

How can I find all the subsets of a set, with exactly n elements?

Pietro Speroni

[How can I find all the subsets of a set, with exactly n elements?](https://stackoverflow.com/questions/374626/how-can-i-find-all-the-subsets-of-a-set-with-exactly-n-elements)

I am writing a program in Python, and I realized that a problem I need to solve requires me, given a set S with n elements (|S|=n), to test a function on all possible subsets of a certain order m (i.e. with m number of elements). To use the answer to produce a partial solution, and then try again with the next order m=m+1, until m=n.I am on my way to write a solution of the form:But knowing Python I expected a solution to be already there.What is the best way to accomplish this?

2008-12-17 14:09:18Z

I am writing a program in Python, and I realized that a problem I need to solve requires me, given a set S with n elements (|S|=n), to test a function on all possible subsets of a certain order m (i.e. with m number of elements). To use the answer to produce a partial solution, and then try again with the next order m=m+1, until m=n.I am on my way to write a solution of the form:But knowing Python I expected a solution to be already there.What is the best way to accomplish this?itertools.combinations is your friend if you have Python 2.6 or greater.  Otherwise, check the link for an implementation of an equivalent function.S: The set for which you want to find subsets

m: The number of elements in the subsetUsing the canonical function to get the powerset from the the itertools recipe page:Used like:map to sets if you want so you can use union, intersection, etc...:Here's a one-liner that gives you all subsets of the integers [0..n], not just the subsets of a given length:so e.g.Here's some pseudocode - you can cut same recursive calls by storing the values for each call as you go and before recursive call checking if the call value is already present.The following algorithm will have all the subsets excluding the empty set.So, for example if s = "123" then output is:Without using itertools:In Python 3 you can use yield from to add a subset generator method to buit-in set class:For example below snippet works as expected:Here is one neat way with easy to understand algorithm.

$ python -c "import itertools; a=[2,3,5,7,11]; print sum([list(itertools.combinations(a, i)) for i in range(len(a)+1)], [])"

[(), (2,), (3,), (5,), (7,), (11,), (2, 3), (2, 5), (2, 7), (2, 11), (3, 5), (3, 7), (3, 11), (5, 7), (5, 11), (7, 11), (2, 3, 5), (2, 3, 7), (2, 3, 11), (2, 5, 7), (2, 5, 11), (2, 7, 11), (3, 5, 7), (3, 5, 11), (3, 7, 11), (5, 7, 11), (2, 3, 5, 7), (2, 3, 5, 11), (2, 3, 7, 11), (2, 5, 7, 11), (3, 5, 7, 11), (2, 3, 5, 7, 11)]

Loading a file with more than one line of JSON into Pandas

user62198

[Loading a file with more than one line of JSON into Pandas](https://stackoverflow.com/questions/30088006/loading-a-file-with-more-than-one-line-of-json-into-pandas)

I am trying to read in a JSON file into Python pandas (0.14.0) data frame. Here is the first line line of the JSON file:I am trying do the following:df = pd.read_json(path).I am getting the following error (with full traceback):What is the Trailing data error? How do I read it into a data frame?Following some suggestions, here are few lines of the .json file:This .json file I am using contains one JSON object in each line as per the specification.I tried the jsonlint.com website as suggested and it gives the following error:

2015-05-06 21:34:39Z

I am trying to read in a JSON file into Python pandas (0.14.0) data frame. Here is the first line line of the JSON file:I am trying do the following:df = pd.read_json(path).I am getting the following error (with full traceback):What is the Trailing data error? How do I read it into a data frame?Following some suggestions, here are few lines of the .json file:This .json file I am using contains one JSON object in each line as per the specification.I tried the jsonlint.com website as suggested and it gives the following error:From version 0.19.0 of Pandas you can use the lines parameter, like so:You have to read it line by line. For example, you can use the following code provided by ryptophan on reddit:The following code helped me to load JSON content into a dataframe:I had a similar problem.It turns out that pd.read_json(myfile.json) will search in the parent folder automatically, but it returns this 'trailing data' error if you're not in the same folder as the file.I figured it out, because when I tried to do it with open('myfile.json', 'r'), and I got a FileNotFound error, so I checked the paths.I had failed to move myfile.json into the same folder as my notebook.Changing it to pd.read_json('../myfile.json') just worked.

Create empty conda environment

dangonfast

[Create empty conda environment](https://stackoverflow.com/questions/35860436/create-empty-conda-environment)

I can create a new conda environment, with program biopython with this:What if I do not want to install any program? It seems I can not do that:

2016-03-08 06:23:18Z

I can create a new conda environment, with program biopython with this:What if I do not want to install any program? It seems I can not do that:You can give a package name of just "python" to get a base, empty install. If you've created a create_default_packages block in your .condarc file, @joelion's answer will install those packages. If you don't want those, use the --no-default-packages flag. For example:To create an environment that is absolutely empty, without python and/or any other default package, just make a new folder in envs directory in your Anaconda installation (Anaconda3 in this example):.The first time that you activate this environment a directory named Scripts in Windows, bin in Linux, with a few batch files are created. At the time of this post this works for Anaconda version 4.3.30 both in Windows and Linux.I have noticed that @cel has suggested the same thing in the first comment under the question, but obviously it hasn't got the attention it deserves!

pip on Windows giving the error - Unknown or unsupported command 'install'

theharshest

[pip on Windows giving the error - Unknown or unsupported command 'install'](https://stackoverflow.com/questions/7469361/pip-on-windows-giving-the-error-unknown-or-unsupported-command-install)

I installed pip on Windows by downloading http://pypi.python.org/packages/source/p/pip/pip-1.0.2.tar.gz#md5=47ec6ff3f6d962696fe08d4c8264ad49 and running python setup.py installInstallation went fine with no errors.But when I tried installing selenium package with it, it gives me the following error -Where I'm making the mistake?

2011-09-19 10:17:07Z

I installed pip on Windows by downloading http://pypi.python.org/packages/source/p/pip/pip-1.0.2.tar.gz#md5=47ec6ff3f6d962696fe08d4c8264ad49 and running python setup.py installInstallation went fine with no errors.But when I tried installing selenium package with it, it gives me the following error -Where I'm making the mistake?Do you happen to have the Perl pip lying around somewhere?Sounds like the problem described here:https://github.com/mike-perdide/gitbuster/issues/62To check, in Windows command prompt execute:This will potentially output the following:If so, this is your problem. Unistall Strawberry Perl or use the full path to python pip.This error is because the system is finding pip.bat before it finds pip.exe.You do NOT need to uninstall Strawberry Perl or type the whole path.What I do is to simply type pip.exe (same number of keystrokes as apt-get) when I want to use the Python utility.  This method seems to work find for me on Win7 with Python(x,y) 2.7x and Strawberry Perl installed.Had the same problem under Ubuntu and did:I had this problem as well, and like Johannes said, it's because the perl pip is interfering with your Python pip.To get around it, you can simply do this as well:  python -m pip install <package_name>In addition to the very helpful nswer of Johannes:

If you don't want to uninstalll Strawberry, you can re-arrange the order of PATH entrys in your Windows system to ensure your Python\Scripts are found before the strawberry entries. If you don't want to do this manually, you can use tools like the "Rapid Environment Editor". You should provide path in environment variable for pip.exe file while executing install command you should use below commandThis will surely work, for me this worked :)You can also solve this problem without removing Strawberry Perl or type the whole path.Move to this C:\Python2.7\Scripts(your Python directory) directory,then use pip command.For Python 3.X and above:In the CMD prompt type: Make sure pip is installed already. 

Setup the environment variable for pip pointing to the exe fileTo upgrade: Same issue with DwimPerl. Uninstalling Dwim fixed the issue as well.C:\Python27\Scripts\pip.exe install -U seleniumI have the same problem in windows 10, finally resolved the problem successfully.I used the following command

where pipI was showing multiple installations of pip. Once removed other installation it worked fine.open cmd and type where pip and you will haveand go to C:\Dwimperl\perl and delete bin folder.

and again type where pip and you will only haveand enjoy python pip.

How can I make a scatter plot colored by density in matplotlib?

2964502

[How can I make a scatter plot colored by density in matplotlib?](https://stackoverflow.com/questions/20105364/how-can-i-make-a-scatter-plot-colored-by-density-in-matplotlib)

I'd like to make a scatter plot where each point is colored by the spatial density of nearby points.  I've come across a very similar question, which shows an example of this using R:R Scatter Plot: symbol color represents number of overlapping pointsWhat's the best way to accomplish something similar in python using matplotlib?

2013-11-20 19:39:30Z

I'd like to make a scatter plot where each point is colored by the spatial density of nearby points.  I've come across a very similar question, which shows an example of this using R:R Scatter Plot: symbol color represents number of overlapping pointsWhat's the best way to accomplish something similar in python using matplotlib?In addition to hist2d or hexbin as @askewchan suggested, you can use the same method that the accepted answer in the question you linked to uses.If you want to do that:If you'd like the points to be plotted in order of density so that the densest points are always on top (similar to the linked example), just sort them by the z-values.  I'm also going to use a smaller marker size here as it looks a bit better:You could make a histogram:Also, if the number of point makes KDE calculation too slow, color can be interpolated in np.histogram2d [Update in response to comments: If you wish to show the colorbar, use plt.scatter() instead of ax.scatter() followed by plt.colorbar()]:

How do I set headers using python's urllib?

ewok

[How do I set headers using python's urllib?](https://stackoverflow.com/questions/7933417/how-do-i-set-headers-using-pythons-urllib)

I am pretty new to python's urllib.  What I need to do is set a custom header for the request being sent to the server.  Specifically, I need to set the Content-type and Authorizations headers.  I have looked into the python documentation, but I haven't been able to find it.

2011-10-28 18:39:49Z

I am pretty new to python's urllib.  What I need to do is set a custom header for the request being sent to the server.  Specifically, I need to set the Content-type and Authorizations headers.  I have looked into the python documentation, but I haven't been able to find it.adding HTTP headers using urllib2:from the docs:For both Python 3 and Python 2, this works:Use urllib2 and create a Request object which you then hand to urlopen.

http://docs.python.org/library/urllib2.htmlI dont really use the "old" urllib anymore.untested....For multiple headers do as follow:

How can I return HTTP status code 204 from a Django view?

Flash

[How can I return HTTP status code 204 from a Django view?](https://stackoverflow.com/questions/12476452/how-can-i-return-http-status-code-204-from-a-django-view)

I want to return status code 204 No Content from a Django view. It is in response to an automatic POST which updates a database and I just need to indicate the update was successful (without redirecting the client).There are subclasses of HttpResponse to handle most other codes but not 204.What is the simplest way to do this?

2012-09-18 11:50:40Z

I want to return status code 204 No Content from a Django view. It is in response to an automatic POST which updates a database and I just need to indicate the update was successful (without redirecting the client).There are subclasses of HttpResponse to handle most other codes but not 204.What is the simplest way to do this?Either what Steve Mayne answered, or build your own by subclassing HttpResponse:When using render, there is a status keyword argument.(Note that in the case of status 204 there shouldn't be a response body, but this method is useful for other status codes.)

How to hide console window in python?

mdec

[How to hide console window in python?](https://stackoverflow.com/questions/764631/how-to-hide-console-window-in-python)

I am writing an IRC bot in Python. I wish to make stand-alone binaries for Linux and Windows of it. And mainly I wish that when the bot initiates, the console window should hide and the user should not be able to see the window.What can I do for that?

2009-04-19 01:35:30Z

I am writing an IRC bot in Python. I wish to make stand-alone binaries for Linux and Windows of it. And mainly I wish that when the bot initiates, the console window should hide and the user should not be able to see the window.What can I do for that?Simply save it with a .pyw extension. This will prevent the console window from opening.Explanation at the bottom of section 2.2.2In linux, just run it, no problem.  In Windows, you want to use the pythonw executable.Okay, if I understand the question in the comments, you're asking how to make the command window in which you've started the bot from the command line go away afterwards?I think that's right.  In any case, now you can close the terminal.Use nohup mypythonprog &, and you can close the terminal window without disrupting the process. You can also run exit if you are running in the cloud and don't want to leave a hanging shell process.Save the program with a .pyw extension and now it will open with pythonw.exe. No shell window. For example, if you have foo.py, you need to rename it to foo.pyw.If all you want to do is run your Python Script on a windows computer that has the Python Interpreter installed, converting the extension of your saved script from '.py' to '.pyw' should do the trick. But if you're using py2exe to convert your script into a standalone application that would run on any windows machine, you will need to make the following changes to your 'setup.py' file. The following example is of a simple python-GUI made using Tkinter:Change "console" in the code above to "windows"..This will only open the Tkinter generated GUI and no console window.This will hide your console. Implement this lines in your code first to start hiding your console at first.Some additional info. for situations that'll need the win32gui solution posted by Mohsen Haddadi earlier in this thread:As of python 361, win32gui & win32con are not part of the python std library.

To use them, pywin32 package will need to be installed; now possible via pip.More background info on pywin32 package is at: How to use the win32gui module with Python?.Also, to apply discretion while closing a window so as to not inadvertently close any window in the foreground, the resolution could be extended along the lines of the following:

Move column by name to front of table in pandas

Boosted_d16

[Move column by name to front of table in pandas](https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas)

Here is my df:How can I move a column by name ("Mid") to the front of the table, index 0. This is what the result should look like:My current code moves the column by index using df.columns.tolist() but I'd like to shift it by name. 

2014-08-04 15:21:31Z

Here is my df:How can I move a column by name ("Mid") to the front of the table, index 0. This is what the result should look like:My current code moves the column by index using df.columns.tolist() but I'd like to shift it by name. We can use ix to reorder by passing a list:Another method is to take a reference to the column and reinsert it at the front:You can also use loc to achieve the same result as ix will be deprecated in a future version of pandas from 0.20.0 onwards:You can use the df.reindex() function in pandas.

df isdefine an list of column namesmove the column name to wherever you wantthen use df.reindex() function to reorderout put is: dfMaybe I'm missing something, but a lot of these answers seem overly complicated.  You should be able to just set the columns within a single list:Column to the front:Or if instead, you want to move it to the back:Or if you wanted to move more than one column:I didn't like how I had to explicitly specify all the other column in the other solutions so this worked best for me. Though it might be slow for large dataframes...?df = df.set_index('Mid').reset_index()Here is a generic set of code that I frequently use to rearrange the position of columns. You may find it useful.I prefer this solution:It's simpler to read and faster than other suggested answers. Performance assessment:For this test, the currently last column is moved to the front in each repetition. In-place methods generally perform better. While citynorman's solution can be made in-place, Ed Chum's method based on .loc and sachinnm's method based on reindex cannot. While other methods are generic, citynorman's solution is limited to pos=0. I didn't observe any performance difference between df.loc[cols] and df[cols], which is why I didn't include some other suggestions.I tested with python 3.6.8 and pandas 0.24.2 on a MacBook Pro (Mid 2015).Results:To reorder the rows of a DataFrame just use a list as follows.This makes it very obvious what was done when reading the code later.  Also use:Then cut and paste to reorder.For a DataFrame with many columns, store the list of columns in a variable and pop the desired column to the front of the list. Here is an example:Now df.columns has.

Positional argument v.s. keyword argument

q0987

[Positional argument v.s. keyword argument](https://stackoverflow.com/questions/9450656/positional-argument-v-s-keyword-argument)

Based on thisQuestion> I assume that both width and height are positional arguments. Then why can we also call it with the keyword argument syntax?

2012-02-26 04:56:37Z

Based on thisQuestion> I assume that both width and height are positional arguments. Then why can we also call it with the keyword argument syntax?That text you quote is for the definition of the function and has nothing to do with calls to the function. In the call to that function, you're using the "named argument" feature. That link you provide is not a very good quality one, the authors seem confused between two different things.The Python reference refers to positional and keyword arguments only in respect to a call to a function (see section 5.3.4 Calls).When they talk about the definition of a function in section 7.6 Function definitions, it's a totally different term "default parameter values".I suspect the people who put together that course-ware weren't totally familiar with Python :-)By way of example, refer to the following definition and calls:The meaning of the = changes, depending on whether it's in the definition or in the call.In the definition, it marks the argument optional and sets a default value.In the call, it simply allows you to specify which arguments should be which values, in whatever order you want.A keyword argument is just a positional argument with a default value.  You must specify all arguments that don't have a default value.  In other words, keyword arguments are only "optional" because they will be set to their default value if not specifically supplied.Since python 3.8 introduced positional arguments only, this post need an update.Positional arguments, keyword arguments, required arguments and optional arguments are often confused. Positional arguments ARE NOT THE SAME AS required arguments. and keywords arguments ARE NOT THE SAME AS optional arguments. Positional arguments are arguments that can be called by their position in the function definition. Keyword arguments are arguments that can be called by their name.Required arguments are arguments that must passed to the function.Optional arguments are argument that can be not passed to the function. In python optional arguments are arguments that have a default value.Positional argument that is optional (python 3.8)Positional argument that is required (python 3.8)Keyword argument that is optionalkeyword argument that is requiredPositional and keyword argument that is optionalPositional and keyword argument that is requiredConclusion, an argument can be optional or required not both at the same time. It can also be positional, keyword or both at the same time.Python 3.8 introduced positional only parameters.Positional arguments can be called either using values in order or by naming each. For example, all three of the following would work the same way:positional arguments: arguments passed to a function in correct positional order. below program understand the positional arguments of a function suppose, we passed 'come' first, 'well' second, then the result will be comewell. also, call the function 3 strings become error. Understand the keyword arguments of a function.Keyword arguments are arguments that identify the parameters by their names. 

Getting indices of True values in a boolean list

Charles Smith

[Getting indices of True values in a boolean list](https://stackoverflow.com/questions/21448225/getting-indices-of-true-values-in-a-boolean-list)

I have a piece of my code where I'm supposed to create a switchboard. I want to return a list of all the switches that are on. Here "on" will equal True and "off" equal False. So now I just want to return a list of all the True values and their position. This is all I have but it only return the position of the first occurrence of True (this is just a portion of my code):This only returns "4"

2014-01-30 05:04:45Z

I have a piece of my code where I'm supposed to create a switchboard. I want to return a list of all the switches that are on. Here "on" will equal True and "off" equal False. So now I just want to return a list of all the True values and their position. This is all I have but it only return the position of the first occurrence of True (this is just a portion of my code):This only returns "4"Use enumerate, list.index returns the index of first match found.For huge lists, it'd be better to use itertools.compress:If you have numpy available:A much more efficient way is to use np.where. See the detailed comparison below, where it can be seen np.where outperforms both itertools.compress and also list comprehension. Below I have compared the solutions proposed by the accepted answer (@Ashwini Chaudhary) with using numpy.where. Also note that in Python 3, xrange() is deprecated, i.e. xrange() is removed from python 3.x.You can use filter for it:The range here enumerates elements of your list and since we want only those where self.states is True, we are applying a filter based on this condition.For Python > 3.0:Using element-wise multiplication and a set:Output:

{4, 5, 7}Simply do this:Use dictionary comprehension way, Input:Output:

permutations with unique values

xyz-123

[permutations with unique values](https://stackoverflow.com/questions/6284396/permutations-with-unique-values)

itertools.permutations generates where its elements are treated as unique based on their position, not on their value. So basically I want to avoid duplicates like this:Filtering afterwards is not possible because the amount of permutations is too large in my case.Does anybody know of a suitable algorithm for this?Thank you very much!EDIT:What I basically want is the following:which is not possible because sorted creates a list and the output of itertools.product is too large.Sorry, I should have described the actual problem.

2011-06-08 19:51:59Z

itertools.permutations generates where its elements are treated as unique based on their position, not on their value. So basically I want to avoid duplicates like this:Filtering afterwards is not possible because the amount of permutations is too large in my case.Does anybody know of a suitable algorithm for this?Thank you very much!EDIT:What I basically want is the following:which is not possible because sorted creates a list and the output of itertools.product is too large.Sorry, I should have described the actual problem.result:EDIT (how this works):  I rewrote the above program to be longer but more readable.I usually have a hard time explaining how something works, but let me try.

In order to understand how this works, you have to understand a similar but simpler program that would yield all permutations with repetitions.This program is obviously much simpler:

d stands for depth in permutations_helper and has two functions. One function is the stopping condition of our recursive algorithm, and the other is for the result list that is passed around.Instead of returning each result, we yield it. If there were no function/operator yield we would have to push the result in some queue at the point of the stopping condition. But this way, once the stopping condition is met, the result is propagated through all stacks up to the caller. That is the purpose of

for g in  perm_unique_helper(listunique,result_list,d-1): yield g

so each result is propagated up to caller.Back to the original program:

we have a list of unique elements. Before we can use each element, we have to check how many of them are still available to push onto result_list. Working with this program is very similar to permutations_with_replacement. The difference is that each element cannot be repeated more times than it is in perm_unique_helper.Because sometimes new questions are marked as duplicates and their authors are referred to this question it may be important to mention that sympy has an iterator for this purpose.This relies on the implementation detail that any permutation of a sorted iterable are in sorted order unless they are duplicates of prior permutations.givesRoughly as fast as Luka Rahne's answer, but shorter & simpler, IMHO.It works recursively by setting the first element (iterating through all unique elements), and iterating through the permutations for all remaining elements.Let's go through the unique_permutations of (1,2,3,1) to see how it works:You could try using set:The call to set removed duplicatesThis is my solution with 10 lines:--- Result ----A naive approach might be to take the set of the permutations:However, this technique wastefully computes replicate permutations and discards them.  A more efficient approach would be more_itertools.distinct_permutations, a third-party tool.CodePerformanceUsing a larger iterable, we will compare the performances between the naive and third-party techniques.We see more_itertools.distinct_permutations is an order of magnitude faster.DetailsFrom the source, a recursion algorithm (as seen in the accepted answer) is used to compute distinct permutations, thereby obviating wasteful computations.  See the source code for more details.It sound like you are looking for itertools.combinations() docs.python.orgHere is a recursive solution to the problem.Bumped into this question while looking for something myself !Here's what I did:Basically, make a set and keep adding to it. Better than making lists etc. that take too much memory..

Hope it helps the next person looking out :-) Comment out the set 'update' in the function to see the difference.The best solution to this problem I have seen uses Knuth's "Algorithm L" (as noted previously by Gerrat in the comments to the original post):

http://stackoverflow.com/questions/12836385/how-can-i-interleave-or-create-unique-permutations-of-two-stings-without-recurs/12837695Some timings:Sorting [1]*12+[0]*12 (2,704,156 unique permutations):

Algorithm L → 2.43 s

Luke Rahne's solution → 8.56 s

scipy.multiset_permutations() → 16.8 s  You can make a function that uses collections.Counter to get unique items and their counts from the given sequence, and uses itertools.combinations to pick combinations of indices for each unique item in each recursive call, and map the indices back to a list when all indices are picked:so that [''.join(i) for i in unique_permutations('moon')] returns:Came across this the other day while working on a problem of my own.  I like Luka Rahne's approach, but I thought that using the Counter class in the collections library seemed like a modest improvement.  Here's my code:This code returns each permutation as a list.  If you feed it a string, it'll give you a list of permutations where each one is a list of characters.  If you want the output as a list of strings instead (for example, if you're a terrible person and you want to abuse my code to help you cheat in Scrabble), just do the following:I came up with a very suitable implementation using itertools.product in this case (this is an implementation where you want all combinationsthis is essentially a combination (n over k) with n = X and somenumber = k

itertools.product() iterates from k = 0 to k = X subsequent filtering with count ensures that just the permutations with the right number of ones are cast into a list. you can easily see that it works when you calculate n over k and compare it to the len(unique_perm_list)To generate unique permutations of ["A","B","C","D"] I use the following:Which generates:Notice, duplicates are not created (e.g. items in combination with D are not generated, as they already exist). Example: This can then be used in generating terms of higher or lower order for OLS models via data in a Pandas dataframe.Creates...Which can then be piped to your OLS regressionAdapted to remove recursion, use a dictionary and numba for high performance but not using yield/generator style so memory usage is not limited:About 30% faster but still suffers a bit due to list copying and management.Alternatively without numba but still without recursion and using a generator to avoid memory issues:This is my attempt without resorting to set / dict, as a generator using recursion, but using string as input.  Output is also ordered in natural order:example:What about The problem is the permutations are now rows of a Numpy array, thus using more memory, but you can cycle through them as before

Does python urllib2 automatically uncompress gzip data fetched from webpage?

mlzboy

[Does python urllib2 automatically uncompress gzip data fetched from webpage?](https://stackoverflow.com/questions/3947120/does-python-urllib2-automatically-uncompress-gzip-data-fetched-from-webpage)

I'm using I want to know:

2010-10-16 00:45:48Z

I'm using I want to know:This checks if the content is gzipped and decompresses it:No. The urllib2 doesn't automatically uncompress the data because the 'Accept-Encoding' header is not set by the urllib2 but by you using: request.add_header('Accept-Encoding','gzip, deflate')If you are talking about a simple .gz file, no, urllib2 will not decode it, you will get the unchanged .gz file as output.If you are talking about automatic HTTP-level compression using Content-Encoding: gzip or deflate, then that has to be deliberately requested by the client using an Accept-Encoding header.urllib2 doesn't set this header, so the response it gets back will not be compressed. You can safely fetch the resource without having to worry about compression (though since compression isn't supported the request may take longer).Your question has been answered, but for a more comprehensive implementation, take a look at Mark Pilgrim's implementation of this, it covers gzip, deflate, safe URL parsing and much, much more, for a widely-used RSS parser, but nevertheless a useful reference.It appears urllib3 handles this automatically now.  Reference headers:  Reference code:  

How to remove symbols from a string with Python? [duplicate]

aaront

[How to remove symbols from a string with Python? [duplicate]](https://stackoverflow.com/questions/875968/how-to-remove-symbols-from-a-string-with-python)

I'm a beginner with both Python and RegEx, and I would like to know how to make a string that takes symbols and replaces them with spaces. Any help is great.For example:into:

2009-05-18 01:55:37Z

I'm a beginner with both Python and RegEx, and I would like to know how to make a string that takes symbols and replaces them with spaces. Any help is great.For example:into:One way, using regular expressions:Sometimes it takes longer to figure out the regex than to just write it out in python:If you need other characters you can change it to use a white-list or extend your black-list.Sample white-list:Sample white-list using a generator-expression:I often just open the console and look for the solution in the objects methods. Quite often it's already there:Short answer: Use string.replace().

Creating lowpass filter in SciPy - understanding methods and units

user3123955

[Creating lowpass filter in SciPy - understanding methods and units](https://stackoverflow.com/questions/25191620/creating-lowpass-filter-in-scipy-understanding-methods-and-units)

I am trying to filter a noisy heart rate signal with python. Because heart rates should never be above about 220 beats per minute, I want to filter out all noise above 220 bpm. I converted 220/minute into 3.66666666 Hertz and then converted that Hertz to rad/s to get 23.0383461 rad/sec.The sampling frequency of the chip that takes data is 30Hz so I converted that to rad/s to get 188.495559 rad/s.After looking up some stuff online I found some functions for a bandpass filter that I wanted to make into a lowpass. Here is the link the bandpass code, so I converted it to be this:I am very confused by this though because I am pretty sure the butter function takes in the cutoff and sampling frequency in rad/s but I seem to be getting a weird output. Is it actually in Hz?Secondly, what is the purpose of these two lines:I know it's something about normalization but I thought the nyquist was 2 times the sampling requency, not one half. And why are you using the nyquist as a normalizer?Can someone explain more about how to create filters with these functions?I plotted the filter using:and got this which clearly does not cut-off at 23 rad/s:

2014-08-07 20:29:12Z

I am trying to filter a noisy heart rate signal with python. Because heart rates should never be above about 220 beats per minute, I want to filter out all noise above 220 bpm. I converted 220/minute into 3.66666666 Hertz and then converted that Hertz to rad/s to get 23.0383461 rad/sec.The sampling frequency of the chip that takes data is 30Hz so I converted that to rad/s to get 188.495559 rad/s.After looking up some stuff online I found some functions for a bandpass filter that I wanted to make into a lowpass. Here is the link the bandpass code, so I converted it to be this:I am very confused by this though because I am pretty sure the butter function takes in the cutoff and sampling frequency in rad/s but I seem to be getting a weird output. Is it actually in Hz?Secondly, what is the purpose of these two lines:I know it's something about normalization but I thought the nyquist was 2 times the sampling requency, not one half. And why are you using the nyquist as a normalizer?Can someone explain more about how to create filters with these functions?I plotted the filter using:and got this which clearly does not cut-off at 23 rad/s:A few comments:Here's my modified version of your script, followed by the plot that it generates.

Why does Popen.communicate() return b'hi\n' instead of 'hi'?

imagineerThat

[Why does Popen.communicate() return b'hi\n' instead of 'hi'?](https://stackoverflow.com/questions/15374211/why-does-popen-communicate-return-bhi-n-instead-of-hi)

Can someone explain why the result I want, "hi", is preceded with a letter 'b' and followed with a newline?  I am using Python 3.3This extra 'b' does not appear if I run it with python 2.7

2013-03-12 23:24:04Z

Can someone explain why the result I want, "hi", is preceded with a letter 'b' and followed with a newline?  I am using Python 3.3This extra 'b' does not appear if I run it with python 2.7The echo command by default returns a newline characterCompare with this:As for the b preceding the string it indicates that it is a byte sequence which is equivilent to a normal string in Python 2.6+http://docs.python.org/3/reference/lexical_analysis.html#literalsThe b indicates that what you have is bytes, which is a binary sequence of bytes rather than a string of Unicode characters.  Subprocesses output bytes, not characters, so that's what communicate() is returning.The bytes type is not directly print()able, so you're being shown the repr of the bytes you have.  If you know the encoding of the bytes you received from the subprocess, you can use decode() to convert them into a printable str:Of course, this specific example only works if you actually are receiving ASCII from the subprocess.  If it's not ASCII, you'll get an exception:The newline is part of what echo hi has output.  echo's job is to output the parameters you pass it, followed by a newline.  If you're not interested in whitespace surrounding the process output, you can use strip() like so:As mentioned before, echo hi actually does return hi\n, which it is an expected behavior.But you probably want to just get the data in a "right" format and not deal with encoding. All you need to do is pass universal_newlines=True option to subprocess.Popen() like so:This way Popen() will replace these unwanted symbols by itself.b is the byte representation and \n is the result of echo output.Following will print only the result data

Why nested functions can access variables from outer functions, but are not allowed to modify them [duplicate]

Dhara

[Why nested functions can access variables from outer functions, but are not allowed to modify them [duplicate]](https://stackoverflow.com/questions/11987358/why-nested-functions-can-access-variables-from-outer-functions-but-are-not-allo)

In the 2nd case below, Python tries to look for a local variable. When it doesn't find one, why can't it look in the outer scope like it does for the 1st case? This looks for x in the local scope, then outer scope:This gives local variable 'x' referenced before assignment error:I am not allowed to modify the signature of function f2() so I can not pass and return values of x. However, I do need a way to modify x. Is there a way to explicitly tell Python to look for a variable name in the outer scope (something similar to the global keyword)?Python version: 2.7

2012-08-16 12:41:45Z

In the 2nd case below, Python tries to look for a local variable. When it doesn't find one, why can't it look in the outer scope like it does for the 1st case? This looks for x in the local scope, then outer scope:This gives local variable 'x' referenced before assignment error:I am not allowed to modify the signature of function f2() so I can not pass and return values of x. However, I do need a way to modify x. Is there a way to explicitly tell Python to look for a variable name in the outer scope (something similar to the global keyword)?Python version: 2.7Workaround is to use a mutable object and update members of that object. Name binding is tricky in Python, sometimes.In Python 3.x this is possible:The problem and a solution to it, for Python 2.x as well, are given in this post. Additionally, please read PEP 3104 for more information on this subject.

pypi see older versions of package [duplicate]

Marijus

[pypi see older versions of package [duplicate]](https://stackoverflow.com/questions/25104154/pypi-see-older-versions-of-package)

This is the package I'm interested in :https://pypi.python.org/pypi/django-filebrowser-no-grappelli/However the latest version no longer supports Django 1.3. I need to find a version that does.

How do I see list of older versions ?

2014-08-03 11:21:39Z

This is the package I'm interested in :https://pypi.python.org/pypi/django-filebrowser-no-grappelli/However the latest version no longer supports Django 1.3. I need to find a version that does.

How do I see list of older versions ?It's perhaps a little inelegant, but it appears that you can go to the URLAnd you will get a bunch of links to tarballs for the package.Ex:This is visible in the new UI for pypi:For example:You can use this short Python 3 script to grab the list of available versions for a package from PyPI using JSON API:Demo:Using pip you can find out all the available versions of that package:This will produce an output of all the available packages:Store the following code in get_version.py file:Run to get a sorted list of all package versions:If you are using pip to install your package, then you may use:Unfortunately the only available version seems to be:However, you can try to find another version on the Internet and install by:

How to test Python 3.4 asyncio code?

Marvin Killing

[How to test Python 3.4 asyncio code?](https://stackoverflow.com/questions/23033939/how-to-test-python-3-4-asyncio-code)

What's the best way to write unit tests for code using the Python 3.4 asyncio library? Assume I want to test a TCP client (SocketConnection):When running this test case with the default test runner, the test will always succeed as the method executes only up until the first yield from instruction, after which it returns before executing any assertions. This causes tests to always succeed.Is there a prebuilt test runner that is able to handle asynchronous code like this?

2014-04-12 17:43:33Z

What's the best way to write unit tests for code using the Python 3.4 asyncio library? Assume I want to test a TCP client (SocketConnection):When running this test case with the default test runner, the test will always succeed as the method executes only up until the first yield from instruction, after which it returns before executing any assertions. This causes tests to always succeed.Is there a prebuilt test runner that is able to handle asynchronous code like this?I temporarily solved the problem using a decorator inspired by Tornado's gen_test:Like J.F. Sebastian suggested, this decorator will block until the test method coroutine has finished. This allows me to write test cases like this:This solution probably misses some edge cases.I think a facility like this should added to Python's standard library to make asyncio and unittest interaction more convenient out of the box.async_test, suggested by Marvin Killing, definitely can help -- as well as direct calling loop.run_until_complete()But I also strongly recommend to recreate new event loop for every test and directly pass loop to API calls (at least asyncio itself accepts loop keyword-only parameter for every call that need it).Likethat isolates tests in test case and prevents strange errors like longstanding coroutine that has been created in test_a but finished only on test_b execution time.pytest-asyncio looks promising:Really like the async_test wrapper mentioned in https://stackoverflow.com/a/23036785/350195, here is an updated version for Python 3.5+Use this class instead of unittest.TestCase base class:Since Python 3.8 unittest comes with the IsolatedAsyncioTestCase function, designed for this purpose.You can also use aiounittest that takes similar approach as @Andrew Svetlov, @Marvin Killing answers and wrap it in easy to use AsyncTestCase class:As you can see the async case is handled by AsyncTestCase. It supports also synchronous test. There is a possibility to provide custom event loop, just override AsyncTestCase.get_event_loop.If you prefer (for some reason) the other TestCase class (eg unittest.TestCase), you might use async_test decorator:I usually define my async tests as coroutines and use a decorator for "syncing" them:In addition to pylover's answer, if you intend to use some other asynchronous method from the test class itself, the following implementation will work better - the only change was - and item.startswith('test_') in the __getattribute__ method.

How to mock users and requests in django

Purrell

[How to mock users and requests in django](https://stackoverflow.com/questions/2036202/how-to-mock-users-and-requests-in-django)

I have django code that interacts with request objects or user objects. For instance something like:If you were going to test with the django python shell or in a unittest, what would you pass in there? Here simply a User object will do, but the need for a mock request object also comes up frequently.For the shell or for unittests:

2010-01-10 05:20:46Z

I have django code that interacts with request objects or user objects. For instance something like:If you were going to test with the django python shell or in a unittest, what would you pass in there? Here simply a User object will do, but the need for a mock request object also comes up frequently.For the shell or for unittests:For request, I would use RequestFactory included with Django.for users, I would use django.contrib.auth.models.User as @ozan suggested and maybe with factory boy for speed (with factory boy you can choose to not to save to DB)Initialise a django.contrib.auth.models.User object. User.objects.create_user makes this easy.Initialise a django.http.HttpRequest object.Of course, there are shortcuts depending on what you want to do. If you just need an object with a user attribute that points to a user, simply create something (anything) and give it that attribute. You can either roll your own mocks, as Anurag Uniyal has suggested, or you can use a mocking framework.In response to those saying you can just create an ordinary user as you would anyway in Django... I would suggest this defeats the point of the unit test. A unit test shouldn't touch the database, but by creating a user, you've changed the database, hence why we would want to mock one.Read about mock objects here

http://en.wikipedia.org/wiki/Mock_object

http://www.mockobjects.com/And use this python lib to mock a user

http://python-mock.sourceforge.net/else you can write a simple User class yourself, use this as a starting pointadd specfic cases etc etcYou don't need to mock Users, as you can just create one within your test - the database is destroyed after the test is finished.To mock requests, use this snippet from Simon Willison.

Flask Value error view function did not return a response [duplicate]

user3890963

[Flask Value error view function did not return a response [duplicate]](https://stackoverflow.com/questions/25034123/flask-value-error-view-function-did-not-return-a-response)

Error message:And my code:and my-form.html is:If I put a while loop there it loads for ever.Then enter a temp higher than the current one the page loads forever.

And if I use the current code listed above it gives the error.

2014-07-30 10:09:34Z

Error message:And my code:and my-form.html is:If I put a while loop there it loads for ever.Then enter a temp higher than the current one the page loads forever.

And if I use the current code listed above it gives the error.The following does not return a response:You must return anything like return afunction() or return 'a string'.This can solve the issueYou are not returning a response object from your view my_form_post. The function ends with implicit return None, which Flask does not like.Make the function my_form_post return an explicit response, for exampleat the end of the function.

set random seed programwide in python

Mischa Obrecht

[set random seed programwide in python](https://stackoverflow.com/questions/11526975/set-random-seed-programwide-in-python)

I have a rather big program, where I use functions from the random module in different files. I would like to be able to set the random seed once, at one place, to make the program always return the same results. Can that even be achieved in python?

2012-07-17 16:36:58Z

I have a rather big program, where I use functions from the random module in different files. I would like to be able to set the random seed once, at one place, to make the program always return the same results. Can that even be achieved in python?The main python module that is run should import random and call random.seed(n) - this is shared between all other imports of random as long as somewhere else doesn't reset the seed.zss's comment should be highlighted as an actual answer:In the beginning of your application call random.seed(x) making sure x is always the same. This will ensure the sequence of pseudo random numbers will be the same during each run of the application.Jon Clements pretty much answers my question. However it wasn't the real problem:

It turns out, that the reason for my code's randomness was the numpy.linalg SVD because it does not always produce the same results for badly conditioned matrices !!So be sure to check for that in your code, if you have the same problems!You can guarantee this pretty easily by using your own random number generator.Just pick three largish primes (assuming this isn't a cryptography application), and plug them into a, b and c:

a = ((a * b) % c)

This gives a feedback system that produces pretty random data.  Note that not all primes work equally well, but if you're just doing a simulation, it shouldn't matter - all you really need for most simulations is a jumble of numbers with a pattern (pseudo-random, remember) complex enough that it doesn't match up in some way with your application.Knuth talks about this.

Is there a list of line styles in matplotlib?

Yotam

[Is there a list of line styles in matplotlib?](https://stackoverflow.com/questions/13359951/is-there-a-list-of-line-styles-in-matplotlib)

I'm writing a script that will do some plotting. I want it to plot several data series, each with its unique line style (not color). I can easily iterate through a list, but is there such a list already available in python? 

2012-11-13 11:28:01Z

I'm writing a script that will do some plotting. I want it to plot several data series, each with its unique line style (not color). I can easily iterate through a list, but is there such a list already available in python? According to the doc you could find them by doing this : You can do the same with markersEDIT: In the latest versions, there are still the same styles, but you can vary the space between dots/lines.plot documentationhttp://matplotlib.org/1.5.3/api/pyplot_api.html#matplotlib.pyplot.plot has a list of line + marker styles:Since this is part of the pyplot.plot docstring, you can also see it from the terminal with:From my experience it is nice to have the colors and markers in a list so I can iterate through them when plotting.Here's how I obtain the list of such things. It took some digging.In python3 the .keys() method returns a dict_keys object and not a list.

You need to pass the results to list() to be able to index the array as you could in python2. e.g. this SO questionSo to create iterable arrays for lines, colors and markers you can use something like.You can copy the dictionary from the Linestyle example:You can then iterate over the linestylesOr you just take a single linestyle out of those,I'm not directly answering the question of accessing a list, but it's useful to have one more alternative on this page: there is an additional way to generate dashed line styles.You can generate lines between A and B with transversal stripes likeby increasing the width of your line and specifying a custom dash pattern:ax.plot(x, y, dashes=[30, 5, 10, 5])The documentation for matplotlib.lines.Line2D says this about set_dashes(seq):I think it could have been said better: as it paints the line, the sequence of numbers specifies how many points are painted along the line, then how many points are left out (in case there are two numbers), how many are painted, how many are unpainted (in case of four numbers in the sequence). With four numbers, you can generate a dash‒dotted line: [30, 5, 3, 5] gives a 30-point-long dash, a 5-point gap, a 3-point dash (a dot), and a 5-point gap. Then it repeats.This page of the documentation explains this possibility. Look here for two different ways of doing it.

Python: How to check a string for substrings from a list? [duplicate]

user1045620

[Python: How to check a string for substrings from a list? [duplicate]](https://stackoverflow.com/questions/8122079/python-how-to-check-a-string-for-substrings-from-a-list)

I can't seem to find an equivalent of code that functions like this anywhere for Python:Basically, I'd like to check a string for substrings contained in a list.

2011-11-14 13:10:04Z

I can't seem to find an equivalent of code that functions like this anywhere for Python:Basically, I'd like to check a string for substrings contained in a list.Try this test:It will return True if any of the substrings in substring_list is contained in string.Note that there is a Python analogue of Marc Gravell's answer in the linked question:Probably the above version using a generator expression is clearer though.

Binary numbers in Python

John La Rooy

[Binary numbers in Python](https://stackoverflow.com/questions/1523465/binary-numbers-in-python)

How can I add, subtract, and compare binary numbers in Python without converting to decimal?

2009-10-06 03:37:55Z

How can I add, subtract, and compare binary numbers in Python without converting to decimal?You can convert between a string representation of the binary using bin() and int()I think you're confused about what binary is.  Binary and decimal are just different representations of a number - e.g. 101 base 2 and 5 base 10 are the same number.  The operations add, subtract, and compare operate on numbers - 101 base 2 == 5 base 10 and addition is the same logical operation no matter what base you're working in.  The fact that your python interpreter may store things as binary internally doesn't affect how you work with it - if you have an integer type, just use +, -, etc.If you have strings of binary digits, you'll have to either write your own implementation or convert them using the int(binaryString, 2) function.If you're talking about bitwise operators, then you're after:Otherwise, binary numbers work exactly the same as decimal numbers, because numbers are numbers, no matter how you look at them. The only difference between decimal and binary is how we represent that data when we are looking at it.Binary, decimal, hexadecimal... the base only matters when reading or outputting numbers, adding binary numbers is just the same as adding decimal number : it is just a matter of representation.Below is a re-write of a previously posted function: I leave the subtraction func as an exercise for the reader.Not sure if helpful, but I leave my solution here:I think you're confused about what binary is. Binary and decimal are just different representations of a number - e.g. 101 base 2 and 5 base 10 are the same number. The operations add, subtract, and compare operate on numbers - 101 base 2 == 5 base 10 and addition is the same logical operation no matter what base you're working in.x = x + 1

print(x)

a = x + 5

print(a)

Initialize a string variable in Python:「」or None?

legesh

[Initialize a string variable in Python:「」or None?](https://stackoverflow.com/questions/1398164/initialize-a-string-variable-in-python-or-none)

Suppose I have a class with a string instance attribute.

Should I initialize this attribute with "" value or None? Is either okay?orEdit: What I thought is that if I use "" as an initial value, I "declare" a variable to be of string type. And then I won't be able to assign any other type to it later. Am I right?Edit: I think it's important to note here, that my suggestion was WRONG. And there is no problem to assign another type to a variable. I liked a comment of S.Lott: "Since nothing in Python is "declared", you're not thinking about this the right way."

2009-09-09 07:49:17Z

Suppose I have a class with a string instance attribute.

Should I initialize this attribute with "" value or None? Is either okay?orEdit: What I thought is that if I use "" as an initial value, I "declare" a variable to be of string type. And then I won't be able to assign any other type to it later. Am I right?Edit: I think it's important to note here, that my suggestion was WRONG. And there is no problem to assign another type to a variable. I liked a comment of S.Lott: "Since nothing in Python is "declared", you're not thinking about this the right way."If not having a value has a meaning in your program (e.g. an optional value), you should use None. That's its purpose anyway. If the value must be provided by the caller of __init__, I would recommend not to initialize it.If "" makes sense as a default value, use it.In Python the type is deduced from the usage. Hence, you can change the type by just assigning a value of another type.None is used to indicate "not set", whereas any other value is used to indicate a "default" value.Hence, if your class copes with empty strings and you like it as a default value, use "". If your class needs to check if the variable was set at all, use None.Notice that it doesn't matter if your variable is a string initially. You can change it to any other type/value at any other moment.Another way to initialize an empty string is by using the built-in str() function with no arguments.In the original example, that would look like this:Personally, I believe that this better conveys your intentions.Notice by the way that str() itself sets a default parameter value of ''.It depends. If you want to distinguish between no parameter passed in at all, and an empty string passed in, you could use None.Either might be fine, but I don't think there is a definite answer.If you have a loop, say:then a potential future optimizer would know that str is always a string. If you used None, then it might not be a string until the first iteration, which would require loop unrolling to get the same effects. While this isn't a hypothetical (it comes up a lot in my PHP compiler) you should certainly never write your code to take this into account. I just thought it might be interesting :)Either is fine, though None is more common as a convention - None indicates that no value was passed for the optional parameter.There will be times when "" is the correct default value to use - in my experience, those times occur less often.Since both None and "" are false, you can do both. See 6.1. Truth Value Testing.EditTo answer the question in your edit: No, you can assign a different type.For lists or dicts, the answer is more clear,

according to http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#default-parameter-values

use None as default parameter.But also for strings, a (empty) string object is instanciated at runtime for

the keyword parameter.The cleanest way is probably:Either way is okay in python.  I would personally prefer "".  but again, either way is okay Python philosophy is to be readable.

That's why it's good practice to define your attributes in __init__() even if it's optional.

In the same spirit, you have to ask yourself what clearer for anyone who reads your code. In fact the type itself give much information about future use of your variable. So:Is syntaxically correct, but reader does not know much. Is it a string, a code as integer, a list, etc. ?Both say a little more, the first has the type in its name and the second in its declaration. I would go for the second, neater.=>Empty string is not set=>Empty string is not None

Using super with a class method

dezza

[Using super with a class method](https://stackoverflow.com/questions/1817183/using-super-with-a-class-method)

I'm trying to learn the super() function in Python.I thought I had a grasp of it until I came over this example (2.6) and found myself stuck.http://www.cafepy.com/article/python_attributes_and_methods/python_attributes_and_methods.html#super-with-classmethod-exampleIt wasn't what I expected when I read this line right before the example:Which is exactly what Python tells me is not possible by saying that do_something() should be called with an instance of B.

2009-11-29 23:45:58Z

I'm trying to learn the super() function in Python.I thought I had a grasp of it until I came over this example (2.6) and found myself stuck.http://www.cafepy.com/article/python_attributes_and_methods/python_attributes_and_methods.html#super-with-classmethod-exampleIt wasn't what I expected when I read this line right before the example:Which is exactly what Python tells me is not possible by saying that do_something() should be called with an instance of B.Sometimes texts have to be read more for the flavor of the idea rather than for the details. This is one of those cases.In the linked page, Examples 2.5, 2.6 and 2.7 should all use one method, do_your_stuff. (That is, do_something should be changed to do_your_stuff.) In addition, as Ned Deily pointed out, A.do_your_stuff has to be a class method.super(B, cls).do_your_stuff

returns a bound method (see footnote 2).  Since  cls was passed as the second argument to super(), it is cls that gets bound to the returned method. In other words, cls gets passed as the first argument to the method do_your_stuff() of class A.To reiterate: super(B, cls).do_your_stuff() causes A's do_your_stuff method to be

called with cls passed as the first argument. In order for that to work, A's

do_your_stuff has to be a class method. The linked page doesn't mention that,

but that is definitively the case.PS. do_something = classmethod(do_something) is the old way of making a classmethod.

The new(er) way is to use the @classmethod decorator.Note that super(B, cls) can not be replaced by super(cls, cls). Doing so could lead to infinite loops. For example,will raise RuntimeError: maximum recursion depth exceeded while calling a Python object. If cls is C, then super(cls, cls) searches C.mro() for the class that comes after C.Since that class is B, when cls is C, super(cls, cls).do_your_stuff() always calls B.do_your_stuff. Since super(cls, cls).do_your_stuff() is called inside B.do_your_stuff, you end up calling B.do_your_stuff in an infinite loop.In Python3, the 0-argument form of super was added so super(B, cls) could be replaced by super(), and Python3 will figure out from context that super() in the definition of class B should be equivalent to super(B, cls).But in no circumstance is super(cls, cls) (or for similar reasons, super(type(self), self)) ever correct.In Python 3, you can skip specifying arguments for super,I've updated the article to make it a bit clearer: Python Attributes and Methods # SuperYour example using classmethod above shows what a class method is - it passes the class itself instead of the instance as the first parameter. But you don't even need an instance to call the method, for e.g.:The example from the web page seems to work as published.  Did you create a do_something method for the superclass as well but not make it into a classmethod?  Something like this will give you that error:I think I've understood the point now thanks to this beatiful site and lovely community.If you don't mind please correct me if I'm wrong on classmethods (which I am now trying to understand fully):I hope this illustration shows ..

How do I install a pip package globally instead of locally?

sorin

[How do I install a pip package globally instead of locally?](https://stackoverflow.com/questions/36936212/how-do-i-install-a-pip-package-globally-instead-of-locally)

I am trying to install flake8 package using pip3 and it seems that it refuses to install because is already installed in one local location. How can I force it to install globally (system level)?Please note that I would prefer a generic solution (that should work on Debian, OS X maybe even Windows), one that should be used on any platform so I don't want to specify the destination myself.For some weird reason it behaves like I already specified --user which in my case I didn't.The only way I was able to install a package globally was to first remove it and install it again after this. Somehow it seems that pip (8.1.1) refuses to install a package globally if it exists locally.Disclaimer: No virtual environments were used or harmed during the experiments.

2016-04-29 10:41:45Z

I am trying to install flake8 package using pip3 and it seems that it refuses to install because is already installed in one local location. How can I force it to install globally (system level)?Please note that I would prefer a generic solution (that should work on Debian, OS X maybe even Windows), one that should be used on any platform so I don't want to specify the destination myself.For some weird reason it behaves like I already specified --user which in my case I didn't.The only way I was able to install a package globally was to first remove it and install it again after this. Somehow it seems that pip (8.1.1) refuses to install a package globally if it exists locally.Disclaimer: No virtual environments were used or harmed during the experiments.Why don't you try sudo with the H flag? This should do the trick.A regular sudo pip install flake8 will try to use your own home directory. The -H instructs it to use the system's home directory. More info at https://stackoverflow.com/a/43623102/Maybe --force-reinstall would work, otherwise --ignore-installed should do the trick.Are you using virtualenv? If yes, deactivate the virtualenv. If you are not using, it is already installed widely (system level). Try to upgrade package.I actually don‘t see your issue. Globally is any package which is in your python3 path‘s site package folder.If you want to use it just locally then you must configure a virtualenv and reinstall the packages with an activated virtual environment.

Convert Python ElementTree to string

Martijn Pieters

[Convert Python ElementTree to string](https://stackoverflow.com/questions/15304229/convert-python-elementtree-to-string)

Whenever I call ElementTree.tostring(e), I get the following error message:Is there any other way to convert an ElementTree object into an XML string?TraceBack:

2013-03-08 22:13:28Z

Whenever I call ElementTree.tostring(e), I get the following error message:Is there any other way to convert an ElementTree object into an XML string?TraceBack:Element objects have no .getroot() method. Drop that call, and the .tostring() call works:For Python 3:For Python 2:For compatibility with both Python 2 & 3:Output:Despite what the name implies, ElementTree.tostring() returns a bytestring by default in Python 2 & 3. This is an issue in Python 3, which uses Unicode for strings.Source: Porting Python 2 Code to Python 3If we know what version of Python is being used, we can specify the encoding as unicode or utf-8. Otherwise, if we need compatibility with both Python 2 & 3, we can use decode() to convert into the correct type.For reference, I've included a comparison of .tostring() results between Python 2 and Python 3. Thanks to Martijn Peters for pointing out that the str datatype changed between Python 2 and 3.In most scenarios, using str() would be the "cannonical" way to convert an object to a string. Unfortunately, using this with Element returns the object's location in memory as a hexstring, rather than a string representation of the object's data.

Create random list of integers in Python

Stiggo

[Create random list of integers in Python](https://stackoverflow.com/questions/4172131/create-random-list-of-integers-in-python)

I'd like to create a random list of integers for testing purposes. The distribution of the numbers is not important. The only thing that is counting is time. I know generating random numbers is a time-consuming task, but there must be a better way.Here's my current solution:v2 is faster than v1, but it is not working on such a large scale. It gives the following error:Is there a fast, efficient solution that works at that scale?Andrew's: 0.000290962934494gnibbler's: 0.0058455221653KennyTM's: 0.00219276118279NumPy came, saw, and conquered.

2010-11-13 10:52:21Z

I'd like to create a random list of integers for testing purposes. The distribution of the numbers is not important. The only thing that is counting is time. I know generating random numbers is a time-consuming task, but there must be a better way.Here's my current solution:v2 is faster than v1, but it is not working on such a large scale. It gives the following error:Is there a fast, efficient solution that works at that scale?Andrew's: 0.000290962934494gnibbler's: 0.0058455221653KennyTM's: 0.00219276118279NumPy came, saw, and conquered.It is not entirely clear what you want, but I would use numpy.random.randint:which gives on my machine:Note that randint is very different from random.sample (in order for it to work in your case I had to change the 1,000 to 10,000 as one of the commentators pointed out -- if you really want them from 0 to 1,000 you could divide by 10).And if you really don't care what distribution you are getting then it is possible that you either don't understand your problem very well, or random numbers -- with apologies if that sounds rude...All the random methods end up calling random.random() so the best way is to call it directly:For example,NumPy is much faster still of course.Your question about performance is moot—both functions are very fast. The speed of your code will be determined by what you do with the random numbers.However it's important you understand the difference in behaviour of those two functions. One does random sampling with replacement, the other does random sampling without replacement.Firstly, you should use randrange(0,1000) or randint(0,999), not randint(0,1000). The upper limit of randint is inclusive.For efficiently, randint is simply a wrapper of randrange which calls random, so you should just use random. Also, use xrange as the argument to sample, not range.You could useto generate 10,000 numbers in the range using sample 10 times.(Of course this won't beat NumPy.)But since you don't care about the distribution of numbers, why not just use:?

should I call close() after urllib.urlopen()?

Nikita

[should I call close() after urllib.urlopen()?](https://stackoverflow.com/questions/1522636/should-i-call-close-after-urllib-urlopen)

I'm new to Python and reading someone else's code:should urllib.urlopen() be followed by urllib.close()? Otherwise, one would leak connections, correct?

2009-10-05 21:59:04Z

I'm new to Python and reading someone else's code:should urllib.urlopen() be followed by urllib.close()? Otherwise, one would leak connections, correct?The close method must be called on the result of urllib.urlopen, not on the urllib module itself as you're thinking about (as you mention urllib.close -- which doesn't exist).The best approach: instead of x = urllib.urlopen(u) etc, use:The with statement, and the closing context manager, will ensure proper closure even in presence of exceptions.Like @Peter says, out-of-scope opened URLs will become eligible for garbage collection.However, also note that urllib.py defines:This means that when the reference count for that instance reaches zero, its __del__ method will be called, and thus its close method will be called as well.  The most "normal" way for the reference count to reach zero is to simply let the instance go out of scope, but there's nothing strictly stopping you from an explicit del x early (however it doesn’t directly call __del__ but just decrements the reference count by one).It's certainly good style to explicitly close your resources -- especially when your application runs the risk of using too much of said resources -- but Python will automatically clean up for you if you don't do anything funny like maintaining (circular?) references to instances that you don't need any more.Strictly speaking, this is true. But in practice, once (if) urllib goes out of scope, the connection will be closed by the automatic garbage collector.You basically do need to explicitly close your connection when using IronPython. The automatic closing on going out of scope relies on the garbage collection. I ran into a situation where the garbage collection did not run for so long that Windows ran out of sockets. I was polling a webserver at high frequency (i.e. as high as IronPython and the connection would allow, ~7Hz). I could see the "established connections" (i.e. sockets in use) go up and up on PerfMon. The solution was to call gc.collect() after every call to urlopen.

Custom PyCharm docstring stubs (i.e. for google docstring or numpydoc formats)

kalefranz

[Custom PyCharm docstring stubs (i.e. for google docstring or numpydoc formats)](https://stackoverflow.com/questions/18666885/custom-pycharm-docstring-stubs-i-e-for-google-docstring-or-numpydoc-formats)

Does PyCharm 2.7 (or will PyCharm 3) have support for custom docstring and doctest stubs?  If so, how does one go about writing this specific type of custom extension?My current project has standardized on using the Google Python Style Guide (http://google-styleguide.googlecode.com/svn/trunk/pyguide.html).  I love PyCharm's docstring support, but it's only two supported formats right now are epytext and reStructureText.  I want, and am willing to write myself, a PyCharm plugin that creates a documentation comment stub formatted in either Google or Numpydoc style (https://pypi.python.org/pypi/sphinxcontrib-napoleon/). Of special importance here is incorporating the type inference abilities that PyCharm has with the other two documentation types.

2013-09-06 21:31:37Z

Does PyCharm 2.7 (or will PyCharm 3) have support for custom docstring and doctest stubs?  If so, how does one go about writing this specific type of custom extension?My current project has standardized on using the Google Python Style Guide (http://google-styleguide.googlecode.com/svn/trunk/pyguide.html).  I love PyCharm's docstring support, but it's only two supported formats right now are epytext and reStructureText.  I want, and am willing to write myself, a PyCharm plugin that creates a documentation comment stub formatted in either Google or Numpydoc style (https://pypi.python.org/pypi/sphinxcontrib-napoleon/). Of special importance here is incorporating the type inference abilities that PyCharm has with the other two documentation types.With PyCharm 5.0 we finally got to select Google and NumPy Style Python Docstrings templates. It is also mentioned in the whatsnew section for PyCharm 5.0.How to change the Docstring Format:There you can choose from the available Docstrings formats: As pointed out by jstol: for Mac users, this is under As CrazyCoder mentions, its a ticket. Right now, you can only use EpyType and reStructuredText.Just to be make @Nras answer explicit, as of PyCharm 5.0:

Converting「yield from」statement to Python 2.7 code

vkaul11

[Converting「yield from」statement to Python 2.7 code](https://stackoverflow.com/questions/17581332/converting-yield-from-statement-to-python-2-7-code)

I had a code below in Python 3.2 and I wanted to run it in Python 2.7. I did convert it (have put the code of missing_elements in both versions) but I am not sure if that is the most efficient way to do it. Basically what happens if there are two yield from calls like below in upper half and lower half in missing_element function? Are the entries from the two halves (upper and lower) appended to each other in one list so that the parent recursion function with the yield from call and use both the halves together?

2013-07-10 21:37:36Z

I had a code below in Python 3.2 and I wanted to run it in Python 2.7. I did convert it (have put the code of missing_elements in both versions) but I am not sure if that is the most efficient way to do it. Basically what happens if there are two yield from calls like below in upper half and lower half in missing_element function? Are the entries from the two halves (upper and lower) appended to each other in one list so that the parent recursion function with the yield from call and use both the halves together?If you don't use the results of your yields,* you can always turn this:… into this:There might be a performance cost,** but there is never a semantic difference.No! The whole point of iterators and generators is that you don't build actual lists and append them together.But the effect is similar: you just yield from one, then yield from another.If you think of the upper half and the lower half as "lazy lists", then yes, you can think of this as a "lazy append" that creates a larger "lazy list". And if you call list on the result of the parent function, you of course will get an actual list that's equivalent to appending together the two lists you would have gotten if you'd done yield list(…) instead of yield from ….But I think it's easier to think of it the other way around: What it does is exactly the same the for loops do.If you saved the two iterators into variables, and looped over itertools.chain(upper, lower), that would be the same as looping over the first and then looping over the second, right? No difference here. In fact, you could implement chain as just:* Not the values the generator yields to its caller, the value of the yield expressions themselves, within the generator (which come from the caller using the send method), as described in PEP 342. You're not using these in your examples. And I'm willing to bet you're not in your real code. But coroutine-style code often uses the value of a yield from expression—see PEP 3156 for examples. Such code usually depends on other features of Python 3.3 generators—in particular, the new StopIteration.value from the same PEP 380 that introduced yield from—so it will have to be rewritten. But if not, you can use the PEP also shows you the complete horrid messy equivalent, and you can of course pare down the parts you don't care about. And if you don't use the value of the expression, it pares down to the two lines above.** Not a huge one, and there's nothing you can do about it short of using Python 3.3 or completely restructuring your code. It's exactly the same case as translating list comprehensions to Python 1.5 loops, or any other case when there's a new optimization in version X.Y and you need to use an older version.Replace them with for-loops:The same about elements:I just came across this issue and my usage was a bit more difficult since I needed the return value of yield from:This cannot be represented as a simple for loop but can be reproduced with this:Hopefully this will help people who come across the same problem. :)I think I found a way to emulate Python 3.x yield from construct in Python 2.x. It's not efficient and it is a little hacky, but here it is:Usage:Produces output:Maybe someone finds this helpful.Known issues: Lacks support for send() and various corner cases described in PEP 380. These could be added and I will edit my entry once I get it working.What about using the definition from pep-380 in order to construct a Python 2 syntax version:The statement:is semantically equivalent to:In a generator, the statement:is semantically equivalent toexcept that, as currently, the exception cannot be caught by except clauses within the returning generator.The StopIteration exception behaves as though defined thusly:I've found using resource contexts (using the python-resources module) to be an elegant mechanism for implementing subgenerators in Python 2.7.  Conveniently I'd already been using the resource contexts anyway.If in Python 3.3 you would have:In Python 2.7 you would have:Note how the complicated-logic operations only require the registration as a resource.

Will OrderedDict become redundant in Python 3.7?

James Hiew

[Will OrderedDict become redundant in Python 3.7?](https://stackoverflow.com/questions/50872498/will-ordereddict-become-redundant-in-python-3-7)

From the Python 3.7 changelog:Would this mean that OrderedDict will become redundant? The only use I can think of it will be to maintain backwards compatibility with older versions of Python which don't preserve insertion-order for normal dictionaries.

2018-06-15 09:16:37Z

From the Python 3.7 changelog:Would this mean that OrderedDict will become redundant? The only use I can think of it will be to maintain backwards compatibility with older versions of Python which don't preserve insertion-order for normal dictionaries.No it won't become redundant in Python 3.7 because OrderedDict is not just a dict that retains insertion order, it also offers an order dependent method, OrderedDict.move_to_end(), and supports reversed() iteration*. Moreover, equality comparisons with OrderedDict are order sensitive and this is still not the case for dict in Python 3.7, for example:Two relevant questions here and here.* Support for reversed() iteration of regular Python dict is added for Python 3.8, see issue33462

Is there a Generic python library to consume REST based services? [closed]

chiggsy

[Is there a Generic python library to consume REST based services? [closed]](https://stackoverflow.com/questions/4355997/is-there-a-generic-python-library-to-consume-rest-based-services)

Ok.  I want to consume REST based services.  I'd like to use python. In fact, I am going to use python.The way I'd like to use it is from the command line/ipython, to try out different REST services, with intention of formally coding it later. ( my usage of the REST service not the generic api thing ) I'm looking for a pretty generic, fully featured REST client/API in python.  Not bare bones, but plush, nice to use.  There are tons of them out there, but I'd kind of like to settle on one and master it.  Any suggestions?EDIT:  This is one:https://github.com/benoitc/restkitEDIT: http://pypi.python.org/pypi/requests  is exactly it.EDIT: http://pypi.python.org/pypi/siesta  is just as perfect!

2010-12-04 21:43:16Z

Ok.  I want to consume REST based services.  I'd like to use python. In fact, I am going to use python.The way I'd like to use it is from the command line/ipython, to try out different REST services, with intention of formally coding it later. ( my usage of the REST service not the generic api thing ) I'm looking for a pretty generic, fully featured REST client/API in python.  Not bare bones, but plush, nice to use.  There are tons of them out there, but I'd kind of like to settle on one and master it.  Any suggestions?EDIT:  This is one:https://github.com/benoitc/restkitEDIT: http://pypi.python.org/pypi/requests  is exactly it.EDIT: http://pypi.python.org/pypi/siesta  is just as perfect!The problem with having a "plush" REST client library is that REST itself isn't that well-defined. REST, in and of itself, just means that you want to use HTTP standards whenever possible, but other than that, the field is wide open.Is the data encoded with JSON? What are the URL schemes and what do they mean?Since REST basically just means HTTP, your best bet is httplib, but I wouldn't describe it as plush.I've mostly used just urllib2 or httplib2. I haven't really found a use for a general purpose REST client.Something like a generic REST library is hardly possible since each REST interface is different. Whith WADL there has been an attempt to establish a WSDL-lik interface description language for RESTful services. Using such a description a generic client would be possible but no one seems to care about WADL. And everyone seems fine with that.

Installing multiple versions of a package with pip

limboy

[Installing multiple versions of a package with pip](https://stackoverflow.com/questions/6570635/installing-multiple-versions-of-a-package-with-pip)

In my application I would like to use:How can I install multiple versions of packageX with pip to handle this situation?

2011-07-04 11:07:22Z

In my application I would like to use:How can I install multiple versions of packageX with pip to handle this situation?pip won't help you with this. You can tell it to install a specific version, but it will override the other one. On the other hand, using two virtualenvs will let you install both versions on the same machine, but not use them at the same time.You best bet is to install both version manually, by putting them in your Python path with a different name.But if your two libs expect them to have the same name (and they should), you will have to modify them so they pick up the version they need with some import alias such as:There is currently no clean way to do this. The best you can hope is for this hack to work.I'd rather ditch one of the two libs and replace it with an equivalent, or patch it to accept the new version of the dependency and give the patch back to the community.Download the source for ea. package.  Install each on its own separate folder. For example.  I had version 1.10 package, but wanted to switch to the dev version for some work. I downloaded the source for the dev module:

git clone https://github.com/networkx/networkx.git

cd netwokrx

I created a folder for this version:

mkdir /home/username/opt/python, then I set the PYTHONPATH env var to: export PYTHONPATH=/home/username/opt/python/lib/python2.7/site-packages/.  Next, I installed it using: python setup.py install --prefix=/home/username/opt/pythonNow, since my PYTHONPATH is now pointing to this other site-packages folder, when I run python on the command line, and import the new module, it works. To switch switch back, remove the new folder from PYTHONPATH.

Django and Bootstrap: What app is recommended? [closed]

jbrendel

[Django and Bootstrap: What app is recommended? [closed]](https://stackoverflow.com/questions/11821116/django-and-bootstrap-what-app-is-recommended)

I want to start using Twitter's Bootstrap for a recently started Django app. I have quite a bit of experience with Django, but I'm totally new to Bootstrap.What's the best way to proceed? Are there any particular Boostrap apps for Django you would recommend or have experience with?I understand that I could use Bootstrap directly, without any special Bootstrap-specific Django apps. However, I also read that the form rendering doesn't come out particularly well without a little server side support (rendering the Bootstrap specific CSS into the form HTML, for example).There seem to be several projects, such as crispy forms, django-bootstrap-toolkit, etc. Looking at their project pages, I can see different levels of activity and support. If I decide to go with one of those, I would of course like to pick one which has some momentum and therefore a good likelihood of staying supported and maintained for a while. This is very important and so even if the particular app doesn't have all possible features or is a bit less flexible, it might still be a good choice, due to support/freshness, availability of examples, etc.Thank you for any recommendations or feedback.

2012-08-05 23:52:11Z

I want to start using Twitter's Bootstrap for a recently started Django app. I have quite a bit of experience with Django, but I'm totally new to Bootstrap.What's the best way to proceed? Are there any particular Boostrap apps for Django you would recommend or have experience with?I understand that I could use Bootstrap directly, without any special Bootstrap-specific Django apps. However, I also read that the form rendering doesn't come out particularly well without a little server side support (rendering the Bootstrap specific CSS into the form HTML, for example).There seem to be several projects, such as crispy forms, django-bootstrap-toolkit, etc. Looking at their project pages, I can see different levels of activity and support. If I decide to go with one of those, I would of course like to pick one which has some momentum and therefore a good likelihood of staying supported and maintained for a while. This is very important and so even if the particular app doesn't have all possible features or is a bit less flexible, it might still be a good choice, due to support/freshness, availability of examples, etc.Thank you for any recommendations or feedback.I used django-bootstrap-toolkit — as the author explains (with reference to other Django/Bootstrap integration apps) In addition to forms, then (which can be as simple as {% form|as_bootstrap %}, this app helps with pagination, inserting static URLs to the Bootstrap media files, and some other bits. Read templatetags/bootstrap-toolkit.py for the full list.@dolan notes that the django-bootstrap-toolkit developer recommends a new project for Bootstrap v3 support, django-bootstrap3. I haven't tried this yet as I haven't started a new project since V3 came out, so YMMV.I've been using django-crispy-forms with bootstrap for the last couple of months and it has been quite useful. Forms render exactly as they're meant to. If you do any custom form rendering though, be prepared to define your forms in code rather than in template, using helpers. Another option to consider is django-bootstrap-form.I found crispy-forms to be too heavyweight for my needs. Django-bootstrap is not maintained any more. I tried django-bootstrap-toolkit and django-bootstrap-form, and while it does look like django-bootstrap-toolkit has more features, I found my needs were met by django-bootstrap-form. In the interest of keeping things simple, I chose the latter, and haven't found it lacking (although I'd be interested to hear other opinions on their relative merits).In addition to django-bootstrap-form, I'd recommend django-widget-tweaks, which allows you to add classes (and other attributes) to your forms using template filters, e.g.:This lets you use class-based formatting from Bootstrap while keeping the presentation logic in the template.I've been using django-bootstrap. No complaints so far. 

python packaging for relative imports

eerne

[python packaging for relative imports](https://stackoverflow.com/questions/4348452/python-packaging-for-relative-imports)

First off all: I'm sorry, I know there has been lots of question about relative imports, but I just didn't find a solution. If possible I would like to use the following directory layout:Now my questions are:So far I couldn't find an elegant solution for this. From what I understand Guido's Decision it should be possible to do from ..src import myClass but this will error:ValueError: Attempted relative import in non-packageWhich looks as it doesn't treat myClass as packages. Reading the docs:It seems I'm missing something that specifies where the scripts of the package are, should I use .pth ?

2010-12-03 18:05:40Z

First off all: I'm sorry, I know there has been lots of question about relative imports, but I just didn't find a solution. If possible I would like to use the following directory layout:Now my questions are:So far I couldn't find an elegant solution for this. From what I understand Guido's Decision it should be possible to do from ..src import myClass but this will error:ValueError: Attempted relative import in non-packageWhich looks as it doesn't treat myClass as packages. Reading the docs:It seems I'm missing something that specifies where the scripts of the package are, should I use .pth ?Means you attempt to use relative import in the module which is not package. Its problem with the file which has this from ... import statement, and not the file which you are trying to import.So if you are doing relative imports in your tests, for example, you should make your tests to be part of your package. This means If you run something as python myClass/test/demo.py, relative imports will not work too since you are running demo module not as package. Relative imports require that the module which uses them is being imported itself either as package module, from myClass.test.demo import blabla, or with relative import.After hours of searching last night I found the answer to relative imports in python!! Or an easy solution at the very least. The best way to fix this is to have the modules called from another module. So say you want demo.py to import myClass.py. In the myClass folder at the root of the sub-packages they need to have a file that calls the other two. From what I gather the working directory is always considered __main__ so if you test the import from demo.py with the demo.py script, you will receive that error. To illustrate:Folder hierarchy:myClass.py:demo.py:main.py:If you run demo.py in the interpreter, you will generate an error, but running main.py will not. It's a little convoluted, but it works :DIntra-package-references describes how to myClass from test/*. To import the package from outside, you should add its path to PYTHONPATH environment variable before running the importer application, or to sys.path list in the code before importing it.Why from ..src import myClass fails: probably, src is not a python package, you cannot import from there. You should add it to python path as described above.

Why does the size of this Python String change on a failed int conversion

jeremycg

[Why does the size of this Python String change on a failed int conversion](https://stackoverflow.com/questions/47062184/why-does-the-size-of-this-python-string-change-on-a-failed-int-conversion)

From the tweet here:We get 74, then 77 bytes for the two getsizeof calls.It looks like we are adding 3 bytes to the object, from the failed int call.Some more examples from twitter (you may need to restart python to reset the size back to 74):77!74, then 77.

2017-11-01 19:21:49Z

From the tweet here:We get 74, then 77 bytes for the two getsizeof calls.It looks like we are adding 3 bytes to the object, from the failed int call.Some more examples from twitter (you may need to restart python to reset the size back to 74):77!74, then 77.The code that converts strings to ints in CPython 3.6 requests a UTF-8 form of the string to work with:and the string creates the UTF-8 representation the first time it's requested and caches it on the string object:The extra 3 bytes are for the UTF-8 representation.You might be wondering why the size doesn't change when the string is something like '40' or 'plain ascii text'. That's because if the string is in "compact ascii" representation, Python doesn't create a separate UTF-8 representation. It returns the ASCII representation directly, which is already valid UTF-8:You also might wonder why the size doesn't change for something like '１'. That's U+FF11 FULLWIDTH DIGIT ONE, which int treats as equivalent to '1'. That's because one of the earlier steps in the string-to-int process iswhich converts all whitespace characters to ' ' and converts all Unicode decimal digits to the corresponding ASCII digits. This conversion returns the original string if it doesn't end up changing anything, but when it does make changes, it creates a new string, and the new string is the one that gets a UTF-8 representation created.As for the cases where calling int on one string looks like it affects another, those are actually the same string object. There are many conditions under which Python will reuse strings, all just as firmly in Weird Implementation Detail Land as everything we've discussed so far. For 'ñ', the reuse happens because this is a single-character string in the Latin-1 range ('\x00'-'\xff'), and the implementation stores and reuses those.

Differences and uses between WSGI, CGI, FastCGI, and mod_python in regards to Python?

Parker

[Differences and uses between WSGI, CGI, FastCGI, and mod_python in regards to Python?](https://stackoverflow.com/questions/3937224/differences-and-uses-between-wsgi-cgi-fastcgi-and-mod-python-in-regards-to-py)

I'm just wondering what the differences and advantages are for the different CGI's out there. Which one would be best for python scripts, and how would I tell the script what to use?

2010-10-14 20:22:45Z

I'm just wondering what the differences and advantages are for the different CGI's out there. Which one would be best for python scripts, and how would I tell the script what to use?A part answer to your question, including scgi.Lazy and not writing it on my own. From the wikipedia: http://en.wikipedia.org/wiki/FastCGIInstead of creating a new process for each request, FastCGI uses persistent processes to handle such requests. Multiple processes can configured, increasing stability and scalability. Each individual FastCGI process can handle many requests over its lifetime, thereby avoiding the overhead of per-request process creation and terminationThere's also a good background reader  on CGI, WSGI and other options, in the form of an official python HOWTO: http://docs.python.org/2/howto/webservers.htmlIn a project like Django, you can use a WSGI (Web Server Gateway Interface) server from the Flup module.A WSGI server wraps a back-end process using one or more protocols:In Detail Diff between FastCGI vs CGI

Combining two sorted lists in Python

dbr

[Combining two sorted lists in Python](https://stackoverflow.com/questions/464342/combining-two-sorted-lists-in-python)

I have two lists of objects. Each list is already sorted by a property of the object that is of the datetime type. I would like to combine the two lists into one sorted list. Is the best way just to do a sort or is there a smarter way to do this in Python?

2009-01-21 09:14:08Z

I have two lists of objects. Each list is already sorted by a property of the object that is of the datetime type. I would like to combine the two lists into one sorted list. Is the best way just to do a sort or is there a smarter way to do this in Python?People seem to be over complicating this.. Just combine the two lists, then sort them:..or shorter (and without modifying l1):..easy! Plus, it's using only two built-in functions, so assuming the lists are of a reasonable size, it should be quicker than implementing the sorting/merging in a loop. More importantly, the above is much less code, and very readable.If your lists are large (over a few hundred thousand, I would guess), it may be quicker to use an alternative/custom sorting method, but there are likely other optimisations to be made first (e.g not storing millions of datetime objects)Using the timeit.Timer().repeat() (which repeats the functions 1000000 times), I loosely benchmarked it against ghoseb's solution, and sorted(l1+l2) is substantially quicker:merge_sorted_lists took..sorted(l1+l2) took..This hasn't been mentioned, so I'll go ahead - there is a merge stdlib function in the heapq module of python 2.6+. If all you're looking to do is getting things done, this might be a better idea. Of course, if you want to implement your own, the merge of merge-sort is the way to go.Here's the documentation.Long story short, unless len(l1 + l2) ~ 1000000 use:Description of the figure and source code can be found here. The figure was generated by the following command:This is simply merging. Treat each list as if it were a stack, and continuously pop the smaller of the two stack heads, adding the item to the result list, until one of the stacks is empty. Then add all remaining items to the resulting list.There is a slight flaw in ghoseb's solution, making it O(n**2), rather than O(n).

The problem is that this is performing:With linked lists or deques this would be an O(1) operation, so wouldn't affect complexity, but since python lists are implemented as vectors, this copies the rest of the elements of l1 one space left, an O(n) operation.  Since this is done each pass through the list, it turns an O(n) algorithm into an O(n**2) one.  This can be corrected by using a method that doesn't alter the source lists, but just keeps track of the current position.I've tried out benchmarking a corrected algorithm vs a simple sorted(l1+l2) as suggested by dbrI've tested these with lists generated withFor various sizes of list, I get the following timings (repeating 100 times):So in fact, it looks like dbr is right, just using sorted() is preferable unless you're expecting very large lists, though it does have worse algorithmic complexity.  The break even point being at around a million items in each source list (2 million total).One advantage of the merge approach though is that it is trivial to rewrite as a generator, which will use substantially less memory (no need for an intermediate list).[Edit]

I've retried this with a situation closer to the question - using a list of objects containing a field "date" which is a datetime object.

The above algorithm was changed to compare against .date instead, and the sort method was changed to:This does change things a bit.  The comparison being more expensive means that the number we perform becomes more important, relative to the constant-time speed of the implementation.  This means merge makes up lost ground, surpassing the sort() method at 100,000 items instead.  Comparing based on an even more complex object (large strings or lists for instance) would likely shift this balance even more.[1]: Note: I actually only did 10 repeats for 1,000,000 items and scaled up accordingly as it was pretty slow.This is simple merging of two sorted lists. Take a look at the sample code below which merges two sorted lists of integers.This should work fine with datetime objects. Hope this helps.The output:I bet this is faster than any of the fancy pure-Python merge algorithms, even for large data. Python 2.6's heapq.merge is a whole another story.Python's sort implementation "timsort" is specifically optimized for lists that contain ordered sections.  Plus, it's written in C.  http://bugs.python.org/file4451/timsort.txt

http://en.wikipedia.org/wiki/TimsortAs people have mentioned, it may call the comparison function more times by some constant factor (but maybe call it more times in a shorter period in many cases!).I believe the Python developers are committed to keeping timsort, or at least keeping a sort that's O(n) in this case.Right, sorting in the general case can't be faster than that.  But since O() is an upper bound, timsort being O(n log n) on arbitrary input doesn't contradict its being O(n) given sorted(L1) + sorted(L2).An implementation of the merging step in Merge Sort that iterates through both lists:I'm still learning about algorithms, please let me know if the code could be improved in any aspect, your feedback is appreciated, thanks!Recursive implementation is below. Average performance is O(n).or generator with improved space complexity:Well, the naive approach (combine 2 lists into large one and sort) will be O(N*log(N)) complexity. On the other hand, if you implement the merge manually (i do not know about any ready code in python libs for this, but i'm no expert) the complexity will be O(N), which is clearly faster.

The idea is described wery well in post by Barry Kelly.Use the 'merge' step of merge sort, it runs in O(n) time.From wikipedia (pseudo-code):If you want to do it in a manner more consistent with learning what goes on in the iteration try thisHave used merge step of the merge sort. But I have used generators.  Time complexity O(n)Will sort the list in place. Define your own function for comparing two objects, and pass that function into the built in sort function.Do NOT use bubble sort, it has horrible performance.This is my solution in linear time without editing l1 and l2:This code has time complexity O(n) and can merge lists of any data type, given a quantifying function as the parameter func. It produces a new merged list and does not modify either of the lists passed as arguments.Hope this helps. Pretty Simple and straight forward:l1 = [1, 3, 4, 7]l2 = [0, 2, 5, 6, 8, 9]l3 = l1 + l2l3.sort()print (l3)[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

Serialising an Enum member to JSON

Bilal Syed Hussain

[Serialising an Enum member to JSON](https://stackoverflow.com/questions/24481852/serialising-an-enum-member-to-json)

How do I serialise a Python Enum member to JSON, so that I can deserialise the resulting JSON back into a Python object?  For example, this code:results in the error:How can I avoid that?

2014-06-30 01:24:12Z

How do I serialise a Python Enum member to JSON, so that I can deserialise the resulting JSON back into a Python object?  For example, this code:results in the error:How can I avoid that?If you want to encode an arbitrary enum.Enum member to JSON and then decode

it as the same enum member (rather than simply the enum member's value attribute), you can do so by writing a custom JSONEncoder class, and a decoding function to pass as the object_hook argument to json.load() or json.loads():The as_enum function relies on the JSON having been encoded using EnumEncoder, or something which behaves identically to it.The restriction to members of PUBLIC_ENUMS is necessary to avoid a maliciously crafted text being used to, for example, trick calling code into saving private information (e.g. a secret key used by the application) to an unrelated database field, from where it could then be exposed (see http://chat.stackoverflow.com/transcript/message/35999686#35999686).Example usage:I know this is old but I feel this will help people. I just went through this exact problem and discovered if you're using string enums, declaring your enums as a subclass of str works well for almost all situations:Will output:As you can see, loading the JSON outputs the string DEBUG but it is easily castable back into a LogLevel object. A good option if you don't want to create a custom JSONEncoder.The correct answer depends on what you intend to do with the serialized version.If you are going to unserialize back into Python, see Zero's answer.If your serialized version is going to another language then you probably want to use an IntEnum instead, which is automatically serialized as the corresponding integer:and this returns:I liked Zero Piraeus' answer, but modified it slightly for working with the API for Amazon Web Services (AWS) known as Boto.I then added this method to my data model:I hope this helps someone.In Python 3.7, can just use

json.dumps(enum_obj, default=str)If you are using jsonpickle the easiest way should look as below.After Json serialization you will have as expected {"status": 0} instead of 

Python way to clone a git repository

Mike

[Python way to clone a git repository](https://stackoverflow.com/questions/2472552/python-way-to-clone-a-git-repository)

Is there a Python way without using a subprocess to clone a git repository? I'm up for using any sort of modules you recommend. 

2010-03-18 18:55:14Z

Is there a Python way without using a subprocess to clone a git repository? I'm up for using any sort of modules you recommend. There is GitPython. Haven’t heard of it before and internally, it relies on having the git executables somewhere; additionally, they might have plenty of bugs. But it could be worth a try.How to clone:(It’s not nice and I don’t know if it is the supported way to do it, but it worked.)Using GitPython will give you a good python interface to Git.For example, after installing it (pip install gitpython), for cloning a new repository you can use clone_from function:See the GitPython Tutorial for examples on using the Repo object.Note: GitPython requires git being installed on the system, and accessible via system's PATH.My solution is very simple and straight forward. It doesn't even need the manual entry of passphrase/password.Here is my complete code:Github's libgit2 binding, pygit2 provides a one-liner cloning a remote directory: Here's a way to print progress while cloning a repo with GitPythonWith Dulwich tip you should be able to do:This is still very basic - it copies across the objects and the refs, but it doesn't yet create the contents of the working tree if you create a non-bare repository.For python 3 First install module:and later, code it :) I hope this helps youThis is the sample code for gitpull and gitpush using gitpython module.Pretty simple method is to just pass the creds in the url, can be slightly suspect though - use with caution.You can use dload

Prepend a line to an existing file in Python

Nick

[Prepend a line to an existing file in Python](https://stackoverflow.com/questions/4454298/prepend-a-line-to-an-existing-file-in-python)

I need to add a single line to the first line of a text file and it looks like the only options available to me are more lines of code than I would expect from python. Something like this:Is there no easier way? Additionally, I see this two-handle example more often than opening a single handle for reading and writing ('r+') - why is that?

2010-12-15 19:59:51Z

I need to add a single line to the first line of a text file and it looks like the only options available to me are more lines of code than I would expect from python. Something like this:Is there no easier way? Additionally, I see this two-handle example more often than opening a single handle for reading and writing ('r+') - why is that?Python makes a lot of things easy and contains libraries and wrappers for a lot of common operations, but the goal is not to hide fundamental truths.The fundamental truth you are encountering here is that you generally can't prepend data to an existing flat structure without rewriting the entire structure. This is true regardless of language.There are ways to save a filehandle or make your code less readable, many of which are provided in other answers, but none change the fundamental operation: You must read in the existing file, then write out the data you want to prepend, followed by the existing data you read in.By all means save yourself the filehandle, but don't go looking to pack this operation into as few lines of code as possible. In fact, never go looking for the fewest lines of code -- that's obfuscation, not programming.I would stick with separate reads and writes, but we certainly can express each more concisely:Python2:Python3:  Note: file() function is not available in python3.Other approach:or a one liner:Thanks for the opportunity to think about this problem :)CheersYou can save one write call with this:When using 'r+', you would have to rewind the file after reading and before writing.Here's a 3 liner that I think is clear and flexible. It uses the list.insert function, so if you truly want to prepend to the file use l.insert(0, 'insert_str'). When I actually did this for a Python Module I am developing, I used l.insert(1, 'insert_str') because I wanted to skip the '# -- coding: utf-8 --' string at line 0. Here is the code.This does the job without reading the whole file into memory, though it may not work on WindowsOne possibility is the following:If you wish to prepend in the file after a specific text then you can use the function below.So first you open the file, read it and save it all into one string.

Then we try to find the character number in the string where the injection will happen. Then with a single write and some smart indexing of the string we can rewrite the whole file including the injected text now.Am I not seeing something or couldn't we just use a buffer large-enough to read-in the input file in parts (instead of the whole content) and with this buffer traverse the file while it is open and keep exchanging file<->buffer contents?This seems much more efficient (for big files especially) than reading the whole content in memory, modifying it in memory and writing it back to the same file or (even worse) a different one. Sorry that now I don't have time to implement a sample snippet, I'll get back to this later, but maybe you get the idea.

What is the most efficient way to get first and last line of a text file?

pasbino

[What is the most efficient way to get first and last line of a text file?](https://stackoverflow.com/questions/3346430/what-is-the-most-efficient-way-to-get-first-and-last-line-of-a-text-file)

I have a text file which contains a time stamp on each line. My goal is to find the time range. All the times are in order so the first line will be the earliest time and the last line will be the latest time. I only need the very first and very last line. What would be the most efficient way to get these lines in python?Note: These files are relatively large in length, about 1-2 million lines each and I have to do this for several hundred files. 

2010-07-27 17:58:24Z

I have a text file which contains a time stamp on each line. My goal is to find the time range. All the times are in order so the first line will be the earliest time and the last line will be the latest time. I only need the very first and very last line. What would be the most efficient way to get these lines in python?Note: These files are relatively large in length, about 1-2 million lines each and I have to do this for several hundred files. docs for io moduleThe variable value here is 1024: it represents the average string length. I choose 1024 only for example. If you have an estimate of average line length you could just use that value times 2.Since you have no idea whatsoever about the possible upper bound for the line length, the obvious solution would be to loop over the file:You don't need to bother with the binary flag you could just use open(fname).ETA: Since you have many files to work on, you could create a sample of couple of dozens of files using random.sample and run this code on them to determine length of last line. With an a priori large value of the position shift (let say 1 MB). This will help you to estimate the value for the full run.You could open the file for reading and read the first line using the builtin readline(), then seek to the end of file and step backwards until you find the line's preceding EOL and read the last line from there.Jumping to the second last byte instead of the last one prevents that you return directly because of a trailing EOL. While you're stepping backwards you'll also want to step two bytes since the reading and checking for EOL pushes the position forward one step.When using seek the format is fseek(offset, whence=0) where whence signifies to what the offset is relative to. Quote from docs.python.org:Running it through timeit 10k times on a file with 6k lines totalling 200kB gave me 1.62s vs 6.92s when comparing to the for-loop beneath that was suggested earlier. Using a 1.3GB sized file, still with 6k lines, a hundred times resulted in 8.93 vs 86.95.Here's a modified version of SilentGhost's answer that will do what you want.No need for an upper bound for line length here.Can you use unix commands? I think using head -1 and tail -n 1 are probably the most efficient methods. Alternatively, you could use a simple fid.readline() to get the first line and fid.readlines()[-1], but that may take too much memory.This is my solution, compatible also with Python3. It does also manage border cases, but it misses utf-16 support:It's ispired by Trasp's answer and AnotherParker's comment.First open the file in read mode.Then use readlines() method to read line by line.All the lines stored in a list.Now you can use list slices to get first and last lines of the file.The for loop runs through the lines and x gets the last line on the final iteration.Here is an extension of @Trasp's answer that has additional logic for handling the corner case of a file that has only one line. It may be useful to handle this case if you repeatedly want to read the last line of a file that is continuously being updated. Without this, if you try to grab the last line of a file that has just been created and has only one line, IOError: [Errno 22] Invalid argument will be raised.Nobody mentioned using reversed:Getting the first line is trivially easy. For the last line, presuming you know an approximate upper bound on the line length, os.lseek some amount from SEEK_END find the second to last line ending and then readline() the last line.The above answer is a modified version of the above answers which handles the case that there is only one line in the file

Python Library Path

Kyle Burton

[Python Library Path](https://stackoverflow.com/questions/135035/python-library-path)

In ruby the library path is provided in $:, in perl it's in @INC - how do you get the list of paths that Python searches for modules when you do an import?

2008-09-25 18:23:50Z

In ruby the library path is provided in $:, in perl it's in @INC - how do you get the list of paths that Python searches for modules when you do an import?I think you're looking for sys.pathYou can also make additions to this path with the PYTHONPATH environment variable at runtime, in addition to:

Docker how to run pip requirements.txt only if there was a change?

Prometheus

[Docker how to run pip requirements.txt only if there was a change?](https://stackoverflow.com/questions/34398632/docker-how-to-run-pip-requirements-txt-only-if-there-was-a-change)

In a Dockerfile I have a layer which installs requirements.txt:When I build the docker image it runs the whole process regardless of any changes made to this file.How do I make sure Docker only runs pip install -r requirements.txt if there has been a change to the file?

2015-12-21 15:01:30Z

In a Dockerfile I have a layer which installs requirements.txt:When I build the docker image it runs the whole process regardless of any changes made to this file.How do I make sure Docker only runs pip install -r requirements.txt if there has been a change to the file?I'm assuming that at some point in your build process, you're copying your entire application into the Docker image with COPY or ADD:The problem is that you're invalidating the Docker build cache every time you're copying the entire application into the image. This will also invalidate the cache for all subsequent build steps.To prevent this, I'd suggest copying only the requirements.txt file in a separate build step before adding the entire application into the image:As the requirements file itself probably changes only rarely, you'll be able to use the cached layers up until the point that you add your application code into the image.This is directly mentioned in Docker's own "Best practices for writing Dockerfiles":

Trailing slash triggers 404 in Flask path rule

Jesvin Jose

[Trailing slash triggers 404 in Flask path rule](https://stackoverflow.com/questions/33241050/trailing-slash-triggers-404-in-flask-path-rule)

I want to redirect any path under /users to a static app.  The following view should capture these paths and serve the appropriate file (it just prints the path for this example).  This works for /users, /users/604511, and /users/604511/action.  Why does the path /users/ cause a 404 error?

2015-10-20 15:44:09Z

I want to redirect any path under /users to a static app.  The following view should capture these paths and serve the appropriate file (it just prints the path for this example).  This works for /users, /users/604511, and /users/604511/action.  Why does the path /users/ cause a 404 error?Your /users route is missing a trailing slash, which Werkzeug interprets as an explicit rule to not match a trailing slash.  Either add the trailing slash, and Werkzeug will redirect if the url doesn't have it, or set strict_slashes=False on the route and Werkzeug will match the rule with or without the slash.You can also set strict_slashes for all URLs.However, you should avoid disabling strict slashes in most cases. The docs explain why:To disable strict slashes GLOBALLY; set url_map.strict_slashes = False like so:This way you do not have to use strict_slashes=False for each view.Then you just define the route without a trailing slash like so:Then /my-route and /my-route/ both work identically.It's because of Werkzeug’s consistency with other HTTP servers. Have a look at Flask's Quickstart documentation. The relevant paragraph:So just add /users/ as well to the routing.

python pandas dataframe columns convert to dict key and value

perigee

[python pandas dataframe columns convert to dict key and value](https://stackoverflow.com/questions/18012505/python-pandas-dataframe-columns-convert-to-dict-key-and-value)

I have a pandas data frame with multiple columns and I would like to construct a dict from two columns: one as the dict's keys and the other as the dict's values. How can I do that?Dataframe:I need to define area as key, count as value in dict. Thank you in advance.

2013-08-02 08:46:33Z

I have a pandas data frame with multiple columns and I would like to construct a dict from two columns: one as the dict's keys and the other as the dict's values. How can I do that?Dataframe:I need to define area as key, count as value in dict. Thank you in advance.If lakes is your DataFrame, you can do something likeWith pandas it can be done as:If lakes is your DataFrame:  You can also do this if you want to play around with pandas. However, I like punchagan's way.

Round integers to the nearest 10

tablo_an

[Round integers to the nearest 10](https://stackoverflow.com/questions/3348825/round-integers-to-the-nearest-10)

I am trying to round integers in python. I looked at the built-in round() function but it seems that that rounds floats. My goal is to round integers to the closest multiple of 10. i.e.: 5-> 10, 4-> 0, 95->100, etc.5 and higher should round up, 4 and lower should round down. This is the code I have that does this: Is this the best way to achieve what I want to achieve? Is there a built-in function that does this? Additionally, if this is the best way, is there anything wrong with the code that I missed in testing?

2010-07-27 23:39:49Z

I am trying to round integers in python. I looked at the built-in round() function but it seems that that rounds floats. My goal is to round integers to the closest multiple of 10. i.e.: 5-> 10, 4-> 0, 95->100, etc.5 and higher should round up, 4 and lower should round down. This is the code I have that does this: Is this the best way to achieve what I want to achieve? Is there a built-in function that does this? Additionally, if this is the best way, is there anything wrong with the code that I missed in testing?Actually, you could still use the round function:This would round to the closest multiple of 10. To 100 would be -2 as the second argument and so forth.round() can take ints and negative numbers for places, which round to the left of the decimal.  The return value is still a float, but a simple cast fixes that:Slightly simpler:That float (double-precision in Python) is always a perfect representation of an integer, as long as it's in the range [-253..253]. (Pedants pay attention: it's not two's complement in doubles, so the range is symmetric about zero.)See the discussion here for details.I wanted to do the same thing, but with 5 instead of 10, and came up with a simple function. Hope it's useful:if you want the algebric form and still use round for it it's hard to get simpler than:This function will round either be order of magnitude (right to left) or by digits the same way that format treats floating point decimal places (left to right:PrintsYou can also use Decimal class:On Python 3 you can reliably use round with negative places and get a rounded integer:On Python 2, round will fail to return a proper rounder integer on larger numbers because round always returns a float:The other 2 functions work on Python 2 and 3

Sum / Average an attribute of a list of objects

jsj

[Sum / Average an attribute of a list of objects](https://stackoverflow.com/questions/10879867/sum-average-an-attribute-of-a-list-of-objects)

Lets say I have class C which has attribute a.What is the best way to get the sum of a from a list of C in Python?I've tried the following code, but I know that's not the right way to do it:

2012-06-04 10:31:57Z

Lets say I have class C which has attribute a.What is the best way to get the sum of a from a list of C in Python?I've tried the following code, but I know that's not the right way to do it:Use a generator expression:If you are looking for other measures than sum, e.g. mean/standard deviation, you can use NumPy and do:I had a similar task, but mine involved summing a time duration as your attribute c.a.

Combining this with another question asked here, I came up withBecause, as mentioned in the link, sum needs a starting value.Use built-in statistics module:

How to get correlation of two vectors in python [duplicate]

Luke Makk

[How to get correlation of two vectors in python [duplicate]](https://stackoverflow.com/questions/19428029/how-to-get-correlation-of-two-vectors-in-python)

In matlab I usewhich returns .9934. I've tried numpy.correlate but it returns something completely different. What is the simplest way to get the correlation of two vectors?

2013-10-17 13:25:50Z

In matlab I usewhich returns .9934. I've tried numpy.correlate but it returns something completely different. What is the simplest way to get the correlation of two vectors?The docs indicate that numpy.correlate is not what you are looking for:Instead, as the other comments suggested, you are looking for a Pearson correlation coefficient. To do this with scipy try:This givesYou can also use numpy.corrcoef:This gives:

pprint dictionary on multiple lines

mulllhausen

[pprint dictionary on multiple lines](https://stackoverflow.com/questions/20171392/pprint-dictionary-on-multiple-lines)

I'm trying to get a pretty print of a dictionary, but I'm having no luck:I wanted the output to be on multiple lines, something like this:Can pprint do this? If not, then which module does it? I'm using Python 2.7.3.

2013-11-24 05:19:47Z

I'm trying to get a pretty print of a dictionary, but I'm having no luck:I wanted the output to be on multiple lines, something like this:Can pprint do this? If not, then which module does it? I'm using Python 2.7.3.Use width=1 or width=-1:You could convert the dict to json through json.dumps(d, indent=4)If you are trying to pretty print the environment variables, use:Two things to add on top of Ryan Chou's already very helpful answer:

How do I multiply each element in a list by a number?

DJ bigdawg

[How do I multiply each element in a list by a number?](https://stackoverflow.com/questions/35166633/how-do-i-multiply-each-element-in-a-list-by-a-number)

I have a list:How can I multiply each element in my_list by 5? The output should be:

2016-02-03 00:54:03Z

I have a list:How can I multiply each element in my_list by 5? The output should be:You can just use a list comprehension:Note that a list comprehension is generally a more efficient way to do a for loop:As an alternative, here is a solution using the popular Pandas package:Or, if you just want the list:You can do it in-place like so:This requires no additional imports and is very pythonic. A blazingly faster approach is to do the multiplication in a vectorized manner instead of looping over the list. Numpy has already provided a very simply and handy way for this that you can use.Note that this doesn't work with Python's native lists. If you multiply a number with a list it will repeat the items of the as the size of that number.If you want a pure Python-based approach using a list comprehension is basically the most Pythonic way to go.Beside list comprehension, as a pure functional approach, you can also use built-in map() function as following:This code passes all the items within the my_list to 5's __mul__ method and returns an iterator-like object (in python-3.x). You can then convert the iterator to list using list() built in function (in Python-2.x you don't need that because map return a list by default).Since I think you are new with Python, lets do the long way, iterate thru your list using for loop and multiply and append each element to a new list. using for loopusing list comprehension, this is also same as using for-loop but more 'pythonic'is one way you could do it ... your teacher probably knows a much less complicated way that was probably covered in classWith map (not as good, but another approach to the problem):Multiplying each element in my_list by k:resulting in: [5, 10, 15, 20]Best way is to use list comprehension: Returns:

[-1, -2, -3]

How to set an HTTP proxy in Python 2.7?

Rolando

[How to set an HTTP proxy in Python 2.7?](https://stackoverflow.com/questions/11726881/how-to-set-an-http-proxy-in-python-2-7)

I am trying to run a script that installs pip: get-pip.py and am getting a connection timeout due to my network being behind an HTTP proxy. Is there some way I could configure an HTTP proxy in my Python 2.7 installation to be able to install what I am trying to install?Note: I am using Windows. Below is the error I am getting:

2012-07-30 17:53:31Z

I am trying to run a script that installs pip: get-pip.py and am getting a connection timeout due to my network being behind an HTTP proxy. Is there some way I could configure an HTTP proxy in my Python 2.7 installation to be able to install what I am trying to install?Note: I am using Windows. Below is the error I am getting:It looks like get-pip.py has been updated to use the environment variables http_proxy and https_proxy.Windows:Linux/OS X:However if this still doesn't work for you, you can always install pip through a proxy using setuptools' easy_install by setting the same environment variables.Windows:Linux/OS X:Then once it's installed, use:From the pip man page:On my network just setting http_proxy didn't work for me. The following points were relevant.1 Setting http_proxy for your user wont be preserved when you execute sudo - to preserve it, do:I got my install working by first installing cntlm local proxy. The instructions here is succinct : http://www.leg.uct.ac.za/howtos/use-isa-proxiesInstead of student number, you'd put your domain username2 To use the cntlm local proxy, exec:You can install pip (or any other package) with easy_install almost as described in the first answer. However you will need a HTTPS proxy, too. The full sequence of commands is:You might also want to add a port to the proxy, such as http{s}_proxy=http://proxy.myproxy.com:8080You can try downloading the Windows binaries for pip from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip.For using pip to download other modules, see @Ben Burn's answer.For installing pip with get-pip.py behind a proxy I went with the steps below. My server was even behind a jump server.From the jump server:On the "python-server"Success.

Multiplying across in a numpy array

Alex S

[Multiplying across in a numpy array](https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array)

I'm trying to multiply each of the terms in a 2D array by the corresponding terms in a 1D array. This is very easy if I want to multiply every column by the 1D array, as shown in the numpy.multiply function. But I want to do the opposite, multiply each term in the row.

In other words I want to multiply:and getbut instead I getDoes anyone know if there's an elegant way to do that with numpy?

Thanks a lot,

Alex

2013-08-29 22:53:00Z

I'm trying to multiply each of the terms in a 2D array by the corresponding terms in a 1D array. This is very easy if I want to multiply every column by the 1D array, as shown in the numpy.multiply function. But I want to do the opposite, multiply each term in the row.

In other words I want to multiply:and getbut instead I getDoes anyone know if there's an elegant way to do that with numpy?

Thanks a lot,

AlexNormal multiplication like you showed:If you add an axis, it will multiply the way you want:You could also transpose twice:I've compared the different options for speed and found that – much to my surprise – all options (except diag) are equally fast. I personally use(or (A.T * b).T) because it's short.Code to reproduce the plot:You could also use matrix multiplication (aka dot product):Which is more elegant is probably a matter of taste.Yet another trick (as of v1.6)I'm proficient with the numpy broadcasting (newaxis), but I'm still finding my way around this new einsum tool.  So I had play around a bit to find this solution.Timings (using Ipython timeit):Incidentally, changing a i to j, np.einsum('ij,j->ij',A,b), produces the matrix that Alex does not want.  And np.einsum('ji,j->ji',A,b) does, in effect, the double transpose.For those lost souls on google, using numpy.expand_dims then numpy.repeat will work, and will also work in higher dimensional cases (i.e. multiplying a shape (10, 12, 3) by a (10, 12)).Why don't you just do??

Assigning a value to single underscore _ in Python/IPython interpreter

eolandro

[Assigning a value to single underscore _ in Python/IPython interpreter](https://stackoverflow.com/questions/17580289/assigning-a-value-to-single-underscore-in-python-ipython-interpreter)

I created this function in Python 2.7 with ipython:later if I call _(somevalue), I get _ = somevalue.The function has disappeared! If I call _(4) I get:Why? What's wrong with this function?

2013-07-10 20:28:53Z

I created this function in Python 2.7 with ipython:later if I call _(somevalue), I get _ = somevalue.The function has disappeared! If I call _(4) I get:Why? What's wrong with this function?The Python interpreter assigns the last expression value to _.This behaviour is limited to the REPL interpreter only, and is intended to assist in interactive coding sessions:The standard Python interpreter goes to some length to not trample on user-defined values though; if you yourself assign something else to _ then the interpreter will not overwrite that (technically speaking, the _ variable is a __builtin__ attribute, your own assignments are 'regular' globals). You are not using the standard Python interpreter though; you are using IPython, and that interpreter is not that careful.IPython documents this behaviour explicitly:In the standard Python REPL environment, if you assigned something to _ you can still access the last expression result via __builtins__._ or by deleting the _ global that shadows it again (del _).Outside of the Python interpreter, _ is by convention used as the name of the translatable text function (see the gettext module; external tools look for that function to extract translatable strings).And, also by convention, using _ as an assignment target tells readers of your code that you are going to ignore that value; e.g. [random.random() for _ in range(5)] to generate a list of 5 random float values, or foo, bar, _ = three_values to signal a 3rd value from a tuple assignment will not be used. When _ is already used for a gettext function, __ can be used for the same purposes._ is a special variable in interpreter, it is always assigned to the result of previous expression. So, you shoudn't use it like that.BTW the problem seems to be related to IPython shell, because your code works fine in normal python shell:In normal python shell when you assign anything to the variable _ then it'll remain assigned to that object only, and looses it special behaviour.Python shell:IPython shell:In IPython _ behaves differently than python shell's _; even if you assign it to some variable then also it is going to be updated as soon as you do some calculation.From IPython's docs:From python docs:If you create a variable assigned to "_" it gets masked/masks the system variable _.

Celery parallel distributed task with multiprocessing

Prometheus

[Celery parallel distributed task with multiprocessing](https://stackoverflow.com/questions/23916413/celery-parallel-distributed-task-with-multiprocessing)

I have a CPU intensive Celery task. I would like to use all the processing power (cores) across lots of EC2 instances to get this job done faster (a celery parallel distributed task with multiprocessing - I think).The terms, threading, multiprocessing, distributed computing, distributed parallel processing are all terms I'm trying to understand better.Example task:Using the code above (with an example if possible) how one would ago about distributed this task using Celery by allowing this one task to be split up utilising all the computing CPU power across all available machine in the cloud?

2014-05-28 15:53:27Z

I have a CPU intensive Celery task. I would like to use all the processing power (cores) across lots of EC2 instances to get this job done faster (a celery parallel distributed task with multiprocessing - I think).The terms, threading, multiprocessing, distributed computing, distributed parallel processing are all terms I'm trying to understand better.Example task:Using the code above (with an example if possible) how one would ago about distributed this task using Celery by allowing this one task to be split up utilising all the computing CPU power across all available machine in the cloud?Your goals are:Celery can do both of these for you fairly easily. The first thing to understand is that each celery worker is configured by default to run as many tasks as there are CPU cores available on a system:This means each individual task doesn't need to worry about using multiprocessing/threading to make use of multiple CPUs/cores. Instead, celery will run enough tasks concurrently to use each available CPU.With that out of the way, the next step is to create a task that handles processing some subset of your list_of_millions_of_ids. You have a couple of options here - one is to have each task handle a single ID, so you run N tasks, where N == len(list_of_millions_of_ids). This will guarantee that work is evenly distributed amongst all your tasks, since there will never be a case where one worker finishes early and is just waiting around; if it needs work, it can pull an id off the queue. You can do this (as mentioned by John Doe) using the a celery group.tasks.py:And to execute the tasks:Another option is to break the list into smaller pieces, and distribute the pieces to your workers. This approach runs the risk of wasting some cycles, because you may end up with some workers waiting around while others are still doing work. However, the celery documentation notes that this concern is often unfounded:So, you may find that chunking the list and distributing the chunks to each task performs better, because of the reduced messaging overhead. You can probably also lighten the load on the database a bit this way, by calculating each id, storing it in a list, and then adding the whole list into the DB once you're done, rather than doing it one id at a time. The chunking approach would look something like thistasks.py:And to start the tasks:You can experiment a bit with what chunking size gives you the best result. You want to find a sweet spot where you're cutting down messaging overhead while also keeping the size small enough that you don't end up with workers finishing their chunk much faster than another worker, and then just waiting around with nothing to do.In the world of distribution there is only one thing you should remember above all :I know it sounds evident but before distributing double check you are using the best algorithm (if it exists...).

Having said that, optimizing distribution is a balancing act between 3 things:Computers are made so the closer you get to your processing unit (3) the faster and more efficient (1) and (2) will be. The order in a classic cluster will be : network hard drive, local hard drive, RAM, inside processing unit territory...

Nowadays processors are becoming sophisticated enough to be considered as an ensemble of independent hardware processing units commonly called cores, these cores process data (3) through threads (2).

Imagine your core is so fast that when you send data with one thread you are using 50% of the computer power, if the core has 2 threads you will then use 100%. Two threads per core is called hyper threading, and your OS will see 2 CPUs per hyper threaded core. Managing threads in a processor is commonly called multi-threading. 

Managing CPUs from the OS is commonly called multi-processing. 

Managing concurrent tasks in a cluster is commonly called parallel programming. 

Managing dependent tasks in a cluster is commonly called distributed programming.So where is your bottleneck ?What about Celery ?Celery is a messaging framework for distributed programming, that will use a broker module for communication (2) and a backend module for persistence (1), this means that you will be able by changing the configuration to avoid most bottlenecks (if possible) on your network and only on your network. 

First profile your code to achieve the best performance in a single computer.

Then use celery in your cluster with the default configuration and set CELERY_RESULT_PERSISTENT=True :During execution open your favorite monitoring tools, I use the default for rabbitMQ and flower for celery and top for cpus, your results will be saved in your backend. An example of network bottleneck is tasks queue growing so much that they delay execution, you can proceed to change modules or celery configuration, if not your bottleneck is somewhere else.Why not use group celery task for this?http://celery.readthedocs.org/en/latest/userguide/canvas.html#groupsBasically, you should divide ids into chunks (or ranges) and give them to a bunch of tasks in group.For smth more sophisticated, like aggregating results of particular celery tasks, I have successfully used chord task for similar purpose:http://celery.readthedocs.org/en/latest/userguide/canvas.html#chordsIncrease settings.CELERYD_CONCURRENCY to a number that is reasonable and you can afford, then those celery workers will keep executing your tasks in a group or a chord until done.Note: due to a bug in kombu there were trouble with reusing workers for high number of tasks in the past, I don't know if it's fixed now. Maybe it is, but if not, reduce CELERYD_MAX_TASKS_PER_CHILD.Example based on simplified and modified code I run:summarize gets results of all single_batch_processor tasks. Every task runs on any Celery worker, kombu coordinates that.Now I get it: single_batch_processor and summarize ALSO have to be celery tasks, not  regular functions - otherwise of course it will not be parallelized (I'm not even sure chord constructor will accept it if it's not a celery task).Adding more celery workers will certainly speed up executing the task. You might have another bottleneck though: the database. Make sure it can handle the simultaneous inserts/updates.Regarding your question: You are adding celery workers by assigning another process on your EC2 instances as celeryd. Depending on how many workers you need you might want to add even more instances. 

Python and Intellisense

Andrew Harry

[Python and Intellisense](https://stackoverflow.com/questions/905005/python-and-intellisense)

Is there an equivalent to 'intellisense' for Python?Perhaps i shouldn't admit it but I find having intellisense really speeds up the 'discovery phase' of learning a new language.  For instance switching from VB.net to C# was a breeze due to snippets and intellisense helping me along.

2009-05-25 01:02:23Z

Is there an equivalent to 'intellisense' for Python?Perhaps i shouldn't admit it but I find having intellisense really speeds up the 'discovery phase' of learning a new language.  For instance switching from VB.net to C# was a breeze due to snippets and intellisense helping me along.This blog entry explains setting Vim up as a Python IDE, he covers Intellisense-like functionality:

(source: dispatched.ch)  This is standard in Vim 7. There are a number of other very useful plugins for python development in Vim, such as Pyflakes which checks code on the fly and Python_fn.vim which provides functionality for manipulating python indentation & code blocks.Have a look at python tools for visual studio, they provide code completion (a.k.a  intellisense), debugging etc ...Below is a screenshot of the interactive shell for python showing code completion.The PyDev environment for Eclipse has intellisense-like functionality for Python. Keeping an interactive console open, along with the help(item) function is very helpful.The dynamic nature of the language tends to make autocomplete type analysis difficult, so the quality of the various completion facilities menitoned above varies wildly.While it's not exactly what you asked for, the ipython shell is very good for exploratory work. When I'm working with a new module, I tend to pull it into ipython and poke at it. Having tried most of the solutions mentioned above (though it's been years since Wing), ipython's completion facilities are consistently more reliable. The two main tools for exploration are tab complete and appending a question mark to the module/function name to get the help text, e.g.:The IDLE editor that comes with Python has an intellisense feature that auto-discovers imported modules, functions, classes and attributes.I strongly recommend PyDev. In Pydev you can put the module you are using in the Forced Buildins, mostly the code-completion will work better than in other IDEs like KOMODO EDIT.Also I think IPython is very helpful. Since it is 'run-time' in IPython, the code-completion in IPython won't miss anything.PyCharm is the best Python IDE with IntelliSense support.I'd recommend Komodo Edit.  However, I should point something out:  you're not going to get anything quite as good as what you're used to with Visual Studio's C# intellisense.  Python's dynamic nature can make it difficult to do these kinds of features.Wingware for example implements auto-completion, see http://wingware.com/doc/edit/auto-completion .ctags + vim works ok, too, although it is not as powerful as intellisense. Using this with ipython, you can get online help, automatic name completion, etc... But that's obviously command-line oriented.Eclipse + pydev can do it as well, but I have no experience with it: http://pydev.sourceforge.net/Well, I think the most dynamic way to learn Python is to use iPython.You got autocompletion when using tab, dynamic behaviour because it's a shell and you can get the full documentation of any object / method typing :When developping, I agree that PyDev is cool. But it's heavy, so while learning, a text editor + iPython is really nice.Pyscripter has the best intellisense i have meet :)For emacs and VI there's also https://github.com/tkf/emacs-jedi.I would recommend jedi-vim, it's perfect to me, try it and you won't regret.IronPython is the way to go. Visual Studio has the best intellisense support and you can utilize that using IronPythonTry visual Studio Code. It has very powerful Python and Django support and thousands of plugins for other languages used in a Python project such as CSS, HTML and Django templates. Best of all, it is free:

https://code.visualstudio.com

Matplotlib/Pandas error using histogram

jonas

[Matplotlib/Pandas error using histogram](https://stackoverflow.com/questions/20656663/matplotlib-pandas-error-using-histogram)

I have a problem making histograms from pandas series objects and I can't understand why it does not work. The code has worked fine before but now it does not.Here is a bit of my code (specifically, a pandas series object I'm trying to make a histogram of):which outputs the result: 

    pandas.core.series.SeriesHere's my plotting code:Error message:

2013-12-18 11:17:05Z

I have a problem making histograms from pandas series objects and I can't understand why it does not work. The code has worked fine before but now it does not.Here is a bit of my code (specifically, a pandas series object I'm trying to make a histogram of):which outputs the result: 

    pandas.core.series.SeriesHere's my plotting code:Error message:This error occurs among other things when you have NaN values in the Series. Could that be the case?These NaN's are not handled well by the hist function of matplotlib. For example:produces the same error AttributeError: max must be larger than min in range parameter. One option is eg to remove the NaN's before plotting. This will work:Another option is to use pandas hist method on your series and providing the axes[0] to the ax keyword:The error is rightly due to NaN values as explained above. Just use:if the value is not numeric and then apply:

Python Mixed Integer Linear Programming

asun

[Python Mixed Integer Linear Programming](https://stackoverflow.com/questions/26305704/python-mixed-integer-linear-programming)

Are there any Mixed Integer Linear Programming(MILP) solver for Python?Can GLPK python solve MILP problem? I read that it can solve Mixed integer problem.

I am very new to linear programming problem. So i am rather confused and cant really differentiate if Mixed Integer Programming is different from Mixed Integer Linear programming(MILP). 

2014-10-10 18:22:08Z

Are there any Mixed Integer Linear Programming(MILP) solver for Python?Can GLPK python solve MILP problem? I read that it can solve Mixed integer problem.

I am very new to linear programming problem. So i am rather confused and cant really differentiate if Mixed Integer Programming is different from Mixed Integer Linear programming(MILP). Pulp is a python modeling interface that hooks up to solvers like CBC(open source), CPLEX (commercial), Gurobi(commercial), XPRESS-MP(commercial) and YALMIP(open source).You can also use Pyomo to model the optimization problem and then call an external solver, namely CPLEX, Gurobi GLPK and the AMPL solver library.You can also call GLPK from GLPK/Python, PyGLPK or PyMathProg.Yet another modelling language is CMPL, which has a python interface for MIP solvers (for linear programs only).All the above solvers solve Mixed Integer Linear Programs, while some of them (CPLEX, GUROBI and XRESS-MP for sure) can solve Mixed Integer Quadratic Programs and Quadratically constrained quadratic programs (and also conic programs but this probably goes beyond the scope of this question). MIP refers to Mixed integer programs, but it is commonly used to refer to linear programs only. To make the terminology more precise, one should always refer to MILP or MINLP (Mixed integer non-linear programming).Note that CPLEX and GUROBI have their own python APIs as well, but they (and also) XPRESS-MP are commercial products, but free for academic research. CyLP is similar to Pulp above but interfaces with the COIN-OR solvers CBC and CGL and CLP.Note that there is a big difference in the performance of commercial and free solvers: the latter are falling behind the former by a large margin. SCIP is perhaps the best non-commercial solver (see below for an update). Its python interface, PySCIPOpt, is here.Also, have a look at this SO question.Finally, if you are interested at a simple constraint solver (not optimization) then have a look at python-constraint.I hope this helps!More solvers and python interfaces that fell into my radar: MIPCL, which appears to be one of the fastest the fastest non-commercial MIP solver, has a python interface that has quite good documentation. Note, however, that the Python API does not include the advanced functionality that comes together with the native MIPCLShell. I particularly like the MIPCL-PY manual, which demonstrates an array of models used in Operations Management, on top of some small-scale implementations. It is a very interesting introductory manual in its own right, regardless of which solver/API one may want to make use of.Google Optimization Tools, which include a multitude of functionalities, such as It has extensive documentation of several traditional OR problems and simple implementations. I could not find a complete Python API documentation, although there exist some examples here. It is somewhat unclear to me how other solvers hook up on the interface and whether methods of these solvers are available.CVXOPT, an open-source package for convex optimization, which interfaces to GLPK (open source) and MOSEK

 (commercial). It is versatile, as it can tackle many problem classes (notably linear, second-order, semidefinite, convex nonlinear). The only disadvantage is that it modeling complex problems may be cumbersome, as the user needs to pass the data in a "Matlab-y" fashion (i.e., to specify the matrix, rhs vectors, etc). However, it can be called from the modeling interfaces PICOS and...CVXPY, a python-embedded optimization language for convex optimization problems, which contains CVXOPT as a default solver, but it can hook up to the usual MIP solvers.Thanks to RedPanda for pointing out that CVXOPT/CVXPY support MIP solvers as well.For a very comprehensive article on optimization modeling capabilities of packages and object-oriented languages (not restricted to Python), check this article.

Logging to two files with different settings

marw

[Logging to two files with different settings](https://stackoverflow.com/questions/11232230/logging-to-two-files-with-different-settings)

I am already using a basic logging config where all messages across all modules are stored in a single file. However, I need a more complex solution now:I have been reading the docs for the module, bu they are very complex for me at the moment. Loggers, handlers...  So, in short:How to log to two files in Python 3, ie:

2012-06-27 17:54:54Z

I am already using a basic logging config where all messages across all modules are stored in a single file. However, I need a more complex solution now:I have been reading the docs for the module, bu they are very complex for me at the moment. Loggers, handlers...  So, in short:How to log to two files in Python 3, ie:You can do something like this:

PyMongo upsert throws「upsert must be an instance of bool」error

ComputationalSocialScience

[PyMongo upsert throws「upsert must be an instance of bool」error](https://stackoverflow.com/questions/5055797/pymongo-upsert-throws-upsert-must-be-an-instance-of-bool-error)

I'm running an update on my MongoDB from Python. I have this line:But it throws this error:But True looks like an instance of bool to me!How should I correctly write this update?

2011-02-20 07:22:44Z

I'm running an update on my MongoDB from Python. I have this line:But it throws this error:But True looks like an instance of bool to me!How should I correctly write this update?The third argument to PyMongo's update() is upsert and must be passed a boolean, not a dictionary. Change your code to:Or pass upsert=True as a keyword argument:Your mistake was likely caused by reading about update() in the MongoDB docs. The JavaScript version of update takes an object as its third argument containing optional parameters like upsert and multi. But since Python allows passing keyword arguments to a function (unlike JavaScript which only has positional arguments), this is unnecessary and PyMongo takes these options as optional function parameters instead.According to http://api.mongodb.org/python/2.3/api/pymongo/collection.html#pymongo.collection.Collection.update you should indeed pass upsert as a keyword rather than just True, that isOris a better approach than just passing True as if you ever wish to pass other kwargs such as safe or multi code might break if order of args is not kept.upsert should be passed as either positional parameter, like soor as a keyword argument, like so

Why is a trailing comma a SyntaxError in an argument list that uses *args syntax?

asmeurer

[Why is a trailing comma a SyntaxError in an argument list that uses *args syntax?](https://stackoverflow.com/questions/16950394/why-is-a-trailing-comma-a-syntaxerror-in-an-argument-list-that-uses-args-syntax)

Why can't you use a trailing comma with *args in Python?  In other words, this worksBut this does notThis is the case with both Python 2 and Python 3.

2013-06-05 21:42:18Z

Why can't you use a trailing comma with *args in Python?  In other words, this worksBut this does notThis is the case with both Python 2 and Python 3.Let's look at the language specification:Let's sift down to the parts we care about:So, it looks like after any arguments to a function call, we're allowed an extra ,. So this looks like a bug in the cpython implementation.Something like: f(1, *(2,3,4), ) should work according to this grammar, but doesn't in CPython.In an earlier answer, Eric linked to the CPython grammar specification, which includes the CPython implementation of the above grammar. Here it is below:Note, that this grammar is not the same as the one proposed by the language specification. I'd consider this an implementation bug.Note that there are additional issues with the CPython implementation. This should also be supported: f(*(1,2,3), *(4,5,6))Oddly though, the specification does not allow f(*(1,2,3), *(4,5,6), *(7,8,9))As I look at this more, I think this part of the specification needs some fixing. This is allowed: f(x=1, *(2,3)), but this isn't: f(x=1, 2, 3).And to perhaps be helpful to the original question, in CPython, you can have a trailing comma if you don't use the *args or the **kwargs feature. I agree that this is lame.After some discussion regarding this bug in issue 9232, Guido van Rossum commented:Subsequently, a patch by Mark Dickinson was committed. So this is now fixed in  Python 3.6.0 alpha 1.

Sending SOAP request using Python Requests

Deepankar Bajpeyi

[Sending SOAP request using Python Requests](https://stackoverflow.com/questions/18175489/sending-soap-request-using-python-requests)

Is it possible to use Python's requests library to send a SOAP request?

2013-08-11 18:51:35Z

Is it possible to use Python's requests library to send a SOAP request?It is indeed possible. Here is an example calling the Weather SOAP Service using plain requests lib:Some notes:For example:Some people have mentioned the suds library. Suds is probably the more correct way to be interacting with SOAP, but I often find that it panics a little when you have WDSLs that are badly formed (which, TBH, is more likely than not when you're dealing with an institution that still uses SOAP ;) ). You can do the above with suds like so:Note: when using suds, you will almost always end up needing to use the doctor!Finally, a little bonus for debugging SOAP; TCPdump is your friend. On Mac, you can run TCPdump like so: This can be helpful for inspecting the requests that actually go over the wire. The above two code snippets are also available as gists:

In python, how to import filename starts with a number

Simon Guo

[In python, how to import filename starts with a number](https://stackoverflow.com/questions/9090079/in-python-how-to-import-filename-starts-with-a-number)

Basically there is a file called 8puzzle.py and I want to import the file into another file (in the same folder and I cannot change the file name as the file is provided). Is there anyway to do this in Python? I tried usual way from 8puzzle import *, it gives me an error.Error is: 

2012-02-01 02:51:30Z

Basically there is a file called 8puzzle.py and I want to import the file into another file (in the same folder and I cannot change the file name as the file is provided). Is there anyway to do this in Python? I tried usual way from 8puzzle import *, it gives me an error.Error is: You could doVery interesting problem. I'll remember not to name anything with a number.If you'd like to import * -- you should check out this question and answer.The above answers are correct, but as for now, the recommended way is to use import_module function:__import__ is not recommended now.Don't use the .py extension in your imports.Does from 8puzzle import * work?For what it's worth, from x import * is not a preferred Python pattern, as it bleeds that module's namespace into your current context.In general, try to import things you specifically want from that module.  Any global from the other module can be imported.e.g., if you have 8puzzle.foo you could do `from 8puzzle import While my .py message is correct, it isn't sufficient.The other poster's __import__('8puzzle') suggestion is correct.  However, I highly recommend avoiding this pattern.For one, it's reserved an internal, private Python method.  You are basically breaking the fundamental assumptions of what it means to be able to import a module.  Simply renaming the file to something else, like puzzle8, will remedy this.This will frustrate the hell out of experienced Python programmers who are expecting to know what your imports are at the top and are expecting code to (try to) conform to PEP8.

Why is '#!/usr/bin/env python' supposedly more correct than just '#!/usr/bin/python'?

Kenneth Reitz

[Why is '#!/usr/bin/env python' supposedly more correct than just '#!/usr/bin/python'?](https://stackoverflow.com/questions/1352922/why-is-usr-bin-env-python-supposedly-more-correct-than-just-usr-bin-pyt)

Anyone know this? I've never been able to find an answer.

2009-08-30 02:30:30Z

Anyone know this? I've never been able to find an answer.If you're prone to installing python in various and interesting places on your PATH (as in $PATH in typical Unix shells, %PATH on typical Windows ones), using /usr/bin/env will accomodate your whim (well, in Unix-like environments at least) while going directly to /usr/bin/python won't.  But losing control of what version of Python your scripts run under is no unalloyed bargain... if you look at my code you're more likely to see it start with, e.g., #!/usr/local/bin/python2.5 rather than with an open and accepting #!/usr/bin/env python -- assuming the script is important I like to ensure it's run with the specific version I have tested and developed it with, NOT a semi-random one;-).From wikipediait finds the python executable in your environment and uses that. it's more portable because python may not always be in /usr/bin/python. env is always located in /usr/bin.It finds 'python' also in /usr/local/bin, ~/bin, /opt/bin, ... or wherever it may hide.You may find this post to be of interest:

http://mail.python.org/pipermail/python-list/2008-May/661514.htmlThis may be a better explanation:

http://mail.python.org/pipermail/tutor/2007-June/054816.html

Python - difference between two strings

user2626682

[Python - difference between two strings](https://stackoverflow.com/questions/17904097/python-difference-between-two-strings)

I'd like to store a lot of words in a list. Many of these words are very similar. For example I have word afrykanerskojęzyczny and many of words like afrykanerskojęzycznym, afrykanerskojęzyczni, nieafrykanerskojęzyczni. What is the effective (fast and giving small diff size) solution to find difference between two strings and restore second string from the first one and diff?

2013-07-28 01:22:19Z

I'd like to store a lot of words in a list. Many of these words are very similar. For example I have word afrykanerskojęzyczny and many of words like afrykanerskojęzycznym, afrykanerskojęzyczni, nieafrykanerskojęzyczni. What is the effective (fast and giving small diff size) solution to find difference between two strings and restore second string from the first one and diff?You can use ndiff in the difflib module to do this. It has all the information necessary to convert one string into another string.A simple example: prints:I like the ndiff answer, but if you want to spit it all into a list of only the changes, you could do something like:What you are asking for is a specialized form of compression.  xdelta3 was designed for this particular kind of compression, and there's a python binding for it, but you could probably get away with using zlib directly.  You'd want to use zlib.compressobj and zlib.decompressobj with the zdict parameter set to your "base word", e.g. afrykanerskojęzyczny.Caveats are zdict is only supported in python 3.3 and higher, and it's easiest to code if you have the same "base word" for all your diffs, which may or may not be what you want.You can look into the regex module (the fuzzy section). I don't know if you can get the actual differences, but at least you can specify allowed number of different types of changes like insert, delete, and substitutions:The answer to my comment above on the Original Question makes me think this is all he wants:This will do the following:For every value in wordlist, set that value of the wordlist to the origional code.All you have to do is put this piece of code where you need to change wordlist, making sure you store the words you need to change in wordlist, and that the original word is correct.Hope this helps!

Why is Python 3 not backwards compatible? [closed]

neelmeg

[Why is Python 3 not backwards compatible? [closed]](https://stackoverflow.com/questions/9066956/why-is-python-3-not-backwards-compatible)

I have learned that Python 3 is not backwards compatible.Will it not affect a lot of applications using older versions of Python?How did the developers of Python 3 not think it was absolutely necessary to make it backwards compatible?

2012-01-30 16:12:23Z

I have learned that Python 3 is not backwards compatible.Will it not affect a lot of applications using older versions of Python?How did the developers of Python 3 not think it was absolutely necessary to make it backwards compatible?Python 3.0 implements a lot of very useful features and breaks backward compatibility. It does it on purpose, so the great features can be implemented even despite the fact Python 2.x code may not work correctly under Python 3.x.So, basically, Python 3.0 is not backward-compatible on purpose. Thanks to that, you can benefit from a whole new set of features. It is even called "Python 3000" or "Python 3K".From "What's new in Python 3.0" (available here):Some of the most notable features that may be considered as breaking backward compatibility, but improving the language at the same time, are:

Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?

megashigger

[Is .ix() always better than .loc() and .iloc() since it is faster and supports integer and label access?](https://stackoverflow.com/questions/27667759/is-ix-always-better-than-loc-and-iloc-since-it-is-faster-and-supports-i)

I'm learning the Python pandas library. Coming from an R background, the indexing and selecting functions seem more complicated than they need to be. My understanding it that .loc() is only label based and .iloc() is only integer based. Why should I ever use .loc() and .iloc() if .ix() is faster and supports integer and label access? 

2014-12-27 13:35:22Z

I'm learning the Python pandas library. Coming from an R background, the indexing and selecting functions seem more complicated than they need to be. My understanding it that .loc() is only label based and .iloc() is only integer based. Why should I ever use .loc() and .iloc() if .ix() is faster and supports integer and label access? Please refer to the doc Different Choices for Indexing, it states clearly when and why you should use .loc, .iloc over .ix, it's about explicit use case:Hope this helps.Thanks to comment from @Alexander, Pandas is going to deprecate ix in 0.20, details in here.One of the strong reason behind is because mixing indexes -- positional and label (effectively using ix) has been a significant source of problems for users.It is expected to migrate to use iloc and loc instead, here is a link on how to convert code.

Finding dead code in large python project [closed]

Brian Postow

[Finding dead code in large python project [closed]](https://stackoverflow.com/questions/9524873/finding-dead-code-in-large-python-project)

I've seen How can you find unused functions in Python code? but that's really old, and doesn't really answer my question. I have a large python project with multiple libraries that are shared by multiple entry point scripts. This project has been accreting for many years with many authors, so there's a whole lot of dead code. You know the drill. I  know that finding all dead code is un-decidable. All I need is a tool that will find all functions that are not called anywhere. We're not doing anything fancy with calling functions based on the string of the function name, so I'm not worried about anything pathological... I just installed pylint, but it appears to be file based, and not paying much attention to interfile dependencies, or even function dependencies.Clearly, I could grep for def in all of the files, get all of the function names from that, and do a grep for each of those function names. I'm just hoping that there's something a little smarter than that out there already.ETA: Please note that I don't expect or want something perfect. I know my halting-problem-proof just as well anyone (No really I taught theory of computation I know when I'm looking at something that is recursively enumerable). Any thing that tries to approximate it by actually running the code is going to take way too long. I just want something that syntactically goes through the code and says "This function is definitely used. This function MIGHT be used, and this function is definitely NOT used, no one else even seems to know it exists!" And the first two categories aren't important. 

2012-03-01 22:03:07Z

I've seen How can you find unused functions in Python code? but that's really old, and doesn't really answer my question. I have a large python project with multiple libraries that are shared by multiple entry point scripts. This project has been accreting for many years with many authors, so there's a whole lot of dead code. You know the drill. I  know that finding all dead code is un-decidable. All I need is a tool that will find all functions that are not called anywhere. We're not doing anything fancy with calling functions based on the string of the function name, so I'm not worried about anything pathological... I just installed pylint, but it appears to be file based, and not paying much attention to interfile dependencies, or even function dependencies.Clearly, I could grep for def in all of the files, get all of the function names from that, and do a grep for each of those function names. I'm just hoping that there's something a little smarter than that out there already.ETA: Please note that I don't expect or want something perfect. I know my halting-problem-proof just as well anyone (No really I taught theory of computation I know when I'm looking at something that is recursively enumerable). Any thing that tries to approximate it by actually running the code is going to take way too long. I just want something that syntactically goes through the code and says "This function is definitely used. This function MIGHT be used, and this function is definitely NOT used, no one else even seems to know it exists!" And the first two categories aren't important. You might want to try out vulture. It can't catch everything due to Python's dynamic nature, but it catches quite a bit without needing a full test suite like coverage.py and others need to work.Try running Ned Batchelder's coverage.py.It is very hard to determine which functions and methods are called without executing the code, even if the code doesn't do any fancy stuff.  Plain function invocations are rather easy to detect, but method calls are really hard.  Just a simple example:Nothing fancy going on here, but any script that tries to determine whether A.f() or B.f() is called will have a rather hard time to do so without actually executing the code.While the above code doesn't do anything useful, it certainly uses patterns that appear in real code -- namely putting instances in containers.  Real code will usually do even more complex things -- pickling and unpickling, hierarchical data structures, conditionals.As stated before, just detecting plain function invocations of the formorwill be rather easy.  You can use the ast module to parse your source files.  You will need to record all imports, and the names used to import other modules.  You will also need to track top-level function definitions and the calls inside these functions.  This will give you a dependency graph, and you can use NetworkX to detect the connected components of this graph.While this might sound rather complex, it can probably done with less than 100 lines of code.  Unfortunately, almost all major Python projects use classes and methods, so it will be of little help.Here's the solution I'm using at least tentatively:Then I look at the individual functions that have very few references (< 3 say)it's ugly, and it only gives me approximate answers, but I think it's good enough for a start. What are you-all's thoughts?With the following line you can list all function definitions that are obviously not used as an attribute, a function call, a decorator or a return value. So it is approximately what you are looking for. It is not perfect, it is slow, but I never got any false positives. (With linux you have to replace ack with ack-grep) If you have your code covered with a lot of tests (it is quite useful at all), run them with code-coverage plugin and you can see unused code then .)IMO that could be achieved pretty quickly with a simple pylint plugin that :Then you would have to call pylint on all your code base to get something that make sense.

Of course as said this would need to checked, as there may have been inference failures or

such that would introduce false positive. Anyway that would probably greatly reduce the number of grep to be done.I've not much time to do it myself yet but anyone would find help on the python-projects@logilab.org mailing list.

Difference(s) between merge() and concat() in pandas

WindChimes

[Difference(s) between merge() and concat() in pandas](https://stackoverflow.com/questions/38256104/differences-between-merge-and-concat-in-pandas)

What's the essential difference(s) between pd.DataFrame.merge() and pd.concat()?So far, this is what I found, please comment on how complete and accurate my understanding is:(Pandas is great at addressing a very wide spectrum of use cases in data analysis. It can be a bit daunting exploring the documentation to figure out what is the best way to perform a particular task. )

2016-07-07 22:12:46Z

What's the essential difference(s) between pd.DataFrame.merge() and pd.concat()?So far, this is what I found, please comment on how complete and accurate my understanding is:(Pandas is great at addressing a very wide spectrum of use cases in data analysis. It can be a bit daunting exploring the documentation to figure out what is the best way to perform a particular task. )A very high level difference is that merge() is used to combine two (or more) dataframes on the basis of values of common columns (indices can also be used, use left_index=True and/or right_index=True), and concat() is used to append one (or more) dataframes one below the other (or sideways, depending on whether the axis option is set to 0 or 1).join() is used to merge 2 dataframes on the basis of the index; instead of using merge() with the option left_index=True we can use join().For example:pd.concat takes an Iterable as its argument. Hence,  it cannot take DataFrames directly as its argument. Also Dimensions of the DataFrame should match along axis while concatenating.pd.merge can take DataFrames as its argument, and is used to combine two DataFrames with same columns or index, which can't be done with pd.concat since it will show the repeated column in the DataFrame.Whereas join can be used to join two DataFrames with different indices.Nice question. The main difference:The other difference, is pd.concat has innerdefault and outer joins only, while pd.DataFrame.merge() has left, right, outer, innerdefault joins.Third notable other difference is: pd.DataFrame.merge() has the option to set the column suffixes when merging columns with the same name, while for pd.concat this is not possible.With pd.concat by default you are able to stack rows of multiple dataframes (axis=0) and when you set the axis=1 then you mimic the pd.DataFrame.merge() function.Some useful examples of pd.concat:The main difference between merge & concat is that merge allow you to perform more structured "join" of tables where use of concat is more broad and less structured.MergeReferring the documentation, pd.DataFrame.merge takes right as a required argument, which you can think it as joining left table and right table according to some pre-defined structured join operation. Note the definition for parameter right.Required ParametersOptional ParametersImportant: pd.DataFrame.merge requires right to be a pd.DataFrame or named pd.Series object.OutputFurthermore, if we check the docstring for Merge Operation on pandas is below:ConcatRefer to documentation of pd.concat, first note that the parameter is not named any of table, data_frame, series, matrix, etc., but objs instead. That is, you can pass many "data containers", which are defined as:Iterable[FrameOrSeriesUnion], Mapping[Optional[Hashable], FrameOrSeriesUnion]Required ParametersOptional ParametersOutputExampleCodeCode OutputYou can achieve, however, the first output (merge) with concat by changing the axis parameterObserve the following behavior,outputs;, which you cannot perform a similar operation with merge, since it only allows a single DataFrame or named Series.outputs;ConclusionAs you may have notice already that input and outputs may be different between "merge" and "concat". As I mentioned at the beginning, the very first (main) difference is that "merge" performs a more structured join with a set of restricted set of objects and parameters where as "concat" performs a less strict/broader join with a broader set of objects and parameters.All in all, merge is less tolerant to changes/(the input) and "concat" is looser/less sensitive to changes/(the input). You can achieve "merge" by using "concat", but the reverse is not always true."Merge" operation uses Data Frame columns (or name of pd.Series object) or row indices, and since it uses those entities only it performs horizontal merge of Data Frames or Series, and does not apply vertical operation as a result.If you want to see more, you can deep dive in the source code a bit;by default:

    join is a column-wise left join

    pd.merge is a column-wise inner join

    pd.concat is a row-wise outer join    pd.concat:

    takes Iterable arguments. Thus, it cannot take DataFrames directly (use [df,df2])

    Dimensions of DataFrame should match along axis    Join and pd.merge:

    can take DataFrame argumentsClick to see picture for understanding why code below does the same thing

How to get reproducible results in keras

Pavel Surmenok

[How to get reproducible results in keras](https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras)

I get different results (test accuracy) every time I run the imdb_lstm.py example from Keras framework (https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)

The code contains np.random.seed(1337) in the top, before any keras imports. It should prevent it from generating different numbers for every run. What am I missing?  UPDATE: How to repro:  UPDATE2: I'm running it on Windows 8.1 with MinGW/msys, module versions:

theano 0.7.0

numpy 1.8.1

scipy 0.14.0c1UPDATE3: I narrowed the problem down a bit. If I run the example with GPU (set theano flag device=gpu0) then I get different test accuracy every time, but if I run it on CPU then everything works as expected. My graphics card: NVIDIA GeForce GT 635)

2015-09-06 02:41:11Z

I get different results (test accuracy) every time I run the imdb_lstm.py example from Keras framework (https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)

The code contains np.random.seed(1337) in the top, before any keras imports. It should prevent it from generating different numbers for every run. What am I missing?  UPDATE: How to repro:  UPDATE2: I'm running it on Windows 8.1 with MinGW/msys, module versions:

theano 0.7.0

numpy 1.8.1

scipy 0.14.0c1UPDATE3: I narrowed the problem down a bit. If I run the example with GPU (set theano flag device=gpu0) then I get different test accuracy every time, but if I run it on CPU then everything works as expected. My graphics card: NVIDIA GeForce GT 635)You can find the answer at the Keras docs: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development.In short, to be absolutely sure that you will get reproducible results with your python script on one computer's/laptop's CPU then you will have to do the following:Following the Keras link at the top, the source code I am using is the following:It is needless to say that you do not have to to specify any seed or random_state at the numpy, scikit-learn or tensorflow/keras functions that you are using in your python script exactly because with the source code above we set globally their pseudo-random generators at a fixed value.Theano's documentation talks about the difficulties of seeding random variables and why they seed each graph instance with its own random number generator. They also provide examples on how to seed all the random number generators. I finally got reproducible results with my code. It's a combination of answers I saw around the web. The first thing is doing what @alex says:Then you have to solve the issue noted by @user2805751 regarding cuDNN by calling your Keras code with the following additional THEANO_FLAGS:And finally, you have to patch your Theano installation as per this comment, which basically consists in:This should get you the same results for the same seed.Note that there might be a slowdown. I saw a running time increase of about 10%.I would like to add something to the previous answers. If you use python 3 and you want to get reproducible results for every run, you have toI have trained and tested Sequential() kind of neural networks using Keras. I performed non linear regression on noisy speech data. I used the following code to generate random seed : I get the exact same results of val_loss each time I train and test on the same data. This works for me:The problem is now solved in Tensorflow 2.0 ! I had the same issue with TF 1.x (see  If Keras results are not reproducible, what's the best practice for comparing models and choosing hyper parameters? ) but I agree with the previous comment, but reproducible results sometimes needs the same environment(e.g. installed packages, machine characteristics and so on). So that, I recommend to copy your environment to other place in case to have reproducible results. Try to use one of the next technologies:The Conference Paper: Non-Random Weight Initialisation in Deep Learning Networks for Repeatable Determinism, publication date Jun 5, 2019  presented at 10th IEEE International Conference Dependable Systems, Services and Technologies (DESSERT-19) at Leeds Beckett University (LBU), United Kingdom, UK, Ireland and the Ukrainian section of IEEE June 5-7, 2019https://ieeexplore.ieee.org/document/8770007shows how to get repeatable results by enforcing critical regions of code.it has been extended to a Journal Paper: Repeatable Determinism using Non-Random Weight Initialisations in Smart City Applications of Deep Learning

publication in The Journal of Reliable Intelligent Environments in a Smart Cities special edition, and uses glorot xavier limts and achieve the same accuracy with perceptron layers but grow the weight in to a linear order which may have an advantage for rule extraction in perceptron layers.

Extracting date from a string in Python

dmpop

[Extracting date from a string in Python](https://stackoverflow.com/questions/3276180/extracting-date-from-a-string-in-python)

How can I extract the date from a string like "monkey 2010-07-10 love banana"? Thanks!

2010-07-18 15:46:02Z

How can I extract the date from a string like "monkey 2010-07-10 love banana"? Thanks!If the date is given in a fixed form, you can simply use a regular expression to extract the date and "datetime.datetime.strptime" to parse the date:Otherwise, if the date is given in an arbitrary form, you can't extract it easily.Using python-dateutil:Invalid dates raise a ValueError:It can recognize dates in many formats:Note that it makes a guess if the date is ambiguous:But the way it parses ambiguous dates is customizable:For extracting the date from a string in Python; the best module available is the datefinder module.You can use it in your Python project by following the easy steps given below.note: if you are expecting a large number of matches; then typecasting to list won't be a recommended way as it will be having a big performance overhead.Using Pygrok, you can define abstracted extensions to the Regular Expression syntax.The custom patterns can be included in your regex in the format %{PATTERN_NAME}.  You can also create a label for that pattern, by separating with a colon: %s{PATTERN_NAME:matched_string}. If the pattern matches, the value will be returned as part of the resulting dictionary (e.g. result.get('matched_string'))For example:The resulting value will be a dictionary:{'month': '07', 'day': '10', 'year': '2010'}If the date_pattern does not exist in the input_string, the return value will be None. By contrast, if your pattern does not have any labels, it will return an empty dictionary {}References:You could also try the dateparser module, which may be slower than datefinder on free text but which should cover more potential cases and date formats, as well as a significant number of languages.If you know the position of the date object in the string (for example in a log file), you can use .split()[index] to extract the date without fully knowing the format.For example:

How can one mock/stub python module like urllib

Dinoboff

[How can one mock/stub python module like urllib](https://stackoverflow.com/questions/295438/how-can-one-mock-stub-python-module-like-urllib)

I need to test a function that needs to query a page on an external server using urllib.urlopen (it also uses urllib.urlencode). The server could be down, the page could change; I can't rely on it for a test.What is the best way to control what urllib.urlopen returns?

2008-11-17 12:07:40Z

I need to test a function that needs to query a page on an external server using urllib.urlopen (it also uses urllib.urlencode). The server could be down, the page could change; I can't rely on it for a test.What is the best way to control what urllib.urlopen returns?Another simple approach is to have your test override urllib's urlopen() function.  For example, if your module hasYou could define your test like this:Then, when your tests invoke functions in mymodule, dummy_urlopen() will be called instead of the real urlopen().  Dynamic languages like Python make it super easy to stub out methods and classes for testing.See my blog posts at http://softwarecorner.wordpress.com/ for more information about stubbing out dependencies for tests.I am using Mock's patch decorator:Did you give Mox a look? It should do everything you need. Here is a simple interactive session illustrating the solution you need:HTTPretty works in the exact same way that FakeWeb does. HTTPretty works in the socket layer, so it should work intercepting any python http client libraries. It's battle tested against urllib2, httplib2 and requestsIn case you don't want to even load the module:And then:Probably the best way to handle this is to split up the code, so that logic that processes the page contents is split from the code that fetches the page.Then pass an instance of the fetcher code into the processing logic, then you can easily replace it with a mock fetcher for the unit test.e.g.The simplest way is to change your function so that it doesn't necessarily use urllib.urlopen.  Let's say this is your original function:Add an argument which is the function to use to open the URL.  Then you can provide a mock function to do whatever you need:Adding onto Clint Miller's answer, to do this I had to create a fake class that implements a read method like this:Then to stub out urllib2.open:

Python.h missing from Ubuntu 12.04

liv2hak

[Python.h missing from Ubuntu 12.04](https://stackoverflow.com/questions/15631135/python-h-missing-from-ubuntu-12-04)

I am very new to python.I installed an openflow controller on my Linux PC (Ubunutu 12.04) called RYU using: I was trying to run a python file using ryu-manager as shown below.Then I tried to install gevent using:What is the cause of this error? I tried to locate the Python.h file using:I couldn't find it on my system.However if I run: I get the output:Should Python.h be there on the system? If so, what should I install to get it? 

2013-03-26 06:58:18Z

I am very new to python.I installed an openflow controller on my Linux PC (Ubunutu 12.04) called RYU using: I was trying to run a python file using ryu-manager as shown below.Then I tried to install gevent using:What is the cause of this error? I tried to locate the Python.h file using:I couldn't find it on my system.However if I run: I get the output:Should Python.h be there on the system? If so, what should I install to get it? This should do it:sudo apt-get  update; sudo apt-get install  python-dev -y  It will install any missing headers. It helped me a lot.Even if you have Python installed, the header file and the library usually aren't installed by default. On Ubuntu, they come in a separate package called python-dev.Install gevent directly - sudo apt-get install python-gevent.

Find Monday's date with Python

Joe

[Find Monday's date with Python](https://stackoverflow.com/questions/1622038/find-mondays-date-with-python)

How do I find the previous Monday's date, based off of the current date using Python? I thought maybe I could use: datetime.weekday() to do it, but I am getting stuck.I basically want to find today's date and Mondays date to construct a date range query in django using: created__range=(start_date, end_date).  

2009-10-25 20:53:47Z

How do I find the previous Monday's date, based off of the current date using Python? I thought maybe I could use: datetime.weekday() to do it, but I am getting stuck.I basically want to find today's date and Mondays date to construct a date range query in django using: created__range=(start_date, end_date).  Some words of explanation:Take todays date. Subtract the number of days which already passed this week (this gets you 'last' monday). Add one week.Edit: The above is for 'next monday', but since you were looking for 'last monday' you could useChristopheD's post is close to what you want. I don't have enough rep to make a comment :(Instead of (which actually gives you the next upcoming monday):I would say:If you want the previous week, add the 'weeks=1' parameter.This makes the code more readable since you are subtracting a timedelta. This clears up any confusion caused by adding a timedelta that has negative and positive offsets.I think the easiest way is using python-dateutil like this:Note: The OP says in the comments, "I was looking for the past Monday". I take this to mean we are looking for the last Monday that occurred strictly before today. The calculation is a little difficult to get right using only the datetime module (especially given the above interpretation of "past Monday" and if you wish to avoid clunky if-statements). For example, if today is a Monday such as 2013-12-23,returns 2013-12-23, which is the same day as today (not the past Monday).The advantage of using the dateutil module is that you don't have to do tricky mental calculations nor force the reader to do the same to get the right date. dateutil does it all for you:Note that days=-1 is needed to guarantee that past_monday is a different day than today.You can use Natty. I tried parsedatetime and dateparser. Comparing these three, I think Natty is the best one.To get your result, use like this:Github Link : https://github.com/eadmundo/python-nattyTry it, It can do more!For future googlers who show up on this page looking for a way to get "the most recent Sunday", rather than "the most recent Monday", you need to do some additional math because datetime.weekday() treats Monday as the first day of the week:If today is Tuesday, this sets start_day to last Sunday. If today is Sunday, this sets start_day to today. Take away the % 7 if you want "last Sunday" to be a week ago if it's currently Sunday.Using timedeltas and datetime module:gives you today's day of the week, counting 0 (monday) to 6 (sunday)(7-d)%7 gives you days until Monday, or leaves you where you are if today is Monday

How to get pip to work behind a proxy server [duplicate]

Annihilator8080

[How to get pip to work behind a proxy server [duplicate]](https://stackoverflow.com/questions/19080352/how-to-get-pip-to-work-behind-a-proxy-server)

I am trying to use python package manager pip to install a package and it's dependencies from the internet. However I am behind a proxy in my college and have already set the http_proxy environment variable. But when I try to install a package like this:I get this error in the log file:I even tried setting my proxy variable explicitly like this:But I still get the same error. How do I get pip to work behind a proxy server.

2013-09-29 16:23:23Z

I am trying to use python package manager pip to install a package and it's dependencies from the internet. However I am behind a proxy in my college and have already set the http_proxy environment variable. But when I try to install a package like this:I get this error in the log file:I even tried setting my proxy variable explicitly like this:But I still get the same error. How do I get pip to work behind a proxy server.The pip's proxy parameter is, according to pip --help, in the form scheme://[user:passwd@]proxy.server:portYou should use the following:Also, the HTTP_PROXY env var should be respected.Note that in earlier versions (couldn't track down the change in the code, sorry, but the doc was updated here), you had to leave the scheme:// part out for it to work, i.e. pip install --proxy user:password@proxyserver:portAt least for pip 1.3.1, it honors the http_proxy and https_proxy environment variables. Make sure you define both, as it will access the PYPI index using https.Old thread, I know, but for future reference, the --proxy option is now passed with an "="Example:First Try to set proxy using the following commandThen Try using the command      On Ubuntu, you can set proxy by usingor if you are having SOCKS error useThen run pip at least pip3 also works without "=", however, instead of "http" you might need "https"Final command, which worked for me:

What do * and ** before a variable name mean in a function signature? [duplicate]

hqt

[What do * and ** before a variable name mean in a function signature? [duplicate]](https://stackoverflow.com/questions/11315010/what-do-and-before-a-variable-name-mean-in-a-function-signature)

I have read a piece of python code, and I don't know what does * and ** mean in this code : I just know about one use of *: extract all attribute it has to parameter of method or constructor.If this true for above function, so what does the rest : ** ?

2012-07-03 16:13:05Z

I have read a piece of python code, and I don't know what does * and ** mean in this code : I just know about one use of *: extract all attribute it has to parameter of method or constructor.If this true for above function, so what does the rest : ** ?Inside a function header:* collects all the positional arguments in a tuple.** collects all the keyword arguments in a dictionary.In a function call:* unpacks a list or tuple into position arguments.** unpacks a dictionary into keyword arguments.** takes specified argument names and puts them into a dictionary. So:Would print:** means named arguments of the functions.argv is a dictionary that contains all named arguments of the function.And you can also reverse it. You can use a dictionary as a set of aruments

for a function:would print

How can I tail a log file in Python?

Eli

[How can I tail a log file in Python?](https://stackoverflow.com/questions/12523044/how-can-i-tail-a-log-file-in-python)

I'd like to make the output of tail -F or something similar available to me in Python without blocking or locking. I've found some really old code to do that here, but I'm thinking there must be a better way or a library to do the same thing by now. Anyone know of one?Ideally, I'd have something like tail.getNewData() that I could call every time I wanted more data.

2012-09-21 01:13:58Z

I'd like to make the output of tail -F or something similar available to me in Python without blocking or locking. I've found some really old code to do that here, but I'm thinking there must be a better way or a library to do the same thing by now. Anyone know of one?Ideally, I'd have something like tail.getNewData() that I could call every time I wanted more data.If you are on linux (as windows does not support calling select on files) you can use the subprocess module along with the select module.This polls the output pipe for new data and prints it when it is available. Normally the time.sleep(1) and print f.stdout.readline() would be replaced with useful code.You can use the subprocess module without the extra select module calls.This will also print new lines as they are added, but it will block until the tail program is closed, probably with f.kill().Using the sh module (pip install sh):[update]Since sh.tail with _iter=True is a generator, you can:Then you can "getNewData" with:Note that if the tail buffer is empty, it will block until there is more data (from your question it is not clear what you want to do in this case).[update]A container generator placing the tail call inside a while True loop and catching eventual I/O exceptions will have almost the same effect of -F.If the file becomes inaccessible, the generator will return None. However it still blocks until there is new data if the file is accessible. It remains unclear for me what you want to do in this case.Raymond Hettinger approach seems pretty good:This generator will return '' if the file becomes inaccessible or if there is no new data.[update]I think the second will output the last ten lines whenever the tail process ends, which with -f is whenever there is an I/O error. The tail --follow --retry behavior is not far from this for most cases I can think of in unix-like environments.Perhaps if you update your question to explain what is your real goal (the reason why you want to mimic tail --retry), you will get a better answer.Of course, tail will display the last 10 lines by default... You can position the file pointer at the end of the file using file.seek, I will left a proper implementation as an exercise to the reader.IMHO the file.read() approach is far more elegant than a subprocess based solution.The only portable way to tail -f a file appears to be, in fact, to read from it and retry (after a sleep) if the read returns 0. The tail utilities on various platforms use platform-specific tricks (e.g. kqueue on BSD) to efficiently tail a file forever without needing sleep.Therefore, implementing a good tail -f purely in Python is probably not a good idea, since you would have to use the least-common-denominator implementation (without resorting to platform-specific hacks). Using a simple subprocess to open tail -f and iterating through the lines in a separate thread, you can easily implement a non-blocking tail operation in Python.Example implementation:So, this is coming quite late, but I ran into the same problem again, and there's a much better solution now. Just use pygtail:We've already got one and itsa very nice.  Just call f.read() whenever you want more data.  It will start reading where the previous read left off and it will read through the end of the data stream:For reading line-by-line, use f.readline().  Sometimes, the file being read will end with a partially read line.  Handle that case with f.tell() finding the current file position and using f.seek() for moving the file pointer back to the beginning of the incomplete line.  See this ActiveState recipe for working code.Adapting Ijaz Ahmad Khan's answer to only yield lines when they are completely written (lines end with a newline char) gives a pythonic solution with no external dependencies:You could use the 'tailer' library: https://pypi.python.org/pypi/tailer/It has an option to get the last few lines:And it can also follow a file:If one wants tail-like behaviour, that one seems to be a good option.All the answers that use tail -f are not pythonic.Here is the pythonic way: ( using no external tool or library)Another option is the tailhead library that provides both Python versions of of tail and head utilities and API that can be used in your own module.Originally based on the tailer module, its main advantage is the ability to follow files by path i.e. it can handle situation when file is recreated. Besides, it has some bug fixes for various edge cases.You can also use 'AWK' command.

See more at: http://www.unix.com/shell-programming-scripting/41734-how-print-specific-lines-awk.html

awk can be used to tail last line, last few lines or any line in a file.

This can be called from python.If you are on linux you implement a non-blocking implementation in python in the following way.Python is "batteries included" - it has a nice solution for it: https://pypi.python.org/pypi/pygtailReads log file lines that have not been read. Remembers where it finished last time, and continues from there.

AssertionError: View function mapping is overwriting an existing endpoint function: main

Kimmy

[AssertionError: View function mapping is overwriting an existing endpoint function: main](https://stackoverflow.com/questions/17256602/assertionerror-view-function-mapping-is-overwriting-an-existing-endpoint-functi)

Does anyone know why I can't overwrite an existing endpoint function if i have two url rules like thisTraceback:

2013-06-23 00:21:28Z

Does anyone know why I can't overwrite an existing endpoint function if i have two url rules like thisTraceback:Your view names need to be unique even if they are pointing to the same view method.This same issue happened to me when I had more than one API function in the module and tried to wrap each function with 2 decorators:I got this same exception because I tried to wrap more than one function with those two decorators: Specifically, it is caused by trying to register a few functions with the name wrapper:Changing the name of the function solved it for me (wrapper.__name__ = func.__name__):Then, decorating more than one endpoint worked.For users that use @app.route it is better to use the key-argument endpoint rather then chaning the value of __name__ like Roei Bahumi stated. Taking his example will be:Flask requires you to associate a single 'view function' with an 'endpoint'. You are calling Main.as_view('main') twice which creates two different functions (exactly the same functionality but different in memory signature). Short story, you should simply doI would just like to add to this a more 'template' type solution.would just like to add a really interesting article "Demystifying Decorators" I found recently: https://sumit-ghosh.com/articles/demystifying-decorators-python/If you think you have unique endpoint names and still this error is given then probably you are facing issue. Same was the case with me.This issue is with flask 0.10 in case you have same version then do following to get rid of this:This can happen also when you have identical function names on different routes.There is a fix for Flask issue #570 introduced recenty (flask 0.10) that causes this exception to be raised.See https://github.com/mitsuhiko/flask/issues/796So if you go to flask/app.py and comment out the 4 lines 948..951, this may help until the issue is resovled fully in a new version.The diff of that change is here: http://github.com/mitsuhiko/flask/commit/661ee54bc2bc1ea0763ac9c226f8e14bb0beb5b1Your view names need to be unique even if they are pointing to the same view method, or you can add from functools import wraps and use @wraps

https://docs.python.org/2/library/functools.htmluse flask 0.9 instead

use the following commands

sudo pip uninstall flask

Python Save to file

Stefan

[Python Save to file](https://stackoverflow.com/questions/9536714/python-save-to-file)

I would like to save a string to a file with a python program named Failed.pyHere is what I have so far:

2012-03-02 16:22:25Z

I would like to save a string to a file with a python program named Failed.pyHere is what I have so far:Here is a more pythonic version, which automatically closes the file, even if there was an exception in the wrapped block:You need to open the file again using open(), but this time passing 'w' to indicate that you want to write to the file. I would also recommend using with to ensure that the file will be closed when you are finished writing to it.Naturally you may want to include newlines or other formatting in your output, but the basics are as above.The same issue with closing your file applies to the reading code. That should look like this:Check out setdefault, it makes the code a little more legible.  Then you dump your data with the file object's write method.

Remove index name in pandas

markov zain

[Remove index name in pandas](https://stackoverflow.com/questions/29765548/remove-index-name-in-pandas)

I have a dataframe like this one:How to remove index name foo from that dataframe?

The desired output is like this:

2015-04-21 07:24:16Z

I have a dataframe like this one:How to remove index name foo from that dataframe?

The desired output is like this:Use del df.index.nameAlternatively you can just assign None to the index.name attribute:From version 0.18.0 you can use rename_axis:

AttributeError: 'module' object has no attribute 'request'

Pruthvi Raj

[AttributeError: 'module' object has no attribute 'request'](https://stackoverflow.com/questions/22278993/attributeerror-module-object-has-no-attribute-request)

When I run the following code in Python 3.3:I get the following error:I did this too to verify:What am I doing wrong?

2014-03-09 06:01:44Z

When I run the following code in Python 3.3:I get the following error:I did this too to verify:What am I doing wrong?Import urllib.request instead of urllib.Interestingly, I noticed some IDE-depending behavior.Both Spyder and PyCharm use the same interpreter on my machine : in PyCharm I need to do while in Spyder, does fineIf this is on PyCharm, as was mine, make sure your file name isn't urllib.py.

How to change fonts in matplotlib (python)?

SirC

[How to change fonts in matplotlib (python)?](https://stackoverflow.com/questions/21321670/how-to-change-fonts-in-matplotlib-python)

It sounds as an easy problem but I do not find any effective solution to change the font (not the font size) in a plot made with matplotlib in python.I found a couple of tutorials to change the default font of matplotlib by modifying some files in the folders where matplotlib stores its default font - see this blog post - but I am looking for a less radical solution since I would like to use more than one font in my plot (text, label, axis label, etc).

2014-01-23 23:43:33Z

It sounds as an easy problem but I do not find any effective solution to change the font (not the font size) in a plot made with matplotlib in python.I found a couple of tutorials to change the default font of matplotlib by modifying some files in the folders where matplotlib stores its default font - see this blog post - but I am looking for a less radical solution since I would like to use more than one font in my plot (text, label, axis label, etc).Say you want Comic Sans for the title and Helvetica for the x label.You can also use rcParams to change the font family globally.The list of matplotlib's font family arguments is here.I prefer to employ:or The Helvetica font does not come included with Windows, so to use it you must download it as a .ttf file.

Then you can refer matplotlib to it like this (replace "crm10.ttf" with your file):print(fpath) will show you where you should put the .ttf.You can see the output here:

https://matplotlib.org/gallery/api/font_file.html

「For」loop first iteration

Rince

[「For」loop first iteration](https://stackoverflow.com/questions/1927544/for-loop-first-iteration)

I would like to inquire if there is an elegant pythonic way of executing some function on the first loop iteration.

The only possibility I can think of is:

2009-12-18 11:03:26Z

I would like to inquire if there is an elegant pythonic way of executing some function on the first loop iteration.

The only possibility I can think of is:You have several choices for the Head-Tail design pattern.Or thisPeople whine that this is somehow not "DRY" because the "redundant foo(member)" code.  That's a ridiculous claim.  If that was true then all functions could only be used once.  What's the point of defining a function if you can only have one reference?Something like this should work.However, I would strongly recommend thinking about your code to see if you really have to do it this way, because it's sort of "dirty". Better would be to fetch the element that needs special handling up front, then do regular handling for all the others in the loop.The only reason I could see for not doing it this way is for a big list you'd be getting from a generator expression (which you wouldn't want to fetch up front because it wouldn't fit in memory), or similar situations.how about:or maybe:Documentation of index-method.I think this is quite elegant, but maybe too convoluted for what it does...This works:In most cases, though, I'd suggest just iterating over whatever[1:] and doing the root thing outside the loop; that's usually more readable. Depends on your use case, of course.Here, I could come with a Pythonic idiom that can look "pertty". Although, most likely I'd use the form you suggested in asking the question, just for the code to remain more obvious, though less elegant.(sorry - the first I posted, before editing, form would not work, I had forgotten to actually get an iterator for the 'copy' object)If something.get() iterates over something, you can do it also as follows:I think the first S.Lott solution is the best, but there's another choice if you're using a pretty recent python (>= 2.6 I think, since izip_longest doesn't seem available before that version) that lets doing different things for the first element and successive one, and can be easily modified to do distinct operations for 1st, 2nd, 3rd element... as well.How about using iter, and consuming the first element?Edit: Going back on the OP's question, there is a common operation that you want to perform on all elements, and then one operation you want to perform on the first element, and another on the rest.If it's just a single function call, I'd say just write it twice. It won't end the world. If it's more involved, you can use a decorator to wrap your "first" function and "rest" function with a common operation.Output:or you could do a slice:Can't you do root.copy(something.get()) before the loop?EDIT: Sorry, I missed the second bit. But you get the general idea. Otherwise, enumerate and check for 0?EDIT2: Ok, got rid of the silly second idea.I don't know Python, but I use almost the exact pattern of your example.

What I do also is making the if condition the most frequent, so usually check for if( first == false )

Why? for long loops, first will be true only one time and will be false all the other times, meaning that in all loops but the first, the program will check for the condition and jump to the else part.

By checking for first being false, there will be only one jump to the else part. I don't really know if this adds efficiency at all, but I do it anyway, just to be in peace with my inner nerd.  PS: Yes, I know that when entering the if part, it also has to jump over the else to continue execution, so probably my way of doing it is useless, but it feels nice. :DYour question is contradictory. You say "only do something on first iteration", when in fact you are saying do something different on first/subsequent iterations. This is how I would attempt it:

Python Decimals format

juanefren

[Python Decimals format](https://stackoverflow.com/questions/2389846/python-decimals-format)

WHat is a good way to format a python decimal like this way? 1.00 --> '1'

1.20 --> '1.2'

1.23 --> '1.23'

1.234 --> '1.23'

1.2345 --> '1.23'

2010-03-05 20:51:59Z

WHat is a good way to format a python decimal like this way? 1.00 --> '1'

1.20 --> '1.2'

1.23 --> '1.23'

1.234 --> '1.23'

1.2345 --> '1.23'If you have Python 2.6 or newer, use format:For Python 2.5 or older:Explanation:{0}tells format to print the first argument -- in this case, num.Everything after the colon (:) specifies the format_spec..3 sets the precision to 3.g removes insignificant zeros. See 

http://en.wikipedia.org/wiki/Printf#fprintfFor example:yieldsUsing Python 3.6 or newer, you could use f-strings:Only first part of Justin's answer is correct.

Using "%.3g" will not work for all cases as .3 is not the precision, but total number of digits. Try it for numbers like 1000.123 and it breaks.So, I would use what Justin is suggesting:Here's a function that will do the trick:And here are your examples:Edit:From looking at other people's answers and experimenting, I found that g does all of the stripping stuff for you. So,works splendidly too and is slightly different from what other people are suggesting (using '{0:.3}'.format() stuff). I guess take your pick.Just use Python's standard string formatting methods:If you are using a Python version under 2.6, use

How to check if array is not empty? [duplicate]

amchew

[How to check if array is not empty? [duplicate]](https://stackoverflow.com/questions/5086178/how-to-check-if-array-is-not-empty)

How to check if the array is not empty? I did this:Is this the right way?

2011-02-23 01:48:56Z

How to check if the array is not empty? I did this:Is this the right way?There's no mention of numpy in the question. If by array you mean list, then if you treat a list as a boolean it will yield True if it has items and False if it's empty.with a as a  numpy array, use:(in Python, objects like [1,2,3] are called lists, not arrays.)Is fine toolen(self.table) checks for the length of the array, so you can use if-statements to find out if the length of the list is greater than 0 (not empty):Python 2:Python 3:It's also possible to use to see if the list is not empty.If you are talking about Python's actual array (available through import array from array), then the principle of least astonishment applies and you can check whether it is empty the same way you'd check if a list is empty.As many languages have the len() function, in Python this would work for your question.If the output is not 0, the list is not empty.An easy way is to use Boolean expressions:Or you can use another Boolean expression :I can't comment yet, but it should be mentioned that if you use numpy array with more than one element this will fail:the error will be:

Using Django database layer outside of Django?

gct

[Using Django database layer outside of Django?](https://stackoverflow.com/questions/2180415/using-django-database-layer-outside-of-django)

I've got a nice database I've created in Django, and I'd like to interface with through some python scripts outside of my website stuff, so I'm curious if it's possible to use the Django database API outside of a Django site, and if so does anyone have any info on how it can be done?  Google hasn't yielded many hits for this.

2010-02-01 22:05:31Z

I've got a nice database I've created in Django, and I'd like to interface with through some python scripts outside of my website stuff, so I'm curious if it's possible to use the Django database API outside of a Django site, and if so does anyone have any info on how it can be done?  Google hasn't yielded many hits for this.You just need to configure the Django settings before you do any calls, including importing your models. Something like this:Again, be sure to run that code before running, e.g.:Then just use the DB API as usual.For django 1.7, I used the following to get up and running.settings.py:In the file containing the startup routineUpdate setup_environ is to be removed in django 1.6If you're able to import your settings.py file, then take a look at handy setup_environ command.A final option no-one's mentioned: a custom ./manage.py subcommand.Here is the code I use. Just replace your_project with your Django project name, yourApp with your Django app name, any_model with the model you want to use in models file and any_fild with the field you want to get from the database:For using Django ORM from other applications you need:1) export DJANGO_SETTINGS_MODULE=dproj.settings2) Add your Django app folder to the path (you can do it in the code of your non-django-app):3) If using SQLite, use the full path to the db file in settings.py:For django 1.5 on (multiple databases are supported) the DATABASE settings also changed.

You need to adapt the previous answer to ...Based on the answer by Hai Hu, here is a working script, tested on Django 1.10 and 1.11.

I first import Django's base apps because they are needed in many other apps. I was looking for answers for django 3.0 and none of the above method exactly worked for me.I read the official docs at https://docs.djangoproject.com/en/3.0/topics/settings/ and this scripts worked for me.my script db_tasks.py is inside mysite like this.

Is it safe to rely on condition evaluation order in if statements?

tgray

[Is it safe to rely on condition evaluation order in if statements?](https://stackoverflow.com/questions/752373/is-it-safe-to-rely-on-condition-evaluation-order-in-if-statements)

Is it bad practice to use the following format when my_var can be None?The issue is that 'something' in my_var will throw a TypeError if my_var is None.Or should I use:or To rephrase the question, which of the above is the best practice in Python (if any)?Alternatives are welcome!

2009-04-15 15:57:19Z

Is it bad practice to use the following format when my_var can be None?The issue is that 'something' in my_var will throw a TypeError if my_var is None.Or should I use:or To rephrase the question, which of the above is the best practice in Python (if any)?Alternatives are welcome!It's safe to depend on the order of conditionals (Python reference here), specifically because of the problem you point out - it's very useful to be able to short-circuit evaluation that could cause problems in a string of conditionals.This sort of code pops up in most languages:Yes it is safe, it's explicitly and  very clearly defined in the language reference:I may be being a little pedantic here, but I would say the best answer isThe difference being the explicit check for None, rather than the implicit conversion of my_var to True or False.While I'm sure in your case the distinction isn't important, in the more general case it would be quite possible for the variable to not be None but still evaluate to False, for example an integer value of 0 or an empty list.So contrary to most of the other posters' assertions that it's safe, I'd say that it's safe as long as you're explicit. If you're not convinced then consider this very contrived class:Yes I know that's not a realistic example, but variations do happen in real code, especially when None is used to indicate a default function argument.It's not that simple. As a C# dude I am very used to doing something like:The above works great and is evaluated as expected. However in VB.Net the following would produce a result you were NOT expecting:The above will generate an exception. The correct syntax should beNote the very subtle difference. This had me confused for about 10 minutes (way too long) and is why C# (and other) dudes needs to be very careful when coding in other languages.I would go with the try/except, but it depends on what you know about the variable.If you are expecting that the variable will exist most of the time, then a try/except is less operations.  If you are expecting the variable to be None most of the time, then an IF statement will be less operations.It's perfectly safe and I do it all the time.

str.format() raises KeyError

Dor

[str.format() raises KeyError](https://stackoverflow.com/questions/2755201/str-format-raises-keyerror)

The following code raises a KeyError exception:Why?I am using Python 3.1.

2010-05-02 22:06:31Z

The following code raises a KeyError exception:Why?I am using Python 3.1.The problem is those { and } characters you have there that don't specify a key for formatting. You need to double them up, so change your code to:

How to do unit testing of functions writing files using python unittest

jan

[How to do unit testing of functions writing files using python unittest](https://stackoverflow.com/questions/3942820/how-to-do-unit-testing-of-functions-writing-files-using-python-unittest)

I have a Python function that writes an output file to disk.I want to write a unit test for it using Python unittest module.How should I assert equality of files? I would like to get an error if the file content differs from the expected one + list of differences. As in the output of unix diff command.Is there any official/recommended way of doing that?

2010-10-15 13:49:09Z

I have a Python function that writes an output file to disk.I want to write a unit test for it using Python unittest module.How should I assert equality of files? I would like to get an error if the file content differs from the expected one + list of differences. As in the output of unix diff command.Is there any official/recommended way of doing that?The simplest thing is to write the output file, then read its contents, read the contents of the gold (expected) file, and compare them with simple string equality.  If they are the same, delete the output file.  If they are different, raise an assertion.This way, when the tests are done, every failed test will be represented with an output file, and you can use a 3rd-party tool to diff them against the gold files (Beyond Compare is wonderful for this).If you really want to provide your own diff output, remember that the Python stdlib has the difflib module.  The new unittest support in Python 3.1 includes an assertMultiLineEqual method that uses it to show diffs, similar to this:I prefer to have output functions explicitly accept a file handle (or file-like object), rather than accept a file name and opening the file themselves. This way, I can pass a StringIO object to the output function in my unit test, then .read() the contents back from that StringIO object (after a .seek(0) call) and compare with my expected output.For example, we would transition code like thisto code like thisThis approach has the added benefit of making your output function more flexible if, for instance, you decide you don't want to write to a file, but some other buffer, since it will accept all file-like objects.Note that using StringIO assumes the contents of the test output can fit into main memory. For very large output, you can use a temporary file approach (e.g., tempfile.SpooledTemporaryFile).ThenI always try to avoid writing files to disk, even if it's a temporary folder dedicated to my tests: not actually touching the disk makes your tests much faster, especially if you interact with files a lot in your code.Suppose you have this "amazing" piece of software in a file called main.py:To test the write_to_file method, you can write something like this in a file in the same folder called test_main.py:You could separate the content generation from the file handling. That way, you can test that the content is correct without having to mess around with temporary files and cleaning them up afterward.If you write a generator method that yields each line of content, then you can have a file handling method that opens a file and calls file.writelines() with the sequence of lines. The two methods could even be on the same class: test code would call the generator, and production code would call the file handler.Here's an example that shows all three ways to test. Usually, you would just pick one, depending on what methods are available on the class to test.Based on suggestions I did the following.I created a subclass MyTestCase as I have lots of functions that need to read/write files so I really need to have re-usable assert method. Now in my tests, I would subclass MyTestCase instead of unittest.TestCase.What do you think about it?

How to implement conditional string formatting?

George Osterweil

[How to implement conditional string formatting?](https://stackoverflow.com/questions/9244909/how-to-implement-conditional-string-formatting)

I've been working on a text-based game in Python, and I've come across an instance where I want to format a string differently based on a set of conditions. Specifically, I want to display text describing items in a room. I want this to be displayed, in the room's description, if and only if the item object in question is in the room object's list of items. The way it is set up, I feel that simply concatenating strings based on conditionals will not output as I want, and it would be better to have a different string for each case. My question is, is there any pythonic method for formatting strings based on the result of a Boolean conditional? I could use a for loop structure, but I was wondering if there was something easier, similar to a generator expression.I'm looking for something similar to this, in string formAs a general example of what I mean:I realize that this example is not valid Python, but it shows, in general, what I'm looking for. I'm wondering if there is any valid expression for boolean string formatting, similar to the above.

After searching around a bit, I was unable to find anything pertaining specifically to conditional string formatting. I did find several posts on format strings in general, but that is not what I'm looking for.If something like that does indeed exist, it would be very useful. I'm also open to any alternate methods that may be suggested. Thanks in advance for any help you can provide.

2012-02-11 23:16:00Z

I've been working on a text-based game in Python, and I've come across an instance where I want to format a string differently based on a set of conditions. Specifically, I want to display text describing items in a room. I want this to be displayed, in the room's description, if and only if the item object in question is in the room object's list of items. The way it is set up, I feel that simply concatenating strings based on conditionals will not output as I want, and it would be better to have a different string for each case. My question is, is there any pythonic method for formatting strings based on the result of a Boolean conditional? I could use a for loop structure, but I was wondering if there was something easier, similar to a generator expression.I'm looking for something similar to this, in string formAs a general example of what I mean:I realize that this example is not valid Python, but it shows, in general, what I'm looking for. I'm wondering if there is any valid expression for boolean string formatting, similar to the above.

After searching around a bit, I was unable to find anything pertaining specifically to conditional string formatting. I did find several posts on format strings in general, but that is not what I'm looking for.If something like that does indeed exist, it would be very useful. I'm also open to any alternate methods that may be suggested. Thanks in advance for any help you can provide.Your code actually is valid Python if you remove two characters, the comma and the colon.More modern style uses .format, though:where the argument to format can be a dict you build in whatever complexity you like.On Python 3.6+, use a formatted string literal (they're called f-strings: f"{2+2}" produces "4") with an if statement:You can't use backslashes to escape quotes in the expression part of an f-string so

you have to mix double " and single ' quotes. (You can still use backslashes in the outer part of an f-string, eg. f'{2}\n' is fine)There is a conditional expression in Python which takes the formYour example can easily be turned into valid Python by omitting just two characters:An alternative I'd often prefer is to use a dictionary:

Connection Timeout with Elasticsearch

Johann Gomes

[Connection Timeout with Elasticsearch](https://stackoverflow.com/questions/28287261/connection-timeout-with-elasticsearch)

This simples code is returning the following error:Very strange, because the server is ready and set (http://localhost:9200/ is returning some json).

2015-02-02 21:39:20Z

This simples code is returning the following error:Very strange, because the server is ready and set (http://localhost:9200/ is returning some json).By default, the timeout value is set to 10 secs. If one wants to change the global timeout value, this can be achieved by setting the flag timeout=your-time while creating the object.If you have already created the object without specifying the timeout value, then you can set the timeout value for particular request by using request_timeout=your-time flag in the query.The connection timed out problem could occur if you are using Amazon Elastic Search service.The above python code where you override the default port from 9200 to 443 and setting the SSL to true will resolve the issue.If no port is specified, it is trying to connect to the port 9200 in the specified host and fails after time outThis is nothing to do with increasing your timeout to 30 seconds.

Do people actually think that elastic search should need up to 30 seconds to return one tiny hit?The way I fixed this problem was go to config/elasticsearch.yml

uncomment the followingNetwork.host  might be set to 192.168.0.1 which might work  But I just changed it to 'localhost' elasticsearch.exceptions.ConnectionTimeout: ConnectionTimeout caused by - ReadTimeoutError(HTTPConnectionPool(host='localhost', port=9200): Read timed out. (read timeout=10))

mean the request didn't end in the specified time (by default, timeout=10).This will work with 30 seconds :res = es.index(index="test-index", doc_type='tweet', id=1, body=doc, timeout=30)my personal problem was solved with (timeout = 10000) which was practically never reached because the entries on server were only 7.000 but it had heavy traffic and its resources were being hogged and that was why the connection was dropping Note that one of the common reasons for timeouts when doing es.search (or es.index) is large query size. For example, in my case of a pretty large ES index size (> 3M documents), doing a search for a query with 30 words took around 2 seconds, while doing a search for a query with 400 words took over 18 seconds. So for a sufficiently large query even timeout=30 won't save you. An easy solution is to crop the query to the size that can be answered below the timeout.Increasing timeout or doing retries on timeout will help you if the cause was in traffic, otherwise this might be your culprit.

Type hint for a file or file-like object?

Mark Amery

[Type hint for a file or file-like object?](https://stackoverflow.com/questions/38569401/type-hint-for-a-file-or-file-like-object)

Is there any correct type hint to use for a file or file-like object in Python? For example, how would I type-hint the return value of this function?

2016-07-25 13:42:50Z

Is there any correct type hint to use for a file or file-like object in Python? For example, how would I type-hint the return value of this function?Use either the typing.TextIO or typing.BinaryIO types, for files opened in text mode or binary mode respectively.From the docs:The short answer:As an example:gives an inspection error (in PyCharm) of Expected type 'BinaryIO', got 'TextIO' instead

Different object size of True and False in Python 3

Simon Fromme

[Different object size of True and False in Python 3](https://stackoverflow.com/questions/53015922/different-object-size-of-true-and-false-in-python-3)

Experimenting with magic methods (__sizeof__ in particular) on different Python objects I stumbled over the following behaviour:Python 2.7Python 3.xWhat changed in Python 3 that makes the size of True greater than the size of False?

2018-10-26 20:28:50Z

Experimenting with magic methods (__sizeof__ in particular) on different Python objects I stumbled over the following behaviour:Python 2.7Python 3.xWhat changed in Python 3 that makes the size of True greater than the size of False?It is because bool is a subclass of int in both Python 2 and 3. But the int implementation has changed.In Python 2, int was the one that was 32 or 64 bits, depending on the system, as opposed to arbitrary-length long.In Python 3, int is arbitrary-length - the long of Python 2 was renamed to int and the original Python 2 int dropped altogether.In Python 2 you get the exactly same behaviour for long objects 1L and 0L:The long/Python 3 int is a variable-length object, just like a tuple - when it is allocated, enough memory is allocated to hold all the binary digits required to represent it. The length of the variable part is stored in the object head. 0 requires no binary digits (its variable length is 0), but even 1 spills over, and requires extra digits.I.e. 0 is represented as binary string of length 0:and 1 is represented as a 30-bit binary string:The default configuration in Python uses 30 bits in a uint32_t; so 2**30 - 1 still fits in 28 bytes on x86-64, and 2**30 will require 32; 2**30 - 1 will be presented as i.e. all 30 value bits set to 1; 2**30 will need more, and it will have internal representationAs for True using 28 bytes instead of 24 - you need not worry. True is a singleton and therefore only 4 bytes are lost in total in any Python program, not 4 for every usage of True.Both True and False are longobjects in CPython:You thus can say that a Boolean is a subclass of a python-3.x int where True takes as value 1, and False takes as value 0. We thus make a call to PyVarObject_HEAD_INIT with as type parameter a reference to PyBool_Type and with ob_size as value 0 and 1 respectively.Now since python-3.x, there is no long anymore: these have been merged, and the int object will, depending on the size of the number, take a different value.If we inspect the source code of the longlobject type, we see:To make a long story short, an _longobject can be seen as an array of "digits", but you should here see digits not as decimal digits, but as groups of bits that thus can be added, multiplied, etc.Now as is specified in the comment, it says that:So in case the value is zero, no digits are added, whereas for small integers (values less than 230 in CPython), it takes one digit, and so on.In python-2.x, there were two types of representations for numbers, ints (with a fixed size), you could see this as "one digit", and longs, with multiple digits. Since a bool was a subclass of int, both True and False occupied the same space.Take a look at the cpython code for True and FalseInternally it is represented as integerI haven't seen CPython code for this, but I believe this has something to do with optimization of integers in Python 3. Probably, as long was dropped, some optimizations were unified. int in Python 3 is arbitrary-sized int – the same as long was in Python 2. As bool stores in the same way as new int, it affects both.Interesting part:+ bytes for object headers should complete the equation.

How to download a file using python in a 'smarter' way?

kender

[How to download a file using python in a 'smarter' way?](https://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way)

I need to download several files via http in Python.The most obvious way to do it is just using urllib2:But I'll have to deal with the URLs that are nasty in some way, say like this: http://server.com/!Run.aspx/someoddtext/somemore?id=121&m=pdf. When downloaded via the browser, the file has a human-readable name, ie. accounts.pdf. Is there any way to handle that in python, so I don't need to know the file names and hardcode them into my script?

2009-05-14 08:21:42Z

I need to download several files via http in Python.The most obvious way to do it is just using urllib2:But I'll have to deal with the URLs that are nasty in some way, say like this: http://server.com/!Run.aspx/someoddtext/somemore?id=121&m=pdf. When downloaded via the browser, the file has a human-readable name, ie. accounts.pdf. Is there any way to handle that in python, so I don't need to know the file names and hardcode them into my script?Download scripts like that tend to push a header telling the user-agent what to name the file:If you can grab that header, you can get the proper filename.There's another thread that has a little bit of code to offer up for Content-Disposition-grabbing.Based on comments and @Oli's anwser, I made a solution like this:It takes file name from Content-Disposition; if it's not present, uses filename from the URL (if redirection happened, the final URL is taken into account). Combining much of the above, here is a more pythonic solution:2 Kender:it is not safe -- web server can pass wrong formatted name as ["file.ext] or [file.ext'] or even be empty and localName[0] will raise exception.

Correct code can looks like this:Using wget:Using urlretrieve: urlretrieve also creates the directory structure if not exists. 

PyQt or PySide - which one to use [closed]

shutefan

[PyQt or PySide - which one to use [closed]](https://stackoverflow.com/questions/6888750/pyqt-or-pyside-which-one-to-use)

I started learning a bit of python and would now like to toy around a bit with gui-building. Qt seems to be a good choice because of its cross-platformishness.

Now there seem to be two bindings available: PyQt by Riverbank Computing and PySide, originally developed by Nokia.

So which one should I choose? All I can find are two year old feature comparisons, but what differences are there nowadays?

Which one is easier to use, has more/better documentation? Are both still in active development?

Licensing isn't of much concern to me since I don't intend to write commercial applications.

2011-07-31 09:58:15Z

I started learning a bit of python and would now like to toy around a bit with gui-building. Qt seems to be a good choice because of its cross-platformishness.

Now there seem to be two bindings available: PyQt by Riverbank Computing and PySide, originally developed by Nokia.

So which one should I choose? All I can find are two year old feature comparisons, but what differences are there nowadays?

Which one is easier to use, has more/better documentation? Are both still in active development?

Licensing isn't of much concern to me since I don't intend to write commercial applications.Both toolkits are actively maintained, and by now more or less equal in features and quality.  There are only few, rather unimportant differences.Still, I'd recommend PySide for Python 2.  It has a more reasonable API, mainly it doesn't expose Qt types, which have a direct equivalent in Python (e.g. QString, QList, etc.) or which are completely superfluous due to Python's dynamic nature, like QVariant.  This avoids many tedious conversions to and from Qt types, and thus eases programming and avoids many errors.  PyQt also supports this modern API, and uses it by default for Python 3, but not for Python 2 to maintain backwards compatibility.There is also the licensing difference.  PySide is LGPL while PyQt is GPL.  This could make a difference if you don't wish to make your project opensource. Although PyQt always has the propriety version available for a fairly reasonable price.I tend to find the PySide documentation more intuitive.  The API, in my opinion is slightly more Pythonic and the rate of bug fixes is quite impressive at the moment.PyQt has the advantage of Python 3 support and incumbency.  There is a lot more 3rd party documentation/tutorials for it.I recently ported a significant code base (over 8,000 lines of code) from PyQt to PySide.Right now I'd say PyQt is a much more mature, performant and stable project. I hit a number of bugs in PySide, and suspect that any big project will hit issues. Having said that, I reported a bug to the project and it was fixed and in a new release within a few weeks. I'm also having a problem where the app takes about 15 seconds to quit. I've not yet spent the time to find out why. However it's only a matter of time before there will be no reasons for choosing PyQt over PySide.If you do decide to go with PyQt for now, make sure you use API v2 throughout. It is a better API, and will ease any future transition to PySide. Also if you do port, just follow the guidelines on the PySide wiki. Even for an 8+ kloc app consisting of about 20 source files it just took an afternoon.An important fact is that PyQt4 has two versions of its APIs for some things. Version 1 items are such things as using QString instead of unicode, and QVariant (basically just a wrapper, I believe - I've never actually done anything which uses it) instead the wrapped. Version 2, which can be enabled in Python 2 and is enabled in Python 3, is much better (though still unpythonic in many places - PySide is too, but it's getting distinctly better. There are still some remaining incompatibilities with them; PyQt4 has QtCore.pyqt(Signal|Slot|Property), PySide has QtCore.(Signal|Slot|Property).For a project of my own, I decided that I wanted to support both with no changes to the code. I prefer PySide, but on Windows I distribute with PyQt4 as at present it's quite a bit smaller for distribution at present. My solution is to check for PySide and if it's there insert an import hook to redirect PyQt4 imports to PySide, or if it's not, fix up PyQt4 to work like it should.The files used:Then you just import pyqt4pysideimporter and pyqt4pysideimporter.autoselect() (as in main.py in that repository). And after that you can just import PyQt4.Aside: it was also stated a few days ago on the PySide mailing list that they are planning on supporting Python 3 fully within the next few months.Although they might have similar interface for Qt/C++ classes, their interface for Qt/C++ macros such as signal/slot/property are very different.

Porting one to another is not an easy job. It would be better to make the right decision at the very beginning.Beyond the grammar/license differences, I just want to point out some deficiency of PyQt in language binding, which might be essential to write QML project in Python.

These differences finfally push me to PySide from PyQt.I have a 20k line Python app that I unsuccessfully tried to convert to PySide.

Conversion is easy and most of the functionality works.

There are several methods that are not implemented because they are 'deprecated', so I had to fix those.  That was OK.

On Windows, using PySide-1.1.2, the '==' operator is not implemented for many Qt objects.  One workaround is to say: "if id(item1) == id(item2):".

The other observation is that PySide seemed noticeably slower.  I did not isolate PySide as the cause of the slowness, but the problem went away when I reverted back to PyQt.Lastly, as of now, the Android kit with PySide does not seem ready for prime time.

Why doesn't zeromq work on localhost?

user756428

[Why doesn't zeromq work on localhost?](https://stackoverflow.com/questions/6024003/why-doesnt-zeromq-work-on-localhost)

This code works great:But this code doesn't* work:It raises this error:Why, can't zeromq use localhost interfaces?Does it only work on IPC on the same machine?

2011-05-16 22:07:46Z

This code works great:But this code doesn't* work:It raises this error:Why, can't zeromq use localhost interfaces?Does it only work on IPC on the same machine?The problem is at line:try to change to:As @fdb points out:The problem is at line:try to change to:However this deserves more explanation to understand why.The documentation for zmq_bind explains (bold emphasis mine):Since your example uses tcp as the transport protocol we look in the zmq_tcp documentation to discover (again, bold emphasis mine):So, if you're not using wild-card or the interface name, then it means you must use an IPv4 address in numeric form (not a DNS name).Note, this only applies to the use of zmq_bind! On the other hand it is perfectly fine to use a DNS name with zmq_connect as discussed later in the docs for zmq_tcp:

Get all text inside a tag in lxml

Kevin Burke

[Get all text inside a tag in lxml](https://stackoverflow.com/questions/4624062/get-all-text-inside-a-tag-in-lxml)

I'd like to write a code snippet that would grab all of the text inside the <content> tag, in lxml, in all three instances below, including the code tags. I've tried tostring(getchildren()) but that would miss the text in between the tags. I didn't have very much luck searching the API for a relevant function. Could you help me out?

2011-01-07 09:24:36Z

I'd like to write a code snippet that would grab all of the text inside the <content> tag, in lxml, in all three instances below, including the code tags. I've tried tostring(getchildren()) but that would miss the text in between the tags. I didn't have very much luck searching the API for a relevant function. Could you help me out?Try:Example:Produces: '\nText outside tag <div>Text <em>inside</em> tag</div>\n'Does text_content() do what you need?Just use the node.itertext() method, as in:The following snippet which uses python generators works perfectly and is very efficient.''.join(node.itertext()).strip()A version of albertov 's stringify-content that solves the bugs reported by hoju:Defining stringify_children this way may be less complicated:or in one lineRationale is the same as in this answer: leave the serialization of child nodes to lxml. The tail part of node in this case isn't interesting since it is "behind" the end tag. Note that the encoding argument may be changed according to one's needs.Another possible solution is to serialize the node itself and afterwards, strip the start and end tag away:which is somewhat horrible. This code is correct only if node has no attributes, and I don't think anyone would want to use it even then.getting urlgetting all html code within   including table tagxpath selectorres is the html code of table

this was doing job for me.so you can extract the tags content with xpath_text() and tags including their content using tostring()or text = tree.xpath("//content/text()")this last line with strip method using is not nice, but it just worksOne of the simplest code snippets, that actually worked for me and as per documentation at http://lxml.de/tutorial.html#using-xpath-to-find-text iswhere etree is a node/tag whose complete text, you are trying to read. Behold that it doesn't get rid of script and style tags though.In response to @Richard's comment above, if you patch stringify_children to read:it seems to avoid the duplication he refers to.I know that this is an old question, but this is a common problem and I have a solution that seems simpler than the ones suggested so far:Unlike some of the other answers to this question this solution preserves all of tags contained within it and attacks the problem from a different angle than the other working solutions.Here is a working solution. We can get content with a parent tag and then cut the parent tag from output.parent_element must have Element type.Please note, that if you want text content (not html entities in text) please leave html_entities parameter as False.lxml have a method for that:If this is an a tag, you can try:

SSL error downloading NLTK data

GoldenGremlin

[SSL error downloading NLTK data](https://stackoverflow.com/questions/41348621/ssl-error-downloading-nltk-data)

I am trying to download NLTK 3.0 for use with Python 3.6 on Mac OS X 10.7.5, but am getting an SSL error:I downloaded NLTK with a pip3 command: sudo pip3 install -U nltk.Changing the index in the NLTK downloader allows the downloader to show all of NLTK's files, but when one tries to download all, one gets another SSL error (see bottom of photo):I am relatively new to computer science and am not at all savvy with respect to SSL.My question is how to simply resolve this issue?Here is a similar question by a user who is having the same problem:Unable to download nltk dataI decided to post a new question with screenshots, since my edit to that other question was rejected.Similar questions which I did not find helpful:NLTK download SSL: Certificate verify faileddownloading error using nltk.download()

2016-12-27 16:22:49Z

I am trying to download NLTK 3.0 for use with Python 3.6 on Mac OS X 10.7.5, but am getting an SSL error:I downloaded NLTK with a pip3 command: sudo pip3 install -U nltk.Changing the index in the NLTK downloader allows the downloader to show all of NLTK's files, but when one tries to download all, one gets another SSL error (see bottom of photo):I am relatively new to computer science and am not at all savvy with respect to SSL.My question is how to simply resolve this issue?Here is a similar question by a user who is having the same problem:Unable to download nltk dataI decided to post a new question with screenshots, since my edit to that other question was rejected.Similar questions which I did not find helpful:NLTK download SSL: Certificate verify faileddownloading error using nltk.download()You don't need to disable SSL checking if you run the following terminal command:In the place of 3.6, put your version of Python if it's an earlier one. Then you should be able to open your Python interpreter (using the command python3) and successfully run nltk.download() there.This is an issue wherein urllib uses an embedded version of OpenSSL that not in the system certificate store. Here's an answer with more information on what's going on.Please see answer by @doctorBroctor. It is more correct and safer to use. Leaving answer below as it might be useful for something else. https://stackoverflow.com/a/42890688/1167890This will work by disabling SSL checking. In Finder, search for Python 3.6.

It will appear under Application folder.

Expand the Python 3.6 folder.

Then install certificates using "Install Certificates.command".To install in codestar only way is manually download modules and save them into nltk_data folder, create a lambda variable environment NLTK_DATA with valie ./nltk_data.

Best way to generate xml? [duplicate]

Joshkunz

[Best way to generate xml? [duplicate]](https://stackoverflow.com/questions/3844360/best-way-to-generate-xml)

I'm creating an web api and need a good way to very quickly generate some well formatted xml. I cannot find any good way of doing this in python.Note: Some libraries look promising but either lack documentation or only output to files. 

2010-10-02 04:14:10Z

I'm creating an web api and need a good way to very quickly generate some well formatted xml. I cannot find any good way of doing this in python.Note: Some libraries look promising but either lack documentation or only output to files. Using lxml:Output:See the tutorial for more information.ElementTree is a good module for reading xml and writing too e.g.Output:See this tutorial for more details and how to pretty print.Alternatively if your XML is simple, do not underestimate the power of string formatting :)Output:You can use string.Template or some template engine too, for complex formatting.I would use the yattag library. I think it's the most pythonic way:Use lxml.builder class, from: http://lxml.de/tutorial.html#the-e-factoryOutput:An optional way if you want to use pure Python:ElementTree is good for most cases, but it can't CData and pretty print.So, if you need CData and pretty print you should use minidom:minidom_example.py:minidom_example.xml:I've tried a some of the solutions in this thread, and unfortunately, I found some of them to be cumbersome (i.e. requiring excessive effort when doing something non-trivial) and inelegant. Consequently, I thought I'd throw my preferred solution, web2py HTML helper objects, into the mix. First, install the the standalone web2py module:Unfortunately, the above installs an extremely antiquated version of web2py, but it'll be good enough for this example. The updated source is here.Import web2py HTML helper objects documented here.Now, you can use web2py helpers to generate XML/HTML.

Tensorflow set CUDA_VISIBLE_DEVICES within jupyter

Tim

[Tensorflow set CUDA_VISIBLE_DEVICES within jupyter](https://stackoverflow.com/questions/37893755/tensorflow-set-cuda-visible-devices-within-jupyter)

I have two GPUs and would like to run two different networks via ipynb simultaneously, however the first notebook always allocates both GPUs. Using CUDA_VISIBLE_DEVICES, I can hide devices for python files, however I am unsure of how to do so within a notebook.Is there anyway to hide different GPUs in to notebooks running on the same server?

2016-06-18 05:55:39Z

I have two GPUs and would like to run two different networks via ipynb simultaneously, however the first notebook always allocates both GPUs. Using CUDA_VISIBLE_DEVICES, I can hide devices for python files, however I am unsure of how to do so within a notebook.Is there anyway to hide different GPUs in to notebooks running on the same server?You can set environment variables in the notebook using os.environ. Do the following before initializing TensorFlow to limit TensorFlow to first GPU.You can double check that you have the correct devices visible to TFI tend to use it from utility module like notebook_utilYou can do it faster without any imports just by using magics:Notice that all env variable are strings, so no need to use ". You can verify that env-variable is set up by running: %env <name_of_var>. Or check all of them with %env.

Finding multiple occurrences of a string within a string in Python

user225312

[Finding multiple occurrences of a string within a string in Python](https://stackoverflow.com/questions/3873361/finding-multiple-occurrences-of-a-string-within-a-string-in-python)

How do I find multiple occurrences of a string within a string in Python? Consider this:So the first occurrence of ll is at 1 as expected. How do I find the next occurrence of it?Same question is valid for a list. Consider:How do I find all the ll with their indexes? 

2010-10-06 14:10:19Z

How do I find multiple occurrences of a string within a string in Python? Consider this:So the first occurrence of ll is at 1 as expected. How do I find the next occurrence of it?Same question is valid for a list. Consider:How do I find all the ll with their indexes? Using regular expressions, you can use re.finditer to find all (non-overlapping) occurences:Alternatively, if you don't want the overhead of regular expressions, you can also repeatedly use str.find to get the next index:This also works for lists and other sequences.I think what you are looking for is string.countHope this helps 

NOTE: this only captures non-overlapping occurencesFor the list example, use a comprehension:Similarly for strings:this will list adjacent runs of "ll', which may or may not be what you want:FWIW, here are a couple of non-RE alternatives that I think are neater than poke's solution.The first uses str.index and checks for ValueError:The second tests uses str.find and checks for the sentinel of -1 by using iter:To apply any of these functions to a list, tuple or other iterable of strings, you can use a higher-level function —one that takes a function as one of its arguments— like this one:For your list example:If you wanted all the items in a list that contained 'll', you could also do that.Brand new to programming in general and working through an online tutorial. I was asked to do this as well, but only using the methods I had learned so far (basically strings and loops). Not sure if this adds any value here, and I know this isn't how you would do it, but I got it to work with this:This version should be linear in length of the string, and should be fine as long as the sequences aren't too repetitive (in which case you can replace the recursion with a while loop).bstpierre's list comprehension is a good solution for short sequences, but looks to have quadratic complexity and never finished on a long text I was using.For a random string of non-trivial length, the two functions give the same result:But the quadratic version is about 300 times slowerThis program counts the number of all substrings even if they are overlapped without the use of regex. But this is a naive implementation and for better results in worst case it is advised to go through either Suffix Tree, KMP and other string matching data structures and algorithms.Here is my function for finding multiple occurrences. Unlike the other solutions here, it supports the optional start and end parameters for slicing, just like str.index:A simple iterative code which returns a list of indices where the substring occurs.You can split to get relative positions then sum consecutive numbers in a list and add (string length * occurence order) at the same time to get the wanted string indexes.  Maybe not so Pythonic, but somewhat more self-explanatory. It returns the position of the word looked in the original string. I think there's no need to test for length of text; just keep finding until there's nothing left to find. Like this:You can also do it with conditional list comprehension like this:I had randomly gotten this idea just a while ago. Using a While loop with string splicing and string search can work, even for overlapping strings.I'm an amateur in Python Programming (Programming of any language, actually), and am not sure what other issues it could have, but I guess it's working fine?I guess lower() could be used somewhere in it too if needed.

Sending Multipart html emails which contain embedded images

John Jiang

[Sending Multipart html emails which contain embedded images](https://stackoverflow.com/questions/920910/sending-multipart-html-emails-which-contain-embedded-images)

I've been playing around with the email module in python but I want to be able to know how to embed images which are included in the html.So for example if the body is something likeI would like to embed image.png into the email, and the src attribute should be replaced with content-id. Does anybody know how to do this?

2009-05-28 13:45:57Z

I've been playing around with the email module in python but I want to be able to know how to embed images which are included in the html.So for example if the body is something likeI would like to embed image.png into the email, and the src attribute should be replaced with content-id. Does anybody know how to do this?Here is an example I found.For Python versions 3.4 and above.The accepted answer is excellent, but only suitable for older Python versions (2.x and 3.3). I think it needs an update.Here's how you can do it in newer Python versions (3.4 and above):Code working

Tkinter:「Python may not be configured for Tk」

Maciej Ziarko

[Tkinter:「Python may not be configured for Tk」](https://stackoverflow.com/questions/5459444/tkinter-python-may-not-be-configured-for-tk)

Today I wanted to start working with Tkinter, but I have some problems.So how can I configure my Python 3.2 to work with Tkinter?

2011-03-28 13:10:45Z

Today I wanted to start working with Tkinter, but I have some problems.So how can I configure my Python 3.2 to work with Tkinter?According to http://wiki.python.org/moin/TkInter :Under Arch/Manjaro just install the package tk:Install tk-devel (or a similarly-named package) before building Python.To get this to work with pyenv on Ubuntu 16.04 and 18.04, I had to:Then install the version of Python I wanted:Then I could import tkinter just fine:Had the same issue on Fedora with Python 2.7. Turns out some extra packages are required:After installing the packages, this hello-world example seems to be working fine on Python 2.7:And through X11 forwarding, it looks like this:Note that in Python 3, the module name is lowercase, and other packages are probably required...I encountered this issue on python 2.7.9.

To fix it, I installed tk and tcl, and then rebuild python code and reinstall, and during configure, I set the path for tk and tcl explicitly, by:Also, a whole article for python install process: Building Python from SourceI think the most complete answer to this is the accepted answer found here:How to get tkinter working with Ubuntu's default Python 2.7 install?Oh I just have followed the solution Ignacio Vazquez-Abrams has suggest which is install tk-dev before building the python.

(Building the Python-3.6.1 from source on Ubuntu 16.04.)There was pre-compiled objects and binaries I have had build yesterday though, I didn't clean up the objects and just build again on the same build path. And it works beautifully.That's it!To anyone using Windows and Windows Subsystem for Linux, make sure that when you run the python command from the command line, it's not accidentally running the python installation from WSL! This gave me quite a headache just now. A quick check you can do for this is just

which <python command you're using>

If that prints something like /usr/bin/python2 even though you're in powershell, that's probably what's going on.This symptom can also occur when a later version of python (2.7.13, for example) has been installed in /usr/local/bin "alongside of" the release python version, and then a subsequent operating system upgrade (say, Ubuntu 12.04 --> Ubuntu 14.04) fails to remove the updated python there.To fix that imcompatibility, one musta) remove the updated version of python in /usr/local/bin;b) uninstall python-idle2.7; andc) reinstall python-idle2.7.If you're running on an AWS instance that is running Amazon Linux OS, the magic command to fix this for me wasIf you want to determine your Linux build, try cat /etc/*releaseSo appearantly many seems to have had this issue (me including) and I found the fault to be that Tkinter wasn't installed on my system when python was compiled. This post describes how to solve the problem by:This worked wonders for me.You need to install tkinter for python3.On Fedora pip3 install tkinter --user returns Could not find a version that satisfies the requirement... so I have to command: dnf install python3-tkinter. This have solved my problem

Can't install pip packages inside a docker container with Ubuntu

Migwell

[Can't install pip packages inside a docker container with Ubuntu](https://stackoverflow.com/questions/28668180/cant-install-pip-packages-inside-a-docker-container-with-ubuntu)

I'm following the fig guide to using docker with a python application, but when docker gets up to the commandI get the following error message:This repeats several times and then I get another message:So for some reason pip can't access any packages from inside a docker container. Is there anything I need to do to allow it internet access?However pip works fine to install things outside of the docker container, and worked fine even with that exact package (blinker==1.3) so that's not the problem. Also this problem isn't specific to that package. I get the same issue with any pip install command for any package.Does anyone have any idea what's going on here?

2015-02-23 06:41:46Z

I'm following the fig guide to using docker with a python application, but when docker gets up to the commandI get the following error message:This repeats several times and then I get another message:So for some reason pip can't access any packages from inside a docker container. Is there anything I need to do to allow it internet access?However pip works fine to install things outside of the docker container, and worked fine even with that exact package (blinker==1.3) so that's not the problem. Also this problem isn't specific to that package. I get the same issue with any pip install command for any package.Does anyone have any idea what's going on here?Your problem comes from the fact that Docker is not using the proper DNS server.

You can fix it in three different ways :Modifying /etc/resolv.conf and adding the following lines at the end# Google IPv4 nameservers

nameserver 8.8.8.8

nameserver 8.8.4.4

If you want to add other DNS servers, have a look here.However this change won't be permanent (see this thread). To make it permanent :

$ sudo nano /etc/dhcp/dhclient.conf

  Uncomment and edit the line with prepend domain-name-server :

prepend domain-name-servers 8.8.8.8, 8.8.4.4;Restart dhclient : $ sudo dhclient.As explained in the docs :When you run docker, simply add the following parameter : --dns 8.8.8.8   I had the same issue and it plagued me for a while and I tried a lot of solutions online but to no avail. However I finally resolved it as follows:This was the solution for me:For me simply restarting docker daemon helped.I needed to add --network=host to my docker build command:You need to add new DNS addresses in the docker configAdd the dns after ExecStar.Should look like that: Then do:Should work.In my case, with docker version 1.13.0 and docker-machine 0.9.0 under Ubuntu 16.04 I had to modify slightly Tanzaho's answer (2. Modifying Docker config) as follows:For me, I was unable to install pip due to the docker's DNS not configured properly. I've tried the above steps, however, configuring docker DNS to Google DNS does not work for my laptop. Docker's DNS can be properly configured only if I set its DNS to my laptop's assigned IP. If you use Ubuntu, you can use the following steps to configure your docker's DNS:As a Docker newbie, I had a problem that manifested itself in this way when I was following the tutorial for Docker at:https://docs.docker.com/get-started/part2I'm using Docker 17.03.1-ce on a corporate LAN.I checked and double checked my DNS settings. I'd used various ways of configuring the DNS that I'd found in my searches across the Internet. Some caused errors on startup. The approach that I ultimately settled upon for configuring the DNS was the one in the Troubleshoot Linux section of the above link above where the DNS is configured via the daemon.json file in the /etc/docker directory.However, I still had this same issue. What finally solved the problem for me was the configuration of the proxy via the http_proxy and https_proxy environment variables. I had them specified in my Dockerfile, but I neglected to do so before the RUN pip command.Even though it appeared to be a DNS issue, moving these ENV commands ahead of the RUN command made the difference for me. In case that is helpful for anyone with this problem.In case someone is reading this using docker-compose. I managed to resolve this by changing my yaml file as followswhich is equivalent to writingI do not know the reason, but the error means that pip is trying to resolve the /simple/blinker/ as a DNS hostname instead of the pypi.python.org part, which seems very odd since I cannot even come up with any URL for which urlparse could return such a string as a  hostname part. I'd check if there is some problem with ~/.pip/pip.confI had same problem.The cause of error is proxy.So, I edit Dockerfile followingFor me, it was caused by being connected to my university VPN. Disconnecting "solved" the problem.Let it run.  Sometimes pypi is having connection issues which are noisily put in your face to make you think it is broke.  Just to be sure, let it roll, you might find it works it out for itself.The bottom line, despite these red error lines, is "Successfully built"Configuring docker DNS to Google DNS (8.8.8.8) or 10.0.0.2 did not work in my company environment. Running: $ drill @8.8.8.8 www.amazon.com or @10.0.0.2 confirmed this.  In order to find a DNS that would work I ran:

$ drill www.amazon.com and it gave me the DNS IP that is being used in my network.  Then I set it in Ubuntu using the following step to configure  docker's DNS.Changed dns in /etc/docker/daemon.jsonIm new to Docker and tried all the methods mentioned here, but still didn't get it right. the Docker version was 18, and ubuntu version was 16. I tried this method:- First i was building docker with company's internet network. this network is blocking some sites or some how things didnt go well here. So secondly i connected to my very own network(which im using in mobile phone, for example) and tried. things went right. requirement.txt was installed successfully, and docker was build.

manage.py runserver

sreekanth

[manage.py runserver](https://stackoverflow.com/questions/5768797/manage-py-runserver)

I am running 

python manage.py runserver

from a machine A

when I am trying to check in machine B

The url I typed is http://A:8000/

I am getting an error like

The system returned: (111) Connection refused

2011-04-24 05:20:22Z

I am running 

python manage.py runserver

from a machine A

when I am trying to check in machine B

The url I typed is http://A:8000/

I am getting an error like

The system returned: (111) Connection refusedYou can run it for machines  in your network byAnd than you will be able to  reach you server from any machine in your network. 

Just type on other machine in browser http://192.168.0.1:8000 where 192.168.0.1 is IP of you server... and it ready to go....or in you case: Source from django docsYou need to tell manage.py the local ip address and the port to bind to. Something like python manage.py runserver 192.168.23.12:8000. Then use that same ip and port from the other machine. You can read more about it here in the documentation.I was struggling with the same problem and found one solution. I guess it can help you. when you run python manage.py runserver, it will take 127.0.0.1 as default ip address and 8000. 127.0.0.0 is the same as localhost which can be accessed locally. to access it from cross origin you need to run it on your system ip or 0.0.0.0. 0.0.0.0 can be accessed from any origin in the network. 

for port number, you need to set inbound and outbound policy of your system if you want to use your own port number not the default one. To do this you need to run server with command python manage.py runserver 0.0.0.0:<your port> as mentioned above or, set a default ip and port in your python environment. For this see my answer on 

django change default runserver portEnjoy coding .....Just in case any Windows users are having trouble, I thought I'd add my own experience. When running python manage.py runserver 0.0.0.0:8000, I could view urls using localhost:8000, but not my ip address 192.168.1.3:8000. I ended up disabling ipv6 on my wireless adapter, and running ipconfig /renew. After this everything worked as expected. in flask using flask.ext.script, you can do it like this:python manage.py runserver -h 127.0.0.1 -p 8000I had the same problem and  here was my way to solve it:First, You must know your IP address.

On my Windows PC, in the cmd windows i run ipconfig and select my IP V4 address. In my case 192.168.0.13Second as mention above:  runserver 192.168.0.13:8000It worked for me.

The error i did to get the message was the use of the gateway address not my PC address.For people who are using CentOS7, In order to allow access to port 8000, you need to modify firewall rules in a new SSH connection:

Converting Snake Case to Lower Camel Case (lowerCamelCase)

luca

[Converting Snake Case to Lower Camel Case (lowerCamelCase)](https://stackoverflow.com/questions/19053707/converting-snake-case-to-lower-camel-case-lowercamelcase)

What would be a good way to convert from snake case (my_string) to lower camel case (myString) in Python 2.7?The obvious solution is to split by underscore, capitalize each word except the first one and join back together.However, I'm curious as to other, more idiomatic solutions or a way to use RegExp to achieve this (with some case modifier?)

2013-09-27 14:48:06Z

What would be a good way to convert from snake case (my_string) to lower camel case (myString) in Python 2.7?The obvious solution is to split by underscore, capitalize each word except the first one and join back together.However, I'm curious as to other, more idiomatic solutions or a way to use RegExp to achieve this (with some case modifier?)Example:Here's yet another take, which works only in Python 3.5 and higher:Obligatory one-liner:another one linerA little late to this, but I found this on /r/python a couple days ago:and then you can just do:Building on Steve's answer, this version should work:Here is a solution using regular expressions:Without using list comprehensions:There is also tocamelcase to easily convert from snake case to camel case.$ pip install tocamelcaseand then you can use it like this:There is also decamelize that is the inverse of this module.So I needed to convert a whole file with bunch of snake case parameters into camel case. The solution by Mathieu Rodic worked best. Thanks.Here is a little script to use it on files.Allow underscore to be escaped by a preceding underscore (e.g. 'Escaped__snake' would become 'Escaped_Snake', while 'usual_snake' becomes 'UsualSnake'. Include ternary test for blank.Don't capitalize the 1st segment (i.e, 'tHERE_is_a_snake' becomes 'thereIsASnake')

Convert pandas DataFrame into list of lists [duplicate]

user2806761

[Convert pandas DataFrame into list of lists [duplicate]](https://stackoverflow.com/questions/19585280/convert-pandas-dataframe-into-list-of-lists)

I have a pandas data frame like this:  Now I want to get a list of rows in pandas like:  How can I do it?

2013-10-25 08:49:36Z

I have a pandas data frame like this:  Now I want to get a list of rows in pandas like:  How can I do it?There is a built in method which would be the fastest method also, calling tolist on the .values np array:you can do it like this:EDIT: as_matrix is deprecated since version 0.23.0You can use the built in values or to_numpy (recommended option) method on the dataframe:If you explicitly want lists and not a numpy array add .tolist():

Merging dataframes on index with pandas

km1234

[Merging dataframes on index with pandas](https://stackoverflow.com/questions/36538780/merging-dataframes-on-index-with-pandas)

I have two dataframes and each one has two index columns. I would like to merge them. For example, the first dataframe is the following:The second dataframe is the following:and as result I would like to get the following:I have tried a few versions using the pd.merge and .join methods, but nothing seems to work. Do you have any suggestions?

2016-04-11 02:13:41Z

I have two dataframes and each one has two index columns. I would like to merge them. For example, the first dataframe is the following:The second dataframe is the following:and as result I would like to get the following:I have tried a few versions using the pd.merge and .join methods, but nothing seems to work. Do you have any suggestions?You should be able to use join, which joins on the index as default.  Given your desired result, you must use outer as the join type.You can do this with merge:The keyword argument how='outer' keeps all indices from both frames, filling in missing indices with NaN.  The left_index and right_index keyword arguments have the merge be done on the indices. If you get all NaN in a column after doing a merge, another troubleshooting step is to verify that your indices have the same dtypes.The merge code above produces the following output for me:

MatPlotLib: Multiple datasets on the same scatter plot

Austin Richardson

[MatPlotLib: Multiple datasets on the same scatter plot](https://stackoverflow.com/questions/4270301/matplotlib-multiple-datasets-on-the-same-scatter-plot)

I want to plot multiple data sets on the same scatter plot:The above only shows the most recent scatter()I've also tried:

2010-11-24 18:35:46Z

I want to plot multiple data sets on the same scatter plot:The above only shows the most recent scatter()I've also tried:You need a reference to an Axes object to keep drawing on the same subplot.I came across this question as I had exact same problem. Although accepted answer works good but with matplotlib version 2.1.0, it is pretty straight forward to have two scatter plots in one plot without using a reference to AxesI don't know, it works fine for me. Exact commands:You can also do this easily in Pandas, if your data is represented in a Dataframe, as described here:http://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html#scatter-plot

Check element exists in array

Jonny

[Check element exists in array](https://stackoverflow.com/questions/8570606/check-element-exists-in-array)

In PHP there a function called isset() to check if something (like an array index) exists and has a value. How about Python?I need to use this on arrays because I get "IndexError: list index out of range" sometimes.I guess I could use try/catching, but that's a last resort.

2011-12-20 03:55:59Z

In PHP there a function called isset() to check if something (like an array index) exists and has a value. How about Python?I need to use this on arrays because I get "IndexError: list index out of range" sometimes.I guess I could use try/catching, but that's a last resort.Look before you leap (LBYL):Easier to ask forgiveness than permission (EAFP):In Python, EAFP seems to be the popular and preferred style (because it's generally more reliable).  So, all other things being equal, I'll recommend to go for the try/except version in this use case - don't see it as a "last resort".  This excerpt is from the official docs linked above, endorsing using try/except for flow control:I understand your dilemma, but Python is not PHP and coding style known as Easier to Ask for Forgiveness than for Permission (or EAFP in short) is a common coding style in Python.See the source (from documentation):So, basically, using try-catch statements here is not a last resort; it is a common practice.PHP has associative and non-associative arrays, Python has lists, tuples and dictionaries. Lists are similar to non-associative PHP arrays, dictionaries are similar to associative PHP arrays.If you want to check whether "key" exists in "array", you must first tell what type in Python it is, because they throw different errors when the "key" is not present:And if you use EAFP coding style, you should just catch these errors appropriately.If you insist on using LBYL approach, these are solutions for you:Did it help?EDIT: With the clarification, new answer:Note that PHP arrays are vastly different from Python's, combining arrays and dicts into one confused structure. Python arrays always have indices from 0 to len(arr) - 1, so you can check whether your index is in that range. try/catch is a good way to do it pythonically, though.If you're asking about the hash functionality of PHP "arrays" (Python's dict), then my previous answer still kind of stands:has_key is fast and efficient.Instead of array use an hash:You may be able to use the built-in function dir() to produce similar behavior to PHP's isset(), something like:dir() returns a list of the names in the current scope, more information can be found here: http://docs.python.org/library/functions.html#dir.

How to create a new instance from a class object in Python

ycseattle

[How to create a new instance from a class object in Python](https://stackoverflow.com/questions/5924879/how-to-create-a-new-instance-from-a-class-object-in-python)

I need to dynamically create an instance of a class in Python. Basically I am using the load_module and inspect module to import and load the class into a class object, but I can't figure out how to create an instance of this class object.Please help!

2011-05-08 00:23:39Z

I need to dynamically create an instance of a class in Python. Basically I am using the load_module and inspect module to import and load the class into a class object, but I can't figure out how to create an instance of this class object.Please help!I figured out the answer to the question I had that brought me to this page.  Since no one has actually suggested the answer to my question, I thought I'd post it.At this point, a and a2 are both instances of the same class (class k).Just call the "type" built in using three parameters, like this:update 

as stated in the comment bellow this is not the answer to this question at all. I will keep it undeleted, since there are hints some people  get here  trying to dynamically create classes - which is what the line above does.To create an object of a class one has a reference too, as put in the accepted answer, one just have to call the class:The mechanism for instantiation is thus:Python does not use the new keyword some languages use - instead it's data model explains the mechanism used to create an instantance of a class when it is called with the same syntax as any other callable:Its class' __call__ method is invoked (in the case of a class, its class is the "metaclass" - which is usually the built-in type). The normal behavior of this call is to invoke the (pseudo) static __new__ method on the class being instantiated, followed by its __init__. The __new__ method is responsible for allocating memory and such, and normally is done by the __new__ of object which is the class hierarchy root.So calling ClassObject() invokes ClassObject.__class__.call() (which normally will be type.__call__) this __call__ method will receive ClassObject itself as the first parameter - a Pure Python implementation would be like this: (the cPython version is of course, done in C, and with lots of extra code for cornercases and optimizations)(I don't recall seeing on the docs the exact justification (or mechanism) for suppressing extra parameters to the root __new__ and passing it to other classes - but it is what happen "in real life" - if object.__new__ is called with any extra parameters it raises a type error - however, any custom implementation of a __new__ will get the extra parameters normally) This is how you can dynamically create a class named Child in your code, assuming Parent already exists... even if you don't have an explicit Parent class, you could use object...The code below defines __init__() and then associates it with the class.If you have a module with a class you want to import, you can do it like this.If you do not know what the class is named, you can iterate through the classes available from a module.

Difference between defining typing.Dict and dict?

Sarit

[Difference between defining typing.Dict and dict?](https://stackoverflow.com/questions/37087457/difference-between-defining-typing-dict-and-dict)

I am practicing using type hints in Python 3.5. One of my colleague uses typing.Dict:Both of them work just fine, there doesn't appear to be a difference. I have read the typing module documentation.Between typing.Dict or dict which one should I use in the program?

2016-05-07 10:36:37Z

I am practicing using type hints in Python 3.5. One of my colleague uses typing.Dict:Both of them work just fine, there doesn't appear to be a difference. I have read the typing module documentation.Between typing.Dict or dict which one should I use in the program?There is no real difference between using a plain typing.Dict and dict, no.However, typing.Dict is a Generic type that lets you specify the type of the keys and values too, making it more flexible:As such, it could well be that at some point in your project lifetime you want to define the dictionary argument a little more precisely, at which point expanding typing.Dict to typing.Dict[key_type, value_type] is a 'smaller' change than replacing dict.You can make this even more generic by using Mapping or MutableMapping types here; since your function doesn't need to alter the mapping, I'd stick with Mapping. A dict is one mapping, but you could create other objects that also satisfy the mapping interface, and your function might well still work with those:Now you are clearly telling other users of this function that your code won't actually alter the new_bandwidths mapping passed in.Your actual implementation is merely expecting an object that is printable. That may be a test implementation, but as it stands your code would continue to work if you used new_bandwidths: typing.Any, because any object in Python is printable.typing.Dict is a generic version of dict:Here you can specify the type of key and values in the dict: Dict[str, int]

Understanding .get() method in Python [duplicate]

3zzy

[Understanding .get() method in Python [duplicate]](https://stackoverflow.com/questions/2068349/understanding-get-method-in-python)

I don't understand what characters.get(character, 0) + 1 is doing, rest all seems pretty straightforward.

2010-01-14 23:35:11Z

I don't understand what characters.get(character, 0) + 1 is doing, rest all seems pretty straightforward.The get method of a dict (like for example characters) works just like indexing the dict, except that, if the key is missing, instead of raising a KeyError it returns the default value (if you call .get with just one argument, the key, the default value is None).So an equivalent Python function (where calling myget(d, k, v) is just like d.get(k, v) might be:The sample code in your question is clearly trying to count the number of occurrences of each character: if it already has a count for a given character, get returns it (so it's just incremented by one), else get returns 0 (so the incrementing correctly gives 1 at a character's first occurrence in the string).To understand what is going on, let's take one letter(repeated more than once) in the sentence string and follow what happens when it goes through the loop. Remember that we start off with an empty characters dictionary I will pick the letter 'e'. Let's pass the character 'e' (found in the word The) for the first time through the loop. I will assume it's the first character to go through the loop and I'll substitute the variables with their values:characters.get('e', 0) tells python to look for the key 'e' in the dictionary. If it's not found it returns 0. Since this is the first time 'e' is passed through the loop, the character 'e' is not found in the dictionary yet, so the get method returns 0. This 0 value is then added to the 1 (present in the characters[character] = characters.get(character,0) + 1 equation).

After completion of the first loop using the 'e' character, we now have an entry in the dictionary like this: {'e': 1}The dictionary is now: Now, let's pass the second 'e' (found in the word jumped) through the same loop. I'll assume it's the second character to go through the loop and I'll update the variables with their new values:Here the get method finds a key entry for 'e' and finds its value which is 1. 

We add this to the other 1 in characters.get(character, 0) + 1 and get 2 as result.When we apply this in the characters[character] = characters.get(character, 0) + 1 equation:It should be clear that the last equation assigns a new value 2 to the already present 'e' key. 

Therefore the dictionary is now: Start here http://docs.python.org/tutorial/datastructures.html#dictionariesThen here http://docs.python.org/library/stdtypes.html#mapping-types-dictThen here http://docs.python.org/library/stdtypes.html#dict.getIf the character is in the dictionary, characters, you get the dictionary object.If not, you get 0.Syntax:If d is a dictionary, then d.get(k, v) means, give me the value of k in d, unless k isn't there, in which case give me v.  It's being used here to get the current count of the character, which should start at 0 if the character hasn't been encountered before.I see this is a fairly old question, but this looks like one of those times when something's been written without knowledge of a language feature. The collections library exists to fulfill these purposes.In this example the spaces are being counted, obviously, but whether or not you want those filtered is up to you.As for the dict.get(a_key, default_value), there have been several answers to this particular question -- this method returns the value of the key, or the default_value you supply. The first argument is the key you're looking for, the second argument is the default for when that key is not present.

What exactly does numpy.exp() do? [closed]

bugsyb

[What exactly does numpy.exp() do? [closed]](https://stackoverflow.com/questions/31951980/what-exactly-does-numpy-exp-do)

I'm very confused as to what np.exp() actually does. In the documentation it says that it: "Calculates the exponential of all elements in the input array." I'm confused as to what exactly this means. Could someone give me more information to what it actually does?

