2010-06-18 21:46:18Z

I can't seem to get the nose testing framework to recognize modules beneath my test script in the file structure.  I've set up the simplest example that demonstrates the problem. I'll explain it below.Here's the the package file structure:foo.py contains:tests/test_foo.py contains:Both init.py files are emptyIf I run nosetests -vv in the main directory (where foo.py is), I get:I get the same error when I run from inside the tests/ directory.  According to the documentation and an example I found, nose is supposed to add all parent packages to the path as well as the directory from which it is called, but this doesn't seem to be happening in my case.  I'm running Ubuntu 8.04 with Python 2.6.2.  I've built and installed nose manually (not with setup_tools) if that matters. You've got an __init__.py in your top level directory. That makes it a package. If you remove it, your nosetests should work. If you don't remove it, you'll have to change your import to import dir.foo, where dir is the name of your directory.Are you in a virtualenv? In my case, nosetests was the one in /usr/bin/nosetests, which was using /usr/bin/python. The packages in the virtualenv definitely won't be in the system path. The following fixed this:To those of you finding this question later on: I get the import error if I don't have an __init__.py file in my tests directory.My directory structure was like this:If I ran nosetests:It would give the ImportError that everyone else is seeing. If I add a blank __init__.py file it works just fine:Another potential problem appears to be hyphens/dashes in the directory tree.  I recently fixed a nose ImportError issue by renaming a directory from sub-dir to sub_dir.Of course if you have a syntax error in the module being imported that will cause this. For me the problem reared its head when I had a backup of a tests file with a path like module/tests.bak.py in the same directory as tests.py. Also, to deal with the init package/module problem in a Django app, you can run the following (in a bash/OSX shell) to make sure you don't have any init.pyc files lying around:I got this error message because I run the nosetests command from the wrong directory.Silly, but happens.I just ran into one more thing that might cause this issue: naming of tests in the form testname.test.py. That extra . confounds nose and leads to it importing things it should not. I suppose it may be obvious that using unconventional test naming conventions will break things, but I thought it might be worth noting.For example, with the following directory structure, if you want to run nosetests in m1, m2 or m3 to test some functions in n.py, you should use from m2.m3 import n in test.py.Just to complete the question:

If you're struggling with structure like this:And maybe you want to run test from a path outside the project,

include your project path inside your PYTHONPATH.paste it inside your .profile.

If you're under a virtual environment, paste it inside the activate in your venv root

What does preceding a string literal with「r」mean? [duplicate]

Nikki Erwin Ramirez

[What does preceding a string literal with「r」mean? [duplicate]](https://stackoverflow.com/questions/4780088/what-does-preceding-a-string-literal-with-r-mean)

I first saw it used in building regular expressions across multiple lines as a method argument to re.compile(), so I assumed that r stands for RegEx.For example:So what does r mean in this case? Why do we need it?

2011-01-24 08:48:09Z

I first saw it used in building regular expressions across multiple lines as a method argument to re.compile(), so I assumed that r stands for RegEx.For example:So what does r mean in this case? Why do we need it?The r means that the string is to be treated as a raw string, which means all escape codes will be ignored.For an example:'\n' will be treated as a newline character, while r'\n' will be treated as the characters \ followed by n.Source: Python string literalsIt means that escapes won’t be translated. For example:is a string with a backslash followed by the letter n. (Without the r it would be a newline.)b does stand for byte-string and is used in Python 3, where strings are Unicode by default. In Python 2.x strings were byte-strings by default and you’d use u to indicate Unicode.

Asyncio.gather vs asyncio.wait

Claude

[Asyncio.gather vs asyncio.wait](https://stackoverflow.com/questions/42231161/asyncio-gather-vs-asyncio-wait)

asyncio.gather and asyncio.wait seem to have similar uses: I have a bunch of async things that I want to execute/wait for (not necessarily waiting for one to finish before the next one starts). They use a different syntax, and differ in some details, but it seems very un-pythonic to me to have 2 functions that have such a huge overlap in functionality. What am I missing?

2017-02-14 16:21:47Z

asyncio.gather and asyncio.wait seem to have similar uses: I have a bunch of async things that I want to execute/wait for (not necessarily waiting for one to finish before the next one starts). They use a different syntax, and differ in some details, but it seems very un-pythonic to me to have 2 functions that have such a huge overlap in functionality. What am I missing?Although similar in general cases ("run and get results for many tasks"), each function has some specific functionality for other cases:Returns a Future instance, allowing high level grouping of tasks:All tasks in a group can be cancelled by calling group2.cancel() or even all_groups.cancel().  See also .gather(..., return_exceptions=True),Supports waiting to be stopped after the first task is done, or after a specified timeout, allowing lower level precision of operations:asyncio.wait is more low level than asyncio.gather.As the name suggests, asyncio.gather mainly focuses on gathering the results. it waits on a bunch of futures and return their results in a given order.asyncio.wait just waits on the futures. and instead of giving you the results directly, it gives done and pending tasks. you have to mannually collect the values.Moreover, you could specify to wait for all futures to finish or the just the first one with wait.I also noticed that you can provide a group of coroutines in wait() by simply specifying the list:Whereas grouping in gather() is done by just specifying multiple coroutines:

Where is a complete example of logging.config.dictConfig?

David Wolever

[Where is a complete example of logging.config.dictConfig?](https://stackoverflow.com/questions/7507825/where-is-a-complete-example-of-logging-config-dictconfig)

I'd like to use dictConfig, but the documentation is a little bit abstract. Where can I find a concrete, copy+paste-able example of the dictionary used with dictConfig?

2011-09-21 23:11:37Z

I'd like to use dictConfig, but the documentation is a little bit abstract. Where can I find a concrete, copy+paste-able example of the dictionary used with dictConfig?How about here!Usage:In case you see too many logs from third-party packages, feel free to unlist the root logger in the config to suppress those messages.The accepted answer is nice! But what if one could begin with something less complex? The logging module is very powerful thing and the documentation is kind of a little bit overwhelming especially for novice. But for the beginning you no need to configure formatters and handlers. You can add it when you figure out what you want.For example:Example with  Stream Handler, File Handler, Rotating File Handler and SMTP Handler I found Django v1.11.15 default config below, hope it helps

Instance variables vs. class variables in Python

deamon

[Instance variables vs. class variables in Python](https://stackoverflow.com/questions/2714573/instance-variables-vs-class-variables-in-python)

I have Python classes, of which I need only one instance at runtime, so it would be sufficient to have the attributes only once per class and not per instance. If there would be more than one instance (which won't happen), all instance should have the same configuration. I wonder which of the following options would be better or more "idiomatic" Python.Class variables:Instance variables:

2010-04-26 15:14:29Z

I have Python classes, of which I need only one instance at runtime, so it would be sufficient to have the attributes only once per class and not per instance. If there would be more than one instance (which won't happen), all instance should have the same configuration. I wonder which of the following options would be better or more "idiomatic" Python.Class variables:Instance variables:If you have only one instance anyway, it's best to make all variables per-instance, simply because they will be accessed (a little bit) faster (one less level of "lookup" due to the "inheritance" from class to instance), and there are no downsides to weigh against this small advantage.Further echoing Mike's and Alex's advice and adding my own color...Using instance attributes are the typical... the more idiomatic Python. Class attributes are not used used as much, since their use cases are specific. The same is true for static and class methods vs. "normal" methods. They're special constructs addressing specific use cases, else it's code created by an aberrant programmer wanting to show off they know some obscure corner of Python programming.Alex mentions in his reply that access will be (a little bit) faster due to one less level of lookup... let me further clarify for those who don't know about how this works yet. It is very similar to variable access -- the search order of which is:For attribute access, the order is:Both techniques work in an "inside-out" manner, meaning the most local objects are checked first, then outer layers are checked in succession.In your example above, let's say you're looking up the path attribute. When it encounters a reference like "self.path", Python will look at the instance attributes first for a match. When that fails, it checks the class from which the object was instantiated from. Finally, it will search the base classes. As Alex stated, if your attribute is found in the instance, it doesn't need to look elsewhere, hence your little bit of time savings.However, if you insist on class attributes, you need that extra lookup. Or, your other alternative is to refer to the object via the class instead of the instance, e.g., MyController.path instead of self.path. That's a direct lookup which will get around the deferred lookup, but as alex mentions below, it's a global variable, so you lose that bit that you thought you were going to save (unless you create a local reference to the [global] class name).The bottom-line is that you should use instance attributes most of the time. However, there will be occasions where a class attribute is the right tool for the job. Code using both at the same time will require the most diligence, because using self will only get you the instance attribute object and shadows access to the class attribute of the same name. In this case, you must use access the attribute by the class name in order to reference it.When in doubt, you probably want an instance attribute.Class attributes are best reserved for special cases where they make sense. The only very-common use case is methods. It isn't uncommon to use class attributes for read-only constants that instances need to know (though the only benefit to this is if you also want access from outside the class), but you should certainly be cautious about storing any state in them, which is seldom what you want. Even if you will only have one instance, you should write the class like you would any other, which usually means using instance attributes.Same question at Performance of accessing class variables in Python - the code here adapted from @Edward LoperLocal Variables are the fastest to access, pretty much tied with Module Variables, followed by Class Variables, followed by Instance Variables.There are 4 scopes you can access variables from:The test:The result:

How to save traceback / sys.exc_info() values in a variable?

codersofthedark

[How to save traceback / sys.exc_info() values in a variable?](https://stackoverflow.com/questions/8238360/how-to-save-traceback-sys-exc-info-values-in-a-variable)

I want to save the name of the error and the traceback details into a variable.  Here's is my attempt.Output:Desired Output:P.S. I know this can be done easily using the traceback module, but I want to know usage of sys.exc_info()[2] object here.

2011-11-23 07:00:30Z

I want to save the name of the error and the traceback details into a variable.  Here's is my attempt.Output:Desired Output:P.S. I know this can be done easily using the traceback module, but I want to know usage of sys.exc_info()[2] object here.This is how I do it:You should however take a look at the traceback documentation, as you might find there more suitable methods, depending to how you want to process your variable afterwards...sys.exc_info() returns a tuple with three values (type, value, traceback). For Example, In the following programNow If we print the tuple the values will be this.The above details can also be fetched by simply printing the exception in string format. Use traceback.extract_stack() if you want convenient access to module and function names and line numbers.Use ''.join(traceback.format_stack()) if you just want a string that looks like the traceback.print_stack() output.Notice that even with ''.join() you will get a multi-line string, since the elements of format_stack() contain \n.  See output below.Remember to import traceback.Here's the output from traceback.extract_stack().  Formatting added for readability.Here's the output from ''.join(traceback.format_stack()).  Formatting added for readability.Be careful when you take the exception object or the traceback object out of the exception handler, since this causes circular references and gc.collect() will fail to collect. This appears to be of a particular problem in the ipython/jupyter notebook environment where the traceback object doesn't get cleared at the right time and even an explicit call to gc.collect() in finally section does nothing. And that's a huge problem if you have some huge objects that don't get their memory reclaimed because of that (e.g. CUDA out of memory exceptions that w/o this solution require a complete kernel restart to recover).In general if you want to save the traceback object, you need to clear it from references to locals(), like so:In the case of jupyter notebook, you have to do that at the very least inside the exception handler:Tested with python 3.7.p.s. the problem with ipython or jupyter notebook env is that it has %tb magic which saves the traceback and makes it available at any point later. And as a result any locals() in all frames participating in the traceback will not be freed until the notebook exits or another exception will overwrite the previously stored backtrace. This is very problematic. It should not store the traceback w/o cleaning its frames. Fix submitted here.The object can be used as a parameter in Exception.with_traceback() function:

How to configure logging to syslog in Python?

thor

[How to configure logging to syslog in Python?](https://stackoverflow.com/questions/3968669/how-to-configure-logging-to-syslog-in-python)

I can't get my head around Python's logging module. My needs are very simple: I just want to log everything to syslog. After reading documentation I came up with this simple test script:But this script does not produce any log records in syslog. What's wrong?

2010-10-19 13:11:38Z

I can't get my head around Python's logging module. My needs are very simple: I just want to log everything to syslog. After reading documentation I came up with this simple test script:But this script does not produce any log records in syslog. What's wrong?Change the line to this:This works for meYou should always use the local host for logging, whether to /dev/log or localhost  through the TCP stack. This allows the fully RFC compliant and featureful system logging daemon to handle syslog. This eliminates the need for the remote daemon to be functional and provides the enhanced capabilities of syslog daemon's such as rsyslog and syslog-ng for instance. The same philosophy goes for SMTP. Just hand it to the local SMTP software. In this case use 'program mode' not the daemon, but it's the same idea. Let the more capable software handle it. Retrying, queuing, local spooling, using TCP instead of UDP for syslog and so forth become possible. You can also [re-]configure those daemons separately from your code as it should be.Save your coding for your application, let other software do it's job in concert.I found the syslog module to make it quite easy to get the basic logging behavior you describe:There are other things you could do, too, but even just the first two lines of that will get you what you've asked for as I understand it.Piecing things together from here and other places, this is what I came up with that works on unbuntu 12.04 and centOS6Create an file in /etc/rsyslog.d/ that ends in .conf and add the following textRestart rsyslog, reloading did NOT seem to work for the new log files. Maybe it only reloads existing conf files?Then you can use this test program to make sure it actually works.I add a little extra comment just in case it helps anyone because I found this exchange useful but needed this little extra bit of info to get it all working.To log to a specific facility using SysLogHandler you need to specify the facility value.

Say for example that you have defined:local3.*    /var/log/mylogin syslog, then you'll want to use:handler = logging.handlers.SysLogHandler(address = ('localhost',514), facility=19)and you also need to have syslog listening on UDP to use localhost instead of /dev/log.Is your syslog.conf set up to handle facility=user?You can set the facility used by the python logger with the facility argument, something like this:the above script will log to LOCAL0 facility with our custom "LOG_IDENTIFIER"...

you can use LOCAL[0-7] for local purpose.From https://github.com/luismartingil/per.scripts/tree/master/python_syslogHere's the yaml dictConfig way recommended for 3.2 & later.In log cfg.yml:Load the config using:Configured both syslog & a direct file. Note that the /dev/log is OS specific.I fix it on my notebook. The rsyslog service did not listen on socket service. I config this line bellow in  /etc/rsyslog.conf file and solved the problem:$SystemLogSocketName  /dev/logYou can also add a file handler or rotating file handler to send your logs to a local file:

http://docs.python.org/2/library/logging.handlers.html

Python - write() versus writelines() and concatenated strings

AbeLinkon

[Python - write() versus writelines() and concatenated strings](https://stackoverflow.com/questions/12377473/python-write-versus-writelines-and-concatenated-strings)

So I'm learning Python. I am going through the lessons and ran into a problem where I had to condense a great many target.write() into a single write(), while having a "\n" between each user input variable(the object of write()).I came up with:If I try to do:I get an error. But if I type:Then it works fine. Why am I unable to use a string for a newline in write() but I can use it in writelines()?Python 2.7

When I searched google most resources I found were way over my head, I'm still a lay-people.

2012-09-11 20:31:47Z

So I'm learning Python. I am going through the lessons and ran into a problem where I had to condense a great many target.write() into a single write(), while having a "\n" between each user input variable(the object of write()).I came up with:If I try to do:I get an error. But if I type:Then it works fine. Why am I unable to use a string for a newline in write() but I can use it in writelines()?Python 2.7

When I searched google most resources I found were way over my head, I'm still a lay-people.line1 + "\n" + line2 merges those strings together into a single string before passing it to write.Note that if you have many lines, you may want to use "\n".join(list_of_lines).Why am I unable to use a string for a newline in write() but I can use it in writelines()?The idea is the following: if you want to write a single string you can do this with write(). If you have a sequence of strings you can write them all using writelines().write(arg) expects a string as argument and writes it to the file. If you provide a list of strings, it will raise an exception (by the way, show errors to us!).writelines(arg) expects an iterable as argument (an iterable object can be a tuple, a list, a string, or an iterator in the most general sense). Each item contained in the iterator is expected to be a string. A tuple of strings is what you provided, so things worked.The nature of the string(s) does not matter to both of the functions, i.e. they just write to the file whatever you provide them. The interesting part is that writelines() does not add newline characters on its own, so the method name can actually be quite confusing. It actually behaves like an imaginary method called write_all_of_these_strings(sequence).What follows is an idiomatic way in Python to write a list of strings to a file while keeping each string in its own line:This takes care of closing the file for you. The construct '\n'.join(lines) concatenates (connects) the strings in the list lines and uses the character '\n' as glue. It is more efficient than using the + operator.Starting from the same lines sequence, ending up with the same output, but using writelines():This makes use of a generator expression and dynamically creates newline-terminated strings. writelines() iterates over this sequence of strings and writes every item.Edit: Another point you should be aware of:write() and readlines() existed before writelines() was introduced. writelines() was introduced later as a counterpart of readlines(), so that one could easily write the file content that was just read via readlines():Really, this is the main reason why writelines has such a confusing name. Also, today, we do not really want to use this method anymore. readlines() reads the entire file to the memory of your machine before writelines() starts to write the data. First of all, this may waste time. Why not start writing parts of data while reading other parts? But, most importantly, this approach can be very memory consuming. In an extreme scenario, where the input file is larger than the memory of your machine, this approach won't even work. The solution to this problem is to use iterators only. A working example:This reads the input file line by line. As soon as one line is read, this line is written to the output file. Schematically spoken, there always is only one single line in memory (compared to the entire file content being in memory in case of the readlines/writelines approach).if you just want to save and load a list try PicklePickle saving:and loading:Actually, I think the problem is that your variable "lines" is bad. You defined lines as a tuple, but I believe that write() requires a string. All you have to change is your commas into pluses (+).should work.Exercise 16 from Zed Shaw's book? You can use escape characters as follows:

Filtering a list based on a list of booleans

Gabriel

[Filtering a list based on a list of booleans](https://stackoverflow.com/questions/18665873/filtering-a-list-based-on-a-list-of-booleans)

I have a list of values which I need to filter given the values in a list of booleans:I generate a new filtered list with the following line:which results in:The line works but looks (to me) a bit overkill and I was wondering if there was a simpler way to achieve the same.Summary of two good advices given in the answers below:1- Don't name a list filter like I did because it is a built-in function.2- Don't compare things to True like I did with if filter[idx]==True.. since it's unnecessary. Just using if filter[idx] is enough.

2013-09-06 20:12:29Z

I have a list of values which I need to filter given the values in a list of booleans:I generate a new filtered list with the following line:which results in:The line works but looks (to me) a bit overkill and I was wondering if there was a simpler way to achieve the same.Summary of two good advices given in the answers below:1- Don't name a list filter like I did because it is a built-in function.2- Don't compare things to True like I did with if filter[idx]==True.. since it's unnecessary. Just using if filter[idx] is enough.You're looking for itertools.compress:Don't use filter as a variable name, it is a built-in function.With numpy:or see Alex Szatmary's answer if list_a can be a numpy array but not filterNumpy usually gives you a big speed boost as wellLike so:Using zip is the pythonic way to iterate over multiple sequences in parallel, without needing any indexing. This assumes both sequences have the same length (zip stops after the shortest runs out). Using itertools for such a simple case is a bit overkill ...One thing you do in your example you should really stop doing is comparing things to True, this is usually not necessary. Instead of if filter[idx]==True: ..., you can simply write if filter[idx]: ....To do this using numpy, ie, if you have an array, a, instead of list_a:The built-in function zip can be used to make it a little simpler.

Disable individual Python unit tests temporarily

coelhudo

[Disable individual Python unit tests temporarily](https://stackoverflow.com/questions/2066508/disable-individual-python-unit-tests-temporarily)

How can individual unit tests be temporarily disabled when using the unittest module in Python?

2010-01-14 18:26:25Z

How can individual unit tests be temporarily disabled when using the unittest module in Python?Individual test methods or classes can both be disabled using the unittest.skip decorator.For other options, see the docs for Skipping tests and expected failures.You can use decorators to disable the test that can wrap the function and prevent the googletest or python unit test to run the testcase.Output:The latest version (2.7 - unreleased) supports test skipping/disabling like so. You could just get this module and use it on your existing Python install. It will probably work.Before this, I used to rename the tests I wanted skipped to xtest_testname from test_testname. Here's a quick elisp script to do this. My elisp is a little rusty so I apologise in advance for any problems it has. Untested. Simply placing @unittest.SkipTest decorator above the test is enough.I just rename a test case method with an underscore: test_myfunc becomes _test_myfunc.The docs for 2.1 don't specify an ignore or skip method.Usually though, I block comment when needed.Focusing on the "temporarily disabled" part of the question, the best answer somewhat depends on the use case. The use case that brought me here is I am doing test driven development on a function. In this process, I successively write tests and often use break points in the function for debugging. If I just run all the tests every time I run the test runner, I end up stopping at break points for tests that already work. Adding "skip" or munging the test name or something like that is not what I want because when I am done writing the function, I want all tests to run. If I used "skip" I would have to go back and "unskip".For my use case, the solution lies in the test runner, not in the test code. I use pytest. With pytest, it is easy to specify a single test from the command line:(replace the caps with your values). I understand the that question was for python-unitest. I have not used that in a long time. I would not be surprised if it had something similar to pytest. If not, you can easily switch to pytest. You do not need to modify your code. Just install it and change your test runner command.Also, I use PyCharm Pro. On the page that shows my test code, there is a small icon next to the def for each test. I can click that icon and run that test individually.

How to prevent errno 32 broken pipe?

SƲmmēr Aƥ

[How to prevent errno 32 broken pipe?](https://stackoverflow.com/questions/11866792/how-to-prevent-errno-32-broken-pipe)

Currently I am using an app built in python. When I run it in personal computer, it works without problems. However, when I move it into a production server. It keeps showing me the error attached as below:. I've done some research and I got the reason that the end user browser stops the connection while the server is still busy sending data. I wonder why did it happen and what is the root cause that prevents it from running properly in production server, while it works on my personal computer. Any advice is appreciated

2012-08-08 14:31:26Z

Currently I am using an app built in python. When I run it in personal computer, it works without problems. However, when I move it into a production server. It keeps showing me the error attached as below:. I've done some research and I got the reason that the end user browser stops the connection while the server is still busy sending data. I wonder why did it happen and what is the root cause that prevents it from running properly in production server, while it works on my personal computer. Any advice is appreciatedYour server process has received a SIGPIPE writing to a socket. This usually happens when you write to a socket fully closed on the other (client) side. This might be happening when a client program doesn't wait till all the data from the server is received and simply closes a socket (using close function). In a C program you would normally try setting to ignore SIGPIPE signal or setting a dummy signal handler for it. In this case a simple error will be returned when writing to a closed socket. In your case a python seems to throw an exception that can be handled as a premature disconnect of the client.It depends on how you tested it, and possibly on differences in the TCP stack implementation of the personal computer and the server.For example, if your sendall always completes immediately (or very quickly) on the personal computer, the connection may simply never have broken during sending. This is very likely if your browser is running on the same machine (since there is no real network latency).In general, you just need to handle the case where a client disconnects before you're finished, by handling the exception.Remember that TCP communications are asynchronous, but this is much more obvious on physically remote connections than on local ones, so conditions like this can be hard to reproduce on a local workstation.  Specifically, loopback connections on a single machine are often almost synchronous.The broken pipe error usually occurs if your request is blocked or takes too long and after request-side timeout, it'll close the connection and then, when the respond-side (server) tries to write to the socket, it will throw a pipe broken error. This might be because you are using two method for inserting data into database and this cause the site to slow down.In above function, the error is where arrow is pointing.  The correct implementation is below:

Python AttributeError: 'module' object has no attribute 'Serial' [duplicate]

hao_maike

[Python AttributeError: 'module' object has no attribute 'Serial' [duplicate]](https://stackoverflow.com/questions/11403932/python-attributeerror-module-object-has-no-attribute-serial)

I'm trying to access a serial port with Python 2.6 on my Raspberry Pi running Debian.

My script named serial.py tries to import pySerial:For some reason it refuses to establish the serial connection with this error:When I try to type the same code in the interactive Python interpreter it still doesn't work.Strangely, it used to work about a couple hours ago.What could be the problem? I've tried to fix this for a while, installing pySerial again, rewriting my code, double-checking the serial port, etc.Thanks in advance!

2012-07-09 22:09:25Z

I'm trying to access a serial port with Python 2.6 on my Raspberry Pi running Debian.

My script named serial.py tries to import pySerial:For some reason it refuses to establish the serial connection with this error:When I try to type the same code in the interactive Python interpreter it still doesn't work.Strangely, it used to work about a couple hours ago.What could be the problem? I've tried to fix this for a while, installing pySerial again, rewriting my code, double-checking the serial port, etc.Thanks in advance!You're importing the module, not the class. So, you must write:You need to install serial module correctly: pip install pyserial.I'm adding this solution for people who make the same mistake as I did.In most cases: rename your project file 'serial.py' and delete serial.pyc if exists, then you can do simple 'import serial' without attribute error.Problem occurs when you import 'something' when your python file name is 'something.py'.I accidentally installed 'serial' (sudo python -m pip install serial) instead of 'pySerial' (sudo python -m pip install pyserial), which lead to the same error.If the previously mentioned solutions did not work for you, double check if you installed the correct library.You have installed the incorrect package named 'serial'.If you are helpless like me, try this:List all Sub-Modules of "Serial" (or whatever package you are having trouble with) with the method described here:  List all the modules that are part of a python packageIn my case, the problems solved one after the other....looks like a bug to me...This problem is beacouse your proyect is named serial.py and the library imported is name serial too , change the name and thats all.This error can also happen if you have circular dependencies. Check your imports and make sure you do not have any cycles.Yes this topic is a bit old but i wanted to share the solution that worked for me for those who might need it anywayAs Ali said, try to locate your program using the following from terminal : print(serial.__file__) --> CopyCTRL+D #(to get out of python)sudo python3-->paste/__init__.pyActivating __init__.py will say to your program "ok i'm going to use Serial from python3". My problem was that my python3 program was using Serial from python 2.7Other solution: remove other python versionsCaoSources :

https://raspberrypi.stackexchange.com/questions/74742/python-serial-serial-module-not-found-error/85930#85930Tryhard

pip install failing with: OSError: [Errno 13] Permission denied on directory

RunLoop

[pip install failing with: OSError: [Errno 13] Permission denied on directory](https://stackoverflow.com/questions/31512422/pip-install-failing-with-oserror-errno-13-permission-denied-on-directory)

pip install -r requirements.txt fails with the exception below OSError: [Errno 13] Permission denied: '/usr/local/lib/.... What's wrong and how do I fix this? (I am trying to setup Django)

2015-07-20 08:58:05Z

pip install -r requirements.txt fails with the exception below OSError: [Errno 13] Permission denied: '/usr/local/lib/.... What's wrong and how do I fix this? (I am trying to setup Django)My recommendation use safe (a) option, so that requirements of this project do not interfere with other projects requirements.We should really stop advising the use of sudo with pip install. It's better to first try pip install --user. If this fails then take a look at the top post here.The reason you shouldn't use sudo is as follows:When you run pip with sudo, you are running arbitrary Python code from the Internet as a root user, which is quite a big security risk. If someone puts up a malicious project on PyPI and you install it, you give an attacker root access to your machine.You are trying to install a package on the system-wide path without having the permission to do so.Just clarifying what worked for me after much pain in linux (ubuntu based) on permission denied errors, and leveraging from Bert's answer above, I now use ...or if running pip on a requirements file ...and these work reliably for every pip install including creating virtual environments.However, the cleanest solution in my further experience has been to install python-virtualenv and virtualenvwrapper with sudo apt-get install at the system level. Then, inside virtual environments, use pip install without the --user flag AND without sudo. Much cleaner, safer, and easier overall.User doesn't have write permission for some Python installation paths. You can give the permission by:So you should give permission, then try to install it again, if you have new paths you should also give permission:So, I got this same exact error for a completely different reason. Due to a totally separate, but known Homebrew + pip bug, I had followed this workaround listed on Google Cloud's help docs, where you create a .pydistutils.cfg file in your home directory. This file has special config that you're only supposed to use for your install of certain libraries. I should have removed that disutils.cfg file after installing the packages, but I forgot to do so. So the fix for me was actually just...rm ~/.pydistutils.cfg. And then everything worked as normal. Of course, if you have some config in that file for a real reason, then you won't want to just straight rm that file. But in case anyone else did that workaround, and forgot to remove that file, this did the trick for me!It is due permission problem,or try,and then say, pip install -r requirements.txt this will install inside your envdont say, sudo pip install -r requirements.txt this is will install into arbitrary python path.

virtualenvwrapper and Python 3

damon

[virtualenvwrapper and Python 3](https://stackoverflow.com/questions/16123459/virtualenvwrapper-and-python-3)

I installed python 3.3.1 on ubuntu lucid and successfully created a virtualenv as belowthis created a folder envpy331 on my home dir.I also have virtualenvwrapper installed.But in the docs only 2.4-2.7 versions of python are supported..Has anyone tried to  organize the python3 virtualenv ? If so, can you tell me how ?

2013-04-20 17:57:10Z

I installed python 3.3.1 on ubuntu lucid and successfully created a virtualenv as belowthis created a folder envpy331 on my home dir.I also have virtualenvwrapper installed.But in the docs only 2.4-2.7 versions of python are supported..Has anyone tried to  organize the python3 virtualenv ? If so, can you tell me how ?The latest version of virtualenvwrapper is tested under Python3.2. Chances are good it will work with Python3.3 too.If you already have python3 installed as well virtualenvwrapper the only thing you would need to do to use python3 with the virtual environment is creating an environment using:Or, (at least on OSX using brew): Start using the environment and you'll see that as soon as you type python you'll start using python3You can make virtualenvwrapper use a custom Python binary instead of the one virtualenvwrapper is run with. To do that you need to use VIRTUALENV_PYTHON variable which is utilized by virtualenv:virtualenvwrapper now lets you specify the python executable without the path.So (on OSX at least)mkvirtualenv --python=python3 nameOfEnvironment will suffice.On Ubuntu; using mkvirtualenv -p python3 env_name loads the virtualenv with python3.Inside the env, use python --version to verify.   You can add this to your .bash_profile or similar:Then use mkvirtualenv3 instead of mkvirtualenv when you want to create a python 3 environment.I find that running and in the command line on Ubuntu forces mkvirtualenv to use python3 and virtualenv-3.4. One still has to do to create the environment. This is assuming that you have python3 in /usr/bin/python3 and virtualenv-3.4  in /usr/local/bin/virtualenv-3.4.This post on the bitbucket issue tracker of virtualenvwrapper may be of interest.

It is mentioned there that most of virtualenvwrapper's functions work with the venv virtual environments in Python 3.3.

Make sure only a single instance of a program is running

Slava V

[Make sure only a single instance of a program is running](https://stackoverflow.com/questions/380870/make-sure-only-a-single-instance-of-a-program-is-running)

Is there a Pythonic way to have only one instance of a program running? The only reasonable solution I've come up with is trying to run it as a server on some port, then second program trying to bind to same port - fails. But it's not really a great idea, maybe there's something more lightweight than this?  (Take into consideration that program is expected to fail sometimes, i.e. segfault - so things like "lock file" won't work)

2008-12-19 12:42:52Z

Is there a Pythonic way to have only one instance of a program running? The only reasonable solution I've come up with is trying to run it as a server on some port, then second program trying to bind to same port - fails. But it's not really a great idea, maybe there's something more lightweight than this?  (Take into consideration that program is expected to fail sometimes, i.e. segfault - so things like "lock file" won't work)The following code should do the job, it is cross-platform and runs on Python 2.4-3.2. I tested it on Windows, OS X and Linux.The latest code version is available singleton.py. Please file bugs here.You can install tend using one of the following methods:Simple, cross-platform solution, found in another question by zgoda:A lot like S.Lott's suggestion, but with the code.This code is Linux specific. It uses 'abstract' UNIX domain sockets, but it is simple and won't leave stale lock files around. I prefer it to the solution above because it doesn't require a specially reserved TCP port. The unique string postconnect_gateway_notify_lock can be changed to allow multiple programs that need a single instance enforced.I don't know if it's pythonic enough, but in the Java world listening on a defined port is a pretty widely used solution, as it works on all major platforms and doesn't have any problems with crashing programs.Another advantage of listening to a port is that you could send a command to the running instance. For example when the users starts the program a second time, you could send the running instance a command to tell it to open another window (that's what Firefox does, for example. I don't know if they use TCP ports or named pipes or something like that, 'though).Never written python before, but this is what I've just implemented in mycheckpoint, to prevent it being started twice or more by crond:Found Slava-N's suggestion after posting this in another issue (http://stackoverflow.com/questions/2959474). This one is called as a function, locks the executing scripts file (not a pid file) and maintains the lock until the script ends (normal or error).Use a pid file.  You have some known location, "/path/to/pidfile" and at startup you do something like this (partially pseudocode because I'm pre-coffee and don't want to work all that hard):So, in other words, you're checking if a pidfile exists; if not, write your pid to that file.  If the pidfile does exist, then check to see if the pid is the pid of a running process; if so, then you've got another live process running, so just shut down.  If not, then the previous process crashed, so log it, and then write your own pid to the file in place of the old one.  Then continue.You already found reply to similar question in another thread, so for completeness sake see how to achieve the same on Windows uning named mutex.http://code.activestate.com/recipes/474070/This may work.You can wrap your program in a shell script that removes the PID file even if your program crashes.You can, also, use the PID file to kill the program if it hangs.Using a lock-file is a quite common approach on unix.  If it crashes, you have to clean up manually.  You could stor the PID in the file, and on startup check if there is a process with this PID, overriding the lock-file if not.  (However, you also need a lock around the read-file-check-pid-rewrite-file).  You will find what you need for getting and checking pid in the os-package.  The common way of checking if there exists a process with a given pid, is to send it a non-fatal signal.Other alternatives could be combining this with flock or posix semaphores.Opening a network socket, as saua proposed, would probably be the easiest and most portable.For anybody using wxPython for their application, you can use the function  wx.SingleInstanceChecker documented here.I personally use a subclass of wx.App which makes use of wx.SingleInstanceChecker and returns False from OnInit() if there is an existing instance of the app already executing like so:This is a simple drop-in replacement for wx.App that prohibits multiple instances. To use it simply replace wx.App with SingleApp in your code like so:Here is my eventual Windows-only solution.  Put the following into a module, perhaps called 'onlyone.py', or whatever.  Include that module directly into your __ main __ python script file.The code attempts to create a mutex with name derived from the full path to the script.  We use forward-slashes to avoid potential confusion with the real file system.The best solution for this on windows is to use mutexes as suggested by @zgoda.Some answers use fctnl (included also in @sorin tendo package) which is not available on windows and should you try to freeze your python app using a package like pyinstaller which does static imports, it throws an error.Also, using the lock file method, creates a read-only problem with database files( experienced this with sqlite3).I'm posting this as an answer because I'm a new user and Stack Overflow won't let me vote yet.Sorin Sbarnea's solution works for me under OS X, Linux and Windows, and I am grateful for it.However, tempfile.gettempdir() behaves one way under OS X and Windows and another under other some/many/all(?) *nixes (ignoring the fact that OS X is also Unix!). The difference is important to this code. OS X and Windows have user-specific temp directories, so a tempfile created by one user isn't visible to another user. By contrast, under many versions of *nix (I tested Ubuntu 9, RHEL 5, OpenSolaris 2008 and FreeBSD 8), the temp dir is /tmp for all users.That means that when the lockfile is created on a multi-user machine, it's created in /tmp and only the user who creates the lockfile the first time will be able to run the application.A possible solution is to embed the current username in the name of the lock file.It's worth noting that the OP's solution of grabbing a port will also misbehave on a multi-user machine.I use single_process on my gentoo;example: refer: https://pypi.python.org/pypi/single_process/1.0I keep suspecting there ought to be a good POSIXy solution using process groups, without having to hit the file system, but I can't quite nail it down. Something like:On startup, your process sends a 'kill -0' to all processes in a particular group. If any such processes exist, it exits. Then it joins the group. No other processes use that group.However, this has a race condition - multiple processes could all do this at precisely the same time and all end up joining the group and running simultaneously. By the time you've added some sort of mutex to make it watertight, you no longer need the process groups.This might be acceptable if your process only gets started by cron, once every minute or every hour, but it makes me a bit nervous that it would go wrong precisely on the day when you don't want it to.I guess this isn't a very good solution after all, unless someone can improve on it?I ran into this exact problem last week, and although I did find some good solutions, I decided to make a very simple and clean python package and uploaded it to PyPI. It differs from tendo in that it can lock any string resource name. Although you could certainly lock __file__ to achieve the same effect.Install with: pip install quicklockUsing it is extremely simple:Take a look: https://pypi.python.org/pypi/quicklockBuilding upon Roberto Rosario's answer, I come up with the following function:We need to define global SOCKET vaiable since it will only be garbage collected when the whole process quits. If we declare a local variable in the function, it will go out of scope after the function exits, thus the socket be deleted.All the credit should go to Roberto Rosario, since I only clarify and elaborate upon his code. And this code will work only on Linux, as the following quoted text from https://troydhanson.github.io/network/Unix_domain_sockets.html explains:linux exampleThis method is based on the creation of a temporary file automatically deleted after you close the application.

the program launch we verify the existence of the file;

if the file exists ( there is a pending execution) , the program is closed ; otherwise it creates the file and continues the execution of the program.On a Linux system one could also ask

pgrep -a for the number of instances, the script 

is found in the process list (option -a reveals the 

full command line string). E.g.Remove -u $UID if the restriction should apply to all users. 

Disclaimer: a) it is assumed that the script's (base)name is unique, b) there might be race conditions.

how to check the dtype of a column in python pandas

James Bond

[how to check the dtype of a column in python pandas](https://stackoverflow.com/questions/22697773/how-to-check-the-dtype-of-a-column-in-python-pandas)

I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:Is there a more elegant way to do this? E.g.

2014-03-27 19:49:31Z

I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:Is there a more elegant way to do this? E.g.You can access the data-type of a column with dtype:In pandas 0.20.2 you can do:So your code becomes:I know this is a bit of an old thread but with pandas 19.02, you can do:http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.DataFrame.select_dtypes.htmlAsked question title is general, but authors use case stated in the body of the question is specific. So any other answers may be used.  But in order to fully answer the title question it should be clarified that it seems like all of the approaches may fail in some cases and require some rework. I reviewed all of them (and some additional) in decreasing of reliability order (in my opinion):Despite the fact that this is accepted answer and has most upvotes count, I think this method should not be used at all. Because in fact this approach is discouraged in python as mentioned several times here.

But if one still want to use it - should be aware of some pandas-specific dtypes like pd.CategoricalDType, pd.PeriodDtype, or pd.IntervalDtype. Here one have to use extra type( ) in order to recognize dtype correctly:Another caveat here is that type should be pointed out precisely:This method has not been mentioned in answers so far.  So if direct comparing of types is not a good idea - lets try built-in python function for this purpose, namely - isinstance().

It fails just in the beginning, because assumes that we have some objects, but pd.Series or pd.DataFrame may be used as just empty containers with predefined dtype but no objects in it:But if one somehow overcome this issue, and wants to access each object, for example, in the first row and checks its dtype like something like that:It will be misleading in the case of mixed type of data in single column:And last but not least - this method cannot directly recognize Category dtype. As stated in docs: So this method is also almost inapplicable.This method yet may work with empty pd.Series or pd.DataFrames but has another problems. First - it is unable to differ some dtypes:Second, what is actually still unclear for me, it even returns on some dtypes None.This is almost what we want. This method designed inside pandas so it handles most corner cases mentioned earlier - empty DataFrames, differs numpy or pandas-specific dtypes well. It works well with single dtype like .select_dtypes('bool'). It may be used even for selecting groups of columns based on dtype:Like so, as stated in the docs:On may think that here we see first unexpected (at used to be for me: question) results - TimeDelta is included into output DataFrame. But as answered in contrary it should be so, but one have to be aware of it. Note that bool dtype is skipped, that may be also undesired for someone, but it's due to bool and number are in different "subtrees" of numpy dtypes. In case with bool, we may use test.select_dtypes(['bool']) here. Next restriction of this method is that for current version of pandas (0.24.2), this code: test.select_dtypes('period') will raise NotImplementedError.And another thing is that it's unable to differ strings from other objects:But this is, first - already mentioned in the docs. And second - is not the problem of this method, rather the way strings are stored in DataFrame. But anyway this case have to have some post processing.This one is intended to be most robust and native way to achieve dtype recognition (path of the module where functions resides says by itself) as i suppose. And it works almost perfectly, but still have at least one caveat and still have to somehow distinguish string columns.Besides, this may be subjective, but this approach also has more 'human-understandable' number dtypes group processing comparing with .select_dtypes('number'):No timedelta and bool is included. Perfect.My pipeline exploits exactly this functionality at this moment of time, plus a bit of post hand processing.Hope I was able to argument the main point - that all discussed approaches may be used, but only pd.DataFrame.select_dtypes() and pd.api.types.is_XXX_dtype should be really considered as the applicable ones.If you want to mark the type of a dataframe column as a string, you can do:

An example:

The answer for your code:

To check the data types after, for example, an import from a fileIllustrative output:

Why does += behave unexpectedly on lists?

eucalculia

[Why does += behave unexpectedly on lists?](https://stackoverflow.com/questions/2347265/why-does-behave-unexpectedly-on-lists)

The += operator in python seems to be operating unexpectedly on lists.  Can anyone tell me what is going on here?OUTPUTfoo += bar seems to affect every instance of the class, whereas foo = foo + bar seems to behave in the way I would expect things to behave. The += operator is called a "compound assignment operator". 

2010-02-27 12:15:59Z

The += operator in python seems to be operating unexpectedly on lists.  Can anyone tell me what is going on here?OUTPUTfoo += bar seems to affect every instance of the class, whereas foo = foo + bar seems to behave in the way I would expect things to behave. The += operator is called a "compound assignment operator". The general answer is that += tries to call the __iadd__ special method, and if that isn't available it tries to use __add__ instead. So the issue is with the difference between these special methods.The __iadd__ special method is for an in-place addition, that is it mutates the object that it acts on. The __add__ special method returns a new object and is also used for the standard + operator.So when the += operator is used on an object which has an __iadd__ defined the object is modified in place. Otherwise it will instead try to use the plain __add__ and return a new object.That is why for mutable types like lists += changes the object's value, whereas for immutable types like tuples, strings and integers a new object is returned instead (a += b becomes equivalent to a = a + b).For types that support both __iadd__ and __add__ you therefore have to be careful which one you use. a += b will call __iadd__ and mutate a, whereas a = a + b will create a new object and assign it to a. They are not the same operation!For immutable types (where you don't have an __iadd__) a += b and a = a + b are equivalent. This is what lets you use += on immutable types, which might seem a strange design decision until you consider that otherwise you couldn't use += on immutable types like numbers!For the general case, see Scott Griffith's answer. When dealing with lists like you are, though, the += operator is a shorthand for someListObject.extend(iterableObject). See the documentation of extend().The extend function will append all elements of the parameter to the list.When doing foo += something you're modifying the list foo in place, thus you don't change the reference that the name foo points to, but you're changing the list object directly. With foo = foo + something, you're actually creating a new list.This example code will explain it:Note how the reference changes when you reassign the new list to l.As bar is a class variable instead of an instance variable, modifying in place will affect all instances of that class. But when redefining self.bar, the instance will have a separate instance variable self.bar without affecting the other class instances.The problem here is, bar is defined as a class attribute, not an instance variable.In foo, the class attribute is modified in the init method, that's why all instances are affected.In foo2, an instance variable is defined using the (empty) class attribute, and every instance gets its own bar.The "correct" implementation would be:Of course, class attributes are completely legal. In fact, you can access and modify them without creating an instance of the class like this:Although much time has passed and many correct things were said, there is no answer which bundles both effects.You have 2 effects:In class foo, the __init__ method modifies the class attribute. It is because self.bar += [x] translates to self.bar = self.bar.__iadd__([x]). __iadd__() is for inplace modification, so it modifies the list and returns a reference to it.Note that the instance dict is modified although this would normally not be necessary as the class dict already contains the same assignment. So this detail goes almost unnoticed - except if you do a foo.bar = [] afterwards. Here the instances's bar stays the same thanks to the said fact.In class foo2, however, the class's bar is used, but not touched. Instead, a [x] is added to it, forming a new object, as self.bar.__add__([x]) is called here, which doesn't modify the object. The result is put into the instance dict then, giving the instance the new list as a dict, while the class's attribute stays modified.The distinction between ... = ... + ... and ... += ... affects as well the assignments afterwards:You can verify the identity of the objects with print id(foo), id(f), id(g) (don't forget the additional ()s if you are on Python3).BTW: The += operator is called "augmented assignment" and generally is intended to do inplace modifications as far as possible.The other answers would seem to pretty much have it covered, though it seems worth quoting and referring to the Augmented Assignments PEP 203:...There are two things involved here:+ operator calls the __add__ method on a list. It takes all the elements from its operands and makes a new list containing those elements maintaining their order.  += operator calls __iadd__ method on the list. It takes an iterable and appends all the elements of the iterable to the list in place. It does not create a new list object.  In class foo the statement  self.bar += [x] is not an assignment statement but actually translates to which modifies the list in place and acts like the list method extend.In class foo2, on the contrary, the assignment statement in the init methodcan be deconstructed as:

The instance has no attribute bar (there is a class attribute of the same name, though) so it accesses the class attribute bar and creates a new list by appending x to it. The statement translates to:Then it creates an instance attribute bar and assigns the newly created list to it. Note that bar on the rhs of the assignment is different from the bar on the lhs.For instances of class foo, bar is a class attribute and not instance attribute. Hence any change to the class attribute bar will be reflected for all instances. On the contrary, each instance of the class foo2 has its own instance attribute bar which is different from the class attribute of the same name bar.Hope this clears things.We see that when we attempt to modify an immutable object (integer in this case), Python simply gives us a different object instead. On the other hand, we are able to make changes to an mutable object (a list) and have it remain the same object throughout.ref : https://medium.com/@tyastropheus/tricky-python-i-memory-management-for-mutable-immutable-objects-21507d1e5b95Also refer below url to understand the shallowcopy and deepcopyhttps://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/

Why in Python does「0, 0 == (0, 0)」equal「(0, False)」?

Piotr Zakrzewski

[Why in Python does「0, 0 == (0, 0)」equal「(0, False)」?](https://stackoverflow.com/questions/44864156/why-in-python-does-0-0-0-0-equal-0-false)

In Python (I checked only with Python 3.6 but I believe it should hold for many of the previous versions as well):But:Why does the result differ between the two approaches? Does the equality operator handle tuples differently? 

2017-07-01 18:29:11Z

In Python (I checked only with Python 3.6 but I believe it should hold for many of the previous versions as well):But:Why does the result differ between the two approaches? Does the equality operator handle tuples differently? The first two expressions both parse as tuples: The expressions are split that way because of the relative precedence of the comma separator compared to the equality operator: Python sees a tuple containing two expressions, one of which happens to be an equality test, instead of an equality test between two tuples.But in your third example, a = 0, 0 cannot be a tuple. A tuple is a collection of values, and unlike an equality test, assignment has no value in Python. An assignment is not an expression, but a statement; it does not have a value that can be included into a tuple or any other surrounding expression. If you tried something like (a = 0), 0 in order to force interpretation as a tuple, you would get a syntax error. That leaves the assignment of a tuple to a variable – which could be made more explicit by writing it a = (0, 0) – as the only valid interpretation of a = 0, 0.What you see in all 3 instances is a consequence of the grammar specification of the language, and how tokens encountered in the source code are parsed to generate the parse tree.Taking a look at this low level code should help you understand what happens under the hood. We can take these python statements, convert them into byte code and then decompile them using the dis module:Case 1: (0, 0) == 0, 0(0, 0) is first compared to 0 first and evaluated to False. A tuple is then constructed with this result and last 0, so you get (False, 0).Case 2: 0, 0 == (0, 0)A tuple is constructed with 0 as the first element. For the second element, the same check is done as in the first case and evaluated to False, so you get (0, False).Case 3: (0, 0) == (0, 0) Here, as you see, you're just comparing those two (0, 0) tuples and returning True.Another way to explain the problem: You're probably familiar with dictionary literalsand array literalsand tuple literalsbut what you don't realize is that, unlike dictionary and array literals, the parentheses you usually see around a tuple literal are not part of the literal syntax.  The literal syntax for tuples is just a sequence of expressions separated by commas:(an "exprlist" in the language of the formal grammar for Python).  Now, what do you expect the array literalto evaluate to?  That probably looks a lot more like it should be the same aswhich of course evaluates to [0, False].  Similarly, with an explicitly parenthesized tuple literalit's not surprising to get (0, False).  But the parentheses are optional;is the same thing.  And that's why you get (0, False).If you're wondering why the parentheses around a tuple literal are optional, it is largely because it would be annoying to have to write destructuring assignments that way:Adding a couple of parentheses around the order in which actions are performed might help you understand the results better:The comma is used to to separate expressions (using parentheses we can force different behavior, of course). When viewing the snippets you listed, the comma , will separate it and define what expressions will get evaluated:The tuple (0, 0) can also be broken down in a similar way. The comma separates two expressions comprising of the literals 0.In the first one Python is making a tuple of two things:In the second one it's the other way around.look at this example:then result:then comparison just does to the first number(0 and r) in the example.

Where to find the win32api module for Python? [closed]

rectangletangle

[Where to find the win32api module for Python? [closed]](https://stackoverflow.com/questions/3580855/where-to-find-the-win32api-module-for-python)

I need to download it for Python 2.7, but can't seem to find it... 

2010-08-27 02:05:16Z

I need to download it for Python 2.7, but can't seem to find it... 'pywin32' is its canonical name.http://sourceforge.net/projects/pywin32/There is a a new option as well: get it via pip!  There is a package pypiwin32 with wheels available, so you can just install with: pip install pypiwin32!Edit: Per comment from @movermeyer, the main project now publishes wheels at pywin32, and so can be installed with pip install pywin32I've found that UC Irvine has a great collection of python modules, pywin32 (win32api) being one of many listed there.  I'm not sure how they do with keeping up with the latest versions of these modules but it hasn't let me down yet.UC Irvine Python Extension Repository - http://www.lfd.uci.edu/~gohlke/pythonlibspywin32 module - http://www.lfd.uci.edu/~gohlke/pythonlibs/#pywin32http://sourceforge.net/projects/pywin32/files/ - 3rd .exe down

Why is str.translate much faster in Python 3.5 compared to Python 3.4?

Bhargav Rao

[Why is str.translate much faster in Python 3.5 compared to Python 3.4?](https://stackoverflow.com/questions/34287893/why-is-str-translate-much-faster-in-python-3-5-compared-to-python-3-4)

I was trying to remove unwanted characters from a given string using text.translate() in Python 3.4.The minimal code is:It works as expected. However the same program when executed in Python 3.4 and Python 3.5 gives a large difference.The code to calculate timings isThe Python 3.4 program takes 1.3ms whereas the same program in Python 3.5 takes only 26.4μs. What has improved in Python 3.5 that makes it faster compared to Python 3.4?

2015-12-15 11:21:05Z

I was trying to remove unwanted characters from a given string using text.translate() in Python 3.4.The minimal code is:It works as expected. However the same program when executed in Python 3.4 and Python 3.5 gives a large difference.The code to calculate timings isThe Python 3.4 program takes 1.3ms whereas the same program in Python 3.5 takes only 26.4μs. What has improved in Python 3.5 that makes it faster compared to Python 3.4?TL;DR - ISSUE 21118The long StoryJosh Rosenberg found out that the str.translate() function is very slow compared to the bytes.translate, he raised an issue, stating that:The main reason for str.translate() to be very slow was that the lookup used to be in a Python dictionary.The usage of maketrans made this problem worse. The similar approach using bytes builds a C array of 256 items to fast table lookup. Hence the usage of higher level Python dict makes the str.translate() in Python 3.4 very slow.The first approach was to add a small patch, translate_writer, However the speed increase was not that pleasing. Soon another patch fast_translate was tested and it yielded very nice results of up to 55% speedup.The main change as can be seen from the file is that the Python dictionary lookup is changed into a C level lookup.The speeds now are almost the same as bytesA small note here is that the performance enhancement is only prominent in ASCII strings. As J.F.Sebastian mentions in a comment below, Before 3.5, translate used to work in the same way for both ASCII and non-ASCII cases. However from 3.5  ASCII case is much faster. Earlier ASCII vs non-ascii used to be almost same, however now we can see a great change in the performance. It can be an improvement from 71.6μs to 2.33μs as seen in this answer.The following code demonstrates this Tabulation of the results:

Shared-memory objects in multiprocessing

Vendetta

[Shared-memory objects in multiprocessing](https://stackoverflow.com/questions/10721915/shared-memory-objects-in-multiprocessing)

Suppose I have a large in memory numpy array, I have a function func that takes in this giant array as input (together with some other parameters). func with different parameters can be run in parallel. For example:If I use multiprocessing library, then that giant array will be copied for multiple times into different processes. Is there a way to let different processes share the same array? This array object is read-only and will never be modified. What's more complicated, if arr is not an array, but an arbitrary python object, is there a way to share it? [EDITED]I read the answer but I am still a bit confused. Since fork() is copy-on-write, we should not invoke any additional cost when spawning new processes in python multiprocessing library. But the following code suggests there is a huge overhead: output (and by the way, the cost increases as the size of the array increases, so I suspect there is still overhead related to memory copying): Why is there such huge overhead, if we didn't copy the array? And what part does the shared memory save me? 

2012-05-23 14:20:21Z

Suppose I have a large in memory numpy array, I have a function func that takes in this giant array as input (together with some other parameters). func with different parameters can be run in parallel. For example:If I use multiprocessing library, then that giant array will be copied for multiple times into different processes. Is there a way to let different processes share the same array? This array object is read-only and will never be modified. What's more complicated, if arr is not an array, but an arbitrary python object, is there a way to share it? [EDITED]I read the answer but I am still a bit confused. Since fork() is copy-on-write, we should not invoke any additional cost when spawning new processes in python multiprocessing library. But the following code suggests there is a huge overhead: output (and by the way, the cost increases as the size of the array increases, so I suspect there is still overhead related to memory copying): Why is there such huge overhead, if we didn't copy the array? And what part does the shared memory save me? If you use an operating system that uses copy-on-write fork() semantics (like any common unix), then as long as you never alter your data structure it will be available to all child processes without taking up additional memory.  You will not have to do anything special (except make absolutely sure you don't alter the object).The most efficient thing you can do for your problem would be to pack your array into an efficient array structure (using numpy or array), place that in shared memory, wrap it with multiprocessing.Array, and pass that to your functions. This answer shows how to do that.If you want a writeable shared object, then you will need to wrap it with some kind of synchronization or locking. multiprocessing provides two methods of doing this: one using shared memory (suitable for simple values, arrays, or ctypes) or a Manager proxy, where one process holds the memory and a manager arbitrates access to it from other processes (even over a network).The Manager approach can be used with arbitrary Python objects, but will be slower than the equivalent using shared memory because the objects need to be serialized/deserialized and sent between processes.There are a wealth of parallel processing libraries and approaches available in Python. multiprocessing is an excellent and well rounded library, but if you have special needs perhaps one of the other approaches may be better.I run into the same problem and wrote a little shared-memory utility class to work around it.I'm using multiprocessing.RawArray (lockfree), and also the access to the arrays is not synchronized at all (lockfree), be careful not to shoot your own feet.With the solution I get speedups by a factor of approx 3 on a quad-core i7.Here's the code:

Feel free to use and improve it, and please report back any bugs.This is the intended use case for Ray, which is a library for parallel and distributed Python. Under the hood, it serializes objects using the Apache Arrow data layout (which is a zero-copy format) and stores them in a shared-memory object store so they can be accessed by multiple processes without creating copies.The code would look like the following.If you don't call ray.put then the array will still be stored in shared memory, but that will be done once per invocation of func, which is not what you want.Note that this will work not only for arrays but also for objects that contain arrays, e.g., dictionaries mapping ints to arrays as below.You can compare the performance of serialization in Ray versus pickle by running the following in IPython.Serialization with Ray is only slightly faster than pickle, but deserialization is 1000x faster because of the use of shared memory (this number will of course depend on the object).See the Ray documentation. You can read more about fast serialization using Ray and Arrow. Note I'm one of the Ray developers.Like Robert Nishihara mentioned, Apache Arrow makes this easy, specifically with the Plasma in-memory object store, which is what Ray is built on.I made brain-plasma specifically for this reason - fast loading and reloading of big objects in a Flask app. It's a shared-memory object namespace for Apache Arrow-serializable objects, including pickle'd bytestrings generated by pickle.dumps(...). The key difference with Apache Ray and Plasma is that it keeps track of object IDs for you. Any processes or threads or programs that are running on locally can share the variables' values by calling the name from any Brain object.One simple way to make large objects (e.g. Pandas DataFrames or ndarrays) available to all processes during multiprocessing is to use the 'sharedmem' option with joblib. Here there is no serialization via pickling or other mechanisms so it's very fast. If lock contention causes performance issues, you should be able to get around it by making local copies of the object in your processes (still much faster than serializing & deserializing). The output below shows that all processes were able to access the shared pandas DataFrame (the 4 as the second value in the output tuples).  The descending order of the first value in the output tuples verifies that the results list contains the values returned in the order in which they were sent, not in which their execution was completed.  

Is it pythonic to import inside functions?

codeape

[Is it pythonic to import inside functions?](https://stackoverflow.com/questions/1024049/is-it-pythonic-to-import-inside-functions)

PEP 8 says:On occation, I violate PEP 8. Some times I import stuff inside functions. As a general rule, I do this if there is an import that is only used within a single function.Any opinions?EDIT (the reason I feel importing in functions can be a good idea):Main reason: It can make the code clearer.

2009-06-21 14:41:00Z

PEP 8 says:On occation, I violate PEP 8. Some times I import stuff inside functions. As a general rule, I do this if there is an import that is only used within a single function.Any opinions?EDIT (the reason I feel importing in functions can be a good idea):Main reason: It can make the code clearer.In the long run I think you'll appreciate having most of your imports at the top of the file, that way you can tell at a glance how complicated your module is by what it needs to import.If I'm adding new code to an existing file I'll usually do the import where it's needed and then if the code stays I'll make things more permanent by moving the import line to the top of the file.One other point, I prefer to get an ImportError exception before any code is run -- as a sanity check, so that's another reason to import at the top. I use pyChecker to check for unused modules.There are two occasions where I violate PEP 8 in this regard:Outside of these two cases, it's a good idea to put everything at the top.  It makes the dependencies clearer.Here are the four import use cases that we useHere's what we do to make the code clearer:One thing to bear in mind:  needless imports can cause performance problems.  So if this is a function that will be called frequently, you're better off just putting the import at the top.  Of course this is an optimization, so if there's a valid case to be made that importing inside a function is more clear than importing at the top of a file, that trumps performance in most cases.If you're doing IronPython, I'm told that it's better to import inside functions (since compiling code in IronPython can be slow).  Thus, you may be able to get a way with importing inside functions then.  But other than that, I'd argue that it's just not worth it to fight convention.Another point I'd like to make is that this may be a potential maintenence problem.  What happens if you add a function that uses a module that was previously used by only one function?  Are you going to remember to add the import to the top of the file?  Or are you going to scan each and every function for imports?FWIW, there are cases where it makes sense to import inside a function.  For example, if you want to set the language in cx_Oracle, you need to set an NLS_LANG environment variable before it is imported.  Thus, you may see code like this:I've broken this rule before for modules that are self-testing.  That is, they are normally just used for support,  but I define a main for them so that if you run them by themselves you can test their functionality.  In that case I sometimes import getopt and cmd just in main, because I want it to be clear to someone reading the code that these modules have nothing to do with the normal operation of the module and are only being included for testing.Coming from the question about loading the module twice - Why not both?An import at the top of the script will indicate the dependencies and another import in the function with make this function more atomic, while seemingly not causing any performance disadvantage, since a consecutive import is cheap.  As long as it's import and not from x import *, you should put them at the top. It adds just one name to the global namespace, and you stick to PEP 8. Plus, if you later need it somewhere else, you don't have to move anything around.It's no big deal, but since there's almost no difference I'd suggest doing what PEP 8 says.Have a look at the alternative approach that's used in sqlalchemy: dependency injection:Notice how the imported library is declared in a decorator, and passed as an argument to the function!This approach makes the code cleaner, and also works 4.5 times faster than an import statement!Benchmark: https://gist.github.com/kolypto/589e84fbcfb6312532658df2fabdb796In modules that are both 'normal' modules and can be executed (i.e. have a if __name__ == '__main__':-section), I usually import modules that are only used when executing the module inside the main section.Example:

Can iterators be reset in Python?

Alex Martelli

[Can iterators be reset in Python?](https://stackoverflow.com/questions/3266180/can-iterators-be-reset-in-python)

Can I reset an iterator / generator in Python?  I am using DictReader and would like to reset it (from the csv module) to the beginning of the file.

2010-07-16 15:00:47Z

Can I reset an iterator / generator in Python?  I am using DictReader and would like to reset it (from the csv module) to the beginning of the file.I see many answers suggesting itertools.tee, but that's ignoring one crucial warning in the docs for it:Basically, tee is designed for those situation where two (or more) clones of one iterator, while "getting out of sync" with each other, don't do so by much -- rather, they say in the same "vicinity" (a few items behind or ahead of each other).  Not suitable for the OP's problem of "redo from the start".L = list(DictReader(...)) on the other hand is perfectly suitable, as long as the list of dicts can fit comfortably in memory.  A new "iterator from the start" (very lightweight and low-overhead) can be made at any time with iter(L), and used in part or in whole without affecting new or existing ones; other access patterns are also easily available.As several answers rightly remarked, in the specific case of csv you can also .seek(0) the underlying file object (a rather special case).  I'm not sure that's documented and guaranteed, though it does currently work;  it would probably be worth considering only for truly huge csv files, in which the list I recommmend as the general approach would have too large a memory footprint.If you have a csv file named 'blah.csv' That looks likeyou know that you can open the file for reading, and create a DictReader withThen, you will be able to get the next line with reader.next(), which should outputusing it again will produceHowever, at this point if you use blah.seek(0), the next time you call reader.next() you will getagain.This seems to be the functionality you're looking for. I'm sure there are some tricks associated with this approach that I'm not aware of however. @Brian suggested simply creating another DictReader. This won't work if you're first reader is half way through reading the file, as your new reader will have unexpected keys and values from wherever you are in the file.No. Python's iterator protocol is very simple, and only provides one single method (.next() or __next__()), and no method to reset an iterator in general.The common pattern is to instead create a new iterator using the same procedure again.If you want to "save off" an iterator so that you can go back to its beginning, you may also fork the iterator by using itertools.teeYes, if you use numpy.nditer to build your iterator.  There's a bug in using .seek(0) as advocated by Alex Martelli and Wilduck above, namely that the next call to .next() will give you a dictionary of your header row in the form of {key1:key1, key2:key2, ...}.  The work around is to follow file.seek(0) with a call to reader.next() to get rid of the header row.So your code would look something like this:This is perhaps orthogonal to the original question, but one could wrap the iterator in a function that returns the iterator.To reset  the iterator just call the function again. 

This is of course trivial if the function when the said function takes no arguments.In the case that the function requires some arguments, use functools.partial to create a closure that can be passed instead of the original iterator.This seems to avoid the caching that tee (n copies) or list (1 copy) would need to doWhile there is no iterator reset, the "itertools" module from python 2.6 (and later) has some utilities that can help there.

One of then is the "tee" which can make multiple copies of an iterator, and cache the results of the one running ahead, so that these results are used on the copies. I will seve your purposes:For small files, you may consider using more_itertools.seekable - a third-party tool that offers resetting iterables.DemoOutputHere a DictReader is wrapped in a seekable object (1) and advanced (2).  The seek() method is used to reset/rewind the iterator to the 0th position (3).Note: memory consumption grows with iteration, so be wary applying this tool to  large files, as indicated in the docs.For DictReader:For DictWriter:list(generator()) returns all remaining values for a generator and effectively resets it if it is not looped.I've had the same issue before. After analyzing my code, I realized that attempting to reset the iterator inside of loops slightly increases the time complexity and it also makes the code a bit ugly.Open the file and save the rows to a variable in memory.Now you can loop through rows anywhere in your scope without dealing with an iterator.One possible option is to use itertools.cycle(), which will allow you to iterate indefinitely without any trick like .seek(0).Only if the underlying type provides a mechanism for doing so (e.g. fp.seek(0)).I'm arriving at this same issue - while I like the tee() solution, I don't know how big my files are going to be and the memory warnings about consuming one first before the other are putting me off adopting that method. Instead, I'm creating a pair of iterators using iter() statements, and using the first for my initial run-through, before switching to the second one for the final run. So, in the case of a dict-reader, if the reader is defined using:I can create a pair of iterators from this "specification" - using:I can then run my 1st-pass code against d1, safe in the knowledge that the second iterator d2 has been defined from the same root specification. I've not tested this exhaustively, but it appears to work with dummy data. 

Running python script inside ipython

Tyler Durden

[Running python script inside ipython](https://stackoverflow.com/questions/11744181/running-python-script-inside-ipython)

Is it possible to run a python script (not module) from inside ipython without indicating its path? I tried to set PYTHONPATH but it seems to work only for modules. 

I would like to executewithout being in the directory containing the file.

2012-07-31 15:55:55Z

Is it possible to run a python script (not module) from inside ipython without indicating its path? I tried to set PYTHONPATH but it seems to work only for modules. 

I would like to executewithout being in the directory containing the file.from within the directory of "my_script.py" you can simply do:That should do itIn python there is no difference between modules and scripts; You can execute both scripts and modules.  The file must be on the pythonpath AFAIK because python must be able to find the file in question.  If python is executed from a directory, then the directory is automatically added to the pythonpath.Refer to What is the best way to call a Python script from another Python script? for more information about modules vs scriptsThere is also a builtin function execfile(filename) that will do what you wantThe %run magic has a parameter file_finder that it uses to get the full path to the file to execute (see here); as you note, it just looks in the current directory, appending ".py" if necessary.There doesn't seem to be a way to specify which file finder to use from the %run magic, but there's nothing to stop you from defining your own magic command that calls into %run with an appropriate file finder.As a very nasty hack, you could override the default file_finder with your own:To be honest, at the rate the IPython API is changing that's as likely to continue to work as defining your own magic is.for Python 3.6.5 

Inserting a Python datetime.datetime object into MySQL

g33kz0r

[Inserting a Python datetime.datetime object into MySQL](https://stackoverflow.com/questions/1136437/inserting-a-python-datetime-datetime-object-into-mysql)

I have a date column in a MySQL table. I want to insert a datetime.datetime() object into this column. What should I be using in the execute statement?I have tried:I am getting an error as: "TypeError: not all arguments converted during string formatting"

What should I use instead of %s?

2009-07-16 09:29:48Z

I have a date column in a MySQL table. I want to insert a datetime.datetime() object into this column. What should I be using in the execute statement?I have tried:I am getting an error as: "TypeError: not all arguments converted during string formatting"

What should I use instead of %s?For a time field, use:I think strftime also applies to datetime.You are most likely getting the TypeError because you need quotes around the datecolumn value.Try:With regards to the format, I had success with the above command (which includes the milliseconds) and with:Hope this helps.Try using now.date() to get a Date object rather than a DateTime.If that doesn't work, then converting that to a string should work:What database are you connecting to? I know Oracle can be picky about date formats and likes ISO 8601 format.**Note: Oops, I just read you are on MySQL. Just format the date and try it as a separate direct SQL call to test.In Python, you can get an ISO date likeFor instance, Oracle likes dates likeDepending on your database, if it is Oracle you might need to TO_DATE it:The general usage of TO_DATE is:If using another database (I saw the cursor and thought Oracle; I could be wrong) then check their date format tools. For MySQL it is DATE_FORMAT() and SQL Server it is CONVERT.Also using a tool like SQLAlchemy will remove differences like these and make your life easy.Use Python method datetime.strftime(format), where format = '%Y-%m-%d %H:%M:%S'.If timezones are a concern, the MySQL timezone can be set for UTC as follows:And the timezone can be set in Python:If you're just using a python datetime.date (not a full datetime.datetime), just cast the date as a string. This is very simple and works for me (mysql, python 2.7, Ubuntu). The column published_date is a MySQL date field, the python variable publish_date is datetime.date.  when iserting into t-sqlthis fails:this works:easy way:

String formatting in Python 3

JoseBazBaz

[String formatting in Python 3](https://stackoverflow.com/questions/13945749/string-formatting-in-python-3)

I do this in Python 2:What is the Python 3 version of this?I tried searching for examples online but I kept getting Python 2 versions.

2012-12-19 04:53:32Z

I do this in Python 2:What is the Python 3 version of this?I tried searching for examples online but I kept getting Python 2 versions.Here are the docs about the "new" format syntax. An example would be:If both goals and penalties are integers (i.e. their default format is ok), it could be shortened to:And since the parameters are fields of self, there's also a way of doing it using a single argument twice (as @Burhan Khalid noted in the comments):Explaining:There are many others things you can do when selecting an argument (using named arguments instead of positional ones, accessing fields, etc) and many format options as well (padding the number, using thousands separators, showing sign or not, etc). Some other examples:Note: As others pointed out, the new format does not supersede the former, both are available both in Python 3 and the newer versions of Python 2 as well. Some may say it's a matter of preference, but IMHO the newer is much more expressive than the older, and should be used whenever writing new code (unless it's targeting older environments, of course).Python 3.6 now supports shorthand literal string interpolation with PEP 498. For your use case, the new syntax is simply:This is similar to the previous .format standard, but lets one easily do things like:That line works as-is in Python 3.I like this approachNote the appended d and s to the brackets respectively.output will be:

jsonify a SQLAlchemy result set in Flask [duplicate]

mal-wan

[jsonify a SQLAlchemy result set in Flask [duplicate]](https://stackoverflow.com/questions/7102754/jsonify-a-sqlalchemy-result-set-in-flask)

I'm trying to jsonify a SQLAlchemy result set in Flask/Python.The Flask mailing list suggested the following method http://librelist.com/browser//flask/2011/2/16/jsonify-sqlalchemy-pagination-collection-result/#04a0754b63387f87e59dda564bde426e :However I'm getting the following error back:What am I overlooking here? I have found this question: How to serialize SqlAlchemy result to JSON? which seems very similar however I didn't know whether Flask had some magic to make it easier as the mailing list post suggested.Edit: for clarification, this is what my model looks like

2011-08-18 05:24:09Z

I'm trying to jsonify a SQLAlchemy result set in Flask/Python.The Flask mailing list suggested the following method http://librelist.com/browser//flask/2011/2/16/jsonify-sqlalchemy-pagination-collection-result/#04a0754b63387f87e59dda564bde426e :However I'm getting the following error back:What am I overlooking here? I have found this question: How to serialize SqlAlchemy result to JSON? which seems very similar however I didn't know whether Flask had some magic to make it easier as the mailing list post suggested.Edit: for clarification, this is what my model looks likeIt seems that you actually haven't executed your query. Try following:[Edit]: Problem with jsonify is, that usually the objects cannot be jsonified automatically. Even Python's datetime fails ;)What I have done in the past, is adding an extra property (like serialize) to classes that need to be serialized.And now for views I can just do:Hope this helps ;)[Edit 2019]:

In case you have more complex objects or circular references, use a library like marshmallow).I had the same need, to serialize into json.  Take a look at this question.  It shows how to discover columns programmatically.  So, from that I created the code below.  It works for me, and I'll be using it in my web app.  Happy coding!Here's what's usually sufficient for me:I create a serialization mixin which I use with my models. The serialization function basically fetches whatever attributes the SQLAlchemy inspector exposes and puts it in a dict.All that's needed now is to extend the SQLAlchemy model with the Serializer mixin class.If there are fields you do not wish to expose, or that need special formatting, simply override the serialize() function in the model subclass.In your controllers, all you have to do is to call the serialize() function (or serialize_list(l) if the query results in a list) on the results:Here's my approach:

https://github.com/n0nSmoker/SQLAlchemy-serializerpip install SQLAlchemy-serializerYou can easily add mixin to your model and than just call

.to_dict() method on it's instanceYou also can write your own mixin on base of SerializerMixinFor a flat query (no joins) you can do thisand if you only want to return certain columns from the database you can do thisOk, I've been working on this for a few hours, and I've developed what I believe to be the most pythonic solution yet. The following code snippets are python3 but shouldn't be too horribly painful to backport if you need.The first thing we're gonna do is start with a mixin that makes your db models act kinda like dicts:Now we're going to define our model, inheriting the mixin:That's all it takes to be able to pass an instance of MyModel() to dict() and get a real live dict instance out of it, which gets us quite a long way towards making jsonify() understand it. Next, we need to extend JSONEncoder to get us the rest of the way:Bonus points: if your model contains computed fields (that is, you want your JSON output to contain fields that aren't actually stored in the database), that's easy too. Just define your computed fields as @propertys, and extend the keys() method like so:Now it's trivial to jsonify:If you are using flask-restful you can use marshal:You need to explicitly list what you are returning and what type it is, which I prefer anyway for an api. Serialization is easily taken care of (no need for jsonify), dates are also not a problem. Note that the content for the uri field is automatically generated based on the topic endpoint and the id.I've been looking at this problem for the better part of a day, and here's what I've come up with (credit to https://stackoverflow.com/a/5249214/196358 for pointing me in this direction).(Note: I'm using flask-sqlalchemy, so my model declaration format is a bit different from straight sqlalchemy).In my models.py file:and all my model objects look like this:In my views I call SWJsonify wherever I would have called Jsonify, like so:Seems to work pretty well. Even on relationships. I haven't gotten far with it, so YMMV, but  so far it feels pretty "right" to me.Suggestions welcome.Here is a way to add an as_dict() method on every class, as well as any other method you want to have on every single class.

Not sure if this is the desired way or not, but it works...I was looking for something like the rails approach used in ActiveRecord to_json and implemented something similar using this Mixin after being unsatisfied with other suggestions.  It handles nested models, and including or excluding attributes of the top level or nested models.Then, to get the BaseQuery serializable I extended BaseQueryFor the following modelsYou could do something likeHere's my answer if you're using the declarative base (with help from some of the answers already posted):Flask-Restful 0.3.6 the Request Parsing recommend marshmallowA simple marshmallow example is showing below.The core features containI was working with a sql query defaultdict of lists of RowProxy objects named jobDict

It took me a while to figure out what Type the objects were.This was a really simple quick way to resolve to some clean jsonEncoding just by typecasting the row to a list and by initially defining the dict with a value of list.I just want to add my method to do this.just define a custome json encoder to serilize your db models.then in your view function it works well though the parent have relationships It's been a lot of times and there are lots of valid answers, but the following code block seems to work:I'm aware that this is not a perfect solution, nor as elegant as the others, however for those who want o quick fix, they might try this.

Python - 'ascii' codec can't decode byte

thoslin

[Python - 'ascii' codec can't decode byte](https://stackoverflow.com/questions/9644099/python-ascii-codec-cant-decode-byte)

I'm really confused. I tried to encode but the error said can't decode.... I know how to avoid the error with "u" prefix on the string. I'm just wondering why the error is "can't decode" when encode was called.  What is Python doing under the hood?

2012-03-10 05:10:46Z

I'm really confused. I tried to encode but the error said can't decode.... I know how to avoid the error with "u" prefix on the string. I'm just wondering why the error is "can't decode" when encode was called.  What is Python doing under the hood?encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent ofBut the decode fails because the string isn't valid ascii. That's why you get a complaint about not being able to decode.Always encode from unicode to bytes.

In this direction, you get to choose the encoding.  The other way is to decode from bytes to unicode.

In this direction, you have to know what the encoding is.This point can't be stressed enough.  If you want to avoid playing unicode "whack-a-mole", it's important to understand what's happening at the data level.  Here it is explained another way:Now, on seeing .encode on a byte string, Python 2 first tries to implicitly convert it to text (a unicode object).  Similarly, on seeing .decode on a unicode string, Python 2 implicitly tries to convert it to bytes (a str object).  These implicit conversions are why you can get UnicodeDecodeError when you've called encode.  It's because encoding usually accepts a parameter of type unicode; when receiving a str parameter, there's an implicit decoding into an object of type unicode before re-encoding it with another encoding.  This conversion chooses a default 'ascii' decoder†, giving you the decoding error inside an encoder.In fact, in Python 3 the methods str.decode and bytes.encode don't even exist.  Their removal was a [controversial] attempt to avoid this common confusion.† ...or whatever coding sys.getdefaultencoding () mentions; usually this is 'ascii'You can try this OrYou can also try followingAdd following line at top of your .py file.If you're using Python < 3, you'll need to tell the interpreter that your string literal is Unicode by prefixing it with a u:Further reading: Unicode HOWTO.You use u"你好".encode ('utf8') to encode an unicode string.

But if you want to represent "你好", you should decode it. Just like:You will get what you want. Maybe you should learn more about encode & decode.In case you're dealing with Unicode, sometimes instead of encode ('utf-8'), you can also try to ignore the special characters, e.g.or as something.decode ('unicode_escape').encode ('ascii','ignore') as suggested here.Not particularly useful in this example, but can work better in other scenarios when it's not possible to convert some special characters.Alternatively you can consider replacing particular character using replace ().If you are starting the python interpreter from a shell on Linux or similar systems (BSD, not sure about Mac), you should also check the default encoding for the shell. Call locale charmap from the shell (not the python interpreter) and you should seeIf this is not the case, and you see something else, e.g. Python will (at least in some cases such as in mine) inherit the shell's encoding and will not be able to print (some? all?) unicode characters. Python's own default encoding that you see and control via sys.getdefaultencoding () and sys.setdefaultencoding () is in this case ignored.If you find that you have this problem, you can fix that by (Or alternatively choose whichever keymap you want instead of en_EN.) You can also edit /etc/locale.conf (or whichever file governs the locale definition in your system) to correct this.

Generate random numbers with a given (numerical) distribution

pafcu

[Generate random numbers with a given (numerical) distribution](https://stackoverflow.com/questions/4265988/generate-random-numbers-with-a-given-numerical-distribution)

I have a file with some probabilities for different values e.g.:I would like to generate random numbers using this distribution. Does an existing module that handles this exist? It's fairly simple to code on your own (build the cumulative density function, generate a random value [0,1] and pick the corresponding value) but it seems like this should be a common problem and probably someone has created a function/module for it.I need this because I want to generate a list of birthdays (which do not follow any distribution in the standard random module).

2010-11-24 10:56:51Z

I have a file with some probabilities for different values e.g.:I would like to generate random numbers using this distribution. Does an existing module that handles this exist? It's fairly simple to code on your own (build the cumulative density function, generate a random value [0,1] and pick the corresponding value) but it seems like this should be a common problem and probably someone has created a function/module for it.I need this because I want to generate a list of birthdays (which do not follow any distribution in the standard random module).scipy.stats.rv_discrete might be what you want.  You can supply your probabilities via the values parameter.  You can then use the rvs() method of the distribution object to generate random numbers.As pointed out by Eugene Pakhomov in the comments, you can also pass a p keyword parameter to numpy.random.choice(), e.g.If you are using Python 3.6 or above, you can use random.choices() from the standard library – see the answer by Mark Dickinson.Since Python 3.6, there's a solution for this in Python's standard library, namely random.choices.Example usage: let's set up a population and weights matching those in the OP's question:Now choices(population, weights) generates a single sample:The optional keyword-only argument k allows one to request more than one sample at once. This is valuable because there's some preparatory work that random.choices has to do every time it's called, prior to generating any samples; by generating many samples at once, we only have to do that preparatory work once. Here we generate a million samples, and use collections.Counter to check that the distribution we get roughly matches the weights we gave.An advantage to generating the list using CDF is that you can use binary search. While you need O(n) time and space for preprocessing, you can get k numbers in O(k log n). Since normal Python lists are inefficient, you can use array module.If you insist on constant space, you can do the following; O(n) time, O(1) space.(OK, I know you are asking for shrink-wrap, but maybe those home-grown solutions just weren't succinct enough for your liking. :-)I pseudo-confirmed that this works by eyeballing the output of this expression:Maybe it is kind of late. But you can use numpy.random.choice(), passing the p parameter:I wrote a solution for drawing random samples from a custom continuous distribution.I needed this for a similar use-case to yours (i.e. generating random dates with a given probability distribution).You just need the funtion random_custDist and the line samples=random_custDist(x0,x1,custDist=custDist,size=1000). The rest is decoration ^^.The performance of this solution is improvable for sure, but I prefer readability.Make a list of items, based on their weights:An optimization may be to normalize amounts by the greatest common divisor, to make the target list smaller.Also, this might be interesting.Another answer, probably faster :)Verification:based on other solutions, you generate accumulative distribution (as integer or float whatever you like), then you can use bisect to make it fast this is a simple example (I used integers here)the get_cdf function would convert it from 20, 60, 10, 10 into 20, 20+60, 20+60+10, 20+60+10+10now we pick a random number up to 20+60+10+10 using random.randint then we use bisect to get the actual value in a fast wayyou might want to have a look at NumPy Random sampling distributionsNone of these answers is particularly clear or simple.Here is a clear, simple method that is guaranteed to work.accumulate_normalize_probabilities takes a dictionary p that maps symbols to probabilities OR frequencies. It outputs usable list of tuples from which to do selection.Yields:Why it worksThe accumulation step turns each symbol into an interval between itself and the previous symbols probability or frequency (or 0 in the case of the first symbol). These intervals can be used to select from (and thus sample the provided distribution) by simply stepping through the list until the random number in interval 0.0 -> 1.0 (prepared earlier) is less or equal to the current symbol's interval end-point. The normalization releases us from the need to make sure everything sums to some value. After normalization the "vector" of probabilities sums to 1.0. The rest of the code for selection and generating a arbitrarily long sample from the distribution is below :Usage :Here is a more effective way of doing this:Just call the following function with your 'weights' array (assuming the indices as the corresponding items) and the no. of samples needed. This function can be easily modified to handle ordered pair. Returns indexes (or items) sampled/picked (with replacement) using their respective probabilities:A short note on the concept used in the while loop. 

We reduce the current item's weight from cumulative beta, which is a cumulative value constructed uniformly at random, and increment current index in order to find the item, the weight of which matches the value of beta.

Flask vs webapp2 for Google App Engine

Anton Moiseev

[Flask vs webapp2 for Google App Engine](https://stackoverflow.com/questions/6774371/flask-vs-webapp2-for-google-app-engine)

I'm starting new Google App Engine application and currently considering two frameworks: Flask and webapp2. I'm rather satisfied with built-in webapp framework that I've used for my previous App Engine application, so I think webapp2 will be even better and I won't have any problems with it.However, there are a lot of good reviews of Flask, I really like its approach and all the things that I've read so far in the documentation and I want to try it out. But I'm a bit concerned about limitations that I can face down the road with Flask.So, the question is - do you know any problems, performance issues, limitations (e.g. routing system, built-in authorization mechanism, etc.) that Flask could bring into Google App Engine application? By "problem" I mean something that I can't work around in several lines of code (or any reasonable amount of code and efforts) or something that is completely impossible.And as a follow-up question: are there any killer-features in Flask that you think can blow my mind and make me use it despite any problems that I can face?

2011-07-21 10:03:51Z

I'm starting new Google App Engine application and currently considering two frameworks: Flask and webapp2. I'm rather satisfied with built-in webapp framework that I've used for my previous App Engine application, so I think webapp2 will be even better and I won't have any problems with it.However, there are a lot of good reviews of Flask, I really like its approach and all the things that I've read so far in the documentation and I want to try it out. But I'm a bit concerned about limitations that I can face down the road with Flask.So, the question is - do you know any problems, performance issues, limitations (e.g. routing system, built-in authorization mechanism, etc.) that Flask could bring into Google App Engine application? By "problem" I mean something that I can't work around in several lines of code (or any reasonable amount of code and efforts) or something that is completely impossible.And as a follow-up question: are there any killer-features in Flask that you think can blow my mind and make me use it despite any problems that I can face?Disclaimer: I'm the author of tipfy and webapp2.A big advantage of sticking with webapp (or its natural evolution, webapp2) is that you don't have to create your own versions for existing SDK handlers for your framework of your choice.For example, deferred uses a webapp handler. To use it in a pure Flask view, using werkzeug.Request and werkzeug.Response, you'll need to implement deferred for it (like I did here for tipfy).The same happens for other handlers: blobstore (Werkzeug still doesn't support range requests, so you'll need to use WebOb even if you create your own handler -- see tipfy.appengine.blobstore), mail, XMPP and so on, or others that are included in the SDK in the future.And the same happens for libraries created with App Engine in mind, like ProtoRPC, which is based on webapp and would need a port or adapter to work with other frameworks, if you don't want to mix webapp and your-framework-of-choice handlers in the same app.So, even if you choose a different framework, you'll end a) using webapp in some special cases or b) having to create and maintain your versions for specific SDK handlers or features, if you'll use them.I much prefer Werkzeug over WebOb, but after over one year porting and maintaining versions of the SDK handlers that work natively with tipfy, I realized that this is a lost cause -- to support GAE for the long term, best is to stay close to webapp/WebOb. It makes support for SDK libraries a breeze, maintenance becomes a lot easier, it is more future-proof as new libraries and SDK features will work out of the box and there's the benefit of a large community working around the same App Engine tools.A specific webapp2 defense is summarized here. Add to those that webapp2 can be used outside of App Engine and is easy to be customized to look like popular micro-frameworks and you have a good set of compelling reasons to go for it. Also, webapp2 has a big chance to be included in a future SDK release (this is extra-official, don't quote me :-) which will push it forward and bring new developers and contributions.That said, I'm a big fan of Werkzeug and the Pocoo guys and borrowed a lot from Flask and others (web.py, Tornado), but -- and, you know, I'm biased -- the above webapp2 benefits should be taken into account.Your question is extremely broad, but there appears to be no big problems using Flask on Google App Engine.This mailing list thread links to several templates:http://flask.pocoo.org/mailinglist/archive/2011/3/27/google-app-engine/#4f95bab1627a24922c60ad1d0a0a8e44And here is a tutorial specific to the Flask / App Engine combination:http://www.franciscosouza.com/2010/08/flying-with-flask-on-google-app-engine/Also, see App Engine - Difficulty Accessing Twitter Data - Flask, Flask message flashing fails across redirects, and How do I manage third-party Python libraries with Google App Engine? (virtualenv? pip?) for issues people have had with Flask and Google App Engine.For me the decision for webapp2 was easy when I discovered that flask is not an object-oriented framework (from the beginning), while webapp2 is a pure object oriented framework. webapp2 uses Method Based Dispatching as standard for all RequestHandlers (as flask documentation calls it and implements it since V0.7 in MethodViews). While in flask MethodViews are an add-on it is a core design principle for webapp2. So your software design will look different using both frameworks. Both frameworks use nowadays jinja2 templates and are fairly feature identical. I prefer to add security checks to a base-class RequestHandler and inherit from it. This is also good for utility functions, etc. As you can see for example in link [3] you can override methods to prevent dispatching a request.If you are an OO-person, or if you need to design a REST-server, I would recommend webapp2 for you. If you prefer simple functions with decorators as handlers for multiple request-types, or you are uncomfortable with OO-inheritance then choose flask. I think both frameworks avoid the complexity and dependencies of much bigger frameworks like pyramid.I think google app engine officially supports flask framework. There is a sample code and tutorial here -> https://console.developers.google.com/start/appengine?_ga=1.36257892.596387946.1427891855I didn't try webapp2 and found that tipfy was a bit difficult to use since it required setup scripts and builds that configure your python installation to other than default. For these and other reasons I haven't made my largest project depend on a framework and I use the plain webapp instead, add the library called beaker to get session capability and django already has builtin translations for words common to many usecases so when building a localized application django was the right choice for my largest project. The 2 other frameworks I actually deployed with projects to a production environment were GAEframework.com and web2py and generally it seems that adding a framework which changes its template engine could lead to incompatibilities between old and new versions. So my experience is that I'm being reluctant to adding a framework to my projects unless they solve the more advanced use cases (file upload, multi auth, admin ui are 3 examples of more advanced use cases that no framework for gae at the moment handles well. 

Is there a Python caching library?

Stavros Korokithakis

[Is there a Python caching library?](https://stackoverflow.com/questions/1427255/is-there-a-python-caching-library)

I'm looking for a Python caching library but can't find anything so far. I need a simple dict-like interface where I can set keys and their expiration and get them back cached. Sort of something like:which will give me the item from the cache if it exists or call the function and store it if it doesn't or has expired. Does anyone know something like this?

2009-09-15 13:42:57Z

I'm looking for a Python caching library but can't find anything so far. I need a simple dict-like interface where I can set keys and their expiration and get them back cached. Sort of something like:which will give me the item from the cache if it exists or call the function and store it if it doesn't or has expired. Does anyone know something like this?Take a look at Beaker:From Python 3.2 you can use the decorator @lru_cache from the functools library.

It's a Last Recently Used cache, so there is no expiration time for the items in it, but as a fast hack it's very useful.You might also take a look at the Memoize decorator.  You could probably get it to do what you want without too much modification.Joblib https://joblib.readthedocs.io supports caching functions in the Memoize pattern. Mostly, the idea is to cache computationally expensive functions.You can also do fancy things like using the @memory.cache decorator on functions. The documentation is here: https://joblib.readthedocs.io/en/latest/generated/joblib.Memory.htmlNo one has mentioned shelve yet. https://docs.python.org/2/library/shelve.htmlIt isn't memcached, but looks much simpler and might fit your need.I think the python memcached API is the prevalent tool, but I haven't used it myself and am not sure whether it supports the features you need.Try redis, it is one of the cleanest and easiest solutions for applications to share data in a atomic way or if you have got some web server platform. Its very easy to setup, you will need a python redis client http://pypi.python.org/pypi/redisYou can use my simple solution to the problem. It is really straightforward, nothing fancy:It indeed lacks expiration funcionality, but you can easily extend it with specifying a particular rule in MemCache c-tor.Hope code is enough self-explanatory, but if not, just to mention, that cache is being passed a translation function as one of its c-tor params. It's used in turn to generate cached output regarding the input.Hope it helpsLook at gocept.cache on pypi, manage timeout.Look at bda.cache http://pypi.python.org/pypi/bda.cache - uses ZCA and is tested with zope and bfg.This project aims to provide "Caching for humans" 

(seems like it's fairly unknown though)Some info from the project page:pip install cachekeyring is the best python caching library. You can use

How to debug a Flask app

Kimmy

[How to debug a Flask app](https://stackoverflow.com/questions/17309889/how-to-debug-a-flask-app)

How are you meant to debug errors in Flask?  Print to the console?  Flash messages to the page?  Or is there a more powerful option available to figure out what's happening when something goes wrong?

2013-06-26 00:51:42Z

How are you meant to debug errors in Flask?  Print to the console?  Flash messages to the page?  Or is there a more powerful option available to figure out what's happening when something goes wrong?Running the app in development mode will show an interactive traceback and console in the browser when there is an error. To run in development mode, set the FLASK_ENV=development environment variable then use the flask run command (remember to point FLASK_APP to your app as well).For Linux, Mac, Linux Subsystem for Windows, Git Bash on Windows, etc.:For Windows CMD, use set instead of export:For PowerShell, use $env:Prior to Flask 1.0, this was controlled by the FLASK_DEBUG=1 environment variable instead.If you're using the app.run() method instead of the flask run command, pass debug=True to enable debug mode.Tracebacks are also printed to the terminal running the server, regardless of development mode.If you're using PyCharm, VS Code, etc., you can take advantage of its debugger to step through the code with breakpoints. The run configuration can point to a script calling app.run(debug=True, use_reloader=False), or point it at the venv/bin/flask script and use it as you would from the command line. You can leave the reloader disabled, but a reload will kill the debugging context and you will have to catch a breakpoint again.You can also use pdb, pudb, or another terminal debugger by calling set_trace in the view where you want to start debugging.Be sure not to use too-broad except blocks. Surrounding all your code with a catch-all try... except... will silence the error you want to debug. It's unnecessary in general, since Flask will already handle exceptions by showing the debugger or a 500 error and printing the traceback to the console.You can use app.run(debug=True) for the Werkzeug Debugger edit as mentioned below, and I should have known.From the 1.1.x documentation, you can enable debug mode by exporting an environment variable:One can also use the Flask Debug Toolbar extension to get more detailed information embedded in rendered pages.Start the application as follows:If you're using Visual Studio Code, replacewithIt appears when turning on the internal debugger disables the VS Code debugger.If you want to debug your flask app then just go to the folder where flask app is. Don't forget to activate your virtual environment and paste the lines in the console change "mainfilename" to flask main file.After you enable your debugger for flask app almost every error will be printed on the console or on the browser window.

If you want to figure out what's happening, you can use simple print statements or you can also use console.log() for javascript code.Quick tip - if you use a PyCharm, go to Edit Configurations => Configurations and enable FLASK_DEBUG checkbox, restart the Run. Install python-dotenv in your virtual environment.Create a .flaskenv in your project root. By project root, I mean the folder which has your app.py fileInside this file write the following:Now issue the following command:To activate debug mode in flask you simply type set FLASK_DEBUG=1 on your CMD for windows and export FLASK_DEBUG=1 on Linux termial then restart your app and you are good to go!!Use loggers and print statements in the Development Environment, you can go for sentry in case of production environments.If you are running it locally and want to be able to step through the code:python -m pdb script.py

Automatically import modules when entering the python or ipython interpreter

user545424

[Automatically import modules when entering the python or ipython interpreter](https://stackoverflow.com/questions/11124578/automatically-import-modules-when-entering-the-python-or-ipython-interpreter)

I find myself typing import numpy as np almost every single time I fire up the python interpreter. How do I set up the python or ipython interpreter so that numpy is automatically imported?

2012-06-20 17:04:51Z

I find myself typing import numpy as np almost every single time I fire up the python interpreter. How do I set up the python or ipython interpreter so that numpy is automatically imported?Use the environment variable PYTHONSTARTUP. From the official documentation:So, just create a python script with the import statement and point the environment variable to it. Having said that, remember that 'Explicit is always better than implicit', so don't rely on this behavior for production scripts.For Ipython, see this tutorial on how to make a ipython_config fileFor ipython, there are two ways to achieve this. Both involve ipython's configuration directory which is located in ~/.ipython.For simplicity, I'd use option 2. All you have to do is place a .py or .ipy file in the ~/.ipython/profile_default/startup directory and it will automatically be executed. So you could simple place import numpy as np in a simple file and you'll have np in the namespace of your ipython prompt.Option 2 will actually work with a custom profile, but using a custom profile will allow you to change the startup requirements and other configuration based on a particular case. However, if you'd always like np to be available to you then by all means put it in the startup directory.For more information on ipython configuration. The docs have a much more complete explanation.I use a ~/.startup.py file like this:Then define PYTHONSTARTUP=~/.startup.py, and Python will use it when starting a shell.The print statements are there so when I start the shell, I get a reminder that it's in effect, and what has been imported already.  The pp shortcut is really handy too...While creating a custom startup script like ravenac95 suggests is the best general answer for most cases, it won't work in circumstances where you want to use a from __future__ import X. If you sometimes work in Python 2.x but want to use modern division, there is only one way to do this. Once you create a profile, edit the profile_default (For Ubuntu this is located in ~/.ipython/profile_default) and add something like the following to the bottom:As a simpler alternative to the accepted answer, on linux:just define an alias, e.g. put alias pynp='python -i -c"import numpy as np"' in your ~/.bash_aliases file. You can then invoke python+numpy with pynp, and you can still use just python with python. Python scripts' behaviour is left untouched.You can create a normal python script as import_numpy.py or anything you likethen launch it with -i flag.python -i import_numpy.py Way like this will give you flexibility to choose only modules you want for different projects.As ravenac95 mentioned in his answer, you can either create a custom profile or modify the default profile. This answer is quick view of Linux commands needed to import numpy as np automatically.If you want to use a custom profile called numpy, run:Or if you want to modify the default profile to always import numpy:Check out the IPython config tutorial to read more in depth about configuring profiles. See .ipython/profile_default/startup/README to understand how the startup directory works.My default ipython invocation is--pylab has been a ipython option for some time.  It imports numpy and (parts of) matplotlib.  I've added the --Inter... option so it does not use the * import, since I prefer to use the explicit np.....This can be a shortcut, alias or script.

Extracting double-digit months and days from a Python date [duplicate]

codingknob

[Extracting double-digit months and days from a Python date [duplicate]](https://stackoverflow.com/questions/15509345/extracting-double-digit-months-and-days-from-a-python-date)

Is there a way to extract month and day using isoformats? Lets assume today's date is March 8, 2013.I want:I can do this by writing if statements and concatenating a leading 0 in case the day or month is a single digit but was wondering whether there was an automatic way of generating what I want.

2013-03-19 20:03:53Z

Is there a way to extract month and day using isoformats? Lets assume today's date is March 8, 2013.I want:I can do this by writing if statements and concatenating a leading 0 in case the day or month is a single digit but was wondering whether there was an automatic way of generating what I want.Look at the types of those properties:Both are integers. So there is no automatic way to do what you want. So in the narrow sense, the answer to your question is no. If you want leading zeroes, you'll have to format them one way or another. 

For that you have several options:you can use a string formatter to pad any integer with zeros.  It acts just like C's printf.Updated for py36:  Use f-strings!  For general ints you can use the d formatter and explicitly tell it to pad with zeros:But datetimes are special and come with special formatters that are already zero padded:

How do you create different variable names while in a loop?

Takkun

[How do you create different variable names while in a loop?](https://stackoverflow.com/questions/6181935/how-do-you-create-different-variable-names-while-in-a-loop)

For example purposes...So I end up with string1, string2, string3... all equaling "Hello"

2011-05-31 00:53:48Z

For example purposes...So I end up with string1, string2, string3... all equaling "Hello"Sure you can; its called a dictionary:I said this somewhat tongue in check, but really the best way to associate one value with another value is a dictionary. That is what it was designed for!    It is really bad idea, but...and then for example:will give you:However this is bad practice. You should use dictionaries or lists instead, as others propose. Unless, of course, you really wanted to know how to do it, but did not want to use it.It's simply pointless to create variable variable names. Why? Using a list is much easier:One way you can do this is with exec(). For example:Here I am taking advantage of the handy f string formatting in python 3.6+Don't do this use a dictionarydon't do this use a dictglobals() has risk as it gives you what the namespace is currently pointing to but this can change and so modifying the return from  globals() is not a good ideafor x in range(9):

    exec("string"+str(x)+" = 'hello'")This should work.I would use a list:This way, you would have 9 "Hello" and you could get them individually like this:Where x would identify which "Hello" you want.So, print(string[1]) would print Hello.I think the challenge here is not to call upon global()I would personally define a list for your (dynamic) variables to be held and then append to it within a for loop.

Then use a separate for loop to view each entry or even execute other operations.Here is an example - I have a number of network switches (say between 2 and 8) at various BRanches. Now I need to ensure I have a way to determining how many switches are available (or alive - ping test) at any given branch and then perform some operations on them.Here is my code:Output is:BR123SW01BR123SW02BR123SW03BR123SW04BR123SW05BR123SW06BR123SW07

Sending mail from Python using SMTP

Eli Bendersky

[Sending mail from Python using SMTP](https://stackoverflow.com/questions/64505/sending-mail-from-python-using-smtp)

I'm using the following method to send mail from Python using SMTP. Is it the right method to use or are there gotchas I'm missing ?

2008-09-15 16:36:35Z

I'm using the following method to send mail from Python using SMTP. Is it the right method to use or are there gotchas I'm missing ?The script I use is quite similar; I post it here as an example of how to use the email.* modules to generate MIME messages; so this script can be easily modified to attach pictures, etc.I rely on my ISP to add the date time header.My ISP requires me to use a secure smtp connection to send mail, I rely on the smtplib module (downloadable at http://www1.cs.columbia.edu/~db2501/ssmtplib.py)As in your script, the username and password, (given dummy values below), used to authenticate on the SMTP server, are in plain text in the source. This is a security weakness; but the best alternative depends on how careful you need (want?) to be about protecting these.=======================================The method I commonly use...not much different but a little bitThat's itAlso if you want to do smtp auth with TLS as opposed to SSL then you just have to change the port (use 587) and do smtp.starttls().  This worked for me:The main gotcha I see is that you're not handling any errors: .login() and .sendmail() both have documented exceptions that they can throw, and it seems like .connect() must have some way to indicate that it was unable to connect - probably an exception thrown by the underlying socket code.Make sure you don't have any firewalls blocking SMTP.  The first time I tried to send an email, it was blocked both by Windows Firewall and McAfee - took forever to find them both.What about this? following code is working fine for me:Ref: http://www.mkyong.com/python/how-do-send-email-in-python-via-smtplib/You should make sure you format the date in the correct format - RFC2822.The example code which i did for send mail using SMTP.See all those lenghty answers? Please allow me to self promote by doing it all in a couple of lines.Import and Connect:Then it is just a one-liner:It will actually close when it goes out of scope (or can be closed manually). Furthermore, it will allow you to register your username in your keyring such that you do not have to write out your password in your script (it really bothered me prior to writing yagmail!)For the package/installation, tips and tricks please look at git or pip, available for both Python 2 and 3.you can do like thatHere's a working example for Python 3.xBased on this example I made following function:if you pass only body then plain text mail will be sent, but if you pass html argument along with body argument, html email will be sent (with fallback to text content for email clients that don't support html/mime types).Example usage:Btw. If you want to use gmail as testing or production SMTP server, 

enable temp or permanent access to less secured apps:

ImportError in importing from sklearn: cannot import name check_build

ayush singhal

[ImportError in importing from sklearn: cannot import name check_build](https://stackoverflow.com/questions/15274696/importerror-in-importing-from-sklearn-cannot-import-name-check-build)

I am getting the following error while trying to import from sklearn:I am using python 2.7, scipy-0.12.0b1 superpack, numpy-1.6.0 superpack, scikit-learn-0.11

I have a windows 7 machineI have checked several answers for this issue but none of them gives a way out of this error.

2013-03-07 15:12:12Z

I am getting the following error while trying to import from sklearn:I am using python 2.7, scipy-0.12.0b1 superpack, numpy-1.6.0 superpack, scikit-learn-0.11

I have a windows 7 machineI have checked several answers for this issue but none of them gives a way out of this error.Worked for me after installing scipy.So, simply try to restart the shell!Restart the python shell after installing scipy! You must haven't restarted the idle after installing yet!After installing numpy , scipy ,sklearn  still has errorSolution:Setting Up System Path Variable for Python & the PYTHONPATH Environment VariableSystem Variables: add C:\Python34 into path

User Variables: add new: (name)PYTHONPATH (value)C:\Python34\Lib\site-packages;My solution for Python 3.6.5 64-bit Windows 10:No need to restart command-line but you can do this if you want.

It took me one day to fix this bug. Hope this help.Usually when I get these kinds of errors, opening the __init__.py file and poking around helps. Go to the directory C:\Python27\lib\site-packages\sklearn and ensure that there's a sub-directory called __check_build as a first step. On my machine (with a working sklearn installation, Mac OSX, Python 2.7.3) I have __init__.py, setup.py, their associated .pyc files, and a binary _check_build.so.Poking around the __init__.py in that directory, the next step I'd take is to go to sklearn/__init__.py and comment out the import statement---the check_build stuff just checks that things were compiled correctly, it doesn't appear to do anything but call a precompiled binary. This is, of course, at your own risk, and (to be sure) a work around. If your build failed you'll likely soon run into other, bigger problems.This is probably because you may have scikit-learn installed along with sklearn. Run the following commandsThis solved the issue for me.I had the same issue on Windows. Solved it by installing Numpy+MKL from http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy (there it's recommended to install numpy+mkl before other packages that depend on it) as suggested by this answer.I had problems importing SKLEARN after installing a new 64bit version of Python 3.4 from python.org.Turns out that it was the SCIPY module that was broken, and alos failed when I tried to "import scipy".Solution was to uninstall scipy and reinstall it with pip3:If you use Anaconda 2.7 64 bit, tryand restart the python shell, that works for me.Second edit when I faced the same problem and solved it:also works for meNone of the other answers worked for me. After some tinkering I unsinstalled sklearn:Then I removed sklearn folder from here: (adjust the path to your system and python version)And the installed it from wheel from this site: linkThe error was there probably because of a version conflict with sklearn installed somewhere else.For me, 

I was upgrading the existing code into new setup by installing Anaconda from fresh with latest python version(3.7)

For this, tono need to uninstall & then re-install sklearntry this:I faced the same issue in my Windows machine and got it solved by installing numpy+mkl package from http://www.lfd.uci.edu/~gohlke/pythonlibs/. After installation, restart the shell.In windows:I tried to delete sklearn from the shell: pip uninstall sklearn, and re install it but doesn't work .. the solution:i had an issue when installed sklearn and try to import datasets 

the problem was cython compatibility.

after creating a new env without cython it worked like a charm.i had the same problem reinstalling anaconda solved the issue for memake sure your file in which you are coding is not named as "sklearn". I did the same mistake and now after renaming the file, it is working fineRecently I met the same mistike as you.Then I found that python is confused by the name "sklearn.py",so the code can not be executed correctly. I notice that there is also sklearn in your path So perhaps you can try to avoid "sklearn" in your path to avoid ambiguity. Hope it can help.

(I am sorry that perhaps I misunderstood the problem and it may not help.)

How to Execute a Python File in Notepad ++?

Reshure

[How to Execute a Python File in Notepad ++?](https://stackoverflow.com/questions/1702586/how-to-execute-a-python-file-in-notepad)

I prefer using Notepad ++ for developing,How do I execute the files in Python through Notepad++?

2009-11-09 17:41:54Z

I prefer using Notepad ++ for developing,How do I execute the files in Python through Notepad++?Open Notepad++. On the menu go to: Run -> Run.. (F5). Type in:Now, instead of pressing run, press save to create a shortcut for it.NotesUse a batch script that runs the Python script and then create a shortcut to that from Notepad++.As explained here: http://it-ride.blogspot.com/2009/08/notepad-and-python.htmlCheck if this key exists, and if does not, you could try creating it.@Ramiz Uddin's answer definitely deserves more visibility : Here is what's worked for me:Open notepad++ and press F5. You'll get a little popup box:First install Python from https://www.python.org/downloads/Run the installer** IMPORTANT ** 

Be sure you check both :Click install now and finish the installation.Open notepad++ and install plugin PyNPP from Plugin Manager. I'm using N++ 6.9.2Save a new file as new.pyType in N++Press Alt+Shift+F5Simple as that.On the menu go to: "Run" --> "Run..." (or just press F5).To understand the py command better:Another helpful link to understand the py command: How do I run python 2 and 3 in windows 7?Thanks to Reshure for his answer that got me on the right track to figure this out. All the answers for the Run->Run menu option go with the "/K" switch of cmd, so the terminal stays open, or "-i" for python.exe so python forces interactive mode - both to preserve the output for you to observe.Yet in cmd /k you have to type exit to close it, in the python -i - quit(). If that is too much typing for your liking (for me it sure is :), the Run command to use isC:\Python27\python.exe - obviously the full path to your python install (or just python if you want to go with the first executable in your user's path).& is unconditional execution of the next command in Windows - unconditional as it runs regardless of the RC of the previous command (&& is "and" - run only if the previous completed successfully, || - is "or").pause - prints "Press any key to continue . . ." and waits for any key (that output can be suppressed if need).exit - well, types the exit for you :)So at the end, cmd runs python.exe which executes the current file and keeps the window opened, pause waits for you to press any key, and exit finally close the window once you press that any key.I also wanted to run python files directly from Notepad++.

Most common option found online is using builtin option Run. Then you have two options:Problem with running your python files via builtin Run option is that

each time you run your python file, you open new console or IDLE window and lose all output from previous executions. This might not be important to some, but when I started to program in python, I used Python IDLE, so I got used to running python file multiple times in same IDLE Shell window. Also problem with running python programs from Notepad++ is that you need to manually save your file and then click Run (or press F5). To solve these problems (AFAIK*) you need to use Notepad++ Plugins. The best plugin for running python files from Notepad++ is 

NppExec. (I also tried PyNPP and Python Script. PyNPP runs python files in console, it works, but you can do that without plugin via builtin Run option and Python Script is used for running scripts that interact with Notepad++ so you can't run your python files.) To run your python file with NppExec plugin you need to go to Plugins -> NppExec -> Execute and then type in something like this (links:  ):With NppExec you can also save your python file before run with npp_save command, set working directory with cd "$(CURRENT_DIRECTORY)" command or run python program in interactive mode with -i command. I found many links (    ) online that mention these options, but best use of NppExec to run python programs I found at NppExec's Manual which has chapter 4.6.4. Running Python & wxPython with this code:All you need to do is copy this code and change your python directory if you use some other python version (e.g.* I am using python 3.4 so my directory is C:\Python34). This code works perfectly, but there is one line I added to this code so I can run python program multiple times without loosing previous output:a+ is to enable the "append" mode which keeps the previous Console's text and does not clear it.m- turns off console's internal messages (those are in green color)The final code that I use in NppExec's Execute window is:You can save your NppExec's code, and assign a shortcut key to this NppExec's script. (You need to open Advanced options of NppExec's plugin, select your script in the Associated script drop-down list, press the Add/Modify, restart Notepad++ , go to Notepad++'es Settings -> Shortcut Mapper -> Plugin commands, select your script, click Modify and assign a shortcut key. I wanted to put F5 as my shortcut key, to do that you need to change shortcut key for builtin option Run to something else first.) Links to chapters from NppExec's Manual that explain how to save you NppExec's code and assign a shortcut key: NppExec's "Execute...", NppExec's script.P.S.*: With NppExec plugin you can add Highlight Filters (found in Console Output Filters...) that highlight certain lines. I use it to highlight error lines in red, to do that you need to add Highlight masks: *File "%FILE%", line %LINE%, in <*> and Traceback (most recent call last): like this.None of the previously proposed solutions worked for me. Slight modification needed.After hitting F5 in Notepad++, type:The command prompt stays open so you can see the output of your script.I use the NPP_Exec plugin (Found in the plugins manager). Once that is installed, open the console window (ctrl+~) and type:This will launch command prompt. Then type:to execute the current file you are working with.I wish people here would post steps instead of just overall concepts. I eventually got the cmd /k version to work.The step-by-step instructions are:No answer here, or plugin i found provided what i wanted. A minimalist method to launch my python code i wrote on Notepad++ with the press of a shortcut, with preferably no plugins.I have Python 3.6 (64-bit), for Windows 8.1 x86_64 and Notepad++ 32bit. After you write your Python script in Notepad++ and save it, Hit F5 for Run. Then write: and hit the Run button. The i flag forces the terminal to stay still after code execution has terminated, for you to inspect it. This command will launch the script in a cmd terminal and the terminal will still lie there, until you close it by typing exit(). You can save this to a shortcut for convenience (mine is CTRL + SHIFT + P). There is one issue that I didn't see resolved in the above solutions. Python sets the current working directory to wherever you start the interpreter from. If you need the current working directory to be the same directory as where you saved the file on, then you could hit F5 and type this:Except you would replace C:\Users\username\Python36-32\python.exe with whatever the path to the python interpreter is on your machine.Basically you're starting up command line, changing the directory to the directory containing the .py file you're trying to run, and then running it. You can string together as many command line commands as you like with the '&' symbol.Extending Reshure's answerta da!My problem was, as it was mentioned by copeland3300, that my script is running from notepad++ folder, so it was impossible to locate other project files, such as database file, modules etc. I solved the problem using standard notepad++ "Run" command (F5) and typing in:Python WAS in my PATH. Cmd window stayed open after script finished.I started using Notepad++ for Python very recently and I found this method very easy. Once you are ready to run the code,right-click on the tab of your code in Notepad++ window and select "Open Containing Folder in cmd". This will open the Command Prompt into the folder where the current program is stored. All you need to do now is to execute:This was done on Notepad++ (Build 10 Jan 2015).I can't add the screenshots, so here's a blog post with the screenshots - http://coder-decoder.blogspot.in/2015/03/using-notepad-in-windows-to-edit-and.htmlIn Notepad++, go to Run → Run..., select the path and idle.py file of your Python installation:add a space and this:and here you are!Video demostration:https://www.youtube.com/watch?v=sJipYE1JT38In case someone is interested in passing arguments to cmd.exe and running the python script in a Virtual Environment, these are the steps I used:On the Notepad++ -> Run -> Run , I enter the following:Here I cd into the directory in which the .py file exists, so that it enables accessing any other relevant files which are in the directory of the .py code.And on the .bat file I have:You can run your script via cmd and be in script-directory:I would like to avoid using full python directory path in the Notepad++ macro. I tried other solutions given in this page, they failed.The one working on my PC is:In Notepad++, press F5.Copy/paste this:Enter.

Python: Bind an Unbound Method?

Dan Passaro

[Python: Bind an Unbound Method?](https://stackoverflow.com/questions/1015307/python-bind-an-unbound-method)

In Python, is there a way to bind an unbound method without calling it?I am writing a wxPython program, and for a certain class I decided it'd be nice to group the data of all of my buttons together as a class-level list of tuples, like so:The problem is, since all of the values of handler are unbound methods, my program explodes in a spectacular blaze and I weep.I was looking around online for a solution to what seems like should be a relatively straightforward, solvable problem. Unfortunately I couldn't find anything. Right now, I'm using functools.partial to work around this, but does anyone know if there's a clean-feeling, healthy, Pythonic way to bind an unbound method to an instance and continue passing it around without calling it?

2009-06-18 21:33:18Z

In Python, is there a way to bind an unbound method without calling it?I am writing a wxPython program, and for a certain class I decided it'd be nice to group the data of all of my buttons together as a class-level list of tuples, like so:The problem is, since all of the values of handler are unbound methods, my program explodes in a spectacular blaze and I weep.I was looking around online for a solution to what seems like should be a relatively straightforward, solvable problem. Unfortunately I couldn't find anything. Right now, I'm using functools.partial to work around this, but does anyone know if there's a clean-feeling, healthy, Pythonic way to bind an unbound method to an instance and continue passing it around without calling it?All functions are also descriptors, so you can bind them by calling their __get__ method:Here's R. Hettinger's excellent guide to descriptors.As a self-contained example pulled from Keith's comment:This can be done cleanly with types.MethodType. Example:Creating a closure with self in it will not technically bind the function, but it is an alternative way of solving the same (or very similar) underlying problem. Here's a trivial example:This will bind self to handler:This works by passing self as the first argument to the function. object.function() is just syntactic sugar for function(object). Late to the party, but I came here with a similar question: I have a class method and an instance, and want to apply the instance to the method.  At the risk of oversimplifying the OP's question, I ended up doing something less mysterious that may be useful to others who arrive here (caveat: I'm working in Python 3 -- YMMV).  Consider this simple class:Here's what you can do with it:

Have the same README both in Markdown and reStructuredText

jlengrand

[Have the same README both in Markdown and reStructuredText](https://stackoverflow.com/questions/10718767/have-the-same-readme-both-in-markdown-and-restructuredtext)

I have a project hosted on GitHub. For this I have written my README using the Markdown syntax in order to have it nicely formatted on GitHub.As my project is in Python I also plan to upload it to PyPi. The syntax used for READMEs on PyPi is reStructuredText. I would like to avoid having to handle two READMEs containing roughly the same content; so I searched for a markdown to RST (or the other way around) translator, but couldn't find any. The other solution I see is to perform a markdown/HTML and then a HTML/RST translation. I found some ressources for this here and here so I guess it should be possible.Would you have any idea that could fit better with what I want to do?

2012-05-23 11:12:53Z

I have a project hosted on GitHub. For this I have written my README using the Markdown syntax in order to have it nicely formatted on GitHub.As my project is in Python I also plan to upload it to PyPi. The syntax used for READMEs on PyPi is reStructuredText. I would like to avoid having to handle two READMEs containing roughly the same content; so I searched for a markdown to RST (or the other way around) translator, but couldn't find any. The other solution I see is to perform a markdown/HTML and then a HTML/RST translation. I found some ressources for this here and here so I guess it should be possible.Would you have any idea that could fit better with what I want to do?I would recommend Pandoc, the "swiss-army knife for converting files from one markup format into another" (check out the diagram of supported conversions at the bottom of the page, it is quite impressive). Pandoc allows markdown to reStructuredText translation directly. There is also an online editor here which lets you try it out, so you could simply use the online editor to convert your README files.As @Chris suggested, you can use Pandoc to convert Markdown to RST. This can be simply automated using pypandoc module and some magic in setup.py: This will automatically convert README.md to RST for the long description using on PyPi. When pypandoc is not available, then it just reads README.md without the conversion – to not force others to install pypandoc when they wanna just build the module, not upload to PyPi.So you can write in Markdown as usual and don’t care about RST mess anymore. ;)The PyPI Warehouse now supports rendering Markdown as well! You just need to update your package configuration and add the long_description_content_type='text/markdown' to it. e.g.:Therefore, there is no need to keep the README in two formats any longer.You can find more information about it in the documentation.The Markup library used by GitHub supports reStructuredText. This means you can write a README.rst file.They even support syntax specific color highlighting using the code and code-block directives (Example) PyPI now supports Markdown for long descriptions!In setup.py, set long_description to a Markdown string, add long_description_content_type="text/markdown" and make sure you're using recent tooling (setuptools 38.6.0+, twine 1.11+).See Dustin Ingram's blog post for more details.For my requirements I didn't want to install Pandoc in my computer. I used docverter. Docverter is a document conversion server with an HTTP interface using Pandoc for this.You might also be interested in the fact that it is possible to write in a common subset so that your document comes out the same way when rendered as markdown or rendered as reStructuredText: https://gist.github.com/dupuy/1855764 ☺I ran into this problem and solved it with the two following bash scripts.Note that I have LaTeX bundled into my Markdown.Its also useful to convert to html. md2html:I hope that helpsUsing the pandoc tool suggested by others I created a md2rst utility to create the rst files. Even though this solution means you have both an md and an rst it seemed to be the least invasive and would allow for whatever future markdown support is added. I prefer it over altering setup.py and maybe you would as well:

Pointers in Python?

mpen

[Pointers in Python?](https://stackoverflow.com/questions/3106689/pointers-in-python)

I know Python doesn't have pointers, but is there a way to have this yield 2 instead?Here's an example: I want form.data['field'] and form.field.value to always have the same value. It's not completely necessary, but I think it would be nice.In PHP, for example, I can do this:Output:ideoneOr like this in C++ (I think this is right, but my C++ is rusty):

2010-06-24 01:28:56Z

I know Python doesn't have pointers, but is there a way to have this yield 2 instead?Here's an example: I want form.data['field'] and form.field.value to always have the same value. It's not completely necessary, but I think it would be nice.In PHP, for example, I can do this:Output:ideoneOr like this in C++ (I think this is right, but my C++ is rusty):This is feasible, because it involves decorated names and indexing -- i.e., completely different constructs from the barenames a and b that you're asking about, and for with your request is utterly impossible.  Why ask for something impossible and totally different from the (possible) thing you actually want?!Maybe you don't realize how drastically different barenames and decorated names are.  When you refer to a barename a, you're getting exactly the object a was last bound to in this scope (or an exception if it wasn't bound in this scope) -- this is such a deep and fundamental aspect of Python that it can't possibly be subverted.  When you refer to a decorated name x.y, you're asking an object (the object x refers to) to please supply "the y attribute" -- and in response to that request, the object can perform totally arbitrary computations (and indexing is quite similar: it also allows arbitrary computations to be performed in response).Now, your "actual desiderata" example is mysterious because in each case two levels of indexing or attribute-getting are involved, so the subtlety you crave could be introduced in many ways.  What other attributes is form.field suppose to have, for example, besides value?  Without that further .value computations, possibilities would include:andThe presence of .value suggests picking the first form, plus a kind-of-useless wrapper:If assignments such form.field.value = 23 is also supposed to set the entry in form.data, then the wrapper must become more complex indeed, and not all that useless:The latter example is roughly as close as it gets, in Python, to the sense of "a pointer" as you seem to want -- but it's crucial to understand that such subtleties can ever only work with indexing and/or decorated names, never with barenames as you originally asked!There's no way you can do that changing only that line.  You can do: That creates a list, assigns the reference to a, then b also, uses the a reference to set the first element to 2, then accesses using the b reference variable.It's not a bug, it's a feature :-)When you look at the '=' operator in Python, don't think in terms of assignment. You don't assign things, you bind them. = is a binding operator.So in your code, you are giving the value 1 a name: a. Then, you are giving the value in 'a' a name: b. Then you are binding the value 2 to the name 'a'. The value bound to b doesn't change in this operation.Coming from C-like languages, this can be confusing, but once you become accustomed to it, you find that it helps you to read and reason about your code more clearly: the value which has the name 'b' will not change unless you explicitly change it. And if you do an 'import this', you'll find that the Zen of Python states that Explicit is better than implicit.Note as well that functional languages such as Haskell also use this paradigm, with great value in terms of robustness.Yes! there is a way to use a variable as a pointer in python!I am sorry to say that many of answers were partially wrong. In principle every equal(=) assignation shares the memory address (check the id(obj) function), but in practice it is not such. There are variables whose equal("=") behaviour works in last term as a copy of memory space, mostly in simple objects (e.g. "int" object), and others in which not (e.g. "list","dict" objects).Here is an example of pointer assignationHere is an example of copy assignationPointer assignation is a pretty useful tool for aliasing without the waste of extra memory, in certain situations for performing comfy code,but one have to be aware of this use in order to prevent code mistakes.To conclude, by default some variables are barenames (simple objects like int, float, str,...), and some are pointers when assigned between them (e.g. dict1 = dict2). How to recognize them? just try this experiment with them. In IDEs with variable explorer panel usually appears to be the memory address ("@axbbbbbb...") in the definition of pointer-mechanism objects.I suggest investigate in the topic. There are many people who know much more about this topic for sure. (see "ctypes" module). I hope it is helpful. Enjoy the good use of the objects! Regards, José CrespoFrom one point of view, everything is a pointer in Python.  Your example works a lot like the C++ code.(A closer equivalent would use some type of shared_ptr<Object> instead of int*.)You can do this by overloading __getitem__ in form.data's class.As you can see a and b are just names that reference to the same object 1. If later you write a = 2, you reassign the name a to a different object 2, but not the b which will continue referencing to 1:What will happen if you had a mutable object? Now, you are referencing to the same list object by the names a and b. You can mutate this list but it will stay the same object, and a and b will both continue referencing to itI wrote the following simple class as, effectively, a way to emulate a pointer in python:Here's an example of use (from a jupyter notebook page):Of course, it is also easy to make this work for dict items or attributes of an object. There is even a way to do what the OP asked for, using globals():This will print out 2, followed by 3.Messing with the global namespace in this way is kind of transparently a terrible idea, but it shows that it is possible (if inadvisable) to do what the OP asked for. The example is, of course, fairly pointless. But I have found this class to be useful in the application for which I developed it: a mathematical model whose behavior is governed by numerous user-settable mathematical parameters, of diverse types (which, because they depend on command line arguments, are not known at compile time). And once access to something has been encapsulated in a Parameter object, all such objects can be manipulated in a uniform way.Although it doesn't look much like a C or C++ pointer, this is solving a problem that I would have solved with pointers if I were writing in C++. The following code emulates exactly the behavior of pointers in C:Here are examples of use:But it's now time to give a more professional code, including the option of deleting pointers, that I've just found in my personal library:

Unittest setUp/tearDown for several tests

swan

[Unittest setUp/tearDown for several tests](https://stackoverflow.com/questions/8389639/unittest-setup-teardown-for-several-tests)

Is there a function that is fired at the beginning/end of a scenario of tests? The functions setUp and tearDown are fired before/after every single test.I typically would like to have this:For now, these setUp and tearDown are unit tests and spread in all my scenarios (containing many tests), one is the first test, the other is the last test.

2011-12-05 17:54:11Z

Is there a function that is fired at the beginning/end of a scenario of tests? The functions setUp and tearDown are fired before/after every single test.I typically would like to have this:For now, these setUp and tearDown are unit tests and spread in all my scenarios (containing many tests), one is the first test, the other is the last test.As of 2.7 (per the documentation) you get setUpClass and tearDownClass which execute before and after the tests in a given class are run, respectively.  Alternatively, if you have a group of them in one file, you can use setUpModule and tearDownModule (documentation).Otherwise your best bet is probably going to be to create your own derived TestSuite and override run().  All other calls would be handled by the parent, and run would call your setup and teardown code around a call up to the parent's run method.I have the same scenario, for me setUpClass and tearDownClass methods works perfectlyFor python 2.5, and when working with pydev, it's a bit hard. It appears that pydev doesn't use the test suite, but finds all individual test cases and runs them all separately.My solution for this was using a class variable like this:With this trick, when you inherit from this TestCase (instead of from the original unittest.TestCase), you'll also inherit the runCount of 0. Then in the run method, the runCount of the child testcase is checked and incremented. This leaves the runCount variable for this class at 0.This means the setUpClass will only be ran once per class and not once per instance. I don't have a tearDownClass method yet, but I guess something could be made with using that counter.

How to select Python version in PyCharm?

Cristian Lupascu

[How to select Python version in PyCharm?](https://stackoverflow.com/questions/10322424/how-to-select-python-version-in-pycharm)

I have PyCharm 1.5.4 and have used the "Open Directory" option to open the contents of a folder in the IDE.I have Python version 3.2 selected (it shows up under the "External Libraries" node).How can I select another version of Python (that I already have installed on my machine) so that PyCharm uses that version instead?

2012-04-25 19:27:22Z

I have PyCharm 1.5.4 and have used the "Open Directory" option to open the contents of a folder in the IDE.I have Python version 3.2 selected (it shows up under the "External Libraries" node).How can I select another version of Python (that I already have installed on my machine) so that PyCharm uses that version instead?File -> SettingsPreferences->Project Interpreter->Python InterpretersIf it's not listed add it.I think you are saying that you have python2 and python3 installed and have added a reference to each version under Pycharm > Settings > Project InterpreterWhat I think you are asking is how do you have some projects run with Python 2 and some projects running with Python 3.If so, you can look under Run > Edit Configurations There is a new feature called Interpreter in status bar (scroll down a little bit). This makes switching between python interpreters and seeing which version you’re using easier.In case you cannot see the status bar, you can easily activate it by running the Find Action command (Ctrl+Shift+A or ⌘+ ⇧+A on mac). Then type status bar and choose View: Status Bar to see it.This can also happen in Intellij Ultimate, which has PyCharm integrated. The issue is as diagnosed above, you have the wrong interpreter selected.The exact method to fix this for any given project is to go to Project Settings...Project and adjust the Project SDK. You can add a New Project SDK if you don't have Python 3 added by navigating to the python3 binary. This will fix the errors listed above. A shortcut to Project Settings is the blue checkerboard-type icon.You can also add Python 3 as the default interpreter for Python projects. On OSX this is in File..Other Settings...Default Project Structure. There you can set the Project SDK which will now apply on each new project. It can be different on other platforms, but still similar.Go to:Files -> Settings -> Project -> *"Your Project Name"* -> Project InterpreterThere you can see which external libraries you have installed for python2 and which for python3. Select the required python version according to your requirements.  [NOTE]:Tested on Pycharm 2018 and 2017.

SQLAlchemy: engine, connection and session difference

ololobus

[SQLAlchemy: engine, connection and session difference](https://stackoverflow.com/questions/34322471/sqlalchemy-engine-connection-and-session-difference)

I use SQLAlchemy and there are at least three entities: engine, session and connection, which have execute method, so if I e.g. want to select all records from table I can do thisand thisand even this- the results will be the same.As I understand it, if someone uses engine.execute it creates connection, opens session (Alchemy takes care of it for you) and executes the query. But is there a global difference between these three ways of performing such a 

task?

2015-12-16 21:29:31Z

I use SQLAlchemy and there are at least three entities: engine, session and connection, which have execute method, so if I e.g. want to select all records from table I can do thisand thisand even this- the results will be the same.As I understand it, if someone uses engine.execute it creates connection, opens session (Alchemy takes care of it for you) and executes the query. But is there a global difference between these three ways of performing such a 

task?A one-line overview: The behavior of execute() is same in all the cases, but they are 3 different methods, in Engine, Connection, and Session classes.What exactly is execute():To understand behavior of execute() we need to look into the Executable class. Executable is a superclass for all「statement」types of objects, including select(), delete(),update(), insert(), text() - in simplest words possible, an Executable is a SQL expression construct supported in SQLAlchemy.In all the cases the execute() method takes the SQL text or constructed SQL expression i.e. any of the variety of SQL expression constructs supported in SQLAlchemy and returns query results (a ResultProxy - Wraps a DB-API cursor object to provide easier access to row columns.) To clarify it further (only for conceptual clarification, not a recommended approach):In addition to Engine.execute() (connectionless execution), Connection.execute(), and Session.execute(), it is also possible to use the execute() directly on any Executable construct. The Executable class has it's own implementation of execute() - As per official documentation, one line description about what the execute() does is "Compile and execute this Executable". In this case we need to explicitly bind the Executable (SQL expression construct) with a Connection object or, Engine object (which implicitly get a Connection object), so the execute() will know where to execute the SQL.The following example demonstrates it well - Given a table as below:Explicit execution i.e. Connection.execute() - passing the SQL text or constructed SQL expression to the execute() method of Connection:Explicit connectionless execution i.e. Engine.execute() - passing the SQL text or constructed SQL expression directly to the execute() method of Engine:Implicit execution i.e. Executable.execute() - is also connectionless, and calls the execute() method of the Executable, that is, it calls execute() method directly on the SQL expression construct (an instance of Executable) itself.Note: Stated the implicit execution example for the purpose of clarification - this way of execution is highly not recommended - as per docs:You're right for the part "if someone use engine.execute it creates connection " but not for "opens session (Alchemy cares about it for you) and executes query " - Using Engine.execute() and Connection.execute() is (almost) one the same thing, in formal, Connection object gets created implicitly, and in later case we explicitly instantiate it. What really happens in this case is:At DB layer it's exactly the same thing, all of them are executing SQL (text expression or various SQL expression constructs). From application's point of view there are two options:Session.execute() ultimately uses Connection.execute() statement execution method in order to execute the SQL statement. Using Session object is SQLAlchemy ORM's recommended way for an application to interact with the database.An excerpt from the docs:Nabeel's answer covers a lot of details and is helpful, but I found it confusing to follow. Since this is currently the first Google result for this issue, adding my understanding of it for future people that find this question:As OP and Nabell Ahmed both note, when executing a plain SELECT * FROM tablename, there's no difference in the result provided. The differences between these three objects do become important depending on the context that the SELECT statement is used in or, more commonly, when you want to do other things like INSERT, DELETE, etc. Here is an example of running DCL (Data Control Language) such as GRANT

What is the most efficient way to create a dictionary of two pandas Dataframe columns?

user1083734

[What is the most efficient way to create a dictionary of two pandas Dataframe columns?](https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co)

What is the most efficient way to organise the following pandas Dataframe:data =into a dictionary like alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']?

2013-07-02 12:58:45Z

What is the most efficient way to organise the following pandas Dataframe:data =into a dictionary like alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']?Speed comparion (using Wouter's method)I found a faster way to solve the problem, at least on realistically large datasets using:

df.set_index(KEY).to_dict()[VALUE]Proof on 50,000 rows:Output:Explaining solution: dict(sorted(df.values.tolist()))Given:[out]:Try:[out]:Then optionally: Or:[out]:Lastly, cast the list of list of 2 elements into a dict. [out]:Answering @sbradbio comment:If there are multiple values for a specific key and you would like to keep all of them, it's the not the most efficient but the most intuitive way is:[out]:In Python 3.6 the fastest way is still the WouterOvermeire one. Kikohs' proposal is slower than the other two options.Results:

Tensorflow Strides Argument

jfbeltran

[Tensorflow Strides Argument](https://stackoverflow.com/questions/34642595/tensorflow-strides-argument)

I am trying to understand the strides argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. The documentation repeatedly says My questions are:Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.

2016-01-06 20:56:26Z

I am trying to understand the strides argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. The documentation repeatedly says My questions are:Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.The pooling and convolutional ops slide a "window" across the input tensor.  Using tf.nn.conv2d as an example: If the input tensor has 4 dimensions:  [batch, height, width, channels], then the convolution operates on a 2D window on the height, width dimensions.strides determines how much the window shifts by in each of the dimensions.  The typical use sets the first (the batch) and last (the depth) stride to 1.Let's use a very concrete example:  Running a 2-d convolution over a 32x32 greyscale input image.  I say greyscale because then the input image has depth=1, which helps keep it simple.  Let that image look like this:Let's run a 2x2 convolution window over a single example (batch size = 1).  We'll give the convolution an output channel depth of 8.The input to the convolution has shape=[1, 32, 32, 1].If you specify strides=[1,1,1,1] with padding=SAME, then the output of the filter will be [1, 32, 32, 8].The filter will first create an output for:And then for:and so on.  Then it will move to the second row, calculating:thenIf you specify a stride of [1, 2, 2, 1] it won't do overlapping windows.  It will compute:and thenThe stride operates similarly for the pooling operators.Question 2:  Why strides [1, x, y, 1] for convnetsThe first 1 is the batch:  You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)The last 1 is the depth of the convolution:  You don't usually want to skip inputs, for the same reason.The conv2d operator is more general, so you could create convolutions that slide the window along other dimensions, but that's not a typical use in convnets.  The typical use is to use them spatially.Why reshape to -1  -1 is a placeholder that says "adjust as necessary to match the size needed for the full tensor."  It's a way of making the code be independent of the input batch size, so that you can change your pipeline and not have to adjust the batch size everywhere in the code.The inputs are 4 dimensional and are of form: [batch_size, image_rows, image_cols, number_of_colors]Strides, in general, define an overlap between applying operations. In the case of conv2d, it specifies what is the distance between consecutive applications of convolutional filters. The value of 1 in a specific dimension means that we apply the operator at every row/col, the value of 2 means every second, and so on.Re 1) The values that matter for convolutions are 2nd and 3rd and they represent the overlap in the application of the convolutional filters along rows and columns. The value of [1, 2, 2, 1] says that we want to apply the filters on every second row and column.Re 2) I don't know the technical limitations (might be CuDNN requirement) but typically people use strides along the rows or columns dimensions. It doesn't necessarily make sense to do it over batch size. Not sure of the 

last dimension.Re 3) Setting -1 for one of the dimension means, "set the value for the first dimension so that the total number of elements in the tensor is unchanged". In our case, the -1 will be equal to the batch_size.Let's assume your input = [1, 0, 2, 3, 0, 1, 1] and kernel = [2, 1, 3] the result of the convolution is [8, 11, 7, 9, 4], which is calculated by sliding your kernel over the input, performing element-wise multiplication and summing everything. Like this:Here we slide by one element, but nothing stops you by using any other number. This number is your stride. You can think about it as downsampling the result of the 1-strided convolution by just taking every s-th result.Knowing the input size i, kernel size k, stride s and padding p you can easily calculate the output size of the convolution as:Here || operator means ceiling operation. For a pooling layer s = 1.Knowing the math for a 1-dim case, n-dim case is easy once you see that each dim is independent. So you just slide each dimension separately. Here is an example for 2-d. Notice that you do not need to have the same stride at all the dimensions. So for an N-dim input/kernel you should provide N strides.@dga has done a wonderful job explaining and I can't be thankful enough how helpful it has been. In the like manner, I will like to share my findings on how stride works in 3D convolution.According to the TensorFlow documentation on conv3d, the shape of the input must be in this order: [batch, in_depth, in_height, in_width, in_channels]Let's explain the variables from the extreme right to the left using an example. Assuming the input shape is 

input_shape = [1000,16,112,112,3]Below is a summary documentation for how stride is used.As indicated in many works, strides simply mean how many steps away a window or kernel jumps away from the closest element, be it a data frame or pixel (this is paraphrased by the way).From the above documentation, a stride in 3D will look like this strides = (1,X,Y,Z,1). The documentation emphasizes that strides[0] = strides[4] = 1. strides[X] means how many skips we should make in the lumped frames. So for example, if we have 16 frames, X=1 means use every frame. X=2 means use every second frame and it goes and onstrides[y] and strides[z] follow the explanation by @dga so I will not redo that part.In keras however, you only need to specify a tuple/list of 3 integers, specifying the strides of the convolution along each spatial dimension, where spatial dimension is stride[x], strides[y] and strides[z]. strides[0] and strides[4] is already defaulted to 1.I hope someone finds this helpful!

How to convert a boolean array to an int array

Kwolf

[How to convert a boolean array to an int array](https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array)

I use Scilab, and  want to convert an array of booleans into an array of integers:In Scilab I can use:or even just multiply it by 1:Is there a simple command for this in Python, or would I have to use a loop?

2013-07-06 19:08:19Z

I use Scilab, and  want to convert an array of booleans into an array of integers:In Scilab I can use:or even just multiply it by 1:Is there a simple command for this in Python, or would I have to use a loop?Numpy arrays have an astype method.  Just do y.astype(int).Note that it might not even be necessary to do this, depending on what you're using the array for.  Bool will be autopromoted to int in many cases, so you can add it to int arrays without having to explicitly convert it:The 1*y method works in Numpy too:If you are asking for a way to convert Python lists from Boolean to int, you can use map to do it:Or using list comprehensions:Using numpy, you can do:If you were using a non-numpy array, you could use a list comprehension:Most of the time you don't need conversion:The right way to do it is:orI know you asked for non-looping solutions, but the only solutions I can come up with probably loop internally anyway:or:or:A funny way to do this is

How do I write a Python dictionary to a csv file? [duplicate]

CraigP

[How do I write a Python dictionary to a csv file? [duplicate]](https://stackoverflow.com/questions/10373247/how-do-i-write-a-python-dictionary-to-a-csv-file)

I have what I think should be a very easy task that I can't seem to solve.How do I write a Python dictionary to a csv file?  All I want is to write the dictionary keys to the top row of the file and the key values to the second line.The closest that I've come is the following (that I got from somebody else's post):The problem is, the above code seems to only be writing the keys to the first line and that's it.  I'm not getting the values written to the second line.Any ideas?

2012-04-29 15:12:21Z

I have what I think should be a very easy task that I can't seem to solve.How do I write a Python dictionary to a csv file?  All I want is to write the dictionary keys to the top row of the file and the key values to the second line.The closest that I've come is the following (that I got from somebody else's post):The problem is, the above code seems to only be writing the keys to the first line and that's it.  I'm not getting the values written to the second line.Any ideas?You are using DictWriter.writerows() which expects a list of dicts, not a dict. You want DictWriter.writerow() to write a single row.You will also want to use DictWriter.writeheader() if you want a header for you csv file.You also might want to check out the with statement for opening files. It's not only more pythonic and readable but handles closing for you, even when exceptions occur.Example with these changes made:Which produces:Your code was very close to working.  Try using a regular csv.writer rather than a DictWriter.  The latter is mainly used for writing a list of dictionaries.Here's some code that writes each key/value pair on a separate row:If instead you want all the keys on one row and all the values on the next, that is also easy:Pro tip:  When developing code like this, set the writer to w = csv.writer(sys.stderr) so you can more easily see what is being generated.  When the logic is perfected, switch back to w = csv.writer(f).Why use csv for this simple task?I use python in two ways:My answer is only useful for the 2nd purpose. And also considers the pain to remember all these modules and their individual functions :P

Haversine Formula in Python (Bearing and Distance between two GPS points)

avitex

[Haversine Formula in Python (Bearing and Distance between two GPS points)](https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points)

I would like to know how to get the distance and bearing between 2 GPS points.

I have researched on the haversine formula.

Someone told me that I could also find the bearing using the same data.Everything is working fine but the bearing doesn't quite work right yet. The bearing outputs negative but should be between 0 - 360 degrees.

The set data should make the horizontal bearing 96.02166666666666

and is:Here is my new code:

2011-02-06 12:41:45Z

I would like to know how to get the distance and bearing between 2 GPS points.

I have researched on the haversine formula.

Someone told me that I could also find the bearing using the same data.Everything is working fine but the bearing doesn't quite work right yet. The bearing outputs negative but should be between 0 - 360 degrees.

The set data should make the horizontal bearing 96.02166666666666

and is:Here is my new code:Here's a Python version:Most of these answers are "rounding" the radius of the earth.  If you check these against other distance calculators (such as geopy), these functions will be off.This works well:There is also a vectorized implementation, which allows to use 4 numpy arrays instead of scalar values for coordinates:The bearing calculation is incorrect, you need to swap the inputs to atan2.This will give you the correct bearing.Here's a numpy vectorized implementation of the Haversine Formula given by @Michael Dunn, gives a 10-50 times improvement over large vectors.You can try the following:You can solve the negative bearing problem by adding 360°. 

Unfortunately, this might result in bearings larger than 360° for positive bearings.

This is a good candidate for the modulo operator, so all in all you should add the line at the end of your method.The Y in atan2 is, by default, the first parameter. Here is the documentation. You will need to switch your inputs to get the correct bearing angle.Refer to this link :https://gis.stackexchange.com/questions/84885/whats-the-difference-between-vincenty-and-great-circle-distance-calculationsthis actually gives two ways of getting distance. They are Haversine and Vincentys. From my research I came to know that Vincentys is relatively accurate. Also use import statement to make the implementation.Here are two functions to calculate distance and bearing, which are based on the code in previous messages and https://gist.github.com/jeromer/2005586 (added tuple type for geographical points in lat, lon format for both functions for clarity). I tested both functions and they seem to work right.

Select between two dates with Django

Jeff Taggarty

[Select between two dates with Django](https://stackoverflow.com/questions/3963201/select-between-two-dates-with-django)

I am looking to make a query that selects between dates with Django.I know how to do this with raw SQL pretty easily, but how could this be achieved using the Django ORM?This is where I want to add the between dates of 30 days in my query:

2010-10-18 20:41:30Z

I am looking to make a query that selects between dates with Django.I know how to do this with raw SQL pretty easily, but how could this be achieved using the Django ORM?This is where I want to add the between dates of 30 days in my query:Use the __range operator:__rangetwo methodsanother methodIf you are using a DateTimeField, Filtering with dates won’t include items on the last day. You need to casts the value as date:

How to log source file name and line number in Python

digy

[How to log source file name and line number in Python](https://stackoverflow.com/questions/533048/how-to-log-source-file-name-and-line-number-in-python)

Is it possible to decorate/extend the python standard logging system, so that when a logging method is invoked it also logs the file and the line number where it was invoked or maybe the method that invoked it?

2009-02-10 16:29:23Z

Is it possible to decorate/extend the python standard logging system, so that when a logging method is invoked it also logs the file and the line number where it was invoked or maybe the method that invoked it?Sure, check formatters in logging docs. Specifically the lineno and pathname variables.Looks something like this:On top of Seb's very useful answer, here is a handy code snippet that demonstrates the logger usage with a reasonable format:Generates this output:To build on the above in a way that sends debug logging to standard out: Putting the above into a file called debug_logging_example.py produces the output:Then if you want to turn off logging comment out root.setLevel(logging.DEBUG).For single files (e.g. class assignments) I've found this a far better way of doing this as opposed to using print() statements. Where it allows you to turn the debug output off in a single place before you submit it. For devs using PyCharm or Eclipse pydev, the following will produce a link to the source of the log statement in the console log output:See Pydev source file hyperlinks in Eclipse console for longer discussion and history.Different of the other answers, this will log full path of file and the function name that might have occurred an error. This is useful if you have a project with more than one module and several files with the same name distributed in these modules.

How can I pass data from Flask to JavaScript in a template?

mea

[How can I pass data from Flask to JavaScript in a template?](https://stackoverflow.com/questions/11178426/how-can-i-pass-data-from-flask-to-javascript-in-a-template)

My app makes a call to an API that returns a  dictionary. I want to pass information from this dict to JavaScript in the view. I am using the Google Maps API in the JS, specifically, so I'd like to pass it a list of tuples with the long/lat information. I know that render_template will pass these variables to the view so they can be used in HTML, but how could I pass them to JavaScript in the template? 

2012-06-24 14:50:39Z

My app makes a call to an API that returns a  dictionary. I want to pass information from this dict to JavaScript in the view. I am using the Google Maps API in the JS, specifically, so I'd like to pass it a list of tuples with the long/lat information. I know that render_template will pass these variables to the view so they can be used in HTML, but how could I pass them to JavaScript in the template? You can use {{ variable }} anywhere in your template, not just in the HTML part. So this should work:Think of it as a two-stage process: First, Jinja (the template engine Flask uses) generates your text output. This gets sent to the user who executes the JavaScript he sees. If you want your Flask variable to be available in JavaScript as an array, you have to generate an array definition in your output:Jinja also offers more advanced constructs from Python, so you can shorten it to:You can also use for loops, if statements and many more, see the Jinja2 documentation for more. Also, have a look at Ford's answer who points out the tojson filter which is an addition to Jinja2's standard set of filters. Edit Nov 2018: tojson is now included in Jinja2's standard set of filters.The ideal way to go about getting pretty much any Python object into a JavaScript object is to use JSON. JSON is great as a format for transfer between systems, but sometimes we forget that it stands for JavaScript Object Notation. This means that injecting JSON into the template is the same as injecting JavaScript code that describes the object.Flask provides a Jinja filter for this: tojson dumps the structure to a JSON string and marks it safe so that Jinja does not autoescape it.This works for any Python structure that is JSON serializable:Using a data attribute on an HTML element avoids having to use inline scripting, which in turn means you can use stricter CSP rules for increased security.Specify a data attribute like so:Then access it in a static JavaScript file like so:Alternatively you could add an endpoint to return your variable:Then do  an XHR to retrieve it:Just another alternative solution for those who want to pass variables to a script which is sourced using flask, I only managed to get this working by defining the variables outside and then calling the script as follows:If I input jinja variables in test123.js it doesn't work and you will get an error.Working answers are already given but I want to add a check that acts as a fail-safe in case the flask variable is not available. 

When you use:if there is an error that causes the variable to be non existent, resulting errors may produce unexpected results. To avoid this:This uses jinja2 to turn the geocode tuple into a json string, and then the javascript JSON.parse turns that into a javascript array.Well, I have a tricky method for this job. The idea is as follow-Make some invisible HTML tags like <label>, <p>, <input> etc. in HTML body and make a pattern in tag id, for example, use list index in tag id and list value at tag class name.Here I have two lists maintenance_next[] and maintenance_block_time[] of the same length. I want to pass these two list's data to javascript using the flask. So I take some invisible label tag and set its tag name is a pattern of list index and set its class name as value at index.       After this, I retrieve the data in javascript using some simple javascript operation.Some js files come from the web or library, they are not written by yourself.

The code they get variable like this:This method makes js files unchanged(keep independence), and pass variable correctly!

How do you send a HEAD HTTP request in Python 2?

fuentesjr

[How do you send a HEAD HTTP request in Python 2?](https://stackoverflow.com/questions/107405/how-do-you-send-a-head-http-request-in-python-2)

What I'm trying to do here is get the headers of a given URL so I can determine the MIME type. I want to be able to see if http://somedomain/foo/ will return an HTML document or a JPEG image for example. Thus, I need to figure out how to send a HEAD request so that I can read the MIME type without having to download the content. Does anyone know of an easy way of doing this?

2008-09-20 06:38:38Z

What I'm trying to do here is get the headers of a given URL so I can determine the MIME type. I want to be able to see if http://somedomain/foo/ will return an HTML document or a JPEG image for example. Thus, I need to figure out how to send a HEAD request so that I can read the MIME type without having to download the content. Does anyone know of an easy way of doing this?edit: This answer works, but nowadays you should just use the requests library as mentioned by other answers below.Use httplib.There's also a getheader(name) to get a specific header.urllib2 can be used to perform a HEAD request.  This is a little nicer than using httplib since urllib2 parses the URL for you instead of requiring you to split the URL into host name and path.Headers are available via response.info() as before.  Interestingly, you can find the URL that you were redirected to:Obligatory Requests way:I believe the Requests library should be mentioned as well.Just:Edit: I've just came to realize there is httplib2 :Dlink textFor completeness to have a Python3 answer equivalent to the accepted answer using httplib.It is basically the same code just that the library isn't called httplib anymore but http.clientAs an aside, when using the httplib (at least on 2.5.2), trying to read the response of a HEAD request will block (on readline) and subsequently fail.  If you do not issue read on the response, you are unable to send another request on the connection, you will need to open a new one.  Or accept a long delay between requests.I have found that httplib is slightly faster than urllib2.  I timed two programs - one using httplib  and the other using urllib2 - sending HEAD requests to 10,000 URL's.  The httplib one was faster by several minutes.  httplib's total stats were: real    6m21.334s

                                                                    user    0m2.124s

                                                                    sys     0m16.372sAnd urllib2's total stats were:                                 real    9m1.380s

                                                                    user    0m16.666s

                                                                    sys     0m28.565sDoes anybody else have input on this?And yet another approach (similar to Pawel answer):Just to avoid having unbounded methods at instance level.Probably easier: use urllib or urllib2.f.info() is a dictionary-like object, so you can do f.info()['content-type'], etc.http://docs.python.org/library/urllib.html

http://docs.python.org/library/urllib2.html

http://docs.python.org/library/httplib.htmlThe docs note that httplib is not normally used directly.

Cannot kill Python script with Ctrl-C

dotancohen

[Cannot kill Python script with Ctrl-C](https://stackoverflow.com/questions/11815947/cannot-kill-python-script-with-ctrl-c)

I am testing Python threading with the following script:This is running in Python 2.7 on Kubuntu 11.10. Ctrl+C will not kill it. I also tried adding a handler for system signals, but that did not help:To kill the process I am killing it by PID after sending the program to the background with Ctrl+Z, which isn't being ignored. Why is Ctrl+C being ignored so persistently? How can I resolve this?

2012-08-05 11:16:13Z

I am testing Python threading with the following script:This is running in Python 2.7 on Kubuntu 11.10. Ctrl+C will not kill it. I also tried adding a handler for system signals, but that did not help:To kill the process I am killing it by PID after sending the program to the background with Ctrl+Z, which isn't being ignored. Why is Ctrl+C being ignored so persistently? How can I resolve this?Ctrl+C terminates the main thread, but because your threads aren't in daemon mode, they keep running, and that keeps the process alive. We can make them daemons:But then there's another problem - once the main thread has started your threads, there's nothing else for it to do. So it exits, and the threads are destroyed instantly. So let's keep the main thread alive:Now it will keep print 'first' and 'second' until you hit Ctrl+C.Edit: as commenters have pointed out, the daemon threads may not get a chance to clean up things like temporary files. If you need that, then catch the KeyboardInterrupt on the main thread and have it co-ordinate cleanup and shutdown. But in many cases, letting daemon threads die suddenly is probably good enough.KeyboardInterrupt and signals are only seen by the process (ie the main thread)... Have a look at Ctrl-c i.e. KeyboardInterrupt to kill threads in pythonI think it's best to call join() on your threads when you expect them to die.  I've taken some liberty with your code to make the loops end (you can add whatever cleanup needs are required to there as well).  The variable die is checked for truth on each pass and when it's True then the program exits.

Get: TypeError: 'dict_values' object does not support indexing when using python 3.2.3 [duplicate]

Jesse Pet

[Get: TypeError: 'dict_values' object does not support indexing when using python 3.2.3 [duplicate]](https://stackoverflow.com/questions/17431638/get-typeerror-dict-values-object-does-not-support-indexing-when-using-python)

This is my code:This works completely fine when using python 2.7.3; however, when I use python 3.2.3, I get an error stating 'dict_values' object does not support indexing.  How can I modify the code to make it compatible for 3.2.3?

2013-07-02 17:06:26Z

This is my code:This works completely fine when using python 2.7.3; however, when I use python 3.2.3, I get an error stating 'dict_values' object does not support indexing.  How can I modify the code to make it compatible for 3.2.3?In Python 3, dict.values() (along with dict.keys() and dict.items()) returns a view, rather than a list. See the documentation here. You therefore need to wrap your call to dict.values() in a call to list like so:A simpler version of your code would be:If you want to keep the same structure, you can change it to:(You can just as easily put list(d.values()) inside the comprehension instead of vlst; it's just wasteful to do so since it would be re-generating the list every time).In Python 3 the dict.values() method returns a dictionary view object, not a list like it does in Python 2. Dictionary views have a length, can be iterated, and support membership testing, but don't support indexing.To make your code work in both versions, you could use either of these:    orBy far the simplest, fastest way to do the same thing in either version would be:Note however, that all of these methods will give you results that will vary depending on the actual contents of d. To overcome that, you may be able use an OrderedDict instead, which remembers the order that keys were first inserted into it, so you can count on the order of what is returned by the values() method.

Django: multiple models in one template using forms [closed]

neoice

[Django: multiple models in one template using forms [closed]](https://stackoverflow.com/questions/569468/django-multiple-models-in-one-template-using-forms)

I'm building a support ticket tracking app and have a few models I'd like to create from one page. Tickets belong to a Customer via a ForeignKey. Notes belong to Tickets via a ForeignKey as well. I'd like to have the option of selecting a Customer (that's a whole separate project) OR creating a new Customer, then creating a Ticket and finally creating a Note assigned to the new ticket.Since I'm fairly new to Django, I tend to work iteratively, trying out new features each time. I've played with ModelForms but I want to hide some of the fields and do some complex validation. It seems like the level of control I'm looking for either requires formsets or doing everything by hand, complete with a tedious, hand-coded template page, which I'm trying to avoid.Is there some lovely feature I'm missing? Does someone have a good reference or example for using formsets? I spent a whole weekend on the API docs for them and I'm still clueless. Is it a design issue if I break down and hand-code everything?

2009-02-20 12:50:26Z

I'm building a support ticket tracking app and have a few models I'd like to create from one page. Tickets belong to a Customer via a ForeignKey. Notes belong to Tickets via a ForeignKey as well. I'd like to have the option of selecting a Customer (that's a whole separate project) OR creating a new Customer, then creating a Ticket and finally creating a Note assigned to the new ticket.Since I'm fairly new to Django, I tend to work iteratively, trying out new features each time. I've played with ModelForms but I want to hide some of the fields and do some complex validation. It seems like the level of control I'm looking for either requires formsets or doing everything by hand, complete with a tedious, hand-coded template page, which I'm trying to avoid.Is there some lovely feature I'm missing? Does someone have a good reference or example for using formsets? I spent a whole weekend on the API docs for them and I'm still clueless. Is it a design issue if I break down and hand-code everything?This really isn't too hard to implement with ModelForms. So lets say you have Forms A, B, and C. You print out each of the forms and the page and now you need to handle the POST.Here are the docs for custom validation.I just was in about the same situation a day ago, and here are my 2 cents:1) I found arguably the shortest and most concise demonstration of multiple model entry in single form here: http://collingrady.wordpress.com/2008/02/18/editing-multiple-objects-in-django-with-newforms/ . In a nutshell: Make a form for each model, submit them both to template in a single <form>, using prefix keyarg and have the view handle validation. If there is dependency, just make sure you save the "parent"

 model before dependant, and use parent's ID for foreign key before commiting save of "child" model. The link has the demo.2) Maybe formsets can be beaten into doing this, but as far as I delved in, formsets are primarily for entering multiples of the same model, which may be optionally tied to another model/models by foreign keys. However, there seem to be no default option for entering more than one model's data and that's not what formset seems to be meant for.I very recently had the some problem and just figured out how to do this.

Assuming you have three classes, Primary, B, C and that B,C have a foreign key to primaryThis method should allow you to do whatever validation you require, as well as generating all three objects on the same page. I have also used javascript and hidden fields to allow the generation of multiple B,C objects on the same page.The MultiModelForm from django-betterforms is a convenient wrapper to do what is described in Gnudiff's answer. It wraps regular ModelForms in a single class which is transparently (at least for basic usage) used as a single form. I've copied an example from their docs below.I currently have a workaround functional (it passes my unit tests). It is a good solution to my opinion when you only want to add a limited number of fields from other models.Am I missing something here ?"I want to hide some of the fields and do some complex validation."I start with the built-in admin interface.Once this is done, you can move away from the built-in admin interface.Then you can fool around with multiple, partially related forms on a single web page.  This is a bunch of template stuff to present all the forms on a single page.Then you have to write the view function to read and validated the various form things and do the various object saves()."Is it a design issue if I break down and hand-code everything?"  No, it's just a lot of time for not much benefit.According to Django documentation, inline formsets are for this purpose:

"Inline formsets is a small abstraction layer on top of model formsets. These simplify the case of working with related objects via a foreign key".See https://docs.djangoproject.com/en/dev/topics/forms/modelforms/#inline-formsets

Removing duplicates from a list of lists

zaharpopov

[Removing duplicates from a list of lists](https://stackoverflow.com/questions/2213923/removing-duplicates-from-a-list-of-lists)

I have a list of lists in Python:And I want to remove duplicate elements from it. Was if it a normal list not of lists I could used set. But unfortunate that list is not hashable and can't make set of lists. Only of tuples. So I can turn all lists to tuples then use set and back to lists. But this isn't fast.How can this done in the most efficient way?The result of above list should be:I don't care about preserve order.Note: this question is similar but not quite what I need. Searched SO but didn't find exact duplicate.Benchmarking:"loop in" (quadratic method) fastest of all for short lists. For long lists it's faster then everyone except groupby method. Does this make sense?For short list (the one in the code), 100000 iterations:For longer list (the one in the code duplicated 5 times):

2010-02-06 17:17:46Z

I have a list of lists in Python:And I want to remove duplicate elements from it. Was if it a normal list not of lists I could used set. But unfortunate that list is not hashable and can't make set of lists. Only of tuples. So I can turn all lists to tuples then use set and back to lists. But this isn't fast.How can this done in the most efficient way?The result of above list should be:I don't care about preserve order.Note: this question is similar but not quite what I need. Searched SO but didn't find exact duplicate.Benchmarking:"loop in" (quadratic method) fastest of all for short lists. For long lists it's faster then everyone except groupby method. Does this make sense?For short list (the one in the code), 100000 iterations:For longer list (the one in the code duplicated 5 times):itertools often offers the fastest and most powerful solutions to this kind of problems, and is well worth getting intimately familiar with!-)Edit: as I mention in a comment, normal optimization efforts are focused on large inputs (the big-O approach) because it's so much easier that it offers good returns on efforts. But sometimes (essentially for "tragically crucial bottlenecks" in deep inner loops of code that's pushing the boundaries of performance limits) one may need to go into much more detail, providing probability distributions, deciding which performance measures to optimize (maybe the upper bound or the 90th centile is more important than an average or median, depending on one's apps), performing possibly-heuristic checks at the start to pick different algorithms depending on input data characteristics, and so forth.Careful measurements of "point" performance (code A vs code B for a specific input) are a part of this extremely costly process, and standard library module timeit helps here. However, it's easier to use it at a shell prompt.  For example, here's a short module to showcase the general approach for this problem, save it as nodup.py:Note the sanity check (performed when you just do python nodup.py) and the basic hoisting technique (make constant global names local to each function for speed) to put things on equal footing.Now we can run checks on the tiny example list:confirming that the quadratic approach has small-enough constants to make it attractive for tiny lists with few duplicated values.  With a short list without duplicates:the quadratic approach isn't bad, but the sort and groupby ones are better.  Etc, etc.If (as the obsession with performance suggests) this operation is at a core inner loop of your pushing-the-boundaries application, it's worth trying the same set of tests on other representative input samples, possibly detecting some simple measure that could heuristically let you pick one or the other approach (but the measure must be fast, of course).It's also well worth considering keeping a different representation for k -- why does it have to be a list of lists rather than a set of tuples in the first place?  If the duplicate removal task is frequent, and profiling shows it to be the program's performance bottleneck, keeping a set of tuples all the time and getting a list of lists from it only if and where needed, might be faster overall, for example.Doing it manually, creating a new k list and adding entries not found so far:Simple to comprehend, and you preserve the order of the first occurrence of each element should that be useful, but I guess it's quadratic in complexity as you're searching the whole of new_k for each element.I don't know if it's necessarily faster, but you don't have to use to tuples and sets. All the set-related solutions to this problem thus far require creating an entire set before iteration.It is possible to make this lazy, and at the same time preserve order, by iterating the list of lists and adding to a "seen" set. Then only yield a list if it is not found in this tracker set.This unique_everseen recipe is available in the itertools docs. It's also available in the 3rd party toolz library:Note that tuple conversion is necessary because lists are not hashable.Even your "long" list is pretty short. Also, did you choose them to match the actual data? Performance will vary with what these data actually look like. For example, you have a short list repeated over and over to make a longer list. This means that the quadratic solution is linear in your benchmarks, but not in reality.For actually-large lists, the set code is your best bet—it's linear (although space-hungry). The sort and groupby methods are O(n log n) and the loop in method is obviously quadratic, so you know how these will scale as n gets really big. If this is the real size of the data you are analyzing, then who cares? It's tiny.Incidentally, I'm seeing a noticeable speedup if I don't form an intermediate list to make the set, that is to say if I replacewithThe real solution may depend on more information: Are you sure that a list of lists is really the representation you need?List of tuple and {} can be used to remove duplicatesCreate a dictionary with tuple as the key, and print the keys.This should work.Strangely, the answers above removes the 'duplicates' but what if I want to remove the duplicated value also??

The following should be useful and does not create a new object in memory!and the o/p is:Another probably more generic and simpler solution is to create a dictionary keyed by the string version of the objects and getting the values() at the end:The catch is that this only works for objects whose string representation is a good-enough unique key (which is true for most native objects).

How can I map True/False to 1/0 in a Pandas DataFrame?

Simon Righley

[How can I map True/False to 1/0 in a Pandas DataFrame?](https://stackoverflow.com/questions/17383094/how-can-i-map-true-false-to-1-0-in-a-pandas-dataframe)

I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?EDIT:

The answers below do not seem to hold in the case of numpy that, given an array with both integers and True/False values, returns dtype=object on such array. In order to proceed with further calculations in numpy, I had to set explicitly np_values    = np.array(df.values, dtype = np.float64).

2013-06-29 17:53:19Z

I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?EDIT:

The answers below do not seem to hold in the case of numpy that, given an array with both integers and True/False values, returns dtype=object on such array. In order to proceed with further calculations in numpy, I had to set explicitly np_values    = np.array(df.values, dtype = np.float64).Just to very explicitly answer the question of how to convert a single column of boolean values to a column of integers 1 or 0:df.somecolumn = df.somecolumn.astype(int)Just multiply your Dataframe by 1 (int)True is 1 in Python, and likewise False is 0*:You should be able to perform any operations you want on them by just treating them as though they were numbers, as they are numbers:So to answer your question, no work necessary - you already have what you are looking for.* Note I use is as an English word, not the Python keyword is - True will not be the same object as any random 1.You also can do this directly on FramesYou can use a transformation for your data frame:

How to get instance variables in Python?

Chris Bunch

[How to get instance variables in Python?](https://stackoverflow.com/questions/109087/how-to-get-instance-variables-in-python)

Is there a built-in method in Python to get an array of all a class' instance variables? For example, if I have this code:Is there a way for me to do this:Edit: I originally had asked for class variables erroneously.

2008-09-20 19:30:24Z

Is there a built-in method in Python to get an array of all a class' instance variables? For example, if I have this code:Is there a way for me to do this:Edit: I originally had asked for class variables erroneously.Every object has a __dict__ variable containing all the variables and its values in it.Try thisUse vars()You normally can't get instance attributes given just a class, at least not without instantiating the class. You can get instance attributes given an instance, though, or class attributes given a class. See the 'inspect' module. You can't get a list of instance attributes because instances really can have anything as attribute, and -- as in your example -- the normal way to create them is to just assign to them in the __init__ method.An exception is if your class uses slots, which is a fixed list of attributes that the class allows instances to have. Slots are explained in http://www.python.org/2.2.3/descrintro.html, but there are various pitfalls with slots; they affect memory layout, so multiple inheritance may be problematic, and inheritance in general has to take slots into account, too.Both the Vars() and dict methods will work for the example the OP posted, but they won't work for "loosely" defined objects like:To print all non-callable attributes, you can use the following function:You can also test if an object has a specific variable with:Your example shows "instance variables",  not really class variables.Look in hi_obj.__class__.__dict__.items() for the class variables, along with other other class members like member functions and the containing module.Class variables are shared by all instances of the class.SuggestIn otherwords, it essentially just wraps __dict__ Although not directly an answer to the OP question, there is a pretty sweet way of finding out what variables are in scope in a function. take a look at this code:The func_code attribute has all kinds of interesting things in it. It allows you todo some cool stuff. Here is an example of how I have have used this:built on dmark's answer to get the following, which is useful if you want the equiv of sprintf and hopefully will help someone...

Removing numbers from string [closed]

user1739954

[Removing numbers from string [closed]](https://stackoverflow.com/questions/12851791/removing-numbers-from-string)

How can I remove digits from a string?

2012-10-12 03:27:57Z

How can I remove digits from a string?Would this work for your situation? This makes use of a list comprehension, and what is happening here is similar to this structure:As @AshwiniChaudhary and @KirkStrauser point out, you actually do not need to use the brackets in the one-liner, making the piece inside the parentheses a generator expression (more efficient than a list comprehension). Even if this doesn't fit the requirements for your assignment, it is something you should read about eventually :) :And, just to throw it in the mix, is the oft-forgotten str.translate which will work a lot faster than looping/regular expressions:For Python 2:For Python 3:Not sure if your teacher allows you to use filters but...returns-Much more efficient than looping...Example:What about this:Just a few (others have suggested some of these)Method 1:Method 2:Method 3:Method 4:Method 5:Say st is your unformatted string, then runas mentioned above.

But my guess that you need something very simple 

so say s is your string

and st_res is a string without digits, then here is your code I'd love to use regex to accomplish this, but since you can only use lists, loops, functions, etc..here's what I came up with:If i understand your question right, one way to do is break down the string in chars and then check each char in that string using a loop whether it's a string or a number and then if string save it in a variable and then once the loop is finished, display that to the user

datetime dtypes in pandas read_csv

user3221055

[datetime dtypes in pandas read_csv](https://stackoverflow.com/questions/21269399/datetime-dtypes-in-pandas-read-csv)

I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:When run gives a error:Converting columns after the fact, via pandas.to_datetime() isn't an option I can't know which columns will be datetime objects.  That information can change and comes from whatever informs my dtypes list.Alternatively, I've tried to load the csv file with numpy.genfromtxt, set the dtypes in that function, and then convert to a pandas.dataframe but it garbles the data.  Any help is greatly appreciated!

2014-01-21 21:24:17Z

I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:When run gives a error:Converting columns after the fact, via pandas.to_datetime() isn't an option I can't know which columns will be datetime objects.  That information can change and comes from whatever informs my dtypes list.Alternatively, I've tried to load the csv file with numpy.genfromtxt, set the dtypes in that function, and then convert to a pandas.dataframe but it garbles the data.  Any help is greatly appreciated!There is no datetime dtype to be set for read_csv as csv files can only contain strings, integers and floats.Setting a dtype to datetime will make pandas interpret the datetime as an object, meaning you will end up with a string.The pandas.read_csv() function has a keyword argument called parse_datesUsing this you can on the fly convert strings, floats or integers into datetimes using the default date_parser (dateutil.parser.parser)This will cause pandas to read col1 and col2 as strings, which they most likely are ("2016-05-05" etc.) and after having read the string, the date_parser for each column will act upon that string and give back whatever that function returns.The pandas.read_csv() function also has a keyword argument called date_parserSetting this to a lambda function will make that particular function be used for the parsing of the dates.You have to give it the function, not the execution of the function, thus this is CorrectThis is incorrect:pd.datetools.to_datetime has been relocated to date_parser = pd.to_datetimeThanks @stackoverYCThere is a parse_dates parameter for read_csv which allows you to define the names of the columns you want treated as dates or datetimes:You might try passing actual types instead of strings.But it's going to be really hard to diagnose this without any of your data to tinker with.And really, you probably want pandas to parse the the dates into TimeStamps, so that might be:I tried using the dtypes=[datetime, ...] option, butI encountered the following error:The only change I had to make is to replace datetime with datetime.datetime

Mac OS X - EnvironmentError: mysql_config not found

daniel_c05

[Mac OS X - EnvironmentError: mysql_config not found](https://stackoverflow.com/questions/25459386/mac-os-x-environmenterror-mysql-config-not-found)

First off, yeah, I've already seen this: pip install mysql-python fails with EnvironmentError: mysql_config not foundThe problemI am trying to use Django on a Google App Engine project. However, I haven't been able to get started as the server fails to start properly due to: I did some research and it all pointed to having to install Mysql-python, as apparently it isn't on my system. I actually tried uninstalling it and got this: Whenever I actually do try to install via: I get an error stating: I've already tried running: but that didn't seem to help, as I ran the installation command again and it still failed. Any ideas? Please note I'm not in a virtualenv. 

2014-08-23 07:08:18Z

First off, yeah, I've already seen this: pip install mysql-python fails with EnvironmentError: mysql_config not foundThe problemI am trying to use Django on a Google App Engine project. However, I haven't been able to get started as the server fails to start properly due to: I did some research and it all pointed to having to install Mysql-python, as apparently it isn't on my system. I actually tried uninstalling it and got this: Whenever I actually do try to install via: I get an error stating: I've already tried running: but that didn't seem to help, as I ran the installation command again and it still failed. Any ideas? Please note I'm not in a virtualenv. Ok, well, first of all, let me check if I am on the same page as you:If you did all those steps in the same order, and you still got an error, read on to the end, if, however, you did not follow these exact steps try, following them from the very beginning.So, you followed the steps, and you're still geting an error, well, there are a few things you could try:Note1: I've seen some people saying that installing python-dev and libmysqlclient-dev also helped, however I do not know if these packages are available on Mac OS.Note2: Also, make sure to try running the commands as root.I got my answers from (besides my brain) these places (maybe you could have a look at them, to see if it would help): 1, 2, 3, 4.I hoped I helped, and would be happy to know if any of this worked, or not. Good luck.I had been debugging this problem forever - 3 hours 17 mins. What particularly annoyed me was that I already had sql installed on my system through prior uni work but pip/pip3 wasn't recognising it. These threads above and many other I scoured the internet for were helpful in eluminating the problem but didn't actually solve things.ANSWERPip is looking for mysql binaries in the Homebrew Directory which is located relative to Macintosh HD @so I found that this requires you making a few changesstep 1: Download MySql if not already done so https://dev.mysql.com/downloads/Step 2: Locate it relative to Macintosh HD and cd Step 3: Once there open terminal and use a text editor of choice - I'm a neovim guy myself so I typed (doesn't automatically come with Mac... another story for another day)Step 4: You will see at approx line 112Change to*you'll notice that this file has read-only access so if your using vim or neovimStep 5: Head to the home directory and edit the .bash_profile fileThenand addto the file then saveStep 6: relative to Macintosh HD locate paths and add to itthen and add *you'll again notice that this file has read-only access so if your using vim or neovimthenthen refresh the terminal with your changes by running FinallyAnd Viola. Remember it's a vibe.If you don't want to install full mysql, we can fix this by just installing mysqlclient

brew install mysqlclient

Once cmd is completed it will ask to add below line to ~/.bash_profile:

echo 'export PATH="/usr/local/opt/mysql-client/bin:$PATH"' >> ~/.bash_profile

Close terminal and start new terminal and proceed with pip install mysqlclientAlso this happens when I was installing mysqlclient,As user3429036 said,I am running Python 3.6 on MacOS Catalina. My issue was that I tried to install mysqlclient==1.4.2.post1 and it keeps throwing mysql_config not found error.This is the steps I took to solve the issue.This answer is for MacOS users who did not install from brew but rather from the official .dmg/.pkg. That installer fails to edit your PATH, causing things to break out of the box:What you have to do is appending the MySQL bin folder (typically /usr/local/mysql/bin in your PATH by adding this line in your ~/.bash_profile file:export PATH="/usr/local/mysql/bin/:$PATH"You should then reload your ~/.bash_profile for the change to take effect in your current Terminal session:source ~/.bash_profileBefore installing mysqlclient, however, you need to accept the XcodeBuild license:sudo xcodebuild -licenseFollow their directions to sign away your family, after which you should be able to install mysqlclient without issue:pip install mysqlclientAfter installing that, you must do one more thing to fix a runtime bug that ships with MySQL (Dynamic Library libmysqlclient.dylib not found), by adding this line to your system dynamic libraries path:export DYLD_LIBRARY_PATH=/usr/local/mysql/lib/:$DYLD_LIBRARY_PATHIf you have installed mysql using Homebrew by specifying a version then mysql_config would be present here. - /usr/local/Cellar/mysql@5.6/5.6.47/binyou can find the path of the sql bin by using ls command in /usr/local/ directoryAdd the path to bash profile like this. Install brew or apt-get is also not easy for me so I downloaded mysql via: https://dev.mysql.com/downloads/connector/python/, installed it. So I can find mysql_config int this directory: /usr/local/mysql/binthe next step is:

Outputting data from unit test in python

Silverfish

[Outputting data from unit test in python](https://stackoverflow.com/questions/284043/outputting-data-from-unit-test-in-python)

If I'm writing unit tests in python (using the unittest module), is it possible to output data from a failed test, so I can examine it to help deduce what caused the error? I am aware of the ability to create a customized message, which can carry some information, but sometimes you might deal with more complex data, that can't easily be represented as a string.For example, suppose you had a class Foo, and were testing a method bar, using data from a list called testdata:If the test failed, I might want to output t1, t2 and/or f, to see why this particular data resulted in a failure. By output, I mean that the variables can be accessed like any other variables, after the test has been run.

2008-11-12 14:10:27Z

If I'm writing unit tests in python (using the unittest module), is it possible to output data from a failed test, so I can examine it to help deduce what caused the error? I am aware of the ability to create a customized message, which can carry some information, but sometimes you might deal with more complex data, that can't easily be represented as a string.For example, suppose you had a class Foo, and were testing a method bar, using data from a list called testdata:If the test failed, I might want to output t1, t2 and/or f, to see why this particular data resulted in a failure. By output, I mean that the variables can be accessed like any other variables, after the test has been run.Very late answer for someone that, like me, comes here looking for a simple and quick answer.In Python 2.7 you could use an additional parameter msg to add information to the error message like this:Offical docs hereWe use the logging module for this.For example:That allows us to turn on debugging for specific tests which we know are failing and for which we want additional debugging information.My preferred method, however, isn't to spend a lot of time on debugging, but spend it writing more fine-grained tests to expose the problem.You can use simple print statements, or any other way of writing to stdout.  You can also invoke the Python debugger anywhere in your tests.If you use nose to run your tests (which I recommend), it will collect the stdout for each test and only show it to you if the test failed, so you don't have to live with the cluttered output when the tests pass.nose also has switches to automatically show variables mentioned in asserts, or to invoke the debugger on failed tests.  For example -s (--nocapture) prevents the capture of stdout.I don't think this is quite what your looking for, there's no way to display variable values that don't fail, but this may help you get closer to outputting the results the way you want.You can use the TestResult object returned by the TestRunner.run() for results analysis and processing.  Particularly, TestResult.errors and TestResult.failuresAbout the TestResults Object:http://docs.python.org/library/unittest.html#id3And some code to point you in the right direction:I think I might have been overthinking this. One way I've come up with that does the job, is simply to have a global variable, that accumulates the diagnostic data.Somthing like this:Thanks for the resplies. They have given me some alternative ideas for how to record information from unit tests.              Another option - start a debugger where the test fails.Try running your tests with Testoob (it will run your unittest suite without changes), and you can use the '--debug' command line switch to open a debugger when a test fails.Here's a terminal session on windows:The method I use is really simple. I just log it as a warning so it will actually show up.

Use logging:Usage:If you do not set LOG_FILE, logging will got to stderr.You can use logging module for that.So in the unit test code, use:By default warnings and errors are outputted to /dev/stderr, so they should be visible on the console.To customize logs (such as formatting), try the following sample:What I do in these cases is to have a log.debug() with some messages in my application. Since the default logging level is WARNING, such messages don't show in the normal execution.Then, in the unittest I change the logging level to DEBUG, so that such messages are shown while running them.In the unittests:See a full example:This is daikiri.py, a basic class that implements a Daikiri with its name and price. There is method make_discount() that returns the price of that specific daikiri after applying a given discount:Then, I create a unittest test_daikiri.py that checks its usage:So when I execute it I get the log.debug messages:inspect.trace will let you get local variables after an exception has been thrown.  You can then wrap the unit tests with a decorator like the following one to save off those local variables for examination during the post mortem.The last line will print the returned values where the test succeeded and the local variables, in this case x, when it fails:Har det gøy :-)How about catching the exception that gets generated from the assertion failure?  In your catch block you could output the data however you wanted to wherever.  Then when you were done you could re-throw the exception.  The test runner probably wouldn't know the difference. Disclaimer: I haven't tried this with python's unit test framework but have with other unit test frameworks.Admitting that I haven't tried it, the testfixtures' logging feature looks quite useful...Expanding @F.C. 's answer, this works quite well for me:

「Too many values to unpack」Exception

Steve Gattuso

[「Too many values to unpack」Exception](https://stackoverflow.com/questions/1479776/too-many-values-to-unpack-exception)

I'm working on a project in Django and I've just started trying to extend the User model in order to make user profiles. Unfortunately, I've run into a problem: Every time I try to get the user's profile inside of a template (user.get_template.lastIP, for example), I get the following error:Any ideas as to what's going on or what I'm doing wrong?

2009-09-25 22:04:27Z

I'm working on a project in Django and I've just started trying to extend the User model in order to make user profiles. Unfortunately, I've run into a problem: Every time I try to get the user's profile inside of a template (user.get_template.lastIP, for example), I get the following error:Any ideas as to what's going on or what I'm doing wrong?That exception means that you are trying to unpack a tuple, but the tuple has too many values with respect to the number of target variables. For example: this work, and prints 1, then 2, then 3But this raises your errorraisesNow, the reason why this happens in your case, I don't know, but maybe this answer will point you in the right direction.try unpacking in one variable,python will handle it as a list,then unpack from the listThis problem looked familiar so I thought I'd see if I could replicate from the limited amount of information.A quick search turned up an entry in James Bennett's blog here which mentions that when working with the UserProfile to extend the User model a common mistake in settings.py can cause Django to throw this error.  To quote the blog entry:If the OP had copied more of the traceback I would expect to see something like the one below which I was able to duplicate by adding "models" to my AUTH_PROFILE_MODULE setting.This I think is one of the few cases where Django still has a bit of import magic that tends to cause confusion when a small error doesn't throw the expected exception.You can see at the end of the traceback that I posted how using anything other than the form "appname.modelname" for the AUTH_PROFILE_MODULE would cause the line "app_label, model_name = settings.AUTH_PROFILE_MODULE.split('.')"  to throw the "too many values to unpack" error.I'm 99% sure that this was the original problem encountered here.Most likely there is an error somewhere in the get_profile() call. In your view, before you return the request object, put this line:It should raise the error, and give you a more detailed traceback, which you can then use to further debug.This happens to me when I'm using Jinja2 for templates. The problem can be solved by running the development server using the runserver_plus command from django_extensions. It uses the werkzeug debugger which also happens to be a lot better and has a very nice interactive debugging console. It does some ajax magic to launch a python shell at any frame (in the call stack) so you can debug.

Rename an environment with virtualenvwrapper

hobbes3

[Rename an environment with virtualenvwrapper](https://stackoverflow.com/questions/9540040/rename-an-environment-with-virtualenvwrapper)

I have an environment called doors and I would like to rename it to django for the virtualenvwrapper.I've noticed that if I just rename the folder ~/.virtualenvs/doors to django, I can now call workon django, but the environment still says (doors)hobbes3@hobbes3.

2012-03-02 20:39:55Z

I have an environment called doors and I would like to rename it to django for the virtualenvwrapper.I've noticed that if I just rename the folder ~/.virtualenvs/doors to django, I can now call workon django, but the environment still says (doors)hobbes3@hobbes3.You can use:So in your case:if you do:you'll notice that will have doors as location and not django, you'll to change each file with the new location.solution:

 after renamed the folder execute the command below.now if you do:

Fast check for NaN in NumPy

Fred Foo

[Fast check for NaN in NumPy](https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy)

I'm looking for the fastest way to check for the occurrence of NaN (np.nan) in a NumPy array X. np.isnan(X) is out of the question, since it builds a boolean array of shape X.shape, which is potentially gigantic.I tried np.nan in X, but that seems not to work because np.nan != np.nan. Is there a fast and memory-efficient way to do this at all?(To those who would ask "how gigantic": I can't tell. This is input validation for library code.)

2011-07-18 17:10:05Z

I'm looking for the fastest way to check for the occurrence of NaN (np.nan) in a NumPy array X. np.isnan(X) is out of the question, since it builds a boolean array of shape X.shape, which is potentially gigantic.I tried np.nan in X, but that seems not to work because np.nan != np.nan. Is there a fast and memory-efficient way to do this at all?(To those who would ask "how gigantic": I can't tell. This is input validation for library code.)Ray's solution is good. However, on my machine it is about 2.5x faster to use numpy.sum in place of numpy.min:Unlike min, sum doesn't require branching, which on modern hardware tends to be pretty expensive. This is probably the reason why sum is faster.edit The above test was performed with a single NaN right in the middle of the array.It is interesting to note that min is slower in the presence of NaNs than in their absence. It also seems to get slower as NaNs get closer to the start of the array. On the other hand, sum's throughput seems constant regardless of whether there are NaNs and where they're located:I think np.isnan(np.min(X)) should do what you want.Even there exist an accepted answer, I'll like to demonstrate the following (with Python 2.7.2 and Numpy 1.6.0 on Vista):Thus, the really efficient way might be heavily dependent on the operating system. Anyway dot(.) based seems to be the most stable one.There are two general approaches here:While the first approach is certainly the cleanest, the heavy optimization of some of the cumulative operations (particularly the ones that are executed in BLAS, like dot) can make those quite fast. Note that dot, like some other BLAS operations, are multithreaded under certain conditions. This explains the difference in speed between different machines.If you're comfortable with numba it allows to create a fast short-circuit (stops as soon as a NaN is found) function:If there is no NaN the function might actually be slower than np.min, I think that's because np.min uses multiprocessing for large arrays:But in case there is a NaN in the array, especially if it's position is at low indices, then it's much faster:Similar results may be achieved with Cython or a C extension, these are a bit more complicated (or easily avaiable as bottleneck.anynan) but ultimatly do the same as my anynan function.Related to this is the question of how to find the first occurrence of NaN. This is the fastest way to handle that that I know of:

What to do with「Unexpected indent」in python?

Alice Purcell

[What to do with「Unexpected indent」in python?](https://stackoverflow.com/questions/1016814/what-to-do-with-unexpected-indent-in-python)

How do I rectify the error "unexpected indent" in python?

2009-06-19 07:53:20Z

How do I rectify the error "unexpected indent" in python?Python uses spacing at the start of the line to determine when code blocks start and end. Errors you can get are:Unexpected indent. This line of code has more spaces at the start than the one before, but the one before is not the start of a subblock (e.g. if/while/for statement). All lines of code in a block must start with exactly the same string of whitespace. For instance:This one is especially common when running python interactively: make sure you don't put any extra spaces before your commands. (Very annoying when copy-and-pasting example code!)Unindent does not match any outer indentation level. This line of code has fewer spaces at the start than the one before, but equally it does not match any other block it could be part of. Python cannot decide where it goes. For instance, in the following, is the final print supposed to be part of the if clause, or not?Expected an indented block. This line of code has the same number of spaces at the start as the one before, but the last line was expected to start a block (e.g. if/while/for statement, function definition).If you want a function that doesn't do anything, use the "no-op" command pass:Mixing tabs and spaces is allowed (at least on my version of Python), but Python assumes tabs are 8 characters long, which may not match your editor. Just say "no" to tabs. Most editors allow them to be automatically replaced by spaces.The best way to avoid these issues is to always use a consistent number of spaces when you indent a subblock, and ideally use a good IDE that solves the problem for you. This will also make your code more readable.In Python, the spacing is very important, this gives the structure of your code blocks.

This error happens when you mess up your code structure, for example like this :You may also have a mix of tabs and spaces in your file.I suggest you use a python syntax aware editor like PyScripter, or NetbeansRun your code with the -tt option to find out if you are using tabs and spaces inconsistently Turn on visible whitespace in whatever editor you are using and turn on replace tabs with spaces.While you can use tabs with Python mixing tabs and space usually leads to the error you are experiencing. Replacing tabs with 4 spaces is the recommended approach for writing Python code.By using correct indentation. Python is whitespace aware, so you need to follow its indentation guidlines for blocks or you'll get indentation errors.If you're writing Python using Sublime and getting indentation errors, view -> indentation -> convert indentation to spacesThe issue I'm describing is caused by the Sublime text editor. The same issue could be caused by other editors as well. Essentially, the issue has to do with Python wanting to treat indentations in terms of spaces versus various editors coding the indentations in terms of tabs.Make sure you use the option "insert spaces instead of tabs" in your editor. Then you can choose you want a tab width of, for example 4. You can find those options in gedit under edit-->preferences-->editor.bottom line: USE SPACES not tabsThis error can also occur when pasting something into the Python interpreter (terminal/console). Note that the interpreter interprets an empty line as the end of an expression, so if you paste in something likethe interpreter will interpret the empty line before y = 7 as the end of the expression, i.e. that you're done defining your function, and the next line - y = 7 will have incorrect indentation because it is a new expression.One issue which doesn't seem to have been mentioned is that this error can crop up due to a problem with the code that has nothing to do with indentation.For example, take the following script:This returns an IndentationError: unexpected unindent when the problem is of course a missing except: statement.My point: check the code above where the unexpected (un)indent is reported!If the indentation looks ok then have a look to see if your editor has a "View Whitespace" option. Enabling this should allow to find where spaces and tabs are mixed.There is a trick that always worked for me:If you got and unexpected indent and you see that all the code is perfectly indented, try opening it with another editor and you will see what line of code is not indented.It happened to me when used vim, gedit or editors like that.Try to use only 1 editor for your code.Simply copy your script and put under """ your entire code """ ...specify this line in a variable.. like,now execute this code, in Sublime Editor using ctrl+b, now it will print indented code in console. that's itAll You need to do is remove spaces   or tab  spaces  from the start of following codes Notepad++ was giving the tab space correct but the indentation problem was finally found in Sublime text editor.Use Sublime text editor and go line by lineIndentation in Python is important and this is just not for code readability, unlike many other programming languages.

If there is any white space or tab in your code between consecutive commands, python will give this error as Python is sensitive to this. We are likely to get this error when we do copy and paste of code to any Python.

Make sure to identify and remove these spaces using a text editor like Notepad++ or manually remove the whitespace from the line of code where you are getting an error.Thanks!

How to delete a record by id in Flask-SQLAlchemy

Sergey

[How to delete a record by id in Flask-SQLAlchemy](https://stackoverflow.com/questions/27158573/how-to-delete-a-record-by-id-in-flask-sqlalchemy)

I have users table in my MySql database. This table has id, name and age fields.How can I delete some record by id?Now I use the following code:But I don't want to make any query before delete operation. Is there any way to do this? I know, I can use db.engine.execute("delete from users where id=..."), but I would like to use delete() method.

2014-11-26 20:45:43Z

I have users table in my MySql database. This table has id, name and age fields.How can I delete some record by id?Now I use the following code:But I don't want to make any query before delete operation. Is there any way to do this? I know, I can use db.engine.execute("delete from users where id=..."), but I would like to use delete() method.You can do this,orMake sure to commit for delete() to take effect.Just want to share another option:http://docs.sqlalchemy.org/en/latest/orm/session_basics.html#deletingIn this example, the following codes shall works fine:Another possible solution specially if you want batch delete

Pandas DataFrame: replace all values in a column, based on condition

ichimok

[Pandas DataFrame: replace all values in a column, based on condition](https://stackoverflow.com/questions/31511997/pandas-dataframe-replace-all-values-in-a-column-based-on-condition)

I have a simple DataFrame like the following:I want to select all values from the 'First Season' column and replace those that are over 1990 by 1. In this example, only Baltimore Ravens would have the 1996 replaced by 1 (keeping the rest of the data intact).I have used the following:But, it replaces all the values in that row by 1, and not just the values in the 'First Season' column.How can I replace just the values from that column?

2015-07-20 08:35:34Z

I have a simple DataFrame like the following:I want to select all values from the 'First Season' column and replace those that are over 1990 by 1. In this example, only Baltimore Ravens would have the 1996 replaced by 1 (keeping the rest of the data intact).I have used the following:But, it replaces all the values in that row by 1, and not just the values in the 'First Season' column.How can I replace just the values from that column?You need to select that column:So the syntax here is:You can check the docs and also the 10 minutes to pandas which shows the semanticsEDITIf you want to generate a boolean indicator then you can just use the boolean condition to generate a boolean Series and cast the dtype to int this will convert True and False to 1 and 0 respectively:A bit late to the party but still - I prefer using numpy where:strange that nobody has this answer, the only missing part of your code is the ['First Season'] right after df and just remove your curly brackets inside.

Python extract pattern matches

Kannan Ekanath

[Python extract pattern matches](https://stackoverflow.com/questions/15340582/python-extract-pattern-matches)

Python 2.7.1

I am trying to use python regular expression to extract words inside of a patternI have some string that looks like thisI want to extract the word "my_user_name". I do something likeHow do I extract my_user_name now?

2013-03-11 14:04:05Z

Python 2.7.1

I am trying to use python regular expression to extract words inside of a patternI have some string that looks like thisI want to extract the word "my_user_name". I do something likeHow do I extract my_user_name now?You need to capture from regex. search for the pattern, if found, retrieve the string using group(index). Assuming valid checks are performed:You can use matching groups:e.g.Here I use re.findall rather than re.search to get all instances of my_user_name.  Using re.search, you'd need to get the data from the group on the match object:As mentioned in the comments, you might want to make your regex non-greedy:to only pick up the stuff between 'name ' and the next ' is valid' (rather than allowing your regex to pick up other ' is valid' in your group.You could use something like this:Maybe that's a bit shorter and easier to understand:You want a capture group.You can use groups (indicated with '(' and ')') to capture parts of the string. The match object's group() method then gives you the group's contents:In Python 3.6+ you can also index into a match object instead of using group():Here's a way to do it without using groups (Python 3.6 or above):You can also use a capture group (?P<user>pattern) and access the group like a dictionary match['user'].

Principal component analysis in Python

Vebjorn Ljosa

[Principal component analysis in Python](https://stackoverflow.com/questions/1730600/principal-component-analysis-in-python)

I'd like to use principal component analysis (PCA) for dimensionality reduction.  Does numpy or scipy already have it, or do I have to roll my own using numpy.linalg.eigh?I don't just want to use singular value decomposition (SVD) because my input data are quite high-dimensional (~460 dimensions), so I think SVD will be slower than computing the eigenvectors of the covariance matrix.I was hoping to find a premade, debugged implementation that already makes the right decisions for when to use which method, and which maybe does other optimizations that I don't know about.

2009-11-13 17:04:31Z

I'd like to use principal component analysis (PCA) for dimensionality reduction.  Does numpy or scipy already have it, or do I have to roll my own using numpy.linalg.eigh?I don't just want to use singular value decomposition (SVD) because my input data are quite high-dimensional (~460 dimensions), so I think SVD will be slower than computing the eigenvectors of the covariance matrix.I was hoping to find a premade, debugged implementation that already makes the right decisions for when to use which method, and which maybe does other optimizations that I don't know about.You might have a look at MDP.   I have not had the chance to test it myself, but I've bookmarked it exactly for the PCA functionality.Months later, here's a small class PCA, and a picture:PCA using numpy.linalg.svd is super easy. Here's a simple demo:You can use sklearn:matplotlib.mlab has a PCA implementation.SVD should work fine with 460 dimensions. It takes about 7 seconds on my Atom netbook. The eig() method takes more time (as it should, it uses more floating point operations) and will almost always be less accurate. If you have less than 460 examples then what you want to do is diagonalize the scatter matrix (x - datamean)^T(x - mean), assuming your data points are columns, and then left-multiplying by (x - datamean). That might be faster in the case where you have more dimensions than data.You can quite easily "roll" your own using scipy.linalg (assuming a pre-centered dataset data):Then evs are your eigenvalues, and evmat is your projection matrix.If you want to keep d dimensions, use the first d eigenvalues and first d eigenvectors.Given that scipy.linalg has the decomposition and numpy the matrix multiplications, what else do you need?I just finish reading the book Machine Learning: An Algorithmic Perspective. All code examples in the book was written by Python(and almost with Numpy). The code snippet of chatper10.2 Principal Components Analysis maybe worth a reading. It use numpy.linalg.eig.

By the way, I think SVD can handle 460 * 460 dimensions very well. I have calculate a 6500*6500 SVD with numpy/scipy.linalg.svd on a very old PC:Pentium III 733mHz. To be honest, the script needs a lot of memory(about 1.xG) and a lot of time(about 30 minutes) to get the SVD result.

But I think 460*460 on a modern PC will not be a big problem unless u need do SVD a huge number of times.You do not need full Singular Value Decomposition (SVD) at it computes all eigenvalues and eigenvectors and can be prohibitive for large matrices.

scipy and its sparse module provide generic linear algrebra functions working on both sparse and dense matrices, among which there is the eig* family of functions : http://docs.scipy.org/doc/scipy/reference/sparse.linalg.html#matrix-factorizationsScikit-learn provides a Python PCA implementation which only support dense matrices for now.Timings :Here is another implementation of a PCA module for python using numpy, scipy and C-extensions. The module carries out PCA using either a SVD or the NIPALS (Nonlinear Iterative Partial Least Squares) algorithm which is implemented in C.If you're working with 3D vectors, you can apply SVD concisely using the toolbelt vg. It's a light layer on top of numpy.There's also a convenient alias if you only want the first principal component:I created the library at my last startup, where it was motivated by uses like this: simple ideas which are verbose or opaque in NumPy.

How to write to a file, using the logging Python module?

Takkun

[How to write to a file, using the logging Python module?](https://stackoverflow.com/questions/6386698/how-to-write-to-a-file-using-the-logging-python-module)

How can I use the logging module in Python to write to a file? Every time I try to use it, it just prints out the message.

2011-06-17 13:44:21Z

How can I use the logging module in Python to write to a file? Every time I try to use it, it just prints out the message.An example of using logging.basicConfig rather than logging.fileHandler()In order, the five parts do the following: Taken from the "logging cookbook":And you're good to go.P.S. Make sure to read the logging HOWTO as well.I prefer to use a configuration file. It allows me to switch logging levels, locations, etc without changing code when I go from development to release. I simply package a different config file with the same name, and with the same defined loggers.Here is my code for the log config filehttp://docs.python.org/library/logging.html#logging.basicConfighttp://docs.python.org/library/logging.handlers.html#filehandlerhere's a simpler way to go about it. this solution doesn't use a

 config dictionary and uses a rotation file handler, like so:or like so:the handlers variable needs to be an iterable. logpath+filename and debug_level are just variables holding the

 respective info. of course, the values for the function params are up

 to you.the first time i was using the logging module i made the mistake of writing the following, which generates an OS file lock error (the

above is the solution to that):and Bob's your uncle!Format DescriptionNormal way of callingOutputUsing Dict, Call values otherMod2.pyOutput

Pickle or json?

Juanjo Conti

[Pickle or json?](https://stackoverflow.com/questions/2259270/pickle-or-json)

I need to save to disk a little dict object whose keys are of the type str and values are ints and then recover it. Something like this:What is the best option and why? Serialize it with pickle or with simplejson?I am using Python 2.6.

2010-02-13 22:12:56Z

I need to save to disk a little dict object whose keys are of the type str and values are ints and then recover it. Something like this:What is the best option and why? Serialize it with pickle or with simplejson?I am using Python 2.6.If you do not have any interoperability requirements (e.g. you are just going to use the data with Python) and a binary format is fine, go with cPickle which gives you really fast Python object serialization.If you want interoperability or you want a text format to store your data, go with JSON (or some other appropriate format depending on your constraints).I prefer JSON over pickle for my serialization. Unpickling can run arbitrary code, and using pickle to transfer data between programs or store data between sessions is a security hole. JSON does not introduce a security hole and is standardized, so the data can be accessed by programs in different languages if you ever need to.You might also find this interesting, with some charts to compare: http://kovshenin.com/archives/pickle-vs-json-which-is-faster/If you are primarily concerned with speed and space, use cPickle because cPickle is faster than JSON.If you are more concerned with interoperability, security, and/or human readability, then use JSON.The tests results referenced in other answers were recorded in 2010, and the updated tests in 2016 with cPickle protocol 2 show:Reproduce this yourself with this gist, which is based on the Konstantin's benchmark referenced in other answers, but using cPickle with protocol 2 instead of pickle, and using json instead of simplejson (since json is faster than simplejson), e.g.Results with python 2.7 on a decent 2015 Xeon processor:Python 3.4 with pickle protocol 3 is even faster.JSON or pickle?  How about JSON and pickle! You can use jsonpickle.  It easy to use and the file on disk is readable because it's JSON.http://jsonpickle.github.com/Personally, I generally prefer JSON because the data is human-readable. Definitely, if you need to serialize something that JSON won't take, than use pickle.But for most data storage, you won't need to serialize anything weird and JSON is much easier and always allows you to pop it open in a text editor and check out the data yourself.The speed is nice, but for most datasets the difference is negligible; Python generally isn't too fast anyways.I have tried several methods and found out that using cPickle with setting the protocol argument of the dumps method as: cPickle.dumps(obj, protocol=cPickle.HIGHEST_PROTOCOL) is the fastest dump method.Output:

Plotting a 2D heatmap with Matplotlib

Karnivaurus

[Plotting a 2D heatmap with Matplotlib](https://stackoverflow.com/questions/33282368/plotting-a-2d-heatmap-with-matplotlib)

Using Matplotlib, I want to plot a 2D heat map. My data is an n-by-n Numpy array, each with a value between 0 and 1. So for the (i, j) element of this array, I want to plot a square at the (i, j) coordinate in my heat map, whose color is proportional to the element's value in the array.How can I do this?

2015-10-22 13:37:08Z

Using Matplotlib, I want to plot a 2D heat map. My data is an n-by-n Numpy array, each with a value between 0 and 1. So for the (i, j) element of this array, I want to plot a square at the (i, j) coordinate in my heat map, whose color is proportional to the element's value in the array.How can I do this?The imshow() function with parameters interpolation='nearest' and cmap='hot' should do what you want.Seaborn takes care of a lot of the manual work and automatically plots a gradient at the side of the chart etc.Or, you can even plot upper / lower left / right triangles of square matrices, for example a correlation matrix which is square and is symmetric, so plotting all values would be redundant anyway.For a 2d numpy array, simply use imshow() may help you:This code produces a continuous heatmap.You can choose another built-in colormap from here.Here's how to do it from a csv:where dat.xyz is in the formI would use matplotlib's pcolor/pcolormesh function since it allows nonuniform spacing of the data.Example taken from matplotlib:

Do python projects need a MANIFEST.in, and what should be in it?

Neil Walker

[Do python projects need a MANIFEST.in, and what should be in it?](https://stackoverflow.com/questions/24727709/do-python-projects-need-a-manifest-in-and-what-should-be-in-it)

The "Python Distribute" guide (was at python-distribute.org, but that registration has lapsed) tells me to include doc/txt files and .py files are excluded in MANIFEST.in fileThe sourcedist documentation tells me only sdist uses MANIFEST.in and only includes file you specify and to include .py files. It also tells me to use: python setup.py sdist --manifest-only to generate a MANIFEST, but python tells me this doesn't existI appreciate these are from different versions of python and the distribution system is in a

complete mess, but assuming I am using python 3 and setuptools (the new one that includes distribute but now called setuptools, not the old setuptools that was deprecated for distribute tools only to be brought back into distribute and distribute renamed to setuptools.....)and I'm following the 'standard' folder structure and setup.py file, 

2014-07-13 23:06:24Z

The "Python Distribute" guide (was at python-distribute.org, but that registration has lapsed) tells me to include doc/txt files and .py files are excluded in MANIFEST.in fileThe sourcedist documentation tells me only sdist uses MANIFEST.in and only includes file you specify and to include .py files. It also tells me to use: python setup.py sdist --manifest-only to generate a MANIFEST, but python tells me this doesn't existI appreciate these are from different versions of python and the distribution system is in a

complete mess, but assuming I am using python 3 and setuptools (the new one that includes distribute but now called setuptools, not the old setuptools that was deprecated for distribute tools only to be brought back into distribute and distribute renamed to setuptools.....)and I'm following the 'standard' folder structure and setup.py file, No, you do not have to use MANIFEST.in. Both, distutils and setuptools are including in source

distribution package all the files mentioned in setup.py - modules, package python files,

README.txt and test/test*.py. If this is all you want to have in distribution package, you do

not have to use MANIFEST.in.If you want to manipulate (add or remove) default files to include, you have to use MANIFEST.in.The procedure is simple:For example:To test it, run python setup.py sdist, and examine the tarball created under dist/.Comparing the situation today and 2 years ago - the situation is much much better - setuptools is the way to go. You can ignore the fact, distutils is a bit broken and is low level base for setuptools as setuptools shall take care of hiding these things from you.EDIT: Last few projects I use pbr for building distribution packages with three line setup.py and rest being in setup.cfg and requirements.txt. No need to care about MANIFEST.in and other strange stuff. Even though the package would deserve a bit more documentation. See http://docs.openstack.org/developer/pbr/Old question, new answer:No, you don't need MANIFEST.in. However, to get setuptools to do what you (usually) mean, you do need to use the setuptools_scm, which takes the role of MANIFEST.in in 2 key places:The historical understanding of MANIFEST.in is: when you don't have a source control system, you need some other mechanism to distinguish between "source files" and "files that happen to be in your working directory". However, your project is under source control (right??) so there's no need for MANIFEST.in. More info in this article. 

while (1) Vs. for while(True) — Why is there a difference?

AndrewF

[while (1) Vs. for while(True) — Why is there a difference?](https://stackoverflow.com/questions/3815359/while-1-vs-for-whiletrue-why-is-there-a-difference)

Intrigued by this question about infinite loops in perl: while (1) Vs. for (;;) Is there a speed difference?, I decided to run a similar comparison in python. I expected that the compiler would generate the same byte code for while(True): pass and while(1): pass, but this is actually not the case in python2.7.The following script:produces the following results:Using while True is noticeably more complicated. Why is this?In other contexts, python acts as though True equals 1:Why does while distinguish the two?I noticed that python3 does evaluate the statements using identical operations:Is there a change in python3 to the way booleans are evaluated?

2010-09-28 17:19:45Z

Intrigued by this question about infinite loops in perl: while (1) Vs. for (;;) Is there a speed difference?, I decided to run a similar comparison in python. I expected that the compiler would generate the same byte code for while(True): pass and while(1): pass, but this is actually not the case in python2.7.The following script:produces the following results:Using while True is noticeably more complicated. Why is this?In other contexts, python acts as though True equals 1:Why does while distinguish the two?I noticed that python3 does evaluate the statements using identical operations:Is there a change in python3 to the way booleans are evaluated?In Python 2.x, True is not a keyword, but just a built-in global constant that is defined to 1 in the bool type. Therefore the interpreter still has to load the contents of True.  In other words, True is reassignable:In Python 3.x it truly becomes a keyword and a real constant:thus the interpreter can replace the while True: loop with an infinite loop.This isn't quite right, as one can still break out of the loop. But it is true that such a loop's else clause would never be accessed in Python 3. And it is also true that simplifying the value lookup makes it run just as quickly as while 1 in Python 2.Demonstrating the difference in time for a somewhat nontrivial while loop:To explain the difference, in Python 2:but in Python 3:Since True is a keyword in Python 3, the interpreter doesn't have to look up the value to see if someone replaced it with some other value. But since one can assign True to another value, the interpreter has to look it up every time.If you have a tight, long-running loop in Python 2, you probably should use while 1: instead of while True:.Use while True: if you have no condition for breaking out of your loop.This is a 7-year-old question that already has a great answer, but a misconception in the question, which isn't addressed in any of the answers, makes it potentially confusing for some of the other questions marked as duplicates.In fact, while isn't doing anything different here at all. It distinguishes 1 and True in exactly the same way that the + example does.Here's 2.7:Now compare:It's emitting a LOAD_GLOBAL (True) for each True, and there's nothing the optimizer can do with a global. So, while distinguishes 1 and True for the exact same reason that + does. (And == doesn't distinguish them because the optimizer doesn't optimize out comparisons.)Now compare 3.6:Here, it's emitting a LOAD_CONST (True) for the keyword, which the optimizer can take advantage of. So, True + 1 doesn't distinguish, for exactly the same reason while True doesn't. (And == still doesn't distinguish them because the optimizer doesn't optimize out comparisons.)Meanwhile, if the code isn't optimized out, the interpreter ends up treating True and 1 exactly the same in all three of these cases. bool is a subclass of int, and inherits most of its methods from int, and True has an internal integer value of 1. So, whether you're doing a while test (__bool__ in 3.x, __nonzero__ in 2.x), a comparison (__eq__), or arithmetic (__add__), you're calling the same method whether you use True or 1.

Should I be adding the Django migration files in the .gitignore file?

Michael Smith

[Should I be adding the Django migration files in the .gitignore file?](https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file)

Should I be adding the Django migration files in the .gitignore file? I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore. If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?

2015-01-19 23:10:31Z

Should I be adding the Django migration files in the .gitignore file? I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore. If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?Quoting from the Django migrations documentation:If you follow this process, you shouldn't be getting any merge conflicts in the migration files.When merging version control branches, you still may encounter a situation where you have multiple migrations based on the same parent migration, e.g. if to different developers introduced a migration concurrently. One way of resolving this situation is to introduce a _merge_migration_. Often this can be done automatically with the commandwhich will introduce a new migration that depends on all current head migrations. Of course this only works when there is no conflict between the head migrations, in which case you will have to resolve the problem manually.Given that some people here suggested that you shouldn't commit your migrations to version control, I'd like to expand on the reasons why you actually should do so.First, you need a record of the migrations applied to your production systems. If you deploy changes to production and want to migrate the database, you need a description of the current state. You can create a separate backup of the migrations applied to each production database, but this seems unnecessarily cumbersome.Second, migrations often contain custom, handwritten code. It's not always possible to automatically generate them with ./manage.py makemigrations.Third, migrations should be included in code review. They are significant changes to your production system, and there are lots of things that can go wrong with them.So in short, if you care about your production data, please check your migrations into version control.You can follow the below process.You can run makemigrations locally and this creates the migration file. Commit this new migration file to repo. In my opinion you should not run makemigrations in production at all. You can run migrate in production and you will see the migrations are applied from the migration file that you committed from local. This way you can avoid all conflicts.IN LOCAL ENV, to create the migration files,Now commit these newly created files, something like below.IN PRODUCTION ENV, run only the below command.Quote from the 2018 docs, Django 2.0. (two separate commands = makemigrations and migrate)https://docs.djangoproject.com/en/2.0/intro/tutorial02/Feels like you'd need to adjust your git workflow, instead of ignoring conflicts.Ideally, every new feature is developed in a different branch, and merged back with a pull request.PR cannot be merged if there's a conflict, therefore who needs to merge his feature needs to resolve the conflict, migrations included.I can't imagine why you would be getting conflicts, unless you're editing the migrations somehow? That usually ends badly - if someone misses some intermediate commits then they won't be upgrading from the correct version, and their copy of the database will be corrupted.The process that I follow is pretty simple - whenever you change the models for an app, you also commit a migration, and then that migration doesn't change - if you need something different in the model, then you change the model and commit a new migration alongside your changes.In greenfield projects, you can often delete the migrations and start over from scratch with a 0001_ migration when you release, but if you have production code, then you can't (though you can squash migrations down into one).The solution usually used, is that, before anything is merged into master, the developer must pull any remote changes. If there's a conflict in migration versions, he should rename his local migration (the remote one has been run by other devs, and, potentially, in production), to N+1.During development it might be okay to just not-commit migrations (don't add an ignore though, just don't add them). But once you've gone into production, you'll need them in order to keep the schema in sync with model changes.You then need to edit the file, and change the dependencies to the latest remote version.This works for Django migrations, as well as other similar apps (sqlalchemy+alembic, RoR, etc).Having a bunch of migration files in git is messy. There is only one file in migration folder that you should not ignore. That file is init.py file, If you ignore it, python will no longer look for submodules inside the directory, so any attempts to import the modules will fail. So the question should be how to ignore all migration files but init.py? 

The solution is: 

Add '0*.py' to .gitignore files and it does the job perfectly. Hope this helps someone. Gitignore the migrations, if You have separate DBs for Development, Staging and Production environment. For dev. purposes You can use local sqlite DB and play with migrations locally.

I would recommend You to create four additional branches:Notes:

2, 3, 4 - migrations can be kept in repos but there should be strict rules of pull requests merging, so we decided to find a person, responsible for deployments, so the only guy who has all the migration files - our deploy-er. He keeps the remote DB migrations each time we have any changes in Models.Short answer

I propose excluding migrations in the repo. After code merge, just run ./manage.py makemigrations and you are all set.Long answer

I don't think you should put migrations files into repo. It will spoil the migration states in other person's dev environment and other prod and stage environment. (refer to Sugar Tang's comment for examples). In my point of view, the purpose of Django migrations is to find gaps between previous model states and new model states, and then serialise the gap. If your model changes after code merge, you can simple do makemigrations to find out the gap. Why do you want to manually and carefully merge other migrations when you can achieve the same automatically and bug free? Django documentation says,; please keep it that way. To merge migrations manually, you have to fully understand what others have changed and any dependence of the changes. That's a lot of overhead and error prone. So tracking models file is sufficient.It is a good topic on the workflow. I am open to other options.

What is the difference between 'content' and 'text'

dotancohen

[What is the difference between 'content' and 'text'](https://stackoverflow.com/questions/17011357/what-is-the-difference-between-content-and-text)

I am using the terrific Python Requests library. I notice that the fine documentation has many examples of how to do something without explaining the why. For instance, both r.text and r.content are shown as examples of how to get the server response. But where is it explained what these properties do? For instance, when would I choose one over the other? I see thar r.text returns a unicode object sometimes, and I suppose that there would be a difference for a non-text response. But where is all this documented? Note that the linked document does state:But then it goes on to show an example of a text response! I can only suppose that the quote above means to say non-text responses instead of non-text requests, as a non-text request does not make sense in HTTP.In short, where is the proper documentation of the library, as opposed to the (excellent) tutorial on the Python Requests site?

2013-06-09 15:51:09Z

I am using the terrific Python Requests library. I notice that the fine documentation has many examples of how to do something without explaining the why. For instance, both r.text and r.content are shown as examples of how to get the server response. But where is it explained what these properties do? For instance, when would I choose one over the other? I see thar r.text returns a unicode object sometimes, and I suppose that there would be a difference for a non-text response. But where is all this documented? Note that the linked document does state:But then it goes on to show an example of a text response! I can only suppose that the quote above means to say non-text responses instead of non-text requests, as a non-text request does not make sense in HTTP.In short, where is the proper documentation of the library, as opposed to the (excellent) tutorial on the Python Requests site?The developer interface has more details:r.text is the content of the response in Unicode, and r.content is the content of the response in bytes.It seems clear from the documentation is that r.contentIf you read further down the page it addresses for example an image file

numpy max vs amax vs maximum

DilithiumMatrix

[numpy max vs amax vs maximum](https://stackoverflow.com/questions/33569668/numpy-max-vs-amax-vs-maximum)

numpy has three different functions which seem like they can be used for the same things --- except that numpy.maximum can only be used element-wise, while numpy.max and numpy.amax can be used on particular axes, or all elements.  Why is there more than just numpy.max?  Is there some subtlety to this in performance?(Similarly for min vs. amin vs. minimum)

2015-11-06 15:02:46Z

numpy has three different functions which seem like they can be used for the same things --- except that numpy.maximum can only be used element-wise, while numpy.max and numpy.amax can be used on particular axes, or all elements.  Why is there more than just numpy.max?  Is there some subtlety to this in performance?(Similarly for min vs. amin vs. minimum)np.max is just an alias for np.amax. This function only works on a single input array and finds the value of maximum element in that entire array (returning a scalar). Alternatively, it takes an axis argument and will find the maximum value along an axis of the input array (returning a new array).The default behaviour of np.maximum is to take two arrays and compute their element-wise maximum. Here, 'compatible' means that one array can be broadcast to the other. For example:But np.maximum is also a universal function which means that it has other features and methods which come in useful when working with multidimensional arrays. For example you can compute the cumulative maximum over an array (or a particular axis of the array):This is not possible with np.max.You can make np.maximum imitate np.max to a certain extent when using np.maximum.reduce:Basic testing suggests the two approaches are comparable in performance; and they should be, as np.max() actually calls np.maximum.reduce to do the computation.You've already stated why np.maximum is different - it returns an array that is the element-wise maximum between two arrays.As for np.amax and np.max: they both call the same function - np.max is just an alias for np.amax, and they compute the maximum of all elements in an array, or along an axis of an array.For completeness, in Numpy there are four maximum related functions. They fall into two different categories: NaNs propagator np.amax/np.max and its NaN ignorant counterpart np.nanmax.NaNs propagator np.maximum and its NaNs ignorant counterpart np.fmax.And finally, the same rules apply to the four minimum related functions:

How do you configure Django for simple development and deployment?

Jim

[How do you configure Django for simple development and deployment?](https://stackoverflow.com/questions/88259/how-do-you-configure-django-for-simple-development-and-deployment)

I tend to use SQLite when doing Django

development, but on a live server something more robust is

often needed (MySQL/PostgreSQL, for example).

Invariably, there are other changes to make to the Django

settings as well: different logging locations / intensities,

media paths, etc.How do you manage all these changes to make deployment a

simple, automated process?

2008-09-17 22:16:43Z

I tend to use SQLite when doing Django

development, but on a live server something more robust is

often needed (MySQL/PostgreSQL, for example).

Invariably, there are other changes to make to the Django

settings as well: different logging locations / intensities,

media paths, etc.How do you manage all these changes to make deployment a

simple, automated process?Update: django-configurations has been released which is probably a better option for most people than doing it manually.If you would prefer to do things manually, my earlier answer still applies:I have multiple settings files.I tie these all together with a settings.py file that firstly imports settings_local.py, and then one of the other two.  It decides which to load by two settings inside settings_local.py - DEVELOPMENT_HOSTS and PRODUCTION_HOSTS.  settings.py calls platform.node() to find the hostname of the machine it is running on, and then looks for that hostname in the lists, and loads the second settings file depending on which list it finds the hostname in.That way, the only thing you really need to worry about is keeping the settings_local.py file up to date with the host-specific configuration, and everything else is handled automatically.Check out an example here.Personally, I use a single settings.py for the project, I just have it look up the hostname it's on (my development machines have hostnames that start with "gabriel" so I just have this:then in other parts I have things like:and so on. A little bit less readable, but it works fine and saves having to juggle multiple settings files.At the end of settings.py I have the following:This way if I want to override default settings I need to just put settings_local.py right next to settings.py.I have two files. settings_base.py which contains common/default settings, and which is checked into source control. Each deployment has a separate settings.py, which executes from settings_base import * at the beginning and then overrides as needed.The most simplistic way I found was:1) use the default settings.py for local development and 2)

create a production-settings.py starting with:And then just override the settings that differ in production:Somewhat related, for the issue of deploying Django itself with multiple databases, you may want to take a look at Djangostack. You can download a completely free installer that allows you to install Apache, Python, Django, etc. As part of the installation process we allow you to select which database you want to use (MySQL, SQLite, PostgreSQL). We use the installers extensively when automating deployments internally (they can be run in unattended mode).I have my settings.py file in an external directory. That way, it doesn't get checked into source control, or over-written by a deploy. I put this in the settings.py file under my Django project, along with any default settings:Note: This is very dangerous if you can't trust local_settings.py. In addition to the multiple settings files mentioned by Jim, I also tend to place two settings into my settings.py file at the top BASE_DIR and BASE_URL set to the path of the code and the URL to the base of the site, all other settings are modified to append themselves to these.BASE_DIR = "/home/sean/myapp/"

e.g. MEDIA_ROOT = "%smedia/" % BASEDIRSo when moving the project I only have to edit these settings and not search the whole file.I would also recommend looking at fabric and Capistrano (Ruby tool, but it can be used to deploy Django applications) which facilitate automation of remote deployment.Well, I use this configuration:At the end of settings.py:And in locale_settings.py:I think it depends on the size of the site as to whether you need to step up from using SQLite, I've successfully used SQLite on several smaller live sites and it runs great.I use environment:I believe this is a much better approach, because eventually you need special settings for your test environment, and you can easily add it to this condition.So many complicated answers!Every settings.py file comes with :I use that directory to set the DEBUG variable like this (reaplace with the directoy where your dev code is):Then, every time the settings.py file is moved, DEBUG will be False and it's your production environment.Every time you need different settings than the ones in your dev environment just use:This is an older post but I think if I add this useful library it will simplify things.Use django-configurationThen subclass the included configurations.Configuration class in your project's settings.py or any other module you're using to store the settings constants, e.g.:Set the DJANGO_CONFIGURATION environment variable to the name of the class you just created, e.g. in ~/.bashrc:export DJANGO_CONFIGURATION=Devand the DJANGO_SETTINGS_MODULE environment variable to the module import path as usual, e.g. in bash:export DJANGO_SETTINGS_MODULE=mysite.settingsAlternatively supply the --configuration option when using Django management commands along the lines of Django's default --settings command line option, e.g.:python manage.py runserver --settings=mysite.settings --configuration=DevTo enable Django to use your configuration you now have to modify your manage.py or wsgi.py script to use django-configurations' versions of the appropriate starter functions, e.g. a typical manage.py using django-configurations would look like this:Notice in line 10 we don't use the common tool django.core.management.execute_from_command_line but instead configurations.management.execute_from_command_line.The same applies to your wsgi.py file, e.g.:Here we don't use the default django.core.wsgi.get_wsgi_application function but instead configurations.wsgi.get_wsgi_application.That's it! You can now use your project with manage.py and your favorite WSGI enabled server.In fact you should probably consider having the same (or almost the same) configs for your development and production environment. Otherwise, situations like "Hey, it works on my machine" will happen from time to time.So in order to automate your deployment and eliminate those WOMM issues, just use Docker.

What is the difference between i = i + 1 and i += 1 in a 'for' loop? [duplicate]

Adam Fjeldsted

[What is the difference between i = i + 1 and i += 1 in a 'for' loop? [duplicate]](https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop)

I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here?After running each for loop, A has not changed, but B has had one added to each element. I actually use the B version to write to a initialized NumPy array within a for loop.

2017-01-03 15:27:00Z

I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here?After running each for loop, A has not changed, but B has had one added to each element. I actually use the B version to write to a initialized NumPy array within a for loop.The difference is that one modifies the data-structure itself (in-place operation) b += 1 while the other just reassigns the variable a = a + 1.Just for completeness:x += y is not always doing an in-place operation, there are (at least) three exceptions:As it happens your bs are numpy.ndarrays which implements __iadd__ and return itself so your second loop modifies the original array in-place.You can read more on this in the Python documentation of "Emulating Numeric Types".In the first example, you are reassigning the variable a, while in the second one you are modifying the data in-place, using the += operator.See the section about 7.2.1. Augmented assignment statements

:+= operator calls __iadd__. This function makes the change in-place, and only after its execution, the result is set back to the object you are "applying" the += on.__add__ on the other hand takes the parameters and returns their sum (without modifying them).As already pointed out, b += 1 updates b in-place, while a = a + 1 computes a + 1 and then assigns the name a to the result (now a does not refer to a row of A anymore).To understand the += operator properly though, we need also to understand the concept of mutable versus immutable objects. Consider what happens when we leave out the .reshape:We see that C is not updated, meaning that c += 1 and c = c + 1 are equivalent. This is because now C is a 1D array (C.ndim == 1), and so when iterating over C, each integer element is pulled out and assigned to c.Now in Python, integers are immutable, meaning that in-place updates are not allowed, effectively transforming c += 1 into c = c + 1, where c now refers to a new integer, not coupled to C in any way. When you loop over the reshaped arrays, whole rows (np.ndarray's) are assigned to b (and a) at a time, which are mutable objects, meaning that you are allowed to stick in new integers at will, which happens when you do a += 1.It should be mentioned that though + and += are meant to be related as described above (and very much usually are), any type can implement them any way it wants by defining the __add__ and __iadd__ methods, respectively.The short form(a += 1) has the option to modify a in-place , instead of creating a new object representing the sum and rebinding it back to the same name(a = a + 1).So,The short form(a += 1) is much efficient as it doesn't necessarily need to make a copy of a unlike a = a + 1.Also even if they are outputting the same result, notice they are different because they are separate operators: + and +=First off: The variables a and b in the loops refer to numpy.ndarray objects.In the first loop, a = a + 1 is evaluated as follows: the __add__(self, other) function of numpy.ndarray is called. This creates a new object and hence, A is not modified. Afterwards, the variable a is set to refer to the result. In the second loop, no new object is created. The statement b += 1 calls  the __iadd__(self, other) function of numpy.ndarray which modifies the ndarray object in place to which b is referring to. Hence, B is modified.A key issue here is that this loop iterates over the rows (1st dimension) of B:Thus the += is acting on a mutable object, an array.This is implied in the other answers, but easily missed if your focus is on the a = a+1 reassignment.I could also make an in-place change to b with [:] indexing, or even something fancier, b[1:]=0:Of course with a 2d array like B we usually don't need to iterate on the rows.  Many operations that work on a single of B also work on the whole thing.  B += 1, B[1:] = 0, etc.

__init__ for unittest.TestCase

ffledgling

[__init__ for unittest.TestCase](https://stackoverflow.com/questions/17353213/init-for-unittest-testcase)

I'd like to add a couple of things to what the unittest.TestCase class does upon being initialized but I can't figure out how to do it.Right now I'm doing this:I'd like all the stubs to be generated only once for this entire set of tests. I can't use setUpClass() because I'm working on Python 2.4 (I haven't been able to get that working on python 2.7 either).What am I doing wrong here?I get this error:...and other errors when I move all of the stub code into __init__ when I run it with the command python -m unittest -v test.

2013-06-27 21:08:51Z

I'd like to add a couple of things to what the unittest.TestCase class does upon being initialized but I can't figure out how to do it.Right now I'm doing this:I'd like all the stubs to be generated only once for this entire set of tests. I can't use setUpClass() because I'm working on Python 2.4 (I haven't been able to get that working on python 2.7 either).What am I doing wrong here?I get this error:...and other errors when I move all of the stub code into __init__ when I run it with the command python -m unittest -v test.Try this:You are overriding the TestCase's __init__, so you might want to let the base class handle the arguments for you. Just wanted to add some clarifications about overriding the init function ofThe function will be called before each method in your test class. Please note that if you want to add some expensive computations that should be performed once before running all test methods please use the SetUpClass classmethodThis function will be called once before all test methods of the class. See setUp for a method that is called before each test method.Install unittest2 and use that package's unittest.and then use the setupModule / tearDownModule or setupClass / tearDown class 

for special initialization logicMore info: http://www.voidspace.org.uk/python/articles/unittest2.shtml Also most likely your are creating an integration test more than an unittest. 

Choose a good name for the Tests to differentiate them or put in a different container module.

Remove an item from a dictionary when its key is unknown

Buttons840

[Remove an item from a dictionary when its key is unknown](https://stackoverflow.com/questions/5447494/remove-an-item-from-a-dictionary-when-its-key-is-unknown)

What is the best way to remove an item from a dictionary by value, i.e. when the item's key is unknown?  Here's a simple approach:Are there better ways?  Is there anything wrong with mutating (deleting items) from the dictionary while iterating it?

2011-03-27 05:47:07Z

What is the best way to remove an item from a dictionary by value, i.e. when the item's key is unknown?  Here's a simple approach:Are there better ways?  Is there anything wrong with mutating (deleting items) from the dictionary while iterating it?Be aware that you're currently testing for object identity (is only returns True if both operands are represented by the same object in memory - this is not always the case with two object that compare equal with ==). If you are doing this on purpose, then you could rewrite your code asBut this may not do what you want:So you probably want != instead of is not.The dict.pop(key[, default]) method allows you to remove items when you know the key. It returns the value at the key if it removes the item otherwise it returns what is passed as default. See the docs.'Example:A simple comparison between del and pop():result: So, del is faster than pop(). items() returns a list, and it is that list you are iterating, so mutating the dict in the loop doesn't matter here. If you were using iteritems() instead, mutating the dict in the loop would be problematic, and likewise for viewitems() in Python 2.7.I can't think of a better way to remove items from a dict by value.I'd build a list of keys that need removing, then remove them. It's simple, efficient and avoids any problem about simultaneously iterating over and mutating the dict.c is the new dictionary, and a is your original dictionary, {'z','w'}

are the keys you want to remove from aAlso check: https://www.safaribooksonline.com/library/view/python-cookbook-3rd/9781449357337/ch01.htmlThere is nothing wrong with deleting items from the dictionary while iterating, as you've proposed.  Be careful about multiple threads using the same dictionary at the same time, which may result in a KeyError or other problems.Of course, see the docs at http://docs.python.org/library/stdtypes.html#typesmappingThis is how I would do it.

「RuntimeError: Make sure the Graphviz executables are on your system's path」after installing Graphviz 2.38

liga810

[「RuntimeError: Make sure the Graphviz executables are on your system's path」after installing Graphviz 2.38](https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft)

I downloaded Graphviz 2.38 MSI version and installed under folder C:\Python34, then I run pip install Graphviz, everything went well. In system's path I added C:\Python34\bin. When I tried to run a test script, in line filename=dot.render(filename='test'), I got a messageI tried to put "C:\Python34\bin\dot.exe" in system's path, but it didn't work, and I even created a new environment variable "GRAPHVIZ_DOT" with value "C:\Python34\bin\dot.exe", still not working. I tried to uninstall Graphviz and pip uninstall graphviz, then reinstall it and pip install again, but nothing works. The whole traceback message is:Does anybody have any experience with it?  

2016-01-28 14:35:00Z

I downloaded Graphviz 2.38 MSI version and installed under folder C:\Python34, then I run pip install Graphviz, everything went well. In system's path I added C:\Python34\bin. When I tried to run a test script, in line filename=dot.render(filename='test'), I got a messageI tried to put "C:\Python34\bin\dot.exe" in system's path, but it didn't work, and I even created a new environment variable "GRAPHVIZ_DOT" with value "C:\Python34\bin\dot.exe", still not working. I tried to uninstall Graphviz and pip uninstall graphviz, then reinstall it and pip install again, but nothing works. The whole traceback message is:Does anybody have any experience with it?  In windows just add these 2 lines in the beginning, where 'D:/Program Files (x86)/Graphviz2.38/bin/' is replaced by the address of where your bin file is.That solves the problem.You should install the graphviz package in your system (not just the python package). On Ubuntu you should try:This one solved the problem for me on MAC:For Windows:This worked for me!Try using:conda install python-graphvizThe graphviz executable sit on a different path from your conda directory, if you use pip install graphviz.OSX Sierra, Python 2.7, Graphviz 2.38Using pip install graphviz and conda install graphviz BOTH resolves the problem.pip only gets path problem same as yours and conda only gets import error.Just add below to your Environmental Variable(system) PATH

on WindowsIf not work Find Graphviz2.38/bin folder in your Program Files not in python libThen, add to your PATHIt's important to find a folder where .exe files existTry conda install graphviz. I had the same problem, I resolved it by mentioned command in MacOS.Step 1: Install Graphviz binaryStep 2: Install graphviz module for pythonUsing conda install graphviz and conda install python-graphviz to install GraphViz on Windows10 the path needed was C:/ProgramData/Anaconda3/Library/bin/graphviz/ for me. I.e. adding solved the issue for me.For Windows, install the Python Graphviz which will include the executables in the path.On Ubuntu Linux this solved it for me:You could also try conda install -c conda-forge graphviz instead of pip if using Anaconda.After you've installed the package (link if you haven't), add the path to dot.exe as a new system variable.Default path is:1)  Graphviz – download unzip in a particular place in the system (pip does not work in windows ) and include  the bin folder in  the path (‘set environment variables in windows’ OR)   set manually in each program 2)  Then put the model to plotI had the same issue on Linux with Jupyter.To solve it I've added the dot library to python sys.path First:  check if dot is installed, Then:

find his path whereis dot -> /local/notebook/miniconda2/envs/ik2/bin/dotFinally in python script : 

sys.path.append("/local/notebook/miniconda2/envs/ik2/bin/dot")I had the same error message on Mac OS (El Capitan), using the PyCharm IDE.

I had installed Graphviz using brew, as recommended in RZK's answer, and installed the graphviz python package using PyCharm (I could check Graphviz was installed correctly by trying dot -V in a terminal and getting: dot - graphviz version 2.40.1 (20161225.0304)).

Yet I was still getting the error message when trying to call Graphviz from PyCharm.I had to add the path /usr/local/bin in PyCharm options, as recommended in the answer to this question to resolve the problem.First, you should use pip install, and then download another package in http://www.graphviz.org/Download_windows.php

and add the install location into the environmental path, then it works.For Linux users who don't have root access and hence can't use sudo command as suggested in other answers...First, activate your conda virtual-environment (if you want to use one) by:Then install graphviz, even if you have already done it using pip:then copy the result of the following command:In my case, its output is:and add it to your PATH variable. Just run the command belowand add these lines to the end of the opened file:now press Ctrl+O and then Ctrl+X to save and exit.Problem should be solved by now.Pycharm users, please note: Pycharm does not always see the PATH variable the same as your terminal. This solution does not work for Pycharm, and maybe other IDEs. But you can fix this by adding this line of code:to your python program. Do not forget to first :)Edit: If you don't want to use conda, you can still install graphviz from here without any root permissions and add the bin folder to your PATH variable. I didn't test this.1.install windows package from: https://graphviz.gitlab.io/_pages/Download/Download_windows.html and download msi fileAdd in Environmental variables 

2. Add C:\Program Files (x86)\Graphviz2.38\bin to User pathIt will work.When solving this issue for myself, I used this GitHub tutorial, which analysed the cause of this issue. If we read in between the lines, it says it needs system as well as python graph viz. In addition to conda install, we would need to run:Then restart the kernel; it works like a charm.I'm on macOS Catalina 10.15.3, and I had a similar error: ExecutableNotFound: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' PATHFixed it with:pip3 install graphviz AND brew install graphviz   Note the pip3 install will only return the success message Successfully installed graphviz-0.13.2 so we still need to run brew install to get graphviz 2.42.3 (as of 10 Mar 2020, 6PM). OS Mojave 10.14., Python 3.6Using pip install graphviz had good feedback in terminal, but lead to this error when I tried to make a graph in a Jupyter notebook. I then ran brew install graphviz, which gave an error in terminal. Then I ran conda install graphviz and the graph worked.From @Leighton's comment: pip only gets path problem same as yours and conda only gets import error.This solved the PATH issue on MAC for me!If you are not using Conda but vanilla Python, 'brew install graphviz' works.This is showing some path issue: So this worked for me:trying doing this in python

import sys

!conda install --yes --prefix {sys.prefix} graphviz

import graphviz 

Removing multiple keys from a dictionary safely

dublintech

[Removing multiple keys from a dictionary safely](https://stackoverflow.com/questions/8995611/removing-multiple-keys-from-a-dictionary-safely)

I know to remove an entry, 'key' from my dictionary d, safely, you do:However, I need to remove multiple entries from dictionary safely.  I was thinking of defining the entries in a tuple as I will need to do this more than once.However, I was wondering if there is a smarter way to do this?

2012-01-24 23:05:51Z

I know to remove an entry, 'key' from my dictionary d, safely, you do:However, I need to remove multiple entries from dictionary safely.  I was thinking of defining the entries in a tuple as I will need to do this more than once.However, I was wondering if there is a smarter way to do this?Why not like this:A more compact version was provided by mattbornski using dict.pop()Using Dict Comprehensionswhere key1 and key2 are to be removed.In the example below, keys "b" and "c" are to be removed & it's kept in a keys list.a solution is using map and filter functionspython 2python 3you get:If you also needed to retrieve the values for the keys you are removing, this would be a pretty good way to do it:You could of course still do this just for the removal of the keys from d, but you would be unnecessarily creating the list of values with the list comprehension.  It is also a little unclear to use a list comprehension just for the function's side effect.Found a solution with pop and mapThe output of this:I have answered this question so late just because I think it will help in the future if anyone searches the same. And this might help.UpdateThe above code will throw an error if a key does not exist in the dict.output:I have no problem with any of the existing answers, but I was surprised to not find this solution:Note: I stumbled across this question coming from here. And my answer is related to this answer.Why not:I don't know what you mean by "smarter way". Surely there are other ways, maybe with dictionary comprehensions:inlineI think using the fact that the keys can be treated as a set is the nicest way if you're on python 3:Example:I'm late to this discussion but for anyone else. A solution may be to create a list of keys as such.Then use pop() in a list comprehension, or for loop, to iterate over the keys and pop one at a time as such.The 'n/a' is in case the key does not exist, a default value needs to be returned.

How do I count unique values inside a list

Joel Aqu.

[How do I count unique values inside a list](https://stackoverflow.com/questions/12282232/how-do-i-count-unique-values-inside-a-list)

So I'm trying to make this program that will ask the user for input and store the values in an array / list.

Then when a blank line is entered it will tell the user how many of those values are unique.

I'm building this for real life reasons and not as a problem set.My code is as follows:..and that's about all I've gotten so far.

I'm not sure how to count the unique number of words in a list?

If someone can post the solution so I can learn from it, or at least show me how it would be great, thanks!

2012-09-05 13:11:47Z

So I'm trying to make this program that will ask the user for input and store the values in an array / list.

Then when a blank line is entered it will tell the user how many of those values are unique.

I'm building this for real life reasons and not as a problem set.My code is as follows:..and that's about all I've gotten so far.

I'm not sure how to count the unique number of words in a list?

If someone can post the solution so I can learn from it, or at least show me how it would be great, thanks!In addition, use collections.Counter to refactor your code:

Output:You can use a set to remove duplicates, and then the len function to count the elements in the set:values, counts = np.unique(words, return_counts=True)Use a set:Armed with this, your solution could be as simple as:For ndarray there is a numpy method called unique:Examples:For a Series there is a function call value_counts():Although a set is the easiest way, you could also use a dict and use some_dict.has(key) to populate a dictionary with only unique keys and values.Assuming you have already populated words[] with input from the user, create a dict mapping the unique words in the list to a number:Other method by using pandasYou can then export results in any format you wantThe following should work. The lambda function filter out the duplicated words.I'd use a set myself, but here's yet another way:

How to write to an existing excel file without overwriting data (using pandas)?

BP_

[How to write to an existing excel file without overwriting data (using pandas)?](https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas)

I use pandas to write to excel file in the following fashion:Masterfile.xlsx already consists of number of different tabs. However, it does not yet contain "Main".Pandas correctly writes to "Main" sheet, unfortunately it also deletes all other tabs.

2013-11-26 14:05:22Z

I use pandas to write to excel file in the following fashion:Masterfile.xlsx already consists of number of different tabs. However, it does not yet contain "Main".Pandas correctly writes to "Main" sheet, unfortunately it also deletes all other tabs.Pandas docs says it uses openpyxl for xlsx files. Quick look through the code in ExcelWriter gives a clue that something like this might work out:Here is a helper function:NOTE: for Pandas < 0.21.0, replace sheet_name with sheetname!Usage examples:With openpyxlversion 2.4.0 and pandasversion 0.19.2, the process @ski came up with gets a bit simpler:Starting in pandas 0.24 you can simplify this with the mode keyword argument of ExcelWriter:Old question, but I am guessing some people still search for this - so...I find this method nice because all worksheets are loaded into a dictionary of sheet name and dataframe pairs, created by pandas with the sheetname=None option.  It is simple to add, delete or modify worksheets between reading the spreadsheet into the dict format and writing it back from the dict.  For me the xlsxwriter works better than openpyxl for this particular task in terms of speed and format.Note: future versions of pandas (0.21.0+) will change the "sheetname" parameter to "sheet_name".For the example in the 2013 question:I know this is an older thread, but this is the first item you find when searching, and the above solutions don't work if you need to retain charts in a workbook that you already have created. In that case, xlwings is a better option - it allows you to write to the excel book and keeps the charts/chart data.simple example:There is a better solution in pandas 0.24:before:after:so upgrade your pandas now:This works perfectly fine only thing is that formatting of the master file(file to which we add new sheet) is lost.The "keep_date_col" hope help you

Find the nth occurrence of substring in a string

prestomation

[Find the nth occurrence of substring in a string](https://stackoverflow.com/questions/1883980/find-the-nth-occurrence-of-substring-in-a-string)

This seems like it should be pretty trivial, but I am new at Python and want to do it the most Pythonic way.I want to find the n'th occurrence of a substring in a string.There's got to be something equivalent to what I WANT to do which is mystring.find("substring", 2nd)How can you achieve this in Python?

2009-12-10 20:58:50Z

This seems like it should be pretty trivial, but I am new at Python and want to do it the most Pythonic way.I want to find the n'th occurrence of a substring in a string.There's got to be something equivalent to what I WANT to do which is mystring.find("substring", 2nd)How can you achieve this in Python?Mark's iterative approach would be the usual way, I think.Here's an alternative with string-splitting, which can often be useful for finding-related processes:And here's a quick (and somewhat dirty, in that you have to choose some chaff that can't match the needle) one-liner:Here's a more Pythonic version of the straightforward iterative solution:Example:If you want to find the nth overlapping occurrence of needle, you can increment by 1 instead of len(needle), like this:Example:This is easier to read than Mark's version, and it doesn't require the extra memory of the splitting version or importing regular expression module.  It also adheres to a few of the rules in the Zen of python, unlike the various re approaches:This will find the second occurrence of substring in string.Edit: I haven't thought much about the performance, but a quick recursion can help with finding the nth occurrence:Understanding that regex is not always the best solution, I'd probably use one here:I'm offering some benchmarking results comparing the most prominent approaches presented so far, namely @bobince's findnth() (based on str.split()) vs. @tgamblin's or @Mark Byers' find_nth() (based on str.find()). I will also compare with a C extension (_find_nth.so) to see how fast we can go. Here is find_nth.py: Of course, performance matters most if the string is large, so suppose we want to find the 1000001st newline ('\n') in a 1.3 GB file called 'bigfile'. To save memory, we would like to work on an mmap.mmap object representation of the file:There is already the first problem with findnth(), since mmap.mmap objects don't support split(). So we actually have to copy the whole file into memory:Ouch! Fortunately s still fits in the 4 GB of memory of my Macbook Air, so let's benchmark findnth():Clearly a terrible performance. Let's see how the approach based on str.find() does:Much better! Clearly, findnth()'s problem is that it is forced to copy the string during split(), which is already the second time we copied the 1.3 GB of data around after s = mm[:]. Here comes in the second advantage of find_nth(): We can use it on mm directly, such that zero copies of the file are required:There appears to be a small performance penalty operating on mm vs. s, but this illustrates that find_nth() can get us an answer in 1.2 s compared to findnth's total of 47 s.I found no cases where the str.find() based approach was significantly worse than the str.split() based approach, so at this point, I would argue that @tgamblin's or @Mark Byers' answer should be accepted instead of @bobince's.In my testing, the version of find_nth() above was the fastest pure Python solution I could come up with (very similar to @Mark Byers' version). Let's see how much better we can do with a C extension module. Here is _find_nthmodule.c:Here is the setup.py file:Install as usual with python setup.py install. The C code plays at an advantage here since it is limited to finding single characters, but let's see how fast this is:Clearly quite a bit faster still. Interestingly, there is no difference on the C level between the in-memory and mmapped cases. It is also interesting to see that _find_nth2(), which is based on string.h's memchr() library function, loses out against the straightforward implementation in _find_nth(): The additional "optimizations" in memchr() are apparently backfiring...In conclusion, the implementation in findnth() (based on str.split()) is really a bad idea, since (a) it performs terribly for larger strings due to the required copying, and (b) 

it doesn't work on mmap.mmap objects at all. The implementation in find_nth() (based on str.find()) should be preferred in all circumstances (and therefore be the accepted answer to this question).There is still quite a bit of room for improvement, since the C extension ran almost a factor of 4 faster than the pure Python code, indicating that there might be a case for a dedicated Python library function.I'd probably do something like this, using the find function that takes an index parameter:It's not particularly Pythonic I guess, but it's simple. You could do it using recursion instead:It's a functional way to solve it, but I don't know if that makes it more Pythonic.Simplest way?This will give you an array of the starting indices for matches to yourstring:Then your nth entry would be:Of course you have to be careful with the index bounds. You can get the number of instances of yourstring like this:Here's another re + itertools version that should work when searching for either a str or a RegexpObject. I will freely admit that this is likely over-engineered, but for some reason it entertained me.Building on modle13's answer, but without the re module dependency.I kinda wish this was a builtin string method.Here is another approach using re.finditer.

The difference is that this only looks into the haystack as far as necessarySolution without using loops and recursion.The replace one liner is great but only works because XX and bar have the same lentghA good and general def would be:Providing another "tricky" solution, which use split and join.In your example, we can useThis is the answer you really want:Here is my solution for finding nth occurrance of b in string a:It is pure Python and iterative. For 0 or n that is too large, it returns -1. It is one-liner and can be used directly. Here is an example:For the special case where you search for the n'th occurence of a character (i.e. substring of length 1), the following function works by building a list of all positions of occurences of the given character:If there are fewer than n occurences of the given character, it will give IndexError: list index out of range.This is derived from @Zv_oDD's answer and simplified for the case of a single character.Def:To use:Output:How about:

How does numpy.histogram() work?

Aufwind

[How does numpy.histogram() work?](https://stackoverflow.com/questions/9141732/how-does-numpy-histogram-work)

While reading up on numpy, I encountered the function numpy.histogram().What is it for and how does it work? In the docs they mention bins: What are they?Some googling led me to the definition of Histograms in general. I get that. But unfortunately I can't link this knowledge to the examples given in the docs.

2012-02-04 14:56:40Z

While reading up on numpy, I encountered the function numpy.histogram().What is it for and how does it work? In the docs they mention bins: What are they?Some googling led me to the definition of Histograms in general. I get that. But unfortunately I can't link this knowledge to the examples given in the docs.A bin is range that represents the width of a single bar of the histogram along the X-axis. You could also call this the interval. (Wikipedia defines them more formally as "disjoint categories".)The Numpy histogram function doesn't draw the histogram, but it computes the occurrences of input data that fall within each bin, which in turns determines the area (not necessarily the height if the bins aren't of equal width) of each bar.In this example:There are 3 bins, for values ranging from 0 to 1 (excl 1.), 1 to 2 (excl. 2) and 2 to 3 (incl. 3), respectively. The way Numpy defines these bins if by giving a list of delimiters ([0, 1, 2, 3]) in this example, although it also returns the bins in the results, since it can choose them automatically from the input, if none are specified. If bins=5, for example, it will use 5 bins of equal width spread between the minimum input value and the maximum input value.The input values are 1, 2 and 1. Therefore, bin "1 to 2" contains two occurrences (the two 1 values), and bin "2 to 3" contains one occurrence (the 2). These results are in the first item in the returned tuple: array([0, 2, 1]).Since the bins here are of equal width, you can use the number of occurrences for the height of each bar. When drawn, you would have:You can plot this directly with Matplotlib (its hist function also returns the bins and the values):Below, hist indicates that there are 0 items in bin #0, 2 in bin #1, 4 in bin #3, 1 in bin #4.bin_edges indicates that bin #0 is the interval [0,1), bin #1 is [1,2), ...,

bin #3 is [3,4).Play with the above code, change the input to np.histogram and see how it works.But a picture is worth a thousand words:Another useful thing to do with numpy.histogram is to plot the output as the x and y coordinates on a linegraph. For example:This can be a useful way to visualize histograms where you would like a higher level of granularity without bars everywhere. Very useful in image histograms for identifying extreme pixel values.

How to write a multidimensional array to a text file?

Ivo Flipse

[How to write a multidimensional array to a text file?](https://stackoverflow.com/questions/3685265/how-to-write-a-multidimensional-array-to-a-text-file)

In another question, other users offered some help if I could supply the array I was having trouble with. However, I even fail at a basic I/O task, such as writing an array to a file.Can anyone explain what kind of loop I would need to write a 4x11x14 numpy array to file?This array consist of four 11 x 14 arrays, so I should format it with a nice newline, to make the reading of the file easier on others.Edit: So I've tried the numpy.savetxt function. Strangely, it gives the following error:I assume that this is because the function doesn't work with multidimensional arrays? Any solutions as I would like them within one file?

2010-09-10 14:13:21Z

In another question, other users offered some help if I could supply the array I was having trouble with. However, I even fail at a basic I/O task, such as writing an array to a file.Can anyone explain what kind of loop I would need to write a 4x11x14 numpy array to file?This array consist of four 11 x 14 arrays, so I should format it with a nice newline, to make the reading of the file easier on others.Edit: So I've tried the numpy.savetxt function. Strangely, it gives the following error:I assume that this is because the function doesn't work with multidimensional arrays? Any solutions as I would like them within one file?If you want to write it to disk so that it will be easy to read back in as a numpy array, look into numpy.save.  Pickling it will work fine, as well, but it's less efficient for large arrays (which yours isn't, so either is perfectly fine).If you want it to be human readable, look into numpy.savetxt.Edit:  So, it seems like savetxt isn't quite as great an option for arrays with >2 dimensions... But just to draw everything out to it's full conclusion:I just realized that numpy.savetxt chokes on ndarrays with more than 2 dimensions... This is probably by design, as there's no inherently defined way to indicate additional dimensions in a text file.E.g. This (a 2D array) works fineWhile the same thing would fail (with a rather uninformative error: TypeError: float argument required, not numpy.ndarray) for a 3D array:One workaround is just to break the 3D (or greater) array into 2D slices. E.g.However, our goal is to be clearly human readable, while still being easily read back in with numpy.loadtxt. Therefore, we can be a bit more verbose, and differentiate the slices using commented out lines. By default, numpy.loadtxt will ignore any lines that start with # (or whichever character is specified by the comments kwarg).  (This looks more verbose than it actually is...)This yields:Reading it back in is very easy, as long as we know the shape of the original array. We can just do numpy.loadtxt('test.txt').reshape((4,5,10)).  As an example (You can do this in one line, I'm just being verbose to clarify things):I am not certain if this meets your requirements, given I think you are interested in making the file readable by people, but if that's not a primary concern, just pickle it.To save it:To read it back:If you don't need a human-readable output, another option you could try is to save the array as a MATLAB .mat file, which is a structured array. I despise MATLAB, but the fact that I can both read and write a .mat in very few lines is convenient. Unlike Joe Kington's answer, the benefit of this is that you don't need to know the original shape of the data in the .mat file, i.e. no need to reshape upon reading in. And, unlike using pickle, a .mat file can be read by MATLAB, and probably some other programs/languages as well. Here is an example:If you forget the key that the array is named in the .mat file, you can always do:And of course you can store many arrays using many more keys.So yes – it won't be readable with your eyes, but only takes 2 lines to write and read the data, which I think is a fair trade-off.Take a look at the docs for scipy.io.savemat

and scipy.io.loadmat

and also this tutorial page: scipy.io File IO Tutorialndarray.tofile() should also worke.g. if your array is called a:Not sure how to get newline formatting though.Edit (credit Kevin J. Black's comment here):There exist special libraries to do just that. (Plus wrappers for python)hope this helpsYou can simply traverse the array in three nested loops and write their values to your file. For reading, you simply use the same exact loop construction. You will get the values in exactly the right order to fill your arrays correctly again.I have a way to do it using a simply filename.write() operation. It works fine for me, but I'm dealing with arrays having ~1500 data elements. I basically just have for loops to iterate through the file and write it to the output destination line-by-line in a csv style output. The if and elif statement are used to add commas between the data elements. For whatever reason, these get stripped out when reading the file in as an nd array. My goal was to output the file as a csv, so this method helps to handle that.Hope this helps!Pickle is best for these cases. Suppose you have a ndarray named x_train. You can dump it into a file and revert it back using the following command:

How to form tuple column from two columns in Pandas

elksie5000

[How to form tuple column from two columns in Pandas](https://stackoverflow.com/questions/16031056/how-to-form-tuple-column-from-two-columns-in-pandas)

I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.The code I tried to use was:However, this returned the following error:...How can I solve this problem?

2013-04-16 07:21:52Z

I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.The code I tried to use was:However, this returned the following error:...How can I solve this problem?Get comfortable with zip. It comes in handy when dealing with column data. It's less complicated and faster than using apply or map. Something like np.dstack is twice as fast as zip, but wouldn't give you tuples.Pandas has the itertuples method to do exactly this:I'd like to add df.values.tolist(). (as long as you don't mind to get a column of lists rather than tuples)

How to install PyQt4 on Windows using pip?

Anna

[How to install PyQt4 on Windows using pip?](https://stackoverflow.com/questions/22640640/how-to-install-pyqt4-on-windows-using-pip)

I'm using Python 3.4 on Windows. When I run a script, it complainsSo I tried to install it, but pip install PyQt4 givesalthough it does show up when I run pip search PyQt4. I tried to pip install python-qt, which installed successfully but that didn't solve the problem.What am I doing wrong?

2014-03-25 16:26:07Z

I'm using Python 3.4 on Windows. When I run a script, it complainsSo I tried to install it, but pip install PyQt4 givesalthough it does show up when I run pip search PyQt4. I tried to pip install python-qt, which installed successfully but that didn't solve the problem.What am I doing wrong?Here are Windows wheel packages built by Chris Golke - Python Windows Binary packages - PyQtIn the filenames cp27 means C-python version 2.7, cp35 means python 3.5, etc.Since Qt is a more complicated system with a compiled C++ codebase underlying the python interface it provides you, it can be more complex to build than just a pure python code package, which means it can be hard to install it from source.Make sure you grab the correct Windows wheel file (python version, 32/64 bit), and then use pip to install it - e.g:Should properly install if you are running an x64 build of Python 3.5.QT no longer supports PyQt4, but you can install PyQt5 with pip:You can't use pip. You have to download from the Riverbank website and run the installer for your version of python. If there is no install for your version, you will have to install Python for one of the available installers, or build from source (which is rather involved). Other answers and comments have the links. If you install PyQt4 on Windows, files wind up here by default:but it also leaves a file here:If you copy the both the sip.pyd and PyQt4 folder into your virtualenv things will work fine.For example:Then with windows explorer copy from C:\Python27\Lib\site-packages the file (sip.pyd) and folder (PyQt4) mentioned above to C:\code\BACKUP\Lib\site-packages\Then back at CLI:The problem with trying to launch a script which calls PyQt4 from within virtualenv is that the virtualenv does not have PyQt4 installed and it doesn't know how to reference the default installation described above. But follow these steps to copy PyQt4 into your virtualenv and things should work great.Earlier PyQt .exe installers were available directly from the website download page. Now with the release of PyQt4.12 , installers have been deprecated. You can make the libraries work somehow by compiling them but that would mean going to great lengths of trouble.Otherwise you can use the previous distributions to solve your purpose. The .exe windows installers can be downloaded from :It looks like you may have to do a bit of manual installation for PyQt4.http://pyqt.sourceforge.net/Docs/PyQt4/installation.htmlThis might help a bit more, it's a bit more in a tutorial/set-by-step format:http://movingthelamppost.com/blog/html/2013/07/12/installing_pyqt____because_it_s_too_good_for_pip_or_easy_install_.htmlWith current latest python 3.6.5 works fineTry this for PyQt5:Use the operating system on this link for PyQt4.Or download the supported wheel for your platform on this link.Else use this link for the windows executable installer.

Hopefully this helps you to install either PyQt4 or PyQt5.download the appropriate version of the PyQt4 from here:and install it using pip (example for Python3.6 - 64bit)install PyQt5 for Windows 10 and python 3.5+.pip install PyQt5If you have error while installing PyQt4.My system type is 64 bit, But to solve this error I have installed PyQt4 of 32 bit windows system, i.e PyQt4-4.11.4-cp27-cp27m-win32.whl - click here to see more versions.Kindly select appropriate version of PyQt4 according to your installed python version.You can also use this command to install PyQt5.I am using PyCharm, and was able to install PyQt5.PyQt4, as well as PyQt4Enhanced and windows_whl both failed to install, I'm guessing that's because Qt4 is no longer supported.

Format numbers to strings in Python

David Ackerman

[Format numbers to strings in Python](https://stackoverflow.com/questions/22617/format-numbers-to-strings-in-python)

I need to find out how to format numbers as strings. My code is here:Hours and minutes are integers, and seconds is a float.  the str() function will convert all of these numbers to the tenths (0.1) place.  So instead of my string outputting "5:30:59.07 pm", it would display something like "5.0:30.0:59.1 pm".Bottom line, what library / function do I need to do this for me?

2008-08-22 15:10:46Z

I need to find out how to format numbers as strings. My code is here:Hours and minutes are integers, and seconds is a float.  the str() function will convert all of these numbers to the tenths (0.1) place.  So instead of my string outputting "5:30:59.07 pm", it would display something like "5.0:30.0:59.1 pm".Bottom line, what library / function do I need to do this for me?Starting with Python 3.6, formatting in Python can be done using formatted string literals or f-strings:or the str.format function starting with 2.7:or the string formatting % operator for even older versions of Python, but see the note in the docs:And for your specific case of formatting time, there’s time.strftime:Starting in Python 2.6, there is an alternative: the str.format() method. Here are some examples using the existing string format operator (%):Here are the equivalent snippets but using str.format():Like Python 2.6+, all Python 3 releases (so far) understand how to do both. I shamelessly ripped this stuff straight out of my hardcore Python intro book and the slides for the Intro+Intermediate Python courses I offer from time-to-time. :-)Aug 2018 UPDATE: Of course, now that we have the f-string feature in 3.6, we need the equivalent examples of that, yes another alternative:It is possible to use the format() function, so in your case you can use:There are multiple ways of using this function, so for further information you can check the documentation.f-strings is a new feature that has been added to the language in Python 3.6. This facilitates formatting strings notoriously:You can use C style string formatting:See here, especially: https://web.archive.org/web/20120415173443/http://diveintopython3.ep.io/strings.htmlYou can use following to achieve desired functionalityYou can use the str.format() to make Python recognize any objects to strings.str() in python on an integer will not print any decimal places.If you have a float that you want to ignore the decimal part, then you can use str(int(floatValue)).Perhaps the following code will demonstrate:If you have a value that includes a decimal, but the decimal value is negligible (ie: 100.0) and try to int that, you will get an error.  It seems silly, but calling float first fixes this.str(int(float([variable])))

Invalid syntax when using「print」? [duplicate]

Lonnie Price

[Invalid syntax when using「print」? [duplicate]](https://stackoverflow.com/questions/937491/invalid-syntax-when-using-print)

I'm learning Python and can't even write the first example:this gives SyntaxError: invalid syntaxpointing at the 2.Why is this? I'm using version 3.1

2009-06-02 00:51:52Z

I'm learning Python and can't even write the first example:this gives SyntaxError: invalid syntaxpointing at the 2.Why is this? I'm using version 3.1That is because in Python 3, they have replaced the print statement with the print function.The syntax is now more or less the same as before, but it requires parens:From the "what's new in python 3" docs:You need parentheses:They changed print in Python 3.  In 2 it was a statement, now it is a function and requires parenthesis.Here's the docs from Python 3.0.The syntax is changed in new 3.x releases rather than old 2.x releases:

for example in python 2.x you can write:

print "Hi new world"

but in the new 3.x release you need to use the new syntax and write it like this:

print("Hi new world")check the documentation:

http://docs.python.org/3.3/library/functions.html?highlight=print#print

Replacing Pandas or Numpy Nan with a None to use with MysqlDB

Rishi

[Replacing Pandas or Numpy Nan with a None to use with MysqlDB](https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb)

I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.Any ideas? 

2013-01-04 18:26:06Z

I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.Any ideas? @bogatron has it right, you can use where, it's worth noting that you can do this natively in pandas:Note: this changes the dtype of all columns to object.Example:Note: what you cannot do recast the DataFrames dtype to allow all datatypes types, using astype, and then the DataFrame fillna method:Unfortunately neither this, nor using replace, works with None see this (closed) issue.As an aside, it's worth noting that for most use cases you don't need to replace NaN with None, see this question about the difference between NaN and None in pandas.However, in this specific case it seems you do (at least at the time of this answer).Credit goes to this guy here on Github issue.You can replace nan with None in your numpy array:After stumbling around, this worked for me:Another addition: be careful when replacing multiples and converting the type of the column back from object to float. If you want to be certain that your None's won't flip back to np.NaN's apply @andy-hayden's suggestion with using pd.where.

Illustration of how replace can still go 'wrong':Quite old, yet I stumbled upon the very same issue.

Try doing this:Just an addition to @Andy Hayden's answer:Since DataFrame.mask is the opposite twin of DataFrame.where, they have the exactly same signature but with opposite meaning:So in this question, using df.mask(df.isna(), other=None, inplace=True) might be more intuitive.

Random number between 0 and 1 in python [duplicate]

Talia 

[Random number between 0 and 1 in python [duplicate]](https://stackoverflow.com/questions/33359740/random-number-between-0-and-1-in-python)

I want a random number between 0 and 1, like 0.3452. I used random.randrange(0, 1) but it is always 0 for me. What should I do?

2015-10-27 03:59:10Z

I want a random number between 0 and 1, like 0.3452. I used random.randrange(0, 1) but it is always 0 for me. What should I do?You can use random.uniformrandom.random() does exactly thatIf you want really random numbers, and to cover the range [0, 1]:random.random() is what you are looking for:And, btw, Why your try didn't work?:Your try was: random.randrange(0, 1)So, what you are doing here, with random.randrange(a,b) is choosing a random element from range(a,b); in your case, from range(0,1), but, guess what!: the only element in range(0,1), is 0, so, the only element you can choose from range(0,1), is 0; that's why you were always getting 0 back. you can use use numpy.random module, you can get array of random number in shape of your choice you wantrandom.randrange(0,2) this works!RTMFrom the docs for the Python random module:That explains why it only gives you 0, doesn't it.  range(0,1) is [0].  It is choosing from a list consisting of only that value. Also from those docs:But if your inclusion of the numpy tag is intentional, you can generate many random floats in that range with one call using a np.random function.My variation that I find to be more flexible.

How to get the index of a maximum element in a numpy array along one axis

Peter Smit

[How to get the index of a maximum element in a numpy array along one axis](https://stackoverflow.com/questions/5469286/how-to-get-the-index-of-a-maximum-element-in-a-numpy-array-along-one-axis)

I have a 2 dimensional NumPy array. I know how to get the maximum values over axes:How can I get the indices of the maximum elements? So I would like as output array([1,1,0])

2011-03-29 07:35:46Z

I have a 2 dimensional NumPy array. I know how to get the maximum values over axes:How can I get the indices of the maximum elements? So I would like as output array([1,1,0])argmax() will only return the first occurrence for each row.

http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.htmlIf you ever need to do this for a shaped array, this works better than unravel:You can also change your conditions:The above gives you results in the form that you asked for. Alternatively, you can convert to a list of x,y coordinates by:

「for loop」with two variables? [duplicate]

Quester

[「for loop」with two variables? [duplicate]](https://stackoverflow.com/questions/18648626/for-loop-with-two-variables)

How can I include two variables in the same for loop?

2013-09-06 01:48:29Z

How can I include two variables in the same for loop?If you want the effect of a nested for loop, use:If you just want to loop simultaneously, use:Note that if x and y are not the same length, zip will truncate to the shortest list.  As @abarnert pointed out, if you don't want to truncate to the shortest list, you could use itertools.zip_longest.UPDATEBased on the request for "a function that will read lists "t1" and "t2" and return all elements that are identical", I don't think the OP wants zip or product.  I think they want a set:The intersection method of a set will return all the elements common to it and another set (Note that if your lists contains other lists, you might want to convert the inner lists to tuples first so that they are hashable; otherwise the call to set will fail.).  The list function then turns the set back into a list.UPDATE 2OR, the OP might want elements that are identical in the same position in the lists.  In this case, zip would be most appropriate, and the fact that it truncates to the shortest list is what you would want (since it is impossible for there to be the same element at index 9 when one of the lists is only 5 elements long).  If that is what you want, go with this:This will return a list containing only the elements that are the same and in the same position in the lists.There's two possible questions here: how can you iterate over those variables simultaneously, or how can you loop over their combination.Fortunately, there's simple answers to both. First case, you want to use zip.will outputRemember that you can put any iterable in zip, so you could just as easily write your exmple like:Actually, just realised that won't work. It would only iterate until the smaller range ran out. In which case, it sounds like you want to iterate over the combination of loops.In the other case, you just want a nested loop.gives youYou can also do this as a list comprehension.Hope that helps.Any reason you can't use a nested for loop?should do it.If you really just have lock-step iteration over a range, you can do it one of several ways:All of the above are equivalent to for i, j in zip(range(x), range(y)) if x <= y.If you want a nested loop and you only have two iterables, just use a nested loop:If you have more than two iterables, use itertools.product.Finally, if you want lock-step iteration up to x and then to continue to y, you have to decide what the rest of the x values should be."Python 3."Add 2 vars with for loop using zip and range; Returning a list.Note: Will only run till smallest range ends.For your use case, it may be easier to utilize a while loop.As a caveat, this approach will truncate to the length of your shortest list.I think you are looking for nested loops. Example (based on your edit):Which can be reduced to a single comprehension:But to find the common elements, you can just do:If your list contains non-hashable objects (like other lists, dicts) use a frozen set:

Creating hidden arguments with Python argparse

Peter Smit

[Creating hidden arguments with Python argparse](https://stackoverflow.com/questions/11114589/creating-hidden-arguments-with-python-argparse)

Is it possible to add an Argument to an python argparse.ArgumentParser without it showing up in the usage or help (script.py --help)?

2012-06-20 07:15:27Z

Is it possible to add an Argument to an python argparse.ArgumentParser without it showing up in the usage or help (script.py --help)?Yes, you can set the help option to add_argument to argparse.SUPPRESS. Here's an example from the argparse documentation:I do it by adding an option to enable the hidden ones, and grab that by looking at sysv.args. If you do this, you have to include the special arg you pick out of sys.argv directly in the parse list if you Assume the option is -s to enable hidden options.

What's the purpose of tf.app.flags in TensorFlow?

flyaway1217

[What's the purpose of tf.app.flags in TensorFlow?](https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow)

I am reading some example codes in Tensorflow, I found following code in tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.pyBut I can't find any docs about this usage of tf.app.flags. And I found the implementation of this flags is in the 

tensorflow/tensorflow/python/platform/default/_flags.pyObviously, this tf.app.flags is somehow used to configure a network, so why  is it not in the API docs? Can anyone explain what is going on here? 

2015-11-26 07:34:17Z

I am reading some example codes in Tensorflow, I found following code in tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.pyBut I can't find any docs about this usage of tf.app.flags. And I found the implementation of this flags is in the 

tensorflow/tensorflow/python/platform/default/_flags.pyObviously, this tf.app.flags is somehow used to configure a network, so why  is it not in the API docs? Can anyone explain what is going on here? The tf.app.flags module is presently a thin wrapper around python-gflags, so the documentation for that project is the best resource for how to use it argparse, which implements a subset of the functionality in python-gflags.Note that this module is currently packaged as a convenience for writing demo apps, and is not technically part of the public API, so it may change in future.We recommend that you implement your own flag parsing using argparse or whatever library you prefer.EDIT: The tf.app.flags module is not in fact implemented using python-gflags, but it uses a similar API. The tf.app.flags module is a functionality provided by Tensorflow to implement command line flags for your Tensorflow program. As an example, the code you came across would do the following:The first parameter defines the name of the flag while the second defines the default value in case the flag is not specified while executing the file.So if you run the following:then the learning rate is set to 1.00 and will remain 0.01 if the flag is not specified.As mentioned in this article, the docs are probably not present because this might be something that Google requires internally for its developers to use.Also, as mentioned in the post, there are several advantages of using Tensorflow flags over flag functionality provided by other Python packages such as argparse especially when dealing with Tensorflow models, the most important being that you can supply Tensorflow specific information to the code such as information about which GPU to use.At Google, they use flag systems to set default values for arguments. It's similar to argparse. They use their own flag system instead of argparse or sys.argv.Source: I worked there before.When you use tf.app.run(), you can transfer the variable very conveniently between threads using tf.app.flags. See this for further usage of tf.app.flags.After trying many times I found this to print all FLAGS key as well as actual value -

Why isn't assigning to an empty list (e.g. [] =「」) an error?

Vardd

[Why isn't assigning to an empty list (e.g. [] =「」) an error?](https://stackoverflow.com/questions/30147165/why-isnt-assigning-to-an-empty-list-e-g-an-error)

In python 3.4, I am typingand it works fine, no Exception is raised. Though of course [] is not equal to "" afterwards.also works fine. raises an exception as expected though, raises an exception as expected though. So, what's going on? 

2015-05-10 02:41:15Z

In python 3.4, I am typingand it works fine, no Exception is raised. Though of course [] is not equal to "" afterwards.also works fine. raises an exception as expected though, raises an exception as expected though. So, what's going on? You are not comparing for equality. You are assigning.Python allows you to assign to multiple targets:assigns the two values to foo and bar, respectively. All you need is a sequence or iterable on the right-hand side, and a list or tuple of names on the left.When you do:you assigned an empty sequence (empty strings are sequences still) to an empty list of names.It is essentially the same thing as doing:where you end up with foo = "a", bar = "b" and baz = "c", but with fewer characters.You cannot, however, assign to a string, so "" on the left-hand side of an assignment never works and is always a syntax error.See the Assignment statements documentation:andEmphasis mine.That Python doesn't throw a syntax error for the empty list is actually a bit of a bug! The officially documented grammar doesn't allow for an empty target list, and for the empty () you do get an error.  See bug 23275; it is considered a harmless bug:Also see Why is it valid to assign to an empty list but not to an empty tuple?It follows the Assignment statements section rules from the documentation,So, when you say"" is an iterable (any valid python string is an iterable) and it is being unpacked over the elements of the list.For example,Since you have an empty string, and an empty list, there is nothing to unpack. So, no error.But, try thisIn the [] = "1" case, you are trying to unpack the string "1" over an empty list of variables. So it complains with "too many values to unpack (expected 0)".Same way, in [a] = "" case, you have an empty string, so nothing to unpack really, but you are unpacking it over one variable, which is, again, not possible. That is why it complains "need more than 0 values to unpack".Apart from that, as you noticed,also throws no error, because () is an empty tuple.and when it is unpacked over an empty list, there is nothing to unpack. So no error.But, when you doas the error message says, you are trying to assign to a string literal. Which is not possible. That is why you are getting the errors. It is like sayingInternalsInternally, this assignment operation will be translated to UNPACK_SEQUENCE op code,Here, since the string is empty, UNPACK_SEQUENCE unpacks 0 times. But when you have something like thisthe sequence 123 is unpacked in to the stack, from right to left. So, the top of the stack would be 1 and the next would be 2 and the last would be 3. Then it assigns from the top of the stack to the variables from the left hand side expression one by one.BTW, in Python, this is how you can do multiple assignments in the same expression. For example,this works because, the right hand values are used to construct a tuple and then it will be unpacked over the left hand side values.but the classic swapping technique a, b = b, a uses rotation of elements in the top of the stack. If you have only two or three elements then they are treated with special ROT_TWO and ROT_THREE instructions instead of constructing the tuple and unpacking.

Numpy argsort - what is it doing?

user1276273

[Numpy argsort - what is it doing?](https://stackoverflow.com/questions/17901218/numpy-argsort-what-is-it-doing)

Why is numpy giving this result:when I'd expect it to do this:Clearly my understanding of the function is lacking.

2013-07-27 18:44:55Z

Why is numpy giving this result:when I'd expect it to do this:Clearly my understanding of the function is lacking.According to the documentation[2, 3, 1, 0] indicates that the smallest element is at index 2, the next smallest at index 3, then index 1, then index 0.There are a number of ways to get the result you are looking for:For example,This checks that they all produce the same result:These IPython %timeit benchmarks suggests for large arrays using_indexed_assignment is the fastest:For small arrays, using_argsort_twice may be faster:Note also that stats.rankdata gives you more control over how to handle elements of equal value. As the documentation says, argsort:That means the first element of the argsort is the index of the element that should be sorted first, the second element is the index of the element that should be second, etc.What you seem to want is the rank order of the values, which is what is provided by scipy.stats.rankdata.  Note that you need to think about what should happen if there are ties in the ranks.input:

    import numpy as np

    x = np.array([1.48,1.41,0.0,0.1])

    x.argsort().argsort() output:

array([3, 2, 0, 1])numpy.argsort(a, axis=-1, kind='quicksort', order=None)Returns the indices that would sort an arrayPerform an indirect sort along the given axis using the algorithm specified by the kind keyword. It returns an array of indices of the same shape as that index data along the given axis in sorted order.Consider one example in python, having a list of values asNow we use argsort function:The output will beThis is the list of indices of values in listExample if you map these indices to the respective values then we will get the result as follows:(I find this function very useful in many places e.g. If you want to sort the list/array but don't want to use list.sort() function (i.e. without changing the order of actual values in the list) you can use this function.)For more details refer this link: https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.argsort.htmlFirst, it was ordered the array. Then generate an array with the initial index of the array.np.argsort returns the index of the sorted array given by the 'kind' (which specifies the type of sorting algorithm). However, when a list is used with np.argmax, it returns the index of the largest element in the list. While, np.sort, sorts the given array, list.It returns indices according to the given array indices,[1.48,1.41,0.0,0.1],that means:

0.0 is the first element, in index [2].

0.1 is the second element, in index[3].

1.41 is the third element, in index [1].

1.48 is the fourth element, in index[0].

Output:Just want to directly contrast the OP's original understanding against the actual implementation with code.numpy.argsort is defined such that for 1D arrays:The OP originally thought that it was defined such that for 1D arrays:Note: This code doesn't work in the general case (only works for 1D), this answer is purely for illustration purposes.

Django-DB-Migrations: cannot ALTER TABLE because it has pending trigger events

guettli

[Django-DB-Migrations: cannot ALTER TABLE because it has pending trigger events](https://stackoverflow.com/questions/12838111/django-db-migrations-cannot-alter-table-because-it-has-pending-trigger-events)

I want to remove null=True from a TextField:I created a schema migration:Since some footer columns contain NULL I get this error if I run the migration:I added this to the schema migration:Now I get:What is wrong?

2012-10-11 11:06:29Z

I want to remove null=True from a TextField:I created a schema migration:Since some footer columns contain NULL I get this error if I run the migration:I added this to the schema migration:Now I get:What is wrong?Another reason for this maybe because you try to set a column to NOT NULL when it actually already has NULL values.Every migration is inside a transaction. In PostgreSQL you must not update the table and then alter the table schema in one transaction.You need to split the data migration and the schema migration. First create the data migration with this code:Then create the schema migration:Now you have two transactions and the migration in two steps should work.Have just hit this problem. You can also use db.start_transaction() and db.commit_transaction() in the schema migration to separate data changes from schema changes. Probably not so clean as to have a separate data migration but in my case I would need schema, data, and then another schema migration so I decided to do it all at once.

python requests file upload

scichris

[python requests file upload](https://stackoverflow.com/questions/22567306/python-requests-file-upload)

I'm performing a simple task of uploading a file using Python requests library. I searched Stack Overflow and no one seemed to have the same problem, namely, that the file is not received by the server:I'm filling the value of 'upload_file' keyword with my filename, because if I leave it blank, it saysAnd now I get Which comes up only if the file is empty. So I'm stuck as to how to send my file successfully. I know that the file works because if I go to this website and manually fill in the form it returns a nice list of matched objects, which is what I'm after.  I'd  really appreciate all hints. Some other threads related (but not answering my problem):

2014-03-21 18:57:13Z

I'm performing a simple task of uploading a file using Python requests library. I searched Stack Overflow and no one seemed to have the same problem, namely, that the file is not received by the server:I'm filling the value of 'upload_file' keyword with my filename, because if I leave it blank, it saysAnd now I get Which comes up only if the file is empty. So I'm stuck as to how to send my file successfully. I know that the file works because if I go to this website and manually fill in the form it returns a nice list of matched objects, which is what I'm after.  I'd  really appreciate all hints. Some other threads related (but not answering my problem):If upload_file is meant to be the file, use:and requests will send a multi-part form POST body with the upload_file field set to the contents of the file.txt file.The filename will be included in the mime header for the specific field:Note the filename="file.txt" parameter.You can use a tuple for the files mapping value, with between 2 and 4 elements, if you need more control. The first element is the filename, followed by the contents, and an optional content-type header value and an optional mapping of additional headers:This sets an alternative filename and content type, leaving out the optional headers.If you are meaning the whole POST body to be taken from a file (with no other fields specified), then don't use the files parameter, just post the file directly as data. You then may want to set a Content-Type header too, as none will be set otherwise. See Python requests - POST data from a file.(2018) the new python requests library has simplified this process, we can use the 'files' variable to signal that we want to upload a multipart-encoded fileIf you want to upload a single file with Python requests library, then requests lib supports streaming uploads, which allow you to send large files or streams without reading into memory.Then store the file on the server.py side such that save the stream into file without loading into the memory. Following is an example with using Flask file uploads.Or use werkzeug Form Data Parsing as mentioned in a fix for the issue of "large file uploads eating up memory" in order to avoid using memory inefficiently on large files upload (s.t. 22 GiB file in ~60 seconds. Memory usage is constant at about 13 MiB.).In Ubuntu you can apply this way, 

In Tensorflow, get the names of all the Tensors in a graph

P. Camilleri

[In Tensorflow, get the names of all the Tensors in a graph](https://stackoverflow.com/questions/36883949/in-tensorflow-get-the-names-of-all-the-tensors-in-a-graph)

I am creating neural nets with Tensorflow and skflow; for some reason I want to get the values of some inner tensors for a given input, so I am using myClassifier.get_layer_value(input, "tensorName"), myClassifier being a skflow.estimators.TensorFlowEstimator. However, I find it difficult to find the correct syntax of the tensor name, even knowing its name (and I'm getting confused between operation and tensors), so I'm using tensorboard to plot the graph and look for the name.Is there a way to enumerate all the tensors in a graph without using tensorboard?

2016-04-27 08:08:29Z

I am creating neural nets with Tensorflow and skflow; for some reason I want to get the values of some inner tensors for a given input, so I am using myClassifier.get_layer_value(input, "tensorName"), myClassifier being a skflow.estimators.TensorFlowEstimator. However, I find it difficult to find the correct syntax of the tensor name, even knowing its name (and I'm getting confused between operation and tensors), so I'm using tensorboard to plot the graph and look for the name.Is there a way to enumerate all the tensors in a graph without using tensorboard?You can doAlso, if you are prototyping in an IPython notebook, you can show the graph directly in notebook, see show_graph function in Alexander's Deep Dream notebookThere is a way to do it slightly faster than in Yaroslav's answer by using get_operations. Here is a quick example:tf.all_variables() can get you the information you want.Also, this commit made today in TensorFlow Learn that provides a function get_variable_names in estimator that you can use to retrieve all variable names easily. I think this will do too:But compared with Salvado and Yaroslav's answers, I don't know which one is better.The accepted answer only gives you a list of strings with the names. I prefer a different approach, which gives you (almost) direct access to the tensors:list_of_tuples now contains every tensor, each within a tuple. You could also adapt it to get the tensors directly:Since the OP asked for the list of the tensors instead of the list of operations/nodes, the code should be slightly different:I'll try to summarize the answers:To get all nodes:

These have the type tensorflow.core.framework.node_def_pb2.NodeDefTo get all ops:These have the type tensorflow.python.framework.ops.OperationTo get all variables:

These have the type tensorflow.python.ops.resource_variable_ops.ResourceVariableAnd finally, to answer the question, to get all tensors:

These have the type tensorflow.python.framework.ops.TensorPrevious answers are good, I'd just like to share a utility function I wrote to select Tensors from a graph:So if you have a graph with ops:Then running returns:This worked for me:

How do I use a Boolean in Python?

Federer

[How do I use a Boolean in Python?](https://stackoverflow.com/questions/1748641/how-do-i-use-a-boolean-in-python)

Does Python actually contain a Boolean value? I know that you can do:But I'm quite pedantic and enjoy seeing booleans in Java. For instance:Is there such a thing as a Boolean in Python? I can't seem to find anything like it in the documentation.

2009-11-17 12:48:39Z

Does Python actually contain a Boolean value? I know that you can do:But I'm quite pedantic and enjoy seeing booleans in Java. For instance:Is there such a thing as a Boolean in Python? I can't seem to find anything like it in the documentation.[Edit]For more information: http://docs.python.org/library/functions.html#boolYour code works too, since 1 is converted to True when necessary.

Actually Python didn't have a boolean type for a long time (as in old C), and some programmers still use integers instead of booleans.The boolean builtins are capitalized: True and False.Note also that you can do checker = bool(some_decision) as a bit of shorthand -- bool will only ever return True or False.It's good to know for future reference that classes defining __nonzero__ or __len__ will be True or False depending on the result of those functions, but virtually every other object's boolean result will be True (except for the None object, empty sequences, and numeric zeros).True ... and False obviously.Otherwise, None evaluates to False, as does the integer 0 and also the float 0.0 (although I wouldn't use floats like that).

Also, empty lists [], empty tuplets (), and empty strings '' or "" evaluate to False.Try it yourself with the function bool():etc..Boolean types are defined in documentation:

http://docs.python.org/library/stdtypes.html#boolean-valuesQuoted from doc:So in java code remove braces, change true to True and you will be ok :)Yes, there is a bool data type (which inherits from int and has only two values: True and False).But also Python has the boolean-able concept for every object, which is used when function bool([x]) is called.See more: object.nonzero and boolean-value-of-objects-in-python.Unlike Java where you would declare boolean flag = True, in Python you can just declare myFlag = TruePython would interpret this as a boolean variable

Run an OLS regression with Pandas Data Frame

Michael

[Run an OLS regression with Pandas Data Frame](https://stackoverflow.com/questions/19991445/run-an-ols-regression-with-pandas-data-frame)

I have a pandas data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:Ideally, I would have something like ols(A ~ B + C, data = df) but when I look at the examples from algorithm libraries like scikit-learn it appears to feed the data to the model with a list of rows instead of columns. This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place. What is the most pythonic way to run an OLS regression (or any machine learning algorithm more generally) on data in a pandas data frame? 

2013-11-15 00:47:00Z

I have a pandas data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:Ideally, I would have something like ols(A ~ B + C, data = df) but when I look at the examples from algorithm libraries like scikit-learn it appears to feed the data to the model with a list of rows instead of columns. This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place. What is the most pythonic way to run an OLS regression (or any machine learning algorithm more generally) on data in a pandas data frame? I think you can almost do exactly what you thought would be ideal, using the statsmodels package which was one of pandas' optional dependencies before pandas' version 0.20.0 (it was used for a few things in pandas.stats.)Note: pandas.stats has been removed with 0.20.0It's possible to do this with pandas.stats.ols:Note that you need to have statsmodels package installed, it is used internally by the pandas.stats.ols function.I don't know if this is new in sklearn or pandas, but I'm able to pass the data frame directly to sklearn without converting the data frame to a numpy array or any other data types.No it doesn't, just convert to a NumPy array:This takes constant time because it just creates a view on your data. Then feed it to scikit-learn:Statsmodels kan build an OLS model with column references directly to a pandas dataframe.Short and sweet:model = sm.OLS(df[y], df[x]).fit()Code details and regression summary:Output:How to directly get R-squared, Coefficients and p-value:

Equivalent C++ to Python generator pattern

Noah Watkins

[Equivalent C++ to Python generator pattern](https://stackoverflow.com/questions/9059187/equivalent-c-to-python-generator-pattern)

I've got some example Python code that I need to mimic in C++. I do not require any specific solution (such as co-routine based yield solutions, although they would be acceptable answers as well), I simply need to reproduce the semantics in some manner.This is a basic sequence generator, clearly too large to store a materialized version.The goal is to maintain two instances of the sequence above, and iterate over them in semi-lockstep, but in chunks. In the example below the first_pass uses the sequence of pairs to initialize the buffer, and the second_pass regenerates the same exact sequence and processes the buffer again.The only thing I can find for a solution in C++ is to mimic yield with C++ coroutines, but I haven't found any good reference on how to do this. I'm also interested in alternative (non general) solutions for this problem. I do not have enough memory budget to keep a copy of the sequence between passes.

2012-01-30 03:58:56Z

I've got some example Python code that I need to mimic in C++. I do not require any specific solution (such as co-routine based yield solutions, although they would be acceptable answers as well), I simply need to reproduce the semantics in some manner.This is a basic sequence generator, clearly too large to store a materialized version.The goal is to maintain two instances of the sequence above, and iterate over them in semi-lockstep, but in chunks. In the example below the first_pass uses the sequence of pairs to initialize the buffer, and the second_pass regenerates the same exact sequence and processes the buffer again.The only thing I can find for a solution in C++ is to mimic yield with C++ coroutines, but I haven't found any good reference on how to do this. I'm also interested in alternative (non general) solutions for this problem. I do not have enough memory budget to keep a copy of the sequence between passes.Generators exist in C++, just under another name: Input Iterators. For example, reading from std::cin is similar to having a generator of char.You simply need to understand what a generator does:In your trivial example, it's easy enough. Conceptually:Of course, we wrap this as a proper class:So hum yeah... might be that C++ is a tad more verbose :)In C++ there are iterators, but implementing an iterator isn't straightforward: one has to consult the iterator concepts and carefully design the new iterator class to implement them. Thankfully, Boost has an iterator_facade template which should help implementing the iterators and iterator-compatible generators.Sometimes a stackless coroutine can be used to implement an iterator.P.S. See also this article which mentions both a switch hack by Christopher M. Kohlhoff and Boost.Coroutine by Oliver Kowalke. Oliver Kowalke's work is a followup on Boost.Coroutine by Giovanni P. Deretta.P.S. I think you can also write a kind of generator with lambdas:Or with a functor:P.S. Here's a generator implemented with the Mordor coroutines:Since Boost.Coroutine2 now supports it very well (I found it because I wanted to solve exactly the same yield problem), I am posting the C++ code that matches your original intention:In this example, pair_sequence does not take additional arguments. If it needs to, std::bind or a lambda should be used to generate a function object that takes only one argument (of push_type), when it is passed to the coro_t::pull_type constructor.You should probably check generators in std::experimental in Visual Studio 2015 e.g: https://blogs.msdn.microsoft.com/vcblog/2014/11/12/resumable-functions-in-c/I think it's exactly what you are looking for. Overall generators should be available in C++17 as this is only experimental Microsoft VC feature.All answers that involve writing your own iterator are completely wrong. Such answers entirely miss the point of Python generators (one of the language's greatest and unique features). The most important thing about generators is that execution picks up where it left off. This does not happen to iterators. Instead, you must manually store state information such that when operator++ or operator* is called anew, the right information is in place at the very beginning of the next function call. This is why writing your own C++ iterator is a gigantic pain; whereas, generators are elegant, and easy to read+write.I don't think there is a good analog for Python generators in native C++, at least not yet (there is a rummor that yield will land in C++17). You can get something similarish by resorting to third-party (e.g. Yongwei's Boost suggestion), or rolling your own.I would say the closest thing in native C++ is threads. A thread can maintain a suspended set of local variables, and can continue execution where it left off, very much like generators, but you need to roll a little bit of additional infrastructure to support communication between the generator object and its caller. E.g.This solution has several downsides though:If you only need to do this for a relatively small number of specific generators, you can implement each as a class, where the member data is equivalent to the local variables of the Python generator function. Then you have a next function that returns the next thing the generator would yield, updating the internal state as it does so.This is basically similar to how Python generators are implemented, I believe. The major difference being they can remember an offset into the bytecode for the generator function as part of the "internal state", which means the generators can be written as loops containing yields. You would have to instead calculate the next value from the previous. In the case of your pair_sequence, that's pretty trivial. It may not be for complex generators.You also need some way of indicating termination. If what you're returning is "pointer-like", and NULL should not be a valid yieldable value you could use a NULL pointer as a termination indicator. Otherwise you need an out-of-band signal.Something like this is very similar:Using the operator() is only a question of what you want to do with this generator, you could also build it as a stream and make sure it adapts to an istream_iterator, for example.Using range-v3:Something like this:Example use:Will print the numbers from 0 to 99Well, today I also was looking for easy collection implementation under C++11. Actually I was disappointed, because everything I found is too far from things like python generators, or C# yield operator... or too complicated.The purpose is to make collection which will emit its items only when it is required.I wanted it to be like this:I found this post, IMHO best answer was about boost.coroutine2, by Yongwei Wu. Since it is the nearest to what author wanted.It is worth learning boost couroutines.. And I'll perhaps do on weekends. But so far I'm using my very small implementation. Hope it helps to someone else.Below is example of use, and then implementation.Example.cppGenerator.hJust as a function simulates the concept of a stack, generators simulate the concept of a queue.  The rest is semantics.  As a side note, you can always simulate a queue with a stack by using a stack of operations instead of data.  What that practically means is that you can implement a queue-like behavior by returning a pair, the second value of which either has the next function to be called or indicates that we are out of values.  But this is more general than what yield vs return does.  It allows to simulate a queue of any values rather than homogeneous values that you expect from a generator, but without keeping a full internal queue.More specifically, since C++ does not have a natural abstraction for a queue, you need to use constructs which implement a queue internally.  So the answer which gave the example with iterators is a decent implementation of the concept.What this practically means is that you can implement something with bare-bones queue functionality if you just want something quick and then consume queue's values just as you would consume values yielded from a generator.

How to avoid reinstalling packages when building Docker image for Python projects?

satoru

[How to avoid reinstalling packages when building Docker image for Python projects?](https://stackoverflow.com/questions/25305788/how-to-avoid-reinstalling-packages-when-building-docker-image-for-python-project)

My Dockerfile is something likeEvery time I build a new image, dependencies have to be reinstalled, which could be very slow in my region.One way I think of to cache packages that have been installed is to override the my/base image with newer images like this:So next time I build with this Dockerfile, my/base already has some packages installed.But this solution has two problems:So what better solution could I use to solve this problem?Some information about the docker on my machine:

2014-08-14 10:25:20Z

My Dockerfile is something likeEvery time I build a new image, dependencies have to be reinstalled, which could be very slow in my region.One way I think of to cache packages that have been installed is to override the my/base image with newer images like this:So next time I build with this Dockerfile, my/base already has some packages installed.But this solution has two problems:So what better solution could I use to solve this problem?Some information about the docker on my machine:Try to build with below Dockerfile.If there are some changes on .(your project), docker skip pip install line by using cache.Docker only run pip install on build when you edit requirements.txt file.I write simple Hello, World! program.Below is output.I update only run.py and try to build again.Below is output. As you can see above, docker use build cache. And I update requirements.txt this time.Below is output.And docker doesn't use build cache. If it doesn't work, check your docker version.To minimise the network activity, you could point pip to a cache directory on your host machine. Run your docker container with your host's pip cache directory bind mounted into your container's pip cache directory. docker run command should look like this:Then in your Dockerfile install your requirements as a part of ENTRYPOINT statement (or CMD statement) instead of as a RUN command. This is important, because (as pointed out in comments) the mount is not available during image building (when RUN statements are executed). Docker file should look like this: Probably it's best if the host system's default pip directory will be used as a cache (e.g. $HOME/.cache/pip/ on Linux or $HOME/Library/Caches/pip/ on OSX), just like I suggested in the example docker run command.I understand this question has some popular answers already. But there is a newer way to cache files for package managers. I think it could be a good answer in the future when BuildKit becomes more standard.As of Docker 18.09 there is experimental support for BuildKit. BuildKit adds support for some new features in the Dockerfile including experimental support for mounting external volumes into RUN steps. This allows us to create caches for things like $HOME/.cache/pip/.We'll use the following requirements.txt file as an example:A typical example Python Dockerfile might look like:With BuildKit enabled using the DOCKER_BUILDKIT environment variable we can build the uncached pip step in about 65 seconds:Now, let us add the experimental header and modify the RUN step to cache the Python packages:Go ahead and do another build now. It should take the same amount of time. But this time it is caching the Python packages in our new cache mount:About 60 seconds. Similar to our first build.Make a small change to the requirements.txt (such as adding a new line between two packages) to force a cache invalidation and run again:Only about 16 seconds!We are getting this speedup because we are no longer downloading all the Python packages. They were cached by the package manager (pip in this case) and stored in a cache volume mount. The volume mount is provided to the run step so that pip can reuse our already downloaded packages. This happens outside any Docker layer caching.The gains should be much better on larger requirements.txt.Notes:Hopefully, these features will make it into Docker for building and BuildKit will become the default. If / when that happens I will try to update this answer.I found that a better way is to just add the Python site-packages directory as a volume. This way I can just pip install new libraries without having to do a full rebuild. EDIT: Disregard this answer, jkukul's answer above worked for me. My intent was to cache the site-packages folder. That would have looked something more like:Caching the download folder is alot cleaner though. That also caches the wheels, so it properly achieves the task.

How does Python manage int and long?

luc

[How does Python manage int and long?](https://stackoverflow.com/questions/2104884/how-does-python-manage-int-and-long)

Does anybody know how Python manage internally int and long types? How should I understand the code below?Update: 

2010-01-20 20:54:09Z

Does anybody know how Python manage internally int and long types? How should I understand the code below?Update: int and long were "unified" a few versions back. Before that it was possible to overflow an int through math ops.3.x has further advanced this by eliminating long altogether and only having int.This PEP should help. Bottom line is that you really shouldn't have to worry about it in python versions > 2.4On my machine:Python uses ints (32 bit signed integers, I don't know if they are C ints under the hood or not) for values that fit into 32 bit, but automatically switches to longs (arbitrarily large number of bits - i.e. bignums)  for anything larger. I'm guessing this speeds things up for smaller values while avoiding any overflows with a seamless transition to bignums.Interesting.  On my 64-bit (i7 Ubuntu) box:Guess it steps up to 64 bit ints on a larger machine.Python 2.7.9 auto promotes numbers.

For a case where one is unsure to use int() or long().Python 2 will automatically set the type based on the size of the value. A guide of max values can be found below.The Max value of the default Int in Python 2 is 65535, anything above that will be a longFor example:In Python 3 the long datatype has been removed and all integer values are handled by the Int class. The default size of Int will depend on your CPU architecture.For example: The min/max values of each type can be found below:If the size of your Int exceeds the limits mentioned above, python will automatically change it's type and allocate more memory to handle this increase in min/max values. Where in Python 2, it would convert into 'long', it now just converts into the next size of Int.Example: If you are using a 32 bit operating system, your max value of an Int will be 2147483647 by default. If a value of 2147483648 or more is assigned, the type will be changed to Int64.There are different ways to check the size of the int and it's memory allocation.

Note: In Python 3, using the built-in type() method will always return <class 'int'> no matter what size Int you are using.From python 3.x, the unified integer libries are even more smarter than older versions. On my (i7 Ubuntu) box I got the following,For implementation details refer Include/longintrepr.h, Objects/longobject.c and Modules/mathmodule.c files. The last file is a dynamic module (compiled to an so file). The code is well commented to follow.Just to continue to all the answers that were given here, especially @James Lanesthe size of the integer type can be expressed by this formula:total range =  (2 ^ bit system)lower limit = -(2 ^ bit system)*0.5

upper limit =  ((2 ^ bit system)*0.5) - 1It manages them because int and long are sibling class definitions.  They have appropriate methods for +, -, *, /, etc., that will produce results of the appropriate class.For exampleIn this case, the class int has a __mul__ method (the one that implements *) which creates a long result when required.From Python 3.x all integer values are part of the Int class. Integers size in now dependent on the CPU architecture.

How to turn on line numbers in IDLE?

User

[How to turn on line numbers in IDLE?](https://stackoverflow.com/questions/18805203/how-to-turn-on-line-numbers-in-idle)

In the main shell of IDLE, errors always return a line number but the development environment doesn't even have line numbers. Is there anyway to turn on line numbers?

2013-09-14 18:45:03Z

In the main shell of IDLE, errors always return a line number but the development environment doesn't even have line numbers. Is there anyway to turn on line numbers?To show line numbers in the current window, go to Options and click Show Line Numbers.To show them automatically, go to Options > Configure IDLE > General and check the Show line numbers in new windows box.Unfortunately there is not an option to display line numbers in IDLE although there is an enhancement request open for this.However, there are a couple of ways to work around this:There's a set of useful extensions to IDLE called IDLEX that works with MacOS and Windows http://idlex.sourceforge.net/It includes line numbering and I find it quite handy & free. Otherwise there are a bunch of other IDEs some of which are free:  https://wiki.python.org/moin/IntegratedDevelopmentEnvironmentsIf you are trying to track down which line caused an error, if you right-click in the Python shell where the line error is displayed it will come up with a "Go to file/line"  which takes you directly to the line in question.As it was mentioned by Davos you can use the IDLEXIt happens that I'm using Linux version and from all extensions I needed only LineNumbers. So I've downloaded IDLEX archive, took LineNumbers.py from it, copied it to Python's lib folder ( in my case its /usr/lib/python3.5/idlelib ) and added following lines to configuration file in my home folder which is 

~/.idlerc/config-extensions.cfg:Line numbers were added to the IDLE editor two days ago and will appear in the upcoming 3.8.0a3 and later 3.7.5.  For new windows, they are off by default, but this can be reversed on the Setting dialog, General tab, Editor section.  For existing windows, there is a new Show (Hide) Line Numbers entry on the Options menu.  There is currently no hotkey.  One can select a line or bloc of lines by clicking on a line or clicking and dragging.Some people may have missed Edit / Go to Line.  The right-click context menu Goto File/Line works on grep (Find in Files) output as well as on trackbacks.As @StahlRat already answered. I would like to add another method for it. There is extension pack for Python Default idle editor Python Extensions Package.As mentioned above (a quick way to do this) :Then I create a shortcut on Desktop (Win10) like this:The paths may be different and need to be changed:(Thanks for the great answers above)

How to suppress Pandas Future warning ?

bigbug

[How to suppress Pandas Future warning ?](https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning)

When I run the program, Pandas gives 'Future warning' like below every time.I got the msg, but I just want to stop Pandas showing such msg again and again, is there any buildin parameter that I can set to let Pandas not pop up the 'Future warning' ?

2013-04-03 02:37:48Z

When I run the program, Pandas gives 'Future warning' like below every time.I got the msg, but I just want to stop Pandas showing such msg again and again, is there any buildin parameter that I can set to let Pandas not pop up the 'Future warning' ?Found this on github...@bdiamante's answer may only partially help you.  If you still get a message after you've suppressed warnings, it's because the pandas library itself is printing the message.  There's not much you can do about it unless you edit the Pandas source code yourself.  Maybe there's an option internally to suppress them, or a way to override things, but I couldn't find one.Suppose that you want to ensure a clean working environment.  At the top of your script, you put pd.reset_option('all').  With Pandas 0.23.4, you get the following:Following the @bdiamante's advice, you use the warnings library.  Now, true to it's word, the warnings have been removed.  However, several pesky messages remain:In fact, disabling all warnings produces the same output:In the standard library sense, these aren't true warnings.  Pandas implements its own warnings system.  Running grep -rn on the warning messages shows that the pandas warning system is implemented in core/config_init.py: Further chasing shows that I don't have time for this.  And you probably don't either.  Hopefully this saves you from falling down the rabbit hole or perhaps inspires someone to figure out how to truly suppress these messages!Warnings are annoying. As mentioned in other answers, you can suppress them using:But if you want to handle them one by one and you are managing a bigger codebase, it will be difficult to find the line of code which is causing the warning. Since warnings unlike errors don't come with code traceback. In order to trace warnings like errors, you can write this at the top of the code:But if the codebase is bigger and it is importing bunch of other libraries/packages, then all sort of warnings will start to be raised as errors. In order to raise only certain type of warnings (in your case, its FutureWarning) as error, you can write:

gunicorn autoreload on source change

Paolo

[gunicorn autoreload on source change](https://stackoverflow.com/questions/12773763/gunicorn-autoreload-on-source-change)

Finally I migrated my development env from runserver to gunicorn/nginx.It'd be convenient to replicate the autoreload feature of runserver to gunicorn, so the server automatically restarts when source changes. Otherwise I have to restart the server manually with kill -HUP.Any way to avoid the manual restart?

2012-10-07 23:36:01Z

Finally I migrated my development env from runserver to gunicorn/nginx.It'd be convenient to replicate the autoreload feature of runserver to gunicorn, so the server automatically restarts when source changes. Otherwise I have to restart the server manually with kill -HUP.Any way to avoid the manual restart?While this is old question, just for consistency - since version 19.0 gunicorn has --reload option.

So no third party tools needed more.One option would be to use the --max-requests to limit each spawned process to serving only one request by adding --max-requests 1 to the startup options. Every newly spawned process should see your code changes and in a development environment the extra startup time per request should be negligible.Bryan Helmig came up with this and I modified it to use run_gunicorn instead of launching gunicorn directly, to make it possible to just cut and paste these 3 commands into a shell in your django project root folder (with your virtualenv activated):I use git push to deploy to production and set up git hooks to run a script.  The advantage of this approach is you can also do your migration and package installation at the same time.  https://mikeeverhart.net/2013/01/using-git-to-deploy-code/Then create a script /home/git/project_name.git/hooks/post-receive.  Make sure to chmod u+x post-receive, and add user to sudoers.  Allow it to run sudo supervisorctl without password.  https://www.cyberciti.biz/faq/linux-unix-running-sudo-command-without-a-password/From my local / development server, I set up git remote that allows me to push to the production serverAs a bonus, you will get to see all the prompts as the script is running.  So you will see if there is any issue with the migration/package installation/supervisor restart.

How do I detect if Python is running as a 64-bit application? [duplicate]

Nick Bolton

[How do I detect if Python is running as a 64-bit application? [duplicate]](https://stackoverflow.com/questions/1842544/how-do-i-detect-if-python-is-running-as-a-64-bit-application)

I'm doing some work with the windows registry. Depending on whether you're running python as 32-bit or 64-bit, the key value will be different. How do I detect if Python is running as a 64-bit application as opposed to a 32-bit application?Note: I'm not interested in detecting 32-bit/64-bit Windows - just the Python platform.

2009-12-03 20:06:50Z

I'm doing some work with the windows registry. Depending on whether you're running python as 32-bit or 64-bit, the key value will be different. How do I detect if Python is running as a 64-bit application as opposed to a 32-bit application?Note: I'm not interested in detecting 32-bit/64-bit Windows - just the Python platform.From the Python docs:While it may work on some platforms, be aware that platform.architecture is not always a reliable way to determine whether python is running in 32-bit or 64-bit.  In particular, on some OS X multi-architecture builds, the same executable file may be capable of running in either mode, as the example below demonstrates.  The quickest safe multi-platform approach is to test sys.maxsize on Python 2.6, 2.7, Python 3.x.

Pythonic way to print list items

Guillaume Voiron

[Pythonic way to print list items](https://stackoverflow.com/questions/15769246/pythonic-way-to-print-list-items)

I would like to know if there is a better way to print all objects in a Python list than this :I read this way is not really good :Isn't there something like :If not, my question is... why ? If we can do this kind of stuff with comprehensive lists, why not as a simple statement outside a list ?

2013-04-02 16:24:15Z

I would like to know if there is a better way to print all objects in a Python list than this :I read this way is not really good :Isn't there something like :If not, my question is... why ? If we can do this kind of stuff with comprehensive lists, why not as a simple statement outside a list ?Assuming you are using Python 3.x:You can get the same behavior on Python 2.x using from __future__ import print_function, as noted by mgilson in comments.With the print statement on Python 2.x you will need iteration of some kind, regarding your question about print(p) for p in myList not working, you can just use the following which does the same thing and is still one line:For a solution that uses '\n'.join(), I prefer list comprehensions and generators over map() so I would probably use the following:I use this all the time : [print(a) for a in list] will give a bunch of None types at the end though it prints out all the itemsFor Python 2.*:If you overload the function __str__() for your Person class, you can omit the part with map(str, ...). Another way for this is creating a function, just like you wrote:There is in Python 3.* the argument sep for the print() function. Take a look at documentation.Expanding @lucasg's answer (inspired by the comment it received):To get a formatted list output, you can do something along these lines:Now the ", " provides the separator (only between items, not at the end) and the formatting string '02d'combined with %x gives a formatted string for each item x - in this case, formatted as an integer with two digits, left-filled with zeros.To display each content, I use:Example of using in a function:Hope I helped.I think this is the most convenient if you just want to see the content in the list:Simple, easy to read and can be used together with format string.OP's question is: does something like following exists, if not then whyanswer is, it does exist which is:Basically, use [] for list comprehension and get rid of print to avoiding printing None. To see why print prints None see this I recently made a password generator and although I'm VERY NEW to python, I whipped this up as a way to display all items in a list (with small edits to fit your needs...Like I said, IM VERY NEW to Python and I'm sure this is way to clunky for a expert, but I'm just here for another exampleAssuming you are fine with your list being printed [1,2,3], then an easy way in Python3 is:Running this produces the following output:

Python: changing value in a tuple

Dawood

[Python: changing value in a tuple](https://stackoverflow.com/questions/11458239/python-changing-value-in-a-tuple)

I'm new to python so this question might be a little basic. I have a tuple called values which contains the following:I want to change the first value (i.e., 275) in this tuple but I understand that tuples are immutable so values[0] = 200 will not work. How can I achieve this?

2012-07-12 18:27:42Z

I'm new to python so this question might be a little basic. I have a tuple called values which contains the following:I want to change the first value (i.e., 275) in this tuple but I understand that tuples are immutable so values[0] = 200 will not work. How can I achieve this?First you need to ask, why you want to do this?But it's possible via:But if you're going to need to change things, you probably are better off keeping it as a listDepending on your problem slicing can be a really neat solution:This allows you to add multiple elements or also to replace a few elements (especially if they are "neighbours". In the above case casting to a list is probably more appropriate and readable (even though the slicing notation is much shorter).Well, as Trufa has already shown, there are basically two ways of replacing a tuple's element at a given index. Either convert the tuple to a list, replace the element and convert back, or construct a new tuple by concatenation.So, which method is better, that is, faster?It turns out that for short tuples (on Python 3.3), concatenation is actually faster!Yet if we look at longer tuples, list conversion is the way to go:For very long tuples, list conversion is substantially better!Also, performance of the concatenation method depends on the index at which we replace the element. For the list method, the index is irrelevant.So: If your tuple is short, slice and concatenate.

If it's long, do the list conversion!Not that this is superior, but if anyone is curious it can be done on one line with:As Hunter McMillen wrote in the comments, tuples are immutable, you need to create a new tuple in order to achieve this. For instance:I believe this technically answers the question, but don't do this at home. At the moment, all answers involve creating a new tuple, but you can use ctypes to modify a tuple in-memory. Relying on various implementation details of CPython on a 64-bit system, one way to do this is as follows:It is possible with a one liner:EDIT: This doesn't work on tuples with duplicate entries yet!!Based on Pooya's idea:If you are planning on doing this often (which you shouldn't since tuples are inmutable for a reason) you should do something like this:Or based on Jon's idea:based on Jon's Idea and dear Trufait changes all of your old values occurrences    Frist, ask yourself why you want to mutate your tuple. There is a reason why strings and tuple are immutable in Ptyhon, if you want to mutate your tuple then it should probably be a list instead. Second, if you still wish to mutate your tuple then you can convert your tuple to a list then convert it back, and reassign the new tuple to the same variable. This is great if you are only going to mutate your tuple once. Otherwise, I personally think that is counterintuitive. Because It is essentially creating a new tuple and every time if you wish to mutate the tuple you would have to perform the conversion. Also If you read the code it would be confusing to think why not just create a list? But it is nice because it doesn't require any library.I suggest using mutabletuple(typename, field_names, default=MtNoDefault) from mutabletuple 0.2. I personally think this way is a more intuitive and readable. The personal reading the code would know that writer intends to mutate this tuple in the future. The downside compares to the list conversion method above is that this requires you to import additional py file.TL;DR: Don't try to mutate tuple. if you do and it is a one-time operation convert tuple to list, mutate it, turn list into a new tuple, and reassign back to the variable holding old tuple. If desires tuple and somehow want to avoid listand want to mutate more than once then create mutabletuple.You can't.  If you want to change it, you need to use a list instead of a tuple.Note that you could instead make a new tuple that has the new value as its first element.I've found the best way to edit tuples is to recreate the tuple using the previous version as the base.Here's an example I used for making a lighter version of a colour (I had it open already at the time):  What it does, is it goes through the tuple 'colour' and reads each item, does something to it, and finally adds it to the new tuple.So what you'd want would be something like:That specific one doesn't work, but the concept is what you need.that's the concept.I´m late to the game but I think the simplest, resource-friendliest and fastest way (depending on the situation),

is to overwrite the tuple itself. Since this would remove the need for the list & variable creation and is archived in one line.   But: This is only handy for rather small tuples and also limits you to a fixed tuple value, nevertheless, this is the case for tuples most of the time anyway.So in this particular case it would look like this:tldr; the "workaround" is creating a new tuple object, not actually modifying the originalWhile this is a very old question, someone told me about this Python mutating tuples madness.  Which I was very much surprised/intrigued, and doing some googling, I landed here (and other similar samples online)I ran some test to prove my theoryNote == does value equality while is does referential equality (is obj a the same instance as obj b)Yields:i did this:and to change, just doand u can change a tuple :Dhere is it copied exactly from IDLEYou can change the value of tuple using copy by reference

asterisk in function call

Ramy

[asterisk in function call](https://stackoverflow.com/questions/5239856/asterisk-in-function-call)

I'm using itertools.chain to "flatten" a list of lists in this fashion:how is this different than saying:

2011-03-08 23:58:47Z

I'm using itertools.chain to "flatten" a list of lists in this fashion:how is this different than saying:* is the "splat" operator: It takes a list as input, and expands it into actual positional arguments in the function call.So if uniqueCrossTabs was [ [ 1, 2 ], [ 3, 4 ] ], then itertools.chain(*uniqueCrossTabs) is the same as saying itertools.chain([ 1, 2 ], [ 3, 4 ])This is obviously different from passing in just uniqueCrossTabs. In your case, you have a list of lists that you wish to flatten; what itertools.chain() does is return an iterator over the concatenation of all the positional arguments you pass to it, where each positional argument is iterable in its own right.In other words, you want to pass each list in uniqueCrossTabs as an argument to chain(), which will chain them together, but you don't have the lists in separate variables, so you use the * operator to expand the list of lists into several list arguments.As Jochen Ritzel has pointed out in the comments, chain.from_iterable() is better-suited for this operation, as it assumes a single iterable of iterables to begin with. Your code then becomes simply:It splits the sequence into separate arguments for the function call.Just an alternative way of explaining the concept/using it.

How to run a Python script in the background even after I logout SSH?

zihaoyu

[How to run a Python script in the background even after I logout SSH?](https://stackoverflow.com/questions/2975624/how-to-run-a-python-script-in-the-background-even-after-i-logout-ssh)

I have Python script bgservice.py and I want it to run all the time, because it is part of the web service I build. How can I make it run continuously even after I logout SSH? 

2010-06-04 15:39:39Z

I have Python script bgservice.py and I want it to run all the time, because it is part of the web service I build. How can I make it run continuously even after I logout SSH? Run nohup python bgservice.py & to get the script to ignore the hangup signal and keep running. Output will be put in nohup.out.Ideally, you'd run your script with something like supervise so that it can be restarted if (when) it dies. If you've already started the process, and don't want to kill it and restart under nohup, you can send it to the background, then disown it.Ctrl+Z  (suspend the process)bg   (restart the process in the backgrounddisown %1 (assuming this is job #1, use jobs to determine)You could also use GNU screen which just about every Linux/Unix system should have. If you are on Ubuntu/Debian, its enhanced variant byobu is rather nice too.You might consider turning your python script into a proper python daemon, as described here.python-daemon is a good tool that can be used to run python scripts as a background daemon process rather than a forever running script. You will need to modify existing code a bit but its plain and simple. If you are facing problems with python-daemon, there is another utility supervisor that will do the same for you, but in this case you wont have to write any code (or modify existing) as this is a out of the box solution for daemonizing processes.You can nohup it, but I prefer screen.Here is a simple solution inside python using a decorator:You can of course replace the content of your bgservice.py file in place of my_func.The zsh shell has an option to make all background processes run with nohup.In ~/.zshrc add the lines:Then you just need to run a process like so: python bgservice.py &, and you no longer need to use the nohup command.I know not many people use zsh, but it's a really cool shell which I would recommend.If what you need is that the process should run forever no matter whether you are logged in or not, consider running the process as a daemon. supervisord is a great out of the box solution that can be used to daemonize any process. It has another controlling utility supervisorctl that can be used to monitor processes that are being run by supervisor. You don't have to write any extra code or modify existing scripts to make this work. Moreover, verbose documentation makes this process much simpler.After scratching my head for hours around python-daemon, supervisor is the solution that worked for me in minutes.Hope this helps someone trying to make python-daemon workYou can also use Yapdi:Basic usage:

How can you set class attributes from variable arguments (kwargs) in python

fijiaaron

[How can you set class attributes from variable arguments (kwargs) in python](https://stackoverflow.com/questions/8187082/how-can-you-set-class-attributes-from-variable-arguments-kwargs-in-python)

Suppose I have a class with a constructor (or other function) that takes a variable number of arguments and then sets them as class attributes conditionally.I could set them manually, but it seems that variable parameters are common enough in python that there should be a common idiom for doing this.  But I'm not sure how to do this dynamically.I have an example using eval, but that's hardly safe.  I want to know the proper way to do this -- maybe with lambda?

2011-11-18 18:18:24Z

Suppose I have a class with a constructor (or other function) that takes a variable number of arguments and then sets them as class attributes conditionally.I could set them manually, but it seems that variable parameters are common enough in python that there should be a common idiom for doing this.  But I'm not sure how to do this dynamically.I have an example using eval, but that's hardly safe.  I want to know the proper way to do this -- maybe with lambda?You could update the __dict__ attribute (which represents the class attributes in the form of a dictionary) with the keyword arguments:then you can:and with something like:you could filter the keys beforehand (use iteritems instead of items if you’re still using Python 2.x).You can use the setattr() method:There is an analogous getattr() method for retrieving attributes.Most answers here do not cover a good way to initialize all allowed attributes to just one default value.

So, to add to the answers given by @fqxp and @mmj:I propose a variation of fqxp's answer, which, in addition to allowed attributes, lets you set default values for attributes:This is Python 3.x code, for Python 2.x you need at least one adjustment, iteritems() in place of items().I called the class SymbolDict because it essentially is a dictionary that operates using symbols instead of strings.  In other words, you do x.foo instead of x['foo'] but under the covers it's really the same thing going on.The following solutions vars(self).update(kwargs) or self.__dict__.update(**kwargs) are not robust, because the user can enter any dictionary with no error messages. If I need to check that the user insert the following signature ('a1', 'a2', 'a3', 'a4', 'a5') the solution does not work. Moreover, the user should be able to use the object by passing the "positional parameters" or the "kay-value pairs parameters".So I suggest the following solution by using a metaclass.Their might be a better solution but what comes to mind for me is:this one is the easiest via larsksmy example:Yet another variant based on the excellent answers by mmj and fqxp. What if we want toBy "directly", I mean avoiding an extraneous default_attributes dictionary.Not a major breakthrough, but maybe useful to someone...EDIT:

If our class uses @property decorators to encapsulate "protected" attributes with getters and setters, and if we want to be able to set these properties with our constructor, we may want to expand the allowed_keys list with values from dir(self), as follows:The above code excludesThis edit is likely only valid for Python 3 and above.I suspect it might be better in most instances to use named args (for better self documenting code) so it might look something like this:

Accessing elements of Python dictionary by index

alessandra

[Accessing elements of Python dictionary by index](https://stackoverflow.com/questions/5404665/accessing-elements-of-python-dictionary-by-index)

Consider a dict likeHow do I access for instance a particular element of this dictionary?

for instance, I would like to print the first element after some formatting the first element of Apple which in our case is 'American' only?Additional information

The above data structure was created by parsing an input file in a python function. Once created however it remains the same for that run.I am using this data structure in my function.So if the file changes, the next time this application is run the contents of the file are different and hence the contents of this data structure will be different but the format would be the same.

So you see I in my function I don't know that the first element in Apple is 'American' or anything else so I can't directly use 'American' as a key.

2011-03-23 11:40:02Z

Consider a dict likeHow do I access for instance a particular element of this dictionary?

for instance, I would like to print the first element after some formatting the first element of Apple which in our case is 'American' only?Additional information

The above data structure was created by parsing an input file in a python function. Once created however it remains the same for that run.I am using this data structure in my function.So if the file changes, the next time this application is run the contents of the file are different and hence the contents of this data structure will be different but the format would be the same.

So you see I in my function I don't know that the first element in Apple is 'American' or anything else so I can't directly use 'American' as a key.Given that it is a dictionary you access it by using the keys. Getting the dictionary stored under "Apple", do the following:And getting how many of them are American (16), do like this:If the questions is, if I know that I have a dict of dicts that contains 'Apple' as a fruit and 'American' as a type of apple, I would use:as others suggested. If instead the questions is, you don't know whether 'Apple' as a fruit and 'American' as a type of 'Apple' exist when you read an arbitrary file into your dict of dict data structure, you could do something like:or better yet so you don't unnecessarily iterate over the entire dict of dicts if you know that only Apple has the type American:In all of these cases it doesn't matter what order the dictionaries actually store the entries. If you are really concerned about the order, then you might consider using an OrderedDict:http://docs.python.org/dev/library/collections.html#collections.OrderedDictAs I noticed your description, you just know that your parser will give you a dictionary that its values are dictionary too like this:So you have to iterate over your parent dictionary. If you want to print out or access all first dictionary keys in sampleDict.values() list, you may use something like this:If you want to just access first key of the first item in sampleDict.values(), this may be useful:If you use the example you gave in the question, I mean:The output for the first code is:And the output for the second code is:As a bonus, I'd like to offer kind of a different solution to your issue. You seem to be dealing with nested dictionaries, which is usually tedious, especially when you have to check for existence of an inner key.There are some interesting libraries regarding this on pypi, here is a quick search for you.In your specific case, dict_digger seems suited.You can use dict['Apple'].keys()[0] to get the first key in the Apple dictionary, but there's no guarantee that it will be American. The order of keys in a dictionary can change depending on the contents of the dictionary and the order the keys were added.I know this is 8 years old, but no one seems to have actually read and answered the question.You can call .values() on a dict to get a list of the inner dicts and thus access them by index. You can't rely on order on dictionaries. But you may try this:If you want the order to be preserved you may want to use this:

http://www.python.org/dev/peps/pep-0372/#ordered-dict-apiSimple Example to understand how to access elements in the dictionary:-Explore more about Python Dictionaries and learn interactively here...Few people appear, despite the many answers to this question, to have pointed out that dictionaries are un-ordered mappings, and so (until the blessing of insertion order with Python 3.7) the idea of the "first" entry in a dictionary literally made no sense. And even an OrderedDict can only be accessed by numerical index using such uglinesses as mydict[mydict.keys()[0]] (Python 2 only, since in Python 3 keys() is a non-subscriptable iterator.)From 3.7 onwards and in practice in 3,6 as well - the  new behaviour was introduced then, but not included as part of the language specification until 3.7 - iteration over the keys, values or items of a dict (and, I believe, a set also) will yield the least-recently inserted objects first. There is still no simple way to access them by numerical index of insertion.As to the question of selecting and "formatting" items, if you know the key you want to retrieve in the dictionary you would normally use the key as a subscript to retrieve it (my_var = mydict['Apple']).If you really do want to be able to index the items by entry number (ignoring the fact that a particular entry's number will change as insertions are made) then the appropriate structure would probably be a list of two-element tuples. Instead ofyou might use:Under this regime the first entry is mylist[0] in classic list-endexed form, and its value is ('Apple', {'American':'16', 'Mexican':10, 'Chinese':5}). You could iterate over the whole list as follows:but if you know you are looking for the key "Apple", why wouldn't you just use a dict instead?You could introduce an additional level of indirection by cacheing the list of keys, but the complexities of keeping two data structures in synchronisation would inevitably add to the complexity of your code.With the following small function, digging into a tree-shaped dictionary becomes quite easy:Now, dig(mydict, "Apple.Mexican") returns 10, while dig(mydict, "Grape") yields the subtree {'Arabian':'25','Indian':'20'}. If a key is not contained in the dictionary, dig returns None.Note that you can easily change (or even parameterize) the separator char from '.' to '/', '|' etc.

Why is an MD5 hash created by Python different from one created using echo and md5sum in the shell?

mailGO

[Why is an MD5 hash created by Python different from one created using echo and md5sum in the shell?](https://stackoverflow.com/questions/5693360/why-is-an-md5-hash-created-by-python-different-from-one-created-using-echo-and-m)

A Python MD5 hash is different than the one created by the md5sum command on the shell. Why?

2011-04-17 11:54:05Z

A Python MD5 hash is different than the one created by the md5sum command on the shell. Why?echo appends a \n since you usually do not want lines not ending with a linebreak in your shell (it looks really ugly if the prompt does not start at the very left).

Use the -n argument to omit the trailing linebreak and it will print the same checksum as your python script:

Explicitly select items from a list or tuple

Kit

[Explicitly select items from a list or tuple](https://stackoverflow.com/questions/6632188/explicitly-select-items-from-a-list-or-tuple)

I have the following Python list (can also be a tuple):I can sayHow do I explicitly pick out items whose indices have no specific patterns? For example, I want to select [0,2,3]. Or from a very big list of 1000 items, I want to select [87, 342, 217, 998, 500]. Is there some Python syntax that does that? Something that looks like:

2011-07-09 01:49:10Z

I have the following Python list (can also be a tuple):I can sayHow do I explicitly pick out items whose indices have no specific patterns? For example, I want to select [0,2,3]. Or from a very big list of 1000 items, I want to select [87, 342, 217, 998, 500]. Is there some Python syntax that does that? Something that looks like:I compared the answers with python 2.5.2:Note that in Python 3, the 1st was changed to be the same as the 4th.Another option would be to start out with a numpy.array which allows indexing via a list or a numpy.array:The tuple doesn't work the same way as those are slices.What about this:It isn't built-in, but you can make a subclass of list that takes tuples as "indexes" if you'd like:printingMaybe a list comprehension is in order:Produces:Is that what you are looking for?You can also create your own List class which supports tuples as arguments to __getitem__ if you want to be able to do myList[(2,2,1,3)].I just want to point out, even syntax of itemgetter looks really neat, but it's kinda slow when perform on large list.Itemgetter took  1.065209062149279Multiple slice took  0.6225321444745759Another possible solution:like often when you have a boolean numpy array like mask[mylist[i] for i in np.arange(len(mask), dtype=int)[mask]]A lambda that works for any sequence or np.array:subseq = lambda myseq, mask : [myseq[i] for i in np.arange(len(mask), dtype=int)[mask]]newseq = subseq(myseq, mask)

How Pony (ORM) does its tricks?

Paulo Scardine

[How Pony (ORM) does its tricks?](https://stackoverflow.com/questions/16115713/how-pony-orm-does-its-tricks)

Pony ORM does the nice trick of converting a generator expression into SQL. Example:I know Python has wonderful introspection and metaprogramming builtin, but how this library is able to translate the generator expression without preprocessing? It looks like magic.[update]Blender wrote:I was thinking they were exploring some feature from the generator expression protocol, but looking this file, and seeing the ast module involved... No, they are not inspecting the program source on the fly, are they? Mind-blowing...@BrenBarn: If I try to call the generator outside the select function call, the result is:Seems like they are doing more arcane incantations like inspecting the select function call and processing the Python abstract syntax grammar tree on the fly.I still would like to see someone explaining it, the source is way beyond my wizardry level.

2013-04-20 01:53:15Z

Pony ORM does the nice trick of converting a generator expression into SQL. Example:I know Python has wonderful introspection and metaprogramming builtin, but how this library is able to translate the generator expression without preprocessing? It looks like magic.[update]Blender wrote:I was thinking they were exploring some feature from the generator expression protocol, but looking this file, and seeing the ast module involved... No, they are not inspecting the program source on the fly, are they? Mind-blowing...@BrenBarn: If I try to call the generator outside the select function call, the result is:Seems like they are doing more arcane incantations like inspecting the select function call and processing the Python abstract syntax grammar tree on the fly.I still would like to see someone explaining it, the source is way beyond my wizardry level.Pony ORM author is here.Pony translates Python generator into SQL query in three steps:The most complex part is the second step, where Pony must

understand the "meaning" of Python expressions. Seems you are most

interested in the first step, so let me explain how decompiling works.Let's consider this query:Which will be translated into the following SQL:And below is the result of this query which will be printed out:The select() function accepts a python generator as argument, and then analyzes its bytecode.

We can get bytecode instructions of this generator using standard python dis module:Pony ORM has the function decompile() within module pony.orm.decompiling which can

restore an AST from the bytecode:Here, we can see the textual representation of the AST nodes:Let's now see how the decompile() function works.The decompile() function creates a Decompiler object, which implements the Visitor pattern.

The decompiler instance gets bytecode instructions one-by-one.

For each instruction the decompiler object calls its own method.

The name of this method is equal to the name of current bytecode instruction.When Python calculates an expression, it uses stack, which stores an intermediate

result of calculation. The decompiler object also has its own stack,

but this stack stores not the result of expression calculation,

but AST node for the expression.When decompiler method for the next bytecode instruction is called,

it takes AST nodes from the stack, combines them

into a new AST node, and then puts this node on the top of the stack.For example, let's see how the subexpression c.country == 'USA' is calculated. The

corresponding bytecode fragment is:So, the decompiler object does the following:After all bytecode instructions are processed, the decompiler stack contains

a single AST node which corresponds to the whole generator expression.Since Pony ORM needs to decompile generators 

and lambdas only, this is not that complex, because

the instruction flow for a generator is relatively straightforward

- it is just a bunch of nested loops.Currently Pony ORM covers the whole generator instructions set except two things:If Pony encounters such expression it raises the NotImplementedError exception. But even in 

this case you can make it work by passing the generator expression as a string.

When you pass a generator as a string Pony doesn't use the decompiler module. Instead

it gets the AST using the standard Python compiler.parse function.Hope this answers your question.

How do you do a simple「chmod +x」from within python?

priestc

[How do you do a simple「chmod +x」from within python?](https://stackoverflow.com/questions/12791997/how-do-you-do-a-simple-chmod-x-from-within-python)

I want to create a file from within a python script that is executable.it appears os.chmod doesn't 'add' permissions the way unix chmod does. With the last line commented out, the file has the filemode -rw-r--r--, with it not commented out, the file mode is ---x------. How can I just add the u+x flag while keeping the rest of the modes intact?

2012-10-09 02:18:47Z

I want to create a file from within a python script that is executable.it appears os.chmod doesn't 'add' permissions the way unix chmod does. With the last line commented out, the file has the filemode -rw-r--r--, with it not commented out, the file mode is ---x------. How can I just add the u+x flag while keeping the rest of the modes intact?Use os.stat() to get the current permissions, use | to or the bits together, and use os.chmod() to set the updated permissions.Example:For tools that generate executable files (e.g. scripts), the following code might be helpful:This makes it (more or less) respect the umask that was in effect when the file was created: Executable is only set for those that can read.Usage:If you know the permissions you want then the following example may be the way to keep it simple.Python 2:Python 3:Compatible with either (octal conversion):reference permissions examplesYou can also do thisCurrent listing of fileNow do this.and you will see this in the terminal.You can bitwise or with 0o111 to make all executable, 0o222 to make all writable, and 0o444 to make all readable.  Respect umask like chmod +xman chmod says that if augo is not given as in:then a is used but with umask:Here is a version that simulates that behavior exactly:See also: How can I get the default file permissions in Python?Tested in Ubuntu 16.04, Python 3.5.2.In python3:Remember to add the 0o prefix since permissions are set as an octal integer, and Python automatically treats any integer with a leading zero as octal. Otherwise, you are passing os.chmod("somefile", 1230) indeed, which is octal of 664.If you're using Python 3.4+, you can use the standard library's convenient pathlib. Its Path class has built-in chmod and stat methods.

How does the class_weight parameter in scikit-learn work?

kilgoretrout

[How does the class_weight parameter in scikit-learn work?](https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work)

I am having a lot of trouble understanding how the class_weight parameter in scikit-learn's Logistic Regression operates.The SituationI want to use logistic regression to do binary classification on a very unbalanced data set. The classes are labelled 0 (negative) and 1 (positive) and the observed data is in a ratio of about 19:1 with the majority of samples having negative outcome.First Attempt: Manually Preparing Training DataI split the data I had into disjoint sets for training and testing (about 80/20). Then I randomly sampled the training data by hand to get training data in different proportions than 19:1; from 2:1 -> 16:1.I then trained logistic regression on these different training data subsets and plotted recall (= TP/(TP+FN)) as a function of the different training proportions. Of course, the recall was computed on the disjoint TEST samples which had the observed proportions of 19:1. Note, although I trained the different models on different training data, I computed recall for all of them on the same (disjoint) test data.The results were as expected: the recall was about 60% at 2:1 training proportions and fell off rather fast by the time it got to 16:1. There were several proportions 2:1 -> 6:1 where the recall was decently above 5%.Second Attempt: Grid SearchNext, I wanted to test different regularization parameters and so I used GridSearchCV and made a grid of several values of the C parameter as well as the class_weight parameter. To translate my n:m proportions of negative:positive training samples into the dictionary language of class_weight I thought that I just specify several dictionaries as follows:and I also included None and auto.This time the results were totally wacked. All my recalls came out tiny (< 0.05) for every value of class_weight except auto. So I can only assume that my understanding of how to set the class_weight dictionary is wrong. Interestingly, the class_weight value of 'auto' in the grid search was around 59% for all values of C, and I guessed it balances to 1:1?My Questions

2015-06-22 04:11:59Z

I am having a lot of trouble understanding how the class_weight parameter in scikit-learn's Logistic Regression operates.The SituationI want to use logistic regression to do binary classification on a very unbalanced data set. The classes are labelled 0 (negative) and 1 (positive) and the observed data is in a ratio of about 19:1 with the majority of samples having negative outcome.First Attempt: Manually Preparing Training DataI split the data I had into disjoint sets for training and testing (about 80/20). Then I randomly sampled the training data by hand to get training data in different proportions than 19:1; from 2:1 -> 16:1.I then trained logistic regression on these different training data subsets and plotted recall (= TP/(TP+FN)) as a function of the different training proportions. Of course, the recall was computed on the disjoint TEST samples which had the observed proportions of 19:1. Note, although I trained the different models on different training data, I computed recall for all of them on the same (disjoint) test data.The results were as expected: the recall was about 60% at 2:1 training proportions and fell off rather fast by the time it got to 16:1. There were several proportions 2:1 -> 6:1 where the recall was decently above 5%.Second Attempt: Grid SearchNext, I wanted to test different regularization parameters and so I used GridSearchCV and made a grid of several values of the C parameter as well as the class_weight parameter. To translate my n:m proportions of negative:positive training samples into the dictionary language of class_weight I thought that I just specify several dictionaries as follows:and I also included None and auto.This time the results were totally wacked. All my recalls came out tiny (< 0.05) for every value of class_weight except auto. So I can only assume that my understanding of how to set the class_weight dictionary is wrong. Interestingly, the class_weight value of 'auto' in the grid search was around 59% for all values of C, and I guessed it balances to 1:1?My QuestionsFirst off, it might not be good to just go by recall alone. You can simply achieve a recall of 100% by classifying everything as the positive class.

I usually suggest using AUC for selecting parameters, and then finding a threshold for the operating point (say a given precision level) that you are interested in.For how class_weight works: It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1. So higher class-weight means you want to put more emphasis on a class. From what you say it seems class 0 is 19 times more frequent than class 1. So you should increase the class_weight of class 1 relative to class 0, say {0:.1, 1:.9}.

If the class_weight doesn't sum to 1, it will basically change the regularization parameter.For how class_weight="auto" works, you can have a look at this discussion.

In the dev version you can use class_weight="balanced", which is easier to understand: it basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.

How can I make an EXE file from a Python program? [duplicate]

minty

[How can I make an EXE file from a Python program? [duplicate]](https://stackoverflow.com/questions/49146/how-can-i-make-an-exe-file-from-a-python-program)

I've used several modules to make EXEs for Python, but I'm not sure if I'm doing it right.How should I go about this, and why?  Please base your answers on personal experience, and provide references where necessary.

2008-09-08 03:59:57Z

I've used several modules to make EXEs for Python, but I'm not sure if I'm doing it right.How should I go about this, and why?  Please base your answers on personal experience, and provide references where necessary.Auto PY to EXE  - A .py to .exe converter using a simple graphical interface built using Eel and PyInstaller in Python.py2exe is probably what you want, but it only works on Windows.

PyInstaller works on Windows and Linux.

Py2app works on the Mac.I found this presentation to be very helpfull.How I Distribute Python applications on Windows - py2exe & InnoSetupFrom the site:Also known as Frozen Binaries but not the same as as the output of a true compiler- they run byte code through a virtual machine (PVM). Run the same as a compiled program just larger because the program is being compiled along with the PVM. Py2exe can freeze standalone programs that use the tkinter, PMW, wxPython, and PyGTK GUI libraties; programs that use the pygame game programming toolkit; win32com client programs; and more. 

The Stackless Python system is a standard CPython implementation variant that does not save state on the C language call stack. This makes Python more easy to port to small stack architectures, provides efficient multiprocessing options, and fosters novel programming structures such as coroutines. Other systems of study that are working on future development: Pyrex is working on the Cython system, the Parrot project, the PyPy is working on replacing the PVM altogether, and of course the founder of Python is working with Google to get Python to run 5 times faster than C with the Unladen Swallow project. In short, py2exe is the easiest and Cython is more efficient for now until these projects improve the Python Virtual Machine (PVM) for standalone files.Not on the freehackers list is gui2exe which can be used to build standalone Windows executables, Linux applications and Mac OS application bundles and plugins starting from Python scripts.Use cx_Freeze to make exe your python programpy2exe:See a short list of python packaging tools on FreeHackers.org.

Why is f'{{{74}}}' the same as f'{{74}}' with f-Strings?

fedorqui 'SO stop harming'

[Why is f'{{{74}}}' the same as f'{{74}}' with f-Strings?](https://stackoverflow.com/questions/59359911/why-is-f74-the-same-as-f74-with-f-strings)

f-Strings are available from Python 3.6 and are very useful for formatting strings:Reading more about them in Python 3's f-Strings: An Improved String Formatting Syntax (Guide). I found an interesting pattern:And this is exactly the case:Now if we pass from two { to three, the result is the same:So we need up to 4! ({{{{) to get two braces as an output:Why is this? What happens with two braces to have Python require an extra one from that moment on?

